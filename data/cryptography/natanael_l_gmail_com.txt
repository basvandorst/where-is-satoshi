
@_date: 2013-12-26 22:11:59
@_author: Natanael 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Why do you think Bitcoin is a ponzi? It rather mimics the mining process of
precious metals in digital form. (There is also no need for it to go up and
up forever. And I'd like to note that I actually have bought things with
And regarding Ripple,  that system relies entirely on mutual trust in
between a set of servers. It can not scale and at the same time ensure a
high level of (well founded) trust. It's a weird form of web of trust,
where the users have very little say in the process.
IMHO the best choice would be a P2P system ("federation" is OK, no need for
most users to run full nodes tracking all public conversations ever posted)
where each post simply references the previous posts seen by that person
(maybe with a Merkle tree hash), where the current state occasionally is
synced to the blockchain in the form of a checksum (for timestamping and
tamper proofing). I don't yet have any particular opinion on how the
details should work.
(And I do not like the design of Bitmessage either, I doubt it can scale.
There is no need to force every peer to keep everything in the chain or to
keep it up to date 24/7.)
Bitcoin SPV nodes are an interesting comparison, they rely on servers
hosting the full chain, themselves they just hold the headers and the full
blocks that apply to themselves. And the servers are fundamentally Bitcoin
nodes like any other (except with an additional API layer for the SPV
clients to use, like Stratum).
I like Bote mail in I2P (DHT based). Syndie is another interesting piece of
software ("syndication based"), but it doesn't seem to perform very well
and doesn't seem all to reliable.
- Sent from my phone
Den 26 dec 2013 19:43 skrev "Bill Cox" :

@_date: 2013-12-27 20:42:29
@_author: Natanael 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Some more off topic about Bitcoin, jump below to the line to see the
security related stuff.
 Everybody is expecting Bitcoin to stop *somewhere*. It's obvious it
can't rise forever. But where it will stop is not something anybody can say
with certainty. But I can say this: I personally believe (I'm aware I might
have judged it wrong, but I feel fairly certain, feel free to argue against
me if you believe you've found an error) has better economical properties
than gold (primarily that this has provable scarcity, is easier to move and
verify, and even is programmable money). For the entire pool of bitcoins to
reach just 1/10 of the value of the entire pool of gold, it still has a
long way to go. But I don't know with certainty if it ever will, and
neither does anybody else, but I personally think it will.
Of course it looks strange and seems like a scam when it rises so fast and
there's people who pushes for it so strongly to practically everybody on
the outside, it would look like a pump and dump to most people, and lots of
the strong believers are also a bit wary (looking at the charts can often
make me feel uncomfortable), but *Bitcoin the protocol* as such essentially
behaves like digital gold and is as such not a scam. If you think most of
the price is driven by pump-and-dumpers, then it's only reasonable to stay
away (just like how any small stock could be manipulated by somebody other
than the company it represents ownership shares for, where the stock isn't
a scam but somebody is using it as a tool for scamming others). I don't
believe it's being controlled by scammers (I can not prove it though), and
I have no idea if it's rising faster than sustainable or not.
 Why not just use timing alone? If you make the device add "random"
delays that look natural on the network, then you could encode data in the
timing differences. The data would be hidden in the timing noise and could
even require a key to decode so that even somebody who knows of the timing
encoding scheme can't decode it without the key.
- Sent from my phone
Den 27 dec 2013 17:57 skrev "ianG" :

@_date: 2013-12-28 17:39:22
@_author: Natanael 
@_subject: [Cryptography] deniable symmetric ciphers? 
Den 28 dec 2013 16:48 skrev "Paul Elliott" :
What kind of deniability are you looking for?
There are perfect forward secrecy/security for communication protocols (OTR
for chat, ECDHE key exchange for SSL), and for file storage there is
Truecrypt hidden volumes. These can all to some degree protect against an
attacker figuring out what the data is (by different means).
- Sent from my phone

@_date: 2013-12-28 22:53:39
@_author: Natanael 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Den 28 dec 2013 22:28 skrev "Jerry Leichter" :
supports all the different messaging modes with security built in rather
than working out how to back-fit security into each legacy protocol
that divides mechanisms with fundamentally different characteristics.
 Synchronous communication can have perfect forward security; asynchronous
communications cannot.
our descriptions so that we fail to capture the nature of this distinction.
 It feels as if there should be a continuum here, where you get full PFS
for communications with an arbitrarily short lifetime, degenerating into
the usual more limited guarantees for things that are stored long term.
 And I suppose you could come up with a simple theory along that line,
where you need to retain keying material only as long as some message isn't
delivered.  But this seems very forced and unnatural.  I think we're
missing something.
Moxie is trying to fix that, have you seen the "axolotl" ratcheting scheme
(not sure on the spelling) that he and another guy developed, with the
intent to establish PFS like security for asynchronous communication? I'd
say it's something more like a very long latency version of regular PFS.
The session keys becomes short term secrets instead.

@_date: 2014-04-02 22:20:03
@_author: Natanael 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Den 2 apr 2014 04:08 skrev "Sampo Syreeni" :
be how you picked the starting point and other details.
first optimal construction of a universal code Levenstein's, and it always
starts from one, so that you can't repeatedly extract zero bits and by that
way get around the idea that you shouldn't be able to precisely choose
where you start your extraction.
ostensibly a novel algorithm needing low-sleeveness numbers might even come
from outside the known mainstream, I can't think of any lower complexity
protocol which just lets you extract arbitrary amounts of nice bits from a
commonly trusted source, than this one, combined with some kind of
plausible enough notarization to establish precedence in case of dispute
over who owns the currently-considered-to-be-spent-bits.
Here's my take on it (might be unnecessarily complicated);
You use one of those Pi digits calculation methods in an algorithm that
decides the depth into the decimals of Pi to calculate the numbers at from
the input number. This can intentionally be computationally heavy ? la
scrypt and bcrypt. So given a unique input with a source that is verifiably
hard to influence you have a (most likely) unique output that is very hard
to influence and no mathematical structure you can influence.
Now, how to calculate the input to it:
You can use a commitment hash of a random value that you publish in the
Bitcoin blockchain together with a number that specifies which future
block's hash to use. This can be a fixed number, like 12 (with a block time
of 10min that's 2h in the future on average). The commitment can both
include a random number and reference the context it is used in (hash of
session key, or time + name of your project) to prevent undetectable reuse
and guarantee uniqueness for each purpose.
When that block is reached, you combine the committed value (including it's
referenced context to prevent reuse), the block hash and the public key
that was used for the commitment transaction. The combination method can be
a hash of all inputs or XOR.
That means the chosen depth is different for every user, is different for
every use, is verifiable and extremely difficult to influence.
Influencing the block hash (in competition with plenty of miners with
SHA256 ASICs) + your committed value + your public key to result in a weak
output by the Pi decimals calculation would require computational power
that by far exceeds the entire Bitcoin mining network many times over -
which itself outperforms the entire top 100 (public) supercomputers
together (on hash calculations)!
You can also directly detect in the blockchain if somebody attempts to
reuse the same input values for something else.
The depth into the Pi decimals is also very likely to be unique, and thus
to provide a unique output. (There's your coordination to not reuse
numbers, the blockchain is global, and also works perfectly fine as a
In case the numbers in Pi would be shown to have an exploitable structure
for your use case, you can switch constant. In case it would still be
unique enough and you have a trusted hash function, you can simply just use
that hash on the output.

@_date: 2014-04-03 02:14:32
@_author: Natanael 
@_subject: [Cryptography] Clever physical 2nd-factor authentication 
Den 3 apr 2014 01:42 skrev "Joseph Ashwood" :
right now.
already activated and printed on the card. So shorten the process further,
each of the middle displays shares the left and right segments (2 per side)
and the end displays share one side each (2 segments).
There are transparent displays. Couldn't you make this into an OTP card by
changing the mask every time?
Just being a mask rather than allowing inversion limits possible entropy
per pixel, but with enough pixels you can do just fine by using visual
cryptography (we don't need XOR where we are going!);
The GIF example clearly shows how one transparent sheet with one share held
over another share can make any pattern you wish visible and readable,
while the individual shares (the image on the computer and the mask on the
device) separately reveals nothing.
Alignment would be annoying (or frustrating), though.
Usefulness of this scheme? Maybe that over-the-phone I-lost-my-token
phishing scams gets harder for the scammers, no code to read over the phone
for them to enter.
Still doesn't block ones where the target can be talked over into
cooperating, but since the server can display any graphics it wants to the
user which can't be tampered with (or else the user can't read the OTP
code!), the server can display all the details about the current action. So
even if the computer is remote controlled and the rest of the screen is
displaying fake info, the target might be alerted and cancel the whole
Although we all know how well average users respond to warnings. Just see
the responses to the recent Windows XP end-of-life alerts.

@_date: 2014-04-12 17:55:34
@_author: Natanael 
@_subject: [Cryptography] Preliminary review of the other Applied 
Den 12 apr 2014 06:52 skrev :
carry the cryptographic keys to access the domains it offers, securely?
Couldn't the public keys be embedded in its blockchain?
You can, since it is flexible in what you can store. If you directly rely
on public key based addressing like with Tor, I2P, CJDNS and more, then the
verification is automatic - the connection will only succeed if the server
has the right private key.
For other systems, you need to create new plugins that do the verification
at some level where the client software can understand the result. Usually
that means you have to proxy most software and often also have the proxy
confirm the certificates in a MITM fashion.

@_date: 2014-08-05 10:23:02
@_author: Natanael 
@_subject: [Cryptography] You can't trust any of your hardware 
Den 4 aug 2014 19:33 skrev "Bear" :
[... ]
Others have mentioned the idea of a USB firewall before. Now I'm thinking
of taking an FPGA and programming it to work as a USB hub, but filtering
out anything not following a predefined set of protocols (USB HID with
limitation to keyboard keys & mouse input, etc).
You could even go further and perform fingerprinting on your whitelisted
devices and refuse anything not exactly matching the fingerprint (timing,
behavior, etc). Potentially you could then be able to reject other devices
of the same model (if the variance is measurable) or anything with modified
There is already FPGA circuit designs that copy a full computer (although
using relatively simple circuits as you'd need a huge FPGA to copy an Intel
i7 with motherboard), with an open source CPU design and USB hub, etc. One
could start from there to create something that can do USB filtering
(although this approach would add latency, potentially noticeable; a more
integrated approach would perform better but is more complex).

@_date: 2014-08-11 22:45:37
@_author: Natanael 
@_subject: [Cryptography] Dumb question -> 3AES? 
Den 11 aug 2014 19:59 skrev "Dan McDonald" :
It is possible, but maybe won't have the effect you're looking for. The
reason for going with 3DES was that DES was strong for its keylength, but
the keylength and thus keyspace was too small = crackable with bruteforce.
Since it wasn't considered likely to fall to cryptoanalysis in a relatively
short period of time, and people demanded something compatible with
existing hardware implementations, 3DES was created rather than ditching
DES for another cipher. 3DES provided acceptable strength without massive
And what about AES? It has a far greater strength, even with AES128 which
actually even have a smaller keyspace (3DES has 3*56 =168 bit keys but 112
bit strength due to attacks like meet-in-the-middle, but AES128 has close
to 128 bits in strength). And AES256 just isn't close to being cracked EVER
unless there's a massive cryptographic flaw found. So what is the worry?
Remember that it is because DES with 56 bit keys provides close to 56 bits
of security even still today that 3DES was considered strong enough.
However, a potential break in AES would not necessarily "just" reduce the
effective strength to above 50 bits per layer of AES (providing a similar
strength for 3AES which 3DES does today), it could trash it completely.
Which means your long term encryption failed.
So 3AES isn't good enough for hedging against flaws found in AES in the
future, unless your ONLY worry is for a PARTIAL reduction in strength. If
there's a total break, 3AES won't help you.
A much better option, if your plan is too hedge against individual
algorithms being cracked, is to use several different types of ciphers.
AES, blowfish, some stream cipher like ChaCha, or whatever else. Even if
you still don't get much more strength in total than the single strongest
cipher provides, all you need is for just one of them to not get broken (or
for the layers of encryption to make known-plaintext attacks impossible in
the worst case scenario where they all break).

@_date: 2014-08-18 11:16:22
@_author: Natanael 
@_subject: [Cryptography] Adapting AES for arbitrarily large keys 
- Sent from my tablet
Den 18 aug 2014 05:24 skrev "Marc W. Abel" :
128-, 192-, and 256-bit keys into key schedules totalling 1408, 1664, and
1792 bits.
AES-1408, AES-1664, and AES-1792, with keys of this size being directly
generated and used, and no subkey expansion at all?  And if not weaker,
would the resulting cipher continue to strengthen as expected every time
another round and 128 key bits is tacked on? (AES-65536, AES-1048576, etc.?)
(Disclaimer: I'm not a cryptographer and this is from memory. Feel free to
correct me if I'm wrong.)
I'm trying to find the source but can't, but IIRC in one of the recent
discussions on AES or DES on one of these crypto mailing lists somebody
asked something similar, and the reply was that the individual rounds do
not contribute as much bit-strength security as the round key has bits, no
matter the key entropy. Because each transformation / permutation isn't
significantly complex by itself. The strength of the full algorithm depends
on the combination of all the rounds, and given a proper key schedule you
just need to feed the cipher with a key with enough entropy to make it
strong enough.
But it wouldn't be *weaker* if each round is supposed to have a fully
(pseudo)random key without any special properties. Although if the cipher
rounds WOULD be designed to require special properties which the key
schedule provides, fully random round keys could weaken it (doesn't seem
like this would be a common design goal).
Define "tacked on", do you mean several layers of encryption? It wouldn't
be more secure than regular AES in layers. Also note that you generally
don't multiply the strength when combining layers of ciphers, you usually
just add it up. 2xAES256 doesn't equal AES512. Look up meet-in-the-middle

@_date: 2014-12-12 05:56:26
@_author: Natanael 
@_subject: [Cryptography] Cipher having a universal polymorphic-decryption 
- Sent from my tablet
Den 12 dec 2014 04:46 skrev "Ray Dillinger" :
[Crazier than usual version of one time pad, built with a block cipher &
CSPRNG/semi-stream cipher hybrid]
You can also make multiple people believe they got the same message when
they didn't.

@_date: 2014-12-28 10:07:28
@_author: Natanael 
@_subject: [Cryptography] General security infrastructure suggestion: 
Den 28 dec 2014 04:59 skrev "Ray Dillinger" :
Capabilities based OS design.
You can find helpful pointers in  and
 (IIRC). Many capabilities based systems
use cryptographic tokens to control access and layers of encryption, such
that even an exploit to gain access to memory it shouldn't have won't give
it access to either reading privileged plaintext or modifying it.
Android does this to some degree, but not fully. It just isolates apps in
sandboxes and then offers a number of standardized channels for
communicating with each other, and there's no granular access controls so
far for your personal data regarding reading (kitkat introduced restricted
write access to the SD card, but not for the internally stored files).
IMHO what should be done is that everything run under a user account should
be run in isolated sandboxes, only giving access for communication between
sandboxes when both programs agree or the user says so. One of the greater
benefits for this is that user level malware can't then tamper with your
terminal when you run a command with sudo, so it can't perform privilege
escalation by piggybacking legitimate software silently (today the terminal
is often unprotected software, there's no way to launch it such that you
can protect it from malicious code).
And all software should eventually be redesigned to support running
sandboxed, to support communicating mainly via limited channels, and so on.
And file access and system privileges and user privileges would have to be
requested and assigned individually to the sandboxed software. Inside user
accounts, it really should be "sandboxes all the way down", as a
configuration screw-up putting malware in control should allow you to
access the account in a "safe mode" to reset permissions and clear it
malware. There shouldn't simply be "inside app sandboxes" and "outside app
sandboxes" within accounts.
A sandboxed version of KDE's workspaces would be even better, this would go
much further than Qubes OS. You'd have multiple completely isolated
desktops with different configurations, one for work, one for browsing,
etc, and software within one wouldn't even know of the other workspaces.
Different folders in your account would only be accessible from certain
workspaces to even further limit the what malware can do.
Also note that Android's runtime ties apps to their sandboxes sandboxes in
a way which allows other apps to verify them by their signature and package
name. This means that apps from developers which trust each other can give
automatic access to certain functionality in between the apps. An office
suite wouldn't need to distribute all apps together, yet they could be
allowed access to each other's private data as you can be sure they aren't
malicious. Just check that the sandbox you get a request from belongs to
program X with the correct signature, and no malicious software can tamper
with it (unless program X has an exploit in how it handles inputs, though).

@_date: 2014-02-14 00:40:13
@_author: Natanael 
@_subject: [Cryptography] BitCoin bug reported 
Den 13 feb 2014 23:39 skrev "Phillip Hallam-Baker" :
bug in the BitCoin protocol:
credible explanation?
that we can't trust the government but take great offense and try to bully
anyone who asks questions about the scheme.
of the names involved in the BitCoin world are people that I know I can;t
trust and neither can anyone else. They have lied to and cheated me, they
have lied to and cheated others.
This is way beyond a science project. If it continues then in a few years
BitCoin will be taking all the electricity generated by the Three Gorges
Dam project. Which is why I suspect China will soon be introducing condign
punishments for mining.
interesting. Since the upper limit on the value of a bitcoin is set by the
cost of electricity to mine it, the value increases as the difficulty of
mining increases. Since that is exponential, there is a built in bubble
Bitcoin handles transactions in the form of inputs and outputs, in a way
that can be compared to taking pieces of gold that previously had been
given to you (inputs), melting it and splitting it into new parts
(outputs). Every input in transactions you make is the output from a
previous one. Every transaction has a hash value that is used to identify
Mt Gox tracked transactions primarily on the hash.
You can modify some parts of the transactions that isn't part of what is
signed, that doesn't change what the transaction does, but that changes the
So some transactions didn't have the expected hash that Mt Gox were
sending, when they ended up in the blockchain, because somebody was
rebroadcasting modified versions.
So somebody got their coins from Mt Gox, told their support they didn't get
them while pointing at the original hash, and Mt Gox saw that original
transaction hash wasn't in the blockchain, and sent coins again. So the
attacker got their coins twice.
The sane services tracks spent outputs. The modified transactions can still
be identified as spending the same outputs with the same amounts to the
same recipients, proving the recipient got the coins. They would not get
fooled by a transaction hash changing.
On a sidenote, if you ever want to resend coins, then claim the same
outputs as you did before, that way only one of the transactions can get
verified in the blockchain. Otherwise the attacker can rebroadcast the
origin transaction, and it would still be valid. That's why you ALWAYS
should track outputs.

@_date: 2014-02-16 02:42:15
@_author: Natanael 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
Den 16 feb 2014 02:12 skrev "Phillip Hallam-Baker" :
all. As Ben Laurie points out, it is wading through treacle.
scheme. Forget the mining part for a moment, lets imagine that there are
100 independent notaries and every ten minutes they each produce an output
value that is based on local inputs plus the outputs of all the other
other notary also defects.
acceptable. It is possible for someone to check if they have finality in a
maintained by parties that form a consortium for that purpose.
to whoever gets the lowest hash output in a given computation chunk and has
the hash notarized in time.
Sounds like Ripple. Which have asset issuing "gateways", and all the
servers check the transactions in batches with the other servers it trusts.
I don't like that system myself. Relies far too much on trust. And systems
like it far to often don't have a good way of reacting to errors.
Also, a 51% attack when there are tons of idle miners will be fought off in
hours or even minutes simply by turning on the miners. It is likely more
economical to pay for the extra electricity than to pay through the loss of
value off the coins.

@_date: 2014-01-11 00:57:21
@_author: Natanael 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
Den 11 jan 2014 00:23 skrev "Bill Cox" :
You just put your trust in that the FPGA isn't backdoored. There's been
backdoored FPGAs before, plenty of times. Secure storage of keys require
custom hardware as well, an FPGA is just a computational device in itself.
You need a smartcard or TPM style chip.
Maybe you want an open source HSM?

@_date: 2014-01-11 11:06:08
@_author: Natanael 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
Den 11 jan 2014 08:44 skrev "grarpamp" :
What do you guys think of a device like this?
You could add capacitive touch, and then you have a very simple and cheap
device that can do basic crypto.

@_date: 2014-01-19 09:22:55
@_author: Natanael 
@_subject: [Cryptography] HSM's 
Den 19 jan 2014 06:43 skrev "Jerry Leichter" :
trust. If I built it I would feel different, but you should be
uncomfortable using my HSM. Getting mutually suspicious people to trust the
same HSM is an interesting social/technical problem.
good properties of HSM's (potential for a very small attack surface)
without the bad ones (you either have to trust a sealed box that someone
else built, or be willing to create it yourself from scratch)?  If you look
at this as analogous to network routing - where there is great utility in
using the large number of "black box" routers out there to communicate,
even if you don't trust any of them - then something akin to the
construction of a mix suggests itself.  That is, could you define a
standard interface to an HSM with the property that it's "securely
composable":  You can combine a bunch of HSM's and get something with the
same interface, but such that as long as at least one of the HSM's lives up
to its security properties, the whole ensemble does?  Can you, under some
assumption like "each box may be separately cheating, but they don't
cooperate" get something even stronger?
some work to actually formalize (a) the relevant security properties; (b)
the HSM interface; and only then (c) the proof that what you have is,
indeed, securely composable.
Wouldn't that simply be a matter of using algorithms like Secure Multiparty
Computation among a number of devices that has a shares of a key split
among them using something like Shamir's Secure Sharing Scheme?
- Sent from my phone

@_date: 2014-01-22 00:48:52
@_author: Natanael 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Den 22 jan 2014 00:32 skrev "Jerry Leichter" :
nomenclature digital signature systems are an inherently pubkey system.
way:  With a signature system, Bob can prove to anyone that a message was
signed by Alice without himself being able to produce messages with Alice's
signature on them.  With a MAC, Bob has everything needed to produce
messages "MAC'ed" by Alice.  But that's fine, because the entire purpose of
a MAC is for Bob to be able to prove *to himself* that Alice produced a
message.  There's not much point in him forging a message and then proving
to himself that he forged it!
system distinction, it's not quite the same thing.  DSA does signatures,
but doesn't in and of itself provide an asymmetric encryption system.  And
while it's much less convenient and requires a trusted third party, you can
construct a signature-like system using only symmetric primitives:  The
trusted third party holds the actual MAC key and will apply it for message
creation only for Alice, but for anyone for message verification.  (Alice's
messages to the trusted third party are MAC'ed using a key known only to
the two of them; the TTP can forge messages from Alice, but we assume that
away because it's *trusted*.  Similarly the TTP shares a unique key with
anyone who might want a signature verification done.  Bob still can't prove
to anyone else that the message was from Alice - but he can point anyone at
the TTP to do it for him.)
You can do signatures directly with symmetric primitives like hashes.
See Lamport signatures (commit to 2*[signature hash bitlength] hashes that
are numbered, reveal one per pair chosen depending on if the corresponding
signature hash bits are 1 or 0) and the Fawkes signature scheme (commit to
a codeword and timestamp it, commit to a message that also reveals the
codeword & timestamp, and optionally commits to a new codeword, then reveal
that message.)
- Sent from my phone

@_date: 2014-07-01 13:11:36
@_author: Natanael 
@_subject: [Cryptography] Bitcoin, litecoin, vertcoin, and derivatives 
Den 1 jul 2014 12:46 skrev "Maarten Billemont" :
specifically Litecoin and the recent Vertcoin?  What are the key
differences in their operation and how trustworthy are their claims (eg.
vertcoin claims to be "ASIC Resistant" - can we believe them and if so,
what does that really give us)?
The majority is just Bitcoin with tweaked constants (block time, supply and
rate of issuance, etc) or replaced hash function (like scrypt, or a combo
of 13 different hashes, or whatever else - the point is to make ASICs have
less of an advantage, unfortunately only helping botnets instead of making
mining "democratic").
NXT and some others are written from scratch rather than copy-pasting. NXT
is using proof-of-stake, which has several problems (such as resusage of
stake in a reversal of the chain (history attack), selling and buying coins
and then selectively reversing transactions to amplify stake, the fact that
there's no incentive to only use it stake to vouch for a single chain (no
guarantee of concensus), etc). Most proof-of-stake coins that aren't dead
already have checkpoints in the blockchain managed by the developer (making
it centralized).
Namecoin adds a key-value store and works for DNS, and also let you
register profiles and more (see onename.io, a web interface for profiles on
the Namecoin blockchain). It is merge-mined with Bitcoin, meaning Bitcoin
miners can reuse Bitcoin PoW in the Namecoin blockchain (requires the miner
to add a reference to the previous Namecoin blocks in the Bitcoin block he
is mining, which means both chains are secured equally by that miner's
Zerocoin / zerocash adds anonymity through covering the tracks of how the
coins are moving (using Zero-knowledge proofs).

@_date: 2014-07-20 10:03:52
@_author: Natanael 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
Den 19 jul 2014 23:46 skrev "John Gilmore" :
I know it was sarcastic, but here you go anyway;

@_date: 2014-07-21 10:47:32
@_author: Natanael 
@_subject: [Cryptography] multi-key encryption of "meta" data 
Den 20 jul 2014 22:59 skrev "John Kelsey" :
such a thing, you could get what you want by having a protocol wherein each
user connected once every day to the mail server over an encrypted channel
(TLS), sent up a fixed amount of information, and pulled down a fixed
amount of information.  No outsider would be able to determine whether you
were sending/receiving any email--all they'd know would be that you *could*
have sent/received email.
are either:
what.  (This looks hard to me--it's related to searching on encrypted data,
but looks harder than that to me.)
controlled by different entities.  What we know how to do right now is
build a remailer network with some kind of longish delay, along with some
kind of service that lets users drop information and chaff into/out of the
Bote mail in I2P uses method B already. Emails are encrypted using your
public key, delivered inside I2P (a traffic anonymization network similar
to Tor) using your public key as address and DHT as delivery and searching
method, it allows for using relay with time delays within chosen ranges,
and you can easily switch addresses often, and your van always use several
addresses at once.
It like to see the Bitcoin stealth address proposal adapted for it so that
you can keep a single address, yet have incoming mail tagged with public
keys derived from yours which nobody but you and the sender can know is for
It works like this: The sender performs an ECDH key exchange with his
keypair and that of the recipient. The resulting number is used to do EC
multiply on the public key of the recipient. To make sure it isn't the same
every time, you can XOR in the hash of a counter. You use that new derived
public key as the receiving addresses (and encrypts the message to it) and
tag it with the sending public key (if you also want to keep yourself
anonymous, use different tagging keypairs each time).
Now to the really clever part, the receiver can perform the same ECDH key
exchange to generate that same number, use it with the sending keypair and
his own to perform EC multiply on his PRIVATE key, and thus generate a
valid new keypair matching what the sender generated which can decrypt the
incoming message.
The main problem is performance, you have to test ECDH let exchange on all
messages you see.
Does anybody know of a more efficient method of tagging messages meant for
the owner of some secret value / private key, that don't reveal who it is
for or where it is from, but lets the recipient see it is for him? My idea
is that headers would be propagated first to everybody so that you can
determine which messages to fetch (anonymously within I2P).
Other than Bote, I do like the idea of pond, but they seem to be too
fixated at needing servers. I like for servers to be optional. In my method
above, you could let servers gather your messages for you, but they
couldn't decrypt them, just see the ciphertext blobs with their anonymous
headers. So you reveal some Metadata to them such as approximately when and
how much mail you get of what size, but not more than that.

@_date: 2014-06-02 18:28:27
@_author: Natanael 
@_subject: [Cryptography] Is it mathematically provably impossible to 
Den 2 jun 2014 17:31 skrev "Phillip Hallam-Baker" :
That's pretty much Ripple.
Please tell me how to convince the world to all trust those notaries.
Will they be anonymous? If so, how can we be certain that they are
reliable? Will they be public? If so, how can we trust they aren't under
coercion (NSL:s, etc)? Why would for example the Chinese or Indian citizens
know who those people are or why they can trust them? How will it be
bootstraped, how are the first coins issued? How are notaries replaced over
time (they will have to be replaced over time)? What do we do if suddenly
half the notaries say one thing and the other half says something else
(network split)?
Too many problems.

@_date: 2014-06-08 22:02:42
@_author: Natanael 
@_subject: [Cryptography] Help investigate cell phone snooping by police 
Den 8 jun 2014 21:52 skrev "Jerry Leichter" :
hid my mobile phone in a fridge.
refrigerator I've ever used has rubber gasket between the door and the body
of the refrigerator.  It's typically 2-3 cm thick.  Cell phone wavelengths
vary with band, but range from somewhere around 30 cm down to around 8 cm.
 A refrigerator should attenuate the signal, probably quite a bit, but it's
not going to block completely.
RF.  A refrigerator will also, to some degree, block sounds.  Since nothing
inside the food box of a refrigerator typically generates any noise (and of
course it's not *sensitive* to noise either), there's no reason to design a
refrigerator to be soundproof.  So how much it will muffle outside sounds
is unclear.
approximation to say whether they are likely to be effective for the task
at hand.  If it's important to you ... let us know how the experiments come
That leaves my conclusion as that you should stuff the phone into pillows
in the microwave before you put the microwave in the fridge, and then
stuffing more pillows in there.
Then you wrap it all with a lot of layers of blankets and hold them in
place with aluminum foil.
Then you do the same separately for the battery. Because you removed the
battery first, right?
Should only take a single working day or so to achieve.

@_date: 2014-06-19 13:06:16
@_author: Natanael 
@_subject: [Cryptography] [cryptography] WG Review: TCP Increased Security 
I've thought of something similar to this when tcpcrypt was new and I
was reading up about Monkeysphere (OpenPGP based Web of Trust
authentication, right now hooked into SSH). I finally wrote up my
thoughts about it in a blog post, and I've just edited it to add some
more thoughts relevant to this. This is very high-level, but could
maybe still be useful;
The short summary: what you need to authenticate a link encryption is
a session ID that can be derived from some session specific data, such
as the key generated in the key exchange. The session ID must be
globally unique and unpredictable to any third party. Authentication
would be done by comparing the session ID.
My blog post describes the basic idea of an authentication manager
that supports arbitary authentication modules, where each application
choses which modules to use and for which connection. The
authentication manager would use a specific API to interact with the
link layer encryption, allowing you to create "wrappers" for various
types of link encryption protocols as long as they are capable of
providing sufficient security. It is the authentication modules that
deals with exactly how to compare the session ID, verify the identity
of the other party and the conditions for when to consider the
connection authenticated.

@_date: 2014-06-19 22:14:16
@_author: Natanael 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
Den 19 jun 2014 21:37 skrev "Jerry Leichter" :
without someone there to type in the disk decryption password.  TPM's are a
great solution to this problem - or would be if they hadn't gotten twisted
to support DRM.
access to encryption keys and are scattered throughout the company campus.
 They decided that these things are almost never shut down, so it's not
worth deploying a secure way to store the keying information.  Instead, a
rebooting machine has enough smarts to ping the support center for help; a
person comes out and supplies the keying information.  Makes for a big
fire-drill after a large power-failure - which has happened - but that's
considered a worthwhile tradeoff.)
Maybe Mandos would be helpful to them. If you trust the servers to run
correctly when they are unattended, you probably won't need more than to be
notified when they reboot as they usually aren't particularly more
sensitive when booting. If you're worried about evil maid attacks, then the
attacker could do similar things when it is already running anyway.
Quoting the wiki: "Mandos is a system for allowing servers with encrypted
root file systems to reboot unattended and/or remotely".

@_date: 2014-03-16 21:48:45
@_author: Natanael 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
2014-03-16 20:45 GMT+01:00 Kriszti?n Pint?r :
Almost sounds like you want an FPGA in the CPU. So do I. :)
It would most certainly be useful for all kinds of tasks, in
particular cryptography and decoding of newer video codecs that don't
hardware acceleration (would make it much easier to speed up VP9 and
Daala, taking away the grip of h264 - and this translated to crypto
too, the grip of AES would be reduced if anything could run just as
fast). The most problematic part right now with simply throwing in an
FPGA is that they can only be reprogrammed as a whole and take a
little while to reprogram, which makes it hard to switch between tasks
or even multitask such as decrypting and decoding a live HD video
stream. Would be great to have a partially reprogrammable FPGA.

@_date: 2014-03-17 11:40:19
@_author: Natanael 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
2014-03-17 0:42 GMT+01:00 Guido Witmond :
What if it never is unique?
Also, Namecoin can do pretty much the same thing that your series of
steps there does. It's based on the Bitcoin blockchain tech, but
instead of simply just being a currency you can register both domain
names and public keys and a variety of other data in it. So there the
steps are to acquire some NMC (it's currency that you need to register
things) and spend it to register your public key with your name. Then
people can look for each other by name in the blockchain. It still has
the same problem, what if the name isn't unique enough? If you don't
know *exactly* what it should be, then you don't know who you're
talking to.

@_date: 2014-03-20 20:41:45
@_author: Natanael 
@_subject: [Cryptography] Mathematically, how do proxy signatures work? 
2014-03-20 19:32 GMT+01:00 Bear :
The Bitcoin folks calls it Stealth addresses (which then refers
specifically to the published public key + the method of generating
secondary keypairs).
BIP38 is mainly for encryption of keys. But yes, it does have one
feature that enables you to create "intermediate codes" you can send
to a physical cold storage wallet maker can imprint, for which your
password decrypts a private key they don't know. And the intermediate
code works like the stealth addresses mentioned above, you can
generate multiple possible outputs from it that all decodes to a
private key that you can decrypt.
This isn't the original Diffie Hellman. This is Elliptic Curve Diffie
Hellman (ECDH).
And the magical bit here is that EC allows for transformations of keys
that translates to the other key from a given pair. You can modify a
public key, tell the owner of that keypair how you modified it, and he
can do the same to his private key - and now he has a private key
which matches that new public key. To generate the shared secret that
modifies the keys, ECDH is used, since ECDH(Public key A, Private key
B) = ECDH(Public key B, Private key A), thus both parties only need to
know the public key of the other part, other than knowing their own
keypairs. What's used here to generate the new keys is called point
multiplication, based on the ECDH computed secret.
Here you go: It's used pretty much the same way for Stealth addresses.
The identification is actually the hard part. If you don't know which
keys are derived from your keypair, you have to try performing an ECDH
key exchange plus EC multiply keypair generation for every single
public key. Not until then do you know which of the keys something
have been sent to that you have the private key for. That's why
there's talk about using various "filters" and prefixes and tags and
other such things to reduce load and still maintain privacy.
The value that is computed with ECDH should be kept secret, because
otherwise anybody who knows the Stealth address and that value can
compute the same secret recipient address, and thus it isn't secret

@_date: 2014-03-30 20:06:57
@_author: Natanael 
@_subject: [Cryptography] OpenPGP and trust 
Secure decentralized authentication? Sounds like Namecoin. It uses a
fork of the Bitcoin protocol to let you register a name and keypair
(and domain name + IP address, etc...) in it's blockchain. Solves
Zooko's triangle (global uniqueness / decentralized / memorable) the
same way Bitcoin does, through the use of proof-of-work to achieve
global concensus and essentially simulate a central entity.
It offers you continuity (unless a user let their registration expire
(which you can look up), you can know that a username still belongs to
the same entity), and if you tell people your exact nickname in person
then they can always look that up in the Namecoin blockchain to see
what keypair you currently have registered. Unfortunately the lack of
a central entity means there's no conflict resolution, first come
first serve is the rule, so if somebody else register your callsign
before you then all you can do is to ask them to transfer it or
register another variant and tell people to use that one (unless you
can identify the person, and the countries involved have strict laws
against impersonation and can force the person to do a transfer).
The quickest way to get started is with the web-based client at

@_date: 2014-05-20 10:33:39
@_author: Natanael 
@_subject: [Cryptography] How to lock registers with GCC? 
Den 20 maj 2014 03:00 skrev :
interesting is that MIPS has more registers than ARM, maybe then that would
offset the need for debug registers. See this comparative regarding the
MIPS-3000 architecture performance by register allocation, page 90, table 1:
twofish. Yet for each extra register used to store the key, more operations
will be necessary to encrypto/decrypt stuff, thus knocking performance down.
good crypto for small keys:
in security would we get if we reduced the 112 bit keys to 96 bit keys?
which we could apply something like a crypto-core. Other architectures
would work with other solutions, ie. most ARM 7 CPUs have 6 or more
registers that could be used.
twofish and forget it. But then only a fraction of people would benefit
from having a crypto-core in their CPUs. Our goal is to make a crypto-core
that runs in most machines, even more so in unattended routers and
cellphones than servers (which already have options to encrypt memory
portions and can run serious PRNGs at little processor cost).
How much do you trust NSA? They published the lightweight ciphers Simon
(intended for hardware implementation) and Speck (for software, including
on microcontrollers). They're meant to perform well. There's a real chance
they're backdoored, though, but they might be useful for experimenting. But
they have versions of Speck with block size/key size of 48/96 and 64/96
bits, up to 128/128 (still faster than AES).
Note that 80 bit keys is considered the current upper limit of what's
practically bruteforcable by large organizations, and that's assuming a
strong cipher. I would not trust 96 bits for very long considering the risk
of a small fast cipher having half it's effective key strength undone by
cryptanalysis and the continuous advancements in hardware, I want a better
security margin. It is ok if it only needs to protect temporary secrets,
Then you also need some secure cipher mode on top of that, ideally an
authenticated one (don't ever do ECB). Don't know any that's fast even on

@_date: 2014-11-04 19:17:22
@_author: Natanael 
@_subject: [Cryptography] Security of wireless keyboards and mices. 
Den 4 nov 2014 18:57 skrev "Phillip Hallam-Baker" :
One big problem with practically all of them is the timing side channels on
the input. I don't know a single one attempts to protect against those. The
standard usage patterns makes the timing side channels trivially
exploitable against almost all text. One could probably also identify what
kind of fields you're filling in, the length of your password, and for
consistent hunt-and-peck typists you can probably also use the timing to
identify individual typed letters.
The existence of this side channel is the result of wanting to reduce power
usage by not transmitting when not necessary + wanting to reduce latency
enough to not be noticeable. This means you can't just transmit potentially
kilobyte sized packets every 5-10 milliseconds, too many of these devices
runs on crappy batteries that would die too fast.
Improve the batteries, ideally add solar cells, and go straight for
constant-everything transmission (constant size, constant time, strongly
encrypted so everything just like like noise) is your best bet. Maybe BLE
would reduce the power usage enough to make this work decently.
Wouldn't dare to dream that any of the 2.4GHz ones with unspecified
protocols are secure. Doubt they're replay protected, or rekey with
reasonable frequency or anything else like that.
Found one source:

@_date: 2014-11-18 00:29:06
@_author: Natanael 
@_subject: [Cryptography] Infinite Noise or Firebug? 
Den 17 nov 2014 21:34 skrev "Dave Horsfall" :
Some simple suggestions of mine that is intended to be somewhat descriptive:
Spinner. Associated to randomness via spin-the-bottle and references the
"loop behavior".
Flipper. As in bitflip.
Scrambler. Not my personal favorite (too generic), but maybe somebody else
likes it?
Dicechip. Self-explanatory name, easy to remember. Beware of potential
preexisting trademarks.

@_date: 2014-10-02 20:50:15
@_author: Natanael 
@_subject: [Cryptography] NSA versus DES etc.... 
Den 2 okt 2014 20:42 skrev "John Gilmore" :
One plausible explanation is that their custom crypto is so mundane that
nobody cares. Could be custom Rijndael variants tweaked for extremely
specific use cases. Could be algorithms designed for hardware primarily,
with overhead (due to tempest resistance) most people won't accept.
Another is that every one of them have been leaked, but without strong
evidence or any convincing reason for experienced cryptographers to take a
close look, nobody noticed. You might be able to find NSA's algorithms
openly on Russian forums, but without knowing where they come from or what
they do, or if there's anything special about them in the first place.

@_date: 2014-10-07 20:50:56
@_author: Natanael 
@_subject: [Cryptography] Best internet crypto clock 
Den 7 okt 2014 20:41 skrev "Arnold Reinhold" :
its images. It could be packaged and certified as a FIPS-140 level 4 HSM.
The camera would have a built-in asymmetric key pair with the public key
available from the manufacturer by camera serial number. It might also
accept additional keys via Bluetooth or USB and sign images using those
keys as well. As with any HSM, secret keys would be erased upon detection
of tampering. The camera could communicate via Bluetooth or USB or an
optical link and be controlled by a cell phone app, perhaps clipping onto
the cell phone or phone case. It might use inductive charging to minimize
electrical connections.
manufacture and non alterable. (When the clock battery dies, the camera is
toast.) The camera would periodically or on command output a signed
certificate containing the current reading of its internal click and maybe
an external nonce like the NIST beacon, which might then be sent to a time
stamping service, creating a record of internal clock drift over time.. The
camera might store a correction factor, so it could output a UTC time, but
the internal clock would be included in any certificate as well.
This approach is still limited. The camera can only attest to what color
values for each pixel it captured, not what really happened.
A very precisely color calibrated HDR display setup can likely fool most
cameras, or why not just a proper stage set up by theater / movie prop
designers, with heads of wax and all. Harder to fake video, but something
convincing can probably be made with face masks.
Light field cameras like lytro could be harder to fool due to the vastly
greater amount of information captured, but they still can't reveal a good
stage being fake.

@_date: 2014-10-16 20:07:37
@_author: Natanael 
@_subject: [Cryptography] Secure transfer of subsequences 
Den 16 okt 2014 19:36 skrev "Jerry Leichter" :
them to Bob, in the sense that Bob can prove that the bits S' he receives
are actually the same as S.  Depending on what we mean by "prove", either a
keyed MAC or a signature will do the trick.
T of S.  (That is, there is a sequence i0 < i1 < ... < ik such that
prove that T is indeed of this form.  Obviously, Bob can accomplish this by
transferring all of S and then constructing T, but if |T| << |S|, this is
very wasteful.  Bob would like to do this by transferring a number of bits
"not much larger" than |T|.  (If the subsequence really is arbitrary, the
cost of sending the i's might dominate.  That's not the interesting part of
the problem, and can be ignored.)
that the subsequences consist of a small number of long contiguous runs of
bits, using a tree hash on substrings of contiguous bits of S might work.
I'm guessing that *some* constraints on S are necessary, but who knows.
storage".  In terms of this problem, Bob sent S to Alice, who promises to
keep it stored; Bob will later ask for something small - T might be an
example, but is not enough - that Alice will be unable to produce unless
she has, indeed, retained all of S. There are some clever ideas here that
might be adapted, but I haven't kept up with the literature.
The only thing I can think of you haven't mentioned already is
Zero-knowledge proofs, showing that what you provided really is exactly the
same as the bits in range X to Y within the file with the hash Z (a less
formal way to describe what you explained in your second paragraph). These
are still very inefficient to generate, OTOH the proofs can be constant
size and quick to verify.
I would think tree hashes are the simplest practical solution.

@_date: 2014-10-27 02:22:38
@_author: Natanael 
@_subject: [Cryptography] Auditable logs? 
Den 27 okt 2014 01:59 skrev "Sandy Harris" :
Look at the conversation about timestamping images. This is essentially the
exact same thing but for text. You're capturing a state in time of
something that needs to be recorded accurately.
Hash the data, timestamp it by publishing it widely and/or hash chaining it
(git, Bitcoin blockchain, as well as various online trusted timestamping
services etc).
To protect the generation of the logs and this authencity, you'll need a
trusted hardware platform, like with a TPM. Something which can enforce
what software has access to what. Something which the admin can't override.

@_date: 2014-10-27 19:49:27
@_author: Natanael 
@_subject: [Cryptography] Paranoia for a Monday Morning 
Den 27 okt 2014 18:00 skrev "Jerry Leichter" :
cryptographic standards towards designs that were extremely difficult to
get right - e.g., Dan Bernstein's claims that the standard elliptic curves
have arithmetic whose implementations need special-case paths that make
side-channel attacks much easier than they need to be.
against fielded cryptographic implementations - but an ever-flowing stream
of attacks against another class of standardized software.  I'm talking, of
course, about browsers.  The complexity of browser standards - and of
ancillary software like Flash - has proved way beyond our capability to
program without error.  It's easy to blame Adobe or the Microsoft of old
for incompetent programming; but even the latest IE, produced under what
may be the best "secure software development chain" in the world; and
Chrome, a clean-sheet, open-source implementation by a team containing some
of the best security guys out there; continue to be found to have gaping
holes.  At some point, you have to step back and admit that the problem
doesn't lie with the developers:  They are being set up to fail, handed a
set of specifications that we simply too hard to get right.
How about "complexity" and "legacy compatibility"?
I'm cautiously optimistic for Mozilla's new engine under development
written in the memory safe language Rust. There's also one browser with
what it calls a formally verified kernel (
The web was bad enough already when the browser wars just was starting.
There was no need for any intelligence agency to fuel it. There were no
chance for defining a common well specified target. XHTML was one attempt
to create strict rules for structuring code on web pages, but browsers
moved towards being more lenient instead in parsing inputs because web
developers kept screwing up and you don't want to see the browser refuse to
even render 90% of all pages.
Maybe HTML6 will be more focused around capabilities thinking and well
defined features that can be implemented securely without breaking stuff?
We can all hope for it.

@_date: 2014-09-30 13:03:34
@_author: Natanael 
@_subject: [Cryptography] The world's most secure TRNG 
Den 30 sep 2014 09:55 skrev "Philipp G?hring" :
Auditable whitening?
Allow the user to verify that the whitener follows the specification by
being able to access the raw RNG output (hardware switch or cables
required?) and to feed the whitener with their own arbitary data for
Once you're confident the whitener behaves as specified, you just leave it
on as intended.
Intel got criticized for not even allowing access to the raw output before
the whitener, at all. So for all you know it might be running a stream
cipher or a counter through a hash function.

@_date: 2015-04-07 20:11:48
@_author: Natanael 
@_subject: [Cryptography] upgrade mechanisms and policies 
Den 7 apr 2015 18:33 skrev "John Denker" :
Here's my thoughts:
If anybody protests against a device going offline if there's no security
patches available to apply: can you tell me what the alternatives are?
Let's say you use crypto because the device is security sensitive (and
almost everything is, at the very least to not become malware vectors for
other devices). If you believe the security hole is unlikely to be exposed,
why do you need crypto in the first place, what is it doing there? Because
if you believe it is protecting against something, you need to accept that
the protection can go away one day if the algorithms are flawed. So then
what, you're using something security sensitive and allowing it to be
exposed to attackers?
Second of all (slightly off topic, though), I'm thinking IoT absolutely
will need some form of local server for a reasonable level of extra
protection. The basic idea is this: the server acts as a proxy, and most
"smart" devices talks ONLY to the server. Absolutely nothing else. The
server has fairly standard APIs (stuff like KNX,  bacnet, etc). This
provides many benefits: easy access control, reduced exposure / attack
surface, easier to update (the server searches for updates and tracks
security alerts, the devices can be quite dumb in and of themselves). Now
you just have one device to worry about, not dozens, and it makes
firewalling infinitely easier. Deployment can be done in a number of ways,
the important thing is that the devices are securely paired dig the server.
My thoughts on that part:
There's many reasons home servers should become a thing, this is just one
of them. And endless number of things would become much simpler with a
reliable always accessible trusted server in your home.
I agree that time limited software, protocols and algorithms is probably
needed. If you aren't planning for updates, then please tell me why your
believe your entire threat model isn't flawed. You're probably not going to
be the first person in the world to deploy perfect software on the first
attempt. If transitions and updates are part of the continous process
everybody participates in, there shouldn't be a problem. Now, the problems
are to set reasonable time limits and to get people to keep things updated
(the latter part would be greatly helped by the above mentioned servers
that also track updates, so that nothing is forgotten).
And for all the things not yet deployed with best before dates and
transition plans, I believe we do need one of these alert systems that's
been discussed as an option (cipher death notes, except it should apply to
every single component in all of the software). Define a reasonably
expressive syntax for what's broken and how, for the relevant metadata for
the bug, how to know if it applies to you (cipher names, package names,
etc), if it can be disabled and how, and what else you think is necessary.
Allow for multiple signatures, to assist policy making (defining which
organizations you trust alerts from).
Regarding ciphers and protocol versions, my thinking right now is that we
should go with the idea of having a tightly defined protocol with limited
or no options (in the official spec). But it should be designed to be
modular, so "emergency patches" doesn't require total rewrites - switching
ciphers should be easy (and private implementations with custom ciphers
also shouldn't be unnecessarily hard to maintain). And there should always
be a standby option ready to be deployed as version X.Y+1, the same moment
a flaw is exposed in the protocol which you have confirmed that the backup
doesn't suffer from. At every given point in time, the standby should be
the strongest known option to what's in the current protocol. Cipher
agility should be part of the design and process. And the old protocol
versions gets deprecated the moment they become insecure.
There should be no such thing as old versions with custom options being
more secure than the latest version on the defaults. You should be on the
latest version, and the latest version should be secure.

@_date: 2015-04-14 15:04:15
@_author: Natanael 
@_subject: [Cryptography] Feedback requested: ECC anonymous signatures with 
This started with the following Reddit thread:
The goal is to be able to publish signed messages anonymously and then
later on prove it was you who signed them, at a time of your choosing.
NOTE: I'm not a professional cryptographer, just a hobbyist, but I think
I've got a good grip of how things work. I'm willing to learn, feel free to
point out errors or flawed assumptions! It might be complete crap, or it
might be useful. I'm trying to keep it reasonably formal, but there's
probably some ambiguity left. I'm happy to answer questions.
Bonus features:
1: To be able to publish multiple messages without revealing at that time
that they're signed by the same person.
2: To be able to selectively link together multiple sets of arbitarily
chosen messages at a later point in time.
3: Not needing to store any state, so you don't need to store any data
beyond your original private key. The random numbers needed are derived
from values you already have whenever you need them.
Here's the most recent version of it that I'm considering;
You start off with a preexisting ECC keypair publicly associated with you,
with private key x_p and public key g^x_p (p means known publicly). You
have a message m_i (there may multiple messages, i is a variable that
identifies the current message). To sign a message anonymously, you create
a new keypair. To be able to prove it was created by you at a later point
in time, without allowing others to claim authorship, this is how it is
You first compute a unique per-message committed value H_ci = HMAC(m_i,
x_p). From that you compute a per-message value from that used for
derivation of a new ECC keypair, H_di = HMAC(H_ci, m_i).
To derive the new keypair (note that we are using the ECC versions of
multiplication/division, etc, not arithmetic), you compute the derived
private key x_i = x_p * H_di and the derived public key g^x_i = g^x_p +
H_di (deriving new valid keypairs is a neat feature of many ECC
implementations; also deriving nonces from the the private key + the
message has precedence in deterministic signatures). This keypair is used
to sign m_i.
To later on prove that you were the signer of the original message, you
compute and publish H_ci, signed with both keypairs x_p;g^x_p and x_i;g^x_i
(you probably should also include m_i and g^x_p and g^x_i in the signed
message). Thus anybody can compute g^x_p + HMAC(H_ci, m_i) = g^x_p + H_di =
g^x_i and validate both signatures to confirm the following;
1: That the signing keypair was derived from the original keypair, using a
value derived from that message (thanks to the hash commitment, it isn't
merely random), so the keypairs are related (the exact origin of H_ci is
irrelevant for verification, it just acts as a committed nonce/salt, H_di
is the important value; H_ci is derived from the message + main private key
in order to keep things stateless).
2: That as a result of 1, the message must have been known to the holder of
the original keypair at the time of signing, as the signature can't be
created before the keypair is created (this binds the message to your
keypair such that nobody else can claim authorship in contradiction to you
- assuming your original signature of the message was preserved when the
message was published and known by the verifying parties). Timestamping a
hash of the signed message before publishing can assist you in your claim
of authorship as you can prove your signature is the earliest timestamped
3: That both public keys represent *valid* keypairs which you hold and can
sign messages with.
4: That you hold the private keys of both keypairs *simultaneously*
(decreases risk of collusion between the signer and another party).
5: That you "approve of this message" with both the signing keypair and
your main keypair (you intend to prove authorship and link the keys).
It is also possible to link together pairs of messages to the same
pseudonymous identity without revealing at that time that that they're
related to your main keypair, and you can even create a chain of these
proofs for multiple messages, which in turn can be linked to your main
keypair at will;
You compute l_ij = x_i/x_j = x_p * H_di / x_p * H_dj (where l means linking
value, j represents the second message), where g^x_i + (x_i/x_j) = g^x_i +
l_ij = g^x_j. l_ij here has the same function as H_di above, and you sign
l_ij with both keypairs x_i;g^x_i and x_j;g^x_j, thus proving you hold both
keypairs simultaneously and intend to link them (again, you should include
g^x_i, g^x_j, m_i and m_j in the signed message).
This version do not show that you already knew one message while signing
the other like it does with showing the tie between x_p;g^x_p and
x_i;g^x_i, so this proves nothing about the origins of the keypairs, but
you probably don't need to do that either in this case. You can however
later on decide to link the messages to your main keypair by revealing the
committed values H_ci, H_cj as above, which then *does* prove the origin of
the keypairs.
Graphic:

@_date: 2015-04-15 09:04:38
@_author: Natanael 
@_subject: [Cryptography] Feedback requested: ECC anonymous signatures 
Clayton: I'm aware of Fawkes signatures. They are somewhat
applicable, but in some circumstances they aren't useful and/or safe.
Here's the best case stateless implementation of Fawkes signatures that I
can see that matches this usecase;
Use a seed and a counter to derive commitment values, which are then
committed with hashes in the message and revealed in the next message in
the chain (for keeping your pseudonym alive). To remain stateless, you also
derive counter encryption keys from the same seed and put encrypted
counters in the messages. To create a new message, you must access your
previous one to decrypt the counter so you can safely iterate it.
Multiple messages can also be posted without being linked to previous
messages (don't reveal earlier commitments), and later linked by a single
message revealing multiple commitments. But in this case of not having
simply a single chain of messages, tracking which commitments you have
revealed already requires additional state to be kept unless you have
access to all your messages (tracking which ones is yours could be made
stateless by having an iterated identifier value in the message, derived
from the seed, where you recalculate all identifiers and look up those
messages - but this access leaks metadata that can correlate your different
messages to your identity).
This scheme breaks if you forget the counter and also fails to access the
most recent message (such as if you have to go offline or can't access the
closed network with your most recent messages, and don't have the
electronics with you where you keep the counter updated). Then you'll
repeat your values and keys and the second message will look like a
forgery. If you screw up and publish the message to early after
timestamping its hash as a commitment, you can also break your pseudonym
through causing uncertainty about if the new commitment in the disputed
message is valid or not.
Due to uncertainties in the general perception of timestamping in various
cases (a single somewhat credible entity claiming to have seen the message
earlier than the timestamp causes uncertainty), Fawkes signatures are most
effective even used towards a small target audience (as higher assurances
can be achieved regarding when it really was first seen).
Accessing your most recent message to decrypt the counter can also put you
at a greater risk of local attackers.
Den 14 apr 2015 22:00 skrev "Mattias Aabmets" :
1: Its a mental exercise, and I want to see if I can construct something
that actually could work. Keeping it too simple wouldn't be an interesting
mental exercise.
2: Its (subjectively) a neat construction.
3: Flexibility. You've got plenty of freedom even after posting a message
in deciding what to link to what and how. You can link together multiple
messages in independent sets to establish two or more independent
pseudonyms to build reputation. You get to decide when to reveal your

@_date: 2015-04-28 16:48:02
@_author: Natanael 
@_subject: [Cryptography] The Friedman Papers 
Den 28 apr 2015 16:31 skrev "Ray Dillinger" :
What trackers is your seed node connected to? Flud on my phone ain't
finding seeding peers right now, I'm about to add some trackers manually.

@_date: 2015-08-22 23:21:30
@_author: Natanael 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
Den 22 aug 2015 20:04 skrev "Henry Baker" :
displays show only garbage.
Not Their Devices
Neat. Another application of visual cryptography that is like to see is for
2FA dynamic authentication cards with transparent displays. Push a button,
hold the card over the corresponding patten on the screen, type what you
see into the 2FA field.
Could also be used for highly sensitive/private notifications to the user,
which you don't need to auto-clear from view electronically as it only is
visible when the patterns are aligned. Accurate alignment and scaling would
be the tricky part, but I've got some ideas there. (Think I've partially
described this before. Gonna look up the details.)

@_date: 2015-08-23 12:08:57
@_author: Natanael 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
Den 23 aug 2015 02:42 skrev "Joshua Marpet" :
screen, read out the "secret message"?
Dynamic transparent screen, not static film. I guess I should have
clarified that the pattern would be derived using deterministic CSPRNGs
with a shared secret between the server and card. Using time as a component
would make that photo absolutely useless unless the attacker also got a
photo within X seconds of the computer screen.

@_date: 2015-12-12 12:29:21
@_author: Natanael 
@_subject: [Cryptography] Talk on encryption to non-crypto audience ? 
Den 12 dec 2015 03:09 skrev "Henry Baker" :
Got all the classical analogies ready? Like key exchange = box with
padlocks being sent back and forth?
Does anybody have a good analogy for perfect forward secrecy? Maybe to use
code books and to burn old used copies? (after the explanation of OTP)
What about analogies for authentication? How about tamper evident seals +
padlocks as the analogy for TLS?
Which other topics are you planning to cover? Anonymity like with onion
routing, more advanced crypto like Zero-knowledge proofs and such?

@_date: 2015-12-22 16:41:39
@_author: Natanael 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
Den 22 dec 2015 16:32 skrev "Emilien Gaspar" :
Backdoor proof of concept with custom parameters:

@_date: 2015-02-17 22:10:01
@_author: Natanael 
@_subject: [Cryptography] What do we mean by ... ??? 
We are getting there!
The FIDO alliance U2F and UAF standards allowd for something very
similar to this.
Here's how U2F dongles work: The dongle has a symmetric key that never
leaves the device. You connect to Achmed's used cars and hit
"register", enter your username and taps the button on the device. The
device generates an ECDSA keypair, encrypts the private key with its
own symmetric key, and uploads the encrypted private key and the
plaintext public key.
Then you come back to log in, enters your username, plugs in the
dongle. The server sends the ciphertext associated with your account
to you, and it generates a challenge that is partially derived from
that particular SSL session such that it can NOT be reused outside it.
The recieving browser confirms the challenge references the particular
SSL session it is coming from (blocks replay attacks) sends the
challenge and private key ciphertext to the device. The device
decrypts the key and signs the challenge, and sends it to the browser
which sends it to the server.
This setup means that Achmed's sister can not see that you're reusing
the same dongle for logging in to her site even if she could see his
database. It means that Achmed can't reuse your reply for his server
to get access to accounts that you have on his sister's service. It
means somebody who pretends to be Achmed either won't get a response
at all (wrong or bad certificate) or will get a response they can't
reuse against Achmed's service (because your device did not attempt to
authenticate *to Achmed*, but to a completely different service that
just have a similar name). And it means a MITM can not tamper with the
authentication, they can only act as a transparent proxy with access
to nothing but ciphertext or break the connection.
This also practically blocks all phishing that targets account credentials.
Google Chrome is getting this integrated currently, and Microsoft is
part of the standardization process and will support this in Windows
10, and likely IE too. I'm assuming Firefox will follow shortly. Using
NFC equipped dongles like Yubico has (Yubikey Neo), it could also be
easily used with smartphones.
Use that with a PIN, and put that dongle on your keychain, and you've
got USABLE high security that resists most common attacks for the vast
majority of users.

@_date: 2015-02-18 00:12:10
@_author: Natanael 
@_subject: [Cryptography] What do we mean by ... ??? 
Den 17 feb 2015 23:38 skrev "Jerry Leichter" :
forge a session from me *to himself*?  It sounds odd, but if he can, he can
create a fake order apparently from me and insist I pay for it.  Sure, I
can add a separate signature to every order - but if it could someone come
out of this protocol, so much the better.
The U2F response is a ECDSA signature of the challenge, and only the
hardware token is capable of decrypting the private key in question and
thus only the token can sign.
However, it isn't meant to sign arbitary plaintext as a way to sign a
contract. Achmed could send you a challenge to sign that references a
different receipt via a hash, but you could argue that the protocol is
designed to not care about that and that it by design wasn't displayed to
Your authentication is only directed to Achmed the service provider, nobody
The Yubikey NEO is however also PGP capable, he could ask you to sign the
data via an interface showing what you're signing. That's however separate
from the U2F functionality.

@_date: 2015-02-18 00:19:49
@_author: Natanael 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
Den 17 feb 2015 23:52 skrev "Kent Borg" :
saying that passwords are the worst authentication possible, except for all
the other systems. It occurs to me there is something deep in that.
one-time-pads: cumbersome--but otherwise perfect.
Even with some fancy Quantum Cryptography, passwords are not going to
suffer a catastrophic failure. Flawed as they are in practice, passwords
are a solid tool in principle.
alternative systems are complicated and brittle. Passwords are simple.
Distributed. Robust.
alternatives to passwords are worse.
ourselves to cleaning up our act: quit reusing password the same passwords
on different sites, pick good passwords, write them down our passwords, but
otherwise keep them secret*.
into phishing site X.
details. All the alternatives are worse, and I think for rather fundamental
Do you have any particular criticism against U2F hardware tokens combined
with shorter passwords verified over protocols like SRP? Strongly
credentials phishing resistant and bruteforce resistant. Easy to use.
Reasonably safe against most thieves. MITM resistant.
And human compatible - no need to make length and high complexity a
mandatory baseline, as rate limiting becomes functional again. No need to
remember tons of passwords. A simple password tier system based on how
important the services are combined with a hardware token would IMHO
achieve a high level of practical security for most people.
It wouldn't be less secure than my current password manager setup, in
particular since malware can't steal hardware tokens.

@_date: 2015-02-23 00:58:29
@_author: Natanael 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
Den 23 feb 2015 00:21 skrev "Rob Meijer" :
start doing dual factor using the person's phone.  It is now clear that
everyone has a phone, to some statistical certainty, and we can rely on
it.  So every system and his dog has now migrated to using something to
couple the phone and the password together.
have gone further.  ApplePay, bitcoin light clients, my stuff, are putting
the whole thing on the phone.  So, actually we are exposing the phone to
single points of failure/attack modes.  But this direction is still so
novel and so far rare that there is no economic case for attack and won't
be for a few years...)
improved might be missing a macro-trend that is going on.
a large keyring with (pseudo) anonymous? keys. So rather than using the
phone as single token for proving identity, it could be a granular tool for
proving our 'specific' authority. If all the authority can be revoked all
at once by a caretaker we control from our home, we could simple revoke all
of our phone's authority if ever we lost it or it got stolen.
proving identity. Two factor authentication makes for better tokens for
proving identity, but kills the (useful)  tokens of authority property that
allows us to share a password in order to delegate authority. If we can
manage to minimize the need for  tokens for proving identity and use tokens
for proving authority (capabilities) instead, than mobile phones could be
an interesting carriage for such tokens if we manage to get access control
on these devices sufficiently locked down to allow individual apps to keep
these authority tokens secret from each-other.
project to work on ;-)
Remarkably relevant: Anonymous credentials. They're implementing credentials wallet software as
part of the project, including for smartphones. Allows you to prove that
arbitary statements about yourself are true (if the verifier trusts your
credentials issuer).

@_date: 2015-02-25 22:14:34
@_author: Natanael 
@_subject: [Cryptography] Layering Web Encryption? 
Den 25 feb 2015 20:18 skrev "Chris Tonkinson" :
Yes you could, but in every place I can think of where you could add
functional support for an encryption layer on top of TLS would be better
served by dropping TLS and using the second encryption scheme alone. You'd
need every application to support it natively.
By the way, both Tor and I2P already works on Android (Orbot and the I2P
client on F-Droid). Both support public key based addresses (hidden
services / eepsites) for use inside the encrypted networks. The address
itself becomes the certificate. Both support access via local proxies on
the device, so most standard software supports it transparently if you can
tell it to connect over HTTP via that proxy to those domains (the Tor and
I2P clients will resolve the domain, encrypt and direct the traffic
There's also CJDNS as a similar option for secure routing, but without the
anonymization (intended for secure mesh networking rather than being an
overlay anonymization network). So far only available for standard Unix-y
operating systems.

@_date: 2015-02-28 11:44:49
@_author: Natanael 
@_subject: [Cryptography] Cheap forensic recorder 
Den 27 feb 2015 18:53 skrev "Phillip Hallam-Baker" :
The nice feature of the Pi being that I can pull the O/S image card, drop
it in an evidence bag and FedEx it to the lawyers.
like a complete, signed record of every keystroke and the video output.
Since this is X-Windows, is there an easy way to insert an X-Windows logger
in the path?
timestamp notary system.
I'm not 100% due, but the ARM TrustZone capabilities of this thing might
work for you:

@_date: 2015-02-28 20:43:37
@_author: Natanael 
@_subject: [Cryptography] Cheap forensic recorder 
Den 28 feb 2015 20:17 skrev "Emin G?n Sirer" :
technology, everything else is (well-intentioned) theater.
That's essentially what TrustZone is, except it isn't under exclusive
control of the CPU manufacturer. If allows for the device designer and/or
end user to specify what runs in the trusted environment.
Burn in a public key for the TrustZone signing verification (the USB Armory
has a write-once memory for a public key) belonging to an independent
entity both parties in court trusts, and have them sign an OS that uses the
secure boot features and OS protection and logging controlled by the code
inside the TrustZone. Then the device will only be able to boot that
particular OS in an untampered state (assuming no security holes!). The key
can be single use, i.e. only used to sign one OS image and then deleted.
For higher security and trustworthiness, I assume you could even use
threshold signatures such that you rely on non-collusion between two
entities each picked by one side in the court. If this is possible depends
on how the details of the signature verification and the public key
implementation in the device, but since one group has managed to implement
transparent ECDSA secp256k1 threshold signatures (the threshold signatures
are indistinguishable from normal signatures) I assume it will be possible
here too. So if everybody can agree on a trustworthy OS configuration to
run on the device, this would at least be plausible.
That last part is the real open question. How to you agree on a trusted
configuration? But that's not directly relevant to the original question of
how to achieve it in the first place, though.

@_date: 2015-01-09 19:23:12
@_author: Natanael 
@_subject: [Cryptography] Compression before encryption? 
Den 9 jan 2015 18:34 skrev "Stephan Neuhaus" :
on the grounds that this makes plaintext recognition through frequency
analysis much harder.
It is a question about entropy density here. Known-plaintext attacks
usually rely on knowing multiple blocks where you know with high certainty
what the plaintext is. RC4 suffers from this, breaking it goes do fast for
WEP because you know so much plaintext (most content in all WiFi and TCP/IP
headers and HTTP headers), and because you can make statistical guesses
about the plaintext (a whole lot of English).
This is because knowing that plaintext allows you to extract biases from
the key stream, which reveals the encryption key. Compressing all plaintext
hides those biases since it is harder to guess the individual bit values of
compressed data than for raw data.
compressed plaintext, but I can't recall any details.
any other hard evidence (not personal opinion, however well-informed,
please) that compression before encryption does or does not help?
That's probably BEAST and CRIME for HTTPS, which turns the browser into an
encryption oracle. The problem here is that the attacker controls a large
fraction of of the plaintext, and can watch the sizes of the compressed
ciphertext. Because  attacker controlled strings with substrings in common
with the secret strings equals shorter compressed outputs, and thus shorter
ciphertext, the attacker can do Hollywood style bruteforce and guess
AAAAAAAA, then AAAAAAAB and watch the ciphertext get shorter as he gets
more and more of the secret plaintext right.
This would typically target session cookies as they are sent with every
HTTP request. All secret plaintext which the attacker can cause to be
resent is a potential target. Javascript from malicious ads would be the
most likely attack vector.

@_date: 2015-01-10 15:48:52
@_author: Natanael 
@_subject: [Cryptography] hash/sign material for distro of IoT params 
Den 1 jan 2015 18:33 skrev "ianG" :
of the groups are wrestling with how to distribute different instruments.
As we know, Natoshi Sakamoto hit upon an elegant simplification by
eliminating the semantics of his issue, by the trick of only having one.
This simplification breaks down as soon as you want more than one issue,
more than one chain, more than one semantics.  As soon as you want choice
in anything, more or less.
value is what I call the Ricardian Contract [0].  This is a contractual
document that has a few smart fields slipped in, and carries its own PKI.
When cleartext signed to fix it, it can then be canonically hashed to form
the identifier for the unit of issuance.
that many people want to run their own, and different ones.  But the basic
pattern is the same, in that the description of any given blockchain
remains largely in the same format, with some different parameters [1].  If
one imagines a commercial service running a chain for some particular
purpose -- call it coffeechain for low value fast retail -- then we could
also include some static contractual information.  Something like an open
combination of params and legal text could be useful to describe
from anywhere.  We need to access the public key of the device, we need
examine the params, and we need to be able to examine the service
these things, we also need some strong identifiers, and ways to go from the
identifier to the description without fail, and ways to go from the
description to the identifier without fail.
where in order to run a chain or put up a new device, we have to get the
permission of someone else, or get enslaved to some facade security model
which incumbents lock up and stop from migrating in OODA time.
the identifier for that document/device achieves some of those goals, at
least on paper.
a device have these characteristics?  Does it have some params that need
exploring?  Does it have a service agreement?  A need to publish keys,
control info, etc?
identifier across the IoT space, and uses (eg) a DHT to find the document
for it?
cryptographically secure data infrastructure be for small cheap devices on
the net?
I'm not sure if I have correctly understood what you are looking for, do
you want a generic public append-only data structures with custom
validation rules?
But the blockchain derived document management system Factom might be
relevant. They've got things like what they call "proof of process" where
you frequently publish documents in a chain where one can verify that
there's just ONE chain of such documents and which is the most recent one
(you can't hide or replace old documents or create forks). Then we have the various Turing complete prototype altcoins where you can
create your own namespaces ? la Namecoin and register and process arbitary
data. But that's still not all the way there.
I've been considering something much more flexible myself by merging the
concepts between Namecoin, Pay-to-script-hash addresses in Bitcoin (P2SH)
and the Turing complete coins plus Zero-knowledge proofs like what Zerocoin
Here is what Zerocoin does: there's a global mixing address which you use
by committing to a unique secret "serial number", a nonce, when sending
coins to it. All these transactions are then entered into an RSA based
one-way accumulator (assuming this is for simplifying ZKP creation). To
withdraw you create an address in a deterministic way from your serial
number (so it can't be used twice this way) and generate a Zero-knowledge
proof of that you did indeed send coins to the mixer and haven't previously
claimed then. This allows you to withdraw a random set of coins of the same
value as what you sent to the mixer.
P2SH is a way in Bitcoin to force the usage of a certain script to withdraw
coins from that address. Multisignature addresses all use P2SH, to enforce
the requirement of signing with multiple specified keys.
Now you could extend this: you would register a data structure / namespace
with predefined validation rules which you then must prove you have
followed by using Zero-knowledge proofs. This way you can keep the contents
and even the validation rules you're using secret, and yet the miners don't
need to know anything more than that your public transaction with its
commitment hashes and its pointers to your previous transactions has a
valid Zero-knowledge proof.
This allows anybody to prove and verify just about anything that can be
stated algorithmically AND simultaneously ensure global consistency of
those same claims. Scoreboards, financial audits, checkpointing your own
internal databases, etc, you could use it for just about anything.
And for your question on identifiers, this would allow you to specify that
too in the transactions as part of the public data. You could refer to an
entry in another DNS data structure, or to a DHT, or a Tor or I2P service,

@_date: 2015-01-26 19:17:15
@_author: Natanael 
@_subject: [Cryptography] 2008 revision of Bitcoin whitepaper 
Den 26 jan 2015 17:59 skrev "StealthMonger" :
I2P with Bote mail, using the random delay option for relay and multiple
So you've got two layers of tracking resistance, one that behaves similar
to Tor and one on top where I2P nodes running Bote participates in a DHT
for storing and delivering messages. End-to-end encryption in every layer.
Given the mixing of traffic from multiple sources at every level and with
the packet switching behavior, the room for traffic analysis is extremely

@_date: 2015-01-27 23:10:47
@_author: Natanael 
@_subject: [Cryptography] traffic analysis 
Den 27 jan 2015 21:05 skrev "Jerry Leichter" :
[... ]
saturate your link with cover traffic - you need to send enough cover
traffic so that a listener can't tell the difference between cover and real
[... ]
requires large numbers of them to conspire to reveal anything?  The
mixmaster stuff looks at this specifically from the point of view of a
store-and-forward node - is there some suitable useful analogue on a single
link?  Can we somehow get the same guarantees without storage inside the
the theory that "We can't do much about them, but who would bother to make
such attacks anyway?", most modern work ignores traffic analysis as
"impractical".  Well ... we had to change our attitudes about DoS, and
we're now going to have to change our attitudes toward traffic analysis.
Quoting their description:
traditional relay-based approaches used by systems such as Tor:
algorithms to offer provable anonymity guarantees, even in the face of
traffic analysis attacks, of the kinds likely to be feasible for
authoritarian governments and their state-controlled ISPs for example.
guarantees of anonymity while also protecting online groups or forums from
anonymous abuse such as spam, Sybil attacks, and sockpuppetry. Unlike other
systems, Dissent can guarantee that each user of an online forum gets
exactly one bandwidth share, one vote, or one pseudonym, which other users
can block in the event of misbehavior.
broadcast communication: for example, bulletin boards, wikis, auctions, or
voting. Members of a group obtain cryptographic guarantees of sender and
receiver anonymity, message integrity, disruption resistance,
proportionality, and location hiding.

@_date: 2015-07-09 02:27:27
@_author: Natanael 
@_subject: [Cryptography] Is there a better way to discuss/publish new 
Den 8 jul 2015 20:13 skrev "Bill Cox" :
no choice but to post attacks without any review by anyone else before
hand, or not post my security concerns at all.  That, plus my lack of
experience in this field lead to most of my posts being half-baked and
ignorant of prior work.  If you say "go research the prior work first", go
suck an egg.  I do that all the time.  You can't absorb this whole field in
a few months.
son told me, "Dad, I didn't know you liked being evil."  I responded, "But
only evil for good."  :-)
We at  would be interested. Discussions about
the cryptographic security of implementations are more than welcome.
Cryptanalysis in particular. (Plus, you can frequently edit prior posts
with updates without spamming all the participants.)

@_date: 2015-06-02 03:03:43
@_author: Natanael 
@_subject: [Cryptography] If diffusion is perfect how much confusion do 
Den 2 jun 2015 02:21 skrev "Ray Dillinger" :
[avalanche effect]
Makes me think of error correction coding with overhead set to 0 (Qr codes
is one example where that's an available option), using the key as the seed
for the randomness commonly employed in error correction algorithms.
I'm no professional, so I might be missing something obvious to the
experts, but here's a reference to what this reminds me of:
XOR-encrypt-XOR mode (XEX): AFAICT I believe it should be secure IF the diffusion (computationally)
hides all statistical biases in the original plaintext and if the diffusion
is (computationally) uncorrelated from the application of XOR before and
after it, and if it is (computationally) indistinguishable from random and
thus effectively unpredictable (like a strong hash).
Attacks against XOR all relies on knowing statistics about the bits of
plaintext (diffusion output in this case). Repeated XOR of the same bit
string of terabytes of fully random data doesn't reveal anything about what
the string is because nobody knows what the original data was supposed to
If your diffusion effectively makes it impossible to predict if any given
bit should be 1 or 0 without knowing the entire plaintext, and if known
plaintext + ciphertext don't reveal the key, I don't see a flaw here (for a
single block).
However, with biased diffusion outputs, you can perform an attack similar
to those against RC4 and XOR blocks against each other (if in ECB mode) and
try statistical attacks to uncover a possible raw diffusion output that you
try to reverse.
Say the worst case is ECB mode encryption of a counter (can you think of a
worse one?). You'll perform the diffusion on what's essentially a bunch of
iterations of your key. All those diffusion outputs must be computationally
uncorrelated and unpredictable. Even knowing it is a counter that has been
encrypted, you shouldn't be able to guess the effect on the diffusion.
If the diffusion output are statistically correlated, you can extract the
XOR'ed key given enough ciphertexts. The diffusion output simply needs to
be computationally indistinguishable from random even against chosen
plaintext attacks.

@_date: 2015-06-07 09:42:57
@_author: Natanael 
@_subject: [Cryptography] Great moments in Bitcoin security 
Den 7 jun 2015 08:12 skrev "Erik Granger" :
blockchain.info wallets. I need to move them off. What do you guys think is
the best way to have a small organization use bitcoin, securely, but in a
way that still allows several non technical peers to remain in the loop?
It is still quite new so code quality isn't perfect yet (so don't rely on
it for large sums just yet), but  is a multisignature
wallet that is easy to grasp. Note that currently all participants in a
multisig wallet using the current version of it would have one vote in an
m-of-n scheme (with a threshold of your choosing), there aren't yet a
watching-only mode. The client is available on both computers and mobile.
The other options which I personally like are almost all limited to a few
platforms, have more complex interfaces and/or is meant for single
individuals, not groups.

@_date: 2015-06-17 11:52:48
@_author: Natanael 
@_subject: [Cryptography] password fatigue; was: Lastpass 
Den 17 jun 2015 05:12 skrev "John Denker" :
Agreed. Hardware tokens for key management is the easiest solution.
Yubikey, smartcards, etc.
FIDO Alliance has these neat protocols, U2F for hardware tokens (uses USB
HID, will support NFC) and UAF for local software.
It uses unique auth keypairs per-server, generated by the device used to
authenticate, then encrypted locally to be sent to be stored on the server,
with the public key known to the server.
On auth, after entering your username, the server generates a challenge for
you linked to the encrypted connection (your SSL/TLS session) such that
replay/relay/MITM attacks can't work, and send your device the encrypted
keypair (this way your token don't need large storage, and your use can't
be linked between sites from the token some). Your device decrypts the
keypair and challenge, verifies that it is linked to the current SSL
connection, signs the challenge, encrypts it to the server, sends it back.
Making it good enough even for use with multiple independent pseudonyms
over Tor. Only technical attacks against your local software has any chance
of success, and as per  if they can attack your local software you
failed already.
This scheme has support by Microsoft, Google, Yubico, Qualcomm and more.
Win10 will support it natively.
Combine with a PIN to unlock the token or similar and you've got pretty
good *simple* security.

@_date: 2015-03-03 19:44:51
@_author: Natanael 
@_subject: [Cryptography] Cheap forensic recorder 
The USB Armory is capable of all that.
Possible if you want to configure it that way.
Same here. The TrustZone runs what you tell it to run. You can chose
to run everything there or just very little. You can run your logger
inside it. You can run code that protects the OS integrity. Or
anything else. Your choice.

@_date: 2015-03-03 22:42:18
@_author: Natanael 
@_subject: [Cryptography] Proof of preservation... 
Den 3 mar 2015 20:15 skrev "Phillip Hallam-Baker" :
claim being that all the mails were archived.
a server run by a GOP political outfit that was probably infiltrated by
every intel agency on the planet), how would folk go about establishing a
proof this had been done?
avoid getting into an Ollie North situation and be proven to have lied to
Congress in the recovered emails. But it is also possible that a concern
was not wanting the NSA to know what State is up to.
a demonstration that the archives are complete with respect to the log
which isn't the same as ensuring they are complete.
(edited to not top post this time)
Fundamentally you have the requirement that every person sending anything
to them and every person receiving anything from them must perform a lookup
in the public log of theirs, in order to confirm that nothing has been left
out. You must also have some form of protections and/or incentives that
ensures most communication is done by notarized email.
Anything less means it is trivial to keep things out of the log,
undetected. You can however make this enforcement a little easier to
achieve by using an intermediate logging server run by an independent
party, and forcing all email to be routed through it. Can't tell you how to
effectively force everything to be routed through it, however. Maybe if you
require all email to be sent encrypted to that server's public key such
that the emails aren't even readable otherwise? But that only ensures
honest and cooperative senders will have their emails logged.
But don't forget that personal email accounts frequently are used by
officials, making all those mechanisms ineffective. So again, you must also
have some form of protections and/or incentives that ensures most
communication is done by notarized email. Now this is mostly *a social
problem*. You would need effective punishments against those using private
email and a policy that requires that all decisions are discussed via the
notarized email systems (anything without sufficient complete notarized
discussions would be at risk of being revoked/reversed follwing an audit,
as a method to ensure the system is used). Audits would ask "where and when
did you discuss this?", asking you to point to the logs. Too many instances
of unlogged conversation = disciplinary action, in proportion to how
serious the topic is.
Assuming nothing is left out of the log, the rest is only a matter of
crypto engineering. Just use hash chains of whatever kind that fits the
purpose. Making sure it is indeed used in append-only mode (no entities
removed, only added) would be a requirement, like with most authenticated
digital timestamping systems, git, blockchains and others.
This could be given higher assurances by adding the requirement that all
blocks (assuming a blockchain type structure) are signed by the log
maintainer. If any entry was added and silently removed again right after
having been checked, then the verifier will be able to show a signed entry
that's been removed from the current official log.
As for proving completeness regarding that the list of entries given to you
during an audit corresponds 1:1 to the logged entries (confirming that no
logged entries are hidden from you), I think Patricia trees would work as
the log structure, (there is of course numerous other possible solutions).
Reference:

@_date: 2015-03-05 19:18:29
@_author: Natanael 
@_subject: [Cryptography] FREAK attack 
- Sent from my phone
Den 5 mar 2015 18:56 skrev "Peter Fairbrother" :
holes for others as well.
less points of attack there are.
essential. Users don't RTFM, so don't expect them to.
6. Don't get too clever.
(Yes, it is partly related to 4, but still distinct as you can have clever
and simple protocols, and vice versa).
I'm thinking of things like 3-shake here, the SSL session resumption MITM
flaw - let a device initiate a session (which you MITM) to a server,
connect yourself to the server, break all the connections, perform session
resumption with both of them separately by using data from the previous
connections to make them think they're talking directly to each other.
Session resumption was supposed to reduce CPU load by reusing keys from
previous connections with the same device. IMHO that's a bad type of
optimization, and here it was done wrong as the authentication mechanism
was flawed.
Any other good examples of supposedly clever but horribly insecure
solutions? There has to be quite a few.

@_date: 2015-03-13 10:39:36
@_author: Natanael 
@_subject: [Cryptography] Securing cryptocurrencies 
Den 13 mar 2015 05:45 skrev "Ray Dillinger" :
This is typically called TAPOS. Already thought of. One of the problems is
that nothing stops you from chaining many unconfirmed transactions in the
same block to inflate stake. There's also the problem that you can more
easily disrupt the network by pumping out forks, getting other users to
reference different conflicting forks.

@_date: 2015-03-25 15:26:32
@_author: Natanael 
@_subject: [Cryptography] How to crypto secure speed limit signs 
Den 25 mar 2015 14:59 skrev "Henry Baker" :
"secure" each speed limit sign with a crypto signature.  Presumably, this
could be done with a QR code.
installing it somewhere else?  Should the QR code crypto sign the sign's
GPS coordinates?  Wouldn't that make speed limit signs pretty expensive to
manufacture & install?
The only thing that makes sense IMHO is a good GPS and signed official map
data kept frequently updated. (yes, GPS spoofing then becomes the new
You could however let the signs double as radio beacons - your vehicle can
ignore any obviously wrong signals, while also letting the beacons assist
positioning. If you keep them online (mesh radio, or listening to
broadcasts?) they can also broadcast their own ID + position + current time
+ given speed limit.
Secure positioning is a whole other matter. one potential solution that
have been considered is letting the client devices compare measured noise
with the servers (or beacons) to verify proximity.
Also plain challenge-response with timing - any relay must inherently add
latency, therefore you know the beacon isn't actually as close as it

@_date: 2015-03-25 17:43:21
@_author: Natanael 
@_subject: [Cryptography] How to crypto secure speed limit signs 
Den 25 mar 2015 17:31 skrev "Henry Baker" :
interpret various road signs ("TSR"/"ADAS"), so it should be possible to
"spoof" these cars already.  I wonder if anyone has already done such a
thing ?
on one of the Los Angeles interstate highways, and this particular sign
remained for a number of years.
movements; one scene in the 1970 movie "Patton" shows the results of such a
Came to think of this:
There's attempts at creating challenge-response tokens using quantum
physics, including by the use of light. I imagine a sufficiently cloning
resistant sign could work well if it for example could be easily identified
with computer vision (predictable shape) and if an IR laser could read it
to authenticate it.
But then you also need to stop people from physically moving them. But
again, combined with GPS and radio beacons you can probably get decent
security. Successfully spoofing all those sources simultaneously would be
hard. Signs in the wrong place would also be flagged when detected.

@_date: 2015-03-26 16:57:58
@_author: Natanael 
@_subject: [Cryptography] Zero Knowledge for Opening the Cockpit of an 
Den 26 mar 2015 14:01 skrev "Thomas Asta" :
cockpit was locked and one pilot was inside and one outside. Even for the
toilet pilots have to leave the locked space. And the crew knows the pin to
open the door and there exists even a crew wide known and for the plane or
company never changed  security code, which opens the door for 30 secs.
many cases possible.
bottles or forks from the restaurant are available at the gates.
knowledge approach (see wikipedia) work or is a third party authentication
from tower needed to unlock the doors? Any suggstions?
I'm not sure what the exact threat model is. Stolen code?
Why not use replay resistant auth mechanisms, like with a properly
configured smartcard and a challenge-response protocol? Then you can also
have revocable personal PINs. The crewmember can also disable his own card
easily to resist attackers - use the wrong code (or an alert code), or
simply break it.

@_date: 2015-03-31 23:27:18
@_author: Natanael 
@_subject: [Cryptography] Cipher death notes (was: Re: Fwd: OPENSSL FREAK) 
============================== START ==============================
Den 31 mar 2015 22:26 skrev "Michael Kj?rling" :
Out of scope, I presume. The purpose is entirely to prove the weakness in
case of publicly known breaks in algorithms.
Define a number of tests designed to be catch-all (as far as possible)
tests for broad classes of weaknesses that could negatively affect security
of software implementing the algorithm.
Once weaknesses are found, attempt to create inputs for the algorithm that
triggers these tests. Distribute these inputs as proofs of weakness that
various devices and services can fetch from security alert subscription
This is explicitly not preemptive against potential not yet proven
weaknesses - you wouldn't be able to create the necessary proofs of
weakness. This is quick-response trust-minimized (proof carrying) reactive
alerts, to attempt to quickly limit exposure *globally* to potential
attacks. To eliminate the ability to break into further systems for anybody
already using it.
IIRC these security alert subscription services should have several types
and degrees of alerts. You could have both proof carrying assertions and
proof-less assertions, descriptions of what use cases are broken and which
aren't (MD5 still works for HMAC and plain integrity checks, but is useless
for 3rd party signatures and anything else where birthday search collisions
and pregenerated pairs with same hash breaks security). Also preferably
with signatures from the organizations that is making the assertions.
This way it is far easier to define revocation policy - multiple trusted
entities simultaneously saying cipher X is broken for my usecase Y probably
means it should halt until manually reviewed. Proof carrying alerts that
shows that a security assumption that is critical for my service is now
broken in practice probably means my service shouldn't just halt, but also
put up red flags and warn all users.

@_date: 2015-05-06 23:13:41
@_author: Natanael 
@_subject: [Cryptography] Is there a good algorithm providing both 
Den 6 maj 2015 22:49 skrev "Francois BERENGER" <
francois.berenger.fun at gmail.com>:
I've heard of this before but can't find that same article right now. The
idea I remember made a number of keyed transformations on sets of symbols
that could compress low entropy density plaintext. Might have the pdf
around somewhere... Meanwhile, other articles Google returned:
Otherwise, one could mix streaming compression with streaming ciphers and
feed the output of one right to the other without touching disk in between.

@_date: 2015-05-07 23:48:51
@_author: Natanael 
@_subject: [Cryptography] Is there a good algorithm providing both 
Den 7 maj 2015 23:01 skrev "Bill Cox" :
sensitive data, by inserting a marker into the stream that disables/enables
compression.  This would not stop all data leakage, but at least it could
help protect security tokens and other keys.
The easier method is to just pre-encrypt, as that requires no special
treatment in the lower layers. Just feed in the data to send. Then you
don't need to worry about overzealous compression or dumb optimization. No
strange edge cases of accidental activation / deactivation of compression.
If you wish for the encryption to be done in the same software layer you
could still add an API in the transport encryption for explicitly
encrypting non-compressed sensitive data, with keys derived from the
session key, so that the data then can be detected at the other end and
decrypted by the software as necessary (no suggestions here for how to tag
it as ciphertext decryptable by API).
In the context of a receiving browser, I wonder if it makes sense with
anything but automatic transparent decryption acting as if there weren't a
second layer of encryption, because otherwise how do you even define the
access controls, why would access control to decryption be an improvement
over trusting the data flow control in the browser? I think that if you
need more control you should encrypt independently from transport
encryption. Then you can at least attempt to enforce your own policy in
your own code.

@_date: 2015-05-12 17:17:54
@_author: Natanael 
@_subject: [Cryptography] AEAD modes for signed ciphertext 
Den 12 maj 2015 07:53 skrev "Ned Ulbricht" :
particular, I'd appreciate pointers to papers, security proofs,
specifications and/or implementations that would throw light on this
specific problem.
store-and-retrieve messaging system.  I am sketching just enough of it to
provide necessary context for the cryptosystem.
whitelisting is initiated through outside channels, when a potential sender
(alice at foo.example) exchanges her user-readable "tofu tag" with a potential
receiver (bob at bar.example).  Within the communications system, Bob then
uses Alice's "tofu tag" to locate Alice's directory server and her
directory entry. Having Alice's directory data in hand, Bob commands his
message relay server to accept and store messages from Alice. Without this
pre-approval, the relay will reject messages.
communication parameters from Bob's directory entry. Those parameters
include the designation of Bob's relay server, where Alice drops off her
message.  Bob must initiate contact with his relay in order to pick up any
messages stored there.
This entire thing reminds me heavily of Pond (which by the way is designed
to use Tor for traffic anonymization).
Look on the tech page. Your goals overlap strongly.
I'm not sure if it deals with signature substitution or not. The easy
solution would be to bind the decryption key to the signing keypair. How to
do that is not my expertise, however.

@_date: 2015-05-12 18:11:09
@_author: Natanael 
@_subject: [Cryptography] Is there a good algorithm providing both 
Den 12 maj 2015 18:05 skrev "John Levine" :
one of the big
prevent censors
public, but
in many places. The
that the encrypted
These are just the first two sidechannel attacks I find by googling. There
are at least a dozen more like them. Attacks on both Google search
autocomplete to find the search query and on Google Maps to detect what
area the user is looking at.

@_date: 2015-05-14 15:36:02
@_author: Natanael 
@_subject: [Cryptography] Any S/MIME or PGP for normal people 
Den 14 maj 2015 08:25 skrev "John Levine" :
Who'd do the configuration? If a techie will do it, I'd suggest a Yubikey
with the OpenPGP applet, just one touch to sign (don't remember which exact
software that supports it, though).
If they do the configuration themselves - OpenKeychain on Android with K9
Mail. That's the only software I can imagine a crypto newbie would tolerate
setting up.

@_date: 2015-05-25 22:52:14
@_author: Natanael 
@_subject: [Cryptography] Is this a "relevant" attack against HMAC-MD5? 
Den 25 maj 2015 22:21 skrev "Dan Kaminsky" :
somebody posted this and I don't know enough to know if it matters.
These are the really serious ones:
Full key recovery (still computationally difficult, but now within
practical range for an entity like NSA).
And using HMAC-MD5 commitment with for say multiparty protocols with key
derivation or other randomness it would allow for precomputation to bias
the output. For games it is a total break when used for something like a
coinflip. But as far as I know there are relatively few widely used
protocols that rely on commitment schemes that expects a fully honest
committer (Fawkes signatures as an example don't care *which* precomputed
message you publish, just that *only you* knew the committed value in
advance). But for those that do expect an honest committer where 1
commitment hash = 1 committed message, don't use MD5.

@_date: 2015-05-30 11:03:02
@_author: Natanael 
@_subject: [Cryptography] open questions in secure protocol design? 
Quoting Tony Arcieri;
scary, but they can't do that easily because the Bitcoin protocol has
nothing to signal that wallet keys are anything but ECDSA with secp256k1.
This is actually wrong.
There's a reason standard Bitcoin addresses starts with 1 - that's the
address version flag. The addresses with a 3 in the beginning are Pay to
Script Hash addresses (P2SH) which you only can pay from by using the
hashed script and inputs which it can validate using that script (such as a
valid signature for its signature validation operation in the script,
against the public key specified). Add a new signature verification
algorithm for another curve and let those who want to use it create such
addresses which enforce usage of that operator. It would also be easy to
create a new version flag for addresses that specifies what algorithm a new
address uses, such as v 5 addresses using NTRU or whatever else you might
feel like.
My opinions about the main topic (I'm no professional though);
Den 25 maj 2015 22:27 skrev "ianG" :
agility [0].  Open questions in secure protocol design:
A little of both. I agree with the plan of having one as the sole standard
with one backup algorithm readily available at all times for instant
switching if necessary. The protocol must be designed to ALLOW algorithm
agility, if only for to reduce switching costs once the old standard cipher
fails. As said before, the real problem is migration AWAY from what's
broken. Make sure you can signal to all devices what's no longer secure so
they can switch to any backup algorithm still considered secure. Or to shut
down *if necessary* (which is likely to happen from time to time).
Don't have experience enough be to have a solid opinion. But small team
with (non-distracting) input from a large group of experts would AFAICT
perform better than most other options. I think...
Depends entirely on use case. In general I believe small efficient
extensible core protocols should be used.
For which part of what? Auth should be connection oriented. For the link
encryption, I don't know.
My current favorite model is a protocol with opportunistic key exchange
which notifies the software about the authentication status, with pluggable
auth modules. If the software don't care (like say a torrent client), it
just moves ahead with no auth. Browsers would not start communicating until
the connection is authenticated with a PKI auth module or similar. As a few
examples. Delayed auth would be an option the software would need to opt in
Core protocol with no switches. Optional extensions.
What's the practical difference?
protocol to do a survey across protocols & time and discover whether there
are any meaningful trends in the above open questions.
Yup, we need to keep track of all the causes of the various holes in
protocols. Which ones were design flaws, implementation flaws, algorithm
flaws, etc, scope, severity, etc...

@_date: 2015-11-19 00:30:26
@_author: Natanael 
@_subject: [Cryptography] A new, 
Den 18 nov 2015 23:07 skrev "Ismail Kizir" :
This looks like a hybrid of a stream cipher and a block cipher in something
like CBC cipher mode, continously updating secret state derived from the
key and mixing it into new blocks.
Your problem is that this very certainly will have detectable biases and
fall to cryptanalysis by making simple assumptions about byte distribution
in longer ciphertexts, such as by guessing location of known headers and
statistics on unencrypted plain English in ASCII and more. Somebody else
here is probably capable of producing a proof of concept for cracking it.
Oh, and you're just using XOR to update the state? That's flawed from the
same reason OTP only works if the OT in the abbreviation is satisfied (one
time pad). In other words, if you only encrypt a plaintext as long as the
key at most.
Not to mention your jumps will make it weak to side channel analysis.
Really really bad for long ciphertexts.

@_date: 2015-11-23 15:55:13
@_author: Natanael 
@_subject: [Cryptography] basic cryptography ... was: key breaking 
Den 22 nov 2015 23:04 skrev "John Denker" :
[AES-CTR like mode, except encryption comes after the last XOR]
We still need MAC. Even scrambled plaintexts can be harmful, we need
[what I call stream cipher keyed counter block mode, similar to your own
description below]
On a second thought I'm reminded of how XTS mode works, though still very
Even the lower margin version ChaCha8 instead of ChaCha20 could be used. In
this mode you'd need a massive statistical bias break in the output of
ChaCha8 to crack the key.
I've seen other suggestions like this on the crypto mailing lists (though
I'm not finding any right now, except for a previous mention of enchilada).
I've thought of this before myself and like the idea. You could potentially
use both an efficient small stream cipher to key a small efficient block
cipher in something like XEX mode. Something in the style of NSA's Speck
and Simon (though I wouldn't use something designed by them, see Dual EC
DBRG). It could easily be parallellized.
Problem is I'm not sure how to do authentication and keep efficiency. Use
Poly1305 the way it is typically combined with ChaCha20, and use the first
stream cipher output for block  (or the output after encrypting an empty
block with it) as the authentication key? Won't this kill performance?
What's the most efficient secure enough authentication mode? Any efficient
single-block authenticated ciphers (taking an encryption key and
authentication key separately)? Maybe Keccak who is supposed to be a very
flexible primitive and be able to provide authentication too.
Slightly off topic, but in order to not make things too simple for NSA
either, preventing ciphertext compression like with simple removal of
authentication tags, would be a nice feature:
In the recent AEAD discussions I've seen links to AEAD modes where the auth
key is encrypted with the plaintext (with ciphertext longer than the
plaintext to match) and other such solutions where the tag/authentication
is mixed in with the ciphertext. Think it was in the nonce misuse resistant
AEAD conversations that it was linked. Also, if there were some
authentication tag algorithm that had an All-or-nothing-transform then that
would be nice too.

@_date: 2015-10-01 09:08:16
@_author: Natanael 
@_subject: [Cryptography] blockchain and trustworthy computing 
Den 1 okt 2015 05:38 skrev "ianG" :
executed on every node, and the ability of the nodes to come together and
find consensus on the state or results provides a way to not only compute,
but also know that we have verifiably computed.
does this mean we now have a trustworthy computing platform?
computer, TV, android tablet, iPhone, and xWatch, have I created a
trustworthy computing platform?  In the process, has the
hardware-I-Don't-Trust conundrum been solved?
the blockchain calculating in turn over the various cars that are providing
the nodes, does this solve the VW problem?
Yes and no.
It does nothing useful in the VW case. Their problem was insufficient data
from testing, no amount of trustworthiness in the processing can fix that.
You need trustworthy measurements in the right places.
The blockchain can provide some assurances that some chain of computations
were done correctly. But with what limits?
First of all, Bitcoin is essentially a shared fully public state machine
with version control - you're free to rewind it at will locally and try
again. It is *your* brain in a jar. The primary purpose of the construction
is to approximate a global concensus well enough to be practically useful
(majority proof-of-work chain wins). It isn't made for keeping secrets or
as an impenetrable C&C platform (for malware OR for your home), or for
providing guarantees about the trustworthiness of input data.
It was made to let the public collectively track the ownership history of
digital tokens, through using game theory to ensure there's a strong
incentive for a majority to cooperate. Essentially it is a big Access
Control List for digital money, and works because your digital signature
proves you are you, because it has a practical distribution method (mining
rewards) and because users are assumed to ensure they're connected to the
majority network, and therefore likely won't be fooled by acting on false
assignments of tokens.
Maybe you've seen the Enigma blockchain is using homomorphic encryption and
threshold key sharing to enable keeping secrets? But that's really just a
clever way of letting strangers perform Multiparty Computation for you,
with what that entails - if any majority of them is hacked or infiltrated
or collude, all secrets can be recovered. But maybe you happen to trust it
won't be compromised while you still need the inputs to remain secret?
There are Zero-knowledge proofs and homomorphic encryption with proofs of
correctness (see Zerocoin, Monero and others). Their limitation is that
they only proves the rules were followed ONCE, not that nobody ever
performed other computations as well on the inputs. It doesn't reveal
bruteforce attempts. If all you need to show is that the output was
computed correctly, that's fine. But if you're protecting a game with the
protocol, you can't stop the other player from evaluating the results of
other moves.
If you have a set of networked devices that trust each other internally,
you have a number of options.
You can move from proof-of-work to a notary network with you as a
gatekeeper (the Ripple model).
Then, first of all, your devices can log all their actions and decisions,
sign it together with the last block hash, and checkpoint it in a shared
blockchain (git?). All devices would only trust a chain signed by the other
trusted devices. This can be used to send commands, send alerts and to
provide a degree of verifiability of past actions. It can also be used for
other kinds of coordination.
If you make the blockchain be THE real world that matters for your own
devices, forced to iterate forwards without forks because all your devices
enforce that collectively, then you can extend the range of verifiable
actions. Just like how the Bitcoin blockchain (with majority proof-of-work)
is THE definition of who has control of what Bitcoins, your blockchain
would be the system that control all of your network's access controls and
capabilities. Make every action that matters be verified collectively. But
the caveats above still apply - hacked devices can test multiple choices
internally to see what outcome is more desirable.
(More comments will come later. Gonna look up some other sources on what
kind guarantees these constructions could provide.)

@_date: 2015-10-11 18:39:30
@_author: Natanael 
@_subject: [Cryptography] Usable Security Based On Sufficient 
Den 11 okt 2015 17:32 skrev "Ralf Senderek" :
Privileged services doing key management is already a thing. The idea
behind it isn't all too crazy.
Nope, impossible. With access to all secrets, there's no capabilities the
malware lacks as it runs in a Turing complete environment. If the malware
is executing as the user and also has all of the secrets, you're screwed.
Also note that all unpredictability (computational entropy) used must be
considered part of your secret keys.
The only exception is if the credentials is only useful when controlling
local hardware that only root can control, but that userspace software can
not send commands to (such as some PCIe devices if the OS is configured
properly). But if you're talking about encryption/decryption or online
services, it can't be done.
There's something *almost* the same: This uses hardware features in the CPU to restrict what software can access
the secrets.
as root.
How would it be made unavailable? Like with TRESOR above? SELinux
unpredictability to
Accessible by root. Doesn't work. See comments above.

@_date: 2015-10-17 11:22:14
@_author: Natanael 
@_subject: [Cryptography] How does the size of a set of target results 
Den 17 okt 2015 09:36 skrev "Rob Meijer" :
organisations spending time on categorizing files and distributing the
hashes with categorization to parties wanting to use these sets of hashes
in a forensic investigations. In the forensic investigation, these hash
sets are then used with the hashes calculated over files found on media in
a set-theory way in order to focus both the automated and manual part of
the digital forensic investigation.  Some of the hash sets are used as a
"we can safely ignore these files" type of set. These include files from OS
distributions, large collections of software packages, clipart collections,
etc,etc. Basically the stuff that you don't want to waste man-hours ,
CPU-cycles or IO activity on.  Some sets are "A person needs to look at
these" type of set. These sets are things like collections of the hashes of
child pornographic images.
a hash from one of the  "we can safely ignore these files" set.
investigation by making a relatively large set of non-legally-problematic
files match with a hash from a "A person needs to look at these" set.
exploit a flaw in a viewer for its file type match with a hash from a  "A
person needs to look at these" set.
attack where the hash of the modified file matches "ANY ONE OF" of the
hashes from the large set of hashes (for what we mostly don't have the
original files).
is defined as 2^{N}, and the size of the hash-set that our "ANY ONE OF"
implies is 2^{M}, what would be the worst case complexity of an  "ANY ONE
OF" geared first preimage attack?
naive assumption of 2^{N-M}.
These are the viable attacks as of today:
MD5: constructed collisions. A legitimate file and a shady file are first
taken. You then process both files and calculate two suffixes, one per
file, that cause then both to get to get a new shared MD5 hash value. A
laptop can do this. (There's possibly also other viable attacks on MD5. )
By trying to attack supply chains of other projects to modify files they're
about to release to match, you can trigger this human DoS.
SHA1: Right now there's the freestart collision that was just proven
possible (freestart = chosen IV / prefix).
128 bit hashes can be attacked the same way with standard birthday
collisions at 2^64 operations. Also note that a larger target set reduces
the work of takes almost linearly IIRC, as 2^64 applies for one single
target hash to match. 2^16 hashes in the set to match thus reduces it to
2^40, I think.
Just stick with strong 196 bit hashes or longer. SHA256 is good enough, but
SHA3 ought to be a good choice too if you're starting from scratch.

@_date: 2015-10-17 21:00:25
@_author: Natanael 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
Den 17 okt 2015 01:36 skrev "Phillip Hallam-Baker" :
Doesn't this attack still work in one way then? Because here it is s2 that
provides the PFS property, but that's the value that can still be attacked
this way. So then you just need to crack the prime, and then start
collecting private keys for the certificates of your targets.
So the difference is that you now need to both crack DH and also get the
private key, not just one or the other (getting the private key in non-PFS,
cracking DH in standard PFS). So less convenient and harder to attack on a
large scale, but NSA still have plenty of zero-days for that.

@_date: 2015-10-18 00:38:12
@_author: Natanael 
@_subject: [Cryptography] blockchain and trustworthy computing 
Den 5 okt 2015 23:58 skrev "ianG" :
always about being able to run my code on someone else's facility.  For
example, I-as-movie-mogul run my movie on your TV, after payment received.
That needs tamper proof trusted hardware. Software can't do it alone. And
the analog hole remains.
allowed to rewind your odometer any more.
Like I noted later in that email and which you commented on further below -
it isn't always a question of reversing the output you've committed to
previously, but about finding alternative ways to make the output you
commit to appear acceptable.
However, regarding HOW to prevent it from rewinding, within internally
trusting groups you can do this;
Coming across Forward Secure Pseudo Random Generators a second time (used
in systemd to implement Forward Secure Sealing for logs) and then
remembering the recent post about Forward Secure Asynchronous Messaging
from Puncturable Encryption, I thought that parts of the latter (those
derived from Forward Secure PKE) can be used in a multiuser model much in
the same way the former is intended to be used for single user models.
 + So we modify this. We take the blockchain and move from PoW to threshold
signatures where the signers are selected by a gatekeeper.
This variant of the threshold signing implementation is the novel part
(AFAICT); Each party still has a public key as usual. But now the private
keys has a number of components, each assigned to one time-slot.
So we then have two layers of thresholds, one per signer and one for the
group. And the per-signer layer is derived from Forward Secure PKE, used as
a signing algorithm instead of as an encryption algorithm (I believe it is
achievable, even if may not be straightforward).
So now with an m-of-m threshold, we only need one signer to delete any old
key component for it to be impossible to sign any new data corresponding to
that time-slot, and with the hash chain still in place there's a heavily
reduced incentive to selectively keep old components to later substitute
individual entries, as everything must be updated at once at serious risk
of detection.
And thanks to the hash chain, collectively signing messages dated into the
future requires either halting operations until that point to not break the
hash chain and thus not creating conflicting forks (and this is immediately
detectable) or speeding up the clocks (and again losing ability to act
instantaneously up until that point in time), although with full collusion
they can create forks at will within the current time-slots.
So if you trust that at least one signer instantly deletes his most
recently used key component for each block / time-slot, even if it is a
different signer doing it for each block, you know that the chain can't be
rolled back in full since a *complete* alternate chain can not be
successfully signed in its entirety. What's been committed stands.
So in your car example, having one small device with no storage beyond
what's enough for this type of private key per relevant agency would let
them all trust that no rewinding is taking place. They don't need to store
the latest signed hash chain headers and verify the headers, they just need
to sign the latest block that's one time-slot past the previous one they
signed and then update their key material. The device won't even be able to
sign two blocks for the same time-slot! This can easily be done using cheap
smartcards (yay cost savings). When looking at the chain later (when the
car is in service?) they can all see it is *one single complete chain*,
with little to no reason to believe that every device key has been
successfully extracted early on in order to rewind the chain past where
illicit modifications were done.
Now making sure the blocks contain all the data they should and that it is
accurate is still a trustable sensor issue. While incomplete, you could
potentially have the smartcards used for this scheme look for current
signatures in each block from every sensor they're programmed to care about
before they sign the block.
(Does anybody know of any Proof-of-stake altcoin doing anything like this?
If not, they should. Currently the naive constructions would make rewind
attacks child's play to TLA:s, also see the Nothing At Stake argument,
FS-PKE modified for group threshold signatures would be a huge improvement.)
Is anything doesn't make sense, just ask. I just fleshed this idea out over
the past hours.

@_date: 2015-10-18 12:44:53
@_author: Natanael 
@_subject: [Cryptography] How does the size of a set of target results 
Den 17 okt 2015 11:22 skrev "Natanael" :
collisions at 2^64 operations. Also note that a larger target set reduces
the work of takes almost linearly IIRC, as 2^64 applies for one single
target hash to match. 2^16 hashes in the set to match thus reduces it to
2^40, I think.
but SHA3 ought to be a good choice too if you're starting from scratch.
I misremembered things here, sorry.
The pre-existing set of files can be considered to be precomputation done
for you, on average 2^(128/2) = 2^64 total hashes still need to be computed
to find a collision. With for example 2^32 existing hashes for 4 billion
files, the effect isn't really noticeable:
2^(Log2(2^64-2^32)) = 2^63.9999999996641 work factor
However you get collisions more often once you finally got to this number
of computed hashes, so at 2^65 work you likely will have 2 collisions
already, and so on.
Since 2^(192/2) = 2^96 which will likely remain out of reach for decades,
that recommendation still stands. Don't go below that number.
Also, I just remembered the Flame MD5 certificate collision attack. That's
yet another attack algorithm faster than bruteforce.

@_date: 2015-10-21 01:01:36
@_author: Natanael 
@_subject: [Cryptography] Other obvious issues being ignored? 
Den 20 okt 2015 23:54 skrev "John Denker" :
A false sense of security. No clue of the origin of the concept. It may
certainly be millenia old.
My own list:
* Universal simple secure authentication. U2F is *almost* all the way
there. It just lacks verification of WHAT action is taken, I want a screen
on the device.
* IoT and networking in general. Networks shouldn't be the equivalent of a
digital lounge (you're either in or not). Communication channels should be
easy to declare, request and allow/deny.
Taking UPNP and remodelling it with cryptographic capabilities / delegation
would make it easy to tell your smart TV that no, it may not talk to the
Internet or any device other than those authorized by you to initiate
connections to it. Its like taking both Android's Intents and app
permission model in one, letting devices say "I can do X" and request
access to those who say they can do Y. This would also effectively firewall
everything the way it should. (And your firewall software on your computer
could also appropriately be the one used to grant or deny your other
devices permission to talk.)
* Home servers. I believe we need them. Let's say everybody's got one,
running I2P/Tor, their own IM proxy for their mobile devices (multiple
sessions? Only need one), secure mail client, secure storage/backup with
capability for sharing and redundancy (Tahoe-LAFS is promising), etc...
We'd cut down massively on metadata leaks, we could trivially firewall all
IoT devices by only allowing them to talk via the trusted server which may
rewrite packets to prevent data leaks and block exploit attempts, we'd get
far better network synchronicity and latency, etc... There's so much they
could improve. Google's OnHub is functionally close to what I want (or will
be), but it needs to be fully open too.
* Key management in general. I would like to combine the approaches of PHB
and his Mesh software, Yubikey NEO, Bitcoin hardware wallets, OpenKeychain
and a few others. And it should rather be credentials management where
secret key material is part of it. And backups should be made easy. I also
like the swarm approach one university studied where all your wearables
needs to cooperate (using a threshold model).
I want smartwatches and card-like devices that can make authorization easy
and secure.
* Connecting new devices for the first time. I want what I call a key
courier device:
Tap, watch the verification blink patterns match, accept. Carry over the
credentials to all the devices to configure, repeat. No need for
horrorshows like WiFi WPS. No need for 4-digit Bluetooth PINs. Just check
if the key exchange succeeded or not. Just accept or not. Transfer with
standardized interfaces like NFC and with standard APIs. (And on first use
this would merely establish a connection to the router - at this point it
is still isolated, just merely known, see the points above.)
* Capabilities and delegation in general. I like Tahoe-LAFS here too, the
key is the credentials. You can have access to one thing or a few or
everything. Read, or read/write.
I'm too tired to be verbose here, I'll add more comments later.

@_date: 2015-10-26 18:52:14
@_author: Natanael 
@_subject: [Cryptography] "Digital" Love Locks? 
Den 26 okt 2015 16:11 skrev "Henry Baker" :
What should be accomplished? My assumptions;
* The creation of some symbolic message
* The message is created together by both participants
* It is somehow made non-malleable
* It is public
* Something is thrown away to symbolically seal it
Actually this mechanism I described here could be reused almost unmodified:
We use threshold cryptography to allow two people to do it together.
We modify libforwardsec with their forward secure public key encryption +
hierarchical IBE with puncturable encryption into a signing algorithm with
otherwise the same properties.
The two participants then sign the message together with a group threshold
keypair linked to (derived from?) both their personal keypairs, to then
revoke (puncture) their ability to sign another message with the same IBE
"identity" (in standard use of libforwardsec assigned to timeslots, but can
also be assigned to different "types" of messages). If either one of the
two punctures their keypair, then the threshold cryptography makes it
impossible to sign another message to replace the first.
If the IBE identity is chosen to be the "type" called "love commitment" and
they sign the most recent blockchain hash for timestamping and then publish
the signed message on the blockchain, I think that accomplishes the goal
effectively. A key is then thrown away when leaving a public message both
parties commit to. And you can easily confirm both parties' keypairs were
used when creating the message.

@_date: 2015-09-19 10:54:17
@_author: Natanael 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
- Sent from my tablet
Den 19 sep 2015 08:02 skrev "Henry Baker" :
Private Information Retrieval algorithms? Multiparty Computation protocols?
Fetching over Tor / I2P / VPN?
Browser plugin that looks for "advertisement box tags" and fetch the json
configuration files to see what keywords should have it make anonymous
requests for what, to display rendered above the page such that the page
javascript can't figure out what ad is displayed?

@_date: 2015-09-23 13:19:49
@_author: Natanael 
@_subject: [Cryptography] Non-Authenticated Key Agreement 
d'' = d XOR ka XOR kb
d''' = d XOR ka XOR kb XOR ka = d XOR kb
All off these are public. See the problem yet? Hint: Key reuse.
kb = d' XOR d''
d = d''' XOR kb
Or in other words, d' XOR d'' XOR d''' = d. This is because each XOR key is
repeated in even numbers, where each version have exactly one difference in
which ones are applied. A XOR B XOR B = A.
XOR encryption is only considered secure if the same unpredictable key
material NEVER is used for more than exactly one plaintext
Even if you managed to make it work with uneven numbers of applications,
the reuse still reveals statistics about the plaintext.

@_date: 2015-09-25 12:19:01
@_author: Natanael 
@_subject: [Cryptography] Wrongware: was VW/EPA tests as crypto protocols ? 
Den 25 sep 2015 04:13 skrev "Ray Dillinger" :
testing protocol.
start investigating these types of misbehavin' SW.
Related: That one is about what's essentially scams deliberately encoded in user
interfaces, such as hiding adware under advanced options in software
Wrongware could be a more general term, encompassing this as well as
software that's not targeting the user, but rather something else - the
intentional error might not affect the user, but another target entirely.
This would be different from bots and other malware - it would not have to
be an active function that target another computer system. Fooling an
auditor would definitely be part of it just like here, but could apply even
in finance and law and not just for machinery.
More generalized, whenever certain behaviors are unwanted for good reasons
by interested parties (regulators, auditors, courts, and civil
organizations) and the software promises either to NOT enable the behavior
or to directly PREVENT it, but yet intentionally doesn't act as promised as
a result of various incentives (profit, career advancement, etc), then that
would classify as wrongware.
FSF's project seems relevant here: This one is about software that screws over the user.
In short, design for auditability.
Somebody else also mentioned the independent probe idea in the previous
mails, and in the case of cars that is fortunately not very hard to measure
in most cases. In the case of software it is typically incredibly much more
difficult - just defining that you need to measure is hard enough already.
But it has to be done.
Even Zero-knowledge proofs would not necessarily prove what you need to
prove - they can only show a certain computation has happened - they can't
prove you have NOT also done something else. For example you could have a
separate program extract what's supposed to be secrets from a program which
is running in a ZKP generator that promises to protect the secrets. There's
a variety of systems based on trusted CPU designs that promises to achieve
assurances for this, but I'm not sure if any are as good as they promise.

@_date: 2016-04-04 11:52:30
@_author: Natanael 
@_subject: [Cryptography] Near field and crypto 
Den 4 apr. 2016 08:48 skrev "Tom Mitchell" :
1: first you have to extend the range compared to standard readers. You
can, but then you'll often end up reading multiple tags at once, and the
energy usage is going to spike. Using many medium range antennas spread out
is easier, but more expensive. (you'll also be triggering many phones to
make an "NFC tag detected" noise)
2: you can still separate multiple simultaneous readings (within limits,
perhaps you can read 5-6 tags from one antenna before you only get noise),
but that's getting complicated and require fancier electronics.
And that's for passive tags - simultaneously talking to multiple active
tags will be incredibly hard. Another argument for many short range
antennas over long range antennas.
3: passive static tags are easy to identify people by. However, not all
tags have static strings, and not all phones and other active NFC devices
transmit any static strings by default.
(there's however another way to identify people here - by how many NFC
devices they have of each type).
4: this is yet another reason for why I want to see anonymous mutual
authentication algorithms, where nothing identifying is discernable UNLESS
you're one of the parties in the authentication AND both parties already
shared some identifier / have each other whitelisted. Wireless devices
should be indistinguishable from each other until they make the choice to
identify themselves.
This could technically already be done with multiparty computation
algorithms and some variants of private set intersections (the latter only
for shared secrets), except that's sloooooow.

@_date: 2016-04-04 16:55:58
@_author: Natanael 
@_subject: [Cryptography] Secure universal message addressing 
I'm crossposting this to a few lists, a few of the relevant mail archives
are here for those who want to follow the replies on the other lists;
After spending way too much time thinking about how to design a secure
universal message passing platform that would work for both IM, email, push
messages and much more, I just ended up with a more complex version of XMPP
that won't really ever have lower latency, be scalable or be simpler to
operate or even be secure at all. So I dropped that idea.
Then I ended up thinking about addressing instead. If building one single
universal communication protocol is too hard, why couldn't it still be
simple to have one single universal protocol for identifying recipients /
users? It would allow each user to have one single unique global identifier
which can be used to find out which communication protocols each other user
supports and how to connect to them.
Email address type identifiers are already familiar and won't go away. We
are practically already heading this way with email too;
Given that we have the combo of protocols like DMARC, DKIM, SPF, TLS,
DNSSEC+DANE and of course the very relevant WebFinger for user profile data
( and that we have companies like Google, Yahoo and
Microsoft working on SMTP STS, we are already building much of the
infrastructure necessary for secure addressing. Every server / node would
continously check which one's the strongest protocol supported by each
other server / node they've communicated with recently, refusing to connect
over weaker protocols.
Add in transparency logs (perhaps Keybase style) and you've got very strong
security for the users. Stopping downgrade attacks suddenly becomes more
than plausible when there are reliable ways to detect if a server truly
supports secure protocols or not, and MITM becomes hard to hide if the
sending client / server / node always logs the recipient's server's
responses (making forged replies trivially provable, hurting the server's
reputation in seconds by publishing the conflicting log entries). A
Perspectives / SSL observatory model would also drastically improve the
detection rate for tampering.
That setup just lacks one capability which I consider major - playing well
with P2P networks lacking classical domain names. Not all users will be
using (fixed) servers (or even IPv4/6 addresses), so perhaps the domain
part of email type addresses could be a domain name format that specifies
that it identifies a P2P node and its protocol (like a Namecoin profile or
an I2P node holding your profile data, or a CJDNS node). Including public
key identifiers in the addresses would most likely be necessary, unless
you're dealing with protocols like Namecoin for fetching profile data.
Those who wants to place their own P2P nodes in the domain field could do
that, having that node carry the profile data which explains how to connect
to you (which would of course require that those you communicate with can
connect to your P2P node), instead of using a third party server. Most
people probably won't opt for this, but it should be possible.
Where the users or (end user trusted) servers are identified by public keys
in the addresses, it could be possible to have public translating proxies
for P2P protocols (kind of like i2p.to and the Tor equivalent, "inproxies")
without any loss of security.
The user experience would end up looking like Keybase.io, but with their
account ownership proof process looking more like the OAuth process
(usually initiated via the third party service/client by entering your
email to register after logging in), where most likely it would be your
existing email provider offering this addressing service.
Every messaging system you add would be linked to your account, both server
based ones and P2P ones (with their respective unique user identifiers),
allowing anybody who want to message you securely to detect that you
support protocols with better security than the arcane SMTP. If both sides
supports this protocol and a hypothetical email2 protocol that's tagged as
an upgrade over SMTP, no mail between you would ever be sent over SMTP. As
more email servers upgrade they would quickly start to detect that the rest
of them also supports stronger security, and upgrade the security for all
the users silently.
It would behave like account addressing within Google's suite of protocols
such as email + IM via Hangouts + Google Cloud Messaging + Google Docs,
etc, where the same email address is the account identifier for them all,
except that this would be an open universal protocol.
The recipient of a message from a third party service you started using
wouldn't be getting an invite email (invite emails are getting boring,
aren't they?) telling them to register to see the message - instead that
service would see which compatible service the friend is using and connect
to it, and pass it on.
We need secure push messaging, IM, mail and much more, and if we can tap
into the existing infrastructure through email and make the user experience
both easier AND more secure, we are much more likely to see secure versions
of all these protocols implemented. If connecting secure protocols to your
account is easy and transparent for everybody involved, there would be much
less resistance towards changing clients.
Another big benefit is that contact management will be much easier (most
likely also hosted via your mail provider like it usually is today, except
now it would make sense to have a single one shared across all your
services), because suddenly you no longer need to be given every single
username for every service a person uses separately. Opening the contact
details for a person would simply show you which protocols you both already
support, and which additional ones they support that you don't.
You only need one single username / address per person, and even if they
were using a server addressed profile (like standard email addresses) and
switch servers, they could set a redirect to the new address that notifies
you about the change the moment you tried to contact them again. It would
be completely effortless.
Here's the existing systems I know of which comes close, with comments;
* The mix of email security protocols I mentioned above. There's no clear
standard for how to achieve this with them. One could build on WebFinger,
but how should it look like?
* Namecoin. A public P2P blockchain system, not universal. Requires a full
node or connecting to a trusted third party node for lookups.
* Keybase.io. Not designed for addressing, URL identifiers.
* PHB's Mathematical Mesh (under development). It uses a private blockchain
per user for something resembling transparency logs, and uses identifying
public keys (IIRC), but I haven't read up on the details on how it works. I
don't know if it works with email address based identifiers.
The key idea here is that you get to have *one* identifier for yourself
under your control, that you can use everywhere, securely. Knowing that
people have your real address should provide a strong guarantee that
messages from them to you will go only to you. And you shouldn't need to
change address because you changed messaging services.
How would you guys go about designing a system like what I describe? How
would you get it to play nice with P2P nodes?

@_date: 2016-04-04 20:11:15
@_author: Natanael 
@_subject: [Cryptography] [Endymail] Secure universal message addressing 
Den 4 apr. 2016 19:23 skrev "Sean Leonard" :
protocol or system that it is addressing. See URI.
People see URL:s and think websites, they see email addresses and think
OpenID essentially died. So did Mozilla's Personas. A bunch of RDF based
protocols too. And many many more. They all shared the URI. It was
*something extra* people has to remember and share besides their email.
If we're going with an URI, what do we put in the protocol field? Do we go
for a magnet link type address where you encode an URL in the URI if you're
using a server, and otherwise specify a protocol and public key, etc? Next
question, the more serious one; who'll even try to share that URI? Who's
gonna read it over the phone? Exactly nobody. It will be forgotten.
Or do we go with HTTP + special HTML tags on websites like OpenID did and
go for standard URL:s? Besides the tendency of HTML tags to get broken in
website redesigns, the total lack of an enforceable standard for how
transparency logs could be implemented (how will you even know where these
tags can be found on arbitary domains, since few will accept using your
page naming conventions?), HTML parsers tend to have bugs.
Also, that situation would also practically by default lead to developers
forgetting about other lookup/connectivity protocols, like Namecoin and
I2P, where their addresses won't even be recognized.
That's why I stick to email addresses here. We can actually enforce a
secure way to implement the lookups if we use them.

@_date: 2016-04-05 11:09:20
@_author: Natanael 
@_subject: [Cryptography] Secure universal message addressing 
- Sent from my phone
Den 5 apr. 2016 09:17 skrev "John Gilmore" :
No problem. This is a per-nickname identifier. Use temporary disposable /
throwaway accounts or context specific accounts if you wish. Then you won't
have everything linked to the same account.
The type of tech Mozilla Personas (or U2F) was using to anonymize the
original account you connected with can be reused, although that would
break the universal addressing aspect.
Or how about this - you can link multiple profiles / personas / nicknames
to your account, including creating throwaways, and get to chose which one
to link third party services too when you register with them.

@_date: 2016-04-19 20:08:38
@_author: Natanael 
@_subject: [Cryptography] [cryptography] Secure universal message 
Den 19 apr. 2016 19:28 skrev "John Levine" :
Why can't it be rephrased as a crypto problem? That means we have a large
stream of transactions where a total lack of history / provenance is normal.
Why aren't we verifiably tracking the path of all such critical data and of
who create and approve what transactions? Why don't we apply crypto here
with for example signature chains (or even fancier crypto like mergable
group signatures to save space), where every system and user involved
involved literally signs off on the data?

@_date: 2016-08-17 10:56:04
@_author: Natanael 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
Den 17 aug. 2016 06:48 skrev "Ray Dillinger" :
Just like how Bitcoin solved the unsolvable Byzantine General's problem by
ignoring the original problem statement and going for a probabilistic
approximation, Bitcoin's got a solution here too.
You know Bitcoin's smart contracts? And how it supports multisignature
transactions, and timeouts? And that it's got "payment channels", where two
people can merge all their transactions they performed during a timeperiod
into just two (a commitment transaction and a settlement transaction on the
blockchain, where the two parties simultaneously keeps a non-final
settlement transaction ready in memory until they're done?
Now combine them.
You create a "wallet" where you commit money to a payment channel against a
Lightning Network routing node (server) for a time period, where that
server can't steal from you (because you never signed a transaction that
leaves the server with all your money). The network of Lightning Network
nodes has payment channels as their backbone links too.
So it kind of becomes an email for instant high volume transactions, with a
blockchain to settle on from time to time - you tell your server who on
what server to send X coins to, and sign the corresponding transaction to
enable it to transfer that sum. All the involved payment channels are
updated accordingly to reflect that transfer of money, updating the
non-final settlement transactions being held in memory.
Even freezing funds would fail, because after a certain amount of time of
non-cooperation from the server it will be possible to just withdraw it
You could question if they wouldn't just reverse a set of transactions that
pay out money and try to run with what's been committed to them. Of course
there's a number of reasons, based on game theory;
The total sum that the server will be able to claim for itself at any
period in time will be relatively small compared to the sums involved (any
attempts at inflating it would trigger a large number of warning systems
that flag unusual behavior). Thus the reward and incentive is small.
Any sign of reversing any transaction is provable thanks to settlement
transactions being signed and mutually updated (you just show the world the
signed transaction that the server reversed through sending a prior one to
the blockchain). Thus there's accountability, and high risk of getting
Because malice can be detected so quickly, the timeframe for attacks given
the automated systems involved is limited to mere seconds before you get
The above points combined with that trust takes time to build, and that
fees for processing transactions is likely to be more profitable (analogous
to the Bitcoin mining situation), the incentive is to follow the protocol
and proceess transactions as told by the users.

@_date: 2016-08-18 03:21:02
@_author: Natanael 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
Den 18 aug. 2016 00:31 skrev "Jerry Leichter" :
by ignoring the original problem statement and going for a probabilistic
approximation, Bitcoin's got a solution here too.
Bitcoin into some bit of magic?
Not trying to make it seem magic. Just emphasizing that the approach is
fact, the very paper that introduced it also provided a solution.
I guess I should have added more detail.
All previous solutions required gatekeepers, because they all failed if the
adversary could perform a Sybil attack (flood the network with nodes). Even
the more recent Ripple type systems use centralized gatekeepers, and
proof-of-stake merely distributes the role of the gatekeeper across more
nodes (the nodes representing the economic majority within the system), and
the classical concensus systems (not designed for cryptocurrencies) mostly
rely on a centralized PKI.
Even then it isn't solved in the mathematical sense in most circumstances
(always having all nodes reach the same conclusion) because of information
asymmetry - everybody must know the same thing, because if they don't then
you'll lose determinism and any guarantee of a resolution. This requires
stable networks and near perfect uptime for the majority of nodes.
Malicious nodes doesn't have to stick to just voting maliciously, they can
even attack the network connectivity and withhold messages. Then what? Now
you have *no* guarantees. In fact your network could get completely stalled
despite having functional honest nodes still running and talking, something
which wouldn't happen to Bitcoin.
Those systems that manage to guarantee a *quick and useful* resolution
using objective metrics of the messages themselves will typically also
strictly require that *all* nodes are trusted (for example by completely
trusting timestamps of all messages, with highly precise calibrated
clocks). That makes them much less useful in most circumstances, because it
just isn't practical or even safe to most people.
Bitcoin took proof-of-work (and hooked incentives to it, too keep it alive)
to much more reliably achieve a quick resolution (chained proof-of-work
makes for a very quickly verifiable objective metric, so you don't get long
lasting netsplits because of disagreements), and it doesn't need
gatekeepers. Instead it relies on game theory to attempt to assure an
honest majority of computing power.
There's no collection of all votes from all online peers during every round
one chain with accumulated proof-of-work exceeding all others. Bitcoin has
a completely deterministic selection mechanism, given a set of chain forks
with different accumulative proof-of-work. No need for individual
crosstalk, just a series of global broadcasts.
Without network instability you have a beyond 99.99% chance of resolution
before any competing forks both reaches 2 blocks length from after the last
shared block. Even with most kinds of real life severe network instability
you'll usually get the supermajority onto the same chain within ~3 blocks,
as long as it gets propagated to the miners.
may not solve a different problem, but let's stick to reasonable use of
language here.
I was hoping my intent was clear;
The original problem statement often makes implicit (or sometimes explicit)
assumptions that are false, for example by having overly strict
requirements, or not actually being representative of the real environment.
Bitcoin rejected the need of a gatekeeper, rejected the need for a 100%
mathematically guaranteed solution (can't have that without gatekeepers
anyway, so no use of fancy selection / voting math), rejected the method of
identifying individual nodes and shifted to proving computing power
instead, etc...
So that's at least 3 things it did very differently as a result of changing
the underlying assumptions. And as a result of these changes it eclipsed
absolutely all other cryptocurrencies, because this combination of features
proved more desirable in practice.
completely general model had no solutions if 1/3 of the participants were
traitors.  That's just true, and no amount of "probabilistic approximation"
(whatever exactly that is) changes it.
In Bitcoin you need 51% malicious computing power for that (assuming no
network isolation attacks, although even those can be circumvented by all
miners broadcasting their blocks over radio), otherwise it will with very
high probability settle on one single globally shared chain.
There's a lot of talk about "selfish mining" that would lower the
percentage to ~33% in the worst case in theory, but it requires that the
other miners are naive, that *nobody else does the same thing* (if
everybody does it as it nearly cancels out itself when everybody does it
(with the exception for marginally improving the profitability for larger
miners/pools) and requires that those doing it are well connected in the
network (which is difficult if you chose to not participate in the
efficient relay networks that most big miners participate in, or if you get
blacklisted from it due to abuse).
Even 3x 33% selfish mining pools will be somewhat stable and equal in
power, still advancing the blockchain. A continous dance of two steps
forward, one step back, still advancing the blockchain over time with a
99.99% probability.
Thus as I said, Bitcoin goes for a probabilistic solution.
Even a +45% selfish mining pool can be defeated by the rest of the network
who'd quickly detect it and who could coordinate to reject its blocks, or
who could temporarily act as a larger pool that selfish mines against the
smaller pool which initiated selfish mining. By simply responding in ways
that makes it unprofitable / counterproductive to do selfish mining it
wouldn't last.
unforgeable message signatures, a deterministic solution exists with any
number of traitors.
This still requires both gatekeepers and total information symmetry,
because otherwise not everybody will know about the entire set of nodes and
all of their messages for certain. This is just basic physics / information
theory as you need to know the exact same things to always reach the same
conclusion, and without gatekeepers you can Sybil attack the system
trivially and roll it back however much the system will let you in order to
alter old decisions arbitrarily (deterministically guaranteed successful
malice also isn't very desirable).
As a contrast see proof-of-stake with its nothing-at-stake problem, in
which the lack of an objective chain selection metric + not having any
ability to effectively prevent cheap re-voting / rollbacks leads to
unstable concensus. It is even easier to attack when you add network based
attacks (given that there's no way for all nodes to be aware of all other
online nodes), as you can abuse netsplits to build up power to approach an
unassailable 51% economic majority.
In fact, most proof-of-stake selection mechanisms can even be gamed such
they start to resemble a poor-man's CPU-bound proof-of-work mining
you want to use actually need, but if they include enough to have
unforgeable message signatures, a complete solution without any kind of
approximation is at hand - and has been for years.
I should have emphasized the requirement of avoiding gatekeepers.
Because with cryptocurrencies, you don't want something that can be
trivially taken down permanently with a few lawsuits or criminal charges.
Or have it die because of a lack of VC funding.
Or being regulated to death. Or die because it can't be used across
And you absolutely DO NOT want netsplits with currency (just see the poorly
managed Ethereum hardfork!), you want automatic global and consistent
resolution to occur quickly.
Bitcoin on the other hand lives as long as there's enough interest.

@_date: 2016-08-18 11:24:48
@_author: Natanael 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
Den 18 aug. 2016 07:11 skrev "Jerry Leichter" :
In fact, the very paper that introduced it also provided a solution.
the adversary could perform a Sybil attack (flood the network with
Generals problem.
It is the generalized one as described here:
against Byzantine failures, in which components of a system fail with
symptoms that prevent some components of the system from reaching agreement
among themselves, where such agreement is needed for the correct operation
of the system. Correctly functioning components of a Byzantine fault
tolerant system will be able to provide the system's service, assuming
there are not too many faulty components.
not only cast a vote for a suboptimal strategy, they may do so selectively.
Bitcoin trades "not too many faulty components" for "not too many malicious
CPU cycles". *You can't cheaply introduce a majority of nodes voting for
your own desired outcome*. It also uses broadcasts for communication
instead of needing to talk to any given set of peers.
The nodes WILL agree on which chain has the most accumulative proof-of-work
as long as they know of the same forks, and as long as they've got
different amounts of accumulative proof-of-work (although not immediately
guaranteed they do when of equal length, across just a few blocks, it is
overwhelmingly likely over time given the difficulty retargeting
As I explained before, it does this so that you don't need to rely on a
central gatekeeper. And not having one is why it got so much interest.

@_date: 2016-08-19 18:48:19
@_author: Natanael 
@_subject: [Cryptography] Confidential Document Management, 
Den 19 aug. 2016 06:29 skrev "Phillip Hallam-Baker" :
messaging infrastructure built on the Mathematical Mesh that uses Proxy
Re-encryption to allow control of access to confidential documents.
data that are confidential for some reason (e.g. the powerpoint slides
describing PRISM) that must be stored on enterprise controlled servers
managed by people who do not have a need to know the contents of the
material they manage (e.g. 29 year old contractors).
would surely want to restrict the distribution of such confidential
material to exactly the set of people with a need to know. And this is
where proxy Re-Encryption is such a powerful tool.
security label (e.g. prism at nsa.gov)
right to administer that security label (e.g. Col. Mustard)
administrator and correspond to a grant of read access to the material
(e.g. Cpt Prang).
baggage that the DRM and CRM terms focus on. My objective is to control the
distribution of confidential documents so that these are secured end-to-end
and are not visible to administrators of servers, anyone who might find a
thumb drive or buy a hard drive off EBay.
too weaselly?
My first thought went to completely different keywords, regarding the
naming. Proxy re-encryption, for example, reminds me of blinded signatures
in both purpose and mechanism. And the whole point is to make it easy and
safe for those with access (in possession of decryption keys) to give
another person in the organization access.
And the keys we want to manage with it might not always be used just for
documents, but maybe as credentials for controlling various systems
(continuing the NSA examples, keys for read-only access to XKeyScore vs
admin access).
So why not something like these;
Blinded Credential Management
Blinded Credential Forwarding
Blinded Document Access Forwarding
Confidential Key Management
Confidential Access Management
These names should all be self-explanatory in context.

@_date: 2016-08-31 17:46:02
@_author: Natanael 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
Den 31 aug. 2016 08:39 skrev "Kristian Gjsteen" <
kristian.gjosteen at math.ntnu.no>:
have keep the key fixed and modify the data.
motivated ECB and CBC. It doesn't require an initialization vector either.
key between encryptions. This is inconvenient in many situations.
related key attacks. As we know, a secure block cipher does not have to be
secure against related key attacks, so you have to redo a lot of analysis.
can do key schedules real quick, but I still have other stuff for them to
do, so not having to do key schedules is a good thing.
choice whether to include an IV or keep a state.
My own preference is one that already has been mentioned a few times
previously: a fast symmetric cipher keyed by a fast (seekable) stream
(wall of text below)
The series of blocks takes consecutive stream cipher keystream bit-ranges
as their keys. This could be considered making the stream cipher your key
For the symmetric cipher I'd want a XEX construction, given that you can
get strong security with just a small and very fast unkeyed permutation
using a single XOR key per block - see this paper for reference (also see
the non-XOR variants, like modular addition);
Minimalism in cryptography: The Even-Mansour scheme revisited
The permutation used could for example be any block cipher using a fixed
known key, or any other function with a random 1:1 mapping between
inputs/outputs. Sidechannel resistance is the only major requirement.
The double XOR protects the permutation from cryptanalysis, the XEX
construction protects the stream cipher from cryptanalysis, and the stream
cipher provides unique unpredictable keys to the XEX construction.
Regarding block size: This is essentially the same as blockwise XOR instead
of bitwise, mathematically speaking (assuming a good permutation). With
very small blocks and partially known plaintexts, the keystream would be
partially exposed since one could bruteforce the individual block keys
(assuming block sizes below ~80 bits, perhaps 16/32 bit if somebody would
want that in an IoT implementation).
These keys then directly represent the keystream, and a weak stream cipher
(for example RC4) would then be easier to break to recover the main key and
thus all plaintext. Assuming the use of a very lightweight stream cipher,
it might be necessary with blocks large enough to protect the stream cipher
from cryptanalysis. Typically 128 bits should suffice. Note that besides
sidechannel resistance, we really only need unpredictability (difficulty to
guess the key stream) from the stream cipher more than any other property,
thanks to the XEX construction.
Given a stream cipher strong enough to resist key recovery / key stream
prediction, small block sizes still should prevent known plaintext attacks,
including distinguishability, given that all block keys are independent
(with computational security).
The block size chosen could be whatever performs best on your intended
hardware, given that you take account for the points above.
The security characteristics of this method should be similar to the block
cipher mode XTS, I believe. XTS uses two layers of symmetric encryption: it
first encrypts the disk blocks using counters (producing a static
ciphertext), then it encrypts the plaintext using both per-disk-block
counters, the disk block ciphertext and the plaintext as inputs.
XTS is resistant to plaintext leakage in the case of key reuse, as all you
can see is when you have identical plaintext + key pairs in the very same
place (unlike CTR which breaks under key reuse). It is also resistant to
controlled bitflipping unlike CTR, and prevents plaintext injection unlike
The problems with XTS is the relatively low maximum ciphertext limit due to
its particular construction and usage of a 128 bit cipher block size (can't
encrypt more than some few terabytes max - less than many harddrives can
store) together with limited performance.
Somebody else described it by saying XTS has the ECB penguin security flaw
in the time dimension: when something's edited in place you can see the
position of the changes, and which ciphertext blocks that are identical /
different. This method would behave the same.
Given that both the stream cipher and the XEX construction individually can
be lighter than AES, we could be able to get similar security to something
like AES-CTR with potentially higher performance, and it would definitely
beat XTS in performance.
A stream cipher with large enough internal state could answer generate a
large enough keystream securely to encrypt much larger amounts of data,
including very large harddrives.
for a whole datacenter with a single key and remain secure. Chacha20 is
also generally faster than AES except when the CPU has AES acceleration
circuits. It also has a security level comparable to AES.
With a construction like this (where the XEX construction hides the key
stream even given partial known plaintexts, assuming large blocks = few
collisions) we could even use a stream cipher which is lighter and faster
than Chacha20 and still remain secure.
A XEX construction is also very easy to parallellize, since the middle
function always is the same and XOR is trivial. A seekable stream cipher
will likewise be easy to parallellize (although initialization might not
always be quick). If both components also are chosen to be efficient in
hardware you could get very high throughputs with a little
By comparison the common cipher mode AES-CBC can't be parallellized,
hurting performance in many circumstances, but is still weaker than this
method. Many other secure cipher modes are also hard to parallellize.
It would be preferable to also pair it with key/IV reuse tolerant
authentication tags similar to the GCM-SIV construction, just for
additional robustness.

@_date: 2016-12-04 01:21:48
@_author: Natanael 
@_subject: [Cryptography] Final words on RNG design 
Sorry in advance for making a new thread. But I thought it might  be
simpler this way - a lot of important information is scattered all over,
and some things I think is important is missing.
Note: this is fairly long. I've tried to keep the wording and formatting
Feel free to add sources to the statements herein.
Feel free to point out mistakes (but please keep it short!).
And feel free to add to anything *important* that is missing.
* What an RNG is needed for;
Anything that requires unpredictable elements.
TLS key exchanges needs unpredictable ephemeral secrets even on the client
side, tons of IoT devices needs to generate keypairs, and that's just the
* How should we define entropy?
The computational definition is the most practical one.
Given our sources of randomness and our pool of collected bits, and given
all of the information and resources accessible to our adversaries, the
computational entropy of the pool is defined as that which is the minimal
amount of computational effort ("work factor", "bruteforce", etc...) that
the adversaries would collectively need to recover the state of our pool,
or to otherwise predict its outputs (expect the adversaries to collaborate,
voluntarily or not!).
The key is that the outputs are unpredictable = out of reach from being
cracked for as long as deemed necessary.
* How the RNG interface should behave;
You ask the system for X bits. You get X bits. Quickly. Always.
It should be easy to implement this request in all relevant software
Those bits shall be indistinguishable from random and can't be guessed in
advance with a chance better than random - those X bits shall represent X
bits of *computational* entropy (see "defining entropy"). They can't be
used to guess future bits, or to derive previous bits. Those bits are given
to you exclusively.
* Who needs to provide the RNG, and what they need to do and consider;
The system designer (for various definitions of "system").
The platform should offer a secure source to all the applications running
on it.
If it can't, chances are you already lost.
The hardware should always have a trustworthy TRNG / CSPRNG that comes with
accurate data on its characteristics, for use by the firmware / software.
The hardware should be auditable, so that the behavior can be validated.
The goal is to allow you to be certain of how much "genuine" entropy that
has been produced at minimum.
Beware that some hardware sources hold large states, but accumulate random
changes (entropy) slowly (lava lamps? Lavarand is already a thing). Asking
for 64 bits four times may give you 64 bits of entropy first, then just 5
new bits on each invocation seconds later.
Metrics like "state flush time" (or "entropy displacement time"?),
"initialization time", "entropy generation rate" and similar must be part
of the available characteristics of hardware sources. Otherwise the
software estimating the entropy can't provide any guarantees. The ability
to detect faulty behavior is also extremely important.
The hardware sources should all be resistant to tampering and fault
injection and similar attacks.
The opinions on using multiple hardware sources are divided. I'm personally
in favor, for the sake of redundancy and defense in depth (increasing the
cost of compromising all sources), perhaps ~4 hardware sources of different
designs. Others believe multiple hardware sources increases complexity and
attack surface.
The OS / firmware MUST be configured to use these hardware sources, and
MUST know their characteristics. If you have a perfectly trustworthy
hardware TRNG that produces 1 bit of entropy per 100 bits out, then you
don't get to consider the pool as filled after feeding it 256 bits. You
need to know when you can be certain to have enough bits, and knowing the
characteristics is the key to it.
If you have multiple sources, you may only add up their individual entropy
estimations if you can guarantee they're all sufficiently independent.
Breaking one should not let anybody break another, and thereby completely
breaking your RNG through simply modeling and bruteforcing the majority of
your sources (such as if your 10 hardware sources all produces just 32 bits
of entropy, all predictably correlated).
If you have multiple machines all belonging to the same entity then entropy
may perfectly well be "provisioned" in the form of seed files, through a
central server, by temporarily plugging in a TRNG, or with similar
You don't care where the hardware source is, only that adversaries can't
guess the outputs. You probably don't consider yourself your own adversary.
Remember that this provisioning too must happen securely (hard to spy on).
In the case of provisioning entropy to tiny embedded devices, it isn't
entirely unreasonable to use private keys as one of the entropy sources for
initial seeding. If doing that breaks something, you've got worse problems
(such as weak keys or algorithms).
Remember that you still need stored state for a CSPRNG to not repeat.
* How the RNG should behave internally;
It shall verify that the hardware RNG is behaving according to the
characteristics, and adjust entropy estimates thereafter. Faulty sources
shall be detected and estimated to 0 (zero) bits of entropy.
The entropy pool must hold a large enough internal state that you can not
possibly enumerate all possible secret states, i.e. at least something like
256 bits or more. It shall at minimum match (or exceed) your defined lowest
entropy threshold.
Mix in fresh entropy securely - personally I favor information theoretic
approaches like "multisource entropy extractors", but hashing and other
such common existing solutions are likely sufficient. Mixing shall never
cause a loss of entropy within your pool.
Don't ever generate any outputs before being sufficiently seeded!
Why? Because you'll effectively leak the internal state, making it possible
to bruteforce the pool contents and the permanently break the security of
the CSPRNG (that is, if it is never reseeded in between outputs with enough
bits to prevent further pool recovery through bruteforce).
The threshold for "sufficiently seeded" (having enough entropy to generate
outputs) should be somewhere around 256 bits of entropy for a good security
margin, which includes defending against Quantum Grover's algorithm. See
the argument in "defining entropy" above.
Reseeding may be done occasionally if you believe your system might leak
secret bits from the pool through sidechannels. When you reseed, attacks
like the above suggest that you should NOT inject the new entropy into the
main pool until you've accumulated enough new entropy for it to be
unpredictable by itself (reaching above your entropy threshold).
Beware of complicated constructions;
Linux recently had a flaw where new injected entropy wasn't spread across
all the bits in the pool, and not all bits in the pool was used
simultaneously for deriving outputs - with their particular design, this
meant that even seeding 256 bits after a state compromise would (for a
while) still allow for bruteforcing all the new entropy piece-wise, yet
again recovering the full internal state!
Seed the entropy pool quickly! And once sufficiently seeded,  just quickly
generate outputs as requested and never block again.
Nobody likes to wait. Your hardware sources must (collectively) be quick
enough to meet your time requirements.
Only use strong cryptographic algorithms to generate the outputs (CSPRNG -
some platforms use modern stream ciphers, which should be sufficient).
Beware of output limits on algorithms, given that many are capped at
something like 2^64 or even 2^32 outputs before security no longer can be
guaranteed, and then you MUST reseed - on long lived public servers, this
is a genuine problem.
It must be rollback resistant (equivalent to the term Perfect Forward
Secrecy in TLS and Signal).
VM:s and similar platforms / runtimes must ALWAYS be reseeded on cloning /
initialization. This extends to all their key material! Otherwise you may
leak key material or plaintext through for example stream cipher key reuse.
On the software side, it must be impossible for an output to be duplicated
and given to multiple applications. The RNG needs to perform well, and if
it is threaded then it must be written to handle that safely.

@_date: 2016-12-04 16:39:02
@_author: Natanael 
@_subject: [Cryptography] Final words on RNG design 
Den 4 dec 2016 13:37 skrev "Jerry Leichter" :
This goes wrong right from the get-go.  A completely random source whose
outputs are leaked to the opponent - even long after they are produced - is
completely useless.
Perhaps I should have explained from the start that I only categorized the
statements into types of properties necessary, trying to stick only to
those that are necessary (some of them with justifications), instead of
making a "linear" argument for one particular way to design an RNG for
cryptography newbies.
This was rather a checklist for what you can't forget, a collection of
critical facts. It was *not* sorted by importance. In other words - to miss
any one of these facts when you design an RNG puts you at tangible risk of
*some* real world attack.
As you later noted, I *did* address this particular fact later on (although
not in a separate point, I can see why you first missed it if you skimmed
it). I do think it was clear enough from the original message that it is
important to prevent leaks, but I guess you disagree?
As I mentioned, to minimize the risks of leakage you need PFS style
properties (backtracking resistance) in the pool to not compromise past
values if the system later is compromised, and you need strong resistance
against any kind of predictive attacks. And each RNG output is only to be
shared with one application / process, not duplicated. Reseeding also helps
you to recover from state leakage. Was this not enough?
Personally, I try to avoid using the word in this context.
I've seen other suggested terminology, but none that seems meaningfully
simpler or more accurate. I'm happy to change it if somebody can argue for
what to use and why.
In any case, the kind of randomness / unpredictability / secret collection
of bits we need has to be defined in terms of the knowledge we can assume
the attacker to have about the system / sources and the work which they
need to perform using that knowledge to compromise the system (interactions
with the target system included). This knowledge includes observable data
leakage (perhaps another point that should have been clearer). This covers
the entire time span for which these secret bits remain sensitive.
The minimum effort required for compromise sets the bar for how much of
this metric we can claim to have. The metric should strongly correlate with
the cost of pulling off a successful attack.
So now not only are we talking about unpredictability - the wrong concept -
but you're making assumptions about the kinds of attacks that can exist
(recovering the state of the pool) *as part of your definition*.  That's
like defining the security of a protocol that uses a block cipher in terms
of attacks that recover the key schedule.
I did also say "or otherwise predict the outputs". I'm aware some RNG
designs (particularly ones based on asymmetric primitives) can be
compromised without ever knowing the original raw pool data.
What's the right concept? How would you phrase it? Perhaps I should have
phrased it in terms of making it computationally infeasible to in any way
compromise the RNG or applications using it...? How would you define
And now I remembered another important statement on the sources, which I
previously forgot to add;
It must be infeasible for an attacker who controls some but not all sources
to compromise the system RNG, through feeding it malicious inputs. An
attacker may attempt to use such outputs to influence the system RNG such
that it produces outputs that are biased in a way favorable to the attacker
(including but not limited to increasing predictability, making the outputs
distinguishable from random, controlling various bit values in the output,
influencing the results of the application using the output (including
attacking fragile crypto), data exfiltration, etc).
Defending against an attacker achieving control of most sources would
effectively require that the defender assumes all but one source is
compromised, and thus do not assume they have more entropy than their
slowest source produces alone. (Although at this point you should probably
just scrap the hardware...)
cracked for as long as deemed necessary.
Ah, you've finally crossed over from "prediction" to "remaining unknown".
That's at least a good thing - but a bit late in the game.
There's a great deal more.  Some of it is good; some of it is "motherhood
and apple pie".  Some of it gets at the important points that knowledge of
any collection of outputs of the RNG gives an opponent no advantage in
computing any other outputs - but it does this in passing, late in the
game, when it's actually pretty fundamental (but, again, had to pin down).
As I said above - these statements are not ordered by importance or how
fundamental they might be. They're sorted by type. And they're meant to be
descriptive rather than academic precision statements, because otherwise I
would never even have gotten past writing the first sentence. I tried to
avoid ambiguity and "empty" words. I'll try to source most of it later, but
it will take take some time.
I'm not going to try to go through it line by line.  As I said in a
previous message:  We have some good theoretical work now on randomness, on
mixers - and on protocols that rely on randomness. It's time to start with
those and see if we can build on them, not keep playing around with gut
I don't expect anybody to actually reply to every single line. A few
comments are just fine. I was hoping to get all the necessary facts
gathered in one place, ideally with accompanied sources (yes, I'm aware it
likely does not all fit in one email thread, but we need to start
If you have a list of such theoretical work to share, please do so. In
particular if any one of them (or collection of them) or other such
resources are complete enough to be sufficient for designing an RNG that
resists all known plausible attacks which it is reasonable to expect an RNG
to defend against.
Perhaps this list and/or discussion fits better on a wiki somewhere. Does
anybody know of a fitting place, in that case? If anybody is interested and
want to help collect sources I can provide edit access to the reddit
place is better.

@_date: 2016-12-24 03:16:40
@_author: Natanael 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras, 
Den 24 dec. 2016 00:22 skrev "Jerry Leichter" :
pictures but never show them (without some key that was not resident in the
I never thought of it this way before, but curiously ... iOS has almost
exactly such a mode:  You can take photos from the lock screen, without
first unlocking the phone.  This begins a "photo session" that continues
until you return to the lock screen (quick press on the top button does
it).  You can scroll back and examine or even delete any photos taken
within a session; but older ones are inaccessible.
Once a session ends, none of its photos are accessible unless you unlock
the phone.
Oh, and the phone can also be configured to upload automatically to the
Apple cloud if it has a connection.  Whether that's a plus or a minus
depends on the situation.
Uploading is something Android phones do, too, of course; I don't know if
they have similar "take pictures from lock screen" functionality.
At least some Motorola phones and Samsung phones has it, probably Sony too.
I'm not sure if it is natively supported in Android.
On at least Motorola and Samsung you can set a third party camera app as
your default, which will be available in this mode.
This opens up for the option to use a camera app with support for public
key encryption in combination with online backup, to fulfill both of these
main requirements (loss prevention and secrecy) discussed here.

@_date: 2016-12-28 20:54:02
@_author: Natanael 
@_subject: [Cryptography] where shall we put the random-seed? 
Den 28 dec. 2016 18:13 skrev "Jason Cooper" :
My idea is to leverage the fact that on boot up, the Installer OS
creates many TLS connections to the distros package servers.  Iff those
TLS connections were DHE or ECDHE, then there is an opportunity to
gather some randomness.
Basically, take the (EC)DHE shared secret, run it through an hkdf with
some data from /dev/urandom, and use the output to do a no-credit seed
of /dev/urandom.  The first connection or two will still have crappy
entropy, but after a few connections, the kernel entropy pools will be
in a significantly stronger state.
The difficulty is that it's impossible to quantify.  We don't know how
strong the peer's DHE random key is.  We hope it's strong, but shouldn't
assume a strength.
The shared secret is a) ephemeral, b) short lived, and c) known to only
two devices.  We could trust the peer, but it makes sense to mix in some
crappy data from /dev/urandom so that even the peer doesn't know the
output of the hkdf.  This assumes that the Installer OS was able to
gather a *little* bit of randomness during boot up.  Which isn't too
unreasonable.  A few interrupts, mac addresses, pci config spaces, etc
means there is at least greater than zero once TLS connections start
opening up.
The reason that I like this approach is that there are no changes to
make to the peers.  No changes to the TLS protocol.  The client just
needs a little extra code to accomplish the task.
Once the installer has done it's job, it can just read 512 bytes from
It should also be noted that this idea would be helpful on embedded
systems where the kernel typically takes 400 to 600 seconds before it
has 128 bits of entropy.  By that point, many TLS connections have been
This doesn't work if your attacker both knows your system and can monitor
all of your network connections.
The reason why - they can (with some limited bruteforce) perfectly predict
your parts of all ephemeral secrets, and thus perfectly simulate your side
of the computations. As a result they can recover the shared session
secrets if they captured all the TLS handshakes.
And we already know that NSA is doing just that as a part of their Quantum
collection of exploits.
Both sides of the network connection needs to have real unpredictable
secrets (keys or ephemeral secrets) for it to work as expected.

@_date: 2016-02-21 23:19:00
@_author: Natanael 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
Den 21 feb 2016 22:11 skrev "Ray Dillinger" :
I too want improved FDE, and am thinking filesystem support is necessary.
In fact, the ideal maximum security solution would probably be a
capabilities type OS design coupled with a user controlled TPM type chip.
Something where the disk starts off encrypted under a single key with
block-wise salts and nonces, where read and write access is controlled by
access to the right tokens. Encrypt the nonce and you now need that
encryption key for access to that file.
Previous writing of mine:
You just described a simplified version of the XTS mode. In it the disk
block numbers are used as salts in layer one, and since blocks can have
arbitary sizes it uses the cipher block number insides the disk block as
salt number two in layer two (IIRC).

@_date: 2016-02-23 09:14:52
@_author: Natanael 
@_subject: [Cryptography] Secure software update protocol? 
Den 23 feb 2016 03:01 skrev "Ray Dillinger" :
Anonymous Credentials, Zero-knowledge proofs, public logs like Certificate
Transparency of all published updates. Prove with math that you're a
customer without revealing which one of them you are. Connect anonymously,
like over Tor.
Or for that matter, run anonymous checksum lookups and compare with other
users (which still can prove anonymously they too are customers).

@_date: 2016-02-26 01:09:58
@_author: Natanael 
@_subject: [Cryptography] Apple: graphically show users that they are 
Den 25 feb 2016 23:40 skrev "Henry Baker" :
and I thought of a change that Apple/Google/etc. should make ASAP. The
reason why most people don't fear govt surveillance and are willing to give
up encryption is that they don't realize that they (their phones &
computers) are *already* under constant attack. It's hard to realize that
you're on a battlefield when you can't hear the bullets whizzing around
your head.
icon that indicates when their phone is being scanned and when someone is
trying to crack their passcode, their SSH login, etc. In addition to
attacks on your particular phone, Apple might also have a threat meter
indicating attacks on iPhone's in general.
also help drive the point home.
Like using your contact lists to allow you to tell your friends about
things like attempted hack attempts against you, both successful and
failed? Showing numbers and graphs for what kind of digital malice that has
hit their own social circle? Subpoena / NSL counts for your extended
Essentially making electronic security warnings a social phenomenon?

@_date: 2016-02-26 01:44:13
@_author: Natanael 
@_subject: [Cryptography] Hope Apple Fights This! 
Den 25 feb 2016 23:39 skrev "Henry Baker" :
In the context of retrieval and cryptography, I'd argue that every location
that holds (by itself indecipherable) data necessary to recover your
plaintext holds a share of the ciphertext, and every location that holds
access and deciphering capabilities (like a server that knows what
harddrives holds shares and what the decryption key is) effectively holds
the plaintext, or plaintext access capability.
For iCloud, Amazon (IIRC) holds ciphertext, Apple holds plaintext access.
For unkeyed solutions like Shamir's Secret Sharing Scheme, each share
remains ciphertext and the access capability may very well be a
non-technical one - the knowledge of where the shares are, perhaps only
stored in your head.
full of random numbers in country  and another file full of random
numbers in country  and so on, so I guess my "data" is in *all* of the
Effectively yes. If it deliberately has been either generated or chosen to
make plaintext recovery possible, then part of the entropy of the original
is encoded there ("projected" or extracted, which one doesn't matter).
of these countries into a form that might actually be useful, so my "data"
is also in *none* of the countries.
They have ciphertext, you have access capability. In the legal context,
only access capability (and that enough ciphertext exists to make recovery
possible) matters when the court want to see the content. Like having a key
to a safety deposit box that they know documents of interest are stored in,
they'll demand it and go get the documents (or demand that you use the key
to get it for them).
Ciphertext location practically only matters if they order destruction of
documents (like in privacy invasion cases) and revoking raw access
capability (destroying all copies of the decryption key) isn't viable or
certain to succeed. (Of course it can also matter if there's contracts or
law demanding that nothing whatsoever leaves a certain border, but that's
more rare.)
numbers that belong to other people, but may also visible to the world at
(Assume that the files are readonly/appendonly for the time being; it makes
the system easier to contemplate.)
actually *public*, so Microsoft, e.g., would simply respond to every
warrant with a browser manual and a "knock yourself out" email.
because everyone can take advantage of the randomness of everyone else's
encrypted data.
Purely practically, a public Tahoe-LAFS grid would work fine. There's one
on I2P you might want to look at. Not much free storage available as far as
I can tell, but whatever that's replicated across nodes will be equally
retrievable from either one of the clients holding access capability, it is
designed for distributed storage.

@_date: 2016-02-26 11:34:34
@_author: Natanael 
@_subject: [Cryptography] Response to "I don't have anything to hide" 
Den 26 feb 2016 03:09 skrev "Matthias Wulfeck" :
their friends and family have heard this response to the question
roots effort to educate our friends of the real risk here (as we see them).
community. How do you respond to "I don't have anything to hide"? How do
you explain to non-technical (and non-paranoid) friends of what's really at
My response would be something like this (fortunately most people near me
aren't *that* naive, so I've never had this "speech" in person);
It is not up to *you* to decide what should be hidden it not. You aren't
the one deciding which actions are to be labeled right or wrong.
Is it your own trustworthy brother or mother that's sitting over there in
the intelligence agency's analysis team or target selection board? Or is it
a complete stranger with morals and agendas and goals and orders that you
don't know, that may be in conflict with yours?
Do you think that your digital trails can not be twisted to be used against
you? Or why not to be used as leverage against your family and friends if
*they* would happen to catch their eyes? Have you not seen or heard of the
examples of exactly that happening before?
Knowledge accumulates.
The more total the knowledge is, the more powerful leverage it can provide.
Even knowledge has network effects (predictive power grows fast). You do
not want the wrong people to get access to that kind of leverage and to be
utterly unaccountable for their actions.
Then you can follow up with sources on proven abuse of surveillance. Like
NSA's loveint, leaked surveillance footage, Snowden documents, etc...
Even if they want them to have access, they must understand that this
demands actively maintained accountability and oversight. If they're not
willing to work continously to verify that, then the default response
should be encryption and other privacy protecting methods.
You either watch them, or you watch out for yourself. (or both)

@_date: 2016-02-28 16:07:55
@_author: Natanael 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Den 27 feb 2016 22:40 skrev "Stephen Farrell" :
"Fringe", "special-purpose" or "specialized", "use-with-caution" (probably
the least likely to be accepted), or anything else that makes it clear that
certain risks and conditions applies since these algorithms likely are far
less audited and likely have multiple lesser known failure modes. Something
that makes it obvious it is only to be used if you understand why the
algorithm even exists in the first place.

@_date: 2016-01-02 23:34:28
@_author: Natanael 
@_subject: [Cryptography] Any Electrical Engineers here who know about 
Den 2 jan 2016 20:58 skrev "Henry Baker" :
Electrical noise (tempest) or signal noise in the OTP encrypted ciphertext?
The former can be solved with paired wires & transistors where the state
isn't 1 or 0, but where it is represented by which one in each pair is 1
and 0 respectively.
For the latter (which I guess you really mean), the pad provably brings up
the noise to a mathematically identical level for all bits, there is no
signal leak. *It isn't addictive noise* like with regular signals such as
electromagnetism or audio. You can't subtract anything based on any filter,
because everything is indistinguishable from random. The whole point is
that OTP eliminates correlations.
As for the proofs, Wikipedia has all the links you need, including fancy
math by academics: In short, the fact that you start with a set of switches (bits) in unknown
starting positions and that  you flip a fully random set of switches means
that you can not tell what the starting points was. Like spinning a bottle
and having somebody guess the starting point after it stopped. All
possibilities have equal probability. For every plaintext bit, you do not
know if the bit was flipped by the corresponding key bit or if it wasn't.
For all plaintext bits, the chance is 50/50 that the key bit is identical,
50/50 that it wasn't.

@_date: 2016-01-03 02:38:37
@_author: Natanael 
@_subject: [Cryptography] Any Electrical Engineers here who know about 
Den 2 jan 2016 23:34 skrev "Natanael" :
electromagnetism or audio.
Autocorrect failure (yet another kind of noise!). Additative was the
correct word.
- Sent from my tablet

@_date: 2016-01-19 23:24:03
@_author: Natanael 
@_subject: [Cryptography] Characteristics of "robust, good cryptography" 
Den 19 jan 2016 20:49 skrev "Notify" :
algorithm, RNG, implementation, key creation/exchange methodology and
hardware platform be robust and good if unknown persons who the sender did
not intend to have access to the plaintext can gain that access to it
through baked-in weaknesses, key sharing or related cryptographic schemes,
without any interaction with the sender and intended receiver(s)?
Insufficient data.
Are you also excluding shoulder surfing and ciphertext size metadata (and
other traffic analysis) and inductive reasoning based on behavior (mostly
data mining on traffic data, but also based on other external observed user
behavior)? Besides that there's also pure software exploits in rendering
software (like browsers) and similar, and "confused deputy" class software
exploits for exfiltration (authorization logic flaws), which *technically*
falls under implementations, but that isn't necessarily normally considered
security software.
Assuming FDE, end-to-end encryption, traffic obfuscation (Tor, I2P), strong
entropy sources, tempest protection, securely coded software, strong opsec,
etc, then good robust systems should prevent plaintext leakage entirely.

@_date: 2016-06-13 03:23:54
@_author: Natanael 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Den 13 jun 2016 02:20 skrev "Ron Garret" :
something from a merchant, there is a period of time where either you have
paid but have not yet taken possession of the goods, in which case the
merchant can reneg by refusing to hand the goods over, or there is a period
of time where you have taken possession but not yet paid, in which case the
merchant is extending you credit, and you can reneg by defaulting on the
debt.  Defections are rare enough that there is usually no formal method
for dealing with them for everyday retail transactions.  In cases where it
really matters, like buying real estate, a trusted third party (an escrow
agent) is employed to effect the transfer.  If someone truly came up with a
way to produce a fair commitment protocol that did not require a trusted
third party, it would destroy the escrow industry.  That is another reason
that I strongly suspect its not possible.
A signs, encrypts with a unique secret key, encrypts the key with a
proof-of-work style time puzzle (parallelization resistant) that takes at
least X time on a single CPU core to compute, where X exceeds the agreed
timeout (the puzzles can be built in parallel, just create many small
chains and encrypt the start of each with the end of another - and throwing
away some bits make it probabilistic and increases the work factor through
bruteforce guessing).
A creates a Zero-knowledge proof of correctness, showing that his signature
under this scheme is valid as agreed, verifiable before the plaintext
becomes accessible.
Demand a signature in response from B within the timeout period.
You can also use cryptographic timestamping of the initiation, etc, so that
you can prove you published your responses as agreed - anybody who cancels
will fail to publish within the time period (to publish a valid response is
by definition to NOT cancel).
The issues remain that ZKP is untested, incredibly slow to generate and
that your usecases are very limited.
Best case: the timeout is weeks to months, but you sign early and share the
key directly after receiving the counter-signature, in a context where you
can predict the maximal single core performance against the puzzle and
where the contents automatically become non-sensitive after the timeout /
deadline. Hardly relevant in any other usecase.

@_date: 2016-06-23 11:26:19
@_author: Natanael 
@_subject: [Cryptography] What to put in a new cryptography course 
Den 23 juni 2016 08:31 skrev "Phillip Hallam-Baker" :
Artisan's Asylum. The plan is to give 8 modules of roughly an hour, two per
week. The whole point being to film the presentation and publish it as free
YouTube videos.
* What's your threat model?
Running on an unprotected computer in the hands of an adversary (DRM,
untrusted hosting services, etc)? No point with crypto, in that case good
old obfuscation is your only chance.
You need well defined threats, and make sure you're addressing them
correctly (and that it is possible at all!
). What would hurt you, how likely is it, and how much effort does it take
to prevent?
* Language security. Heartbleed, anyone? No point in using perfect
algorithms if your implementation leaks your secrets when sent unexpected
inputs. (extension of your point on complexity?)
* Key management matters. Perfect crypto and perfect implementations means
nothing if your system trusts all keys equally, including random ones
(extension of your point on endpoints?). Reference: It also doesn't matter if you leak your keys, or lose your keys (back to
data loss). Also try to use independent keys for separate purposes, instead
of one key for everything.
* Semantics, replay attacks, various creative MITM attacks used to create
crypto oracles out of naive software - What does your securely
authenticated messages actually mean? And can I forward messages you sent
to me to somebody else when I shouldn't? See TLS triple handshake.
Each message should have only one purpose and only be usable in one
context. Every step of every protocol should be designed with the full
environment in consideration.
A security proof within one instance of an algorithm / protocol is useless
if it is running next to another algorithm / protocol where the pair are
decryption oracles for each other (I've seen examples of this before, but
can't remember the source). Context separation is important.
* Sidechannels and metadata. Sometimes it will leak your entire message if
it is predictable enough. Always encrypting Yes to 3 letters and No to 2 is
a trivial example. Sources of sidechannel leaks and metadata can also
sometimes act as oracles (see the point above).

@_date: 2016-06-24 02:53:58
@_author: Natanael 
@_subject: [Cryptography] Digital currencies 
Den 24 juni 2016 01:32 skrev "Ray Dillinger" :
Are you sure of that? Because Lightning Network (LN) is a system within
Bitcoin built on extending what I'd call valid "draft transactions" in
between users and LN nodes, using timelocked multisignature transactions.
At first you create a payment channel between you and your LN node where
you commit done coins. After X time, the commitment expire and you've got
control of then again. Before time X and after commitment, you and your LN
node act together to iteratively assign coins in both directions by
updating the "draft transactions" as a result of payments. With networked
LN nodes, you'll tell your LN node to pay X to Y, so you assign coins in
your payment channel to your node, they assign coins to the node of the
recipient, and then the recipient gets coins assigned to him.
Upon withdrawal your "draft" holds the final tally and gets published to
the Bitcoin network, giving you your final balance.
There's multiple levels of safeguards in here, so the LN node *can not*
steal coins or pretend to have more coins than it really does, and
everything is logged by the users, and they can not seize funds (only
temporarily block transfers). Censorship is the worst plausible attack in
the system. Even internal rollbacks is hard (somebody else would have to
explain why, but as I mentioned users always hold a valid transaction ready
to publish, with their final balance).

@_date: 2016-06-29 12:34:48
@_author: Natanael 
@_subject: [Cryptography] RFC: block cipher randomization 
Den 28 juni 2016 13:24 skrev "Jerry Leichter" :
streams, Zip packages, even compressed streams), I have always fancied
shuffling the ciphertext or the plaintext, as most appropriate, and having
the means of determining the permutation obtained by key expansion or
something equally devious....
you can beat the classic DESX construction - W1 XOR DES(W2 XOR P) - where P
is the plaintext and W1 and W2 are "whitening" keys.  This construction was
analyzed by Rogoway years ago - quick intro at
 - and is
surprisingly strong.  (Granted, its strength is analyzed against brute
force attacks - an issue for DES but not for modern ciphers.)  Still, it
will hide any fixed pieces of plaintext quite nicely.  (And, yes, you need
to apply the XOR both before and after encryption or the construction adds
You can make W1 = W2, see Minimalism in cryptography: the Even-Mansour
scheme revisited
The abstract: In this paper we consider the following fundamental problem:
What is the simplest possible construction of a block cipher which is
provably secure in some formal sense? This problem motivated Even and
Mansour to develop their scheme in 1991, but its exact security remained
open for more than 20 years in the sense that the lower bound proof
considered known plaintexts, whereas the best published attack (which was
based on dierential cryptanalysis) required chosen plaintexts. In this
paper we solve this long standing open problem by describing the new Slidex
attack which matches the T = (2n=D) lower bound on the time T for any
number of known plaintexts D. Once we obtain this tight bound, we can show
that the original two-key Even-Mansour scheme is not minimal in the sense
that it can be simplied into a single key scheme with half as many key bits
which provides exactly the same security, and which can be argued to be the
simplest conceivable provably secure block cipher. We then show that there
can be no comparable lower bound on the memory requirements of such
attacks, by developing a new memoryless attack which can be applied with
the same time complexity but only in the special case of D = 2n=2. In the
last part of the paper we analyze the security of several other variants of
the Even-Mansour scheme, showing that some of them provide the same level
of security while in others the lower bound proof fails for very delicate
Basically, an unkeyed permutation + two applications of XOR or other
functions of similar effect (like modular additions which they also analyze
the effect of).

@_date: 2016-03-03 01:25:26
@_author: Natanael 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Den 2 mar 2016 22:58 skrev "Perry E. Metzger" :
The solution is to let us choose our walled garden, including the option of
building our own. Let us be our own root authority if we wish. Most might
prefer the factory default, but the option to switch mode to insert our own
root so that we can control every security setting and every behavior
should be there for us.
- Sent from my tablet

@_date: 2016-03-03 14:20:50
@_author: Natanael 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Den 3 mar 2016 14:12 skrev "Perry E. Metzger" :
On Android, bootloader unlocking wipes the phone. Having a static lowest
level bootloader and TPM that keeps track of the active mode and chosen
root can enforce this. Something as simple as a hardware token for
enrolling could make it easy - when you get the phone, boot to
reconfiguration mode and plug in your hardware token, load the root key.
The phone resets and now uses your key.
Want to use the factory key again? Follow a simple guide to do a factory
reset from the bootloader, and it resets to its original state directly.
Telling users to always do this with new phones is really the best we can

@_date: 2016-03-08 10:29:41
@_author: Natanael 
@_subject: [Cryptography] Two questions of security 
Den 8 mar 2016 06:14 skrev "Phillip Hallam-Baker" :
I've got plenty of ideas  :)
It seems like the more I read about your idea of the Mesh, the more they
overlap with my ideas of coordinating personal devices and creating digital
"entity declarations", digital representations of identity (of all kinds),
and hooking in permissioned cryptographically authenticated API:s to it. I
haven't written down most of it yet, but you can look at my blog. The
currently most relevant post is the latest one that's covering my key
courier idea.
I should also write down my case for why everybody should have a home
server (one part of it is to have it be an IoT API firewall). And the
Raspberry Pi 3 with its integrated WiFi and Bluetooth 4.1 is very well
timed, it would be an excellent prototyping tool, using it as an
always-online Mesh node could be useful (including for keeping an updated
cache of all your permissions and Mesh configurations, as a proxy between
networks, etc).
Now, for my way of re-expressing the intent of the Mesh;
We used build secure personal sandboxes out of big shared sandboxes on
shared machines.
Now we have so many networked machines with unique capabilities that
sandboxes are just not enough.
The Mesh can let us securely define what each device can do (their
capabilities), who or what they can do it for (their permissions) and then
tell each device what they can ask of others (making it *your* mesh
This way we can make our devices communicate securely in the exact way we
want them to.
We could for example securely tell our garage doors to only accept commands
to open up from our cell phones and car computers. Or a friend's phone, its
your choice. And this extends to all our devices.

@_date: 2016-03-16 23:29:19
@_author: Natanael 
@_subject: [Cryptography] Trust & randomness in computer systems 
Den 16 mar 2016 18:16 skrev "Henry Baker" :
Expirations and death signals have been discussed in the past on the
various crypto lists. No broad concensus yet, but I favor predefined
expiration dates in each and every software component which only can be
updated in a signed software update. And to decrease the risk of broken
signature algorithms, after a second expiration date (?) it can't be
updated remotely, only in-person.
Also, I advocate grateful degradation where possible. An expiration should
where possible only take out networking and other untrusted interfaces.
Where the functionality needs networking to be meaningful or safe, that too
would have to be disabled. A smart fridge becomes just a fridge, a traffic
light becomes a brick.
Components that relies on different security assumptions doesn't need to
all go offline when only a few has their assumptions broken. Traffic lights
for example only need authenticated commands (hash based signatures should
last), no secrecy (perhaps the particular block cipher mode in use breaks),
so only the traffic camera on that same pole goes offline.
My currently favored IoT architecture is an electrically segmented design.
Based on the previous ideas of trusted electrical paths and a necessity
because of  all the security bugs in all of the more advanced CPUs.
First you create a trusted (trustworthy!) controller segment with the
minimal amount of code and circuitry it needs to handle the I/O for all of
a device's local sensitive inputs and outputs. Sensitive is defined as
anything that can cause harm either by accident or malice. Microphones,
cameras, radios, engines, safety mechanisms, etc. This controller provides
an optical I/O gate to the computing segment (no electrical glitching),
together with a minimal carefully defined API.
This controller segment can have multiple groups of electrical paths for
I/O - what it is directly responsible for is connected straight to the
controller IC:s (like an engine).  However, for non-controlling
intelligence-supporting connections (such as sensors that feed the
computing segment with data) there could be direct low-latency lines to the
computing segment although with switches that allow the controller segment
to electrically disconnect them.
Then there's the computing segment. It has all the intelligence, but it
needs permission from the controller segment to do anything (it is an
electrical island, the controller is its only bridge). Even for networking,
as you might have noticed - the controller segment acts as a proxy for it
and can electrically cut the connection. If the controller segment says
it's expiration date has passed, then it goes offline - but up until then
the computing segment can download and pass on signed firmware updates with
extended expiration dates, if one is available.
For as long as the controller accepts commands, the computing segment is
free to send any command over the API which the API offers, and the
controller will happily comply (the controller may have safety limitations
programmed in).
Crypto operations should only happen on circuitry and with code which you
can with reasonable certainty assure won't have sidechannels. Either with
dedicated crypto acceleration chips or on well understood microcontrollers
with safe crypto code adapted for the chip.
Also, for death signals to be effective, the devices issuing them to their
local networks should know what exactly is running in there (for
efficiency, so you don't need to mirror every such signal ever issued,
To do that and preserve privacy, it would be easier if we had a setup like
PHB's Mesh concept where each user has their own master keys and there's a
hierarchy between a user's devices, where you could for example have a
secure home server (which I'm advocating) which all your devices reports in
to and declares their status to (including running software,
configurations, etc). The home server would benefit too from the segmented
design, with one powerful computing part and one dedicated to security
This home server would then be responsible for polling for security flags
and issue death signals when necessary to devices under that user's
control. It would also check for software updates and may very well
download and cache them while the devices they're for are powered off or
otherwise busty.
If the IoT devices at least have a secure connection to the home server,
there might be an option to issue a command to bypass the expiration date
by making the home server an inspecting proxy / DPI firewall on its
external API, by telling the server exactly how to detect and filter
malicious commands so that only benign commands will reach the device. This
way we can also use software to rewrite commands (when possible) for the
IoT device when the old API version gets deprecated and a new one is put in
use, but there's no corresponding firmware update issued.
Any questions, suggestions, criticism?

@_date: 2016-03-22 16:28:39
@_author: Natanael 
@_subject: [Cryptography] And they're off... 
Den 22 mar 2016 16:21 skrev "Phillip Hallam-Baker" :
Swedish, but not much:
mer och mer sofistikerade vilket gr att det r svrare att frutsga deras
kommunicera med krypterad information, sger Ranstorp.
Translated it means they are using sophisticated methods and encrypted
information for communication.

@_date: 2016-03-31 23:19:08
@_author: Natanael 
@_subject: [Cryptography] On the 'regulation proof' aspect of Bitcoin 
Den 31 mars 2016 9:07 em skrev "Phillip Hallam-Baker" :
replication. You might
join the
Here's a few of the properties of collaboratively generated chained PoW:
* It is an algorithmic proof of spent scarce resources. Cryptographic proof
of work is the only such known method that exists that can with near total
certainty be used to confirm remotely with no knowledge of anything but the
PoW output itself that scarce resources were spent once,  and thus can't be
spent again (for free). Those transistor cycles and that energy (enthalpy?)
has been claimed and doesn't come back. This allows for verifiable digital
voting where cheating is impossible - given that you're interested in
knowing what side can generate the most PoW for their cause.
* Given an assumption of a collaborative majority of computational power
(here I'm not calling it "honest", for precision) we can with very high
certainty be sure that the public agreed ruleset will be followed, giving
us a public transparent computing system with an auditable result and
predictable behavior. In Bitcoin, a script controlled mechanism of
generating and assigning scarce tokens was implemented.
What made people care from the beginning was that this enables the creation
of verifiably scarce tokens, without gatekeepers.
Almost every suggested change to Bitcoin lands in a gatekeeper model
(Ripple like systems, like PHB's suggestion) or loses provability (PoS).
If Bitcoin only were banking over Git commits, nobody would care. If it
could be taken down permanently with no possible method of recovery in a 5
minute BGP hack with a few malicious nodes (like many PoS schemes), we
would never even have heard of it.
Models like Ripple means there's always token issuers that need to stay
alive, there's fungibility issues with multiple issuers of the same types
of token due to trust issues (unlike on stock exchanges where most futures
issuers hopefully have gone through auditing), reliable doublespending
protection disappears if the issuer's "native" network goes down (who else
will be the authority for who the new owner is in case of conflict?),
you'll have a much worse regulatory mess, etc...
Who gets to be a token issuer, for what, and who are allowed to connect to
what? Would unfiltered connections even be allowed across borders? Would
only large banks be allowed to process international transactions due to
All we would gain is some hacking protection, and lose a lot of future
design flexibility, and on top of that we'd create a network effect around
a protocol that would be nearly impossible to upgrade (worse than the
current case with Bitcoin, because you'd have *multiple countries'
governments* and their trade agreements involved in the network design!).
In other words, without PoW you can't reliably trade scarce tokens by your
own personal arbitary rules (pay-to-script-hash addresses) anymore.
Then there's just this public computing system where all you really know
about it in advance is what it DOES NOT allow.

@_date: 2016-05-13 20:45:10
@_author: Natanael 
@_subject: [Cryptography] 
Den 13 maj 2016 19:37 skrev "Christoph Gruber" :
collect some buzzwords for your next bullshit-bingo, here you will find
Posted on Reddit here, with plenty of comments;
(I'm one of the moderators of that sub)

@_date: 2016-05-23 12:02:20
@_author: Natanael 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
Den 23 maj 2016 7:25 fm skrev "David Johnston" :
security person, I hold the opinion that ignorance beats entropy.
system having entropy relative to the thing that is ignorant of the state
of the system.
Information IS surprise:
Meaning that ignorance actually is the source of entropy. We can't learn
anything new of we know the seed and position of a deterministic system.
Entropy (bits) is how much we can learn about a system from a given amount
of information about that system.
In cryptography we tend to settle with a small pool of secret entropy and
deriving computational entropy from it (i.e. an adversary with unbounded
computational power can break it and find the seed, but not a limited one).

@_date: 2016-05-24 21:18:37
@_author: Natanael 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
Den 24 maj 2016 7:15 em skrev "Jerry Leichter" :
are there to protect against external code that finds holes.  Early during
boot, *there's no external code running*.  We're before network
initialization, so there's nothing coming in from the network links.
Basically, if an attacker has managed to get code running at this point
during boot, you don't have much hope anyway.
get enough randomness to set up kernel ASLR and related mechanisms early in
boot, but how to I *put off* setting up kernel ASLR and related mechanisms
until I have a usable source of randomness?
Let the bootloader / UEFI do it?
If the question is how to load and run code complex enough to interact with
potentially arbitary hardware to fetch meaningful entropy, before the
kernel has had a chance to prepare anything, without making the boot
process more complex, it seems like that's the only clear answer.
The bootloader has to already know a thing or two anyway about the hardware
and get a few things up and running, and letting it pass on entropy to the
kernel as it loads it to memory keeps the overhead low.

@_date: 2016-11-15 10:13:15
@_author: Natanael 
@_subject: [Cryptography] On the deployment of client-side certs 
Den 15 nov 2016 09:17 skrev "Thierry Moreau" :
serious alternative to password-based authentication.
concept, but that's may not be applicable as a mass market solution since
the other aspects are unresolved.
carry it between devices (admittedly easier said than done), and
self-issues certificates at will.
has been trained (more or less implicitly) to relegate the PRIVATE key
protection issue as a minor system configuration management duty (to be
isolated from the user mental model). How many security experts ever tried
to explain (e.g. to a computer-literate user audience) the very
foundational principle of public key digital signatures?
I keep seeing hardware tokens being NOT mentioned.
Is it really that hard to convince people to carry a U2F / OpenPGP token
with USB/NFC/BLE capabilities in their keychain? It shouldn't be.

@_date: 2016-11-15 10:38:57
@_author: Natanael 
@_subject: [Cryptography] Need a better name 
Den 13 nov 2016 08:25 skrev "Phillip Hallam-Baker" :
static file.:
from the digest value itself and fingerprint is as good as anything for
public key signature in some way. In the Mesh, I use this to authenticate
profiles as follows:
checkpointing against a linked log but that is the basic concept.
The Mesh is essentially a per-user central database + a bunch of protocols
based on cryptography, right? There's an initial "entity declaration"
(database object?) containing sub-entries like basic profile data and the
public key, as well as Mesh specific policy rules.
Given the blockchain style checkpointing and one authoritive source of
events (each update must be signed by a trusted key), all entries are
protected from manipulation.
If it was Git we were talking about, we'd just be talking about file paths.
I don't know what terminology is used for transparency logs. In Factom
there's "proof-of-process" that can be applied to an individual document to
prove it isn't being forked when updated.
So, you want a name for the non-static entries belonging to some given
user's Mesh profile? And you want to produce some static identifier from
this...? (Your question isn't actually entirely clear)
I suggest something as simple as "Mesh entry" as the name, and "Mesh entry
ID" as the fingerprint equivalent, with the value derived from the initial
entry value + probably the hash of the initial "genesis block", or "Mesh
root", for that user (to guarantee it is unique).

@_date: 2016-11-15 15:07:32
@_author: Natanael 
@_subject: [Cryptography] securing the ballot scanners 
Den 12 nov 2016 23:06 skrev "John Denker" :
In the model of paper and pen, 3-ballot already fixes it. Omission of votes
is provable, and voting is easy, yet anonymity is preserved even if
somebody gets your voting receipt paper (it alone doesn't reveal what your
vote was).
You don't need to trust the counting machines, because the result is
If you're really paranoid, put votes on a conveyor belt under glass and let
multiple independent machines photograph them. Hash and timestamp the
images and checkpoint them in public (Bitcoin blockchain?).
Of course the votes would be sealed after filled in, then scrambled once
put in the box (lottery style), then only get unsealed and counted in
batches (resists timing attacks to some degree).
The electronic voting model is much harder, the one in which the voters has
personal cryptographic keypairs to sign the votes with.
You have to limit the power of coercion, so remote voting that can't be
overridden by an in-person vote in a voting booth is ruled out. As an
example, blockchain voting doesn't make much sense for anything where the
vote contents is sensitive.
You have to make it auditable, and allow people an ability to trust that
their votes was correctly processed. Yet it must remain anonymous - a vote
receipt must not reveal what your vote was, and still provide confidence to
the voter that everything was handled correctly.
If votes are submitted in plaintext, they must be submitted anonymously
with timing attack resistance. This is very difficult to guarantee.
If votes are encrypted, you need advanced crypto like homomorphic
encryption or secure multiparty computation to count the votes - this hurts
transparency in the process (from the perspective of the average voter),
although a combination with zero-knowledge proofs would make the guarantee
of correctness stronger - assuming you trust the algorithms!
You need to guarantee to the voter that their vote ciphertext corresponds
to their intended vote, and that it was counted correctly (signed and
timestamped?), and that votes can't be individually decrypted and traced to
With multi-part votes (to resist tracking), you need to confirm the parts
were all counted and yet not linkable to each other and/or linkable to you.
The input hardware must be secure and privacy preserving. You don't want a
machine that can spy on the voters. Multiple voting machines that people go
between only makes it worse (if entering the full vote in plaintext on
each) .
Potentially you can divide the electronic vote in 3-ballot style for using
multiple machines to enter the vote (except electronically), but that hurts
usability severely.
So ideally you want personal hardware to enter the votes, but how do you
get that hardware to voters? How do you make it cheap, yet secure? You can
piggyback on existing ID card issuance processes (which would suddenly
become a much higher value target), make the ID:s smartcards and then
provide hardware with some interface for voting - but what if security
holes are found in these devices?
And then you want to only use them inside the voting booth during the vote
(to resist coercion). You need strong Tempest resistance, but is that even
plausible (both button pressed and capacitive screens can be spied on, as
can reflections of the screen image)? And then you hand the vote to a
machine that records it (perhaps display an encrypted Qr code to it?).
My prior writings on the topic:

@_date: 2016-11-19 11:32:27
@_author: Natanael 
@_subject: [Cryptography] Crypto and rustling 
Den 19 nov 2016 04:28 skrev "Dave Horsfall" :
Why not just use symbols which have no shared sub-patterns? Like for
example just using A, B, C, H, J, K, M, N, R, S, T, W, X, Y, Z (depending
on font, the symbol set may differ).
Every symbol should be excessively clear and stylized such that it can't be
easily transformed.
An alternative is some kind of individual logos, complex patterns where
everybody local knows all logos in use. They should likewise be designed
such that no additions (except "blacking out"  the full pattern) would
cause it to become hard to distinguish.
Now that I have I think of it, Qr codes have a similar problem. It must
tolerate any given part up to a certain size of its surface to become
completely covered, and uses error correction codes to provide the
redundancy to handle it.
What's the simplest way to design visual error correction codes that can be
processed using pen and paper? (mechanical tools are allowed)

@_date: 2016-11-23 10:43:40
@_author: Natanael 
@_subject: [Cryptography] On the deployment of client-side certs 
Den 23 nov 2016 02:09 skrev "Phillip Hallam-Baker" :
important. I am aware of Secure NTP of course. But... Well for my purposes,
I really want multiple statements about time:
'this time has passed' which can be 24, 48 hours in the past.
trusted party asserts that the current time is within 10 seconds of a
stated value.
100ms or better
do X provided you can complete before the time you think is Y' allow me to
address most of my time based security concerns.
Uses multiple servers and nonces + hash chaining to establish a verifiable
direction of time / order of events, with high accuracy in timing.

@_date: 2016-11-28 14:55:09
@_author: Natanael 
@_subject: [Cryptography] OpenSSL and random 
Den 28 nov 2016 14:34 skrev "Salz, Rich" :
at least N+1 opinions.
Until then, OpenSSL will proceed as best as it can and get slammed for it
at some point.
Me: use both :)
This was actually mentioned previously in another thread by somebody else,
and I like the idea;
At boot, you create /dev/random normally and map /dev/urandom to it such
that both work the same - until enough entropy is estimated to have been
gathered, they both block and you can't get random numbers.
Once the entropy pool is assumed to be filled, you create/restore the
standard /dev/urandom and now remap /dev/random to point to the (regular)
After all you just want to block until you have enough entropy, and then
never block again (for as long as the pool remains uncompromised), given
that we already trust all these symmetric ciphers with the random numbers
coming out of the CSPRNGs. Because why trust them with terabytes of data,
but yet not trust that the CSPRNG works using the same class of symmetric
algorithms based on similar sized entropy pools?
The real difficulty on the software side is entropy estimation. Everything
else is mostly normal engineering. Once you know you have enough secret
entropy in your pool, deriving unpredictable outputs from it is a solved

@_date: 2016-11-28 23:51:18
@_author: Natanael 
@_subject: [Cryptography] What's the point of low-latency cryptography? 
Den 28 nov 2016 23:05 skrev "Ryan Carboni" :
think that most computations on a computer require at least a hundred
cycles before the result is presented to the user. RAM to begin with
requires ten nanoseconds to retrieve data. Nearly all single-cycle
implementations are probably smaller than the gate area required for a DDR4
Actual uses;
* Multi-player gaming - sub-millisecond response time desired in
competition settings
* High-frequency trading - these companies are literally paying extra to
have their hardware one room closer to the stock broker's server to reduce
* Autonomous vehicle coordination protocols - you can imagine why...
There's probably much, much more.
generate independent subkeys for a block cipher the size of each
addressable word (which appears to be the byte). This would be vulnerable
to codebook attacks if one can somehow read the encrypted output and submit
256 plaintexts if one knows the address of the data one wishes to
Given a proper implementation with software and hardware protections, you
can't get race-conditions or similar conflicts in assignment of key
material. Thus no such attacks.
I and a few others have already considered the use of stream cipher keyed
block ciphers.
Given the result on XEX from the paper "Revisiting minimalism in
cryptography", you just need one XOR key of N bits per block of N bits,
both before and after the permutation (keeping latency low). And the
permutation in XEX can even be unkeyed and public (essentially static), and
thus much faster and smaller in hardware than most classical block ciphers
(no key schedule!). And if the permutation is customizable / tweakable, you
could even choose your own block size (excellent for FDE, since you can
match arbitrary disk block sizes).
Combined with a fast seekable stream cipher (which really only need to
achieve unpredictability, not much else when combined with XEX) like you
said, you can massively parallelize the encryption and decryption, perhaps
even saturating the bandwidth of high-end supercomputer GPUs (and ASIC
implementations will essentially always be as fast as your I/O).
won't protect against all attacks.
grsec, and the architecture would aid in securing it.
How would a capabilities based OS taking full advantage of TPM / TrustZone
functionality look like, if it was designed in the open and made FOR  the

@_date: 2016-11-29 21:27:02
@_author: Natanael 
@_subject: [Cryptography] Gaslighting ~= power droop == side channel attack 
Den 29 nov 2016 19:45 skrev "Chris Tonkinson" :
Try asking Tesla.
from the utility grid.
14 kWh a piece (5kW max continuous load). Given that they support solar
panel connections, they must be able to rapidly switch between charging and
discharging. You can run on solar power until the cloud comes out, switch
to battery, run on solar again, then on battery when the night comes, then
on grid power when you've got neither solar or battery power. Also works as
battery backup when the grid goes offline.
It shouldn't require much reprogramming to make it smooth out your power
draw from the grid to nearly constant (they almost definitely already are
capable of selective charging and discharging different groups of cells
simultaneously). But perhaps they don't have the capacitors required to not
wear out the batteries too fast, if a full household is supposed to use it
for years.
Most of the cost goes into to the batteries themselves. If you think
they're expensive now, then the cost for the same kWh would have given you
a heart attack 10 years ago.
This however doesn't solve your issues with RF leakage (Faraday's cage
wallpapers is a thing) or other side channels like noise (much harder to
solve) and heat.

@_date: 2016-10-13 16:14:02
@_author: Natanael 
@_subject: [Cryptography] Blockchain to Secure Nuclear Weapons? 
Den 13 okt 2016 11:50 skrev "Ben Laurie" :
The gap is the ability vs lack of ability to say that there's no data being
hidden from you, the ability to know when you have access to one entry that
there is no entries that conflict with yours.
A pure timestamping service has no concept of conflicts or versioning.
There's no concept of exclusivity or "rivalry". But in Bitcoin's blockchain
you can't spend an UTXO twice, in Certificate Transparency you can't issue
two separate certificates for one domain undetected (revocations are also
Factom (works on top of Bitcoin) goes even further, when you look up
entries you should be able to confirm that what you hold is the latest
version, or to otherwise find the one correct latest version. It ensures
there's only one active version at a time, no forking. Each new one
replaces the previous one. They call it proof of process.

@_date: 2016-10-27 14:38:26
@_author: Natanael 
@_subject: [Cryptography] A PKI without CRLs or OCSP 
Den 27 okt. 2016 07:35 skrev "Francisco Corella" :
This seems relevant:  -
Improving Authenticated Dynamic Dictionaries, with Applications to
Self-authenticating data structures are incredibly useful, and this one
appears to be quite relevant here. This is essentially like an upgraded SPV
mode - the mode with Bitcoin's headers-only sync where clients only verify
that transactions exists in a blockchain with X amounts of accumulated
proof-of-work (ever growing). No actual verification of transactions is
done besides that.
The difference here is that with the linked scheme, you can produce compact
proofs of correct results. Instead of relying on inclusion proofs +
economical game theory as with Bitcoin SPV, you rely on inclusion +
exclusion proofs and the fact that there's a single authorative append-only
chain where everybody agrees on the history of the header hashes, given
that the "miner" must publish all headers which everybody in the network
can cache.
The relevant paragraph:
authenticated dictionary. In such a data structure, provers (who are, in
our case, miners) hold the entire data structure and modify it as
transactions are processed, publishing proofs that each transaction
resulted in the correct modification of the data structure (these proofs
will be included with the block that records the transaction). In contrast,
verifiers, who hold only a short digest of the data structure, verify a
proof and compute the new digest that corresponds to the new state of the
data structure, without ever having to store the structure itself. We
emphasize that with authenticated data structures, the verifier can perform
these checks and updates without trusting the prover: the verification
algorithm will reject any attempt by a malicious prover or
man-in-the-middle who tries to fool the verifier into accepting incorrect
results or making incorrect modifications. In contrast to the
unauthenticated case discussed above, where the verifier must store the
entire data structure, here verifier storage is minimal: 32 bytes suffice
for a digest (at 128-bit security level), while each proof is only a few
hundred bytes long and can be discarded immediately upon verification.
Also relevant, making it useful for a certificate log:
known as a map): it allows insertion of (key, value) pairs (for a new key),
lookup of a value by key, modification of a value for a given key, and
deletion by key.
The differences here is that the CA would be the only "miner", there's no
need for proof of work (just signatures of the headers) and instead of
transactions there's certificate issuance and revocations being stored. As
noted above the blockchain headers would still have to be published,
obviously (otherwise the chain can be secretly forked, "split brain"
problem if the users don't communicate).
Since proofs are small and easy to verify, they can be transmitted
trivially on request in place of a CRL lookup. And whenever you've
performed a lookup and received a proof, you must need to ask a small group
of trusted peers of they agree on what the blockchain header hashes are -
any conflicting signed header hashes are undeniable proof of misuse (and
likely proof of malice).
The blockchain could be mirrored by whoever is willing to dedicate the
necessary storage. Anybody storing the blockchain can generate proofs. And
AFAICT proofs are at least valid and reusable for as long as the header it
refers to remain the latest valid published header (so the TTL of proofs
should match time remaining until next header is published).
I'm not going to argue which is best or worst of this or certificate
transactions or any of the other public log schemes under development. I
just mentioned this because it seems relevant.

@_date: 2016-09-03 11:05:58
@_author: Natanael 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
Den 3 sep. 2016 01:01 skrev "Bart Preneel" :
is more attractive for an attacker:
specific memory location. Flipping any bit of a public key will lead
are useful to flip in executable code typically requires manual work
through reverse engineering (and this work has to be repeated for every
hand, protecting the complete code brings a large overhead.
I saw this linked on Hacker News yesterday;
Flip ANY bit, the code still behaves exact the same.

@_date: 2016-09-07 14:58:05
@_author: Natanael 
@_subject: [Cryptography] Strong DNS Names 
Den 7 sep 2016 11:36 skrev "Phill" :
combined a PGP fingerprint like identifier with an email address:
fingerprint of a key under which a policy for sending mail to
alice at example.com is signed. That might have statements like emails must
be signed, use PGP, etc. [...]
I'd like to make minor modifications.
First, many systems that would be incompatible uses simple regexp of
similar in the string. That would likely (favorably) cut out the end of an
appendix using an invalid first character in the domain, such as using ;
(semicolon) as the separator, so that the system still can communicate with
Although I'm not sure if too many systems would outright not allow you to
enter it (I've seen plenty examples of that, still forcing the user to edit
it out manually. But are these systems significantly more common than
systems that simply have length limits that would *also* deny it? (I'll
have to admit that this first point is more bikeshedding than anything
else, unless somebody can bring data...)
Second, shouldn't we allow for changing ciphers? As an example, Bitcoin
already uses the prefix 1 to signify an address using ECDSA secp256k1
encoded in a particular way with double hashing. The prefix 3 signifies a
hashed script which itself can invoke secp256k1, SHA256 or any potential
future algorithm additions. More prefixes can be added if necessary. All we
need to do here is to define prefixes that identify ciphersuites.
We don't need to define more than one or require the support for more than
one default choice (we don't need a new TLS mess), the point is simply to
enable a transition without breaking compatibility if the previous choice
of cipher becomes insecure. Perhaps to be able to switch to a post quantum
cipher suite in a near future. A system seeing an address with unsupported
ciphers could suggest that the user checks for any available software
updates to potentially add support for it.
alice at eop.gov or alice at microsoft.com or the like, we want to make sure that
we are talking to the implied domain if it matters. Which is something that
enterprises can do with an appropriate CAA record requiring messages to
that domain be supported by a suitably validated cert.
We can tie into DNSSEC+DANE, perhaps make it a requirement for clients
using this scheme. Not that it matters much if the message is encrypted.

@_date: 2016-09-16 01:50:22
@_author: Natanael 
@_subject: [Cryptography] Physical security risks of onetime pads just 
Den 16 sep. 2016 00:31 skrev "Tom Mitchell" :
While I do get the humoristic tone of your message, I have a somewhat
serious reply.
The problem have already been encountered by lottery ticket markers, and
more recently also Bitcoin paper wallet designers. In the latter case, you
really don't want to hand over a private key with money and realize it got
stolen without the paper wallet ever even being opened. There's multi-fold
paper wallet designs with patterns meant to obscure the Qr codes which is
holding the raw secret keys (no guarantees on the effectiveness!);
The attack method is called "candling", and to find relevant info you'll
want to search for "candling lottery tickets" (that's the only reliable set
of search terms to get relevant info which I could find).
One idea of mine is to use visual cryptography to stack multiple layers of
sheets with seemingly random patterns, and intentionally misalign and
rotate them, and to place fully random decoy sheets in the mix.
The recipient then breaks the (tamper-evident) seals, discards the decoys
and then aligns the sheets according to instructions. The message will then
become readable.
This still breaks if the adversary can still manage to get a high enough
resolution scan of every sheet, but with high resolution visual
cryptography patterns this should be very hard for the foreseeable future.
Though the adversary has the advantage of being able to bend the stack of
sheets to cause controlled realignment, and that visual cryptography is
entirely "linear" (you always just need higher resolution images of the
blob of printed pixels to be able to figure it out).
Perhaps another defense could be to use scrambling sheets in the stack with
a variety of optical properties to inject noise and diffuse any images.
Small prisms, lenses and pigments with holographic properties and other
effects. Beware of unintentionally helping the adversary, though! You might
just create something resembling a Fresnel lens and make it easier to
Of course the security model of this method is very limited today - it is
your best option if and only if...
* You only have one single reliable communication channel, in the form of
physical delivery of messages (courier, postal service or similar)
* You have no option to use proper cryptography (no shared key, don't know
each other's public keys, no trusted hardware, etc)
* You trust the couriers and other middlemen to not actually *open* the
envelopes, or even substitute them
In real world usage I'd only use it for one-off messages to deliver
encryption keys (with mandatory confirmation of receipt before use), in
mostly-trusted environments with completely trusted couriers, where you're
defending against opportunistic spies who can do anything but to open the
envelope (no unmonitored access, or too little time to tamper with the
The kind of stuff where you have two armed men carrying a briefcase cuffed
to the hand, and you're defending against X-ray vans;

@_date: 2016-09-23 02:57:13
@_author: Natanael 
@_subject: [Cryptography] Spooky quantum radar at a distance 
Den 22 sep. 2016 23:07 skrev "Henry Baker" :
targets 100km away
splitting the original photon with a crystal, a change to one entangled
photon will immediately affect its twin, regardless of the distance between
shooting one twin into the air, would be capable of receiving critical
information about a target, including its shape, location, speed,
temperature and even the chemical composition of its paint, from returning
quantum radar would be much better at detecting stealth planes, which use
special coating materials and body designs to reduce the radio waves they
deflect, making them indistinguishable from the background environment.
and speed even if managed to retrieve just one returning photon.  It would
be able to fish out the returning photon from the background noise because
the link the photon shared with its twin would facilitate identification.
If you think that's mind-bending, you should take a look at another one of
the even crazier examples of quantum physics in action - you can combine
the two principles of quantum counterfactual definiteness and the quantum
zeno effect to do the following;
Detecting things with *one* photon isn't quite the same as detecting them
with *none*, interaction-free detection!
Although the big practical difference here is that this Chinese tech works
in free air since it relies on a normal 180 reflection off an object back
to your sensors (it behaves like you'd expect a normal radar to behave),
while this other quantum system relies on a fully controlled path (with
mirrors and optics all the way) where you just can detect if your measured
path is being obscured or not. The TL;DR: It relies on photon
self-interference being broken in a detectable manner. To paraphrase the
thread title, "spookier quantum detector in proximity".
The latter detector system system could be useful in many controlled
environments where you're able to provide a controlled path for it to
detect interruptions in. It would work anywhere you would normally use
something like laser detection but want to get rid of the light (and can
assure a single photon wouldn't be lost!).
Alarm systems and tamper detection are two obvious examples, and in the
latter case you could for example use a fiber optic cable to provide the
"dark" measured path which can't be broken undetected. Counting light
sensitive objects is another possibility, such as in a manufacturing line
in a dark room (items being counted as they pass by the pair of mirrors).
More:

@_date: 2016-09-23 20:33:06
@_author: Natanael 
@_subject: [Cryptography] Spooky quantum radar at a distance 
Den 23 sep. 2016 8:12 em skrev "Jerry Leichter" :
measurements"   The original
presentation of these was the following problem:  Inside a box there may -
or may not - be a nuclear bomb.  The bomb is attached to a trigger that
will go off if the bomb absorbs even a single photon.  You want to
determine if there's a bomb in the box, but obviously you would rather not
set it off if it's there.  Can you do so?
chance of accidentally triggering the bomb against the chance that the
answer you get to your question is wrong (beyond the classical extremes
of:  Don't test, never cause an explosion, give an arbitrary answer and be
right half the time; and always test, give an always-correct answer the
half of the times the bomb isn't there, blow up the other half.)
As you see in my other reply, I've mentioned this possibility. Also, this
can be improved upon to become effectively interactionless on average,
using the zeno effect as I also mentioned.
you aim a radar at them, almost none of the energy gets reflect back to the
sender, so it can't "see" the plane.  There's some odd language in there
that kind of hints that you create pairs of correlated particles, send one
toward the plane, and then learn about what happens to it based on what
happens to its stay-at-home pair.
I read it differently. Instead they use the anti-correlation between the
photons in the pair to be able to identify the returning reflected photon
out of all the noise (since the rest of the detected photons will be
effectively randomized). At least that seems to be what they're implying.
You can read the local photon from the pair to see what its properties are,
and then configure your sensor to only look for photons with the exact
opposite properties. Then you look at the timing of the detection.
I'm not entirely sure though how the reflection against the target will
affect the properties of the photon, or how it affects the entanglement. An
actual physicist has to answer that.

@_date: 2016-09-24 23:52:09
@_author: Natanael 
@_subject: [Cryptography] Threat Model: Bluetooth tracking beacons 
Den 24 sep. 2016 23:14 skrev "Henry Baker" :
protocols to render these "beacons" (actually trackers) useless?
Bluetooth 4.2 LE Privacy 1.2
ability to  track a LE device over a period of time by changing the
Bluetooth device address on a frequent basis. The privacy feature is not
used in the GAP discovery mode and procedures but it is used, when
supported, during connection mode and connection procedures.
devices, the device address, referred to as the private address, must be
resolvable by the other device. The private address is generated using the
devices  resolving identity key (IRK)  exchanged during the  bonding
in any packet type used on the advertising channels.
which  makes it more difficult for an attacker to track a device over a
period of time. The  requirements for a device to support the privacy
feature are defined in  Table 10.3.
And so on...

@_date: 2016-09-25 15:18:35
@_author: Natanael 
@_subject: [Cryptography] Threat Model: Bluetooth tracking beacons 
Den 25 sep. 2016 14:43 skrev "Tom Mitchell" :
  Cell phone, WiFi, Bluetooth, near field
at the entrance and exit there
pets as you walk the dog
and recorded.
inserted in jackets,
to passive
knowledge connections without
than 15 min....  oh wait I just started
I feel like the real challenge is to hide in the noise.
The crypto is easy by comparison. You don't want even a powerful antenna
array to track your emissions while your devices are active.
You wouldn't want to overpower the noise with your transmissions, but to
try to hide in it with your many physically distributed very weak
transmitters (finally a use for smart clothing!). You would want to be a
small blip on the radar (radio), below the noise floor.
You would be using shared keys and antenna configuration profiles and
advanced MIMO to identify your friends out of the noise, a bit like already
done with GPS. You would be communicating using many weak signals that each
are indistinguishable from random to anybody that isn't the intended
You would have to coordinate your many EM signals to only interfere just
right to rise above the noise to form a single meaningful signal just
around the receiving antenna, which would still require that the receiving
antennas also are aware of the direction the signals are coming from, and
even then they'll just see a (pseudo)random signal above the noise floor
that they still need to decrypt.
The hardest part of all is to put together so many receiving antennas,
correlating their signals to extract such a weak signal and also get decent
bandwidth, and still not draw so much energy that you have to carry a
gasoline driven generator....
The closest hardware today would be FPGA driven SDR MIMO implementations,
and that's not exactly running at milliwatts.
I feel like the only short term practical solution is powerful access
points and weak clients which are capable of hiding their location to
everything except to the access points. Of course that just shifts the
problem... Real P2P anonymized communication would require drastic advances
in radio efficiency and signal processing algorithms.
Although, you know, we could use those entanglement powered single-photon
detectors and carry sets of entangled particles instead :)

@_date: 2017-04-06 20:48:02
@_author: Natanael 
@_subject: [Cryptography] Should the IV of an encryption operation be 
Den 6 apr. 2017 20:15 skrev "Phillip Hallam-Baker" :
Further to my explorations of HKDF (RFC 3394) I am thinking as follows:
Most cryptographic modes use the same key to encrypt a message. CBC, GCM ,
etc all perform operations on the input data, the key is constant.
When using a public key for exchange, we choose a session key which is
random every time. This each message is guaranteed to be encrypted under a
different key.
If I use KDF with a fixed salt (the norm for most protocols) I get
inter-protocol separation but not separation per message. The IV provides
some protection but I am still handing the attacker a possible advantage.
So what if I was to use the IV of the encrypted data as a part of the salt
in the Key derivation function? Is this a good idea or a bad one?
By comparison, deterministic ECC signatures hashes the message + private
key for the secret nonce. This takes away the need for having a strong RNG
available after key generation.
I see no reason for why reusing the IV with a KDF would break anything if
both the KDF and encryption are using distinct symmetric algorithms (no
cross-protocol exploits), which both allow using a public IV value without
breaking security (that would be most of them).
IV reuse is usually just problematic in the context of the same algorithm +
key (some exceptions may apply, like when when even a public IV absolutely
must be unpredictable ahead of time, in contrast to algorithms that only
requires that it doesn't repeat).
Also, I'm assuming you mean something like encryption key = KDF(root key,
IV), ciphertext = encryption(encryption key, IV, message)? I do think I'd
do it slightly differently by having a derived IV too for the encryption
algorithm, simply by making the KDF output larger and splitting it in two.
I don't think it really changes the security considerations, but it makes
for cleaner separation.

@_date: 2017-04-27 08:07:31
@_author: Natanael 
@_subject: [Cryptography] 
Den 27 apr. 2017 07:20 skrev "John Denker via cryptography" <
cryptography at metzdowd.com>:
I agree with that 100%.
A cipher "mode" is a kludge that kinda maybe sorta allows people
to survive in situations where they can't (or won't) re-key ...
but you should always ask yourself, why not just re-key?  If
your cipher cannot be efficiently or securely re-keyed, maybe
you need a better cipher.
Constructive suggestion:  Use something like ChaCha20, which
is designed to do a good job with file encryption (and a lot
of other things).  It has an enormous keyspace, and can be
re-keyed efficiently.  Construct the key from at least:
  -- The master key.
  -- The block number.
This part;
  -- A sequence number, depending on how many times the
   block has been rewritten.  (This allows random access,
   as well as rewriting the whole file from the beginning.)
This is assuming the file won't be duplicated / shared / restored from
backup. It is not sufficient alone. It also means any duplicated VM fails
spectacularly in keeping the plaintexts secret.
You need a guarantee that every single write will use a unique IV. That
means you need OS level support for your encryption routine on every device
touching the file.
Deriving the IV from the message helps, but for small IV:s it isn't a
guarantee for security (in particular when you write often). It is also
impractical for large messages.
Good analogy.

@_date: 2017-12-21 20:49:08
@_author: Natanael 
@_subject: [Cryptography] Bitcoin, fork you very much 
Den 21 dec. 2017 20:33 skrev "John Levine" :
Let's say I'm with the Chinese government and decide that I am tired
of people evading currency controls and money laundering with Bitcoin.
So we adjust the Great Firewall of China to block port 8333.  We also
add some MITM proxies that take newly mined blocks from the Chinese
side, rewrite them to put the newly mined btc into government-approved
wallets, fill up the blocks with transactions from outside China, and
send them along.
Blocks are considered valid if;
* The syntax is correct for the header and for all transactions. The header
includes a Merkle tree hash of the transactions.
* If all values are within the correct limits, such as that the time of one
block may not be too far back or into the future compared to the prior one.
* If the miner's "coinbase transaction" is valid, that first transaction in
the block in which the miner claims the reward and fees to his own address.
He can't claim a larger payout than the available reward + fees in the
transactions bundled in the block.
* If all transactions only try to claim valid unspent transaction outputs
from previous transactions, and if they follow all the scripting rules
correctly (different address formats enforce different scripts), and if
outputs are not larger than the inputs.
And critically, where your idea fails:
* If the proof of work is valid, meaning that the integer representation of
SHA256(SHA256(block header)) is less than the current mining difficulty
target value.
Changing the coinbase transaction to steal the coins changes the Merkle
tree hash in the header and thus invalidates the proof of work, because the
header hash changes too - randomly. With a very very tiny probability to be
valid PoW.
(Note that the above criteria are not complete, there are more factors
involved. But they're sufficient to describe the concept.)
It's relatively much easier to just attempt to isolate your people from any
cryptocurrency nodes.

@_date: 2017-12-28 19:30:24
@_author: Natanael 
@_subject: [Cryptography] Open source encrypted file system for cheap IoT 
Den 28 dec. 2017 18:58 skrev "Henry Baker" :
Why is this so hard?
IoT = very cheap, very low power, very small memory.
As I said, I want to access files on a uSD card or a USB stick,
so the encrypted file system is on a *passive* device.
I want to be able to do this from either the $10 device (not as
powerful as a Raspberry Pi, because Pi's take too much power),
or a larger machine -- e.g., Linux/MacOS/Windows.
FYI, to access it both from PC and the IoT device, there's three main
1: Make the device the interface to the storage, which requires emulation
of a FAT32 filesystem or similar protocol that all three major operating
systems supports. This may be too much for a tiny microcontroller, or just
too slow. You'll need a somewhat powerful CPU in your device to handle this
well if you have megabytes of data, close to an RPi.
2: Use a memory card with a FAT32 filesystem, put an encrypted container on
it, which can be read from a PC with usermode software. Adds some overhead
during normal use.
3: Use an encrypted filesystem on a memory card, cry salty tears over the
file system drivers you'll need to install on Windows / Linux / Mac to make
it work.
Only  is practical if you want it really cheap, but I would prefer

@_date: 2017-02-08 02:23:00
@_author: Natanael 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
Den 7 feb 2017 20:18 skrev "Joseph Kilcullen" :
This is a link to my original post back in June 2016. You people never told
me why my solution is wrong.
So please tell me. Why is my solution wrong?
Here's a link to the latest version of the paper:
(edited to not top post this time)
What's with the attitude?
Trusted interfaces is an old idea. You have at least that idea right.
Qubes OS is a recent example of using trusted interfaces;
Browser based controls have been proposed previously. Problem is that that
isn't enough by itself if such interfaces aren't used everywhere and if the
the users are careless. You need to users to be educated on how it works,
and proactive.
Also, it would work better if it used trusted inputs mixed with phishing
proof authentication protocols like FIDO's U2F / UAF that binds the
authentication response to the TLS session, blocking replay attacks and
MITM. This way the user secret isn't useful anywhere outside his own
Consider a process such as having the browser always tell you "on this
site, press the interception safe keyboard button X to open the login
prompt", and using something like a YubiKey that require you to press its
button to perform the challenge-response protocol, and then to ask the user
to enter his PIN / password to prove it is him at the computer.
This may be combined with a unique and secret color scheme per user to
further reduce the risk of forgery in the case of targeted attacks.
The user would be conditioned that the website can't open a safe login
prompt by itself and that only the browser provides the prompt, and that he
shouldn't share his secrets (password, PIN) outside it. It is also simpler.
The more similar the correct process is every time, the more likely it is
that the flaws of an attempt at phishing will alert the user that something
is wrong.
Meanwhile a fake website can not in any anyway use your credentials in any
manner without stealing them directly, which would require hacking your
browser to directly forge the 2FA request. Otherwise, compromise requires
stealing your hardware token and also figuring out your password or PIN.
Just getting your memorized credentials (PIN) alone gets them nowhere if
they can't also get the accompanied 2FA secrets / hardware.
In your scheme (if I read it right), a user just have to be forgetful once
and it fails. Sending plain passwords is a dated solution that should be

@_date: 2017-02-08 02:41:21
@_author: Natanael 
@_subject: [Cryptography] What is total world transaction volume? 
Den 8 feb 2017 02:16 skrev "James A. Donald" :
What I have been thinking on for many years is that you do not really need
every full node to process the entirety of the network's transaction
history, or indeed any nodes processing the entirety of the network's
transaction history.  Rather, your node needs to be able to prove that
every node connected to your node by a rather short chain of transactions
is in agreement about all transactions directly or indirectly affecting you.
... and you just reinvented Lightning Network. (It also sounds a bit like
Alice and Bob creates a 2-of-2 multisignature transaction that they publish
to to the blockchain. They assign X coins each, both paying themselves.
Then they create a follow-up transaction which they don't publish, this
works as a draft transaction, a digital tab. One that is signed by both
It uses the Bitcoin locktime functionality to prevent doublespends before
the locktime expiry (before expiry both parties needs to agree to spend
coins to the blockchain, after expiry you only need one - this prevents
permanently freezing funds).
Alice tells Bob "you owe me $10 for that pizza", the two then modify the
draft transaction to assign the payment output values accordingly. This is
the basics of payment channels in Bitcoin.
Lightning network is the networked version of that. People have payment
channels to payment processors. Processors have payment channels between
each other.
Payments are routed across the nodes, and the sender verifies that it
reaches the receiver.
Unfortunately I know very little about the server side pieces of Lightning
Network nodes. Somebody else can probably fill in on that.
Performance for the same-server usecase is at least essentially unlimited
since there's very little overhead necessary. Can't say anything about what
happens in large loosely connected networks.

@_date: 2017-02-09 21:32:29
@_author: Natanael 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
1a) Many businesses have no canonical / one single true business name.
They just have brands, or sometimes only individual products. They could be
owned by a single random person somewhere, be owned by a holding company or
be managed by some intermediate that the business owner has outsourced most
of the operations to.
1b) Relevant to the one above, many businesses have old legal names from
prior to rebranding, or there may even be multiple business *in the same
country* with identical names, located in different counties / states /
local jurisdictions. Clusters of businesses that collaborate can also have
confusing legal names. Mergers and splits just make things worse.
1c) Phonetically similar names, usage of visually similar characters, etc,
can break it too. You can never stop phishing only by relying on names. And
it is just too easy to get a CA to sign off on a name that can fool users.
1d) Even if it was practical, it means no businesses can share a domain
name (and thus share certificates). Even if they use different subdomains.
So users need to know who their host is. But hosts can change too!
2) You need one image per site. It just doesn't scale. The average person
have ~50 logins, IIRC. You also need to sync them between devices, which is
just more attack surface.
3) In case of failure, the phisher still gets the password.
If you're going to modify how TLS is used, just to with the U2F style
phishing resistant solutions where the password never is sent to the
server. Or SRP protocols. Because this solution doesn't require CA:s to do
even more than before, is more robust, and is actually useful.
And instead of images, just use a color scheme the phisher can't guess. You
only need one per user for a browser. Can be shared across all devices of
that user.
If you don't need to tell sites apart because your authentication proof
isn't reusable, then you only need one (1) secret shared with your browser
(assuming an attacker that only wants your credentials - impersonation
leading to the user divulging secrets remains a risk).
And as I also said before, a trusted keyboard shortcut to open the password
prompt is probably also one of the most effective protections possible
(relying on muscle memory as a phishing defense!).
Also note that technically EV certificates does what you ask for. Kind of.
Because of reasons 1a-d above, it still isn't what you want.

@_date: 2017-02-15 00:54:25
@_author: Natanael 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Den 14 feb. 2017 21:08 skrev "Joseph Kilcullen" :
True but I figured people who understand cryptography should 'get it' that
it's just a shared secret.
We get it. But many of us knows it isn't enough, which is why we skipped
right to trying to explain why it isn't enough.
HCISec is the field of study. Human computer interaction studies, focused
on security.
Since the image is shared between 'your web browser' and 'you' a MITM
attack would involve the criminal standing between you and your computer
monitor!! I'm sure this happens but its not called a phishing attack.
Shoulder surfing. Or perhaps just abusing the graphics card to steal the
image buffer?
With my solution a MITM attack must remove TLS entirely or substitute a new
TLS certificate. Either way the user, or your browser, will see something
is happening. The login window won't appear if TLS is missing. If a new
certificate is used then who's identity or CA will be used in it? The
computer user will see the fake identity named on the login window.
But they won't. They typically don't notice and and don't know what to look
And phishing works because people slip up. People forget. People become
Oh, and did anybody mention dyslexics yet?
Right now users look for a picture of a padlock! If pictures of padlocks
are proper cryptography authentication mechanisms then find me a book, or
paper, which documents this cryptography authentication mechanism. Or an
army which uses this tea leaf reading level cryptography solution!
It has the same practical effect if it has a protected UI surface to render
in. Such as if the URL bar can't be hidden.
The custom image just doesn't beat a custom color theme. In particular
since it would be shared for everything you do.
As previously explained, the names will never be more meaningful than the
domain name itself. There's no practical chance you'll recognize the
organization name as fake if you didn't react on the domain.
The image has no effect on security. There's nothing that it does that you
can't achieve with better URL domain highlighting (perhaps again with a
unique color scheme, to prevent forgery?). The image just says "read this
name and see if it is right", which we already know don't work.
It also literally does nothing meaningful to protect you if you share the
raw password with the site anyway - the presence of a unique personal image
does nothing if you already failed to identify the domain as fake. The site
will be getting your password anyway.
Muscle memory is infinitely more reliable than depending on awareness and
recognition. Make it mentally expensive to do the wrong thing and cheap to
do the right thing!
U2F / UAF which binds to both the domain name / certificate and TLS session
is infinitely more effective. You just can't get it wrong!
You may call this complexity, but it is the right kind of complexity that
helps us. It is already implemented and works. The design is fairly simple
too! It doesn't change anything outside requiring a small addition to the
browser and server. It doesn't modify the TLS protocol or CA behavior or
certificates, it uses them as they already are.
Using it means that *you authenticate to the browser* and not to the server
(there's your shared secret!), while the browser securely authenticates to
the server on your behalf (based on which server it is talking to).
Forcing users to choose and remember images and always pay attention is a
UX complexity that will fail.
And forcing a change in CA policies will fail - there's over 600
intermediate CA:s that needs to all cooperate, as well as all browsers!
Trusted inputs simply beats automatic prompts and forced awareness. Logins
are just like security warnings to people, something to quickly get around
- so lets not give the control over them to untrusted parties.
As I said before, requiring that the user uses a secure input to initiate
the interaction / login process would be infinitely more secure.
By for example using a key combo (or a button on a security hardware token)
that can't be intercepted or faked in combination with an authentication
protocol that can't be phished (like U2F / UAF), you train the user to only
use prompts he intentionally triggered and simultaneously help prevent
unwanted authentication / authorization of anything he didn't intend (like
not accidentally approving a bank transfer).
It not only blocks phishing and preserves the password protection against
nosy colleagues, but it also provides a defense against for example XSS
attacks (as does other 2FA) and exploits against password autofill;

@_date: 2017-02-26 22:32:40
@_author: Natanael 
@_subject: [Cryptography] More efficient block-chain ledger: Micali's 
Den 26 feb. 2017 20:39 skrev "Henry Baker" :
This approach cryptographically selects ---in a way that is provably immune
from manipulations, unpredictable until the last minute, but ultimately
universally clear--- a set of verifiers in charge of constructing a block
of valid transactions.  This approach applies to any way of implementing a
shared ledger via a tamper-proof sequence of blocks, including traditional
This isn't the first attempt to do this. The arguments against it are old.
Here's a good article on why everything that isn't centralized degrades to
proof of work when there's an incentive to influence the globally shared
Some examples of why this is flawed;
1: Sybil attacks. We could just stop here, but...
2: NSA (or any other entity with similar technical capabilities) can just
hack the current leader nodes. They're just a bunch of random nodes in the
first place, so they're unlikely to be highly secure.
Then they can try a million inputs until all further blocks appoints
themselves only as future leaders. This is a huge weakness with using
elected central master nodes in networks without any central gatekeeper.
This system is just a distributed gatekeeper that you can hijack. A full
takeover is unrecoverable with technological measures and needs a social
effort to revert.
If there actually existed any automated method capable of restoring normal
trusted behavior after a takeover, then this method would by itself be able
to replace the standard concensus system.
3: There's no way their proof will be meaningful in any way, because it
implicitly assumes that all nodes are secure and it assumes a perfect
network. "Unpredictable until the last minute" means nothing even if
they're doing their randomized node selection with hash commitments.
This is both because of the above mentioned Sybil attack, and because if
you've got a large number of nodes participating then you can always delay
publishing commitments until everybody else has published theirs, and opt
to publish OR not publish committed values as desired to influence
selection (giving you as many "bits worth" of effective bruteforce as
you've got participating nodes) - and of course also because of the
following problem;
4: Netsplits. Or worse, induced netsplits and direct network level
censorship by somebody capable of spying directly on the internet
connections of the individual nodes.
Since a protocol like this must be tolerant to node failures, NSA can just
delay the responses of everybody and chose the set to allow through that
gives their own nodes more control (amplifying the effect of the selective
publishing attack in 5: The classical nothing-at-stake problem. There was a valid block A at
time B (in the past) from set of leaders C.
Then network troubles happened and most nodes fell away, and a new leaders
were appointed from the candidates.
Except not, the "real" chain is a different one, but your fresh node
doesn't know that. And it has no way to know.
Nothing enforces that this blockchain truly is append-only for anybody not
well connected. What's worse is that if your node later finds the real
chain, your node has no object way to know which is real, while it does
have internal state that is incompatible with the current chain (the set of
appointed leader nodes).
There's not even any useful heuristics to detect an attack, because any
sign of a forgery attack in the blockchain is indistinguishable from an
actual real world event (and can be induced in the real chain). And any
strong heuristics would lead to a looks-least-fake fork competition with
ugly play = another modified proof of work scheme.

@_date: 2017-02-28 23:52:46
@_author: Natanael 
@_subject: [Cryptography] Attaching the signing public key to data being 
Den 28 feb. 2017 22:32 skrev "Ron Garret" :
One of the things you have to decide when designing a signature protocol is
exactly what is to be signed.  Simply signing a raw document is a bad idea
because that leaves you vulnerable to chimera/Dali attacks (
f938d01c82b75c7.pdf).  At the very least you need to integrate the
mime-type (or something equivalent) and maybe even the file name into the
data being signed.
My question is: would it help to also integrate the public key being used
to produce the signature into the data being signed?  Are there any attacks
that this would help prevent?  Has this construction been studied?  Can
anyone point me to a paper?
Don't have the papers right now, but at least in some cases with RSA you
can take an existing message and signature pair, and construct a matching
keypair (different from the original signing key) for which it still
Attack scenario: Somebody thought a stand-alone signature was as good as a
secure hash in every way.
Based on this, the adversary takes a trusted message (previous validated,
key data discarded...? CT logged signature...?), generates a matching
keypair, and then lies about which signing keypair was used, allowing him
to spoof that message signer by using another keypair which he also can use
to sign other messages.
Definitely a very unlikely attack. Still possible, though.
Note that instead of including the full public key you really just need to
include some pointer / address / identifier for the key that allows you to
look it up and confirm it. Anything unambiguous.
Related previous comment of mine:

@_date: 2017-03-01 01:17:07
@_author: Natanael 
@_subject: [Cryptography] XTS mode IV generation 
Den 28 feb. 2017 14:59 skrev "Darren Moffat" :
I'm looking for information on wither it is safe (not ideal) to use XTS
mode in a Copy On Write filesystem. For ZFS (which is copy on write and
always has a merkle tree of checksums)I used CCM or GCM because there was
space to store the IV and the MAC.
I have a use case where the system is still copy on write but there is no
possibility of storing the IV or a MAC.  In some (but not all) cases the
ciphertext of the blocks are still checksumed.
Tl;dr: It doesn't leak contents, but reveals plaintext repeats (if you can
spy on multiple ciphertext writes).
As for your use with COW, then if revealing plaintext repeats* isn't a
problem, or if your inputs never repeats**, it is safe.
* Repeating in the exact same position. Moving data from one block to
another is indistinguishable from two random writes.
Note: this makes it possible to detect behavior like
** Such as a never decreasing counter.
Note: this is assuming a passive adversary! An active one can deliberately
reverse block contents to previous versions. This can only be mitigated
with some per-session IV or full-volume MAC or anything similar where the
volume is only valid if fully intact.
XTS is deterministic, and encrypts every plaintext block together with
randomized inputs that are separately generated for every block.
There's (AFAIK) no support for using a per-session IV, so these per-block
inputs are static (but are key dependent).
That's a very oversimplified explanation, however, partially because it
needs to be able to support arbitary sized disk blocks (equal to or larger
than the block cipher's block size).
The way it is implemented also limits the maximum size of the encrypted
volume (haven't looked up why) to some multiple of the number of cipher
blocks. For AES with its 128 bit blocks, that's about 4TB (IIRC).
As a result of its design, XTS doesn't reveal if a plaintext block repeats
*within* a volume (meaning that all zeroes is not distinguishable from
random), but...
It DOES reveal to somebody who has *multiple snapshots of the same
encrypted volume* which blocks has changed or not between snapshots, AND it
reveals if they have changed back to a previous plaintext value (such as
repeatedly writing data, and then frequently zeroing out unused blocks).
File headers are also particularly susceptible to end up in the same places
with identical contents in some blocks, which would be detectable.
Without something like a per-session IV and with deterministic encryption
algorithms, this is unavoidable. Rekeying is a possible mitigation, but
requires either full re-encryption (expensive) or tracking multiple keys
(complicated, adds storage overhead).
XTS works great against the harddrive loss / theft threat model. Perhaps
even for your own servers. But it works less great for outsourced cloud
storage, in case you care about metadata secrecy.

@_date: 2017-01-16 02:43:36
@_author: Natanael 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
Den 15 jan 2017 23:21 skrev "Ron Garret" :
As an aside, the underlying situation with respect to inflation is actually
exactly the same for both bitcoin and fiat currencies.  Bitcoin could
inflate if a majority of the mining pool agreed to a change in the
It wouldn't be Bitcoin if they did.
Miners can only enforce "softfork rules" using a majority - a stricter
interpretation of what block contents are valid than what the default rules
implemented by the client allows (any new rules and behaviors would have to
be defined such that the old clients would still accept them as valid, even
if it was entirely unaware of their existence). A softfork don't require
patching any client software to work, only that the miner majority upholds
the new rules and don't allow conflicting behavior.
Hard rules that break compatibility with with the clients, like changing
the mining reward schedule, requires changing the clients to accept it
(which is called a hardfork). Otherwise they will reject your block, no
matter how much hashing power you may have. Your modified blockchain will
be ignored by everybody. To them it's just some random string of invalid
The regular uninflated Bitcoin blockchain would remain intact, with all the
users still on it.
It would take take the majority of the Bitcoin users agreeing to run
software implementing that hardfork change (inflationary reward schedule)
for it to work.

@_date: 2017-01-22 23:34:18
@_author: Natanael 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
Den 22 jan 2017 23:27 skrev "John Levine" :
But I'm wondering how real the MD5 threat is in practice.  Java JAR files
are ZIP files containing a manifest that lists the other files and can
contain signed hashes of the other files.  So I can see how I could generate
a collision and replace one of the other files with garbage, which might
crash a poorly debugged Java implmentation.  But how likely is it that I
could replace one of the other files with a different Java program?
hash collision generators with GPU acceleration and more.
You can trivially generate valid files with colliding hashes.

@_date: 2017-01-27 09:01:46
@_author: Natanael 
@_subject: [Cryptography] HSM's to be required for Code Signing 
Den 27 jan. 2017 06:09 skrev "Peter Gutmann" :
This seems a lot like security by press release, if you look at the changes:
Since level 2 HSMs are expensive, not so easy to find, and a pain to use,
companies are probably going to take the other option of moving their keys
into the cloud.  So instead of having the key on an, at least on theory,
isolated machine on a private LAN it's now in the cloud.  Wonderful.
While unlikely to be implemented that way, it *could* be secure. Strong
emphasis on *could*.
Program the HSM to only accept customer requests that are signed by their
trusted keys, or sent over a trusted channel directly to the HSM. Any
overrides by the cloud company MUST be logged and audited by an independent
entity (such as if the customer reports they lost the authentication key).
No web interface. Too hard to protect (XSS and all that). Only a simple
secure interface with authentication protocols like U2F (phishing and
replay protected), from a secure machine.
So the problem here was that if a malware researcher requested a revocation,
the CA typically did nothing.  Now they're still free to do nothing, as long
as they claim they're investigating.
The first of those two arguably makes things worse rather than better, and
second is just business as usual.  The final one, use of TSAs, is
by the way certs work and mostly a no-op.  "Realizing the importance of the
case, my men are rounding up twice the usual number of suspects".
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2017-07-02 20:11:43
@_author: Natanael 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Den 2 juli 2017 19:40 skrev "Florian Weimer" :
I'm leaning towards AES-128 in CTR mode with a per-process key, a
global counter, and per-thread counters and output blocks (so around
20 bytes of TLS storage, instead of >176 bytes if we stored an AES key
schedule for every thread).  We'd probably put the per-process key in
non-dumpable memory.  Fork safety is somewhat easier for us than for
others because we can directly invalidate the per-process key from the
fork implementation.  (Clone safety is more complicated.)  Seeding is
still tricky, of course.  [...]
Proper reseeding after fork will be complicated if getrandom is not
available; we'll probably do something theoretically unsound in that
case (and tell people to use a fixed kernel if they dislike this), or
use a MAP_SHARED mapping to maintain counters shared across processes.
Seems reasonable enough at a first glance.
Most *sane* VM:s / hypervisors / other virtualizating load distribution
mechanisms that can fork / clone software instances should have API:s for
provisioning unique entropy to every instance as soon as its created /
forked. I believe this is often implemented via a kernel driver in the
guest OS, and where possible you should tap into this to as quickly as
possible replace all cloned seeds.
Every time a new instance of *anything* that needs unique private entropy
is spawned, it needs to aquire this entropy in some safe way before
continuing with anything security sensitive.

@_date: 2017-03-08 10:42:32
@_author: Natanael 
@_subject: [Cryptography] In ECDSA, 
garbage
Den 8 mar 2017 00:10 skrev "Phillip Hallam-Baker" :
It is the property that gives rise to the malleability property (I think)
Yes you can create a valid ECDH signature for garbage. But the garbage
does not match the hash of any data you know the value of.
And yes, it is known because this is the mechanism that it is claimed was
used to empty Mt Gox.
ECDSA includes the hash function. It is not an optional part.
That wasn't exactly it.
They did use malleability, yes, but not to sign garbage. The signature
still signed the same original data - it  just wasn't encoded bitwise
identically, which changed the *transaction hash* (the hash of all
transaction data with signature included).
Mt gox tracked payments by transaction hash, not by UTXO identifiers
('unspent transaction output", comparable to individual coins) which was
already being recommended.
In other words, I could request a withdrawal, replay a slightly modified
version of their payment transaction to me, hope it gets chosen by the
miners, and then tell Mt Gox' support that I didn't get my money (which I
actually did), so then they pay me again with a NEW transaction, spending
*other* UTXO's (giving me "different coins", pretty much). While they
instead should have verified the UTXO and said "we spent UTXO 123 going to
his withdrawal address, and sure enough that's in the blockchain".
If the transaction really disappeared, they should resend it using the same
(then still unspent) UTXO's.
If they really had used it to sign garbage, the Bitcoin network would have
rejected that garbage data for not being a valid transaction.

@_date: 2017-03-08 17:24:22
@_author: Natanael 
@_subject: [Cryptography] encrypting bcrypt hashes 
Den 8 mars 2017 16:01 skrev "Robin Wood" :
I've been asked by a client to give some advice on hashing and as it isn't
my area I'm looking for someone who knows what they are talking about.
The client is hashing 4-6 digit PINs (mostly 4 digit) with bcrypt, they
have the work factor set as high as the business will allow them but they
are worried that due to the small key space it will still be possible to
reverse individual PINs. They are now thinking of encrypting the hashes
before storing them to add an extra layer of protection. The encryption is
fast enough to not affect login times so my suggestion of using the
additional processing to increase the work factor instead was rejected.
What do people think? I can't see it hurting and adding the additional
hurdle probably won't hurt but I've heard of odd interactions between
different algorithms so don't want to say to do it without someone who
knows their stuff looking at it.
I'm assuming this is to protect against password database leaks.
Security people tend to really not like seeing reversible functions used
around passwords. For password hashing, mixing in a secret value in the
hash is just as effective as encrypting its output.
This is just regular salts when done per-user.
When you have an extra server side secret shared across users, that's often
called a pepper. They're usually used in parallel with salts. I seen
examples of this implemented using HMAC, where the authentication server
uses their well protected secret to calculate the HMAC of the hashed
password, to then compare the outputs to the stored value. This
authentication server doesn't need to be the one doing the password
These two options (encryption vs HMAC with a pepper) are about as fast, and
both require access to a server side value that is extra protected.
The other option is to not use something that can be trivially bruteforced,
such as a form of public key authentication if possible (something like U2F
preferably). Then you have to store the secret client side.
What kind of client software / hardware is used?

@_date: 2017-03-20 19:54:20
@_author: Natanael 
@_subject: [Cryptography] Crypto best practices 
Den 19 mar 2017 23:50 skrev "Kristian Gjsteen" :
What practical people appear to worry about is that you need to process the
entire message before you can begin producing ciphertext. This seems to be
a requirement for misuse-resistant modes, and it is probably possible to
prove a theorem to that effect. With AEAD modes such as GCM, you can begin
producing ciphertext the moment you get the first bytes of the message,
which is convenient in some settings, I am told.
As for a quick demonstration of that you can have secure "streaming
behavior" without needing to start off with a full pass on the plaintext /
ciphertext before final encryption / decryption:
Option A: Use CBC mode (or another non-stream cipher mode), but instead of
computing an HMAC authentication tag on the full ciphertext first when the
encryption is done, you compute HMAC tags on one or a few blocks at a time
and append the tag to those blocks. Then for each set of blocks after the
first set you also use the previous HMAC tag as an input (chaining the HMAC
This requires both linear encryption and then linear validation of every
block from start to end as you decrypt, but proves at every stage that the
full message is intact from the start to the block you're currently
processing. (It can still be decrypted in parallel.)
Option B: As above use CBC, and for ciphertext authentication you
iteratively build a Merkle tree hash of the ciphertext blocks as you go
during the encryption, and sign each Merkle root hash version, then you
validate each of the ciphertext blocks against the signed Merkle root on
For reference, Tahoe-LAFS already uses Merkle tree hashes to enable
"seekable decryption" on static files and simpler sync: This could be used for example to stream video over a network, where you
first would continously transmit the signed partial Merkle tree roots, each
substituting the prior one, until done streaming (this would however be
fairly inefficient compared to the other options). Then you can just reuse
that last signed Merkle root for the full video file if retransmitting it.
It can be decrypted and validated in parallel, allowing you to jump to any
part of the file at will to decode, but for encryption it requires some
linearity in computing the Merkle tree (although with the right underlying
ciphermode you can still encrypt in parallel).
Technically this does mean that you're chaining together many small
plaintexts if going by your description, so I guess you're technically
correct. Also all full-message authentication mechanisms seems to require a
linear processing component during encryption.
With authenticated CBC mode you only reveal how many initial plaintext
blocks that are identical in the case of key+IV reuse. With both of these
authentication methods there's no malleability or block substitution - you
know that the entire full message is intact, no cut-and-paste of individual
blocks across different messages with the same key. No encryption key
leakage or authentication key leakage.
As for single-pass methods that *probably* would simultaneously be much
simpler, faster, have less overhead and yet be robust and secure (although
neither with full-message authentication), here's the two approaches that I
1: Tweakable block ciphers.
For example, see AEZ, with Rogaway as one of the authors:
A tweakable cipher like AEZ encrypts the plaintext in a single pass, and
has three inputs: the key, the data and the tweak value (variable size).
What this means is that every trio of plaintext block + key + tweak will
produce "independent" encrypted blocks, since the tweak behaves as an
extension of the symmetric key (and the tweak can be public knowledge,
(Note: AEZ also has a variable block width, which makes it a convenient
replacement for XTS mode as well.)
This tweak allows you to define your cipher modes by for example feeding
AEZ a tweak input composed of an IV + a counter (mimicking CTR). This also
gives you much higher security than CTR mode, since reuse doesn't leak
anything other than information on which blocks in the same locations that
have identical plaintexts across the repeated encryptions (same as XTS
mode). It also isn't malleable like CBC mode. Every block is still only
processed once.
For AEZ, their own recommended way of implementing authentication (and
preserving the single-pass encryption) is to use the variable block width
to make the block size larger than the plaintext, and setting the remaining
bits to a fixed authentication string. For example, you could have a 256
bit block with 192/64 bits of plaintext / authentication string. Or any
other size of your choice (within the limits supported by AEZ).
On decryption, you look for the presence of this authentication string
(which may be all zeroes) in the plaintext before continuing. Any
modification of the ciphertext (or tweak) will unpredictability randomize
the decrypted data in the affected blocks, with the authentication string
length determining the probability of the string unintentionally matching
(assuming a string of 64 zeroes, a random match has a probability of 1 /
This makes it non-malleable and gives you a security level at least
matching AES-GCM-SIV using *one* primitive and with none of the fragility
(GCM-SIV still only tolerates a limited number of nonce repetitions before
breaking too).
(Note that the tweak input also allows it to double as an AEAD, since the
tweak by design is validated together with the plaintext.)
Adding an authentication tag like Poly1305 as an option allows you to
separate authentication from decryption (with a latency penalty), which
could be a useful feature in for example edge routers in datacenters
(allows you to reject invalid incoming ciphertext blocks in hardware that
doesn't know the more sensitive decryption key). It would also allow you to
authenticate any AD (additional data) without decryption, which could
assist session management.
2: A hybrid mode with a stream cipher keyed block cipher.
See Enchilada for reference: As mentioned previously this can be done with XEX mode (XOR-encrypt-XOR)
with a static block key (see "Minimalism in cryptography") . Authentication
could still be implemented using either one of the same methods as above,
tags or fixed authentication strings in the plaintext blocks. Every block
is keyed individually from the stream cipher, the stream cipher is fed the
Note that to make this an AEAD, you either have to use an authentication
tag over the ciphertext or something similar to a tweakable cipher for the
encryption primitive.
The main advantage of this over AEZ is the reduced implementation
complexity; a simple stream cipher and simple XEX block (small, static and
parallelizable) vs a tweakable variable-width block cipher (needs to be
duplicated in full to parallelized). A tweakable cipher is presumably also
more sensitive to sidechannel attacks. However AEZ has fewer moving parts,
which is a plus. The tweak also enables in-protocol domain separation
between packets by using different tags for different data channels, with
minimal overhead.
The hybrid method should also have reduced latency - the stream cipher core
can feed a large number of parallel XEX instances with keystream inputs
every cycle, and you feed each XEX instance with a piece of plaintext /
ciphertext. (If using authentication tags, you just add that in the XEX
block data pipeline, same for both types of ciphers.)
Both options should share the advantage that they aren't limited by a
relatively low maximum number of plaintext blocks that you can encrypt
per-key before it becomes insecure (GCM) or that it just halts (XTS, if I
remember correctly), assuming that the stream cipher used in the hybrid
method has a large enough internal state. You should be able to encrypt
arbitrarily large amounts of data without worrying.
Since neither of these methods have full-message authentication, then for
key+IV reuse their biggest problem is that blocks in the same position can
be swapped arbitrarily between the two encryptions. This would allow file
headers to be substituted, and other similar attacks. Preventing that does
require processing the full plaintext in one pass, but the authentication
modes explained in the top solves that without breaking streaming.

@_date: 2017-03-21 07:55:31
@_author: Natanael 
@_subject: [Cryptography] [FORGED] Re:  Crypto best practices 
Den 21 mars 2017 07:22 skrev "Peter Gutmann" :
It's not too hard to come up with a Rube-Goldberg mechanism that deals with
particular issues like misuse of IVs or streaming, but the problem with
schemes is that they take the initial issue, that IVs are too confusing and
complex to deal with so people get it wrong, and make the problem ten times
worse.  The reason why RC4 was so popular is because it's incredibly simple,
it takes a key and magics plaintext in-place into ciphertext and back again
with no message expansion or other complications.  I'm not saying that you
do the equivalent of RC4 in a sound manner, but if you've got something that
gets misused because of its complexity then the proposed replacement
involve even more complexity.
True, but that also wasn't meant as an actual suggestion of what to do.
Only to show it *can* be done (because he assumed it could be proven
impossible). There's probably much better ways to do it in case streamable
full-message authentication would ever be necessary (I can't really see any
need for it outside of offsite FDE backups), but I'll let somebody else
figure that out. This is also why I mentioned the two later options,
because they would actually be easy to use (I assume).

@_date: 2017-03-21 13:04:32
@_author: Natanael 
@_subject: [Cryptography] Crypto best practices 
Den 21 mars 2017 10:36 skrev "Kristian Gjsteen" :
20. mar. 2017 kl. 19.54 skrev Natanael :
the entire message before you can begin producing ciphertext. This seems to
be a requirement for misuse-resistant modes, and it is probably possible to
prove a theorem to that effect.
behavior" without needing to start off with a full pass on the plaintext /
ciphertext before final encryption / decryption:
Note that misuse-resistant means that in the event of IV reuse, all you
reveal is whether two plaintexts are equal.
Your proposals reveal whether two plaintexts have an identical prefix,
which is too much. So in the context of my claim, they are irrelevant.
Going by such a strict requirement, then of course - in order to completely
prevent any leakage of which ranges of plaintext blocks that are identical
across different encryptions under the same key+IV, then the change of any
plaintext bit must also change the full ciphertext. (I believe that's also
called an "all-or-nothing transform", where any change anywhere must modify
all ciphertext bits, not just make a local change.)
This means you do need to process the entire plaintext before encryption,
and making the encryption dependent on the full plaintext. For example it
could mean that you first would need to hash (HMAC?) the full plaintext
file and use that hash as an input to the encryption, such as using it as
the IV. (Another sidenote, Tahoe-LAFS already does something like this too
with its "convergent encryption" mode - and streaming decryption is still
I'm however not convinced that such a strict misuse resistance requirement
is *always* necessary, there's definitely a lot of different levels to the
degree of protection necessary. Hopefully in most cases where this level of
metadata protection matters you wouldn't actually need the cipher mode to
be the (only) component to achieve this (I'm assuming there normally would
be multiple layers of security in the way).
Full misuse resistance creates a lot of overhead in both bandwidth/disk
access and CPU load, so there's good reasons to not always use it. This
level of protection would definitely be impractical for most interactive
network protocols, to start with, where you would have to hope that the key
exchange provides unique keys for every session (a predictable
all-or-nothing encryption key protects exactly nothing).
And if your FDE software can't securely handle IV:s, then you probably have
bigger problems.
There is simply limits to how far you can idiot-proof something, and if
your higher layers are broken then the crypto can't save you. Garbage in -
garbage out. Crypto can only package it better.

@_date: 2017-05-26 18:56:45
@_author: Natanael 
@_subject: [Cryptography] key lengths in different places 
[a summary of 3DES details...]
Yes this is all normal for 3DES. When talking about bit strength we talk
about the approximate equivalent amount of CPU power required to crack it
for the best known applicable attack versus raw bruteforce.
A regular secure cipher designed for 112 bits would then require
approximately as much power to crack as 3DES despite the 168 bits worth of
key bits.
This is due to meet-in-the-middle attacks, which you likely already have
heard of as an explanation of why it's just estimated at 112 bits of
security. Encrypt-decrypt-encrypt then gives us just 56x2 = 112 bits of
security instead of 56x3 = 168.
Unfortunately there's no real standard for how to report bit length vs
strength in cases like this. Some people suggest using the term work factor
(WF) with the same bit metric as above to describe it, separate from the
keyspace / bit length of the key.

@_date: 2017-10-15 00:44:41
@_author: Natanael 
@_subject: [Cryptography] Suggestions for wearable wireless technology ? 
Den 14 okt. 2017 23:06 skrev "Henry Baker" :
I'm working with a small medical device company which is making wearable
The problem is: what wireless technologies are there that have the
following characteristics: [...]
The other low power protocols -- e.g., Zigbee -- don't seem to have
security & privacy necessary for a human wearer designed in.
I was thinking about some sort of low power optical signaling which could
wake up a wifi radio for brief high bandwidth bursts.  Alternatively, it
might be possible to utilize some sort of optical sensor and a high
frequency LED to establish a high bandwidth optical link, but what kind of
protocol would be ideal?
There still needs to be highly encrypted signaling and the highest quality
authentication (e.g., for the doctor's office and/or firmware upgrades)
You could use low power radios like zigbee for discovery / signaling and
WiFi Direct to communicate. Both have open source software stacks
available, cheap integrated radio chips available and have wide support.
Alternatively, if line of sight is guaranteed at all times, IrDA has
gigabit versions available now. Pricing / licensing unknown, though.

@_date: 2017-10-18 22:12:28
@_author: Natanael 
@_subject: [Cryptography] [FORGED] Re: Severe flaw in WPA2 protocol leaves 
Den 18 okt. 2017 21:09 skrev "Peter Gutmann" :
  RC4 is a stream cipher for which key/nonce reuse results in a catastrophic
  failure of the cryptosystem.
  GCM is a stream cipher for which key/nonce reuse results in a catastrophic
  failure of the cryptosystem.
For the benefit of similar non-experts on this list, could you please point
out which cryptographers disagree with that?  Since the view that they fail
the same way is one that is not widely shared, there must be lots of names
can cite to support this.
(The reason for asking for names is so I can avoid any cryptosystem they've
While I do agree that stream ciphers should be avoided whenever you can not
guarantee perfect key management and nonce generation, that's clearly not
what his comment was about.
The main difference between the two is that RC4 can be cracked even without
nonce repetition occurring, as in the now infamous WEP standard.
You can not do that with AES-GCM, and even less so with GCM-SIV mode which
also adds some limited misuse resistance (tolerates some  number of nonce

@_date: 2017-10-27 02:52:17
@_author: Natanael 
@_subject: [Cryptography] Mesh Key Recovery 
Den 27 okt. 2017 00:09 skrev "Phillip Hallam-Baker" :
In the real world, houses are destroyed by flood and fire, people are
displaced by war or tyrannical governments. If we are to meet the full
security requirements of users we must consider data availability to
be at least as important as data confidentiality.
Quick thought: isn't it possible / likely to have different sets of data,
where for one you'd rather lose all copies than give it away (like say a
diary) while other data would rather be made public than lost (such as
family photos)?
Would it be practical to define independent sets of data with different
recovery keys for such purposes? One key you can be less careful with, one
that you protect carefully? (Or even more tiers)
I've already been thinking about how a personal identity system should have
some representation of "personas" / contexts, and it could be something
simple in terms of interface like "family stuff" vs "secrets", or however
you want to represent it. Like having different "workspaces", or several
"Facebook pages" tied to the same person. Rather than micromanaging
permissions for most data you add into the system, you link it to contexts.
And recovery options with it. (although I believe most people would choose
the same recovery procedure for everything just for simplicity).

@_date: 2017-09-25 00:40:17
@_author: Natanael 
@_subject: [Cryptography] Crypto basic income 
Den 25 sep. 2017 00:15 skrev "Guilherme Campos" :
I've written a "food for thought" kind of article around the usage of
cryptocurrencies for universal basic income (
guilhermepcampos/universal-crypto-basic-income-460fc46207f6). And, although
this is not 100% on topic, I would definitely like some opinions around
usage of biometrics for key generation. More precisely on:
   - Do you think it will be possible, at some point, to have some sort of
   biometric public key infrastructure, where public and private keys are
   directly pegged to a persons biometric details?
No, not unless you accept the use of schemes like Identity Based Encryption
which invariably requires central servers (although there exists a few
versions of these schemes where the server doesn't need to be trusted all
that much).
Just the biometric data alone in isolation is never good enough for
cryptographic secrets.
   - Could biometrics completely replace the need for passwords?
No, not unless they're just used to identify cryptographic public keys
where you hold the private key. Universal usernames, basically. I don't
care if you surgically place your hardware authentication token in your
skull or whatever to not need to remember to carry it with you, you just
need some kind of digital secret.
   - If compromise of biometric details occurs, how would one go around to
   solve this, since it's not possible to, for example, create a new
   fingerprint for a person?
You don't. If they're your last line of defense, you've already lost if
they're compromised. So don't make them your last line of defense.

@_date: 2018-04-05 11:26:59
@_author: Natanael 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
Den lr 24 feb. 2018 08:19Ray Dillinger  skrev:
I've already posted my idea to solve exactly this on the Bitcoin
development mailing list. (didn't get much response, though)
Scale difficulty with block size.
Making large blocks makes it more expensive to solve the proof of work.
This is only profitable if creating larger blocks for some reason is
profitable to you.
This requires somehow getting paid for making the blocks larger, typically
meaning that you are including fee paying transactions from *other* people
(fees may be paid via another channel). Bloating it with your own
transactions is then only an accounting trick that still lose you money
because of increased difficulty.
(This doesn't solve the superrational attack by a rich attacker, but
increases the cost.)
The subsidy per block wouldn't necessarily need to scale much. We're still
aiming for a consistent block rate, that now adjusts difficulty based on
both current average block sizes + block rate.
Also, my favorite solution for efficient large blocks since I first saw
somebody mention application of Zero-knowledge proofs to Bitcoin is proof
carrying blocks - while the raw blockchain is just as large as before,
regular users don't need to keep anything other than the headers - a simple
Merkle tree hash root with a Zero-knowledge proof of validity, and the
block hash (PoW hash).
All other data is referenced by that Merkle tree hash, and can be served /
fetched as necessary (such as payer -> payee) to show the exact contents.
Also this allows secure checkpointing such that all old still relevant data
(unspent outputs) can be frequently collected into a compressed block with
its own Zero-knowledge proof. This is a safe way to use a rolling
blockchain that in practice only grows approximately logarithmically with
usage, instead of superlinearly.
Another sidenote, nobody seems to have mentioned tech like Lightning
Network (LN) yet.
By using a combination of multisignature transactions, time expiration
scripts, notary/relay LN nodes and collateral provided by these nodes, as
well as transaction channels (a protocol for two parties to use a valid but
non-committed "transaction template" as a running tab for payments), it
creates a very robust system with strong theft and abuse resistance
It's a federated system like email, where instead of paying directly via
the blockchain, you use a node to coordinate a larger number of payments
that then gets routed across the network of relays. These then gets settled
periodically on the blockchain. This reduces the amount of traffic that the
miners need to handle.
As mentioned above, I'm a fan of Zero-knowledge proofs. My ideal
cryptocurrency system would essentially be a system that makes LN "native",
supports execution of complex scripts within this system, and which uses
Zero-knowledge proofs to create compressed blocks that represents the full
state of the system.
This would essentially be a stronger version of what Ethereum promises to
be. Programmable money, for real. You would have genuinely smart contracts
that can resolve in seconds. By having everybody provide well structured
data (signed by the respective publishers) into the system, everybody would
have the capabilities that currently only HFT traders and similar has,
where everybody can work with public API:s.
It would even allow you to enforce these complex contracts without
revealing their contents!

@_date: 2018-08-18 10:21:21
@_author: Natanael 
@_subject: [Cryptography] Rescuing Encrypt-then-Sig 
Den lr 18 aug. 2018 09:23Ray Dillinger  skrev:
There should be easier (or faster) ways. Why not HMAC both the plaintext
and the ciphertext and bundle those two tags with the ciphertext? No need
for multiple encryption, no new data leaks. Perhaps you can even sign the
tags if you want to involve public keys, and maybe only separately encrypt
the tags + signature if you want added privacy without encrypting the full
plaintext message twice (but how would you know which key to use to
decrypt, trial and error?).

@_date: 2018-02-07 00:32:41
@_author: Natanael 
@_subject: [Cryptography] Samsung Begins Manufacturing ASIC Chips for 
Den 6 feb. 2018 19:38 skrev "Tom Mitchell" :
Samsung has designed and will be shipping ASIC Chips for Mining
Does this have impact on the economics and security of normal encryption.
Short key lengths seem like a bad idea.
Nope. They're not FPGA:s.
They're single-algorithm single-mode highly optimized massively parallel
A template for a block is created, and then every mining unit creates a
bunch of valid variations of it to increase the available entropy, then
thousands of individual cores in the unit just run the chosen proof-of-work
algorithm based on this block (header) after inserting a random number in
the specified field.
It will then check the output against the difficulty threshold, reject and
increase the counter for the next cycle if it fails. Requesting new
templates when the core is running out of random numbers to test (Bitcoin's
header nonce is only 32 bits or so).
To be at risk of attack due to this, you either have to be using an
algorithm configured exactly as one of these popular PoW methods (very
unlikely) or use one where the PoW mining engineering research by itself is
sufficient to make an attack against you cheaper (imagining NSA stealing
ASIC blueprints to build a dedicated crypto cracker).

@_date: 2018-01-29 17:20:51
@_author: Natanael 
@_subject: [Cryptography] DAG vs Blockchain 
Den 28 jan. 2018 22:09 skrev "Ray Dillinger" :
The proof-carrying Directed Acyclic Graph (the rough parallel to a
block chain) needs some additional magic to provide proof of
completeness.  IOW, you have to find a way for someone to guarantee
that they have all the information relevant to what they want to do.
This is a problem that probably has a good crypto solution, but so far I
haven't seen it solved in a way that would allow a secure cryptocurrency
application to scale better than a block chain.
In a cryptocurrency application, let's say Alice has a TxOut she got in
an earlier transaction (from Zebulon) and wants to make a payment to
Bob.  Alice can use either a block chain or a DAG to show Bob that the
transaction where she got the payment is valid. She can show the
block containing that transaction, and then show previous blocks,
with hashes that verify, in a path that leads all the way back to the
genesis block.  Nice enough.
But that isn't enough for Bob to know that the payment is valid.  He
must also know that Alice hasn't already used the tx output to pay
Carol, in transaction after she got it.  With a block chain, any such
transaction from Alice to Carol must be in one of the blocks on the
singular path from Zebulon's payment to Alice, to the present.  Bob
knows that if he follows the block chain back and doesn't see it, then
it doesn't exist.
But With a DAG, the payment from Alice to Carol (if it exists) may be on
some branch Bob hasn't seen.  So unless Bob has every block of every
branch from the moment Alice got that payment, that transaction
invalidating Alice's proposed transaction to him could be somewhere he
hasn't seen.
There's no proof of non-existence possible unless everybody can agree on
the scope of the proof (this includes proof of non-knowledge). Unbound
scope means proof is impossible.
I can prove there's no black swan in my box by showing it is empty. The
problem becomes exceedingly difficult as the physical space gets larger.
In terms of a blockchain you can prove no doublespending by referencing the
UTXO set (unspent transaction outputs), which can include either showing
all later transactions (as you described), or fancier math like a
combination of the relevant UTXO ID, the latest block hash and an
Zero-knowledge proof (ZKP) that there exists no transaction spending that
UTXO in the blockchain which has this block hash as the latest hash.
As new blocks arrive its easy in the short term to prove there's no
doublespend as the full dataset is a few MB at most, while in the long term
you create a more fresh ZKP when requested.
In terms of acyclic graphs, there exists no way to completely prove
non-knowledge. You can only prove non-existence of conflicts within
branches which you admit to knowing about, but since you can't be expected
to know all branches it is likewise impossible to produce meaningful proofs
of knowing there's no conflicting doublespend transaction.
There's no well known / well defined domain / space of what dataset in
which you would show non-existence.
At best you can have a challenge-response protocol where both parties show
what branches they admit to knowing, and then a ZKP would be generated
which cover those branches. This is only a complete proof if the given
combined sets of branches is complete.

@_date: 2018-02-01 03:36:44
@_author: Natanael 
@_subject: [Cryptography] canonicalizing unicode strings. 
Den 31 jan. 2018 03:58 skrev "Nico Williams" :
We had homoglyph problems before computers ('l' vs '1', for example).
They got worse not because of Unicode, but because the world is more
connected now.  Yes, we could, eg, have had a unified CJK codepoint
assignment set, but it turns out people didn't want that.
Basically, we just have to accept these issues and deal with them as
best we can: with code to heuristically detect phishing based on
homoglyphs, and code to fuzzily match Unicode identifiers.  Such code
has to evolve as scripts are added to Unicode and/or new homoglyph sets
are discovered (if we don't already know all of them).
I only see one plausible solution for multilingual text form identifiers,
although definitely not an easy one (wall of text coming).
It is to actually perform large scale testing with real people, in multiple
languages and cultures, and then feed that collected data to algorithms to
figure out what patterns (symbols and sets of symbols) that people
*actually* are at risk of confusing.
We can not rely exclusively on heuristics without any data from humans.
Finding collision candidates (potential homoglyphs) could be done with
fuzzy visual comparison algorithms / heuristics to find pretty much *all
plausible* pairs of confusable symbols, and also sets of symbols. This is
not for determining what's actually confusable, but to find candidates.
Multiple methods would need to be used. We don't care if it suggests
obviously distinct pairs here, it will be filtered out later. We'd rather
have many false positives than many false negatives.
To perform the test you could for example first come up with many, many
different kinds of samples of texts and other usage of symbols meant to
convey meaning (in multiple fonts!). Not only standard plain texts, but
also all forms of lists and instructions and more.
Then we would be randomly replacing symbols in the sample texts with
similar ones.
Then you ask people to try to read these modified texts, as well as
originals. You would ask people to identify symbols, and to tell which
symbols and/or sentences that are distinct or identical, and whichever
other tests may be necessary.
When you have all that real world data on how real people actually read all
the symbols, context included, you would probably need machine learning
algorithms to process it all (simple statistics are likely to miss a lot of
Then finally you could use the results to produce guidelines for real world
usage, producing a model of how real people parse visual symbols.
After all that, if we want a globally usable set of visually distinct
symbols complete enough to write in most languages, then we could try to
create a sufficiently complete list of non-ambiguous symbols - most likely
by "dumbing down" pairs of similar symbols to single "universal" symbols,
or "canonical homoglyphs" (?), with less detail and which depending on
context is still easily recognizable as the correct intended writing
However I can imagine a lot of people of non-latin script languages
wouldn't be too happy with that solution, if it were the only one (a large
number of symbols would be "inaccurate" to various degrees). Another
solution, IIRC already used for domain names, is to create such lists per
individual script and forbid mixing different scripts. That could still be
problematic in but cases obviously, because of an infinite number of
multilingual texts and much more.
However, for anything meant to be an identifier in text form, a script
mixing ban SHOULD be enforced.
Alternatively, limit the number of combined scripts to fixed sets depending
on context, and then ALSO apply the above solution for filtering homoglyphs
to replace them with "canonical homoglyphs". Reducing the number of
simultaneous scripts limits the number of visual collisions.

@_date: 2018-07-31 22:45:44
@_author: Natanael 
@_subject: [Cryptography] Signal double-ratchet vs. future breaks in ECC? 
Den tis 31 juli 2018 20:47Nemo  skrev:
When the Signal app rekeys, it does a key exchange and then hashes together
that new key material with the most recent ratchet state.
When you add a completely new device (or reinstall the app), it will
however rekey from scratch from that new install.

@_date: 2018-06-08 21:52:04
@_author: Natanael 
@_subject: [Cryptography] Data Protection Against Quantum Computing Brute 
Den fre 8 juni 2018 21:22Govind Yadav  skrev:
In practical terms, application = algoritm. All of the math and code and
instructions becomes electrical impulses in the CPU:s. There's no tangible
This is simply not possible without using specialized security hardware
like TPM or smartcards - the algorithm can not know if the attacker lies
about the provided device ID. The attacker controls *everything*, he can
perfectly emulate the target device for as long as he knows everything he
need to knows about it (hardware / firmware / OS / applications).

@_date: 2018-06-10 20:57:44
@_author: Natanael 
@_subject: [Cryptography] Data Protection Against Quantum Computing Brute 
Den sn 10 juni 2018 20:19Govind Yadav  skrev:
This sounds like you require perfect knowledge of the client hardware, and
essentially computes a fingerprint of its behavior, and then uses the
fingerprint as part of the secret.
The first problem is that this fingerprint likewise can be attacked, and
secondly with hardware attacks like debuggers and JTAG cables and various
sidechannel data leaks, it's possible to extract even these secrets from
the target hardware to replicate it.

@_date: 2018-06-20 15:25:23
@_author: Natanael 
@_subject: [Cryptography] How to make rowhammer less likely 
Den tis 19 juni 2018 18:27Bill Frantz  skrev:
At most you could label some memory as sensitive and some as not sensitive.
However I don't exactly expect most developers to use it correctly...
But if say all CPU instructions always were stored encrypted in RAM, as
well as stacks and related important data structures, while leaving for
example most client application data in plaintext (unless the application
asks for encryption), then malware using rowhammer could only plausibly
target application data in RAM, but not target code or access controls. It
couldn't escalate privileges outside its sandbox, but for example
Javascript in a browser iframe could potentially insert a call home into
the page it's embedded in unless the browser also opts in to encrypt the
runtime memory.
IMHO it's better to restart a failed process than let it keep running after
being targeted by rowhammer or otherwise getting the memory corrupted.
Designing software to be crash tolerant is simply more effective.

@_date: 2018-06-27 19:09:34
@_author: Natanael 
@_subject: [Cryptography] Wi-Fi WPA3 announced 
Independent commentary:
Other discussions:
Notable new features:
Encrypted open wifi networks. Resistance to dictionary attacks against
passwords. Protected management frames are mandatory (better resistance
against abusive peers).
Possible downsides: Dragonfly is the primitive behind the authentication
protocol, and it's gotten a fair bit of criticism - primarily for very poor
sidechannel resistance.

@_date: 2018-03-26 00:40:06
@_author: Natanael 
@_subject: [Cryptography] Justice Dept. Revives Push to Mandate a Way to 
Den mn 26 mars 2018 01:12Erik  skrev:
The legal equivalent of the nuclear option is something like the clipper
chip / access via TPM:s or equivalent circuitry, demanding full access to
observe everything that happens, perhaps remotely. Perhaps even banning
non-approved hardware from accessing the phone network.
That would mean devices would ship essentially irrevocably compromised. You
would need to physically tamper with the CPU, probably breaking it, to
remove it. You can't really achieve meaningful security otherwise.
For anything less, where they only got access if they have physical access
to your device: just only use apps with encrypted communication and
storage. Use strong passwords. If they decrypt your device they still don't
know the keys to for example your Signal database or your OpenKeychain
You could otherwise repurpose other clean hardware, like using an RPi (with
the necessary accessories) as phone. Much less fancy and usable, sure, but
it would be the safest option.

@_date: 2018-03-26 01:02:45
@_author: Natanael 
@_subject: [Cryptography] What everyone is saying about mobile OS security 
Den sn 18 mars 2018 07:54Ryan Carboni  skrev:
This would cause all OEM:s to do an Amazon and ditch Google, because even
if they could afford it they would consider it unprofitable compared to the
option of selling their devices with alternative app stores and services.
Google is already applying as much leverage in terms of security as they
can. Trying to be stricter would make them lose the grip.
Samsung already have copies of pretty much every important service or tool
Google has for Android. Too hard requirements from Google would make it
worth it to ditch Google and put more funding into their own competing
services. Samsung even has their own OS, Tizen. LG also has WebOS.
You should look into project Treble. Google has officially parted the
Android userspace from the kernel and HAL in Android 8.0 with standardized
API:s in a way that makes updates much easier. They're reducing the cost of
developing updates.
Once every new device ships with Treble, then Google will finally be able
to put more pressure on issuing updates more frequently without too much
resistance from OEM:s, because then the profitability calculations will
finally be in favor of security.
So if nothing has changed in about a year or two from now, then your
criticism would be completely fair. But right now it's not taking market
dynamics into consideration.

@_date: 2018-05-15 00:10:27
@_author: Natanael 
@_subject: [Cryptography] FW: [13 Principles] An important urgent notice 
Den mn 14 maj 2018 21:35Jack Harper  skrev:
It affects all PGP encrypted files accessible to a malicious third party,
when you'll also later open them in a client that don't enforce secure
ciphertext authentication (correct usage of MDC in this case, alternatively
signature validation).
This include evil maid attacks (somebody messing with the files on your
devices as you're away from them), or malicious tampering on cloud hosts by
hackers or rouge employee.

@_date: 2018-11-21 16:01:10
@_author: Natanael 
@_subject: [Cryptography] Buffer Overflows & Spectre 
Den ons 21 nov. 2018 04:07 skrev Jon Callas :
Here's how I see it:
There's things designed to an exact intent, and those who are not.
Of those that are designed to an exact intent, there are those who follow
it as expected (bug free) and those who don't (buggy).
(Things without an exact intent can still be buggy, but when there's no
expectation to compare to it's not easy to define what's buggy or not.)
The bug can be in code that don't follow the specification. The bug can be
in a design / specification / architecture that don't follow the intent.
The bug can be in unexpected interactions with other systems in the typical
environment, or in a false assumption about how the user will behave.
All of those are bugs, because those are properties about the thing we
built which can be changed such that it no longer behaves in violation of
the intent during typical use. This means it previously *did something
wrong*, given that the intent behind the thing must take typical use into
consideration. Doing something wrong is a bug.
As soon as you advertise a computer as safe for usage for things like
encryption, then you have advertised an intent where every property of the
computer that violates this security is a bug.
As a sidenote: The bug can also be in your expectation. If you the correct
answer to your question isn't what you expected, your expectation was
Sometimes the unexpected or unwanted behavior also comes down to
fundamental physics, in which case it's not a bug because you *can't*
change it. In this case, it's once again the expectation of impossible
behavior that was wrong.
TL;DR: Whenever your expected behavior is possible to achieve, unexpected
behavior is a bug.

@_date: 2019-12-14 12:33:46
@_author: Natanael 
@_subject: [Cryptography] FBI: Don't trust IoT devices 
Den fre 13 dec. 2019 21:55Mo Balaa  skrev:
I know of Mozilla's IoT gateway project.
My own belief is that the solution requires firewalling off ALL not
perfectly trusted devices behind a secured gateway, ideally with
controllable API proxies.
As in, when you connect something like a smart bulb it talks to absolutely
nothing else than the gateway, and it exposes its API to it.
All other devices that wants to control these IoT devices does so via the
gateway, *IF* they have been given the correct permissions to do so, *IF*
they even have permission to know it exists.
One of the hardest technical parts here is making rule based access
controls simple. Much of the other work is just "just" engineering.

@_date: 2019-01-06 21:52:08
@_author: Natanael 
@_subject: [Cryptography] Came up with a weird use case, got questions 
Den sn 6 jan. 2019 06:23 skrev Joshua Marpet :
There's a few possible variants, each with different properties that you
may or may not want.
Time-lock cryptography, which is essentially hashcash / proof-of-work,
wherein the work task (for symmetric algorithms) is created in parallel and
then chained such that they only can be solved serially, which means that
assuming a certain maximum CPU speed (!) you can enforce that a minimum
time passes (by enforcing that a minimum number of CPU cycles passes).
However, without incentive nobody will try to solve it.
Dead man's hand schemes. They're typically not based on cryptography,
instead they rely on a mechanism that will outlive you to trigger an event.
You can combine it with cryptography - it would for example work just fine
to combine this with Shamir's Secret Sharing Scheme by setting up multiple
computers that will run non-stop (perhaps a few solar powered Raspberry
Pi:s), placed in different locations, and when the time is up they each
release their share of the secrets.
This is quite fragile, partially because you can't know for sure what
electronics will last 100 years, and because it's just as hard to know what
companies you could outsource the task to which would last 100 years and
uphold the promise.
Lastly, assuming there's some entity that can simply be trusted just to
continously perform some repetitive task for the entire time (such as just
publishing the current time, digitally signed), not tampering with the
clock, then functional encryption / indistinguishable obfuscation could
work. You'd create a cryptographic program that only releases its secret if
given a signed message with that future date.
The issue with this one is that we don't even have any proper prototypes of
cryptographic algorithms that can do this yet, we're still in the
theoretical academic phase and don't know for sure if these algorithms can
be sufficiently secure.

@_date: 2019-01-10 22:37:44
@_author: Natanael 
@_subject: [Cryptography] DAG vs Blockchain 
Den tis 30 jan. 2018 19:21 skrev Ray Dillinger :
Posting this because I found it relevant;
Somebody just claimed to have a scheme for proving non-knowledge, in
limited settings. Unfortunately it doesn't seem applicable to this problem
domain, but nevertheless it's an interesting claim:
ignorant. We explore the settings in which one could give a convincing
proof of ignorance.
x along with a corresponding proof of ignorance on her own. For example,
suppose there is a way for Alice to sample an instance x through a
provably random process. Then Alice can convince a verifier that she does
not know a witness for x (assuming the language is hard on average). In
some sense, a proof of ignorance is a proof that the instance x has been
generated correctly with good randomness.
This rather seems applicable to the type of schemes where one would use
nothing-up-my-sleeve numbers, or "trustless setup", and not something as
open and ambiguous as knowledge of branches. Unfortunately, that latter
class of problem might be impossible to solve.

@_date: 2019-01-21 18:37:57
@_author: Natanael 
@_subject: [Cryptography] Implementing full Internet IPv6 end-to-end 
Den mn 21 jan. 2019 18:31 skrev Ttttabcd via cryptography <
cryptography at metzdowd.com>:
I would suggest CJDNS instead. It's a different implementation of the same
idea. It uses the private network prefix, and thus sacrifice much fewer
bits of the 128 bits available.

@_date: 2019-06-03 23:32:22
@_author: Natanael 
@_subject: [Cryptography] About Secret Sharing Schemes and a Question 
Den mn 3 juni 2019 17:38Osman Kuzucu  skrev:
What you're looking for is the general class of threshold cryptography, of
which Shamir's secret sharing scheme is just one variant.
For example, consider threshold signatures and threshold encryption.
In both of those cases you have a number of asymmetric keypairs that are
combined in an operation without revealing your private key to any other
party. Depending on the specific implementation, such a scheme might work
by deriving a combined group public key (from the public keys of the
participants) to be used for verification of signatures or encryption of
data, while a special protocol is used by the private key holders to work
together to sign or decrypt data.
Does that sound like what you're looking for?

@_date: 2019-06-04 13:21:07
@_author: Natanael 
@_subject: [Cryptography] [FORGED] Crptographic ticket tape (fake news) 
Den tis 4 juni 2019 08:17Peter Gutmann  skrev:
To build on this, the much more productive approach is to apply
cryptographic signatures to the full image frames in the video stream.
Any timestamps can then be added on top of that via a variety of digital
timestamping services (the above mentioned logging systems usually work in
a similar way to these). But keep in mind that they only prove "this data
existed at this date", nothing else.
(You can also timestamp first, and sign the video with verified timestamp
To prevent a reconstruction of the video which lacks the digital signatures
is nearly impossible. Digital watermarking schemes meant to prevent edits
tends to get broken. Even a perfect embedding scheme can be defeated by
something as simple as recording a reenactment (or maybe even using machine
learning to "repaint" the frames automatically?).
But if we just assume we want unmanipulated videos to be easy to verify,
and we assume we can't just put a signature in file metadata:
You could do something like hashing the top bits of each pixel value and
embedding the signature of them in the bottom bits. Or use a fuzzy hash
function over the frame and embed the signature with water marking
techniques. Or just make every 30th frame a data frame holding a signature
of the collection of hashes for the previous visual frames. Some fuzzy
hashing mechanisms can also handle cropping and rescaling (resizing) of the
video and still allow verification of that the video is visually the same.
There's an endless number of ways to embed cryptographic data in this way
to detect alterations that substantially changes the frame contents.
But all of these methods requires trust in the key holder / signer. Because
pixels are just too easy to edit.

@_date: 2019-06-20 11:57:29
@_author: Natanael 
@_subject: [Cryptography] =?utf-8?q?Shamir=E2=80=99s_secret_sharing?= 
Den tors 20 juni 2019 02:50Adrian McCullagh Tldr it's secure even against quantum computers (if used with a secure
Both classical and (general) quantum computers are approximately Turing
complete*, and any and every Turing complete computer can perfectly
simulate every other. Which means that everything computable by one Turing
machine is computable by them all (given enough time). And to the best of
our knowledge, everything which is computable can be computed by Turing
complete computers, we don't know of any solvable problem which they can't
* the full definition assumes infinite memory, which we don't have. Thus,
the machines we have are only approximately Turing complete.
What differentiates quantum computers from classic ones are which classes
of problems they solve how fast, but not their mathematical capabilities.
Every Turing complete computer can solve RSA of any size *with enough
resources*, but given the *available* resources our classical computers
gets stuck at around 2 kilobit RSA keys while quantum computers with our
available resources can in theory break RSA all the way to up to sizes of
multiple million bits or more (after that you hit the limits described in
the pqRSA papers). We also know exactly how to bruteforce AES256, but
neither classical nor quantum computers can actually do it when limited by
the resources we have available.
To the point;
Shamir's secret sharing scheme, when used with a secure unpredictable RNG,
is information theoretically secure. As such, it can't be broken by *any*
computer which follows known physical assumptions. When you don't have
enough shares to reach the threshold, there simply doesn't exist *any*
information that allows you to guess the secret better than random.
Just make sure your RNG itself is actually secure! A predictable RNG can
reveal correlations between the shares that was created and thus reveal the
So if used right, breaking Shamir's secret sharing scheme would require
some computer with access to a yet unknown mathematical "oracle", a
capability that exceeds the capability of Turing complete machines, and we
have no reason to believe that's possible. Quantum computers certainly
can't, not with what we know about them.

@_date: 2019-05-03 11:01:06
@_author: Natanael 
@_subject: [Cryptography] OS and encryption for quantum computing machines 
Den fre 3 maj 2019 03:42Tom Mitchell  skrev:
(First of all, disclaimer: I'm not an expert on this. The information below
is a summary of what I've read about quantum computers so for.)
The typical quantum computer won't run an OS in the quantum components.
They'll just repeat the same algorithm over and over.
Every construction proposed so far uses classical computers, qubit hardware
(superconductors, suspended electrons, etc), something that link together
the qubits, and control hardware. From an algorithmic perspective, the
quantum setup is treated as a blackbox function. They don't really have an
equivalent for continous threads/processes (yet, if ever). The internal
state loses the quantum properties when you read it out.
The classical computer programs the qubits via the control hardware, then
performs a readout, checks the results, starts over if the result isn't
I have not yet seen a discussion of an OS that is crypto strong to deploy
I can note that in the case of actual networked quantum computers and
quantum information networks (communicating by entangled photons, etc),
there are indeed classical algorithms otherwise assumed secure that fail,
because the act of linking two quantum computers expose more private
information than classical communications would expose.
Quantum key recovery on AEZ: Is it sufficient to toss any older encryption method that is not quantum
As seen above, if you have quantum communications, then no.
Otherwise, if you're only communicating the results of symmetric algorithms
and other quantum resistant algorithms with classical communications, then

@_date: 2019-05-14 18:51:38
@_author: Natanael 
@_subject: [Cryptography] Dieharder & /dev/urandom 
Den tis 14 maj 2019 18:37Michel Arboi  skrev:
I have admit I don't know exactly how the dieharder tests are implemented,
I'd like to point one thing out;
All these tests are heuristic by design. Randomness does not lie in
numbers, it lies in the sources. And heuristic tests for randomness needs
to rely on randomness themselves. That means sometimes when it runs a
statistical test with random numbers it will then claim they aren't random,
as well as claiming the predictable numbers are random, because it just
tries to guess how *probable* it is that a random source would come up with
the series of numbers that you gave to it.
So passing or failing some tests is not the interesting question - it is
*how many* tests you pass or fail and of what kind.

@_date: 2019-05-23 02:09:10
@_author: Natanael 
@_subject: [Cryptography] Craig Steven Wright's Bitcoin Copyright 5-20-2019 
Den tors 23 maj 2019 01:41John Young  skrev:
registration, the claimant certifies as to the truth of the statements made
in the submitted materials. The Copyright Office does not investigate the
truth of any statement made.
courts, including disputes over authorship of a work.
Also, it seems like multiple other people have filed similar registrations
now, and somebody even did it before Wright himself.

@_date: 2019-09-30 08:32:02
@_author: Natanael 
@_subject: [Cryptography] "Strong" passwords too clever by half... 
Den m?n 30 sep. 2019 07:53Udhay Shankar N  skrev:
There's also the option of password auto-typing keyboards, for example the
password manager KeePass2Android includes one. This one can be used by
opening a password entry and then selecting its soft keyboard in Android
for text input, then pressing its buttons to enter username and password.
Safer option if you're worried about any apps snooping on the clipboard.

@_date: 2020-08-01 21:15:40
@_author: Natanael 
@_subject: [Cryptography] Cryptographically securing a two-phase commit 
Den tors 30 juli 2020 08:35Peter Gutmann  skrev:
Tahoe-LAFS uses signed Merkle trees over the ciphertext. This allows
continuous partial validation and arbitrary reads while being able to
verify full file version consistency (relevant for things like volume
encryption, where an evil maid attack might selectively replace part of a
ciphertext with an older version), but it does add a bit overhead for
memory/storage/CPU versus something simpler.
Trying to stream (encrypt and authenticate as the data is generated) this
would require sending intermediate nodes of the Merkle tree along with the
ciphertext packets as you're building this tree over the ciphertext, using
additional memory overhead and network overhead, but it doesn't add much
CPU overhead.
Protocols like TLS does something related and simpler than Merkle trees
with the standard steam cipher + polynomial hash tags, like
ChaCha20+poly1305 or AES-GCM. Since the tags are applied per packet in for
example TLS this also allows partial reads, but IMHO isn't ideal for
rewritable at-rest encryption (to be robust it requires full overwrite per
section on each write, and nonce tracking). This may also be vulnerable to
the evil maid attack mentioned above, with partial section reversals. It
uses less CPU and memory than Merkle trees, but uses more storage/network
overhead (many tags instead of one).

@_date: 2020-02-16 20:26:50
@_author: Natanael 
@_subject: [Cryptography] Extracting TOTP credentials 
Den l?r 15 feb. 2020 21:03Ron Garret  skrev:
Found some info here, don't forget to check the comments;

@_date: 2020-05-03 10:12:00
@_author: Natanael 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
Den s?n 3 maj 2020 07:39Whitfield Diffie  skrev:
We could go for a compromise of "end-user to end-user encryption" and keep
the good old acronym while still making it clearer. (Assuming we can
convince people to update their definitions of the term.)

@_date: 2020-05-03 10:25:25
@_author: Natanael 
@_subject: [Cryptography] The EFF 650 CAs lie 
Den s?n 3 maj 2020 07:40  skrev:
A handful of instances of compromised certificates and erroneously issued
certificates being used maliciously in the wild has been discovered.
I think a major issue that makes detection hard is that we don't have
reliable means of tracking worldwide certificate use. Who exactly will
realize that the valid certificate for site X is being used by a server on
the wrong IP? What browser will be logging this and phoning back home about
what servers it saw using which certificates?
As for mistakenly issued certificates, a major part of the issue is that
the issuer often don't log them properly, so we don't see that much either.
These things are most often detected when somebody performs a broad scan
for certificates in use (rarely) or when somebody spots strange behavior in
their networks and tracks down the source of the traffic.
So we don't really know how widespread MITM with bad/stolen certs really

@_date: 2020-11-18 14:32:13
@_author: Natanael 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Den ons 18 nov. 2020 01:15jrzx via cryptography Making the proof of work a required part of the network protocol is likely
to cause trouble for low energy devices.
There's a few options, in particular "Privacy Pass" fits in here (designed
for the same purpose, also supported by Cloudflare). It's basically an
anonymized token by some issuer that will allow you to avoid captchas and
other bot & DDoS protection mechanisms on sites which trust the issuer.
Your client device would first need to get issued a privacy pass ticket
(this is where you may want to integrate proof of work, or solving
captchas, or authenticating by other means to "earn" a ticket). Low energy
devices (like your smartwatch) could have a ticket relayed to it by another
of your devices in case the service it connects to requires proof of work
or similar.

@_date: 2020-11-23 12:58:31
@_author: Natanael 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Den m?n 23 nov. 2020 07:37  skrev:
While the consequences under both extremes of very low and very high server
load are reasonable, you're not accounting for the very common case of
servers by design running very near max capacity during peak load. The
margin between "all ok" and "engage full DDoS defenses" can be small.
So the question is then how do you scale PoW difficulty in such noisy and
low margin environments? Will your smartwatch need to run hot for a minute
a few times a day, every day?

@_date: 2020-10-30 21:32:59
@_author: Natanael 
@_subject: [Cryptography] A simple RNG reseeding tweak to prevent malicious 
There's some literature on various CSPRNG designs and malicious sources,
covering the different possible capabilities they may have.
One of these attack models involves a hypothetical malicious CSPRNG source
for seeding / reseeding the entropy pool, where the source would have total
visibility into the system but would not tamper with it in any other way
but a single one, in that it would feed the CSPRNG with maliciously
generated "degenerate" seed values.
This would be done in an attempt to attack the mixing function in the RNG,
attempting to induce bias which could break things like ECDSA signing via a
predictable k value. Hypothetically, a modified Intel RDRAND chip or
similar could perform an attack like this while attempting to stay covert
and not take actions that might lead to visibly altered state in computer.
To generate this bias in the output, the mixing function would be attacked
by this source by iterating through candidate inputs and testing them to
find one which produces an output from the CSPRNG with the desired bias
(when combined with the other sources' inputs, which here is known).
(Depending on the mixing function, the limits to bias introduction can have
a computational limit similar to proof-of-work functions, but the adversary
might still be able to achieve their goal under this constraint.)
In a distributed network where each source is an independent computer you
can easily perform distributed entropy generation by using commitments of
the inputs while they're still secret, but you can't use that against a
malicious chip inside your own computer as it would already know all inputs.
However, we can introduce another recent development, by using a VDF -
verifiable delay functions, a type of function similar to timelock puzzles
where the output takes a certain minimum amount of time to solve for.
So instead of using classical hash function commitments, the input value to
feed into the CSPRNG becomes the output of the VDF:s derived from each
source's generated seed (note, using a type of VDF where even the generator
don't know what the output will be in advance).
We can then make the CSPRNG sample inputs from each source on a schedule
(using short contribution time windows) and creating a VDF based on the
inputs, and then using the final output of the VDF to reseed the CSPRNG
Let's say the sources collects entropy for 10 minutes and contributions to
the system CSPRNG are then immediately afterwards open for 1 minute, then
you can generate a 20 minute VDF to then start the solving process. This
way the malicious source can not have determined what the final output from
any other source will have been at the point in time when it is forced to
submit its own contribution.
Thus it is not possible for a malicious RNG source which refrains from
otherwise tampering with the system to introduce bias in the final output
of the CSPRNG.
Any feedback?
In particular, what would this look like when integrated into an existing
CSPRNG design such as Fortuna? Or any suggestions for simplification?

@_date: 2020-09-12 16:24:21
@_author: Natanael 
@_subject: [Cryptography] algorithm for offline spend? 
Den l?r 12 sep. 2020 07:46  skrev:
David Chaum built this. Think it was called digital gold, or something. The
main technique used for signing these transactions is called chaumian
blinding. There's variants of this signing algorithm that don't reveal the
key when signing double, but the variant used here was specifically
designed to do so as a disincentive against acting maliciously.
Keep in mind that designs that rely on incentives where you do not have a
clear estimate of the true cost & reward of each option are very likely to
fail because people will always end up behaving in unpredictable ways, and
some of these ways might break your assumptions so bad that the whole
system fails.
For example, it's not guaranteed that the private key disclosure will be
discovered before the adversary has managed to run off with the money.

@_date: 2020-09-29 10:45:47
@_author: Natanael 
@_subject: [Cryptography] Is this a solved problem? 
Den tis 29 sep. 2020 05:29Henry Baker  skrev:
Don't make it an auto-login link, instead make it equivalent to a 2FA token
that requires the user to *already* have an active user session (account
session cookies) on the same browser where the link is opened.
If they're not logged in then the link may suggest that they log in with
that associated account, but it would otherwise be an unauthenticated
You can even put restrictions on the actions that can be done under such an
"unprompted" 2FA-like token, such as being able to open your wishlist, etc,
but not to complete a new order without additional 2FA.
