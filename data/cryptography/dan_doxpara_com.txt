
@_date: 2004-12-06 16:14:49
@_author: Dan Kaminsky 
@_subject: MD5 To Be Considered Harmful Someday 
I've been doing some analysis on MD5 collision announced by Wang et al.  Short version:  Yes, Virginia, there is no such thing as a safe hash collision -- at least in a function that's specified to be cryptographically secure.  The full details may be acquired at the following link:
A tool, Stripwire, has been assembled to demonstrate some of the attacks described in the paper.  It may be acquired at the following address:
 Incidentally, the expectations management is by no means accidental -- the paper's titled "MD5 To Be Considered Harmful Someday" for a reason.  Some people have said there's no applied implications to Joux and Wang's research.  They're wrong; arbitrary payloads can be successfully integrated into a hash collision.  But the attacks are not wildly practical, and in most cases exposure remains thankfully limited, for now.  But the risks are real enough that responsible engineers should take note:  This is not merely an academic threat, systems designed with MD5 now need to take far more care than they would if they were employing an unbroken hashing algorithm, and the problems are only going to get worse.
Some highlights from the paper:
* The attack itself is pretty limited -- essentially, we can create "doppelganger" blocks (my term) anywhere inside a file that may be swapped out, one for another, without altering the final MD5 hash.  This lets us create any number of binary-inequal files with the same md5sum.
* MD5 uses an appendable cascade construction -- in other words, if you happen to find yourself with two files that MD5 to the same hash, an arbitrary payload can be applied to both files and they'll still have the same hash.  This leads to...
* Attacks are possible using only the proof of concept test vectors released by Wang -- the actual attack is not necessary.
* Stripwire emits two binary packages.  They both contain an arbitrary payload, but the payload is encrypted with AES.  Only one of the packages ("Fire") is decryptable and thus dangerous; the other ("Ice") shields its data behind AES.  Both files share the same MD5 hash.
* Digital Signature systems are vulnerable, as they almost always sign a hashed representation of data rather than the data itself.
* This is an excellent vector for malicious developers to get unsafe code past a group of auditors, perhaps to acquire a required third party signature.  Alternatively, build tools themselves could be compromised to embed safe versions of dangerous payloads in each build.  At some later point, the embedded payload could be safely "activated", without the MD5 changing.  This has implications for Tripwire, DRM, and several package management architectures.
* HMAC's invulnerability has been slightly overstated.  It's definitely possible, given the key, to create two datasets with the same HMAC.  Attacker possession of the key violates MAC presumptions, so the impact of this is particularly questionable.
* Very interesting possibilities open up once the full attack is made available -- among other things, we can create self-decrypting executables (fire.exe and ice.exe) that exhibit differential behavior based on their internal colliding payloads.  They'll still have the same MD5 hash.
* Several doppelgangers may (relatively quickly, as per Joux) be computed within a single multicollision-friendly block.  As such, the particular selection of doppelganger sets within a file can itself be made to represent data.  It's relatively straightforward to embed a 128 bit signature inside an arbitrary file, in such a way that no matter the value of the signature, a constant MD5 hash is maintained.  This is curiously steganographic.
* Many popular P2P networks (and innumerable distributed content databases) use MD5 hashes as both a reliable search handle and a mechanism to ensure file integrity.  This makes them blind to any signature embedded within MD5 collisions.  We can use this blindness to track MP3 audio data as it propagates from a custom P2P node.  "Strikeback" capacity against executable trafficking is even more pronounced -- it's possible to create application installers that self-modify with host identifying characteristics but still successfully retransmit on P2P networks under the global search hash.
I hope this paper proves useful to the security community at large, and I welcome feedback.
--Dan Kaminsky
dan at doxpara.com

@_date: 2005-08-31 08:56:40
@_author: Dan Kaminsky 
@_subject: MD5 Collision, Visualised 
Thpppppt ;)
    (That being said -- I do like your output.  Very nice.)

@_date: 2005-02-02 15:45:56
@_author: Dan Kaminsky 
@_subject: Dell to Add Security Chip to PCs 
Uh, you *really* have no idea how much the black hat community is looking forward to TCPA.  For example, Office is going to have core components running inside a protected environment totally immune to antivirus.  Since these components are going to be managing cryptographic operations, the "well defined API" exposed from within the sandbox will have arbitrary content going in, and opaque content coming out.  Malware goes in (there's not a executable environment created that can't be exploited), sets up shop, has no need to be stealthy due to the complete blockage of AV monitors and cleaners, and does what it wants to the plaintext and ciphertext (alters content, changes keys) before emitting it back out the opaque outbound interface.
So, no FUD, you lose :)

@_date: 2005-02-04 02:02:43
@_author: Dan Kaminsky 
@_subject: Dell to Add Security Chip to PCs 
TCPA eliminates external checks and balances, such as antivirus.  As the user, I'm not trusted to audit operations within a TCPA-established sandbox.  Antivirus is essentially a user system auditing tool, and TCPA-based systems have these big black boxes AV isn't allowed to analyze.
Imagine a sandbox that parses input code signed to an API-derivable public key.  Imagine an exploit encrypted to that.  Can AV decrypt the payload and prevent execution?  No, of course not.  Only the TCPA sandbox can.  But since AV can't get inside of the TCPA sandbox, whatever content is "protected" in there is quite conspicuously unprotected.
It's a little like having a serial killer in San Quentin.  You feel really safe until you realize...uh, he's your cellmate.
I don't know how clear I can say this, your threat model is broken, and the bad guys can't stop laughing about it.
I do a fair number of conferences with exploit authors every few months, and I can tell you, much worse.  "Licking chops" is an accurate assessment.
Honestly, it's a little like HID's "radio barcode number" concept of RFID.  Everyone expects it to get everywhere, then get exploited mercilessly, then get ripped off the market quite painfully.

@_date: 2005-02-04 13:20:50
@_author: Dan Kaminsky 
@_subject: Dell to Add Security Chip to PCs 
No, it can't.  That's the point; it's not like the code running inside the sandbox becomes magically exploitproof...it just becomes totally opaque to any external auditor.  A black hat takes an exploit, encrypts it to the public key exported by the TCPA-compliant environment (think about a worm that encrypts itself to each cached public key) and sends the newly unauditable structure out.  Sure, the worm can only manipulate data inside the sandbox, but when the whole *idea* is to put everything valuable inside these safe sandboxes, that's not exactly comforting.

@_date: 2005-02-07 10:38:33
@_author: Dan Kaminsky 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
SRTP...heh.  Take a look at RFC3711 for a second.
   Specification of a key management protocol for SRTP is out of scope
   here.  Section 8.2, however, provides guidance on the parameters that
   need to be defined for the default and mandatory transforms.
VOIP KEX.  *shudders*  Voice is...unique.  Session redirection is a first class function, as is active proxying, up to and including proxies that are payload-destructive (conference stream mixing).  KEX in such an environment is a really painful problem, compared to the relatively solvable one of specifying a loss-tolerant encryption protocol.  So, they only solved the latter, and figured something would come along for the former.
Didn't really happen.
(Full Disclosure:  I work for Avaya, whose had a proprietary KEX implementation that handles all of this for the last few years.  So it's not an unsolvable problem or anything like that.  It's just really annoyingly hard.)

@_date: 2005-02-10 13:42:36
@_author: Dan Kaminsky 
@_subject: Desire safety on Net? (n) code has the solution 
This passed by without too many people noticing:
The SEC also asserts that the company's 10-Q bore an unauthorized electronic signature of Guccione -- who was Penthouse's principal executive officer and principal financial officer at the time. The signature indicated that Guccione had reviewed and signed the filing and the accompanying Sarbanes-Oxley certification. ?This representation was false,? the SEC stated in its complaint.
"You got your SOX in my Digital Signature Repudiation!"
"You got your Digital Signature Repudiation in my SOX!"
"Someone order a failed porn empire?"

@_date: 2005-02-16 14:16:36
@_author: Dan Kaminsky 
@_subject: SHA-1 cracked 
It is worth emphasizing that, as a 2^69 attack, we're not going to be
getting test vectors out of Wang.  After all, if she had 2^69
computation available, she wouldn't have needed to attack MD5; she could
have just brute forced it in 2^64.
This means the various attacks in the MD5 Someday paper aren't going to
cross over to SHA-1, i.e. don't expect these anytime soon for SHA-1.

@_date: 2005-02-17 11:16:32
@_author: Dan Kaminsky 
@_subject: SHA-1 cracked 
Depends if you care about HMAC collisions being computationally
infeasible or not.  The attack against MD5 adapts to arbitrary initial
states, and you can basically consider HMAC a complex mechanism for
introducing a password into the initial state.  So, as an attacker, I
can indeed create two payloads with the same HMAC-MD5 hash, presuming I
know the password.  But, as several people pointed out, this is a little
like saying AES is insecure if the attacker learns the key.  The
primitive itself specifies that this must remain secret; behavior when
it doesn't isn't specified. Presumably, the attack against SHA-1 has similar output to the attack
from MD5 (though we can't be sure -- specifically, the padding was
totally orthogonal to the crypto break for MD5, so it's odd that some
people are saying it's making a difference for SHA-1).  So, I don't
expect things to be any different.

@_date: 2005-02-17 11:30:06
@_author: Dan Kaminsky 
@_subject: Digital Water Marks Thieves 
Why would it matter?
We leave fingerprints on everything we touch, but generally only LEO's
have access to the fingerprint DB's that can route back to identity.
I don't really understand the complaints here.  Is there something wrong
with luggage tags?  How 'bout writing your name in the corner of a
textbook?  Degenericizing property is sort of an inherent part of owning
it; note for instance that few homes are purchased fully furnished. Really the only concern I have is the effects on the Right of First
Sale, i.e. the ability as a purchaser to sell what you bought to someone
else.  Since "I bought it second hand" and "I stole it and sprayed my
own tagents on it" are similar across so many dimensions, I can imagine
EBay UK eventually having to deal with this head on.

@_date: 2005-02-22 08:51:00
@_author: Dan Kaminsky 
@_subject: SHA-1 cracked 
As a couple people saw fit to remind me, arbitrary appending only works
if your two vectors are multiples of the blocksize.  Otherwise, the
padding gets shuffled into the colliding rounds.
If she's specifically saying padding is a problem, then her attack on
SHA-1 cannot adapt to arbitrary input sizes as well as her attack
against MD5.  IOW, she might only be able to create a collision between
two 689 bit files at present time.
The "word on the street" from Wang herself concurs with your
assessment...she said privately to a couple people that SHA-256/512 were
"seemingly outside of her reach".  Of course, the same people reported
she said "SHA-1 looked like it could be interesting."
It's worth pointing out that we won't know until Eurocrypt how --
precisely -- Wang's attack works.  Until then, it's premature to say
what Wang can and cannot defeat.

@_date: 2005-02-22 08:57:50
@_author: Dan Kaminsky 
@_subject: Digital Water Marks Thieves 
There are tens of thousands of places inside a vehicle that a VIN# can
be stashed.  Sometimes you don't always want the attacker to know where
the marks are.
The point is that the thief should think anything expensive is
protected, by which I mean it's too traceable to fence.  At least right
now, this is working.  Hard to argue with success.

@_date: 2005-07-03 12:19:02
@_author: Dan Kaminsky 
@_subject: /dev/random is probably not 
So the funny thing about, say, SHA-1, is if you give it less than 160
bits of data, you end up expanding into 160 bits of data, but if you
give it more than 160 bits of data, you end up contracting into 160 bits
of data.  This works of course for any input data, entropic or not. Hash saturation?  Is not every modern hash saturated with as much
entropy it can assume came from the input data (i.e. all input bits have
a 50% likelihood of changing all output bits)?
Incidentally, that's a more than mild assumptoin that it's pure noise
coming off the sound card.  It's not, necessarily, not even at the high
frequencies.  Consider for a moment the Sound Blaster Live's E10K chip,
internally hard-clocked to 48khz.  This chip uses a fairly simple
algorithm to upsample or downsample all audio streams to 48,000 samples
per second.  It's well known that scaling algorithms exhibit noticable
properties -- this fact has been used to detect photoshopped works, for
instance.  Take a look how noise centered around 15khz gets represented
in a 48khz averaged domain.  Would your system detect this fault?
Of course not.  No extant system can yet detect the difference between a
quantum entropy generator and an AES or 3DES stream.  (RC4's another
story.)  You can't externally calculate entropy levels; you can only assume.

@_date: 2005-07-08 10:08:18
@_author: Dan Kaminsky 
@_subject: Why Blockbuster looks at your ID. 
Credit card fraud has gone *down* since 1992, and is actually falling:
1992:  $2.6B
2003:  $882M
2004:  $788M
We're on the order of 4.7 cents on the $100.
If it's any consolation, I was rather surprised myself.

@_date: 2005-07-08 14:48:19
@_author: Dan Kaminsky 
@_subject: Why Blockbuster looks at your ID. 
I did the math.  15.7 / 4.7 ~= 3.34.  3.34 * $778M = $2.6B.  There's a problem here, but I'll get to it in a sec.
Hmm...lets verify the rest of this:
4.7 cents per 100 is 0.047 dollars per 100 dollars is 0.00047 dollars per dollar.
x * 0.00047 = $778M
x = $778M / 0.00047
x = 1655319M = 1.65T
Looking at Federal Reserve data (  ), there was about $2T in overall consumer credit.  I can envision the vast majority, but not all of this being on plastic.  So, $1.65T works.
If you try to repeat this for 1992, though, you'll find an interesting bug...total transactions in 1992 were also about 1.65T.  Gee, it's almost like I assumed credit card usage rates were constant over the 12 year period...oops :)  But then there's inflation, which alters dollar figures substantially.  So oops in the other direction.
The fundamental point stands, though...credit fraud has been managed surprisingly well (though some people have said fraud is understated by

@_date: 2005-07-13 10:36:11
@_author: Dan Kaminsky 
@_subject: mother's maiden names... 
Bank Of America put my photo on my ATM card back in '97.  They're shipping me a new one right now, so I assume they kept it in the DB.

@_date: 2005-07-13 15:31:17
@_author: Dan Kaminsky 
@_subject: ID "theft" -- so what? 
It's 2005, PKI doesn't work, the horse is dead.  The credit-card sized number dispensers under development are likely to be what comes next.
Amusingly, your face is an asymmetric authenticator -- easy to recognize, hard to spoof.

@_date: 2005-06-07 10:24:01
@_author: Dan Kaminsky 
@_subject: [Clips] Citigroup Says Data Lost On 3.9 Million Customers 
The inability to procure hardware or understanding in the age of eBay
and Google is simply not a credible defense.  Encrypt in transit or face
the consequences.
Free advertising for your credit monitoring service does not qualify.

@_date: 2005-06-08 12:26:37
@_author: Dan Kaminsky 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
Yes, because key management is easy or free.
Also, reliability of encrypted backups is problematic:  CBC modes render
a single fault destructive to the entire dataset.  Counter mode is
sufficiently new that it's not supported by existing code.

@_date: 2005-06-24 11:03:20
@_author: Dan Kaminsky 
@_subject: WYTM - "but what if it was true?" 
I had something much more complicated, but it comes down to.
    You trust Internet Explorer.
    Spyware considers Internet Explorer crunchy, and good with ketchup.
    Any questions?
    A little less snarkily, Spyware can trivially use what MS refers to
as a Browser Helper Object (BHO) to alter all traffic on any web page. Inserting a 1x1 iframe in the corner of whatever, that does nothing but
transmit upstream data via HTTP image GETs, is trivial.  And if HTTP is
a bit too protected -- there's *always* DNS ;).  gethostbyname indeed.
P.S.  Imagine for a moment it was profitable to give people cancer.  No,
not just a pesky side effect, but kind of the idea.  Angiostatin
wouldn't stand a chance.

@_date: 2005-06-24 12:25:07
@_author: Dan Kaminsky 
@_subject: Optimisation Considered Harmful 
The problem is with edges:
Suppose the timer goes off every 10ms.
You have an operation that takes either 5ms or 15ms, depending on
whether a chosen bit of the key is 1 or 0.
Whether or not a given time slot is occupied with results will emit
whether the bit was 1 or 0.
Now, suppose your timer goes off every 200ms.  No problem, right?
At time=190ms, you force an encryption.  If it's done by the time=200ms
deadline, you know.
Things get trickier when there's random noise in the timer, and it
matters whether the distribution of 1's and 0's is equal or not.  But
this is fundamentally a difficult problem to handle.

@_date: 2005-06-26 21:28:08
@_author: Dan Kaminsky 
@_subject: WYTM - "but what if it was true?" 
A reasonable critique.
It is not necessary though that there exists an acceptable solution that
keeps PC's with persistent stores secure.  A bootable CD from a bank is
an unexpectedly compelling option, as are the sort of services we're
going to see coming out of all those new net-connected gaming systems
coming out soon.

@_date: 2005-06-27 23:12:42
@_author: Dan Kaminsky 
@_subject: Cracking Biometric Hashes 
Ah!  I was looking for this info, and finally found it in something I
posted in an old gadget blog.  Short version, biometric hashes are
reversable, since the algorithms provide confidence levels and you can
always alter towards higher confidence.
It is repeated that hashes generated by biometric systems cannot be
reversed back into the biological component to be recognized in the
future. This claim is false. Several researchers have noted that
biometric algorithms, implementing fuzzy matches against fundamentally
noisy data, must inherently make their decisions with some level of
confidence. While the level of confidence is usually exported to
administrators to determine how precise the system has to be to allow or
reject a given candidate, it can also be used by an attacker to discover
whether a given small change in a sample biometric element (say, making
a person's lips wider) makes that person look more or less like the
hashed target. This approach is generally devastating, and has been used
to great effect to attack fingerprint readers
( and face recognizers
Irises are not likely to be an exception.

@_date: 2005-03-02 08:01:27
@_author: Dan Kaminsky 
@_subject: MD5 collision in X509 certificates 
Semantic gap, and I do apologize if I didn't make this clear.  Wang
adapts to any initial state, so you can create arbitrary content to
prepend your collision set with, adapt to its output, and then append
whatever you like.  The temporal ordering is indeed important though;
you can't create the doppelganger set before you know what's prepended
to it.
    The fact that we can have arbitrary content adapted to allows for a
critical expansion of the applied risks (i.e. we wouldn't be seeing
colliding certs w/o it).  I don't think it's fair to say my attacks --
in some vague, general sense -- are "wrong", given what was at best a
small difference in interpretation.
    The x.509 cert collision is a necessary consequence of the earlier
discussed prime/not-prime collision.  Take the previous concept, make
both prime, and surround with the frame of an x.509 cert, and you get
the new paper.  Still nice to see...Rescorla specifically thought it
wasn't possible.  I look forward to actually having the code to work on
this myself.

@_date: 2005-03-02 09:05:04
@_author: Dan Kaminsky 
@_subject: MD5 collision in X509 certificates 
*laughs* Yes, I suppose it would be difficult for pq to be prime now
wouldn't it :)
So they've basically solved:
md5(pq) == md5(p'q')
For integer values of p, q, p' and q'.  You are right, this is much more

@_date: 2005-03-03 16:43:21
@_author: Dan Kaminsky 
@_subject: I'll show you mine if you show me, er, mine 
There is no actual description publically available (there are three
completely different protocols described in the press).  I talked to the
author about this; he sent me a fourth, somewhat reasonable document. At *best*, this is something akin to SRP with the server constantly
proving its true nature with every character (yes, shoulder surfers get
to attack keys one at a time).  It could get pretty bad though, so
rather than support it or bash it, I'd just reserve judgement until it's
publically documented at Financial Crypto.

@_date: 2005-03-07 09:23:14
@_author: Dan Kaminsky 
@_subject: comments wanted on gbde 
Re, GDBE--
    Some initial thoughts:
    I wouldn't be surprised if platters couldn't be analyzed for usage
levels / magnetic degradation (Peter?).  Even without a clean room, ATA
is pretty rich -- anyone remember the guy who graphically plotted the
spiral damage caused by a falled drive head w/ nothing but a massively
hacked ATA driver?  There's also likely to be useful information from
drive sectors duplicated by the drive firmware (there's extra space in
every drive; when particular sectors are judged "buggy" content from
them is migrated onto the spare space).
    I saw nothing establishing the integrity of sectors during
*decryption* in 7.5.  Random / polluted sectors will decrypt, though
into unpredictable noise (which tends to do bad things to file system
code).  Previous versions of sectors will also decrypt successfully --
the cleaning lady can take lessons from Mallory, as it were.  It's
useful to immediately grant though that their threat model is much more
aligned towards drives that will never be hot again.
    One wonders if there is a delivery service for Key-key's.

@_date: 2005-03-21 11:44:06
@_author: Dan Kaminsky 
@_subject: Propping up SHA-1 (or MD5) 
x can equal either test vector released by Wang, and H(x) will be
identical.  With H(x) identical, the rest of the HMAC stays identical too.     As a couple people pointed out, it's OK that HMAC is "vulnerable" to
the Wang attack, since in order to execute the attack the key is
required (and like AES, if the key is compromised, all bets are off). But you're not using HMAC as a MAC; you're using it to prop up a broken
hash.     Regarding the "Random" appendage, people don't seem to realize how
important syncronized initial states are to many hashing algorithms. One of the major uses of a hashing algorithm is to act as an
*exchangable* handle to data, in other words, you and I can both hash
our respective datasets and see if they're identical.  If we're each
using different initial states, that process fails utterly.
P.S.  Your construction *will* work if you replace H(x) with H(x xor
ipad) and H(x xor opad), with ipad and opad as defined in the HMAC
spec.  (We can collide against either permutation of our block data, but
not both simultaneously.)  This does end up rather significantly
reducing throughput though.

@_date: 2005-03-21 11:46:25
@_author: Dan Kaminsky 
@_subject: how to phase in new hash algorithms? 
I've been rather continually pinging people, asking them for an
explanation as to the design decisions of Whirlpool (namely -- it's
similar but noticably not identical to AES/Rijndael, and isn't just a
straightforward expansion of the block size up to 512 bits).  I'm not
saying anything bad about Whirlpool, but I get alot of people
approaching me about the hash and I don't really know what to tell them.

@_date: 2005-03-25 10:24:42
@_author: Dan Kaminsky 
@_subject: What is to be said about pre-image resistance? 
The Wang attack does nothing (yet) for second preimages.
    The best attack I know of against them refers is in Kelsey and
Schneier's "*Second Preimages on n-bit Hash Functions for Much Less than
2^n Work".*  It's at:      Once you cut through the verbiage, it's really pretty simple:  The
bigger a file, the more intermediate hashes there are inside of it.  The
more intermediate hashes, the more points there are to collide against. So, a 700MB CD image vs. a hash with a 512 bit blocksize will have
734,003,200 bytes / 64 bytes = 11,468,800 intermediate hashes that may
be collided against.  That's a little more than 2^23.  Against MD5,
you're looking at 2^105 for a CD; against SHA-1, you're looking at
2^137.  This is of course work that's far outside the range of
feasibility (and, in a small ahem to the paper, 2^60 byte messages are
equally ridiculous).
    You may say this isn't a true second preimage attack, because you
only acquire an intermediate collision.  But all intermediate collisions
can be appended to with legitimate data until they return to the correct
final hash state.  So you generate your malicious data, search for
random garbage that, when appended, equals one of the 11M potential
states, and then append the rest of the legitimate file from that point.
    MD Hardening, i.e. the conclusion of a data stream with its own
length, *does* create a problem though.  We cannot alter the final
length of the file, or the hash will fail.  So if we find an
intermediate collision at an earlier point in the file than our
malicious payload requires, we must discard it.  For large malicious
payloads (say, replacing one CD entirely with another), this eliminates
our attack window entirely.
    Of course, again, 2^105 work is ridiculous.

@_date: 2005-03-25 13:32:27
@_author: Dan Kaminsky 
@_subject: Secure Science issues preview of their upcoming block cipher 
Secure Science is basically publishing a cipher suite implemented by
Tom St. Denis, author of Libtomcrypt.  Though not the most ...
diplomatic of characters haunting sci.crypt, the guy's quite bright, is
an absurdly prolific author (has quite literally written several hundred
page books documenting use of Libtomcrypt and mechanisms for
multiprecision math), and can be expected to generate cool things in the
years to come.
    As for the manner of this cipher's publication...Tom actually did
release the paper some time ago.  See eprint @
 .  Lance has Tom on staff, and...well,
sort of blew the announce.  He understands rather well the error of his
ways, and is in all sorts of damage control.
    So, quick summary -- yes, that's a very cranky way to announce a
cipher, no, it's not a crank cipher.

@_date: 2005-05-25 13:49:08
@_author: Dan Kaminsky 
@_subject: How secure is the ATA encrypted disk? 
specs to see they're well aware a worm could brick a couple hundred
thousand hard drives.

@_date: 2008-08-08 10:43:53
@_author: Dan Kaminsky 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
Funnily enough I was just working on this -- and found that we'd end up adding a couple megabytes to every browser.   NONSTARTER.  I am curious about the feasibility of a large bloom filter that fails back to online checking though.  This has side effects but perhaps they can be made statistically very unlikely, without blowing out the size of a browser.
Updating the filter could then be something we do on a 24 hour autoupdate basis.  Doing either this, or doing revocation checking over DNS (seriously), is not necessarily a bad idea.  We need to do better than we've been.

@_date: 2008-02-01 01:07:19
@_author: Dan Kaminsky 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
People don't use it because the workload of getting signed up is vastly
beyond their skillset, and the user experience using the things is
pretty bad too.
There's always a few people using a technology.  It's certainly a
nonplayer out there.  Probably more servers out there authing with
Digest, honestly.
Who?  Seriously, that's pretty significant, I'd like to know who does this.
The word "public" in Public Key isn't exactly subtle.
Make it really easy to use some future version of SSL client certs, and
quietly add the property you seek.  Ease of use drives technology
adoption; making the tech actually work is astonishingly secondary.
Heh, you asked :)
To be clear, we'd *have* an issue, if any serious number of people used
SSL client certs. I think you have a point that if SSL client certs
become very popular going forward, then every website you go to will
quietly grab your identity through their ad banners. What solution could there be?  You're actually going to SSL to the
banner ad network, and you're going to give them your client cert.
People by in large do not use SSL client cert authentication.  This is
problematic, as there's some very nice cryptographic aspects of the system.
I used to code for SSH.  SSL is an entire top-to-bottom stack, replete
with a deep PKI infrastructure.  SSH?  Tunneling transport, barely even
The problem here isn't checksums.  SSH is notoriously buggy when packets
are dropped.  I think there are certain windows in which OpenSSH assumes
it will get a response.  If it doesn't, it just dies.  So, outages more
than a few hundred milliseconds have a small percentage chance of
causing the session to permanently stall.
"Corrupted MAC on input" -- this is a decent sign of corruption at the
app layer.
Did you really try this with OpenSSL?  I've had much better luck there.
Actually, 256*65536 = 1677216 :)  In actuality, you have both IP and TCP
checksums.  So you get 8 bits from link, 16 bits from IP, and 16 bits
from TCP.  A random corrupt packet has about 2^40 odds of getting through.
Of course, one real problem is that the checksum algorithms don't
exactly distribute noise randomly, and noise is not random.  Still,
corruption doesn't start being a problem until you get some pretty
serious amounts of transfer.  (Interestingly, I've been looking at IPsec
lately, not for encryption, but for better checksumming.)

@_date: 2008-02-15 01:47:45
@_author: Dan Kaminsky 
@_subject: Toshiba shows 2Mbps hardware RNG 
Peter, you've just hit on something that's genuinely confused me for
quite some time.  Combining hash functions has always seemed naive --
the problem with chaining two different functions is that it creates a
midpoint; you can collide half the bitspace independently of the other
half.  Better to just thoroughly mix them both.  But shouldn't it be an
improvement to XOR a theoretically correct RNG with a well seeded PRNG,
based on the theory that:
1) Either generator could be safely XOR'd against a repeated series of
0x41's, and the output would still be just as random
2) The flaws of a subtlety broken RNG would be difficult to exploit
through the noise of a sufficiently validated cryptographic function,
and vice versa
For example, the following construction:
Start with an RNG.  Retrieve 64K of "random data".  Assume there might
be a bias somewhere in there, but that at least 256 bits are good. SHA-256 the data.  AES-256 encrypt the data with the result from the
SHA-256.  XOR the random data against its encrypted self.  Return 64K of
PNRG-hardened RNG data.
Aside from the obvious rejoinder to maybe XOR *another* batch of entropy
against the previous batch's encrypted self (a change that halves
performance), I can't see much wrong.  I rather deeply doubt I'm the
first to come up with a suggestion like that either.  So, uh, why do
weak RNG's keep showing up?  Is there something fundamentally breakable
in the above design?

@_date: 2008-01-04 09:42:11
@_author: Dan Kaminsky 
@_subject: Death of antivirus software imminent 
Yawn.  IDS is dead, has been for a while now.  The bottom line discovery
has been that:
1) Anomaly detection doesn't work because anomalies are normal, and
2) Unless you're scrubbing up and down the application and network
stacks, you just have no idea what the host endpoint is parsing.
At the point where crypto shows up, it's already too late.

@_date: 2010-07-01 06:46:30
@_author: Dan Kaminsky 
@_subject: 1280-Bit RSA 
I've got a "perfect vs. good" question.
   NIST is pushing RSA-2048.  And I think we all agree that's probably a
good thing.
   However, performance on RSA-2048 is too low for a number of real world
   Assuming RSA-2048 is unavailable, is it worth taking the intermediate
step of using RSA-1280?  Or should we stick to RSA-1024?

@_date: 2010-07-10 00:19:44
@_author: Dan Kaminsky 
@_subject: [TIME_WARP] 1280-Bit RSA 
Here's the actual data, in terms of transactions per second, I'm getting for
a sample app:
512: 710.042382
1024: 187.187719
1280: 108.592265
1536: 73.314751
2048: 20.645645
2048 ain't happening.  The relative diff between 1280 and 1536 is
interesting though.
Do other cracking mechanisms have similar curves to GNFS (with different

@_date: 2013-10-04 16:09:32
@_author: Dan Kaminsky 
@_subject: [Cryptography] Sha3 
Because not being fast enough means you don't ship.  You don't ship, you
didn't secure anything.
Performance will in fact trump security.  This is the empirical reality.
 There's some budget for performance loss. But we have lots and lots of
slow functions. Fast is the game.
(Now, whether my theory that we stuck with MD5 over SHA1 because variable
field lengths are harder to parse in C -- that's an open question to say
the least.)

@_date: 2015-05-07 17:14:10
@_author: Dan Kaminsky 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
Practical HMAC-MD5 Collisions!
Not that they should ever matter...

@_date: 2015-05-10 18:12:44
@_author: Dan Kaminsky 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
I read through the papers -- they get close, but never quite reach "if you
have a collision in the hash function, you have a trivial collision in HMAC
itself".  Maybe you can help me find that?
I really tried to find some reasonable way to complain about this property
but it just isn't HMAC's job to provide collision resistance.
Curious myself.

@_date: 2015-05-10 18:17:32
@_author: Dan Kaminsky 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
Looks like Stevens is getting close to SHA-1 collision, meaning HMAC-SHA1
will collide too.
HMAC actually encodes a bunch of very interesting and useful design
constraints; lose the constraints and there's hell to pay.  That is indeed
not specific to HMAC.
Thanks, it's not often you have something simple enough that it *could* be
documented this way.
There's this insanely cool JS embeddable Python environment called Skulpt,
I'd have used that if it worked on Wordpress.com.

@_date: 2015-05-15 01:52:59
@_author: Dan Kaminsky 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
Not wrong at all.  There's no (known to me) security impact to HMAC
collisions, in the context of the security guarantees made by HMAC'.  It's
just if the underlying hash to HMAC collides (already bad) tnen HMAC can't
save you.
Now that I can confirm :)

@_date: 2015-05-25 12:25:29
@_author: Dan Kaminsky 
@_subject: [Cryptography] Is this a "relevant" attack against HMAC-MD5? 
I pretty much never see bit commitment protocols in the field, but somebody
posted this and I don't know enough to know if it matters.
