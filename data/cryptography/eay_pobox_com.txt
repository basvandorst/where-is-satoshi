
@_date: 2001-10-22 22:19:18
@_author: Eric Young 
@_subject: RC4 [was: RE: Passport Passwords Stored in Plaintext] 
Considering that RSA Security changed their logo to a red 'brick' a while back (they called
it a brick as part of the corporate PR around the change).  I can't help
but see the irony of using this analogy to describe that which they claim is a trade secret
but is widely known :-).

@_date: 2001-09-19 02:42:42
@_author: Eric Young 
@_subject: Rijndael in Assembler for x86? 
I've done quite a bit of assembler for crypto
in the last few years and it very much depends
on the CPU/compiler (obviously).  The only platform that I have not
been able to beat the compiler by %10 or more for an algorithm is
PA-RISC.  Either my C code is good enough to give the compiler all
the help it needs or I need to revisit the architecture :-).
The biggest wins are for algorithms where the C compiler does not
give access to underlying primitives, ie 32*32->64, or where special tricks
can be used due to data relationships that the compiler cannot know about.
For x86, there are too few registers and lots of black magic going on.
At least for the pentium VTune would reveal everything.  For the Ppro/II/III,
depending on the compiler (gcc vs VisualC) the C code could sometimes get
within %30 of the ASM.
For the Pentium 4, ASM is good again.  It seems to be a very 'brittle' CPU.
Eg. for sha1 (The numbers are relative but may or may not have any relation
to something in the real world :-)
 P4     |Athalon   |P2 Celeron
 1.7ghz | 1.4ghz   | 333mhz|
lnx gcc |cygwin gcc|lnx gcc|
SHA1 586            |  78.594| 135.937  | 26.038|
SHA1 686            |  81.986| 141.996  | 32.481|
SHA1 786            | 135.419| 137.804  | 29.106|
SHA1 fast           |  47.864|  83.846  | 20.828|
SHA1 small          |  54.322|  62.599  | 12.534|
Notice how the different assembler version are all around the same
speed for the P2 and Athalon. Even the ratio between the
C code versions is similar.  But now look at the P4.  Special
magic can be used to make things very fast, and the 'small' C code
version is faster than the loop unrolled version
(trace cache thrashing?)
For PA-RISC, I've done 1.1, 2.0 and 2.0W code and for some algorithms I cannot
beat the optimizer.  For others, specifically bignum stuff, 2 to 4 times faster.  In
this case
all multiples are done in the FP unit and data has to be swapped between CPU and FPU
via memory so there are lots of opportunities to use 64bit loads etc.
HP has good optimizers for a rather tricky architecture.
Sparc, I've only done digests, 30-40% speedup seem normal.  Simple architecture,
simple for the compiler to do a good job.
ARM, the compilers are good and the CPUs are simple, I consistently only get
20-40%.  The real win comes when trying to get fast with small code size.
It is possible to make things much faster for the same reduced footprint.  Xscale
be interesting since there are now inter instruction register dependencies.  I've
normally worked
on StrongARM where there are none.
Itanium.  Amazing speedups with assembler, but hard to write.  It is a vector
processor if there ever was one.  Anything that can do two 64*64->128
multiplies per cycle but with a 15 cycle latency, and no OOO  is going to be tricky.
For both ARM and PA-RISC I've been able to use instruction set
features to improve performance.  In theory the algorithms could be
coded in C, but it takes CPU architecture/compiler knowledge to write the code :-).

@_date: 2002-03-23 17:00:12
@_author: Eric Young 
@_subject: RSA on general-purpose CPU's [was:RE: Secure peripheral cards] 
I don't know what the OpenSSL people did to the x86 ASM code, but SSLeay
 (the precursor to OpenSSL, over 3 years old)  did/does 330 512bit and 55 1024 bit
RSAs a second on a 333mhz celeron (linux and/or win32).  You should be numbers like 600 and 100. The company I work for has 70 1024's per second
on the celeron 333.  It used to build this way (and with this performance) for
most x86 boxes.....
PS    The SSLeay version I tested was '0.9.1a 06-Jul-1998', which is what is in
    \bin on my laptop.

@_date: 2003-06-04 01:05:24
@_author: Eric Young 
@_subject: Maybe It's Snake Oil All the Way Down 
Well the point here is that the data encryption in GSM is not relevant to
the people running the network.  The authentication is secure,
so there is no fraud, so they still get the money from network
usage.  Privacy was never really there since
the traffic is not encrypted once it hit the base station, so the
relevant government agencies can be kept happy.
The encryption was only relevant to protect the consumers
from each other.
eric (hopefully remembering things correctly)

@_date: 2004-10-14 12:08:16
@_author: Eric Young 
@_subject: AES Modes 
Quoting Brian Gladman :
Are you running on a P4?  ASM for sha1 on a P4 takes about 11.9 cycles per byte.  The P4 is a very touchy x86 implementation.
On most other architectures I nearly always see a bit less than 2 times faster
sha1 vs AES.  On AMD64, asm, I have
AES-cbc at 12.2 cycles per byte and sha1 at 6.8.  This is about
as good a CPU as it gets (IPC near 3 for both implementations).

@_date: 2006-09-12 10:53:31
@_author: Eric Young 
@_subject: Exponent 3 damage spreads... 
Well, since this in not really an issue about forging signatures, rather invalid verification,
I've appended 2 self-signed certs (resigned apps/server.pem), one with a valid signature,
and one with a signature block with an extra byte appended after the ASN.1 (but before signing).
For openssl 0.9.8a
eay at huboo ~/work>openssl verify -CAfile cert-ok.pem cert-ok.pem
cert-ok.pem: OK
eay at huboo ~/work>openssl verify -CAfile cert-bad.pem cert-bad.pem
cert-bad.pem: OK
For openssl 0.9.8c
eay at huboo ~/work>openssl-0.9.8c/apps/openssl verify -CAfile cert-ok.pem cert-ok.pem: OK
eay at huboo ~/work>openssl-0.9.8c/apps/openssl verify -CAfile cert-bad.pem cert-bad.pem: /C=AU/ST=Queensland/O=CryptSoft Pty Ltd/CN=Server test cert (512 bit)
error 7 at 0 depth lookup:certificate signature failure
28900:error:04077068:rsa routines:RSA_verify:bad signature:rsa_sign.c:192:
28900:error:0D0C5006:asn1 encoding routines:ASN1_item_verify:EVP so this appears to trigger the relevant condition.
For my own recent pkcs implementations, they do not ASN.1 decode the signature block, rather then generate a signature block and do a memcmp with the output from the RSA decrypt.  I did this since it is easy to generate small amounts of ASN.1 relative to parsing and checking all the boundary cases.  In this case this 'simpler' approach seems to have paid off :-).
eay at huboo ~/work>cat cert-ok.pem
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
eay at huboo ~/work>cat cert-bad.pem
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

@_date: 2006-09-17 07:31:51
@_author: Eric Young 
@_subject: [cryptography] Re: Why the exponent 3 error happened: 
This is a question I would not mind having answered; while the exponent 3 attack works when there are low bits to 'modify', there has been talk of an attack where the ASN.1 is correctly right justified (hash is the least significant bytes), but incorrect ASN.1 encoding is used to add 'arbitrary' bytes before the hash.  So in this case some of the most significant bytes are fixed, the least significant bytes are fixed, but some in the middle can be modified.  Does the exponent 3 attack work in this case?  My personal feel is that his would be much harder, but is such an attack infeasible?
This issue about ASN.1 parameters being an evil concept goes away if the attack can only work when the least significant bytes need to be modifiable.

@_date: 2008-08-24 14:44:22
@_author: Eric Young 
@_subject: [cryptography] 5x speedup for AES using SSE5? 
I've only just seen this, but I've been playing with the VIA's AES and
looking at Intels AES instructions.
I believe the PPERM instruction will be rather important.  Combined with
the packed byte rotate and shift some rather
interesting SIMD byte fiddles should be possible.
tables, doing SIMD operations on all 16 bytes at once.
I've not looked at it enough yet, but currently I'm doing an AES round
in about 140 cycles a block (call it 13 per round plus overhead) on a
AMD64, (220e6 bytes/sec on a 2ghz cpu) using normal instructions.  I
don't believe they will be taking 30 instructions , so they probably
have 4-8 SSE instructions per round, it then comes down to how many SSE
execution units there are to execute in parallel.
As for VIA, on a 1ghz C7 part, cbc mode, 128bit key, for 16byte aligned,
I'm getting about 24 cycles per block, for unaligned, about 67 cycles. The chip does ECB mode at 12.6 cycles a block if aligned (2 at a time). It does not handle unaligned ECB, so with manual alignment, 75 cycles. Not bad for a single issue cpu considering the x86 instruction version
of AES I have
takes 1010 cycles per block.
For the intel AES instructions, from my readings, it will be able to do
a single AES (128bit) in a bit more that 60 cycles
(10 rounds, 6 cycle latency for the instructions).  The good part is
that they will pipeline.  So if you say do 6
AES ecb blocks at once, you can get a throughput of about 12 cycles a
block (intel's figures).  This is obviously of relevance for counter
mode, cbc decrypt and more recent standards like xts and gcm mode.
Part of the intel justification for the AES instruction seems to stop
cache timing attacks.  If the SSE5 instructions allow AES
to be done with SIMD instead of tables, they will achieve the same
affect, but without as much parallel upside.
It also looks like the  GF(2^8) maths will also benefit.
eric (who has only been able to play with via hardware :-(

@_date: 2008-08-25 09:58:19
@_author: Eric Young 
@_subject: 5x speedup for AES using SSE5? 
Urk, correction, I forgot I've recently upgraded from a 2ghz machine to
So that should read about 182 cycles per block, and 18 cycles per round.
I though the number seems strange :-(.  I tent to always quote numbers
from a 2-3 second run encrypting a 4k buffer, not a machine cycle
counter over one or two blocks, so I leave myself open to this kind of
error :-(
Still, looking further at the various SSE5 instructions, I'm having
difficultly seeing how
to avoid instruction dependencies when using the SIMD instructions
(specifically using PPERM to implement the sbox).

@_date: 2008-08-26 21:41:12
@_author: Eric Young 
@_subject: [cryptography] 5x speedup for AES using SSE5? 
SSE5 (to get back on topic :-), you should be able to do 2 blocks in the
one instruction stream.  I can't remember enough of the other SSE
instructions to know if the relevant 64bit shifts are present before SSE5.
The only place where I've used multiple CPUs in crypto so far has been
in RSA's CRT, where, due to the magic of
OpenMP support, and a little bit of state splitting, I get the following
throughput numbers for dual core 2.5ghz, athlon64
doing 1024-2 RSA private key operations (number per second)
For normal single threaded, 4650 per cpu second and wall clock second.
OpenMP, 4330 per CPU second, 7360 wall clock second.
So in this case, the OpenMP overhead is about 8% CPU.  MD6 has smaller
chunks, and lots of them, so it will probably scale quite well.
OpenMP, it makes it very easy to put in parallelism.  In this CRT
implementation, it was a simple
 omp parallel for
        for (i=0; i<2; i++)
             /* CRT code */
A few changes were made to make sure the structures were not shared, but
nothing that affects performance.
OpenMP is now in gcc 4.2 which is nice.
MD6, should be just as stupidly easy,
 omp parallel for
    for (block_num=0; block_num<(data_len/512); block_num++) {
       MD6_block(&(ret_st[block_num]), input + block_num*512, block_num,
level, not_root, ....)
    }
Repeat up the levels (depending of memory availability).

@_date: 2008-05-24 20:10:21
@_author: Eric Young 
@_subject: The perils of security tools 
I just re-checked, this code was from SSLeay, so it pre-dates OpenSSL
taking over from me
(about 10 years ago, after I was assimilated by RSA Security).
So in some ways I'm the one at fault for not being clear enough about
why 'purify complains' and why it was not relevant.
Purify also incorrectly companied about a construct used in the digest
gathering code which functioned correctly, but purify was
also correct (a byte in a read word was uninitialised, but it was later
overwritten by a shifted byte).
One of the more insidious things about Purify is that once its
complaints are investigated, and deemed irrelevant (but left in the
anyone who subsequently runs purify on an application linking in the
library will get the same purify warning.
This leads to rather distressed application developers.  Especially if
their company has a policy of 'no purify warnings'.
One needs to really ship the 'warning ignore' file for purify (does
valgrind have one?).
I personally do wonder why, if the original author had purify related
comments, which means he was aware of the issues,
but had still left the code in place, the reviewer would not consider
that the code did some-thing important enough to
ignore purify's complaints.

@_date: 2009-09-05 08:25:30
@_author: Eric Young 
@_subject: [cryptography] AES-GMAC as a hash 
Regarding the speed of GMAC,  Intel has added a
carry-less-multiplication instruction to their next generation CPUs
The core is the Westmere, and is shipping in engineering samples, now. This is also the CPU generation to contain the AES instructions.
Unfortunately I'm only running my implementation under the intel
simulator which is not cycle accurate, so I'm not sure just how fast
this hardware support will make things.  My understanding is that the
next generation AMD CPUs, (bulldozer) will also support these instructions.

@_date: 2013-09-10 20:58:20
@_author: Eric Young 
@_subject: [Cryptography] [cryptography] Random number generation 
I may have missed this part of the thread, but I'm interested in knowing
the rational for letting the hyper-visor intercept the RDRAND call and
return any value it likes, bypassing the random hardware.
I've had one person speculate it would be useful for keeping 2 CPUs in
sync, (the TSC can also be intercepted), but it does worry me that
RDRAND calls can be rendered predictable by a compromised VM.
For those interested,
Intel document 325462.pdf, "Intel? 64 and IA-32 Architectures Software
Developer?s Manual Combined Volumes: 1, 2A, 2B, 2C, 3A, 3B and 3C"
Page 'Vol. 3C 27-23', Table 27-12. Format of the VM-Exit
Instruction-Information Field as Used for RDRAND

@_date: 2014-03-03 22:53:53
@_author: Eric Young 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
I must admit that for the crypto library I mostly develop now
(algorithms, not so much protocols), I have ruby bindings and I am doing
some positive and negative testing using rubys rspec testing framework.
I use the ruby FFI library to bind to my shared library.  The FFI style
bindings are nice because they are defined in your scripting language,
not some C/C++ glue code.  The main negative of using this system is you
don't get all full platform coverage.
BTW I always considered the 'goto err' programming style as a way of
exceptions in C, but without the magic :-).
The way I now handle errors in my bignum library is to have an error
flag in the context that is always passed to functions.  It is checked
on function entry, if there is an error, this is returned, else do the
if ((ret = bn_mul(bn_ctx,r,a,b)) != 0) goto err;
if ((ret = bn_add(bn_ctx,r,r,c)) != 0) goto err;
ret = bn_ctx->error;
It makes the code easier to read, and in this case, if the function
fails, I don't actually care that much about which line it was.  You do
need a little care to check ctx->error sometimes, especially if your
algorithm has loops.
