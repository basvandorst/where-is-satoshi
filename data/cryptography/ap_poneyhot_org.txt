
@_date: 2007-10-07 21:15:31
@_author: Alex Pankratov 
@_subject: Trillian Secure IM 
I've been poking around Oscar (ICQ/AIM) protocol parsing and had a look at Trillian's SecureIM handshake protocol.
For those who don't know, Trillian is a very popular multi-
protocol instant messanging application for Windows. One of
its notable features, for which is got some rave/positive
reviews, is an ability to encrypt ICQ/AIM IMs exchanged by two Trillian instances. AOL made repeated attempts to block SecureIM, but eventually stopped them [1].
The protocol is closed, but it was reversed engineered by
some guys over at GAIM project. It appeared to be a Blowfish
encryption of bulk IMs using a key derived from an anonymous DH exchange [2]. This was also indirectly confirmed by another
project [3].
Leaving aside the lack of authentication and replay protection,
here's what is even more striking -
SecureIM handshake between two version 3.1 (latest) clients takes about .. 48 bytes. That's altogether, 32 bytes in one direction, and 16 in another. And that's between the clients that have never talked to each other before, so there's no "session resuming" business happenning.
If that's DH exchange, then it's 128 bit one. Fertile ground
for some interesting speculation, don't you think ?
[3]

@_date: 2007-10-08 09:10:35
@_author: Alex Pankratov 
@_subject: Trillian Secure IM 
Primitive form - maybe, weak form - absolutely not. It is actually worse than having no security at all, because it tends to create an _illusion_ of protection. Which is by the way exactly the case with SecureIM. How hard is it to brute-force 128-bit DH ? My "guesstimate"
is it's an order of minutes or even seconds, depending
on CPU resources.

@_date: 2007-10-08 09:17:48
@_author: Alex Pankratov 
@_subject: Trillian Secure IM 
Yickes. Ignorance is a bliss. I am actually curious to see what was the DH modulus size in T's versions that were blocked by AOL. Given T's installation
base, strong SecureIM would've dramatically complicated "lawful intercepts", which AOL is probably required to implement.

@_date: 2007-10-08 20:47:48
@_author: Alex Pankratov 
@_subject: Trillian Secure IM 
Well, I view it from a slightly different perspective. Even the most ignorant person knows a difference between the privacy and the lack of thereof. Cryptography or not. Therefore, if he is being told that A offers a privacy, it may lead this person to assume the level of this privacy protection is adequate ... simply because if it weren't, it wouldn't be offered. Needless to say that
this sort of an assumption in case of a weak crypto is
When there's a choice between no and weak protection, I am of course in favour of latter *if* it is clearly labeled as Yup, I am familiar with the methodology. My point was that
128bit DH is "breakable" in terms of the people from those
forum's threads.

@_date: 2007-10-25 21:16:21
@_author: Alex Pankratov 
@_subject: Password vs data entropy 
Say, we have a random value of 4 kilobits that someone wants to keep secret by the means of protecting it with a password. Empirical entropy estimate for an English text is 1.3 bits of randomness per character, IIRC.
Assuming the password is an English word or a phrase, and the secret is truly random, does it mean that the password needs to be 3100+ characters in size in order to provide a "proper"
degree of protection to the value ? Or, rephrasing, what should the entropy of the password be compared to the entropy of the value being protected (under
whatever keying/encryption scheme) ? I realize that this is rather .. err .. open-ended question, and it depends on what one means by "protected", but I'm sure you can see the gist of the question. How would one deem a
password random enough to be fit for protecting an equivalent
of N bits of random data ? Is it a 1-to-1 ratio ?

@_date: 2007-10-26 21:41:21
@_author: Alex Pankratov 
@_subject: Password vs data entropy 
Essentially the entropy measure alone is not sufficient to make a decision, we should also account for the algorithms being used. This certainly makes sense .. now that you said it :)
Is there any published research into entropy estimates of PBKDF2 transformation ? Perhaps, for specific PRF(s) and fixed iteration counts. I.e. if I have a password with N bits of entropy in a password, what the entropy of the key going to be like given *this* set of PBKDF2 parameters.
Also, can you elaborate on this remark ? Specifically, the
second part of it -
Are you referring to RSA-like secrets that involve prime
numbers, which are therefore selected from a smaller subset
of Z(n) ?

@_date: 2008-08-19 20:57:33
@_author: Alex Pankratov 
@_subject: [p2p-hackers] IETF rejects Obfuscated TCP 
CC'ing cryptography mail list as it may be of some interest to the folks over there.
This is just a quick thought, but a variation of SYN cookies for TLS
appears to be quite easy to do. It does require defining new record type, but latter is permitted by TLS spec as per Section 6, RFC 2246.
The idea, obviously, is to include a copy of ClientHello message in a
second batch of records sent by the client. This should allow server
to generate ServerKeyExchange parameters from the original ClientHello
message (ClientHello.random + IP/port quintet + server "cookie secret"),
then discard ClientHello and delay creating the state .. exactly the
same way SYN cookies mechanism does it.
This still doesn't protect against host CPU exhaustion attacks, because
ServerKeyExchange may require some heavy crypto. But since all this is
being discussed in a context of "obfuscated TCP" and "opportunistic
encryption", then using anonymous DH suite might be a feasible option.
The above is trivial to implement, it is backward compatible with existing TLS implementations (as per the same section of RFC - records of unknown
types are silently discarded) and all it requires little standardization
As I said, this is just a quick thought, so in all likelihood I might be
reinventing a (broken) bike here.

@_date: 2008-08-20 11:59:48
@_author: Alex Pankratov 
@_subject: [p2p-hackers] IETF rejects Obfuscated TCP 
My comment was in a context of a thread discussing Obfuscated TCP.
One of the suggestions was to piggyback SSL handshake on TCP handshake, to which someone pointed at an issue with SYN-flood like DoS attacks. My response was to the latter comment.

@_date: 2008-08-20 13:15:15
@_author: Alex Pankratov 
@_subject: [p2p-hackers] IETF rejects Obfuscated TCP 
Based on this reply alone I'm not sure I follow. I also read quickly through your exchange on TCPM and your comments appear to be specific to Adam's draft.
My comment was not related to either a latency or a potential performance problems of TLS. It was in a response to another idea - that of bundling
TLS handshake with TCP handshake. The goal of this (and I apologize for stating the obvious, just want to make sure we are on the same page here) is to provide transparent to application layer opportunistic encryption of TCP streams. Whether this goal makes any sense and if it is worth pursuing is a separate issue; it's the DoS aspect of the implementation idea that I was commenting on.

@_date: 2009-08-10 15:07:25
@_author: Alex Pankratov 
@_subject: Entropy USB key 
Just spotted this on one of the tech news aggregators - The Entropy Key, or eKey, is a small, unobtrusive and easily installed USB stick that generates high-quality random numbers, or entropy, which can improve the performance, security and reliability of servers.

@_date: 2009-08-10 15:07:25
@_author: Alex Pankratov 
@_subject: Entropy USB key 
Just spotted this on one of the tech news aggregators - The Entropy Key, or eKey, is a small, unobtrusive and easily installed USB stick that generates high-quality random numbers, or entropy, which can improve the performance, security and reliability of servers.

@_date: 2009-10-04 14:42:22
@_author: Alex Pankratov 
@_subject: Trusted timestamping 
Does anyone know what's the state of affairs in this area ? This is probably slightly off-topic, but I can't think of
a better place to ask about this sort of thing.
I have spent a couple of days looking around the Internet,
and things appear to be .. erm .. hectic and disorganized.
There is for example timestamp.verisign.com, but there is no documentation or description of it whatsoever. Even the
website itself is broken. However it is used by Microsoft's code signing tool that embeds Verisign's timestamp into Authenticode signature of signed executable files.
There is also a way to timestamp signed PDFs, but the there appears to be nothing _trusted_ about available Trusted Timestamping Authorities. Just a bunch of random companies
that call themselves that way and provide no indication why
they should actually be *trusted*. No audit practicies, not even a simple description of their backend setup. The same
goes for the companies providing timestamping services for arbitrary documents, either using online interfaces or a
downloadable software.
There are also Digital Poststamps, which is a very strange
version of a timestamping service, because their providers
insist on NOT releasing the actual timestamp to the customer and then charging for each timestamp verification request.
I guess my main confusion at the moment is why large CAs of Verisign's size not offering any standalone timestamping Any thoughts or comments ?

@_date: 2009-10-06 22:21:19
@_author: Alex Pankratov 
@_subject: Trusted timestamping 
These players are sitting in the wrong place then. I have run into a fairly well defined need for a timestamping service in a graphic design community. Interestingly enough they do not need the timestamps for the courts, they need them more as a deterrent to a blatant theft of their creative ideas. If someone copies their work, verbosely or at a concept level, then the clone is wortheless unless it can be sold or used as a promotion vehicle. The copycat's goal is to get the copy published in as many online galleries and auction/specwork sites as possible, and the goal of the original author is to prevent that from happening. At the moment the challenge frequently boils down to searching through archive.org contents, and using that as a proof of who was first. In this context archive.org, clearly, serves as a coarse time
stamping service, implicitly trustworthy. There is obviously
a room for improvement, and that's why I asked what I asked.
