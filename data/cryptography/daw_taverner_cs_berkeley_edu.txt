
@_date: 2003-12-06 22:45:57
@_author: David Wagner 
@_subject: safety of Pohlig-Hellman with a common modulus? 
Yes, I believe so.  The security of Pohlig-Hellman rests on the difficulty
of the discrete log problem.  Knowing the discrete log of g^y doesn't help
me learn the discrete log of g^x (assuming x,y are picked independently).
This is not like RSA, where using a common modulus allows devastating
There is a small caveat, but it is pretty minor.  There are some
precomputation attacks one can do which depend only on the prime p; after
a long precomputation, one can compute discrete logs mod p fairly quickly.
The more people who use the same modulus, the more attractive such a
precomputation effort will be.  So the only reason (that I know of)
for using different modulii with Pohlig-Hellman is to avoid putting all
your eggs in one basket.

@_date: 2003-12-07 06:20:29
@_author: David Wagner 
@_subject: safety of Pohlig-Hellman with a common modulus? 
Ok, I was being slightly loose.  To be more precise, the security of
Pohlig-Hellman is based on the Decision Diffie-Hellman (DDH) problem,
I believe.  But the best known attack on DDH (when you're working in
a prime-order subgroup) is to compute discrete logs.
Sure they do.  If I have a known plaintext pair (M,C), where
C = M^k (mod p), then with two discrete log computations I can
compute k, since k = dlog_g(C)/dlog_g(M) (mod p-1).  This works for
any generator g, so I can do the precomputation for any g I like.
Ok, that sounds like it should work.  To break the composed scheme,
one would need to break both P-H and the other symmetric cipher.

@_date: 2003-12-16 22:14:17
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
I don't understand why you say that.  You can build perfectly good
secure computing kernels that don't contain any support for remote
attribution.  It's all about who has control, isn't it?

@_date: 2003-12-21 01:08:48
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
But you missed my main point.  Leichter claims that any secure kernel is
inevitably going to come with all the alleged harms (DRM, lock-in, etc.).
My main point is that this is simply not so.
There are two very different pieces here: that of a secure kernel, and
that of remote attestation.  They are separable.  TCPA and Palladium
contain both pieces, but that's just an accident; one can easily imagine
a Palladium-- that doesn't contain any support for remote attestation
whatsoever.  Whatever you think of remote attestation, it is separable
from the goal of a secure kernel.
This means that we can have a secure kernel without all the harms.
It's not hard to build a secure kernel that doesn't provide any form of
remote attestation, and almost all of the alleged harms would go away if
you remove remote attestation.  In short, you *can* have a secure kernel
without having all the kinds of things we don't want.  Leichter's claim
is wrong.
This is an important point.  It seems that some TCPA and Palladium
advocates would like to tie together security with remote attestion; it
appears they would like you to believe you can't have a secure computer
without also enabling DRM, lock-in, and the other harms.  But that's
simply wrong.  We can have a secure computer without enabling all the
alleged harms.  If we don't like the effects of TCPA and Palladium,
there's no reason we need to accept them.  We can have perfectly good
security without TCPA or Palladium.
As for remote attestion, it's true that it does not directly let a remote
party control your computer.  I never claimed that.  Rather, it enables
remote parties to exert control over your computer in a way that is
not possible without remote attestation.  The mechanism is different,
but the end result is similar.

@_date: 2003-12-23 04:38:33
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
I must confess I'm puzzled why you consider strong authentication
the same as remote attestation for the purposes of this analysis.
It seems to me that your note already identifies one key difference:
remote attestation allows the remote computer to determine if they wish
to speak with my machine based on the software running on my machine,
while strong authentication does not allow this.
As a result, remote attestation enables some applications that strong
authentication does not.  For instance, remote attestation enables DRM,
software lock-in, and so on; strong authentication does not.  If you
believe that DRM, software lock-in, and similar effects are undesirable,
then the differences between remote attestation and strong authentication
are probably going to be important to you.
So it seems to me that the difference between authenticating software
configurations vs. authenticating identity is substantial; it affects the
potential impact of the technology.  Do you agree?  Did I miss something?
Did I mis-interpret your remarks?
P.S. As a second-order effect, there seems to be an additional difference
between remote attestation ("authentication of configurations") and
strong authentication ("authentication of identity").  Remote attestation
provides the ability for "negative attestation" of a configuration:
for instance, imagine a server which verifies not only that I do have
RealAudio software installed, but also that I do not have any Microsoft
Audio software installed.  In contrast, strong authentication does
not allow "negative attestation" of identity: nothing prevents me from
sharing my crypto keys with my best friend, for instance.

@_date: 2003-12-30 01:43:27
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
Good.  I'm glad we agree that one can build a remote kernel without
remote attestation; that's progress.  But I dispute your claim that remote
attestation is critical to securing our machines.  As far as I can see,
remote attestation seems (with some narrow exceptions) pretty close to
worthless for the most common security problems that we face today.
Your argument is premised on the assumption that it is critical to defend
against attacks where an adversary physically tampers with your machine.
But that premise is wrong.
Quick quiz: What's the dominant threat to the security of our computers?
It's not attacks on the hardware, that's for sure!  Hardware attacks
aren't even in the top ten.  Rather, our main problems are with insecure
software: buffer overruns, configuration errors, you name it.
When's the last time someone mounted a black bag operation against
your computer?  Now, when's the last time a worm attacked your computer?
You got it-- physical attacks are a pretty minimal threat for most users.
So, if software insecurity is the primary problem facing us, how does
remote attestation help with software insecurity?  Answer: It doesn't, not
that I can see, not one bit.  Sure, maybe you can check what software is
running on your computer, but that doesn't tell you whether the software
is any good.  You can check whether you're getting what you asked for,
but you have no way to tell whether what you asked for is any good.
Let me put it another way.  Take a buggy, insecure application, riddled
with buffer overrun vulnerabilities, and add remote attestation.  What do
you get?  Answer: A buggy, insecure application, riddled with buffer
overrun vulnerabilities.  In other words, remote attestation doesn't
help if your trusted software is untrustworthy -- and that's precisely
the situation we're in today.  Remote attestation just doesn't help with
the dominant threat facing us right now.
For the typical computer user, the problems that remote attestation solves
are in the noise compared to the real problems of computer security
(e.g., remotely exploitable buffer overruns in applications).  Now,
sure, remote attestation is extremely valuable for a few applications,
such as digital rights management.  But for typical users?  For most
computer users, rather than providing an order of magnitude improvement
in security, it seems to me that remote attestation will be an epsilon
improvement, at best.

@_date: 2003-12-30 02:02:00
@_author: David Wagner 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
This is a good example, because it brings out that there are really
two different variants of remote attestation.  Up to now, I've been
lumping them together, but I shouldn't have been.  In particular, I'm
thinking of owner-directed remote attestation vs. third-party-directed
remote attestation.  The difference is who wants to receive assurance of
what software is running on a computer; the former mechanism allows to
convince the owner of that computer, while the latter mechanism allows
to convince third parties.
If I understand correctly, TCPA and Palladium provide third-party-directed
remote attestation.  Intel, or Dell, or someone like that will generate
a keypair, embed it inside the trusted hardware that comes with your
computer, and you (the owner) are never allowed to learn the corresponding
private key.  This allows your computer to prove to Intel, or Dell, or
whoever, what software is running on your machine.  You can't lie to them.
In owner-directed remote attestation, you (the owner) would generate the
keypair and you (the owner) would learn the private key -- not Intel, or
Dell, or whoever.  This allows your computer to prove to you what software
is running on your machine.  However, you can't use this to convince Intel,
or Dell, or anyone else, what software is running your machine, unless they
know you and trust you.
I -- and others -- have been arguing that it is remote attestation that
is the key, from a policy point of view; it is remote attestation that
enables applications like DRM, software lock-in, and the like.  But this
is not quite right.  Rather, it is presence of third-party-directed remote
attestation that enables these alleged harms.
Owner-directed remote attestation does not enable these harms.  If I know
the private key used for attestation on my own machine, then remote attestation
is not very useful to (say) Virgin Records for DRM purposes, because I could
always lie to Virgin about what software is running on my machine.  Likewise,
owner-directed remote attestation doesn't come with the risk of software
lock-in that third-party-directed remote attestation creates.
So it seems that third-party-directed remote attestation is really where
the controversy is.  Owner-directed remote attestation doesn't have these
policy tradeoffs.
Finally, I'll come back to the topic you raised by noting that your
example application is one that could be supported with owner-directed
remote attestation.  You don't need third-party-directed remote
attestation to support your desired use of remote attestation.  So, TCPA
or Palladium could easily fall back to only owner-directed attestation
(not third-party-attestation), and you'd still be able to verify the
software running on your own servers without incurring new risks of DRM,
software lock-in, or whatever.
I should mention that Seth Schoen's paper on Trusted Computing anticipates
many of these points and is well worth reading.  His notion of "owner
override" basically converts third-party-directed attestation into
owner-directed attestation, and thereby avoids the policy risks that so
many have brought up.  If you haven't already read his paper, I highly
recommend it.

@_date: 2003-12-30 02:20:05
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
I'm a bit puzzled why you'd settle for detecting changes when you
can prevent them.  Any change you can detect, you can also prevent
before it even happens.  So the problem statement sounds a little
contrived to me -- but I don't really know anything about kiosks,
so maybe I'm missing something.
In any case, this is an example of an application where owner-directed
remote attestation suffices, so one could support this application
without enabling any of the alleged harms.  (See my previous email.)
In other words, this application is consistent with an "Owner Override".
It sounds like the threat model is that the sysadmins don't trust the
users of the machine.  So why are the sysadmins giving users administrator
or root access to the machine?  It sounds to me like the real problem
here is a broken security architecture that doesn't match up to the
security threat, and remote attestation is a hacked-up patch that's not
going to solve the underlying problems.  But that's just my reaction,
without knowing more.
In any case, this application is also consistent with owner-directed
remote attestation or an "Owner Override".
If I don't trust the administrators of that machine to protect sensitive
data appropriately, why would I send sensitive data to them?  I'm not
sure I understand the threat model or the problem statement.
But again, this seems to be another example application that's compatible
with owner-directed remote attestation or an "Owner Override".
Summary: None of these applications require full-strength
(third-party-directed) remote attestation.  It seems that an "Owner
Override" would not disturb these applications.

@_date: 2003-11-14 03:00:27
@_author: David Wagner 
@_subject: Are there... 
Every encryption algorithm is injective, otherwise decryption
would be ambiguous.  In other words, if x and x' are two different
plaintexts, then E_k(x) != E_k(x').
Ok, in that case, use a public-key encryption algorithm.  Same deal.
And, if you want to ensure that E_k(x) != E_k'(x') whenever
(k,x) != (k',x'), define E_k(x) = (k, EE_k(x)) where EE is some
public-key encryption algorithm; EE_k(x) denotes the result of encrypting
plaintext x under public key k.  It can't hurt security to include the
public key in the ciphertext.

@_date: 2003-11-17 02:36:01
@_author: David Wagner 
@_subject: A-B-a-b encryption 
It's called Pollig-Hellman.  It only works if your encryption scheme
is commutative.  Most symmetric-key encryption schemes aren't commutative,
but one scheme that does work is A(M) = M^A mod p.  One scheme that doesn't
work is A(M) = M xor A; XOR is indeed commutative, but it becomes insecure
when used in the above protocol.
Anyway, the Pollig-Hellman protocol is no better (and probably no worse)
than a straight Diffie-Hellman, so there seems to be little reason to adopt
it.  Just stick to standard Diffie-Hellman.

@_date: 2003-11-17 19:21:32
@_author: David Wagner 
@_subject: Are there...one-way encryption algorithms 
I'd ignore the risk.  If you've got a 160-bit hash function
(and you probably should), then the risk of a collision is truly
negligible.  If you try to come up with some fancy alternative,
there will be a greater risk that the fancy alternative is insecure
than the risk that you ever experience a collision in SHA.

@_date: 2003-11-20 23:30:53
@_author: David Wagner 
@_subject: Are there...one-way encryption algorithms 
You're right.  The above protocol is essentially Shamir's 3-pass
protocol, not Pohlig-Hellman.
Pohlig-Hellman is the encryption scheme A(M) = M^A mod p.  If you
instantiate Krafft's proposal with the Pohlig-Hellman encryption scheme,
you get a working (and secure) instance of Shamir's 3-pass protocol.
Thank you for correcting my error!

@_date: 2004-04-14 16:29:41
@_author: David Wagner 
@_subject: DRM of the mirror universe 
This kind of idea has been discussed before.  Personally, I'm not
convinced we're likely to see this take off any time soon.  There are
some technological barriers, but a bigger one may well be incentives.
I'd argue the true source of many privacy problems may be more from the
imbalance in bargaining power between the consumer and the corporation
(the corporation can often more or less dictate terms -- or, at least,
there is not much room for personalized negotiation and bargaining).
Fixing the power imbalance may well be a precondition to deploying
technology-based privacy defenses (be it DRM, or anything else).

@_date: 2004-01-04 04:46:52
@_author: David Wagner 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: 
I think you may be giving up too quickly.  It looks to me like
this situation can be handled by owner-directed attestation (e.g.,
Owner Override, or Owner Gets Key).  Do you agree?
To see why, let's go back to the beginning, and look at the threat
model.  If multiple people are doing shared development on a central
machine, that machine must have an owner -- let's call him Linus.  Now
ask yourself: Do those developers trust Linus?
If the developers don't trust Linus, they're screwed.  It doesn't how
much attestation you throw at the problem, Linus can always violate their
security model.  As always, you've got to trust "root" (the system
administrator); nothing new here.
Consequently, it seems to me we only need to consider a threat model
where the developers trust Linus.  (Linus need not be infallible, but the
developers should believe Linus won't intentionally try to violate their
security goals.)  In this case, owner-directed attestation suffices.
Do you see why?  Linus's machine will produce an attestation, signed
by Linus's key, of what software is running.  Since the developers trust
Linus, they can then verify this attestation.  Note that the developers
don't need to trust each other, but they do need to trust the owner/admin
of the shared box.  So, it seems to me we can get by without third-party
This scenario sounds pretty typical to me.  Most machines have a single
owner.  Most machines have a system administrator (who must be trusted).
I don't think I'm making unrealistic assumptions.
Yes, this is a fair point.  I suppose I would say I'm arguing that
feature X (third-party attestation) seems to have few significant uses.
It has some uses, but it looks like they are in the minority; for the
most part, it seems that feature X is unnecessary.  At the same time,
many people are worried that feature X comes with significant costs.
At least, this is how it looks to me.  Maybe I've got something wrong.
If these two points are both accurate, this is an interesting observation.
If they're inaccurate, I'd be very interested to hear where they fail.

@_date: 2004-01-04 05:50:47
@_author: David Wagner 
@_subject: example: secure computing kernel needed 
1. If you can check signatures on running software, you might as well
check the signature when the software is first executed, and prevent
it from being executed if it is not allowed.  The operating system is
going to have to be involved in the process of checking which user-level
applications are running (because on the OS knows this fact), so you
might as well get it involved in deciding which user-level applications
are allowed to be executed.  Proactive prevention is just as easy to
build as after-the-fact detection, and is more useful.
2. Your system administrator can only enumerate what applications you're
running if you have a trustworthy OS.  Today's OS's typically aren't
very trustworthy.
Let me explain.  Let's say the sysadmin want to check whether I'm running
Doom.  Fine.  That means the TPM (the hardware) is going to check what
bootloader (or nexus) was loaded at boot time, and sign that fact.
Then, the bootloader/nexus is going to check what operating system
kernel it loaded, and sign that fact.  Finally, the OS kernel is going
to check what user-level applications are running, and sign that fact.
This chain of signatures then tells you what software is running.  But,
if you want to believe in the chain of signatures, you have to believe
in the trustworthiness of all the software involved here: the TPM, the
bootloader/nexus, and the OS.  If my OS is Windows XP, you're screwed;
that chain of signatures doesn't tell you much.  There is no reason to
think Windows XP is trustworthy enough to know what software is running.
If Windows XP has a buffer overrun vulnerability somewhere -- as seems
likely -- then it could have been compromised, and my OS might be
signing false statements.  Maybe I hacked my OS to falsely report that
Doom is not running, when in fact I'm happily gaming away.  Maybe the
Doom installer hacked the OS to hide its presence.  You, and my sysadmin,
have no way of knowing, if Windows has any security vulnerability anywhere.
Conclusion: in today's machines, there's no reliable way to identify
which user-level applications are running, even if we all have a TCG
that supports remote attestation.
Now, there is an exception.  If my application is running natively on
top of the bootloader/nexus, not on top of Windows, then this issue
goes away.  In Microsoft's Palladium architecture, this is known as
"the right-hand side", and the TPM protects the "right-hand side" from
Windows and apps running on Windows.  (I don't recall whether TCPA has
a similar concept.)  But it seems unlikely that everyday applications,
like Doom, are going to be running on the "right-hand side".
To rephrase, my system administrator can reliably tell what is running
on the "right-hand side" of my machine, but my sysadmin has no reliable
way to tell what apps I'm running on top of Windows.

@_date: 2004-05-25 20:41:44
@_author: David Wagner 
@_subject: The EU pursues quantum crypto because of Echelon 
Based on the way the article is describing it, this sounds like a newly
funded research grant to study quantum crypto.  Grants certainly don't
represent settled policy, and it appears the bit about "defeating Echelon"
may be coming primarily from the researchers getting funded, rather than
from the funders.  So I wouldn't read too much into this news article --
I certainly don't view it as an authoritative statement about the EU's
policy on Echelon and quantum crypto.
I certainly agree with everyone's skepticism about quantum crypto.
Amusingly enough, if quantum crypto was deployed everywhere, it might
indeed reduce the take from satellite surveillance, Echelon, and such
types of SIGINT.  Not because of the quantum crypto, mind you, but
because of the use of fiber optics.  Today's quantum crypto systems
all pretty much require a dedicated fiber optic link between the two
endpoints, and as we know, such links are harder to intercept remotely
than RF traffic.  Consequently, deployment of quantum crypto might reduce
susceptibility to SIGINT by increasing use of dedicated non-RF links
that are hard to tap remotely.  Of course, you'd get basically the same
benefits from sending signals unencrypted over a dedicated, unswitched,
point-to-point fiber optic link, so the quantum crypto is irrelevant.
Also, such dedicated non-RF links are typically very expensive --
they have horrible scaling properties compared to a shared network.
So there are probably far cheaper ways to secure one's infrastructure
against SIGINT, and I'm not going to defend quantum crypto.
