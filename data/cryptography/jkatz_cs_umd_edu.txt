
@_date: 2008-08-27 16:10:51
@_author: Jonathan Katz 
@_subject: Decimal encryption 
But he probably wants an encryption scheme, not a cipher.
Also, correct me if I am wrong, but Black and Rogaway's approach is not efficient for large domains. But if you use their approach for small domains then you open yourself up to dictionary attacks.

@_date: 2008-08-27 19:17:47
@_author: Jonathan Katz 
@_subject: Decimal encryption 
A block cipher is a primitive that can be used, in particular, to construct encryption schemes. But you can construct encryption schemes without block ciphers, and you can use block ciphers to construct other things besides encryption. Moreover, "good" encryption should generally be randomized, while a block cipher is deterministic.

@_date: 2008-08-28 11:26:26
@_author: Jonathan Katz 
@_subject: Decimal encryption 
Yes, I can see this might cause confusion.
Just to clarify: I had emailed the original poster off-line and he
told me that he was willing to use other information already being
sent in the clear as a non-repeating IV. Given this, secure (and, in
particular, non-deterministic) encryption is possible.

@_date: 2008-06-29 15:51:18
@_author: Jonathan Katz 
@_subject: Using a MAC in addition to symmetric encryption 
As the other posters have already commented, encryption alone does not
(in general) provide integrity. Furthermore, care must be taken in how
the encryption scheme and the MAC are combined, with
encryption-followed-by-MACing-the-ciphertext being the best choice
unless you know what you are doing. For further discussion, see the
textbook by Katz-Lindell (Section 4.9), and/or the following paper:

@_date: 2008-05-04 13:20:19
@_author: Jonathan Katz 
@_subject: New result in predicate encryption: disjunction support 
This is actually pretty easy to do by, e.g., padding all valid messages with sufficiently-many 0s. Decryption with an incorrect key will result in something "random" that is unlikely to end with the requisite number of 0s (and so will be discarded).

@_date: 2008-05-05 22:15:31
@_author: Jonathan Katz 
@_subject: New result in predicate encryption: disjunction support 
Predicate encryption sounds very different from the work you are referencing above. (In particular, as we discuss in the paper, predicate encryption for equality tests is essentially identity-based encryption.) I refer you to the Introduction and Definition 2.1 of our paper, which should give a pretty good high-level overview.

@_date: 2008-10-24 08:51:41
@_author: Jonathan Katz 
@_subject: combining entropy 
[Moderator's note: top posting is not tasteful. --Perry]
I think it depends on what you mean by "N pools of entropy".
Are you assuming that one of these is sources is (pseudo)random, but you don't know which one? Are you assuming independence of these difference sources? If both these assumptions hold, then XOR will do the trick.
If your only assumption is that one of the sources has high min-entropy (but may not necessarily be uniform), or if the independence assumption does not hold, then you may need to use some form of randomness

@_date: 2008-10-27 18:53:38
@_author: Jonathan Katz 
@_subject: combining entropy 
If you are interested in something with a formal analysis, you should check out work on (single-source or multiple-source) extractors.

@_date: 2009-02-20 12:34:31
@_author: Jonathan Katz 
@_subject: Shamir secret sharing and information theoretic security 
The scheme is defined over a finite field *not* over the integers. When Shamir's scheme is run over a finite field, it is information theoretically secure.

@_date: 2009-02-23 15:47:33
@_author: Jonathan Katz 
@_subject: Shamir secret sharing and information theoretic security 
I'm not sure what is the motivation for all this. Shamir's scheme is supposed to be done over a finite field (or else, as was previously pointed out, there are issues with sampling a uniform element of the field). Since we have fields of size 2^k for all k, any bit-string can be encoded nicely in a finite field of appropriate size. (And very long strings can be broken into shorter chunks, each chunk being shared on its

@_date: 2009-03-02 19:04:54
@_author: Jonathan Katz 
@_subject: How to Share without Spilling the Beans 
I believe this is the paper describing the protocol in question:

@_date: 2009-05-22 11:06:41
@_author: Jonathan Katz 
@_subject: End-of-chapter questions for "Practical Cryptography"? 
Let me humbly suggest my own book: "Introduction to Modern Cryptography", co-authored with Y. Lindell. You may find it a bit theoretical for your taste, but it was written exactly to address the need for an introductory text covering modern cryptography. (And it covers some basic cryptanalysis as well.) See
      for further details.

@_date: 2009-11-11 10:57:04
@_author: Jonathan Katz 
@_subject: TLS break 
Anyone care to give a "layman's" explanation of the attack? The explanations I have seen assume a detailed knowledge of the way TLS/SSL handle re-negotiation, which is not something that is easy to come by without reading the RFC. (As opposed to the main protocol, where one can find textbook descriptions.)

@_date: 2009-10-05 12:43:03
@_author: Jonathan Katz 
@_subject: Question about Shamir secret sharing scheme 
Just to add two comments to what others have already said:
- You can use any finite field. In particular, if your secret is a bit string of length k you can use the field GF(2^k) to get share size equal to secret size. (Whereas if you work mod p you lose a bit.)
- As you describe the scheme above, note that you actually leak an upper-bound on the size of the secret (namely, it is at most p). The setup for Shamir secret sharing (and any other scheme, for that matter) assumes the range of the secret is public knowledge already.

@_date: 2010-04-22 14:40:07
@_author: Jonathan Katz 
@_subject: What's the state of the art in factorization? 
It is not known, and considered extremely unlikely, that P \neq NP implies symmetric-key cryptography, much less public-key cryptography.
The paper you cite reduces security to a hard-on-average problem, whereas all that P \neq NP guarantees is hardness in the worst case.

@_date: 2010-04-22 22:18:38
@_author: Jonathan Katz 
@_subject: What's the state of the art in factorization? 
As one of the authors of the above paper, I have an obvious interest in this thread. =)
While I don't know of any attack, the proof of security does not appear to be correct.
On the other hand, there is one published scheme that gives a slight improvement to our paper (it has fewer on-line computations): it is a paper by Chevallier-Mames in Crypto 2005 titled "An Efficient CDH-Based Signature Scheme with a Tight Security Reduction".

@_date: 2010-04-29 21:18:19
@_author: Jonathan Katz 
@_subject: What's the state of the art in digital signatures? Re: What's 
============================== START ==============================
Not to worst-case hardness of an NP-complete problem, no. Quite the contrary, there has been some body of work showing that a result of this sort is unlikely. (Though, as with all things related to complexity theory where our state of knowledge is so limited, such a statement must be taken wit ha grain of salt. In any case, such a result is well beyond anything we can currently prove.)
See above.

@_date: 2010-08-17 01:46:01
@_author: Jonathan Katz 
@_subject: 2048-bit RSA keys 
Many on the list may already know this, but I haven't seen it mentioned on this thread. The following paper (that will be presented at Crypto tomorrow!) is most relevant to this discussion:
   "Factorization of a 768-bit RSA modulus",

@_date: 2010-07-09 13:55:12
@_author: Jonathan Katz 
@_subject: Question w.r.t. AES-CBC IV 
CTR mode seems a better choice here. Without getting too technical, security of CTR mode holds as long as the IVs used are "fresh" whereas security of CBC mode requires IVs to be random.
In either case, a problem with a short IV (no matter what you do) is the possibility of IVs repeating. If you are picking 32-bit IVs at random, you expect a repeat after only (roughly) 2^16 encryptions (which is not very

@_date: 2010-07-09 14:23:18
@_author: Jonathan Katz 
@_subject: A Fault Attack Construction Based On Rijmen's Chosen-Text 
Err...I read that paper by Rijmen as a bit of a joke. I think he was poking fun at some of these unrealistic attack models.

@_date: 2010-07-31 21:42:16
@_author: Jonathan Katz 
@_subject: Is this the first ever practically-deployed use of a threshold 
============================== START ==============================
This is "just" Shamir secret sharing, not "real" threshold cryptography. (In a threshold cryptosystem, the shares would be used in a protocol to perform the desired cryptographic operation [e.g., signing] without ever reconstructing the real secret.) Has real threshold cryptography never been used anywhere?

@_date: 2010-03-22 08:56:49
@_author: Jonathan Katz 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
[Moderator's Note: please don't top post... --Perry]
Sounds like a bad idea -- at a minimum, your encryption will be What are you actually trying to achieve? Usually once you understand that, you can find a protocol solving your problem already in the crypto

@_date: 2010-03-22 09:25:03
@_author: Jonathan Katz 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
[Moderator's Note: Please please don't top post. --Perry]
That paper was from 1980. A few things have changed since then. =)
In any case, my point still stands: what you actually want is some e-cash system with some special properties. Commutative encryption is neither necessary nor (probably) sufficient for what you want. Have you at least looked at the literature (which must be well over 100 papers) on e-cash?

@_date: 2011-08-08 21:46:14
@_author: Jonathan Katz 
@_subject: [Cryptography] Homomorphic encryption prototype by microsoft 
Here's the accompanying technical article: They only implemented a "somewhat-homomorphic" encryption scheme, though.

@_date: 2013-12-16 15:26:44
@_author: Jonathan Katz 
@_subject: [Cryptography] A new digital signature scheme based on the RSA 
Your scheme is similar to several schemes in the literature based on the so-called *strong RSA* assumption (as compared to the [regular] RSA assumption). See, for example:
      (But make sure to also check google scholar for the followup work.)
Note further that there is no real reason to make your base 'v' depend on the message; you may as well have the signer fix it as part of their public key once and for all.

@_date: 2013-09-11 14:45:10
@_author: Jonathan Katz 
@_subject: [Cryptography] Books on modern cryptanalysis 
Really depends what you mean by "attacking"; there are attacks at the protocol level (e.g., padding-oracle attacks), at the crypto level (e.g., differential cryptanalysis), and at the physical level (e.g., side-channel As a general introduction to modern crypto that covers the first two categories a bit, I recommend "Introduction to Modern Cryptography" by myself and Y. Lindell (soon to come out with a 2nd edition containing even more attacks!).
For block-cipher cryptanalysis, I have been very impressed by the material in "The Block Cipher Companion" by Knudsen and Robshaw.

@_date: 2014-08-03 17:43:53
@_author: Jonathan Katz 
@_subject: [Cryptography] The original "public" public key algorithm 
This sounds similar to (though worse than) "Merkle puzzles," which date
back to the '70s:

@_date: 2014-01-09 21:12:59
@_author: Jonathan Katz 
@_subject: [Cryptography] Advances in homomorphic encryption 
It's important to distinguish between homomorphic encryption and *fully*
homomorphic encryption. (Unfortunately, even the academic literature has
begun using the terms carelessly.) The former goes back ~35 years, can be
roughly as efficient as standard public-key crypto, but only supports
(essentially) *addition* of encrypted values. The latter goes back ~5
years, is orders of magnitude less efficient than standard public-key
crypto, and supports arbitrary computations on encrypted data.
cryptdb uses homomorphic encryption (as part of a larger system that leaks
more information that "pure" HE would, but that is an irrelevant tangent).

@_date: 2014-06-13 16:18:19
@_author: Jonathan Katz 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
Getting a bit off track here, but I don't think this claim true for at
least two reasons:
First, the issue with being a group is that it implies that *double* or
*triple*-key encryption does not yield the expected level security.
Vulnerability of *single*-key encryption to a meet-in-the-middle attack is,
as far as I know, specific to DES.
Second, vulnerability to a meet-in-the-middle attack just means that the
algorithm does not achieve security equal to its bit-length; it does not
mean the algorithm is not secure. (Note that public-key algorithms do not
achieve security equal to their bit-length either...)

@_date: 2014-05-21 22:11:05
@_author: Jonathan Katz 
@_subject: [Cryptography] New attacks on discrete logs? 
Best I can tell, the top article is talking about this paper:
  but it's not clear to me what "family of algorithms presented as candidates
for the next generation of encryption keys" the article is referring to.
The second article is talking about this, earlier paper:

@_date: 2014-11-19 16:03:06
@_author: Jonathan Katz 
@_subject: [Cryptography] Where should I start with cryptography? 
Actually, do yourself a favor and buy [1b] instead of [1]. =)

@_date: 2014-10-02 19:50:17
@_author: Jonathan Katz 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
There are several issues. Most obvious is that your hash is homomorphic,
  digest(B_0, B_1) ^ digest(B'_0, B_1) ^ digest(B_0, B'_1) = digest(B'_0,
Also, collisions in your hash function can be found in faster than
square-root time using Wagner's generalized birthday attack.

@_date: 2014-10-03 13:25:34
@_author: Jonathan Katz 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Yes, I was. By digest(., .), I meant to apply your scheme.

@_date: 2014-10-13 18:11:32
@_author: Jonathan Katz 
@_subject: [Cryptography] Secure parallel hash, with constant time update 
It is well known (going back to the '90s) that the following function is
collision resistant if the discrete log problem is hard:
  public constants: g_1, g_2, ..., g_n, all elements mod p
  input: x_1 | x_2 | x_3 | ... | x_n
  output: \prod_i g_i^{x_i} mod p
A proof for the case of n=2 is in my cryptography textbook. Note that this
function is also updateable in constant time.
If we model H as a random oracle, we can modify the above to:
  input: x_1 | ... | x_n
  output: \prod_i H(i)^{x_i} mod p
This has also been suggested in the literature.
You are suggesting instead to look at the construction
   input: x_1 | x_2 | ... | x_n
   output: \prod_i H(i | x_i) mod p
This has the advantage of being more efficient than the above
constructions. This, too, is secure -- but was already proposed in the
paper "A New Paradigm for collision-free hashing: Incrementality at reduced
cost" available here: I'm not sure if that's good or bad news vis-a-vis your constructions. =)
an attacker found two sets of message blocks that hash to the same digest,
then this hash would be broken.
difficult as finding the discrete log of y.

@_date: 2014-10-13 19:23:24
@_author: Jonathan Katz 
@_subject: [Cryptography] factoring small(ish) numbers 
I'm curious if anyone can point me to references that would indicate values
of n for which n-bit numbers can be factored "easily."
One can debate what "easily" means, but for my purposes I am thinking of
something where (1) the factoring is done on a single, standard PC, (2) in
less than a month, using (3) code that is either readily available or could
be written by a talented undergraduate CS student.
I am aware of the RSA factoring challenges, but those are solved by large,
distributed efforts run by academics using special-purpose setups and
taking much more than 1 month.
Thanks in advance for any pointers.

@_date: 2014-09-04 21:23:22
@_author: Jonathan Katz 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Once you allow known-plaintext attacks, symmetric-key crypto is also in NP.
Even in the ciphertext-only setting, if the underlying plaintext has any
"structure" that can be recognized by an algorithm in P (and the plaintext
is long enough compared to the key length), symmetric-key crypto is in NP.
So basically if P=NP, you are left with the one-time pad and variants.

@_date: 2014-09-05 15:55:57
@_author: Jonathan Katz 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Let's try to clarify things in an attempt to put this to rest.
Assumption 1: We are talking about asymptotic security (as a function of key length), not concrete security. Why? Because if you care about concrete security, then P and NP are irrelevant since, as a previous person noted, any scheme with some fixed key length k can be broken in *constant* time 2^k.
Let's let n -- a parameter! -- denote the length of the key.
Assumption 2: Encryption runs in time polynomial in n.
You can question this assumption, and in fact it might be meaningful to allow encryption to run in time  n^{log n}. But this is the assumption I am making.
Assumption 3: You want security to hold against all polynomial-time You can question this assumption, and in fact it might be meaningful to only require security for adversaries running in time n^10 (but not those running in time n^11). But this is the assumption I am making.
We must also fix a notion of security. For concreteness, let's use the notion of CPA-security (i.e., security against chosen-plaintext attacks); see the Katz-Lindell textbook for formal definitions. CPA-security, as defined there, already incorporates Assumptions 1, 2, and 3.
Now, what I claim is:
If P=NP, then no public-key or symmetric-key encryption scheme is

@_date: 2014-09-15 11:41:23
@_author: Jonathan Katz 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
There is no cryptosystem whose security (in any standard sense) can be reduced to an NP-hard problem. The issue, intuitively, is that NP-hardness is a *worst-case* notion, whereas we want cryptosystems to be hard *on the

@_date: 2014-09-29 17:50:55
@_author: Jonathan Katz 
@_subject: [Cryptography] Based on Even Mansour 
There seems to be a misunderstanding about what Even-Mansour show.
The Even-Mansour analysis shows that given access to a public (unkeyed),
truly random permutation, it is possible to construct a secure (keyed)
block cipher.
In practice, however, we don't have public random permutations. Instead, we
build block ciphers from "somewhat random-looking" permutations that I will
call round functions. Because these are far from random, block ciphers in
practice do not use a single round function, but instead use many rounds.

@_date: 2015-01-09 12:48:37
@_author: Jonathan Katz 
@_subject: [Cryptography] Compression before encryption? 
See  for some of the dangers of
doing compression before encryption.

@_date: 2015-01-14 12:52:28
@_author: Jonathan Katz 
@_subject: [Cryptography] Summary: compression before encryption 
Slightly tangential, but list members may be interested to know that
in some cases, and under certain assumptions, encrypted data *can* be
compressed; see:

@_date: 2015-07-08 12:53:22
@_author: Jonathan Katz 
@_subject: [Cryptography] Is this parallel DPL solver for elliptic curves 
Parallel algorithms for the discrete logarithm problem have been
looked at before. A good starting point is the paper "Parallel
collision search with cryptanalytic applications" available here:

@_date: 2015-06-02 06:33:56
@_author: Jonathan Katz 
@_subject: [Cryptography] If diffusion is perfect how much confusion do 
One round of your construction is the Even-Mansour cipher, which can
indeed be proven secure. Multi-round versions have also been analyzed
more recently.

@_date: 2015-11-12 12:12:03
@_author: Jonathan Katz 
@_subject: [Cryptography] Does anyone use ZKP systems based on graph 
I doubt it.
It's not clear that ZKPs based on GI ever had any benefit as compared
to those based on factoring or discrete log. Moreover, it turns out
that even before Babi's result, there were several heuristic
algorithms that solved GI very quickly. See also this recent post:
  which explains that it was never known how to generate instances of GI
that were believed to be hard in the first place.
Are there any?

@_date: 2016-04-27 15:50:17
@_author: Jonathan Katz 
@_subject: [Cryptography] Pragmatic, 
There has been a whole line of work on this in the past few years;
trying googling for "searchable encryption". and "order-preserving
encryption." You may also look specifically for CryptoDB. Beware,
though, that the jury is still out on the usefulness of these schemes.

@_date: 2016-07-01 18:27:32
@_author: Jonathan Katz 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
The discussion is going in circles; it just illustrates the futility
of discussing such things without formal definitions in place.
Here is another "fair" contract signing protocol that is simpler and
seems to achieve the same things you claim:
0. Let C be the contract to sign. Let C' be the contract plus the
statement "this contract is valid only if signed by Alice and Bob."
1. Alice signs C' and sends the signature to Bob.
2. Bob signs C' and sends the signature to Alice.
Note that if Bob does not follow step 2, then he does not have a valid
contract. (Since it is valid only if signed by Alice and Bob.) On the
other hand, once Bob follows step 2, both parties are bound to the
Don't bother telling me why this protocol is bad; I will agree with you.
Please move this discussion off list.

@_date: 2016-03-13 00:51:01
@_author: Jonathan Katz 
@_subject: [Cryptography] Is Non-interactive Zero Knowledge Proof an 
Zero-knowledge proofs, zero-knowledge proofs of knowledge, and
non-interactive zero-knowledge proofs all have formal definitions; see
Goldreich's book or my own lectures notes
( or papers in
the academic literature.
Although signatures can be constructed from zero-knowledge proofs,
signatures themselves are not zero-knowledge proofs and signatures can
be constructed in other ways as well.
The terms tend to get mixed up, misused, or abused outside an academic
context. If that happens, all bets are off. =)

@_date: 2016-03-27 11:58:10
@_author: Jonathan Katz 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
For the record, it's worth noting that the above attack applies to
(public-key) encrypt-then-sign, but not to (private-key)

@_date: 2017-01-15 17:45:26
@_author: Jonathan Katz 
@_subject: [Cryptography] ZK meeting scheduling protocol? 
I think what you actually want to look at is secure multi-party
computation, which is related to (but not the same as) zero-knowledge
The problem doesn't really have anything to do with the Byzantine
generals problem, multiparty key exchange, or the dining
cryptographers problem.

@_date: 2018-08-08 13:37:21
@_author: Jonathan Katz 
@_subject: [Cryptography] Perfect Integrity? 
I have to say, I find some of the discussion surrounding this question
rather silly.
We need to first rigorously define a notion of "information-theoretic
message authentication" before asking whether it is achievable. (You
may or may not like the definition I propose, but arguing without any
definition in place is just a waste of energy. And you are free to
suggest your own definition as long as you can express it formally so
I know precisely what you mean.)
It turns out that information-theoretic message authentication codes
(MACs) have a long history, dating back at least to the 1970s.
One standard definition (covered in "Introduction to Modern
Cryptography, 2nd edition" by Katz and Lindell, Section 4.6) considers
the following experiment:
- Sender and receiver share a uniform key, unknown to the attacker.
- Attacker chooses a message m in some defined message space and the
sender authenticates it. The attacker gets to see the resulting
authentication tag.
- Attacker "wins" if it can output a different message m' along with
an authentication tag that will cause the receiver to accept the pair
as valid.
Then an \epsilon-secure (information-theoretic) MAC is one in which no
attacker can win with probability better than \epsilon.
Now you can start asking all kinds of questions:
- Does a 0-secure MAC exist? (It is easy to see that the answer is no.)
- Does an \epsilon-secure MAC exist for every \epsilon > 0? (Not as
easy to see, but well known that the answer is yes.)
- What is the minimum tag length required in order to achieve a
2^{-n}-secure MAC? (It is easy to see that the tag must have length at
least n.)
- What are the various tradeoffs between size of the message space,
length of the tag, length of the key, and achievable \epsilon? (These
are all research questions, most or all of which have been studied
over the last 4+ decades.)

@_date: 2018-12-01 22:16:50
@_author: Jonathan Katz 
@_subject: [Cryptography] What if Responsible Encryption Back-Doors Were 
It seems to me that part of the problem with this debate on the side of
those who argue for "no backdoors" is that they refuse to actually engage
with the arguments of the other side. The above seems to be a good example
of this.
FWIW, I don't know who you count as an "actual crypto person" but I spoke
to two people who publish regularly at the Crypto conference who attended.
And I would count Josh Benaloh as an "actual crypto person" as well,
whether I agree with his opinions or not. And since the workshop was
co-located with the Crypto conference, it was open to anyone who wanted to
If you read Benaloh's post, you will see that he comes out firmly against
law-enforcement access.
