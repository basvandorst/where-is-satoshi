
@_date: 2001-04-06 18:59:14
@_author: Bill Frantz 
@_subject: fyi: Content Protection for Recordable Media -- Jeffrey B.    
I went to the talk.  The basic scheme is to build a system where everything
in it is covered by intellectual property law (patents), so legal users can
be bound by license agreement to implement things a certain way, and not
implement other things.  Unlicensed uses will be confronted in the courts,
and sites distributing them will be ordered to remove the offending items.
Following this philosophy, they have their own cypher, vetted by in house
cryptographers, and outside consultants.
It looks like there is at least one interesting sociological attack on it.
Cheers - Bill

@_date: 2001-07-02 16:44:55
@_author: Bill Frantz 
@_subject: Crypographically Strong Software Distribution HOWTO 
I have another quibble with what is a really good start on a HOWTO.
You say in your section on anonymous software development groups,
"The identity of the maintainer is established through the possession
of the secret key of a project key pair, therefore possession of the
secret key could be presented as proof in a courtroom as evidence
that an individual is a maintainer or developer in a Guerrilla
development project. This evidence would be very difficult to refute
in court. The only possible argument that could be used to deny
authorship would be to state the the secret key was stolen. However,
              typeo =======> that
the theft of a secret key suggest other felony crimes where
committed. To a lesser extent, possession of the revocation
certificate has similar ramifications."
If the secret key and/or revocation certificate was widely distributed, say
by being posted to the cypherpunks mailing list, it seems unlikely that
mere possession would constitute strong proof of membership in the
development group.
If the key becomes widely distributed, the development group must
immediately take steps to establish the reputation of a new key.  There
might be an interesting scramble between the development group, and other
group(s) wishing to obtain the reputation of the development group.
Cheers - Bill

@_date: 2001-06-25 13:06:52
@_author: Bill Frantz 
@_subject: crypto flaw in secure mail standards 
I think Greg is probably right when it comes to email messages.  The places
that attacks like this worry me the most are in program-to-program
messages.  The "Cross-Site Request Forgeries" confused deputy attack
( described in
 and
, seem a
place where two-way SSL cryptographic authentication can make a bad
situation worse, because more value is likely to be entrusted to the
In this attack, a user's browser is tricked into sending certain URLs which
exercise authority without the user's permission.  The specific URLs can be
hidden behind redirect requests, making it difficult to recognize that the
attack is taking place.
Cheers - Bill

@_date: 2001-11-27 13:46:23
@_author: Bill Frantz 
@_subject: FBI-virus software cracks encryption wall 
There are a lot of interesting parallels between biological systems and
computer systems.  One of them is that a virus can spread like wildfire in
a monoculture.  Diversity is good for both kinds of systems.
Cheers - Bill

@_date: 2001-10-17 11:34:18
@_author: Bill Frantz 
@_subject: limits of watermarking (Re: First Steganographic Image in the 
Probably to maximize profit.  Look at the DVD encryption.  Encode the media
differently for different markets, thereby allowing you to sell at higher
prices in rich countries while still being able to make a modest profit at
lower prices in poorer countries.
I don't see much use for individually watermarked media.  It is too easy to
collect several copies and find the watermark with a diff operation.
Cheers - Bill

@_date: 2001-09-13 16:27:52
@_author: Bill Frantz 
@_subject: Congress mulls crypto restrictions in response to attacks 
So the honorable gentlemen are proposing unilateral crypto disarmament?  Or
perhaps a world where many governments can read the business plans and
strategies of US companies?  Or perhaps a world where the terrorists
themselves can read the travel plans of their targets?

@_date: 2001-09-19 11:42:28
@_author: Bill Frantz 
@_subject: chip-level randomness? 
If I am generating one time pads, I would certainly prefer /dev/random
output to /dev/urandom output.  There is much less algorithm exposure.
(Although I do still have to worry about the whitening and combining
Cheers - Bill

@_date: 2001-09-20 00:48:46
@_author: Bill Frantz 
@_subject: chip-level randomness? 
Does anyone know what algorithm the "whitening" uses?  If you apply FIPS
140 to it's output, are you likely to catch the most common failure modes?
(All ones, All zeroes, line frequency dependances?)
Also when reseeding /dev/random, be careful to prevent continuation
attacks.  Gather enough entropy in a private buffer before reseeding to
prevent someone who has compromised the state of /dev/random from being
able to calculate the new state by exhaustive search.  (I would say 80+
bits would be enough.)
Cheers - Bill

@_date: 2001-09-20 14:26:20
@_author: Bill Frantz 
@_subject: Field slide attacks and how to avoid them. 
I found some stuff searching for draft-paajarvi-xml-spki-cert-00.txt
in Google's cache:
Slides about the proposal:
The draft itself:
Carl Ellison's site (cached above) also has it:

@_date: 2001-09-24 14:26:23
@_author: Bill Frantz 
@_subject: New encryption technology closes WLAN security loopholes 
Or in other words, the first requirement for perimeter security is a perimeter.
Cheers - Bill

@_date: 2001-09-24 14:31:47
@_author: Bill Frantz 
@_subject: [FYI] Did Encryption Empower These Terrorists? 
It seems to me that because of the $50 liability limit under US law, most
of the risk is carried by the credit card issuers.  They are also in a
position to require proper security by contract with the merchant.
Cheers - Bill

@_date: 2001-09-25 12:08:01
@_author: Bill Frantz 
@_subject: [FYI] Did Encryption Empower These Terrorists? 
FWIW the merchant that accepts the fraudulent transaction is probably not
the one running the web site from which the credit card number was stolen.
Cheers - Bill

@_date: 2002-08-23 08:35:16
@_author: Bill Frantz 
@_subject: Privacy-enhancing uses for TCPA 
Note that if the user can modify the wallet, a "fat, dumb, and happy"
implementation may be vulnerable to the following attacks.
Attack 1:
(1) Withdraw $0.01 from the bank.
(2) Change a random bit in the encrypted wallet.  (Picking the bit to
change will be easier if the storage format in known.)
(3) Fire up the application as see how much money you have.
Attack 2:
(1) Withdraw many $$$ from the bank.
(2) Copy the wallet.
(3) Deposit the $$$ back in the bank.
(4) Restore the wallet using the copy.
While there are certainly ways to notice modifications to the wallet, and
prevent the replay attack, they result in considerable additional
complexity for what was a very simple implementation
Cheers - Bill

@_date: 2002-08-30 11:16:49
@_author: Bill Frantz 
@_subject: Palladium and malware 
All general purpose computers require a way to move data space to code
space to support compilation.  Even if you don't allow compilation, most
modern systems have enough different powerful scripting languages that
interpretation is sufficient to support viruses.
Cheers - Bill

@_date: 2002-12-08 21:18:10
@_author: Bill Frantz 
@_subject: PGPfreeware 8.0: Not so good news for crypto newcomers 
My wife is using GPG on OS X.  She has integrated with Mail in GUI mode
using a package called PGPMail.  She says, "It seems to be working OK."  (I
remember spending some time helping her get it up.  Knowledge of Unix shell
Cheers - Bill

@_date: 2002-02-04 12:41:43
@_author: Bill Frantz 
@_subject: Welome to the Internet, here's your private key 
It may depend on the public key system you are using.  Where you have to
search for numbers which have certain mathematical properties (like with
RSA), then you can indeed use a bunch of CPU.  For systems like DSA, where
the private key is in essence a random number, there is not searching, and
key generation is a lot faster.
Cheers - Bill

@_date: 2002-02-04 16:24:30
@_author: Bill Frantz 
@_subject: Welome to the Internet, here's your private key 
The quality of random numbers available to a smart card is a very important
point.  Unless you can trust the external source of random numbers, DSA
signatures (and elliptic curve DSA) don't strike me as very secure.  In
Applied Cryptography II, Schneier says, "If Eve ever recovers a K that
Alice used to sign a message, perhaps by exploiting some properties of the
random-number generator that generated K, she can recover Alice's private
key, X.  If Eve ever gets two messages signed using the same K, even if she
doesn't know what it is, she can recover X."  I can easily imagine a POS
terminal hacked to record both the random number, and the signature, as
part of a card cloning scam.
On the other hand, building a good source of random numbers into the card
doesn't strike me as being that difficult.  (Although running a FIPS-140
test every time a signature is generated (card is powered up), might be a
performance problem.)
It is probably worth examining the protocols for bad random number attacks
on the nonces.
Cheers - Bill

@_date: 2002-02-05 11:16:40
@_author: Bill Frantz 
@_subject: Welome to the Internet, here's your private key 
I expect you could initialize the random data in that memory during
manufacture with little loss of real security.  (If you are concerned about
the card's manufacturer, then you have bigger problems.  If anyone does,
the manufacturer has the necessary equipment to extract data from secret
parts of the card, install Trojans etc.)
Note that if the card generates its own keys, it needs the same kind of
memory to store the keys as it needs to store a random seed.
Cheers - Bill

@_date: 2002-02-05 11:25:09
@_author: Bill Frantz 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
IMHO, interactive protocols (e.g. certain modes of SSL/TLS) which are
subject to this attack should be retired.  Non-interactive protocols (e.g.
PGP email), are much more difficult to fix.
Cheers - Bill

@_date: 2002-01-08 11:33:17
@_author: Bill Frantz 
@_subject: On ISPs Not Filtering Viruses 
There is one case where it makes sense for people to have their ISPs filter
packets, and this is in response to a denial of service (DOS) attack.  If
you are connected to your ISP thru a link which is slow compared with the
main internet (e.g. connected thru DSL or a T1), then a DOS attack can
saturate the link between you and your ISP.  The only solution I know of is
to have your ISP filter the offending packets before they hit your link.  A
number of people have used this technique to remain "on the air" in the
midst of a DOS attack.
Cheers - Bill

@_date: 2002-01-22 13:41:18
@_author: Bill Frantz 
@_subject: I-P: WHY I LOVE BIOMETRICS BY DOROTHY E. DENNING 
At 3:50 PM -0800 1/19/02, R. A. Hettinga quoted Dorothy Denning:
It seems that this situation is true only if you can trust the device
reading the biometric data.  In the case an ATM, or a building entry
system, this trust is reasonable.
However, encryption will buy you nothing if the biometric reading device is
compromised or faked.  If an attacker can take the freshness challenge from
the other end of the connection, along with your public biometric data (say
an iris pattern), then it can simulate an uncompromised device in
responding with your "credentials".
Now there may be enough physical security in many places to be able to
place reasonable trust in the devices which are installed there.  However,
it strains my belief that we will be able to trust the devices in every
Internet Cafe in the world.
Unless your smart card has a fingerprint reader, it must trust the finger
print reader it uses.  An attacker can feed your smart card your public
finger print data, responding to any freshness challenge your card may use,
and get your card to authenticate someone else as you.  It seems to me in
this scheme, you must protect your smart card in much the same manner as
you protect your credit cards.  (Note that the smart card scheme is better
than the credit card because, the secret data is not transmitted over a
network like the credit card number.  Modulo breaking the smart card
protection, an attacker will need physical access to the card.)
Cheers - Bill

@_date: 2002-01-23 13:44:03
@_author: Bill Frantz 
@_subject: biometrics 
I must admit that I worry about the ATMs in places like bars.  These
machines do not seem to have a lot of physical protection.
Cheers - Bill

@_date: 2002-01-26 18:49:55
@_author: Bill Frantz 
@_subject: [ISN] What Billg's new security effort will cost 
A serious attempt to address the many security flaws in current systems
will result in major changes to Microsoft software or its culture.
Microsoft has always been a features oriented company.  In the early days,
this orientation served the company well.  Microsoft achieved its early
successes by selling software to the corporate market.  This market is
characterized by a desire to have company wide standard software packages
to minimize support costs, and maximize the ability to transfer information.
When a company was deciding which package is to be anointed the corporate
standard, one of the measures used was a feature comparison chart.  The
chart was used to ensure the chosen package has the features needed by all
the users in the company.  Unfortunately, in general the people who
selected the package did not know which features would be required in
obscure corners of their company.  As a result, they tended to pick the
package with the most features.
Being the "firstest with the mostest" provider established Microsoft
products such as Windows, Word, and Excel in the corporate market.  When
people purchased computers for their homes, they tended to buy what they
knew from work, and these packages then migrated to the home.
Now Microsoft is attempting to make security a new feature of their
software.  However, security is different from all the other features they
have implemented.  It is not about what you can do, which can be
implemented and demonstrated, but about what you can't do, which is usually
the absence of implementation, and ducedly hard to demonstrate.  It is not
about showing there is a path to achieve a result, but about showing that
no such path exists.
One of the things that makes security assurance much harder is having a
complex set of features, which interact to effect the security of the
system.  There are many more possible paths, and much more code where a
flaw can bring the whole system down.
Microsoft can chose to simplify by removing features, which will require a
tidal shift in their culture, or they can make drastic changes in their
implementation strategy in an attempt to maintain their feature set.  My
bet is they try to maintain their feature set.
The first step is to attack the easy problems.  Bruce Schneier and Adam
Shostack, in their paper, "Results, Not Resolution, A guide to judging
Microsoft's security progress" came up with an excellent list of areas to
address first.  Presumably Microsoft will also address the simple coding
problems, like buffer overruns, with improved languages or development
procedures.  Even with these changes, they will still have a system where a
bug in one feature can still break the security of the entire system.
To maintain its feature set, and have a reasonable degree of security,
Microsoft will have to adopt architectures where bugs in most of the
features can not effect security.  These architectures will need much
smaller and more flexible security compartments than systems such as
Windows and Unix provide.  These compartments need to isolate the
privileges given to feature implementations from those given to the users
and those given to other parts of the application system.  This kind of
compartment supports the principle of least privilege, which is the best
path to limit the effect of bugs.
Cheers - Bill

@_date: 2002-01-26 22:17:38
@_author: Bill Frantz 
@_subject: Results, Not Resolutions 
At 7:42 PM -0800 1/25/02, R. A. Hettinga quoted Schneier and Shostack:
Well, the line between code and data is fuzzier than that.  That 7 bit
ASCII email is properly thought of as a series of instructions for a text
rendering engine which is implemented in software on modern machines.  If
there is a bug in that rendering software, then it may be possible to
design a sequence of text which executes arbitrary code on the receiving
Admittedly ASCII text isn't a very powerful instruction set, and rendering
code tends to be well debugged, but deciding what is instructions, and what
is text is not as easy as it might seem.  I believe that certain JPEG
renderers have been found to have exploitable flaws.
There is really no substitute for limiting the authority of code which
processes potentially hostile input, such as email and web pages, so that
the consequences of flaws are limited.  One way to limit authority in
current systems is to use an operating system that provides a measure of
real security between users, and then have an account which is only used
for email, web surfing etc.
Cheers - Bill

@_date: 2002-01-29 11:45:41
@_author: Bill Frantz 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
DSA shares this nice characteristic since the secret key is a (not quite)
160 bit number.  (Some EC libraries implement DSA-style signatures in
elliptic fields.)
Cheers - Bill

@_date: 2002-01-29 13:14:27
@_author: Bill Frantz 
@_subject: biometrics 
Or to state it another way.  These cards attempt to use two factor
authentication, what you have (the card) and what you know (the PIN).  When
a user writes the PIN on the card, it becomes one factor authentication.
Almost anything that returns it to being two factor security would be an
improvement.  (Biometrics offers the possibility of 3 factor authentication.
What would be really nice is to be able to have the same PIN/password for
everything.  With frequent use, forgetting it would be less of a problem,
as would the temptation to write it down.  However, such a system would
require that the PIN/password be kept secret from the verifier (including
possibly untrusted hardware/software used to enter it.
Cheers - Bill

@_date: 2002-01-30 17:25:51
@_author: Bill Frantz 
@_subject: biometrics 
It may be that we gain more from having this data not written down than we
lose from the "compromise one, compromise all" problem.  For things like
credit/debit/ATM cards, you probably don't increase the risk too much by
using the same PIN for all of them.  I admit that I use the same password
for all those web sites that simply must have a username and password for
their own reasons, and not to secure anything of mine.  For web sites like
Amazon that want to remember a credit card number for you, I generally
choose a password that even I can't remember (and paste it into both the
entry and verification windows).  This means I must set up a new account
for every purchase, but that doesn't happen very often.
Lets assume a PDA/smart card with a fingerprint reader for the sake of
argument.  The device keeps one or more secret keys used to sign
challenges, and only signs them if the fingerprint has been recently
verified.  (Perhaps using the infrared link, you put it near the point of
sale computer or you web browsing computer.  The computer sends it the
challenge and an indication of which public key will be used to verify the
authorization.  The device shows you your name for the keypair being used,
and asks you to press the fingerprint reader to authorize (or click NO to
reject authorization).)
If we accept Dr. Denning's criterion that the biometric data must be
public, anyone who steals this device can, with enough work, fool it into
accepting a false finger print.  Even with this weakness, such a device is
more secure than the current credit card system.
If instead of using biometric identity, we use some kind of pass
phrase/PIN, we introduce the risk of shoulder surfing, and brute force
attacks against the hash(salt || PIN) stored in the device.
It may be easier to just extract the signing keys from the device rather
than perform the above attacks.  If we can build the device so it resists
attacks long enough for the user to notice that it is missing, and notify
the verifiers, then the above attacks become less of a problem.
Cheers - Bill

@_date: 2002-06-20 23:08:37
@_author: Bill Frantz 
@_subject: Shortcut digital signature verification failure 
I have been thinking about how to limit denial of service attacks on a
server which will have to verify signatures on certain transactions.  It
seems that an attacker can just send random (or even not so random) data
for the signature and force the server to perform extensive processing just
to reject the transaction.
If there is a digital signature algorithm which has the property that most
invalid signatures can be detected with a small amount of processing, then
I can force the attacker to start expending his CPU to present signatures
which will cause my server to expend it's CPU.  This might result in a
better balance between the resources needed by the attacker and those
needed by the server.
Cheers - Bill

@_date: 2002-06-21 19:19:28
@_author: Bill Frantz 
@_subject: Recommended key sizes and lifespans 
I have been reading a draft of, "Key Management Guideline", from NIST
describing key management requirements for non-classified, but confidential
government information.  When complete, it is expected to become a FIPS.
While the guidence in it is subject to change, I found the recommendations
for key sizes and lifetimes interesting.
They define equivalent strength of algorithms in terms of a symetric
algorithm with no known attacks better than brute force.  DES is 56 bits,
Triple DES (with three different keys) (TDES) is 112 bits, AES is 128, 192
or 256 bits.
Secure hashes are defined as having a strength equal to half the bit-length
of their output.  So SHA1 is 80 bits, SHA-256 is 128 bits, etc. for SHA-384
and SHA-512.
Algorithms based on the descrete log problem on integer fields (DSA, Diffie
Hellman, MQV) are based on the size of the modulus and the size of the
private key.  The equivalents are:
modulus  private   symetric
           key    equilavent
 1024      160        80
 2048      224       112
 3072      256       128
 7680      384       192
15360      512       256
RSA is defined in terms of it's modulus size.  The equilavents are the same
as for DSA etc. in the above table.
Algorithms based on the descrete log problem in elliptic curve files
(ECDSA, EC Diffie Hellman etc.) are defined in terms of the base point G of
the curve.  This number is commonly considered to be the key size of the
curve   symetric
size   equilavent
160        80
224       112
256       128
384       192
512       256
The document then goes on to recommend key sizes for information which must
be protected past certain dates in the future:
   date   symetric
            size
 now-2015    80
2016-2035   112
2036-       128
For E, the weakest part of the system is the 1024 bit Diffie-Hellman key
agreement, the use of SHA1, and the use of DSA-1024.  We should consider
that users of E with long-term data confidentality requirements will need
bigger keys.
Cheers - Bill

@_date: 2002-06-22 11:24:07
@_author: Bill Frantz 
@_subject: Shortcut digital signature verification failure 
I had already thought of this approach, but wanted to add to it a CPU limit
on the client end.  Hash cash with a server provided problem seems a good
approach there.
Cheers - Bill

@_date: 2002-03-29 13:49:30
@_author: Bill Frantz 
@_subject: ciphersaber-2 human memorable test vectors 
If this issue seems to be a problem for a particular cypher, there are a
couple of ways to try to solve it:
* Compress out the eighth bit (requiring 10 characters for a 64 bit block
cypher instead of 8).
* Remember a pattern of high order bits.  Something like 11110000 would be
relatively easy to remember, and would help mitigate signed vs. unsigned
number problems on 32 bit machines.
Cheers - Bill

@_date: 2002-11-05 22:24:41
@_author: Bill Frantz 
@_subject: patent free(?) anonymous credential system pre-print 
The classic example is Arthur C. Clarke's invention of the communication
satellite, published in Wireless World in 1945.  Never mind that the
rockets to launch such satellites were not available until the 1960s.
Cheers - Bill

@_date: 2002-11-08 17:40:15
@_author: Bill Frantz 
@_subject: Did you *really* zeroize that key? 
There is a common example of this corner case where the memory is paged.
The page containing the key is swapped out, then it is read back in and the
key is overwritten, and then the page is deallocated.  Many OSs will not
zero the disk copy of the key.
Crypto coders have discussed many kludges to ensure that keys are not
swapped out, but they are all quite system specific.  Since the problem we
were trying to solve is different environments producing different results,
I don't feel we are any closer to safe, portable code.
Cheers - Bill

@_date: 2002-10-02 14:37:28
@_author: Bill Frantz 
@_subject: Gaelic Code Talkers 
While vacationing in Scotland this summer I had a conversation with a
gentleman who said that the British had used Scottish Gaelic speakers as
"code talkers" during World War II.  He added that they were not used in
the European theatre, as there were too many Irish Gaelic speakers who
sympathized with the Axis.
A quick glance at Kahn didn't turn up an information on these code talkers.
Has anyone else heard anything about it?
Cheers - Bill

@_date: 2002-10-15 20:42:02
@_author: Bill Frantz 
@_subject: [Bruce Schneier <schneier@counterpane.com>] CRYPTO-GRAM, 
Does anyone run a service that will check an IP address for open ports?
(I'd like to test my firewall.)
Cheers - Bill
[Moderator's note: I'm not aware of one -- it would doubtless be
 abused -- but just running nmap against the machine should work
 fine. That's what I do. --Perry]

@_date: 2002-09-03 19:54:10
@_author: Bill Frantz 
@_subject: Palladium and malware 
Usually, but not always.  Just-in-time compilation systems take interpreted
code sequences and compile it, in RAM, to machine instructions.  A number
of Java virtual machines make use of this technique.  More relevant, it is
also applicable to some of the Microsoft languages.
Well, some vendors might want to protect their scripts.  Just because a
program is written in an interpreted language instead of a compiled
language doesn't mean that vendors don't want to protect their code.  There
is an active market in Java obfuscators for just this reason.
Cheers - Bill

@_date: 2002-09-17 10:32:09
@_author: Bill Frantz 
@_subject: Cryptogram: Palladium Only for DRM 
The KeyKOS work  shows an approach to
using existing hardware protection (in the case of KeyKOS, the protection
available in the IBM 370 hardware) to building a system that is very
resistant to Trojan horses and Virii.  A very closely related open source
OS is Eros .
Use of these technologies is illustrated by "A Security Analysis of the
Combex DarpaBrowser Architecure" by David Wagner & Dean Tribble
 and a presentation
at the O'Reilly Emerging Technology Conference, "The E Development
Platform: Exploiting Virus-Ridden Software"
Cheers - Bil

@_date: 2002-09-24 13:51:19
@_author: Bill Frantz 
@_subject: unforgeable optical tokens? 
If the challenger selects several of his stored challenges, and asks the
token reader to return a secure hash of the answers (in order), no
information will be leaked about the response to any individual challenge.
This procedure will allow the challenger to perform a large number of
verifications with a relatively small number of stored challenge-response
Cheers - Bill

@_date: 2002-09-26 11:49:20
@_author: Bill Frantz 
@_subject: [SIMSOFT] machine shop -Biometrics Slouches Toward the 
The last time I flew, I was scanned with a face recognition system.  The
airline employee who checked my ID looked at the ID, them my face, then the
ID, them my face ...  It seemed to work quite well.
Cheers - Bill

@_date: 2003-04-02 13:24:58
@_author: Bill Frantz 
@_subject: Logging of Web Usage 
The  URL says:
Note that since IPv4 addresses are 32 bits, anyone willing to dedicate a
computer for a few hours can reverse a one way hash by exhaustive search.
Truncating IPs seems a much more privacy friendly approach.
This problem would be less acute with IPv6 addresses.
Cheers - Bill

@_date: 2003-04-03 11:32:03
@_author: Bill Frantz 
@_subject: Logging of Web Usage 
Ah yes, I haven't updated my timings for the new machines that are faster
than my 550Mhz.  :-)
The only other item is importance is that the exhaustive search time isn't
the time to reverse one IP, but the time to reverse all the IPs that have
been recorded.
Cheers - Bill

@_date: 2003-04-08 10:49:01
@_author: Bill Frantz 
@_subject: Via puts RNGs on new processors 
I wonder if the instruction to read the RNG is privileged?  (Certain
systems want to be able to control/intercept all inputs to a process so
they can reproducibly rerun it later.  If the RNG is not privileged, then
these techniques can't be used on this processor.)
Cheers - Bill

@_date: 2003-04-08 12:07:44
@_author: Bill Frantz 
@_subject: article on NSA technology 
Nice article.  I'm surprised it doesn't mention traffic analysis though.
Cheers - Bill

@_date: 2003-04-09 10:29:49
@_author: Bill Frantz 
@_subject: Swiss ISPs Required to Log and Store Email for Six Months 
I see a market oppertunity for SMTP servers outside Switzerland which use
SSL/TLS for communication, and perhaps listen on non-standard ports.
Cheers - Bill

@_date: 2003-04-10 16:23:59
@_author: Bill Frantz 
@_subject: Via puts RNGs on new processors 
The most credible CPU attack I have heard of (from Norm Hardy) is to have
two particular floating point operands cause the next instruction to be
executed in privileged mode.  If these values occur in a normal program, it
won't notice that it had system control for one instruction.  A Trojan
horse program can use it to get control of the machine.
Cheers - Bill

@_date: 2003-04-23 11:18:15
@_author: Bill Frantz 
@_subject: DRM technology and policy 
Your unpublished notes are protected by trade secret.  Should they become
published, then you can assert copyright to try to limit the damage.
Cheers - Bill

@_date: 2003-04-26 14:36:25
@_author: Bill Frantz 
@_subject: DRM technology and policy 
I think it very likely that people with musical talent will continue to
make music, even if they don't get paid for it.  (I certainly know a lot
more people who make music for fun than those who make money at it.)
However, for these people, there isn't much incentive to record music.
Lets consider some of the classes of creative talent that perhaps should be
encouraged by being rewarded.  I can identify 3 broad classes, and I hope
others can come up with more classes.
Class 1 is what we've already discussed.  This class in characterized by
the ability of an individual to afford the necessary tools to create works.
Small (including one) musical groups are a find example.  Writers and poets
also generally fall into this category.
Class 2 is the highly capitalized media.  Movies are the example that comes
immediately to mind.  Getting $1,000,000 to $10,000,000 to make a movie
takes investors, and investors tend to expect to be paid.
Class 3 is the "event" media.  This is where hype makes an event out of
something which would otherwise be considered quite ordinary.  Milli
Vanelli is an example.  (While many may not consider this category worth
protecting, it certainly takes talent to make these events happen.
Catering to the crowd psychology of the young teenager is an art form.)
Here are some revenue models for each of these classes:
Class 1:  Schneier's Street Performer Protocol (SPP) should work nicely.
That is, take contributions until enough money is received and then release
a new recording/story/poem/essay.
Class 2:  The SPP could work for established artists.  I, for one, would be
happy to throw $10 into the pot to get to see, "Return of the King".  The
problem is unknown artists.  The advantage of the current, investor
supported, model is that the investors can afford the time to find
up-and-coming new artists, while most of us can't.
Class 3:  This class is where the MPAA and RIAA members provide the most
value.  Perhaps staging real events, where admission is charged, can
provide a revenue model.  Many teenagers would gladly pay for tickets to
see Britney Spears or Nsync.  (But Milli Vanelli better be really good at
lip syncing.)
Cheers - Bill

@_date: 2003-04-30 14:06:40
@_author: Bill Frantz 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
I am always suspicious of "impossible" statements and "proofs".  After
attending last weeks EE380 on Real-Time Watermarking systems
, I think it may
be possible to add multiple watermarks, with enough redundancy to survive
the combination of a large number of copies.  The need for a large number
of copies will work against the casual "file sharer".
Given that the above is possible, the idea that the player watermarks its
output might actually act as a plug to the "analog hole".  Moral of the
story: If you are going to share, buy your player with cash in a remote
Cheers - Bill

@_date: 2003-12-19 15:16:09
@_author: Bill Frantz 
@_subject: Difference between TCPA-Hardware and other forms of trust 
One should note that TCPA is designed to store its data (encrypted) in the
standard file system, so standard backup and restore techniques can be
used.  However, being able to backup my bank balance, buy a bunch of neat
stuff, and then restore the previous balance is not really what a banking
application wants.  Smart cards address this situation by storing the data
on the card, which is designed to be difficult to duplicate.
[I always considered the biggest contribution from Mondex was the idea of
deposit-only purses, which might reduce the incentive to rob late-night
Cheers - Bill

@_date: 2003-02-10 16:02:33
@_author: Bill Frantz 
@_subject: Columbia crypto box 
The technical reasons people might chose RC4 for an embedded application
like 802.11 WEP include:
  * Code size so close to zero as to make no never mind.
  * Intermediate data size so close to zero as to make no never mind.
  * Fast key setup (Forget tossing the 256 bytes of key stream.
    The designers weren't crypto engineers.  Personally, I'd toss the
    first 1024.)
  * Fast encrypt/decrypt.
  * Commonly used in respected security applications (e.g. SSL).
I agree.  WEP is what you get when you don't seek public review.
Cheers - Bill

@_date: 2003-02-10 16:55:44
@_author: Bill Frantz 
@_subject: Columbia crypto box 
The reason I would discard so much is that when I did some statistics on
RC4 output, I kept getting distribution lumps out to about 1024.  They made
me worry about what someone who knew what were doing could do.
Cheers - Bill

@_date: 2003-02-11 11:14:05
@_author: Bill Frantz 
@_subject: Columbia crypto box 
I have a lot of respect for the way IBM dealt with ITAR in this device.
Note that they did not call it secure, they called it the "Commercial Data
Masking Facility", and did not advertise it as secure against NSA level
attackers.  As Steven says, the most effective attack is exhaustive search
through the 40 bit key space.  (IIRC, basically what the device did was
reveal 16 bits of a DES key.)
Cheers - Bill

@_date: 2003-02-11 22:43:54
@_author: Bill Frantz 
@_subject: Columbia crypto box 
It has been pointed out to me that they were even more clever than that.
(This technique could allow a dictionary attack on known/probable plain
text.)  What they did instead was, take a 56 bit DES key through a one way function, zero certain bits so only 40 are variable, take the result through another one way function, and use the result as a DES key for encryption.
For details see US patent 5,323,464: Cheers - Bill

@_date: 2003-02-25 11:35:42
@_author: Bill Frantz 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security Advisory: 
I have always preferred to have the MAC check as much of the transfer logic
as possible.  If you pad-then-MAC-then-encrypt, then the MAC checks both
the encryption and decryption stages.  If you MAC last, all the MAC checks
is whether errors have been introduced into the transmission (by an
attacker or just through failure of the TCP checksum).
Cheers - Bill

@_date: 2003-07-15 10:22:49
@_author: Bill Frantz 
@_subject: [Fwd: BugTraq - how to coverup the security] 
The idea of "trusted path" seems to have been lost in history.  Both Redhat
Linux and Macintosh System X have the worrisome habit of asking you for
your administrator password (root password in the case of Redhat) as part
of their online system update procedure.  It seems to me that any program
could pop up such a dialog, and it wouldn't look any different.
Back in the old days, flipping the online/offline switch on a 3270 terminal
would cause VM/370 to disconnect the currently logged on user and display
the logon screen.  KeyKOS uses the "SysReq" key for the same purpose.
Trusted path was an Orange Book requirement.  What happened?
Cheers - Bill

@_date: 2003-07-16 14:52:09
@_author: Bill Frantz 
@_subject: Announcing httpsy://, a YURL scheme 
It seems to me that we are just pushing around the problem that, in the
end, we must trust at least one key to be:
* Not compromised
* Follows a trustworthy key usage policy
In the YURL case, we are trusting the key used by the target.  In the
classic X.509 case, we are trusting the CA key.  Following Trevor Perrin's
 suggestion, we are trusting a key trusted by the target.
Lets look more carefully at these three systems.
+ Bob knows that the Carol he connected to is the same Carol that Alice
meant because the key hash matches.
+ Stealing a key affects only a limited number of YURLs
- We depend on a key which is in constant use, and therefore more
vulnerable to theft.
- Carol can't revoke her key without invalidating all the YURLs that
mention it.
+ We depend on a key which is highly protected, and probably less
vulnerable to theft.
+ A new key, with a new cert can be rolled into use easily.
- We must ensure that all the CA keys we trust agree about who the name
Carol designates.
- A CA key is a big, fat target.  Compromise will affect a lot of URLs
Trevor Perrin's Suggestion
(I am going to assume for the moment that Carol acts as her own CA, and
protects her CA key by keeping it offline and/or splitting it into
distributed shares.)
+ Bob knows that the Carol he connected to is the same Carol that Alice
meant because the CA key hash matches and has issued a current cert for the
temporary key.  (The connection protocol delivers the public CA key, the
public temporary key, and the cert from Carol to Bob.)  There is no
confusion about who Carol is.
+ A new key, with a new cert can be rolled into use easily.  (Even easier
than dealing with Verisign.)
+ Stealing Carol's CA key only affects her site YURLs.
+ We depend on a key which is highly protected, and probably less
vulnerable to theft.
What am I missing?
Cheers - Bill

@_date: 2003-06-03 12:48:37
@_author: Bill Frantz 
@_subject: Maybe It's Snake Oil All the Way Down 
I've spent some time recently looking at Voice over IP (VoIP)
implementations.  My immediate reaction to reading the standards is that
they a complete answer to a telephone company executive's wet dreams.
Conferencing, Automatic call forwarding, Billing etc. etc., they're all
covered.  The result is a protocol that is beyond baroque and well into
rococo.  I think the various standards bodies are still trying to deal with
issues in the protocols that weren't thought of from the start.
Of course, once you have your call set up, you have to encrypt it.  Most of
the VoIP implementations use Real Time Streaming Protocol (RTSP, RFC2326),
which requires two UDP ports through your firewall.  Then you have to
encrypt the RTSP traffic.  I have seen reference to an encryption protocol
specifically for RTSP, but a quick scan of STD1 didn't turn it up, so it is
probably still a draft.  I don't know anything about its security.
The other choice is IPSec.  IPSec seems happiest securing traffic between
machines with permanent IP addresses.  It is a nightmare to use with
Network Address Translation.
What would be really nice would be a VoIP system that used TCP instead of
UDP.  (I know that if TCP goes into error recovery, there is going to be
major jitter in the voice.  I know it will be hard to support conferencing.
I know it will not gracefully bridge to the POTS network.  Etc. I'm willing
to put up with that to avoid the pain that comes with UDP.)  Then I can
just tunnel it through SSH, or hack it to use SSL/TLS.  Oh well.
Cheers - Bill

@_date: 2003-06-03 15:15:16
@_author: Bill Frantz 
@_subject: Maybe It's Snake Oil All the Way Down 
I know of one system that takes credit cards over HTTPS, and then sends the
credit card number, encrypted with GPG to a backend system for processing.
It isn't perfect, but it's better than storing the credit card number on a
database accessible to the web server.  (I would feel a lot better if
Amazon didn't remember my credit card number.)
Cheers - Bill

@_date: 2003-06-04 13:59:40
@_author: Bill Frantz 
@_subject: Maybe It's Snake Oil All the Way Down 
I'm not sure SPKI was ever meant to be a global PKI.  It was more meant to
authorization in a "verifier-centric" system.
Cheers - Bill

@_date: 2003-06-04 14:13:22
@_author: Bill Frantz 
@_subject: Maybe It's Snake Oil All the Way Down 
Well, I've only ordered from Amazon 2 or 3 times since they've been in
business.  Having my CC on file gives a much longer exposure time than the
brief periods of time it would be "in transit".  So, no I don't feel much
safer.  The $50 limit on unauthorized charges is what makes me feel safer.
Cheers - Bill

@_date: 2003-06-10 15:39:38
@_author: Bill Frantz 
@_subject: An attack on paypal 
A nice essay, partially on the need to include technological protections
against human error, included the above paragraph.
IMHO, the problem is that the C language is just too error prone to be used
for most software.  In "Thirty Years Later:  Lessons from the Multics
Security Evaluation",  Paul A. Karger and Roger R. Schell
 credit the use of PL/I for
the lack of buffer overruns in Multics.  However, in the Unix/Linux/PC/Mac
world, a successor language has not yet appeared.
YMMV - Bill

@_date: 2003-06-11 17:47:02
@_author: Bill Frantz 
@_subject: Keyservers and Spam 
To try to reflect some of David's points with a real-world situation.  I
was at work, with a brand new installation of PGP.  I wanted to send some
confidential data home so I could work with it.  However I didn't have my
home key at work, so I didn't have a secure way to send either the data, or
the work key.  I didn't even have the fingerprint of the home key.
My solution was to pull Carl Ellison's business card out of my pocket.  It
had his key fingerprint on it, and I remember getting it directly from him,
so I could trust the fingerprint.  Now Carl had signed my key, so when I
downloaded it from the key server, I could verify that it was indeed mine
(to the extent I trusted Carl).  Carl's signature, and the key server
allowed me to bootstrap trust into my own key.
But with a key server, I didn't have to bother Carl to send me my key.  Or
depend on him being online when I needed it.
Cheers - Bill

@_date: 2003-06-12 15:43:22
@_author: Bill Frantz 
@_subject: Keyservers and Spam 
I didn't.  I do now.
I don't think key servers are a significant cause of email address leakage
compared with posting to open mailing lists, so I am not compelled by the
original reason for this thread.
And I could have followed Jill's suggestion of using symmetric encryption
had I thought of it.  Get a pass phrase from /dev/random and write it down.
Put the piece of paper in my wallet and take it home.  Decrypt the data and
burn the paper.  That's enough protection for low level commercial secrets.
Cheers - Bill

@_date: 2003-06-13 15:41:29
@_author: Bill Frantz 
@_subject: Keyservers and Spam 
The HighFire project at Cryptorights
 is planning on building a
"web of trust" rooted in the NGOs who will be using the system.  Each NGO
will have a signing key.  A NGO will sign the keys of the people working
for it.  In this manner, we have way of saying, "The John Jones who works
for Amnesty International".  A NGO may decide to sign another NGO's signing
key.  Now we have a way to say to someone in Amnesty, "Send a message to
Steve Smith in M?decins Sans Fronti?res."  The plan is to show the trust
relationship in the UI as a path of keys.
I would appreciate your comments.
Cheers - Bill

@_date: 2003-06-17 15:35:11
@_author: Bill Frantz 
@_subject: Keyservers and Spam 
I think, at least in the beginning, we will depend on having the NGO level
signing keys kept in parts of the world where strong arm tactics are
unlikely and having written guidelines for their use.  I assume that
NGO_Alice won't sign NGO_Bob's signing key unless NGO_Alice has reason to
believe that NGO_Bob is following the guidelines.
I should note that we have a bunch of things to do, and getting something
out is more important than getting the best possible thing out.  We can
improve things later, once we have a structure to enhance.
We will probably have all communications with the key server go through one
of SSH/SSL/TLS/IPSec.
Cheers - Bill

@_date: 2003-03-02 20:48:03
@_author: Bill Frantz 
@_subject: Columbia crypto box 
IIRC, 243.0 is the military flight emergency frequency.  (Corresponding to
121.5 for civilian use).  I would expect the shuttle to have that frequency
Cheers - Bill

@_date: 2003-03-05 23:14:40
@_author: Bill Frantz 
@_subject: Scientists question electronic voting 
The best counter to this problem is widely available systems to produce
fake photos of the vote, so the vote buyer can't know whether the votes he
sees in the photo are the real votes, or fake ones.
The easiest way to implement is to let people photograph the paper on the
sample/practice -- not for real voting -- machine that poll workers use to
teach voters how to use the real machines.
Cheers - Bill

@_date: 2003-03-06 17:58:18
@_author: Bill Frantz 
@_subject: Proven Primes 
Having set a computer to the problem of coming up with a Sophie Germain
prime for the E startup protocol (Diffie-Hellman),  I offer you:
    static final BigInteger g = new BigInteger("2");
    static final BigInteger modulus =
        new BigInteger("11973791477546250983817043765044391637751157152328012"
                        + "72278994477192940843207042535379780702841268263028"
                        + "59486033998465467188646855777933154987304015680716"
                        + "74391647223805124273032053960564348124852668624831"
                        + "01273341734490560148744399254916528366159159380290"
                        + "29782321539388697349613396698017627677439533107752"
                        + "978203");
Cheers - Bill

@_date: 2003-03-06 18:09:53
@_author: Bill Frantz 
@_subject: 3-rotor enigma on ebay: $5200 
I got that on my Safari beta browser.  (Safari is Apple's new browser.)  On
the same machine with the same IP address I got the page using Netscape
There seems to be some strangeness with eBay.  (I looked for a way to
report the problem, but lost interest after shuffling through a few of
their web pages.)
Cheers - Bill

@_date: 2003-03-06 23:57:35
@_author: Bill Frantz 
@_subject: Proven Primes 
Sorry, an exercise for the student. :-)
I thought that finding them was the hard part, and verifying one once found
was relatively easy.  I used the probable prime test in the Java BigInteger
package.  It sounds like, from some of the list traffic, that there are
better tests.
I guess I'm dumb, but how to you verify a proof of Sophie Germain primeness
with less effort than to run the tests yourself?
Cheers - Bill

@_date: 2003-03-07 09:43:41
@_author: Bill Frantz 
@_subject: Proven Primes 
Yes.  And I do know that the Sophie Germain prime is the smaller of the two
related primes.
Cheers - Bill

@_date: 2003-03-07 22:46:06
@_author: Bill Frantz 
@_subject: Active Countermeasures Against Tempest Attacks 
It has occurred to me that the cheapest form of protection from tempest
attacks might be an active transmitter that swamps the signal from the
computer.  Such a transmitter would still be legal if its power output is
kept within the FCC part 15 rules.
Take, for example, the signal from a CRT monitor.  The monitor signal
consists of large signals which are the vertical and horizontal sync
pulses, and smaller signals which are the levels of each of the phosphor
The simplest countermeasure would be random RF noise which is many orders
of magnitude stronger than the signal from the monitor.  However, with this
system, the attacker can average many fields from the monitor and perhaps
still recover the signal because any give pixel is the same, while the
noise is random.  (Or at least the pixels change slowly compared with the
fields, giving lots of data to average.)
The next more complex version sends the same random screen over and over in
sync with the monitor.  Even more complex versions change the random screen
every-so-often to try to frustrate recovering the differences between
screens of data on the monitor.
Can such a device be built and still stay within the Part 15 rules?
Cheers - Bill

@_date: 2003-03-31 18:31:56
@_author: Bill Frantz 
@_subject: Run a remailer, go to jail? 
However, this provision shouldn't interfere with NAT on a home network.
All the machines are at the same address, the origin of the communication.
(The actual source of email communication is the keyboard processor, not
the computer with the IP address and email client.)
OTOH, the sections dealing with "theft of service" may apply.  Moral is to
get your service from a provider that allows NAT.
Cheers - Bill

@_date: 2003-05-05 15:58:39
@_author: Bill Frantz 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
It occurred to me that one way to attack a watermarking system that I
haven't heard proposed before is instead of removing the watermark, bury it
in other watermarks.  While this approach would degrade the quality, it
might not degrade it enough to bother the people who trade video camera
pointed at the screen copies of movies.
The system for watermarking audio I heard described at Stanford worked
(1) Use the public key, and a hash of the 8.6 millisecond block to be
watermarked to produce a spreading code which determines which bits carry
the watermark.
(2) Use a psycho - acoustic model to determine where the watermark can be
placed without significantly degrading the S/N ration of the block.
(3) Use the secret key to "sign" the watermark.
(4) Add the watermark to the block and output it.
Note that to recover the watermark, you need an un-watermarked copy of the
block, as well as the public key.  The system described continued to
watermark every 8.6 ms block in the entire work.
Cheers - Bill

@_date: 2003-05-05 16:24:05
@_author: Bill Frantz 
@_subject: The Pure Crypto Project's Hash Function 
The need to show that the object code is a correct implementation of the
algorithm described by the source code is a general problem for validating
any kind of code.  My approach, and why I have some sympathy for Ralf's
minimum code approach is:
(1) Code the algorithm in assembler.
(2) Explain each instruction as a comment on the instruction.
(3) Run the code thru the assembler
(4) Show that the output of the assembler matches the input, thereby
avoiding the need to prove the assembler.
Note that I fully agree with the many others who are seriously concerned
about the security of new, unexamined algorithms.
Cheers - Bill

@_date: 2003-05-05 21:31:41
@_author: Bill Frantz 
@_subject: The Pure Crypto Project's Hash Function 
As long as you don't try to claim that the C family is a high level
language, I'll agree.  (I still think my assembler code from the old days
had fewer bugs in released code than my current C code.  Of course that
just may mean that I've lost my edge to old age.  Or that, knowing the
risks, my testing was much more thorough.)
 How do you trust the program proving equivalence? I think I agree with Eric:
However, we must recognize that in taking this approach, we are building on
a foundation of sand.  The reason I like Ralf's approach is because he is
trying to minimize the amount of sand.  (OTOH, his implementation of that
approach has a multitude of problems, as others have noted.  Trusting
Perl/Python is only one.)
I am reminded of a conversation with the National Computer Security Center
contractors who were helping us when we were considering evaluating KeyKOS
under the Orange Book criteria.  We asked them about the microcode in the
IBM disk controller.  They essentially shook their heads and didn't want to
address the problem.  (BTW - KeyKOS was never evaluated because we couldn't
find a VC willing to invest on the order of $1M to generate paper for the
Cheers - Bill

@_date: 2003-05-07 13:26:11
@_author: Bill Frantz 
@_subject: Randomness 
I assume the people who are using randomness to generate UUIDs are doing so
in a distributed manner.  (If it was centralized, then a counter would
provide better assurance of non-duplication.)  I am also going to assume
that the seed they get from the secure random process which is used to
support the "void insecureprng(void *out, int nbytes)" function is run
through a cryptography strong mixing process like MD5 or SHA1.
The question is, does having only a few bits different in the seed between
the various instances of the generator compromise the collision resistance
of the generator?  If it does, how many bits do you need?  (This issue
seems to me to be closely related to the issue of using a counter as an IV
in CBC mode encryption.)
Given you have very little "conditional entropy" (random data unknown to an
attacker), what is the best way to get "unconditional entropy" (random data
that may be known to an attacker)?  Clearly time, which has very little
"conditional entropy", also has a significant risk of duplication between
two instances started nearly together.
How do you protect against this risk?  Adding in IP address might yield a
lot of 192.168.0.1s.  Ethernet MAC addresses seem to be good, but not all
machines have Ethernet cards.
Cheers - Bill

@_date: 2003-05-09 11:04:36
@_author: Bill Frantz 
@_subject: Randomness 
The relation between this mode of PRNG and IVs has been eating away at my
brain.  Let me try an analysis:
Under our assumptions, a one bit change in the seed will change about half
the bits of the output of the mixing process.  Given that there is feedback
in the mixing process, so these differences continue to influence the
output, the one bit difference should be enough to get probabilisticly
different UUIDs.
In the case of the one bit different IV, we must consider the case where
the plaintext also has a single bit difference, and in the same bit
position.  (This situation might occur if the plaintext starts with a
message serial number in ASCII.)  In this case, the XOR if the IV and the
plaintext block followed by encryption will result in the same cyphertext
block.  Since that cyphertext block is the "IV" for the next block of
encryption, this error allows an attacker to determine where the messages
first differ, an unpleasant wedge into the crypto system.
Cheers - Bill

@_date: 2003-05-09 10:55:25
@_author: Bill Frantz 
@_subject: Requim for DRM: Apple music sells one million in less than a  
Note that while there is DRM, it is mostly there to keep honest people
honest (and to satisfy the record companies).  You can burn a CD from your
AAC file, and then rip that back to MP3 if you want.  (There are other
techniques as well, but this one is easy on the newer Macs.)
What really interests me is whether this service will serve to
disintermediate the record companies.  A series this week in the San
Francisco Chronicle describes how modern computers have basically put the
recording studio out of business, but cutting the cost to what an amateur
musician can afford.  With a distribution arm like the iTunes Music Store,
and the others sure to follow, I suspect that many musicians will decide
they no longer need the major record companies, since they can now record
and sell without them.
As an aside, some musician friends have managed to attract enough of a
following by posting a few freebees on mp3.com that they have a profitable
business selling CDs.  See  for all the details
(and some good music).
Cheers - Bill

@_date: 2003-05-13 14:24:08
@_author: Bill Frantz 
@_subject: A Trial Balloon to Ban Email? 
This problem could be eliminated if the ISP(s) collected the money,
regardless of whether the mail could be delivered or not.
It's sounding more and more like the postal system.  Perhaps we can get
spam-stamps to subsidize regular email.
Cheers - Bill

@_date: 2003-05-14 11:14:09
@_author: Bill Frantz 
@_subject: A Trial Balloon to Ban Email? 
If you go to an ISP collects model, see how this changes the picture.
ISP receives mail header.  As soon as the coin appears, it:
(1) Check it against the in-memory Bloom filter of already seen coins.  If
it passes, goto collect.
(2) Check it against the local database of already seen coins.  (Because
Bloom filters can give false positives.)  If it is in the database, drop
the mail and the connection.  Result: no mail in the spool, and minimum
bandwidth lost.
(collect) Add the coin to the Bloom filter and to the database.  Collect
the money from the bank.  If the bank says, "double spent", drop the
connection and the mail as above.
Note that this system will work well against spammers who blast out
identical coins to a lot of addresses at an ISP.
Now spammers can engage in a DOS attack against this system by using junk
coins.  It won't help them get the spam thru, and it will be detected when
there is a TCP connection between their machine/open relay/etc. and the ISP
machine.  That will go a long way toward locating them in meat space, so
fraud charges can be brought.
Cheers - Bill

@_date: 2003-05-16 10:43:14
@_author: Bill Frantz 
@_subject: Secret Computer Security 
In the spirit of I. F. Stone...
Buried in the last paragraphs of an article in yesterday's San Jose Mercury
News (Thursday, May 15) that starts out talking about members of congress
saying that the US is ill-prepared to defend against an attack on critical
computer systems is the following gem.
"Last fall's legislation authorized the National Science Foundation to
spend $110.25 million on cyber-security research, but the agency is
requesting only about $51 million.  DARPA's unclassified budget for
cyber-security research has actually declined, from about $90 million in
2000 to $30 million in 2003.  But Tether [Tony Tether, director of DARPA]
said those figures were misleading, because more projects are now
classified.  He estimated the agency will spend about $100 million on
cyber-security research in 2004."
Note also that DARPA's support of the OpenBSD project has been dropped (see
Do these changes mean that the US is trying to protect "critical
infrastructure" using classified techniques so other nation's systems can
be hacked while US ones are safe?  Inquiring minds want to know.
Cheers - Bill

@_date: 2003-11-13 17:39:13
@_author: Bill Frantz 
@_subject: Clipper for luggage 
I've never seen a luggage lock that provides anything like what I would
call security.  On the other hand, unlocked luggage does sometimes open in
transit.  (I saw a suitcase open when it was dropped while being loaded
onto an airplane.)
I usually travel with zipper closed duffel bags.  I fasten the zipper
closed with a screw link.  Anyone can unscrew the link and get into the
bag, but it does effectively keep the zipper closed in transit.  I suppose
it also provides some level of security because someone wanting to do a
quick grab from luggage will probably pick a less-secured piece.\
Cheers - Bill

@_date: 2003-10-01 12:56:25
@_author: Bill Frantz 
@_subject: Monoculture 
I am very glad that before I started the E communication protocol
, I looked at the
problems that existed with other protocols.  I avoided a number of mistakes.
Cheers - Bill

@_date: 2003-10-02 11:45:33
@_author: Bill Frantz 
@_subject: Reliance on Microsoft called risk to U.S. security 
Peter has raised a number of important points.  Let me start by saying that
I do not see a strong distinction between a file to be viewed and a
program.  Both are instructions to the computer to perform some actions.
While we might think the renderer showing us flat ASCII text is quite
trustworthy, our degree of trust in an HTML should be less, and we
shouldn't trust a Word format renderer at all (thanks to Word Macro
I do not envision running either programs or viewers under the privileges
of the mail agent.  Since I am not really a Unix person, let me take a stab
at a design and let the people who know what they are doing take stabs at
What we need is an environment of very limited privilege to use to confine
our untrusted code.  Specifically:
* No ability to make connections to other services, either over the network
or locally.  (I think this item requires a kernel change.)
* Very limited access to the file system.  We might take the view that
since we control all the ways the confined process can send data out of the
system, it can have full read-only access to the file system without
risking anything important.  I am told we can build general limits of file
system access with chroot, but I am also told that processes can break out
of these limits.  This is an area where I would love to learn more.
* We can pass in the privileges we think the process should have via open
file handles.  These will probably include the ability to render on a
portion of the screen, and the ability to get mouse and keyboard focus.
(We need a way for trusted code to mediate these accesses so the user can
have a "secure attention" function.)
* Strict control of other communication paths I haven't thought of.  :-)
An HTML renderer should be able to run in the above environment.
For viewing Word etc. documents, the applications should run in the above
environment.  The interesting case is where someone has sent you something
like a Word document and asked you to mark it up.  Everything should
proceed well in the above until it comes time to save a local copy or mail
the changed document back.
 describes the "Powerbox"
pattern for allowing the user to specify an output file for a confined
process such as we are discussing.  I would think the best way to return
such a file in Unix would be as an open file handle.  However I don't know
of a way for a program to call a service with greater privilege than it has
and accept a return value unless that service is part of the kernel.
Perhaps some Unix experts can come up with ideas.
As for mailing the document back, if the mail agent gave the confined
program read-write access to a copy of the file, the confined program could
write its output over the input and the mail agent could then make that
file available to the user when the confined program returns, and the user
could include it in the reply email.
Since most users do not have these facilities running on their machines, I
suspect that most Word/Excel/Powerpoint files would render quite well from
a confined process.  I would say that having random, perhaps hostile, files
able to update my local data bases is a violation of my security policy.
UIs have changed considerably since the 1980s.  It turns out that modern
UIs make it much easier for users to run programs with only the privileges
they need than the UIs of the 80s.  (See the email thread at
)  Running
programs with only the privileges they need is a good way of controlling
viruses and other hostile programs.
I am not sure that classical MLS really models well our current security
problem.  Consider the *-property.  Systems which enforce the *-property
say that less trusted processes can send data to more trusted processes but
they can't read data from more trusted processes.  The reverse of that is
also true, more trusted processes can read data from less trusted
processes, but they can't write data to them.  The application library is
available to everyone, so the policy allows the least trusted program in
the system to write the library, obviously a silly result.
I think our security problems today are much more in the area of protecting
systems from unauthorized modification than they are in the area of
protecting data from unauthorized access.  When I was doing Orange Book
stuff in the 1980s, the people I was working with called this "Integrity",
and said that is was no where as well understood as security (by which they
meant the Bell and LaPadula model, D. E. Bell and L. J. LaPadula , Secure
computer system: unified exposition and Multics interpretation).
Cheers - Bill

@_date: 2003-10-02 13:14:17
@_author: Bill Frantz 
@_subject: Monoculture 
In most of the US, a homeowner can install electrical systems in their
house.  However, their installation must be up to code, and inspected by a
government inspector.  The analog for crypto protocols seems to be obvious,
although the inspector part seems to be more ad hoc and community based.
(But there's no building permit either.)
Cheers - Bill

@_date: 2003-10-02 14:50:21
@_author: Bill Frantz 
@_subject: Protocol implementation errors 
This is the second significant problem I have seen in applications that use
ASN.1 data formats.  (The first was in a widely deployed implementation of
SNMP.)  Given that good, security conscience programmers have difficultly
getting ASN.1 parsing right, we should favor protocols that use easier to
parse data formats.
I think this leaves us with SSH.  Are there others?
Cheers - Bill

@_date: 2003-10-09 19:45:01
@_author: Bill Frantz 
@_subject: Trusting the Tools - was Re: Open Source ... 
With KeyKOS, we used the argument that since the assembler we were using
was written and distributed before we designed KeyKOS, it was not feasible
to include code to subvert KeyKOS.  How do people feel about this form of
Cheers - Bill

@_date: 2003-10-09 21:55:37
@_author: Bill Frantz 
@_subject: Open Source (was Simple SSL/TLS - Some Questions) 
Given that the application I had which needed IPSec for encryption was
Voice over IP, I'd settle for a secure, point-to-point voice system which
works when both ends have NAT with appropriately configured firewalls.
Multi-point voice would be icing on the cake.
Cheers - Bill

@_date: 2003-10-10 18:29:25
@_author: Bill Frantz 
@_subject: [e-lang] Re: Protocol implementation errors 
I think there were several additional reasons for this:
* Most of the strings passed were very simple, consisting of just one
string.  We didn't get much more complex than, for example, the record
collection (which used byte string names to look up data and capabilities).
Its string was:
  * 1 byte - length of name
  * n byte (0-255) name
  * the rest data.
* We did not try to handle "infinite length" strings.  In general, strings
were limited to 4096 bytes.
* We were programming in 370 assembler, which, IMHO, is better suited to
writing secure code than C.  For example, the string copy primitive in C is
strcpy, where the source string determines the length.  In 370 assembler,
the primitive is MVCL (move long) which takes 5 parameters, the source
address and length, the destination address and length, and the fill
character.  Having to specify those 5 parameters made the issues involved
in a string copy very obvious each time one was coded.
* We did not, as a general rule, have a stack.  As a result, it was less
likely to have program control data (return addresses) and buffers next to
each other, where a buffer overrun could result in both hostile code, and
the necessary changes to the program control data to pass control to it.
I think Peter has convinced me that ASN.1 (by which I mean DER
specifically, since that is the form that the run-time code parses), is
probably not a whole lot worse than the other formats (which also have
shown significant parsing bugs).  While some of the problems come from the
details of the format, most probably come from the complex data structures
that are part of the problem space.  Given that this is a correct
assessment, then we need to think of ways to protect ourselves against the
programs that parse these data.
If we were coding in KeyKOS or EROS, I would put the parsing code in a
separate domain.  To protect against denial of service that domain would
have a limited space bank, a keeper, and a timeout to signal parse failure
to the caller.  To guard against attack, that domain would only have
capabilities to:
* The input string and/or the input byte stream
* "No hole" creators for the output objects.
Further protection can be built by limiting the privilege of the domains
that use these output objects.  (Note that the cost of a domain call is
very small compared with the cost of a public key operation.)
I don't really know how to apply this approach to a UNIX like system.  I
think for these systems, it might be best to write the parsing code in a
"safe" language such as Java, Smalltalk, E, Scheme etc.  That would result
in an additional layer of protection from the runtime.  If the cost of
calling into and return from the language is small enough, the extra cost
of the language should be tolerable.  (Having to fire up a Java Virtual
Machine each time could make public key operations look fast.)
Cheers - Bill

@_date: 2003-10-15 12:14:27
@_author: Bill Frantz 
@_subject: Trusting the Tools - was Re: Open Source ... 
I can see a possible attack here.  Assume you notice the instruction that
loads the base address of the memory map into the machine's control
register.  You add some instructions that make all the pages in that memory
map R/W.  Now as an attacker you can change the (shared) code of any
program you can map into your address space.
Of course, this attack might be noticed if, for example, a program which
makes copies of pages (aka virtual copies) as they are modified is included
in the system.  This program would totally fail to work, and that failure
is likely to be noticed.
It seems difficult develop code to access a file system that hasn't been
designed yet.  Particularly the KeyKOS "file system" where all data was
kept in permanent virtual memory.
Cheers - Bill

@_date: 2003-09-23 14:45:23
@_author: Bill Frantz 
@_subject: End of the line for Ireland's dotcom star 
Note that proposals such as Tyler Close's YURL
 avoid the issue of trust in the
TTP/CA.  As such, I find them attractive whenever they can be used.
Cheers - Bill

@_date: 2003-09-26 17:04:05
@_author: Bill Frantz 
@_subject: Reliance on Microsoft called risk to U.S. security 
The real problem is that the viewer software, whether it is an editor, PDF
viewer, or a computer language interpreter, runs with ALL the user's
privileges.  If we ran these programs with a minimum of privilege, most of
the problems would "just go away".
Cheers - Bill

@_date: 2003-09-28 13:07:15
@_author: Bill Frantz 
@_subject: Reliance on Microsoft called risk to U.S. security 
Given a strange program that has just arrived on my machine, my current
policy is not to run it at all.
I might be willing to adopt a policy of giving it a small amount of memory,
CPU, and some space to render on the screen.  That would allow people to
exchange active amusements with a degree of safety.
If the program required more privilege (for example, a new version of a
utility from a co-worker), I would be happy to move it to an environment
where it had the resources it needs to run.
Limiting the privilege of the "View" program would provide protection
against flaws in the viewer.  Given the number of flaws in very basic
software, such protection seems of have real value.
Cheers - Bill

@_date: 2004-04-15 14:19:32
@_author: Bill Frantz 
@_subject: voting 
One area we are not addressing in voting security is absentee ballots.  The
use of absentee ballots is rising in US elections, and is even being
advocated as a way for individuals to get a printed ballot in jurisdictions
which use electronic-only voting machines.  Political parties are
encouraging their supporters to vote absentee.  I believe that one election
in Oregon was recently held entirely with absentee ballots.
For classic polling place elections, one strength of an electronic system
which prints paper ballots is that there are two separate paths for the
counts.  The machine can keep its own totals and report them at the end of
the election.  These totals can then be compared with the totals generated
for that precinct by counting the paper ballots.  This redundancy seems to
me to provide higher security than either system alone.
Cheers - Bill

@_date: 2004-04-21 11:14:43
@_author: Bill Frantz 
@_subject: Cryptography Expert Paul Kocher Warns: Future DVDs Prime 
Continuously changing the protection on permanent storage media is a much
more difficult problem than changing broadcast protection.  With broadcast,
you give current subscribers the new smart card, change what's broadcast,
and away you go.  With permanent storage media, once the protection is
broken, the content is still available to pirate.  Only new releases can be
protected with new protection schemes.
These technical considerations would seem to lead to a marketing strategy
of short product cycles driven by big advertising campaigns, to reap as
much profit as possible while piracy is still difficult.  This approach is
not new to the movie industry.  In recent years, the number of theaters
opening a big movie release has increased greatly, and the time it runs in
theaters has become shorter.
It is ironic to compare the marketing strategy of reaping most of the
profit quickly, with the public policy stance that long copyright terms are
necessary to provide incentive for production.
Cheers - Bill

@_date: 2004-12-14 22:31:01
@_author: Bill Frantz 
@_subject: The Pointlessness of the MD5 "attacks" 
One scenario that might form an attack is to take code which is normally distributed in executable form, for example RPMs, and make it possible to have two different programs that pass the same signature check.  Given that someone has arranged to have the doppleganger blocks generated as part of the output of the compiler, different binaries can later be injected into the distribution system without a signature verification failure.
Cheers - Bill

@_date: 2005-08-03 20:31:01
@_author: Bill Frantz 
@_subject: [Clips] Escaping Password Purgatory 
On 8/3/05, rah at shipwright.com (R.A. Hettinga) quoted:
Try Site Password, .  It takes a "good" master password, and a site name, and hashes them together to produce a site-specific password.
Cheers - Bill

@_date: 2005-08-06 15:27:40
@_author: Bill Frantz 
@_subject: Possible non-extension property for hash functions 
In Steve Bellovin and Eric Rescorla's paper, "Deploying a New Hash Algorithm"*, the author's note the well known property of hash functions:
For two different stings x and y,
H(x) = H(y) ==> H(x||s) = H(y||s)
It seems to me that there might be a class of hash functions for which this property did not hold.  While hashes in this class might require random access to the entire input, they could prevent the "message extension" class of attack exploited by Lucks and Daum (see ) when they generated two Postscript files with very different output, but the same MD5 hash.
* A draft of Bellovin and Rescorla's paper is available at  and .)
Cheers - Bill

@_date: 2005-01-05 14:11:11
@_author: Bill Frantz 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
What I have heard from the people building watermark systems is:
(1) The system requirements are that the water mark repeat every few seconds to mark every part of the work being watermarked.  Some systems encrypt the watermark in each repeat block with a different, repeat-block dependent key.
(2) By using error correction codes, a short sequences of bits, like a serial number, can be encoded to survive re-encoding.  The claim is that you have to do serious damage to the quality of the work before the serial number becomes unreadable.
Removing the water mark may be quite difficult, but any professional movie thief would find the business expense of new players quite affordable.  Some individuals might decide that giving a bunch of movies to their friends from an about-to-be-retired player was a reasonable thing to do.  Someone who works in a store might use an off-the-shelf player.  And ...  Once a copy gets into circulation, it could spread quite widely.
Cheers - Bill

@_date: 2005-06-08 19:15:52
@_author: Bill Frantz 
@_subject: encrypted tapes 
I think I would be tempted to keep a private key in those safe deposit boxes, and when writing the backup tape, pick a "random" (as best you can with the hardware and software available) session key, encrypt it using the public key, hard coded in the backup procedure, and write the encrypted result as the first part of the backup.  This procedure allows you to keep your secrets hidden away, at least until you need to use one of the tapes.
Cheers - Bill
IP note:  This technique is so obvious to any practitioner skilled in the art as to be non-patentable (except in the USA, where obviousness is no barrier).  In any case I put it into the public domain.

@_date: 2005-03-06 21:06:02
@_author: Bill Frantz 
@_subject: MD5 collision in X509 certificates 
The real concern, and there is no evidence that it is easy, is that if a certificate is signed using a MD5 hash, and another certificate, with a different (RSA) public key, can be substituted, maintaining the signature, then it will be probable that the new public key will be the product of many primes, and (relatively) easy to factor.  If this were possible, it would lead to identity theft.
While this scenario is not, as far as I know, easy, it seems to me that it is time to abandon MD5 in signatures.  The issues with SHA1 are worrisome, but not yet, IMHO, fatal.  However, it would be prudent to plan on moving beyond SHA1 in the near future.
All IMHO.
Cheers - Bill

@_date: 2005-03-24 15:38:02
@_author: Bill Frantz 
@_subject: Petname Tool version 0.5 
Tyler Close has written an anti-phishing tool for the Firefox browser called the Petname tool.  It works with SSL sites, including those with self-signed certificates, and is available at .
Mark Stiegler has written an overview of petname systems, including a list of existing examples, available at: .  Note that Amir Herzberg and Ahmad Gbara's Trustbar system is an example of a pet name system.
From the Petname Tool web site, "Need help avoiding phishing and spoofing attacks? The petname tool can help you keep it all straight by clearly distinguishing your online relationships.
"Using the petname tool, you can save a reminder note about a relationship you have with a site. The petname tool will then automatically display this reminder note every time you visit the site. After following a hyperlink, you need only check that the expected reminder note is being displayed. If so, you can be sure you are using the same site you have in the past."
Cheers - Bill

@_date: 2005-10-06 19:53:55
@_author: Bill Frantz 
@_subject: Pseudonymity for tor: nym-0.1 (fwd) 
This solution is subject to a rather interesting attack, which to my
knowledge has not yet been named, although it is occasionally used
against web sites which have a Turing test in their registration.  Say
you want to automatically register a number of identities.  You set up a
porn site, and advertise, "Answer this question and see some porn."  Of
course, the question comes from the target registration page.  You get
lots of people who can pass the Turing test to help you.
Cheers - Bill

@_date: 2005-09-15 20:51:02
@_author: Bill Frantz 
@_subject: Clearing sensitive in-memory data in perl 
I agree.  I also note that Paul A. Karger and Roger R. Schell, in their
paper, "Thirty Years Later: Lessons from the Multics Security
Evaluation" state:
    2.3.1 Programming in PL/I for Better Security
    Multics was one of the first operating systems to be
    implemented in a higher level language.(1) While the Multics
    developers considered the use of several languages,
    including BCPL (an ancestor of C) and AED (Algol Extended
    for Design), they ultimately settled on PL/I [15].
    Although PL/I had some influence on the development
    of C, the differences in the handling of varying length
    data structures between the two languages can be seen as
    a major cause of buffer overflows. In C, the length of all
    character strings is varying and can only be determined by
    searching for a null byte. By contrast, PL/I character
    strings may be either fixed length or varying length, but a
    maximum length must always be specified, either at compile
    time or in an argument descriptor or in another variable
    using the REFER option. When PL/I strings are used
    or copied, the maximum length specifications are honored
    by the compiled code, resulting in automatic string truncation
    or padding, even when full string length checking is
    not enabled. The net result is that a PL/I programmer
    would have to work very hard to program a buffer overflow
    error, while a C programmer has to work very hard
    to avoid programming a buffer overflow error.
    Multics added one additional feature in its runtime
    support that could detect mismatches between calling and
    called argument descriptors of separately compiled programs
    and raise an error.
    PL/I also provides richer features for arrays and structures.
    While these differences are not as immediately
    visible as the character string differences, an algorithm
    coded in PL/I will have less need for pointers and pointer
    arithmetic than the same algorithm coded in C. Again,
    the compiler will do automatic truncation or padding,
    even when full array bounds checking is not enabled.
    While neither PL/I nor C are strongly typed languages
    and security errors are possible in both languages, PL/I
    programs tend to suffer significantly fewer security problems
    than the corresponding C programs.
    (1) Burroughs? use of ALGOL for the B5000 operating system was well
    known to the original Multics designers.
    15. Corbat?, F.J., PL/I As a Tool for System Programming.
    Datamation, May 1969. 15(5): p. 68-76. URL:
    Cheers - Bill

@_date: 2005-09-19 16:45:20
@_author: Bill Frantz 
@_subject: Java: Helping the world build bigger idiots 
This is obviously just an attempt to make Java array access more like Java file access.  :-)
Seriously, the real flaw in this approach, which I did not see mentioned in the comments on the web page Peter references above, is the masking of IndexOutOfBoundExceptions that may be generated by displayProductInfo.  This code will treat such errors as "end of array".  A more normal coding of the loop:
    for (int i=1; i<prodnums.length; i++) {       displayProductInfo(prodnums[idx]);
      idx++;     } would let the exception pass up the call chain, and with good error handling, the problem would come to the attention of those responsible for fixing the program.
If ArrayIndexOutOfBoundException were used instead of IndexOutOfBoundException, errors in string indexing would pass up the call chain, while catching array problems.
Cheers - Bill

@_date: 2005-09-21 00:11:04
@_author: Bill Frantz 
@_subject: [Clips] Contactless payments and the security challenges  
One issue I have not seen addressed in these "contactless" payment systems is the needs of people who carry multiple payment instruments.  A simple example is a personal and a corporate credit card.
Cheers - Bill

@_date: 2005-09-21 15:17:33
@_author: Bill Frantz 
@_subject: [Clips] Contactless payments and the security challenges 
It seems to me a use case is paying for a meal.  The cost may be
personal: I've taken my wife out to dinner and a show; or corporate: I'm
on a business trip.  I need to specify which payment instrument is to be
used, and not have it automatically sniffed out of my wallet or cell
If payment means putting the token next to the reader, i.e. a read
distance of only a few centimeters, then there should be no problem.  If
payment happens at RFID distances, then I'll need Faraday shields for the
tokens, eliminating most of the value of contactless payments.
Cheers - Bill

@_date: 2005-09-23 00:23:40
@_author: Bill Frantz 
@_subject: Java: Helping the world build bigger idiots 
I guess insisting on correct error handling is just for old people.
Peter's example:
has a serious bug in error handling.  We do not know where the
IndexOutOfBoundException was raised.  Was it raised in the while loop,
the expected case; or was it raised in the displayProductInfo method,
due to some bug in that method?  (It could also be raised in some other
method called by displayProductInfo.)
In order for this code to be correct, we would have to prove that the
displayProductInfo method either could not raise this exception, or that
it caught and handled any IndexOutOfBoundException exceptions raised in
it or in methods it calls.  In either case, we must examine the details of
displayProductInfo, and depend on our conclusions remaining correct
during maintenance.  This level of coupling between caller and callee is
too risky for reliable software.
Cheers - Bill

@_date: 2005-09-26 22:21:02
@_author: Bill Frantz 
@_subject: PKI too confusing to prevent phishing, part 28 
One important point is that the dialog box will appear the same, even if
the self-signed cert is signed by a different key.  It has no memory of
previously accessed sites.  It takes something like the petname or
trustbar tools to provide the memory that make self-signed certs like
SSH keys.
Cheers - Bill

@_date: 2006-01-05 15:12:56
@_author: Bill Frantz 
@_subject: browser vendors and CAs agreeing on high-assurance certificat es 
I responded in private email:
Ben responded that I should post my comments to the list.
There are two scenarios I see as being viable for separating the private
keys with a security barrier.  One is the single machine case alluded to
above.  Here the private keys would be in separate security domains, and
the common part of the web server, which listens on the socket, would
read the initial data on the TCP connection, select the correct security
domain, and "pass" the connection to that domain. While the common part
could continue to examine all the data, those data would be encrypted,
so the it would have the same access as any other untrusted node in the
The other scenario involves a network switch which performs the function
of the common code of the web server.  It uses network address
translation to forward the connection's packets to the back-end computer
with the correct private key.  Here the keys are protected by being kept
on separate computers.
Both approaches allow a web hosting ISP to protect its customers from
each other.  This mutual protection is much the same requirement as
existed in the time-sharing systems KeyKOS was designed to support.
Cheers - Bill

@_date: 2006-03-22 14:31:37
@_author: Bill Frantz 
@_subject: Linux RNG paper 
One of my pet peeves: The idea that the "user" is the proper atom of
protection in an OS.
My threat model includes different programs run by one (human) user.  If
a Trojan, running as part of my userID, can learn something about the
random numbers harvested by my browser/gpg/ssh etc., then it can start
to attack the keys used by those applications, even if the OS does a
good job of keeping the memory spaces separate and protected.
Cheers - Bill

@_date: 2006-03-22 19:21:10
@_author: Bill Frantz 
@_subject: Linux RNG paper 
Why should any program (except for the "power box"
) run in my security context. They should all run in their own security contexts.  Plash
 and Polaris
 are examples
of systems where programs do not run in the security context of the
I would agree.  Remember however, (that user == security context) should
not be the norm.  The norm should be to run each program in its own
security context, and allow programs to run parts of themselves in other
security contexts.  (One example of the last would be running the
programs which implement a web site in separate security contexts from
the web server that parses the url to find out what program to run.
The general requirement here is to have authorities that are derived
from the call, along with authorities that are carried with the program.
Think setuid programs in Unix.  They get the ability to read/write the
user's terminal, along with a bunch of privileges that the user doesn't
have.  (IMHO, setuid is a kludge, but a useful kludge, given Unix has
little better.)
This suggestion is a good way of kludging around the basic problem that
most systems equate user with security context, so you, as the writer of
the program, may be sharing a flaky RNG with some other program you
don't trust.
Cheers - Bill

@_date: 2006-05-02 14:07:02
@_author: Bill Frantz 
@_subject: encrypted file system issues (was Re: PGP "master keys") 
[A bit off topic but I thought I'd let it through anyway. Those
uninterested in OS design should skip the rest of this message. --Perry]
I haven't seen the failure specs on modern disk systems, but the KeyKOS
developers ran into an interesting (and documented) failure mode on IBM
disks about 20 years ago.  Those IBM systems connected disks to a
"controller" which was connected to a "channel" which was a specialized
processor with DMA access to the main storage of the system.  Note that
these systems were designed in the days when memory was expensive, so
there was an absolute minimum of buffering in the channel, controller,
and disk.
There are many possible failure modes, including power failure on the
individual components, hardware failure/microprogram failure in the
components, etc.  The failure we experienced was a microcode hang in the
channel (probably caused by a transient hardware failure), which also
stopped the CPU.  The failure occurred while the controller and disk was
writing a block, and the channel ceased providing data.  The
specification for the controller was if the channel failed to provide
data, it filled the block with the last byte received from the channel. If the channel and CPU had been running, the overrun would have been
reported back to the OS with an interrupt.  As it was, all we had was a
partially klobbered disk block.
Since KeyKOS was supposed to be a high reliability OS, we needed to code
for this situation.  Because of the design of the disk I/O system, there
were only two disk blocks (copies of each other) where this kind of
failure could cause a problem.  We defined the format of these blocks so
the last two bytes were 0xFF00.  By checking for this pattern, we could
determine if the block has been partially klobbered.  We then had to
ensure that we checked for correct write on one of the blocks before
starting to write the other.
Does anyone have any idea how modern disks and computers handle similar
Cheers - Bill

@_date: 2006-09-14 16:26:16
@_author: Bill Frantz 
@_subject: Exponent 3 damage spreads... 
In SPKI we used S-Expressions.  They have the advantage of being simple,
perhaps even too simple.
In describing interfaces in the KeyKOS design document
we used a notation similar to S-Expressions which was:
(length, data)
These could be combined into a structure: e.g. (4, len), (len, data) for
data proceeded by a four byte length field.  If you standardize that the
data is always right justified in a field of length "len", and that
binary data is encoded with a standard encoding (hexadecimal,
6-bit/character, decimal etc.), most of the problems I have seen
described in this thread should "just go away".
Some might object that having a specific number of bits for the length
field limits future expansion of this approach.  Indeed, ASN.1 avoids
this issue by allowing the encoding of "infinite" length integers, and
XML does the same.  The cost of that flexibility is much more difficult
encoding and decoding.  If a length field length of 4 to 8 bytes (32 to
64 bits) is chosen, as a practical matter, any length data that is
transmittable in an exchange can be represented.  (A terabit/second is
10**12 bits/second.  32 bits can represent a million seconds at that
data rate.  64 bits can represent much longer data items.)
Cheers - Bill

@_date: 2007-12-31 13:46:55
@_author: Bill Frantz 
@_subject: Death of antivirus software imminent 
My favorite virtual machine use is for the virus to install itself
as a virtual machine, and run the OS in the virtual machine.  This
technique should be really good for hiding from virus scanners.
Cheers - Bill

@_date: 2007-05-14 14:49:31
@_author: Bill Frantz 
@_subject: no surprise - Sun fails to open source the crypto part of Java 
I think it is likely that Sun architected the Java crypto framework to
be able to obey the letter of the export regulations then in effect. They made it so the actual crypto implementations could differ from
country to country, and from supplier to supplier, while trying (not
very successfully IMHO) to provide a consistent application interface.
Cheers - Bill

@_date: 2007-10-12 19:23:18
@_author: Bill Frantz 
@_subject: kernel-level key management subsystem 
Encrypted swap makes the swap hazard go away, and is certainly a sweet
spot for cryptography because most of the difficult problems in key
handling "just go away".  Also, many operating systems support it.
Since keys do not have to last longer than the boot of the OS, they can
be kept in kernel memory. inaccessible to user processes, and safe from
postmortem disk analysis.  If, as most, if not all implementations do,
you have a different key for each address space, the key can go away
when the address space goes away.  You don't need to generate a key
until the first swap out for the address space, so there is time to
gather "entropy" from the timing of events such as disk I/O, network I/O
etc., making the issue of random number generation less of a problem.
For the really paranoid, keep a bit with each key saying whether it is
inverted or not, and invert it regularly to avoid memory "burn in". Also quickly overwrite any key-dependant data used by your symmetric
encryption algorithm.
Cheers - Bill

@_date: 2008-08-26 14:40:01
@_author: Bill Frantz 
@_subject: road toll transponder hacked 
I could see where knowing what the license plate should be, from
the transponder code, could feed back into the OCR and only
generate a hit when the disagreement was obvious.
In the San Francisco Bay Area, they are using the transponder codes
to measure how fast traffic is moving from place to place. They
post the times to various destinations on the electric signs when
there are no Amber alerts or other more important things to
display. It is quite convenient, and they promise they don't use it
to track people's trips.
If one were paranoid, one could put a different ID into the
transponder for each trip, and only put the one it was issued with
into it for toll crossings. :-)
Cheers - Bill

@_date: 2008-12-08 15:33:25
@_author: Bill Frantz 
@_subject: Request for Input (RFI)--National Cyber Leap Year 
NATIONAL SCIENCE FOUNDATION
Request for Input (RFI)--National Cyber Leap Year
AGENCY: The National Coordination Office (NCO) for Networking Information Technology Research and Development (NITRD).
ACTION: Request for Input (RFI).

@_date: 2008-12-14 15:40:10
@_author: Bill Frantz 
@_subject: CPRNGs are still an issue. 
I find myself in this situation with a design I'm working on. I
have an ARM chip, where each chip has two unique numbers burned
into the chip for a total of 160 bits. I don't think I can really
depend on these numbers being secret, since the chip designers
thought they would be useful for DRM. It certainly will do no harm
to hash them into the pool, and give them a zero entropy weight.
The system will be built with SSD instead of HDD, so Damien's
comment hits close to home. I hope to be able to use timing of
external devices, the system communicates with a number of these,
along with a microsecond counter to gather entropy from clock skew
between the internal clock and the clocks in those devices.
Unfortunately the system doesn't normally have a user, so UI
timings will be few and far between.
Short of building special random number generation hardware, does
anyone have any suggestions for additional sources?
Cheers - Bill

@_date: 2008-02-06 17:29:15
@_author: Bill Frantz 
@_subject: Gutmann Soundwave Therapy 
If there had been a separation between the key exchange and
validation part of SSL (early TLS) and the transport part, the E
language protocol[1] almost certainly would have used the transport
part of the protocol.  The reasons at the time for not using SSL are
described in [2].  They are all associated with the connection and
cryptograph setup.
Simplified overview:
When an E program needs to contact a remote E program, it starts
with a hash of the other program's public key and large random
number, the "Swiss number".  It gets the IP and port of the remote
program from a well-known network service called the Process Location
Service.  It then contacts that IP and port, sends its public key,
receives the remote public key, performs a Diffie Hellman exchange
for forward secrecy, checks the hash of the remote public key, and
sends a signature over the exchange.  It checks the remote programs
signature over the exchange, and if all the checks pass, sends the
encrypted Swiss number to identify the specific remote resource.
I couldn't see any way to take this self-authenticating key exchange
and jam it into a x.509 structure.  Perhaps I wasn't inventive
enough, but I ended up rolling my own transport protocol, at certain
extra cost in development and testing, and a significant risk of
security errors.
Cheers - Bill
[1] [2]

@_date: 2008-02-21 12:41:49
@_author: Bill Frantz 
@_subject: cold boot attacks on disk encryption 
Their key recovery technique gets a lot of mileage from using the
computed key schedule for each round of AES or DES to provide
redundant copies of the bits of the key.  If the computer cleared
the key schedule storage, while keeping the key itself when the
system is in sleep mode, or when the screen-saver password mode
kicks in, this attack would be less possible.
If, in addition, the key was kept XORed with the secure hash of a
large block of random memory, as suggested in their countermeasures
section, their attacks would be considerably more difficult.
These seem to be simple, low overhead countermeasures that provide
value for machines like laptops in transit.
Cheers - Bill

@_date: 2008-01-02 23:50:15
@_author: Bill Frantz 
@_subject: Death of antivirus software imminent 
I fully agree that a better OS would go a long way toward helping in
the battle.  We even know some techniques for building a better OS. Consider plash , and Polaris
, both of
which run programs for a user with less than that user's privilege. This technique helps prevent viruses from infecting computers by
denying them write privileges to system files and most of the user's
The model that any program a user runs can do anything that user is
permitted to do is fundamentally broken.  It is the model that all
current popular OSes support, so in that sense these OSes are
insecure.  The only mistake users make in many cases is running
software with bugs such as buffer overruns, where the virus then
uses the user's privileges to take over their system.  In these
cases, IMHO, blaming the user is inappropriate.  And in all cases,
OSes should give the user more support in making sound decisions. See for example: Cheers - Bill

@_date: 2008-01-03 10:04:02
@_author: Bill Frantz 
@_subject: Death of antivirus software imminent 
If, as seems likely, we are moving into a world where virtual
machines are a popular security mechanism, the problem isn't
detecting if you are on a virtual machine, because all useful OSes
will expect to be running on a virtual machine.  The problem will be
to detect if the virtual machine is hostile.  That code will
probably have to be part of the virtual machine monitor (VMM)
implementation.  There may be less use for running VMMs in virtual
machines, although we got a lot of use from the ability to run
VM/370 in a VM/370 virtual machine back in the 70's and 80's.
Neat.  An interrupt at the wrong time would also upset the
TLB/Mapping table mismatch.  I wonder if a VMM could be built to
simulate the mismatch?  Note that on the Intel architecture, there
are certain instructions that behave differently in supervisor mode
and user mode.  These instructions can also be used to leverage VM
Cheers - Bill

@_date: 2008-06-02 17:34:10
@_author: Bill Frantz 
@_subject: Can we copy trust? 
In my real-world experience, this way of gaining trust is only
really used for strangers. For people we know, recognition and
memory are more compelling ways of trusting.
We can use this recognition and memory in the online world as well.
SSH automatically recognizes previously used hosts. Programs such
as the Pet Names Tool recognize public keys used by web sites, and provide us with a
human-recognizable name so we can remember our previous
interactions with that web site. Once we can securely recognize a
site, we can form our own trust decisions, without the necessity of
involving third parties.
Cheers - Bill

@_date: 2008-06-30 17:54:57
@_author: Bill Frantz 
@_subject: The wisdom of the ill informed 
I would say, even if you can do it yourself, hire another expert to
review your design.
When these systems are announced, we should get in the habit of
asking the people announcing them, "Which recognized crypto protocol
and algorithm experts have reviewed your design?"
Cheers - Bill

@_date: 2008-03-17 15:13:15
@_author: Bill Frantz 
@_subject: delegating SSL certificates 
Is that a religious dogma, or a business model masquerading as a
religious dogma?
Whichever it is, it is an impediment to improving protection
against fishing attacks. Consider a large organization like Amazon
and users using tools like the Petname toolbar
. Amazon has many servers,
each of which needs a TLS signing key. In a more ideal world,
Amazon would have a CA signed public certification key, which it
would use to certify each server's TLS signing key. The Petname
toolbar would use Amazon's public certification key as the identity
matching the user's petname for Amazon. Once that petname has been
established, AKA the introduction problem, the identity would be
safe, regardless of what happens higher in the PKI hierarchy. The
higher levels of the PKI hierarchy would only be used during the
introductory contact.
Given the current situation, with the CAs having a monopoly on
issuing certificates, there are many different public keys
associated with Amazon. In addition, Amazon may chose to change the
CA it uses. To handle this situation, the Petname toolbar the DN
instead of a public key, which opens the Petname tool bar to
spoofing by any of the 100 or so CAs configured in the standard
browsers. Does anyone know what happened to Baltimore's signing key
when they went out of business?
Cheers - Bill

@_date: 2008-09-20 14:41:35
@_author: Bill Frantz 
@_subject: Password Recovery Attack 
One attack on services, which use personal questions as a backup
form of user verification, works well for high-profile users of
these systems. The attack is very simple. Go into the password
recovery page, and use Google to look up the answers to the
personal questions asked. There is enough Googleable data around
for high-profile people, and perhaps not so high profile people,
that the attack can be successful often enough to be useful. My
sources say Sarah Palin's email account was breached using this
Cheers - Bill

@_date: 2008-09-24 15:50:18
@_author: Bill Frantz 
@_subject: Fake popup study 
I would suggest that, in the "real world", most of the people that
are nearly impossible to protect, don't have much money. Now "real
world" scams have been around for quite a while, and we teach about
them in school. However they still work with some people, which is
why those people don't have much money.
Online scams are newer, and many of their victims left school long
before the scams became popular. I expect the online situation will
stabilize in about the same way as the "real world" one has.
Cheers - Bill

@_date: 2008-09-24 15:53:50
@_author: Bill Frantz 
@_subject: Fake popup study 
My 96 year old mother does not have a check book or credit cards.
All her bills are paid through her lawyer's office. QED.
Cheers - Bill

@_date: 2009-01-24 15:22:21
@_author: Bill Frantz 
@_subject: Bitcoin v0.1 released 
Some people tell me that the 0wned machines are among the most secure on
the network because botnet operators work hard to keep others from
compromising "their" machines. I could see the operators moving toward
being legitimate security firms, protecting computers against compromise in
exchange for some of the proof of work (POW) money.
Cheers - Bill

@_date: 2009-01-28 16:14:30
@_author: Bill Frantz 
@_subject: Proof of Work -> atmospheric carbon 
While I think this statement may be true for POW coinage, because for a bot
net it "grows on trees", for money that traces back to the international
monetary exchange system, it may not be completely true.
Snail mail postage limits, but does not eliminate junk mail. I think,
without proof, that most people can live with the amount of junk mail they
receive. At least I don' hear a lot of conversations about the "Junk mail
Now it is certainly true that if machines have a small amount of money
stored within them for "postage", someone who 0wns that machine could steal
some of that money. There is a limit to the amount that can be stolen based
on the person who pays for the machine noticing and being bothered. There
is probably safe profit in skimming small amounts from large number of
machines just like there was profit in skimming the round off in payroll
Cheers - Bill

@_date: 2009-07-08 15:31:11
@_author: Bill Frantz 
@_subject: Weakness in Social Security Numbers Is Found 
How separate algorithms reduce security when used together:
The last 4 digits of the SSN are frequently used as an authenticator. These
may be the hardest digits to recover with the technique which, according to
the researchers (Alessandro Acquisti and Ralph Gross) at CMU, would not be
easy for cybercriminals to reconstruct but would be within the grasp of
sophisticated attackers.
My solution is to have the Social Security Administration announce that
they will publish names and SSNs for everyone in their database on a
certain date. Fat chance it will happen.
Cheers - Bill

@_date: 2009-06-28 13:02:03
@_author: Bill Frantz 
@_subject: password safes for mac 
You should probably use the encrypted swap feature on the Mac.
System Preferences -> Security -> Use secure virtual memory.
Cheers - Bill

@_date: 2009-05-06 14:51:13
@_author: Bill Frantz 
@_subject: Has any public CA ever had their certificate revoked? 
It seems to me that there are a number of problems with the current CA
situation. Since no CAs have been identified by name (except Verisign for a
very old problem), it is hard for me to reduce the reputation of a specific
CA. Even if one was identified, it's not clear what I could do to move
business to more responsible CAs.  So my reaction is to say that it's all a
big stinking pile and try to develop systems and procedures that don't rely
on CAs. (e.g. curl with a copy of the server's self-signed certificate, the
Petname toolbar, etc.)
If SSL/TLS had as part of its handshake, a list of CAs that are acceptable
to the client, I could configure my browser with only high-reputation CAs.
This step would probably make it desirable for servers to get certificates
from more than one CA so they could return a certificate signed by an
acceptable CA. It would certainly allow for some market pressure on CAs,
and high reputation CA might be able to charge more for certificates.
(The last time I ran into a case where the server certificate was not
signed by a CA on my browser's default list, I used the 800 number instead.
That was for activating a credit card.)
In addition, I am worried that some countries cyber-warfare department has
a copy of some well-installed CA's signing key and can generate
certificates whenever it wants. When D-day comes, it will spoof DNS and use
the certificates to disrupt the economy of its target country. If we had a
2 level security system, with CAs for the first introduction, and something
more robust for subsequent sessions, these attack scenarios would be less
Cheers - Bill

@_date: 2009-05-06 23:32:00
@_author: Bill Frantz 
@_subject: Has any public CA ever had their certificate revoked? 
The client hello message would include the list of acceptable CAs. The
server could use that list to select an acceptable certificate to return to
the client. In the rare cases where there is a client certificate, the
server hello could include a similar list and the client could use it to
select an acceptable certificate. If the lists aren't included in the hello
messages, the behavior is the same as the current versions of SSL/TLS.
Yes, I know I'm way out in left field, but I just might not go to a web
site if I cared about security with my transaction and the site didn't use
a reasonable CA. There are many alternatives both with competitor
organizations, and competitive communication techniques. For example, if I
didn't like the CA my bank used, I could either change banks or do my
banking by phone or in person at a local branch.
I have avoided many sites that want user names and passwords, or want me to
turn on Javascript. The popularity of the noscript plugin for Firefox means
that perhaps I'm not the only one "out in left field".
Cheers - Bill

@_date: 2009-11-01 18:30:15
@_author: Bill Frantz 
@_subject: deterministic random numbers in crypto protocols -- Re: Possibly questionable security decisions in DNS root management 
One concern is that if the encryption key is deterministically generated
from the data, then the same plain text will generate the same cypher text,
and a listener will know that the same message has been sent. The same
observation applies to a DSA signature. If this leakage of information is
not a problem, e.g. the signature is encrypted along with the data using a
non-deterministic key, then there doesn't seem to be anything obvious wrong
with the approach. (But remember, I'm far from an expert.)
Cheers - Bill

@_date: 2009-11-18 09:22:01
@_author: Bill Frantz 
@_subject: Crypto dongles to secure online transactions 
Perhaps I'm missing something, but my multiple banks will all accept my
signature when made with the same pen. Why wouldn't they not accept my
signature when made with the same, well protected, signing/user verifying
device. I might have to take it to the bank to give them its public key in
person, but that seems a minor inconvenience.
This kind of device sounds like a fine device for a banking industry
committee to specify.
Cheers - Bill

@_date: 2009-11-21 15:12:22
@_author: Bill Frantz 
@_subject: Crypto dongles to secure online transactions 
So surely someone has built a portable reader for counterfeiting the cards
they read in restaurants near big target companies...
Cheers - Bill

@_date: 2010-08-01 14:59:59
@_author: Bill Frantz 
@_subject: A mighty fortress is our PKI, Part II 
Err, umm, this last week. I'm in a place where cell coverage (AT&T, Verizon has a better reputation) is spotty and internet is a dream due to a noisy land line. I needed to find a ceramic tile store. The paper yellow pages had survived being left in the driveway in the rain and I used it.
However, I agree that this is the <2% case for many parts of the world.
Cheers - Bill

@_date: 2010-08-24 15:04:14
@_author: Bill Frantz 
@_subject: [IP] Malware kills 154 
This came in from SANS NewsBites Vol. 12 Num 67 : Did a computer virus cause the 150 deaths in the Spanair crash?
  --Judge to Examine Evidence on Malware in Spanair Fatal Air Crash Case
(August 20 & 23, 2010)
A Spanish judge will investigate whether or not malware on a Spanair
computer system had anything to do with the system's failure to raise
alerts prior to a 2008 airplane crash that killed 154 of 172 people on
board.  The official cause of the crash was pilot error; the pilots were
found to have failed to extend the airplane's take-off flaps and slats.
However, the investigation also found that a warning system failed to
alert the pilots that the flaps and slats had not extended and had also
failed to do so on two previous occasions.  Each failure should have
been logged into Spanair's maintenance system, which was found to be
infected with malware.  Three failures would have triggered an alarm
that would have kept the airplane grounded until the problem was fixed.
The judge has called for Spanair to release computer logs for the days
before and after the crash.  The malware infection appears to have
spread through a flash drive.
Internet Storm Center: [Editor's Note (Schultz): This is a potentially very significant turn
of events. If the loss of 172 lives can be traced to the presence of
malware, corporate executives and government officials are likely to
take security risk management much more seriously than they generally
now do.]
OBLegal: Please feel free to share this with interested parties via email, but
no posting is allowed on web sites. For a free subscription, (and for
free posters) or to update a current subscription, visit
Cheers - Bill

@_date: 2010-07-25 09:24:52
@_author: Bill Frantz 
@_subject: A Fault Attack Construction Based On Rijmen's Chosen-Text Relations Attack 
On 7/21/10 at 11:49 AM, daw at cs.berkeley.edu (David Wagner) wrote, with some drastic editing which I hope doesn't change David's meaning:
My favorite paper in this style is one which has not (yet) been published. It turns out that at one time there were at least three Mark Millers active in computer science. One of them, cced above, wanted to publish a paper:
   Global Names Considered Harmful
   by Mark Miller, Mark Miller, and Mark Miller
And the paper really doesn't need to go any further than this.
Cheers - Bill

@_date: 2010-03-23 12:00:40
@_author: Bill Frantz 
@_subject: "Against Rekeying" 
Eric didn't mention it in his blog post, but he has been deeply involved
in cleaning up the mess left by a protocol error in in SSLv3 and
subsequent TLS versions. This error was in the portion of the protocols
which supported rekeying and created a vulnerability that affected all
users of those protocols, whether they used the rekeying part or not.
The risks from additional protocol complexity must be balanced with the
benefits of including the additional facility. My own opinion is that in
this case, the benefits didn't justify the risk. The few applications
which desired rekeying could have been designed to build a completely
new TLS connection, avoiding the risk for everyone.
Cheers - Bill

@_date: 2010-09-14 14:57:05
@_author: Bill Frantz 
@_subject: Intel plans crypto-walled-garden for x86 
Does that include monetary indemnity when the "known-good" turns out to be bad? I bet not.
If we could "know good", security would be a lot easier, but nobody has a clue how to actually achieve that knowledge.
I expect Steve Jobs will get them to approve MacOS too.
For the rest, there's always AMD.
Cheers - Bill

@_date: 2013-12-06 15:51:03
@_author: Bill Frantz 
@_subject: [Cryptography] Kindle as crypto hardware 
When planning small hardware for security, the more implementations on different hardware the better. Each chip will require its own backdoor which will probably have to be specifically tailored for each software implementation. Life got harder for massive data collection.
Cheers - Bill

@_date: 2013-12-11 10:59:40
@_author: Bill Frantz 
@_subject: [Cryptography] Kindle as crypto hardware 
The evidence from the pieces of backdoored technology that NSA has been involved in is that they prefer backdoors which they can use and no one else can. Clipper has a specific NSA key to encrypt the session key. DUAL_EC_DRBG had a similar feature. These protocols remained "secure" from those that didn't have the keys.
Note that many business organizations might be quite happy knowing that NSA could read their traffic as long as NSA maintains its "Never Say Anything" reputation. NSA's mistake was passing information about criminal activities to law enforcement rather than sticking to national security. That change of policy scared many businesses, since a clever prosecutor can find something illegal in almost any activity.
I think the SSL/PKI debacle speaks more of incompetence and a strong desire to preserve a revenue model. NSA has just taken advantage of what they found.
Cheers - Bill

@_date: 2013-12-19 09:17:36
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
One should be fairly safe using commodity FPGAs and probably get a decent improvement in speed. Of course the question is, how much speed will be needed? A fair bit for 4096 bit RSA. Considerably less for 128 bit AES. Very little for secure random source to be read from another CPU.
Cheers - Bill

@_date: 2013-12-22 08:09:35
@_author: Bill Frantz 
@_subject: [Cryptography] Passwords are dying - get over it 
Passwords for high levels of security are a dying technology.
The level of entropy we can reasonably ask a human to remember is small compared with the cost of exhaustive search attacks. They can still be useful for medium levels of security like Facebook pages -- where powerful attackers don't need to break the password, but not for the higher levels of security needed for uses like banking.
Using passwords securely is inconvenient. You need a different password for each site because of the risk of site compromise. It is insecure to use variants of a common base because they are too easy to guess once one of them is known.
The only reasons passwords hang on is old habits and the need to support secure usage from computers at cyber cafes. The need for this latter use is dying with the popularity of laptops, tablets and smart phones. (As if anything could be done securely from a compromised public computer.)
Password safes such as the Apple key chain offer a solution, but with them we are applying computation and memory that are personal to each user, so can use solutions which don't involve the site storing a version of the password.
Discussion of ways to eke out a bit more life for passwords seems kind of pointless. Lets instead build things that are both more secure and easier to use. We need to define a protocol and a migration strategy.
Cheers - Bill

@_date: 2013-12-22 18:27:09
@_author: Bill Frantz 
@_subject: [Cryptography] how reliably do audits spot backdoors? (was: Re: 
See  Building Reliable Voting Machine Software
    Ka-Ping Yee
A dissertation submitted to the Graduate Division
of the University of California, Berkeley
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Computer Science
p136ff for an experiment where reviewers attempted to find three bugs that had been inserted in some voting machine code.
The results were (p148):
"David Wagner and I decided to insert three bugs into Pvote to
see if the reviewers would find them. We inserted what we
thought would be an ?easy? bug, a ?medium? bug, and a ?hard
bug? to find, and chose each bug individually in such a way that
an insider could conceivably exploit the bug to influence the
results of an election. These bugs are detailed in Appendix
We decided to insert all of these bugs in a 100-line region of
a single file, lines 11 to 109 of Navigator.py, and told the
reviewers to look in this region. We did this both because the
navigator was the most interesting in terms of the program
logic and because we knew the reviewers would have limited
time. The new version of the code that we gave the reviewers
contained all three bugs, but we did not tell the reviewers how
many bugs there were.
Yoshi Kohno, Mark Miller, and Dan Sandler participated as
reviewers on the third day of the review. Dan was very familiar
with Python and found the ?easy? and ?medium? bugs quickly,
within about 70 minutes. Yoshi Kohno and Mark Miller found
the ?easy? bug after about four hours of reviewing. None of the
reviewers found the ?hard? bug
Ian Goldberg and Yoshi Kohno participated as reviewers on
the fourth day of the review. Ian Goldberg also found the ?easy?
bug within about two hours; none of the other bugs were found
on the fourth day.
The reviewers spent a total of about 20 reviewer-hours
focused on the task of finding the bugs in this 100-line section
of Navigator.py.
Cheers - Bill

@_date: 2013-12-23 09:36:11
@_author: Bill Frantz 
@_subject: [Cryptography] how reliably do audits spot backdoors? (was: Re: 
A minor correction: This is Ka Ping Yee's work. David Wagner was one of his thesis advisors.
I know some of the people who doing the code review. They are very good at finding obscure bugs in pieces of code, including timing bugs and overflow bugs. The small number of bugs actually found is quite scary.
BTW, Ping has done some excellent work in the area of UIs and secure systems.
Note that the bugs were limited to 100 lines of code because of the limited amount of time available for the code review. A real system would probably consist of many times 100 lines of code, especially if the compiler and runtime environments are included. Since backdoors can be designed that depend on "innocent" insertions in several separate parts of the code, the complexity of the search goes up faster than linearly with code size.
Obscure code has no place in any security system.
Cheers - Bill

@_date: 2013-12-23 09:36:12
@_author: Bill Frantz 
@_subject: [Cryptography] Passwords are dying - get over it 
I suggest a signature scheme which operates automatically. Each user has a private key which is kept secure using the normal technologies. There is no magic here, and there is no improvement over the current practices of keeping private keys in TPMs, dongles, files, password encrypted files etc.
I'll use web site login as an example, because it is common and a necessary authentication problem to solve. There are several directions one could go, and they aren't mutually exclusive, although a single web site would probably use only one of them.
     * The private key can be used with a client-side cert and
     TLS. This solution could provide automatic login, which is
     easier for the user than entering a username and password.
     * The web page site can do authentication at the HTTP level
     by offering a nonce to which the client adds another nonce
     and signs both of them. This solution can be coded to
     transparently revert to user name + password as a migration
     strategy.
     * etc.
One thing to remember, don't let the impossible best be the enemy of the better. Keeping secrets from well-funded attackers with direct access to all the user's hardware is an unsolved problem. Don't throw out improved resistance to attacks such as cross-site password guessing and low entropy secrets because the solution doesn't solve a problem that passwords can't solve either.
Cheers - Bill

@_date: 2013-12-24 15:27:29
@_author: Bill Frantz 
@_subject: [Cryptography] Why don't we protect passwords properly? 
I must say, these attacks don't seem to be common. Are there any examples of these attacks being used in the real world?
Swap encryption is the sweet spot of cryptography because all the key management problems go away. You don't even need to generate the key until the first swap out so you have lots of event timings to seed your random number generator. Use it and the swap problem goes away.
The cold boot attack goes away if you leave your device off during the times of greatest risk, like going through airport security or customs.
These attacks pale into insignificance compared with the know attacks on passwords. It is better to spend effort mitigating the common attacks than worrying about attacks that are easily avoided.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2013-12-24 15:27:30
@_author: Bill Frantz 
@_subject: [Cryptography] Passwords are dying - get over it 
This is a password that I will have to be entering every day or write down. (I'm an old man and my memory isn't as good as it used to be.) There are three words, hamlin, naiad, and allyn that I, as a native English speaker can't define. (The spell checker fails hamlin and allyn.) I'd have to learn to spell at least two of them.
The need for entropy in passwords has already passed my diminished abilities. If you're looking for universal adoption, there's a problem.
Cheers - Bill

@_date: 2013-12-24 20:25:03
@_author: Bill Frantz 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Possibly correct, but only if it is a cheaper attack than others. There are so many cheaper attacks that this attack is only a long range consideration. It's an interesting intelectual challenge, but not yet of practical importance.
OK, when is the cold boot attack a practical attack?
The original question was attacks against swap images on disk. Encryption prevents that attack and is available on many systems by checking an option.
In general, protecting one VM from another VM running on the same hardware is a hard problem. As with many things in life, if it hurts, don't do it.
Probably the best approach is to provide virtual clocks to prevent timing attacks. Tough luck for timestamping and random number generation though. Probably you can allow timestamping accurate to the second if you don't use the secret too frequently. Random numbers can be provided through virtual hardware random number generators, but addressing all the justified paranoia in this area will require examining the whole system from hardware to final use.
EM attacks can be interesting practical attacks, but Tempest packaging prevents them. While we are worrying about this style of attack, lets consider the nanotech quadcopter the size of a dust mote which can look over our shoulders and monitor our key strokes. That device is probably not too many years in the future.
Password stretching can only limit certain attacks. It does nothing to limit attacks involving compromise of a site's password file and the ensuing damage. The tendency of people to use the same or similar passwords on multiple sites makes this attack quite effective.
Working to eliminate passwords entirely seems to me to be a much better approach. We could use client side TLS certs today, but there are probably better solutions.
As an aside, and if you decide to respond to this paragraph, please fork a new subject: Does there need to be a revenue model which can support a business to get wide adoption for a security technology? The revenue for CA certs certainly has encouraged a number of businesses to support the CA model for TLS security. This support is evidenced by standards body participation and PR flacks.
Cheers - Bill

@_date: 2013-12-25 18:25:57
@_author: Bill Frantz 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Let me try to describe where I'm coming from. The way I see it, we live in a world where major portions of our online life are under constant attack from adversaries with far few resources than the National Scale Adversaries (NSAs) we all worry about. In addition to these run-of-the-mill attacks, I expect the NSAs are quite busy doing industrial spying for their national champions, a situation which should worry any technology company.
This statement is not universally true. It takes only one example to prove my statement. Shortly after DES was standardized with its 56 bit key, some people published a paper, I think Whit Diffie was one of the authors, suggesting that a DES cracking machine could be built. Anyone wanting to use DES could include that published paper in their risk analysis. At the time of course, only NSAs had the budget for such a machine. Many years later Moore's law allowed the EFF to build the first publicly acknowledged DES cracker.
However, given the current security situation, I am find ways of protecting against attacks which aren't seen in practice at best of academic interest. Discovering protections is a fun exercise, but it isn't addressing the problems which are killing us today.
If one believes Snowden, our algorithms are OK, but our protocols and procedures are questionable. For my part, I worry about random number generators, CAs, spear phishing, and the Hoovering of unencrypted metadata. There are probably other things I should worry about, but side channel attacks, EM emission, and power analysis don't seem to be a real threat to me and my laptop. (GIven the current state of OS security, it is easier for an attacker to get root than to setup to use any of these attacks.)
Achieving security in todays network world in a fascinating combination of technology, psychology, economics, and politics. Pure technical solutions don't cut it now, and as I learned late in my career, never did.
So you have no realistic attack model. And I gave you at least two in a previous email along with a practical protection against them. Sad, with so many attack methods that are succeeding today, to spend time worrying about cold boot.
So long as you can make it not hurt. Even modern medicine can't make everything not hurt. When I fell 4 meters in a cave, it hurt. I intend to try very hard not to fall again.
Some of the timing attacks between VMs running on common hardware seem very hard to protect against. With current technology dedicated hardware is clearly the cheaper choice. The same kind of argument applies to sources of secure random numbers. If you need to generate a SSH key early in bringing up a system, either include a hardware USB random source, or plug in a KVM and run the mouse around on the screen. Don't compromise your security for ease of administration unless it is a low security system.
I like Jerry's analysis:
Cheers - Bill

@_date: 2013-12-25 19:05:01
@_author: Bill Frantz 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
When the US supreme court decided that a woman had no "expectation of privacy" for what went on under her skirts, I knew privacy was dead. I then thought about what my grandmother would say, but didn't want be exposed to something so icy.
When I translated the German I was literally laughing out loud:
The bigest problem I can see with leaving the third parties out is that is -- where's the revenue model that provides an economic incentive to drive adoption? Even when third parties start out with a privacy goal, they provide a place to pry as seen by RIM's and Skype's dance with the national security agencies. There needs to be a revenue model, perhaps a distributed revenue model like Bitcoin's enabling of low cost electronic monitory exchange and the opportunity to make money by minting.
General purpose hardware manufacturers are as rare as Unicorns, making them a logical target for black coercion. A possible solution to hardware compromise is to run crypto code through one or more layers of interpretation, so it will be hard for the hardware to detect what computations are being performed.
Cheers - Bill

@_date: 2013-12-28 10:56:59
@_author: Bill Frantz 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
There is an amusing story about SCOMP. SCOMP was designed to get an A1 security rating, and after jumping through all the hoops actually received that rating. Somewhat after that, someone discovered a quite high bandwidth covert channel in SCOMP. One of the developers was heard to quip, "I don't want to call is secure. I just want to call it A1."
Cheer - Bill

@_date: 2013-11-01 21:16:23
@_author: Bill Frantz 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
The finger swipe used to wake up an iPhone provides a bunch of entropy.
If we assume that the swipe takes 250 milliseconds and we sample the finger x,y position every 100 microseconds then we get 2500 samples. The screen resolution is 960x640. The finger will travel at least 500 pixels horizontally with a vertical uncertainty of at least 800 pixels. (Yes, I tried it. The swipe works over almost the whole vertical extent of the screen.)
We will see 1 pixel of horizontal motion approximately once every 5 samples. The exact sample pair where we see it provides the horizontal entropy, or 500 bits for the swipe. The vertical motion will provide a few more bits of entropy -- say about 200 for starting position and another 50 for up/down motion.
It shouldn't be hard to seed a random number generator from just the wakeup swipe.
[I'm all in favor of seed pools, etc. etc. etc. The more sources the better. But high precision UI event timings are really hard to guess, even with a camera watching the interaction.]
Cheers - Bill

@_date: 2013-11-04 20:23:15
@_author: Bill Frantz 
@_subject: [Cryptography] Embedded device key generation problems due to bad 
describes net scans for bad RSA keys on the web performed by Zakir Durumeric, Eric Wustrow, Alex Halderman, and Nadia Heninger. Among their conclusions:
We manually verified that 59,000 duplicate keys were repeated due to entropy problems, representing 1% of all certificates, or 2.6% of self-signed certificates. We also found that 585,000 certificates, or 4.6% of all devices used the default certificates pre-installed on embedded devices.
More surprisingly, we discovered that entropy problems can allow a remote attacker with no special access to factor a significant fraction of the RSA keys in use on the Internet. We were able to factor 0.4% of the RSA keys in our SSL scan. We did this by computing the greatest common divisor (GCD) of all pairs of moduli from RSA public keys on the Internet.
However, there?s no need to panic as this problem mainly affects various kinds of embedded devices such as routers and VPN devices, not full-blown web servers.
The list of vulnerable devices that we have already identified includes more than thirty different manufacturers, including almost all of the biggest names in the computer hardware industry. The kinds of products that we identified include firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones.
This is a problem, but it?s not something that average users need to worry about just yet. However, embedded device manufacturers have a lot of work to do, and some system administrators should be concerned. This is a wake-up call to the security community, and a reminder to all of how security vulnerabilities can sometimes be hiding in plain sight.
Cheers - Bill

@_date: 2013-11-07 07:15:28
@_author: Bill Frantz 
@_subject: [Cryptography] randomness +- entropy 
In this case there is an active UI with an attached keyboard and mouse. It is probably a really major change to Linux boot, but use any excuse to get user to move the mouse and you have plenty of "randomness" to seed the RNG.
Cheers - Bill

@_date: 2013-11-23 22:56:42
@_author: Bill Frantz 
@_subject: [Cryptography] Dark Mail Alliance specs? 
The one time I used the web of trust for a PGP key it was for my own key. (I usually get a fingerprint directly from a person.)
I needed to send some confidential information from work to home. I didn't have my key fingerprint with me, or my key. I downloaded my key from a key server. It was signed by Carl Ellison, who I trusted to not have signed any other "Bill Frantz" key. I had Carl's business card with his key fingerprint, so I was able to complete the trust chain. QED.
Cheers - Bill

@_date: 2013-10-01 13:12:40
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
In cave rescue the National Cave Rescue Commission (a training organization) uses a 7:1 system safety ratio in its trainings. This is for building systems where people could be seriously hurt or killed if the system fails.
Cheers - Bill, NCRC instructor

@_date: 2013-10-01 13:16:01
@_author: Bill Frantz 
@_subject: [Cryptography] are ECDSA curves provably not cooked? (Re: RSA 
Or NSA could have done what it did with DES and chosen a construct that didn't have that weakness. We just don't know.
Cheers - Bill

@_date: 2013-10-01 15:38:50
@_author: Bill Frantz 
@_subject: [Cryptography] Why is  emailing me my password? 
Let Mailman assign you a password. Then you don't have to worry about someone collecting all your mailing list passwords and reverse engineering your password generation algorithm. You'll find out what the password is in a month. Save that email so you can make changes. Get on with life.
Lets not increase the level of user work in cases where there isn't, in fact, a security problem.
I'm interested in cases where Mailman passwords have been abused.
Cheers - Bill

@_date: 2013-10-02 22:34:16
@_author: Bill Frantz 
@_subject: [Cryptography] Why is  emailing me my password? 
And while you're at it, show me the cost of the abuse.
Cheers - Bill

@_date: 2013-10-07 22:11:28
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
We seriously need to consider what the design lifespan of our crypto suites is in real life. That data should be communicated to hardware and software designers so they know what kind of update schedule needs to be supported. Users of the resulting systems need to know that the crypto standards have a limited life so they can include update in their installation planning.
Cheers - Bill

@_date: 2013-10-08 13:46:07
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I think the situation is much more serious than this comment makes it appear. As professionals, we have an obligation to share our knowledge of the limits of our technology with the people who are depending on it. We know that all crypto standards which are 15 years old or older are obsolete, not recommended for current use, or outright dangerous. We don't know of any way to avoid this problem in the future.
I think the burden of proof is on the people who suggest that we only have to do it right the next time and things will be perfect. These proofs should address:
     New applications of old attacks.
     The fact that new attacks continue to be discovered.
     The existence of powerful actors subverting standards.
     The lack of a "did right" example to point to.
Users of this old equipment will need to make a security/cost tradeoff based on their requirements. The ham radio operator who is still running Windows 98 doesn't really concern me. (While his internet connected system might be a bot, the bot controllers will protect his computer from others, so his radio logs and radio firmware update files are probably safe.) I've already commented on the risks of sending Mailman passwords in the clear. Low value/low risk targets don' need titanium security.
The power plant which can be destroyed by a cyber attack, c.f. STUXNET, does concern me. Gas distribution systems do concern me. Banking transactions do concern me, particularly business accounts. (The recommendations for online business accounts include using a dedicated computer -- good advice.)
Perhaps the shortest limit on the lifetime of an embedded system is the security protocol, and not the hardware. If so, how do we as society deal with this limit.
Cheers -- Bill

@_date: 2013-10-09 22:41:18
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
We should try to characterize what "a very long time" is in years. :-)
We had barely-strong-enough crypto because we couldn't afford the computation time for longer key sizes. I hope things are better now, although there may still be a problem for certain devices. Let's hope they are only needed in low security/low value applications.
I fully agree that this is a valuable area to research.
Defense in depth has been useful from longer ago than the Trojans and Greeks.
I think that the attacks on MAC-then-encrypt and timing attacks were first described within the last 15 years. I think it is only normal paranoia to think there may be some more equally interesting discoveries in the future.
Most definitely! Lots of eye. Formal proofs because they are a completely different way of looking at things. Simplicity. All will help.
This is the direction I'm pushing today. If you look at auto racing you will notice that the safety equipment commonly used before WW2 is no longer permitted. It is patently unsafe. We need to make the same judgements in high security/high risk applications.
Cheers - Bill

@_date: 2013-10-09 22:41:17
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
When I developed the VatTP crypto protocol for the E language  about 15 years ago, key sizes of 1024 bits were high security. Now they are seriously questioned. 3DES was state of the art. No widely distributed protocols used Feige-Fiat-Shamir or Schnorr signatures. Do any now? I stand by my statement.
... long post of problems with TLS, most of which are valid criticisms deleted as not addressing the above questions.
I agree with this general direction, but I still don't have the warm fuzzies that good answers to the above questions might give. I have seen too many projects to "do it right" that didn't pull it off.
See also my response to John Kelsey.
Cheers - Bill

@_date: 2013-10-11 14:50:58
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Look at the E language sturdy refs, which are a lot like the Foolscap references. They are documented at .
Cheers - Bill

@_date: 2013-10-15 15:15:54
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Well, lets be careful if they are both stream cyphers. And downright scared if they are both the same stream cypher. :-)
Layering a stream cypher on top of a block mode cypher seems a better bet. Even if the block mode is CBC. BTW, AES in counter mode is a stream cypher.
[I like the direction John is proposing, even if I haven't had time to chase down the modes he's suggesting.]
Cheers - Bill

@_date: 2013-10-17 19:38:19
@_author: Bill Frantz 
@_subject: [Cryptography] /dev/random has issues 
I am quite happy to have the discussion here on the Cryptography list. It doesn't produce enough volume to be annoying, and it has good technical content.
Real world vs. theory is always interesting. :-)
Cheers - Bill

@_date: 2013-10-21 17:06:10
@_author: Bill Frantz 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
The simplest version is to share the "secret" PGP key. The one time I used the technique, in a very small group of people, it worked quite well.
Cheers - Bill

@_date: 2013-10-26 20:11:00
@_author: Bill Frantz 
@_subject: [Cryptography] provisioning a seed for /dev/urandom 
Pity. All that fan noise should make a good random source.
In all seriousness though, you can cut costs so far that you can no longer implement a security architecture that depends on cryptographic random numbers. If you need that kind of architecture, then pay for a random source. Retrofitting a USB dongle might be the cheapest solution for existing hardware. A random source should be a required feature for new hardware which will run applications requiring cryptographic random numbers.
Cheers - Bill

@_date: 2013-10-29 20:47:02
@_author: Bill Frantz 
@_subject: [Cryptography] My comments regarding using CPU jitter for 
And in 5 years time, someone will build hardware that uses the same oscillator for both the CPU clock and the Ethernet NIC, doing to clock jitter entropy what solid state disks did to Don Davis' "Cryptographic randomness from air turbulence in disk drives" approach.
Cheers - Bill

@_date: 2013-09-05 21:03:28
@_author: Bill Frantz 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
I'm currently over 250 messages behind, so please pardon me if this item has already been mentioned.
Back in 2009, Charlie Landau and I worked on a DARPA contract to demonstrate a secure web key server[1]. We used CAPROS[2] as the underlying operating system and build a HTTP interpreter to act as the server. The system is GPL and the source for the web key server is available on Sourceforge[3].
Charlie comments that the IDL files are quite useful, but there really isn't any documentation. Let me give a brief overview:
When a new TCP connection arrives, a new instance of the web key server is created. It can not communicate with any other instance of the web key server, and the only real authority it has, beyond sending and receiving on the TCP circuit, is to a name lookup system.
This name lookup system takes a string -- the secret part of the web key -- and returns a resource. The web key server then returns the contents of that resource to the requestor.
Since the name lookup system does not allow enumeration of its contents, even if an instance of the web key server is compromised, an attacker will still have to guess the secret part of the web key to retrieve authorities from the name lookup system.
Cheers - Bill
[1] Web key: [2] , [3]

@_date: 2013-09-16 09:44:01
@_author: Bill Frantz 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
After Rijndael was selected as AES, someone suggested the really paranoid should super encrypt with all 5 finalests in the competition. Five level super encryption is probably overkill, but two or three levels can offer some real advantages. So consider simple combinations of techniques which are at least as secure as the better of them.
Unguessable (aka random) numbers:
   Several generators, each reseeded on its own schedule, combined
   with XOR will be as good as the best of them.
Symmetric encryption:
   Two algorithms give security equal to the best of them. Three
   protect against meet-in-the-middle attacks. Performing the
   multiple encryption at the block level allows block cyphers to
   be combined with stream cyphers. RC4 may have problems, but
   adding it to the mix isn't very expensive.
Key agreement:
   For forward security, using both discrete log and elliptic
   curve Diffie-Hellman modes combined with XOR to calculate
   keying material is as good as the better of them. Encrypting a
   session key with one public key algorithm and then encrypting
   the result with another algorithm has the same advantage for
   "the normal mode" of TLS key agreement if you don't want
   forward security (which I very much want).
   Two MACs are better than one. :-)
All this has costs, some of them significant, but those costs should be weighted against the security risks. Introducing a new algorithm with interesting theoretical security properties is a lot safer if the data is also protected with a well-examined algorithm which does not have those properties.
Cheers - Bill (who has finally caught up with the list)

@_date: 2013-09-16 15:20:37
@_author: Bill Frantz 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
This kind of result is why us crypto plumbers should always consult real cryptographers. :-)
I am not so much trying to make the construction better than the algorithms being used, like 3DES is much more secure than 1DES, (and significantly extended the useful life of DES); but to make a construction that is at least as good as the best algorithm being used.
The idea is that when serious problems are discovered with one algorithm, you don't have to scramble to replace the entire crypto suite. The other algorithm will cover your tail while you make an orderly upgrade to your system.
Obviously you want to chose algorithms which are likely to have different failure modes -- which I why I suggest that RC4 (or an extension thereof) might still be useful. The added safety also allows you to experiment with less examined algorithms.
Cheers - Bill

@_date: 2013-09-16 17:47:11
@_author: Bill Frantz 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Let me apply the ideas to the E communication protocol . The code is available on the ERights site .
Cutting out the details about how IP addresses are resolved, the initiator sends a series of messages negotiating the details of the connection and uses Diffie-Hellman for session key agreement.  --  Change the protocol to use both discrete log and elliptic curve versions of Diffie-Hellman, and use the results of both of them to generate the session key. I would love to have a key agreement algorithm other than Diffie-Hellman to use for one of the two algorithms to get a further separation of failure modes.
Authentication is achieved by signing the entire exchange with DSA.  --  Change the protocol to sign the exchange with both RSA and DSA and send and check both signatures.
In all cases, use algorithm bit lengths acceptable by modern standards.
The current data exchange encryption uses SHA1 in HMAC mode and 3DES in CBC mode with MAC then encrypt. The only saving grace is that the first block of each message is the HMAC, which will make the known plain text attacks on the protocol harder. -- I would replace this protocol with one that encrypts twice and MACs twice. Using one of the modes which encrypt and MAC in one operation as the inner layer is very tempting with a different cypher in counter mode and a HMAC as the outer layer.
I'm not sure you can avoid that one-way hash function in practice. Either it will be distilling randomness in your RNG or it will be stretching the "pre-master secret" in your key/IV/etc generation. You could use several and XOR the results if you can prove that their outputs are always different.
The above proposal uses two different digital signature algorithms, sends both, and checks both. I think it meets the "no worse than the best of the two" test.
Cheers - Bill

@_date: 2013-09-17 14:46:29
@_author: Bill Frantz 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Both Perry and Ian point out:
Absolutely! The techniques I suggested used the simplest combining function I could think of: XOR. But complexity is the mortal enemy of reliability and security.
I don't have any evidence, which is why I included Paranoid it the message subject. I do know that NSA is well served when people believe things about cryptography which aren't true. If you believe TLS is broken[1] then you might use something much weaker in its place. If you believe AES/RSA/ECDSA etc. are strong when they aren't you will continue to rely on them.
I see this a the crux of our problem as responsible crypto people. The systems we thought were working are broken. For both professional and political reasons we need to fix them quickly.
My morning paper includes the comic "Non Sequitur". Today's strip has one of the regular characters being visited by two NSA agents. This story is front and center in the public's attention. There is no better time to press for whatever disruptive changes may be needed.
What we need is working code we can get adopted. It can be prototype code with more complete versions to come later. But our best chance of adoption is now.
And I happen to have one in my back pocket. :-)
Yes, CapROS[2] isn't proven, but it is mature enough to build Perry's household encryption box. (There are ports to both X86 and ARM. Device drivers are outside the kernel. The IP stack works. You probably don't need much more.) Any code that works in CapROS will probably port easily to a proven capability OS.
[And yes Perry, I was very impressed by your arguments for program proving technology. It is a bit out of my area of expertise. But I have always thought that different ways of looking at programs can only help them be more reliable, and proving is a different way.]
Hay, I like playing in the crypto sandbox, and redundancy is a classic technique. I have seen questions about DH -- factoring and key sizes, and EC -- cooked curves. If you worry about these issues, and don't have a third alternative, combining them seems like a good idea.
[1] And TLS is big enough to share with the internet the characteristic that it can be two things. The internet is always up somewhere. Some parts of TLS are secure for certain uses. The internet is never all up. Some parts of TLS are seriously broken.
[2]

@_date: 2013-09-17 17:27:52
@_author: Bill Frantz 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
When I chose MAC then encrypt I was using the MAC to check the crypto code. CRC would have worked too, but the MAC was free. (I really don't trust my own code very much.)
Cheers - Bill

@_date: 2013-09-18 13:58:22
@_author: Bill Frantz 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
I know I would be a lot more comfortable with a way to check the mail against a piece of paper I received directly from my bank (the PGP model). I would have no problem in entering a magic authentication string (the key fingerprint) into my mail agent to authenticate my bank. The security of my money is of more that trivial importance.
Second would be having my mail agent tell me that the mail came from the same place as the previous piece of email I received (the SSH model). This model would work for most of my friends where MitM is unlikely. In the cases where MitM worries became important, I could then check fingerprints.
The CA model lets a powerful attacker subvert the CA at any time ignoring both out of band and same-as-the-last-time authentications. I'm OK with CAs for credit card transactions. There's a $50 limit on my risk from fraud.
Cheers - Bill

@_date: 2013-09-19 10:24:15
@_author: Bill Frantz 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Do you have any evidence to support this contention? Remember we're talking about money, not just social networks.
I can support mine. ;-)
If organizations like Consumers Union say that you should take that number from the bank paperwork you got when you signed up for an account, or signed up for online banking, or got with your monthly statement, or got as a special security mailing and enter it into your email client, I suspect a reasonable percentage of people would do it. It is, after all a one time operation.
Cheers - Bill

@_date: 2013-09-21 16:20:10
@_author: Bill Frantz 
@_subject: [Cryptography] Specification: Prism Proof Email 
This approach certainly meets my requirements. As a UI designer/user I want it to JFW (Just ... Work) invisibly under the covers. As a boarder-line paranoid, I want a indicator of which methods passed. :-)
Let's add to the list of methods the SSH method of, "The same key used the last time".
I assume users of the CA method would register with the CA in some maner which would probably cost money. (How the CA separates me from Bill Frantz, the professional photographer in Illinois is not going to be cheap.) I understand there is still a trademark dispute between the US beer Budwiser and the German beer of the same name.
In the WoT case, having your key fingerprint written on a QR code is a neat hack. Put it on the back of your business card[1].
I think CAs will be most useful for businesses while WoT will be most useful for individuals. Everyone will be more comfortable when the SSH test passes.
Cheers - Bill
[1] Back in days of yore, I needed to send some company private data to my home computer. I didn't have the fingerprint of my key at work, but I did have Carl Ellison's business card with the fingerprint of his key. He had signed my key which was available on a key server, so I had good enough reason to trust that the key was actually mine.

@_date: 2013-09-22 12:16:06
@_author: Bill Frantz 
@_subject: [Cryptography] RSA equivalent key length/strength 
On 9/21/13 at 5:07 PM, code at funwithsoftware.org (Patrick I think that this comment is a serious misinterpretation of the discussion on the TLS list.
The RFC under discussion is a Best Current Practices (BCP) RFC. Some people, including me, think that changes to the protocol or current implementations of the protocol are out of scope for a BCP document.
There are several implementations of TLS which will only do 1024 bit Diffie-Hellman ephemeral (DHE)[1]. The question as I see it is: Are we better off recommending forward security with 1024 bit DHE, with the possibility that large organizations can brute force it; or using the technique of having the client encrypt the keying material with the server's RSA key with the probability that the same large organizations have acquired the server's secret key.
Now there are good arguments on both sides.
The nearly complete database of who talks to who allows "interesting" communications [2] to be singled out for attacks on the 1024 bit DHE. Cracking all the DHE exchanges is probably more work than these large organizations can do with current technology. However, it is almost certain that these sessions will be readable in the not too distant future.
It is widely believed that most large sites have had their RSA secret keys compromised, which makes all these sessions are trivially readable.
I think that the vast majority of TLS list commenters want to have TLS 1.3 include fixes for the problems that have been identified. However, getting TLS 1.3 approved is at least a year, and getting it through the FIPS process will add at least another year. We already know that these large organizations work to delay better crypto, sometimes using the argument that we should wait for the perfect solution rather than incrementally adopt better solutions in the mean time.
Cheers - Bill
[1] Implementations which will only do 1024 bit DHE are said to include: Apache with OpenSSL, Java, and Windows crypt libraries (used by Internet Explorer). If longer keys are used by the other side, they abort the connection attempt.
[2] I actually believe NSA when they say they aren't interested in grandma's cookie recipe. I am, but I like good cookies. :0)

@_date: 2013-09-24 13:36:13
@_author: Bill Frantz 
@_subject: [Cryptography] Hardware Trojan Protection 
On 9/22/13 at 6:07 PM, leichter at lrw.com (Jerry Leichter) wrote in another thread:
You might get a reasonable level of protection implementing the core of the crypto operations in a hardware security module (HSM) using Field Programmable Gate Arrays (FPGA) or Complex Programmable Logic Device (CPLD). There is an open source set of tools for programming these beasts based on Python called MyHDL . The EFF DES cracker may have some useful ideas too.
The largest of these devices are also pressing the current chip limits. There isn't a lot of extra space for Trojans. In addition, knowing what to look at is somewhat difficult if pin assignments etc are changed from chip to chip at random.
As with any system, there are tool chain issues. Open source helps, but there is always the Key Thompson attack. The best solution I can think of is to audit the output. Look very carefully at the output of the tool chain, and at the final piece that loads the configuration data into the device.
Cheers - Bill

@_date: 2013-09-25 15:55:10
@_author: Bill Frantz 
@_subject: [Cryptography] RSA equivalent key length/strength 
Agreed, however eventually we will want to do (3):
(3) Make insecure obsolete servers show as insecure in the user's UI or refuse to communicate with them. Embedded systems are the Achilles Heel of this suggestion. The only way to upgrade them is to replace them, which might be too costly.
Cheers - Bill

@_date: 2013-09-30 10:00:16
@_author: Bill Frantz 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
I talked with a park ranger who had used a high-precision GPS system which decoded the selective availability encrypted signal. Access to the device was very tightly controlled and it had a control-meta-shift-whoopie which erased the key should the device be in danger of being captured. And this was a relatively low security device.
Cheers - Bill

@_date: 2013-09-30 15:45:28
@_author: Bill Frantz 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Rich - Thanks for chasing this study down. There is a lot of food for thought for all of us in it.
Found at: To quote from the above:
     The idea is that if customers do not see their [preselected]
     image, they could be at a fraudulent Web site, dummied up to
     look like their bank?s, and should not enter their passwords.
     The Harvard and M.I.T. researchers tested that hypothesis. In
     October, they brought 67 Bank of America customers in the
     Boston area into a controlled environment and asked them to
     conduct routine online banking activities, like looking up
     account balances. But the researchers had secretly withdrawn
     the images.
     Of 60 participants who got that far into the study and whose
     results could be verified, 58 entered passwords anyway. Only
     two chose not to log on, citing security concerns.
This approach requires the customer to verify the image every log on. Conning them by replacing the image with, "Site undergoing maintenance"[1] is fairly easy. With my approach, I would authenticate the bank's key once, when I establish an account or sign up for online banking. My software would check that authentication every time I log on after that. (If the bank decides to change it's key every year, I might need a new piece of paper every year -- which might get old after a few years.)
Found at: I believe this study is the one referred to in the NYT article above. This study started with 67 people, the same number mentioned above and the authors are also affiliated with Harvard and MIT. The steps they took to ethically use real accounts are worth reading.
The last test involved presenting a IE warning page, "There is a problem with this website's security certificate. The result was:
     Of the 60 participants whose responses to prior tasks had
     been verified, we were able to corroborate 57 participants?
     responses to the warning page. Despite the overtness of the
     warning page and its strong wording, 30 of 57 participants
     (53%) entered their passwords. 27 participants (47%) did
     not login.
Leaving me to say you shouldn't give the user an option to ignore security. I don't think I get a choice if an Apple or Microsoft software update fails signature verification.
Their conclusions:
     Users will enter their passwords even when HTTPS
     indicators are absent.
     Users will enter their passwords even if their site-
     authentication images are absent.
     Site-authentication images may cause users to disre-
     gard other important security indicators.
The last conclusion is interesting for evaluating other studies. They divided their subjects into three groups. Two used dummy accounts and one used their own accounts.
     Role playing has a significant negative effect on the
     security vigilance of study participants. Participants who
     played roles disregarded more attack clues before withholding
     their passwords than participants whose own passwords were at
     risk.
Cheers - Bill
[1] The text used in the second reference's study is very enticing:
     SAI Maintanance [sic] Notice:
     [bank name] is currently upgrading our award
     winning SAI feature. Please contact customer
     service if your SAI does not reappear within the
     next 24 hours.

@_date: 2013-09-30 17:23:06
@_author: Bill Frantz 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
The effect of NSA's work with Lucifer to produce DES was:
   DES was protected against differential cryptanalysis without making this attack public.
   The key was shortened from 64 bits to 56 bits adding parity bits.
I think the security side of NSA won here. It is relatively easy to judge how much work a brute force attack will take. It is harder to analyze the effect of an unknown attack mode. DES users could make a informed judgment based on $$$, Moore's law, and the speed of DES.
Cheers - Bill

@_date: 2014-04-01 12:10:34
@_author: Bill Frantz 
@_subject: [Cryptography] TLS/DTLS Use Cases 
As the principal instigator of using use cases to think about the proposed TLS 1.3, I have been trying to come up with a small number of use cases that cover most of the application space. So far for TLS I have:
   HTTP -- the original SSL use case
   Email -- POP, SMTP, IMAP which are not well served by TLS authentication
   Are there any others that should be included?
I would like to have some for DTLS as well, DTLS applies cryptography to UDP. It seems that DNS might be a good candidate, but can encryption even help DNS privacy?
Cheers - Bill

@_date: 2014-04-01 14:45:06
@_author: Bill Frantz 
@_subject: [Cryptography] TLS/DTLS Use Cases 
With UDP you don't get either message ordering or "deliver once" semantics. At that level it is all DIY. Most people who need these features and DIY reinvent a poor version of TCP.
Cheers - Bill

@_date: 2014-04-13 23:27:55
@_author: Bill Frantz 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Assembler subroutines are your friend. Clearing memory is easy to write in assembler, but not very portable between architectures.
Cheers - Bill

@_date: 2014-04-14 19:18:10
@_author: Bill Frantz 
@_subject: [Cryptography] Preliminary review of the other Applied 
I certainly use my phone when the local WiFi gets too obnoxious.
The best experiences I have had were in out-of-the-way places. Newfoundland and Labrador were quite nice. I think there was even one WiFi that didn't have a password.
In the US they seem to want to have you agree to their terms of service. But it is really an excuse to direct you through their web page. I wonder what they are collecting there.
Cheers - Bill

@_date: 2014-04-16 00:04:58
@_author: Bill Frantz 
@_subject: [Cryptography] I don't get it. 
On 4/16/14 at 3:15 PM, l at odewijk.nl (Lodewijk andr? de la When you consider the number of Fortune 1000 companies who have products which use OpenSSL, it is quite shameful the support they give it. Should they now feel the urge to pitch in, funding QA would be a good start.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-04-16 22:31:26
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
If I may toot my own horn here, I worked for many years on such a system.
The KeyKOS operating system  was such a system. It has also been known over the years as Gnosis and GuardOS -- they are all the same code base. (Well, kind of. The first implementation was in IBM 370 Assembler. Later versions were in C and ran on some Motorola 88000 hardware and some Sparc hardware.)
The KeyKOS Design Document is a comprehensive manual of the API for the 370 version. It also includes a number of design writeups which were not implemented. I think by reading it, you can tell which is which.
The CapROS system  is a clean room clone which runs on Intel x86 and some ARM processors.
Any questions?
Cheers - Bill

@_date: 2014-04-17 11:34:50
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
Being retired, I guess I am seriously out of touch. I have not seen Genode.org's system. A quick glance at their architecture page  indicates that it is worthy of more study.
Cheers - Bill

@_date: 2014-04-17 11:34:44
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
On 4/17/14 at 5:41 AM, l at odewijk.nl (Lodewijk andr? de la Well, the 370 version of KeyKOS could run IBM's CMS, which was a desktop environment back when the desktop meant timesharing. Otherwise, desktop environments take a lot of development work. There doesn't seem to be a market for anything which provides least privilege for desktop applications. HP labs has done some excellent work in this area with their Polaris system, which runs each Windows application in a separate user ID, and CapDesk which runs the file dialog as a secure, privilege-granting widget for applications wanting to access files.
My bottom line is, where's the money?
Remember that the money supporting these systems ran out towards the end of the last millennium. There has been a lot of changes in the economics of processors in the last 15-20 years. The 370 version of KeyKOS had a design for a multi-processor (shared main memory) version, but with less than 5% of the installed 370s having a second processor, it wasn't a high priority.
At the application level, each KeyKOS domain (or CapROS process) can execute independently, allowing use of many processors. Each one is single-threaded and communicate with messages, which makes it possible to build reliable applications. (Jeff Frantz's observation, "Concurrency is hard. 12 out of 10 programmers get it wrong." applies here.
I'm not sure the Unix model is the correct model, but that's what we have in Windows, MacOS, Linux, and Unix. KeyKOS had a Unix emulator. The question always came up, "How compatible is it?" When we probed the answer seemed to be, "We need bug for bug compatibility."
There is nothing proven. The last funded project, thanks DARPA, demonstrated a "Web Key Server" on CapROS. It allowed control of physical devices attached to the system from remote locations. A Web Key is a kind of URL which includes a secret value designating the resource. The server code ran enough HTTP for a browser to access the controls. One interesting thing about its security architecture is that it used a Btree object to look up the objects associated with the web key secret values. This Btree object would take a value and return a reference (key) to the associated object. It would not allow the server to enumerate all the references it held. This architecture provided protection against a hacked server. To access any resources the attacker would have to know the secret web key values, which were long random numbers.
Cheers - Bill

@_date: 2014-04-18 08:54:00
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
Polaris was a research project. I don't think it is available. (HP is a hardware company, not a software company.)
Plash I think had similarities to Polaris. Capsicum is also moving in that direction.
This is an area where it is vital to not let the better be the enemy of the good. Incremental improvement is probably the best we can hope for.
In this sense, while running each application in its own limited authority protection domain is an excellent step forward, splitting them into multiple domains is the next logical step. Postfix proved a multi-protection domain architecture was practical and had significant benefits years ago. If OpenSSL had merely separated the public key operations into a separate protection domain, people would not have lost their master signing key in the last SNAFU.
Even with slow IPC, the message time would be small compared with the public key operation time. Ironically, the OpenSSL goal most impacted by such an architecture change is portability. I would have found it much more work to port OpenSSL to CapROS if I had to learn enough about it to port the separate protection domains.
I see two problems: (1) Us. We know programming correct programs is hard, but we all say to ourselves, "I can do it!" As a result we avoid tools which can help us. We use C. We don't use separate protection domains. etc. History shows that, even with excellent programmers, this path leads to serious bugs.
The second is marketing. No one seems to want to buy better security. It is a checkoff item, like many of the features in early Microsoft applications. It has to be there, but it doesn't have to work.
One question that might shed light on the marketing problem: Why hasn't Postfix completely displaced Sendmail? Perhaps some of the Postfix people have some insights.
Cheers - Bill

@_date: 2014-04-18 20:04:32
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
Actually, there are a number of ideas which just need to be implemented. Two off hand are the Powerbox and objects.
The Powerbox is used by an application to gain access to the user's files. To the user, it looks just like the OS file selection dialog which now appears. However under the covers, the power box has access to the user's files while the application does not. When the user selects a file for the application, the necessary permissions for current instance of the application are changed and it can now open the file. Polaris worked this way, with no changes to Windows or to the applications. A story from the Polaris developers (Alan Karp and Marc Stiegler): They installed Polaris on a HP Labs executive's PC while he was out of town with a training session scheduled immediately after he returned. Scheduling problems forced a delay on the training. When the training session started the executive asked, "When are you going to install it?" He had been running it for a week.
Obviously there are more elegant ways to accomplish this goal, but MIcrosoft could implement a Powerbox in the next version of Windows. This change would be less trauma for Windows programmers and users than the redesign of the desktop look and feel that comes with every new release.
Object are really an old idea. However, if properly used, they can provide security barriers within an application. Java does a good job here as do a number of other languages. Java lost out in the desktop market because it took forever for the Java Virtual Machine to initialize itself. This problem admits to several solutions and other object languages do not have this problem.
With current object languages it takes discipline to keep objects from accessing each other's innards. Coding standards are something most programmers are familiar with. Enforcing them is a somewhat solved management problem. But, you need someone who gets information hiding in their guts to write the standards and keep rewriting them until they are right.
Cheers - Bill

@_date: 2014-04-19 08:11:58
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
When I last looked closely -- Java 1.3 -- reflection couldn't access private variables. Have they changed that? (I always worry about people adding new features that break the security invariants I am depending on.)
Cheers - Bill

@_date: 2014-04-19 08:11:59
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
Absolutely agreed. But don't let the perfect be the fatal enemy of the better. Even if we only have control on file access and no control on network access, the only things the application can leak are the files it has been given, not every file the user can access, which is an improvement.
I think we can do better because many programs, e.g. web browsers, that access the network don't make much use of the user's files. Sure you explicitly up and down load files, but those come in with standard file requestors.
The idea of limiting the sites a web browser can access seems very difficult. Between links to resolve in-page content, and caches like Akamai, there is almost no limit on the sites a browser can legitimately access to render a page.
Cheers - Bill

@_date: 2014-04-20 07:12:52
@_author: Bill Frantz 
@_subject: [Cryptography] Simpler programs? 
On 4/18/14 at 10:25 AM, cryptography at dukhovni.org (Viktor The lesson I read from this story is that in general familiarity trumps better when better means fewer bugs and/or better security[1]. However, if people suffer for long enough from the bugs, then they will change. However features can be worth more than reliability.
[1] I have long contended, and continue to contend that reliability and security are two sides of the same coin. You can't have one without the other. It's nice that freedom from crashes also makes you safer.
Cheers - Bill

@_date: 2014-04-20 20:18:18
@_author: Bill Frantz 
@_subject: [Cryptography] It's all K&R's fault 
And encrypted swap is really the sweet spot for encryption because all the key management problems just go away:
   Randomness: You don't need to generate the key until the first swap out. Lots of time since boot and lots of interrupts to feed the pool.
   Key life: Keep the key in memory. It only has to last until shutdown.
   Key distribution: You don't need to send it anywhere.
   Key authentication: Yea, right! You don't need to.
Cheers - Bill

@_date: 2014-04-21 12:41:40
@_author: Bill Frantz 
@_subject: [Cryptography] bounded pointers in C 
Two things I have found useful in this kind of effort:
Carefully compare the code with the comments. SInce most of my professional career was writing in assembler, I tended to comment each instruction with a reason for its existence. I frequently found bugs where the comment said one thing and the code something slightly different. Only one time did they seem different, but weren't. That time produced even more comments as to why the comment and the code were both correct. In a sense, my code included an informal proof of correctness in the comments.
Use an automatic formater to reformat your code for review. (Keep the original format for your source code control system. People make more readable formatting than machines.) Seeing the same code from a different point of view can overcome personal blind spots.
Cheers - Bill

@_date: 2014-04-22 11:53:41
@_author: Bill Frantz 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
I'm not sure the memory advantage of sharing library code overcomes the disadvantages described elsewhere in this thread with modern systems. Even when I consider my Raspberry Pi system, it has 512 meg of memory divided between the video card part of the SoC and the Arm processor portion. If I have 50 meg of shared library, that's less than 10% and might be a cheap price to pay for software stability.
Does anyone have any information on how much sharing actually takes place?
Cheers - Bill

@_date: 2014-04-22 13:23:46
@_author: Bill Frantz 
@_subject: [Cryptography] It's all K&R's fault 
I really don't understand any of the Unix systems (including Linux). Can someone translate these items?
      I found that my swap device should be /dev/sda3 instead of /dev/sdb3.
      So, just changing /etc/crypttab contents to
      cryptswap1 /dev/sda3 /dev/urandom swap,cipher=aes-cbc-essiv:sha256
      solved my problem.
I thought when Linux at least partitioned the disk, it wrote swap as a particular partition type in the partition table. It should have no problem finding that partition even if the contents are unreadable garbage. It has no business trying to read old swap -- that's what swap encryption is trying to prevent.
This one appears when the user tried to reinstall after having an encrypted home directory. Why does that affect swap encryption?
This bug writeup is Greek to me. :-)
Not knowing the technical meaning of "suspend" and "hibernate", I can make two guesses:
   (1) Place the CPU in a very low power state and keep main
   memory alive either by slow refresh -- DRAM or just stopping
   the clock -- SRAM.
   (2) Write the contents of main memory to disk and prepare
   things so a restart will read it back and run the system again.
In case (1), there should be no problem. Case (2) produces interesting design decisions.
Do we write the swap encryption key on the disk? Do we encrypt it with a passphrase? Do we wipe the disk copy on restart which may be difficult with some devices?
It appears that the developers were thinking along these lines:
     The trick is to wrap swap, and all other partitions into a VG
     which is put on an encrypted PV wholesale. So during boot,
     the initramfs asks for a password for decrypting this PV,
     which also works for resuming from hibernation.
but I have no idea what they are saying.
     On my system, gnome-power-manager hibernates when idle by
     default when on battery. Since my system uses encrypted swap,
     this causes the machine to switch off without saving state.
     Ideally, gnome-power-manager would know that swap was
     encrypted and suspend to RAM instead of trying to hibernate.
Again my comments about key management above apply.
Indeed John, you are correct. When you try to snapshot the system to disk, you bring back key management problems. The problems with re-install in the beginning of the list seem to be a normal case of the complexity of Unix shooting itself in the foot.
I will note that if KeyKOS tried to encrypt its disk, it would definitely have key management problems. KeyKOS does not have swap in the sense of a virtual memory spill area. The whole disk is virtual memory spill area, and kept for the life of the system so it is also permanent memory.
Cheers - Bill

@_date: 2014-04-27 07:14:51
@_author: Bill Frantz 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I must admit being an IBM architecture fan by reflex. I like having channels where the memory locations an I/O transfer can reference are determined in a device independent maner. I'm always surprised by little endian thinking, even though we big endian people lost the battle everywhere but the IETF. The piece of IBM think most valuable for this discussion is records.
IBM systems are built around records while Unix descendants are built around streams. Records have the advantage that a whole record is by definition accessible in memory at the same time. You can put all the fixed length items at the start of the record. (If you are careful, you can align them on hardware boundaries for access speed.) There is a attraction to making data items fixed length if possible, which speeds access, rather than variable length which generates the parsing problems we have been discussing.
Of course there are disadvantages too. Input from a sound card is naturally stream like and breaking it into records is unnatural.
However, more record-like thinking might make data structures which are less bug prone to parse.
OBSecurity: Note that compressing any data collection which spans security classes is a serious security flaw.
Cheers - Bill

@_date: 2014-04-27 14:28:22
@_author: Bill Frantz 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I have this occasional dream. I'm having a candid conversation with a NSA person. He tells me, "No, we didn't have to subvert the standard. We just had to step back and let you do it yourselves."
Cheers - Bill

@_date: 2014-08-22 17:33:06
@_author: Bill Frantz 
@_subject: [Cryptography] On 40-bit encryption 
It also gave foreign companies the opportunity to install their own back doors. c.f. CryptoAG.
Cheers - Bill

@_date: 2014-12-10 17:13:09
@_author: Bill Frantz 
@_subject: [Cryptography] North Korea and Sony 
More importantly, we should automatically open any program in a limited authority space which limits its ability to access/change things to "need to know".
Cheers - Bill

@_date: 2014-12-10 17:31:39
@_author: Bill Frantz 
@_subject: [Cryptography] Toxic Combination 
In explanation, I offer "Frantz's law": No security protocol will achieve wide adoption unless it includes a revenue model which someone can use to build a business.
In explanation for why we have CAs, I offer "Frantz's law": No security protocol will achieve wide adoption unless it includes a revenue model which someone can use to build a business.
We can go back to Machiavelli' observation that to introduce a new way of doing things requires a strong supporter to overcome the inertia and hostility of all the people doing it the old way. The profit motive generates strong supporters.
Some may claim that SSH is a counterexample, but in the beginning, SSH was only available as a software product, and while it is now universal for sysadmins accessing servers, that is really a very narrow market in the scheme of things.
Cheers - Bill

@_date: 2014-12-11 22:15:42
@_author: Bill Frantz 
@_subject: [Cryptography] North Korea and Sony 
Marc Stiegler, Alan H. Karp, Ka Ping Yee, and Mark Miller addressed this specific issue for Windows in "Polaris: Toward Virus Safe Computing for Windows XP", . The basic approach (described on page 5) is to run these programs under a separate userID, unique to the program. It seems likely that their approach would extend to other popular systems.
Cheers - Bill

@_date: 2014-12-11 22:15:47
@_author: Bill Frantz 
@_subject: [Cryptography] Toxic Combination 
In the case of AES, many companies agreed they needed a common, trusted encryption algorithm to replace DES. NIST ran a competition to select one and AES was the result. Many companies have build products that incorporate AES. QED
In the case of OpenPGP, PGP was built by Phil Zimmermann for political reasons to protect activist's data. Commercial use required a fee. I agree that PGP was not primarily motivated by the profit motive. Its success was due to being the only choice available, and the fact that it worked. Note that PGP Corporation was a purely commercial enterprise based on PGP.
SHA3's history is similar to AES. It is also driven by commercial interests which need compatibility and trust.
The Diffie Hellman algorithm was granted U.S. Patent 4,200,770 and was assigned to Stanford University. While Stanford's interest was primarily in encouraging academic research, they did try to make money licensing the patent. ECDHE is an implementation of Diffie Hellman using elliptic curves instead of modular integer multiplication which gives faster implementation with smaller data. Again, the commercial interests drove ECDH's adoption (after negotiating a minefield of patents).
I wouldn't be surprised if the developer of the Bitcoin protocol didn't extract some money as an early adopter. There is, of course, no proof.
I don't recognize Pond or FOSS, and I've done enough free research for tonight.
Cheers - Bill

@_date: 2014-12-12 15:31:28
@_author: Bill Frantz 
@_subject: [Cryptography] North Korea and Sony 
I think Polaris dynamically added UserIDs as needed. I don't see how that would interfere with having two or more users on a machine, as long as the "real" users had different names from the shadow IDs, and that could be done with a naming convention.
Cheers - Bill

@_date: 2014-12-12 15:31:34
@_author: Bill Frantz 
@_subject: [Cryptography] Toxic Combination 
Well, to finish, FOSS isn't a security protocol, its a development method, in particularly good repute with the readers of this list (including me).
Since I hadn't heard of Pond, I think I can clearly argue that it hasn't achieved the necessary market penetration to have "achieved wide adoption."
I agree Ben. PGP is perhaps the best counter example. It was clearly developed for political reasons, and the requirement for payment for commercial uses was really and after thought.
Perhaps restating the claim as, "No security protocol without a revenue model can win in the marketplace against one that has one."
Note that in retrospect, I'm not sure that Diffie Hellman is a security protocol. It can be part of one such as TLS (nee SSL), but by itself is not a protocol.
It is clear to me that the TLS CA model won because people could build a business as a CA. Lynn Wheeler makes this point explicit when he talks of strong arm methods used to suppress alternatives.
Cheers - Bill

@_date: 2014-12-18 19:08:52
@_author: Bill Frantz 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
I think it would be very hard to find a backdoor suggested by Norm Hardy. Modify the CPU to detect when two specific floating point numbers are multiplied. When they are, execute the next instruction in privileged mode.
Cheers - Bill

@_date: 2014-12-24 07:24:31
@_author: Bill Frantz 
@_subject: [Cryptography] Certificates and PKI 
Ben limits CT transactions to recognized CAs to limit spam to the CA CT registries. Could the DNS registrars be enlisted as intermediaries in a DNS based CT registry to similarly limit spam?
Cheers - Bill

@_date: 2014-12-24 07:37:43
@_author: Bill Frantz 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
And the IBM 1401 did right to left decimal add/subtract as well. At least it didn't use table look up for the results like the 1620, saving a number of memory cycles.
The Burroughs 2500/3500 machines did addition/subtraction left to right depending on the fact that any digits in the result that depend on whether there is a carry to the right will be either 0 or 9. The kept the address of the first of these, and when they knew if there was a carry, they filled in the correct value from left to right. A bit complex, but this was a Burroughs machine after all.
Cheers - Bill

@_date: 2014-01-31 21:58:36
@_author: Bill Frantz 
@_subject: [Cryptography] Unified resource on Random Number Generation 
Go ahead and use my postings, with attribution if that is the style, otherwise as you wish.
Cheers - Bill

@_date: 2014-01-31 22:06:27
@_author: Bill Frantz 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
The man page for random/urandom on MacOS is an interesting contribution to the random
number debate:
XXXXX:~ yyyyy$ man random
RANDOM(4)                BSD Kernel Interfaces Manual                RANDOM(4)
     random , urandom -- random data source devices.
     pseudo-device random
     The random device produces uniformly distributed random byte values of
     potentially high quality.
     To obtain random bytes, open /dev/random for reading and read from it.
     To add entropy to the random generation system, open /dev/random for writing
     and write data that you believe to be somehow random.
     /dev/urandom is a compatibility nod to Linux. On Linux, /dev/urandom will
     produce lower quality output if the entropy pool drains, while /dev/random
     will prefer to block and wait for additional entropy to be collected.  With      Yarrow, this choice and distinction is not necessary, and the two devices behave
     identically. You may use either.
     The random device implements the Yarrow pseudo random number generator algorithm      and maintains its entropy pool.  Additional entropy is fed to the generator
     regularly by the SecurityServer daemon from random jitter measurements of the      kernel.  SecurityServer is also responsible for periodically saving some
     entropy to disk and reloading it during startup to provide entropy in early system      operation.
     You may feed additional entropy to the generator by writing it to the random device,
     though this is not required in a normal operating environment.
LIMITATIONS AND WARNINGS
     Yarrow is a fairly resilient algorithm, and is believed to be resistant to non-root.
     The quality of its output is however dependent on regular addition of appropriate      entropy. If the SecurityServer system daemon fails for any reason, output quality      will suffer over time without any explicit indication from the random device itself.
     Paranoid programmers can counteract this risk somewhat by collecting entropy of their      choice (e.g. from keystroke or mouse timings) and seeding it into
     random directly before obtaining important random numbers.
     /dev/random
     /dev/urandom
     A random device appeared in the Linux operating system.
Darwin                         September 6, 2001                        Darwin
Cheers - Bill

@_date: 2014-02-02 20:36:04
@_author: Bill Frantz 
@_subject: [Cryptography] cheap sources of entropy 
My experience with working on several OS kernels is that OS scheduling is a black art. Its a lot more arcane than cryptography.
If I were working on the scheduler for a modern VM hypervisor, I would spend considerable effort trying to maximize the efficiency of the L1 and L2 caches. Disk I/O just doesn't count. If you have to wait for a disk operation, you're in a long-term penalty box.
Now, if you can measure program execution time accurately enough to determine the L1 and L2 cache misses, they might be a good source of bits unknown to outside observers. :-)
Cheers - Bill

@_date: 2014-02-02 21:44:10
@_author: Bill Frantz 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
Adding yet more evidence that FIPS standards work against improved security. I wonder how much NSA advice had to do with this situation.
Cheers - Bill

@_date: 2014-02-04 11:03:24
@_author: Bill Frantz 
@_subject: [Cryptography] request for consideration: VM guest entropy: 
These suggestions fall in the category of "include anything that might be useful". I support them.
Cheers - Bill

@_date: 2014-02-04 11:03:26
@_author: Bill Frantz 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
I admit that 15 years ago, when I designed the E language communication protocol random source, I was concerned about the 160 bit internal state of the Java SecureRandom object. I can't remember my code too well and am having difficulty reading it from the server, but I kept a somewhat larger pool.
I know of no such attack, but as a programmer, if I see such a comment in a piece of code after an attack is found, it is a red flag saying, "Don't fix it! Live with it!"
A number of people on this list have suggested combining NIST approved randomness with other sources because they don't trust the NIST sources. The cost of certification is an important reason they aren't suggesting improving the NIST standards.
As a side question, how much does compliance certification cost?
What exactly do we want in Yarrow? What are the characteristics of the mix function that are important. Why do we think, and I indeed do think, that crypto hashes are good mix functions.
Given the results of code security review experiments I have seen, they do not catch deliberately inserted loopholes. Period. Proofs of correctness have their own problems. Too bad thats all we have.
Cheers - Bill

@_date: 2014-02-09 23:42:14
@_author: Bill Frantz 
@_subject: [Cryptography] RAM memories as one source of entropy 
On 2/7/14 at 4:37 AM, Joachim at Strombergson.com (Joachim One friend who has looked into quantum secure random numbers says that there are many good sources of quantum secure numbers, but all the hardware he has seen that actually reads them out into a computer has had problems, sometimes fatal. Strong nearby electric fields can overwhelm the quantum noise to give but one example.
You might look for a way to allow the user of your HSM to provide "random" input and include it along with your internal sources. This feature will keep those of us in the "the more sources, the merrier" camp happy.
Cheers - Bill

@_date: 2014-02-14 22:45:30
@_author: Bill Frantz 
@_subject: [Cryptography] The ultimate random source 
OK, you have two sources of information unknown to the attacker. (1) The precise distribution of the candy in the jar. If you have a big enough jar and enough different colored candies, that alone is probably enough. (2) The quantum noise i the camera sensor. Cheaper cameras have poorer sensors and generate more noise.
This statement is completely wrong. Noise elimination post-processing's success does not mean the noise can be predicted, only that the image can be changed in a way that is pleasing to the human eye where there is less noise, and less entropy as a result. (Sensor noise is caused by quantum effects, particularly noise in the amplifiers, and so deserves to be called entropy.)
When I listen to weak signals on my ham radio, the DSP noise suppression isn't based on predicting the noise profile, because it to is quantum based. It is based on recognizing the signal as compared with the broad spectrum noise, and boosting the signal.
Any algorithm that reduces noise will also reduce the resulting entropy, but if you still have noise in the resulting photo, some entropy has made it through the processing, and can be used. High ISOs are your friend, as is poor noise suppression. Look in Popular Photography, dpreview.com, etc. for cameras with poor noise performance, and use them.
Cheers - Bill

@_date: 2014-02-20 17:09:53
@_author: Bill Frantz 
@_subject: [Cryptography] The ultimate random source 
I feel better with a real image that can test that the camera is still working.
I did some experiments with the large bag of M&Ms in our pantry. I poured them into a 10" x 15" baking sheet and counted the number of M&Ms by weight and came up with slightly more than 600 M&Ms.* (Note that the sheet wasn't absolutely densely packed because it was easier to not have a M&M sitting on top of another M&M that way.)
There were 6 different colors of M&Ms in the package.
Now my statistics is a bit rusty, and I know we can't assume equal color probability for each position, but I think we have so much margin here that a simplifying assumption is justified. We get 6 ** 600 possibilities and we haven't even considered small changes in position of each M&M. 6**600 overflows my HP15 calculator and makes me feel comfortable seeding a PRNG.
Now the issue is to keep the photo secret. :-)
Cheers - Bill
* 50 M&Ms weigh 1.5 ounces on my digital scale with a resolution of 1/8 ounce. The total collection of M&Ms weighed 18.375 ounces.

@_date: 2014-02-21 15:38:23
@_author: Bill Frantz 
@_subject: [Cryptography] The ultimate random source 
John, Thanks for emphasizing the need for careful analysis. Your points (originally at the end of your post) are quire valid:
I would quibble with point (c). If you have much more entropy than you need, the need for precision in estimating the lower bound is reduced. Having more than you need can make analysis tractable where analysis of a quantum source, such as radiation, Johnson noise -- pick your real source -- that only gave you just what you need is intractable because of the real-world engineering issues, such as hardware limits prevent achieving the precision you need.
I played with the M&Ns because the exact color and position is something that a camera has to do fairly well in order to pass muster as a camera. I don't have to worry about post-processing getting rid of all the noise, because any noise that comes thru is gravy and can only help a process that is already quite acceptable.
It is more complex than that, and actually somewhat better. I used 18 3/8 ounces of M&Ms out of a 42 ounce bag. The color of the final M&M is not completely determined. (Computing this probability is where my statistics ran out.) However I have no reason to believe the M&Ms are evenly distributed between the colors, but the MK1A eyeball says it isn't too bad. (Actually counting them is above my pay scale.) Also we have the problem that we didn't really start with 42 ounces of M&Ms because some had been eaten. :-)
Again, why we include a margin.
Seeding was my application. Also Phil was seeding. It is fairly obvious from the situation that whitening of the photo will be needed and seeding is a good way to whiten.
If you want to do it right, put them in a lottery barrel and turn the crank for 5 minutes. :-) Even reaching into the bag with a hand and stirring them is "good enough" given the margin above the 128-512 bits needed for seeding.
I'm not sure there is any quantum physics based entropy here. What we do have is massive unguessability, unless an attacker can get a copy of the photo. At least we know what we have to guard, and how to check the full chain from M&Ms to CSPRNG for errors/failures etc. I have not seen how you check hardware sources, e.g the Intel one, for hardware failures.
Cheers - Bill

@_date: 2014-01-01 12:38:55
@_author: Bill Frantz 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
One thing to remembrer in this mess: NSA isn't the only capable National Scale Adversary. While I might believe that NSA and GCHQ could, in the future, again be restrained by the rule of law, I don't believe we can use law to control the Russians and Chinese, to name just two. We have a wonderful worked example of the kind of threat we need to defend against. If we manage to rein in our eavesdropping agencies by use of law, we still have ones that aren't ours to worry about.
Cheers - Bill

@_date: 2014-01-05 22:03:28
@_author: Bill Frantz 
@_subject: [Cryptography] defaults, black boxes, APIs, 
On 1/5/14 at 12:25 PM, jthorn at astro.indiana.edu (Jonathan There is a path to make running the current versions of these programs safe.
As a thought experiment: Get a piece of hardware. Install an OS from R/O media -- e.g. DVD. Read your .docx/.pdf file from CD/DVC. Wipe the system and start over for the next file.
In addition to systems like KeyKOS/Capros/etc., which implement this model, a group a HP labs built a system called Polaris. Polaris started apps under a separate userid and tossed the userid when the app completed. Calling up an open file dialog box let the app access a specific file -- outside that separate userid -- specified by the human user of the system. Polaris depended on the integrity of the user/user security controls in Windows. If they failed, at least you could submit a bug report to Microsoft. :-)
Cheers - Bill

@_date: 2014-01-06 17:08:08
@_author: Bill Frantz 
@_subject: [Cryptography] defaults, black boxes, APIs, 
On 1/6/14 at 1:01 PM, jthorn at astro.indiana.edu (Jonathan Well, it is barely possible to use sprintf() securely, although it is hard and very error prone. I would prefer to be counting hollerith fields in Fortran II format statements. (At least with format statements you are likely to find out quickly that you miscounted.) There is no hope for gets().

@_date: 2014-01-08 22:20:07
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: Building a Trustworthy Business in the 
I attended the lecture described below. IMHO it was an interesting and useful contribution to the list of desirable system security directions in the post-Snowdon era.
The talk was video taped and will be distributed on iTunes and YouTube The release schedule to these channels is highly variable since it depends upon how much time SCPD staff has available outside of critical class-related work. See  for details.
Cheers - Bill
====== Forwarded Message ======
              Stanford EE Computer Systems Colloquium
                 4:15PM, Wednesday, January 8, 2014
      NEC Auditorium, Gates Computer Science Building Room B3
                    Topic:    Building a Trustworthy Business in the Post-Snowden Era
Speaker:  Alex Stamos
           Artemis Internet
About the talk:
The revelations caused by Edward Snowden's leak of thousands of
NSA documents have caused a great deal more political noise than
actionable intelligence for the infosec community. No matter what
you believe personally about Snowden, his motivations and the
morality of his actions, now that this information has become
public it is important for all professionals in our space to arm
themselves with any useful knowledge they can find and to apply
that knowledge to protecting the public from current and future
In this talk we will summarize the technical details that have
come out as well as make some educated guesses as to how the NSA
is able to accomplish some of the feats claimed by Snowden. We
will then discuss what these details mean for the current
capabilities of nation-state level attackers and the future
capabilities of lesser online adversaries. Detailed
recommendations on how to protect yourself will be provided and
we will look into the crystal ball to predict what kinds of
advances we can foresee in the upcoming years.
About the speaker:
Alex Stamos is the CTO of Artemis, the division of NCC Group that
is proud to be launching the .Secure gTLD. He was the co-founder
of iSEC Partners, one of the world's premier security
consultancies and also a part of NCC Group. Alex has spent his
career building or improving secure, trustworthy systems, and is
a noted expert in Internet infrastructure, cloud computing and
mobile security. He is a frequently requested speaker at
conferences such as Black Hat, Defcon, Amazon ZonCon, Microsoft
Blue Hat, FS-ISAC and Infragard. He holds a BSEE from the
University of California, Berkeley and his personal security
writings are available at Embedded Links:
[ 1 ]    [ 2 ]    ABOUT THE COLLOQUIUM:
See the Colloquium website,  for scheduled
speakers, FAQ, and additional information.  Stanford and SCPD students
can enroll in EE380 for one unit of credit.  Anyone is welcome to attend;
talks are webcast live and archived for on-demand viewing over the web.
====== End Forwarded Message ======

@_date: 2014-01-13 19:08:52
@_author: Bill Frantz 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
I think about how my car keys work. I have two cars, one with a RF dongle, and one without. The one with the RF dongle is clearly less secure than the one without. The RF dongle + the key are two ways to get into the car. If you find a lost key with the dongle, just walk thru the parking lot and push the buttons -- the car will announce itself.
OTOH, the only way into the one without a dongle is to find the car and open the lock. Either car is at risk from a MIT/Cal Tech student who knows lock picking.
I think this analysis makes a good metaphor for the design of a USB dongle.
Cheers - Bill

@_date: 2014-01-15 20:59:58
@_author: Bill Frantz 
@_subject: [Cryptography] [cryptography] Boing Boing pushing an RSA 
If the system uses a PRNG, it isn't a OTP. It is a stream cypher using the PRNG to generate the cypher stream.
Cheers - Bill

@_date: 2014-01-17 17:06:56
@_author: Bill Frantz 
@_subject: [Cryptography] [cryptography] Boing Boing pushing an RSA 
You could XOR enough physical streams together to overcome the bias in the physical process.
And, of course, you could use several approaches, some with PRNGS, but at least one without, XORed together and still call it a OTP.
Still it's a pain to distribute the keying material, particularly if you use Verner Vinge's technique of XORing the keys carried by 3 independent messengers to avoid compromise of the key in transit.
Cheers - Bill

@_date: 2014-01-18 10:07:07
@_author: Bill Frantz 
@_subject: [Cryptography] cheap sources of entropy 
I think what is happening here is the effective ISO is being pushed up by the low light so there is a lot of noise in the amplifiers used to read out the sensor cells. What you are using is thermal noise in the amplifiers. You get a lot of readings in one photo, and it should be a good source.
I have always looked at HSMs as black boxes built by people I don't trust. If I built it I would feel different, but you should be uncomfortable using my HSM. Getting mutually suspicious people to trust the same HSM is an interesting social/technical problem.
Cheers - Bill

@_date: 2014-01-19 10:54:15
@_author: Bill Frantz 
@_subject: [Cryptography] HSM's 
I'm tempted to start with: This is a new use of the word "simply" with which I was not previously familiar. :-)
There seem to be at least three approaches to the problem: (1) Split the key into enough pieces that a single rogue HSM can't compromise security. (2) Isolate the HSM(s) such that they can't communicate the key or perform rogue signatures. (3) Require signatures from all the HSMs for validity.
Cheers - Bill

@_date: 2014-01-20 13:04:59
@_author: Bill Frantz 
@_subject: [Cryptography] cheap sources of entropy 
No hardware device should be trusted unless it passes periodic tests. Most computer hardware is tested automatically as part of its normal operation. Lets look at the following "what ifs" for random number generation with a camera pointed at streamers in a fan, or a fish tank, or leaves on a tree. Because these arguments apply to much simpler hardware random generators, I am going to assume no back doors in the camera.
One issue that is almost never revealed in these discussions is how random number generation fits into the specific situation. Ian's ceremonies are far different from someone building an OTP system because it is "theoretically perfect", ignoring all the major engineering difficulties that the theory ignores. The following tests should work for ID dongles, DSA signing, session key generation, etc.
Let's use one of the statistical random number tests. These will detect certain failure modes, e.g. all ones, reliably. They will also have a certain number of false positives. We deal with the false positives by not trusting the output after a test failure until a subsequent test has passed, logging the failure for human attention.
The result will be all ones or oll zeros, no problem.
This is an unlikely situation since aging generally results in more, not less, noise. However the test will fail due to too much repetition in the output.
How is this situation different from "degrades with time"?
Ditto. No data should be easy to detect.
You're hosed. Don't allow it.
Ditto. Get a good OS, or configure the one you are using correctly.
How is this situation different from the camera breaking above?
If this approach isn't good enough, replicate it several times and combine the outputs.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-01-20 13:05:00
@_author: Bill Frantz 
@_subject: [Cryptography] cheap sources of entropy 
Hmm, 12AX7s cost about $15 and burn a bunch of power. How many do I need to perform useful computation? Using the LGP-30 computer as an example, not very many. In any case, there is probably a simple enough technology where concerns about back doors devolve into pure paranoia. Of course this approach doesn't scale to high performance, so we can't use it. :-)
Note that paper and human voting systems have been successfully attacked. My favorite attack occurred in Chicago in the mid-20th century. Ballot counters from one party glued pencil leads under their fingernails. When they counted a ballot for the "wrong" candidate, they marked an X for the correct candidate as well, thereby invalidating the ballot.
Cheers - Bill

@_date: 2014-01-21 14:27:49
@_author: Bill Frantz 
@_subject: [Cryptography] cheap sources of entropy 
I am going to assume that John means hardware sources here.
Now I contend that as a sound engineering principle, any hardware device that is being relied on for important functions

@_date: 2014-01-21 14:27:50
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
Well, it depends on your threat model. If you are a US company concerned about the Chinese stealing your IP, vulnerability to NSA is a better bet.
Cheers - Bill

@_date: 2014-01-29 21:08:35
@_author: Bill Frantz 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
If they are applied to hardware that is supposed to be providing random bits, they can catch a number of common hardware failures including all zeros and all ones. (Ironically, noisy hardware might be just what the doctor ordered.)
Cheers - Bill

@_date: 2014-07-23 14:30:02
@_author: Bill Frantz 
@_subject: [Cryptography] hard to trust all those root CAs 
I assume that all my email is as private as messages to this list.
I assume that the government can listen to my phone calls and IMs.
The one time I actually thought I was having a private conversation was in a cave. We were in a passage that had never been entered before. Hidden recording devices were a possibility, but since we were in on a week-long trip, unlikely without collusion from one of the participants. Too bad we didn't have anything to say that required that level of security.
Cheers - Bill

@_date: 2014-07-23 19:39:18
@_author: Bill Frantz 
@_subject: [Cryptography] hard to trust all those root CAs 
I fully agree, which is why I contribute to this list.
But the flaws in our computer systems, protocols, and procedures, coupled with our failure to use what we have makes the assumption of complete openness the only safe assumption.
I personally don't worry about NSA too much. Revealing what they can gather from my electronic world isn't worth the cost revealing the extent of their spying. As a US person, I worry much more about foreign intelligence agencies. They are attempting to undermine US companies which I depend on economically. Helping these companies keep secrets from them is in my best interests.
I think there are examples on record of these signals being intercepted.
I agree, it probably wouldn't be played at all.
Fortunately, information transfer, particularly the results of R&D, is much more complex than just reading correspondence between developers. In real world cases, it frequently takes hours/days/weeks of one-on-one teaching to transfer the ideas.
Trade secrets are in trouble in a completely open world, although there is still some legal protection if a trade secret is stolen. Patents, copyrights, and trademarks can continue unscathed. Those are the four kinds of legal IP I know of.
Cheers - Bill

@_date: 2014-07-25 20:26:30
@_author: Bill Frantz 
@_subject: [Cryptography] propaganda on "hurdles for law enforcement" 
This is a bit of a misinterpretation the point I was trying to make.
The crypto is, in theory, effective. But in practice, there is a large body of things around it which render its effectiveness irrelevant in important cases:
   The OS it runs on
   The hardware it runs on
   Mistakes by the people using it
   The RNGs it depends on
If you need absolute assurance that your secrets are being kept, don't give them to a computer, because doing so involves some level of risk.
It would be interesting to compare the computer risks with the risks of sending the secret through the US mail. If the threat model is massive, non-targeted snooping, then clearly the mail is safer. If it is targeted snooping, envelopes are easy to steam open.
Cheers - Bill

@_date: 2014-06-03 15:28:54
@_author: Bill Frantz 
@_subject: [Cryptography] Retrofiting Access Control 
On 6/3/14 at 2:49 PM in a thread titled, "Re: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto library leaves Linux, apps open to drive-by attacks"", leichter at lrw.com (Jerry Probably the best solution for these systems is a security front end. To my mind, these systems need authorization and integrity more than secrecy. All of these can be implemented in a small, inexpensive system which acts as a gateway to the big bad Internet. I see it as a Raspberry Pi kind of application. The good news here is that NSA is much more on your side (if you're a US operator) then they are on the side of systems which need privacy. Keeping cyber attacks from making factories, power systems etc. go boom in the night is part of their job.
OTOH, being able to make these facilities go boom on command is part of the job of every potential enemy military,
Cheers - Bill

@_date: 2014-06-06 18:53:21
@_author: Bill Frantz 
@_subject: [Cryptography] What has Bitcoin achieved? 
OK, I'm not "most people". I pay cash for a lot of things. I shop in farmer's and flea markets where cash is king. What I don't pay cash for is mostly:
   Times when I don't have that much cash.
   Vendor's who will take a personal check (mostly friends).
   Gasoline (credit cards are just too easy).
   Online buys (ditto).
Most everything else is cash. I think I mostly use cash for the faster transactions, but privacy paranoia may also play a role.
Cheers - Bill

@_date: 2014-06-06 18:53:27
@_author: Bill Frantz 
@_subject: [Cryptography] To what is Anderson referring here? 
Hehe. I remember being a fly on the wall for a conversation between a RACF designer, and some people from the Stanford Linear Accelerator Center (SLAC). The designer asked about the SLAC security policy and the SLAC people replied: SLAC is an open laboratory. You must get special permission to keep any file secret.
The RACF designer's jaw dropped. She had never heard of such a security policy, and had no idea of how to use RACF to enforce it.
Cheers - BIll
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-06-10 16:08:52
@_author: Bill Frantz 
@_subject: [Cryptography] Yet more formal methods news: seL4 to go open 
The most important value of open source, and I am thinking back to the days when IBM published the source for its systems, is that the source is the best documentation around. If you don't know how to use OpenSSL, looking at the source is a big help. Don't know what an option does? Look at the source.
GIven the terrible state of internals documentation, and all to frequently user documentation, "may the source be with you."
Cheers - Bill

@_date: 2014-06-11 23:35:58
@_author: Bill Frantz 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
This problem is similar to the problem which would occur if an encryption algorithm was a group. If the algorithm is a group, then there is a key C which can decrypt a message which is encrypt(B, encrypt(A, text)). DES was proven to not be a group, making triple-DES a viable way to get the security of a longer encryption key.
Cheers - BIll

@_date: 2014-06-12 17:56:45
@_author: Bill Frantz 
@_subject: [Cryptography] Swift and cryptography 
OSs protect processes from each other. If you have data from more than one security context in a single process, you can't depend on the OS for protection. Use multiple processes and IPC. PostFix is a working example of this kind of architecture.
Consider Heartbleed for example. If OpenSSL is used to set up TLS sessions for a server which creates a separate process for each connection, at least the OpenSSL private key signing operations need to be performed in a separate process because otherwise the connection's security context is being mixed with the server's security context (the server's private key).
I wrote such a server for CapROS which had this bug. (CapROS makes interprocess communication easy and fast.) The only saving grace is the version of OpenSSL I used was from before Heartbleed was introduced into the code base.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-06-19 14:40:12
@_author: Bill Frantz 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
It would make perfect sense to have enough internal battery power to be able to get the write buffers in RAM to either NVRAM or disk after the power drops. That way the disk can keep its contract with the OS that when a write is signaled "complete" the data is safe and still boost performance.
Cheers - Bill

@_date: 2014-03-02 23:49:20
@_author: Bill Frantz 
@_subject: [Cryptography] Testing crypto protocol implementations 
On 3/2/14 at 12:21 PM, cryptography at dukhovni.org (Viktor It needs to be easy to add "interesting combinations" to the test suite. Good tests of this nature grow as new problem chains, TLSA records etc. are discovered in the wild.
If it is possible, generating a complete set of combinations of flaws is not unreasonable. I fear that the standards are much too complex for exhaustive testing however.
Cheers - Bill

@_date: 2014-03-06 15:25:34
@_author: Bill Frantz 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
We have a remarkably poor record of learning from past mistakes. Buffer overruns are a very clear example. We could use languages, PL/I is one of the early ones, where buffer overruns are not possible, but we don't. We could use languages where use after free is impossible, but we don't. etc. We could even do things with macros, libraries etc., to address these problems, but we don't.
Admittedly, all these safer languages are implemented in unsafe languages -- machine language at the bottom. But if we fix a bug at the language level, we have fixed it for all the applications written in that language, a much better use of debugging resources.
What we have is a history or having a problem and doing nothing to solve it. Instead we rely on techniques which don't work, like code reviews. We are unwilling to accept any solution to problems, even ones which repeatedly show up in the wild, if it makes the program slower. It is as if speed trumps correctness.
We also have a serious problem with demonstrating the correctness of security programs. For must programs we are interested in what they can do. That is relatively easy to test for. You try it and if it works the program passes.
With security programs we are mostly interested in what you can't do. That is much harder to demonstrate through testing. Sometimes we can turn the problem around into what the program can do. Can it reject a TLS connection where the signature hash in the signature isn't actually the hash of the connection setup exchange. Can it reject a connection where the certificate presented didn't actually certify the key presented. etc. etc. But for many issues, the attack hasn't been invented yet.
It is depressing to think that the problem is primarily cultural.
CHeers - Bill

@_date: 2014-03-13 15:43:42
@_author: Bill Frantz 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
First, we get no relief from the danger of exhaustive search. It is trivial to parallelize.
If we are interested in security, then we must (a) be willing and financially able to throw away the device, (b) be able to upgrade it, or (c) be willing to lose security. The cynic in me says we will always choose the (c), at least until we have been personally burned.
Cheers - Bill

@_date: 2014-03-14 18:02:40
@_author: Bill Frantz 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Well, since reliability is the same issue as security, security should be job one. :-)
Seriously, you can't have security without having reliability and you can't have reliability with our having security. Systems that fall to attacks just aren't reliable.
Cheers - Bill

@_date: 2014-03-16 14:58:23
@_author: Bill Frantz 
@_subject: [Cryptography] The role of the IETF in security of the 
I'm not convinced this analysis is correct. One analysis I like takes from Machiavelli the thought that introducing new systems is hard, because the old systems have a lot of inertia, including the people who depend on them to make a living.
The only successful security model we have seen in wide deployment is the CA model. (SSH is not generally used outside small communities.) Guess what? The CA model comes complete with a revenue model where a company can make a living.
With companies making a living from the CA model, their employees have an incentive to join IETF standards efforts effecting the companies business. Perhaps the companies will even pay for them to attend IETF meetings, although one can participate in IETF standards without leaving home.
WIth many people on the committee committed to the model, of course it has strong support and appears in the resulting standard.
My conclusion is that for wide spread adoption, we need to have a way for companies to make money so they will push adoption. Doing this with a distributed trust system is a neat trick I don't know how to do.
What may be useful here is the YRUL . It includes a hash of the server's public key so the client can know it is connecting to the correct server without using a third party.
Cheers - BIll

@_date: 2014-03-16 16:19:33
@_author: Bill Frantz 
@_subject: [Cryptography] Client certificates as a defense against MITM 
On 3/16/14 at 3:49 PM, cryptography at dukhovni.org (Viktor There is a way to do rekeying easily. You use a trusted key, whether it's the key remembered by the application, or the key whose hash is passed along with the URL as an authenticator, or some other scheme as a "CA" which can sign shorter lived keys which are used for signing the session initiation data (ala TLS). It is the identity of this trusted key which remembered. This is kind of a mini-CA model where the CA and the sub-keys are controlled by the same organization.
I don't know if PKIX can support this use. They have a CA sign keys which aren't permitted to sign sub-keys. Probably another epicycle could be invented to make this usage work with CAs though.
Cheers - Bill

@_date: 2014-03-20 18:01:39
@_author: Bill Frantz 
@_subject: [Cryptography] The role of the IETF in security of the 
I have been following the TLS committee for many years. It appeared to me that the organizations which captured it were the certificate authority companies, not NSA moles. They were the people who blew off my suggestion of having key continuity in browsers so the browser noticed when a site's public key changes. They said in essence, "But how to you handle the cold introduction problem?" In the post Snowdon era, most TLS contributors recognize that there are significant weaknesses in the CA model.
The group is beginning to discuss TLS3. There is a movement to work with competing drafts to avoid the "by committee" problems. If anyone has interest in influencing the discussions, the proper place is:
     TLS mailing list
     TLS at ietf.org
     I agree. The IETF is the most open standards organization I know.
Cheers - BIll

@_date: 2014-03-23 14:00:38
@_author: Bill Frantz 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I am of the opinion that crypto algorithms (and most other software) are like wine, best aged a bit. It takes a while for the community to beat on an algorithm before one can have any degree of trust in it. (I originally wrote "faith" instead of "trust" and perhaps faith is the right word.) After a while, the issues are better known. The really good news is that the analysis techniques continue to be refined and people know about more things to examine. (But I still expect that algorithms will continue to fall to new insights.)
Consider for example DES. It has been closely studied since its publication in 1977, over 35 years ago. Yes it has problems: weak keys, 56 bit key length, slow in software, etc., but they are known problems with known fixes, such as 3DES. I really don't expect new surprises with DES, but see above about new insights.
Because of the improvement in analysis techniques and the number of people who can use them, AES may be close to DES in its degree of maturity even though it is a lot newer.
In the area of hashes, I feel less confidence. I'm not convinced that the analysis techniques are as mature, and I'm not sure we even have a good handle on the right questions to ask. When AES was selected, some people suggested that the really paranoid could used super encryption with each of the five finalists, each with its own key. I haven't heard of anyone actually using this suggestion -- even in hardware there would be many gate delays before the first block came out of the pipeline, but the combination is probably at least as secure as the strongest of the five.
Now a similar approach might work for hashes. (N.B. I am not a cryptographer!). Take two (or more) of the SHA3 finalists, use them in parallel, and XOR the results. Pick two that are built on significantly different principles to minimize the risk of common mode failure. The result should be at least as strong as the stronger of the two. (Warning: this idea may be seriously wrong.)
Cheers - Bill

@_date: 2014-03-24 18:38:50
@_author: Bill Frantz 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
The reason here for combining algorithms is not to get something better than any of them, but rather to have protection against a serious break in one of them. Pick two (or more) algorithms each of which is strong enough for the application. Make sure they are built on different principles if you can. Combine the outputs. Now if one is broken, you still have the protection of the other.
BTW, this paper talks of concatenates the outputs of the hashes because it is trying for additional strength. If we just want, "As strong as the best of the bunch", can we XOR instead of concatenating? The result is more likely to fit in with existing protocols and will be cheaper to transmit on the wire.
Cheers - Bill

@_date: 2014-03-29 18:11:51
@_author: Bill Frantz 
@_subject: [Cryptography] OpenPGP and trust 
On 3/29/14 at 6:43 PM, stuartl at longlandclan.yi.org (Stuart The amateur radio community is a very interesting example for people interested in networked security. In amateur radio, everyone has unique true name. Mine is AE6JV. These names are government assigned and more interestingly, unique, global, and memorable (all 3 sides of Zooko's triangle). (Authenticating that someone the "real" AE6JV is a separate, unsolved, problem.)
By looking at interpersonal interactions in the amateur radio community, one can gain insight how humans react to such an identity situation.
Note that for US hams, the QRZ database is directly downloaded from the FCC. In that database, I am William, not Bill. When someone greets me as William, I know they got the name from the database, and not directly from me.
Cheers - Bill

@_date: 2014-03-30 08:15:21
@_author: Bill Frantz 
@_subject: [Cryptography] Amateur Radio Authentication - was: OpenPGP and 
[Belatedly changing the subject to match the discussion.]
One aspect of authentication in the amateur radio context is the very low bit rate used by some communication modes. JT65, which is used to bounce low power (100-600 watt) signals off the moon, is operating at 10-15 dB below the noise level. It uses the fact that a signal is coherent while noise is random to allow the signal to build over time to be able to separate it from the noise. A contact which basically only sends a little more than the call signs of the two stations takes several minutes, each station transmitting for one minute and then listening the next minute. (Yes, the clocks need to be synchronized.)
Classic RTTY is 45 words per minute (wpm) using Baudot code. PSK31 is designed so a good typest can keep up with the transmissions, making it good for casual conversations. For comparison, the model 33 Teletype at 110 bps is classified as a 60 wpm device.
The lowest rate I know of is the recent one way signaling from a large collection of amateurs around the world to the Juno space probe at a distance of about 37,500 kilometers. There a single morse code dit took 30 seconds. The signal was successfully received. These low rates significantly limit the authentication codes that can be used. OTOH, they also significantly limit the rate at which guesses can be validated.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-05-01 08:26:36
@_author: Bill Frantz 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in  C) 
I think approaching this problem like a legal problem is a very poor approach. As I see it, we have two goals which may be in conflict. We want fast programs, and we want safe programs. We may lean one way for some programs and the other way for others. We may also have programs which are designed for a specific architecture and not to be portable.
As I see it, the C language standards need to move some of the "undefined" behaviors into a different category, lets call it "target machine defined". If I understand correctly, the C standard assumes that arithmetic will be performed in one of three ways, depending on the underlying hardware architecture. Optimization should remove tests like if (x > x+1) only if that statement is always true in the target architecture. If the compiler doesn't know the target architecture, then it should assure the test is true in the three possible target architectures as well as standard, unlimited size, arithmetic.
These well defined forms of arithmetic seem to be quite different from other "undefined" operations like addressing outside the bounds of an array.
While all of these behaviors may be under control of compiler options, and GCC certainly has plenty of them, the default values should be for least surprise and maximum warning. Documenting a set of options for maximum safety, a set for maximum speed, and a set which will allow use of architecture-dependent arithmetic would also be useful.
Cheers - Bill

@_date: 2014-05-01 13:08:05
@_author: Bill Frantz 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in  C) 
I'm sorry, but I don't have much time, and am about to go away from the Internet for a while, so I won't give you response the careful thought it deserves (and probably not see your responses for several weeks).
While compilers don't always know the target architecture, the optimizer phase in some implementations may not. I have seen compilers that produced a intermediate language which is then translated to machine code.
 From the view of safe programming, if x > x+1 traps and aborts the program, this is usually better than throwing out the statement entirely. In the very common case of twos compliment modular arithmetic the statement has meaning and should be executed.
While I certainly don't know all architectures and compilers, every one I know permits arithmetic overflow traps to be masked so they don't occur and the result of the operation is modulo the arithmetic base of the machine. Both the IBM 650 and the 1620 behaved this way with their decimal arithmetic. The 1620 was variable word length so the modulus depended on the word length.
But what I think I really want is a safe mode where dangerous optimizations are avoided or terminal compile errors are generated. I hate thinking that the only safe low level language is assembler.
[I have heard mention of optimizing assemblers which will remove code, but never encountered one. The only optimizing assembler I know is the IBM 650 SOAP assembler which arranged instruction and data words around the rotating drum main memory to minimize access time.]
Cheers - Bill

@_date: 2014-05-31 14:28:27
@_author: Bill Frantz 
@_subject: [Cryptography] DOJ Wants to Expand Authority to Break Into 
From SANS NewsBites Vol. 16 Num. 038
(May 9, 2014)
The US Justice Department (DOJ) has issued a request to the US Judicial
Conference standing committee to expand its authority to gain remote
access to computers during investigations. DOJ maintains it needs the
authority to access computers outside the jurisdiction of an
investigation because criminal schemes are increasingly crossing
jurisdictions. The proposal has raised concerns among civil rights
groups, which say that allowing this activity could pose a threat to
Internet security and Fourth Amendment protections. The remote access
would be achieved through vulnerabilities known to DOJ but kept secret
from the public, thus posing a security threat. The US court system
currently allows magistrate judges to issue search warrants for property
outside their districts only in limited cases. The DOJ request will be
considered at the meeting of the US courts' Committee on Rules of
Practice and Procedure later this month.
[Editor's Note (Pescatore): The remote access part is worrisome, opens
up huge potential for cyber-damage to innocent bystanders in many ways.
It is pretty straightforward to turn off a wiretap or remove a tracking
device from a suspect's vehicle. I don't think it removing a remote
access Trojan is quite as simple, let alone giving law enforcement the
authority to keep vulnerabilities secret from the public.  I don't want
to be too hyberbolic, but to me this has the potential for backfire as
the "Fast and Furious" ATF project to smuggle guns *into* Mexico to see
who buys them. ]
I would add to John Pescatore's comment: I can see the TLAs delivering a NSL to developers of major software requiring them to install backdoors and keep quiet about it. Do Apple and Microsoft have a canary? How about Firefox, Opera, Crome etc?
Cheers - Bill

@_date: 2014-11-01 09:37:27
@_author: Bill Frantz 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
Isn't this possibility best handled by ECC and other hardware error detection/correction? When you get really paranoid about such failures you end up with parity predicting adders, redundant hardware with voting etc.
In practical cases, the most likely result is failure to verify a signature, disagreement about what key was agreed etc. It seems to me that all of these failures end up secure. If the key with the error is a long-term key, then there is a recovery problem. If it is a PFS key agreement, then a retry will correct the problem.
There may be some head scratching over "how did this happen" when examining the logs.
Cheers - Bill

@_date: 2014-11-14 20:43:27
@_author: Bill Frantz 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
I hope these alternative architectures which put the services in the end point include an economic analysis of the load of email that would be sent over cell phone connections with their limited "free" data.
I remember the mail bomb effect when my email address was used as a From: address by a spamer. I got an incredible number of "I have treated your email as spam ..." message from Barracuda email gateways, well over 5000. I'm glad I didn't get that when my net access was through my phone.
So long as there are telephone companies involved, bits won't be free. :-(
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-11-17 13:14:56
@_author: Bill Frantz 
@_subject: [Cryptography] FW: IAB Statement on Internet Confidentiality 
While the identity of humans and their programs is important for audit trails, what is really needed for security is the authority of humans and programs. If you know who is making a request, you really don't know much. You still must make a decision to grant access or not, and you run a significant risk of confused deputies. On the other hand, if you know the request is authorized, you can just honor it.
Many operating systems, KeyKOS among them, have demonstrated the ability to provide "reference monitor" functionality with out special hardware support. KeyKOS used supervisor/problem modes and standard virtual memory protection. Cheers - Bill

@_date: 2014-11-17 13:15:02
@_author: Bill Frantz 
@_subject: [Cryptography] Paranoia for a Monday Morning 
The problem with this statement is the assumption that "secure" is binary. I don't think anyone would argue that Javascript is a more secure language than C, C++ or Assembler because Javascript is memory safe. In fact, Javascript is getting more secure with the addition of "use strict" which makes it easier for a web page to include untrusted Javascript code and control what it can and can not do. There may be languages that prevent errors that Javascript allows. Compile-time type checking comes immediately to mind, but duck typing fans will probably argue that compile time type checking really isn't that valuable.
The bottom line is that there is a collection of language security features. There probably isn't a way to put languages on a linear scale and say that one is more secure than another without looking at the application domain.
Now, I do have concerns with the assurance that Javascript implementations actually implement the standard, and that the standard itself does not have bugs, but I have these concerns about every deployed system today.
Cheers - Bill

@_date: 2014-11-17 21:10:50
@_author: Bill Frantz 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Isn't is straight forward to analyse the logs looking for connections which used to be encrypted and no longer are? Depending on how many different servers/endpoints/etc. you connect to, this analysis may or may not be a data base problem. If the number is less that 10000, a hash table in memory would work well.
Cheers - Bill

@_date: 2014-11-20 15:34:37
@_author: Bill Frantz 
@_subject: [Cryptography] New free TLS CA coming 
I can't resist the ad I got from Digikey today. Go and roll your own:
Atmel? offers their ATSHA204A full turnkey security device
The Atmel? ATSHA204A is a full turnkey security device. It includes a 4.5 Kb EEPROM divided into 16 slots. This array can be used for storage of keys, miscellaneous read/write, read-only, password or secret data, and consumption tracking. Access to the various sections of memory can be restricted in a variety of ways and then the configuration locked to prevent changes.
Access to the chip is through a standard I?C interface at speeds up to 1 Mb/sec. The chip also supports a single-wire interface that can reduce the number of GPIOs required on the system processor and/or reduce the number of pins on connectors. It is compatible with most UART or serial I/O controllers. System integration is eased with a wide supply voltage range and an ultra-low sleep current of less than 100 nA.
Cheers - Bill

@_date: 2014-11-21 22:48:56
@_author: Bill Frantz 
@_subject: [Cryptography] encrypted list mail, was FW: IAB Statement 
The one encrypted list I have used used PGP. Everyone on the list had a copy of the list's secret key. Everyone encrypted to the matching public key. Its UI was as good as PGP. :-)
Cheers - Bill

@_date: 2014-11-22 17:05:21
@_author: Bill Frantz 
@_subject: [Cryptography] encrypted list mail, was FW: IAB Statement 
Just to be clear, it wasn't my list anymore than this list is my list. I have no idea what authorization the list server machine had.
Everyone who was invited to join the list was authorized to read list traffic. Men in the middle, eavesdroppers etc. were not. The decryption key, which was in fact a PGP secret key, was shared with everyone authorized to read the traffic.
Note that some of the advantages of this system are:
  * No special software, just PGP and a mail list daemon.
  * No extra load on the list server. It just forwards messages.
  * Messages in the list archive are encrypted.
  * If the list server needs to read messages, it can be given a copy of the key.
Disadvantages include:
  * New keys need to be distributed when people lose their list authorization.
  * One can forget to encrypt a message and send it in the clear.
  * Messages in the list archive are encrypted, making search harder.
If one wants to have an unencrypted list archive, one could modify the list daemon software to decrypt the messages before putting them in the archive. You would probably want to control access to the archive with HTTPS and a logon. And you would lose the advantage of standard software.
In fact the system worked quite well.
Cheers - Bill

@_date: 2014-11-28 16:14:47
@_author: Bill Frantz 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
This is a wonderful way to audit code reviews. Just add a few old attacks and see if the reviewers find them.
Cheers - Bill

@_date: 2014-10-14 17:46:28
@_author: Bill Frantz 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
Here I'm sitting, using my phone for Internet with a 2 gig limit before the charges start coming in. I avoid 30MB downloads like the plague.
At home, with "unlimited" (i.e. how much bandwidth does DSL have anyway), I would feel differently. I also have friends with only dialup, and they will indeed feel very differently from me.
Cheers - Bill

@_date: 2014-10-15 17:18:18
@_author: Bill Frantz 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
If deltas are tiny, then there is much less of a problem. If a site can continue to use its old key while phasing in a new one, then if I can select the old key I can wait until I get to an area of high bandwidth/cheap bandwidth before updating the keys.
If, on the other hand, it all happens "automagically" in the background, I may buy a big bill with out knowing it. I have this problem with auto-download of replacement phone software. That download takes about an hour on DSL. I don't know what it will do to my cell phone bill, which is why I have the system set to download only on command.
I have no idea, but I could see them doing banking.
Well, I think there will always be people with poor connectivity. I think we should make it possibile for them to enjoy as much of the online world as possible. We probably can't show them movies, but email and text messaging are low bandwidth. Some level of web browsing is also possible, limited by their patients.
How do you think we should treat them?
Cheers - Bill

@_date: 2014-10-28 13:58:46
@_author: Bill Frantz 
@_subject: [Cryptography] Paranoia for a Monday Morning 
Let's not underestimate the value of "secure" languages in improving the security of systems. Indeed they are not a panacea, but they can effectively eliminate common problems such as buffer overrun and use after free. One of the strengths of Firefox is the large body of code that is in Javascript, a secure language.
Indeed, as a friend says, "You can write Fortran in any language.", but a secure language makes the secure way the easy way.
The immediate question is, why do we still write security sensitive code in C? (Or other insecure languages like C++.) IMHO, some of the reason is in trying to interface with an infrastructure built on C programs and designed for C calling conventions. If any of the secure systems programming languages, like Rust, can get enough of a toe hold in the sea of C conventions, perhaps we can, at least, make the successful attacks more interesting than the same-old same-old of buffer overrun and use after free.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-09-04 16:14:55
@_author: Bill Frantz 
@_subject: [Cryptography] What is the difference between a code and a 
It seems to me that "code" is short for "encoding". In ham radio we are not permitted to try to obscure the meaning of transmissions, but we can and do use a large number of codes. We have lots of examples of codes which are not intended to provide secrecy. Examples from ham radio are Morse code, ASCII code, Baudot code, and Varicode. Other codes attempt to reduce the length of information. The ham radio "Q" codes perform this role and have the added advantage of allowing people who do not speak a common language to communicate. For example, QTH? means, "What it your location?"
Cyphers on the other hand are always meant to obscure meaning. They do not shorten messages.
One story from David Kahn's "Code Breakers" serves to illustrate this point. In the time leading up to World War II, the US government forbid the use of codes on international telegraph lines. Many companies objected strongly. Their issue wasn't secrecy. It was an increase of 5 to 10 in the cost of the telegrams. The solution was the "Uniform Commercial Code", which had the data compression function without the secrecy.
Cheers - Bill

@_date: 2014-09-09 14:39:53
@_author: Bill Frantz 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Well, first, why do we believe the aliens? Assuming we do and ignoring the first two, if they offer evidence for the statement that P = NP, then that evidence may offer useful hints for developing attacks. The same comment applies if we develop that evidence without space aliens.
However, I don't expect to wake up one day with no warning to find I can download a RSA/ECC breaker. (Unless the math is developed in NSA/GCHQ/KGB/etc. and the break is developed in secret.) If the math is public, it will take some time to develop breaks. However, that time may be shorter than the eons it takes to update SSL/TLS. (Look at the number of sites still vulnerable to Heartbeat.)
Cheers - Bill

@_date: 2014-09-10 14:34:57
@_author: Bill Frantz 
@_subject: [Cryptography] Enigma presentations 
There are a couple of PowerPoint presentations on the Enigma cypher machine in english in a ham radio archive in Germany. I thought they might be of interest to some.
Cheers - Bill

@_date: 2014-09-15 22:44:37
@_author: Bill Frantz 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
The requirements of crypto turn what is wanted around like a lot of things in security. Most often people ask, "What can that computer system do?" The security guys ask, "What can I be sure it can't do?"
With computational difficulty, we aren't asking, "What is the hardest case?", instead we are asking, "What is the easiest case?". Sometimes, like with DES weak keys, we find a limited number of cases where the answer is, "Too easy." and we avoid those cases. If there are too many of these easy cases, we worry about the security of the crypto.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2014-09-18 22:33:27
@_author: Bill Frantz 
@_subject: [Cryptography] [cryptography] Email encryption for the wider 
I've tried using the phonetic alphabet to clerks on the telephone. It just confuses them. Perhaps if I said, "B as in bravo, I as in India, L as in Lima, L as in Lima" they might understand better.
Cheers - Bill

@_date: 2014-09-25 17:21:41
@_author: Bill Frantz 
@_subject: [Cryptography] Of writing down passwords 
I remember sometime around 1980 taking a photo of the "passing of the check" ceremony when Univac bought a license for KeyKOS. The amount of the check was confidential, however the Nikon lens and Kodachrome film showed the value quite clearly. Oops! We fuzzed that part of the picture when we printed it.
Cheers - Bill

@_date: 2014-09-28 23:37:49
@_author: Bill Frantz 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
If that "bucket of public keys" is a collection of signing key provided by each site for the "temporary" (1-12 month) keys they generate for themselves, you are still only trusting the site and the provider of the bucket. Trust can be improved by distributing hashes of the distributed keys via paper. For high security sites, my bank for example, I'll be happy to walk over to the branch and pick up the piece of paper.
I class sites into three security levels. (1) I don't care. The New York Times, the local caving club, etc. (2) I don't want my credit card hijacked. and (3) I don't want hackers at my financial accounts. Compromise of sites in class 1 can only confuse me. Compromise of sites in class 2 cost me a maximum of $50 and some embarrassment. Compromise of sites in class 3 can wipe me out financially.
There are very few sites in class 3. I am happy to check the fingerprints for those. Even if I don't, probably someone else will, protecting me and the rest of the herd.
It would be nice to have a mechanism to ensure that my copy of the bucket is the same as everyone else's.
Cheers - Bill

@_date: 2014-09-30 14:05:38
@_author: Bill Frantz 
@_subject: [Cryptography] The world's most secure TRNG 
Educate the devs. Include in the instructions for each device the following:
"This device, like all hardware random number sources, does not produce completely random bits. If completely random bits are needed a "whitening" operation will be needed. (references). If is is being used to seed a pseudo random number generator, use XX% more bits than you would use from a perfect source.
"This device does not whiten its output in order to allow users to more easily test the quality of the random bits it produces."
Cheers - Bill

@_date: 2015-04-06 11:55:23
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
What I find most useful in this discussion is the idea that you can send an implementation a message and cause it to permanently turn off an option. If we have an implementation which is widespread in a corporation, and we wish to turn off an option, sending it a suitably signed message might be a viable option. The big questions are:
   Who gets to sign the message.
   Why didn't we just upgrade the software.
I think there may be useful application areas where there are good answers to these questions. The IoT seems a likely place. However, I still think the option of regular software upgrades is probably a better option for most uses.
Cheers - Bill

@_date: 2015-04-06 23:05:15
@_author: Bill Frantz 
@_subject: [Cryptography] Cipher death notes 
Frequently. Most of the time I ignore the signature. I don't really know the authors, so the mail must stand on its own as far as creditability goes.
All of the mail I have tried to validate has validated OK.
I only get text email. (My mail agent does not do HTML or links.) I generally look at the text.
Cheers - Bill

@_date: 2015-04-07 17:46:39
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: OPENSSL FREAK 
There are a number of fairly inexpensive microprocessor chips which can implement remote management.
The ATmega48 series processors used in the Arduino have writable flash memory for their programs. They are available in quantity one at between $3.00 and $5.00, with more capable ones going up to $20.00 or so. These might make a nice Internet interface device which talks to smaller embedded systems over local radio links and protects them from the rest of the Internet.
However, in the drive to minimum cost, I expect that all these niceties will go by the wayside and we'll have to go with a throw them out approach. The resulting planned obsolescence may appeal to the manufacturers, particularly if they can point to a "best practices" or law that makes them do it, so a drop dead on  may be adoptable.
Cheers - Bill

@_date: 2015-04-10 23:28:35
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
I don't think it makes much difference if you have a protocol which allows negotiation of algorithms from within the protocol, think TLS, or one that has only one protocol, but lets you negotiate which version of the protocol you use, like the E protocol. The only issue with only one crypto suite per version is that you can't assume that version n+1 is better than version n.
The former kind of protocol rather reminds me of my great grandfather's axe. It's the same axe, it's just had 7 new handles and 3 new heads.
Cheers - Bill

@_date: 2015-04-11 11:50:00
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
All of the protocols I know of that have version negotiation design the protocol to try to agree on the highest numbered version. What I think we need is to have each end have a preference order for versions numbers and pick the offered version which is highest in the preference list.
With this kind of mechanism, each version could have only one algorithm for each function, eliminating algorithm negotiation, and a lot of complexity from implementations. The preference ordered list is the place for each end to specify policy, as John Denker suggests. Note that if an endpoint considers a version to be spyware, it can leave it out of the list entirely, causing the connection attempt to fail unless another version can be agreed on. As Michael Kj?rling points out:
Cheers - Bill

@_date: 2015-04-11 17:38:16
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
Indeed. There was a paper published in the 1060s or 1970s which argued that as systems aged, the number of bugs introduced per bug fixed became larger. It also contend that for OS/360, the number of bugs introduced with each bug fix had grown to be larger than 1.
However, this leaves the policy decision with the standards group which assigns the numbers. IMHO, the policy decisions should be more diverse and made by people who understand the security requirements of the application. For example, some applications may place a higher priority on authentication than on secrecy, while others would make that tradeoff the other way. People closer to the application than the standards group should set the policy of which version to trust.
Cheers - Bill

@_date: 2015-04-11 17:38:16
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
This comment brings up an interesting question. Assuming both ends have a priority order of versions, how do you pick between versions that are acceptable to both ends. Assume the preference order:
  Alice: 54 27 62 11 81
  Bob:   81 10 27 22 99
Both Bob and Alice are willing to use versions 27 and 81. 81 is Bob's 1st choice and Alice's 5th while 27 is Alices 2nd choice and Bob's 3rd. We might prefer 27, since its average preference number is higher than 81, but I'm not sure there is a strong principle for this method. If both parties consider their first 4 choices essentially the same with the 5th choice a desperation measure to get some protection, then 81 might be better.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2015-04-12 14:10:07
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
I think this analysis is too simplistic. There are many more players than just the standards committees and end users. Many IT departments are quite capable of deciding which security tradeoffs meet their organizations requirements. Browser publishers are better situated than standards committees, but it should be noted that there is a good representation of browser publishers on the standards committees. Perhaps a better example is opportunistic email encryption, where the requirements are quite different from the browser case, and are not as well represented on the standards committees. Almost completely absent from the committees is the SCADA world, and their requirements probably are radically different from either browser or email requirements. I expect I'm leaving a whole bunch of areas out of this list.
Now mapping requirements to algorithm choices requires some knowledge of the characteristics of the various algorithms. It is possible for IT departments to learn this information, or they can hire high priced consultants. :-)
In any case, the standards committees don't have enough knowledge to make good decisions.
Cheers - Bill

@_date: 2015-04-14 17:16:38
@_author: Bill Frantz 
@_subject: [Cryptography] the TOFU lie - or why I want my meat... 
I'll start with the standard rant about the word "trust". Standing alone, "trust" is meaningless. The NSA is reported to say that the people you trust are those who can break your security. I certainly know people I have trusted with my children who I wouldn't trust with my investments, and vice versa. When one uses the work "trust", one must specify what is at risk.
With this fact in mind, lets look at a few scenarios.
I learned that SparkFun had the Arduino LillyPad for sale. (True story) I typed SparkFun into Google and went to their web site. It looked legit to me, and there were no security warnings. So I placed an order. Lets look at what I was risking. I was trusting the CA mess to validate that the URL,  really went to SparkFun. If it didn't, my risk was (1) Stolen credit card, and (2) Delay in getting my LillyPad. The stolen credit card is the big one. It will cost me $50 + a bunch of hassle. But, I can tell my bank that I followed "best practices[1]" in my order, so they will probably eat the $50. I have no reason to believe that the items I ordered is private information. X.509 works here.
I had to SSH into a server at work from home. (True story) I first connected my laptop while connected to the internal company network so SSH could learn the server's key. I could then connect from home. My risk was that there was a MITM in the internal network. If there was a MITM in the internal network, the company was in deep dodo and MITMing my connection wouldn't make things worse. TOFO works here.
I needed to send some company confidential data to my home, but didn't have my PGP key fingerprint with me. (True story) I down loaded the key from the MIT key server. It had been signed by Carl Ellison. I had Carl's business card with his key fingerprint which matched the signature key. I had validated the key from the server. I had to trust Carl not to sign some other "Bill Frantz" key. I had reason to have faith in his integrity, and had verified the rest of the chain myself. The web of trust works here.
Someone wants to buy illegal drugs. (Probably true, but not me.) They use an anonymous market which identifies sellers by a public-private key pair. The key pair allows the seller to develop reputation which the buyer checks. The buyer uses anonymous channels to set up a deal which results in the buyer sending some bitcoins to the seller who then informs the buyer about the location of a dead drop with the drugs. One risk to the buyer is that the seller will decide to cash in his reputation capital and run off with the bitcoins[2]. The second risk is that the law has taken over the seller's private key and is using it to nab buyers. The buyer can limit the first risk by not buying large quantities. She can, with the cooperation of the seller, check out the dead drop before committing to picking up the drugs and risking arrest. The seller needs to pick a location which is easy to check. The Economist reports that this kind of system is in current use and works.
Cheers - Bill
[1] Best Practices n. "No one ever got fired for following 'best practices'", Alan Karp.
[2] Cashing in reputation capital has happened. Back in the early 1960s, there was a company in Hong Kong which mail ordered cameras to the US (and Europe). I ordered and received a Nikon F and was quite satisfied. There came a time when they stopped sending cameras, although then still accepted orders and cashed the checks. After 9 months or so, visitors to Hong Kong reported that they had completely disappeared.

@_date: 2015-04-14 18:17:34
@_author: Bill Frantz 
@_subject: [Cryptography] the TOFU lie - or why I want my meat... 
[Re: NSA etc.]
The big danger from NSA's scarfing up everybody's data isn't today. The evidence is that the people who run NSA today are generally in favor of some view of traditional American freedom. The danger is that someone like J. Edgar Hoover notices how much power is stored in those archives and works himself into a position to start the American Stasi or KGB.
Cheers - Bill

@_date: 2015-04-28 08:32:21
@_author: Bill Frantz 
@_subject: [Cryptography] upgrade mechanisms and policies 
If you want security solutions to be widely deployed, nothing beats having a revenue model for someone. With a revenue model, you have a champion who will work hard to sell the solution to the world. Without a revenue model, you just have a bunch of geeks with a good idea. We have TLS with CAs because there is a revenue model, not because it is the best solution.
Cheers - Bill

@_date: 2015-08-10 15:06:49
@_author: Bill Frantz 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
I think it is too late for capability model OSs. The change in thinking needed to program in the KeyKOS, CapRos, Coyotos, etc. model is too far from the way people put applications together with Apache, shell scripts etc. and the Unix file system and security models.
Never mind the the capability model is almost exactly the object model without globally available objects, a model that most programmers have used. That's how you write a program, not integrate a system.
I agree with Tom that the only bright side is the attention these issues are getting. It seems to me that the TLS 1.3 effort is greatly simplifying the protocol. Of course if you must be backward compatible, then it won't help much. We didn't get into this mess in a day and it will take many days to get out of it.
A wise man once said, "If you find yourself in a hole, the first thing to do is stop digging."
Cheers - Bill

@_date: 2015-12-22 13:45:49
@_author: Bill Frantz 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I wanted to use random chance in a knitting pattern I was building, so I tossed a coin. I quickly gave up on the coin when I discovered that my tosses were quite biased. I moved to using /dev/random.
Even with a reasonably unbiased item like a coin, getting random tosses can be hard.
Cheers - Bill

@_date: 2015-12-23 13:49:59
@_author: Bill Frantz 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
To some extent, this is a serious discussion. Flipping coins and rolling dice has often been mentioned as a good source of random values. I flipped a coin because for the small number of random choices I needed, it was a lot faster than writing a program. When I discovered my flipping technique was biasing the output, I broke down and wrote a program.
I will point out that the Super Bowl, and all professional US football games start with a coin flip. The procedures they use are designed to minimize issues of poor flips: (1) The other team calls the flip while the coin is in the air. (2) The reverse of the outcome is used at start of the 2nd half. But there are still some small advantages to winning the flip.
Cheers - Bill

@_date: 2015-12-25 13:56:59
@_author: Bill Frantz 
@_subject: [Cryptography] Questions about crypto that lay people want to 
The next step is when the mass surveillance data is interpreted by AIs so we can get universal surveillance. Just what is NSA running in those big computer centers?
However, when assessing a government, we should keep the example of Germany in mind. In the 1920s, the German government was a reasonable democracy. In 1933, they freely elected the Nazis who quickly changed it to a monster. Germans to this day remember this history and have strong privacy protections in place to try to prevent the rounding up of "enemies of the state" using public databases as happened when the Nazis took over.
Cheers - Bill

@_date: 2015-02-01 19:56:50
@_author: Bill Frantz 
@_subject: [Cryptography] best practices considered bad term 
This statement encapsulates the real value of "best practices". If you follow them, you won't get fired.
Cheers - Bill

@_date: 2015-02-01 21:43:54
@_author: Bill Frantz 
@_subject: [Cryptography] best practices considered bad term 
Well, there are some important differences between best practices in security and the NEC. The NEC has the force of law for one. When I first started doing electrical work I was fortunate to find a version of the code which had been annotated with the reasons for the provisions, very useful for my style of learning.
Another difference is following the NEC is not always enough to keep you out of trouble with the local building inspector. At the time I started, San Francisco's rules forbid the use of Romex plastic electrical cable. The effect was that everyone ran conduit and then pulled wires. This rule kept the local electrical worker's union quite happy. About the time I moved out of the city, they had changed the rules under fire from the federal housing department which was tired of spending too much money building in San Francisco. They changed the rules to allow Romex in federally assisted housing, but nowhere else.
And, of course, the best practices NEC will probably result in a house that doesn't have enough electrical outlets. This is very similar to the question about how long a RSA key to use. Old guidance tends to live on beyond its time. We need a lot more outlets now than are required by the code.
In security best practices we have the debate on how often to change passwords. The original guidance of once a month was based on the thought that a brute force attack on the password file would take about two months. I think the current guidance is every 3 months. HP recently changed from once a year to once every 3 months to follow this best practice. They also reduced the minimum password length from 12 to 8 characters so the more transient passwords would be easier to remember. Is that a change for better security? YMMV.
Indeed, anyone trying to set up a security policy should look at best practices and try to understand why each item is being recommended. This examination will require a knowledgable individual since the examination will have to determine if the practice matches current reality, and if there are any other important factors that are not covered by the best practice.
Cheers - Bill

@_date: 2015-02-02 12:59:59
@_author: Bill Frantz 
@_subject: [Cryptography] Best Practices for Passwords 
I'll take a crack at it. :-)
Use a password manager with the following characteristics:
   A significantly different password for each site.
   Easy to change a single password (if only to follow political requirements).
   Actual passwords not stored anywhere.
See for example .
Note that Alan Karp is connecting the pet name tool, which shows your name for the site based on its TLS certificate, with site password. The pet name tool gives significant protection from common fishing attacks.
See: Cheers - Bill

@_date: 2015-02-03 17:59:19
@_author: Bill Frantz 
@_subject: [Cryptography] best practices considered bad term 
I think there may be an additional reason. Microsoft was always strong in selling to the enterprise. The enterprise has an IT department, and Microsoft designs its software to help the IT department. The most important way Microsoft helps the IT department is by helping it justify its existence. The justification is that ordinary users can't maintain a complex Microsoft Windows based system.
Apple always built systems to be simple to install and use. Many people liked systems that felt "right", and Apple did its best to deliver those systems.
To some extent, Apple had better QA as well. People like things the JFW. (Just Work)
Cheers - Bill

@_date: 2015-02-07 16:05:25
@_author: Bill Frantz 
@_subject: [Cryptography] What do we mean by Secure? 
The more I hear people talk about making thing secure, the more I hope they will explain what they mean by secure. What I mean, in the broadest sense, is "Bad Things Won't Happen". Now this is a bit over nebulous. :-)
In general, we think computers should enforce a policy. But what policy? When I ask this question, the answer I generally get is, "Any policy you want". But there are many policies we can't implement with our current security mechanisms.
On our home computers, my wife and my security policy is that both of us should have full ownership permissions on all of our files since the owner is the only one who can change certain meta-data, like who can access the file.. However, on our Unix based systems, a file can have only one owner. Our solution is to share accounts. As far as the computer is concerned, there is only one of us.)
Another security policy which gave IBM's RACF security policy team fits come from the Stanford Linear Accelerator Center (SLAC). Their policy was, "We are an open facility. All files are to be world read-only unless approved by management." RACF was made to keep secrets. I couldn't implement that policy.
We have more serious problems with the policies our systems let us implement, so while we have a large number of policies, they are all quite narrow and all of them share a degree of impracticality. One of the ways to see if a policy is bad is to see if otherwise loyal employees are violating it to get their jobs done. For example, does an executive share account passwords with an admin? Sharing account access is against almost all written and unwritten security policies.
For an example of the flexibility we need in our policies, consider a real-world situation (from: ): Alice, in a race to her next meeting, turns thunder-struck to Bob and says, "Bob, I just remembered I need to get my daughter Carol?s car to Dave?s repair shop. I?ve got to go to this meeting. Can you take Carol?s car over there?"
Now consider the computer equivalent: Alice, in a race to her next meeting, turns thunder-struck to Bob and says, "Bob, I just remembered I need to get my daughter Carol's prom posters to Dave's print shop. I've got to go to this meeting. Can you get Carol's PDF from her computer and take to Dave?"
The first example is an easily solved problem. The second is impossible with our current security structures. Marc Stiegler has more detail at .
In another interesting policy area, Alan Karp has developed the idea of Voluntary Oblivious Compliance (VOC) . With VOC, the system will help a user follow a policy the user doesn't even understand. My favorite version of VOC detects a violation of policy and prompts the user, "This action appears to be a violation of our security policy. Please click "Cancel" or enter an explanation for your manager."
The above comments really only deal with the data secrecy facit of policy. There are really two broad areas of security policy, data secrecy, and authority limitation. Data secrecy keeps your competitors from reading your business plan. Authority limitation keeps attackers from destroying your uranium enrichment centrifuges.
While there is a lot of overlap between these areas of policy, they are different. A power plant will probably be much more concerned about strangers remotely turning the dials than they will be concerned about strangers reading the meters. On the other hand, while a passive observer can steal data, it is hard to see how a passive attack can change the state of the system. For many important systems, data loss is a minor problem compared with authority problems.
The area of security structures to support a broader range of policies is where the work is needed.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2015-02-08 22:56:00
@_author: Bill Frantz 
@_subject: [Cryptography] What do we mean by Secure? 
I certainly hope Yourdan was wrong. If he is right, we need to keep our computers as game machines only and go back to paper for anything where there is real value to be stolen. But, I know we can do a lot better than we have.
These TCSEC systems never addressed the problems of data sharing. They were designed to follow the US military policies, which were unsuitable for the needs of commercial enterprises.
They were also probably unsuitable for military needs as well. An example is how you share information with allies. Detail weather forecasts are a real-world example.
My favorite A1 story comes from an A1 system which was discovered to have a big covert channel. However it still met all the Orange Book criteria. One of the designers of the system was heard to say, "I don't want to call is secure, I just want to call it A1." The moral of this story is that it is really hard to enumerate all the attacks that haven't been invented yet.
This thought seems more oriented toward how much assurance you have that the system actually implements the policy under attack, rather than my main interest in this thread: Whether the system can implement reasonable policies, and what are reasonable policies anyway.
If this thought means how much deviation is from their ideal policy due to implementation constraints is a organization willing to tolerate, then this statement addresses my topic.
Well, the Microsoft autorun feature for floppy disks and flash drives did exactly what it was supposed to do and was one of the bigest security problems of a system that was rife with problems.
Cheers - Bill

@_date: 2015-02-08 22:58:13
@_author: Bill Frantz 
@_subject: [Cryptography] best practices considered bad term 
I have always maintained that security and reliability are opposite sides of the same coin. If you don't have reliability, you have security flaws. If you don't have security, then attackers can destroy your reliability.
Cheers - Bill

@_date: 2015-02-09 17:29:30
@_author: Bill Frantz 
@_subject: [Cryptography] What do we mean by Secure? 
On 2/9/15 at 8:23 AM, phill at hallambaker.com (Phillip This statement is absolutely correct. When KeyKOS was being reviewed as an Orange Bood candidate, Deborah Downs said something that suddenly made the whole Orange Book criteria make sense. She said, "We trust the users, they've been cleared. We don't trust the programs they run."
There have been several such systems built. All of them enforce some level of least authority on applications. The Polaris system is perhaps the most approachable for ordinary users.
What Polaris did was run applications in a separate new Windows userID and provide a "power box" which allowed the user to select files for the application to use.
A power box is a piece of code that runs with full user privileges and can pass resources selected by the user to the application that calls it. The Polaris power box added the new user ID to the file's access control list and gave the file name to the application, which could then open the file normally. The UI presented by the power box looked like a normal Windows file chooser.
This system was good enough to prevent Word/Excel viruses, although its security correctness depended on Windows security correctness. (I do want to separate discussions of function/policy from assurance. If we get useful function/policy, working on assurance is "a simple matter of engineering". :-)
People in HP Labs were test subjects for Polaris. One executive who volunteered to run it had a busy travel schedule. The result was that Polaris got installed on his computer before he had been trained in its use. After he had been using polaris for about a week he asked when it was going to be installed. He hadn't noticed any differences from normal WIndows.
Polaris was offered to Microsoft, but they decided not to include it as part of Windows. HP did not consider itself to be a software company, so Polaris disappeared into that vast sea of good ideas that didn't go anywhere.
Cheers  - Bill

@_date: 2015-02-09 23:39:50
@_author: Bill Frantz 
@_subject: [Cryptography] What do we mean by Secure? 
OK, Ben has outed me. I confess. I've been working on capability systems since the early 1970s. The capability model is the only way I can understand how authority moves from actor to actor in a system. When I hear about other models, I think, "That complex mess. How can it possibly work?"
A brief bio for people who don't know me. I was one of the original Gnosis/KeyKOS developers . If you want the design document, see . I worked on the E language system  where I developed the cryptographic communication protocol used between E objects . I have also worked on EROS/CapROS , a clean-room version of Gnosis/KeyKOS available under GPL as an application-level programmer.
Capability systems are very good at least authority. Least authority is the best way I know to approach the ideal of a secure system.
I recently heard a tale about how the NSA suppressed research into capability systems in the 1980s by ensuring that no one who knew anything about them could get government funding. Two facts give this story credibility. (1) The people who had been working with capability systems in universities moved to other projects (because they and their grad students couldn't get funded). (2) The only papers published about capability systems in this era were written by people who didn't know anything about capability systems.
Cheers - Bill

@_date: 2015-02-10 13:05:36
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
It is probably better to look at who is asking only when producing the audit trail. Otherwise you won't be able to handle the situation I mentioned earlier in this thread:
   For an example of the flexibility we need in our policies,
   consider a real-world situation (from:
   ):
   Alice, in a race to her next meeting, turns thunder-struck to Bob
   and says, "Bob, I just remembered I need to get my daughter
   Carol?s car to Dave?s repair shop. I?ve got to go to this
   meeting. Can you take Carol?s car over there?"
Good audit trails provide security in many non-computer situations. They can work with computers as well.
Cheers - Bill

@_date: 2015-02-10 13:05:37
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
There is a bit of large scale experience with what I like to call web keys. These are unguessable URLs which provide access to a resource. As far as I can tell, DropBox uses them for files. When I put a bunch of photos into my DropBox and then send the URL to someone else, they can read the files with no user ID, just the URL.
The stated goal of Javascript/ECMAscript standardization is to move it to being a capability language. This direction isn't much of a stretch since memory-safe languages with some kind of object support already have a very strong capability base. You can't access an object without a reference, and the only way you can get a reference is to either create the object or to have the reference passed to you. Removing ways to get references which go around these two mechanisms is how you take an object language and make it a capability language.
Cheers - Bill

@_date: 2015-02-10 16:08:48
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
We shouldn't ask our computer systems to make these judgements any more than we ask our car keys to make them. What we can ask our computer systems to do is to track the responsibility for how the authorization was obtained and how it was used. Note that when our computer systems can keep these audit trails, they are doing better that our car keys.
Well, it seems to me that is what we have in our computer systems. They don't let people adapt the policies on the fly with an audit trail, which would be a lot better for most businesses. Just imagine, "No Mr. CEO, we can't give you legitimate access to that file so you can prepare your report to board tomorrow. The sysadmin has gone home. But here, use my account."
Using capabilities to track the chain of responsibility is a good way of tracking the chain of authority. See Miller, Donnelley, and Karp, "Delegating Responsibility in Digital Systems:
Horton?s ?Who Done It??" . The basic idea is to build a new capability to the object being audited tagged with the identity of the object getting the authority and pass that new capability to the second object. This pattern is an example of using capability patterns. Note that here you can get the entire delegation chain at the level of identified objects. Getting that kind of information from ACL systems is, at best, much harder.
I think we're on the same page.
Cheers - Bill

@_date: 2015-02-12 15:04:04
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Relatively late coming systems in the scheme of things. User centric, as opposed to object centric, security goes back at least as far as Multics, the SDS 940 timesharing system, and the Dartmouth timesharing system (of the GE 235 & Datanet 30). :-)
In all these systems, the security model was that users were running code they had written, and so trusted it. It should also be noted that these systems were designed before many of the attacks we worry about had been invented. At Tymshare, which ran SDS 940s (+ PDP 10 & VM/370), the early response to people, usually teenagers, breaking into the system was to hire them as interns to help maintain the game library.
Generally people were thinking of academic environments, where a reasonable response to students vying with each other to discover ways to crash the timesharing system, was to install a "crash" command so anyone could crash the system. Then it wasn't fun anymore.
The 940 had the "home files" privilege. It permitted a running executable to read and write files in the directory it was loaded from. This kind of privilege was responsible for the first "confused deputy" attack, where a compiler kept a usage statistics file for optimization use that happened to be in the same directory as the system billing file. A user discovered he could get the program to over-wrote the billing file by passing its name as the a compiler output file name. See: This is the first time anyone in this discussion has mentioned how you change ACLs. Access to them is, as far as I can tell, outside the ACL security system. Not good.
Cheers - Bill

@_date: 2015-02-14 20:38:06
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
I will note in passing that capabilities can be used to implement ACLs.[1] (Why you would want to is another question.) If you also use capabilities to control how ACLs may be changed, you have a principled answer to the problem of controlling that access.
My wire and I could set up our systems so we both have "owner" control over all of our shared files. Not being able to do that is the principle reason we share an ID and password. (Note: I'm saying the security controls on our MacOSX system can't implement our security policy.)
Cheers - Bill
[1] There are two published approaches to implementing ACLs with capabilities. describes the KeyKOS Unix implementation, which included Unix style ACLs. Another approach would be to use the a Horton-like system to implement ACLs. I'd be glad to consult on such an effort at my customary rates, but I'm officially retired now, and such an effort sounds like work. :-)

@_date: 2015-02-15 19:50:55
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
I had to stop for a long time to figure out what "working in Area 51" could mean. First of all, unless the computer is in A51, we must be talking about a remote user of the computer and not an actor[0] running on the computer. The actor is actually what is doing the access, presumably for the benefit of one or more users. Wow, it's complex already. We could have zero to N users.
Does working in A51 mean job responsibilities include A51? How about connected from within A51 or GPS says is in A51? What happens if the user is not connected when the actor requests access? Fortunately, we can abstract these questions away so at the overall system level, it doesn't matter.
As I respond below, I am going to assume we can program an actor in the system that can be called passing an identity token for whom/what ever is doing the access and it comes back with a boolean isInA51(). We can hide these difficulties and others in this actor.
?? Is "comprehension" a technical term? What about "extension"? Please explain.
This approach seems like Role Based Access Control (RBAC). If they work in A51, they have the A51worker attribute in their role set. When we set up the system for this policy, we would add the role A51worker, identify which actors were in that role, and which objects require that role for accessing them. I don't know if the above approach allows for removing someone from the role, but classic RBAC systems do. Just remove the A51worker role from their role set.
Ah, I think I'm beginning to see from where my disconnect from this approach stems. This approach is assuming a static implementation of the policy mechanisms, with no place to add my isInA51 predicate and tests for it.
One of the advantages of working with the Unix simulator in KeyKOS is that all the code affected by the addition of support for the A51 policy is user code. None of it is privileged. This makes such changes much easier to implement, test, and deploy.
Hmmm. If I were to modify the KeyKOS Unix environment[1] to support this class of policy, I would add a reference to an accessPermitted() object to each file object. The open() call in the Unix environment would call it to see if access should be granted. These accessPermitted() objects can be dynamically changed as new policies are created. Once this general support has been implemented, the Unix simulator would not have to be modified for new policies since it is only interested in the Yes/No answer, and not the details of the new policy being implemented.
I don't understand the issue here. While it might be interesting to know if P could have accessed F at time T, it seems to me that it is more important to know, did P access F at time T, and even more important, should P be granted access to F now.
Depending on requirements, and details of how the accessPermitted() return changes over time, these data could be kept. In many cases it will be relatively inexpensive, but worst case -- cell phone GPS polled every 5 seconds for being in A51

@_date: 2015-02-15 21:34:42
@_author: Bill Frantz 
@_subject: [Cryptography] Capability Myths Demolished was: Do capabilities 
Well yes, there is an overtone of annoyance that these myths persist.
Well, the way I read it is that the paper is comparing various misunderstandings of the capability model with the actual one, the object capability model. Note that Hydra was built in 1971 and the Cambridge CAP computer was built in the 1970s as was KeyKOS. Given the early appearance of object capability systems, I think the paper's limiting the use of "capability" to object capabilities is reasonable given this history.
There are no extensions to the object capability model used in that paper.[1]
Well, I think I addressed the Area 51 problem, but I'll wait for your comments. In any case, they mention so many security policies that capability systems support and ACL systems don't, for example least privilege, it is clear to me which is a better platform, but I've felt that way since the 1970s. :-)
Cheers - Bill
[1] The easiest way to think about the object capability model is to use an object oriented language as an example. Consider Java. Java references are capabilities. If we get rid of the parts of the Java library which provide ambient authority, for example the File class or the network classes, Java programs are constrained to obey the object capability rules:
     The only way you can get a capability is to be born with it,
     be passed it, or create the object it represents.
     The only thing you can do with a capability is invoke it or
     pass it in a capability invocation.
Note that in a system where we have removed the ambient authority, we still have to provide the functions that were accessed through ambient authority to programs. The E language passes these authorities to the "main" entry point. If you want to ensure that an E program can't access the file system, start it without the file system access capability. If you want to keep it off the network, don't give it the network access capability.

@_date: 2015-02-16 17:43:47
@_author: Bill Frantz 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
While I agree that getting the source code by asking for it, perhaps as part of a security review, is the most likely route, I really think the NSA could reverse engineer hard drive control code. My guess is that reverse engineering is much easier than decrypting Verona. This technique might be useful for a secret foreign piece of hardware.
Cheers - Bill

@_date: 2015-02-16 18:09:21
@_author: Bill Frantz 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
On 2/16/15 at 5:45 PM, phill at hallambaker.com (Phillip We have plash and Polaris as worked examples for IX systems and Windows. In one case, a development version of Polaris was installed on a user's system and he didn't even notice until informed somewhat later.
Polaris used a power box pattern that looked and behaved identically to the Windows file chooser. The magic is below the user interface if you assume that when a user designates a file, she also intends to grant access to that file. There are many ways to implement such a system.
Cheers - Bill

@_date: 2015-02-16 21:35:36
@_author: Bill Frantz 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
OK. I haven't assumed that the test is already there. I have assumed there is a place to stand to ask the question, ways to get the data needed for the answer, and a security system coded to correctly respond to the answer. We both agree that to implement this security policy, someone is going to have to write isInA51().
No. We're not inside the accessPermitted() function -- which for your Area 51 example could the the isInA51() function. I am describing how its result gets turned into an access permitted/denied decision.
In the implementation I sketched, to perform audit, we would have to scan all the emulated inodes to discover what reference they had in their accessPermitted() slot. The number of references would probably be small, about one/policy, so the scan should not be too difficult. To get an idea of historical data we would either scan regularly, or log when we change policies.
Well, I think when we are implementing "exotic" policies, like your Area 51 policy is meant to be, someone will have to write some code to implement the predicate. The important thing is that code can be installed as the need arises, without major trauma to the rest of the system. That is the kind of system I have described.
Implementing the accessPermitted() function is exactly what you are describing the user to provide. We are really on the same page here.
Note that when I described how to implement this policy in a capability system, I was lazy and started with a capability system that had already implemented an ACL system. Note also that my goal was to demonstrate that capability systems can implement ACL systems.
Well, none of the patterns are specific to KeyKOS, they can all be implemented in object systems, capability based or not. I chose KeyKOS because:
  (1) I'm familiar with it. :-)
  (2) It had a running implementation which was good enough to benchmark.
  (3) That implementation was documented in a paper I could reference.
Indeed, the range of desirable security policies is not well studied. Mostly people limit themselves to the ones they think can be implemented. Your Area 51 policy is a refreshing change. And reflecting them to the policy definers is a serious problem. We all want a computer which does what we mean and not just what we tell it to do. :-)
I'm not convinced there are important things in implementing security policy which are missing. There certainly is missing mind-share, and the ability to use a system is much more important for its sale than what kind of security policies it can implement.
We have a number of legacy systems which would be very expensive to replace, which seems a complete explanation for where we are. Heck, we can't even get the old systems to run programs with less that all their user's privileges, although there have been several demonstrations of how to do that without major system changes. Only newer systems like iOS move in that direction.  From these facts, I would conclude that we will only get security improvements in completely new application areas -- hardware + software. Somehow, I don't think we will see an improvement with the Internet of Things though. :-(
I really have no idea of the issues in database security.

@_date: 2015-02-17 14:14:57
@_author: Bill Frantz 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
The scenario I have in mind is China deciding that all hardware used for sensitive applications -- government, banking, military, etc. -- must be designed and made in China. I consider this scenario at least reasonably likely. A security review ruse probably wouldn't work, and we probably wouldn't trust that hardware for sensitive applications anyway.
Cheers - Bill

@_date: 2015-02-18 14:30:56
@_author: Bill Frantz 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
In the case of OBL, the body is a hot potato. What is the proper level of respect to give to it? The possibilities range from mutilation to a state funeral. Leave that problem with the Pakistanis.
I would expect that standing orders for any FBI/Military/etc. person is to grab the data. The ones with any likelihood of actually being in a position to grab some data would get training in how to keep it from self destruction.
Cheers - Bill

@_date: 2015-02-20 13:49:26
@_author: Bill Frantz 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
The real quote was more like: "We trust the users, they've been cleared. We don't trust the programs they run."
I posted it, but the quote is from Deborah Downs.
Cheers - Bill

@_date: 2015-02-22 22:36:24
@_author: Bill Frantz 
@_subject: [Cryptography] Lenovo laptops with preloaded adware and an evil 
In the late 1990s (the design document is dated July 1998), the E communication protocol[1] provided PFS. It ran well enough on the generic WIndows 95 machines of the time. The protocol used 1024 bit DH, with 1024 bit DSA signatures for authentication, 3-DES and HMAC(SHA1) for confidentiality and authentication.
Of course, Moore's law makes off by a few years not very significant. :-)
Cheers - Bill
[1] >

@_date: 2015-01-02 14:40:42
@_author: Bill Frantz 
@_subject: [Cryptography] New Encryption Standard of the Russian 
The post I saw on this question in the TLS working group is that TLS should mandate two algorithms for every crypto class (hash, symmetric cypher, etc.) Then if any single algorithm is compromised, a parameter change eliminating it will allow continued secure communication. Since connector proposes and connectee choses, fixing either would work.
YMMV - Bill

@_date: 2015-01-12 09:22:09
@_author: Bill Frantz 
@_subject: [Cryptography] open hardware as a defence against state-level 
It seems to me that using FPGAs offers a good route to secure hardware.
When we worked on KeyKOS, we were concerned about a Ken Thompson like attack on the assembler we were using. We decided that such an attack was a minor concern, since the assembler we were using was written before KeyKOS was designed, and it seemed quite hard to insert a Trojan in a system you had no knowledge of.
One advantage of FPGAs, is the large semantic difference between the programming level of the FPGA and the target architecture of the CPU. I think that large difference makes Trojans harder. As has been suggested by others, randomizing the location of various functions on the FPGA may be worthwhile.
The disadvantage of a FPGA CPU is performance. It almost certainly won't be as fast as Intel etc.'s latest.
In a very real sense, programming your own FPGA is a way to follow much of the production in your own lab, following grarpamp's advice.
Cheers - Bill

@_date: 2015-01-12 15:39:44
@_author: Bill Frantz 
@_subject: [Cryptography] The Crypto Pi 
One advantage of a machine like the Pi is that there is low level access to I/O pins, so it should be easy to add a hardware random number generator and get the data into the system.
I don't know if Paul is doing this, but adding it should be straight forward.
Cheers - Bill

@_date: 2015-01-12 19:32:31
@_author: Bill Frantz 
@_subject: [Cryptography] The Crypto Pi 
I have both boards, and I can think of advantages for both of them. When Both are based on "Systems on a Chip" (SOC), which have the CPU, memory, and all the controllers on one chip. I think they were both designed for set top boxes.
I went to get the hardware manual for the Broadcom based Raspberry PI, their web site said, "Contact our sales department." When I went to the Texas Instruments site for the Beaglebone, I downloaded an almost 5000 page PDF.
As for hardware Trojans, I think your attacker here is more likely to be the MPAA than the NSA, but I certainly could be wrong.
Cheers - Bill

@_date: 2015-06-03 17:39:58
@_author: Bill Frantz 
@_subject: [Cryptography] Forget the Enigma Code; 
It's hard to call US health care the best in the world facing the fact that the average US lifespan is 10 years shorter than the best (Monaco). We are 45th in the world in life expectancy, just behind Cuba and well behind much of western Europe.
All data from The Economist, Pocket World in Figures 2015.
Cheers - Bill

@_date: 2015-06-06 23:30:36
@_author: Bill Frantz 
@_subject: [Cryptography] A "koan" about crypto 
I have done the same thing. Perhaps we should be saying, "Sure, go ahead. But don't deploy it for anything important until it has been reviewed by comptent reviewers."
Cheers - Bill

@_date: 2015-03-01 23:11:04
@_author: Bill Frantz 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
Which means, if you are looking for the R/O tab to protect you from hostile software running the SD card, you're hosed.
Cheers - Bill

@_date: 2015-03-03 15:18:53
@_author: Bill Frantz 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
On 3/3/15 at 1:14 PM, jthorn at astro.indiana.edu (Jonathan Verified Voting  has spend a lot of time and effort in this issue. It is much more complex than appears at first.
My favorite attack on paper systems was a piece of pencil "lead" glued under a finger nail on one of the vote counters. If he encountered an ballot voting for the "wrong" candidate, he simply used the pencil lead to add a vote for another candidate, making the ballot a spoiled ballot and negating the vote for the wrong candidate.
I like the system that uses a scanner, but takes between 1 and 5 percent of the precincts and does a full manual audit of their paper ballots and electronic results.
Cheers - Bill

@_date: 2015-03-04 14:29:41
@_author: Bill Frantz 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
On 3/3/15 at 6:59 PM, jthorn at astro.indiana.edu (Jonathan I like two independent mechanisms which are compared, rather than depending on just one. If an electronic system is miscounting a large number of precincts, then an audit of randomly chosen after the election precincts is quite likely to catch the electronic systems miscount.
If we are looking at a local election, or a close national election, like the first George W. Bush election, then it doesn't take many individual attackers to successfully attack an unaudited system. I think polling is good enough to know which precincts will allow attackers to be the most effective.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2015-03-05 10:08:35
@_author: Bill Frantz 
@_subject: [Cryptography] FREAK attack 
While they are getting kind of long in the tooth, RSA-2048, 3DES, DHE-1024, & SHA1 would not have been bad choices. (For the E language protocol, we used RSA-1024 for authentication with the other three for PFS and HMAC). I think even nation state level attackers will still have problems attacking these ciphers.
However, your point is well taken. If we stretch our time horizon to 25 years, we need to have concerns about really massively parallel attacks, and the quantum systems are making continued, if slow, progress.
Cheers - Bill

@_date: 2015-03-05 10:21:07
@_author: Bill Frantz 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
If you're into tilting at windmills: I have run my browser with flash turned off for years. I read the rate of serious security bugs in flash, and it is worse that IE in a bad month. The result is that until utube went to HTML5, I couldn't see any of the videos people linked in their emails. Oh well. I don't generally go to movies either. YMMV
I could see an organization being hard nosed about the situation. Put up a browser which only supports a limited suite of protocols, based on a security analysis. Use it on the internal network, with no access to an external network (via firewall perhaps) so your operators can't upgrade it to run things like flash. (And fire them if they try.) (If they want to watch movies, let them use their phones.) Specify in your RFPs for equipment that the management interfaces for that equipment must run with a browser that only supports the listed protocols. Even a failed attempt, if well publicized, might push the industry in the right direction.
Cheers - Bill

@_date: 2015-03-14 12:19:46
@_author: Bill Frantz 
@_subject: [Cryptography] IBM looking at adopting bitcoin technology for 
And consider why a large nation state might want to disrupt a crypto-currency.
Cheers - Bill

@_date: 2015-03-18 12:17:40
@_author: Bill Frantz 
@_subject: [Cryptography] Kali Linux security is a joke! 
I suspect that guessing the software being installed from the length of the download(s) is also possible. Probably the best defense today is to install large quantities of software, and then delete the ones you don't want. You can also install from other sources, like CDs and DVDs which aren't as public.
Cheers - Bill

@_date: 2015-03-23 18:59:49
@_author: Bill Frantz 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
Our local police force has a handout for people who have installed security cameras. It includes minimum technical recommendations (pixel resolution), recommendations on where to point it, and a way to inform the police so they can get the pictures when a crime has been committed near the camera.
It seems to me that this arrangement has both plusses and minuses.
Cheers - Bill

@_date: 2015-03-26 12:52:28
@_author: Bill Frantz 
@_subject: [Cryptography] How to crypto secure speed limit signs 
Wet blanket time:
There are a few system requirements which are being missed by some of the commentators.
* The system will have to support old technology for a long time. I still occasionally drive a 1939 truck. It doesn't have seat belts. It does have a manual choke. (I think the first self driving car feature was automatic spark advance, which the '39 does have.)
* It may well be that a majority of the drivers will not support the system with the current speed limits. I drive a lot in California. The highway patrol has stated that they focus their speed enforcement on roads with a high accident rate. In practice, it is frequent to have the only cars going at or below the speed limit be the ones with out of state license plates.
* GPS may not be good enough to separate the 70 MPH freeway from the 45 PH service road running next to it. As my GPS maven son says, "GPS isn't good enough to get you back to your car. It's good enough to get you back to the parking lot."
* The system needs to fail operational. Even Ford recognizes this fact by allowing a firm press of the accelerator pedal to override their system. Being speed limited while trying to pass on a 2 lane highway sounds suicidal.
* Any solution which involves printing signs as they are erected needs to deal with the problem of sign lifetime in the hostile environment of weather and sunlight.
* Any solution needs to deal with the jurisdictions which use speeding as a revenue source.
BTW - I really like the slow speed limit sign on the bumper as a tailgater hack.
CHeers - Bill

@_date: 2015-05-01 08:33:23
@_author: Bill Frantz 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
One thing that might help is to display, by default or simple UI action, the trust chain. E.g. "Verisign.com says that this page is from WellsFargo.com." The CAs should like it since it puts their brand in front of users. It also might make organizations reluctant to change CAs.
If I ran the zoo, I would phrase the display of intermediate CAs so the top level CA accepted responsibility for their behavior. E.g. "CACert.com says that Citi.com says that this page is from CitiTrustManagement.com."
Cheers - Bill

@_date: 2015-05-04 07:25:30
@_author: Bill Frantz 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
I think it is quite frequent that we engineers both over estimate and under estimate the abilities of our users. We expect them to think like engineers, which only works for a small minority of them. When they don't think like engineers, we assume they can't make security decisions, even though they have been making security decisions in real life ever since childhood.
In real life, if I go to a URL to read something that has been posted, I really don't care about authentication or secrecy. If I get a techie security dialog, I'll click through because I am going to apply the same filters to that information that I apply to information on this list, and all of the other lists I read. I will use the same technique for the cute kitten pictures.
I will be more concerned if I am about to send my credit card, but I have protection for those transactions, so security dialogs aren't a real concern.
BTW, I don't do online banking. :-)
We, as engineers, need to present security information to our users in a way that is meaningful to them. They might be more concerned about a revoked certificate than about an expired one, just as they might be about a driver's license. They might want to know the chain of trust they are depending on, but we don't tell them either of these things. If we show them the MITM certificates, they will be in a much better position to judge how much trust to place in the connection. If we show them the convoluted trust chain, the organizations depending on those chains may decide to make the users decision easier by cleaning up their acts. And enough users will look at this information in the same way they check businesses with the chamber of commerce and friends living in the community.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2015-05-08 14:23:33
@_author: Bill Frantz 
@_subject: [Cryptography] Is there a good algorithm providing both 
We do have the well known attacks when the attacker can insert data to be compressed in the stream. In this case, the attacker can infer the presence of sequences the compressor compresses if the compressor is adaptive, i.e. changes its compression dictionary based on the data to be compressed. (N.B. most general purpose compressors are adaptive.)
These attacks have been sufficient to recover security cookies from web traffic in repeated, iterated attacks.
Probably compression is safe if each source of data is compressed separately.
Cheers - Bill

@_date: 2015-05-08 15:10:12
@_author: Bill Frantz 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
I think we are in violent agreement about what is needed. We need to speak the user's language.
Note that the users are the only ones who have the context necessary to make these decisions. We need to show our users which institutions they are depending on for the security assertions. They can make the decision of whether these institutions are trustworthy enough for the current context. In some contexts, e.g. reading a blog post, not much assurance is needed. More is needed for online banking. While we probably don't need to show this information unless asked, we do need to make it available.
Technical detail isn't needed. Only geeks like us care if the hash is MD5 or SHA512. Until we see actual attacks against one of them in the field, they are equivalent. As geeks, we know that MD5 has significant weaknesses, and we need to work to remove MD5 from use, but that is background work that doesn't need to be seen in the UI. It is like upgrading physical locks to use pins along with wards instead of just using wards.
Cheers - Bill

@_date: 2015-05-10 23:52:09
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: [EE CS Colloq]  * 4:15PM, Wed May 13, 
People on this list may be interested in this lecture. If you are not able to be there in person, it will be webcast in real time and uploaded to utube about 24 hours later. See the web site for details.
Cheers - Bill
====== Forwarded Message ======
Received: 5/9/15 9:19 AM -0400
              Stanford EE Computer Systems Colloquium
                  4:15PM, Wednesday, May 13, 2015
       HP Auditorium, Gates Computer Science Building Room B1
                        Stanford University
                               How not to generate random numbers
Speaker:  Nadia Heninger
           University of Pennsylvania
About the talk:
Randomness is essential to cryptography: cryptographic security
depends on private keys that are unpredictable to an attacker.
But how good are the random number generators that are actually
used in practice? In this talk, I will discuss several
large-scale surveys of cryptographic deployments, including TLS,
SSH, Bitcoin, and secure smart cards, and show that random number
generation flaws are surprisingly widespread. We will see how
many of the most commonly used public key encryption and
signature schemes can fail catastrophically if used with faulty
random number generators, and trace many of the the random number
generation flaws we encountered to specific implementations and
vulnerable implementation patterns.
No slides are available at this time.
Join the live presentation.[2] Wednesday May 13, 4:15-5:30. Requires
Microsoft Windows Media player.
View video by lecture sequence. [3] Spring 2015 series only,
HTML5. Available after 8PM on the days of the lecture.
View video on YouTube. Link available here approximately 48 hours
following the presentation.
About the speaker:
[speaker photo] Nadia Heninger is an assistant professor in the
Computer and Information Science department at the University of
Pennsylvania. Her research focuses on security, applied
cryptography, and algorithms. She is best known for her work
identifying widespread entropy problems in cryptographic keys on
the Internet (2012 Usenix Security best paper award), and
developing the "cold boot" attack against disk encryption systems
(2008 Usenix Security best student paper award). Previously, she
was an NSF Mathematical Sciences Postdoctoral Fellow at UC San
Diego and a visiting researcher at Microsoft Research New
England. She received her Ph.D. in computer science in 2011 from
Princeton and a B.S. in electrical engineering and computer
science in 2004 from UC Berkeley.
Contact information:
Nadia Heninger
Computer and Information Science
University of Pennsylvania
Embedded Links:
[ 1 ]    [ 2 ]    [ 3 ]    ABOUT THE COLLOQUIUM:
See the Colloquium website,  for scheduled
speakers, FAQ, and additional information.  Stanford and SCPD students
can enroll in EE380 for one unit of credit.  Anyone is welcome to attend;
talks are webcast live and archived for on-demand viewing over the web.
MAILING LIST INFORMATION:
This announcement is sent to multiple mailing lists. If you are signed
up on our private EE380 list you can remove yourself using the widget
at the upper left hand corner of the Colloquium web page. Other lists
have other management protocols.
====== End Forwarded Message ======

@_date: 2015-05-11 17:45:54
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: [EE CS Colloq]  * 4:15PM, Wed May 13, 
Yeah. You should have heard what Richard Stallman said when he was the speaker. My solution is to wait for utube. (I have a Mac and an ARM-Linux machine.)
Cheers - Bill

@_date: 2015-05-17 14:17:07
@_author: Bill Frantz 
@_subject: [Cryptography] NYT on Nick Szabo and Bitcoin 
I think I'll credit Hal Finney. At least he's safe from any government retaliation.
Cheers - Bill

@_date: 2015-05-19 19:12:43
@_author: Bill Frantz 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
My understanding is they changed it to protect against differential cryptanalysis. At the time, differential cryptanalysis was known by NSA, but not generally known in the public crypto community. (The public crypto community was almost non-existant at the time.) NSA protected a classified attack while making DES's security basically as good as a brute force attack. All users of DES had a clear ideas of how much protection it offered with its 56 bit key. Some have said that this is the last thing NSA did to improve civilian security.
Cheers - Bill

@_date: 2015-05-22 13:43:00
@_author: Bill Frantz 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
I remember hearing about a small business owner who developed an effective "child repeller" -- actually a teenager repeller. He played classical music. :-)
Cheers - Bill

@_date: 2015-05-27 23:14:15
@_author: Bill Frantz 
@_subject: [Cryptography] open questions in secure protocol design? 
On 5/26/15 at 1:21 PM, stephen.farrell at cs.tcd.ie (Stephen I would say that "one true cipher suite" means that the designers have picked a suite they consider adequate and chose to avoid the complexity of negotiation, not that the other choices are wrong.
I consider the possibility that each version of TLS only supports one suite, and you have version negotiation to chose both the version and suite to be an interesting design space that has not been adequately investigated. It would eliminate one level of negotiation. YMMV.
Cheers - Bill

@_date: 2015-10-31 21:57:36
@_author: Bill Frantz 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
As a programmer from the 1970s, I never expect  the compiler to remove my paranoid tests as modern gcc seems to do. Of course, in the 1970s, most of my code was in assembler, where removal behind my back is much much less likely.
I have always thought of C as a machine independent assembler. Having it remove code I have written completely blows that idea. And removing code is a poor way to trust the programmer.
Cheers - Bill

@_date: 2015-11-01 16:28:25
@_author: Bill Frantz 
@_subject: [Cryptography] Are zero knowledge authentication systems safe? 
The good news here is that people are actively doing security modeling and proof for the evolving TLS 1.3 standard, AND the TLS working group is enthusiastically welcoming their results.
Cheers - Bill

@_date: 2015-11-01 17:50:29
@_author: Bill Frantz 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Let me go a bit meta here. Hopefully I will be able to answer Watson's questions as I go along.
I must ask what we are trying to accomplish with a compiler/development system. I hope our goal is to help programmers, many of whom will not be true language experts, write code that is robust and correct.
To accomplish this goal, I think we need clear communication between the programmer and the development system. Part of that comes from the language standard, but compilers should warn about undefined behavior and common errors (like if (a=0)... gcc warns about). Ideally, the only observable changes from increasing the optimization level of a compiler would be slower compiles and faster object execution.
When I read Peter's challenge to define the behavior of:
           extern void *
           my_memcpy (void *dest, const void *src, size_t len)
             __attribute__((nonnull (1, 2)));
My first reaction was, of course we are defining a pre-condition for the correct execution of my_memcpy. The compiler will try to prove the parameters are not null and generate warnings if it could not complete these proofs.
But, of course, given the tone of this discussion thread, this interpretation seemed very unlikely. However, it would be a realistic interpretation if the compiler was trying to help non-expert, or inattentive expert programmers write correct code.
No. He's after my time. I lost interest in the details of compiler code generation about 1970-1775 when he was 10 or 15 years old. But, I shouldn't have to be a compiler student to use one safely.
I don't understand your point here. How is C not a machine independent low level language, like assembler is a machine dependent low level language? Yes, C compilers do memory address assignment, as do assemblers. They also do register assignment which assemblers normally don't do. Still their target application areas (e.g. operating systems, drivers, etc.) are the same.
At the level we're discussing, we care about what the object code does, or more importantly omits doing, not how it does it. Most of the discussion centers around the compiler removing tests the programmer inserted, not issues of memory or register address assignment.
This statement is clearly wrong for most languages. In the first compiler I used, FORTRANSIT (a subset of Fortran) for the IBM 650, the compiler didn't do any optimizations. It was 4 passes on punch cards with punch card intermediate storage. I was a wonder it worked at all.
I remember looking at the object code of the IBM 360 PL/I compiler at it's highest optimization level sometime around 1970. It was quite clever using a memory to memory move to initialize two integer variables. However it also stored a register into a memory location and reloaded the same register from that memory location in the next instruction. Not the best optimization imaginable.
IBM Fortran H was one of the first compilers to generate highly optimized object code. One of the jokes about it was that every time they fixed a bug, they removed an optimization.
I think the problem here is mostly social. While we could tighten up the C standard for signed integer overflow, that's only a bandaid.
I know I'm a poor expert programmer union man for wanting to help non-union people write correct, safe code. I should be protecting union jobs.
Cheers - Bill

@_date: 2015-11-03 14:36:27
@_author: Bill Frantz 
@_subject: [Cryptography] How programming language design can help us 
Linus Torvalds made a rant which seems relevant to this thread:
Even if you're not interested in the integer arithmetic issues, his use of invective is quite nice.
Cheers - Bill

@_date: 2015-11-14 11:54:58
@_author: Bill Frantz 
@_subject: [Cryptography] Atomic level precision applied to quantum computers 
Interesting item on atomic level precision applied to quantum computers.
Perhaps the first and most important application in which atomically precise fabrication will make a critically important contribution is quantum computing. Back in 2012 we noted successes of Australian researchers in atomically-precise positioning of a single atom transistor and in writing of a single-atom qubit in silicon. Also in 2012 Foresight Update reported on a workshop sponsored by the Atomically Precise Manufacturing Consortium, NIST, and Zyvex Labs to discuss the fabrication of such atomically precise devices. Now Australian researchers have provided a blueprint for operational quantum computers. A hat tip to Nanotechnology-Now for reprinting this University of New South Wales news release written by Myles Gough Researchers design architecture for a quantum computer in silicon: ...
Cheers - Bill

@_date: 2015-11-15 10:15:15
@_author: Bill Frantz 
@_subject: [Cryptography] Satoshi's PGP key. 
However, keys can be stolen, a particular problem with nym keys where there's no other to validate the key.
Cheers - Bill

@_date: 2015-11-16 14:29:45
@_author: Bill Frantz 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
I think these may be the best example we have of wide-spread imbedded systems that are still in use well after their "use by" date.
Many of them could be updated with security fixes, but their manufacturers have stopped supporting them. But people still use the old ones because they continue to work, and replacing them costs money, time to configure the replacement, and worst yet, an unpleasant time learning enough about Internet protocols and the ISP logon procedure to perform the configuration.
These devices are also a counterargument to the idea that Internet connected devices will have to be upgraded because the underlying protocols will change. Internet protocol designers spend a lot of effort making sure new protocols are backward compatible. The basic protocols used by home routers haven't changed for many years, so the old ones work fine.
One bright spot is the major browser vendors flexing a bit of muscle and turning off old, insecure protocols and algorithms. It may be that the home router continues to run, but can not longer be administrated because there are no browsers in the house that are willing to speak the old protocol. Of course users go to configure their routers only when they no longer run so will the even notice.
Another ray of light is in smoke alarms. The new ones come with a battery designed to last the life of the device. When the battery runs down, replace the whole box. (This will also replace the nuclear material used to detect smoke, which is why regular replacement is recommended.) If people actually replace their smoke alarms, they may be willing to replace other devices just because time has past.
I certainly don't have a good answer in general. For internet connected devices, ISPs could send a health report to encourage replacement.
Cheers - BIll

@_date: 2015-11-16 21:06:43
@_author: Bill Frantz 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
I was hoping that some would use HTTPS and the browsers would refuse SSL 3. Fat chance.
Cheers - Bill

@_date: 2015-11-18 14:34:14
@_author: Bill Frantz 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
The thermostat use case for internet connection is coming home a bit early after a trip where you put your house in low-temperature vacation mode. You want it to be warm when you come through the door, so you send it a message from your phone while waiting for your luggage to arrive.
Probably the best way to attach things to the Internet with some degree of security is to have an interface box which will greatly limit the things you can do on the box, and be regularly updated with security fixes. Some cars have a version of this architecture with the entertainment system, the phone, the nav, etc. being on a separate network from the engine controls and the brakes. The connection between the networks limits the things that can pass between them.
This message, from the SANS Security Digest is a harbinger of things to come in the Internet of Targets space:
====== Forwarded Message ======
Received: 10/27/15 1:16 PM -0400
  --Closed-Circuit Camera Botnet
(October 26, 2015)
A botnet made up of nearly 1,000 closed-circuit television (CCTV)
cameras has been detected. The devices were remotely accessible and had
easily guessed or default passwords. The botnet was identified by
Incapsula while investigating an attack on a client's system. The
compromised cameras were all running a Unix utility bundle known as
[Editor's Note (Murray):  Welcome to the Internet of Things.  Even if
most appliances are resistant to compromise or misuse, there will always
be enough that are insecure as to represent a risk to the Internet that
will be difficult to mitigate.]
====== End Forwarded Message ======
Cheers - Bill

@_date: 2015-11-19 20:54:25
@_author: Bill Frantz 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Well, assuming that Apple can charge higher prices because people think they are better machines, people are looking for one of two values:
  (1) It will last longer.
  (2) They will fail less often.
SInce I'm running on a MacBook Pro which is no longer supported with parts, I guess I'm at least partially in the first camp. I do enjoy having a reliable machine too, although my machine is starting to show its age.
Cheers - Bill

@_date: 2015-11-22 11:32:45
@_author: Bill Frantz 
@_subject: [Cryptography] Chrome dropping DHE (was Re: [FORGED] Re: 
Yes, it is unfortunate. TLS 1.3 is shaping up to be a big improvement over previous versions. The current roadmap has RFC publication in late Q1 or early Q2 2016. One of the unsolved issues is how to get quick, widespread, adoption.
The security of a system should be judge by its weakest link. However, it may make good engineering sense to have some links considerably stronger if the costs are low. Then a successful weakening of their security may still leave a satisfactory safety margin.
Cheers - Bill

@_date: 2015-10-21 07:26:29
@_author: Bill Frantz 
@_subject: [Cryptography] Other obvious issues being ignored? 
Assembler is your friend.
With fewer hardware architectures now than in the past, it is actually practical to write separate assembler routines for each architecture to perform simple tasks like clearing sensitive data.
With RISC architectures it is probably impossible to write code which keeps sensitive data out of registers and therefore out of kernel memory on task switch. Is it possible on the X86 architecture? In any case, assembler will offer higher assurance of what the code actually does than any compiled language.
The key here is to keep the assembler code simple enough that you can get reasonable assurance of correctness. Saying assurance of correctness is, of course, opening a can of worms. The proof people argue that proof is good enough. People like me disagree, but do think proofs are useful because they provide another way of looking at code. For some attacks, like the Linux "if ((userno = 0)) ..." attack, which was almost committed to the source tree, assembler may actually be easier to check.
I will note that there have been systems which automatically checked machine code for certain characteristics. The one I have heard of permitted untrusted code to run in an operating system kernel. It checked for termination by assuring there were no backward branches, and for safety by assuring there were no stores. (The result was returned in a register.) These are severe limitations on coding style, but still permitted the untrusted code to do useful things.
This experience shows that you don't have to accept every program which is actually safe, that you can impose some fairly severe limitations on coding style, you can still do useful work, and you can get significant assurance advantages.
Cheers - Bill

@_date: 2015-10-21 14:48:44
@_author: Bill Frantz 
@_subject: [Cryptography] Other obvious issues being ignored? 
Then don't use inline assembly. Use a separate compile, and if necessary dynamic loading so the compiler isn't even around to mess with your code.
Sorry about having to manually perform register allocation, but it isn't really that hard. I have written assembly code for the IBM 650 -- ACC and MQ registers only, the IBM 1620 -- no registers, all memory to memory, the IBM 370 -- 16 general registers, and Sun SPARC -- register windows, among other processors. The register allocation process was basically, start with register 0 (or 1) and work up, paying attention to the registers with conventional uses. If you run out, spill them to the stack. If necessary look at what the compiler does when it compiles the algorithm as an example.
There is the question of how you divide up a function (like hashing or encrypt/decrypt) between a compiler that might optimize your memory clearing away and separately compiled assembler. There are a lot of engineering tradeoffs.
If your problem is zeroing memory, then the assembler solution is easy and separate from the more complex functions. Call zero memory at the end of the function. If necessary, add it's return value to your return value, and have the assembly code always return zero to ensure the call isn't optimized away.
If your problem is keeping sensitive data, like keys, out of registers so it isn't saved to kernel task switch data structures, then the problem is harder, and I think unsolvable on RISC machines (like ARM) and very unlikely to be solvable on X86. Probably the best approach is to be able to disable interrupts while the data is in a register and clear the register before re-enabling.
What C function? The one calling the assembler subroutine? What kind of odd things are you concerned about?
Macro assemblers that let you write macros to implement IF - THEN - ELSE - FI and looping structures go a long way toward making assembler code easier to read. The IBM 370 Assemblers were wonderful tools for just that reason. Just avoid optimizing assemblers. :-)
Cheers - Bill

@_date: 2015-10-30 00:08:12
@_author: Bill Frantz 
@_subject: [Cryptography] letter versus spirit of the law ... Eventus 
This is the opposite of the Erlang approach, where a program crashes when it is confused so its recovery code can handle the issue. Since Erlang was built for high reliability telephone switches, and has been used for them for many years, with the right recovery environment, crashing is the right thing to do.
Cheers - Bill

@_date: 2015-09-11 17:33:28
@_author: Bill Frantz 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
What I find interesting is that the security failure at Ashley Madison is the first security failure I know of which has seriously impacted individual people. When your bank account is cracked, the bank makes it right. When you SSN is misused, there are ways to make it right. Admittedly any of these problems can be a royal PITA, but you don't lose any reputation when your accounts get cracked this way, even if your own negligence contributed to the incident.
With the Ashley Madison crack, people are being outed for behavior that does not have approval in much of society. The affected people are seeing the results in their relations with friends, neighbors, coworkers, employers, etc.
The next question is, will an event which affects individuals have an effect on net security? The costs of cracks have had an effect on businesses. Will this crack change individual's behavior?
Cheers - Bill

@_date: 2015-09-13 19:51:28
@_author: Bill Frantz 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
I don't think it has happened to me. (How would I know for sure?)
Please explain how that statement is unfair and inaccurate. It would seem a victim should be able to get a new SSN. Why not?
Cheers - Bill

@_date: 2015-09-18 20:55:59
@_author: Bill Frantz 
@_subject: [Cryptography] FBI: Weaker Encryption Is a Worthwhile Tradeoff 
My analysis says, your second biggest threat with airline luggage is having luggage handlers open your luggage and steal things. (The biggest threat is having it misplaced for a while. If you have a 3rd party delivery service bring it to your destination, then all bets are off.)
What you need is something that will slow down the thief opening your luggage. (I don't have to outrun the bear. I just have to outrun you.) A TSA lock may accomplish this, but with a 3-D printed master key will open fairly quickly. For myself, I use a mini-quick link which requires several turns of the fastener to open. There is no security here, just delay.
TSA sometimes puts it back the way I put it. Most of the time it is replaced differently. At least they leave a love note when they open my luggage.
Cheers - Bill

@_date: 2015-09-19 21:14:53
@_author: Bill Frantz 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
Systems like SES  allow building confined environments to run code. If these environments do not have access to send network packets, then the confined code can not send data to Google (or anyone else).
Left as an exercise for student is assuring that the confined environment is correct and is being used correctly. :-)
Cheers - Bill

@_date: 2015-09-19 21:15:04
@_author: Bill Frantz 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
I would love to have a reference for "You can't get a new SSN." Is there something on a government web page? But not all people are screwed by their banks. So being screwed by a bank falls in the "there but for the grace..." bucket where people can say, but it won't affect me.
I think this is an important point. Most of the other attacks hit individuals who can be blamed for doing things like answering Nigerian email. Most people think they are too smart to be a victim.
We have the same problem with scams in general. People/businesses/governments are too embarrassed to report their losses, so it is hard to come up with a number.
I think there may be more effective ways to improve security, but on the other hand, I've been trying to improve security for many many years, without notable success. If could publicize the costs, organizations and individuals might have a rational reason to opt for better security.
Cheers - Bill

@_date: 2015-09-21 14:45:11
@_author: Bill Frantz 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
We can give the advertising code the ability to either navigate this tab to another URL or open another tab with a URL. While the act of visiting a web page leaks data, it is obvious that one has popped up.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2015-09-21 19:57:26
@_author: Bill Frantz 
@_subject: [Cryptography] Sixteen chars crashes chrome.... 
The Internet is for entertainment, not privacy. (We won't say who's being entertained.)
Cheers - Bill

@_date: 2015-09-23 14:37:07
@_author: Bill Frantz 
@_subject: [Cryptography] Follow up on my password replacement idea 
Our current Net is funded by advertising. I think that any privacy solution which does not support advertising will fail in the market place.
How can we support advertiser's interests while protecting the users' privacy? Can we have the advertiser's privacy sensitive algorithms run in a confined place where they can not leak information about individuals? Can we audit their systems to show that they keep private information private? Are there other ways?
If targeted advertising lives up to its promise, I think I would find it much more tolerable than the shotgun stuff that crosses my screen now. It even runs the risk of being interesting and distracting. Perhaps there is a grand bargin between net users and advertisers. Make advertising interesting and fun enough and we'll recognize that it is the price we pay for a "free" Internet.
Trust relationships are much more complex that just binary. When my lawyer introduces me to his paralegal, I automatically assign the same level of trust to her that I have in him. Ont of the new items of trust is I trust her to know when to kick things upstairs to him.
When I go caving with a new caver, my trust in her is based on her reputation in the caving community. Here many people contribute to the trust. It is only after I have decided she is trustworthy enough for the trip we are planning that I actually go on the trip and observe her underground.
How far can we get not using authentication of individuals? We are seeing increasing use of "Web Capabilities", where the URL gives everything needed to authorize and perform the request. These URLs don't need user authentication in the classic sense. The can be shared between more than one individual when needed. etc.
Cheers - Bill

@_date: 2015-09-23 18:53:12
@_author: Bill Frantz 
@_subject: [Cryptography] Follow up on my password replacement idea 
You're preaching to the choir. The only trust in the browser web is that your credit card charges liability is limited to $50 (practically it is less). Living online is like living in a glass house.
Cheers - Bill

@_date: 2016-04-04 12:23:13
@_author: Bill Frantz 
@_subject: [Cryptography] Have you seen... 
I am hoping that the move toward voting by mail will spread the voting over a long enough period that this strategy won't work as well. Voting by mail brings up its own vulnerabilities, but security is tradeoffs.
Cheers - Bill

@_date: 2016-04-06 22:42:25
@_author: Bill Frantz 
@_subject: [Cryptography] Hillery's Email 
What do we know about the management of Hillery's email? I do know that the government systems have had enough breaks that a private system may well have been better protected. Also that while Obama managed to get a more-or-less secure Blackberry, Hillery couldn't get one when she was Secretary of State. Otherwise, I'm woefully ignorant.
Cheers - Bill

@_date: 2016-04-11 15:33:16
@_author: Bill Frantz 
@_subject: [Cryptography] At what point should people not use TLS? 
In addition, there are implementations which are tracking the evolving, but now mostly stable standard. The main lack of stability is with 0-RTT issues, which are still being discussed in the working group.
0 Round Trip setup has issues with forward security and client authentication privacy. It is also a "must have" option for all the big web sites you have ever heard of. The working group is trying to come up with a solution which minimizes the risks. If you care more about security than about session setup time, don't use 0-RTT. The 1-RTT seems solid.
Cheers Bill

@_date: 2016-04-13 14:22:32
@_author: Bill Frantz 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
I normally carry a USB flash drive in my pocket. With the connector retracted, it is a bit less than 3 US nickels (2.25 inches, 58mm) long. I would have no problem with either of the pictured devices.
A note in general. It is possible to make such devices too small. If I drop it and lose it on the floor, like I do with surface mounted electronic components, then it is, IMHO, too small.
Cheers - Bill

@_date: 2016-04-14 14:53:54
@_author: Bill Frantz 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
Vice President Dick Cheney was very concerned, to the point of paranoia, about his pacemaker.
Cheers - Bill

@_date: 2016-04-19 16:03:02
@_author: Bill Frantz 
@_subject: [Cryptography] Is "drivers for foo" a major malware vector? 
The best security I have seen for device drivers on a widely distributed system was in IBM's VM/370. VM/370 made a virtual 370 for each user -- somewhat like modern virtual machine systems do for the i86. The 370 did its I/O through a "channel" which attached several devices and would run a "channel program" on each of these devices. The VM/370 monitor took a channel program from the virtual machine and translated it to run on the real channel. This translation made sure the channel program could not address other I/O devices on the channel or memory which was not part of the virtual machine [1].
CapROS  is a capability operating system which runs it's device drivers as normal application programs. It supports easy porting of device drivers from Linux.
The best kind of security for device drivers is to limit their authority. The need the authority to communicate with their device, the ability to read and write certain portions of memory, and perhaps the ability to append entries to a system log. They don't need much else and shouldn't have much else. The idea of running them at maximum privilege is just crazy.
Cheers - Bill
[1] There was one major security bug in this translation involving relaxing a restriction to support an IBM written channel program. That bug was fixed with patch.

@_date: 2016-08-19 08:49:19
@_author: Bill Frantz 
@_subject: [Cryptography] Confidential Document Management, 
Phil and others interested in this area might be interested in an idea developed by Alan Karp and others at HP research called "Voluntary Oblivious Compliance". The idea is that, since sharing information is necessary to accomplish almost any job, you make it easy to share information through the appropriate security checks. This way, the system can warn the user that the sharing violates policy.
An example would be a accountant who inadvertently posts the corporate financial report to an outside web site before the official release. (This has happend.) He doesn't want to violate policy, but makes mistakes. In other cases, the employee may not even know the policy.
The system is voluntary in the sense that the controls can be bypassed by doing things the hard way. It is not a solid enforcement mechanism.
Cheers - Bill

@_date: 2016-08-23 12:38:10
@_author: Bill Frantz 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
I seem to remember reading about an issue that turned up when duplicate bridge tournaments moved from hand shuffling to machine shuffling many years ago. People noticed that there were many more distributional hands with the machine shuffling. Analysis indicated that the typical hand shuffling wasn't really mixing the cards enough, so the ordering left from the last hand presisted into the new hand. Machine shuffling was doing a better job of mixing.
Typically in rubber bridge, the cards get shuffled 3 or 4 times before being dealt again. This turned out to be far too few mixings. I think I remember recommendations for 7 or so mixings, but I'm not sure even this level is enough.
Moral: Random is hard. :-)
Cheers - Bill

@_date: 2016-08-25 18:48:35
@_author: Bill Frantz 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
I suppose it's time for my periodic recall of forgotten OSes. But this time I will take a slightly different tack.
Many OSes have been built in the search for a "secure" OS, but none of them have succeeded, even in niche markets. Perhaps the problem is in what we mean by secure. Usually we mean that the OS can enforce "policy". We don't do a particularly good job of defining policy, and we do a terrible job of defining how to convey policy to the system, and how to enforce controls on who can change the policies being enforced.
Perhaps we should take a different tack. Build a system that can support the principle of least authority. (We say authority rather than privilege because privilege is what an object is permitted to do, but authority includes privilege and adds what an object can get other objects to do for it.) Such a system would make the job of an attacker harder because fewer objects would have authority to make dangerous changes. (Hint: there would be no root privilege.)
Least authority systems can look a lot like object oriented programming languages without global mutable state. This makes them quite familiar to modern programmers, a definite plus.
A few systems which have addressed least authority:
Polaris: A system to improve safety on Windows machines. Polaris made a new user for each execution and gave that user very limited privileges. For example, if you opened an attached word document in your mail agent, word would run as a different user with access to just the document. Programs could gain access to other files by using the system open dialog. If the user selected a file in the dialog, it was added to the program's ACL.
KeyKOS, Eros, CapROS: Eros and CapROS are the same system which is a clean room implementation of KeyKOS. Objects run in their own protection domain and can pass authority via capabilities in calls. These systems have a much smaller TCB than Polaris, but they don't run standard Unix/WIndows/MacOS programs that expect to open files by presenting their names to the open operation, which is basically all of them.
There are probably others, but these are the ones that pop to the top of my head.
Cheers - Bill

@_date: 2016-08-29 16:08:27
@_author: Bill Frantz 
@_subject: [Cryptography] ORWL - The First Open Source, 
As a long time capability bigot, I can't resist this opening.
Capability systems are a nearly perfect match for what the Orange Book calls  "discretionary security". The authority passed through a capability can be very tightly matched to the needs of the operation. The, "Why does this cellphone flashlight application need my address book and access to the net?" situations can easily be avoided. For those not familiar with capabilities, object invocations in languages such as Java and Javascript are examples. In fact, there are capability implementations in both languages.
Discretionary security is sufficient to enforce the principle of least authority (POLA). We need POLA because, as Alan Karp recently said (not a direct quote), "I'm not smart enough to keep the bad guys out. So I want to limit the damage they can do when they get in." It helps to limit that damage if objects in the system are protected at a fine grain. Individual objects or files are good. Individual file systems less so. Capabilities are excellent at supporting fine grain authority because with one object, the capability, you both name the specific object to use and demonstrate the authority to use it. If we were to try object-level fine grain authority with access control lists, administration would become a nightmere.
An orthogonal issue is assurance. Assurance is what Perry is looking for when he wants to have the processor design formally verified. The best systems will have both capabilities and assurance, but either can be valuable with some level of weakness in the other. Generally for assurance, the smaller the system the better. Even without proof, an OS seems more likely to be correct than a language based system, and hardware should be better than an OS. However we can get a lot of useful protection from language based capabilities even without the level of assurance we would really like.
When people talk about "security" they generally mean "mandatory security". Mandatory security can keep you from passing certain things (data/objects/files etc.) to other objects in the system even when you have access to both those things and those objects. There have been a number of papers saying the capability systems can not implement mandatory security and therefor they can be secure. The problem with these papers is that there are a number of published techniques for implementing mandatory security in capability systems. These techniques use the objects in the system, rather than low level system primitives, so assurance for them is a form of program assurance.
One advantage of building mandatory security on top of capabilities is that there is a first class story for how the privilege of changing the mandatory controls is controlled, It can even be made fine grain if necessary, which might make administration easier.
Cheers - Bill

@_date: 2016-08-30 11:49:46
@_author: Bill Frantz 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
On 8/30/16 at 9:22 AM, phill at hallambaker.com (Phillip I think it is the cost of recalculating the key schedule. Some ciphers, e.g. Blowfish, deliberately have expensive key schedules to deter exhaustive search attacks.
Cheers - Bill

@_date: 2016-08-30 18:16:04
@_author: Bill Frantz 
@_subject: [Cryptography] N. Korean radio broadcasts string of random 
I think the FCC would want to know about your coded transmissions, which along with transmitting music are not permitted on ham radio.
I think the coded transmission restriction is a sop to the national security professional paranoids, but it was much more useful in the 1920-1990 time frame than it is now, given how easy international communication is via the Internet. The music limitation is to protect radio hobbyists from having to compete with wanna-be-broadcasters.
There are some exceptions to the coded transmission restriction including control of amateur radio satellites and protocols that support authentication without secrecy. The latter makes me occasionally think about all those authentication only protocols I didn't discuss during the crypto wars of the 1990s.
73 Bill AE6JV

@_date: 2016-12-01 21:24:18
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL and random 
I think we would all love to be in this state. How do we get there? What does OpenSSL do since it lives in the real world? I think that Bear's solution is the best I've seen. The disto people have to figure out how to delay using random/urandom until it is initialized. Not screwing this up will become a requirement for components used in early boot.
Cheers - Bill

@_date: 2016-12-02 18:26:58
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL and random 
This is the wrong characterization of the problem. Once getrandom() has been seeded, it will remain seeded until the computer is rebooted, and not cause hangs. The hangs are far from random since they only occur soon after boot. What we actually have is that if a program which uses getrandom() is run too soon, it will hang. If it is critical part of the boot process the system boot is likely not to not complete. If it is not a critical part, it is likely that getrandom() will be seeded by other activity and the program will finish. Here the details of how randomness is collected and accounted for is critical. Those details are the proper area the system integrators.
As a practical matter, for those systems which have hardware random sources, which I think includes all modern x86 chips, this is not a problem. getrandom() will not block for long. As John points out, it can also be made a non-problem if there is a sound card and a number of other UI devices. The most common systems that may have a problem are Raspberry Pis, Beaglebones, and many IOT devices.
In mitigating these problems, it may be useful to consider when what John calls "squish" is good enough. Clearly it isn't good enough for generating long-term SSH keys, but is it good enough for randomizing hash tables? Can we use a weaker source of random bits for these less critical cases which can be made available before the random pool has been properly initialized?
Cheers - Bill

@_date: 2016-12-04 10:57:25
@_author: Bill Frantz 
@_subject: [Cryptography] TV set power correlates to TV channel? 
Since at one time, England collected license fees on a per-TV basis, technology has been used to count TVs. I believe they drove around in trucks with directional antennas and listened for the signal from the TV's local oscillators.
Cheers - Bill

@_date: 2016-02-05 15:17:27
@_author: Bill Frantz 
@_subject: [Cryptography] =?utf-8?q?EE380=3A_How_to_Co_mpute_with_Schr=C3=B6?= 
A nice introduction to the state of the art in quantum computing and insight into the issues practitioner are facing.
Available at: Eleanor Rieffel     Wolfgang Polak
QuAIL,NASA Ames Research Center
The success of the abstract model of classical computation in terms of bits, logical operations, algorithms, and programming language constructs makes it easy to forget that computation is a physical process. Our cherished notions of computation and information are grounded in classical mechanics, but the physics of our universe is quantum. A natural question to ask is how computation would change if we adopted a quantum mechanical, instead of a classical mechanical, model of computation.
In the early 80s, Richard Feynman, Yuri Manin, and others recognized that certain quantum effect could not be simulated efficiently on conventional computers. This observation led researchers to speculate that some difficult computational problems could be solved efficiently using these hard-to-simulate quantum effects. Slowly, a new picture of computation arose, one that gave rise to a variety of faster algorithms, novel cryptographic mechanisms, and alternative methods of communication.
In the first part of the talk, we will introduce key concepts underlying quantum computing and describe alternative quantum computational models. In the second half of the talk, we will discuss applications of quantum computing, known advantages and limitations, and briefly touch on the current state-of-the-art in building quantum computers, quantum error correction, and fault tolerance, and the many open research questions that remain.
Cheers - Bill

@_date: 2016-02-12 17:19:56
@_author: Bill Frantz 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
Perhaps I'm being dumb, but you should be able to generate an IV for disk encryption by using the disk block address. Throw in the encryption key and a hash if necessary. Or use the output of the hash as a disk sector specific encryption key. CBC mode is one possibility given an IV.
If your opponent can read the cipher text of a sector multiple times, he may be able to learn things from your update pattern. I don't see an easy way to eliminate this leak.
What am I missing?
Cheers - Bill

@_date: 2016-02-15 20:51:18
@_author: Bill Frantz 
@_subject: [Cryptography] 
The heck with what the law says. People who know more than NSA about crypto systems are really hard to find. Certainly no one working for other government departments.
Cheers - Bill

@_date: 2016-02-17 18:31:12
@_author: Bill Frantz 
@_subject: [Cryptography] Hope Apple Fights This! 
I have an iPhone 4S. Every time I update the iOS on it I have to enter my passcode.
Cheers - Bill

@_date: 2016-02-25 17:39:07
@_author: Bill Frantz 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
The IBM 3830 disk controller and other similar controllers has a specified behavior when the the IBM channel (think DMA) did provide bytes fast enough.
The specified action was to fill the block on disk with the last byte received from the channel and then write a good error checking code. Why the didn't write a bad error checking code to indicate the write had failed is beyond me.
Cheers - Bill

@_date: 2016-01-02 13:55:52
@_author: Bill Frantz 
@_subject: [Cryptography] Alice, Bob, Eve, Mallory, Maxwell ??? 
Isn't that Mallet? He can read, modify, inject, and discard any messages between Alice and Bob.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2016-01-02 13:55:53
@_author: Bill Frantz 
@_subject: [Cryptography] Formal definition of lightweight crypto 
Another possible direction is block crypto with small block sizes. It wouldn't be useful for large amounts of data, but there may be a niche for it on small devices with low data requirements.
Cheers - Bill

@_date: 2016-01-02 16:04:09
@_author: Bill Frantz 
@_subject: [Cryptography] List of 64 open spec Single Board Computers 
I built a system using a BeagleBone Black a over a year ago. I built it of low power ham radio use without worrying about more than consumer grade security. (It is rarely connected to the Internet and doesn't hold any important secrets.)
I used a 12 volt HDMI display from ADAFruit and a USB hub. It works nicely as a Linux system, although it is a bit low on horsepower for some ham radio digital modes.
Being based on a TI chip, I was able to download the chip documentation, all 5000!!! pages of it. I was not able to obtain similar documentation for the Raspberry Pi as it is based on a Broadcom chip and their web page said to contact a salesman. In both cases, I have no idea about the possible backdoors built into the chips, but I do know that installing back doors is high on the priority list of certain government organizations, both US and non-US.
Cheers - Bill

@_date: 2016-01-08 15:49:52
@_author: Bill Frantz 
@_subject: [Cryptography] Plan to End the Crypto War 
One advantage a split key has is more security against insider attack. You need N of M insiders inorder to get at the data. If those insiders have to be inside different organizations, all the better.
Cheers - Bill

@_date: 2016-01-14 19:38:32
@_author: Bill Frantz 
@_subject: [Cryptography] Verisimilitrust 
The basic trust model used to implement capabilities in Waterken and E, to name two systems off the top of my head, is useful. They use references (ala URLs) which identify the receiver of the connection (aka server) and a secret which represents a resource on that server. E has a hash of the public key of the server and the secret number. I think Waterken is using TLS and its trust model.
With the public key, or its hash, there is no PKI to speak of. The trust model is that you trust the place you got the URL, which includes the ones you generate yourself. Since you have assurance you are contacting the same place over and over, you can develop trust based on behavior.
I think modern versions of TLS can be made to use a similar method of trusting the other end, although it is much less commonly used than it should be.
Cheers - Bill

@_date: 2016-01-20 18:44:01
@_author: Bill Frantz 
@_subject: [Cryptography] TRNG related review: rngd and /dev/random 
So, having a cron job which just reads /dev/urandom every second or so would make a good countermeasure. (I hope I have the Unix randoms correctly separated. I think random blocks, and urandom provides output by remixing a pool. MacOS only has urandom, but they call it random.)
I must say, I think people spend too much time trying to characterize the entropy available from ransom sources. In many cases, the estimates are quite inaccurate. However, if you have a good idea of the physical source of the
With most sources -- disk timings, network timings, Intel hardware, CPU temperature changes, Bill Cox's dongle, etc. etc,

@_date: 2016-06-29 11:42:47
@_author: Bill Frantz 
@_subject: [Cryptography] 40 years of "Diffie-Hellman" 
The way I like to think about this kind of situation is the Columbus was the last person to discover America. We all know that many other people had discovered America before Columbus, including the people who immigrated over the Bering Straight and Leif Ericson. But after Columbus announced his discovery, no one else could make a credible claim to having discovered America.
In the case of Malcolm Williamson, he was working for GCHQ and everything he invented was classified.
Cheers - Bill

@_date: 2016-03-15 19:20:50
@_author: Bill Frantz 
@_subject: [Cryptography] MSFT doesn't retain keys for its own German cloud 
The traditional way of handling these situations is to ask some foreign agency to the collect the data, so it is operating outside its own borders. The word on the street is that GCHQ and NSA are good buddies with these arrangements. Of course, proof that it happens is much harder to get.
Cheers - Bill

@_date: 2016-03-21 19:14:19
@_author: Bill Frantz 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
Ignoring the ability of  formal methods to actually find bugs, they are valuable because they make you look at code and systems from a different prospective. This different view will be particularly valuable if the system was developed informally e.g. using agile methods. Another viewpoint has great value for finding bugs.
Cheers - Bill

@_date: 2016-03-27 10:59:01
@_author: Bill Frantz 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
The result depends on the break, but let's assume worst case. New messages are broken until a new protocol can be deployed.
I think this assumption is quite weak. We need to get new code (even if it is just a configuration file) deployed. Our history of actually achieving this goal isn't particularly good, although with powerful devices (phones, laptops etc.) and Internet based operations, automatic update is improving the situation.
We still have a slow process in older devices which are no longer receiving first class support from their distributors. Android phones are a good example, but the older Macintosh which runs our livingroom projector hasn't seen a update from Apple in quite a few months. There's no compelling reason to replace it. It still works fine, and newer devices don't offer any compelling advantages. (Support for Bluray might be compelling.)
Until we can speed up switching to new standards or algorithms in old standards, we have a problem.
I this case we also have the problem of bugs in the algorithm selection protocol and its implementations. We have a long history of these kinds of bugs in widely deployed protocols.
Cheers - Bill

@_date: 2016-11-09 22:11:58
@_author: Bill Frantz 
@_subject: [Cryptography] protecting information ... was: we need to 
Since I think of threat model as, "What is trying to attack me and how is it attacking?", I would add, "What do you mean by secure?"
Sometimes you need data secrecy, but don't have any actions to authorize and sometimes you need strong authorization, but don't need secrecy.
Cheers - Bill

@_date: 2016-11-10 15:00:01
@_author: Bill Frantz 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
[Reordered as there are two topics.]
Hillary's concern about the State Department employees seems quite rational to me. A technical solution to the problem is to build email servers where the admins can't read the emails. Encryption seems to be a logical tool to use. Such a solution would probably protect against outsiders as well.
We use a similar system to the one Arnold uses here in Santa Clara County, California. Assuming a small percentage of the precincts are randomly selected for audit after the votes have been counted, it should be quite secure.
There is an organization working to make sure our voting systems can be audited, . People interested in this area should know about them.
I don't think we can get to 100% certainty in voting system, or any other systems for that matter. Attackers are just too resourceful. I remember as a child hearing of a voting attack using only paper ballots. The attacker was one of the vote counters. He had a pencil lead glued under a fingernail. When he counted a ballot for the "wrong" candidate, he used the pencil to mark the ballot so the vote for that race would be invalid (spoiled). I don't remember how the technique was discovered, but apparently it was quite effective.
Cheers - Bill

@_date: 2016-11-10 21:31:52
@_author: Bill Frantz 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
I think that risk is fairly low. Unless the ballots are dusted for fingerprints, I don't see a way to connect the ballot paper with the voter. But, with the ballot paper, an audit can show that the electronic scanning read and recorded the ballot correctly.
What am I missing?
Cheers - Bill

@_date: 2016-11-11 18:57:24
@_author: Bill Frantz 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Our walk-in polling places do not scan the ballots while the voter is still there. The just put them in a ballot box. No chance for an automated reality check of a person's vote.
Cheers - Bill

@_date: 2016-11-17 16:52:54
@_author: Bill Frantz 
@_subject: [Cryptography] On the deployment of client-side certs 
Since there is a good way of securing the camera with a piece of sticky tape, it is much more important to control the microphone which can't be easily defeated.
Of course, both camera and microphone are good sources for seeding a random number generator. But we wouldn't want to scare a security conscience user by turning on the lights just to hash the input. It's a complex problem.
Cheers - Bill

@_date: 2016-11-18 19:26:36
@_author: Bill Frantz 
@_subject: [Cryptography] On the deployment of client-side certs 
With private email to Dave, I got:
so I'll sent it to the list
Here I'm worried about audio in to a hostile program running on my computer. Is you audio chip busted in both directions?
Cheers - Bill

@_date: 2016-11-19 17:55:25
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto and rustling 
Photographers were active toward the end of the 19th century. I have seen my great grandfather's glass plate negatives from the 1885 era, which are now held at the Monadnock Center for History and Culture. (We're outsourcing the care of the family heirlooms.)
I also know of a case where some antique furniture was recovered because of photos of the grain patterns in the wood. These photos were mid-twentieth century.
Cheers - Bill

@_date: 2016-11-28 21:12:21
@_author: Bill Frantz 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
But triple DES IS better than single DES.
The algorithms used matter. Rot13 is bad encryption algorithm because it has no mixing. XOR is a poor combiner because A XOR A ==> 0, losing all randomness and producing a predictable output when the two inputs are perfectly correlated.
Probably a secure hash in hash(1, A) XOR hash(2, A) ==> not zero, perserving some of the input randomness in A. BUT, I know of no proof of this assertion.
I must admit that I am very nervous about trusting only one source and feel more comfortable combining sources. If I combine RDRAND with mouse movement locations and timings I have sources that require my opponent to have very different mechanisms to compromise. If RDRAND is compromised, it is most likely by a nation state attacker who probably doesn't have access to the detailed timing of operations on my computer. Using dev/random or dev/urandom (they are the same on my computer) as a combiner is fine. If my opponent has access to both sources, or has compromised the combiner in the OS, then I am truly toast. He can control anything my computer can control and access any information stored on my computer. Good random numbers won't help.
Cheers - Bill

@_date: 2016-10-06 14:04:16
@_author: Bill Frantz 
@_subject: [Cryptography] Security Fatigue 
, proof the security needs to be usable.
Cheers - Bill

@_date: 2016-10-08 23:28:38
@_author: Bill Frantz 
@_subject: [Cryptography] Security Fatigue 
Oops. Copy/paste error. I should have liked . I didn't have any problem viewing the paper at this link. A text version of what I saw is:
Security Fatigue Can Cause Computer Users to Feel Hopeless and Act Recklessly, New Study Suggests
October 04, 2016
After updating your password for the umpteenth time, have you resorted to using one you know youll remember because youve used it before? Have you ever given up on an online purchase because you just didnt feel like creating a new account?
If you have done any of those things, it might be the result of security fatigue. It exposes online users to risk and costs businesses money in lost customers.
A new study from the National Institute of Standards and Technology (NIST) found that a majority of the typical computer users they interviewed experienced security fatigue that often leads users to risky computing behavior at work and in their personal lives.
Security fatigue is defined in the study as a weariness or reluctance to deal with computer security. As one of the studys research subjects said about computer security, I dont pay any attention to those things anymorePeople get weary from being bombarded by watch out for this or watch out for that.
The finding that the general public is suffering from security fatigue is important because it has implications in the workplace and in peoples everyday life, cognitive psychologist and co-author Brian Stanton said. It is critical because so many people bank online, and since health care and other valuable information is being moved to the internet.
If people cant use security, they are not going to, and then we and our nation wont be secure, Stanton said.
The study, published this week in IEEEs IT Professional, draws on data from a qualitative study on computer users perception and beliefs about cybersecurity and online privacy. The subjects ranged in age from their 20s to their 60s, hailed from urban, suburban and rural areas, and held a variety of jobs.
The interviews focused on the subjects work and home computer use, specifically about online activity, including shopping and banking, computer security, security terminology, and security icons and tools.
We werent even looking for fatigue in our interviews, but we got this overwhelming feeling of weariness throughout all of the data, computer scientist and co-author Mary Theofanos said.
Years ago, you had one password to keep up with at work, she said. Now people are being asked to remember 25 or 30. We havent really thought about cybersecurity expanding and what it has done to people.
The multidisciplinary team learned that the majority of their average computer users felt overwhelmed and bombarded, and they got tired of being on constant alert, adopting safe behavior, and trying to understand the nuances of online security issues.
When asked to make more computer security decisions than they are able to manage, they experience decision fatigue, which leads to security fatigue.
Researchers found that the result of weariness leads to feelings of resignation and loss of control. These reactions can lead to avoiding decisions, choosing the easiest option among alternatives, making decisions influenced by immediate motivations, behaving impulsively, and failing to follow security rules.
Comments among those who expressed feelings of security fatigue included:
     I get tired of remembering my username and passwords.
     I never remember the PIN numbers, there are too many things for me to remember. It is frustrating to have to remember this useless information.
     It also bothers me when I have to go through more additional security measures to access my things, or get locked out of my own account because I forgot as I accidentally typed in my password incorrectly.
Participants also wonder why they would be targeted in a cyberattack. The data showed that many interviewees did not feel important enough for anyone to want to take their information, nor did they know anyone who had ever been hacked.
Commenters also expressed the sentiment that safeguarding data is someone elses responsibility, leaving computer security up to their bank, online store or someone with more experience.
Individuals also questioned how they could effectively protect their data when large organizations frequently fall victim to cyberattacks.
The data provided evidence for three ways to ease security fatigue and help users maintain secure online habits and behavior. They are:
     Limit the number of security decisions users need to make;
     Make it simple for users to choose the right security action; and
     Design for consistent decision making whenever possible.
To obtain a clearer picture of computer security behavior, the researchers will be interviewing additional computer users of varying levels of responsibility, including cybersecurity professionals; mid-level employees with responsibilities to protect personally identifiable information in fields such as health care, finance and education; and workers who use computers but for whom security is not their primary responsibility.
Stanton and Theofanos suggest it will take a multidisciplinary team of computer security experts, psychologists, sociologists and anthropologists working together to improve computer security issues, including behavior, to manage security fatigue.
Paper: B. Stanton, M.F. Theofanos, S.S. Prettyman, S. Furman. Security Fatigue. IT Professional, Sept.-Oct. 2016. DOI: 10.1109/MITP.2016.84
Cheers - Bill

@_date: 2016-10-25 14:21:15
@_author: Bill Frantz 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
How easy would it be for a nation-state attacker to modify these signatures? There are some indications that nation state attackers are interested in influencing the US elections. Improving the believability of damaging lies seem a perfect technique.
Cheers - Bill

@_date: 2016-10-30 11:38:03
@_author: Bill Frantz 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
While it is possible that forgeries might be produced by a gang, I think the motivation for a nation=state attacker is a lot better. The whole purpose of releasing these emails is to affect the US election and its perception around the world. That is most likely nation-state business. Nation-state attackers are likely to have better attack tools too. I could see a nation-state cooperating with a gang to perform the attack as well.
Cheers - Bill

@_date: 2016-09-03 12:09:59
@_author: Bill Frantz 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
Jerry hits the nail on the head here. The bug is unreliable hardware. Rowhammer raises the probability of this bug occurring, but it could occur without an attack. So the short answer is, "Fix the hardware." Any other fix is a bandaid.
There are a couple of hardware fixes mentioned in the paper. DDR4 chips apparently refresh more often in areas that are frequently hit, which might be enough. Otherwise, ECC checked memory has been around for a long time, since the days of the IBM 370 at least. ECC can be set up to correct n bit errors and detect m bit errors where m>n. Getting more protection requires more bits to store the ECC check code, but memory is cheap. The good news is that you can probably figure out how to use the old memory chips/boards, which will lower the cost. The bad news is that the problem could be in the cache memory on the CPU chips, which would require new CPU chips. (Intel smiles.)
So the question is, what values do we need for n and m in a system under attack?
While I'm asking questions, I'll echo Jerry in asking about ECC key vulnerability?
In the real world, if the probability of failure without attack is at all significant, fixed hardware could be a marketing point for a cloud provider, even without the attack. "Our systems get the right answer more often than our competitor's systems." Even if the probability is too small to worry about, protection against this attack would be an attractive marketing pitch.
Cheers - Bill

@_date: 2016-09-20 22:52:10
@_author: Bill Frantz 
@_subject: [Cryptography] Secure erasure 
When the 360 came out, IBM was a fairly open company. The source for the OS was available. The System 360 Principles of Operation (PofO) was probably released in much the same maner as the similar document for the DEC VAX, with history and implementation suggestions eliminated from the public version. (I don't know this for a fact, but it seems reasonable.)
The real genius in the PofO was the way it was produced. The story goes that when some implementator asked, "How is  supposed to work?", the question was not answered. Instead, the relevant section of the PofO was rewritten and the questioner asked, "Now do you understand?" The result was a very precise and understandable manual about a very complex system.
10-15 years later, IBM started drifting down the secrecy path. :-(
Cheers - Bill

@_date: 2016-09-21 14:30:54
@_author: Bill Frantz 
@_subject: [Cryptography] Secure erasure 
For a look at more modern systems: I got interested in the Raspberry PI and bought a board to make a low power computer. The PI is based on a Broadcom system =-on-chip designed for set top boxes. Since I share Jerry's liking of manuals, I went the the Broadcom web site to see what documentation was available for the PI. Their answer was, "Click here to talk to a salesman." I didn't bother.
When I wanted a bit more performance, I switched to a BeagleBone Black board. That board uses a TI chip. I went to the TI web site and was able to download a PDF manual for the chip -- all 4966 pages of it! It seems to be complete, with descriptions of the CPU and all the integrated I/O gear (USB, HDMI, etc.) I haven't delved into it deeply enough to determine if I could program or build hardware based on the manual, but I can't complain about too thin a manual. :-)
Cheers - Bill

@_date: 2016-09-28 15:16:17
@_author: Bill Frantz 
@_subject: [Cryptography] Use Linux for its security 
I will note that most of the kernel security bugs they found were in device drivers. CapROS has managed to run Linux device drivers as "user mode" programs. That means that many of the protections the OS provides apply to device drivers also. CapROS is released under GPL. Perhaps Linux should adapt the idea, and even some of the code, to their problem.
Cheers - Bill

@_date: 2017-04-01 18:46:04
@_author: Bill Frantz 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
"Doctor, it hurts when I do this." Why are you doing that?
In this case, why have spaces in the password at all. Initial and reset passwords should be painfull enough to type that the user will change them. One coworker at a job a long time ago always set up accounts with the password of all the letters A thru Z followed by the 10 digits 0 thru 9. He didn't have to tell people to change their initial password. They would always ask him how to change a password.
Cheers - Bill

@_date: 2017-04-04 14:42:41
@_author: Bill Frantz 
@_subject: [Cryptography] Tempest and limits on receiving 
I think this "law" is a FCC regulation. It exists because some people strenuously objected to having recordings of their phone conversations publicized. Many of them were people whose names you might recognize. WEP was also a later response to this problem.
Cheers - Bill

@_date: 2017-04-06 15:30:04
@_author: Bill Frantz 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
Lets get real here. A specific example:
A reasonable receiver to use to get these warnings is a cell phone. Our county allows you to sign up to get the warnings by cell phone, which I have done. Stanford University has a class, EE380, which I attend occasionally. It is held in the basement of the Gates Computer Science building, and there is no cell phone reception there. Is the assertion that Stanford is violating the law because it built a building from concrete and rebar? Give me a break.
Cheers - Bill

@_date: 2017-04-10 18:40:54
@_author: Bill Frantz 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
In the US, and I suspect almost everywhere else, it is illegal to attempt to hide the meaning of communications via ham radio. (There's an exception for controlling a satellite.) All of the codes used by hams (e.g. the Q codes) are published, the data encoding techniques are also published, and cyphers aren't used. Hams generally want to be seen by government regulators as being good actors as a way of protecting the spectrum allocations they have. Hence bristling when you start using crypt.
As an aside, I think we are safer if people with scanners can understand police radio communication than if these communications are encrypted, as is becoming more common.
As I read the regulations, crypto can be used for authentication. There are a number of applications where this would actually be useful, including limiting control of repeaters to authorized people. In this application, replay prevention is the biggest problem.
When I was working on the E communication protocol, the US government was trying like mad to suppress the legal use of crypto for secrecy. I looked at what it would take to yank the secrecy while keeping the authentication, and it turned out to be a hard problem. The tokens the E runtime passed between machines to identify remote objects needed to be kept secret. I think the problem could have been solved, but I am very glad I didn't have to solve it.
Cheers - Bill

@_date: 2017-08-10 19:24:22
@_author: Bill Frantz 
@_subject: [Cryptography] Code Breakers by Craig Collie 
I found an interesting book in a Sydney, Australia airport book stall:
Code Breakers
Inside the shadow world of signals intelligence in Australia's two Bletchley Parks
Craig Collie
First published in 2017 by:
Allen & Unwin
83 Alexander Street
Crows Nest, NSW 2065
ISBN 978 1 74331 210 0
This book describes the signals intelligence efforts in Australia during WWII. It goes into detail about the politics, personalities, code breaking, and traffic analysis used in some of the epic battles in the south Pacific. It describes the early use of IBM tabulating equipment in code breaking. It gives enough detail about the structure of the Japanese codes so the reader can appreciate the effort needed to break them and re-break them after they were changed.
A brief description of the typical code structure: First the messages was converted to numbers using a code book which took words or phrases as input. Then "additive tables" were used like a one time pad to scramble the output of the code book for transmission. Since the additive tables were reused many times, there were obvious security issues, although these issues don't seem to have been the major way the codes were actually broken.
Traffic analysis based on the strength and direction of radio signals allowed prediction of air attacks without being able to read the messages, although since the airplane codes were designed for easy use in the air, they were relatively easy to break.
The book is also a fun read. Recommended.
Cheers - Bill

@_date: 2017-08-28 13:33:02
@_author: Bill Frantz 
@_subject: [Cryptography] How to find hidden/undocumented instructions 
The IBM 370 had what I think was a design bug involving data which spans pages. There was a quite useful instruction, TR (translate) which would take a string of up to 256 bytes and translate it in place to new values in the bytes. It used a table of 256 bytes which was indexed by the input value. The table entry was the output value. This instruction was left over from the 360, so it wasn't designed for paged memory.
If the table spanned a page boundary, the hardware did a "dry run" where it scanned the input string to ensure that all the table entries were mapped. If all the entries were mapped it proceeded to execute the instruction, otherwise it raised a page fault.
If some other functional unit (another CPU or an I/O channel) changed the input string to reference a table entry that wasn't mapped between the dry run and the execution, the hardware generated a machine check. It couldn't start over, because the input string had been partially modified. It couldn't continue because the needed table entry was not addressable. It was up a creek.
I never saw, or heard of this machine check occurring in real life, but you could, in theory, write a program to crash a 370 in a way that was supposed to be impossible to do from a program.
Cheers - Bill

@_date: 2017-12-29 15:47:15
@_author: Bill Frantz 
@_subject: [Cryptography] Build your own fab, a project for teenagers 
For those who worry about backdoors being inserted in their chips:
Two articles, one in QST (January 2018) and one in the IEEE Spectrum describe how Sam Zeloof, KD2ENL, age 17, has built his own semiconductor fabrication facility following suggestions from Jeri Ellsworths YouTube channel , where she demonstrated how she had made some home-brew silicon transistors. Sam is using a 10 micrometer feature size, well below the current state of the art. However, surplus equipment for that feature size is available inexpensively. He has built a few transistors and is targeting building a 4004 CPU as his next project.
Cheers - Bill

@_date: 2017-02-01 18:38:53
@_author: Bill Frantz 
@_subject: [Cryptography] Happy birthday, Ralph Merkle! 
Yes. Happy birthday. He has also made significant contributions in theoretical nanotechnology. (If you can't build it today, then it is theoretical.)
Note that buying in person with a credit card has its risks. I had my card number "borrowed" in Heathrow once for buy a bottle of scotch. Carrying cash or gold to make purchases also has risks. You just can't avoid those pesky things.
Cheers - Bill

@_date: 2017-02-10 16:34:48
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: [Trans] Internet-level Consensus - new list and 
I thought some readers of this list might be interested in this new IETF group.
Cheers - Bill
====== Forwarded Message ======
Several applications depend on Internet-wide consensus to secure an
append-only log, provide tamper-resistant timestamps, and atomically
commit transactions across mutually distrustful parties with no
preexisting relationship. Examples include:
* The IETF trans working group is specifying data structures and
  operational mechanisms for providing secure logging and auditing of
  TLS server certificates, but lacks a mechanism for determining
  consensus among logs (or consensus about whether or not a resource
  should be logged). These functions are currently served by an
  experimental gossip protocol that can potentially be strengthened
  through global consensus.
* The Stellar payment network, is used by remittance companies to trade
  currencies and send payments across the Internet.
* UCSD's SPAM (Secure PAckage Manager) project relies on a secure global
  log both to enable revocation of previously published vulnerable
  software packages and to guarantee that a particular software release
  has been publicly available for audit (somewhat like certificate
  transparency).
* Stellar has an ongoing secure naming project that aims to allow domain
  name owners to assign human-readable names to end-user public keys
  without retaining the ability to lie about those public keys
  undetected.
We have just created a new IETF mailing mailing list to discuss such
applications, the mechanisms that can meet their consensus needs, and a
potential role of the IETF in devising a stable specification for an
Internet-level consensus mechanism.  The home page for the list is here:
        If there is interest in the topic, we would like to organize a BoF in
Chicago.  Please join the discussion on the list if you might be
interested in participating.
Trans mailing list
Trans at ietf.org
====== End Forwarded Message ======

@_date: 2017-02-16 20:54:56
@_author: Bill Frantz 
@_subject: [Cryptography] Security proofs prove non-failproof 
It will be interesting to see what kind of bugs show up in these formally verified systems over years of use.
It is clear there may still be bugs in the verification methodology as well as in the choice of what to verify.
The real value today of formal methods is that they force people to take a different view of their programs. Looking at things from a different viewpoint frequently shows things you didn't see before. This feature alone makes them worthwhile. Making them easier to use will spread this advantage.
Cheers - Bill

@_date: 2017-02-20 15:29:34
@_author: Bill Frantz 
@_subject: [Cryptography] Security proofs prove non-failproof 
The TLS working group has held up publication of TLS 1.3 so people can do formal analysis and feed back into the standard. That work has turned up some issues.
The critical mass is probably when someone reports that they saved real money using formal methods to achieve a level of software reliability. Some things that will help:
   Make the UI better than teco/CMS EDIT.
   Teach formal methods in computer science courses.
My knowledge is limited. I've been exposed to some of the type theory at the Friday morning FRIAM meeting, and someday I may be able to explain what a monad is.
Cheers - Bill

@_date: 2017-02-28 15:46:27
@_author: Bill Frantz 
@_subject: [Cryptography] jammers, nor not 
Our house is made of stucco. I can easily contact the local 2 meter wavelength ham radio repeater about 1.5 miles away using 100mW on a hand-held radio. Theory says that shorter wavelengths should work better because they will fit through smaller gaps in the shielding. I'm probably going out a window, but it could be a wooden door.
Cheers - Bill

@_date: 2017-01-15 14:46:58
@_author: Bill Frantz 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
The TTP could decide to cash in its reputation capital and disappear. This has occurred in real life. One story:
Back in the 1960s, Japanese cameras were very expensive in the US as there was only one importer and it could and did charge monopoly prices. A company in Hong Kong addressed this situation by mail ordering cameras from Hong Kong. I actually ordered a camera from then. It came without the name plate -- trademark law was used to enforce the one importer policy. I also receive a letter which contained the name plate and the two screws to attach it. I was happy.
This company became quite popular and received many orders. There came a time where it was cashing checks/money orders etc., but not shipping. After about 9 months people realized that it was never going to ship and checked the address in Hong Kong. It had flown the coop with all the cash. It had cashed in its reputation capitol.
Cheers - Bill

@_date: 2017-01-28 12:44:28
@_author: Bill Frantz 
@_subject: [Cryptography] HSM's to be required for Code Signing 
On 1/27/17 at 12:52 AM, pgut001 at cs.auckland.ac.nz (Peter It seems to me one could build a HSM auditor which passively monitors the interface to the HSM and records the time of every signing operation. If the communication between the computer and the HSM is in the clear, more information could be recorded, but just the time the signing operations are performed would provide a useful audit trail.
Cheers - Bill

@_date: 2017-07-03 14:03:58
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL CSPRNG work 
I agree with Ted here. If you can't find /dev/urandom, then crash with a message. This crash should happen during testing the chroot, and strongly encourage whomever is building it to fix the problem. They can still fix it badly, like give a maintain security when the platform is fubared.
Cheers - Bill

@_date: 2017-07-05 17:14:16
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL CSPRNG work 
This is certainly a bad knot to deal with. There if at least one other option that might be useful:
   Replace that temporary SSH key as soon as a good one can be generated.
The "Don't generate the key until it is needed" solution also sounds good.
My own take on this problem is that stopping the boot process waiting for randomness that neve comes is likely to be quite reproducible. Given that it is reproducible, glibc should specify getrandom() to block. If getrandom() is called when it must block, write messages to the log (if it is up and running) about the situation. In any case describe the situation in the man page. The system will fail often enough during testing that fixing the problem becomes a problem for QA, or if the system is released without testing, customer support. That way at least the developers will at least have to think about the problem.
Cheers - Bill

@_date: 2017-07-07 15:38:33
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Actually I think generating random seeds is easier. Include a hardware random number generator -- e.g. a diode, and you have a solution.
The reason this is easier is that every widget that comes off the assembly line is the same. The problem with MAC addresses is that every widget is different.
Cheers - Bill

@_date: 2017-07-12 14:30:30
@_author: Bill Frantz 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
Last year, I saw a 6502 processor implemented in discrete components/SSI gate chips. (I can't remember which.) It was about a cubic foot and contained a lot more LEDs, which made it a light show when operating. I could run some Apple ][ code.
Since it was done as a hobby project, this kind of development is not out of reach for an individual effort. Probably an early IBM PC 8088 chip is the same scale of effort.
Cheers - Bill

@_date: 2017-06-11 21:49:36
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto Books, 2017 
I learned cryptography from Schneier E2. When I started writing code, I needed to use the Handbook as well.
I would think that for implementors, knowing about attack methods is even more critical than knowing about all the algorithms. Attack methods have grown since the 1990s, with timing and power analysis being proven practical. Implementors need to know about these methods and defenses against them.
On the other hand, for people analising crypto systems, knowing about the major algorithms and their families, may be more important.
What are these students supposed to learn?
Cheers - Bill

@_date: 2017-06-29 08:33:01
@_author: Bill Frantz 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Does OpenSSL support any platforms that do not have a system-supplied secure random number generator? The world isn't all MacOS/Unix/Linux/Windows.
I am also suspicious of performance arguments applied to security code. I would like to see measurements of the possible improvement gained by avoiding a system call to /dev/urandom (aka /dev/random on MacOS). My guess any improvement gained by avoiding system calls is trivial when weighed against the cost of doing the crypto in OpenSSL. YMMV.
Cheers - Bill

@_date: 2017-03-10 22:45:40
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: SANS NewsBites Vol. 19 Num. 020 :Industry to 
The following complaints are entirely compatible with the idea that the Three Letter Agencies have decided that there is more advantage in maintaining security flaws in commercial systems so they can be broken than it in having them fixed so US organizations are protected.
Cheers - Bill
====== Forwarded Message ======
Industry Officials to House Committee: Government is Not Sharing Enough Cyber Threat Info
(March 9, 2017)
Tech industry officials testified before the U.S, House Homeland Security Committee's cybersecurity panel, saying that there is an imbalance in threat information sharing between the private sector and the government. Legislation passed in 2015 grants companies protection from legal liability when they share threat information with the government, but the government has been less forthcoming with threat information that could help protect IT systems in the private sector. Intel Security Vice president Scott Montgomery noted that when the government classifies a cybersecurity event, it "restrict[s] the number of people who can lend assistance and... allow[s] the adversary to operate with impunity." Witnesses said that if information about threats could be stripped of identifiable information and provided to members of private organizations who hold security clearances, companies would be better positioned to take action against similar threats.
Editor's Note
[John Pescatore]
This has been the standard complaint about all such government intelligence "sharing" initiatives for over a decade. Many proposals have been put out for how to overcome government worries about exposing sources and methods, but no movement on the govt. side. On the enterprise side, no reason to think this will change any time soon.
[Jake Williams]
Over-classification of cyber threat data is a real problem. I've worked incidents where threat data has been shared with federal law enforcement, only to see small portions of that same data shared with a limited distribution community weeks later in "Flash" messages. When we inquired why the most important data we shared with the feds wasn't shared with the broader community, we were told it was classified.
[Stephen Northcutt]
This is complicated, but also historical. For the last 25 years, the US Government's policy has been, "give us your data and we might share 1% back." If we are talking about a partnership, "that dog don't hunt".
Read more in:
====== End Forwarded Message ======

@_date: 2017-03-11 16:24:27
@_author: Bill Frantz 
@_subject: [Cryptography] Has formal verification actually been useful in 
Reporting in 2002, there was a DARPA funded project which resulted in "The DARPA Browser". Although not formally verified, it was designed to severely limit the privileges available to the renderer so a hostile renderer could not damage either the browser or the underlying system. The code and design were attacked by a tiger team. See: .
Cheers - Bill

@_date: 2017-03-13 18:46:32
@_author: Bill Frantz 
@_subject: [Cryptography] USB firewall/condom HW/SW 
On 3/13/17 at 12:47 PM, crypto-metzdowd at bmt-online.org (Bertrand While I agree with the comments of the form, "But how do we know we can trust it?", there is one advantage to using it if it really does what it claims in terms of protecting your computer from random USB devices. I gives you only one attacker, rather than a world of them. YMMV!
Cheers - Bill

@_date: 2017-03-28 16:38:21
@_author: Bill Frantz 
@_subject: [Cryptography] "Perpetual Encryption" 
OK. With this scheme you can calculate message 2 (M2) encoded with K1 by: M2 xor (K1 xor K2). Now if you have any known plaintext in message 1 (M1) you can decode the same offset in M2.
I knew there was a reason I was ignoring the source material for this thread.
Cheers - Bill

@_date: 2017-05-01 08:29:36
@_author: Bill Frantz 
@_subject: [Cryptography] CFB/OFB/CTR mode with HMAC for key stream 
Exactly this idea was suggested by at least one well-known cryptographer* during the 1990s crypto wars as a thought experiment to show that even if encryption algorithms were controlled, MACs could be substituted.
Cheers - Bill
* I don't currently have the bandwidth to search out the name.

@_date: 2017-05-15 22:49:22
@_author: Bill Frantz 
@_subject: [Cryptography] CFB/OFB/CTR mode with HMAC for key stream 
Bingo! Thanks for the trip down memory lane.
Cheers - Bill

@_date: 2017-11-15 16:00:54
@_author: Bill Frantz 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Back when I was working the the Simple Public Key Infrastructure (SPKI) with the IETF, Carl Ellison had very strong complaints about ASN.1's signing protocols. He told me that to follow the standard, when you received a message, you broke it down into the native formats of your computer, and then re-encoded it before checking the signature. This procedure failed miserably when the two computers used different floating point formats: e.g. IBM 370 and IBM PC.
Cheers - Bill

@_date: 2017-11-18 11:57:08
@_author: Bill Frantz 
@_subject: [Cryptography] Is ASN.1 still the thing? 
It may be nonsense, but Carl Ellison was severely burned by this issue on a project before we started developing the SPKI spec. That experience, along with the large number of security flaws in ASN.1 implementations, made him hate ASN.1. Since he was a principle author of SPKI, we followed his wish to avoid ASN.1 like the plague. That's the history.
Cheers - Bill

@_date: 2017-10-30 14:53:59
@_author: Bill Frantz 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
This situation may be similar to the discovery of America. Columbus was not the first person to discover America, he was the last. After he "discovered" it, no one else could make a credible claim of discovery.
If Google leads average users to better security hygiene, the online world will be safer. They may be thought of as the last to discover hardware security tokens, although they may have made some significant advances in account recovery procedures for the general Internet.
Cheers - Bill

@_date: 2017-09-10 17:12:54
@_author: Bill Frantz 
@_subject: [Cryptography] Zero Knowledge: Have I Been Pwned? 
My suggestion is that you, or the Pawned Password Database make a Bloom filter  from the the passwords. It will be small enough to store on your disk

@_date: 2017-09-19 14:37:12
@_author: Bill Frantz 
@_subject: [Cryptography] POLA, was Re:  letsencrypt.org 
On 9/19/17 at 5:02 AM, cryptography at lakedaemon.net (Jason This kind of structuring is a necessary practice for any program where security is important. (It can also help with reliability.) It is called the Principle of Least Authority (POLA) and protects the system when an attacker finds a bug in one of the components. It requires an environment which enforces access controls with a high degree of assurance that they can not be bypassed.
Programs with this kind of structuring have bee implemented in Linux-like systems (e.g. Postfix), and at the language level using Javascript, Java, and other memory-safe languages.
If Bluetooth stacks had been written using this kind of structuring, the BlueBorne attacks would have been a lot less effective[1].
I would say that anyone who says, "Oh, I don't need to use POLA. I can write perfect code." is a fool. I would also point them to the first exploit in the referenced paper. It is a classic buffer overrun written by a Linux kernel programmer, who should know better, and not caught by the code review process.
Cheers - Bill
[1] describes the Bluetooth protocol and the BlueBorne set of exploits. It notes that the spec for Bluetooth is 2822 pages.

@_date: 2018-08-06 16:24:36
@_author: Bill Frantz 
@_subject: [Cryptography] Krugman blockchain currency skepticism 
OK, I can't resist any longer. Let's look at some payment and store of value systems:
Gold metal: Terrible for electronic payment. Likely to be accepted anywhere fiat currencies are weak. Can be converted to fiat currencies where they are strong, but the transaction may have to be reported. If you become a refugee, gold can be carried with you. Can be stolen at gunpoint. Resistant to environmental disasters like fire and disk crashes. Relatively stable in value over the long term. Doesn't earn interest. The gold standard. :-)
Dollar, Euro, Pound Sterling (well regarded fiat currencies): Can be used with electronic payment systems. Likely to be accepted anywhere. Easy to convert to other currencies, but transactions may have to be reported. If you become a refugee, notes are transportable and accounts can be accessed if not frozen. (Pick a good jurisdiction for that account.) Notes can be stolen at gunpoint, accounts can be protected from that attack. Notes are not resistant to fire. Relatively stable in value over the long term, but a bit less than gold. Accounts can earn interest. What world commerce runs on.
Bitcoin: Can be used for electronic payments. Not accepted everywhere. Can be transferred to other bitcoin users, but the transaction is recorded in the blockchain, leaving a record. Can be converted to fiat currencies for wide acceptance everywhere, but fiat currencies may have to be reported. If you become a refugee, transport is limited by the electronic devices you can take with you. Such devices can be stolen at gunpoint. You decide what your policy is on unlocking the device at gunpoint. Fire resistance depends on what you need to access your bitcoins. Wildly changing value over the recent short term and no long-term track record. Can't earn interest, but may increase in value like any commodity. Not widely used in commerce.
Eggs (from the Economist Buttonwood column 7/21/2018 on hyperinflation in Venezuela): Not usable for electronic payment. Generally accepted for payment in Venezuela. Can be sold as a commodity in most locations and converted to other currencies. Not normally tracked by governments. Hard boiling recommended should you become a refugee to improve transportability. Destroyed by fire, but small fires may help them make a good meal. Relatively stable value over the long term, but individual eggs spoil in a matter of weeks to months. Can't earn interest. Used in commerce in limited locations suffering hyperinflation. Can directly provide human nourishment.
Cheers - Bill

@_date: 2018-08-18 08:44:25
@_author: Bill Frantz 
@_subject: [Cryptography] God Mode backdoors 
The big question with any approach is where is the plain-text secret kept. If it is on an Internet connected computer, game over, you lose. So let's assume it is on a computer which does not have the hardware to connect to the Internet -- no WIFI or Bluetooth and no Ethernet connection. A Raspberry Pi or the like might do.
We still have a computer with back doors -- one for NSA, one for GCHQ, one for China, etc. We can improve our confidence in the containment by housing the computer in a metal box and adding ferite suppression to all the wires going into and out of the box. (These wires might connect to a keyboard and a display.) If we can keep our plain-text inside the box, then we are still safe. We can even have that computer do the encryption, with a lot of ifs ands and buts.
If we use trusted hardware to do the encryption, or verify the encrypted data does not include backdoor information, how do we built that hardware? I can see three ways:
   (1) Build it out of small scale ICs -- hex inverters, and quad nand gates for example. It seems very hard to put a backdoor into this kind of system. I have seen a 6502 built this way. It was about a cubic foot and could run Apple ][ programs.
   (2) Build it using a FPGA. There could be backdoors in the FPGA, but going from a device programmed at the gate level to a useful backdoor at the CPU level seems like it might be hard.
   (3) Run your own fab. Old fab equipment is available at prices that are affordable by individuals. I read about one guy who has his own fab in QST. I know another fab owner personally. She says her yield is currently limited by not having a good clean room, but she is getting some functioning chips in her garage.
So, just how much performance do we need from the trusted hardware?
Cheers - Bill

@_date: 2018-02-01 16:58:10
@_author: Bill Frantz 
@_subject: [Cryptography] canonicalizing unicode strings. 
Pinyin was developed by the Chinese communists as part of their successful attempt to spread literacy in China. It is a phonetic script that accurately describes the sound of Chinese characters. Resemblance to European pronunciation is intentional, but there are some letters, like "c" which don't resemble any European pronunciation, but fill a hole in the spectrum of Chinese sound. When I studied Chinese, I was just glad that they chose the Roman alphabet. They considered using the Cyrillic alphabet because of their friendship with the Russians.
The other major inovation was the simplified character set. These characters are a lot quicker to write than the traditional characters. Simplified characters are used in Singapore while traditional characters are used in Taiwan.
Cheers - Bill

@_date: 2018-02-12 21:10:08
@_author: Bill Frantz 
@_subject: [Cryptography] RISC-V branch predicting 
There are two worked examples of using Unix processes for isolation, Postfix and Qmail. In both cases, the solution to mail server vulnerability has been addressed by isolating the parts of the server which parse user input.
One security rule I was able to articulate after reading Bernstein's Qmail paper  is: Give the parts of the program most likely to have bugs the fewest possible privileges. Bernstein reduces the privileges of some code to reading it's input stream and writing it's output stream. I say, any part of the program which parses arbitrary strings is a good candidate.
This kind of isolation is a bit klunky in Unix. It is a lot easier in some object oriented languages, but using languages to implement isolation requires including the language runtime in the security kernel which, in practice, defeats most of the benefit. Capability operating systems give OS levels of assurance with the ease of use of object languages. Unfortunately, these OSes have been experimental for the last 35 or so years.
Cheers - Bill

@_date: 2018-02-18 16:16:46
@_author: Bill Frantz 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
Norm Hardy has doen a lot of thinking about this approach which he call The Digital Silk Road. See  for the top of the tree of web pages.
Cheers - Bill

@_date: 2018-01-04 20:38:27
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
I can imagine speculative execution which does not proceed if the needed data is not already in the cache. If the caching is working as advertised, this will have a small effect on overall performance. (MOst of the data needed is already in the cache.) It will defeat the attacks which use cache presence or absence as a signaling path.
Cheers - Bill

@_date: 2018-01-05 14:47:41
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
I am informed by an expert in computer speculation that much of the performance gain from speculation comes from pre-loading the cache with data that will be needed soon, so this idea just won't fly.
Cheers - Bill

@_date: 2018-01-06 14:54:24
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
For those who like unfiltered information about these problems:
Spectre: Meltdown: On 1/6/18 at 7:34 PM, stephen.farrell at cs.tcd.ie (Stephen Part of the problem here is that performance is so easy to measure, sometimes even easier than, "Did I get the correct answer". Just how do you measure security or privacy? There is a human tendency to ignore things that can't be measured and pay attention to those that can.
For the Meltdown attack, which obtains information from kernel memory as fast as 500KB/sec, there is clearly a bug in the Intel chips. They do not check the permission bits before fetching during speculative execution. The attackers could not successfully perform their attack on AMD and ARM chips. Perhaps these chips do perform the permission check.
I think the approach taken by KeyKOS and other systems -- which didn't have the kernel pages mapped when user processes are running -- is a powerful way to avoid these problems. However this approach does have a significant performance hit. There may be ways to reduce the performance hit, but I think this approach will always be slower.
It should also be noted that the Spectre paper lists other possible covert channels in CPUs, but the authors haven't investigated them.
It seems to me that if two different protection domains share any kind of memory then there is a risk of using the speed of access to that memory as a covert channel to communicate secrets. With Meltdown, you can actually code you write to run as the transmitter (in the address space holding the secret).
The Spectre attack also requires some form of transmitter code in the sending address space. You can't, in general, write this code yourself, so you must discover it. Libraries are a fertile source of possible code "gadgets". Since you will be executing this code during speculation, the rules are very different, although the approaches are similar to those used in "Return Oriented Programming"[1]. In the case of the speculation attack, the attacker mistrains the branch predictor instead of overlaying the stack with a buffer overrun.
Cheers - Bill
[1] [from the Spectre paper] Return-Oriented Programming is a technique for exploiting buffer overflow vulnerabilities.  The technique works by chaining machine code snippets, called gadgets
that are found in the code of the vulnerable victim. More specifically, the attacker first finds usable gadgets in the victim binary. She then uses a buffer overflow vulnerability to write a sequence of addresses of gadgets into  the victim program stack. Each gadget performs some computation before executing a return instruction. The return instruction takes the return address from the stack, and because the attacker control this address, the return instruction effectively jumps into the next gadget in the chain.

@_date: 2018-01-06 15:09:30
@_author: Bill Frantz 
@_subject: [Cryptography] Crypto for optimistic transactions ? 
The Spectre attack can use "Return Oriented Programming", which is used to empower buffer overrun attacks on systems which don't allow execution from read/write pages. See the Spectre paper below. Note that instead of overlaying the stack, the Spectre attack sets the addresses in the branch prediction table in the processor. No write access to victim memory needed.
For those who like unfiltered information about these problems:
Spectre: Meltdown:

@_date: 2018-01-09 15:39:19
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation considered harmful? 
On 1/9/18 at 1:36 PM, cryptography at dukhovni.org (Viktor My reading is the same as Victor's. Note that the Branch Target Buffer is addressed by virtual address -- sometimes just a truncation of a virtual address. It does not have address space information or process ID or anything which would confine it to one process/security domain.
To quote from the Spectre paper:
     In most cases,  the attack begins with a setup phase,
     where  the  adversary  performs  operations  that  mistrain
     the  processor  so  that  it  will  later  make  an  exploitably
     erroneous speculative prediction.  In addition, the setup
     phase  usually  includes  steps  to  that  help  induce  spec-
     ulative execution, such as performing targeted memory
     reads that cause the processor to evict from its cache a
     value that is required to determine the destination of a
     branching instruction.   During the setup phase,  the ad-
     versary  can  also  prepare  the  side  channel  that  will  be
     used for extracting the victims information, e.g. by per-
     forming  the  flush  or  evict  portion  of  a  flush+reload  or
     evict+reload attack.
     During the second phase, the processor speculatively
     executes instruction(s) that transfer confidential informa-
     tion  from  the  victim  context  into  a  microarchitectural
     side channel.
Cheers - BIll

@_date: 2018-01-09 15:44:35
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation considered harmful? 
When we developed the S370 version of KeyKOS, a capability OS, we developed on a running KeyKOS system. We used IBM's CMS system -- a part of VM/370 -- which ran using some code we wrote called a "CP Simulator". The CP Simulator was like a virtual machine monitor for a single virtual machine. It simulated the privileged CPU operations and provided I/O support, including simulated disks for CMS storage and access to real tape drives for backup and recovery.
When we talked with people about Unix compatibility, we always asked, "How compatible does it have to be." After a conversation, the answer was almost always, "Bug for bug compatible." If you are bug for bug compatible, it's hard to improve the security.
Cheers - Bill

@_date: 2018-01-10 14:41:46
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation considered harmful? 
For people interested in new approaches to hardware architecture, the Mill might be interesting  The Technology->docs/videos/slides page is a good place to learn about the technology.
Full disclosure: I worked briefly for Mill Computing on a sweat equity basis. I still have a claim on a small bit of equity. I stopped working because I didn't think I was contributing much of value. I still wish them well because they have interesting ideas.
Cheers - Bill

@_date: 2018-01-15 15:22:21
@_author: Bill Frantz 
@_subject: [Cryptography] Speculation considered harmful? 
Forward from Mill Computing:
A White Paper covering how the Mill CPU is affected by the recently disclosed Spectre and Meltdown exploits is available here (
Cheers - Bill

@_date: 2018-01-22 14:34:51
@_author: Bill Frantz 
@_subject: [Cryptography] RISC-V isn't the answer 
The immediate question that occurs to me is, "How do we handle shared memory? Both R/O and R/W?".
If we have a separate cache for each protection domain, then that domain is the only thing that can affect what is in that cache. Can we afford to fetch a shared word from a sister cache entry, or does that signal too much information? What about memory which is shared between mutually suspicious actors? (Probably R/O, but there may be uses for R/W.) There are a whole lot of questions.
One approach I like is a massive number of simple cores that don't speculate.
Cheers - Bill
Bill Frantz        | There are now so many exceptions to the
408-356-8506       | Fourth Amendment that it operates only by
 | accident.  -  William Hugh Murray

@_date: 2018-07-16 16:39:29
@_author: Bill Frantz 
@_subject: [Cryptography] storage encryption 
On 7/15/18 at 9:28 AM, cryptography at metzdowd.com (John Denker I know some of the people who developed Tahoe LSFS. It might serve the needs of your group. It's open source.
Cheers - Bill

@_date: 2018-07-17 15:51:31
@_author: Bill Frantz 
@_subject: [Cryptography] storage encryption 
On 7/15/18 at 9:28 AM, cryptography at metzdowd.com (John Denker I know some of the people who developed Tahoe LSFS. It might serve the needs of your group. It's open source.
Cheers - Bill

@_date: 2018-06-18 09:13:28
@_author: Bill Frantz 
@_subject: [Cryptography] How to make rowhammer less likely 
For most programs, death before confusion is the right answer. If the program is part of a security system, even more so.
Consider electronic building locks. When the power fails, do you open the doors or leave them locked? I think the solution generally used is to leave the building locked except to those who have a physical key. Similar solutions can be used for computer security systems.
Cheers - Bill

@_date: 2018-03-08 13:14:18
@_author: Bill Frantz 
@_subject: [Cryptography] How fast can a blockchain go ? like Zilliqa? 
Note the Ethereum has built in smart contracts. They had a blockchain split a while ago because of a mis-coded contract. I wouldn't bet the farm on Ethereum contracts yet. Small contracts are like betting at Reno. There is a significant chance of losing, but it may fall within your fun budget.
Cheers - Bill

@_date: 2018-03-09 15:29:03
@_author: Bill Frantz 
@_subject: [Cryptography] Mutually authenticated TLS 
This was our conclusion when we did the Simple Public Key Infrastructure (SPKI) work in the IETF. SPKI certs can't be revoked. They have to time out.
Cheers - Bill

@_date: 2018-03-20 14:36:48
@_author: Bill Frantz 
@_subject: [Cryptography] Blockchain currencies: The Death of 1000 Cuts 
[Note to moderators. I think the ship has sailed on the label used for these currencies and this list may be the only holdout. I have avoided the objectionable term in my writing, but am using it in the writing of others that I quote so my quotes will be accurate.]
The IRS decision to tax blockchain currency gains, along with the following stories from SANS NewsBites Vol. 20 Num. 022 seem to indicate a policy of killing them with the death of 1000 cuts.
Cheers - Bill
Ethereum Falls in Wake of SEC ICO Investigations
(March 18 & 19, 2018)
The value of Ethereum cryptocurrency fell below $500 USD over the weekend; the cryptocurrency was trading at more than $1,400 USD in January. The drop occurred after the US Securities and Exchange Commission (SEC) made clear that it intends to scrutinize initial coin offerings (ICOs). Other cryptocurrencies have taken a hit over the past several months as well.
Read more in:
New York State Municipality Power Companies Can Charge Cryptocurrency Miners Higher Rates
(March 16, 2018)
The New York State Public Service Commission (PSC) has ruled that power companies in that state may charge cryptocurrency mining operations higher rates than other customers. Cryptocurrency miners have been attracted to New York because of the state's abundance of relatively inexpensive hydroelectric power. A group of 36 New York state municipalities petitioned the PSC to raise rated for these customers because the demands they place on local power grids is causing rate increases for local residents. Unlike other industries that may consume larger amounts of power, cryptocurrency mining does not bring economic development to the municipalities in which it operates.
Read more in:

@_date: 2018-03-24 23:15:54
@_author: Bill Frantz 
@_subject: [Cryptography] Does RISC V solve Spectre ? 
People who are interested in these issues might be interested in looking at the Mill architecture. The TLDR is: The mill is a family of statically scheduled chips with significant differences in number and type of functional units. Using them to best effect falls on the compiler and a post compiler phase called the specializer which prepares the program for specific chip.
The web site has quite a bit about the technology, including videos of public presentations of the technology.
Full disclosure: I worked with the Mill designers and implementers for a short period of time. I didn't think I was contributing very much so I dropped out, but I still think their project is interesting and wish it success.
Cheers - Bill

@_date: 2018-05-07 09:11:33
@_author: Bill Frantz 
@_subject: [Cryptography] Security weakness in iCloud keychain 
I'm reading the comments about the evils of storing passwords with somewhat rye amusement. Here we have what is generally considered a really bad authentication mechanism where you don't need to have the computer store the secret. When we go to stronger authentication, it is much more likely that we will need to have the computer store the secret. Consider:
(1) Signed challange with public key crypto, user certs, or other similar trickery. The secret key probably needs to be stored in the computer because very very few people could remember it, or even copy it correctly from a piece of paper into the computer. If the secret is in an enclave/TCM, you have authenticated the computer and not the user -- which may be the correct behavior for some applications.
(2) Two factor authentication using a cell phone: These schemes usually use a password + a nonce sent to the cell phone. Good for low and medium security applications, but a nation state attacker could intercept the call.
Are there any schemes that we should consider?
Cheers - Bill

@_date: 2018-05-21 20:49:21
@_author: Bill Frantz 
@_subject: [Cryptography] Fwd: Our new startup, Agoric, is now visible 
I wouldn't normally forward this kine of message, but I have worked with the principals of the company over the years, and I think they may have a very useful addition to the area of smart contracts and blockchain currencies of interest to readers of this list.
Cheers - Bill
====== Forwarded Message ======
Received: 5/21/18 4:42 PM -0400
e-lang at googlegroups.com (Discussion of E and other capability languages)
Our new startup, Agoric, is now visible with three announcements:
Feel free to forward ;)

@_date: 2018-11-07 16:05:18
@_author: Bill Frantz 
@_subject: [Cryptography] Massive CIA communications compromise starting 
I sometimes get into arguments with people when I say that the fact that newspaper cryptograms never have a cypher text letter map to itself as a plain text letter makes the cryptogram easier to solve. I now mention that an important part of the breaking of Enigma was when key searching in Bombe was to check if a letter mapped to itself. Since the Enigma hardware could not map a letter to itself, any test decode where this happened was incorrect.
Cheers - Bill

@_date: 2019-02-20 12:46:58
@_author: Bill Frantz 
@_subject: [Cryptography] A seemingly simple question ... 
On 2/20/19 at 6:55 AM, thierry.moreau at connotech.com (Thierry I'm not really an expert, but TLS has the Pre-Shared Key (PSK) mode where the initial keys are shared offline. These two organizations could share keys via messenger, avoiding concerns about CAs and other third parties.
Cheers - Bill

@_date: 2019-01-07 10:38:08
@_author: Bill Frantz 
@_subject: [Cryptography] Came up with a weird use case, got questions 
On 1/7/19 at 10:38 PM, phill at hallambaker.com (Phillip There are a lot of causes of risk of data loss. Bit rot in storage media is a real worry. The best solution is to copy the data regularly. For the encrypted data, the only downside is the storage cost. For the keys it introduces a new complication in maintaining secrecy.
There is also risk of transistor failure in the HSM due to dopant migration over time. We don't have experience with transistor equipment over long periods of time. Our experience with tube equipment, which is about 100 years, is that electrolytic capacitors die unless treated with a low voltage for a while to rebuilt their insulation layer. Sometimes they die anyway. I can't think of a way of keeping a HSM alive over long periods of time, certainly not one that is anywhere near as easy as copying data.
Cheers - Bill

@_date: 2019-03-01 16:09:52
@_author: Bill Frantz 
@_subject: [Cryptography] In the event of my death, master password 
You don't want to put your will in a safe deposit box as it will take a court order to open the box and find out who is named as executer. Putting your passwords, along with other valuables, in the box is much more reasonable.
If the password needs to be used more quickly than the legal process will give access to your safe deposit box, consider giving it to your executor before you die.
All this applies to your parent's passwords.
Cheers - Bill

@_date: 2019-03-22 22:58:02
@_author: Bill Frantz 
@_subject: [Cryptography] Best/simplest document encryption 
One function that might be quite low risk is to use a long hash, perhaps SHA512 to generate a hash of the key. Then select fields from the hash to feed into a Bloom filter.
If we take the example of 3 keys for 180 days and choose a Bloom filter with 10 entries for each key and a 50% fill, then we need 10,800 bits for the filter. So we use a filter of 2**14 (16384) bits, take 140 bits from the hash and divide them up into 10 indexes of 14 bits each for the filter.
This procedure leaves 372 bits unused from the hash, which means that a lot of incorrect "keys" will pass the bloom filter, but not be able to decrypt the data. Someone trying exhaustive search of the key space won't get a lot of help by using the filter.
Cheers - Bill

@_date: 2019-10-02 16:06:27
@_author: Bill Frantz 
@_subject: [Cryptography] Encryption and anonymity as top tools for images 
Or as an Australian said to me, "We're lucky. We got the criminals, you got the religious fanatics."
Cheers - Bill

@_date: 2019-09-12 18:42:08
@_author: Bill Frantz 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
It's not just NSA. Remember, old technology fabs are now the stuff of home experimenters. I know one person who has a chip fab in her kitchen. (I don't know what her SO thinks of this, but they seem to get along just fine with each other.)
I have also read about another experimenter with a similar setup.
Her problem is that her kitchen isn't a clean room, but I bet she gets enough usable chips to let her solve this kind of problem for herself.
Indeed, Jerry is completely correct. When the data gets to a modern computer, it's toast.
Cheers - Bill

@_date: 2019-09-13 15:48:31
@_author: Bill Frantz 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
Right on Peter. She is an interesting and resourceful lady.
For another one, see: There was an article in QST a while ago about a teenager with a home fab. I don't have the time to search it out at the moment and the ARRL's search facility for QST either isn't obvious or it is really bad.
Cheers - Bill

@_date: 2019-09-13 17:24:31
@_author: Bill Frantz 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
There is one good thing about secure random number generators (or whatever is the PC term for them these days). If you use a good combining function and your attacker can successfully predice 7 of your 8 sources, but not the 8th, then you have good output. There aren't many things in this world where you combine 7 bad things with 1 good thing and get a good thing.
Good combining functions are an interesting question, although I still think that cryptographically secure hashes are a good choice.
Cheers - Bill

@_date: 2019-09-17 17:57:07
@_author: Bill Frantz 
@_subject: [Cryptography] "Exclusive: Russia carried out a 'stunning' 
I can't find the book just now, but it is a description of code breaking in Australia during WWII. Most of the time, they could break the conversations between Japanese airplanes. But even when the codes were changed, and had not yet been broken, direction finding and signal strength told them almost everything they needed to know about coming attacks.
Cheers - Bill

@_date: 2019-09-23 16:28:34
@_author: Bill Frantz 
@_subject: [Cryptography] "Exclusive: Russia carried out a 'stunning' 
[Reply via list since the last time I tried a personal email to Dave, a year or so ago, his robot seriously didn't like my ISP.]
I picked up my copy in the Sydney airport while leaving after the International Congress of Speleology. It's a good read.
Cheers - Bill

@_date: 2020-04-07 22:50:28
@_author: Bill Frantz 
@_subject: [Cryptography] "Zoom's end-to-end encryption isn't 
I think this might allow an attacker to find out what parts of the image and what parts are not. My understanding of compressed video is that the entire image is sent fairly frequently to allow newcomers to start displaying the image, and to recover from dropped packets. It sounds like it would be straight forward to pull out the entire image packets based on timing and/or size and then see where the differences are in the cypher text.
It may not be easy, but it seems possible enough to cause me to worry.
Cheers - Bill

@_date: 2020-12-21 20:08:56
@_author: Bill Frantz 
@_subject: [Cryptography] Cryptographic archive format 
The decoder should not have write access to any files/directories/etc. that the user doesn't also have write access to. It is nearly impossible to make software that does its own management of these kinds of things without introducing security holes. A worked example is Postfix.
If necessary, create a new user that has the right kind of limited privileges and do the decode in that user. This is basically what the Polaris system, built at HP labs did with Windows.
Cheers - Bill

@_date: 2020-02-11 17:07:20
@_author: Bill Frantz 
@_subject: [Cryptography] SSL Certificates are expiring... 
When you make good money selling certificates, you love the hammer you have.
Cheers - Bill

@_date: 2020-02-14 16:43:26
@_author: Bill Frantz 
@_subject: [Cryptography] 'The intelligence coup of the century' 
I assume by TOS and DOS, you are referring to early IBM 360 operating systems, Tape Operating System and Disk Operating System. These systems were very similar to the previous generation (7090, 1401) input/output control systems. They were also largely compatible, except TOS did not support disks.
Cobol code for these systems was fairly easy to port to OS/360 which was easy to port to OS/370.
The big cause of the Y2K problem was the designers' desire to minimize file size and disk usage. Many systems were designed with a 2 digit field for the year, and my wife worked on one system that used a single digit. It ran into trouble in the next decade.
The Y2K problem primarily affected business systems. I think DEC was more concentrated in the time sharing, scientific, and industrial control areas, where dates were a smaller part of the systems.
I think you would need to process the signals to read the individual bits being processed. Also, in those days, people were much less concerned about passive listening to the processor's internal operations, and security in general.
One person at Reed Collage wrote a program, called MuTran, for the IBM 1620, which translated its input into music which it played using the AM radio signals generated by the computer.
One of the standard IBM demos included having the 1403 printer on the 1401 play, "Anchors Away" by controlling the timing of the hammers hitting the type chain using the specific characters printed.
When I used the technique on the 1620, I had to put the transistor AM radio close to the console. I'm sure using a directional antenna, one could receive the computer from outside the building.
Cheers - Bill

@_date: 2020-02-14 16:43:26
@_author: Bill Frantz 
@_subject: [Cryptography] SSL Certificates are expiring... 
On 2/13/20 at 6:40 PM, phill at hallambaker.com (Phillip Phillip and I will perhaps have to agree to disagree. I have always objected to having to rely on a "Trusted Third Party" (TTP) to validate any web connection. When I deal with individuals and businesses outside of the computer communications world, I use the model of recognition, not attestation. I may buy something inexpensive to start developing trust in my counter-party. I'll use the physical location or face as an anchor for that developing trust, not a TTP.
For the web, I would like to have my trust anchor for a site be through a key it controls, not a CA. When I go to a site using a CA as a trust anchor, I will keep my financial and secret data exposure low until I have some transaction experience. I want to know I'm talking to the same site I was talking to when I developed the trust I have, not a intruder site attested to by an untrustworthy TTP. (Do browsers still have over 80 trust anchors?)
I think we have the current system because that was the only system people could build a business model around, and that the need to support that business model was reflected in contributions to the standards bodies.
Phillip may have meant the following, but here's my take for clarity.
It seems to me that an IoT device doesn't need a traditional PKI. It needs to validate the devices it talks to -- the light switch and the bulbs need to validate each other, which is better done through direct introduction. The phone app which allows remote control should be verifying the device using the public key pair built into it.
When the IoT device talks to the mother ship to upload your behavior profile, it would be better to include the necessary public keys in the device when it is purchased.
Cheers - Bill

@_date: 2020-02-18 21:47:00
@_author: Bill Frantz 
@_subject: [Cryptography] SSL Certificates are expiring... 
On 2/16/20 at 12:15 PM, phill at hallambaker.com (Phillip Defecting is, as far as I know, an unsolved problem. Back in the early 1960s, when the US offered strong protection to the official importers of foreign goods, there was a company in Hong Kong which mail ordered Japanese cameras to the US for about 1/2 price. When I ordered a Nikon F from them, I got the camera in a package shipment, and the nameplate arrived in a first class letter. (The US importer owned the name "Nikon".)
About a year later, they continued to collect orders, but stopped shipping. Then they completely disappeared, having cached in their reputation capital.
What I meant by business models is the certificate authorities. They make good money based on their monopoly of being listed in major browsers as trust roots. As far as I can tell, the WebPKI model is the only model that provides this kind of niche to build a business.
BTW, I'm not sure that you can get a protocol adopted with out a business model which will allow companies to make money.
Cheers - Bill

@_date: 2020-01-22 17:47:43
@_author: Bill Frantz 
@_subject: [Cryptography] Proper Entropy Source 
In this area, I'm with John von Neumann, "Anyone who attempts to generate random numbers by deterministic means is, of course, living in a state of sin."
Fortunately, for most cryptography, we don't need randomness, we need unguessability.
But, as John points out, that is squish.
On 1/22/20 at 6:53 AM, cryptography at metzdowd.com (John Denker And harder to predict is something we want. If I can combine 100 sources that each give me 1 bit of unguessability, I have a good start on having a cryptographically useful unguessable number. High precision timing of packet arrival may be useful here.
Ideally, you have a hardware generator with a theoretical reason to think it is random. But if you don't, combining multiple sources, with very small guesses as to the number of bits of unguessability may be the best you can do.
If you don't trust the organization that made your hardware source, combine it with some squish to make the opponent's job harder.
If I have a hardware generator that generates bits that are 75% ones, but is "provably random", I just need to draw out more bits and combine them. But, I think this approach is what John is describing.
Now, the combining function is a useful place to use program proofs.
Cheers - Bill

@_date: 2020-07-01 20:40:02
@_author: Bill Frantz 
@_subject: [Cryptography] Statement from Attorney General William P., 
On 7/1/20 at 11:12 AM, phill at hallambaker.com (Phillip There isn't a prohibition on using codes in ham radio. The prohibition is on trying to conceal the meaning of a transmission. Hams use many codes, the "Q" codes are very common. Their meaning is listed in may places, so they are like the Uniform Commercial Code before/during WW2.
Spread spectrum is permitted, if the spreading codes are published. Different digital encodings are permitted, if encoding is published. etc.
Cheers - Bill

@_date: 2020-07-02 21:41:08
@_author: Bill Frantz 
@_subject: [Cryptography] Statement from Attorney General William P., 
If you are using ham radio, using TLS (nee SSL) to hide the meaning of a message is illegal. TCP/IP is fine. HTTP should be find. HTTPS isn't.
One interesting bit here is that many hams are active in emergency communication groups. These groups are working in the direction of offering email and net access over ham radio. I think the just say lalalala very loudly when the issue of encryption comes up.
Cheers - Bill AE6JV

@_date: 2020-07-06 19:32:23
@_author: Bill Frantz 
@_subject: [Cryptography] Statement from Attorney General William P., 
OK, I assume this comment is tongue in cheek. But I'll bite anyway.
If the higher level protocol is not encrypted this would be legal. But, the reason for the other digital modes (Morse code is considered to be the first digital mode), is that they are much more amenable to machine decoding.
Cheers - Bill

@_date: 2020-05-04 21:35:54
@_author: Bill Frantz 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
Since Zoom usually uses a camera and microphone, for any meeting where the moderator knows the invitees, seeing their picture and talking with them in the lobby makes a fairly useful and effective identification system.
I think building a way to fool this system either requires a really good mimic, or a beyond state of the art AI system.
Cheers - Bill

@_date: 2020-05-05 23:39:23
@_author: Bill Frantz 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
On 5/4/20 at 10:11 PM, phill at hallambaker.com (Phillip This approach sounds to me like a DTLS application. There might be an issue if the reflector can't tell when one datagram stops and the next one starts.
Cheers - Bill

@_date: 2020-11-21 21:15:21
@_author: Bill Frantz 
@_subject: [Cryptography] Possible reason why password usage rules are 
Some people may be interested in Alan Karp's Site Password tool. .
The basic idea is the user has a single, hard to guess password, and an easy to remember name for the site being accessed. The tool computes a password to be used for that site based on the password and the site name.
YMMV - Bill

@_date: 2020-10-19 22:52:39
@_author: Bill Frantz 
@_subject: [Cryptography] Fun with printers, 
I have one of those cheap home printers, an HP MFP M277c6 laser printer, which I got at Costco. (Toner works better in a lightly used printer than ink, which clogs the jets.)
It does store at least one page internally, given that the scanner finishes before the printer starts.
Cheers - Bill
Bill Frantz        | Art is how we decorate space,
408-348-7900       | music is how we decorate time.
 |          -Jean-Michel Basquiat

@_date: 2020-09-23 16:50:45
@_author: Bill Frantz 
@_subject: [Cryptography] A naming and key distribution infrastructure for 
It sounds like we're getting into Zooko's triangle territory. The general solution to this dillema is Pet Names. A pet name only has local significance. My address book is full of pet names, which resolve to real email addresses, postal addresses, and telephone numbers.
Cheers - Bill

@_date: 2020-09-27 11:29:32
@_author: Bill Frantz 
@_subject: [Cryptography] Levels of Spam 
On 9/26/20 at 12:41 PM, in Re: [Cryptography] A naming and key distribution infrastructure for the Mesh, phill at hallambaker.com I agree with you about telephones. My current situation isn't too bad because my phone's area code is 3000 miles from where I live. Real unknown callers that I actually want to talk with are most likely to call from near where I live. I will, of course, answer calls from numbers in my address book. Things will probably get worse, but I'll enjoy it while it lasts.
BTW, since Covid-19 contact tracers don't leave phone messages, they are very likely to miss contacting the people they want to talk with, since so many people send unknown numbers directly to voice mail.
The spam situation on my email is much better. Between the spam filter on my email agent, and the one at my ISP, very few spam messages get through -- factors of 100-1000 less than comes in the postal mail.
I think both POTS and email may last a long time with simple hacks to avoid bothering their users with the spam that exists. One might even dream that the levels of spam will go down as it becomes less effective. But I do worry about what might replace the current forms of spam.
Cheers - Bill
