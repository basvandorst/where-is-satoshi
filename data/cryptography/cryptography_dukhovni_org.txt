
@_date: 2013-12-08 06:43:22
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Fun with hardware RNGS: the Infinite Noise 
That would be true of the mathematically ideal circuit.  Yes, the
baker's transformation is ergodic.  However, the physical circuit
has finite sensitivity.  Noise below a certain threshold or above
certain frequencies may not elicit any response.
Given sufficiently little noise, if the circuit ever chances on an
output sufficiently close to zero, it may output zeros forever
after.  In any case, with sufficiently little noise a discrete
system with finite sensitivity becomes deterministic.  If the system
can be in N states, then the period of doubling mod N is at best
Counting the number states of a thermal system and estimating noise
brings us right back to physical entropy.  What's special about
this particular circuit is the ergodicity and rapid diffusion of
the ideal classical dynamics.  Claims that the classical model
matches the real behaviour to infinite precision are rather suspect.
One needs to assume sufficiently linear response to the ambient
thermal noise in the presence of whatever signal the adversary can
If super-imposed signals can reduce sensitivity to the noise, or
the noise is too weak in the first place (sufficient cooling?),
the ideal model may fail to be a good match for reality.

@_date: 2013-11-01 04:08:27
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Speaking of hardware-level malware... 
This seems to advanced to be true:
    If it is real, we certainly did not have to wait long for my concerns
(shared by many it seems, and definitely not originally mine, rather
these worry me more than imperfect crypto) to become reality.

@_date: 2013-11-04 20:55:59
@_author: Viktor Dukhovni 
@_subject: [Cryptography] DNSSEC = completely unnecessary? 
Nothing: provided:
- You're trying to secure HTTP over TLS.
- You assume the destination website has a certificate from a trusted public CA.
- You assume that the HTTPS client does not trust any rogue CAs.
- You assume that the CA issued the certificate based on criteria stronger
  than verifying that the requestor seems to control the DNS for the domain.
- You assume that CA certificates assert a stronger claim than domain
  ownership, i.e. some sort of brand validation, as in EV certificates.
- You're only trying to secure the small minority of HTTP sites with EV
  certificates for brand-name domains.
- If your protocol is not HTTP, there is no DNS-based indirection from
  client destination to server domain as with MX or SRV records.
- ...
When one defines all problems to be nails, the solution will always
be a hammer, and people making axes will appear to be wasting their
Perhaps the bunch of people "wasting" time on DNSSEC are interested
in a broader class of problems.

@_date: 2013-11-25 15:05:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Email is unsecurable 
The "failure" is easy to understand.  E-mail is not just ephemeral
correspondence within your existing network of direct acquaintances.
It is not too difficult to design instant-messaging systems that
support end-to-end encryption.  Perhaps Skype was once approximately
such a system.  It is less clear that such systems will be competitive
against the existing IM systems from the free consumer providers
whose service is monitored users sold to advertisers.
E-mail is basically business correspondence.
    - E-mail is stored.
    - E-mail is sent to many people outside your personal social network.
    - Business recipients of email are often subject to corporate and/or
      regulatory policy constraints that are in conflict with end-to-end
      encryption.
The above list of features can be greatly expanded, and the
consequences elaborated, but I doubt may on this list truly need
to be re-educated about email.
So I will confidently predict that end-to-end secure email will
remain a niche service used by a tiny minority.
For the rest of the world, what we may be able to secure is the
SMTP hop-by-hop transport with TLS and DANE (provided we can get
DNSSEC off the ground).
Even businesses that one might expect to implement at least encryption
to the "gateway", are in many cases choosing to out-source their
gateway to 3rd-party providers (anti-spam and anti-virus offerings
only work with un-encrypted email, and in many cases the provider
also operates the entire mail store).
Security of stored email will at best be a service offered at the
recipient's mailstore, which encrypts all incoming mail to the
recipient's public key just before it is stored.
I say "at best", because most users will not choose this option.
    - This precludes server-side search.
    - This precludes advertising supported web access, so users
      might have to *pay* for a service which is more secure, but
      offers less convenience.
    - A complacent public does not perceive pervasive surveillance
      as a personal threat.
So my prediction is that end-to-end secure communication will be
possible by various means, but for most people email will not be
where they encounter end-to-end security.

@_date: 2013-11-26 06:49:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Lawyer: "Are you familiar with public key 
Too bad they lost for now.
It seems to me (IANAL) that if TQP has a valid claim with SSL +
RC4 that is essentially based on RC4 being a stream cipher (the
SSL part seems irrelevant) then they'd have a valid claim on
Vigenere.  That prior art is 460 years old.
There I think no need to show that you need PKI to enable E-commerce,
and RC4 alone is not sufficient.  Even if symmetric crypto were
sufficient, there's nothing in TQP's claims that applies to RC4
alone that hasn't been known for nearly 500 years.
It is rather sad that TQP is getting away with this.

@_date: 2013-10-02 14:46:05
@_author: Viktor Dukhovni 
@_subject: [Cryptography] encoding formats should not be committee'ized 
MIME: RFC 2045 - 2048, ...
A rather complex nested structure, and frankly rather more ambiguous
in practice than ASN.1.  For example, what is the meaning of:
This type of ambiguity is not possible with ASN.1.  There are many
more edge cases in which the same MIME input is parsed very
differently by different implementations.  There are many more
interesting cases, but I shall not bore you with them here.
Yes, and not just boundaries.

@_date: 2013-10-20 03:36:58
@_author: Viktor Dukhovni 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Some years back a few brave users of Postfix attempted to use the
GnuTLS OpenSSL API compatibility layer to run Postfix over GnuTLS.
It was found that the GnuTLS library would call exit() if it did
not find an entropy on startup instead of returning an error to
the application.  This approach was deemed safer by GnuTLS.  When
this was discovered, Postfix dropped support for the GnuTLS OpenSSL
emulation.  From the TLS_README file:
    NOTE: Do not use Gnu TLS.  It will spontaneously terminate a
    Postfix daemon process with exit status code 2, instead of
    allowing Postfix to 1) report the error to the maillog file,
    and to 2) provide plaintext service where this is appropriate.
I don't know whether this has changed since, but I concur that the
security/availability tradeoff is not always clear-cut.
As for RNG use, Postfix does not use the RNG at boot time.  The
tlsmgr(8) process is started when TLS is first used, in addition
to periodic seeding from /dev/urandom, it keeps its own persistent
seed file across restarts.  Each SMTP server or client process gets
initial and periodic seed material from tlsmgr(8) and stirs a few
bits of randomness between connections.  If /dev/urandom were
configurable to block at boot time, that would likely be tolerable
in most cases, as the first use of TLS with SMTP will likely happen
late enough for enough entropy to have been accumulated for tlsmgr(8)
to get (IIRC) 32 bytes of seed material.

@_date: 2013-10-22 04:50:38
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Hasty PRISM proofing considered harmful 
There have been many recent efforts to harden the cryptographic
security of various systems.  I would like to urge anyone considering
taking steps in that direction to exercise due caution.
Multiple recent attempts at improvement backfire in various ways:
    - RedHat has been under pressure for some time to enable EC support
      in their OpenSSL RPM package.
    - GnuTLS sets aggressive client-side EDH prime-size lower bound.     - Some email administrators disable RC4 (enable only the OpenSSL "HIGH"
      ciphers) in opportunistic TLS.  Many extant Microsoft Exchange servers
      support only RC4-SHA1, RC4-MD5 and 3DES (whose implementation is
      breaks post handshake in data transfer).
        # Result: TLS handshakes fail, and mail is sent in the clear.
    - There's lots of press about CRIME, BEAST, ... and some SMTP
      administrators configure their systems to prefer RC4 and
      avoid CBC ciphersuites.
There are I expect similar examples of good intentions, but poor
outcomes outside the world of SMTP.  Raising the bar on Internet
security will take considerable time and effort.  Updated standards
will have to be developed, toolkits extended to support them and
applications updated.  Rolling improved security out to end-users
will likely take on the order of a decade.
In the mean-time, users should make an effort to configure their
systems to employ current best-practice security, trying to go
beyond that into unchartered territory may well be counter-productive.
Endpoint security and misuse of data at rest are still IMHO the
bigger issues.  I am much more concerned about the proliferation
of miniature programmable computers inside our computers (CPUs and
programmable firmware in disk controllers, battery controllers,
BMC controllers, with opaque binary firmware update blobs, and
complex supply chains) that about secp256r1 vs secp521r1.
We thought embedded devices were for physical infrastructure
engineers to worry about, but now they are proliferating inside
our general purpose computers.  The next Stuxnet will run on one
of the invisible computers inside your computer.
With concerted effort we can improve the crypto protocols, but will
it matter if the architecture on top of which the crypto runs has
an ever growing attack surface.

@_date: 2013-09-08 04:31:28
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Speaking of EDH (GnuTLS interoperability) 
Some of you may have seen my posts to postfix-users and openssl-users,
if so, apologies for the duplication.
     at openssl.org/index.html
The short version is that while everyone is busily implementing
EDH, they may run into some interoperability issues.  GnuTLS clients
by default insist on a minimum EDH prime size that is not generally
interoperable (2432 bits).  Since the TLS protocol only negotiates
the use of EDH, but not the prime size (the EDH parameters are
unilaterally announced by the server), this setting, while
cryptographically sound, is rather poor engineering.
The context in which this was discovered is also "amusing".  Exim
uses GnuTLS and has a work-around to drop the DH prime floor to
1024-bits, which is interoperable in practice.  Debian however
wanted to "improve" Exim to make it more secure, so the floor was
raised to 2048-bits in a Debian patch.  As a result STARTTLS from
Debian's Exim (before sanity was restored in Exim 4.80-3 in Debian
wheezy, AFAIK it is still broken in Debian squeeze) fails with Postfix,
Sendmail, and other SMTP servers.
In all probability this "stronger" version of Exim then needlessly
sends mail without TLS, since with SMTP TLS is typically opportunistic,
and likely after TLS fails delivery is retried in the clear!

@_date: 2013-09-08 22:55:09
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
Nice in theory of course, but in practice applications don't write
their own PRNGS.  They use whatever the SSL library provides, OpenSSL,
GnuTLS, ...  If we assume weak PRNGS in the toolkit (or crypto chip,
...) then EDH could be weaker than RSA key exchange (provided the
server's key is strong enough).
The other concern is that in practice many EDH servers offer 1024-bit
primes, even after upgrading the certificate strength to 2048-bits.
Knee-jerk reactions to very murky information may be counter-productive.
Until there are more specific details,  it is far from clear which is     - RSA key exchange with a 2048-bit modulus.
    - EDH with (typically) 1024-bit per-site strong prime modulus
    - EDH with RFC-5114 2048-bit modulus and 256-bit "q" subgroup.
    - EECDH using secp256r1
Until there is credible information one way or the other, it may
be best to focus on things we already know make sense:
    - keep up with end-point software security patches
    - avoid already known weak crypto (RC4?)
    - Make sure VM provisioning includes initial PRNG seeding.
    - Save entropy across reboots.
    - ...
Yes PFS addresses after the fact server private key compromise,
but there is some risk that we don't know which if any of the PFS
mechanisms to trust, and implementations are not always well
engineered (see my post about GnuTLS and interoperability).

@_date: 2013-09-10 16:15:38
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Time for djb's Edwards curves in TLS? 
Is there a TLS WG draft adding djb's Curve1174 to the list of named
curves supported by TLS?  If there's credible doubt about the safety
of the NIST curves, it seems that Curve1174 (in Edwards form) would
make a good choice for EECDH, perhaps coupled with a similar curve
with ~512 bits.
Slides with rationale:
    Detailed paper motivating Curve1174:
    The current situation with EECDH over the NIST prime curves not
shown compromised, but no longer trusted is rather sub-optimal.

@_date: 2013-09-11 18:27:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
This is not the case in TLS.  The EDH or EECDH key exchange is
performed in the clear.  The server EDH parameters are signed with
the server's private key.
    In TLS with EDH (aka PFS) breaking the public key algorithm of the
server certificate enables active attackers to impersonate the
server (including MITM attacks).  Breaking the Diffie-Hellman or
EC Diffie-Hellman algorithm used allows a passive attacker to
recover the session keys (break must be repeated for each target
session), this holds even if the certificate public-key algorithm
remains secure.

@_date: 2013-09-17 21:31:21
@_author: Viktor Dukhovni 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
There is also I suspect a lot of software with compiled-in EDH
primes (RFC 5114 or other).  Without breaking EDH generally, perhaps
they have better precomputation attacks that were effective against
the more popular groups.
I would certainly recommend that each server generate its own EDH
parameters, and change them from time to time.  Sadly when choosing
between a 1024-bit or a 2048-bit EDH prime you get one of
interoperability or best-practice security but not both.
And indeed the FUD around the NIST EC curves is rather unfortunate.
Is secp256r1 better or worse than 1024-bit EDH?

@_date: 2013-09-18 14:30:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
This is only realistic with DANE TLSA (certificate usage 2 or 3),
and thus will start to be realistic for SMTP next year (provided
DNSSEC gets off the ground) with the release of Postfix 2.11, and
with luck also a DANE-capable Exim release.
For HTTPS, there is little indication yet that any of the major
browsers are likely to implement DANE support in the near future.

@_date: 2013-09-18 20:47:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
X.509 name constraints (critical extensions in general) typically
don't work.

@_date: 2013-09-18 21:50:31
@_author: Viktor Dukhovni 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
And public CAs don't generally sell intermediate CAs with name
constraints.  Rather undercuts their business model.

@_date: 2013-09-22 20:13:22
@_author: Viktor Dukhovni 
@_subject: [Cryptography] RSA equivalent key length/strength 
GnuTLS is reasonably sound engineering in electing 2048-bit groups
by default on the TLS server.  This inter-operates with the majority
of clients, all the client has to do is to NOT artificially limit
its implementation to 1024 bit EDH.
GnuTLS fails basic engineering principles when it sets a lower
bound of 2048-bit EDH in its TLS client code.  TLS clients do not
negotiate the DH parameters, only the use of EDH, and most server
implementations deployed today will offer 1024-bit EDH groups even
when the symmetric cipher key length is substantially stronger.
Having GnuTLS clients fail to connect to most servers, (and e.g.
with opportunistic TLS SMTP failing over to plain-text as a result)
is not helping anyone!
To migrate the world to stronger EDH, the GnuTLS authors should
work with the other toolkit implementors in parallel with and
through the IETF to get all servers to move to stronger groups.
Once that's done, and the updated implementations are widely deployed
raise the client minimum EDH group sizes.
Unilaterally raising the client lower-bound is just, to put it
bluntly, pissing into the wind.

@_date: 2013-09-28 17:34:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] RSA equivalent key length/strength 
Or smaller (e.g. GnuTLS minimum client-side EDH strength).  And
given that with EDH there is as yet no TLS extension that allows
the client to advertise the range of supported EDH key lengths (
with EECDH the client can communicate supported curves), there is
no timely incremental path to stronger EDH parameters.
In addition to the protocol obstacles we also have API obstacles,
since the protocol values need to be communicated to applications
that provide appropriate parameters for the selected strength
(EDH or EECDH).
In OpenSSL 1.0.2 there is apparently a new interface for server-side
EECDH curve selection that takes client capabilities into account.
For EDH there is need for an appropriate new extension, and new
interfaces to pass the parameters to the server application.
Deploying more capable software will take a O(10 years).  We could
perhaps get there a bit faster, if the toolkits selected from a
fixed set of suitable parameters and did not require application
changes, but this has the drawback of juicier targets for cryptanalysis.
So multiple things need to be done:
    - For now enable 1024-bit EDH with different parameters at each server,
      changed from time to time.  Avoid non-interoperable parameter choices,
      that is counter-productive.
    - Publish a new TLS extension that allows clients to publish supported
      EDH parameter sizes.  Extend TLS toolkit APIs to expose this range
      to the server application.  Upgrade toolkit client software to advertise
      the supported EDH parameter range.
    - Enable EECDH with secp256r1 (and friends) unless it is
      reasonably believed to be cooked for efficient DLP by its creators.
    - Standardize new EECDH curves (e.g. DJB's Curve1174).

@_date: 2013-09-30 04:34:50
@_author: Viktor Dukhovni 
@_subject: [Cryptography] NIST about to weaken SHA3? 
I call FUD.  If progress is to be made, fight the right fights.
The SHA-3 specification was not "weakened", the blog confuses the
effective security of the algorithtm with the *capacity* of the
sponge construction.
The actual NIST Proposal strengthens SHA-3 relative to the authors'
most performant proposal (
section 6.1) by rounding up the capacity of the sponge construction
to 256 bits for both SHA3-224 and SHA3-256, and rounding up to 512
bits for both SHA3-384 and SHA3-512 (matching the proposal in
section 6.2).
The result is that the 256-capacity variant gives 128-bit security
against both collision and first preimage attacks, while the 512-bit
capacity variant gives 256-bit security.  This removes the asymmetry
in the security properties of the hash.  Yes, this is a performance
trade-off, but it seems entirely reasonable.  Do you really need
256 bits of preimage resistance with 128-bit ciphersuites, or 512
bits of preimage resistance with 256-bit ciphersuites?
SHA2-256's  O(256) bits of preimage resistance was not a design
requirement, rather it needed 128-bits of collision resistance,
the stronger preimage resistance is an artifact of the construction.
For a similar sentiment see:

@_date: 2013-09-30 14:44:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] NIST about to weaken SHA3? 
Have you read the SAKURA paper?
    In section 6.1 it describes 4 capacities for the SHA-2 drop-in
replacements, and in 6.2 these are simplified to two (and strengthened
for the truncated digests) i.e. the proposal chosen by NIST.
Should one also accuse ESTREAM of maliciously weakening SALSA?  Or
might one admit the possibility that winning designs in contests
are at times quite conservative and that one can reasonably
standardize less conservative parameters that are more competitive
in software?
If SHA-3 is going to be used, it needs to offer some advantages
over SHA-2.  Good performance and built-in support for tree hashing
(ZFS, ...) are acceptable reasons to make the trade-off explained
on slides 34, 35 and 36 of:

@_date: 2013-09-30 23:09:27
@_author: Viktor Dukhovni 
@_subject: [Cryptography] NIST about to weaken SHA3? 
Weakening SHA3 to gain cryptanalytic advantage does not make much
sense.  SHA3 collisions or preimages even at 80-bit cost don't
provide anything interesting to a cryptanalyst, and MITM attackers
will attack much softer targets.
We know exactly why it was "weakened".  The the proposed SHA3-256
digest gives 128 bits of security for both collisions and preimages.
Likewise the proposed SHA3-512 digest gives 256 bits of security
for both collisions and preimages.
The lower capacity is not weaker in obscure ways.  If Keccak delivers
substantially less than c/2 security, then it should not have been
chosen at all.
If you believe that 128-bit preimage and collision resistance is
inadequate in combination with AES128, or 256-bit preimage and
collision resistance is inadequate in combination with AES256,
please explain.
The contest led to an excellent new hash function design.
Just because they're after you, doesn't mean they're controlling
your brain with radio waves.  Don't let FUD cloud your judgement.

@_date: 2014-04-02 19:31:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve 	numbers 
No, he seems to want N.U.M.S *and* lacking problematic structure.

@_date: 2014-04-06 17:55:25
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Verifying X.509 Verification - how about an 
This link was posted to the TLS WG mailing list on Friday 2014-04-04.

@_date: 2014-04-07 17:17:57
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Verifying X.509 Verification - how about an 
One needs all of the above, machine-generated randomized testing
that probes unexpected test cases, expert selected corner-case
testing, and reasonably comprehensive coverage testing of the
basic requirements.
However, to Peter's point, the comprehensive test-suite that merely
follows the spec, good, bad and (mostly) ugly, needs to be taken
with a grain of salt, failing some of those tests is sometimes a
For DANE-EE(3) (RFC 6698 certificate usage 3) I am working to
simplify X.509 verification to a simple digest comparison of the
leaf certificate with the digest from the DNSSEC TLSA record.
    * No name checks (subsumed by TLSA base domain)
    * No expiration checks (subsumed by DNSSEC RRset validity)
This makes DANE-EE(3) dramatically more usable, and the horror of
X.509 trust verification largely irrelevant.  I must however admit
that DANE for SMTP will also support DANE-TA(2), a bastard child
of DANE and PKIX, where trust verification and name checks are
still in scope.

@_date: 2014-04-10 17:16:16
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Preliminary review of the 
It is interesting to note that the subject and issuer DNs in the
original X.509 PKI where supposed to be delivered via an-online
global X.500 directory.  The fact that no such directory ever got
deployed is a signficant part of the many issues with X.509.
DANE is an attempt to put the distributed directory back into the
Internet PKI.  Time will tell whether it will succeed or fizzle.

@_date: 2014-04-11 00:27:47
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Postfix with a couple of minor exceptions that need to interoperate
with external libraries, calls malloc() and free() only via mymalloc()
and myfree() that always overwrite memory on free() and initialize
on malloc(), both with a non-zero fill byte.  mymalloc() indeed
returns a pointer to a non-zero offset into a larger block obtained
from the real malloc().  The larger block has a header and a trailer,
both of which are unconditionally verified on free().
The Postfix way to process packet content would not be to use a
local pointer that is an offset into the packet which requires each
function that is handling packet content to implement its own bounds
checks.  Rather the packet would be a data-structure with an explicit
offset, and there would be read/write routines for consuming and
producing packet data and the bounds checks would be in one place!
In other words, C++ discipline in a C program.  There are multiple
structural changes (or a complete rewrite) that would dramatically
improve the robustness of OpenSSL, but who's going to pay for this?
MIT Kerberos are beginning to dig themselves out of the Kerberos
code quality hole they were in after creating the MIT Kerberos
Consortium, and getting enough funding to run the project.
If we're to have a free (as in beer) open source SSL implementation,
perhaps it needs a more robust funding structure and stake-holders
who are focused on improving code quality over feature of the day.
There are many good programming practices, I will not attempt to
list those here, I want to add a couple of practices that are less
about project management that help maintain code quality:
 * High quality software projects are not democracies.  The code
   needs strong guardians who defend it against hastily conceived
   features.  This goes beyond "code review".  The guardian is
   responsible for the long-term quality of all or a subset of the
   code base and must be able to reject features that mess up the
   product architecture or are not implemented robustly.
 * All new code must be documented, do not accept contributions
   of code without documentation.
 * No new features in patch updates of stable releases.

@_date: 2014-04-11 17:17:36
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Preliminary review of the other 
It is premature to ask them to do that.  The underlying TLS toolkits,
NSS, OpenSSL, GnuTLS, ... don't yet have robust (or any) support
for DANE TLSA validation.  It is unreasonable to expect applications
to do all the heavy lifting.  The first step is adding DANE support
to the toolkits.
It is even a bit early to extend the toolkits, we've still not
agreed on digest algorithm agility for DANE, semantics for DANE-TA(2)
certificate usage edge-cases, ...  RFC 6698 is just a starting
point, it is not the destination.
I've also arrived at the insight that among any particular set of
clients and servers (say general purpose browsers and web servers
on the public Internet) there needs to be agreement about which of
the PKIX-{TA,EE} or DANE-{TA,EE} certificate usages are applicable.
It makes no sense to support all four certificate usages.  If choose
to support all four, you get the intersection of the security
benefits and the union of the interoperability problems.
Thus for SMTP, the DANE TLS draft proposes ONLY DANE-{TA,EE} as
supported usages, with PKIX-{TA,EE} undefined.
A similar choice needs to be made in a follow-on to RFC 6698 that
more explicitly defines how DANE is to be used with HTTP on the
public Internet.  This needs to be driven by the HTTP security
community, to be standardized under the DANE WG charter, but designed
by one or two engineers immersed in HTTP security architecture.
[ HTTPS libraries would need a configurable switch to choose between
PKIX-style TLSA and non-PKIX DANE-only TLSA records.  The switch
would be set by default to match general-purpose browser policy,
whatever that might be.  Here we run into some major philosophical
obstacles.  Is it the job of TLS certificates to ensure that you're
connected to whichever server you asked to connect to, or is it to
protect you from your own folly when you visit the websites of
typo-squatters, phishers, ... The presumed value-add of PKIX EV
validation rests I believe on the premise that users need protection
from themselves as much or more than from MiTM attackers, and that
it is the job of browser TLS to address this problem. ]
Therefore, a small group of browser engineers would need to take
up the task of thinking through how to really use DANE with browsers.
In addition, browsers, much more than MTAs, are used in a variety
of DNSSEC-hostile environments, and the "last mile" problem for
DNSSEC is not yet solved AFAIK, this will take time.

@_date: 2014-04-12 05:44:11
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Preliminary review of the other 
My personal bias is quite transparent from the language I used to frame
the question.  So while you and I unlikely to disagree on this point, I
run into rather experienced folks who indeed believe that the PKI ought
to protect users in the manner I allude to, and that therefore the PKIX
CAs in this manner offer a higher level of assurance than DNSSEC ever
can by allowing each domain to self certify its own keys.
Spin resists precise measurement, you can simultaneously only measure its
magnitude and one of the bias directions.
My objection is more fundamental.  I believe that any connection
between a domain name and some business name that consumers trust
needs to be at a higher level than the security mechanism that
builds a secure channel to a service at some domain name.  The
basic network security plumbing cannot and should not in my view
attempt to correctly set the "evil bit" on network flows.
If one accepts the view that TLS connects you to the owner/operator
of a domain, then I think it is quite clear which pair of DANE
usages is the right one for HTTPS on the public Internet.
If one believes that EV is the best way to protect consumers on
the Internet, and that without EV consumers would be subject to
substantially greater fraud, and no better ways can be found to
bind popular brands to their DNS domains, then one would choose
to standardize DANE for HTTPS with the PKIX pair of usages.
In this thread, I just wanted to highlight the fact that we're not
quite done designing DANE yet.  The Postfix DANE implementation is
the first real test case for RFC 6698 and considerable additional
work needed to be done to define the use of DANE with SMTP.
Regardless of the final design, I expect that considerable additional
work also needs to be done to properly scope DANE for HTTPS.  Thus,
in my view broad adoption in browsers is premature.  The base DNS
record format has been defined.  Now begins the hard work of applying
it to real applications.
Someone has to be sufficiently motivated to do this work.  It took
me a year to get from RFC 6698 to a reasonably complete I-D for
SMTP with DANE and a production-ready implementation.
Many of the implementations I've seen along the way are experimental
at best.  Given the state of the browser industry, a key developer
at Google, Mozilla or Microsoft would have to marshal the time,
resources and mind-share to flesh out DANE for HTTPS.  This despite
competing efforts to shore-up PKIX via CT.
I should explain that I have no personal gripes with CT, it is an
interesting idea and may well substantially help to keep the public
CAs trusted by browsers accountable.  As far as I can tell, it is
as yet unclear exactly how one might apply CT to private CAs as
with DANE-TA(2) or the CA-less case of DANE-EE(3).
So if the browser architecture remains committed to public CAs and
EV, but with enhanced accountability via CT, then we're looking at
PKIX-{TA,EE} usages for DANE in browsers.
If browsers on the other hand implement DANE with DANE-{TA,EE}
usages, then the security model largely excludes the public CAs,
EV, CT, and all that, and allows any domain to self-certify its
keys (after enrolling DNSSEC zone KSKs via its registrar).
One might imagine a hybrid approach where "secure" connections are
protected with PKIX and CT (and maybe a bit of DANE for extra safety
with PKIX-{TA,EE}), while "opportunistic" connections employ
DANE-{TA,EE} to achieve SMTP-like downgrade-resistant encryption
that is not represented as "secure" in the browser UI.
Of course all of this is predicated on the notion that the DNSSEC
last-mile problems will be solved, which may require pressure for
them to be solved, which may require some non-trivial adoption, a
catch-22 perhaps.

@_date: 2014-04-12 21:58:43
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
This is a profound failure to understand the kind mutability required
to securely erase sensitive data from memory.  Just because one
can mutate values seen by users of an object's interface, does NOT
mean that the underlying memory has been overwritten.

@_date: 2014-04-14 05:55:25
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Preliminary review of the other 
DNSSEC works fine on the Internet backbone, but is not yet widely
compatible with "last-mile" networks.  Various hotel, airport,
coffee-shop captive portals, behind some firewalls, ...
These are rarely environments in which SMTP MTAs find themselves,
but they are rather more common for browsers.
There are probably folks on this list more knowledgeable than I
on the various last-mime barriers to DNSSEC.

@_date: 2014-04-16 21:28:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
It is not true that C "does "no handle" names with NUL bytes in
ASN.1 strings.  ASN.1 strings have a value *and* an explicit length,
which is all a C programmer needs to use them safely, the rest is
attention to detail.
The Postfix code for checking names in X.509 certificates checks
that any extra bytes after the first NUL in the ASN.1 string are
all also NUL (allowing at most NUL padding).
I could have published an advisory about potential NUL bytes in
X.509 peer names a year or two before the eventual public disclosure
by others, but the issue seemed to obvious to make a big fuss.  So
I just made sure the Postfix code was correct.

@_date: 2014-04-18 05:02:09
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Simpler programs? 
And "cat -v" on my machine determines what's non-printable based
on the LANG environment variable, but does not actually correctly
handle multi-byte UTF-8 input, so with LANG != "C" the results are
simply wrong.  So the fancy options are not even always implemented
The ants were doing a fine job of tearing down Irvine Calornia when
I was there.

@_date: 2014-04-18 17:25:16
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Simpler programs? 
A major impediment to displacing Sendmail in businesses is that
Postfix has no sales organization and no commercial support.  Some
companies want to pay somebody (will somebody *please* take our
money) to feel that they are "supported".
Also, Postfix is not delivered in the form-factor that businesses
expect these days, which is as an "appliance", with a web-UI for
management and an integrated vertical stack of anti-spam filters,
data-leakage detection, anti-virus scanners, ...
Hence, Sendmail appliances, Cisco Ironport appliances, Symantec
Appliances, ...
As for MTAs in open source O/S distributions, Sendmail has largely
been displaced, roughly in order of "popularity":
    - Exim is the default MTA on Debian and Ubuntu
    - Postfix is the default MTA on RedHat, NetBSD and MacOSX
    - Sendmail is the default MTA on FreeBSD and OpenBSD
My sense is that while Exim is on more systems, Postfix handles
more mail, but whichever way the numbers break, each substantially
outnumbers the open-source Sendmail install base.
Exim is more popular in part, IIRC, because 'e' is before 'p' in
the alphabet, and some Debian developer long ago decided to pick
Exim on that basis, since no rational consensus for either Exim or
Postfix was in sight.  Exim has a built-in macro language with
conditionals and uses it for all kinds of content inspection and
policy extensions.  People who want built-in flexibility tend to
choose Exim as a more modern alternative to Sendmail.  Exim is not
as secure by design as Postfix, and its vulnerability history
reflects that.  And yet it is at least by some (if not all) measures
the more popular of the two.
Getting back to security, I think that Postfix is used primarily
because its configuration is human-readable and what it does, can
be done without resorting to a Turing-complete customization
language, though content filter and milter support pushes that
problem out to the extension interface.  The fact that Postfix has
a sound security architecture motivates only a minority of users.
Also the formerly endless series of Sendmail security advisories
seem to have dried-up, IIRC NetBSD switched to Postfix as a result
of frustration with the constant advisories right after the last
substantive Sendmail advisory.

@_date: 2014-04-18 17:51:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] bounded pointers in C 
Well written C software solves this with suitable string libraries,
which though they are not part of the base language, are used
consistently to handle variable length character data.
    - Perl, Tcl, Python, ... all have internal data types that
      are strings with a length.
    - Postfix has "vstring" and "vstream".
A major step forward would be to simply extend the standard library
with a suitably safe set of new interfaces.  Basically safe strings
and a safe stdio library that works with these.  We don't have to
make incompatible changes to the language.

@_date: 2014-04-18 18:44:34
@_author: Viktor Dukhovni 
@_subject: [Cryptography] bounded pointers in C 
I agree 100% with the key observation: "culture matters".  Most of
what's wrong with software in C is cultural.
In Postfix, Wietse has created a project-specific style (sub-culture)
of C programming, which has been used consistently as the project
evolved, over the past 17 years.  This sub-culture emphasizes
safety, comprehensive documentation, and code written for readability.
The architecture assumes that bugs are inevitable (though in fact
due to the above substantially less frequent than in other projects
of a similar size) and aims to minimize their impact through
privilege separation.
In OpenSSL, there a different style of C programming prevails,
which has also been consistent on a similar time-scale.  This style
does not emphasize safety, allows undocumented features and the
code is noticeably more cryptic (no pun intended).
So while much of the "blame" can be apportioned to lack of suitable
out of the box safe interfaces in the C standard library, the rest
is the result of choices made by the project to deliver on features
despite inadequate resources to invest in internal interfaces and
programming styles that could have made errors much less likely
and typically less severe.

@_date: 2014-04-18 22:31:57
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Just turn off C-optimization? 
There is a widely circulated urban myth that C compilers are more
likely to behave correctly when optimizing, because that's how they
are used most of the time, and most code is tested compiled with
So turning off optimization may lead expose rare compiler bugs.
If one writes a safe string library with great care, it need not
avoid optimization.  It just needs to not be improved by non-experts.

@_date: 2014-04-19 04:23:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Something that's bothering me about the 
Tales from the culture wars:
Postfix 2.11 added support for LMDB databases.  It took Wietse and
I considerable effort (45 email messages in the thread over 16
days), to convince the LMDB author to not leak uninitialized heap
memory into the LMDB database file in the name of performance.
Ultimately, measurement of the performance impact of initializing
new database pages before writing them to disk was reported:
    I've been profiling the code the past few days. In the ideal
    case (all data items smaller than 1 page, since single-page
    mallocs are reused and only pay the memset cost once) the perf
    difference is unmeasurable. It looks like for the worst case
    data size, the additional memset is costing a 7% performance
    drop on my machine. This is using a data record slightly larger
    than one page, which forces the DB to allocate 2 pages and
    fully initialize the 2nd page for every record. For any data
    sizes larger than that the performance loss will be smaller
    than 7%.
It took two weeks of good-cop, bad-cop persuasion and many different
formulations of the problem to get from "all databases make this
same optimization", to "OK, it costs little and makes sense".
LMDB is now safer by default, though IIRC it still allows users to
set a flag that turns off the memory initialization.
Vigilance can be exhausting.

@_date: 2014-04-25 01:11:56
@_author: Viktor Dukhovni 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
That's nice, but how do I do that for an "off_t", or other typedefed
integral type, which has no explicit INTFOO_MAX macro?
Ideally something that does not introduce a C99 dependency and
works even with older ANSI C compilers.

@_date: 2014-04-25 04:08:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
That may be a reasonable constraint on the target architecture in
my case, but:
Left shift of signed quantities, is undefined and hostile compilers
are free to do as they please here.

@_date: 2014-04-25 16:23:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
Indeed, nothing obvious comes to mind.  My work-around requires
C99 intmax_t.  I do all arithmetic for off_t using safe intmax_t
arithmetic instead (which comes with INTMAX_MAX), and then at the
end make sure that casting to off_t does not truncate the value:
    off_t mumble(off_t input, ...)
    {
    }
This is a pain.  And as I mentioned, I'd like to avoid a C99
dependency, but don't see any portable way to do that.

@_date: 2014-04-27 22:48:57
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
This sure looks like a sound argument for DNSCurve, which I believe
has even been field tested in shipping products.  I wish you all
luck you can muster to persuade the working group to see the light.

@_date: 2014-08-09 01:31:33
@_author: Viktor Dukhovni 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Given that there is and likely cannot be such a mechanism, and that
random curves have noticeably bad performance.  And may take
considerable time to generate, this approach seems unlikely.
We already a few more good new curves than we may need, plus all
the old ones whose reputation is now in doubt, but are needed for

@_date: 2014-08-17 03:44:43
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Not yet, but I'm working on it.  Or rather changing the perspective
from "fallback" to "step-up", with cleartext as the baseline.  While
some polishing may yet happen, the basic thrust of the document is
stable enough:
    Mental models don't change without a fight, and plenty of cognitive
dissonance.  Thus there is also a very long and active thread
cross-posted on ietf at ietf.org and saag at ietf.org.

@_date: 2014-08-19 13:21:40
@_author: Viktor Dukhovni 
@_subject: [Cryptography] [cryptography] STARTTLS for HTTP 
Only if the marketers are allowed to misrepresent it that way.  I
don't think it is a serious risk.  The real issue is the latency
cost, STARTTLS introduces multiple extra round-trips, and would
have to happen on every request.  A redirect to a port where TLS
is automatic is more performant, but then all clients need to do
Instead of a redirect, in     an "alternative service advertisement" is used:
    which is optionally employed by suitably capable clients.

@_date: 2014-08-29 13:35:59
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Phishing and other abuse issues [Was: Re: 
This is of course at that point no longer an IETF issue, rather an
application and user-interface design issue.
Speaking of applications, part of the problem is that banks don't
provide the client side of banking applications.  Rather a general
purpose browser is now the universal application platform, and the
browser has no knowledge of either the user's intentions, or the
boundaries of the set of remote resources that comprise any given
remote service.
I solve this problem with a dedicated account on a MacOSX machine,
that is used solely for banking, and has "Parental Controls" enabled,
restricting the Safari browser to connect exclusively to the bank
and nowhere else.  The account in question does not read email, nor
browse the rest of the web.
By jumping through these hoops I get a simulacrum of a banking
application which is moderately trustworthy.
To make progress the application folks will have to figure out how
to deliver safe function-specific interfaces to users.  This is a
difficult problem, compounded by economic externalities.  Banks
own ATM machines, but they don't own any of the various mobile
platforms, and have little influence over their design.

@_date: 2014-12-03 19:25:04
@_author: Viktor Dukhovni 
@_subject: [Cryptography] MITM watch - Tor exit nodes patching binaries 
Whether this is the crux of the matter or not, I think the remedy
is in large part to drive down the false negatives.  This is why
the DANE OPS draft updates RFC 6698 to explicitly ignore the
certificate expiration date with DANE-EE(3) TLSA records.  One of
the most frequent false negatives is "expiration", with administrators
not always deploying new certificates (a manual process in most
cases) in a timely manner.
Provided DNSSEC zone re-signing is automated, as it can and should
be, "IN TLSA 3 1 1" records, re-signed as-is, work indefinitely.
Key rotation is when the administrator decides, rather than when
a clock tick catches him unawares.
A parallel effort to drive down false negatives is to make sure
that buggy DNSSEC nameservers are upgraded.  At present, and
particularly in .nl, there are (early adopter) DNSSEC hosting
providers whose nameservers don't do Denial-of-Existence to spec.
I've notified the registrars in question and the .nl registry is
working with them to address the problem.
Patrick Koetter at sys4.de and I are developing a DANE TLSA test
site, that will help administrators test their SMTP server TLSA
deployment, so that they can catch errors before the rest of the
world does.  (And help them avoid errors in the first place).
We'll be announcing that soon.
I hope that by early 2015 false negatives with DANE TLSA for SMTP
can be made sufficiently rare to allow sending systems to enable
verification without having to engage in constant exception
management, that might lead one to always disable verification on
failure, even when there's a real attack (Bayesian impossibility
syndrome if you like).
If one of the large 800lb primates enables outbound verification,
(no need to deploy DNSSEC for their own domain to check others),
problems will become self-correcting, as loss of a large fraction
of inbound mail will quickly alert any site that screws up that
they have a problem to fix.  Everyone else benefits from the
pressure exerted by the large players.
If anyone on this list is in a position to influence SMTP (at least
outbound validation) DANE TLSA deployment at Gmail, Yahoo, AOL,
Microsoft, cable providers,  ... please get in touch.  Q1/Q2 of
2015, is I think a good target for such deployments, in that I
think the .nl problem should be under control by then.

@_date: 2014-12-04 07:07:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Mapping numbers to permutations 
A uniformly random stream of integers selected from [0, n-1] does
not directly yield an unbiased stream of bits.
Sampling the first $k$ elements of the stream results in any of
$n^k$ possible states with uniform probability.  Suppose such a
sample is used to generate the first $m$ bits of a binary output
stream.  This is accomplished via some function
    f: n^k -> 2^m
The only way that $f$ can result in a uniform distribution on $2^m$
is if $2^m$ divides $n^k$.  So in particular with $n = 5$ the
claimed uniformity is provably impossible.  What is possible is to
discard some subset of $n^k$ so that $2^m$ divides the number of
allowed states.  Whenever a forbidden state is reached, you throw
away your $k$ inputs and try another $k$.
This is reasonably efficient when $n^k$ is just larger than a
power of $2$.  For $n = 5$, and given the continued fraction
for $log_2(5) = [2; 3, 9, 2, 2, 4, 6, 2, 1, 1, 3, 1, 18, ...]$
we get the rational lower bound approximations:
    2/1
    65/28
    339/146
    ...
So for example 2^65 is just less than 5^28:
    $ echo "10k 2 65 ^ 5 28 ^ / p" | dc
    .9903520314
Therefore, after generating 28 base 5 digits, the resulting number
fits into 66 bits, but 99% of the time only 65 bits are required
and the top bit is zero.  If the top bit is non-zero discard the
state, and generate another 28 base 5 digits.  Otherwise, output
the bottom 65 bits.  This is uniformly random, and reasonably
efficient (discards are rare).  If we take the next approximation:
    $ echo "10k 2 339 ^ 5 146 ^ / p" | dc
    .9989595361
The discard rate drops to ~0.1%, but the output arrives in clumps
of 339 bits at a time (of course this bit stream can be buffered).
The simplest thing of course is to use [0, 3] and discard all the
4's base 5.  The discard rate is 20%, and a run of "4" will
occasionally stall the generator for a few cycles until the bad
luck (Chinese numerology anyone) runs out.

@_date: 2014-12-04 19:54:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Mapping numbers to permutations 
Depends on one's perspective, one might consider that the original
stream simply never contained the discarded states, and then the
mapping is 1-to-1 on the restricted states.
The technique you posted provably fails.  There is no function that
maps an odd number of uniformly random (unbiased) inputs to a
uniformly random (unbiased) even number of outputs.
This is a trivial counting argument.  The probability p(O) of any
output state O is equal to the fixed probability p of each input
state I multiplied by the number of input states N(O) that map to
    p(O) = N(O) * p
If p(O) is independent of O (no bias) then N(O) = N(O') = N for
all output states O, O'.  Which means that the number of input
states  =  * N.  Which of course means that  is a divisor of
  Since no power of 2 divides any power of 5, no such function
Note that your generator cannot wait forever to see the entire
infinite stream of base 5 digits before it produces a corresponding
infinite stream of base 2 digits.  It needs to output at least one
bit after seeing a finite number of base 5 digits.
Suppose that finite number is bounded independently of the input
stream by some number K.  Then for each of the equvi-probable 5^K
input states you need to output either 0 or 1 as the first output
bit.  But with an odd number of equi-probable inputs you cannot
generate an equi-probable 1-bit output (partition an odd cardinality
set into two equi-numerous subsets).
So no procedure that outputs the first bit in bounded time can be
unbiased.  In particular there are arbitrarily long input streams
that produce no bits of output.  If output is never produced only
after seeing countably infinitely many input digits, then by Zorn's
Lemma there is an infinitely long input stream that produces no
output, and the mapping cannot be one-to-one.
I think I have proved that discards are required to avoid bias.
So either your scheme or the proof is incorrect.

@_date: 2014-12-04 22:48:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Mapping numbers to permutations 
[ Sorry about my "Reply-To", your message went to the list. ]
No, when I was considering the case that N is odd.  So 6 is not
covered.  For N even a sufficiently large power $N^k$ is divisible
by $2^m$ and one is then able to output $m$ unbiased bits (though
the mapping is certainly not one-to-one if N has odd factors).
That's the case I was proving, N is odd.
We are on the same page.

@_date: 2014-12-14 02:20:47
@_author: Viktor Dukhovni 
@_subject: [Cryptography] When did zero day attacks become commonplace? 
The fist professional use of botnets I encountered to send spam
and malware started in 2002 with the "JEEM" trojan sending mail
via botnet nodes through open proxies.
    Before that, spammers sent through their own machines and most
email malware was a "prank".  JEEM IIRC was the first major commercial
email worm.  The JEEM author IIRC was also a pioneer in selling
botnet nodes to others.

@_date: 2014-12-21 19:19:44
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Yes, but we're not quite there yet.  There's some work left to do
in this space:
    * Who's writing non-toy DANE validator plugins for browsers?
      Has the code in question been independently reviewed?
    * With HTTP plus DANE, should the PKIX-TA(0)/PKIX-EE(1)
      certificate usages be supported by browsers, or should
      DANE-TA(2)/DANE-EE(3) be supported instead?  Supporting both
      essentially reduces the former to a less reliable version
      of the latter, so a design choice needs to be made.
    * More progress needs to be made on the DNSSEC last-mile
      problem, and a number of large DNSSEC hosting registrars need
      to fix broken nameserver implementations.
    * Some middleboxen appear to drop TLSA queries over IPv4, but
      allow same over IPv6.
    * With other middleboxen an "NXDOMAIN" result is returned just
      fine for "_25._tcp.mail.example.com IN A ?" and would have
      worked for "_25._tcp.mail.example.com IN TLSA ?", were the
      "TLSA" query not dropped.
    * Over UDP, DNSSEC creates a greater DDoS exposure, that in
      part is holding back adoption.  Over TCP the latency cost of
      doing 3-way handshakes for every lookup is too high.  We
      don't have MinimalT or similar deployed for DNS, but this
      could potentially address the latency, confidentiality and
      DDoS problems for DNSSEC.  Of course rolling out a new
      transport will take decades.
Solving the problem in theory is much easier than solving it in

@_date: 2014-12-22 15:38:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Recall that DANE is a better DV, it is not trying to be "EV".
CA-based DV verifies domain control to a much weaker extent (if at
all) than the syllogistic domain control established via the
relationship between registrant and registrar.
There is one registrar at a time for any given domain.  This part
of DANE is *not* the problem, a registrar or DNS hosting provider
can also convince any of the panoply of CAs to issue a DV cert.
The last mile problem is not a problem for MTA to MTA SMTP, or
server to server XMPP, and thus unsurprisingly deployment is starting
with these.  It will take some time and effort to make inroads in
the mobile device space.  DNSSEC stapling (if/when revived) could
be a big help there.

@_date: 2014-12-22 17:16:39
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Speaking of DNS hosting providers, just today one of them has staged
PowerDNS 3.3.1 to address reported problems with an earlier release.
Initial tests show no issues with PowerDNS 3.3.1.  So that provider
will soon be remediated.  I expect more progress will be made over
the next few months.
If anyone reading this is using older PowerDNS releases to serve
DNSSEC zones, please upgrade.  If you're on Debian wheezy, consider:

@_date: 2014-12-23 16:54:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
But it very much is not, because you don't acquire and retain
ownership of a domain through a CA, and every CA can issue
certificates, not just the one you're paying to do so.
Domain control *is* having your domain registered to you, through
some registrar/registry.  Paying a CA for a certificate for a domain
is NOT domain control.
Ben, there are various obstacles to widespread use of DANE today,
I mentioned most of them, but being less secure for DV than a CA
is *not* one of them.  On the contrary, DV by a CA is strictly
weaker, since control of registration or ability to make DNS changes
gets you a DV cert.  The new Let's Encrypt initiative (an example soon to be CA) will
I fear not take into account the DNSSEC status of domains.  And
thus issue DV certs based on MiTM vulnerable ability to respond to
insecure email for admin at example.com or spin up an unauthenticated
HTTP or HTTPS site.
So, I think you're being a bit obstinate on this one, let's discuss
something more substantive.

@_date: 2014-12-23 18:42:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Nico will cheer loudly if you also design a usable CT for DNSSEC,
that augments DS records with CT evidence.  Which means that CT
logs now expose all secure delegations.  Getting it deployed is
of course an uphill battle.
I think they should, but I've seen no evidence that they will.  My
questions about this on the acme list either went unanswered, or
I missed the answers. :-(
Yes, I think we're in fact much more in agreement than appears at
first glance:
    * DANE is a better DV, and most domains are and *will be*
      protected by DV not EV (EV with CT for high-value, DANE DV
      for large-scale).
    * CT for DNSSEC is worth exploring, will take time, and much
      evangelism to deploy.
    * Sure the 0.1% of domains that use EV are "important" to
      consumers, and are major targets of "phishing", ... so EV is
      not "solved" by DANE.  (Though many of us feel that ultimately
      the solution to phishing should not be any kind of certificates,
      but that other thread is bogged down).
    * DANE is not going to displace EV, and presumably the list of
      EV CAs is smaller, and CT may keep them honest.
    * So, some day, it would be nice to see (CA-based) DV go away
      to be replaced by DANE, with any CAs that remain doing just EV.
    * Which means that Let's Encrypt is a useful multi-year stop-gap,
      and with luck ultimately goes away.
Related to this, is a hope that some day we'll have additional
transport options along the lines of "MinimalT" that yield security
and make source address forgery difficult, while having the latency
of UDP rather than TCP (for repeat visits).  Then DNS could run
over such a transport and provide some confidentiality too, provided
of course one trusts the peer to not disclose query logs to

@_date: 2014-12-24 16:55:33
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
It seems rather easy, all log submissions of signed DS RRsets can
be validated up to the root.  If any registry or individual domain
is spamming the log with fake delegations, this is easily detected
and their ability to continue to participate may suffer.

@_date: 2014-12-24 21:59:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
I was just talking about spam prevention, nothing else.  A DS RRset
which validates up to the root, is either not spam, or if it is,
we know whom to blame.
Do you mean the parent pretending the delegation does not exist,
and returning a signed answer rather than a referral?
The parent can also actively deny the existence of DS records and
make the child zone "insecure".  Signed denial existence of DS RRs
at a delegation can be logged, when there is previous (not expired)
evidence of DS records.
However evidence of the parent serving the child zone, as if no
delegation existed, is more difficult to accomodate in a transparency
Surely for CT that distinction is not relevant.  CT exposes the
change to anyone who cares, and they decide whether it is legitimate,
and what they want to do about it.
Folks operating non-public DNS views, cannot make use of public CT
logs at or below non-public delegations.
An "A" record takeover is more easily done with BGP or other on-path
attacks at the network rather than DNS layer.  A more security-relevant
"takeover" is for "TLSA" records.

@_date: 2014-12-27 01:05:54
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
How is this different from a root CA signing some leaf cert,
bypassing the intermediate that is normally used to sign leaf
certs?  Or employing some never-before-seen intermediate?
It seems to me that any chain of validated delegations leading
ultimately to a TLSA RRset or similar key material is a candidate
for CT logging, evidenced by the full chain of signed DS and DNSKEY
RRsets that make the key material "secure".
CT for parent domains serving entries in what should be a child
domain is doable I think.  A more difficult problem is CT for denial
of existence.  Here the number of potential NXDOMAIN responses is
effectively limitless.  For TLSA records, one might insist that
any query for "_._" be anchored to a name that does
exist, but this still leaves 128K deniable RRsets per host.
I've not been following the "trans" working group, is there a
plausible design for CT for DNSSEC, or do the problems look

@_date: 2014-12-27 05:52:59
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Storm in a teacup, that makes for well intentioned, but transparent
All evidence to the contrary notwithstanding in that the vast bulk
of opportunistically encrypted MTA to MTA traffic is unimpeded,
and the fraction encrypted has gone up substantially.
Cleartext is not more secure than opportunistic TLS, which is
intended to protect the bulk of communications, not individual
sessions.  To harden any specific session against MiTM indeed
authentication is required.

@_date: 2014-12-28 15:05:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
This had occured to me, but there are some issues:
    * With "_._.mxhost.example.com" one might
      now need to make 5 queries instead of 3, unless there
      is way to "tune" minimization.  I am concerned about the
      impact on latency.
    * Validating stub resolvers would need to retrieve each
      of the relevant intermediate nodes, increasing the number of
      messages sent to the recursive resolver.
    * This still might not address denial of existence "spam".
Thanks for the update.  Understood.

@_date: 2014-12-28 15:13:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
"Fallback" is not a core part of the Opportunistic Security
definition.  In fact fallback is to be avoided in design and/or
deployment unless fielded systems exhibit significant obstacles to
sticking to encryption when advertised.
For example, Sendmail does not do STARTTLS fallback, if a peer
promises STARTTLS Sendmail will not employ cleartext.  While Postfix
does fallback now (reverts to cleartext when the TLS handshake
fails), this will become configurable in a year or so.
Of course negotiation of STARTTLS is not MiTM resistant without
a secure channel for signalling.  So SMTP with DANE can provide
reasonably comprehensive protection, while opportunistic TLS
alone is still vulnerable to active attacks, no surprise there.

@_date: 2014-12-29 00:38:19
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
A few holdouts on the specific phrase notwithstanding, I encourage
you to take a few minutes to read the draft (soon to be RFC 7435).
This puts opportunistic use of encryption in a more comprehensive
context of potential opportunistic use of MiTM-resistant protocols.
I am not religious on the term, Opportunistic Security was accepted
as a rough consensus compromise, it'll have to do.

@_date: 2014-12-29 14:51:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
PowerDNS is often used in "narrow mode", where denial of existence
proofs are generated and signed on the fly, with non-existentent
nodes evidenced not with NSEC3 RRs of actual nodes, but rather with
synthetic hashes that are the hash of the would be node +/- 1.
And the closest encloser node's right neighbour is again a "+1"
Here's a live example with the actual domain replaced by "example.":
    example.               SOA     ns1.example. email.example. 2014120803 10800 3600 604800 3600
    394ukt9jn8uge3d86433hgogtm429bqc.example. NSEC3 1 0 1 BEEF 394UKT9JN8UGE3D86433HGOGTM429BQD A AAAA RRSIG
    3te5k6ninjarg2rhegq886jo6kqlaurd.example. NSEC3 1 0 1 BEEF 3TE5K6NINJARG2RHEGQ886JO6KQLAURF
    m9ktr713mv89v98bat896geo4it3stbd.example. NSEC3 1 0 1 BEEF M9KTR713MV89V98BAT896GEO4IT3STBF
client zones one can elicit an essentially unlimited number of
signed responses.

@_date: 2014-12-29 18:45:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Certificates and PKI 
Denial of existence of TLSA RRs is significant.  As is denial of
existence of NS/DS records which might allow the parent to serve
a child zone directly.
I am not saying the issue a show-stopper, just that it is not
obvious exactly which records one can safely avoid logging.

@_date: 2014-03-01 04:20:56
@_author: Viktor Dukhovni 
@_subject: [Cryptography] GOTO Considered Harmful 
============================== START ==============================
With OpenSSL, the latest released code is 1.0.1f.  At the tip of
each of the OpenSSL 0.9.8, 1.0.0, 1.0.1, 1.0.2 (in beta) and "master"
branches a crude goto count (including dead code, test code, ...) shows:
    OpenSSL_0_9_7-stable 4483
    OpenSSL_0_9_8-stable 6166
    OpenSSL_1_0_0-stable 6441
    OpenSSL_1_0_1-stable 6884
    OpenSSL_1_0_2-stable 7264
    master               7759
So the trend in OpenSSL is to continue to use goto for error cleanup
in new code.  For what it is worth, the corresponding trend for
Postfix is:
    postfix-20010228     0
    postfix-1.1          0
    postfix-2.0          0
    postfix-2.1          0
    postfix-2.2          0
    postfix-2.3          0
    postfix-2.4          0
    postfix-2.5          0
    postfix-2.6          0
    postfix-2.7          0
    postfix-2.8          0
    postfix-2.9          0
    postfix-2.10         0
    postfix-2.11         0
    master               0
Both projects show long-term consistency in the coding style.

@_date: 2014-01-03 19:37:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography]  Timing of saving RNG state 
Speaking of the timing of RNG state save/restore, Nico Williams
observes that it would be prudent to save state not only on (clean)
shutdown, but also at startup, immediately after the previously
saved seed is loaded.  That way after a power-outage, panic, ...
the seed does not start in the same state as on previous boot.
[ Clearly the saved seed must be derived and later restored in a
way that ensures that the resumed PRNG stream is not identical with
stream that will be generated from the state at the time of the
checkpoint.  This is not a difficult requirement to meet.  ]

@_date: 2014-01-03 21:05:13
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Timing of saving RNG state 
Good to know, thanks.  We must have been looking at some older
systems last time this issue came up.

@_date: 2014-01-14 16:10:15
@_author: Viktor Dukhovni 
@_subject: [Cryptography]  TLS anon-(EC)DH 
Postfix uses anonymous cipher-suites by default when opportunstic
TLS is enabled, though in practice one still needs a self-signed
certificate as not all other client MTAs enable ADH/AECDH (even
though they generally make no use of the certificate they insisted on).
One annoyance is that there are no updated anonymous AECDH
cipher-suites with TLS 1.2.  If one wants SHA2 or AEAD one needs
to use ADH rather than AECDH, but with no means to negotiate prime
sizes, and performance disadvantages relative to AECDH at reasonable
strengths, ADH is sub-optimal.
    $ openssl ciphers -v aNULL+kEECDH
    AECDH-AES256-SHA        SSLv3 Kx=ECDH     Au=None Enc=AES(256)  Mac=SHA1
    AECDH-DES-CBC3-SHA      SSLv3 Kx=ECDH     Au=None Enc=3DES(168) Mac=SHA1
    AECDH-AES128-SHA        SSLv3 Kx=ECDH     Au=None Enc=AES(128)  Mac=SHA1
    AECDH-RC4-SHA           SSLv3 Kx=ECDH     Au=None Enc=RC4(128)  Mac=SHA1
    AECDH-NULL-SHA          SSLv3 Kx=ECDH     Au=None Enc=None      Mac=SHA1
    $ openssl ciphers -v aNULL+kEDH+TLSv1.2
    ADH-AES256-GCM-SHA384   TLSv1.2 Kx=DH       Au=None Enc=AESGCM(256) Mac=AEAD
    ADH-AES256-SHA256       TLSv1.2 Kx=DH       Au=None Enc=AES(256) Mac=SHA256
    ADH-AES128-GCM-SHA256   TLSv1.2 Kx=DH       Au=None Enc=AESGCM(128) Mac=AEAD
    ADH-AES128-SHA256       TLSv1.2 Kx=DH       Au=None Enc=AES(128) Mac=SHA256
IIRC Nico Williams tried to raise this issue for me on the TLS WG
mailing list, but did not get much support.  Any chance you're
willing to help?  I think the AECDH cipher-suites deserve some more

@_date: 2014-06-04 04:55:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Interesting new article by Ross Anderson... 
For what it is worth, for modestly sized projects that can manage
on volunteer effort alone, the economics is not always an obstacle.
Postfix takes the high road on security, and ignores the economics.
As I mentioned on this list before, Postfix is somewhat behind Exim
on market-share, and Exim has more features, but a noticeably more
fragile code-base.  So the observation that features trump security
is once again validated, but this economic reality has no bearing
on the Postfix developers, we're not in it for the sales.

@_date: 2014-06-05 03:28:19
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Even when they get it right they get it wrong 
On an IMAP or submission server, certificates are maximally
applicable, since unlike a browser which visits every web site on
the 'Net, the MUA connects to just one server for each of IMAP and
SMTP.  If PKIX should work right anywhere, it should be this use-case.
Indeed an embarrasing operational failure.  This is why DANE-EE(3)
associations won't have explicit expiration times (the dates in the
certificate will be ignored).

@_date: 2014-06-11 15:09:35
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Subject: Re: Swift and cryptography 
I think you're advocating for Cryptol DSL (or equivalent of they
have competition):
        which was mentioned on this list some time back.

@_date: 2014-06-27 16:27:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] "Perfect Forward Secrecy - The Next Step in Data 
To quote: "Receive Your Complimentary Executive Brief NOW!"
Cryptographers don't write executive briefs, the marketing department
does that.  Bite-size ideas for people who aren't expected to
understand, from people who are selected for not understanding.
I don't see anything wrong here.

@_date: 2014-06-27 16:42:16
@_author: Viktor Dukhovni 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
When a server no longer offers an algorithm, if the protocol involves
negatiation, then clients will negotiate some other shared algorithm
or fail.
When a client no longer offers an algorithm, if the protocol involves
negatiation, then servers will negotiate some other shared algorithm
or fail.
When algorithms are ranked by preference, those shared algorithms
ranked most preferable by whichever party selects the preferance
ranking will be selected ahead of those ranked least preferable.
On either the server or client (if not a fixed-function black-box)
adjustments to the list of algorithms supported and their ranking
are made by the crypto library (defaults), application developer
(further tuning for application needs) and application administrator/user
(post-release tuning, vulnerability/interoperability work-arounds,
cargo-cult knob twiddling, ...).

@_date: 2014-03-02 20:21:36
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Testing crypto protocol implementations 
This is a good point.  My view is that a tool that tests the security
of a protocol implementation, likely needs to be a separate highly
scriptable implementation of that protocol which can be programmed
to deviate from the protocol at every step, (modify and/or re-order
the expected protocol messages in controlled ways).
This would be a costly project for each implementation to develop.
Ideally such a tool would be available for testing multiple
implementations, but it is far from clear how it would be funded.
Having lived and breathed DANE TLSA for the last year, and looked
at a bunch of flawed implementations, I can see the need for such
a test suite for DANE, in the form of test services with interesting
TLSA records and even more "interesting" certificate chains.
There are a handful of DANE TLSA test sites, but their "interesting"
combinations of certificate chains and TLSA records are far from
sufficiently comprehensive.  Lack of a reasonably comprehensive
test-bed almost assures that flawed implementations will continue
to be produced, and users will continue to use them unaware of
their defects.

@_date: 2014-03-10 13:40:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] End-to-End Protocols and Wasp Nests 
What I had in mind was a set of TLS servers which exhibit a number
of important edge cases with respect DANE TLSA chain validation,
but these should, if possible, also cover TLS protocol tests.
An example anomaly is "IN TLSA 2 0 1 {TA digest}" matching a CA
presented by the server, where the server's chain is disconnected.
I've seen implementations of DANE verifiers that accept any "chain"
which contains a matching usage 2 (private CA) trust anchor, without
checking that a path can be built from this trust anchor to the
leaf certificate.  This is just a feature test, there is no
combinatorial explosion.
We can add a test to make sure that DHE and ECDHE server key exchange
messages are properly validated, but I see little reason to test
combinatorics of this second anomaly with the disconnected server
chain anomaly.  Thus we'd have two tests so far, not 4.
Similarly if one tests for correct enforcement of path length
constraints or name constraints, it seems unlikely that this would
be dependent on either of the previous tests.  (Though one should
test enforcement of path length constraints for each of the four
DANE certificate usages).
Many of the important cases that implementations should reject can
be tested in isolation, and are the least well tested cases in
practice.  Much testing focuses on accepting valid inputs, where
combinatorial testing is more likely to be needed.
If the test-suite becomes too expensive to run on every build, it
can be used only for pre-release builds...

@_date: 2014-03-12 01:34:04
@_author: Viktor Dukhovni 
@_subject: [Cryptography] RC4 again (actual security, 
You're saying DHE, but you mean "anon DH".  With ECDHE, authenticated
TLS cipher suites are defined for modern modes and hashes, but the
anon DH variants are missing.

@_date: 2014-03-16 22:49:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Client certificates as a defense against MITM 
We might recognize the fact that introduction and key continuity
are potentially separate problems.  After DNSSEC + DANE act as an
introducer of strangers, where a TTP is largely unavoidable, we
might from that point on use ideas along the lines of:
    to make network MITM considerably more difficult.  The main difficulty
is that with domains key continuity cannot be assured.  Domains
are sometimes transferred to new owners, and in those cases it is
not clear whether the new owner's keys will be signed by the previous
This means that automated lights-out applications (say MTA to MTA
email) can't rely on Tack working indefinitely for all domains, and
some process for handing re-keying on domain transfer needs to be
designed that does not immediate bring back the MITM risks that
Tack is designed to avoid.

@_date: 2014-03-16 23:45:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Client certificates as a defense against MITM 
One potential approach is for the server to store the client PKCS12
bundle encrypted with a strong user password unknown to the server.
The client sends a message with the bundle id and the server returns
the bundle to the client, which the client decrypts...
This is actually more robust than shared secret schemes, because
the the shared secret is asymmetric, the server just solves the
multi-device storage problem for the client.

@_date: 2014-03-25 23:22:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Dark Mail Alliance specs? 
There is no TLS 1.3 specification yet, let alone running code.  As
for opportunistic TLS for SMTP, this is supported by a lot more
MTAs than Postfix and Exim, but as yet a majority of MTAs have not
enabled TLS.  The fraction of SMTP traffic that is TLS encrypted
is somewhere between 10% and 50%.
Indeed, but getting users to turn on TLS (even though with SMTP
nobody cares if your certificate is self-signed) is not so easy.
Even ietf.org have not yet enabled STARTTLS on their Postfix server.
Of course with ietf.org (and also the server for this list) the
vast majority of the traffic is publicly archived, so enabling
TLS would be more a matter of making a statement, rather than
protecting content confidentiality.
I must protest!  This is an absurd waste of CPU cycles.  Without
DNSSEC and DANE, essentially nobody can or is checking TLS certificates
with SMTP.  So your 16k-bit certificate is a useless source of
false confidence.
The good part of nobody checking SMTP certificates is that deployment
is easy.  On the server just spin-up a self-signed cert and off
you go.  On the client no root CAs to worry about, just enable
opportunistic TLS and harden the traffic against passive eavesdropping.
If you want security in the face of active attacks on SMTP, you
need DNSSEC and DANE.  At the moment this requires a Postfix 2.11.0
client with a local validating resolver and a remote server with
MX records, MX host addresses and MX host TLSA records in a DNSSEC
signed zone.

@_date: 2014-03-30 20:41:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Point of terminology: the above as stated is essentially nonsense.
There is no such thing as a "random constant", let alone a provably
random constant.  A value is either constant (0 entropy) or is
selected "at random" from a finite set of values via a probability
distribution with more than 0 bits of entropy.  The security of
any PRNG keystream generated from a seed with n-bits of entropy is
never stronger than n-bits.
Therefore, either:
    - a constant is just fine, and what you're looking for is not
      "random", but rather "unlikely to be backdoored in some
      non-obvious way".
    - constant is not fine, and you need randomly chosen data with
      sufficiently high entropy.
For the former, given the large number of bits to generate, you
could indeed go with go with digits of sqrt(2), e, or pi, whichever
is more convenient (to store or even generate at run-time).
For the latter, your provable keystream generator is only as secure
as its initialization.
I would contact the authors and see what they have to say about how to
prepare the "known" "random matrix".  While apparently, unlike the key,
the "random" matrix need not be kept secret, it is not clear whether it
can be simply constant, or needs to be "random" is some stronger sense.

@_date: 2014-03-31 18:20:57
@_author: Viktor Dukhovni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Too many degrees of freedom in choosing real-world data sources.
One really should stick to the *initial* sequence of binary digits
of one of the 3 or so best known irrational numbers.
    * sqrt(2)	(First to be proved irrational)
    * e		(First of the well known constants proved transcendental)
    * pi	(Most famous transcendental)
If none of these yield a sufficiently *generic* (as opposed to
random) sequence of bits, the problem requirements are likely too
narrow to admit a nothing up my sleeve sequence.
If the problem involves algebraic operations on the bit string (as
a large integer rather than bit-wise) one might want to avoid
sqrt(2) since its square is certainly not terribly generic.

@_date: 2014-05-07 21:19:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] cryptography Digest, Vol 13, Issue 6 
We're not done with non-quantum crypto yet.
    - There is significant recent progress in performant and
      side-channel resistant elliptic curves by D.J. Bernstein et. al.
    - There is significant recent progress on RC4 cryptanalysis.
    - The sponge construction in Keccac is a novel new building block.
    - We're starting to move away from CBC mode to AEAD modes.
    - Poly1305 is an interesting new MAC.
There is also active research on "post-quantum" cryptography, that
is asymmetric cryptography that resists both classical and quantum
I would ignore QKD, which is interesting Physics, but otherwise
largely marketing hype.
If/when practical and scalable QC is available, then we lose most
of the currently deployed asymmetric algorithms for signature and
key exchange.  Thus the interest in post-quantum crypto.
However, classical crypto is by no means all done yet.

@_date: 2014-05-09 15:31:13
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Heartbleed and malloc 
Done, but the OpenBSD critique did have a point, in that OpenSSL
maintained its own memory pool for some allocations that bypassed
malloc/free, and therefore was not covered by any security options
in malloc() and free().  Disabling that pool and always using
malloc()/free(), uncovered a use after free bug.
In particular plaintext of previous packets would be present in
memory even after the content was freed.  Thus for example, with
Postfix and an OpenSSL with HeartBleed unpatched, one could retrieve
fragments of email messages from the heap, even though Postfix
always wipes freed memory, and the Postfix SMTP server is not
multi-threaded, rather ther is a dynamically sized pool of
servers accepting connections serially.

@_date: 2014-05-13 14:28:16
@_author: Viktor Dukhovni 
@_subject: [Cryptography] What faults would you inject to test crypto 
This one further refines into a lot of sub-cases, having to do with
chain validity, when the certificate is verified via a "trust-anchor".
Many DANE implementations I've seen fail to check that there is a
properly linked chain from the leaf to the root.  This matters with
DANE certificate usage 2, where the chain is not otherwise also
validated by PKIX.  Such implementations find a matching trust-anchor
in the server's "certificate message" and call the whole thing good!

@_date: 2014-05-19 04:47:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
Indeed, somewhat better than I expected at this juncture, but not
entirely surprising given the current incentives for large providers.
I am pleased they posted the report, and would like to see more
reports like this going forward.  I am somewhat disappointed it
appears to support the fallacy that somehow PKIX authentication is
applicable to SMTP and thus aplauds the fact that some SMTP servers
throw away money on public CA signed certificates, when opportunistic
TLS, or no TLS is required in their absense, and even their presence
cannot usefully preclude active attacks.
The misconception that TLS for SMTP is rather like TLS for HTTP is
sadly still rather prevalent.

@_date: 2014-05-19 18:49:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
My point is not that the CA certs are expensive in this case, they
could well in have been priced quite reasonably, rather the issue
is that even at $0.01 they are entirely futile for SMTP.  So whether
you spend $0.01 or $1,000.00 you still get nothing.

@_date: 2014-05-20 18:08:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
Fortunately, I have the analysis written down:
                    start at the top of 1.3 and read through the end of 1.3.4 (which
ends with Goedel's CA PKI theorem, any set of PKIX CAs is either
incomplete or inconsistent).

@_date: 2014-05-20 22:34:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
Yes, that's the short version.  The long version in the DANE draft
explains why we can't really do much better without DNSSEC+DANE
even if we wanted to.
Note, that here too we have some "mile-high pole" security.  With
or without DANE, STARTTLS only protects email transport (the part
of the problem we know how to solve), and deliberately neglects
the question of end-to-end security.
The end-to-end security problem for email is rather non-trivial,
because in most cases people expect their anti-virus/anti-spam
outsourced provider to scan the message content for malware or
spam, in regulated industries employers may be required to archive
cleartext of email, ... and of course we still have not demonstrated
usable human to human internet-scale key management.
So STARTTLS (opportunistic TLS and/or opportunistic DANE TLS) is
only a passive monitoring counter-measure (hardened against MiTM,
but not end-point, attacks with DANE), which paradoxically can only
succeed *because* you don't know it is there (and thus can't signal
a requirement for it, or know it happened).
The fact that STARTTLS is opportunistic and does not require any
bilateral coordination between sending and receiving parties or
user signalling, ... makes it possible to incrementally deploy it
at scale without anybody noticing that it is even happening.
I bet that a lot of folks were rather surprised by the magnitude
of the Facebook reported percentage of TLS encrypted transport.
The deployment of opportunistic STARTTLS for SMTP happened quietly
one domain at a time, without email users having to do anything to
turn it on (of course server operators had to enable use of STARTTLS
as a default).
So you can have ubiquitous opportunistic encryption that is deployed
transparently, but its use or non-use is opaque to users, or you
can have visibly strong mandatory encryption.  Getting ubiquitous
strong encryption that you can enforce/audit is rather more

@_date: 2014-05-24 23:21:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] New attacks on discrete logs? 
This is simply not the case.  A lot depends on the actual group.
For example Euclid's algorithm makes it possible to efficiently
reverse iterated addition mod p (in $\mathbb{Z}_p$), but we don't
know how to efficiently reverse iterated multiplication mod p
(multiplication in the associated group of units $\mathbb{Z}^*_p$).

@_date: 2014-05-27 13:38:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Langsec & authentication 
Fully agree with the sentiment.
No SSL library (or accelerator) has any business delivering raw
decrypted payload whose MAC has not been checked to the application.
Any that do are seriously defective.   My imagination does not
stretch far enough to admit fatally broken implementations as
plausible straw-man examples.
The API that checks message signatures could also be fatally flawed.
It is turtles all the way down.
The straw-man is on fire.  In the mean time real implementations
don't return plaintext that has not been MACed.
This is back on much firmer footing, signed messages are sometimes
semantically preferrable to signed transport, although with message
signatures not channel-bound to a transport, one has to be much
more careful about replay attacks.  No silver bullet.

@_date: 2014-11-14 05:24:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
Storm in a teacup.  These are anti-spam measures to filter botnet
outbound direct-to-mx traffic.  This is not a conspiracy to spy on
your mail, and does not apply to the vast majority of MTA to MTA
And the reported practice is by means universal, time warner cable
in new york certainly does neither suppresses nor MITMs STARTTLS
from my home network (DANE TLSA records work just fine for the
domains that publish them).
As for SMTP being the problem, that's just ignorance.  Most security
difficulties with email are a consequence of requirements, not
   * Allow total strangers to communicate.
   * Allow asynchronous communication.
   * Allow mail to be delivered to multiple recipients.
   * Allow forwarding via mailing lists, virtual mailboxes, ...
   ...
By the time you've built something with the flexibility and
universality of SMTP, you have SMTP's problems.
As for S/MIME headers in the clear, that a feature, and the problem
is with MUA S/MIME support not SMTP.  S/MIME can encapsulate complete
mesages, with a "vanilla" header in the outer message.  Server side
searching, sorting, ... becomes difficult.
I think very few people would likely want to use end-to-end encrypted
mail, even if all the key-management usability issues were addressed
and it became easy to send encrypted mail and read a given encrypted
message.  Subtantial problems remain:
    * Lose your key, lose all your mail.
    * Substantially reduced server-side spam filtering.
    * No server-side search.
    ... and many more ..
For better or for worse, securing the transport is by far the more
tractable problem, and this does not hobble usability.

@_date: 2014-11-14 22:59:36
@_author: Viktor Dukhovni 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
Yes, folks with gmail.com addresses complaining about providers
intruding on other users' privacy.  If you really want to walk the
walk, protect your own privacy first, run your own mail server,
then write-up and implement a usable design that starts solving
the problems.
I agree that Paul will be able and willing to use end-to-end
encrypted email, I might even send some to him some day, but I
still don't see mass adoption as very likely.

@_date: 2014-11-17 22:38:08
@_author: Viktor Dukhovni 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Yes, we feel good.  Opportunistic security is what you do when
you'd otherwise send in the clear.  It is not weaker than cleartext.
For some protocols (SMTP and XMPP at present), one can opportunistically
attain authenticated encryption via a downgrade-resistant channel
(DNSSEC + DANE), implementations are just starting to support this.
For other protocols we don't even have a specification yet.  This
takes time.  My curated list of SMTP with DANE domains has 372
domains.  It is early days yet.
I encourage the folks here to deploy DNSSEC and publish DANE TLSA
records for their MX hosts.  That said, better to not do it at all,
than do it wrong.  Make sure you're prepared to keep the DNSSEC
signatures and TLSA records correct, and that you understand key
rotation for both DNSSEC and TLSA.
The necessary invariants are:
    * Never have dangling DS RRs, each DS RR should always map
      to a DNSKEY in the zone, even when the DS RR is cached,
      so keep TTLs in mind.  [ Add DNSKEY RRs before adding DS.
      Remove DSs before removing DNSKEYs. ]
    * Never deploy server certificate chains that don't match
      some TLSA RR. [ Add TLSA RRs and wait a TTL or two, before
      changing the server chain. ]
For DNSSEC sanity checks,
For SMTP with TLSA sanity checks, stay tuned, new test site coming

@_date: 2014-11-19 04:05:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
SMTP used for submission is called submission, when one wants to
be more specific.  This is appropriate here.
With submission they're attacking a single system that often quickly
shuts down the compromised account, and many such systems impose
prior rate ceilings to limit future damage.
Port 25 blocking and filtering from consumer devices is simply
sound network management, it would be nice if we also had near
universal egress filtering of spoofed network addresses.  Would
sure help avoid many a DDoS attacks.
The ISP has a network to operate for the benefit of all users on
the Internet, theirs and remote.  Abuse prevention is part of that
duty.  Given the spam problem, restricting port 25 access from MUA
users is best practice.  MUAs have long used port 587 to reach
their favourite non-ISP submission service.
I'm afraid the real world crushed that dream long ago.  Over and out.

@_date: 2014-11-19 18:16:47
@_author: Viktor Dukhovni 
@_subject: [Cryptography] New free TLS CA coming 
Since this is the cryptography list, we should perhaps discuss the
actual technology.
Is it reasonable to infer control of the domain based an the ability
to publish content at a chosen location on the domain's current
website?  Should any HTTP site hosting provider be able to
independently acquire new certificates for the domain?  Which CAs
are trusted to validate the current website?  Are http:// URLs
supported for initial bootstrap?
A non ad-hoc demonstration of both domain and site control is I
think a DNSSEC validated DANE TLSA RR attesting to the validity of
the public key:
    _443._tcp. IN TLSA 3 1 1 this allows domain owners to bootstrap (non-DANE) HTTP client
compatibility from a DANE record validated by the issuing CA.
Yes, other domain "control" demonstrations will likely be necessary
and acceptable, but they need to be chosen with care.
It is not always the case (or even desirable) that one can easily
automate either site content modification or DNS updates from the
edge server that terminates SSL connections and can use the signing
Making good choices of mechanism is important.  It can't just
be "a miracle":

@_date: 2014-11-19 19:43:41
@_author: Viktor Dukhovni 
@_subject: [Cryptography] New free TLS CA coming 
Not beyond it, but definitely on the bleeding edge for now.  This
would work great for some, I for one would use the service.  One
might reasonably argue that if the domain is signed, this should
be the only way to provide the proof.  FWIW, out of ~88,000 Alexa
top domains tested (so far), 745 or just under 1% are DNSSEC signed,
so indeed this mechanism is not applicable to most sites yet.  [
Making it available and marketing it a bit might spur deployment.
Boostraping such domains to LE can be made stronger by requiring
not only the TLSA RR, but also when starting with no prior cert
from LE, that the server have an interim self-signed cert with a
public key matching the TLSA records.
As I said, this can't be the only mechanism, and it will initially
be far from widely used, but whatever mechanisms are used deserve
close scrutiny.  They need to be usable, and yet somehow not give
away the store on the security front.

@_date: 2014-11-19 23:42:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] SUBMIT is not SMTP, 
This is the cryptography list.  I think we can have naive discussion
of email protocols and infrastructure somewhere else.
 *  An ISP was observed to strip STARTTLS on port 25, rather than
    block the port outright.
 *  While this could have been nefarious, it is far more likely an
    anti-spam measure.  Of course opinions about whether anti-spam
    measures by ISPs are nefarious or not vary.
 *  Most MUAs should be using port 587 submission, and would not have
    been affected.
 *  The clients in question were exceedingly unlikey to have been MTAs.
 *  MITM resistance for MTA-to-MTA traffic is available non-scalably
    via bilateral agreements between peer systems.
 *  MITM resistance for MTA-to-MTA traffic is also available scalably
    via DNSSEC + DANE, but current deployment numbers for DNSSEC
    and the sheer novelty of DANE for SMTP mean that coverage is
    for now *very* thin.  Initial deployments are mostly in Germany.
 *  Some folks on this list don't give a hoot about TLS for SMTP
    transport, since it is not end-to-end.  Indeed SMTP transport
    security does not protect data at rest.
I think that about covers it.  Perhaps we can move on.

@_date: 2014-11-20 20:09:23
@_author: Viktor Dukhovni 
@_subject: [Cryptography] FW: IAB Statement on Internet  Confidentiality 
This list has a public archive.  Anything other than clear-signing
seems silly.  Not all the machines on which I read my email have
copies of my S/MIME keys.  Encryption of this list would IMHO be
a nuisance.

@_date: 2014-11-20 20:18:08
@_author: Viktor Dukhovni 
@_subject: [Cryptography] New free TLS CA coming 
Exactly, Web PKI certificates are unfortunately not specific as to
the service endpoint, they certify the host only.  So an HTTPS cert,
could be misused for other purposes, and it is not yet clear whether
the LE CA will issue wildcard certs covering all hosts in the domain.
Well control of the domain is essentially control over the DNS,
try as one might to distinguish between the registrant and the DNS
provider, the real operational control is by the DNS provider,
though ideally via lights-out automated processes that authenticate
the registrant as the authorized user to make changes.
So for DV validation, you can't do much better than effective
control of the DNS.  If the domain is DNSSEC signed, I would (and
I guess did in this thread) argue that this should be the only
means to prove control.  For the (majority) unsigned domains, other
"proofs" will need to be supported.

@_date: 2014-11-21 07:29:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] FW: IAB Statement on Internet  Confidentiality 
Perhaps the IETF endymail list would be a better place for that
experiment.  Unless that special right someone is only here, and
is IETF-averse.
Here, there are plenty of non-email discussions, and forcing all
of these to endure experimental encrypted email is likely not a
good idea.
What's more, if this is to be more than just clear-signing, the
user agent would have to encrypt email to the list, with the
moderators removing the the sender's signature and encryption in
the appropriate order, and releasing the cleartext back to the list
through a filter than re-signs as the list and re-encrypts (to each
recipient separately, so as not to expose the lurkers in the enveloped
That's a lot of work.  A list friendly tool would allow preparation
of an encrypted, signed message once, with a given symmetric key,
that can be enveloped to each recipient without repeating the
encryption and signature steps.
Such work probably needs to happen on a smaller scale first, with
a working presented here if the authors truly feel it is suitable
for adoption.  Speculation of whether to adopt in the absence of
already working/usable code seems futile.

@_date: 2014-12-01 00:02:47
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Toxic Combination 
[ Note, I am not saying that I expect imminent DANE adoption in the
  HTTP stack. ]
The nature of the difficulty is much greater for PGP than for
DNSSEC.  I am not going to deep dive into the details of that.
Email content encryption imposes usability barriers on both parties,
while DNSSEC is only a burden on the zone maintainer side and
tooling to address this "local" problem is improving.
This also faces adoption barriers.

@_date: 2014-10-04 20:20:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
This is plainly wrong for 1024-bit moduli, in the example below, p and q
are respectively:
    00d71a4e9865d1cdcdd8e6e4cbc5309971e52c121efee4a080a7d11af6fa7096e18470cbef6034c096b4170133d9edb45bd90a2906b34f58bf66278ed1dba8ffad
    00d26e1e81fde36b9daec7acbee3279b70d00fe771b65dbf8786f2f006621d4b517e5970801b517be34b7c483678ac99cfa9b22b075bde85a6d069dce9ef53a3f9
both are consist of 128 nibbles after the initial 00 to ensure that
the number is unsigned, the high nibble of both is "d", so both
are 512 bit numbers.
    $ openssl genrsa 1024 2>/dev/null | openssl rsa -text
    Private-Key: (1024 bit)
    modulus:
    publicExponent: 65537 (0x10001)
    privateExponent:
    prime1:
    prime2:
    exponent1:
    exponent2:
    coefficient:
    writing RSA key
    -----BEGIN RSA PRIVATE KEY-----
    MIICXAIBAAKBgQCw0BtpF81oH/nRnoKg658Ydg0yU10v6URPHtcDAhN+QpTFLQOD
    HweCUAf408uRbWKapZoiH0H2N/XxB4q2PCikzLZhMdrHAKT3G9vv9sKJsIpTurzb
    8FD4GMOsQnvgaWPj8YizQ7RWqxF67Cda7hgKDFft5ObWpmBdBOftqkLWRQIDAQAB
    AoGBAKSOapRapL8d02F2BtlBsWYQqKOH1pi6nuqMJ0wTaJT/3nnMNRKZlGGBnonE
    hBcrGLQZH+RV9wvydSEIBd8pCiEaorAkDpsrMZc0viKe4nNexc4/6JlvFWgT/efX
    7xjd3W4PJvmGmvGhbaqJWSkg4iYNKBX7T+czhuq2XIYF6M1BAkEA1xpOmGXRzc3Y
    5uTLxTCZceUsEh7+5KCAp9Ea9vpwluGEcMvvYDTAlrQXATPZ7bRb2QopBrNPWL9m
    J47R26j/rQJBANJuHoH942udrsesvuMnm3DQD+dxtl2/h4by8AZiHUtRfllwgBtR
    e+NLfEg2eKyZz6myKwdb3oWm0Gnc6e9To/kCQH3YMD9M4pArbEi5dtXo9v0BfOUl
    KS8ND/geiE4SeyhqzBdJ2MRKWJpSxlq3wTommM3D+finkzZy1As0rWZ72wkCQB4r
    U4xnjhd7v/c4uRVwNET0T5NrJi5Cq3eZlPgVUQXfZTIFgxhnkk+AHw1rYdm9I5y8
    wpaHgVvAEtlapt99KmECQDV2oymV7+6YoA46LlxBwA+cTUjwkgZy2Uc2ip+JQQ9P
    J6fDIvfqIkSUqCCEc/D5qTtjcMi32CGbZGVnkikJcZE=
    -----END RSA PRIVATE KEY-----
It is also far from clear how having p/q ~ 4 rather than p/q ~ 1
would help the attacker.  IIRC on the contrary having p and q too
close to each other (sharing too many top bits) is known to be
I have a simple proof of the Goldbach conjecture and the Riemann
hypothesis.  I like his approach, so I'm not going to publish

@_date: 2014-10-04 20:39:01
@_author: Viktor Dukhovni 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
For a modulus with 2k bits, p and q will both have k bits.  For a
module with 2k+1 bits, p will have k+1 bits and q will have k bits.
The bit counts in question are completely unremarkable.  If there
are bugs, they are somewhere else (random number generation, prime
sieving, ...).

@_date: 2014-10-11 16:26:55
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Sonic.net implements DNSSEC, 
No.  Their recursive resolver validates data from upstream sources.
It then serves some synthetic data of its own, which is seen as bogus
by downstream validating resolvers.
A similar thing was done briefly to me by Time Warner Cable.  They
"quarantined" my cable modem by making it serve the same bogus A
record for all domains with a 5s TTL.  The reason was apparently
that they wanted me to "request" a cable modem upgrade.
I did not notice for some time, because my OpenWrt router runs its
own validating resolver and does not use the ISP's recursive caches.
When I power-cycled the router a few days back, I was in for a
protracted trouble-shoot.  The router could not sync its clock,
because none of the openwrt NTP pool servers would resolve.  With
an incorrect clock the signature on the root zone looked wrong, so
the local DNS resolver failed to work.   Eventually I figured out
what happened, and "volunteered" for the upgrade.

@_date: 2014-10-14 17:09:44
@_author: Viktor Dukhovni 
@_subject: [Cryptography] factoring small(ish) numbers 
Are you looking at factoring RSA moduli (product of two primes with
~n/2 bits) or general $n-bit$ numbers?  Many numbers have small
factors, which are easily found by ECM with run-time dependent only
on the size of the factors not the number itself.  The remaining
large factors can be subjected to primality tests, and either found
composite or if desired proved prime.
It is only at that point that any large composites with no small
factors found by ECM should be subjected to GNFS.  Of course with
properly generated RSA moduli you'll start with GNFS immediately.

@_date: 2014-10-15 19:24:25
@_author: Viktor Dukhovni 
@_subject: [Cryptography] factoring small(ish) numbers 
Modern pseudo-primality tests are very good.
Primes are very common.  Read about the prime-number theorem.  For
512 bit numbers roughly one in every 384 is prime.
This is wrong.

@_date: 2014-09-16 16:22:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Is there any concern with respect to server applications that create
a thread per client request, and consume very little random data
in each thread, but create pthreads at a high rate?  Or is there indirection between pthreads and kernel threads resulting
in kernel threads being reused for multiple application threads
over time (akin to LWP re-use in Solaris)?

@_date: 2014-09-17 22:16:55
@_author: Viktor Dukhovni 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Why should the key be per-process, and not system-wide?  Instead
each process could simply have a non-secret key identifier, allowing
periodic key rotation.  At which point, is this really much better
than existing encrypted swap devices?

@_date: 2014-09-19 05:33:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] CloudFlare reinvents crypto offload 
[ I guessing you're not actually confused, merely sarcastic, but for
  the record, in case anyone else is, my best guess of the idea is below. ]
There's no magic here, just delegation of handshake signing not to
an HSM, but to the back-end web-server, so that the client's (bank's)
key is not shared with CloudFlare.  Presumably CloudFlare deflects
most of the attack traffic before forwarding a manageable rate of
signing requests to the client, and relies on client session caches
to further limit the rate of signature requests, so that the client
only sees a fraction of the load.
This would work even better if the client could sign a short-term
proxy certificate enabling CloudFlare to do the crypto locally.
However, I am led to believe that proxy certificate support is
still rather thin...
With DANE + DNSSEC, the client could just CNAME their TLSA RRs to
CloudFlare's TLSA RRs, but DNSSEC is not terribly widely deployed
yet, and in any case IIRC you're on the record as a DNSSEC skeptic.

@_date: 2015-08-03 04:35:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] More efficient and just as secure to sign 
This analysis is too naive.  The risk is internal collisions in
the hash function, which might enable extension attacks.  The
Ed25519 construct is resistant against internal collisions and
extension attacks, while SHA-2 is not.
Now of course internal collisions on the full SHA-2 are far from
feasible at present, but not depending on unexpected progress on
that front is reasonable defense in depth.

@_date: 2015-08-03 18:26:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] More efficient and just as secure to sign 
So long as the full hash function remains resistant to internal
collisions, the extra care is not required.  The Ed25519 proposal
however survives failures in internal collision resistance.  It is
a more conservative design.  You might conjecture it to be too
conservative, but that's no excuse for arguing that there's no
added robustness from defending against as yet impractical attacks.

@_date: 2015-08-04 00:54:10
@_author: Viktor Dukhovni 
@_subject: [Cryptography] More efficient and just as secure to sign 
If what the curve is signing is the message hash, a stronger curve
cant't help.  There is merit in constructs that rely on fewer
unproven assumptions about the underlying primitives.
Few of us expect SHA2-256 to SHAKE-256 to fail soon.  Attacking
them may even lie beyond human ingenuity, but history has been on
the side of the pessimists.
In many applications the message size is bounded by protocol
constraints, and hashing twice is not a significant bottleneck.
A typical use might be signing the parameters for an ephemeral key
exchange, where the message size is quite small.  Another is signing
X.509 certificates.  In neither case is it onerous to hash twice.
The real obstacle is existing combined digest+sign IUF (initialize,
update, final) APIs.  If one is to plug the new signatures into a
generic framework, the new algorithm would have to buffer the data.
In some cases libraries also offer an all-in-one primitive that
combines I, U and F, and that would be more efficient with the full
message already in memory, by avoiding memory allocation and copying
So the reason why the proposal might get traction is not the modest
CPU cost, but API impedance mismatch.  We've learned to expect
signing APIs to turn meat into sausage in a single pass, and changing
the model is difficult.
The proposal seems sound on its merits, but may be too difficult
to adopt.  Thus CFRG seems to have decided to preserve IUF, but
internally the signature algorithm may still take H(M) as the
message, and sign that more securely (yes even though it is too
late).  Separately, new interfaces might be made available for
conservative designs that choose to bypass IUF and not bolt the
door after the horse has left the barn.

@_date: 2015-08-07 21:16:27
@_author: Viktor Dukhovni 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
SHAKE-128 is essentially SHA3-256 with variable length output.
SHAKE-256 is essentially SHA3-512 with variable length output.
Not "only", rather "as expected".  The name reflects the collision
resistance, not the output width, because the latter is variable.
Most likely use case is as DRBG, but perhaps also as a keystream
for a stream cipher.
Variable length output d, with security min(128, d/2).  No surprises.

@_date: 2015-08-08 02:47:32
@_author: Viktor Dukhovni 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
The hash functions are the SHA3 ones, the SHAKE functions serve a
different (useful) purpose, and generate variable width output at
the stated security.  NIST is doing the right thing.
If you find the novelty disturbing, I expect you'll get used to it.

@_date: 2015-08-13 15:33:30
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
The history of both RSA and ECC is quite short.  Prime numbers
indeed go back to antiquity, but deep insights into prime number
theory start with Fermat, and the foundations of modern number
theory are laid by Euler, Legendre, Gauss and Dirichlet in the late
18th and the 19th century.  Analytic number theory and group theory
are both 19th century advances.  Around the same time Elliptic
curves are studied by Abel and Weierstress in the 19th century.
And yes, many of the major advances in the arithmetic of Elliptic
Curves are then made in the early to mid 20th centries.
Significant attention to and progress in factoring algorithms
(quadratic sieve, ECM, and GNFS) is quite recent.
So just because the concept of prime number dates back to antiquity,
while Elliptic curves do not, it is I think a false meme that
therefore we have a multi-millenium lead on understanding RSA vs.
ECC.  (It is amusing in this context to note the role of ECM in
factoring composites).
This seems irrelevant, elliptic curves date back to the mid 18th
century, so what?

@_date: 2015-08-13 23:26:23
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
An Edwards curve over the reals is a compact subset of R^2 bounded
away from (0,0).  Therefore, your scaled metric gives the image of
the curve on S^2 a finite diameter, but there are points of infinite
order on the curve when the generator "G" is not a torsion element.
Therefore, any proportionality between "n" and the "distance" of
"nG" from some reference point (pick any continuous metric), fails
for large enough "n".  Thus, before we even consider whether any
of this applies to the discrete case, it seems clear that this must
fail in the continuous case.

@_date: 2015-08-14 04:11:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
Note that, in essence, what you're trying to do is contruct a group
homomorphism from the real Edwards curve to the circle.  If "d" is
not a real square (i.e. d is negative), then the Edwards group law
is a differtiable function of its arguments, and the Edwards curve
is a compact one dimensional "Lie group".
Such a group is necessarily diffeomorphic to the unit circle, via
the "exponential map" (for which you've stumbled into a closed form
via Elliptic functions).  More precisely, if e() is the exponential map on the Edwards curve
mapping real numbers t to group elements e(t), and T > 0 is the
smallest number with e(T) = identity, then we get a group isomorphism
to the unit circle:
where e() is the exponential map (from theory of Lie groups) on
the Edwards curve, and exp() is the exponential map on the unit
circle which (not coincidentally) maps t -> e^{it}.
If G is not a torsion element, and you know a bound for "nG" and
a bound for "n", then sufficiently precise preimages for the
exponential map, log(G) and log(nG) (both known only modulo T) are
in principle sufficient to find "n".
This does not carry over to the discrete case.  A discrete Edwards
curve is not a Lie group, and the functions you're looking for are
the logarithms, which if you had at hand, would trivially identify
any cyclic subgroup of the curve abelian group with Z/nZ (discrete
circle).  These functions of course exist, but that does not make
them computable in practice.

@_date: 2015-08-14 04:21:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
That's backwards, factoring trivially solves RSA.  I'm not aware
of any proof that solving RSA necessarily factors the modulus.
However, there are partial results in that direction:

@_date: 2015-08-14 21:31:31
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
The neutral element is infinity, not 0 (makes sense in terms of
the parallel resistor model).
As for cryptography: point doubling gives: a + a -> 1/(1/a + 1/a)
= a/2.  And (a/n + a) = 1/(n/a + 1/a) = a/(n+1)).
So scalar multiplication is just division.  Not especially useful
for DH! :-)

@_date: 2015-08-17 05:21:07
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
No matrix algebra is needed here.  See my messages from June 30th.
    Message-ID: <20150630145621.GF14121 at mournblade.imrryr.org>
    Message-ID: <20150630182121.GG14121 at mournblade.imrryr.org>
For p = 1 mod 4, the circle group is isomorphic to the multiplication
group in F_p, which is cyclic of order p-1.  For p = 3 mod 4, the
circle group is isomorphic to a cyclic subgroup of order p+1 of
F_{p^2}.  DLP for both is IIRC believed roughly comparable in
difficulty to RSA with moduli of the same size.
That's not so clear.  A reduction of EC to circle arithmetic would
substantially weaken EC, because the primes in EC are much smaller
than the primes in regular finite-field DLP (or degree-2 extensions
thereof).  Also, it is hypothetically possible that EC groups are
easier to attack than the circle group, and we just have not figured
out how just yet.
That's just one dead-end, a failure of naive intuition from "real"
analytic geometry to carry over to the discrete case.  This does
not rule out more sophisticated attacks, based on deeper theory.
Now for the discrete circle, we have computationally efficient
isomorphisms from the circle group to the finite-field DLP problem,
so any successful attack on the circle group is a successful attack
on finite-field DLP with primes of the same size.
For elliptic curves, there is no analogous isomorphism, which is
a feature, not a bug, since we're trying to avoid giving the attacker
a leg up via "smooth bases".
Folks like Adam Langley at Google should be able to provide better
guidance than you'd get from naive analysis of circle groups and
New curves (newer than P256) for EC are under development in CFRG.
The ECDH curves are done IIRC, but the signature scheme is still
under discussion.  Depending on your timetable, you might be better
off going with the new CFRG curves.
There is no published weakness in P256, just implementation pitfalls.
Of course there's no way to know that the curve is not "cooked" by
an adversary with a much deeper theoretical grasp of EC crypto.

@_date: 2015-08-17 17:56:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
The performance cost is ridiculous:
    rsa 1024 bits 0.000467s 0.000022s   2143.0  44570.3
    rsa 2048 bits 0.002530s 0.000074s    395.3  13592.8
    rsa 4096 bits 0.014179s 0.000198s     70.5   5047.2
What sort of numbers do you expect for RSA at 15k bits?  I would
conjecture around 2 signatures per second, and thus entirely
unsuitable for key agreement.  Perhaps still usable for verifying
certificate signatures, but with enough such certificates in a
chain, the chain will exceed TLS message size limits.
For the record I don't see a compelling difference between a 112-bit
work-factor and a 128-bit work-factor, provided the estimates hold
up reasonably well.  Also it seems that memory requirement for the
matrix stage of GNFS for large moduli are quite prohibitive.  Are
the work-factor estimates for large RSA moduli too conservative?

@_date: 2015-08-18 18:15:23
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
I'm afraid this argument is largely misguided.  The security of
Elliptic curves rests on deeper mathematics than mere lack of a
birational equivalence to the circle group.
Such a birational equivalence, if it existed, would of course spell
trouble for EC, but lack thereof just precludes the carrying over
of geometric attacks from continuous to discrete curves.
Even though the Lie group isomorphism of "d < 0" real Edwards curves
to the circle group is no use mod p, we can't immediately jump to
the conclusion that DH on Elliptic curves mod p is adequately
The reasoning (which I did not quote) is much too naive.  You'll
just have to trust experts (not me, I just enough more to know that
I don't know enough) on the security of ECC.

@_date: 2015-08-28 18:48:39
@_author: Viktor Dukhovni 
@_subject: [Cryptography] AES Broken? 
Yes, this matches many significant figures of $\pi \cdot 10^{38}$:
Wolframalpha's free interface displays log_2(pi * 10^38) as:
    127.88476373519208801711541761570483401788 206986169710
(I added a space before the additional digits that show the security
of AES even more precisely).

@_date: 2015-12-03 03:44:15
@_author: Viktor Dukhovni 
@_subject: [Cryptography] JSC notifies on introduction of National 
Original:            
      ,  ,  
     ,   ,  
          .
         
          
       .
My translation:
    According to the Law network operators are required to implement
    use of the [national] security certificate for transmission of
    traffic which employs encryption-capable protocols, with the
    exception of traffic, encrypted by cryptographic security
    systems on the territory of the Republic of Kazakhstan.
    The national security certificate will ensure the protection
    of Kazakhstan users when using encrypted protocols to access
    foreign Internet resources.
It goes on to say:
           ,
          
       ,     
       (      iOS/Android,
          Windows/MacOS).
    Kazakh-telekom specifically brings to the attention of users
    that the installation of the [national] security certificate
    must be completed on each customer device that will be used to
    access the Internet (mobile phones, iOS/Android tablets, personal
    computers and notebooks running Windows/MacOS).

@_date: 2015-02-03 04:13:36
@_author: Viktor Dukhovni 
@_subject: [Cryptography] best practices considered bad term 
I am currently involved in two starkly different OSS projects (also
others, but these two are extremes on the security track-record
spectrum), the first since 2001 and the second only recently.
In the first project:
    * The project founder is still its primary maintainer and takes
      pride of ownership in the project.
    * The project founder believes that security and code quality
      considerations trump features.  Contributions of code that
      addresses too narrow a problem, is poorly documented, or does
      not meet the coding standards are not accepted.  If a requested
      feature is sufficiently compelling, new code is written to
      implement it when time permits.
    * A lot of attention is paid to internal interfaces, which
      comprise the bulk of the code.  The code that implements
      features rides on top of carefully crafted libraries of
      general purpose code and is not polluted with platform-
      specific   All internal interfaces are documented.
    * Preliminary documentation is, when appropriate, developed in
      advance of code for complex features.  Undocumented code is
      never released to the public.  All public interfaces are
      documented.
    * Snapshot releases are made often and expected to be production
      quality, what distinguishes  production releases is interface
      stability.
    * Official releases happen annually, and 4 official releases
      at a time are supported with bug-fix patches.  Patches for
      official releases do not add new features.  Only bug fixes
      go into official releases, and backwards compatibility is
      expected to be the rule across official releases, not just
      patches.
    * No features are developed to meet commercial contracts.
    * The project has never had a major security vulnerability,
      and the minor of minor ones is remarkably small.
In the second project:
    * The project founder has not been involved in the project
      for a long time.
    * For quite some time no project member has been the primary
      gatekeeper with the authority to reject poorly implemented
      or documented contributions.  This project is more democratic
      than the first.
    * The project has been feature driven, with contributions
      accepted based on their functionality first, and code
      quality considerations at best second.  Many features
      lack both internal and user documentation.  Commercial
      contracts work has driven the adoption of some features.
    * Releases have been infrequent, incompletely tested code
      accumulates over time, and stable releases introduce many
      features for the first time.  There was until recently poor
      separation of feature and bug-fix releases.
    * With nobody owning the overall project architecture, feature
      contributions have avoided refactoring internal code for the
      long-term health of the project.  Thus the internals have
      been somewhat neglected, with too much fragile low-level
      logic in the high-level feature code.
    * The project has a long and continuing history of security
      issues.
The above lists of contrasting project properties are not exhaustive.
So indeed, just being open-source does not ensure security, "many
eyes make all bugs shallow" has not worked out so well.  You need
the right eyes at the right time.
Open source just means that there are likely fewer secret "features"
that address the needs of a constituency other than the users.
Whether any given open-source project produces generally secure
code requires some scrutiny of the development practices of that

@_date: 2015-02-27 03:25:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] information, Shannon, and quantum mechanics 
IIRC Quantum computers (Grover's algorithm) are hypothesized to
reduce the classical $2^n$ search time for brute-forcing symmetric
keys to $2^{n/2}$.  Not sure how that plays into the analysis, but
perhaps the estimates you quote assume that the attack is essentially

@_date: 2015-01-14 00:13:08
@_author: Viktor Dukhovni 
@_subject: [Cryptography] SSL combines two aspects of communication 
The introduction to the below document is not SMTP specific:
    Regardless of the key length?  Not terribly credible.  Once the
paranoia sets in, people seem to become lot more gullible about
claims that validate their fears.
Only if RSA key transport is used with a key length they can crack.
Your system is likely needlessly complex.  If they want your traffic,
they'll attack something other than the crypto.  Enable forward-secrecy
on the server and client, either 2048-bit DHE or 256-bit ECDHE.

@_date: 2015-01-14 07:01:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Summary: compression before encryption 
The key point being "collections".  One can compress sets of
substantially random short strings much more efficiently than
sequences of same.
A set of 16 distinct 5 bit strings can be compressed to a 32-bit
bitmap in which 16 bits are 1 (this is not claimed to be the optimal
A 16 element sequence of uniformly randomly selected 5-bit strings
is an 80 bit uniformly random string, and takes 80 bits to represent.
So the set requires at most 32 bits, while the sequence 80 bits.
A more precise claim would be that n-bit strings (sequences) with
n bits of entropy are not amenable to lossless compression.  Sets,
as you note, lead to a very nice design for compressed CRLs.
When considering compression of encrypted data, one is generally dealing with a stream (sequence) not a set.

@_date: 2015-01-29 02:56:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Introduction to EC that is actually an 
Pick a nice curve modulo a modestly sized prime, like     y^2 = x^3 + x + 20 (mod 41)
Then a base point g=(3,3) generates the following cyclic group of
(prime) order 53 (xn and yn are the coordinates of g^n or n*g if
you prefer):
     n   xn yn
    --   -- --
     0   infty
     1    3  3
     2   34 30
     3   24 25
     4   23 19
     5   14 21
     6   26 19
     7   30 21
     8   13  4
     9    0 26
    10   33 22
    11    9 26
    12   38 20
    13   16 27
    14    6 23
    15   40 10
    16   19 25
    17   10 13
    18   20  2
    19   39 16
    20   32 15
    21    7  1
    22   21  6
    23   25  7
    24   11  3
    25   27 38
    26   29 24
    27   29 17
    28   27  3
    29   11 38
    30   25 34
    31   21 35
    32    7 40
    33   32 26
    34   39 25
    35   20 39
    36   10 28
    37   19 16
    38   40 31
    39    6 18
    40   16 14
    41   38 21
    42    9 15
    43   33 19
    44    0 15
    45   13 37
    46   30 20
    47   26 22
    48   14 20
    49   23 22
    50   24 16
    51   34 11
    52    3 38
Any of the 52 non-identity points is as good a generator (base-point)
as any other.  Just multiply all the logs by the reciprocal of the
new base-point's logarithm taken mod 53.  For example 15*46 is 1
mod 53, so if G is changed to (40,10) all the logarithms are
multiplied by 46 mod 53 (shuffling the table).  The sequence of x
values with non-zero logs is a palindrome, with the y value changing
sign as expected.
Each x-value appears twice, if we take only the first
half of the list, and sort by x we get:
 log x    x  y
 -----   -- --
     9    0 26
     1    3  3
    14    6 23
    21    7  1
    11    9 26
    17   10 13
    24   11  3
     8   13  4
     5   14 21
    13   16 27
    16   19 25
    18   20  2
    22   21  6
     4   23 19
     3   24 25
    23   25  7
     6   26 19
    25   27 38
    26   29 24
     7   30 21
    20   32 15
    10   33 22
     2   34 30
    12   38 20
    19   39 16
    15   40 10
Which shows a rather non-uniform mapping from x to log x.

@_date: 2015-07-02 21:05:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Amazon releases open source cryptographic module 
The code is easy enough to read, the provenance is not so important.
What matters more is that as yet, the library is incomplete:
            /* At present s2n is not suitable for use in client mode, as it
         * does not perform any certificate validation. However it is useful
         * to use S2N in client mode for testing purposes. An environment
         * variable is required to be set for the client mode to work.
         */

@_date: 2015-07-03 23:15:09
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Amazon releases open source cryptographic module 
There was no audit, and the legacy code quality in OpenSSL is lower.
The s2n code is objectively easier to read and audit.
However, it is still far from complete.  We'll see how large the
code base is once it is complete.
The limitations of C have not been a problem for Postfix.  You can
write Fortran in any language.  Sure some bugs are more common in
C than other languages, but I see plenty of security bugs in Python
code, they're just not buffer overflows.

@_date: 2015-06-01 07:21:29
@_author: Viktor Dukhovni 
@_subject: [Cryptography] open questions in secure protocol design? 
Two is too few in any case.  There needs to be a way to start using
a bleeding edge new algorithm without immediately deprecating the
backwards-compatible legacy algorithm.  So I see 3 as the minimum.
Once support for the mainstream algorithm is essentially universal,
it becomes the legacy, and the "bleeding edge" becomes mainstream.
At that point it becomes possible to introduce a new bleeding edge
In terms of where to start practicing this, I think that DNSSEC is
a reasonable place to start retiring legacy algorithms once we have
a new EC signature scheme from CFRG.  All algorithms prior to 7
should go (and perhaps the GOST algorithms should also be deprecated).

@_date: 2015-06-03 00:18:42
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
Standard definition of infinity:
The axiom of infinity, the axiom of choice, ... are powerful tools
that amplify the power of discrete mathematics.  Provided these
are non inconsistent with more elementary axioms, any results they
prove about elementary arithmetic are correct (have no elementary
You can stop right there, points on elliptic curves over F_p (prime
fields or perhaps their Galois extensions) do not in any sense
"correspond" to points on real-number curves (characteristic 0).
Further-more, unlike circle arithmetic, even with real elliptic
curves addition of points is highly non-monotone in any smooth
parametrization of the curve.
This is simply false.
If we're still talking about real curves, what does "between" mean?
Because there is no computationally feasible way given a*G and b*G
(but not a,b) to determine which of them arises from the smaller
If grandma had wheels she'd be a wagon.
If there are effective attacks on EC, they're not nearly so naive.
These posts would be embarrassing, if only you knew enough to know
what you don't know.  It is best to stop here.

@_date: 2015-06-10 13:26:29
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Please "ERDJ" Congress to keep the FBI from 
One might legislate that nobody ever needs better than 1 part in
10^7 accuracy and set Pi as 113|355 (divide latter by former).
    $ echo 7k 355 113 / p | dc
    3.1415929
Anything more precise is simply compulsive.

@_date: 2015-06-13 13:41:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
But d=0 is NOT an Edwards curve, the d=0 degeneracy makes it a very
different beast.
Since circle arithmetic is just multiplication of complex numbers
on the unit circle the relevant formula is:
It is easy to verify that if norm((a,b)) = (a^2 + b^2) = 1 =
norm((c,d)), mod p or not, then also norm((a,b)*(c,d)) = 1.  The
inverse of any point (a,b) is (a,-b) and the identity is (1,0).
In this context, a useful mapping from the circle mod p to the
characteristic 0 (rational) circle needs to preserve the product,
(that is it needs to be a group homomorphism), otherwise the mapping
is of no use in mapping the DLP from (mod p) to the rationals.
Since there are only finitely mod p points, each point has a finite
order, i.e. (x,y)^n = 1 for for some n.  The same would have to be
true for any image of (x,y) under a homomorphism into the rational
circle.  However, there are only four rational points on the circle
with finite order:
all other rational (p, q) with p^2 + q^2 = 1 have infinite order.
Otherwise, there'd be a primitive n^th root of unity for n > 2,
with rational real and imaginary parts, but all such numbers
have degree 2 over the rationals.
So the only viable homorphisms are into Z_4, and cannot be injective
for all but the smallest primes.
So this approach is a dead-end even for the circle, which is much
simpler than an actual Edwards curve with a non-zero "d".

@_date: 2015-06-16 02:08:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
Note, that I glossed over the details for the "n=3" and "n=4" cases.
For prime p, the p^th roots of unity lie in an extension of degree
p-1, not p.  Thus cube roots of unity have degree 2, but these are well known
to have coordinates (-1/2, +/- sqrt(3)/2) which are not both
Similarly, we have to handle n=4 (which gives the +/- i  points).
After that we either have larger odd factors or higher powers of
two which give a degree of at least 4.
I don't know whether the proof outline is "standard" or not, the
result is surely well known.  It just seemed like the simplest way
to rule out additional rational points of finite order on the
unit circle.
If you abandon the requirement of a homomorphism, yes in principle
you could map each n*G in the mod p circle to its smallest non-negative
integer discrete logarithm.  No need for the unit circle at all.
At which point you get to read off the logarithm.
The problem is doing this *effectively* in practice.  If we posit
a log-table oracle, then we solve DLP.
It seems that your idea of mapping to the unit circle where discrete
logs may be viable for geometrical reasons boils down to proposing
an oracle.
Without claiming that DLP on the mod p circle group is secure (it
likely is not), I am willing to guess that any attacks are not
based on a concrete correspondence with the characteristic 0 circle.
A quick Google search, shows that the group has order p-1 if p is
1 mod 4, and p+1 if p is 3 mod 4.  The order of the group is always
divisible by 4.  (Not surprising given the subgroup corresponding
to the 4 rational points on the unit circle).
    Perhaps someone else on the list knows more about these (C_p)

@_date: 2015-06-30 14:56:21
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
That said, with a bit more thought (and a hint from Watson Ladd I
should not have needed), there is a useful homomorphism if we're
willing to abandon the rational circle for multiplication in a
finite field.
If p == 3 mod 4, choose $i^2 = -1$ in a quadratic extension $F_{p^2}$.
If p == 1 mod 4, choose $i^2 = -1$ in the base field $F_p$.
In either case the mapping (x,y) -> (x+iy) is an injective homomorphism
(monic) from the circle group into the multiplication group of the
corresponding finite field.  In fact for the 1 mod 4 case the
mapping is an isomorphism.  So DLP in the 1 mod 4 circle group is identical to DLP in F_p (plus
the cost of finding a square root of p-1).
DLP in the 3 mod 4 circle group is a sub-problem of DLP in F_{p^2},
which is subject to essentially the same index calculus attacks as
F_p, but now the smooth factor base uses gaussian integers.
None of this helps with d != 0 (actual Edwards curves), where the
above homomorphisms don't exist.
If DLP for Edwards curves uses ~256-bit primes for conjectured ~128
bit security, DLP on the circle would need around 3000 bits for a
similar security level.

@_date: 2015-06-30 18:21:22
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
Can you clarify the question?  It is fairly easy to find points on
the unit circle mod p (if that's what you're asking for):
    1. Standard birational equivalence:
       whose inverse is t = (y+1)/x.  For p == 1 mod 4, choose any
       t in F_p with t^2 != -1, or t = infinity which maps to (0,
       1).  For p == 3 mod 4, choose any t in F_p or infinity.
       For example, if p = 37 choose t != +/-6, say t = 5, then
       (x, y) = (10, 24)/26 = (26, 18).  26^2 + 18^2 = 1000 which
       is 1 mod 37 (since 111 = 3 * 37 so 999 = 27 * 37).
    2. For p = 1 mod 4, and $i^2 = -1$, the reciprocal of r = (x + iy)
       is 1/r = (x - iy).  Therefore, take any non-zero residue $r$,
       then compute (r + 1/r) giving $2x$ and $r - 1/r$ giving $2iy$.
       Then just compute (x, y) = ((r+1/r)2, i*(1/r-r)/2).  For
       example, with p = 37, and i = 6, take r = 5, then 1/r = 15,
       giving x = 10, and y = 6*5 = 30.  Once again we get
       x^2 + y^2 = 1000 or 1 mod 37.
As for finding rational points on the ordinary unit circle that
somehow "correspond" to (x,y) mod p, it is still far from clear
what correspondence you have in mind...

@_date: 2015-03-03 18:53:30
@_author: Viktor Dukhovni 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Yes, the traditional ASCII LDH domain parts of email addresses are
case-insensitive.  With IDNA 2008 and EAI, U-labels are simply
required to be lower case.  IDNA Punycode encoded A-label are then
also lower case, and case-folding of A-labels that happen to contain
an upper-case ASCII letter is fine (should generally be unnecessary
if the A-labels are generated by fully compliant software and not
subsequently mangled).
How and whether a user-agent chooses to convert a non-ASCII domain
name provided by a user to a (possibly corresponding) lower-case
U-label is left to the user-agent, with strong warnings against
naive automatic case folding by software that does not know
the language context.
Case-insensitive local parts are a legacy of gatewaying email
to/from mainframe computers (BITNET) where all addresses were
upper-case and email lists kept in mainframe databases where they
were generally also upper case.  This was then further entrenched
by LDAP schemas that define case-insensitive matching rules for
email addresses (and often define their encoding as IA5STRING
complicating the transition to UTF-8).
SMTP however has never *required* that local parts be case-insensitive,
and instead requires relays to preserve the case of localparts,
leaving the option of case folding to the destination system.
If you choose to define ASCII locaparts as case-insensitive in
DIME, you'll probably not run into too many problems.  If you
attempt to extend this to UTF-8, things get much more complicated,
and you're not supposed to do that.
If you're building a brand new protocol suite, it may as well be
UTF-8 by definition.  In practice most users will want ASCII addresses,
as the usability of non-ASCII addresses will be limited to just systems
that support DIME or EAI (Postfix 3.0 has usable if not yet mature EAI
That works for domains, but there is no standard ASCII encoding of
non-ASCII localparts.  (There probably should have been, and perhaps
will yet be such an encoding).
If this is to be useful on mobile devices that might want to download
message bodies sans attachments, then probably yes, and in that case,
indeed encrypted message-part metadata.
One way to do that is to use a modified format in which the message
starts with a "MIME skeleton" (all the headers for the body and
attachments) with the content of all leaf parts replaced by an
offset+length from a stream of content blobs appended to the message.
The boundaries between invidiual content parts should not be directly
apparent.  You could chunk the content encoding to allow integrity
verification of incompletely downloaded parts.
That way the client can retrieve the complete MIME skeleton with
placeholders for the remote message parts, then download whichever
parts are small enough or are explicitly requested.
The MIME skeleton would be encrypted as a single object.  One could
encrypt just the primary headers separately from all other MIME
headers if there is a concern about edge cases in which the MIME
metadata is excessively large.

@_date: 2015-03-07 19:33:44
@_author: Viktor Dukhovni 
@_subject: [Cryptography] FREAK attack 
In the context of Curve 22519 ECDHE, what are the WF256 keys?  Are
you suggesting a combination of static ECDH based on the server's
EC-448 certificate with Curve 25519 ECDHE?  Would it not be simpler
to just use EC-448 for the ephemeral key exchange also?

@_date: 2015-03-12 06:46:19
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Digital Certificate Forensics: Clinton Email 
Since we're talking about email, one must remember that email isn't
just stored on the server in question, it travels via SMTP between
that server and correspondents in other domains.
The MX service for clintonemail.com is provided by third-party
service that likely handles pesky anti-spam/anti-virus filtering
before forwarding the email onward.
The transmission via SMTP is not necessarily encrypted, and even
when TLS is used, it is generally opportunistic and unauthenticated.
So my conclusion is that Venafi are doing some unsurprising
opportunistic marketing, and the factual contribution to the story
has little merit.
TLS certificates don't protect data at rest (a key risk for email),
and very rarely protect SMTP traffic against active attacks
(especially before June 2013).

@_date: 2015-03-17 18:32:37
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Kali Linux security is a joke! 
All Debian-style repositories use HTTP, not HTTPS which makes them
to mirror.  The Release files are GPG signed by the distribution
maintainers.  The distribution keys should be part of the base
installation media.  Of course if you bootstrap via PXE, your MiTM
attack starts there (the turtle at the bottom of the stack).
I would take some time to study the "apt" security model.  It is
not perfect, but the use of http is not a significant problem.

@_date: 2015-03-18 17:36:16
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Kali Linux security is a joke! 
Sending a TCP RST is just as easy for HTTPS as for HTTP.  TLS does
not authenticate the signalling at the transport layer.
For a user who's just comparing the MD5 checksum of the software
downloaded, with the MD5 checksum published by the maintainers and
not using anything stronger, MD5 is just fine.  Second preimages
for MD5 are still difficult IIRC.
If there are significant security issues with Kali, publishing MD5
checksums on their website and offering repositories via HTTP are
not the droids you're looking for.

@_date: 2015-03-21 21:05:15
@_author: Viktor Dukhovni 
@_subject: [Cryptography] FFS 
It's too late to change history.  Forward Secrecy will do.

@_date: 2015-03-27 17:50:38
@_author: Viktor Dukhovni 
@_subject: [Cryptography] D-Wave, RSA, and DLP 
No such thing follows.  Look at page 3 of:
        The equations obtained from adding the columns in the multiplication
    table are then:
    and when the simplification rules are applied automatically by
    a computer program, most pi and qi are already determined, and
    the result is this set of equations:
and guess how what fraction of large semiprimes admit the radical
algebraic simplifications (performed by a classical computer) to
equations such as equations 16/17/18.  What is the run-time of the
portion of the algorithm that simplifies the equtions?
Notice the comment that for the system shown the simplifications
are due to the fact that:
    "In fact, it turns out that the product of any two numbers
    differing at only 2 bits will lead to the equations: ..."
So this is a highly-specialized result.  No need to panic (yet).

@_date: 2015-05-04 21:07:42
@_author: Viktor Dukhovni 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Because they don't need to verify anything.  The domain is registered
with them, the account holder is the domain owner by *definition*.
The problem they have is not verification of ownership, rather it
is an account security problem.  Compromise of registrar login
accounts (social engineering password resets, ...) is the problem
to solve.
This is rather different from the much weaker "verification" (none
really) performed by CAs for DV certs.
Yes, there is still a problem to address, for many registrars, the
account security is not very strong.  Domain owners may shop around
for a registrar with better practices when protecting a high-value

@_date: 2015-05-14 20:25:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Any S/MIME or PGP for normal people 
S/MIME on MacOS/X (Mail.app) works rather transparently after
initially accepting a cert as valid for a given correspondent, that
trust setting is saved in the user's Keychain.  Similarly the user's
signing/decryption key is saved in the Keychain.
After that you just send and receive mail as usual.  The remaining
key management problem is enabling users to generate keys or providing
them with PKCS files to import.
One can also deploy trusted CAs into the System Keychain.
IIRC Windows S/MIME support tends to frown on TOFU PKI, and I don't
think that with Outlook et. al. it is possible to trust a given
cert for a given correspondent.  There one does need a corporate
CA, and I don't recall how easy it is to sign or decrypt mail.

@_date: 2015-05-30 08:07:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
The RSA cryptosystem is not that much older.  And the study of the
arithmetic of elliptic curves dates back to Abel, Weierstrass, ...
Your doubt are is not evidence of lack of effort.
This is not a meaningful limit to take.
This does not simplify the arithmetic.  When the characteristic is
1 mod 4, and d is not a square, there is no such z for any x,y on
the curve.
Compute what?  EC point addition is a rather non-trivial transformation
on the x, y (and possibly your z) coordinates.
Effectve reduction of arithmetic in a cyclic group to modulular
addition is essentially solving the DLP for that group.  Good luck
doing that for general elliptic curves.
The claims that the keys are too short is baseless.
Always with any cryptosystem, but not particularly more for ECC
than for RSA or other well-designed systems.

@_date: 2015-11-12 09:51:01
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Post Quantum Crypto 
On the other hand, this works now, with existing code, and certificates
(at least DV) are free.  While a hybrid certificate would require
new CA software, new PKIX extensions, new TLS standards and a new
SSL/TLS software stack.  And what happens when NTRU is found wanting,
a certificate with 3 public keys (always rotated concurrently)?
Support for multiple certificates is considerably simpler to deploy.
    0 + 0 = 0
    There is no such limit.
    For a while, we'll likely end up with fewer roots, that would
    be a feature, not a bug.
    I would assume that for web services LE would help automate
    provisioning both algorithms.  It should also be noted that
    your 5 year estimate for scalable QC is rather more aggressive
    than expert consensus.

@_date: 2015-11-13 02:51:26
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Post Quantum Crypto 
With OpenSSL it is possible to field separate concurrent certificates
for ECDSA, RSA and DSA.  If NTRU came along, it would be possible to
start deploying and testing NTRU certificates and ciphersuites.
You'd not be protected agaist a "downgrada" to RSA once RSA is
sufficiently vulnerable to enable impersonation.  At that point
we'd have to promptly abandon RSA.
Postfix works with multiple certificates out of the box, and has done
so for over a decade:
            No "studies" are required, this is not an interoperability issue,
rather it is a question of application configuration file semantics.
Some server software might not expose the feature to its users, or
might use a TLS library that has no such feature.
This is a real requirement of the TLS protocol, if more roots were
needed, Apple would make appropriate accomodations.

@_date: 2015-11-16 07:58:09
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Fwd: Re:  Post Quantum Crypto 
Quantum computers don't appear to require anything more than routine
non-relativistic quantum mechanics.  As such making one appears to
be a (very difficult) problem in applied physics, applied mathematics,
materials engineering and nano-fabrication.  I've not seen any
evidence that building scalable QC or proving it impossible requires
new insights into theoretical physics.  The Quantum Mechanics of ordinary materials (found outside of stars
and particle accelerators) has not changed in decades.
Does anyone know of any credible references to suggest that QC is
bottle-necked on progress in Physics beyond QED (let alone beyond
the Standard Model)?
It rather seems you're just making this part up.

@_date: 2015-11-19 04:34:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] A new, 
Here's the bottom line as I see it.  Encryption algorithms that
are taken seriously don't get introduced via posts spamming links
to code on either this, LKML or the IETF TLS WG lists.
Serious algorithms are introduced mathematically or via block
diagrams of the changing internal state and the resulting output.
Not via sample code.
If you don't want to look like a crackpot, publish a peer-reviewed
academic paper in a reputable journal or a Crypto conference
If you really want to be taken seriously, publish novel improved
attacks of established algorithms.  Otherwise all we know is that
you've designed an algorithm you don't know how to break, but you
need to demonstrate that you have some skill at breaking algorithms,
before proposing designs of your own.
In other words, further posts here or the other lists will only
increase the perception that you're a crackpot, so it is best to
stop now, and direct your energies more productively.

@_date: 2015-11-21 15:40:46
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Chrome dropping DHE (was Re: [FORGED] Re: 
Nothing interoperable.  Until TLS 1.3 (i.e. not at this time), the prime sizes
are not negotiated.  If the server chooses DHE, you either accept its prime or
close the connection and retry without DHE.

@_date: 2015-11-22 05:54:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Chrome dropping DHE (was Re: [FORGED] Re: 
Seems unlikely, rather the protocol is what you'd expect, the public
key of the server signs the key exchange.
In TLS 1.3, the DH groups are standardized, and the client advertises
its supported groups.  That way it can avoid advertising weak
groups, and servers don't have to use "lowest common denominator"
primes to stay interoperable.
There is no, and never was, any "Static Agreed" that got discarded.
You've never clearly explained what "Static Agreed you had in mind".

@_date: 2015-11-23 16:48:55
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Dan Bernstein has a new blog entry on key 
See also:
        "Minimalism in Cryptography: The Even-Mansour Scheme Revisited"

@_date: 2015-11-23 18:23:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Dan Bernstein has a new blog entry on key 
The other key can simply be the same for every block, no need for
Same K_1 for every block.  The DESX trick is not as cheesy as it
might seem.  The DJB attack fails provided no single K_1 is shared
by many K_2's.  (See the "Even Mansour" paper).

@_date: 2015-11-24 19:09:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Dells are shipping with a rogue root level CA 
I think of LE as opportunistic security for HTTPS.  Sites that were
in the clear before now resist passive monitoring.  LE continues
the illusion of security of DV certificates.  What's new is the
combination of the right price with automated enrollment.
DV is essentially leap of faith as a public service.

@_date: 2015-10-27 01:42:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Other obvious issues being ignored? 
The main security issue in software systems is not the difficulty
of writing reliable cryptographic code (which is necessary, but
far from sufficient).  Rather, by far the more important issue is
just writing safe code to handle the usual marshaling, unmarshaling
and processing of untrusted data.
Yes, cryptography introduces a minor twist in the form of a need
for secure erasure of keys, but this twist is rather easy to
accomplish.  The much more difficult issues around optimizing away
unspecified overflow behaviour for signed types is a problem for
all C code, not just cryptography code.
So if there is to be a community drive to implore the compiler
developers to relent on some optimizations, that drive should
represent a community broader than just cryptographers.
Implementing crypto already requires extraordinary skills beyond
those of most mortal developers, and if it were just the few super-men
who work on crypto that would have to work a bit harder, the compiler
developers might well be argued to be making a sensible trade-off.
The real issue is that that the optimizations expose all developers
to risks, not just cryptographers (who largely know where the mines
are, even if not always successful in stepping over them).
A C specification that lays traps for intuitive implementations of
basic sanity checks is I think problematic.
I know I depend on undefined unsigned overflow in Postfix (in the
case I am about to describe this is a matter of protocol correctness
not security).  Postfix converts decimal strings representing
message sizes to an "offset_t" since this is the signed data type
for the size of stored files.  There is no defined unsigned version
of "offset_t".  We also restrict ourselves to pre-C99 dialects of
C, since Postfix still supports rather dated systems that lack
I am the author of the code that handles this conversion:
    off_t   off_cvt_string(const char *str)
    {
    }
This code assumes that addition of positive signed integers, either
produces a larger signed integer, or else overflows to a smaller
signed integer, and that this can be tested.
A sufficiently "clever" compiler could optimize this away, since
every test depends on signed overflow.  So far we've seen no compiler
that clever, but this could change.  If some day some SMTP server advertised a message size limit in
excess of 2^31 or 2^63 (!) bytes, the Postfix SMTP client might
then misparse the server's message size limit and bounce mail that
fits well within that limit.  This possibility seems remote, and
we have better things to spend our precious time on fine-tuning
than this function, so UB it remains.
Yes, now I've outed myself in public as a UB-perpetrator, I could
perhaps repent and write a portable solution in C (that predates
C99).  But, this is unlikely to happen in the near future.  More
important things to do...

@_date: 2015-09-03 15:44:13
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Introducing the phone-directory certificate 
For " here in the US, I get just "
as a SAN, but for "google.com" the list has "just" 50 entries,
which is a great deal shorter than your impressive 502!  What
transport address and SNI hint (if any) got you the phone directory?
    DNS:*.android.com
    DNS:*.appengine.google.com
    DNS:*.cloud.google.com
    DNS:*.google-analytics.com
    DNS:*.google.ca
    DNS:*.google.cl
    DNS:*.google.co.in
    DNS:*.google.co.jp
    DNS:*.google.co.uk
    DNS:*.google.com
    DNS:*.google.com.ar
    DNS:*.google.com.au
    DNS:*.google.com.br
    DNS:*.google.com.co
    DNS:*.google.com.mx
    DNS:*.google.com.tr
    DNS:*.google.com.vn
    DNS:*.google.de
    DNS:*.google.es
    DNS:*.google.fr
    DNS:*.google.hu
    DNS:*.google.it
    DNS:*.google.nl
    DNS:*.google.pl
    DNS:*.google.pt
    DNS:*.googleadapis.com
    DNS:*.googleapis.cn
    DNS:*.googlecommerce.com
    DNS:*.googlevideo.com
    DNS:*.gstatic.cn
    DNS:*.gstatic.com
    DNS:*.gvt1.com
    DNS:*.gvt2.com
    DNS:*.metric.gstatic.com
    DNS:*.urchin.com
    DNS:*.url.google.com
    DNS:*.youtube-nocookie.com
    DNS:*.youtube.com
    DNS:*.youtubeeducation.com
    DNS:*.ytimg.com
    DNS:android.com
    DNS:g.co
    DNS:goo.gl
    DNS:google-analytics.com
    DNS:google.com
    DNS:googlecommerce.com
    DNS:urchin.com
    DNS:youtu.be
    DNS:youtube.com
    DNS:youtubeeducation.com

@_date: 2015-09-24 02:58:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Yet another dumb crypto system 
Don't waste time on this.
The y^2 = 2y reduction rule is not so good.  This means that
which makes the set of non-zero polynomials not a group under
multiplication.  To avoid such problems you'd want x and y to be roots of distinct
irreducible quadratic polynomials.  This then turns your system
into a degree 4 Galois extension of F_p.
DLP for Galois extensions is not a new problem.

@_date: 2015-09-25 03:24:26
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Yet another dumb crypto system 
For the record the degree would of course be 2 not 4.  For odd p,
all quadratic irreducible polynomials split once the F_p is augmented
with a square-root of any non-square element.  (And for p=2, there
is only one quadratic irreducible polynomial over F_p in the first

@_date: 2015-09-30 15:40:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Why is ECC secure? 
The existence of the "exponential map" for compact one-dimensional
Lie-groups (such as Edwards curves, at least for d < 0) is not at
all surprising.  The "exponential map" exists for *all* Lie-groups,
and yields a group homorphism from the tangent vector space at the
identity under addition into the group.
In the special case of compact one-dimensional Lie-groups the
exponential map is necessarily a group isomorphism with the real
circle (the exponential map is periodic with some period T).
It is likely feasible to compute a local inverse of the exponential
map (near the identity element) with enough precision to make
discrete logarithms practical on Edwards curves over the real
numbers (find $n$ given $nP$ for some base point $P$).
But this applies only to curves over the reals, which are not
terribly relevant to cryptography.  It does not carry over to curves
over prime fields (or Galois extensions).
It still seems like you're ignoring the lack of a generic correspondence
between the continuous and discrete cases.  Yes *some* things work
the same way, but important distinctions remain.

@_date: 2016-04-17 07:02:46
@_author: Viktor Dukhovni 
@_subject: [Cryptography] At what point should people not use TLS? 
This fails to account for the fact that a single cached session is
generally used for multiple resumptions.
The solution, already widely practice, is a reasonably short cap
the lifetime of server session state.
Recommended server-side configuration in Postfix is no server-side
session cache, with all state stored on clients via session tickets.
The server instantiates a new session ticket key once an hour, and
discards a previous key an hour after that.  So server compromise
yields at most 2 hours of sessions.  If you can generally detect
and stop successful intrusion or long-term-key compromise in well
under two hours, and the additional disclosure window from extant
session keys is not acceptable, you probably need to build your
own silicon, and run a formally verified stack from the bare metal
on up.
For the rest of world, session reuse is a non-problem that is
fashionable to get all worked about.

@_date: 2016-04-18 21:59:54
@_author: Viktor Dukhovni 
@_subject: [Cryptography] How to get certificates on email server? 
Make that:
    $ hostport=smtp.example:587 # Season to taste
    $ (sleep 2; printf "QUIT\r\n") |
Which dumps the entire chain into "chain.pem".
That's for STARTTLS on port 25 and 587.  For "smtps" on port 465
drop the "-starttls" option from the s_client(1) command.
Additional tools in this space include "swaks" and "posttls-finger
-C", with the latter not necessarily available with your vendor's
Postfix release, some don't include it in their binary packages.
The posttls-finger source is available with Postfix 2.11 and later.
        The binary is in bin/posttls-finger, but is not automatically

@_date: 2016-04-19 17:55:59
@_author: Viktor Dukhovni 
@_subject: [Cryptography] How to get certificates on email server? 
For most users, their email provider is one of the large behemoths
whose certificate chains are rarely if ever invalid.
For desktop MUA users, many an MUA will provide a built-in method
to inspect the chain.
Only a negligible minority of smartphone users with lightweight
UIs would even know what a certificate is, or why they'd care to
look at a certificate chain.
If it doesn't just work, they either "click OK" or sometimes wait
for the problem to go away.
Essentially none.  Security for the masses needs to just work.
Like it does with WhatsApp, Facetime, ...
My audience is MTA administrators, they are not typical end-users.
The "posttls-finger" utility and the "postfix tls ..." helper in
Postfix 3.1 make it easier for technically literate system
administrators to deal with crypto plumbing.
Postfix is a customizable component not a complete system.  For
an easy to manage turnkey system consider "mail in a box":
    As for debugging tools, if your MUA is not sufficiently geek-friendly,
do take a look at "posttls-finger" or "swaks".  The first is best
if you're operating an outbound MTA, and want to trouble-shoot TLS
issues with remote peers using a tool that behaves largely like
the real Postfix SMTP delivery agent.  The second is more likely
to be useful for troubleshooting problems between MUAs and MSAs,
by emulating the MUA including SASL logings, sending messages, ...

@_date: 2016-04-19 18:14:30
@_author: Viktor Dukhovni 
@_subject: [Cryptography] How to get certificates on email server? 
Forgot to mention one thing, this dumps the certificates sent by
the server, not any chain that may or may not have been constructed
with the aid of those certificates.
To verify that chain:
    $ trusted=trust-anchors.pem		# Root CA certs in one file
    $ untrusted=chain.pem		# Intermediate certs from server
    $ subject=chain.pem			# Leaf cert from server
    $ openssl verify -show_chain -trusted $trusted -untrusted $untrusted $subject
The "-show_certs" option may require a reasonably current OpenSSL
version, and prints the DNS of the verified chain:
    $ openssl verify -show_chain \
    ee-cert.pem: OK
    Chain:
    depth=0: CN = server.example (untrusted)
    depth=1: CN = CA (untrusted)
    depth=2: CN = Root CA

@_date: 2016-08-23 06:24:13
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
Which factors to: 7 * 443 * 739 * 11003
    $ printf "%d %d * %d * %d * p 16o p\n" 7 443 739 11003 | dc
    25214903917
    5DEECE66D
    $ printf "%d %d * p 16o p\n" 7 3602129131 | dc
    25214903917
    5DEECE66D
Yes, but the second factor is clearly not a prime.  FWIW, I used
    to do the heavy lifting, presumably it applies ECM, which handles
numbers of this size with no noticeable delay.

@_date: 2016-08-24 05:04:27
@_author: Viktor Dukhovni 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
Postfix and Tcl both contain internal safe string libraries,
(VSTRINGs and Tcl_DStrings respectively) and use them consistently,
as do Tcl extensions.  Postfix also has VSTREAMs that do safe I/O
with VSTRINGs).  These libraries also don't suffer buffer overflows.
These libraries also date back to the 90's.  OpenSSH also has had
decent safe buffer management for some time.  Each project rolled
their own, but did a reasonably good job.  Not all projects fared
so well.
Sadly incorporating safer standard facilities into the C library
is a herculean effort.  My take is that the difficulty with C is
not so much the language as the rather minimal runtime.  If the C
library were substantially richer, most programmers would use
safer built-in interfaces rather than write unsafe code, or roll
their own "safe" code badly.

@_date: 2016-12-03 02:32:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL and random 
Surely this is not needed.  The device never blocks except in early
boot.  So the issue is rather moot for the vast majority of applications.
As for Python, it needs to accept crappy entropy for its hash table salt,
if it is to be usable in initrd.
It rather seems that there's good consensus around never block except
perhaps for a few seconds after boot, and for some care in early boot
applications that block the boot process.
We can probably wrap up much of this perma-thread at this point.  To
the extent that much of it may have been non-productive, apologies to
everyone for starting it, I should have known better...
In the mean-time I have some pull-requests sitting in the queue for
Haskell's TLS and X.509 stacks that look like they'll likely get
adopted, and perhaps at some point later I'll get around to the RNG,
but that'll be a while, I have other priorities just at the moment.

@_date: 2016-02-09 16:07:02
@_author: Viktor Dukhovni 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
This is of course nonsense.  We have fully general algorithms
for proving primality, even reasonably performant ones.

@_date: 2016-02-15 23:21:27
@_author: Viktor Dukhovni 
@_subject: [Cryptography] The Mathematical Mesh 
But will it eat the dogfood! :-)  Yes, the mesh appears to be a good idea,
I hope it works out.

@_date: 2016-02-19 14:59:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Is Apple correct? 
Does anyone know whether the IMEI number stored inside tamper-resistant hardware
in the phone? How difficult would it be to boot the same custom firmware in another
phone, but arrange for the firmware to see the white-listed IMEI number?

@_date: 2016-02-19 19:43:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Is Apple correct? 
Don't change the IMEI whitelist burned into the LE firmware, change
the IMEI the firmware reads from the phone.  My guess is that
the datapath to the IMEI is not protected, but it is just a guess.

@_date: 2016-02-20 19:48:17
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Apple 3rd Party dilemma 
Except that, in that case, most of the "your own root" installations
would be some attacker's "own root" installations.  In practice,
curated security works better for the vast majority of users.
The vast botnets of Legacy Windows installations are compelling
evidence that expecting the average user to secure a general-purpose
computing platform is unreasonable.  You pay a premium price for
Apple to take care of the details.

@_date: 2016-02-21 21:37:08
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Yes, Apple is correct 
Well, the UID is not the IMEI, and in particular, quoting from the
    Each Secure Enclave is provisioned during fabrication with its
    own UID (Unique ID) that is not accessible to other parts of the
    system and is not known to Apple. When the device starts up, an
    ephemeral key is created, entangled with its UID, and used to
    encrypt the Secure Enclaves portion of the devices memory space.
In particular it seems that it is not possible to write software tied
to a particular UID, since nobody knows the UID.  The IMEI is likely
outside the security boundary (still a guess, no mention of the IMEI
in that document).
This is all at first glance, so could be entirely bogus, apologies if
I'm not paying close enough attention to detail.

@_date: 2016-02-28 00:09:00
@_author: Viktor Dukhovni 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Yes, DJB-crypto:
   ChaCha20 + Poly1305 + Ed25519 + BLAKE2 (DJB derived)

@_date: 2016-01-08 20:44:43
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Verisimilitrust 
The semantics of name constraints are still problematic.  There
are name constraints X.400 on names in the non-existent global
X.400 directory, and name constraints on DNS-IDs.
In various implementations DNS name constraints are not applied to
X.400 names, even though applications often validate peers based
on the CN int he X.400 name.  Which brings us to the second problem,
the code that's enforcing name constraints if often no the code
doing the name checks, so neither knows what the other is doing.
If FF imports a name-constrained .kz root, they can certainly
restrict it to .kz names in FF, but many O/S distributions import
the FF bundle as a "default" vetted trust store, at which point
the name constraints are likely to not be enforced in many cases.

@_date: 2016-01-11 04:18:56
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
OpenSSL includes two libraries:
    * A general purpose crypto library that must be able to handle
      data at rest, including the ability to read 10 or 20-year
      old S/MIME messages, even ones that have MD5 signatures. and
    * An SSL/TLS library that can support a broad or narrow range
      of cipher suites depending on your needs.
Compiling OpenSSL without MD5 breaks the crypto library, and rather
cripples the MD5+SHA1 construction required for TLS 1.0 and TLS
1.1.  It is more sensible to control which algorithms are enabled
for use with SSL/TLS than to try to excise them from the library.
The DEFAULT SSL cipher-site can be tuned at compile time.  Since you
want bleeding-edge, try the master version from Github with:
    $ openssl ciphers -s -v 'DEFAULT:!aDSS:!CAMELLIA:
    ECDHE-ECDSA-AES256-CCM8       TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESCCM8(256) Mac=AEAD
    ECDHE-ECDSA-AES256-CCM        TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESCCM(256)  Mac=AEAD
    ECDHE-RSA-AES256-GCM-SHA384   TLSv1.2 Kx=ECDH Au=RSA   Enc=AESGCM(256)  Mac=AEAD
    ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(256)  Mac=AEAD
    ECDHE-RSA-AES256-SHA384       TLSv1.2 Kx=ECDH Au=RSA   Enc=AES(256)     Mac=SHA384
    ECDHE-ECDSA-AES256-SHA384     TLSv1.2 Kx=ECDH Au=ECDSA Enc=AES(256)     Mac=SHA384
    DHE-RSA-AES256-CCM8           TLSv1.2 Kx=DH   Au=RSA   Enc=AESCCM8(256) Mac=AEAD
    DHE-RSA-AES256-CCM            TLSv1.2 Kx=DH   Au=RSA   Enc=AESCCM(256)  Mac=AEAD
    DHE-RSA-AES256-GCM-SHA384     TLSv1.2 Kx=DH   Au=RSA   Enc=AESGCM(256)  Mac=AEAD
    DHE-RSA-AES256-SHA256         TLSv1.2 Kx=DH   Au=RSA   Enc=AES(256)     Mac=SHA256
    ECDHE-ECDSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=ECDSA Enc=CHACHA20/POLY1305(256) Mac=AEAD
    ECDHE-RSA-CHACHA20-POLY1305   TLSv1.2 Kx=ECDH Au=RSA   Enc=CHACHA20/POLY1305(256) Mac=AEAD
    DHE-RSA-CHACHA20-POLY1305     TLSv1.2 Kx=DH   Au=RSA   Enc=CHACHA20/POLY1305(256) Mac=AEAD
Be prepared for significant interoperability barriers.

@_date: 2016-01-13 18:16:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
You and Henry will be able to use that version of the software safely
on your own private Internet.
For the rest of the world, being able to communicate trumps all other
concerns, and if security breaks communication, security will be turned
off, not communication.
OpenSSL is getting improved, and weaker algorithms are being disabled
in default configurations, and refused when proposed inappropriately,
downgrade issues that are implementation errors (rather than protocol
issues) are getting fixed.
For now, default configurations will be vulnerable to mostly impractical
cost 2^67 attacks.  Upgrading the Internet so that only a negligible set
of peers support only SHA-1/MD5 will take a few years.  In the mean time
these will need to remain supported.  When communicating with a known
to be TLSv1.2-capable peer, applications will be able to disable lower
protocol versions and disable weak signature algorithms.
Removing support for TLS 1.0/1.1 is not going to happen any time soon.
The real world sometimes imposes constraints one might not like.

@_date: 2016-01-19 15:24:32
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
Yes in fact two potential problems, the first is an MD5 rather than
SHA1 or SHA2-256 self-signature, and the second is the weak DH
parameters.  I would assume that it is possible to configure Sendmail
to use a stronger DH group than the 512-bit one that's compiled-in?
    $ openssl s_client -starttls smtp -connect new.toad.com:25
    140187582834340:error:1416D18A:SSL routines:tls_process_key_exchange:dh key too small:statem/statem_clnt.c:1567:
    ---
    Certificate chain
     0 s:/CN=new.toad.com
       i:/CN=new.toad.com
    ---
    Certificate:
    ---
    No client certificate CA names sent
    Server Temp Key: DH, 512 bits
    ---
    SSL handshake has read 1781 bytes and written 7 bytes
    ---
    New, (NONE), Cipher is (NONE)
    Server public key is 2437 bit
    ---
That's Sendmail behaviour, Postfix will send in cleartext when TLS
fails and the message "age" is is more than the queue retry time.
(Barring queue congestion, STARTTLS failure defers on the first
delivery attempt, but delivers via cleartext on the second).
In the mean-time, in OpenSSL 1.1.0-dev, I've added support for
Configuring the build to disable any or all the TLS or DTLS protocols.
The no-tls option disables *negotiation* of all TLS protocols via
the version-flexible TLS_method(), the no-dtls does the same for
DTLS.  The no-...-method variants also disable support for the
corresponding version-specific method.
This does not remove all possible dead code that may result from
disabling all the protocol methods that use it, that will be a
future enhancement.
Enjoy.  There are also many options to disable various crypto
primitives, but removing sha1 or md5 entirely presently breaks the

@_date: 2016-07-11 19:08:41
@_author: Viktor Dukhovni 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
The vast majority of cryptography lies in NP, since verifying the
correctness of a secret key is often rather simple.  To prove that
a typical cryptographic system is "hard", one would have to prove
that P != NP, for otherwise the system is likely insecure (though,
given the asymptotic nature of the distinction, the constants in a
reduction of a problem to P could make the P-time algorithm
Given Goedel's incompleteness theorem, it is quite possible that
a system is secure and yet no proof of said security is possible.
The words mean generally worst-case lower bounds on circuit sizes
and execution time.
That fact was known since at least the early 90's.  Logjam was
merely the observation that servers that support export-grade DH
are vulnerable to a downgrade attack.
This is simply not true.  D-H is hard for large-enough primes.
The fact that it is not hard for 512-bit primes is not new (known
since at least the early 90's).  1024-bit D-H is now perhaps within
reach of nation states, but 2048-bit likely requires new mathematical
breakthroughs that are not known in the public domain.

@_date: 2016-03-02 20:22:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] LibreSSL unaffected by DROWN 
OpenSSL 1.0.x is a stable release with a stable ABI used on many
platforms for many diverse purposes.  Therefore, changes in 1.0.x
are slower than is possible in a release than leaves compatibility
OpenSSL 1.1.0 (days away from beta), which does break compatibility
with 1.0.x, also removes SSLv2, and has many improvements that
LibreSSL does not.  Neither is strictly better, there are surely
things that are better in LibreSSL than in OpenSSL 1.1.0.
Theo is an avid marketer, the reality is a bit more complex.

@_date: 2016-03-03 00:08:46
@_author: Viktor Dukhovni 
@_subject: [Cryptography] LibreSSL unaffected by DROWN 
Somebody would have to build such a thing, but I don't think
that comparing a fresh start with 1.0.2 a reasonable comparison.
After 1.1.0 ships in a couple of months, one can compare one fresh
start against another applying whatever metrics one deems appropriate.
Choosing useful metrics and keeping the data current becomes the difficult

@_date: 2016-03-03 23:48:28
@_author: Viktor Dukhovni 
@_subject: [Cryptography] More Apple news 
"The iPhone is a county owned telephone that may have connected to
    the San Bernardino County computer network. The seized iPhone may
    contain evidence that can only be found on the seized phone that
    it was used as a weapon to introduce a lying dormant cyber pathogen
    that endangers San Bernardino's infrastructure," according to a court
    filing (PDF) by Michael Ramos, the San Bernardino County District
    Attorney.
This seems so far beyond remotely plausible that one wonders whether
the responsible lawyers can be disbarred for blatant fabrication...

@_date: 2016-03-06 17:49:04
@_author: Viktor Dukhovni 
@_subject: [Cryptography] DROWN attack on SSLv2 enabled servers 
Democracy is the worst form of government, except for all the others.
In fact algorithm agility (and protocol negotiation) make it possible
to move on.  OpenSSL 1.1.0 (beta slated for later this week) moved on
before DROWN.  The 1.0.x stable branches remained backwards compatible
with SSLv2 too long, this has now been addressed.

@_date: 2016-03-06 23:45:58
@_author: Viktor Dukhovni 
@_subject: [Cryptography] DROWN attack on SSLv2 enabled servers 
IMHO, fallback (self-downgrade after negotiation failure) is a browser
hack that has outlived its usefulness.  This too needs to go.  It is not
part of TLS, and TLS 'support' for mitigating the damage from fallback
needs to also be phased out over time.
Fallback is not part of TLS.

@_date: 2016-03-11 07:22:01
@_author: Viktor Dukhovni 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
You seem have conflated the Underhanded C contest (which grew out
of the Obfuscated V contest) with the obfuscated C contest.  The
Underhanded C contest has clear code with subtly hidden errors.
The obfuscated C code contest has impenetrable and concise code
doing unbelievable things.
The voting shennanigans were in the Obfuscated V contest.

@_date: 2016-03-17 05:08:26
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Trust & randomness in computer systems 
Well, there you've lost my respect.  Dan is a most unlikely shill
for his current or past employers.

@_date: 2016-03-18 03:55:50
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
I'm confused.  Are you claiming that formal methods failed here?
On the face of it, they were quite successful...
  Introduction:
  ... In 1999 the flight software flew on a space mission, and a
  deadlock occurred in a *sibling* subsystem to the one which was
  the focus of the first verification effort. ...
  Conclusion:
  This paper describes two major verification efforts carried out within
  the Automated Software Engineering Group at NASA Ames Research Center.
  The first effort consisted of analyzing part of the RA autonomous
  space craft software using the SPIN model checker. One of the errors
  found with SPIN, a missing critical section around a conditional wait
  statement, was in fact reintroduced in a *different* subsystem that was
  not verified in this first pre-flight effort. This error caused a real
  deadlock in the RA during flight in space. ...
The software that failed was not verified, and contained a bug isomorphic to
the one that the verifier found.  Had it been verified, the bug would have
been avoided.

@_date: 2016-03-18 20:24:42
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
In this case only a part of the system was formally verified,
failure to verify more of the code was not a result of an incomplete
specification, it simply was not done.
There was no bug.  Just verification of a part of a system.  A
verified wheel on a car does not prevent fatal accidents, but that's
not a bug a formal verification of wheels.

@_date: 2016-03-23 20:19:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] e-mail giants / IETF 
This needs a bunch of work before it is sufficiently clear and
simple to be deployable.  The discussion will take place on the
UTA WG mailing list.
This approach lets the large providers off the hook on DNSSEC,
and has a number of consequent shortcomings, but represents
some progress, so I'll help as much as I can.

@_date: 2016-05-31 16:30:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Blue Coat has been issued a MITM encryption 
In the specs and in OpenSSL it means that the CA can only issue EE
certificates, it cannot issue subsidiary intermediates.
I'd be suprised if other X.509 toolkits interpreted pathlen == 0
differently.  I would not be suprised to find toolkits that completely
ignore path length constraints, but don't know of any that do.
The extension should be "critical", which might help with those
toolkits that don't ignore unhandled critical extensions.

@_date: 2016-05-31 20:38:38
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Blue Coat has been issued a MITM encryption 
If BlueCoat had the key for the path-constrained intermediate CA
they could indeed create additional self-issued intermediates.
However, allegedly they don't have the key.  So the self-issued
intermediate would have to be issued to BlueCoat by Symantec.
Many distibutions/builds of the Exim MTA are linked with GnuTLS.

@_date: 2016-11-20 03:59:54
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
I've been teaching myself Haskell lately, while developing some
new code for my ongoing DANE TLSA survey[*].  As part of that, I
need a TLS stack, and have started exploring Network.TLS.
A syscall trace of the resulting TLS client code shows that while
it probes for the existence of /dev/random and /dev/urandom to seed
the per-context DRBG (it uses ChaCha for that), it does not (on my
Intel laptop) end up reading either device.  This is because the
Entropy sources are configured as:
    Crypto.Random.Entropy.Backend:
and for each request each backend is used in turn (non-blocking in
the /dev/random case) to obtain up to the requested number of bytes
until the full requirement is met.  With RDRAND always ready-and-
willing to provide bytes, the others go unused.
What is the crypto community's current state of concern around
RDRAND?  Should Haskell's Crypto avoid seeding exclusively from

@_date: 2016-11-20 15:57:57
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Thanks Bill.
At this time, just for my own project, "random looking" numbers,
indeed even non-random looking numbers happen to be sufficient.
While learning the toolchain, I just happen to be paying close
attention to the details, in order to make sure that I'm doing
my part right.
Anyone else care to comment on the wisdom or folly of RDRAND as
a principal (sole) seeding mechanism for a TLS stack?  Note that
in this case the raw RDRAND output is never leaked directly, it
(just 40 octets of RDRAND output) is only used to seed a ChaCha
DRBG, which then provides all the randomness for a TLS context.
I am quite confident that the seeding is performed as reported.
I don't think it would be fair to challenge the author for an
explanation.  Much of the Haskell ecosystem is volunteer work.
However, constructive criticism (a Github pull request with a
good quality patch) might do the trick, if the seeding method
is definitely in need of improvement.
I am perhaps still too new at Haskell to produce a "natural"
improved implementation.  My Haskell code might look too much
of Fortran. :-)  If someone else wants to give it a go, the
code in question is at:
    The loop that tries each backend in turn is in:
    ("Unsafe" here is about lack of "referential transparency"[1]),
while the backend configuration is in:

@_date: 2016-11-21 03:18:41
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
OK, got the message.
The RDRAND support in the Haskell cryptography package is conditional
on the compilation environment of that module.  While Haskell code is
generally portable, I/O libraries can provide (sometimes by linking
with suitable external C-code) system-dependent features. Since entropy
sources are definitely I/O, their implementation is platform-dependent.
So Haskell's TLS uses RDRAND for entropy on just the CPUs in which it
detects RDRAND support.  Both of my laptops sport RDRAND-capable CPUs.
These days it probably makes sense to implement backends for the various
new Unix entropy APIs:
    when available.  Any care to volunteer a patch?
I'm working on DANE support for the Haskell X509 chain validation code,
where I feel I am not out of my depth.  If someone else cares to contribute
patches that improve the Unix entropy backend, that'd be just swell.
Relevant upthread messages are:

@_date: 2016-11-26 00:41:51
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
It seems you're hinting at:
        RDSEED first appears in Broadwell CPUs, while RDRAND appears earlier in Ivy Bridge.

@_date: 2016-11-28 12:02:25
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL and random 
Go with Ian:
   Mind you, after seeding from /dev/urandom (or as available the
"getentropy" syscalls on new-enough OpenBSD and Linux), we still
need a reseedable CSPRNG, and for that I think Keccak has a mode
that looks like a good candidate...
Yes, the topic of random number generation does not appear to be
one on which you'll find broad consensus...

@_date: 2016-11-28 15:09:14
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL and random 
No.  OpenSSL is a library, not a sub-system started at boot time
that runs for the lifetime of the system.  The above advice would
block at the start of every (often short-lived) process that uses
OpenSSL.  This is entirely impractical.
If /dev/urandom fails to provide good entropy at boot time then
the problem needs to be fixed there, not OpenSSL.  What OpenSSL
can do, is mix /dev/urandom with RDSEED or RDRAND when available,
just in case it is running "early" (perhaps on first boot) and
[ The kind of "improvement" described recently on list for Ubuntu
  is not what I have in mind.  That particular approach should
  IMHO be   withdrawn and archived as a good example of what not
  to do. ]

@_date: 2016-10-02 03:42:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] distrusted root CA: WoSign 
Well, DANE is strictly stronger than DV, because it is tied to
direct evidence of domain control, via the domain management account
of the domain owner at the registrar/registry that publishes the
DS records on the owner's behalf.
Whereas, DV is a point in time, MiTM-vulnerable, leap-of-faith by
any one of a multiplicity of CAs that perform cursory "verification"
of domain control.  So DANE would certainly be progress, but it is
not currently practical for mobile clients that find themselves
regularly on airport WiFi networks, hotel captive-portal networks,
and other DNSSEC-unfriendly environments.  DANE stapling is still
on the drawing board, in part waiting for the TLS WG to be less
attention starved by TLS 1.3 to the exclusion of much other work.
Which is not to say that transparency is a bad idea.  We each have
technologies we've invested a bunch of effort into, there's no need
to disparage either, they each have their place.  DANE is well
suited to SMTP + STARTTLS, CT is more applicable to the Web.

@_date: 2016-10-02 23:02:42
@_author: Viktor Dukhovni 
@_subject: [Cryptography] distrusted root CA: WoSign 
Well every failure mode for DANE (loss of domain control) is also
a failure mode for DV, but DV has additional failure modes.  Anyway
we don't need to dissect this in gory detail.  There are no silver
bullets, it is trade-offs all the way down.

@_date: 2016-10-03 18:02:49
@_author: Viktor Dukhovni 
@_subject: [Cryptography]  Debunking the "SMTP TLS "s a mess" myth. 
Actually, it is rather a success.  The fraction of SMTP traffic
that's encrypted in transit (between organizations over the public
Internet) may be larger than the corresponding metric for HTTP.
    (TL;DR as observed by Gmail, varying by weekday, 84-87% outbound,
76--80% inbound).  Opportunistic TLS does a rather decent job of
defending most traffic against passive wiretap.
Yes, protection against active attacks is also desirable, hence
the interest in DANE and STS.  Today, there are at least (limited
by the extent to which I've been able to find them):
    ~2200 distinct DANE-validated MX host certificates, serving
    ~60700 DANE-enabled domains, of which enough volume is sent by
    ~76 to have appeared in the above Gmail transparency report
Of the 60700 domains, ~700 have DNS for some, but not all of their
MX hosts, so their DANE deployment is a work-in-progress.  A year
ago the domain count was around 12000 and the intersection with
the transparency report was 24 domains.  Below is a sample of
some of the more prominent early adopters:
    gmx.at
    nic.br
    registro.br
    gmx.ch
    open.ch
    switch.ch
    gmx.com
    mail.com
    xfinity.com
    bund.de
    fau.de
    gmx.de
    jpberlin.de
    kabelmail.de
    lrz.de
    posteo.de
    uni-erlangen.de
    unitymedia.de
    web.de
    octopuce.fr
    comcast.net
    gmx.net
    t-2.net
    xs4all.net
    mkbbelangen.nl
    overheid.nl
    uvt.nl
    xs4all.nl
    domeneshop.no
    debian.org
    freebsd.org
    gentoo.org
    ietf.org
    isc.org
    netbsd.org
    openssl.org
    samba.org
    torproject.org
The gmx.de and comcast.net deployments cover tens of millions
of users.
Much of the early deployment (by domain count) is for small domains
hosted by a few large providers:
    31859 transip.nl
    15144 udmedia.de
     1795 bhosted.nl
     1261 nederhost.net
      905 ec-elements.com
      376 core-networks.de
      208 omc-mail.com
      181 hot-chilli.net
      168 mailbox.org
      164 networking4all.net
I expect to see more large populations of hosted domains to support
DANE TLS for SMTP by the end of this year, and the total to grow
by a factor of 10 or so.

@_date: 2016-10-04 17:58:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Debunking the "SMTP TLS "s a mess" myth. 
Pinning fails in the face of MX indirection and STARTTLS
stripping.  MTA-to-MTA SMTP is not HTTP, the security model
does not carry over:
   Real alternatives take the form of a detailed specification,
and implementation in a couple of MTAs...

@_date: 2016-09-01 17:32:15
@_author: Viktor Dukhovni 
@_subject: [Cryptography] MATH: Unlikely correctness of paper will break 
It is not a serious paper, it is some sort of joke.  Just not clear
whether it was composed by a human or machine generated.
Shoddy mathematics aside, the final dedication:
    This paper is dedicated to S. Ramanathan on his birth centenary.
    The second author is his daughter and the first author is his
    son-in-law.
is nonsense, Ramanujan was born in 1887, so the paper would have
to have been from 1987, and Ramanujan had no children.

@_date: 2016-09-07 15:19:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Strong DNS Names 
This can be less ad-hoc.  See page 10 of section 2.3.1 of RFC 5890:
    where the diagram shows a taxonomy of DNS labels.  In particular
labels starting with "??--" are reserved, with "xn--" used for
IDNA.  Your scheme could define a new two letter code for
destination-specific in-name trust anchors.  Perhaps "ta--"?
Right, this I assume because the trust-anchor fingerprint is
(unavoidably) in this case for the domain and not the end-user.
(which certainly improves email usability).
A difficulty I see is that key rotation becomes difficult to
impossible.  Is the assumption that this key is off-line, and is
only used infrequently (by the domain owner) to sign other keys
that do all the interactive work?  So it can't be changed withoout
invalidating saved addresses of this form?  (A change of TA, might
typically mean a change of domain ownership)?
The CA/B forum strangled CAA in its crib, by deciding to make
implementation optional for CAs. :-(

@_date: 2016-09-10 07:47:47
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Strong DNS Names 
No, address extensions are not part of any standard.  They are an
ad-hoc syntax introduced in the Andrew Messaging System (AMS) at
CMU in the early 80's.   The {8,28,53}2{1,2} series of RFCs strive
to avoid prescribing any particular structure or semantics to the
"localpart" of an email address, even to the extent of not specifying
the widely implemented (and incorrectly assumed to apply everywhere)
case-insenstive treatment thereof.  The interpretation of address
localparts is quite intentionally *local*.  When (double) quoted,
and/or sprinkled with quoted-pairs where appropriate, localparts
can contain pretty much anything.
AMS used "+" as the recipient delimiter (Postfix name for the
separator).  Later, in the 90's, DJB observed that "-" is more
typically tolerated by overly strict address parsers, particularly
in HTML forms, and so used "-" as the recextensively in Qmail.
Postfix documentation tends to suggest "+", which seems to be more
common, but the recipient delimiter is empty (disabled) by default,
and each site can choose whatever recipient delimiter suites its
needs best.

@_date: 2016-09-23 20:03:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Spooky quantum radar at a distance 
They did mention advances in single-photon detection.  So if there's
any meat to this story, perhaps they are able to make do with a
lot fewer scattered photons because they can detect them and then
correlate with the entangled twins.  A smaller scattering cross-section
might then not sufficiently deter detection.  Mere speculation of course,
but the most charitable skim-reading of the article I could come up with.

@_date: 2016-09-28 02:41:35
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Two distinct DSA keys sign a file with the same 
That depends... Does this only happen with 1.0.1t or with any other releases?
Do the 1.0.1t signatures appear valid with 0.9.8?  1.0.0?  1.0.2?  ...
How easily are you able to create more inputs for which both keys yield the
same signature?  Are you at liberty to post the PEM files for the two public
keys, the input file and the signature file?
If key "A" is able to produce signatures that validate as signed by "B"
that's a potential problem, though somewhat less so if "2nd pre-image"
keys are not feasible, and all one can do is generate a pair of keys
that can generate colliding signatures.  If only the latter, then it
may still be difficult to forge someone else's signature...

@_date: 2016-09-29 20:01:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct DSA 
What is the practical significance of one's ability to craft weak
DSA keys?

@_date: 2017-04-13 02:34:05
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Key escrow scheme 
My impression is that Grover's algorithm has nothing to do with
related keys.  It is a generic QC algorithm for finding which of
N inputs produces a given output in sqrt(N) time.
So if the master secret is 128 bits, recovering it by "brute force"
may take only 2^64 quantum time.  The holder of the key may long
be dead before scalable universal QCs are built, or such QCs may
arrive all too soon.
Perhaps I've misunderstood the descriptions I've read of Grover's
algorithm, but if not, the design probably does not achieve your
goals.  To get 2^128 quantum work-factor for symmetric algorithms
IIRC you unavoidably need a 2^256 key space.

@_date: 2017-02-27 11:34:32
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Just in case it isn't obvious... 
Please don't.  This will not interoperate.  The team that announced the break
also announced a reasonably robust fix that interoperates will SHA-1 unless
the digest is vulnerable to the known attack differentials.  It detects and
modifies hashes for which a second pre-image can be found using currently
known attacks.  The probability of false-positives (accidental rather than
malicious weakness) was reportedly 2^{-90} (~10^{-48}).
Git could adopt the hardened SHA-1 implementation as a stop-gap, and as Ted
reports work to adopt newer hashes in an interoperable way.  Simply pretending
that Blake2b is SHA-1 is not a productive direction.

@_date: 2017-01-11 04:59:40
@_author: Viktor Dukhovni 
@_subject: [Cryptography] nytimes.com switches to https 
The npr.org certificate looks good when I connect, however the
HTTPS site redirects to HTTP.
    $ (sleep 3; exit) |     ...
    $ openssl verify -untrusted /tmp/npr.pem -trusted /tmp/root.pem /tmp/npr.pem
    /tmp/npr.pem: OK
The npr.org chain and root CA PEM files attached.

@_date: 2017-01-22 19:03:21
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
Let's not confuse collision attacks with second pre-image attacks.
Tampering with existing signed objects requires a second pre-image
attack.  What is the estimated complexity of the best known second
pre-image attack on MD5?

@_date: 2017-03-29 12:41:54
@_author: Viktor Dukhovni 
@_subject: [Cryptography] "Perpetual Encryption" (or why bother?) 
Actually, by transmitting the tag in the clear it fails to
have information theoretic security.  For that, you'd first
append the tag, and then apply the OTP covering both the data
and the checksum.
Switching topics, I am perplexed why Rich chose to nerd-snype
this group with this particular snake-oil instance.  Is there
really something interesting or unusual in this case?  If not,
we can perhaps find more amusing distractions.
If there is nothing compelling, perhaps it would be if this
were the last message in the thread (silence would be enough

@_date: 2017-05-02 19:10:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] [FORGED] Re:  Escrowing keys 
On a recent evening, Mr. Habersham walked along the train tracks near 34th Street
   in Manhattan as workers replaced antiquated switches and cables. A signal system
   should last about 50 years, he said, but the one that guides trains through this
   slice of Manhattan has been in place for about 80.

@_date: 2017-11-16 01:25:20
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I doubt that's at all common.  OpenSSL caches the "wire-form" of TBSCertificate,
and verifies that instead of re-encoding.  We don't hear too many complaints
(about that).

@_date: 2017-10-30 15:15:32
@_author: Viktor Dukhovni 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
The real question is whether it gets widespread adoption.  Often
the real novelty is figuring out how to get things deployed in
practice.  Time will tell whether this effort to get beyond
passwords will gain some traction.

@_date: 2017-10-30 21:23:19
@_author: Viktor Dukhovni 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
Indeed it is far from clear they'll succeed, but they may have an
advantage of scale that the other organizations may not have had.
Controlling half the planet's email, and the software of half of
the planet's mobile devices, could just make it possible, even if
most likely still doomed to fail to make a difference (at scale).

@_date: 2017-09-13 17:11:45
@_author: Viktor Dukhovni 
@_subject: [Cryptography] letsencrypt.org 
Let's Encrypt just makes it ever more clear that the WebPKI (a few
EV certificates aside along with the few users who notice the
difference) is and has been a leap of faith by the DV-issuing CA.
Thus certificate issuance is fundamentally vulnerable to MiTM
attacks on the CA by folks in position to launch active attacks
on the network backbone.  You're really only protected from
WiFi and similar attacks at cafes, airports, ... by attackers
who can MiTM the end-users network connection.
With BGP attacks and the like, a determined adversary will
be able to get a DV certificate for most domains from some
DV-issuing CA.
I tried to suggest at a recent IETF meeting that CAs should
use DNSSEC-validating resolvers when querying CAA records,
to reduce this MiTM risk, but got rather strange pushback
from PHB on behalf of Comodo.  FWIW, Let's Encrypt does in
fact do validated DNS resolution.
Given their reasonably clear and transparent practices, I'd
be pleased if they became the *only* non-EV CA on the market.
The price is right, and the security is about as good as it
gets with DV.  The commercial CAs can then focus on properly
verifying the minority of customers who need EV certs.

@_date: 2017-09-14 02:23:55
@_author: Viktor Dukhovni 
@_subject: [Cryptography] letsencrypt.org 
Yep, nice to see that you concur...
If the case for DANE were the price of CA certificates, then indeed
you'd be right.  For me the case for DANE is not the price.
* DANE is a more secure DV, since there's no leap of faith, the
  keys are published via the login account that controls the domain.
* DANE supports downgrade resistant policy signalling for opportunistic
  TLS, which is well suited to SMTP.
But we digress...

@_date: 2017-09-16 23:45:26
@_author: Viktor Dukhovni 
@_subject: [Cryptography] letsencrypt.org 
Sorry about that, I'm relieved to infer from the below that there's
in fact no disagreement.
Which is all I was trying to ask for.
I agree that should not be a requirement.  Glad we can put that
misunderstanding behind us.

@_date: 2018-04-15 23:47:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Claimed Breakthrough Slays Classic Computing 
The data on the article is "November 13, 2015".  Babai's graph-isomorphism advance is a settled result (and was recognized correct in short order), but "encryption could be next" is simply hype.

@_date: 2018-08-31 10:44:38
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Need a list of Solinas/pseudo Mersene Primes. 
According to   (1 SHL 257) - B(1 SHL 257) = 93
So you can take n = 93.

@_date: 2018-08-31 11:26:43
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Need a list of Solinas/pseudo Mersene Primes. 
If you want the lowest possible "n", and would prefer a lower "n" at the
cost of a slightly higher exponent, then 2^266 - 3 is the best you can do
for exponents modestly above 256.
That was found with:  x=257;x=x+1;x<=272;x*1000 + ((1 SHL x) - B(1 SHL x))
with "266003" the winner.

@_date: 2018-08-31 13:41:01
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Need a list of Solinas/pseudo Mersene Primes. 
For $2^{32k+1}-n$ prime with $n$ minimal, the values are:
  	  2^33 -   9
[ via "x=33;x=x+32;x<=513;x*1000 + (1 SHL x) - B(1 SHL x)" plugged into
   ]

@_date: 2018-08-31 16:02:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] WireGuard 
============================== START ==============================
The right way to do single-suite protocols, is to tie all the choices
to a single protocol version.  For shiny new parameters, bump the
protocol version.  Client proposes its list of protocol versions,
and server chooses the highest supported.  If some protocol version
later proves vulnerable to downgrades of this negotiation step,
support for that version is expeditiously phased out.
This model typically means that security protocols are vertically
integrated with the application, since general-purpose security
protocols (e.g. TLS) tend to have to accomodate a range of requirements
that makes a single choice of cryptographic parameters difficult.

@_date: 2018-02-13 15:19:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Existence of point of order 4 in a Montgomery 
No.  Z/2Z x Z/2Z has no elements of order 4, it has 3 elements of order 2, and
the identity.  The element of order 4 in whichever group has one is not a member
of this subgroup.  The subgroups in question were just used to establish that
the order of both groups is divisible by 4, so given the sum of their orders
one has order 4 mod mod 8 and the other order 0 mod 8.
The group with order 4 mod 8 is Z/2Z x Z/2Z x "odd-order", so has no
order 4 elements.  Now you just need to observe why the group whose order
is divisible by 8 must have an order 4 element.  This is not explained,
and relies on a non-trivial result about elliptic curves.
The reason is that an elliptic curve can have at most 3 points of order
2 (elements of order n are found in a subgroup of Z/nZ x Z/nZ) while
Z/2 x Z/2 x Z/2 has 7 elements of order 2, and so can't occur.  Thus,
Z/2 x Z/2 x Z/2 is ruled out as a subgroup of order 8, but a subgroup
of order 8 must exist to yield 0 mod 8 for the group order.  Therefore,
there must be an element of order 4 for the group to have order
divisible by 8.

@_date: 2018-02-16 02:40:39
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Quantum computers will never overcome noise 
You keep saying that, but without any plausible justification.  Scalable universal quantum computers (if they can be made!) would provide *asymptotic* algorithmic speedups.  Merely "faster by some constant" classical computers do no such thing, and for the problems of interest it is the asymptotic speedups that matter, no realistically attainable constant speedup is large enough.

@_date: 2018-02-18 00:22:33
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Quantum computers will never overcome noise 
Sorry, I'm not playing this game.  (Yes I know the units of Plank's
Faster classical computers are great, but they don't solve exponential-cost
problems for significantly larger inputs.  A large constant factor is just
a small logarithm increment.

@_date: 2018-02-23 14:04:52
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
Now make it work for mailing lists...
Suppose I run a system that monitors DANE adoption and, as free
service, notifies domain owners when their TLSA records fail to
match their certificate chain.  How will that work?
The IETF "uta" working group is working on a "tlsrpt" standard
to go along along with "smtp-sts", which solicits email reports
from senders when TLS authentication fails.  The messages sent
are a courtesy to the receiving system, who pays for those?
"Simple" it ain't.

@_date: 2018-02-26 18:40:39
@_author: Viktor Dukhovni 
@_subject: [Cryptography] [dns-operations] IP address encryption: 
Keyed ciphers try to be indistinguishable from a random permutation, an
actual random permutation is about as strong as it gets.
What is unavoidably problematic is that the same input needs to
produce the same output each time.  But this, IIRC, is a requirement,
the mapping should keep distinct inputs distinct and identical inputs
identical.  That is a permutation (random or secret-keyed) is required.

@_date: 2018-01-06 17:28:36
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Speculation considered harmful? 
There's not much point in debating on this list largely impractical
seat-of-the-pants CPU-design changes.  Making the cache transactional
is I expect almost certainly far too complex a design option.
Measures to improve process isolation are likely much more realistic.

@_date: 2018-01-09 16:36:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Speculation considered harmful? 
My reading of the Spectre paper finds no such constraint.
Concurrent execution that trips over the vulnerable "gadget"
seems to suffice even across process boundaries.  Did I miss
some crucial text that narrows the exposure?

@_date: 2018-01-16 22:23:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] canonicalizing unicode strings. 
Except that, for example, an email address may have a non-LATIN
localpart alongside a LATIN (ASCII) domain name.  Or some of the
labels of a domain may be in a different script that the parent
domain.  (I have .org, in which the first label is
Cyrillic, and .org is of course Latin US-ASCII).

@_date: 2018-06-04 21:47:07
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Odd bit of security advice 
Pseudo-random 64-bit data has a high collision probability after 2^32 outputs.
While encryption of a 64-bit counter with a 64-bit block cipher maintains
a non-repeating sequence for 2^64 outputs.

@_date: 2018-03-05 21:22:59
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Mutually authenticated TLS 
If the internal CA in question only grants certificates for names in
this OU to clients authorized to use the application in question, and
if revocation is considered not important (e.g. short-lived certs), or
CRLs are being checked. Then there's nothing wrong with this approach.
TLS makes sure that the public key in the certificate signs the client
key exchange.  So the key is always checked.  The CA binds the key to
the OU.  So this is all fine.  The full DN has little authentication
value if all clients get the same access level.  It might be useful
in an audit trail, but is not necessarily needed for access control.
Since the server was not looking to communicate with a specific client,
but rather *some* client started communicating with the server, there's
little to be gained by checking the "CN" (against what exactly?) if all
possible "CN" values get the same access.
But the access control token is the "OU"...
They're right.
You're asking them to implement a more fine-grained access check in
the application, but they're comfortable doing that in the CA, and
either don't have a revocation requirement, or process CRLs.
The RFCs are out of scope here.  Access control decisions are up to
the verifier.  The certificate is valid, what part of it they choose
to base access control on is up to them.

@_date: 2018-05-18 22:35:03
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Vulnerability found in badly broken email apps 
TLS does not return unverified data, TLS data is broken up into
records, and each record is verified before it is given to the
application.  This is the only way to handle stream integrity,
you have to break the stream into chunks, authenticate the
chunks, and ensure that reordering is not possible by including
the chunk number or offset in the MAC.

@_date: 2018-11-01 13:58:55
@_author: Viktor Dukhovni 
@_subject: [Cryptography] hash size 
Sorting them all is not required to detect collisions.
Nor is that much storage.  A rainbow table could suffice.
Parallelizing the rainbow table computation at reasonable
communication cost is the tricky part.  The key/value
store can be sharded, but naively you still need O(2^64)
table queries during table construction, and to construct
the table in one year, one needs to move O(2^70) bits in
O(2^25) seconds, so the interconnect appears to need an
aggregate bandwidth of O(2^45) bps.  I don't know how to
make the construction phase practical.
Some clever people may have figured efficient ways to
build rainbow tables for a 2^64 keyspace without prohibitive
communications costs...

@_date: 2018-09-01 16:18:18
@_author: Viktor Dukhovni 
@_subject: [Cryptography] WireGuard 
Yes, definitely, designing the downgrade protection correction requires care.
For, this Mike Hamburg's "Strobe" protocol framework[1] looks promising:
   Whatever the counter-measures are, they should be no more exploitable than
the lowest version supported by the client and server, and if one manages
to reasonably promptly phase out versions vulnerable to downgrade attacks,
it should be possible to evolve the system in a reasonably secure manner.

@_date: 2019-04-17 22:57:23
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Bitcoin - closing the coin 
Finding a SHA2-256 pre-image to 256-bits of zeros is not something
one would expect to compute by chance before the heat-death of the
universe.  Rather, it would require a significant advance in SHA2
cryptanalysis, that enables preimage generation, and breaks all use
of SHA2-256 as a hash function.

@_date: 2019-12-02 20:26:13
@_author: Viktor Dukhovni 
@_subject: [Cryptography] 795-bit factoring and discrete logarithms 
The content behind that link has some garbled unicode text, which was
converted from Windows-1252 to UTF-8 twice (interpreting UTF-8 as
Windows 1252 and encoding it to UTF-8 again).  The ungarbled text is
below my signature.  The difference is minor, just some accented French
names of people and institutions, on three lines towards the end of the
     -68 +68     -computing center at Universit?? de Lorraine, Nancy, France [6], an
    +computing center at Universit? de Lorraine, Nancy, France [6], an
     -76 +76     -Fabrice Boudot, ??ducation Nationale and Universit?? de Limoges, France
    +Fabrice Boudot, ?ducation Nationale and Universit? de Limoges, France
     -80 +80     -Emmanuel Thom??, INRIA, Nancy, France
    +Emmanuel Thom?, INRIA, Nancy, France
I happen to have a Perl script that undoes this type of encoding
breakage, and decided that the institutions and Emmanuel Thom? deserve
proper credit...

@_date: 2019-12-22 20:37:05
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL: rsa_builtin_keygen: key size too small 
Is this some sort of embedded device with extremely limited CPU
resources?  Can it be simulated at higher speed during development?
Is OpenSSL the right toolkit for the device in question?  There are
likely other libraries specifically targeted at that segment of the
That's compiled in and not configurable.
You'd have to recompile OpenSSL:
crypto/rsa/rsa_local.h:14: RSA_MIN_MODULUS_BITS    512
crypto/rsa/rsa_gen.c:76:    if (bits < RSA_MIN_MODULUS_BITS) {
crypto/rsa/rsa_gen.c-77-        ok = 0;             /* we set our own err */
crypto/rsa/rsa_gen.c-78-        RSAerr(RSA_F_RSA_BUILTIN_KEYGEN, RSA_R_KEY_SIZE_TOO_SMALL);
crypto/rsa/rsa_gen.c-79-        goto err;
crypto/rsa/rsa_pmeth.c:464:        if (p1 < RSA_MIN_MODULUS_BITS) {
crypto/rsa/rsa_pmeth.c-465-            RSAerr(RSA_F_PKEY_RSA_CTRL, RSA_R_KEY_SIZE_TOO_SMALL);
crypto/rsa/rsa_pmeth.c-466-            return -2;

@_date: 2019-12-23 01:59:24
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL: rsa_builtin_keygen: key size too small 
Then it is surprising that on your development machine you notice
adverse performance effects from using RSA-512.
Are you slowed down by RSA operations, or RSA key generation, and how
many of those do you attempt per second?

@_date: 2019-12-24 16:31:06
@_author: Viktor Dukhovni 
@_subject: [Cryptography] OpenSSL: rsa_builtin_keygen: key size too small 
Well, I'm on the OpenSSL team, and did explain how to build a custom
version that will admit smaller keys.
We can't and don't try to stop users from building derived versions with
a more lax security policy.  Cryptanalytic weakness aside, an inherent
problem with very short RSA keys, is that they can't be used to sign
message digests whose length combined with the padding bits exceed the
length modulus.
Thus 512-bit RSA is already too small for SHA2-512, and RSA-128 can't
even sign MD5 digests.
Not a problem if both ends use the OpenSSL in question, and with TLS
the security level is set to 0 (e.g.  in the cipherlist).

@_date: 2019-01-11 19:39:34
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Government shutdown: TLS certificates not 
A non-trivial UI question is whether on error, the UI reports all
the failures or just the first failure, and if so which one!
Suppose the certificate is both expired and has the wrong hostname?
Will the user figure that out?

@_date: 2019-06-19 02:15:34
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Announcing the P11_260 Signature System 
FWIW, the numerator is of course (t^11-1) not t^11.

@_date: 2019-06-20 00:28:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] =?utf-8?q?Shamir=E2=80=99s_secret_sharing?= 
It is information-theoretically secure, just like a one-time pad.
Without the last requisite key-share you get zero information
about the shared secret, unless you know something about how
the missing share was generated (flaw in the RNG, ...).
A polynomial with coefficients in a field of degree $n$ is
uniquely determined by its values at $n+1$ distinct points
(e.g. a line is determined by two points), and any fewer
points get you no information about what value it takes
at some other distinct point.
With all but one share fixed, the secret is an affine linear
function (Ax + B) of the remaining share "x" with a non-zero
A.  Since we're in a field, the function is one-to-one and
the secret value is equidistributed if "x" is.  Thus same
security as a one-time-pad.

@_date: 2019-03-21 16:14:10
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Mix Messaging in the Mesh 
Complexity needs to stop short of the point at which a non-negligible
fraction of intended users would struggle to deploy the system reliably
and sufficiently securely to meet their needs.  Sometimes this means
that some (deliberately left out of scope) threats remain, in order to
usably address the threats that are the primary focus of the technology.
And the system has to remain usable.  Even relatively simple systems
can be onerous to use/operate.

@_date: 2019-09-24 17:09:42
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Crown Sterling debunked 
Yes, in both you have an abelian group in which the problem is
recovering $x$ from $n$ and $x^n$, where $x^n$ is obtained from $x$
by applying the group operation between $n$ copies of $x$.
In an abelian group, calling the group operation "addition" and
writing it as "+" is equivalent to callng it "multiplication" and
writing it as "*".  Point addition on elliptic curves can equally
be considered point multiplication, with repeated addition of a
point with itself (scalar multiplication) then considered to be
exponentiation.  It makes no difference.
Both the FFDH and ECDH problems are therefore discrete log problems
in abelian groups.

@_date: 2019-09-30 19:16:30
@_author: Viktor Dukhovni 
@_subject: [Cryptography] "Strong" passwords too clever by half... 
For a 94 bit password, with a guaranteed final punctuation mark:
$ openssl rand -base64 12 | perl -lpe 's/(.)$/chr(ord($1)&0x0f|0x20)/e; s/ $/~/'
With the possible final punctuation characters:
     20  ~    21  !    22  "    23  #    24  $    25  %    26  &    27  '
     28  (    29  )    2a  *    2b  +    2c  ,    2d  -    2e  .    2f  /
FWIW, my Android keyboard has "^" in two places.  Holding down "t"
produces a pop-up list with "^" as one of the options.  The other
way is to switch to the non-alphabetic (numeric) set, and then the
secondary non-alphabetic set, where "^" is directly available.
But the above recipe will never generate "^", just alphabet letters,
numbers and the above punctuation.  The output is 16 characters which
is not infrequently an upper bound on the allowed password length.

@_date: 2020-04-29 16:20:48
@_author: Viktor Dukhovni 
@_subject: [Cryptography] The EFF 650 CAs lie 
Does the exact number matter much?  Suppose it is ~65, is that
qualitatively different?
On a Fedora 31 system, the OS installs 138 self-signed trust anchors,
when I extract just the "O=" component of the subject DN (falling back
on "CN=" when "O=" is absent), I gest 69 unique case-insensitive names:
    AC Camerfirma S.A.
    ACCV
    AS Sertifitseerimiskeskus
    Actalis S.p.A./03358520967
    AddTrust AB
    AffirmTrust
    Agencia Catalana de Certificacio (NIF Q-0801176-I)
    Amazon
    Atos
    Autoridad de Certificacion Firmaprofesional CIF A62634068
    Baltimore
    Buypass AS-983163327
    COMODO CA Limited
    China Financial Certification Authority
    Chunghwa Telecom Co., Ltd.
    Cybertrust, Inc
    D-Trust GmbH
    Dhimyotis
    DigiCert Inc
    Digital Signature Trust Co.
    Disig a.s.
    E-Tu?Fra EBG Bili?im Teknolojileri ve Hizmetleri A.?.
    Entrust, Inc.
    Entrust.net
    FNMT-RCM
    GUANG DONG CERTIFICATE AUTHORITY CO.,LTD.
    GeoTrust Inc.
    GlobalSign
    GlobalSign nv-sa
    GoDaddy.com, Inc.
    Google Trust Services LLC
    Government Root Certification Authority
    Hellenic Academic and Research Institutions Cert. Authority
    Hongkong Post
    IZENPE S.A.
    IdenTrust
    Internet Security Research Group
    Japan Certification Services, Inc.
    Krajowa Izba Rozliczeniowa S.A.
    LuxTrust S.A.
    Microsec Ltd.
    NetLock Kft.
    Network Solutions L.L.C.
    QuoVadis Limited
    SECOM Trust Systems CO.,LTD.
    SECOM Trust.net
    SSL Corporation
    SecureTrust Corporation
    Sonera
    Staat der Nederlanden
    Starfield Technologies, Inc.
    SwissSign AG
    T-Systems Enterprise Services GmbH
    TAIWAN-CA
    TeliaSonera
    The Go Daddy Group, Inc.
    The USERTRUST Network
    TrustCor Systems S. de R.L.
    Trustis Limited
    Turkiye Bilimsel ve Teknolojik Arastirma Kurumu - TUBITAK
    UniTrust
    Unizeto Technologies S.A.
    VeriSign, Inc.
    WISeKey
    XRamp Security Services Inc
    certSIGN
    eMudhra Inc
    eMudhra Technologies Limited
    thawte, Inc.
A few of these are clearly alternative names for the same organisation,
but it seems safe to estimate that there are at least ~50 distinct
entities on the above list.
So, OK perhaps an order of magnitude fewer than 650, but does it make
enough of a difference to call the larger na?ve estimates "lies"?
I still don't know who most of these are, and probably would prefer to
not unconditionally (i.e. without name constraints) trust all of them.

@_date: 2020-02-09 02:05:12
@_author: Viktor Dukhovni 
@_subject: [Cryptography] lavabit.com appears down... 
Does anyone know what happened to lavabit.com?  Has the plug been
pulled, or is it just enduring a transient outage?
None of the glue nameservers are reachable, and the most recent IP
address of the MX host is also down.  Traceroute to the last known IP
address of the MX host shows a loop:
    traceroute to 38.107.241.66 (38.107.241.66), 13 hops max, 60 byte packets
    ...
     6  ae-2-3604.ear1.Dallas3.Level3.net (4.69.210.185)  156.750 ms  156.766 ms  156.754 ms
     7  CYBERVERSE.ear1.Dallas3.Level3.net (4.35.186.82)  156.668 ms  129.779 ms  129.732 ms
     8  edge-gw01.dc2.hackingand.coffee (76.8.22.102)  129.730 ms  157.089 ms  157.099 ms
     9  gi0-0-0-5.nr11.b062167-0.dfw06.atlas.cogentco.com (38.122.58.89)  157.341 ms  157.314 ms  157.336 ms
    10  edge-gw01.dc2.hackingand.coffee (38.122.58.90)  157.013 ms  157.051 ms  157.012 ms
    11  gi0-0-0-5.nr11.b062167-0.dfw06.atlas.cogentco.com (38.122.58.89)  157.035 ms  156.623 ms  156.607 ms
    12  edge-gw01.dc2.hackingand.coffee (38.122.58.90)  156.308 ms  156.231 ms  156.276 ms
    13  gi0-0-0-5.nr11.b062167-0.dfw06.atlas.cogentco.com (38.122.58.89)  156.271 ms  156.516 ms  157.473 ms
    ...
The domain registration has not changed recently or expired.
    Domain Name: LAVABIT.COM
    Registry Domain ID: 170683923_DOMAIN_COM-VRSN
    Registrar WHOIS Server: whois.godaddy.com
    Registrar URL:     Updated Date: 2016-02-03T23:29:37Z
    Creation Date: 2005-06-15T17:51:40Z
    Registrar Registration Expiration Date: 2021-06-15T17:51:40Z
    Registrar: GoDaddy.com, LLC

@_date: 2020-02-09 23:27:04
@_author: Viktor Dukhovni 
@_subject: [Cryptography] lavabit.com appears down... 
It seems the outage was transient, all the nameservers and the
MX host are back.

@_date: 2020-03-11 18:16:53
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Products that prevent DoH? 
For example, if you're using unbound, it is enough to add:
    server:
            local-zone: "use-application-dns.net." always_nxdomain
to the configuration of the local resolver, in order to disable implicit
DoH in Firefox.

@_date: 2020-03-12 00:59:33
@_author: Viktor Dukhovni 
@_subject: [Cryptography] Products that prevent DoH? 
Perhaps I should posted the details, but at least in my case I am well
aware of this limitation, my Firefox is also configured to not use DoH.
