
@_date: 2001-10-05 13:22:31
@_author: Joseph Ashwood 
@_subject: Passport Passwords Stored in Plaintext 
I'm sorry.
The short answer is that it isn't secure. There are two main problems with
it being secure. The first is the password vulnerability that you replied
to. The second is that it uses a custom blended Kerberos-esque
implementation. I say Kerberos-esque because it has some significant
problems. First it uses RC4, a cipher which is increasingly being considered
insecure, and in using it windows doesn't take the precautions necessary to
make it secure. They are the only company foolish enough to have embedded
access control information in the kerberos ticket, this adds even more
leaking information, and just enough of it to determine the users password.
Basicly they have made nearly every effort to eliminate the security of the
system while making it appear secure to a layman. For further evidence that
Microsoft can't do anything secure I point to (in no particular order) IIS,
pptp, pptp2, Internet Explorer, Outlook Express, Windows 95, Windows98,
WindowsME, WindowsNT, Windows2000, and while I haven't verified it yet I
believe also WindowsXP. Some of these probably need some explaination, IIS
is the script kiddie choice it has more holes than a pound of Swiss cheese.
pptp was severely broken, pptp2 was slightly less severely broken. Internet
Explorer has had so many security vulnerabilities I can't even count that
high. Outlook Express is a virus writers dream. Windows95 offered no
security, same with 98 and ME. WindowsNT is subject to extremely basic
attacks on the password system that Microsoft refused to recognise, same
with 2000, and probably the same with XP. In 2000 MS introduced a "secure"
encrypted filesystem which lacked any reasonable ability to encrypt
documents securely (it put the keys in a file in plaintext, the file is
easily readable). Even the cryptoAPI that Microsoft designed and offered has
holes in it, allowing arbitrary code to be run in the place of what the
programmer intended. I am unaware of anything microsoft has ever written
that could be considered secure and there is evidence that they plan to
continue this less than stellar performance with .NET.
                    Joe

@_date: 2002-08-07 20:05:49
@_author: Joseph Ashwood 
@_subject: deterministic primality test 
I have my doubts about the thoroughness of the examination of this
algorithm. From Page 4 (any inconsistencies are due to my typing):
Input: integer n > 1
1: if (n is of the form a^b, b>1) output COMPOSITE;
2: r=2
3: while(r < n) {
4:    if(gcd(n,r)=/=1) output COMPOSITE;
5:    if(r is prime)
6:        let q be the largest prime factor of r-1;
I don't need to finish the algorithm. Simply run this for n = 3
1: n is not of the form
2: r=2
3: 2 < 3
4: gcd = 1
5: 2 is prime
6: q = ?
    r=2
    q = largest prime factor of (2-1) = largest prime factor of 1. 1 has no
prime factors, since 1 is by definition not prime, but has no integer
divisors except 1.
So it fails to be executable on the second prime. I haven't done an in depth
analysis of the algorithm beyond this. It is entirely possible that it only
needs the small revision from:
Input: integer n > 1
Input: integer n > 3
but regardless the algorithm as it stands fails a basic test.
                    Joe

@_date: 2002-08-09 02:26:22
@_author: Joseph Ashwood 
@_subject: deterministic primality test 
[I've got some doubts about the content here but I think the
discussion is certainly on charter --Perry]
Since I have received a number of private replies all saying approximately
the same thing; lookup for small n, use algo for large. Allow me to extend
my observation.
To quote myself from earlier:
n = prime number > 2
1: n is not of the form (we already know it is prime)
2: r=2
3: 2 < 3
4: gcd = 1 (since N is prime)
5: 2 is prime
6: q = ?
    r=2
    q = largest prime factor of (2-1) = largest prime factor of 1. 1 has no
prime factors, since 1 is by definition not prime, but has no integer
divisors except 1.
So the algorithm cannot be executed on any prime value with the exception of
2 (which it correctly states is prime). It is possible that the algorithm is
simply incomplete. The apparent intension is:
6: if(r == 2) q = 1;
    else q is the largest prime factor of r-1
A few additional observations under the assumption that this is the desired
The algorithm is equivalent to (I've left the numbering for the original
instruction order:
1: if (n is of the form a^b, b>1) output COMPOSITE;
2: r=2
3: while(r < n) {
5:    if(r is prime)
4:        if(gcd(n,r)=/=1) output COMPOSITE;
6:         if(r == 2) q = 1;
            else q is the largest prime factor of r-1
7.        if(q >= 4sqrt(r)log(n)) and (n^((r-1)/q) =/= 1 (mod r))
8                break
9:    r <-r+1
proving this is trivial. Since r is being stepped sequentially the only new
results will occur when r is prime (otherwise gcd is not dependable), this
will reduce the number of calls to gcd, and so reduce the asymptotic time.
This is obviously bounded by the time it takes to check if r is prime.
However note that we can now replace it conceptually with, without changing
1: if (n is of the form a^b, b>1) output COMPOSITE;
2: r=2
3: while(r < n) {
4:        if(gcd(n,r)=/=1) output COMPOSITE;
6:         if(r == 2) q = 1;
            else q is the largest prime factor of r-1
7.        if(q >= 4sqrt(r)log(n)) and (n^((r-1)/q) =/= 1 (mod r))
8                break
9-2:    r <-nextprime(r)
The asymptotic time is now bounded primarily by the runningtime of
nextprime(), the current best running time for this is not polynomial (but
there are better ways than their step by 1, prime check method). In fact
assuming that the outer while loop is the primary exit point (which is
certainly true for the worst case), the algorithm presented takes O(n) time
(assuming binary notation, where n is the function input n), this is
equvalent to the convential notation O(2^m), where m is the length of the
input. The best case is entirely different. Best case running time is
O(gcd()), which is polynomial. Their claim of polynomial running time is
certainly suspect. And over the course of the entire algorithm, the actual
running time is limited by a function I've dubbed listAllPrimesLessThan(n),
which is well studied and the output has exponential length, making such a
function strictly exponential.
Additionally it has been noted that certain composite values may pass the
test, regardless of the claim of perfection. It is unlikely that this
function is of much use.
                    Joe

@_date: 2002-08-10 21:46:12
@_author: Joseph Ashwood 
@_subject: Seth on TCPA at Defcon/Usenix 
[brief description of Document Revocation List]
Actually it does, in order to make it valuable. Without a hardware assist,
the attack works like this:
Hack your software (which is in many ways almost trivial) to reveal it's
private key.
Watch the protocol.
Decrypt protocol
Grab decryption key
use decryption key
problem solved
With hardware assist, trusted software, and a trusted execution environment
it (doesn't) work like this:
Hack you software.
DOH!!!!! the software won't run
revert back to the stored software.
Hack the hardware (extremely difficult).
Virtualize the hardware at a second layer, using the grabbed private key
Hack the software
Watch the protocol.
Decrypt protocol
Grab decryption key
use decryption key
Once the file is released the server revokes all trust in your client,
effectively removing all files from your computer that you have not
decrypted yet
problem solved? only for valuable files
Of course if you could find some way to disguise which source was hacked,
things change.
Now about the claim that MS Word would not have this "feature." It almost
certainly would. The reason being that business customers are of particular
interest to MS, since they supply a large portion of the money for Word (and
everything else). Businesses would want to be able to configure their
network in such a way that critical business information couldn't be leaked
to the outside world. Of course this removes the advertising path of
conveniently leaking carefully constructed documents to the world, but for
many companies that is a trivial loss.
                Joe

@_date: 2002-08-13 22:58:58
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
Lately on both of these lists there has been quite some discussion about
TCPA and Palladium, the good, the bad, the ugly, and the anonymous. :)
However there is something that is very much worth noting, at least about
There is nothing stopping a virtualized version being created.
There is nothing that stops say VMWare from synthesizing a system view that
includes a virtual TCPA component. This makes it possible to (if desired)
remove all cryptographic protection.
Of course such a software would need to be sold as a "development tool" but
we all know what would happen. Tools like VMWare have been developed by
others, and as I recall didn't take all that long to do. As such they can be
anonymously distributed, and can almost certainly be stored entirely on a
boot CD, using the floppy drive to store the keys (although floppy drives
are no longer a "cool" thing to have in a system), boot from the CD, it runs
a small kernel that virtualizes and allows debugging of the TPM/TSS which
allows the viewing, copying and replacement of private keys on demand.
Of course this is likely to quickly become illegal, or may already, but that
doesn't stop the possibility of creating such a system. For details on how
to create this virtualized TCPA please refer to the TCPA spec.
                Joe

@_date: 2002-08-14 15:10:44
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
Actually that does nothing to stop it. Because of the construction of TCPA,
the private keys are registered _after_ the owner receives the computer,
this is the window of opportunity against that as well. The worst case for
cost of this is to purchase an additional motherboard (IIRC Fry's has them
as low as $50), giving the ability to present a purchase. The
virtual-private key is then created, and registered using the credentials
borrowed from the second motherboard. Since TCPA doesn't allow for direct
remote queries against the hardware, the virtual system will actually have
first shot at the incoming data. That's the worst case. The expected case;
you pay a small registration fee claiming that you "accidentally" wiped your
TCPA. The best case, you claim you "accidentally" wiped your TCPA, they
charge you nothing to remove the record of your old TCPA, and replace it
with your new (virtualized) TCPA. So at worst this will cost $50. Once
you've got a virtual setup, that virtual setup (with all its associated
purchased rights) can be replicated across an unlimited number of computers.
The important part for this, is that TCPA has no key until it has an owner,
and the owner can wipe the TCPA at any time. From what I can tell this was
designed for resale of components, but is perfectly suitable as a point of
                    Joe

@_date: 2002-08-15 13:06:26
@_author: Joseph Ashwood 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
This is going to be a very long, and very boring message. But it should
highlight why we have differing opinions about so very many capabilities of
the TCPA system. For the sake of attempting to avoid supplying too little
information, I have simply searched for the term and will make comments on
each location that it appears.
----- Original Message -----
I wholeheartedly agree. 332 pages to say 5 pages worth of real information
is not helpful.
[Search criteria "endorsement"]
While I haven't found any solid evidence either way, they seem to almost
deliberately avoid that discussion on Page 22 I found a fatal errorcode
TCPA_NO_ENDORSEMENT at TCPA_BASE+35 "The TPM does not a EK installed"
attempting to interpret the bad grammar, I believe this should state "The
TPM does not [have] an [Endorsement Key] installed" which seems to indicate
that the platform may ship without one.
On page 35 the endorsement key is listed as persistent data. Which at first
would indicate that the endorsement key happens before shipping, but since
there is also an RNG state variable stored persistently, my confidence in
this is undermined. Adding to the complications, down near the end of the
page, in the table it says "This is the TPM's endorsement key pair. See 9.2.
The default value is manufacturer-specific" which indicates that it does
ship with an endorsement key, but that the key can be changed by the owner.
Page 38, the existance of the CBKPUsed flag hints that the endorsement key
pair need not always be present. Unfortunately the spec goes on to say
"NOTE: This flag has no default value as the key pair MUST be created by one
or the other mechanism." Which certainly confuses things.
Page 41 "TPM_CreateEndorsementKey may be called before TPM_Startup. This is
necessary because TPM_Startup will fail unless an endorsement key exists" is
of no help either way. As with all the others, it states that there may
exist conditions where the EK may not exist, but does not give any hints
whether this is before or after the TPM leaves the plant.
On page 79, the EK is metioned twice. The first time if useless for our
purpose. The second time states "This SHALL be the TPM endorsement
credential" which indicates that an endorsement credential must exist. Other
locations though seem to hint that a void endorsement credential may be
Starting on Page 84 is section 4.32.1, which seems to be as close to an
authority on the EK as possible, but lacks a statement of whether the EK is
shipped with or added later. It does however clearly indicate that the
creation of the EK occurs before the Privacy CA is contacted, which was
already agreed on.
[somewhere around here I stopped addressing everyt occurance of the word
"endorsement" because most of them are frivolous]
Page 135, Section 5.11.1, clearly states "The new owner MUST encrypt the
Owner authorization data and the SRK authorization data using the PUBEK."
Which clearly indicates that the EK must exist before ownership can be
taken. Other places have hinted that ownership may be taken and then the EK
updated, which completely contradicts the one-timeness, or this statement.
Page 135 "If no EK is present the TPM MUST return TCPA_NO_ENDORSEMENT" which
indicates that one can at least attempt to take ownership before an EK is
present, which would contradict the requirement that the EK come from the
Page 178, Section 7.3 I am only mentioning because it presents a rather
interesting possibility. It hints that under some circumstances it may be
acceptable for a manufacturer to copy the data from one TCPA to another.
This portion begins with "The manufacturer takes the maintainance blob . .
." This may however only be to update an existing one to address flaws or
meet new capabilities.
Page 183, hints that even the manufacturer is not allowed to known EK public
key, which complicates things no end, because the Privacy CA certainly
cannot at that point be permitted to view it. This would indicate that even
if the EK is shipped with the system, it can never leave the system. This
would limit the ability of the EK to simply certifying the owner, if that is
true then it confuses me even further.
Page 213 section 8.10 clearly states that if the owner clears the TCPA,
everthing is cleared "except the endorsement key pair." Which would indicate
that this is truly a one-shot deal.
Page 240, states "This is a dead TPM. It has failed it's startup smoke test.
It should not leave the factory floor." This indicates that the EK must be
created before the TPM leaves the factory.
Section 9.2, page 261, states that TPM_CreateEndorsementKeyPair can only be
called once, but does not state if this is done by the owner, or by the
plant. Later on the page is a hint that it may be shipped with it. "The
PRIVEK and PUBEK MAY be created by a process other than the use of
TPM_CreateEndorsementKeyPair" and related statements, which indicate rather
well that the endorsement key created before shipping. It also states that
the credential could be stored after "an Owner has taken ownership of the
platform," confusing the matter even more. Of course at the end of this
section they change the mandatory return value for
TPM_CreateEndorsementKeyPair (beginning TCPA_FAIL, end TCPA_DISABLED_CMD).
Page 268, "The TPM creates an identity-binding signature (the value of a
signature over the
TCPA_IDENTITY_CONTENTS structure). Among other things, this proves
possession of the new private key, which does the signing of the
TCPA_IDENTITY_CONTENTS structure." Indicates that the endorsement key (they
only possible plant binding factor) is never used outside the box. The
TCPA_IDENTITY_CONTENTS structure contains only TCPA_VERSION, ordinal, label
PrivCADigest, and identityPubKey. Note the absence of anything identifying
the TPM and binding it to the manufacturing. On the next page they state
that there is some method used of certifying that it came from a genuine
TPM, but I can't immediately find such evidence.
I suppose just to make life more interesting for everyone involved, they
included the following "The form of the following certificates is out of
scope for this version of the TPM specification:
. TPM endorsement entity certificate"
Section 9.5.1 page 283 states "If the data structure
 is stored on a platform after an Owner has taken
ownership of that platform, it SHALL exist only in storage to which access
is controlled and is available to authorized entities." Which indicates very
strongly that the EK can be created after shipping. More interesting though
is the TPM_ENDORSEMENT_CREDENTIAL itself, which includes only the
information to gaurantee that the cert was grabbed from a valid instance of
a TPM, but does not from what I can tell actually certify that the current
request comes from a valid TPM. Although they do take the odd step of making
certain that CRLs are not used, under CRL dictribution points "If present
and marked critical, then reject" which of course means that CRLs will not
be used for this.
The other credentials have the same interesting quirks.
Page 311, Section 10.8 gives an interesting view of the situation, which
indicates that the EK may be produced later discussing power-on self-tests
"If an endorsement key has not yet been generated the TPM action is
manufacturer specific."
In the Glossary I think is the clearest statement about the EK in the entire
document, the problem is the sentence is too long, shortening it for the
valid point "Endorsement Key [-] A term used ambiguously" is entirely
accurate enough for our purposes.
Result: I have no idea whatsoever about where/when the EK is created, there
are a number of conflicting statements regarding it, and at least once where
they even change the return value of a function.
Agreed. After almost two hours of picking through poorly written
specification, I'm just going to state that I agree.
Probably correct, but I'm not sure.
Only to the owner, the manufacturer is not supposed to have a copy
The privacy CA never recieves a copy of the PUBEK, the PUBEK is only to be
seen by the owner.
I agree with everything except the "during manufacture" which while
seemingly is the correct way to do it to meet the majority of the spec,
isn't necessarily the only way.
c) the
I believe this is incorrect. The complete contents of the endorsement
credentials can be found starting on page 283, and does not include the
endorsement key in any way. Which is good because knowledge of the
endorsement public key, would allow someone to claim ownership of the
It is not encrypted with the endorsement public key, it can't be since the
Privacy CA does not receive a copy of it. I'm not sure whether or not it is
encrypted at all, but at the very least it cannot be encrypted by the EK.
Certainly correct, but I don't believe the Privacy CA necessarily has to be
in on it.
This would probably be easier done with a court order forcing the Privacy CA
to perform the operation under the guise of law enforcement.
But it could be easily combined with a smaller, easier to get court order
along the lines of B where a company is required to allow the FBI to work
under the company's name to perform the necessary work. This eliminates the
phony Privacy CA from existance.
I didn't notice that, but it does seem to be true, as with you I can see no
reason for that decision. Perhaps it was a non-decision that made it into
the spec?
It is discussed surprisingly briefly for this spec, it occupies a mere 10
pages, beginning on page 178. The reliance on trust here stems from the
manufacturer's ability to replicate TPMs (although the EK does not appear to
be changable this way).
Thinking about it more, I don't like that this spec assumes there is 1
primary user of a machine, and that this user is also the owner. A better
design would use something akin to a smartcard, and a large chunk of
non-volatile RAM. Making properly use of this would allow for authentication
of families of people, simply plugging in your card, and entering your
passphrase would allow users to authenticate to the system easily, allowing
the home system to separate the identities of Mom, Dad, Son, Daughter, Aunt,
Uncle. And could still bind people to a specific machine.
I think everyone who reads the spec, or even our commentary on it can safely
conclude that all the chips supporting this should have a "WTF" somewhere in
their identifier.
Now leaving the topic.
Fortunately in my current state of (lack of) employment I have plenty of
time to do this kind of examination, otherwise I wouldn't bother. With that
said, if anyone knows of a company currently hiring software
engineer/cryptanalyst/etc I'd appreciate any information.
                    Joe

@_date: 2002-08-14 19:23:29
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
I certainly don't believe many people to believe me simply because I say it
is so. Instead I'll supply a link to the authority of TCPA, the 1.1b
specification, it is available at
 . There are other
documents, unfortunately the main spec gives substantial leeway, and I
haven't had time to read the others (I haven't fully digested the main spec
yet either). From that spec, all 332 pages of it, I encourage everyone that
wants to decide for themselves to read the spec. If you reach different
conclusions than I have, feel free to comment, I'm sure there are many
people on these lists that would be interested in justification for either
Personally, I believe I've processed enough of the spec to state that TCPA
is a tool, and like any tool it has both positive and negative aspects.
Provided the requirement to be able to turn it off (and for my preference
they should add a requirement that the motherboard continue functioning even
under the condition that the TCPA module(s) is/are physically removed from
the board). The current spec though does seem to have a bend towards being
as advertised, being primarily a tool for the user. Whether this will remain
in the version 2.0 that is in the works, I cannot say as I have no access to
it, although if someone is listening with an NDA nearby, I'd be more than
happy to review it.
                    Joe

@_date: 2002-07-23 14:06:40
@_author: Joseph Ashwood 
@_subject: building a true RNG (was: Quantum Computing ...) 
It will not serve a cryptographic use, however if you can find a fast enough
truly lossless compressor it can be very useful. Since I assume you'll be
taking a picture purely in the dark a usable compressor would be (please
pardon the severely abused pseduo-code)
SHA1 pool
    if pixel is not black
        SHA1_update(pool, pixel, pixel coordinates);
    SHA1 temp
    SHA1_update(pool, "1")
    temp = SHA1_duplicate(pool)
    return SHA1_finalize(temp)
It is believed to fare quite well, and considering that the line for proper
entropy distillation is actually well below the line for cryptographic
security SHA-1 is likely to remain very good for this purpose. If you are
more concerned about speed than maximum entropy containment (or require less
than 128-bits of entropy) you might also consider MD5. If you are extremely
concerned about this (and are willing to lose a few other desirable
behaviors) it is possible to use a block cipher, basically in CBC mode, to
accumulate entropy, this has the advantage that under some reduced
assumptions it is possible to compute the maximal entropy of the state at a
given time.
                Joe

@_date: 2002-07-27 14:04:31
@_author: Joseph Ashwood 
@_subject: building a true RNG 
Hill'" ; Sent: Saturday, July 27, 2002 10:52 AM
I disagree. The whitening stage can perform a very important step of
allowing for stretching of the entropy. Ideally a true RNG would be fast
enough to supply all our needs, but that typically doesn't happen. Instead
we have to depend on stretching tricks, basically using a primitive in
counter mode, to stretch the bits we have as far as is needed. This can be
combined with the distillation stage itself by inserting a fixed string,
copying the state, and finishing the calculation, but can conceptually be
thought of as a seperate stage.
A construct that simply does not exist, more on this later.
I seperated this because I agree with teh statement completely, that is the
one and only function of the distilling function, whether it is a hash, a
cipher, or a miracle construct.
I disagree. There are several ways of viewing the problem. Modelling the
distillation function as a perfect entropy smoother over k bits (other than
that I will be assuming a worst case). We see the following happen:
When the first bit is inserted, each of the k bits receives 1/k entropy
When the second bit of entropy is inserted one of those bits received 1 bit
of entropy, the entire system now has 1 + (k-1)/k bits of entropy, already
there is a small but measurable discrepancy
Each bit now has (2k-1)/k bits of entropy
The third bit results in 1 of the k bits having entropy 1 giving 1+
((2k-1)/k)(k-1)/k bits of entropy in total
While the system slowly loses entropy, it does lose entropy. It's also worth
noting that this agrees with the presented analysis at 3 points, 0 has 0
entropy, 1 has 1-bit of entropy, and inf has k bits, but that for all useful
lengths having k bits of entropy can never occur.
These don't even have to be hash collisions, it is possible for the hash
function to discard entropy simply through its behavior, as in my example
where entropy starts being discarded beginning with the second bit. From a
theoretic standpoint, it is entirely possible that a function exists that
discards entropy beginning with the first bit.
This statement is true of all deterministic algorithms. In fact it it one of
the points where all presented models agree, you cannot reach a completely
entropic state until you reach infinite lengths for the input. If the input
if purely entropic there is some grey area surrounding this, but for a hash
function to be secure the grey area disappears.
Not necessarily. Take a two phase system, in hardware you have a hardware
random bit generator feeding a perfect hash function, in software you have
something collecting several of these inputs, and mixing them using a strong
cipher (no stretching is occurring). This system is no easier to analyze
than the bit generator->hash, but also no weaker. It is certainly the case
that certain designs are more likely to be weak when encountered in the
wild, but in general this is due simply to the frequency with which
under-educated people design/implement them versus the rate at which
properly educated people design/implement.
                        Joe

@_date: 2002-06-25 14:08:49
@_author: Joseph Ashwood 
@_subject: Ross's TCPA paper 
I disagree that these are entirely contradictory. The first is a statement
in the realm of logic; that if Bob is prepared to deal with whatever
consequences will occur because of the publication of the conversation,
there is nothing Alice can do to stop him (short of killing him before he
The second is a statement in the realm of law, that companies will try to
rid themselves of any requirements that are painful to their bottomline. In
the current case there is a perceived threat of this nature (right or wrong)
regarding the TCPA, that corporations are using their monetary power to
revoke rights that are currently enjoyed.
These statements are not contradictory; Alice can still do as she pleases to
the TCPA devices in her computer, the problem arises that she may have to
deal with substantial civil and criminal penalties for doing so. This is
similar to the question "Is Alice free to walk into a bank carrying a
full-automatic weapon, kill everyone inside, and steal all the money?" yes
she is, but she has to be prepared to deal with the (wo)manhunt that will
begin the moment something like that happens, and the inevitable that she
simply won't live to stand trial (with high probability).
                Joe

@_date: 2002-03-31 17:51:44
@_author: Joseph Ashwood 
@_subject: authentication protocols 
Sent: Monday, March 25, 2002 3:14 PM
Depending on your exact requirements, there are many potential options.
Let's start with the most basic:
The parties securely exchange a verifier. There are many variations at this
point, but the basic version is (with details omitted for brevity):
A->B: I am A
B->A: If you are A then you can decrypt this E(K) and use K as our shared
A decrypts and now both A and B share a one time secret
This is generally done using symmetric keys
More sophisticated, and scaling much better requires a trust model of some
kind. This does however get very tricky. There has to be some verification
of the key by a 3rd party (which can typically be the same as one of the
first 2 parties). However it is possible to build something usable, as long
as on one occasion you can verify the identity of the other party. This type
of protocol works approximately as so:
B has a verified public key for A
A has a verified public key for B
A->B: S(I am A, our temporary key is E(K), the current time is T)
B verifies signature, and decrypts K, K is now the shared secret
There are of course variations where it is E(S(........K...)) instead of
There are many different variations on this, some patented, some
unencumbered, some that are secure down to the use a an ATM-like PIN, some
that require larger quantities of secret data, some that take 1 pass from A
to B, some that take 10 passes between them, some that have nice provable
reductions, some that don't. It all depends on what your needs are. But all
of these require some trust model, and initial verification of the key is
the problem.
Moving over to situations where you are not forced to perform an initial key
verification requires a trusted 3rd party, which is what you requested to
avoid so I won't introduce them.
                        Joe

@_date: 2002-10-09 14:59:36
@_author: Joseph Ashwood 
@_subject: Microsoft marries RSA Security to Windows 
Unfortunately, SecurID hasn't been that way for a while. RSA has offered
executables for various operating systems for some time now. I agree it
destroys what there was of the security, and reduces it to basically the
level of username/password, albeit at a more expensive price. But I'm sure
it was a move to improve their bottom line.
                    Joe

@_date: 2003-04-19 14:34:34
@_author: Joseph Ashwood 
@_subject: the futility of DRM (Re: DMCA Crypto Software) 
Adam stated one avenue rather well, but I believe he missed a far more
important one; provide services for licensed copies. Something as simple as
access to early previews, discounts on merchandise, creative input on media
to be produced, etc. Use these options to take advantage of the p2p model,
sell a DVD for $15 (already very feasible) the DVD has a license card in it
(one time use 10 digits number or similar), sell licenses on a per movie
basis for $5. Let the p2p networks do all the real work, and the content
providers make all the real money. This gives better market research (you
not only know the monetary threshold for the target audience better, but you
get direct feedback from them about future media), and cheaper overhead (you
actually have to produce fewer DVDs, which lowers expenses and increases
profits because of the second channel). Already many movies are partially
paid for by advertising, simply ratchet this a little bit higher, make it
more pervasive (e.g. feature the products in the script of TV shows), this
lowers the actual risk of production, and the end profit is higher with much
higher margins. Of course you build a tiered system into this, with
licensing for private home exhibition of new movies being a few hundred
dollars, public viewing licenses being several thousand, old movies driving
quickly towards $5, I'm sure a movie executive could supply much stronger
numbers, and these numbers could even be changed on a per movie basis. If
anyone has a contact at the MPAA feel free to inform them of the idea,
hopefully they see that p2p networks can be used to actually increase their
                Joe
Trust Laboratories

@_date: 2003-12-12 15:51:18
@_author: Joseph Ashwood 
@_subject: "Zero Knowledge Authentication"? (was Cryptolog Unicity Software-Only Digital Certificates) 
Sent: Wednesday, December 10, 2003 8:47 AM
I've snipped the rest, because it is primarily not useful beyond this. They
are highly incorrect about their lauch being the "first commercial use" of
ZKA, as a matter of fact I was involved in implenting one for commercial
use, and I was a part of a "mandatory workfoce reduction" (aka laid off)
from that company 2 1/2 years ago. I will admit we never referred to it as
"Zero Knowledge Authentication" which just sounds like a mass of crap thrown
together to sound geeky. Instead we used zero knowledge proof of knowledge
(in particular a PIN), and used that proof to provide authentication. I can
also tell you that if you're dealing with some high security requirements
(such as the claim of "high security" in the press release), there are some
very tricky situations and I found a number of unpublished attacks against
such systems (all were addressed before the product shipped, except the one
I address below which is inherent). So to anyone looking at such a system, I
recommend that they give it at least 2 years to mature and be attacked, and
even then make sure that a number of worthwhile names have actually looked
at the protocols involved, and the implementation.
With that said, I see little reason that such systems need to exist, you
continually end up coming back to "but what is it actually good for" the
truth is that with a small piece of knowledge, only a small number of
accounts need their existance known to compromise the system. An example,
simple PIN-based system, e.g. ATM bank card network, PIN must be at least 4
digits, and a maximum of 6. First, statistically the vast majority of PINs
will be 4 digits. Now contrary to reality, we will assume that the pins are
chosen randomly (most people choose a pattern). The fact is that with 4
digits there are only 10,000 possible pins, so only 5000 guesses need to be
made to on average have broken into one account. From there the standard is
that each account is given 3 guesses before disabling, so only 1667 cards
have to be uncovered in order to break into an account. Now realistically,
how long will this take? Here in the US ATM cards can be uniquely identified
by 16 digits (it's been linked into the Visa network), this makes acquiring
the card number easy. Acquiring the number of 1667 cards is almost trivial.
On such "high security" systems, they invariably have further problems. The
base information required for a user to log in can be downloaded free of
security (for roaming), this allows an attacker to simply download all the
login credentials for the entire enterprise. In many cases large companies
will have more than 1667 people who have root access on the network. This is
a fatal flaw for the design, and unfortunately for such systems this is a
flaw that cannot be addressed except by switching to passphrases, something
that would lower their usability (their biggest selling point) to the same
level of all other "secure" systems.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-01-20 16:48:02
@_author: Joseph Ashwood 
@_subject: Key Pair Agreement? 
I can think of a more efficient algorithm off the top of my head.
1. Scott creates P,Q,G
2. Scott sends P,Q,G
3. Alice verifies P,Q,G and creates (x,y)
4. Alice sends (x,y)
Scott can be certain that, within bounds, P,Q,G have never been found
before, and it vastly reduces the computations (since there was no stated
requirement that Alice believe the pair had never been used before).
Now on to Jack's question:
I believe it is unfortunately so, depending on how strong P is. If (P-1)/2Q
is always prime, then I believe there is little erosion (but some remains).
The foundation of this thus.
X can be determined from Y if the attacker can find inverses in all the sub
groups, by the CRT.
I believe there is a way to extend this such that provided you have enough
sub-groups to uniquely identify a number > Y, X can be determined.
If this is the case then having a large number of (Y,P,G,Q) around can erode
the security of the DLP.
I also believe that it is the case that each of the factors must be unique,
otherwise it is simply a collision in the CRT context (although a change of
G may correct this)
If this is the case then it would be important that Alice generate a new X
for each pair in existence, which is simply good policy to begin with.
                Joe

@_date: 2003-01-27 13:11:01
@_author: Joseph Ashwood 
@_subject: Shamir factoring machine uninteresting? 
[snips omitted, reordering also occurred for response coherency]
----- Original Message -----
I refrained from much in the way of comment because, while it is an
interesting result, and quite damning for 1024-bit keys, it is not a very
significant boost of the state of the art, and the costs given are somewhat
Considering the actual erosion of the problem that occured, TWIRL simply
adds to the foundation of 1024-bit RSA keys are not strong, although we
still don't have enough evidence to blanketly declare them weak. Bernstein's
observations were a much stronger catalyst, since the apparent result was a
3x increase in the size of numbers that could be factored in a given time.
Mathematically, Shamir's recent factoring advancements have been somewhat
middle of the road, his results don't change the underlying asymptotic
relationship. While TWINKLE did push the envelope quite a bit by changing
the underlying behavior of the algorithm (even though the algorithm was
unchanged), TWIRL doesn't seem to have the same major advancement
capability. After my cursory examination of the paper it appears to me that
TWIRL is at least near the bounds of reason, but I have not done an in depth
analysis of any kind.
There in lies a problem, and where we get into the possibility of calling
the numbers "misleading." It appears that these numbers are for a run of
sufficient quantity, so the cost of the process of choosing where to put the
circuits on the board is not accounted for. Considering the size of the end
device, this would be an extremely expensive step, and the dominant factor
in creating a one-off. TWINKLE had much the same problem. Given the current
state of the art in taking algorithms and making them in transistors, I
wouldn't expect this number to drop too dramatically for several years. So I
don't expect an example of a large size unit (more than a few hundred bits)
within the realistic future.
That is certainly one thing that has been said several times, TWINKLE might
not be realistically implementable. TWIRL appears to address a substantial
part of that, using technology that is fairly standard (0.13 micron
lithography, very much like Intel, IBM, etc currently use in their fabs or
are planning soon), it also rearranges a number of portions in an apparent
attempt to address the questions raised against TWINKLE.
[later comment: this opinion was changed in a later message that I just
I disagree though
that 3-4 orders of magnitude is not a big deal, 3 orders of magnitude can
generally be anywhere from 8 to 1000 fold increase (magnitude is
exponential). I haven't examined the paper in any real depth, so I do not
yet have a supportable opinion about whether it is a 3 fold increase of a
1000 fold increase, but I suspect that given the current rate of
advancement, it is currently closer to the bottom than the top, and that the
evolution to 0.09 micron technology in the near future will yield
substantial improvements to TWIRL as well.
Seem like you have a pretty firm grasp on the results claimed, the
underlying technology is a bit interesting but 99% of the time it will be
wasted information, of course cryptanalysts/cryptographers live in a realm
where 1/2^80 is odds that are uncomfortably high, so that 99% may be low
enough that the paper is of interest.
                    Joe
Trust Laboratories

@_date: 2003-03-08 21:06:56
@_author: Joseph Ashwood 
@_subject: Comments/summary on unicity discussion 
There appears to be an error in there. The Unicity Distance has a very
strong correlation with the uncertainty of the plaintext (entropy per
message). By having access to the plaintext/ciphertext pair (often it takes
multiple pairs), this removes all uncertainty as to the plaintext, this
changes the unicity distance calculation by making the unicity distance as
short as possible, which would make "Once you get a known
plaintext/ciphertext pair, a high unicity distance works against you"
Seem more than a little odd as a statement.
On K complexity, while K complexity offers a convenient, if somewhat
inaccurate, upperbound of the entropy, that is basically where the
relationship ends. Permit me to give the basic example. Which of these
strings has higher entropy:
One was created by slapping my hands on the keyboard, and so contains some
entropy, the other was created through copy and paste, and so contains none.
However the K complexity of the two is identical. The portion of the
equation you are forgetting is that the key to the pRNG may itself be
compressible. This leads to somewhat of a logic loop, but at the end of it
is the absolute smallest representation, as a compression of a given
language (the only sense in which this makes sense).
                Joseph Ashwood
Trust Laboratories

@_date: 2003-05-10 19:52:31
@_author: Joseph Ashwood 
@_subject: CipherActive? 
It appears that either their sampling methods aren't at all accurate, or
that they are unclear. In both figure 2 and figure 3 the measurements are
performed on percentages and 1000'of bytes. Looking closely at these
samplings you'll note that there is only 1 shared sampling point, 32 1000's
of bytes, looking closely at the two graphs it is fairly easily noted that
"standard" RSA modexp uses apparently a little less than 50% of the
processor, using CipherActive's modexp method results in taking a little
less than 50% of the processor. In both cases it is easier to read the
location of the top of the bulk encryption portion, and in both cases it
lies slightly above the 60% marker. In fact the measurements are so close
that if you move them both into a picture editer, and stretch them to the
same distance between the markers (figure 2 is actually smaller), you will
see that the bars line up on a pixel by pixel basis.
In short unless their measurement methods have varied substantially between
the two measurements, the two methods are identical.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-05-13 12:28:27
@_author: Joseph Ashwood 
@_subject: Payments as an answer to spam 
Won't work. Here's what happens there.
Let's pretend for a moment that all the mail systems throughout all the
world require this. The spammers will now send out duplicate checks in their
batches; why? simple because most of the messages will reach the inbox
before the check is cashed (a smart spammer will use duplicate checks and
then cash the check once the bulk is in inboxes, just on the chance of
getting their money back), so for $0.01 they can send let's say 1 million,
999,999 of those won't be able to cash the check, but it won't matter, most
users will read before cashing, goal accomplished. As an added bonus look
what this does to the intermediate systems (since my home DSL line isn't
filled to capacity by my daily dose of spam), the intermediate systems now
have to verify checks (compute intensive for a server), this leads to
increased duty loads, and high value targets (any attacker that wanted to
make money would just hack such an intemediate system and steal all the
checks that go through). The overall result though is that the email system
slows to a crawl as the checks have to be verified at every step, increasing
the email in transit, and sucking up disk space like there's no tomorrow;
while simultaneously costing the spammers a few dollars a day, and creating
prime targets for instant wealth. This proposal does the opposite of its
intent, it increases the difference between the spammer and regular user
The best solution I've seen is still the sign everything model. Digitally
sign the outgoing messages, endpoint servers start checking signatures
against an active database (unfortunately right now this would mean
Verisign), email clients start verifying signatures before display, and you
start discarding automatically all unsigned emails. This ends up costing the
spammers marginally less (a few dollars a day), but done properly would
actually enforce the one time computation (include the end target ID in the
signature). But as was pointed out, this won't work unless everyone does it
at the same time, or functionally just AOL and MSN start doing it at the
same time, everyone else will follow suit soon enough. Then the spammers can
be identified, traced and properly persued for the costs they incur. In my
view the ultimate goal should not be to get rid of unsolicited email, it
should instead be to create an environment where unsolicited email has to
pull it's own weight from an infrastructure standpoint, this should put the
junk mail in your inbox at roughly the same level as the junkmail in your
postal box, a livable level.
                    Joe

@_date: 2003-05-13 20:21:16
@_author: Joseph Ashwood 
@_subject: A Trial Balloon to Ban Email?  
I disagree. If you assume that the entire internet will eventually take up
on the process, start with a rule that says "if it has a hashcash token
don't process the other rules." Obviously at first this rule would be hit
rarely, but a big PR campaign surrounding it would get to people, as would
implementing it in Outlook. Eventually your other rules would be rarely hit,
and you could change them to simply discard. Once it's everywhere you can
begin culling the bad ones. I just don't see where the necessary overhead
bult into the servers will take place, or be justified.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-05-30 13:08:01
@_author: Joseph Ashwood 
@_subject: Nullsoft's WASTE communication system 
It should've been pulled for several reasons. The primary one being that it
is basically worthless securitywise. It uses RSA PKCS v1.5 (the one
everyone seems to pick on, and always seems to find a way to be insecure),
Blowfish which supplied a maximum of 150-some gigabytes before insecurity
(birthday paradox), used PCBC which only serves one function and that's
having the longest name. MD5 which should be retired. In short
cryptographically it simply wasn't any good. Now if it was pulled bacause
AOL decided to pull it, I don't have a problem with that.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-05-30 13:53:23
@_author: Joseph Ashwood 
@_subject: "PGP Encryption Proves Powerful" 
The article appears to use PGP simply as the most prominent example, and is
clearly undereducated in the realities of cryptography. It not only says
that there is little chance that it is actually PGP in use, but goes on to
indicate that hackers are a magic bullet. As far as real reporting goes
here, this is laughable. The article is titled "PGP Encryption Proves
Powerful" then says that there's little likelihood that PGP was used. It
flatly says that there are no backdoors, and that it would take millions of
years to break, then hints that hiring hackers to break it would work. While
the sentences individually make sense, the whole is somewhat lacking in
anything resembling English.
                Joe

@_date: 2003-10-13 16:56:34
@_author: Joseph Ashwood 
@_subject: NCipher Takes Hardware Security To Network Level 
Sent: Saturday, October 11, 2003 1:22 PM
Actually, there are reasons to believe that they won't be able to, just as I
would not be qualified to evaluate the functionality of a sewage pump
(except from the perspective of "it seems to work").
I take the counter view, assuming that a independent assessor can be found
that is truly independent, that assessor helps the small companies _more_
than the larger ones. To make a pointed example I will use a current
situation (which I am active in).
Trust Laboratories is a software assurance firm, whose first service is the
assurance of PKCS  modules. From the marketting perspective the large
incumbents (e.g. nCipher which started this conversation) have little
incentive to seek such assurances, they already have a solid lock on the
market, and the brand recognition to keep it that way. The small companies
though have a much stronger incentive, with an assurance they can hint and
in some cases maybe even outright claim technological superiority over the
encumbents, giving them a strong road into the market. The only purpose the
encumbents have for such assurances is combatting the small companies
assurances (not that I wouldn't love to have nCipher as a customer, I think
it would lend a great deal of credibility to the assurance, as well as
solidifying their marketshare against the under-developed technologies).
That will likely always be the case. In order to judge what level of
security is required they simply must have some knowledge of security.
Otherwise it is very much like asking John Smith what Ian Grigg's favorite
food is, (a typical) John Smith simply does not have the knowledge to give a
useful answer.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-08 16:25:22
@_author: Joseph Ashwood 
@_subject: Is cryptography where security took the wrong branch? 
Sent: Sunday, September 07, 2003 12:01 AM
Actually they do target very different aspects. SET, 3D-Secure, and any
other similar have a different target then SSL. To understand this it is
important to realize that instead of the usual view of two-party
transactions, credit card transactions actually take 3 parties; Issuer,
Seller, and Buyer. SSL covers the Seller-Buyer communication, and can also
be applied to the Seller-Issuer communication, but on a transaction basis it
offers nothing for the Issuer-Buyer (the important one for minimizing costs
for the Issuer).
SET/3D-Secure/etc address this through various means but the end target is
to create a pseudo-Buyer-Issuer link, through the Seller. This allows the
Issuer to minimize costs (less chance of having to make a call) and because
it is behind the scenes technology has no reason to be accompanied by a
reduction in fees (and actually because of the reduced likelihood of buyer
fraud, it may be possible to charge the seller _more_).
In the end SSL and SET/3D-Secure/etc target entirely different portions of
the problem (the former targets seller fraud against the buyer, latter
seller against issuer). Both of these are important portions, of course the
real upside of SET/3D-Secure/etc is that the seller doesn't have a choice,
and the fees in accordance with the "fraud-reduction" may very well increase
the costs to the seller, the buyer costs of course stay the same. End
result: lower fraud, increased fees->higher profit margins.
However, if it meets expectations, it is entirely possible that all
legitimate parties (non-fraud entities) will see improved profits (seller
has reduced fraud and charge-backs, buyer less likelihood of the $50
penalty, issuer higher fees). Will it meet those expectations? I have no
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-08 16:51:04
@_author: Joseph Ashwood 
@_subject: Digital cash and campaign finance reform 
[anonymous funding of politicians]
Simple attack: Bob talks to soon to be bought politician. "Tomorrow you'll
recieve a donation of $50k, you'll know where it came from."
Next day, buyer makes 500 $100 donations (remember you can't link him to any
transaction), 50k arrives through the mix. Politician knows where it came
from, but no one can prove it.
By implementing this we'll see a backwards trend. It will be harder to prove
the buyout (actually impossible), but the involved parties will know exactly
who did the paying. Right now you can actually see a similar usage in the
Bustamante (spelling?) campaign in the California Recall Election, the
Native Americans donated $2M to him in spite of a limit of ~22k by donating
from several people. Same method only now we know who did the paying.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-08 22:32:51
@_author: Joseph Ashwood 
@_subject: Digital cash and campaign finance reform 
You act like they aren't already used to addressing that "problem." I'll go
back to the Bustamante, simply because it is convenient right now.
Bustamante recieved a multi-million dollar donation from the Native
Americans, this was not done through a single check, that would be illegal,
instead it was done through multiple smaller checks, each of which ends up
randomized and delayed in processing (USPS is wonderful source of
randomness), so the actual occurance of the donations is scattered acros
several days, from several accounts, by several people, and I'm sure
Bustamante never even looked to see who the donations were actually from,
just that the full amount arrived. The "problem" that you found, is already
addressed, and already not a problem.
            Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-09 17:07:21
@_author: Joseph Ashwood 
@_subject: Is cryptography where security took the wrong branch? 
Now that the waters have been muddied (by several of us). My point was that
3D-Secure (and SET and whatever else comes along) covers a different
position in the system than SSL does (or can). As such they do have a
purpose, even though they may be horribly bloated and nearly non-functional.
Visa at least seems to be supporting the 3D-Secure concept (they should,
they developed it), and looks like anyone can grab the spec from
 . SET seems to be a
bit more elusive, although still available from
 . Mastercard SecureCode
appears to be the current direction for MasterCard it's available at
 . All of these differ in
target from SSL in the same way, they are designed to address the
Buyer-Issuer link (although some not as simply as others, e.g. SET). And yes
I am using a much simplified view of the credit card transaction, with only
3 (buyer, seller, issuer) parties instead of the absurd number actually
present in a real transaction (buyer seller, issuer, accepter, processor,
central card distributor, plus whoever I missed), I did this for clarity.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-26 18:26:16
@_author: Joseph Ashwood 
@_subject: Tinc's response to "Linux's answer to MS-PPTP" 
And a response. I have taken the liberty of copying the various portions of
the contents of the webpage to this email for response. I apologize for the
formatting confusion which may mistake Peter Gutmann's comments with those
of the semi-anonymous misinformed person under scrutiny.
I would have CC'd the author of the response page, but it fails to mention
an author, in spite of the "Comments are welcome" statement at the
writeup which he
identifies several security issues
some flaws or
daemon like tinc.
encourage you to read all the
documentation and source
Proposed solution: provide the option of a "full IV and move the sequence
number out of the ciphertext," note that this is an _option_, instead of the
necessary for security always.
Simply put this is unacceptable from a security standpoint. The view taken
is that the extra 128 bits represents a significant overhead in the
communication. So I did the math, sending the extra 128 bits over a 52kbs
would take 0.002 seconds, and over a T1 the time cost is an absolutely
enormous 0.00008 seconds. The other consideration is the potentially the
computation time argument, but SHA-1 is used regardless, the computation
time is identical. There is no justification in even a dialup environment
for not using the full HMAC.
 Yeah authentication is such a minor thing that major flaws are
completely aceptable.
I really wish people would actually read documentation *before* making
stupid claims like this, in fact to quote the OpenSSL docs "These functions
implement a cryptographically secure pseudo-random number generator (PRNG).
" Any claim that OpenSSL implements a "real" random number generator are
completely false.
Read the docs, the message has 0 entropy (actually marginally above 0, but
these are simple rounding errors), that's what a pseudo-random number
generator means.
There goes that authentication doesn't matter problem again, remind is tinc
supposed to have any sembalnce of security? or is it just a stupid toy
designed for young children to keep they're even younger siblings out of
their personal matters?
send. A
Whoever designed and stated this has no idea about cryptographic security.
Using a "part" of a shared secret, generated by a pRNG on only one side,
introduces horrific flaws in the protocol. Pretending that poorly done RSA
encryption magically solves the problem will only risk everything that has
ever been encrypted using tinc.
Good luck keeping that assumption true, with the oracle attack listed above
that won't stay true very long at all.
But he does, he spoofed each connection and acted as initiator for both, now
it's a simple matter of resending. Your entire model is based on a
misunderstanding of what RSA does and does not offer.
What you're missing is that the connection iniator sets all the keys and can
determine all the keys (assuming the uncontested simplified message flow is
correct). Mallet can very easily perform a complete man-in-the-middle attack
skipped, configuration is the least of the issues here.
Everywhere in the authentication protocol does this happen. RSA is used only
once, and only a public key is necessary. From there the keys are a
derivative of the same key, so the same key is used for encryption and
My guess is that you will once again use it in an insecure method, use
either signed ephemeral keys, or introduce randomness form both sides,
otherwise you will have the same problems from slightly different angles.
Completely incorrect. When Mallet(Alice) contacts Bob, Bob is authenticated,
but Mallet(Alice) is not. Additionally Bob is not properly authenticated due
to the gaping holes in the initial protocol.
Exactly the problem. Once a message type has been sent in the connection,
change the message type (even if it's just a transfer number), this will
address yet another gaping hole.
You provide all the chosen ciphertext information Mallet could want. You act
as an oracle.
Only the really bad ones, all the ones that are used by real cryptographic
engineers miss attacks by large amounts.
And tinc goes out of it's way to avoid any semblance of good security.
Someone doesn't pay much attention to what they write. Both SSL and SSH
_ARE_ VPN protocols, that you don't recognise them as such reflects a great
deal on lack of knowledge in the area of security.
Ummmm, do you realize how dumb that sounds?
They may not be perfect but in neither case can Mallet do as much damage as
easily, even the recent break in OpenSSH did not allow a compromise as big
as even the smallest of the problems briefly explored in tinc.
Yeah but the "great names" admit they are wrong, and fix things. You have
instead taken every possible moment to insist that tinc is good, something
that even the barest educated layperson can see is simply and completely
I think here Gutmann went a bit overboard in his recommendation, but
regardless the idea that someone should replace SSH/SSL with something
designed by an amateur without the knowledge necessary to make it correct is
a bad idea. In fact I have several years of experience and I have a
potential replacement protocol that I think may in some cases be better than
SSL/SSH, even with my experience I have held off publishing it for about 4
years now while I verify that it will in fact stand up to attack. How long
was it between the inducement of this idea and it's release?
Actually you have that backwards. In security you have every obligation to
justify every single portion of every line of code in every product.
However, SSL is far from the be-all, end-all of security. In fact if it were
Gutmann would have simply recommended that everyone use one of the many
SSL-VPN products available (in spite of the errant belief that SSL is
"unsuitable for VPNs").
If anyone wants to make a toy that is only useful at keeping baby siter
grade atackers out tinc is suitable (at least until someone writes a small
script to perform any of the numerous breaks outlined by Gutmann), for
anyone that actually needs to protect something worth more than $0.05 use
something that offers real security.
                    Joe
Trust Laboratories
Changing Software Development

@_date: 2004-04-12 18:00:26
@_author: Joseph Ashwood 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to authenticate software releases 
Sorry about being late to the party, I've been a bit busy lately.
I'm not quite sure that's a good solution, that random tail provides exactly
what the attacker needs to make this as easy as possible. Since the random
tail cannot be know beforehand it cannot be known by the user of the patch.
If anything this would actually make an attack easier. It is only if the
random data is from a _bad_ random source that you might actually gain some
security (a bad source would at the very least have redundancy, internal or
external, that could be verified by the end user, making it more complex to
compute valid numbers). Instead it would probably be more useful to include
the same random number between each file, this should short circuit all but
the most fatal of hash flaws, but might open up other possibilities (I don't
have the time right now to prove things about it).
On a related note does anyone happen to know of any useful papers on
patching, specifically patch integrity/source verification?
                Joe

@_date: 2004-04-28 13:21:48
@_author: Joseph Ashwood 
@_subject: Can Skype be wiretapped by the authorities? 
While Skype is generally rather protective of their protocol, there have
been leaks, in fact one elak that I am aware of was to me personally,
unfortunately I do not have the protocol any more it just wasn't worth
saving. With that said the protocol is horribly and completely worthless,
they brag about using 1536-2048 bit RSA, but what they dont' tell you is
that when I saw the protocol the key was directly encrypted without padding,
it's also worth noting that when I said "key" that wasn't a typo, there was
only one, although it was hashed to create two. There was a complete lack of
message authentication, a complete lack of key verification, a complete lack
of one-timeness to the transfers, basically a complete lack of security,
even their user verification was flawed to the point where it was completely
worthless. Assuming that they have not changed their protocol substantially
(likely considering no one would listen to the individual that leaked it to
me, and hence was given the breaks) the protocol is still horribly insecure,
and pointlessly complex. The ONLY functional security it has is that it is
peer2peer and as such it is harder to eavesdrop.
                            Joe
Trust Laboratories
Changing Software Development

@_date: 2004-08-22 13:42:03
@_author: Joseph Ashwood 
@_subject: On hash breaks, was Re: First quantum crypto bank transfer 
Since the rest has been covered quite well, I will instead focus on the comparison of AES and SHA-0, RIPEM, MD5, etc.
----- Original Message ----- Actually for years the cryptography community has been saying "retire MD5," SHA-0 has been required to be replaced by SHA-1 for some time, the RIPEM series is functionally-speaking unused and represented the only real surprise. Except for RIPEM there were known to be reasons for this, MD5 was known to be flawed, SHA-0 was replaced because it was flawed (although knowledge of the nature of the flaw was hidden). Even with RIPEM (and SHA-1 for the same reason) I have plans in place (and have had for some time) the move away from 160-bit hashes to larger ones, so the attack on RIPEM had little effect on me and my clients, even a full attack on SHA-1 would have little effect on the clients that actually listen (they all have backup plans that involve the rest of the SHA series and at the very least So basically I encourage my clients to maintain good business practices which means that they don't need to have belief in the long term security of AES, or SHA-1, or RSA, or ......... This is just good business, and it is a process that evolved to deal with similar circumstances.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2004-08-24 15:32:24
@_author: Joseph Ashwood 
@_subject: On hash breaks, was Re: First quantum crypto bank transfer 
The key expansion problem is why the rest of the SHA series is present, and Whirlpool is present because of the fundamental flaw problem. The truth is that having a diversity of options for this is simple enough, it takes only a small amount of additional work to allow a cryptographic function to be easily replaced, and making it replacable by 1000 is only marginally more difficult than 2, the four I listed are well-built, which is why they are the recommended ones.
I think it would be important to change the phrasing a bit to make the odds more quantifiable, simply chagne "At the next Crypto" to "By the end of the next Crypto." With that said considering history, I would've put the odds at ~~5:1 (Current hash functions seem to be broken quite often, and being the house I want the odds in my favor). But you are correct in that this represents a major advance in the state of the art, one that has taken large portions of the security community completely blind, I simply took the opportunity to push the concept of good business planning into this as a way that allows a good escape plan should anything happen.
Very different odds actually, we as a group have a much better understanding of block ciphers than hash functions, as evidence the just published 4 for the price of 2 break (cryptography list post by "Hal Finney" Subject: More problems with hash functions 8/20/2004). However AES has one of the smallest security margins available, so let's put it around 10:1, I really don't expect a break, but I would not be excessively shocked to see one made. It is for this very reason that again I recommend to all my clients that the have backup plans here as well, all the AES finalists, and Camellia because of it's Nessie selection.
SHA series     1:1
AES               3:1
Whirlpool       3:1 (even though it wasn't asked)
Camellia         3:1
Of SHA and Whirlpool being felled by the same attack in the next 5 years AES and Camellia by the same attack within 5 years 30:1
SHA in five years because the SHA methodology is showing some cracks, there are only minor differences between SHA-0 and SHA-1, and the differences between SHA-1 and SHA-256/384/512 are basically just matters of scale, I expect to see a major break against the methodology within 10 years, and with the current renewed interest in hash functions I expect the manpower to be available very soon to find that break.
AES is a very solid algorithm, but it's security margin is too close for me, this is always solid evidence that a break may be just around the corner, that the evidence is that various agencies don't have a break is irrelevant, the current evidence is that the general cryptographic community is < 10 years behind and gaining quickly..
Whirlpool has the same odds as AES because the underlying cipher is based on the same methodology, by the same people, so if it has a flaw it is likely to be extremely similar.
Camellia simply does not have the examination behind it that the AES finalists do, something that makes me nervous and why it is only a backup SHA and Whirlpool are unlikely to all at the same time because they have fundamentally different cores, SHA is a hash constructed primitive, Whirlpool a block cipher constructed primitive based on a chaining mode. This makes the odds of a single attack felling both slim at best. This odd is probably slanted too far in my favor.
AES and Camellia by the same attack is more likely because the tools against block ciphers are generally cross borders capable, and the differences between the styles in Camellia and AES are simply not great enough to prevent this. The difference in the styles though represents the additional 3.333:1 odds.
All my odds on this are conservative and based on sloppy meanings (you and I may have very different meanings for "substantially better than brute force), but I believe them to be conservative by approximately the same amount. Obviously these odds are dependent on variables that are not covered, for example if some information only requires 2^80 security the odds of AES surviving 50 years is vastly better than the odds I gave, but if it requires 2^255.999999 the odds of a break are much higher, and for such a case I would already be recommending a layered solution (e.g. 3-AES).
                Joe
Trust Laboratories
Changing Software Development

@_date: 2004-12-08 20:29:45
@_author: Joseph Ashwood 
@_subject: 3DES performance 
Good estimates for the speed of many algorithms can be found at  , while the system is a bit old, the numbers are still relatively valid, considering that you will have other overheads involved. Just to save you a trip 3DES comes in around 10MByte/second, and AES up to 6 times that speed.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2004-06-03 01:37:52
@_author: Joseph Ashwood 
@_subject: A National ID 
Although I am against any national ID, at least as far "terrorist
identification" goes (note that the Social Security Number that every
American has IS a national ID card), I feel that a discussion on how to do
it properly is a worthwhile endeavor.
----- Original Message ----- The solution then is obvious, don't have a big central database. Instead use
a distributed database. I first suggested this concept some time ago on
sci.crypt. It's very simple, use cryptography so we don't have to be
concerned about duplication (although fraudulent acquisition of valid id
would be an issue). Issue each person a Flash RAM card, on the card is
biometric information, name, birthdate, etc, a Law Enforcement Only Field,
and a signature across all the information, most importantly DO NOT print
anything resembling what we currently see as an ID card (no picture, no
drivers license number, etc) just print a name on the card for ease of card
identification. At this point (assuming the cryptography is good) people can
make as many copies as they'd like, it's not going to make any difference.
The Law Enforcement Only Field (which I'll call LEAF for historical reasons)
serves a unique purpose, it is either a random number, or an encrypted old
identity. There are several possible reasons for the old identity;
undercover police, witness protection, support for pseudo-nyms, etc. This
field allows the police and only the police to identify undercover officers,
and provides tracability back through the process to identify granting a new
identity to someone.
The most important part though is the search time required for verifying an
ID. In the case of a giant central database it is O(log(n)) time, with the
cryptographic ID it is O(1). This reduces the cost of the national overhead,
while a database is still necessay for reissuing, and a new signing setup is
required, the access requirements are reduced by several orders of
magnitude. Further reduction comes from the ability of each police precinct
to have their own local "known" database, as well as every bar/nightclub
having their own banned list without the possibility of cross-corruption,
because there is no direct link. This further increases the security because
access to the main database can even be restricted to key personnel. This
personnel access reduction will again lower the speed requirements for the
central database, probably down to the point where a single Oracle server
with a few Terabytes of disk space could easily handle the load (I come up
with a horrible case size of about 300 Terabytes, and a minimum size of 70
gigabytes for storing only the signature and LEAF because everything else
can be reconstructed). (Sizes assume 1MB maximum data set, and DSA/ECDSA
with SHA-512)
This would also have a knock-on effect of creating a small ID customization
industry, because the ID can take any form-factor within certain reasonable
bounds there is no reason that it cannot be as customizable as a cell-phone.
As for security, this would put the citizen in general control of their
information, and with the minimum database size used would give the citizen
complete control over their own data. The additional overhead for the
current law enforcement databases would be minimal, each entry would only be
expanded by the size of the signature to mark the ID card.
The invasiveness for your average citizen would be minimized because there
is no chance of leakage between the big central database (which could be
very small) and the corner market, because the central database does not
have to be online.
Now as to the level of cryptographic security that would be necessary for
this. It is important to realize that the potential market for fraudulent ID
of this caliber would be massive, so a multi-decade multi-trillion dollar
effort to break the key is not unreasonable. This poses a risk of a
magnitude that cryptanalysts really haven't dealt with. Even at the level of
protecting the drivel from Shrub II, the possibility of a multi-decade,
multi-trillion dollar is simply inconceivable, and it is important to
remember that this signature has to remain secure not for a few years, or
even a couple of decades, it has to remain secure for longer than the
longest concievable lifespan for a human, which means 150 years (I've
rounded up from the record), which is a timeframe that we cannot even
conceive of at this time. A 100 trillion dollar, 150 year effort to break
the security is simply beyond our ability to predict cryptographically, with
Celerons at about $35 per GHz right now, that timeframe works out to
approximately 2^95 (again being generous to the attacker), that already
means that SHA-1 cannot be used simply because the workload is available to
defeat it. With just the march of Moore's law we would need >2^235 security,
SHA-512 simply isn't big enough. To have any safety margin at all would
require something like SHA-1024. Going further combatting the probability of
Quantum Computers would require something like SHA-2048, but now we're
getting into absolutely absurdly sized numbers. The only way to combat this
would be to accept a small number of fraudulent users and replace cards
every couple decades which would limit the requirements to an immediate
2^128 and a movement to 2^256 within a couple decades. The down side of this
is that we quickly end up exactly where we are now, even if the entire
population is cleaned of fake IDs, once the reissue starts happening we'll
see fake IDs creep up again.
Certain people may contend that if we force ID renewal on everyone at the
same time, that this simply won't happen. That is true, iff you succeed in
forcing EVERYONE to switch on the switch date. Let's face it, I look old
enough that no one doubts if I'm old enough to drive, no one doubts if I'm
old enough to buy wine, no one would doubt that I'm old enough to buy
cigarettes, so I will only be carded if pulled over by the police, which can
be avoided by simply not driving, I could live with my current ID for the
next 50 years and not have any real problem (the oldest expired license I've
heard of in active use was 47 years expired, so this is not unreasonable to
attempt) the security MUST be good for at least 50 years, and preferably 100
(at 100 years of age the field of options is narrow enough that they can be
a special case), that once again leaves us in the "we simply don't know how
to do it" stage.
The security requirements for a proper installation are so high that we
simply cannot do it, we can do better than we have now, and make it
extremely costly for the fake manufacturers, but the security problem is
simply too hard.
                    Joe

@_date: 2004-06-10 01:02:56
@_author: Joseph Ashwood 
@_subject: threat modelling tool by Microsoft? 
I played with it for a bit, short story: it crashed. Long version: it feel
very clunky, and lacking in features. The output isn't very pretty either,
and rather difficult to understand. Additionally, although it can find users
easily (in fact it already does this) it doesn't import them without manual
intervention. With a large userlist though I suspect that the user listing
interface would become rather unusable.
With that said, for a small installation it should be fairly usable, and
certainly better than nothing. For a large installation though or a
situation where depth of security analysis is necessary it will probably
become unwieldly, and it seems likely to collapse under it's own weight.
                    Joe

@_date: 2004-06-18 13:57:42
@_author: Joseph Ashwood 
@_subject: A National ID: AAMVA's Unique ID 
; Sent: Thursday, June 17, 2004 10:31 AM
I think you misunderstood my point. My point was that it is actually
_easier_, _cheaper_, and more _secure_ to eliminate all the silos. There is
no reason for the various silos, and there is less reason to tie them
together. My entire point was to put my entire record on my card, this
allows faster look-up (O(1) time versus O(lg(n))), greater security (I
control access to my record), it's cheaper (the cards have to be bought
anyway), it's easier (I've already done most of the work on defining them),
and administration is easier (no one has to care about duplication).
I think they are drawing the line a bit finer than either of us would like.
They don't call it a national ID because it being a national ID means that
it would be run by the federal government, being instead run by state
governments, it is a state ID, linked nationally.
As I said in the prior one, I disagree with any efforts to create forced ID.
Well then create a High-Security ID card company, build it on the technology
I've talked about. It's fairly simple, file the paperwork to create an LLC
with you and Robyn, the LLC acquires a website, it can be co-located at your
current office location, the website talks about my technology, how it
allows the unique and secure identification of every individual, blah, blah,
blah, get a credit card issued in the correct name. They'll almost certainly
let you in, you'll look and smell like a valid alternative (without lying
because you could certainly offer the technology), if you really want to
make it look really good I'm even willing to work with you on filing a
patent, something that they'd almost certainly appreciate.
Of course it won't, their "mission and service" is to offer the strongest
identity link possible in the ID cards issued nation-wide, as such the
citizen's course of action has to be to govern the states issuing these
identication papers. However, if you offer them technology to actually make
their "mission and service" cheaper, more effective, and as a side-benefit
better for their voters. Besides, if you can't beat them (you won't stop
them, no matter what you do) at least improve the situation, you could
easily become a far wealthier individual and improve our general security
versus the alternatives.
Very much so, but I also realize that there are far more people who are more
than willing to be "ear-tagged" than those of us willing to fight, as such
what we need to do is fight on a fundamental basis, the most fundamental
benefit offered by good technology in doing this is the cost savings
(regardless of the improved security). As such we need to wage a war on two
fronts, on one front we work to destroy the basis on which they can enstate
these measures, this will work to scale-back the deployment. The second
front is to make it more secure as it does get rolled out, and to build the
technology in such a way that their invasive tactics can be thrown out by
the voting population without destroying the core usefulness of the system
(e.g. it can still be a driver's license).
Such a two-front war is complex and difficult, but if the first front is
completely successful we have gained our desires, the second-front is only
there to erode the invasiveness and provide an abort-path for getting rid of
the technology.
I guess my further point is that sometimes disruptive activities only
results in them hiding from you while they work, but delicate adjustments
can result in real changes.
                Joe

@_date: 2004-06-18 13:58:23
@_author: Joseph Ashwood 
@_subject: recommendations/evaluations of free / low-cost crypto libraries 
Generally the two most suggested free products are Crypto++
( and OpenSSL
( I have used both and both are very good toolsets,
each has small advantages, but mostly it's just a programming style
As a personal preference, I generally preper OpenSSL for most purposes,
because the interface feels better to me, but lately I've been using
Crypto++ more because it supports a wider selection of algorithms (including
the very important for me ECC variants) which is recently of extreme value
to me.
I will say that if either of these was pay-ware, I would gladly pay for it.
                Joe

@_date: 2004-06-30 13:10:07
@_author: Joseph Ashwood 
@_subject: Question on the state of the security industry (second half not necessarily on topic) 
I am continually asked about spam, and I personally treat phishing as a
subset of it, but I have seen virtually no interest in correcting the
problem. I have personally been told I don't even know how many times that
phishing "is not an issue."
I personally know it's an issue because between my accounts I receive ~3-5
phishing attempts/day, and the scams apparently account for a major portion
of the GNP of many small countries.
In large part that's the way it looks to me as well. We have an effectively
impotent security community, because all the "solutions" we've ever made
either didn't work, or worked too well. We basically have two types of
security solutions the ones that are referred to as "That doesn't work, we
had it and it did everything it shouldn't have" and those that result in "I
don't think it works, but I can't be sure because we were never attacked."
The SSL/TLS protocol is an example of this second type, I am unaware of any
blackhats that bother attacking SSL/TLS because they simply assume it is
impenetrable. At the same time we have the situation where Windows is
continually not because it is less secure than the others, but because it is
_believed_ to be less secure than the others, so the Windows security is
clearly of the first type. The biggest problem I've seen is that we're
dealing with generally undereducated peoople as far as security goes. We
need to start selling that we facilitate a business process, and that
because of this all you will see are the failures, the successes are almost
always be invisible.
Also as with all business processes, there is never a final state, it must
be often reanalyzed and revised. This puts us in a rather strange situation,
where somethign that I have always offered becomes important, we become an
outsourced analyst, almost an auditor situation. To build this properly the
security model that is constructed needs to be built to include emergency
threshholds and revision timeframes. By supporting the security process as a
business process it allows the concepts to more easily permeate the CXO
offices, which means that you are far more likely to make more money, build
a long term client, and create a strong security location.
To make the point clearer, I have ended up with clients that were previously
with better known cryptanalysts, including some worldwide names. These
clients have been told by their previous consultants that there security is
good, but their consultant never told themthat it needs reanalysis, they
never encouraged the creation of a business process around it, it was always
"Ask me when you have questions." I did not poach these clients, they left
their previous consultants, and found me through referrals. These
relationships are extremely profitable for me, for many reasons; I actually
cost less than their prior consultants, but I make more, because everything
is done quickly, efficiently, and effectively.
This security process builds stronger security, and while I admit I am still
rarely asked about phishing, and even rarer is my advice listened to, my
clients are rarely successfully hacked, and have lower than average losses.
Our biggest problem is that we view the security process as distinct from
business processes. I truly wish I could make the Sarbanes-Oxley 2002
( act
required reading for every security consultant, because it demonstrates very
much that proper security consulting is actually a business process.
Getting back to the topic, by doing this we can help them move from the
"dick swinging" phase to a best practices security infrastructure used
accurately and appropriately. We also need to start putting our money where
our mouth is, I've seen too many "security consultants" whose primary job
was to sell the add-on services available from their employer, instead we
need to follow Sarbanes-Oxley in spirit and seperate our security auditing
from other services, even  to the point where I am not invested in any
company who's products I recommend (obviously I'm not shooting myself in the
foot and investing in their competitors either). Unfortunately, a large
number of cryptanalysts will have a lot of penance to take before they can
do this because their "dick swinging" has been highly visible.
                Joe

@_date: 2004-05-08 20:04:34
@_author: Joseph Ashwood 
@_subject: The future of security (bulk reply, long) 
I've moved this to the top because I feel it is the most important statement
that can be made
Hadmut said :
----- Original Message ----- I actually expect quite the opposite, we seem to be reaching an age in
cryptanalysis where we are developing techniques faster than they can be
functionally applied, and the speed of development is only increasing here.
We've now gone from a time when we were seeing a new functional attack about
every five years (differential to linear), to now just during the AES
selection proces we had a number of potential new avenues opened up. I
expect this trend to continue for a while, and the news taht this generates
should bring greater light, and more active people to studying cryptography.
I expect this trend to continue for approximately 1 human generation (about
20 years), but that human nature being what it is, that the second human
generation in this timeframe will have substantially fewer cryptanalytic
Actually I'm seeing an increasing trend in moving away from RSA and DH
because the keys are becoming too big. The required key length to match the
strength of AES-256 is simply too large to offer functional speed, instead
we're going to have to switch over to the assymptotically superior
encryption/decryption/signing/verifying algorithm, because of this we should
see a major increase in the research moneys applied towards public key
techniques, this compounded with my expected increase in the number of
cryptanalysts should result in some very interesting times.
I agree.
Again I have to disagree with you, we're already seeing some backlash
against SSL/TLS, where many people are beginning to see the value in
protecting the data not the link. This methodology fairly well eliminates
the usability of SSL/TLS, the added complexity of the new PK algorithms will
almost certainly spell doom for the current protocols in use.
Again I have to disagree, I can only speak for what Trust Laboratories is
doing, but we are at this moment working on projects that will lower the
necessary threshhold for PKI implementations (through client proliferation).
This combined with the already solidly known presence of NGSCB in the
majority of future PCs should have the added effect that, while
Verisign-like PKI may remain unusual, the availability of what can be
treated as a smartcard in every computer will certainly increase the
availability of PKI to the common man.
For the short term yes, but longer term I actually think that HTTPS will
diminish, in fact some measurements are already showing a trend where per
capita web usage is already decreasing, so HTTP may soon be decreasing, lead
ing to an obvious decrease in the usage of HTTPS. This combined with the
"protect the data not the link" movement should have substantial further
Agreed, but I also think we'll start seeing distributed file system, I know
we are working on them, and have already had some interest form companies.
These distributed file systems will make use of smart cards (although the
form factor WILL be different). With the proliferation of high speed data
connections (US cell phones are already available at 150 Kbps, and 3G can
bring speeds of up to 1Mbps, in the next few years WiMax, and great future
cell potential e.g. Flarion) I suspect that removable storage will actually
decrease, that leaves moving those USB/removable drives over to distributed
file systems or even in some cases p2p networks (more on this from Trust
Laboratories in the future) which will massively reduce cost. I'm even
expecting that we will see cell phones begin to include streamed audio files
for playback, effectively eliminating the need for large quantities of
Very much agreed, the VPN market will grow substantially, and I believe
again long-term the IPsec market will grow at the expense of the SSL VPN
market. Longer term I'm expecting that within 20 years IPsec will be
outdated by the movement of VPN technology into TCP/IP (or it's replacement)
which would at the same time eliminate SSL/TLS.
Here I'm not so sure about the cryptographic implications. The truth is that
most phone conversations are not worth protecting, and that the common man
does not care about creating coverfire for those that do need it. I'm
actually more expecting that those that do require this will for now run
over TLS (see SIP specification) and that in the future these will be done
over IPsec, until both are outdated.
Of course.
CC is already being done, the Visa 3D-Secure initiative which should become
the Visa requirement (support only, 2008 should see saturation) next year
should vastly improve the situation.
Short term I agree, but longer term there's already a movement that I can't
discuss (sorry guys NDA) where the form factor is changing.
I'll actually go a step further, I believe that within the next decade we
will see strong cryptography blanketly allowed in virtually every country in
the world. The reasoning is fairly easy to follow, Visa has the ability to
prod as many politicians as they would like, and they have found that strong
cryptography is invaluable to them. They will almost certainly push for
government to step out of the way of cryptographic advances. Although we
will probably see an increase in laws that effectively prevent cryptanalysis
in the short-term, longer-term we will see most of these laws voided.
I agree there will be casualties in the security area, people are
increasingly using email as equivalent to a phone call andnot expecting
security. This means that PGP is likely to become less used, but I also
predict that we will see technologies take it's place. For example, for
secured communication secure p2p connections can be used safely, and
businesses tend to like having specific processes that are for security,
which means that such things are kind to business.
It is business that will lead the next crypto revolution as they find that
strong cryptography is of great value to them, already we've seen them adopt
SSL/TLS broadly, with many even using it for purposes that it really isn't
required. This business leadership will continue and many of the
technologies that we'll be using in 5-10 years will have been designed with
businesses in mind instead of revolutionaries.
I have to agree, S/MIME seems to be becoming extinct even as it becomes
usable for everyone. SMTP over TLS is probably not going to see much action
either, again because of the "protect the data not the link" movement.
I see cryptography more acting in support of law in the future, we're
already seeing an impact in cryptography of the Sarbanes-Oxley act which has
already formed a small boost in the cryptographic security of many companies
(accountability requires strong identification) especially when dealing with
section 404 (generally regarding offshoring, but also applies to remote
offices), and with the requirements for inproved reporting speed we should
see a strong increase in the use of computers over postal service, which
will again result in cryptographic security being called in.
I'm personally nto sure that 1-2 years is long enough for that bust cycle, I
suspect more than in about 5 years we'll see more users, simply because
users generally requries competition something that is severely lacking from
QC, but I agree that it will generally be of little use with the suppliers
becoming niche players, but never quite disappearing.
I absolutely agree, short-term, but I believe longer term that certain
hidden trends will emerge (again sorry NDA, but also trade secrets here)
that will start to move self-signed certs out, simply because hierarchically
signed certs will be just as available, if for no other reason than NGSCB
and the like.
I agree as well, the hier.. certs I mentioned above will only proliferate
because of the hardware sales, but they actual signing party will be less
relied on, as we move more towards "protect the data not the link"
I'm not so sure, I think the general programming populus has had "use SSL"
ground into them so far, that we'll see a short-term increase in it's use,
longer term I think other protocols will take it's place, but just as with
the certs only because it will be easily available and cheaper to implement
than a home-grown solution.
I'd suggest instead of "create self-signed cert now) we simply begin signing
every email with a self-signed cert and let the market adapt. Using XML-SIG
or PGP would leave the email still readable by those individuals that have
not switched over. Once the entire population is using self-signed certs for
signing, then we can also begin encrypting. But it's not gonna happen
anytime soon.
The encryption of IM is already happening, with most clients already
supporting corporate servers for security (and encryption either local or at
the server), it should proliferate as the IM solution creaters begin to
realize that it simply is not worth maintaining what is effectively two
protocols so reduce the protocol overhead to one by removing the
In addition I'm also predicting a split in p2p networks, those designed for
businesses will fully identify the introducer (and possibly the
intermediaries), and those designed for anonymity. Each has it's place, but
the business networks will have very little in the way of illegal content,
while the anonymous ones will begin to move towards almost exclusively
illegal content. The reason is simple, if all the legal content you want is
on one network, and that network is available everywhere (again we're
working on it) then there is no reason to place it on the other network. The
unfortunate collateral of this is that the illegal network will be a prime
target for legal attacks, and those that are a part of it will be persecuted
(prosecuted as well, but mostly persecuted).
I agree, the press will go to the startups with all the appeal, but at the
same time I predict that we'll see a proliferation of crypto under
everything. From algorithms to prevent piracy, to secure distributed file
systems much of the idea in many sectors will be that security is a
necessity and as such it will simpy be there.
Agreed, except for the last part. As the ability to do wire-speed
cryptography continues to spread I actually believe that we'll see
cryptography spread, because there will be no reason not to use it, and it
increases the paranoia allowed in the system.
Spam and viruses will not be defeated in the lifetime of any living person,
unless email completely disappears (which would only get rid of spam).
Viruses are here to stay, but the immune system for computers will become
better, leading to greater difficulty in writing viruses (this of course
assumes that either Windows shapes-up or is eliminated).
DRM is a different story. I believe a solution will be found, but not down
the current investigation avenues. I think instead we will see light-weight
DRM used to supplement legal and education activities, along side progress
towards deriving revenue from the "illegal" content. This revenue from
unlicensed content will allow DRM to be used only half-heartedly, and only
to stop huge-scale distribution.
I still forsee cryptography everywhere, but I also see it being generally
hidden, similar to the safety provided by the airbag in a car.
            Joe
Trust Laboratories
Changing Software Development

@_date: 2004-09-03 16:23:58
@_author: Joseph Ashwood 
@_subject: Kerberos Design 
Be brave, there's more convolutions and trappings there than almost anywhere
Actually the primary reason Iv'e heard had more to do with the licensing
costs (at the time they were not free) than with anything else. You will
however find PKI extensions to Kerberos, don't remember the RFC off-hand.
it was broken, and patched, and broken and patched, until it was relatively
recently qualified to be implemented in Windows, so you're not likely to
find much in the way of well thought-out arguments governing the little
details. In fact many of the decisions seem to be based on "My pet project
is . . . ."
Kerberos is a very sane choice, it may not be the cleanest design ever but
it has withstood a great deal of analysis. Actually, I was a member of a
group that was working on a replacement for Kerberos because of it's age and
potential issues in the future, but we fell into substantial disarray, and
eventually it collapsed. Given this, I can confidently say that it is unlikely that you will find something in the Kerberos vein taht is newer.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2005-08-01 20:20:51
@_author: Joseph Ashwood 
@_subject: Possibly new result on truncating hashes 
Actually it does. Such an attack would reduce the difficulty of producing a collision in SHA-256 to 2^(64+(96/2)) or 2^112. The math for this is fairly easy, the remaining 96 bits will collide in on average 2^(96/2) tries, since it takes 2^64 work for each of these tries, we get 2^112 work, hence an attack on the original hash has been found.
There's the mistake. To find a collision in the remaining bits requires 2^(96/2) work, not 2^96 work. For a chosen initial value you will of course have the 2^96 work, but there you'll only have 2^(64+96) work instead of 2^256, the attack still works.
                Joe

@_date: 2005-12-01 01:27:57
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
The random numbers tested were almost certainly not all odd, no filtering was done on random.
I'm running an abbreviated test right now, and it's looking less impressive, I have to assume I'm hitting a bad streak somehow. Real bad, 30 numbers tested, no primes at all so far, I see one that has passed 79 tests. I have to assume I'm missing something really stupid at this point in my new number chooser that I don't have the time to find right now. So I'm asking for anyones help in pointing out to me, why after I let it go the full 128 runs (that is 128 numbers that passed a single round of MR) I didn't get a single number to pass more than 79? Did I just hit a really, really bad streak?
The exact code for the function and the support variables :
 static int lenNum = 512;
 static SecureRandom rand = new SecureRandom();
 static BigInteger two = BigInteger.valueOf(2);
 static BigInteger chooseValue()
 {
  //pick a random integer
  BigInteger curNum = null;
  byte [] rawBytes = new byte[lenNum/8];
  rand.nextBytes(rawBytes);
  curNum = new BigInteger(rawBytes);
  //make sure it's odd
  if(curNum.mod(BigInteger.valueOf(2)).compareTo(BigInteger.ZERO) == 0)
  {
   curNum = curNum.add(BigInteger.ONE);
  }
  //it's 0 or negative try again
  if(curNum.compareTo(BigInteger.ZERO)<=0)
  {
   return chooseValue();
  }
  return curNum;
 }
This should choose a 512-bit random odd positive number, unless I'm missing something horribly, horribly braindead.
Anyway, back to trying to design a "cool" user interface (anyone who knows me knows that the cue to begin laughing, I can't design a UI for sh*t).
                    Joe

@_date: 2005-12-04 15:54:38
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
Ok after making that change, and a few others. Selecting only odd numbers (which acts as a small seive) I'm not getting much useful information. It appears to be such that at 512 bits if it passes once it passes 128 times, and it appears to fail on average about 120-130 times, so the sieve amplifies the values more than expected. Granted this is only a test of the generation of 128 numbers, but I got 128 primes (based on 128 MR rounds). For the sake of full code review and duplication I've appended the entire 64 lines of code. You'll see I made a few optimizations, and removed writing the data to a csv. I developed compiled and ran it only through Eclipse.'
                Joe
import java.math.BigInteger;
import java.security.SecureRandom;
import java.io.IOException;
public class millerMain {
 static int numTests = 128;
 static int lenNum = 512;
 static SecureRandom rand = new SecureRandom();
 static BigInteger two = BigInteger.valueOf(2);
 public static void main(String[] args) throws IOException {
  BigInteger curNum = null;
  int totalPrimes = 0;
  int [] successes = new int[numTests];
  int failuresBetween = 0;
  for(int i = 0; i < numTests; i++)
  {
   failuresBetween = -1;
   //choose starting number
   do
   {
    curNum = BigInteger.ONE.or(new BigInteger(lenNum, rand));
    failuresBetween++;
   }
   while(testOnce(curNum) == false);
   System.out.println("Failed " + failuresBetween+ " times");
   //passed once
   //run 127 more tests
   for(int j = 0; j < 127; j++)
   {
    if(testOnce(curNum))
    {
     successes[i]++;
    }
   }
   if(successes[i] == 127) totalPrimes++;
   System.out.println("Test Number "+i+" successes "+(successes[i]+1)+" Total Prime so far "+totalPrimes);
   BigInteger temp = BigInteger.valueOf(successes[i]);
   String num = temp.toString();
  }
 }
 static boolean testOnce(BigInteger N){
  BigInteger A = null;
  // 0 < A < N
  A = new BigInteger(lenNum, rand).mod(N);
  if(A.compareTo(BigInteger.ZERO) == 0) return testOnce(N);
  BigInteger Nminus1 = N.subtract(BigInteger.ONE);
  //shouldBeOne = A^(N-1)     mod N
  BigInteger shouldBeOne = A.modPow(Nminus1, N);
  if(shouldBeOne.compareTo(BigInteger.ONE)!= 0) return false;
  return true;
 }

@_date: 2005-12-05 01:06:21
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
Apparently, they are, I'm ran a sample, but even with the added second sanity check, every one of them that passes a single round comes up prime.
I then proceeded to move it to 2048-bit numbers. It takes longer and the gaps between primes is averaging around 700 right now, but once again if it passes a single test it passes all 128+128. This sample is currently statistically completely insignificant, but even after the currently 8 tries I'd expect something different.
                    Joe

@_date: 2005-12-05 14:03:17
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
I should've said that the the quantity of numbers that failed the first test between each success was about 120-130. Apparently, even sieving based solely on "is it odd" is enough to substantially influence the outcome.
                Joe

@_date: 2005-12-07 17:31:22
@_author: Joseph Ashwood 
@_subject: Countries that ban the use of crypto? 
I'm not going to out anyone on this, but even a quick search of Skype finds quite a few individuals who make use of cryptography in China. So I strongly suspect that there is a great deal of lenience on that front. In fact, I have it on dependable authority that there are a number of places in China where the only IM system that functions is Skype.
I have no doubt that China does place controls on it, and it has been published a few places that their telecom industry has a particular distaste for Skype, but it appears that there is more to it.
                        Joe

@_date: 2005-02-16 21:15:32
@_author: Joseph Ashwood 
@_subject: SHA-1 cracked 
I will argue that the threat is realizable today, and highly practical. It is well documented that in 1998 RSA Security's DES Challenge II was broken in 72 hours by $250,000 worth of custom machine. Scale this forward to today, and $500,000 worth of custom equipment and 2^69 is not out of reach for 3 days worth of work. So assuming that your attackers are smallish businesses, you have 3 days of security, and large businesses with a vested interest in breaking your security you are looking at minutes if not seconds before break.
While most uses of SHA-1 actually end up searching for collisions against fixed outputs (e.g. given A find B such that A<>B and SHA1(A) == SHA1(B)), this attack does not immediately cause the collapse of all e-commerce
This attack means that we need to begin the process for a quick and painless retirement of SHA-1 in favor of SHA-256/384/512 in the immediate future and begin further preparations to move to Whirlpool and other hashes in the near future. I say this because with MD5 completely broken, SHA-0 effectively completely broken, and SHA-1 showing big cracks, the entire SHA series is in doubt, and needs to be heavily reconsidered, otherwise we're looking at a continuing failure of hash functions apparently in a yearly fashion until we run out of the SHA series.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2005-02-18 03:11:30
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
Sent: Thursday, February 17, 2005 2:49 AM
I believe you substantially misunderstood my statements, 2^69 work is doable _now_. 2^55 work was performed in 72 hours in 1998, scaling forward the 7 years to the present (and hence through known data) leads to a situation where the 2^69 work is achievable today in a reasonable timeframe (3 days), assuming reasonable quantities of available money ($500,000US). There is no guessing about what the future holds for this, the 2^69 work is NOW.
----- Original Message ----- ; "Cryptography" What you're missing in this is that Deep Crack was already a year old at the time it was used for this, I was assuming that the most recent technologies would be used, so the 1998 point for Deep Crack was the critical point. Also if you check the real statistics for RC5-64 you will find that Distributed.net suffered from a major lack of optimization on the workhorse of the DES cracking effort (DEC Alpha processor) even to the point where running the X86 code in emulation was faster than the native code. Since an Alpha Processor had been the breaking force for DES Challenge I and a factor of > 1/3  for III this crippled the performance resulting in the Alphas running at only ~2% of their optimal speed, and the x86 systems were running at only about 50%. Based on just this 2^64 should have taken only 1.5 years. Additionally add in that virtually the entire Alpha community pulled out because we had better things to do with our processors (e.g. IIRC the same systems rendered Titanic) and Distributed.net was effectively sucked dry of workhorse systems, so a timeframe of 4-6 months is more likely, without any custom hardware and rather sad software optimization. Assuming that the new attacks can be pipelined (the biggest problem with the RC5-64 optimizations was pipeline breaking) it is entirely possible to use modern technology along with GaAs substrate to generate chips in the 10-20 GHz range, or about 10x the speed available to Distributed.net. Add targetted hardware to the mix, deep pipelining, and massively multiprocessors and my numbers still hold, give or take a few orders of magnitude (the 8% of III done by Deep Crack in 23 hours is only a little over 2 orders of magnitude off, so within acceptable bounds).
2^69 is achievable, it may not be pretty, and it certainly isn't kind to the security of the vast majority of "secure" infrastructure, but it is achievable and while the cost bounds may have to be shifted, that is achievable as well.
It is still my view that everyone needs to keep a close eye on their hashes, make sure the numbers add up correctly, it is simply my view now that SHA-1 needs to be put out to pasture, and the rest of the SHA line needs to be heavily reconsidered because of their close relation to SHA-1.
The biggest unknown surrounding this is the actual amount of work necessary to perform the 2^69, if the workload is all XOR then the costs and timeframe I gave are reasonably pessimistic, but if the required operations are dynamically sized mulitplies then the time*cost is off by some very large Even simple bulk computation assuming full pipelining says that 4700 4 GHz to complete 2^69 operations in 1 year, even assuming using full 3.8 GHz pentium 4s instead of a more optimal package only leads to a processor cost of 3.1 million for a 1 year 2^69, dropping that down to 2.4GHz celerons requires 7800 of them, but only $538,000. Moving to DSPs and FPGAs the costs will drop substantially, but I don't feel like looking it up, and as the costs drop the number of processors that can be used increases linearly additionally as the individual speeds drop the purchase cost drops better than linearly. I am quite confident that with careful engineering a custom box could be produced for the $500,000 mark that would do 2^69 operations in the proper timeframe. With deep pipelining any complexity of 2^69 operations could be done in the timeframe, but will scale the price. I suppose I should also point out an unspoken qualifier, I am assuming a large number of these machines will be built reducing the engineering overhead to miniscule, for a one-off project this will likely be the dominant cost.
2^69 work is achievable, the cost multiplier associated will be the determining factor.
                Joe

@_date: 2005-02-18 03:14:26
@_author: Joseph Ashwood 
@_subject: ATM machine security 
Not directly what you are looking for but the BITS standard is applied by banks so should probably fit well within your needs, it is available at  . Also if that doesn't work I've been considering doing a security requirements for Trust Laboratories, and a solid project would give me an incentive.
                Joe

@_date: 2005-02-18 22:46:34
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
Sent: Friday, February 18, 2005 3:11 AM
[the attack is reasonable]
Reading through the summary I found a bit of information that means my estimates of workload have to be re-evaluated. Page 1 "Based on our estimation, we expect that real collisions of SHA1 reduced to 70-steps can be found using todays supercomputers." This is a very important statement for estimating the real workload, assuming there is an implicit "in one year" in there, and assuming BlueGene (Top 500 list slot 1) this represents 22937.6 GHz*years, or slightly over 2^69 clock cycles, I am obviously still using gigahertz because information gives us nothing better to work from. This clearly indicates that the operations used for the workload span multiple processor clocks, and performing a gross estimation based on pure guesswork I'm guessing that my numbers are actually off by a factor of between 50 and 500, this factor will likely work cleanly in either adjusting the timeframe or production cost.
My suggestion though to make a switch away from SHA-1 as soon as reasonable, and to prepare to switch hashes very quickly in the future remains the same, the march of processor progress is not going to halt, and the advance of cryptographic attacks will not halt which will inevitably squeeze SHA-1 to broken. I would actually argue that the 2^80 strength it should have is enough to begin its retirement, 2^80 has been "strong enough" for a decade in spite of the march of technology. Under the processor speed enhancements that have happened over the last decade we should have increased the keylength already to accomodate for dual core chips running at 20 times the speed for a total of 40 times the prior speed (I was going to use Spec data for a better calculation but I couldn'd immediately find specs for a Pentium Pro 200) by adding at least 5 bits preferrably 8 to our necessary protection                 Joe

@_date: 2005-02-20 18:41:18
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
That is only misreading my statements and missing a very large portion where I specifically stated that the new machine would need to be custom instead of semi-custom. The proposed system was not based on FPGAs, instead it would need to be based on ASICs engineered using modern technology, much more along the lines of a DSP. The primary gains available are actually from the larger wafers in use now, along with the transistor shrinkage. Combined these have approximately kept the cost in line with Moore's law, and the benefits of custom engineering account for the rest. So for exact details about how I did the calculations I assumed Moore's law for speed, and an additional 4x improvement from custom chips instead of of the shelf. In order to verify the calculations I also redid them assuming DSPs which should be capable of processing the data (specifically from TI), I came to a cost within a couple orders of magnitude although the power consumption would be substantially higher.
                Joe

@_date: 2005-01-11 23:48:47
@_author: Joseph Ashwood 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
Actually that is not entirely true. While Skype has been getting more than it's fair share of publicity lately surrounding it's security the truth is that shortly after it's first release I personally had a discussion in their forums (should still be there if you find something by holomntn that's the correct one, I haven't discussed anything since). In that discussion it was shown that they clearly did not have a solid grasp on security, nor apparently had anyone of them read the SIP specification. During that conversation, and some future private ones, it has been revealed to me that Skype's security is questionable at best, and that they are in fact basically relying on security through obscurity. It is likely that this will work for quite some time simply because most IM conversations, and most phone conversations for that matter are simply not worth listening to.
With that said, in their favor they do have substantial qualities. Because they effectively form a routed network an intermediate evesdropping attempt will have to sort through a substantial amount of undesired traffic (see Rivest on Wheat and Chaff for explaination of the security offered), this is possible because although there are security holes, the end stream is difficult to determine from random (AES/CBC). This creates a substantial boost in the amount of effort required to acquire a stream of significance unless the endpoints are known. The other big thing in their favor is that apparently very few people want to be bothered by analysing the security, basically if no one is looking it is secure. Additionally, in version 1.1 Skype appears to have begun providing a moving target for a break, between version 1.0 and 1.1 Skype performed some changes to the protocol, while I do not know the exact nature of these, even a simple investigation of the GUI shows some changes (IM someone with a different version you will be cautioned about protocol changes even though security is not listed), this moving target creates the possibility to generate some security through obscurity, and the ability to upgrade the security at a moments notice.
Working against them. The biggest thing working against them is that a growing number of teenagers are using Skype (a significant portion of Gunderson High School in San Jose, Ca actually uses Skype during class, and has been busted by me for it). This poses a substantial risk for common hacking to occur. This is something that I am unclear on whether or not Skype has prepared. As the general populus begins to use Skype more the security question becomes of greater importance (reference the attacks on Windows that go on every day).
With all that said it is important to note that I have no access to the current Skype protocol and I only briefly had limited access to an early one, so my analysis may be substantially off.
                    Joe

@_date: 2005-07-14 16:26:22
@_author: Joseph Ashwood 
@_subject: EMV [was: Re: Why Blockbuster looks at your ID.] 
I'd say that you've fairly well hit the nail on the head. I've actually been meaning to reply to this for about a week now. The truth is that each credit card transaction actually has either 3 or 4 parties; User U, Merchant M, Credit Card Issuer CCI, and Merchant Insurer MI (this is simplified there are generally multiple parties under CCI).
Under legitimate circumstances the process is fairly simple; Legitimate User LU agrees to pay CCI, CCI already has an agreement to pay M, and M supplies the product/service to LU. During billing LU pays CCI, CCI pays M, everyone is happy.
Things are different in the case of False User FU. FU goes to M, FU agrees for LU to pay CCI, CCI (believing FU is LU) agrees to pay M, M supplies the product/service to FU. During billing is where things get strange. LU reports the bad transaction to CCI. CCI informs M and does not pay M. FU gets the product, M accepts the loss. In the normal case MI and M are the same entity so the buck stops there, if MI is seperate from M, then MI reimburses M for some portion.
It's important to understand exactly who loses what when FU is in the picture. CCI loses the commision, generally a small flat fee on the order of $0.35, and a percentage generally <2%, this is not a large amount to lose, and the phone call to report the problem actually costs more than is lost, followed by the filing and tracking of the correct paperwork, this is the ACTUAL loss for CCI. MI loses the cost of the product/service reimbursed. LU loses basically nothing except time. FU obviously gains.
The point being that expecting CCI to foot a multi-billion dollar bill to change the process so that MI doesn't lose the money doesn't make sense. CCI will only work to increase CCIs profits. It is up to MI to pay for the upgraded systems by working with CCI towards CCIs goals (fewer losses for MI also means fewer reports to CCI so fewer losses). LU may be willing to foot part of the bill for the perceived improvements, CCI will only foot the portion that is in CCIs favor, MI will have to foot the majority of the bill and will only do so when it is in MIs favor. With credit card fraud decreasing, it is not in MIs favor to examine it at this time.
                    Joe

@_date: 2005-03-05 21:40:37
@_author: Joseph Ashwood 
@_subject: comments wanted on gbde 
I'll just deal with it piece by piece.
Page 3 "decrypting and re-encrypting an entire disk would likely take more than a day with currently available hardware" is wrong. Assuming 256-bit AES, using a two-pass encryption system, on a 2.1 GHz pentium 4 (currently below low end) this would equal a disk of over 1000GB (numbers taken from Crypto++ benchmarks), personally I don't have a laptop with that kind of space. I have a rackmount server with that space, and my desktop is getting close, but no where near full. Even by page 3 I'm becoming doubtful about the abilities and there isn't any real information yet.
Page 3 again, system supports multiple access paths. Clearly this means that either the entire disk, or per block or per sector or whatever has an encrypted key, this is how they will meet the false criteria above, and the multiple access path. Of course this is a standard technique, and has been around for ages, nothing new here.
Page 4 "It has been said that there is only one really hard problem in cryptography: the problem of key management. GBDE does not try to address that." They're already admitting to heavy flaws, downplaying the abilities of the design, not a good sign.
Page 5 finally begins the actual information.
Page 5 "plaintext sector data should be encrypted with one-time-use (pseudo-)random keys" serves no purpose if a strong mode is used.  The only purpose this serves is to slow the system down as additional searches have to be made. This is claimed to provide protection from when AES is broken. It offers nothing except wasted cryptographic and disk overhead.
Page 5 "RSA2/512 as strong one way hash" just in case I was wrong and this does exist, I ran a quick google for it, and found the only references to it are in reference to this paper. I suppose they meant SHA-512, which further brings the question: Why are they using a hash function at all? Obviously this is where the decrypt/encrypt taking more than a day came from, SHA-512 is slow, using 256-bit AES in a properly secure mode using a MAC would have been better cryptographically, better for storage, computationally easier, and would have substantially reduced the window of exposure (A break of AES-256 would have been required instead of a break of either AES-128 or SHA-512). Further the choice of MD5 as a "bit blender" is extremely questionable, and again it would have created a much smaller window of exposure to use an AES CBC-MAC with a known key and IV. Obviously this was not built by someone with anything more than a rudimentary knowledge of Still on page 5, (apparently unkeyed) MD5 is used as a MAC of the lock sector. Very, very poor security decision.
Using variable formatting. How many times do we have to kill this before it stays dead? Obviously more, probably many more. Variable formatting gains you nothing, consumes entropy, and consumes cpu time, it also often is the foundation for the break.
Page 6 covers the paranoia plausible deniability. Strangely although they seem to imply that the current GBDE does this they apparently have chosen not to even begin covering this, so most likely they either make no real attempt at it, it doesn't work, or both.
Page 6, section 7.4 covers using MD5 to make sure the performance is as bad as possible by maing it impossible to precompute where data will be.
The footnote on page 6 should read "In both cases the encoded sector address was placed in the middle to ensure the worst possible performance, cryptographically it makes no difference"
In the Known Weaknesses section they forget something important THERE IS NO MAC ON THE DATA. This means that there is no possibility of detecting an attack, not possibility of determining what has been tampered with. In short the real level of security fails miserably.
Section 16 likes to hint that David Wagner and Lucky Green peer reviewed this paper. Assuming this is true, my best guess is that the authors mistook a statement of "This should be secure" for a statement of "This is good" that is unless there are enormous holes in the paper.
GBDE is built on concepts that are nonsensically put together, the design itself is out of date by at least a decade, and makes a wide number of extremely amateur decisions.
The most important point to make is that this paper shows rather well that while developing something "secure" is difficult, something that is "good and secure" is substantially more difficult.
                        Joe

@_date: 2005-03-20 22:08:09
@_author: Joseph Ashwood 
@_subject: how to phase in new hash algorithms? 
Phase 1 is to change the hash function choice from implicit to explicit. Specifically instead of having hash = "457253W4568MM48AWA2346", move to hash = "SHA-1:lq23rbp8yaw4tilutqtipyu.".
Then over time ratchet down the default.
There is also an easy argument that it may be beneficial to skip SHA-256 entirely. The argument put succinctly is:
64-bit computing is arriving
on 64-bit systems SHA-512 is nearly twice as fast as SHA-256 (crypto++ SHA-512 is at least as strong, and faster.
                Joe

@_date: 2005-11-09 04:20:30
@_author: Joseph Ashwood 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
The authoritative reference for TLS is the TLS RFC ( The authoritative reference for IPsec is of course the IPsec RFC ( As to why they wouldn't use these as they stand, synchronized protocols often require finer control over the data block size than these offer, but modification is easy enough, and would certainly have caused fewer concerns than a roll your                     Joe

@_date: 2005-11-11 21:19:59
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
I would say that finding any Carmichael number without deliberately looking for it is vanishingly small.
I can confirm that that number of completely wrong. I just implemented a small Java program to test exactly that. Each number was vetted by a single pass of Miller-Rabin (iterations = 1). With 512-bit numbers the first 52 random guesses that pass the first test resulted in 26 numbers that failed to pass 128 iterations. I find it rather odd that this is exactly half, and I also notice that of those that failed they almost all seem to have failed at least half of them.
It appears that the minimum estimate of 1/2 probability is necessary, but that 1/4 is more likely.
                    Joe

@_date: 2005-11-18 00:17:36
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
My own tests disagreed with this, 512-bits seemed to have a threshhold around 70 passes for random candidates, I'm thinking you forgot a sieving step there (which would change the number substantially).
I think much of the problem is the way the number is being applied. Giving a stream of random numbers that have passed a single round of MR you will find that very close to 50% of them are not prime, this does not mean that it passes 50% of the numbers (the 2^-80 probability given above is of this type). In fact it appears that integers fall on a continuum of difficulty for MR, where some numbers will always fail (easy composites), and other numbers will always pass (primes). The problem comes when trying to denote which type of probability you are discussing. What are the odds that a random 512-bit composite will be detected as composite by MR in one round? I don't think anyone has dependably answered that question, but the answer is very different from 1-(probability that MR-* says it's a prime)^-k. Any discussion needs to be more accurately phrased.
For example, my phrasing is that in the tests that I performed 50% (+/- experimental noise) of those numbers that passed a single round of MR also passed 128 rounds, leading me to conclude that 50% of the numbers that passed a single round of MR are in fact prime. Since each number that passed a single round was subjected to 127 additional rounds, a number of additional statistics can be drawn, in particular that of those that failed at least one round none failed less than 40 rounds, and that few passed less than 40 rounds. Due to the fact that this was only iterated 65536 times there is still substantial experimental error available. These pieces of information combined indicate that for 512-bits it is necessary to have 80 rounds of MR to verify a prime.
                    Joe

@_date: 2005-11-22 02:50:18
@_author: Joseph Ashwood 
@_subject: Fermat's primality test vs. Miller-Rabin 
No I did not, since this was specifically to test the effectiveness of MR I determined that it would be better to test purely based on MR, and not use any sieving. The actual algorithm was:
16384 times
    question = random 512-bit number
    //this is not the most efficient, but it should remove bias making this just MR
    while(question does not pass a single round of MR) question = random 512-bit number
    127 times
    {
        perform an MR round
        log MR round result
    }
Then I performed analysis based on the log generated. I will gladly disclose the source code to anyone who asks (it's in Java).
No you're not. The seiving is important from a speed standpoint, in that the odds improve substantially based on it, however it is not, strictly speaking, necessary, MR will return a valid result either way.
If we are discussing that aspect, then yes we can agree to it. That is the probability I gave, at exactly a single round (i.e. no sieving involved), approaching 1/2 (my sample was too small to narrow it beyond about 2 significant digits). I know this result is different from the standard number, but the experiment was performed, and the results are what I reported. This is where the additional question below becomes important (since it gives how quickly the odds of being incorrect will fall).
Actually I'm not, the probability is a subtley different one and the key different is in Y. Instead it is given random composite RC what is P(MR(RC, k) | Comp X). This appears to me to be a complex probability based on the size of the composite. But this is the core probability that governs the probability of composites remaining in the set of numbers that pass MR-k. Fortunately, while it is a core probability, it is not necessary for MRs main usefulness. Performing log_2(N)/4 rounds of MR appears to be a solid upper bound on the requirements, and as this is the probability given by Koblitz, and the most common assumption on usage it is a functionable But that would be entirely insufficient to isolate MR, which is what I was discussing. Including sieving does speed up the process enormously, but it fails to discuss what the actual behaviors of MR alone are, a very critical And I'm saying that if you have accurately represented their analysis, their analysis does not apply to MR itself, but on a combination of MR with other techniques. As such it is not a valid analysis of MR alone.
                Joe

@_date: 2005-10-14 16:45:20
@_author: Joseph Ashwood 
@_subject: NSA Suite B Cryptography 
Uhhhh, no. The NSA only licensed the right to use (and sublicense under special circumstances) the patents, they did not purchase the patents, and they do not have exclusive rights to them. You would have to negotiate with Certicom, the NSA would only be an alternative licensing agency under special circumstance.
[snip the rest, it was based on a failed assumption]
                Joe

@_date: 2005-10-18 14:01:42
@_author: Joseph Ashwood 
@_subject: SecurID and garage door openers 
My understanding is that since it is a purely monotonic counter it is plenty possible to do one of two things:
send {counter, data} instead of {data}, receiver stores last counter to avoid replays
have the receiver just keep counting forward for a while (not a good idea
I'd be willing to take a look at the protocol, but dissection is not my Currently, not very many, with proper designs openly published, not very many because not very many companies will use it. However, it could be a useful way for some cipherpunks to make some extra money. Anyone else up for it? and how about the car alarm?
                Joe

@_date: 2005-10-23 16:17:38
@_author: Joseph Ashwood 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
Tom Berson's conclusion is incorrect. One needs only to take a look at the
publicly available information. I couldn't find an immediate reference
directly from the Skype website, but it uses 1024-bit RSA keys, the coverage
of breaking of 1024-bit RSA has been substantial. The end, the security is flawed. Of course I told them this now years ago, when I told them that 1024-bit RSA should be retired in favor of larger keys, and several other people as well told them.
                    Joe

@_date: 2005-10-25 13:06:31
@_author: Joseph Ashwood 
@_subject: semi-preditcable OTPs 
You've pretty much got it. In order for a OTP to work you simply need what I commonly refer to as an overflow of entropy. The source of this entropy doesn't matter and it can be from the plaintext as much as it can be from the key. This extends the unicity distance (as you noted) and can render it impossible to decrypt.
                    Joe

@_date: 2006-01-18 14:24:44
@_author: Joseph Ashwood 
@_subject: quantum chip built 
Probably one of the best statements so far, certainly QC and easy don't go together very well at this time.
At this time pretty much everything is potentially at risk from QC mostly because we know so little about how they really behave. Will ECC-160 fall to QC within 20 years? Probably not, but I wouldn't offer insurance against it. Right now we can safely assume that for our lifetime QC will be less of a threat than classical computation, but my standard recommendation of checking your security in depth at least every 6 months (depending on the safety buffer you have less decrease this) along with some continual critical point examination (e.g. check every paper on cryptanalysis of AES if you use AES) should be more than sufficient.
                Joe

@_date: 2006-03-21 20:25:27
@_author: Joseph Ashwood 
@_subject: passphrases with more than 160 bits of entropy 
Use a bigger hash, SHA-512 should be good to basically 512-bits, same for Whirlpool. If those aren't enough for you, use the chaining mode from Whirlpool and Rijndael with appropriate size.
                Joe

@_date: 2006-03-26 19:07:07
@_author: Joseph Ashwood 
@_subject: Creativity and security 
The one I find scarier is the US restaurant method of handling cards. For those of you unfamiliar with it, I hand my card to the waiter/waitress, the card disappears behind a wall for a couple of minutes, and my receipt comes back for to sign along with my card. Just to see if anyone would notice I actually did this experiment with a (trusted) friend that works at a small upscale restaurant. I ate, she took my card in the back, without hiding anything or saying what she was doing she took out her cellphone, snapped a picture, then processes everything as usual. The transaction did not take noticably longer than usual, the picture was very clear, in short, if I hadn't known she was doing this back there I would never have known. Even at a high end restaurant where there are more employees than clients no one paid enough attention in the back to notice this. If it wasn't a trusted friend doing this I would've been very worried.
                Joe

@_date: 2006-05-23 17:01:38
@_author: Joseph Ashwood 
@_subject: Is AES better than RC4 
RC4 should have been retired a decade ago, that it has not is due solely to the undereducated going with "whatever's fastest." It's time we allowed RC4 to stay dead.
                    Joe

@_date: 2006-05-24 04:05:14
@_author: Joseph Ashwood 
@_subject: [!! SPAM] Re: Is AES better than RC4 
It is in general distuingable from random, actually quite quickly.
The first few bytes are so biased that any security is imaginary.
Using it securely requires so much in the way of heroic efforts that the overall system slows down into the same speed class as a much simpler, more secure design based on AES (or 3DES, or a dozen other ciphers).
The key anti-agility slows it down to the point of being functionally unusable for any system that requires rekeying.
It's only redeeming factors are that the cipher itself is simple to write, and once keyed it is fast. Neither of these is of any substantial use after considering the previous major issues.
                Joe

@_date: 2006-05-24 16:15:29
@_author: Joseph Ashwood 
@_subject: Is AES better than RC4 
This is part of the lack of key agility.
There is far more to using RC4 securely than sumply hashing the key. Hashing the key only prevents recovering the original key (to the limits of the hash used) it does not provide for anything close to all the heroic efforts. If you look at the design of SSL/TLS a very significant portion of the effort that has gone into design of the frame/cell/whatever they call them is specifically to address issues like those seen in RC4.
You simply cannot code around the fact that the RC4 key processing is dog slow, and that even after the original keying design there is the necessity to discard the first several bytes of data. So just in the keying you have to deviate substantially from the original design.
A Viginere cipher is easier to code, we don't recommend it. Just as with a Viginere cipher, building a secure protocol (even for storage) with RC4 quickly becomes an arms race requiring heroic efforts on the design side along with huge amounts of compute cycles on the execution side to avoid a PFY with a laptop. The same amount of effort in design with AES leads to a simpler, more compact design of approximately the same speed. And exactly as Ed noted : "simple to ... verify is good for security too."
The truth is that because AES is so much simpler to build a secure protocol around the end result is actually easier to analyse.
                Joe

@_date: 2006-05-25 03:15:38
@_author: Joseph Ashwood 
@_subject: Is AES better than RC4 
Yes I did snip that out. I figured everything we agreed on could be left out easily enough. I apologize for removing something you considered core to your view.
even 1 byte of RC4 discards slows the rekeying process, and as a result it does affect the effective key agility. That only 256 discards are necessary does not mean that those extra 256*(clock cycles per pull) clock cycles don't affect key agility. At what point do we say "This affects key agility" when it increases the time by 1%? 10%? 100%? If we don't consider every cycle to reduce key agility it's all just a matter of scale. This does mean that different implementations will have different key agilities, but if you look hostorically RC2 makes a great example of where the attacker has substantially more key agility than the legitimate user, so it is not without precedent.
                    Joe
                Joe

@_date: 2006-05-30 21:21:37
@_author: Joseph Ashwood 
@_subject: Status of SRP 
The problem is that you're attempting to treat the wrong aspect. Yes SRP verifies the server, but requiring even more work on the part of the client will not solve the problem. Attempting to use SRP to solve this problem is basically saying "You must be this smart to be worth protecting."
                    Joe

@_date: 2007-02-03 20:52:35
@_author: Joseph Ashwood 
@_subject: Intuitive cryptography that's also practical and secure. 
Sent: Tuesday, January 30, 2007 12:33 PM
If I'm reading the design correctly, the biggest failure I see is that it is open to coersion. It is possible to hold someone's family or other personally important stuff for ransom for a receipt that reflects voting                     Joe

@_date: 2007-01-11 19:28:15
@_author: Joseph Ashwood 
@_subject: Private Key Generation from Passwords/phrases 
I think you need some serious help in learning the difference between 2^112 and 112, and that you really don't seem to have much grasp of the entire concept. 112 bits of entropy is 112 bits of entropy, not 76 bits of entropy, 27 bits of bull, 7 bits of cocaine, and a little bit of alcohol, and the 224 bits of ECC is approximate anyway, as you noted the time units are inconsistent. Basically just stop fiddling around trying to convince yourself you need less than you do, and locate 112 bits of apparent entropy, anything else and you're into the world of trying to prove equivalence between entropy and work which work in physics but doesn't work in computation because next year the work level will be different and you'll have to redo all your figures.
                    Joe

@_date: 2007-01-17 17:21:43
@_author: Joseph Ashwood 
@_subject: Private Key Generation from Passwords/phrases 
I'm going to try to make this one a bit less aggregious in tone. I'm also
going to sometimes use (3DES) and (ECC) for designation of work and time
----- Original Message ----- Sent: Monday, January 15, 2007 2:31 AM
You also ended up removing a large portion of my point. My primary point was
that 2^76 <> 2^112. You made several assumptions in coming to your
conclusion that are at least suspect.
You observe that currently 3DES:ECC-224 speed is about 4000:1, and assume
that it will stay this way. However, if you look at the history of ECC, the
speed of ECC doesn't scale linearly against 3DES, in fact a single thread of
3DES might've actually been faster 1 year ago (during the GHz war, before
multi-core), whereas ECC is still improving as the ability of the CPU to
perform complex routing and branch prediction improves. So next year the
ratio could be 100:1, I doubt it will be that dramatic but it is possible.
As a result this assumption will have to be reexamined every time a CPU
revision is released, a slight timing change can actually make a big
difference to the 3DES:ECC speed ratio. Also you need to consider the attack
speed of the attacker, versus the production speed of the user.
You assume that the fastest way to computer point exponents is through
counting up to the exponent. While I admit I'm not familiar with point
exponentiation algorithms, I strongly suspect this one to be incorrect, I
see no immediate reason that (k*k*k*k) must be decomposed to (((k*k)*k)*k)
instead of ((k*k)*(k*k)), at exp=4 there is no difference but at
exp=16million there is an enormous difference. You might be able to make this assumption true for your system by somehow proving that the only way to compute k^x requires b^x steps (for some b.
I see no reason it the arguments presented that the attacker would still
have to perform 2^76 (ECC)work in 2^112 (3DES)time, instead of 2^76
(ECC)work in substantially less than 2^112 (3DES)time.
That is not to say that I don't think you will have > 2^76 (ECC)work
required to break it, taking longer than 2^76 (ECC)work, but that I disagree
with your result. If you set your target at 2^80 (ECC)work, then I believe
you can achieve it this way. I also think that if you were to perform
result[i] = hash(result[i-1], passphrase) (result[0] = 0x000...000) it is a
safe assumption that there is no faster way than counting to i, which would
vastly improve your chances of security (see "Secure Applications of
Low-Entropy Keys" by Hall, Kelsey, Schneier and Wagner for reference). I also think that most users would be unwilling to wait the full day you spec'd, it is hard enough to get users to wait 5 seconds.
                Joe

@_date: 2007-10-13 01:44:33
@_author: Joseph Ashwood 
@_subject: Password hashing 
Just combining several of my thoughts into a single email.
On the Red Hat proposal:
Why does every undereducated person believe that complexity==security? It is far better to rely on little things called "proofs." There are several proofs out there with significant impact on this. In particular the really nice HMAC proof. The absurd complexity makes it highly likely that there is at least some shortcut in it that hasn't been seen yet.
On SALT || PASSWORD:
In doing that you are assuming collision resistence, and no shortcuts in computation. It is better than the RedHat proposal, but not optimal.
On NetBSD HMAC-SHA1:
There is a shortcut in the design as listed, using the non-changing password as the key allows for the optimization that a single HMAC can be keyed, then copied and reused with each seed. this shortcut actually speeds attack by a factor of 3. The fix is to use the salt as the HMAC key, this assumes much less of the hash function.
On PDKDF2:
Also appears to suffer from the same precomputation flaw, possibly more I haven't looked at it too closely for this purpose.
On USERID || SALT || PASSWORD:
Close, anything that is fixed (USERID and PASSWORD) should be put at the end, so the there is round to round variation before it, preventing precomputation. It also assumes the same collision resistence and no The better solution, with aspects borrowed from the others:
IV[0] = Salt
IV[i] = HMAC(key=IV[i-1], data=USERID||PASSWORD)
Of course nonambiguous formatting for USERID||PASSWORD is necessary to avoid any shortcuts or precomputations, but any nonambiguous method is sufficient, including a fixed length USERID.
By using an HMAC instead of just a hash function allows it to make use of most of the HMAC proof, reducing the assumptions on the underlying hash to the effective minimum. By ordering everything to place the SALT and later prior result as the HMAC key this prevents any precomputation under the assumption that there is no method of computing the hash shorter than 3 hash compression iterations, a quite small window of opportunity, and any result will likely benefit the rightful computation of the PasswordHash resulting in a simple increase in the value of k.
                    Joe

@_date: 2007-10-13 15:21:28
@_author: Joseph Ashwood 
@_subject: Password hashing 
Sent: Saturday, October 13, 2007 1:25 PM
No, each application of the HMAC is seperate, this is to incur the finalization penalty in the computation. if you want it closer to IV = SALT
for(n iterations)
    IV = HMAC(key=IV, data=USERID||PASSWORD)
Why put each field in
It really is to incur as many necessary performance penalties as possible. The HMAC keying requires 2 compressions, then the USERID||PASSWORD formatting can be created to make it consistently 2 more compressions, and a finalization per round.
More inflation is of course possible, but I don't think it is reasonable, too much possibility of stretching too far, giving too much leverage for an attack on the compression function (i.e. the more times you use the compression function the more likely a shortcut exists, but by resetting the state such attacks become much less likely).
                Joe

@_date: 2007-10-15 19:12:53
@_author: Joseph Ashwood 
@_subject: Password hashing 
Sent: Monday, October 15, 2007 5:47 AM
It is true that the first two iterations of the compression function in my supplied solution are computationally irrelevant, while in the current design the first two are computationally relevant, but the second time through the HMAC the situation reverses, the password keyed HMAC has exactly the same pre-salt state as the in the first HMAC iteration, and so in the second and subsequent HMAC iteration the first two applications of the compression function are computationally irrelevant, but in my solution there is no prior knowledge of the key for the second and subsequent HMAC iteration and so the first two applications of the compression function are computationally relevant. So my given solution trades the computation in the first two compression function computations for the millions of subsequent compression function computations. Asymptotically this is a 3 fold improvement, and so it is a very good change.
It is also worth noting that most passwords, even so called "good" passwords, have only a small amoutn of entropy, and a 50,000 word list will contain a significant number of all passwords on a system, there are more salts, and so storing the precomputations of the passwords versus the precomputations of even a 32-bit salt is radically different.
There is a choice, do it right, or keep the API. I am firmly on the side of doing it right. While the USERID is irrelevant if the SALT can be made to never repeat, that is a very hard thing to truly accomplish, especially across multiple disconnected systems.
So it prevents people from doing something that is poor security.
While the design is being changed (as you noted making this change would necessitate other changes) it is worthwhile to eliminate other security poor decisions as well.
                Joe

@_date: 2007-10-26 23:28:27
@_author: Joseph Ashwood 
@_subject: Password vs data entropy 
Sent: Thursday, October 25, 2007 9:16 PM
There are two answers:
1) Attacker has no oracle to tell him it is correct, information with probility is useless
It is only necessary that entropy(plaintext)+entropy(password) >= sizeof(ciphertext). As a result the password only needs 1 bit of entropy, and the combiner to generate the ciphertext MUST decrypt to at least two different plaintexts based on the password.
2) an oracle is available, or probability matters
It is necessary that entropy(password) exceed the computation efforts for the attacker. This is more difficult to determine because it changes continually, but 128-bits of entropy is a "good" estimate. It is always at least difficult, possibly impossible, to prove a useful limit here.
            Joe

@_date: 2008-02-01 20:43:16
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement 
Sent: Friday, February 01, 2008 1:53 PM
I would actually recommend sending all the public data. This does not take significant additional space and allows more verification to be performed. I would also suggest looking at what exactly the goal is. As written this provides no authentication just privacy, and if b uses the same private key to generate multiple yb the value of b will slowly leak.
You can then use the gpb trio for DSA, leveraging the key set for more                 Joe

@_date: 2008-02-03 23:32:08
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement  
Sent: Saturday, February 02, 2008 12:56 PM
It is assuming that the total value of the data protected by those parameters never crosses the cost to break in the information value lifetime. For 1040 bits this is highly questionable for any data with a lifetime longer than 6 months.
Very bad idea, for two reasons, the rand() function does not have sufficient internal state, and the rand() function is far from cryptographically All flaws in the private key generator will show in the public key selection, do yes.
Only if the value of p changes, if the value of p remains static, then the private key doesn't leak. A simple proof of this is simple, Eve can easily, and trivially generate any number of public/private key pairs and thereby generate any number of viewable sets to determine the unknown private key.
                Joe

@_date: 2008-02-05 01:49:49
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement  
Sent: Monday, February 04, 2008 5:18 PM
*nix /dev/urandom should work well, the entropy harvesting is reasonably good, and the mixing/generating are sufficient to keep it from being the weak link.
Yep, my typos show I'm far from perfect. I meant "so."
Actually I'm saying that if p and g do not change, then there is no additional leakage of the private key beyond what Eve can compute anyway.
There are many, many factors involved in any deep security examination, making it truly a business decision with all the complexities involved in that, and very easy to get wrong.
                Joe

@_date: 2008-02-08 00:36:28
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement 
Sent: Wednesday, February 06, 2008 8:54 AM
I am not immediately aware of any known attacks that have been published about it, but it is fairly obvious that Eve has more information about the private key by having a second key set with the same unknown. With only a single pair Eve's information set is:
g_1,p_1,q_1,y_1 where y_1 = g_1^x mod p_1
By adding the second key set Eve now has
g_1,p_1,q_1,y_1 where y_1 = g_1^x mod p_1
g_2,p_2,q_2,y_2 where y_2 = g_2^x mod p_2
This is obviously additional information, and with addition key set _i eventually Eve has the information to guess x with improves probability.
I agree with that, presuming that the private key values are different, there is at least no harm in using different parameters, and it avoids some possible avenues of attack.
Actually there is nothing stopping parameters for DSA from being prime(160 bit)*prime(50000 bit)*2+1 which would have a large enough subgroup as to be effectively unbreakable. Now obviously 50000 bits is excessive, but my point is that finding p with a moderately sized subgroup q and a large additional subgroup is entirely possible, even though it is arguably unnecessary.
I would consider that except for (semi)ephemeral parameters the cost of finding an appropriate prime are minor relative to the other considerations. This is especially true with signature parameters where a signing pair can be worth more than all the data authenticated by it.
I don't know if they are "common" but they are definitely a good idea, or at the very least using parameters with very large factors of p-1. Primes of the form q*k+1 for small k are certainly a good idea.
I'm not as certain about that last point. My experience has been that on most occassions the parameters are close to the same size.
                        Joe

@_date: 2008-02-08 00:51:01
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement  
[to and CC trimmed]
----- Original Message ----- ; ; "Joseph Ashwood" Sent: Thursday, February 07, 2008 2:17 PM
With any authentication the biggest consideration is to examine what the intention is. Using a single-use one time key for a symmetric MAC works for local authenticity, but not for remote authenticity. That is to say that the local process knows that it didn't generate the MAC, and the MAC is shared with only one other, so the authenticity is known, but any external source can only say that an entity knowing the key generated it. This may or may not be the intended condition, in that auditing this is very, very This should be entropy.
It all depends on the intended meaning. If this is intended to authenticate to a third party, it fails completely. If it is specifically intended NOT to authenticate to a third party it may be exactly what is needed.
                    Joe

@_date: 2008-02-11 00:29:48
@_author: Joseph Ashwood 
@_subject: questions on RFC2631 and DH key agreement 
Sent: Sunday, February 10, 2008 9:27 AM
We obviously disagree. Security is alway about information control, and disclosing additional information for no gain is always a bad idea.
Expressing the equations differently:
Y_i = g_i^X - k_i*p_i
is equivalent to
Y_i = g_i^X mod p_i
Since Y_i, g_i, and p_i are known, k_i is irrelevant, and g_i and p_i can even be chosen, simple algebra shows that not all Xs can be discovered from a given set, but it also says that sets of possible X can be determined from each triple, and by choosing g,p the overlap of the sets can be reduced. Creating an oracle for Y,g,p triples out of the client is begging for an adaptive attack.
Actually there is an additional random variable in the signature, and 3 additional k_i so the algebra says that the sets will overlap simply too much for a similar set-based attack to work.
This is a largely fuzzy-logic based attack. And while I obviously haven't sorted it through that far should allow for a probability sorting of values for X.
                        Joe

@_date: 2008-09-16 21:03:00
@_author: Joseph Ashwood 
@_subject: RSA modulus record 
Sent: Tuesday, September 16, 2008 2:08 PM
I have to agree that it is impressive, in the same way that having a nickname "Tripod" is impressive. Doesn't actually mean much in terms of reality neither is all that useful, or realistic for actual use.
                            Joe

@_date: 2009-08-01 05:33:23
@_author: Joseph Ashwood 
@_subject: Fast MAC algorithms? 
By the same argument a Viginere cipher is "tricky" to use securely, same with monoalphabetic and even Ceasar. Not that RC4 is anywhere near the brokenness of Viginere, etc, but the same argument can be applied, so the argument is flawed.
The question is: What level of heroic effort is acceptable before a cipher is considered broken? Is AES-256 still secure?3-DES? Right now, to me AES-256 seems to be about the line, it doesn't take significant effort to use it securely, and the impact on the security of modern protocols is effectively zero, so it doesn't need to be retired, but I wouldn't recommend it for most new protocol purposes. RC4 takes excessive heroic efforts to avoid the problems, and even teams with highly skilled members have gotten it horribly wrong. Generally, using RC4 is foolish at best.
                    Joe

@_date: 2009-08-01 21:18:13
@_author: Joseph Ashwood 
@_subject: AES, RC4 
If a completely unrelated new key is used, and the key has sufficient entropy, and it isn't used for too long, and the entropy of the key is fairly smoothly distributed, and the first several bytes are discarded, and I'm probably missing a couple of requirements, then RC4 is reasonably secure. On the other hand using AES-128 in CTR mode, the key requires sufficient entropy. That is the difference, particularly attempting to make sure there the RC4 kys are truly unrelated is continually difficult.
The last few weeks have not been kind to AES-256, a couple new attacks, the related key on the full structure, and the more recent significant erosion in other areas. Like I said, not enough to force an immediate retirement, AES-256 remains functionally secure, but the argument for usage is getting more difficult, AES-256 seems to be no more secure than AES-128, and is                     Joe

@_date: 2009-08-02 04:07:51
@_author: Joseph Ashwood 
@_subject: Fast MAC algorithms? 
The way to use a Viginere securely is to apply an All-Or-Nothing-Transform to the plaintext, then encrypt, this results in the attacker entropy of the system that is in excess of the size, and therefore a OTP. There are other ways, but this method is not significantly more complex than the efforts necessary to secure RC4 and results in provable secrecy. It is just tricky to use a Vigenere securely.
                    Joe

@_date: 2009-08-02 05:46:12
@_author: Joseph Ashwood 
@_subject: Protocol Construction WAS Re: Fast MAC algorithms? 
Because we have no idea how to do that. If you were to ask 6 months ago we would've said AES-256 will last at least a decade, probably 50 years. A few years before that we were saying that SHA-1 is a great cryptographic hash. Running the math a few years ago I determined that with the trajectory of cryptographic research it would've been necessary to create a well over 1024-bit hash with behaviors that are perfect by todays knowledge just to last a human lifetime, since then the trajectory has changed significantly and the same exercise today would probably result in 2000+ bits, extrapolating the trajectory of the trajectory, the size would be entirely unacceptable. So, in short, collectively we have no idea how to make something secure for that long.
And that is why Kelsey found an attack on GOST, and why there is a class of weak keys. That is the problem, all future attacks are rather by definition a surprise.
By scheduling likely times for upgrades the prices can be assessed better, scheduled better, and works far better for business than the "OH ****. OUR **** IS BROKEN" experience that always results from trying to plan for longer than a few years at a time. It is far cheaper to build within the available knowledge, and design for a few years.
Neither of those is a strong indicator of security. AES makes a great example, AES-256 has more rounds than AES-128, AES-256 has twice as many key bits as AES-128, and AES-256 has more attacks against it than AES-128. An increasing number of attack types are immune to the number of rounds, and key bits has rarely been a real issue.
There is no way predicting the far future of cryptography, it is hard enough to predict the reasonably near future.
                    Joe

@_date: 2009-08-05 22:51:12
@_author: Joseph Ashwood 
@_subject: Attacks against GOST? Was: Protocol Construction 
My apologies for the delay, I had forgotten the draft message.
I just said there are attacks, the situation is open for interpretation because of the nature of the attacks and the unknown S-box. Kelsey and Schneier published the first related key attack in 1996, in 1997 Kelsey enhanced the attack. My point was that the proposed method of boosting security (increased key size and rounds) does not necessarily correlate to increased security and since GOST was given as an example of how to do it "right" the attacks by Kelsey, et al mattered.
Good to know, I didn't remember that part.
                Joe

@_date: 2009-02-16 23:15:29
@_author: Joseph Ashwood 
@_subject: how to properly secure non-ssl logins (php + ajax) 
Sent: Sunday, February 15, 2009 4:30 AM
I'm going to edit this, since I assume most of the code is completely database stores Hash(password | salt) on server
challenge = Hash(random bits), challenge specifically does NOT change every user_hash = Hash( Hash( password | salt) | challenge)
There are so many ways to attack this I'm not sure where to begin:
1) Man-in-the-middle - user <-> Jerk <-> server, Jerk can easily highjack the session
2) Fake server sends out known predictable challenges, user is now an oracle
3) hack the real server, retrieve Hash(password| salt) hacker can now log in to server FASTER than user
4) hash attacks, you mention specifically that MD5 is available as a hash for this, DONT EVER USE MD5
the list continues
Now how to (mostly) fix it:
g, p, q are DSA parameters of sufficient size
Hash is a secure Hash, SHA256 will work, but SHA512 will work faster
database stores g^Hash(password | salt) mod p, call it gUser
Challenge = time | random bits, make it 256 bits, using time reduces the number of random bits used
gChallenge = g^Challenge mod p
Signed-gChallenge = cryptographically signed gChallenge | time, this does not take a certificate, just a trusted known signature key
Client receives  Signed-gChallenge and salt
Client verifies signature including time on Signed-gChallenge and extracts Client computes Y = gChallenge^Hash(password | salt) mod p
Server computes Y = gUser^Challenge mod p
If Y=Y client knows the password. For proof of security, this is signed-half ephemeral Diffie-Hellman key agreement with reveal.
1) This does not fix the MITM attack, only an encrypted tunnel can do that, so use SSL
2) Fake server only repeat a challenge created by the real server because of the signature, but the public key of the signing key needs to be verifiable, this is where certificates come in
3) Retrieveing gUser from the database is exactly identical to retrieving a Diffie-Hellman public key, no risk, database can be public
4) ALWAYS REMEMBER TO NEVER USE MD5
Also with SSL you don't need to have a paid for certificate, have a look at  for an example.
                Joe

@_date: 2009-07-02 20:51:47
@_author: Joseph Ashwood 
@_subject: MD6 withdrawn from SHA-3 competition 
Sent: Wednesday, July 01, 2009 4:05 PM
I find this disappointing. With the rate of destruction of primitives in any such competition I would've liked to see them let it stay until it is either broken or at least until the second round. A quick glance at the SHA-3 zoo and you won't see much left with no attacks. It would be different if it was yet another M-D, using AES as a foundation, blah, blah, blah, but MD6 is a truly unique and interesting design.
I hope the report is wrong, and in keeping that hope alive, the MD6 page has no statement about the withdrawl.
                    Joe

@_date: 2009-07-21 20:32:44
@_author: Joseph Ashwood 
@_subject: Fast MAC algorithms? 
I didn't see the primary requirement, you never give a speed requirement. OMAC-AES-128 should function around 100MB/sec, HMAC-SHA-512 about the same, HMAC-SHA1 about 150MB/sec, HMAC-MD5 250MB/sec. I wouldn't recommend MD5, but in many situations it can be acceptable, and none of these make use of parallelism to achieve the speeds.
                        Joe

@_date: 2009-07-22 14:05:24
@_author: Joseph Ashwood 
@_subject: Fast MAC algorithms? 
Sent: Tuesday, July 21, 2009 10:43 PM
There's a reason everyone is ignoring that "requirement" rekeying in any modern system is more or less trivial. As an example take AES, rekeying every 10 minutes will have a throughput of 99.999% of the original, there will be bigger differences depending on whether or not you move the mouse.
I would NEVER recommend it, let me repeat that I would NEVER recommend it, but Panama is a higher performing design, IIRC about 8x the speed of the good recommendations, but DON'T USE PANAMA. You wanted a bad recommendation, Panama is a bad recommendation.
If you want a good recommendation that is faster, Poly1305-AES. You'll get some extra speed without compromising security.
I would argue that they use it because they are stupid. ARCFOUR should have been retired well over a decade ago, it is weak, it meets no reasonable security requirements, and in most situations it is not actually faster due to the cache thrashing it frequently induces due to the large key expansion.
The general preference is to permanently retire them. The better algorithms are generally at least as fast, that's part of the problem you seem to be having, you're not understanding that secure is not the same word as slow, in fact everyone has worked very hard in making the secure options at least as fast as the insecure.
New ones tend to be faster than the old.
New ones are designed with more recent CPUs in mind.
New ones are designed with the best available knowledge on how to build New ones are simpler by design
New ones make use of everything that has been learned.
I think the answer surprised you more than you expected. You had hoped for some long forgotten extremely fast algorithm, what you've instead learned is that the long forgotten algorithms were not only forgotten because of security, but that they were eclipsed on speed as well.
I've moved this to the end to finish on the point
I very strongly disagree. One of the fundamental assumptions of creating secure protocols is that sooner or later someone will bet their life on your work. This isn't an idle overstatement, instead it is an observation.
How many people bet their life and lost because Twitter couldn't protect their information in Iran?
How many people bet their life's savings on SSL/TLS?
How many people trusted various options with their complete medical history?
How many people bet their life or freedom on the ability of PGP to protect People bet their life on security all the time, it is a part of the job to make sure that bet is safe.
                Joe

@_date: 2009-09-16 16:11:22
@_author: Joseph Ashwood 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
That's a good solution, except for the missing MAC, considering the environment I would suggest hard coding it, there's really no reason to offer options.
You are looking at the correct type of construct, the solution for your problem is known as a Message Authentication Code or MAC. The biggest concerns are what are you MACing, and that it must use a second key. There are a number of solutions.
The first part of the solution is to store an additional key, you're storing 128-bits hopefully securely store 256, and split it, trust me you'll thank yourself when the security issues are investigated. The second key is for the MAC algorithm.
Since you already have CBC available, my first suggestion would be CBC-MAC (IV = 0x0000000, okcs5 padding works fine, MAC = final block of ciphertext), it has good strong security proofs behind it, and is fast. Apply the MAC algorithm, prepend the MAC value to the plaintext (just because indexing is easier this way), then encrypt to ciphertext, store. To decrypt, retrieve, decrypt the ciphertext, parse out the MAC value and the plaintext, run MAC algorithm on plaintext to find MAC2, check MAC == MAC2 . This will give you a failure rate of 1/2^128 or something on the order of 0.0000000000000000000000000000000000003%, you are more likely to have the system burst into flame than see a false positive on this. Overall, this will reduce your speed to 50% of the current.
You might also want to take a look at Poly1305, it is faster.
As you noted HMAC is also available, but I would recommend against using SHA-1 for anything, it simply raises too many questions that will take too long to answer. The secure hashes available are simply not as fast unless you have bare-metal coding ability which Java really doesn't like.
The other, and arguably better, option would be a combined mode. The good combined modes are a legal minefield, so using them for an open project is difficult. It is claimed that GCM and EAX are public domain, but I'm more inclined to recommend the conservative approach and avoid them.
While there has been no concern raised by cryptanalysts about CBC with CBC-MAC, to many laypeople it doesn't look good. So, for purely politic reasons, I'm recommending the shift to CCM mode. At the same time I recommend moving to an IV counter instead of random IV, in CTR mode it is necessary to make sure an IV is never reused and randomness is irrelevant. This will give you a thoroughly analyzed standard mode of operation.
                    Joe

@_date: 2009-09-16 18:13:29
@_author: Joseph Ashwood 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
Sent: Wednesday, September 16, 2009 5:19 PM
I've actually had a few clients ask for a more detailed explaination of why it is ok, so there are people who are confused. Some people get confused.
Actually I think EAX great, and if I had known you were replying while I was writing mine I wouldn't have replied at all. My problem is that I haven't taken the time to look over the patents on bordering technologies to see if I believe it is patent safe. Lately, I've been dealing with a lot of patent weirdness, so I'm more aware of patent issues.
I could try and justify my position, but honestly, CMAC really doesn't any real downsides, and the proof is tighter.
(I moved this down here)
As opinions go, its hard to find a better source than David Wagner.
                Joe
BTW: Anyone looking to make a venture capital investment?

@_date: 2009-09-18 00:17:33
@_author: Joseph Ashwood 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
It isn't difficult to implement CMAC and CTR modes in pure Java. The NIST specs for CMAC and CTR are plenty clear. You'll be looking for the AES/ECB/NoPadding option. From there use update it returns a byte []. I've used the standard JCE implementation in this way to implement unsupported modes before, it works.
                    Joe

@_date: 2010-08-16 22:46:06
@_author: Joseph Ashwood 
@_subject: 2048-bit RSA keys 
FAIR DISCLOSURE: I am the inventor of some of the technology quoted, specifically US Patant Application 20090094406. And just to plug myself even more, yes the technology is for sale.
I'm not so convinced. Since we're discussing cost it makes sense to look at the cost based structure from The storage required for 2048 is approximately 2^64 bytes, this is usually cited as the limitation. Considering technologies like US Patent Application 20090094406 (mass quantities of Flash at better than DRAM speed), this is actually an achievable capacity with more speed than any current cpu can handle (2^64 storage could operate at up to millions of TB/sec). The cost is very signficant, from  the best price per capacity is 32Gbit Flash, this is 2^32 bytes, so 2^32 such chips are required, session average of $6.99 each, this is "only" 2^32*6.99 about $30 billion. Adding in the cost for the glue logic needed to build the 20090094406 adds less than 10% to the cost, so its still under $35billion. Its worth noting that since we're talking about disk access protocols, the systems in place already handle addresses longer than 64-bits, so there are no redesign costs on the processors from this. So the cost resulting from the storage requirement for 2048 bit factoring is only about $35 billion.
If, as the page suggests, the storage is still truly the dominant cost factor 2048 is bordering on within reach for high value targets. Fortunately, this does not appear to be the case, storage jumped ahead of The computation cost is not as clear to me, I didn't invent the technologies so I'm not as intimately familiar. Computation costs are given by "A Cost-Based Security Analysis of Symmetric and Asymmetric Key Lengths" at 9 x 10^15 times more complex than a 512-bit factoring, but does not immediately appear to offer good cost estimates, a few quick searches foun RSA-155 took about 8400 MIPS*years. Wikipedia gives a number of 147600 MIPS for an Intel Core i7. Intel gives prices at $560 per cpu ( Assuming a full year is an acceptable time frame the 2048 factoring would require 5.1*10^14 processors, costing, well bluntly, a crapload, or about I'm sure in such volume the price for the cpus could be brought down significantly, and other cpus may be more cost efficient.
Considering that google gives a number of $14.59 trillion, the purchase would require nearly 20,000 years of US GDP.
So unless someone can bring the computation cost down significantly (very possible, since I used a very brute force method) it seems unlikely that 2048-bit numbers can be factord any time soon.
The most important part though is that the cost structure has changed signficantly. A few years ago the dominant cost was the storage, this has changed significantly.
            Joe

@_date: 2010-08-31 17:30:13
@_author: Joseph Ashwood 
@_subject: RSA question 
============================== START ==============================
It really depends. It comes down to the number of possible message, and their probabilities, typically expressed as entropy. There are message recovery attacks against RSA with insufficent message entropy, and this is probably widely the case. Worst case for you, there are only two possible messages, the attacker only has to test one to determine the message. Best case for you is completely entropy saturated messages. The way to bring the environment closer to your best case/attackers worst case is through random padding like that used in OAEP.
I'm also a bit unclear about how you're using it. You said the attacker knows the plaintext, but all encryption can really do is hide the plaintext. In many ways it sounds like you're looking for a digital signature algorithm, all the good ones have entropy injected.
                    Joe

@_date: 2010-03-23 23:24:40
@_author: Joseph Ashwood 
@_subject: "Against Rekeying" 
Typically, rekeying is unnecessary, but sooner or later you'll find a situation where it is critical. The claim made that everything hinges on is that 2^68 bytes is not achievable in a useful period of time, this is not always correct.
Cisco recently announced the CRS-3 router, a single router that does 322 Tbits/sec, that's 40.25 TBytes/sec. Only 7 million seconds to exhaust the entire 2^68. This is still fairly large, but be around the industry long enough you'll see a big cluster where they communicate as fast as possible, all with the same key. I've seen clusters of up to 100 servers at a time, so in theory it could be just 70,000 seconds, not even a full day, its also worth keeping in mind the bandwidth drain of an organization as cmopute intensive as Google of Facebook will easily exceed even these limits.
Certainly an argument can be made that the protocol used is wrong, but this kind of protocol gets all too frequently, and since it is usually used for high availability (one of the primary reasons for clustering) the need to rekey becomes all too real.
So there are times where rekeying is a very necessary requirement. I prefer a protocol reboot process instead of an in-protocol rekey, but sometimes you have to do what you have to do. Rekeying probably should never have been implemented in something like SSL/TLS or SSH, even IPsec it is arguable, but extreme environments require extreme solutions.
                    Joe

@_date: 2010-09-03 19:38:03
@_author: Joseph Ashwood 
@_subject: RSA question 
Actually it's a fairly straight forward calculation. Given the known computational requirements for a 512-bit factoring gives a scale multiplier for the asymptote for complexity of factoring, so it is a simple matter of using that scalar S in 2^256 = S*O(factoring n), then length of n is very close to 15360.
The cost-based analysis is really only valid at a single point in time, as technology progresses is does not do so smoothly. So while the lengths given by RSA were accurate at the time of their computation, they are no longer accurate and need to be reanalyzed.
The different approachs are very much like the difference between a sniper and a massive bomb to kill someone. Both will eliminate the target, but the bomb (NIST numbers) will have significant collateral damage, the sniper (cost based analysis) though you have to make sure you've got the right For most purposes the best solution is something between a massive bomb and a sniper, just as for most cryptographic purposes your actual security equivalence will be somewhere between the old cost analysis numbers and the NIST numbers.
                    Joe

@_date: 2014-04-02 16:37:27
@_author: Joseph Ashwood 
@_subject: [Cryptography] Clever physical 2nd-factor authentication 
Since it doesn't seem to have been broken yet, I'll go ahead and do that right now.
It is a linear collection of 8-segment displays with certain segments already activated and printed on the card. So shorten the process further, each of the middle displays shares the left and right segments (2 per side) and the end displays share one side each (2 segments).
I'm going to begin the attack after the P has been displayed, even though they seem to be relying on this for security, a brute force attempt to display a P is easy enough on such a display.
To attack.
Start with a selected display, I will start with the left most.
Choose a segment to test.
Display 8 on that display minus the test segment. To test the middle segment send 0.
If the user enters the number 8, the segment is present on the card, otherwise the segment is not present on the card.
Repeat steps for each segment.
There are by my count 9 displays on the initial implementation, leading to a maximum work effort of 64 to solve the entire grid.
Scaling is not a problem, this attack scales linearly with the length of the unknown code.
The only downside is that this is an active attack.
Passive attacks are possible, but more complex.
I don't feel like providing the exact details, but they can be worked out easily enough.
Record a successful login for playback
At each step where a number was typed in there was at least one segment that was printed on the card.
Thus from each number recorded in this manner at least 1 segment on the device was recovered.
Assuming the selection is perfectly random it will take an observation of about 400 such numbers, multiple numbers per logins makes this passive attack relatively quick and efficient.
In the end you are using the simple fact that the user provides the data to find eliminate the apparent entropy that is the entire security of the Sorry, the system is extremely weak.
                    Joe

@_date: 2014-04-04 02:55:49
@_author: Joseph Ashwood 
@_subject: [Cryptography] Clever physical 2nd-factor authentication 
[an attack]
Actually there is a vastly more useful oracle that I used, the user. "At each step where a number was typed in there was at least one segment that was printed on the card" gives a time, and so a frame with that number. An user oracle failure will result in a failed login, so the login oracle works to eliminate any false signals. This kind of attack is also well within their proposed security model of assuming the machine is compromised.
I suspect that if I were to spend more than a few moments on it, I could have found a significantly more efficient attack. The first step to this would be to realize that I made a rather foolish mistake, it uses 7 segment (as opposed to my earlier largely mythical 8 segment) display, leading to 47 locations to be mapped. Giving something like 250 digits (my mistake earlier on stating this was logins, each login has multiple digits) to break the card. From there the oracle that you pointed out, the one that prevents any such mapping on the false images should be good for significant gains. I suspect with work the number for a high probability of success could be brought under 100 digits, assuming 5 digits per login, that would be every 20 login attempts. More "secure" logins that use a larger number of digits would actually reduce the number of logins needed.
I don't see any possible way they could gain anything with such perfect oracles presented. The login oracle is perfect in revealing which user oracle responses to ignore, and the user oracle timing perfectly reveals the important frame.
With only 47 total segments, and my attack being rather clumsy, with proper analysis that number could probably be cut by two thirds, we're only talking about highjacking a tiny portion of each login attempt. Increasing the number of digits by 1, for roughly a dozen logins assuming you combined the data with the user oracle data from the login. A dozen login attempts before card replacement is horribly weak.
The whitepaper is garbage. It provides exactly 0 in the way of analysis. But since you brought it up, how about I analyze that a bit as well.
I won't bother going past the authors.
Matthew Slyman
His listed areas of specialty (according to his wikipedia user profile, *Business automation software
*SQL databases
*Specialised scientific and engineering tools
His own credentials reveal he simply lacks the background necessary.
Gadescu Horatiu Nicolae - currently employed at Passwindow. Immediately disqualified, not independent.
Sean O?Neil
Having been a part of the team that developed a single cipher (VEST) submitted to eStream he is likely the most qualified. However having found some additional writing by him, he is apparently a VLSI implementer. It actually took finding his email address ending in vest.fr, finding a paper he co-wrote in 2009 "VLSI implementations of the cryptographic hash functions MD6 and ?rRUPT" to begin finding information. I reached the conclusion about his purpose based on VEST having been horrible in software but fast in hardware, apparently badly broken the moment anyone took a look (not unusual, a first cipher is typically dreadful), and his only other publication being about the hardware implementation of someone else's design. So I conclude he is not qualified to analyze such a system either.
Ben van der Merwe
Appears to now be a software development manager at Amazon in South Africa. So once again, he simply does not have the credentials.
So there you go, a piece of garbage written by people who have no knowledge on the subject, all about a pathetically weak login system. Unless there is something substantially new that comes up, I am ending my contribution to the thread.
                Joe

@_date: 2014-08-18 01:14:51
@_author: Joseph Ashwood 
@_subject: [Cryptography] Adapting AES for arbitrarily large keys 
What you're looking for is Rijndael, that is the original name for AES before it was selected to be AES. IIRC the keying system is specified for any n*32 bit key, the key expansion itself it actually necessary (more rounds are needed for security), and while I'm not certain the original paper had the necessary relationship between key length and rounds, I know it is fairly easily derived and I'm quite certain has been published a number of times.
                Joe

@_date: 2014-02-13 20:08:56
@_author: Joseph Ashwood 
@_subject: [Cryptography] The ultimate random source 
Sent: Friday, February 07, 2014 8:14 AM
I'm not confident it will have as much entropy as you think. The design is a fairly basic modification of the lavarand design. Just as with lavarand the actual entropy comes form the entropy in the movement source (the heating coil in lavarand, the shaking in this design). So the total entropy in the random source at any given time will be a function largely of the variability of the arm motions, along with sensor noise (I'll get there). This will certainly have a fairly significant amount of entropy but I doubt the average person's vigorous arm movements will collect 256-bits quickly Of more interest to me is the sensor noise. With sensors of any kind as you push the limit of the sensor there is noise of various kinds. In the case of a webcam sensor on the flask the primary noise will be diode over-power noise from the shiny white areas. You'll also find some noise that will appear because the temperature of the sensor will change over time creating heat noise in the system. With proper sampling the heat noise and sensor overpower noise should be sufficient to provide reasonable amounts of Maximizing this entropy requires the user of incandescent lights, and shiny candy. The incandescent lights have their own noise pattern which will be echoed in the shiny candy, and the flask, as well as the heat from the light, especially if the light is close to the webcam, will induce heat noise in the webcam.
Will these be sufficient? Honestly it depends on the webcam. Using the cheap webcam that is in your laptop, phone, tablet will almost certainly not be enough. You need to have enough precision to actually collect the noise, so you'll be looking for at least 10 stops (10 bits per color) in the camera. These are not that difficult to find, but are notably more expensive than the really, really cheap cameras generally used.
As an example of just how bad it can be, even with a camera that is vastly better than the one in your laptop/phone/tablet the GoPro Hero3 Black has a significant amount of apparent noise in the image. This noise is almost exclusively deterministic and can be virtually eliminated with simple post-processing. So the 1/2.3" sensor in the Hero3 Black is just not enough sensor. At the other extreme, fighting the sensor noise in the Red Scarlet Dragon can be a nightmare if you want the maximum sensitivity, same with the BlackMagic Pocket Cinema Camera, these both have large amounts of apparently non-deterministic components to their noise, but it is worth noting that the Dragon sensor has 20 stops (basically 16-bits per color) while the BlackMagic has 13 stops (slightly over 10-bits per color). In both of these cases though the noise is a tiny fraction of a bit per sensor site.
The next big problem you will find is that modern sensors use a Bayer pattern (some use a modified, but fundamentally the same here), this is a system where a given sensor node will have a red, green, or blue filter, but to count the resolution of the device you actually count all the sensor nodes. So for example a 2048x2048 sensor will likely have 1024 red and 1024 green on a single row, the next row will have 1024 green and 1024 blue sensors, alternating in both cases. The debayering process is used to calculate the approximate color at each of these sensor nodes. The process itself is quite complex. The result though is that to acquire maximum entropy will require getting the pure RAW data. This actually eliminates all cheap cameras, they all have built in debayering. Moving to more powerful sensors it actually eliminates using the Red cameras as well, they exclusively use a lossy compression based on wavelets which works very well at reducing and virtually eliminating the noise, normally a good thing, but we want the noise. That pretty much leaves only the CinemaDNG cameras, at this point that is largely just the BlackMagic cameras. If you were to build your own camera there are a wide range of sensors available, and the RAW data could be easily fed completely uncompressed into the hash function.
I know you thought you found one that is simple, and assuming the color blocks are big enough and that the rearrangement of candy is good enough, yes it is, but I don't have any evidence that either is genuinely true. It will take a detailed analysis of the entropy sources, and the harvest engine to determine.
                Joe

@_date: 2014-02-21 04:52:13
@_author: Joseph Ashwood 
@_subject: [Cryptography] The ultimate random source 
Just make sure there is absolutely 0 post-processing that destroys entropy, this is a lot harder than it sounds. This is exactly why I spent so much of my initial email about the debayering/demosaicing process, the popular processes destroy a significant amount of the entropy. I know it may seem irrelevant at first, but any system that starts by throwing away 2/3 of the data in a deterministic process, and then proceeds to process the remaining data to smooth out any strange bumps throws away most of the entropy.
Certainly the 6^600 starting states sounds like a lot, but even assuming that it is somehow perfect, that is not much more than 1200 bits of entropy. Harvesting this 1200 bits of entropy 512 bits at a time is only 2 full samples. This is why the continued introduction of entropy matters, and why the general lack of entropy introduction matters even more.
                    Joe

@_date: 2014-06-20 02:24:38
@_author: Joseph Ashwood 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
It is actually worse than you feared, and worse than I think anyone has pointed out (I only glanced through the messages, apologies if someone else covered this).
Inherent in the design of modern SSDs is wear leveling, this makes secure erase an absolute nightmare. Although I am greatly simplifying the core problem remains the same, wear leveling goes something like:
You have block [0....n]
Each block contains not only the data, but the functional address, and whether or not it is the most recent data.
block[i] = When a write happens the controller begins where it left off, I will simply begin with the first write to the system, it won't matter, and assume n=1024, a very small disk.
write 1:
write <0x000, the quick brown fox jumped over the lazy dog> is the command <0(active), 0x000, The quick brown fox jumped over the lazy dog> is the expanded version
this is written to physical sector 0
now if we want to secure erase this, the logical thing would be to write over block 0x000, but this is what happens
write 2:
write <0x000, 000...0000> (the overwrite)
the drive writes to physical sector 0, setting the !active bit
the drive writes to physical sector 1 <0(active), 0x000, 000...0000>
The data in physical sector 0 is untouched, effectively creating the same problem of a journaling file system in hardware.
There is no way of knowing for sure that you are actually overwriting a particular sector from the outside. You can approximate this by writing the sector many, many times, but this will burn out large portions of the flash in the drive before you actually can be certain of overwriting.
I wish I had better news for you, but as everything is today, you really have no way of knowing. This applies even at the flash chip level, each layer in the flash chip often has its own wear leveling going on, this is done in order to disguise that the chip has flaws, in any high capacity chip there are many sectors that are unusable straight from the factory.
This sucks, and it is only going to get worse for the foreseeable future. How do I know? My name is on the patent (US patent 8397011).
                    Joe

@_date: 2015-02-21 16:08:44
@_author: Joseph Ashwood 
@_subject: [Cryptography] =?utf-8?q?Sony_to_Offer_=E2=80=98Premium_Sound?= 
On the consumer side it is certainly a waste of money, but on the recording side it may not be. You would be amazed at some of the lengths that are gone through to get the cleanest possible recording. A single degree change in temperature can actually change the recording. Radio interference is a big source. But also variations in power consumption can be a headache. The card at least claims to address both.
Just to give you an idea how far we sometimes have to go. I have actually run a system where the powered microphone was plugged into its own separate car battery, the amplifier/ADC it was attached to had its own separate battery, the USB hub was powered by its own battery, all connected to a laptop that was to be run only from battery during recording. All this because the [bleep bleep bleep bleep bleep] AC in the building was noisy, the laptop-hub cable although shielded was becoming a broadcast antennae, the amplifier/ADC is always extremely sensitive, the cable to the mic was working as a reception antennae for the laptop-hub broadcast, and the mic always has variable resistance consuming variable power inducing noise in the amplifier. I would gladly have paid the $160 for this card on that day, might have been able to eliminate half the setup.
Is it ideal? No. But it may be a needed solution for some environments.
                    Joe

@_date: 2015-03-11 06:05:17
@_author: Joseph Ashwood 
@_subject: [Cryptography] Is there a point to key schedules? 
Absolutely yes.
Very, very slowly, and incompletely, and every refresh of hardware actually works to eliminate sources of entropy as much as possible. Turing type machines are a nightmare to get entropy from.
This is as much as anything to try and distill down to just the entropy, working on concentrating the extremely slow entropy collection so as not to store all the non-entropy.
I separated out the part where you answered your own question. It is at best secure, everything around it is to attempt to help make it secure.
32*128= 4096 bits. 4096 bit asymmetric ciphers have 128-bit security and could transmit 4096 bits. so everything is mathematically comparable.
You?re assuming an inherent compatibility where none exists. First, 4096 bit asymmetric ciphers do not offer 128 bits of security. Depending on the exact numbers you want to believe, as there is significant debate, the value ranges from 108-142 bits and is often banned from standards on principal. Second, the security of the various bits is not equal, both RSA and DH have very weak low order bits. Third, as mentioned before, in increasing numbers of environments RSA and DH are not acceptable. Fourth, when using ECC 4096 bit keys offer approximately 2048 bit security. Far exceeding any requirement today.
Fifth, at 4096 bits RSA and DH are slower and more costly than similar security with ECC.
Sixth, these numbers keep moving around, there was a time where 500 bit RSA was effectively as secure as 128-bit ciphers. It is my view that attacks on factoring are extremely likely to advance far faster than counting.
So that is a brief initial list of the problems with the argument.
Now as to why key schedules are necessary.
First, entropy is hard to find. Having a reliable, smooth method of spreading the entropy around the keys for a cipher is extremely important.
Second, maintaining compatibility across key transfer methods. Third, addressing weaknesses in the cipher itself. A tiny key schedule change can remove weak keys in a cipher that would not be possible otherwise.
Fourth, I?m actually tired of writing this email so I?m going to stop.
                    Joe

@_date: 2015-03-25 15:37:55
@_author: Joseph Ashwood 
@_subject: [Cryptography] How to crypto secure speed limit signs 
That is of course the actual correct solution. The frequent updates aren't even a problem, an increasing number of new cars come with their own data connection. Today the connection is used to unlock your car via cell phone, there's not reason it can't be used to update not just the speed limits but all the road advisories in the internal database.
The cost of updating a central database is tiny. The cost of making diff files is tiny. The cost of distributing these diff files is small but not tiny, done properly even at typical cloud data costs less than $1 per year per vehicle, easily absorbed into the registration cost.
This gets rid of a real problem. The cost of the signs. The signs can now be taken down. This reduces infrastructure cost, reduces environmental impact. Reduces maintenance costs. Reduces visible intrusions. Eliminates sign theft. And of course, the signs in place can simply be left in place. This also works under the implied speed limits that Darren noted (they'd no longer be implied but explicit in the database).

@_date: 2015-03-26 18:20:05
@_author: Joseph Ashwood 
@_subject: [Cryptography] How to crypto secure speed limit signs 
Replying to both Jonathan and Bill in one message.
It would be cheaper to require those vehicles have a compatible receiver installed. This role can be quickly covered by the smart phone. Considering that the entire US roadway system is the world's largest, and has been storable in flash memory for well over a decade, the costs are trivial. Today's smart phone could store the entire Eurasia super continent and not miss the storage space.
That's what a border crossing is for. It isn't a massive dataset, only a few MB. A quick data update at the border, and everything works just fine.
By not moving to the horribly expensive high tech signs. Instead, we retire the signs.
Not very long. Use the available technology. Sure that 1939 truck won't have it, but that 2015 cellphone in the cab with you certainly has the power.
Differential calculations are important here. At driving speeds the differential calculations works to identify the road itself precisely and it is entirely possible to determine not only which lane the vehicle is in (important due to lane restrictions for some vehicles) but even to determine where in the vehicle the GPS receiver is located.
Just as a side note on that, Android actually has a feature that tracks where you parked using GPS has always been accurate to within a couple of parking spaces for me. Today even stationary GPS is bordering on being able to get you back to your car, moving GPS with differential calculation is good enough to get you back to the random peanut that fell under your seat 6 months ago.
speeding as a revenue source.
My proposal: They're screwed. Any change to the speed limit would have to be registered, or the jurisdiction will have to answer for it in court, safe bet that a lawyer will take this case. It is my firm opinion that such jurisdictions should be required to find other sources of revenue, revenue that is actually legitimate.
I am deliberately leaving alone policy decision like Bill brings up:
Answers to those are policy not technology.
                Joe

@_date: 2016-12-16 01:14:28
@_author: Joseph Ashwood 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to 
Actually very few want it. In fact so many find it outright offensive that they have consistently refused to buy cameras with that feature. Don't believe me?
There is a reason it is called Secure Digital cards, more popularly known as SD cards. In order to support SD cards, the device has to support encrypted cards. This feature has literally been there for 17 years.
The feature is so negatively desired that manufacturers have hide that the camera is even capable of it.
