
@_date: 2005-12-22 10:28:47
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG quality verification 
I have been asked by to verify the quality of the random numbers which are used for certificate requests that are being sent to us, to make sure that they are good enough, and we don?t issue certificates for weak keys.
The client applications that generate the keys and issue the certificate requests are the usual software landscape OpenSSL, IE, Firefox, SmartCards, ... and we would like to be able to accept all normally used We are being asked to either issue the keys for our users (I don?t want to), or alternatively demand the users to have good quality random numbers with a contract for the user. Now it might be easy that I demand the user to have good random numbers, but the first question will likely be "and how do I do that?" or "which software/hardware does that?"
So I guess I have to ask the vendors, whether ther random numbers are good enough. But what if they just say "yes" or "no"? I think the better way would be if I had a possibility to verify the quality of the random numbers used in a certificate request myself, without the dependence on the vendor.
From what I remember of the usual RSA key generation, random numbers gathered are being put into a field with the expected keysize. Then the first and last bit is set to 1, to make sure that the key has the necessary size, and to have it odd (not to be devidable by 2). Then it is verified for primeness, and if the check is ok, the number is used.
So if I extract the key, remove the first and the last bit, then I should have the pure random numbers that are being used. If I do that with lots of keys, I should have a good amount of random material for the usual statistical Am I right? Am I wrong?
Has anyone done that before?
Any other, better ideas?
Should I do it that way?
Best regards,
Philipp G?hring

@_date: 2005-12-22 21:35:50
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG quality verification 
Hi Travis,
That?s true, but I can test non-randomness. And if I don?t detect non-randomness, I can assume randomness to a certain extent.
Wasn?t that one of the reasons, why statistic was invented?
I don?t control the software everyone is using on this world.
The problem is that I have to live with COTS (Common-off-the-shelf) software out there, that is generating the certificate requests. The only thing I can do is create a blacklist or a whitelist of known bad or known good software, to tell the users: Use this software, or don?t use that software.
Has anyone done this yet?
Hmm, every key should deliver about 1000 bits of randomness, I guess. How many bits should I collect for the tests in your opinion?
I guess someone would have noticed already, if Microsoft, Mozilla or OpenSSL had done that.
Wait. How many LOC(lines of code) does the King James Bible have? Mozilla had something like 13 Mio. LOC as far as I remember ... perhaps they really hid the KJ Bible in it! ;-)
Contrary to the normal challenge of developing a new random number generator, I don?t have the possibility to change the existing software, I just have to evaluate them, and find out, whether it?s ok or not.
I first thought about a black-box test, by simply tracing in the operating system where the software gets its numbers from. A open("/dev/random") systemcall at the time of key generation might be a good hint for good random numbers. But as Netscape proofed some years ago, you can ch=read(stream,&ch,1) one perfectly random number, and overwrite it with the value 1 (which is not soo random anymore) in one single line of error, and invisibly to the operating system failing to use the random numbers given.
So since the random numbers might be modified between gathering and using for the keypair, I thought that I need to evaluate the quality at the end of the keypair generation.
Best regards,
Philipp G?hring

@_date: 2005-12-23 15:47:53
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG quality verification 
Hi David,
At first I had the same answer. But then I started to think it through.
And it makes far more sense to me now.
They are just trying to fulfill the legal requirements.
They just told me their requirements, and how it is normally solved. I am known for inventing my own mechanisms to solve requirements.
Actually, I already developed one mechanism, that solves that problem as a But it?s dedicated for hardware implementations, and I need mechanism for  software implementations (mostly for the browsers) now.
Yes. That?s what I am planning to do.
But what do I do, if the users ask "And how do I do that?"
It?s easy to say that it?s their responsibility.
But how should they do it?
At the moment, I wouldn?t even know how to do it myself, if someone asked me to care for it.
Ok, let?s forget the CA and the users. How do I do it myself?
How do I make sure myself, that the browser is generating good random numbers, and actually using them properly for the certificate requests?
I will be personally liable for it, it that random thing breaks.
Well, I could get a lot of paper, a good hex editor, and start calculating my own RSA keys with pencil and paper, read through the ASN.1 specifications, and manufacture my certificate request myself.
(Has anyone actually does that yet, and can give some time-estimations?)
Perhaps because I am working for a CA that actually does care.
Do you know any browser vendor that guarantees the correct generation of secure random numbers and their correct usage, that offer to take liability, if it goes wrong?
I am already in that business. And yes, it?s great fun, and I like it.
Well, I have to start somewhere. And the best way to start that I could find is by fulfilling the requirements that are already given. So yes, I start here now. And I?ll try not to stop, before I haven?t found good answers to all the open questions.
I don?t want to control it, I want to audit it. I want the users to have it under their control. At the moment, nobody gave them much control over the random number quality of the keys they are using.
Yes, it?s reasonable, if you aren?t paranoid enough. I thought exactly the same way, before I started to think more about this specific topic more detailled. Now I think it?s a bit negligent to ignore the topic completely.
But perhaps there are bigger problems, yes. (Sometimes little problems are easier to solve than bigger problems ...)
Yes. Do you have a TODO list for me?
Thanks for your input,
Philipp G?hring

@_date: 2005-12-23 16:09:15
@_author: Philipp =?utf-8?q?G=C3=BChring?= 
@_subject: RNG quality verification 
Hi Peter,
Perhaps there is some mis-understanding, but I am getting worried that the common conception seems to be that it is an unsolveable problem.
What is wrong with the following black-box test?
* Open browser
* Go to a dummy CA?s website
* Let the browser generate a keypair through the  or cenroll.dll
* Import the generated certificate
* Backup the certificate together with the private key into a PKCS * Extract the private key from the backup
* Extract p and q from the private key
* Extract the random parts of p and q (strip off the first and the last bit)
* Automate the previous steps with some GUI-Automation system
* Concatenate all random bits from all the keypairs together
* Do the usual statistical tests with the random bits
Is this a valid solution, or is the question of the proper usage of random numbers in certificate keying material really mathematically unsolveable?
(I am not a RSA specialist yet, I tried to stay away from the bit-wise details and the mathematics, so I might be wrong)
But I would really worry, if it is mathematically impossible to attestate the correct usage (to a certain extent, I know about the statistical limitations) of random numbers with the software I am using to get certificates.
Best regards,
Philipp G?hring

@_date: 2005-07-21 18:55:45
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: Qualified Certificate Request 
Peter Saint-Andre invited me here to present my concept of Qualified Certificate Requests to you.
It is a long-term goal of CAcert to be able to provide qualified certificates.
Regarding the requirements for qualified certificates, the only obstacle we still have is the problem, that CAcert has to make sure, that the private key for the certificate is generated and stored securely in a SmartCard, or another Hardware Token.
Since the users should be able to issue the certificates at home, we need a technical solution to make sure, that the private key is from within a SmartCard, when we receive a certificate request.
Therefore I designed "Qualified Certificate Requests", which cryptographically signs the public key in the CSR with a vendor key, to state that it comes from a secure device.
Now I created a software-based reference implementation, so that the security of the system can be evaluated, and that the Token Vendors can see how to do it, and can do interop testing.
And here is the documentation:
Please test it, analyze it, try to break it.
Philipp G?hring

@_date: 2006-04-04 14:58:54
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG quality verification 
Let?s try it.
I have setup a small randomness collection website, and I am asking everyone to submit as many random numbers as possible from as many different sources as possible.
Please upload any kind of randomness (none, PRNG, special patterns, your personally preferred HRNG, ...) to the website.
I will try to do a larger statistical analysis of all random samples, and also try to find ways to identify PRNGs ...
I have a couple of goals with the project:
* perhaps being able with a high number of samples to differentiate statistically between PRNG and HRNGs
* a HRNG commercial market overview, which random speed you can get for which * at the end, I am planning, to run the analysis tools as a web service, so that everyone can have his own random numbers tested anytime, by simply uploading them, and having them analyzed and compared to the others Best regards,
Philipp G?hring

@_date: 2006-02-24 15:29:02
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Phil Z doesn?t know how to do it himself, at least with PGP. He told me that he doesn?t sign people?s keys who ask for it, simply because it would pollute his keyring on his computer, and he couldn?t work with a keyring with thousands of people on it anymore. So PGP obviously has a usability and scalability problem.
So he only signs the keys of his friends because of that.
I wonder now, why he didn?t tried to solve that usability/scalability problem himself yet, but gave up instead.
Best regards,
Philipp G?hring

@_date: 2006-01-03 18:13:24
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG quality verification 
Ok, now I did the first test.
I took OpenSSL, generated 10000 RSA keys, and took them apart.
First I analyzed the raw keys:
It isn?t perfect random quality, but I also don?t see any big problems with You can get the program and the extracted data here:
Perhaps I should have stated the quality demands for possible solutions:
Since I am working on a practical solution, and not a theoretical solution, the following demands apply:
* A 99.999% solution is ok.
Has anyone tested yet, how much samples are needed to detect those PRNGs?
Yes, sure.
Sure. To secure against compromised machines, you need Hardware Tokens with a qualified certificate request mechanism. But in the scenario, I am currently working on, the assumption is that we only have a software engine, and that the machine of the user is not compromised.
But still the quality of the random number generator and the correct usage of the random numbers for the certificate request are not known yet.
I will not ask the users to send in their private keys for testing!
As you write below, I would like to test the standard generation packages (Firefox, IE, Opera, OpenSSL), and I also want to offer a guideline (or even the testing software) for the advanced users that they can test their own generation package, if they really want to.
That?s exactly what I wanted to do. (Sorry if I didn?t wrote that clear enough Best regards,
Philipp G?hring

@_date: 2006-11-28 17:33:07
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: RNG Summary 
============================== START ==============================
I would like to inform you about the current status of our RNG Market Survey. We have included most Hardware and Software RNG vendors now. (If we missed some, please tell me)
The current results are available here:
The general project page:
	
The service is fully automated online now, so you can easily test your own RNG now, and compare them to the rest of the market.
Best regards,
Philipp G?hring

@_date: 2006-09-25 01:28:34
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: Exponent 3 damage spreads... 
We have been researching, which vendors were generating Exponent 3 keys, and we found the following until now:
* Cisco 3000 VPN Concentrator
* CSP11
* AN.ON / JAP (they told me they would change it on the next day)
(perhaps more to come)
My current estimate is that 0.26% of the certificates in the wild have Exponents <=17
Best regards,
Philipp G?hring

@_date: 2007-07-03 18:12:33
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: The bank fraud blame game 
The problem I found (during my research for  )
for Smartcards and other external devices for secure banking is the following:
About 50% of the online-banking users are doing personal online banking on company PCs, while they are at work. Company PCs have a special property: They are secured against their users. A user can?t attach any device to a company PC that would need a driver installed. So any solution like Smartcard-readers, or USB Tokens that needs any special application or driver will not work for 50% of the online-banking customers.
(And the banks aren?t that happy about loosing 50% of their customers).
So I would say there are 2 possibilities left:
* An external device, where you have to enter the transaction details a second time to generate some security code
(Can you show me the user that wants to enter each transaction twice?)
* An external device that lets the user verify the transaction independently from the PC.
The second possiblity has been realized by some european banks now, based on SMS and mobile phones, which sends the important transaction details together with a random authorisation code, that is bound to the transaction in the bank?s database. The user can then verify the transaciton, and then has to enter the authorisation code on the webinterface.
(And the good thing is that they succeeded to get the usability so good that it?s more convenient than the previous TAN solution, and the cost increase of SMS compared to paper TANs is irrelevant)
So I personally would declare the online-banking problem solved (with SMS as second channel), but I am still searching for solutions for all others, especially non-transactional applications.
Best regards,
Philipp G?hring

@_date: 2007-07-05 18:01:04
@_author: Philipp =?utf-8?q?G=C3=BChring?= 
@_subject: The bank fraud blame game 
5 characters, including numbers and letters. I think you have something like 4 tries to enter a code correctly.
(rough estimation: 5^30 = 931322574615478515625 / 4 = 232830643653869628906 , so you have a chance of 1:232830643653869628906 per transaction if you try it 4 times)
Well, the security depends on an attacker not being able to infect a specific users?s computer with a MitB and knowing and being able to clone this specific users?s mobile phone at the same time.
Neat idea!  It only has the problem that I know several companies already where you have to register your USB-stick, and only registered USB-sticks are allowed on the network ..., but it?s a neat workaround, yes. I think SecurityLayer should be easily adaptable to that concept.
Do you already have an demo implementation of that external device, Peter?
Best regards,
Philipp G?hring

@_date: 2008-04-02 18:46:44
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: Levels of security according to the easiness to steel biometric data 
There are several relevant threats:
* Accidental leaking the biometric data (colour-photos for face, fingerprints on glasses for fingers, public documents for human signature)
* Intentional stealing of biometric data (cellphone cameras, hidden cameras, ...)
Stealing fingerprints is easy if you hand the target person a glass of water.
With "face" you have to differentiate between the different kinds of faces.
Taking colour photos of faces is easy. Taking infrared photos of faces, or taking 3D scans of faces, ... is much harder.
Yes, stealing retina is harder. (It's even harder in the normal usage ...)
Yes. Choosing the right biometrics for the right application, implementing it correctly and educating/training the users properly can be challenging.
But in the end, you can steal any biometric data if you really want to.
(Take a look at the film Gattaca to see how this can be done in practice. I didn't noticed any technically really unrealistic things in the film Another important question is whether you can apply a faked/copied biometric at a certain place. It could be difficult to mount an attack with a full face mask at a guarded entrypoint. But applying fake fingerprints is far less noticable for guards.
(It might be easy to steal the face, but you can't apply it due to all entries being guarded)
Tamper evidence, Tamper protection, Tamper proof, Tamper resistance ...
As usual, it depends on your threat-models, on your environment, on your resources, on your enemies, ...
Best regards,
Philipp G?hring

@_date: 2008-08-03 02:55:42
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: On the "randomness" of DNS 
Hi Ben,
Thanks for the notice, that was a broken upload by a user.
Best regards,
Philipp G?hring

@_date: 2008-08-27 17:05:44
@_author: =?ISO-8859-15?Q?Philipp_G=FChring?= 
@_subject: Decimal encryption 
I am searching for symmetric encryption algorithms for decimal strings.
Let's say we have various 40-digit decimal numbers:
As far as I calculated, a decimal has the equivalent of about 3,3219
bits, so with 40 digits, we have about 132,877 bits.
Now I would like to encrypt those numbers in a way that the result is a
decimal number again (that's one of the basic rules of symmetric
encryption algorithms as far as I remember).
Since the 132,877 bits is similar to 128 bit encryption (like eg. AES),
I would like to use an algorithm with a somewhat comparable strength to AES.
But the problem is that I have 132,877 bits, not 128 bits. And I can't
cut it off or enhance it, since the result has to be a 40 digit decimal
number again.
Does anyone know a an algorithm that has reasonable strength and is able
to operate on non-binary data? Preferrably on any chosen number-base?
Best regards,
Philipp G?hring

@_date: 2008-02-11 14:28:30
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
I can fully confirm this.
Microsoft claimed that they had to rewrite the API to make it more secure, but I only found one small security-relevant weakness that they fixed, the others are still there. (And even that fix wouldn?t have justified a rewrite of the API for websites. They could have kept the frontend-API compatible in my I had the feeling that Microsoft wants to abandon the usage of client certificates completely, and move the people to CardSpace instead.
But how do you sign your emails with CardSpace? CardSpace only does the realtime authentication part of the market ...
If anyone needs more information how to upgrade your Web-based CA for IE7:
Best regards,
Philipp G?hring

@_date: 2008-01-30 02:14:55
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: two-person login? 
I would like to have a two-person remote login:
The server is in the datacenter, two sysadmins login remotely (SSH or something similar), and the login only works if both are there. As soon as either one drops the connection, the other one is frozen too.
They should see what each other is doing (key-press logging of the other admin in the bottom line)
(In case they detect the other sysadmin doing something evil, they can simply disconnect, which also disconnects/freezes the other one)
I would be happy about such an implementation in a SSH server. (combined with screen perhaps ...)
Best regards,
Philipp G?hring

@_date: 2008-01-30 11:25:04
@_author: Philipp =?utf-8?q?G=C3=BChring?= 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
Yes, sending client certificates in plaintext while claiming that SSL/TLS is secure doesn?t work in a world of phishing and identity theft anymore.
We have the paradox situation that I have to tell people that they should use HTTPS with server-certificates and username+password inside the HTTPS session, because that?s more secure than client certificates ...
Does anyone have an idea how we can fix this flaw within SSL/TLS within a reasonable timeframe, so that it can be implemented and shipped by the vendors in this century?
(I don?t think that starting from scratch and replacing SSL makes much sense, since it?s just one huge flaw ...)
SSL already looks quite round-trip optimized to me (at least the key-agreement TCP could need some stronger integrity protection. 8 Bits of checksum isn?t enough in reality. (1 out of 256 broken packets gets injected into your TCP stream)  Does IPv6 have a stronger TCP?
The SSL implementations I analyzed behaved quite nicely, I didn?t noticed any round trip problems there. (But feel free to send me a traffic capture file that shows the problem)
I once implemented SSL over GSM data channel (without PPP and without TCP), and discovered that SSL needs better integrity protection than raw GSM delivers. (I am quite sure that?s why people normally run PPP over GSM channels ...)
SSH has the same problems. It also assumes an active attack in case of integrity problems of the lower layer, and terminates the connection.
Sounds like an interesting idea to me.
Best regards,
Philipp G?hring

@_date: 2008-01-31 03:04:00
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
* It?s a privacy problem
* It?s a security problem for people with a security policy that requires the their identities to be kept secret, and only to be used to authenticate to the particular server they need
* It?s an availability problem for people that need high-security authentication mechanisms, combined with high-privacy demands
* It?s a identity theft problem in case the certificate contains personal data that can be used for identity theft
Quoted from Lynns email:
* It?s a liability issue
(Lynn, can you go into more details here? On the other hand, I would say it?s self-explaining ...)
Guess why so few people are using it ...
If it were secure, more people would be able to use it.
If you want a "public" example of client certificate usage:
(You need a (free) client certificate from  to be able to access this page)
There are ISPs out there who provide internet access based on client certificates, authenticated in HTTPS sessions
Creative Commons is running a registry for digital works, based on authors client certificate authentication:
The Austrian governmental inhabitant register is using client certificates for about 10,000 users all around Austria since 2001. (If I remember the details correctly)  And there are hundreds of internal systems I heard of that are using client certificates in reality every day.
Validated email addresses for spamming. Spear-phishing perhaps, ...
Why doesn?t SSH leak the client identity in plaintext?
The problem isn?t a key-agreement problem. The problem is a client-authentication problem. It does let them impersonate the client to anyone who doesn?t care about the public key. (There are applications that just use the DN+Issuer information that they normally extract out of the certificates, ...)
But impersonation is just one threat out of the huge SSL/TLS threat-model.
There are CA?s on this planet that put things like social security numbers into certificates.
(I guess those CA?s would say that SSL shouldn?t leak certificates in plaintext anyway.) Shovling around responsibility won?t help us. Let?s fix the problems. (Yes, we are already trying to get those CA?s to stop doing that ... but it?s a bit like asking credit card companies to not print those sensitive creditcard numbers on those credit cards ...)
And there are a lot of people who would be interested to use certificates for more applications than pure identity. (which aren?t necessarily sensitive, but they are personal related data)
Where does the SSL specification say that certificates shouldn?t contain sensitive information? I am missing that detail in the security consideration section of the RFC.
There is a market demand for using sensitive information in certificates, dating back to the mid 90's (according to Lynn), and showing itself in various forms like Stefan Brands credentials, Attribute Certificates, and even the OACerts by Jiangtao Li and Ninghui Li. I have been talking to many people about client certificates and client authentication, and a lot of them are interested in using client certificates for authentication, and also to add other attributes to the certificates.
Using username+password inside HTTPS does not leak the client?s identity in cleartext on the line. (If I am wrong and HTTPS leaks usernames sent as HTTP Forms or with HTTP Basic Authentication, please tell me)
Do we have any more ideas how we can get this flaw fixed before it starts hurting too much?
Yes, there are regularly people popping up there that need it, but they always get ignored there, it seems.
I think we have the boiling frog problem here. (Frog not recognizing that the water in the pot gets hotter and hotter, since it happens to slowly and not at once ...)
Since all those people asked one after each other on the list, they were all ignored, since everyone had just one single case and one single argument.
If they had come up at the same time and coordinated their arguments ...
(But I don?t think that we can blame all those people for not coordinating their arguments.)
We have an issue here. And the issue isn?t going to go away, until we deprecate SSL/TLS, or it gets solved.
Do you think the the security arguments I summed up above qualify on the tls list? Should I go into more detail? Present practical examples?
Or does it take a Slashdot article with some governmental CA?s certificates that contain social security numbers, some SSL sniffing logfiles, ... for the responsible people to react? Or is it possible that we can pro-act and fix  this issue, without giving SSL and TLS a bad name in the press?
I am not interested in reading "SSL leaks personal details" in the media.
Has anyone counted the amount of people that asked for it in all the years on the TLS mailinglist? I see several possible options:
* We fix SSL  Does anyone have a solution for SSL/TLS available that we could propose on the TLS list? If not: Can anyone with enough protocol design experience please develop it?
* We deprecate SSL for client certificate authentication.
We write in the RFC that people MUST NOT use SSL for client authentication.
(Perhaps we get away by pretending that client certificates accidently slipped into the specification.)
* We switch from TLS to hmmm ... perhaps SSH, which has fixed the problem Hmm, there we would have to write all the glue RFCs like "HTTP over SSH" again ...
* We will all have to answer nasty questions, why we didn?t do anything about it that SSL leaks personal certificates in plaintext ...
* We change the rules of the market, and tell the people that they MUST NOT ask for additional data in their certificates anymore
* Does anyone have any better and perhaps more realistic options?
Come on guys, let?s solve this issue together before it hurts.
Ok, what I can do to get it fixed?
Different topic: Fixing TCP/SSL
Try to send a DVD iso image (4GB) over a SSL or SSH encrypted link with bit errors every 10000 bits with a client software like scp that cannot resume downloads. I gave up after 5 tries that all broke down in average after 1 GB.
(In that case it was a hardware (bad cable) initiated denial of service attack ;-)
The problem is that you can?t work around this issue with standard software. You can?t tell Putty or OpenSSH or any normal IP stack or any network card to add more protection there, to solve that problem. You could try to setup some tunneling to get more protection, but that?s usually highly impractical for copying a single file from one computer to the next.
If the link layer gives you 1/256, and the TCP layer gives you 1/65536, and the SSL layer demands 0/16777216, then end up with 1/16777216 too much.
(And there is no guarantee that the link layer actually gives you the 1/256. It could also give you 1/1)
Best regards,
Philipp G?hring

@_date: 2008-07-31 11:28:43
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: On the "randomness" of DNS 
I would suggest to use  to test the randomness of the DNS source ports. Due to the large variety of random-number sources that have been tested there already, it's useful as a classification service of unknown randomly looking numbers.
You just have to collect 12 MB of numbers from a DNS server and upload it there. (If you get 2 Bytes per request, that's 6 million requests you have to do)
We successfully used statistical tests to detect broken random number generators, we informed the vendors and they fixed them.
Best regards,
Philipp G?hring

@_date: 2008-06-03 15:09:31
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: The perils of security tools 
Ah! Thanks a lot!
Ok, I think that should be written into the man-pages of /dev/random and fgetc/fread and other related howtos.
Best regards,
Philipp G?hring

@_date: 2008-05-28 10:34:53
@_author: Philipp =?iso-8859-1?q?G=FChring?= 
@_subject: The perils of security tools 
Yes. Still, some people are using fopen/fread to access /dev/random, which does pre-fetching on most implementations I saw, so using open/read is preferred for using /dev/random.
Implementations can be rather easily checked with strace.
Best regards,
Philipp G?hring

@_date: 2009-08-03 19:06:14
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: Unattended reboots (was Re: The clouds are not random enough) 
I (re?)invented a concept for that application, which can be applied in
certain situations.
I have started from the assumption that we are talking about something
like an E-Business system that is hosted in a normal commercial
environment with wired, routed networks.
The attack-vector I wanted to secure against was stealing the machines.
So in the scenario, an attacker would break into the building, steal the
server, get out again, and would try to get access to the data afterwards.
As long as the machine stays in place, it should be able to reboot
unattendedly, as soon as it's somewhere else, it shouldn't be able to
reboot unattendedly anymore.
The concept is to have a secondary server (or several secondary servers)
somewhere else, which has the necessary key available. It should be
situated in a place where it is highly unlikely that it also gets stolen
when the primary server gets stolen, and it has to be connected through
a somewhat trusted routed network.
Now the secondary server has configured the IP address of the primary
server, and regularly tries to contact the primary server, every minute
or something. (Or it uses a different method to detect when the primary
server needs the key).
The contact-tries are done over the routed network, and the routers must
be somewhat secured, and the links in between also have to be trusted.
When the secondary server succeeds the connection to the primary server,
 it authenticates the connection to the primary server (with a key that
is stored in plaintext on the primary server, or perhaps generated from
the hardware configuration). If the authentication succeeds, the
secondary server sends the private key to the primary server, and the
primary server can continue to boot normally.
If an attacker steals the server, and connects it at a different place
in the network, or somewhere else, then the secondary server will not be
able to reach the IP address due to the routing, and won't be able to
provide the key. So the attacker could only try to break in again, and
trace back where the server is, where it comes from, ...
Due to the connection originating from the secondary server and not from
the primary server, you have to have both the server, and you have to be
on the right place of the network.
It's not perfect security, but I think it's a reasonable tradeoff for
the given threats and the need for high-availability in those certain
Please let me know if you hear about any other interesting solutions too.
Best regards,
Philipp G?hring

@_date: 2013-12-12 00:02:38
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Public Key Vulnerabilities 
Does anyone have a somewhat comprehensive list of vulnerabilities and
attacks on the various existing public key cryptography systems?
(RSA-encrypt, RSA-sign, ECC, Knapsack, ElGamal, ...)
I am interested in anything from side-channel attacks to secret-key leaks
in signatures to timing-attacks, to low-exponents, padding attacks, ...
I am even interested in attacks that do not seem practical on first sight,
or which are less efficient than brute-force.
Best regards,
Philipp G?hring

@_date: 2013-07-08 13:22:21
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: [Cryptography] *** SPAM *** dead man switch [was: Re: Snowden 
I would suggest Secret Key Splitting (e.g. Shamir's scheme), with an n-out-of-m scheme. Add decryption instructions, give everyone you trust and who is not easily discoverable a share of the key, the complete encrypted backups, and tell them to follow instructions when they believe you are dead or imprisoned. (The instructions could be as easy as "boot your PC from this DVD and keep it running for at least a week". Given enough secret shares, it should work and be interference-safe, and still only be decryptable if n of the m trusted parties collaborate.
Best regards,
StealthMonger  schrieb:

@_date: 2013-07-08 13:22:20
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: [Cryptography] *** SPAM *** dead man switch [was: Re: Snowden 
I would suggest Secret Key Splitting (e.g. Shamir's scheme), with an n-out-of-m scheme. Add decryption instructions, give everyone you trust and who is not easily discoverable a share of the key, the complete encrypted backups, and tell them to follow instructions when they believe you are dead or imprisoned. (The instructions could be as easy as "boot your PC from this DVD and keep it running for at least a week". Given enough secret shares, it should work and be interference-safe, and still only be decryptable if n of the m trusted parties collaborate.
Best regards,
StealthMonger  schrieb:

@_date: 2013-10-03 00:47:47
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
Am 2013-09-30 10:16, schrieb ianG:
Perhaps it is a DLP (Data Leakage Prevention) technology. At least the
same method works great for Creditcard numbers.
"Oh, there is a 14 digit number being sent on a unclassified network,
and all the checksums are correct? Someone is trying to leak ...
terminate the connection, forensically analyze the machine, ..."
Best regards,

@_date: 2013-10-21 23:21:24
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] [RNG]   on RNGs, VM state, rollback, etc. 
Why aren't more crypto projects are using HAVEGE? I would expect HAVEGE to
provide enough randomness fast enough so that it would be sufficiently
enough at boot-time, if implemented correctly.
I have the feeling that HAVEGE is currently unmaintained, does anyone want
to maintain it?
I think a CPU is the only thing that is guaranteed to be available in a
computer, so in the absence of properly embedded hardware-RNG like the one
from Intel, from my point of view, something like HAVEGE should be the
second choice.
Or has anyone found any good reasons, why HAVEGE would be insecure or
problematic in some way, that I haven't heard about yet?
(Or is the Linux kernel already doing what HAVEGE did standalone in the past?)
I would like to invite everyone concerned about RNGs at boottime to take a
look at HAVEGE, to research it's security, to try to take over
maintainership of HAVEGE, and to try to port it to additional processors
where possible.
Another completely different idea I had for the boot-time scenario would
be to mix in the whole (or a part if it is too much) of the physical RAM
(/dev/mem) into the RNG pool at boot time, or on first demand when there
isn't enough in the pool already to satisfy the demand (so you don't need
to do it if nobody needs /dev/random)
Best regards,
Philipp G?hring

@_date: 2013-10-27 23:53:14
@_author: =?UTF-8?B?UGhpbGlwcCBHw7xocmluZw==?= 
@_subject: [Cryptography] [RNG]   on RNGs, VM state, rollback, etc. 
Hmm, if someone is able to run secret opcodes, then we already have
local code execution, right? And in this case there might be far more
powerful secret opcodes that give ring 0, ring -1 , ... access, and we
usually have to care about much larger problems than RNG attacks.
So if I compare it to the network-traffic -> interrupt -> RNG concept,
where attackers can gain information about the RNG by observing or
manipulating the network, without having local code execution, I think
HAVEGE is more safe due to requiring local code execution  to be attacked.
Do you really see a threat-model against RNGs where someone can execute
opcodes to get more information about the cpu state? If yes, please
explain further.
Which one do you prefer? A RNG provided by the CPU vendor, or a RNG that
was developed independently from the CPU vendor, or an RNG that can be
influenced with network traffic? (I prefer a mix of all of them)
I would suggest to credit it 0.0001 entropy for mixing it into
have enough entropy almost all of the time.
Shouldn?t a good mixing / whitening function care about adversarially
controlled input?
Well, I would expect the RAM to contain all of:
* Kernel image (which should be different after every kernel update)
* MAC address
* ACPI/Bios/UEFI tables
* various serial numbers of various internal devices like chipset,
RAM-chips, realtime-clock, ...
* Information about USB devices that are currently attached, ...
* Current date+time
But I did not investigate yet, whether that?s the case.
Best regards,
Philipp G?hring

@_date: 2013-09-30 23:56:28
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] TLS2 
What I personally think would be necessary for TLS2:
* At least one quantum-computing resistant algorithm which must be useable
either as replacement for DH+RSA+EC, or preferrably as additional
strength(double encryption) for the transition period.
* Zero-Knowledge password authentication (something like TLS-SRP), but
automatically re-encrypted in a normal server-authenticated TLS session
(so that it's still encrypted with the server if you used a weak password).
* Having client certificates be transmitted in the encrypted channel, not
in plaintext
Best regards,
Philipp

@_date: 2014-04-12 20:46:42
@_author: =?UTF-8?B?UGhpbGlwcCBHw7xocmluZw==?= 
@_subject: [Cryptography] Question regarding Proof of Possession 
I have a question regarding the Proof-of-Possession for certificate
If I want to get certificates issued for Encryption-Only keys
(for algorithms that can only be used for encryption, not for digital
signatures, like e.g. El-Gamal)
then my question is, whether Proof-of-Possession is really necessary, or
For signing applications, I am well aware that it would be possible for
to exchange the certificates after a signature is done,
so Mallory could modify an existing signature.
But for encryption, I do not have an idea, what kind of attack or problem
someone would be able to do with a certificate that uses someone else?s
public-encryption-only key,
without having the private key to it.
The only area where I have a weak idea about a potential problem is
in that it could confuse a a forensic expert,
if the forensic expert finds encrypted data, and then the wrong
and thinks that it is actually encrypted to Mallory.
But I do not see much value in such an attack.
Does anyone have any idea or experience for an attack scenario?
Or does anyone agree that it should not be a problem to issue certificates
to any encryption-only public key that has no Proof-of-Possession attached?
Thanks a lot for all feedback and best regards,
Philipp G?hring

@_date: 2014-08-02 17:37:59
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] *** SPAM IP *** Re: You can't trust any of 
It?s actually worse than that:
Most USB sticks (and similar storage devices, the same applies to
SD-cards, ...) contain at least 2 chips: A controller chip (usually a
reverse-engineerable ARM chip) and the actual flash/memory chip. There are often several companies involved, one that creates controller
chips, one that creates the flash/memory chips, and perhaps even other
companies that create the firmware for the ARM chip, and finally the
company that assembles all 3 and creates the final product. (The
flash/memory chips are usually quite standard/simple and don?t contain
Now the thing is that you can attach any amount of memory to the
controller chip, so you have to program the controller chip with the
firmware and configure it for how much memory there actually is, since the
memory chips themselves can?t tell the controller how much they are.
To configure that and to upload the firmware, there are Windows based
Tools floating on the internet, which are usually in
chinese/mandarin/japanese/russian (or wherever those companies are from),
so most american/european users aren?t able to understand those tools, and
therefore don?t even know about their existance.
If you want to learn more:
I had some more links, if anyone is interested, ping me, then I will
search some more and post them here...
Best regards,

@_date: 2014-08-17 10:37:03
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Which big-name ciphers have been broken 
I thought that the standardisation institutes were responsible for that.
Isn?t NIST regularly publishing what they currently think is acceptably
useable? In Austria, I think I regularly heard statements about the current
viability and expiration plans of various algorithms and keylengths. (for
commercial and governmental applications)
Expirations are usually planned and notified ahead of time there.
Recently I received a letter stating that they had to revoke a certificate
in my citizen-card (smartcard), since the algorithms that were used for it
had expired now, and that I would have to get a new card now if I wanted a
new certificate.
But since those are all just national standards, perhaps it?s really a
good idea to write a RFC about it and update it yearly, yes.
Best regards,
Philipp G?hring

@_date: 2014-02-14 22:52:38
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] RNG exploits are stealthy 
One of the candidates for this kind of attack I stumbled across lately is
rigidized interrupt handling with the potential cover-up to "save battery
The argumentation line goes like this:
The CPUs currently have various different sleep-states, and the longer and
deeper they can sleep, the less energy they need. So the best thing is to
save battery is to maximize the time between the interrupts. How to this?
There are interrupts that are so regular, that they can be planned, by
configuring the devices accordingly.
And there are some interrupts that can be delayed a little without any
The idea now is to synchronize those regular interrupts and/or to delay
interrupts where necessary.
So in the name of battery saving, some people actually developed interrupt
rigidizing beasts:
Best regards,

@_date: 2014-07-01 09:55:46
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Cisco FNR; block cipher without padding 
If I remember correctly, then it some serious limitations, in that it
cannot encrypt values with less than 32 bits, or only even sized
bitlengths, ... something like that, which makes it useless for my
Best regards,

@_date: 2014-06-12 09:24:32
@_author: =?ISO-8859-1?Q?Philipp_G=FChring?= 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
Hmm, its obviously not yet secure, but perhaps it leads in the right direction:
Alice knows all the C* keys.
She first encrypts a larger packet to Bob which is encrypted with A, she additionally sends the A xor C1, A xor C2, ...
When asked by Charlie2, Bob applies A xor C2 and sends it to Charlie2,  Charlie2 decrypts it with C2.
Is it possible traffic-wise, that Alice sends all the keys always or at least for every key change? Theoretically Alice can keep A the same as long as she does not have to revoke a compromised key, but she can renew the key more often when she wants.
If Alice has enough bandwidth, she might want to partition the users into multiple segments, and use different A keys for the different segments. She will also want to use different watermarks in the different segments, if the data gets leakes by Charlie3 that she knows from which customer segment the leak came.
Best regards,
Kent Borg  schrieb:

@_date: 2014-05-13 18:17:59
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] What faults would you inject to test 
Some more:
* Bad random numbers (see if they are being detected by the systems using
* Duplicate fields/paramters/extensions (What happens when a
field/parameter/extension that is supposed to be there only once actually
occurs more times?)
* Duplicate messages (for crypto protocols)
* Injecting faults into memory allocation (malloc)
Best regards,

@_date: 2014-05-19 17:48:02
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] [cryptography] Is it time for a revolution 
No. For static content (images, html pages, ...) where the size is known
upfront, the length header is sent. For dynamic pages (php, ...), and for
streams, the length header is not sent, and the client has to see when the
transfer ends.
Another variant is the chunked transfer, where the server can respond with
several chunks, and it sends a size for every chunk. But it? s not known
upfront how many chunks there will be...
Best regards,

@_date: 2014-10-05 00:56:09
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
Ok, I think there is a simple solution:
Take OpenSSL, generate a 1024 bit RSA key.
Extract the public key, send it to him.
Ask him to factorize it.
Receive the p and q from him.
Verify whether they are correct.
If they are, please tell us.
Best regards,

@_date: 2014-09-30 09:28:58
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] The world's most secure TRNG 
Hi Ian,
But when you do that, (like Intel did with their RdRand), people will
accuse you of providing malicious randomness that they can?t audit
anymore, since you whitened it.
Has anyone found a solution to that paradox yet?
I recently heard about a funny concept, to attach your application as a
PDF attachement into a PDF that contains the documentation for your
application, so that none of your users can claim that he did not
read/found the documentation, since there is no other way to get your
software than to open that PDF documentation and getting your software
from there.
Perhaps a way would be to build the RNG in a way that it needs some
special activation code being sent in before it actually turns on, and to
make sure that the activation code is only known when the user has read
the documentation, which states how the whitener has to be implemented if
the users implements his own driver, Best regards,

@_date: 2015-02-10 01:26:14
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] What do we mean by Secure? 
Yes, I saw that in Microsoft's Security Development Lifecycle.
At the moment, I can find a "Attack Surface Analyzer 1.0":
Best regards,

@_date: 2015-01-31 23:39:48
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] traffic analysis 
If anyone wants to implement it, please take at my paper about it?s
weakness and my proposed solution:
Best regards,
Philipp G?hring

@_date: 2015-11-12 09:45:57
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Post Quantum Crypto 
2 years ago, I stumbled across news from D-Wave and took a few days to
investigate their technology, quantum computers in general and their specific impact on cryptography, on
theory and on practice.
I came to the conclusion that itÂ´s highly likely that quantum computers
will become a serious threat for ECC, RSA, and DH.
Reasoning: There were 2 different open questions:
Does it work at all that quantum computers can break crypto, and is it
possible to scale it up enough?
and the scalability was demonstrated by D-Wave in the recent years.
Combining both possibility and scalability into one product is a third
challenge, but I do not see any real blockers why this should be
impossible anymore.
It seems that some people now agree with my results from two years ago.
The next question was about the timeframe. I expected a high likelyhood
for Quantum computer being able to attack within 5 years.
Then the next question was about the impact. It seemed to me that quantum
computers will likely destroy ECC first (because itÂ´s keys are far too
short), Diffie-Hellman and RSA later on.
But the exact points of time when the necessary inventions to break them
will come hard to predict, especially for the future ;-)
So Signing, PublicKey-Encryption and Key-Agreement are all affected and we
need replacements.
So I started to look around for replacements algorithms. What I found was a handful of academic algorithms for signing and
encryption, where most of them were unsuitable for practical use. (unrecyclable One-time-signatures, too large keys, too slow, ...)
The most promising algorithm I found was NTRU, which had variants for
Signing and Encryption (NTRUencrypt + PassSign are the current ones).
(I will write NTRU instead of NTRUencrypt+PassSign from now on, since most
of the things also apply to PassSign)
NTRU has a long history of repeatedly getting broken theoretically and
getting improved/fixed theoretically. So it is properly maintained, algorithm-wise for far more than a decade.
I took some time to audit the reference implementation, dust it off, make
sure that it compiles again and works according to the specs, found a few
minor bugs and sent pull requests that were happily accepted.
But I think it still needs a lot of love and care for engineering it into
an integrated practical solution that we can use the large-scale internet
to rely on.
But I couldnÂ´t find any suggestions for Post-Quantum Key-Agreement algorithms.
So I thought about the requirements Diffie-Hellman fulfills, found an
inspiring post on the crypto mailinglist and developed a concept for a
replacement algorithm, based in a modular fashion on an encryption
primitive and something like a hash.
I named it NTRU Key Exchange, but later on I realized that it should also
work with any other encryption primitives underneath.
After I wrote it down
( I
searched again and suddendly I found a new paper about another concept for
a Post-Quantum Diffie-Hellman replacement, which was also based on NTRU
and is called NTRU KE, which is similar protocol-wise, but far more complex and it required more
parameters that need to be defined.
So now I had proposals for Signing, Encryption and Key-Aggreement, all the
necessary primitives for a whole crypto-stack, and I turned my attention
to the crypto markets.
I quickly thought through the OpenPGP and the Bitcoin market, and came to
the conclusion that a migration should be easy for those market, and does
not need much design effort. Implement new algorithms, deploy them, create new keys, migrate from the
old keys to the new keys.
But for SSL/TLS I found a bigger problem:
All the things around it are heavily designed for having just one single
public key algorithm, and it does not contain any upgrading, transitioning or migration
mechanisms for the public key algorithms.
Without those, we have a chicken-and-egg-and-henn problem between the
clients, the servers, and the certificate authorities.
Practically, the servers would have to wait for deploying new Post-Quantum
certificates until their last client at the othere end of the internete
has finally implemented the new algorithms.
On the large public internet, for larger websites and systems this means
something around 10 years of delay, because some users only upgrade when
the hardware breaks down (embedded systems), and the supply chain from algorithm developers to cryptostack developers
to crypto applications to distributions to distributors to end-users takes
itÂ´s time.
So the big problem that I saw is that quantum computers will likely
destroy our crypto within 5 years, and we likely need 10 years to migrate
to a new solution. Ouch.
The other problem is that sophisticated attacks by the crypto analyst
community on new Post-Quantum algorithms will only start to happen when
they are widely used.
(Could you guys and girls be a bit more pro-active please?)
But we want to know now which of those algorithms are good to be widely
So I started to think about how we can do a large-scale migration of the
SSL universe.
My idea was that I wanted to make sure that by adding Post-Quantum
algorithms we do not lower the security level below the Pre-Quantum level
we had with RSA-only certificates.
So if in one year someone invents a new attack against NTRU, I donÂ´t want
to have downgraded the security level like we did with reintroducing RC4
after the BEAST attack (if I remember correctly).
So the idea is to have both RSA+NTRU for the forseeable future(e.g. 10
years), until we decide that RSA really does not help anymore and NTRU has
been vetted enough that we can trust it alone as our only public key
Then we could drop RSA, and switch to NTRU-only.
(I am talking only about NTRU here. If some other algorithm comes up that
prooves to be better, I donÂ´t mind replacing NTRU with it. NTRU just looks
like the most promising candidate from a practical perspective to me)
And I want opportunistic Post-Quantum protection, so as soon as both the
browser(client) and the server support the new PostQuantum algorithms,
they should be protected from Quantum attacks.
To achieve that, we either need 2 different certificates, or one
certificate that contains both RSA and NTRU.
I believed that handling 2 different certificates will be too complex
(errorprone) and therefore too costly for the users. And for commercial CAs you would have to buy 2 certificates instead of one... And then we would have to have twice as many root certificates, ... and
most root certificate list vendors already have limits on the number of
root certificates per certificate authority (3 if I remember correctly).
ItÂ´s already hard for users to generate 1 keypair, get a certificate for
it, install the certificate correctly, asking them to do that twice will
make it less likely for them to succeed.
So I was looking for a solution to integrate both RSA and NTRU into a
single certificate.
I started by developing a concept to integrate a second key and signature
into a X.509 certificate in a downwards compatible way, called "Additional
Public Key" APK.
So old browsers/clients still see a normal valid RSA certificate, and can
ignore the NTRU addons.
And new browsers have a RSA+NTRU certificate, and can check both. So we
donÂ´t loose security and we add Quantum Protection.
Then for the CA (and Sub-CA) certificates, I developed a concept to also
have an additional signature in the certificate.
So now we can have a complete certificate chain, where every link in the
chain has double strength for new systems and downwards compatibility for
existing systems.
So with these concepts we can opportunistically activate Post-Quantum
protection for SSL/TLS.
But there are still a lot of things that need to be done.
We need TLS ciphersuites that make use of the Additional keys. I already have high-level ideas how those ciphersuites should look like,
but I havenÂ´t figured out the details and implemented them yet.
For example we need a PKCS extension so that you can backup and
transfer your keys.
(We need to enhance the PKCS API,...)
And we will have to figure out a lot of other details, bits and blobs, and
engineer a good solution for it all.
I think that we will need 2 of the big SSL/TLS stack vendors to join this
project and commit themselves to get it prototyped
Unfortunately about a year ago I got distracted from this project, and
didnÂ´t pursued it further since then, partly because I had the feeling
that I couldnÂ´t convince others about the importance of this project back
(And recently one of them came back to me and asked me to publish about
all my experience from 2 years ago now)
These are the documents and presentations I was working on back then:
I am thinking about preserving them at that location for historical
reasons, and creating new working copies elsewhere if I decide to continue
with this project.
Recently the NTRU guys developed a new TLS extension concept to start
bring NTRUencrypt into TLS in a opportunistic way (in with RSA), which is
a good first step on the long way to fully protect SSL/TLS:
(It heard that the first one got implemented in WolfSSL, if anyone wants
to play around with it)
I think a lot of help is needed to migrate from Pre-Quantum to Post-Quantum.
If you have a clue about lattice maths, number theory, ... then please
take a deep look at NTRU, PassSign and the other PostQuantum algorithms on
the table and my NTRU Key Exchange proposal, try to find further holes in
it, search the weaknesses that have not been thought of yet. And validate NTRU and the others in a structured way against all the
attacks that were used against other public key cryptography algorithms.
And please try to attack it in a completely new way.
If you are good in auditing sourcecode, please audit the reference
implementations and other implementations (Tim BuktuÂ´s).
If you know about implementation attacks side-channel attacks, then please
help to secure the PostQuantum algorithms against them.
If you are good in testing, please setup a big Post-Quantum crypto
interoperability testing lab.
If you have experience in developing Wireshark plugins, please develop
Wireshark decoders for the NTRU certificates and the new ciphersuites.
If you run a certificate authority, please join the existing projects to
develop Post-Quantum mechanisms and provide your knowledge and experiences
about the demands and requirements from the CA point of view. (2 years ago I had the feeling that the academic world did not understand
those market forces very well, and designed systems that were not
compatible with the processes in the industry)
Another thing I am missing is a Post-Quantum SRP replacement. If anyone
has any ideas how to achieve that, please let me/us know.
If you know any other Post-Quantum algorithms that should be considered,
please let me know. We should have at least one alternative to NTRU in
preparation in case it fails unexpectedly.
If you find a way to proof that NTRU is safe against Quantum computers,
please publish it. (ItÂ´s currently assumed, but not proven yet)
If you find a way to proof that NTRU will be broken by Quantum computers,
please publish it as well.
If you think that I should start again to invest my time into this
project, please let me know. (Sometimes I need some motivation ;-)
I hope I didnÂ´t forget too much, perhaps you will find some more
information in the papers and presentations I wrote (link above).
I would like to thank a number of people for their discussions, ideas,
Best regards,
PS: And please donÂ´t forget the most important point regarding crypto: You
can it, but when you break it, you have to fix it!

@_date: 2015-11-12 23:59:09
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Post Quantum Crypto 
Yes, I am well aware that D-WaveÂ´s processor architecture isnÂ´t suitable
for ShorÂ´s algorithm.
algorithm or GroverÂ´s algorithm, but is a quantum computer in the larger
sense. Yes, those qubits are quite analog, I agree. So I would categorize
it as mixed-signal technology (both digital and analog). ItÂ´s not purely
analog, I would say.
The important thing D-Wave showed from my point of view is that Quantum
computers (albeit not the ones we cryptographers are interested in) are
scalable in practice. In 2007, it wasnÂ´t clear, whether *any* kind of
quantum computer could be scalable to something larger than 5 Qubits. Now,
D-Wave has more than 1000 Qubits, so they prooved that there is at least
one kind of quantum computer that can be scaled up.
2 years ago, I worked myself through the D-Wave website for about 3 weeks,
reading and understand the papers and the architecture and scalability of
D-WaveÂ´s chip design. I compared the technical details that D-Wave was
providing and the arguments the people who spoke against D-Wave, and back
then D-WaveÂ´s documents looked more plausible, coherent and meaningful to
me. IÂ´ve been personally studying electrical engineering and chip-design
for about 3 years now. There is a chance that I got it wrong, and itÂ´s
likely that things will turn out differently, because inventions in the
future are hard to guess. But I think I invested a reasonable amount of
time and energy to research the topic myself to come to my own conclusions
about the topic, to be able to build a reasonable risk-analysis.
I tried to keep the reasoning short, because I didnÂ´t wanted to bore
people with details and wanted to concentrate on the crypto-related
things, if everyone agrees on the result of the risk-analysis already. It
seems I made it too short, so feel free to dig deeper and ask me more
questions, I donÂ´t mind discussing it in more detail and explaining the
reasoning behind my risk-analysis more deeply.
I think that the algorithm I designed is a bit more symmetrical, but
perhaps your algorithm is faster.
I just noticed that I hadnÂ´t provided PDF versions of my papers and
slides, so here they are:
Best regards,
Philipp GÃ¼hring

@_date: 2015-11-13 01:14:58
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Post Quantum Crypto 
The timing has been an interesting thing in this project, yes.
In the beginning, when I realized the state-of-the art of quantum
computers, I was shocked and ashamed that I had not taken quantum
computers seriously back in the days when I was caring about the security
of a certification authority, and I had the feeling that the nearly the
whole crypto community had completely missed the opportunity to develop
sustainable algorithms in time, and to design proper public key migration
mechanisms. After I had developed solutions for the (from my point of
view) big migration problems, I cooled down again, thinking that I had
solved the big migration puzzle, and that my migration scenario just has
to be implemented now, but we have now a plan and a good chance to survive
the cryptocalypse.
It felt like solved and ticked off to me.
When I read the announcement of Quantum-Safe Suite-A/B, I wondered why it
seems to have taken them one year to find and read my papers, and it made
clear to me that they currently arenÂ´t 10 years ahead of us.
And in the end, if my friend hadnÂ´t come back to me and clearly argued
that itÂ´s time to publish it now, I wouldnÂ´t have sat down and written it
all up, and I hadnÂ´t sent that email. So it actually wasnÂ´t my sense of
timing. But I am used to develop things years before the market is ready
for them.
Thanks for the links. I think I will have to invest quite some time again
to get up-to-date.
Ok, it seems, I shortened that too much.
What I was thinking about back then was the likelyhood of a capable
quantum computer to be buildable within 5 years, not the specific point of
time when a quantum computer will be actually built (which is much harder
to guess). I started with the 5 years assumption, and thought about how
realistic that could be. I didnÂ´t tried to guess how long it will actually
take. And I thought about the likelyhood of the public being told about
the existance of the first Shor-capable large-scale quantum computer.
So I wasnÂ´t saying that a quantum computer is 5 years away, but that I saw
it likely that it might be buildable in 5 years.
Some more reasoning about the timeframe:
I studied the D-Wave architecture, and found that the current architecture
is fundamentally a 2-dimensional architecture, and it scales accordingly.
So I expected 1-2 further steps where the amount of qubits can be doubled,
but then I expect a decline in the growth rate, since the chip will get
too big. (they are at 1000-2000 qubits now, so the slowdown of growth
should start now) Within 5 years, I think I expected about 5000 qubits, if
I remember correctly. And I expected that growth beyond 10000 qubits will
be hard, similar to the hardness of CPU speeds above 4GHz.
I assumed that an attacker has the budget of a three-letter-agency, and
owns a chip fab with the best processes available, and has access to
not-yet-published research of most of the quantum computer researchers,
and has full access to D-Wave technology.
In 2014, D-Wave gave the comment that they could develop a Shor-capable
quantum computer if they wanted (and I had to change my predictions), but
they obviously donÂ´t want to, since the Return-on-Investment likely has to
be done with the sale of 1 single computer, and given that itÂ´s much
cheaper to break into nearly every computer on the planet, than to do such
an investment, which will also rapidly loose itÂ´s value, itÂ´s an extremely
tough business-case.
By the way, has anyone set up a quantum-computer honeypot yet?
Best regards,
Philipp GÃ¼hring

@_date: 2015-11-13 02:21:52
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Post Quantum Crypto 
specific kind of quantum computers. I have been researching on the history of quantum computers, from the
first ideas about them by Richard Feynman
 to the
development of ShorÂ´s algorithm in 1994. And in the period between ShorÂ´s
algorithm and 2001, there were 2 big open questions: Is it possible at all
to build a quantum computer that can run ShorÂ´s algorithm? And between
1994 and 2010 the second big question was, whether it would be possible to
scale it up. The qubits are living in a 3-dimensional world where you need
to entangle them, and there are things like noise and 3-dimensional
topology and stability and a few other effects that make it really hard to
scale it up, and it wasnÂ´t clear whether scaling it up much wider is
possible at all.
I read through the papers of IBM 2001 and D-Wave, to understand the
architecture any potential scalability issues (in space and in time), to
be able to extrapolate from that. For example, I saw some scalability
problems in the communication link between the quantum computer and the
outside world in the IBM concept, and saw that D-Wave seems to have solved
that particular issue.
I havenÂ´t found a proof for that claim yet. Do you have one?
I agree that itÂ´s not a universal-register based quantum computer, and
that it does not fulfill the wishes people have from a quantum computer
due to the expectations that were raised. So from a simplified public
opinion point of view, I might agree with this sentence, but from the
technical point of view, I think I disagree.
Possibly, yes. There are currently very very few "algorithms" available
for Quantum computers.
Those are interesting discussions, and I think I read the first one back
then. But I am wondering why they canÂ´t provide a few simple easily
verifyable facts that dismiss the technological basis of D-Wave.
The points that I agree with are that D-WaveÂ´s marketing had overstreched
their abilities. But I based my risk analysis on the technical papers and
documents of D-Wave, not on their marketing.
And I think the question, whether the D-Wave machines are faster than
classical algorithms or not is not really relevant for crypto.
There are a huge number of architectural issues around the quantum part of
the quantum computer that have a huge impact on the performance.
In 2014 D-Wave claimed in a comment that they could extend it if they
wanted. I am not 100% sure about that claim, and whether the comment was
really meant that way, but it fits the impression I got from all their
underlying technology and the way they are working.
Best regards,

@_date: 2015-11-13 03:23:51
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Post Quantum Crypto 
Are you sure? I would expect quite a number of systems to not be able to
handle 2 different certificates for the same identity. And it would likely
leave us open to downgrade attacks against NTRU in case NTRU were found to
be weaker than RSA, if both certificates are equal. I am not sure, whether
we should propose that.
I think that there are lots of software-stacks out there that can only
manage one certificate for an identity. Has anyone done large-scale
interoperability tests for a 2-certificate scenario? Could I see the
results please?
We will need new CA software to support NTRU anyway, to check
NTRU/PassSign signature on CSRÂ´s, and to sign the certificates with the
new signing algorithms.
I think that we have to renew the whole stack anyway, whether we go the
one-certificate way, or the 2-certificate way.
Then we just drop NTRU and replace it with a different algorithm. But the
migratability is already in place then.
For the software developers who develop the crypto stacks, yes.
But for the millions of end-users?
I am sorry, but this is wrong:
"A maximum of three roots per CA provider can be accepted because each
additional root negatively impacts users by increasing download time."
And the problem for the CA is that it simply canÂ´t decide to have
dedicated or different root certificates just for Apple, so this simple
rule automatically creates a lower bound for all root certificates for the
internet for all internet-relevant CAÂ´s.
Well, perhaps you could create a new company to get around this
restriction, but itÂ´s likely more economic to adhere to the restriction.
And perhaps you can negotiate exceptions with them. I havenÂ´t tried that.
I decided to accept this restriction for my designs.
I really liked Mozillas response a few years ago on this topic, which went
something like this: "In the beginning there was just one CA, and
everybody was complaining. So we fixed it, and now there are hundreds of
CAÂ´s, and again everybody is complaining"
I think the problem is that people arenÂ´t happy with the structure of the
CA market, not just with the size.
I had a different question for this topic:
"Do you want to have a CA in your country that speaks your language and is
able to provide support locally?" If the answer is yes, you automatically have to accept up to 196 CAÂ´s on
this planet, if you concede to others what you want yourself.
Now do you feel good about 196 CAÂ´s in your browser? No? Ok, I am sorry,
but there are not enough CAÂ´s to provide one with support in your country.
I think any number of CA`s are not good, because whoever you ask, itÂ´s
either too many or too few.
Yeah, LE wasnÂ´t on my roadmap 2 years ago, if I remember correctly.
Yes, LE should automate provisioning both algorithms. I hope it does.
Yes, thatÂ´s because I have been working on worst-case scenarios, not on
most-likely-timeframe scenarios. I didnÂ´t tried to extrapolate the
academic situation, instead I tried to estimate based on a hypothetical
well-funded attacker, willing to invest considerable ressource to buy into
the abiltity.
There are likely a lot of economic incentives not to push the speed that
much. But given a well-funded and willing attacker ...
The other thing is that I also looked back in time. IBM prooved the
viability of Shor in 2001, but kept quiet since then. They later claimed
that they got into a technological dead end. But if that was just an
excuse, then it might be that quantum computers are already existing and
we just werenÂ´t told yet.
So I also investigated the likelihood whether we will be told about it, or
Best regards,
Philipp GÃ¼hring

@_date: 2015-11-14 18:37:30
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Fwd: Re:  Post Quantum Crypto 
Interesting question.
I am not sure, whether the processes at runtime are only 3-dimensional or
more dimensional, but the production process in the chip fab is
What impact on the security could it have from your point of view?
Best regards,

@_date: 2015-10-22 21:15:02
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Other obvious issues being ignored? 
I agree that it´s impossible to catch every boneheaded issue someone could
produce. But I think that there is some value in a list of crypto related
mistakes people make that are not extremely obvious, and that should still
be avoided.
* Do not deplete /dev/*random by using fopen() fread() without disabling
* Random number generators have a reasonable usecase for reading
uninitialized memory
I´ve seen such things too often in code that was developed by skilled and
experienced developers, that I think we really should develop a checklist
for crypto and it´s applications. Best regards,

@_date: 2015-09-23 20:04:11
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Non-Authenticated Key Agreement 
Diffie-Hellman has one (from my point of view) important, non-obvious and
seemingly largely unknown security property:
It protects both parties from an insecure key resulting from the protocol,
if and only if they follow their part of the protocol correctly.
If Alice originally creates an insecure variable d (data), e.g. due to bad
random number generator, or due to some other problem, then Bob will end up with an insecure key, although he followed your
protocol correctly.
Diffie-Hellman makes sure that both parties end up with a secure key d if
at least any one of them has a secure random number generator and follows
the protocol correctly.
Best regards,
Philipp GÃ¼hring

@_date: 2016-08-07 22:14:13
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Where to Find PQC Crypto Libraries? 
I am not aware of a comprehensive PQC library.
NTRU is available here:
Best regards,
Philipp Gühring
-----Original Message-----

@_date: 2016-05-02 19:25:34
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] [FORGED] Re: USB 3.0 authentication: 
I agree with that problem, but it seems to me that this spec will not
help, since it does end-to-end authentication, but you have a
cable-in-the-middle problem. So you have to authenticate the whole chain,
and I currently do not see any technology that could do that.
Perhaps a slow start mechanism like TCP would be a better idea (to ramp up
the power slowly), and to measure the temperature of the cable all the time.
Best regards,

@_date: 2016-05-05 19:58:00
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] USB 3.0 authentication: market power and  DRM? 
And even UART got a Plug&Play extension, to enable automatic driver
loading, at least for Mice and Modems:
Since Microsoft Windows 95 is referenced in the standard, I guess that at
least Windows 95 and perhaps some more versions would have implemented it,
but I am not sure.
Best regards,
Philipp Gühring

@_date: 2016-11-23 22:54:25
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] On the deployment of client-side certs 
Yes, I tried to argued that too for some time, that HSMs for
keymanagement/keysecurity are somewhat superfluous, since they are not
sufficient to protect a PKI.
The practical example that drove the point home was the DigiNotar hack,
where they demonstrably brought down not just the HSM's security model,
but the whole company, a good part of the government and thereby the whole
Because the attackers told the HSM to sign their keys, but since the HSM
only used the key but did not know or record what it was used for,
DigiNotar could not revoke the fraudulent certificates that were signed by
their FIPS certified HSM.
The light at the end of the tunnel was Apple recently who built an
ingenious security concept based on HSM's, which convinced me from the
conceptual point of view, and prooved that at least someone was able to
build a reasonable security system on HSMs, so we don't need to give up
the HSM form factor as a whole. And there Apple developed their own software on their HSMs.
So if you want a secure system, you have to build your own software on HSMs.
Just running standard PKCS or similar software on it is not secure enough.
If you want to run a PKI, develop your certificate issueing software
inside the HSM.
Best regards,

@_date: 2016-09-04 10:11:33
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in 
Yes, that's what I was thinking about too in the past few days.
Do we have a cryptographically strong ECC algorithm?
For Non-Error correction codes, we have simple CRC Checksums (which are
not cryptographically strong) and we have hashes (which are
cryptographically strong).
For Error correction codes, we have ECC/FEC (which is not
cryptographically strong as far as I remembere), but I don't remember
having seen any cryptographically strong algorithm.
I guess that there are modes that combine both cryptographical hashes and
ECC, but are they secure? (Think about Encrypt-then-sign vs.
Sign-then-Encrypt and similar issues which can have non-obvious issues)
Best regards,

@_date: 2017-03-29 08:47:18
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] "Perpetual Encryption" 
What I am missing is some kind of authentication. Did I overlook it?
The big problem I have with OTP's is that they are not tamper-evident.
Best regards,

@_date: 2017-03-29 08:54:56
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] AES related issue 
I am currently playing around with an encryption/decryption module, which
claims to be using AES-256. Under normal circumstances, Encryption and
Decryption seem to work fine. Now I tried to play a bit with the key during the decryption operation,
and when I seem to insert the wrong key, the module returns high entropy
Surprisingly, the high entropy blocks contains only on average 7.58 bits
of entropy
per byte (it was in the range of 7.53-7.604). The keys I inserted were similar to the original key, so possibly just
single bit-flips difference to the original key.
entropy per byte. 7.58 looks like a problem to me, thats a bit more than
5% too deterministic, I would say.
I took a look at bit biases, but I couldn't find any yet.
I took a look at ECB encryption artifacts, but I couldn't find that either.
Are there any encryption modes where you would expect such problems?
If anyone is interested in further analyzing this issue, I can produce any
amount of cleartext+wrongly-decoded samples.
Best regards,

@_date: 2019-06-03 18:30:57
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] About Secret Sharing Schemes and a Question 
Well, it depends on how you handle it, and what your application really
is. You could put the key-restoration and key-using into a secured
environment, like a High-Security-Module, so that the parts are assembled
in a secure environment, used, and erased shortly afterwards, so that none
of the knights gets to know the key or the shares of the others.
There are other schemes for e.g. digital signatures (which are different
to Shamir's Secret Sharing Scheme), where different mathematics and
algorithms are used underneath, and the actual application of signing is
spread across all participants with their individual shares, and then only
the results are combined into the combined result, not the shares
themselves. So neither of the knights would have to disclose his share to
the others or to any central place, but they could still collectively do
something ... digitally sign the command that the door should unlock itself.
But yes, what if the knights leave the door open in the end, or what if
the last knight leaving blocks the door so that it cannot be closed again? ...
In practical implementation you will potentially run into debugging
problems. What if the crypto implementation of one knight is faulty, or
what if one knight deliberatly sends in wrong data? How do you identify
whether one of the shares that is offered is a valid share or not? What do
you do with replay-attacks, ... How do you exchange the keys? How do you exchange the keys of those
knights that are not present? What do you do when there are not enough
knights available in one year, will that cause a denial-of-service?
Best regards,
Philipp Gühring

@_date: 2020-03-05 14:45:07
@_author: Philipp =?iso-8859-1?Q?G=FChring?= 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His 
Remapping bad blocks has been done for a long time on rotating media.
Well, I wouldn't call it sins. The involved physical processes have their physical limits. And there are lot of things where you have to make trade-offs during design and production (on the other hand, those tradeoffs are a possibility to optimize for a specific target application). At the moment it seems to me that it's impossible to produce 100% perfect flash memory in an economic way. And just throwing away 99% of the produced flash because it's only 99% perfect would be a huge waste of ressources from my point of view. So the manufacturers have to deal with imperfections. There are 2 technologies: Remapping and Wear-Leveling. Remapping only remaps single blocks, where you have a low risk of leaking data from my point of view. Wear-Leveling on the other hand never overwrites the pages, and it keeps the old data in place until all other space is reused, then it clears/flashes the whole block, and then it copies the new data and other old pages into the empty block. HDDs usually use Remapping and Flash usually uses Wear-Leveling, but for both categories I know about exceptions. Economic-wise, Wear-Leveling has a higher demand for RAM inside the disk, because it needs to know for every block or every page (or some other addressing granularity) where it is. For further information search for Flash-Translation-Layer (FTL). e.g. pages 25-29 of The numbers I heard are 1-2% for low cost products, 7% for consumer products, 50% or more for enterprise products. But this likely depends on the manufacturer, and in some areas it also depends on the reseller, because resellers have "Mass Production Tools", where they can arbitrarily define the percentage they want for their specific application. So in practice, anything is possible.
And on the other hand it can also be increased by the user with the "Host Protected Area" or a partitioning scheme that does not use the whole SSD.
Yes, 50% or more is what I heard.
Identifying the percentage should be rather easy: Open the disk (or search the internet for a photo of an opened disk), identify the flash chips, lookup the capacity of the flash chips, and see how much the SSD claims to provide.
You dont need new firmware. All firmware I have seen and heard of is configurable. During manufacturing and often also afterwards.
The deeper you dig the more layers with "spare" space you encounter. There is the Host-Protected-Area which is visible to the operating system if it knows about HPA but most operating systems likely dont care about it. The HPA is often used by RAID controllers and other system tools. And I heard recommendations to increase the HPA or to reduce the partitions used, to voluntarily reduce the used size of the SSD and to increase the size of the spare blocks to enlarge the lifetime.
Then inside the drive there are spare blocks, and they need to be there, because the wear-leveling always merges pages from old blocks into a new block, so it needs space to do that if the disk would be full. The blocks usually have spare pages, and the pages themselves also have a lot of space in them which is used for tracking the usage, for ECC, and I would not be surprised if there isnt even some space left that might be reserved for formware updates.
And by the way, there are also complex ECC (Error correction codes) in place, some of them even doing something like a RAID between the individual chips in a SSD...
Best regards,
Philipp G?hring
