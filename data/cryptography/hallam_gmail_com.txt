
@_date: 2013-08-20 14:26:06
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What is the state of patents on elliptic curve 
It is almost certain that most uses of EC would not infringe the remaining
But the patent holder can force anyone attempting to use them to spend
about $3-5 million to defend their right to use EC and so there is very
little incentive to do so given that RSA 2048 is sufficient for almost any
The situation might change depending on who buys RIM.

@_date: 2013-08-22 10:06:13
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What is the state of patents on elliptic curve 
Given that I am an expert witness specialising in finding prior art for
patent defences, my original post was a statement against interest as I
would be in with a good shot of getting a $100K gig if someone did decided
to test patentability of EC.
There is no way to be sure that anything is free of patents, but in this
case we are pretty sure that there will be a suit.
This is not an exception to the usual approach either, quite a few of my
design proposals in IETF have been shot down as 'too clever', i.e. someone
might have filed a patent.
What worries me on the Certicom patents is whether the 20 year from filing
or 17 years from issue applies since they are continuations in part on a
filing made prior to 7 June 1995.

@_date: 2013-08-22 10:36:48
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM PROOF Email 
Thanks to Snowden we now have a new term of art 'Prism-Proof', i.e. a
security scheme that is proof against state interception. Having had an
attack by the Iranians, I am not just worried about US interception.
Chinese and Russian intercepts should also be a concern.
We have two end to end security solutions yet Snowden used neither. If PGP
and S/MIME are too hard to use for the likes of Snowden, they are too hard
to use. The problem Snowden faced was that even if he could grok PGP, the
people sending him emails probably couldn't.
So to be properly PRISM-Proof an email security solution must be both proof
against state interception and easy enough to use that it can build
critical mass.
By easy enough to use, I mean 'absolutely no additional effort to
installation of an email client'. Everything has to be transparent to the
end user who is not a crypto expert and may well be a bit of a doof.
The traditional approach to making a system intercept proof is to eliminate
the intermediaries. PGP attempts to eliminate the CA but it has the
unfortunate effect on scalability. Due to the Moore bound on a minimum
diameter graph, it is only possible to have large graphs with a small
diameter if you have nodes of high degree. If every PGP key signer signs
ten other people then we have to trust key chains of 6 steps to support
even a million users and nine to support a global solution of a billion
End to end has an unfortunate effect on usability as well.
My solution is to combine my 'Omnibroker' proposal currently an internet
draft and Ben Laurie's Certificate Transparency concept.
Omnibroker introduces a trust agent for the relying party to match the
trust agent for the key holder (the CA). So rather than have the usual
problem of the CA being selected by one side but delivering trust to the
other, each side in the transaction has 'representation'.
Since email is an asynchronous protocol there is no reason not to check for
CT proofs everywhere we can. But the omnibroker selected by a user has the
primary responsibility for keeping them safe. This means finding an
encryption key for the intended recipient of an email and verifying it, if
this is possible. If not, the broker returns evidence to the contrary (if
this is possible).
So the way this would work is that to use the system you would either
install a PPE aware email client or a pair of SMTP/IMAP proxies that are
PPE aware.
Whenever an email is sent the PPE client queries its local cache and the
omnibroker to determine if there is an encryption key on file for it. If
so, the email will be sent encrypted. The PPE client could in theory access
any key store including existing PGP key stores. It can also implement
rules like 'email sent to x.com must be encrypted'.
I believe that the user interface can be made essentially zero effort. If
the installer for a proxy set knows how the email app stores the
configuration data for mail servers, it can even rewrite these
Preventing key substitution will require a combination of the CT ideas
proposed by Ben Laurie (so catenate proof notaries etc) and some form of
'no key exists' demonstration.
For spam control reasons, every email sent has to be authenticated which
means using digital signatures on the message (and likely DKIM + SSL client

@_date: 2013-08-23 18:53:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM PROOF Email 
The number of CAs would not need to be very large, I would expect it to be
in the hundreds in a global system but that is pretty much a function of
their being hundreds of countries.
If example.com wanted to run their own CA for their own email certs then
the way to do it would be to issue them a cert signing cert that has name
constraints to limit its use to just name at example.com.
The idea is that there are multiple CAs but their actions are all vetted
for transparency and they all check up on each other.
Any one CA can be served with an NSL, but if they issue a coerced
certificate it will be immediately visible to the target. So a government
can perform a DoS attack but not get away with an impersonation attack.
Well the way that was solved in practice for PGP was Brian LaMachia's PGP
Key server :-) Which turned into a node of very high degree...

@_date: 2013-08-23 19:05:29
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM PROOF Email 
Thats what the IETF folk told us when I worked on HTTP 0.9 against Gopher
and FTP.
Usability matters. In fact it is all that matters for adoption. Angry Birds
has made a billion dollars because it is so nice to use that people will
pay to use it.

@_date: 2013-08-23 18:51:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM PROOF Email 
Yeah, I think it is just a matter of being clear about the requirements and
making sure that we fully justify the requirements for email rather than
assume that email is the same.

@_date: 2013-08-25 23:40:35
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Traffic Analysis (was Re: PRISM PROOF Email) 
There has to be a layered approach.
Traffic analysis is probably going to demand steganography and that is
almost by definition outside standards work.
The part of Prism that I consider to be blatantly unconstitutional is that
they keep all the emails so that they can search them years later should
the need arise. Strikes me that is the type of sophistry that John Yoo used
when he wrote those memos claiming that torture isn't torture.
There will be a reckoning in the end. Takes about twenty to thirty years
before the point is reached that nobody in the establishment has a reason
to protect the war criminals of years past.
I have a little theory about the reason the CIA engineered coups were so
successful from 53 to 73 and then suddenly stopped working. Seems to me
that the CIA would have been nuts to try operation Ajax without some very
powerful intel like being able to break the Persian codes. CIa stopped
being able to mount those exercises after electronic ciphers were
Given how the NSA used their powers last time round to topple democracies
and install dictators I don't think they deserve a second chance.

@_date: 2013-08-26 08:46:34
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
Which is why I think Ted Lemon's idea about using Facebook type friending
may be necessary.
I don't think we can rely on that for Key distribution. But I think it
needs to be a part of the mix.
I have a protocol compiler. Just give it an abstract schema and out pops a
server and client API library. Just need to add the code to implement the
semantics. It is up on Sourceforge, will update later this week.

@_date: 2013-08-26 08:50:32
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
I am doing a history of the Web. I came to the conclusion that the clever
part is the problems it decides not to solve. Ted Nelson was absolutely
right on what was desirable, but what he considered 'essential' turned out
to be easily added as layers (search for example).
A confidentiality solution that tells the user 'you can't send mail right
now because you may be subject to an intercept' is more than acceptable.

@_date: 2013-08-26 16:12:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Using Raspberry Pis 
I really like RPis as a cryptographic tool. The only thing that would make
them better is a second Ethernet interface so they could be used as a
firewall type device.
However that said, the pros are:
* Small, cheap, reasonably fast, has ethernet and even a monitor output
* Boot from an SD card which can be preloaded with the OS and application
build. So it is really easy to use RPi as an embedded device controller.
The main con is that they are not so fast that you want to be routing
packets through them unnecessarily. So they are a great device to make use
of for connection brokering, not such a great idea to tunnel video packets
through them.
It is entirely reasonable to tell someone to get an RPi, download a config
onto an SD card, plug it into their network and apply power and ethernet.
And they take so little power that we could even tell them to install a
pair so that they had a fault tolerant setup (although they are low enough
power, low enough complexity that this may not be necessary or helpful).
In the home of the future there will be hundreds of devices on the network
rather than just the dozens I have today. So trying to configure security
at every point is a non starter. Peer to peer network configurations tend
to end up being unnecessarily chatty and are hard to debug because you
can't tell who is meant to be in command.
The approach that makes most sense to me is to have one or two network
controller devices built on something like RPis and vest all the trust
decisions in those. So rather than trying to configure PKI at hundreds of
devices, concentrate those decisions in just one logical point.
So I would like at minimum such a device to be my DNS + DHCP + PKI + NTP
configuration service and talk a consistent API to the rest of the network.
Which is the work I am doing on Omnibroker.
Putting a mail server on the system as well would be logical, though it
would increase complexity and more moving parts on a trusted system makes
me a little nervous.

@_date: 2013-08-26 17:54:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Using Raspberry Pis 
I don't think the video adds much to the cost.
I do have a USB ethernet adapter... but that cost me as much as the Pi.
Problem with all these things is that the Pi is cheap because they have the
volume. Change the spec and the price shoots up :(

@_date: 2013-08-26 21:46:16
@_author: Phill 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
I am thinking that I want to make face to face exchange of keys via an iPhone 'bump' type app possible
Also I want to be able to use friend relationships as a spam filtering control. Perhaps you only want to accept encrypted email from people if you know them. My spam problem is a little larger than most. While I was doing anti-span at VeriSign I received a quarter of the mail for the company. I have been under a DoS attack on my mail for a considerable time.
But in any case, at the moment we have email, I'm, voice and video all as separate apps unless we go through a proprietary scheme when they become one. The missing piece for email security is key discovery. If we are going to solve that problem for email we should do it for all the other apps as well.
The market for secure email is going to be tiered. There will be folks like us who want to have full control and do a lot of the work ourselves and there will be people who want to buy in the expertise and then there will be institutions that need to outsource.
As folk probably know, I work for Comodo and so I am interested in the possibility of establishing an enterprise market for secure email services. But that is only an interesting commercial prospect if there is a chance that secure email will become ubiquitous. In the near term, the critical mass for secure email has to come from another sector. People concerned about PRISM seems to be the constituency most likely to drive adoption. Even if the threat from other sources (Iran, Russia) is actually greater in my view. The code should be uploaded later this week or early next. Just got back from Europe and having some hardware issues of the expensive kind.

@_date: 2013-08-27 19:00:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
<521CE337.6030706
On Tue, Aug 27, 2013 at 5:04 PM, Wendy M. Grossman <
True, but you are probably willing to tolerate a higher level of spam
getting through in that case.
One hypothesis that I would like to throw out is that there is no point in
accepting encrypted email from someone who does not have a key to encrypt
the response.

@_date: 2013-08-27 22:34:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
How about the fact that the US govt de facto controls the organization
controlling the root key and it is a single rooted hierarchy of trust?
But in general, the DNS is an infrastructure for making assertions about
hosts and services. It is not a good place for assertions about users or
accounts. So it is a good place to dump DANE records for your STARTTLS
certs but not for S/MIME certs.
 I very much agree that deployment is all.
One thing I would like to do is to separate the email client from the
crypto decision making even if this is just a temporary measure for testbed
purposes. I don't want to hack plugs into a dozen email clients for a dozen
experiments and have to re-hack them for every architectural tweak.

@_date: 2013-08-28 10:15:38
@_author: Phill 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
My target audience, like Perry's is people who simply can't cope with anything more complex than an email address. For me secure mail has to look feel and smell exactly the same as current mail. The only difference being that sometime the secure mailer will say 'I can't contact that person securely right now because?'
I also agree that the devil is in the deployment. And that is why I think we need to separate concerns. We are not going to get anywhere if each and every one of us who has an idea has to implement an email client to make it work. And we certainly won't get any deployment if we have to deploy plug ins and other stuff for 50+ MUAs.
My experience of SET was that the scheme failed largely because it lacked agility. The first draft of the protocol was to burdensome to use. But we could not persuade banks who had paid $6 million to a vendor to deploy SETv1 to pay the same vendor another $6 million for the changes necessary to deploy a V2.
In the case of secure email there is an asymmetry. I think that deployed S/MIME has the problem of receiving and decrypting mail solved pretty well. The part that remains broken is establishing keys and sending the messages.
Which is why I think a critical step is to separate out the parts of the problem which can fixed for all proposals from the parts where innovation is possible.
I consider the following parts of the problem to be fixed:
1) Message formats: S/MIME
There is no intrinsic advantage to PGP or S/MIME format so choose one format according to which has the larger installed base. S/MIME wins. End of story. This does not mean committing to S/MIME PKI, or not supporting Web of Trust, but it does mean using CMS/PKCS for message packaging.
2) MUA crypto User Interface: None
There must be no demand made of the user whatsoever. No button to press to send the message encrypted instead. The message is encrypted if the MUA can send it encrypted, end of story. The only UI that may be needed in the MUA would be to give the user feedback as to why something can't be sent.
As it happens, I have been working on a protocol to provide exactly this degree of separation. The idea being that the MUA makes a (secured) remote procedure call to a trusted service that tells it:
1) Whether the email should be sent encrypted
2) The crypto parameters (key, algorithm, etc,) to use if so
3) (optional) proof that allows the MUA to validate the action of the trusted service if the assertion of the trusted service is audit able and the MUA understands how to validate the assertion.
So here is how I would see it working, I have a scheme that combines elements of Certificate Transparency with a meta-notary scheme I have been working on. To implement this scheme I would write the necessary handlers for an omnibroker server to allow us to deploy the scheme and test it. If we find we need to tweak the scheme we tweak the omnibroker side of the scheme, the MUA side stays constant. If we think it is ready for prime time we can reduce the trust dependency on the broker by migrating some or all of the checking into the MUA.
In practice it is likely that we would have omnibrokers that support more than one scheme and in particular provide support for legacy schemes as well. If we have six schemes and three get some sort of traction then we are likely to want to combine ideas into a seventh rather than fight a VHS vs Betamax.
In my particular scheme the omnibroker is a permanent fixture as my approach is to attempt to mitigate the risks of using trusted third parties through separation of duties and multiple controls rather than eliminate them entirely. But I think that people will still find a value in using my scheme as a testbed even if they believe that the only acceptable approach is to eliminate the Trusted Third Parties entirely.
In practice the cost of crypto expertise is always going to exceed the cost of crypto products. I don't know what folk charge in the bargain basement for crypto clues but I rather doubt its less than my plumber. If someone can make a buck from a PRISM Proof email scheme then they will have a motive to facilitate deployment and spread it quicker.

@_date: 2013-08-28 13:36:31
@_author: Phill 
@_subject: [Cryptography] Why human-readable IDs (was Re: Email and IM are 
<521CE337.6030706
 I don't see that at all. In fact I think that nothing has hurt deployment of PKI more than LDAP. The problem for the email client is very simple:
"What is the key etc. to send email to alice at example.com"
I can solve that very easily with a HTTP lookup or a very short Web Service with JSON query syntax. If LDAP is involved there will be a consultant setting up the directory and building fancy DIT trees and racking up bills of $100,000+ for something that makes no difference to the actual query.
Now if the certs are already in an LDAP directory then fine, lets pull data from that resource. But if they are not in LDAP already there are much easier ways to interface a database of certs to a query interface.

@_date: 2013-08-28 13:47:01
@_author: Phill 
@_subject: [Cryptography] IPv6 and IPSEC 
(This is the last week before school goes back which is stopping me getting to the big iron and my coding platform if folk are wondering where the code is).
I had a discussion with some IETF types. Should I suggest a BOF in Vancouver? Maybe this is an IRTF effort rather than IETF. One thing that we maybe should face is IPR considerations and move what is becoming a design discussion to a list with an established IPR rubric like Note Well. In the past I have had whole standards efforts collapse because Microsoft or whoever objected to the IPR being possibly contaminated by being discussed in a forum without an IPR regime (though I suspect that was a pretext rather than a reason).
One question is whether we could make use of IPSEC and/or IPv6. Now I do not for an instant accept that we should make any proposal dependent on deployment of either. However IPv6 does have some very convenient characteristics for traffic analysis hardening. My view has always been that the proper approach to security is to have multiple layers so I would see IPSEC as being an addition to TLS and message layer security.

@_date: 2013-08-28 14:31:05
@_author: Phill 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
You have to have key backup to address that security goal. And that will necessarily mean that you increase your coercion risk. And which security goal you choose to satisfy is likely to depend on your situation.
One solution would be to back up your private key and put the shares in one or more bank safes. But then you are vulnerable to a coercion attack on your bank. Which you can address by putting the shares in a tamper evident bag but only if you go to the bank regularly to audit it.
One of the features of this problem is that if you make absolute security a requirement you are going to go absolutely potty trying to solve every element. Fortunately we can still do a lot of good by providing a system that prevents wholesale abuses.
I am not a crypto-absolutist. I don't particularly want to be giving crypto to terrorists. When I was 18 I woke up to hear that the IRA had attempted to murder my cousin. However I don't want to be giving intercept power to Putin who murders people with poisoned teapots on the streets of London either. And I certainly don't trust the NSA and GCHQ with the wholesale intercept capability revealed by Snowden.
And for a company it is almost certain that 'secure against intercept by any government other than the US' is an acceptable solution.
I am currently working on a podcast history of the web to publicize my expert witness practice. Which had me looking at the reason Tim Berners Lee succeeded where Ted failed. The thing that distinguished their efforts was not the problems they solved. Ted had 120% of the Web ten years before Tim started.
The difference was that Tim realized that some of the problems were very hard and could be punted on for a first draft. Then after the Web took off it built out infrastructure that made it possible for others to fill in the gaps. So Ted had search built in. Tim had a hole which was filled by others.

@_date: 2013-08-28 16:24:29
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Source for protocol compiler 
The source is up on sourceforge now. It does need some spring cleaning and
documenting which I hope to get to next week.
The documentation is in the following directory
The origins of this work is that about 70% of the effort in working groups
goes into debating 'bikeshed' type issues (as in what color to paint it)
that really don't matter. Things like choice of encoding (just use JSON) or
binding (Any reason to not use HTTP + SSL) and so on.
And 70% of the effort of the editor would go into making changes to the
spec which would need to be reflected accurately in six different parts of
the document and the reference code and then conformant examples generated
and inserted at the right place and then other folk would have to check it
was all done right.
So JSONSchema converts an abstract schema definition (in a programming
language syntax, not JSON encoding!) and produces a stub client API and a
stub server with the appropriate holes to plug in your semantics. You can
then write documentation and insert examples from running code (provided
you like documentation in either HTML or Internet Draft format at the
It is all written in C# and has been run on OSX and Linux under Mono
(binary distributions to follow). The synth currently only generates code
in C# but I plan to add C and probably Objective C down the line. The
meta-synthesiser is also on sourceforge and open source:
The compiler only supports RPC like interactions at the moment, i.e.
query/response. But I am planning to expand the generator to support an
additional interaction pattern in which the client opens a transaction and
receives a series of async callbacks. That would be suited to supporting
chat like protocols.
One of the things I realized as I was doing all this is that all Web
Services really consist of are glorified RPC calls in a different syntax.
The code generated right now is targeted at being reference code but there
is no reason why the synth should not generate production code.

@_date: 2013-08-29 12:38:17
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
Systems do need to be usable in practice and too much security can be a bad
thing. I am thinking about 'PRISM Proof' as a hierarchy of needs:
0 No confidentiality requirement
1 Content Confidentiality Passive intercept (met by STARTTLS)
2 Content Confidentiality Active Intercept (met by STARTTLS + validated
recipient server cert)
3 Content Confidentiality Coercion or compromise of Mail service provider
4 Content Confidentiality Coercion or compromise of Trusted Third Party
5 MetaData Confidentiality
6 Traffic Analysis Confidentiality
At present we only have a widely deployed solution for level 1.
The constituency that has a requirement for level 6 is probably very small.
Certainly none of us would benefit. Is is a hard goal or a stretch goal?
It is certainly a desirable goal for people like journalists but the cost
of meeting the requirement may not be acceptable.
At any rate, I think that starting by trying to build something to level 4
would be a good start and provide an essential basis for getting through to
levels 5 and 6.
It might be that to get from level 4 to level 6 the solution is as simple
as 'use a German ISP'.
Since we are talking about Snowden and Greenwald, folk might be amused to
learn that I was the other party who contacted Baghdad Boylen, General
Pertreaus's spokesperson who sent Greenwald a bizarre email which he then
lied about having sent (to me, Greenwald and Petreaus), apparently unaware
that while an email message can indeed be faked, it is improbable that
these particular message headers are faked.
Further, had any such attempted impersonation of Boylan taken place it
would have been a very serious matter requiring urgent investigation. Since
I was never contacted it is clear that no investigation took place which
can only mean that Boylen did send the emails and then lied about sending
If a UK military officer had sent a similar email he would be cashiered.
But then again, in the British army Colonels are not minted by the thousand
as in the US.

@_date: 2013-08-29 16:31:57
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
Quite, I had a conversation with a government type this morning. His
question, 'what if the intercepts are shared with the IRS'
Moreover Snowden has proved that the internal controls in the NSA are lax.
If a low level grunt working for a contractor has such access to the NSA's
own crown jewels it is idiotic to imagine that they keep the confidential
secrets of IBM or Microsoft or GE with greater care.
And, they told us so.  In the comments made by the NSA, they have very
They will keep the data anyway. They will query it if there is evidence of
a crime but otherwise they are keeping everything.
And worse, they are creating fake stories to explain how the data was
collected. So they have perjured themselves in numerous criminal
prosecutions that are likely to be found unsafe when the full extent of the
scheme emerges.
This is not a stable situation. It is easy to see why Obama was infatuated
with the intelligence community and thus willing to give them carte
blanche. He came into office with the US losing two wars and a military in
which every staff officer who had had the courage to tell Rumsfeld his
plans were insane was dismissed. The intelligence services were the only
part of the military Obama could trust to provide an exit strategy.
But the next President is not going to be beholden to the intel services in
quite the same way. Even Obama appears to be starting to ask questions
about how the intelligence results are being achieved.
Not necessarily.
We have lots of technology. This is not a technology problem, it is a
deployment problem. The greater the level of concern, the easier deployment

@_date: 2013-08-29 16:38:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] IPv6 and IPSEC 
It is a stupid and incorrect requirement.
The DNS has always allowed multiple A records to point to the same IP
address. In the general case a mail server will support hundreds, possibly
tens of thousands of receiving domains.
A PTR record can only point to one domain.
The reason that an MX record has a domain name as the target rather than an
IP address is to facilitate administration. Forcing the PTR and AAAA record
to match means that there has to be a one to one mapping and thus defeats
many commonly used load balancing strategies.
Google is attempting to impose a criteria that is simply wrong.

@_date: 2013-08-29 17:51:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Keeping backups (was Re: Separating concerns 
<521CE337.6030706
Now this is an area where QR codes might be useful
First point of simplification is that we only ever need to worry about
symmetric key backup since we can always add a private key to the rest of
our encrypted archive. We can encrypt the key and backup the encryption key
or we can use a deterministic keygen algorithm and escrow the seed. either
way we only need to escrow 256 bits.
Second point is that we can reduce exposure to risk by using some sort of
key splitting scheme. We can also use this to effect key transport between
devices. The problem with 'end to end' encryption these days is that most
of us have multiple devices we receive email on, which is why WebMail has
become so attractive.
I have to be able to read my email on any of my 5 principal machines
(Windows, 2 MacBooks, iPhone, iPad). Any email scheme that does not support
all of them is useless.
Third point of simplification: ditch key rollover. Don't expire a key
unless it is necessary because of a suspected or known compromise. Use a
sufficiently strong key to make cryptanalysis infeasible.
I know that key rollover is part of the ideology but it introduces more
vulnerability than it eliminates. Any encryption key you have that ends up
compromised is likely a maximum damage situation. So using ten keys in ten
years gives the attacker ten opportunities to compromise you if you muck up
the distribution or they manage to compromise a CA.
Fourth point of simplification: Just duplicate the key into every end point
rather than attempting a key service with split control and key shares.
A better way to manage crypto in a mobile device like a phone would be to
split the prime into two (or more parts) for each mobile device to be
enabled. To decrypt data the device would have to ask a service(s) with the
other part(s) of the key to do their work and then combine with the local
So lets imagine the full key establishment sequence from the user's point
of view.
Key Generation.
To generate my first key, I tell my MUA my email address and the CA domain
name[1]. It checks to see if the email address already has a key
registered. If so it will ask if I am really, really sure I want to replace
it etc. Otherwise it generates for me a new keypair.
The CA is hopefully going to do validation of my key before issuing the
certificate. At minimum an email callback. We might push the encrypted
escrowed key out to the CA at the same time. But that is orthogonal to the
private key backup and distribution.
To backup the key we tell the device to print out the escrow data on paper.
Let us imagine that there there is a single sheet of paper which is cut
into six parts as follows:
1) Three copies of the encrypted private key, either as raw data or a link
to the raw data.
2) Three key shares allowing the key to be reconstructed from 2 of them.
For a 256 bit key that would be no more than 512 bits doing it the simple
way and there is probably a cleverer encoding.
The data for each would be presented as both a QR code (for installing in a
phone) and a BASE32 alphanumeric code (for installing on a machine without
a camera.
The user can easily escrow the keys by cutting the paper into 3 parts and
storing them in an acceptably safe location.
In my case that would probably mean mailing the shares to my parents and
family for offsite backup. Or I might give them to my broker or a bank or...
Banks are quite possibly going to be interested in helping this type of
scheme because it helps them meet their own commercial goals.
Escrowing a key share with my bank means that I may be subject to a
warranted intercept. But that is lightyears from the abuses PRISM allows
the government to commit. And in any case every device I have that can read
email has a copy.
To enable a new device I either ask one of my existing devices to print out
a new distribution paper or pull them from the safe. I can use them in a
cell phone with the camera or on a desktop by typing them in. Exposure can
be limited in this case by deleting the encrypted key rather than keeping
[1] All Key Signers are a CA by definition. All CAs must have a domain name
to support the enrollment protocol but someone can set up their own CA if
they have a domain name (even if that domain name is local.)

@_date: 2013-08-29 18:46:38
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] IPv6 and IPSEC 
So Lucky's problem seems to be that the ISPs providing IPv6 have decided on
a convention that they identify residential IPv6 ranges by not filling in
the reverse PTR info....
And the problem he has is that Google won't take email from a residential

@_date: 2013-08-29 19:00:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The Case for Formal Verification 
Whitt Diffie was meant to be working on formal methods when he came up with
public key crypto...
My D.Phil Thesis was on applying formal methods to a large, non trivial
real time system (raw input bandwidth was 6Tb/sec, the immediate
fore-runner to the LHC data acquisition scheme). My college tutor was Tony
Hoare but I was in the nuclear physics dept because they had the money to
build the machine.
The problemI saw with formal methods was that the skills required were
already at the limit of what Oxford University grad students were capable
of and building systems large enough to matter looked like it would take
tools like category theory which are even more demanding.
The code synthesis scheme I developed was an attempt to address the scaling
problem from the other end. The idea being that to build a large system you
create a very specific programming language that is targeted at precisely
that class of problems. Then you write a back end that converts the
specification into code for that very restricted domain. If you want a
formal proof you have the synthesizer generate it from the specification as
well. That approach finesses the problem of having to validate the
synthesizer (which would likely take category theory) because only the
final target code need be validated.
That is the code I re-implemented in C after leaving VeriSign and released
onto SourceForge earlier this year and the tool I used to build the JSON
Schema tool.
I would probably have released it earlier only I met this guy at CERN who
had some crazy idea about hypertext.

@_date: 2013-08-29 19:13:14
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
<521CE337.6030706
I have enough problems with mine. hallam at gmail.com, someone else registered
hallan at gmail.com.
But more generally, I want to make it easy for people to send me email. If
they already have my address then it does not matter how easy it would be
to add an encryption key, the opportunity to do so has passed.
What I did realize would be useful is some sort of verification code. So
this morning I was arranging a delivery of a screw for the shower. I give
them the email address but they were going to do hallambaker at gmail.cominstead.
So it would be nice if there was a code that someone could read back to
tell you that they got the address right. It does not need to be
particularly long, two maybe three letters. Just enough to provide a
And extending the concept. Let us imagine that I have a separate email
address that I am only going to use for online purchases and that I have
filled out a delivery address form somewhere for it and that agent will
only give out the address to a party that presents an EV certificate to
show that they are accountable and keep a record of everyone who asks.
This does not really raise particular confidentiality concerns to me
because it is simply a form of compression. My delivery addresses appear
many times in my email inbox, I have a new entry every time I buy something
online. If the mails travel through my ISP's server they will get that info
soon enough (unless the sender encrypts). But it would make filling in
online forms a lot easier and less error prone.

@_date: 2013-08-31 21:39:08
@_author: Phill 
@_subject: [Cryptography] Backup is completely separate 
<521CE337.6030706
So I was thinking about Jon's claim that keys should be 'disposable'. Not sure if I buy that. But I did decide that key backup is a completely separate problem and demands a separate infrastructure.
Let us imagine that I do the key-splitting and share in 5 places thing for my Comcast email. I probably need the same for my file system backups as well. And I probably want to be able to rely on the same in the future if I roll keys or whatever.
So the way to deal with that problem is to have a separate key escrow protocol. Probably a JSON Web Service. The protocol results in a 'key escrow identifier' which is essentially just a retrieval index on the public key. So mine might be CBK:w9idkw992ksl3. Whenever I initialize a new public key, I give the keygen system that URI and that provides the information necessary to do a backup against my escrow setup. To check that I have the right one the system comes back and says 'Daleks are bad' which is the check phrase I chose when I initialized the system.
Beneath the covers the backup scheme is simply locating a public key cert that matches the hash code I gave it, encrypting the private key under the specified public and sending the package to the email address registered in the cert. Probably want some sort of escrow agent that can be trusted to keep the encrypted bits of the private key pair and not lose them (Fort Meade would serve for that) and give them back when needed (ok, Google then).
The reason I suggest a hash rather than a domain name is that this system has to work for decades and domain names are not stable enough over those periods.

@_date: 2013-12-03 20:29:43
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
I disagree. I have looked at a lot of security usability studies and most
are utter junk. The problem is that the usability field is really about how
to sell stuff to people and focuses on the fifteen minutes or so evaluation
that a prospective buyer makes. That has little to do with long term
Test subjects are completely aware that they are in an artificial lab
setting. So they are far more accepting of errors etc. thinking that they
are accidental rather than part of the test.
I think that usability by comparison is a better approach. First take the
existing scheme that the user has and examine the number of steps taken to
do each operation and the information required to make a decision. Then
provide a secure scheme that never requires more effort than the existing
one in terms of number of mouse clicks, amount the user is expected to
remember, complexity of decisions etc.
Secure systems really do have to be that good for users to actually make
use of them.
Not that testing the end results on users wouldn't hurt. But the approach
is used as an excuse for inaction.
Every time we try to improve usability in IETF there is some idiot who will
try to TALK US OUT OF IT by saying that we shouldn't try to do anything
like that without being Pavlov first.
Demanding testing becomes another way to filibuster progress.

@_date: 2013-12-03 23:39:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
What I really want from a crypto key management device is that it be
* Small and light
* Have processor and display capabilities
* Be possible to control the operating system build completely
* Be cheap enough to be a burner machine
Which is how I started thinking about the Kindle. It is pretty much ideal
in every respect, at least after it is jailbroken.
And very unlikely that anyone has backdoored the existing stocks.

@_date: 2013-12-04 09:46:43
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
I stick stuff in a tamper evident bag. They are pretty cheap as the police
use them for evidence.
Even pro gear is not particularly tamper evident. I have had a safekeyper
apart and could probably do it again without loss of the keys now that I
know the position of the switch.
What is really desirable is to have the hardware zero itself if there is an
attempt to tamper with it. But that is not something I think is feasible
for any hardware that is not expensively purpose built.

@_date: 2013-12-04 10:40:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
I would not choose an Arduino due to the lack of a display capability. But
I have certainly been considering the Raspberry Pi which has far more
capability for essentially the same price.
But the cost of a Kindle is $69 including shipping for the device and
display combined. That is a pretty hard price point to beat. And it is a
ready to run device rather than a kit. They can be bought off the shelf in
ready to run condition from numerous retail outlets. So it is pretty easy
to pin down the potential for compromise.
And further, Amazon is a company that is very net.friendly that faces a
massive problem as a result of Snowdonia. So they might well be willing to
cooperate if not participate.
The worst case risk they face would be if they are selling the Kindle at
below cost to make up the difference by selling content. Which might not
sit well with my type of application where certified destruction of the
device is a requirement in some ceremonies.
But for your typical law firm or the like looking to secure the apex of the
enterprise trust infrastructure, a Kindle kept in a tamper-evident pouch
could well be the best compromise between convenience and security.
If I was running a ceremony for a law firm I would imagine the process
would be something like the following:
1) Show up with some number of Raspberry Pi computers that have been potted
in transparent epoxy.
2) Download and confirm the boot disk for the Pi onto an SD card.
3) Disable the WiFi function on the Kindle
4) Download the key management application onto the Kindle from the Pi
5) Generate the keys, copy the encrypted versions onto the Pi, distribute
the key shares to the client key holders.
6) Either bag up the Kindle in a tamper proof bag or perform verifiable
physical destruction.
7) Invoice the client
What would make the system easier to audit would be a special edition
Kindle that had a removable SD card instead of the built-in firmware.

@_date: 2013-12-04 11:19:26
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
The Pi has HDMI out so it can hook into an existing display so depending on
the application it is a wash. It also has the random number generator and
the operating system boots from SD card which I find more comforting than
loading up a black box via USB.
Yes, there are some applications for which this is essential. Generating EC
curves for example.
But as I showed in a previous post, there are techniques that we can use
that allow us to audit the operation of a device without performing a full
code audit.
Basically we use that NSA DUAL_EC_DRNG with a backdoor for the purpose it
was probably originally designed which is to enable the devices to be
audited by making their behavior deterministic.
So I don't dispute that we will want to use Arduino or Pi class hardware
for some purposes. But there are other options available as well.

@_date: 2013-12-04 14:58:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
Has to fit in a tamper-proof evidence bag, should be small enough to store
in a safe.
Light is really a proxy for 'easy to destroy verifiably and completely'. My
preferred method involves a transparent jig and a belt sander.
Anything with a hard drive is an utter pain.
I think the Pi is good for some purposes. But it does not come with a
display or input device.

@_date: 2013-12-05 17:53:12
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
I don't think that is what DUAL_EC_DRNG started as.
It would make perfect sense to have a mechanism that allowed the NSA to
check cryptohardware to see if the random number generator has been
bongoed. And one way to do that is to put a backdoor in it so you can dump
out the random number seed being used and check.
The point at which the spec was released was just after a leadership change
at the NSA and at a time when the military thought itself completely above
any form of accountability.
I don't think they would have done that before because the people inside
the agency saying 'this is going to be found out' would be listened to. And
I am pretty certain that there were such people because they are not
stupid. Like the numerous analysts at the CIA telling the administration
that there was no evidence of WMD in Iraq or collusion with Al Qaeda, the
experts were ignored by a bunch of arrogant showboats.

@_date: 2013-12-08 08:36:10
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
Actually a very common and real problem in SCADA systems is getting an
unwanted CPU.
For example, many parts come with the option of an Ethernet port. But this
is simply the old design with a serial to ethernet converter added. What
looks like an ethernet port is actually a full CPU with a comprehensive
TCP/IP stack including HTTP and FTP server and an unknown number of
intentional or unintentional vulnerabilities.
I don't think there is any provision for updating the firmware typically.

@_date: 2013-12-08 22:23:45
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Fun with hardware RNGS: the Infinite Noise 
What about RF emissions made by the circuit?
This contraption seems like it would bleed its output into the RF spectrum.

@_date: 2013-12-11 08:51:29
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
I am trying to get an estimate of the size of the PGP userbase for a
presentation. Does anyone have figures?
I have some figures for S/MIME and I am trying to make sense of them. When
I started looking at end to end email I was thinking that the problem was
we never hit critical mass. I am now thinking that we did hit critical mass
and the product is not wanted beyond a niche.
It does not make much difference since it is clear either way that we are
not going to get ubiquitous deployment by just waiting around for secure
email to go viral. The figures suggest that it already did as much as that
as it was going to.

@_date: 2013-12-11 16:35:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
Thanks for all the responses on and off the list.
The reason I wanted to ask is that I wanted to have a figure from a source
that the PGP community is not going to see as an attack. I am not doing
this as a competitive thing, I want to look at the ugly facts so that we
can change them.
Comodo has 155K active free S/MIME certs and the same number of paid certs.
Which is a lot more than I had expected.
Estimating like with like (current versus expired) I think we can safely
estimate a pool of people who have established keys for both specs at
roughly 3 million and a pool with currently active keys of between a half
million and a million.
My view is that both groups have hit a wall and that there is not going to
be a big uptick in deployment unless we do something to change the game.
They are big numbers that I read as indicating a big demand for email
security. But when we look at actual day to day use the results are far
more depressing. Schemes like dropbox and tumbleweed have far more use.

@_date: 2013-12-11 20:38:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
Takeup of PGP and S/MIME seems to be very much like takeup for IPSEC. There
are some big intranet deployments and possibly a few extranet deployments.
What is the gap we have to close to turn this on by default?

@_date: 2013-12-12 19:04:28
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
OK not wanting to re-iterate the conversation, just want to let folks know
that this is helpful. I want to be really sure that I have covered all the
The goal is 'Frictionless cryptography'.
The legacy S/MIME deployment is the code base to build on in my view. There
is more of it for a start and S/MIME is fully integrated into the IETF mail
infrastructure. It is the format that has received ongoing updates as the
IETF has changed other parts of mail.
So that means adding the features of the PGP trust mechanism to the S/MIME
environment. Which is not that difficult.
One aspect of the problem neither PGP nor S/MIME solves at present is
telling me when to send email encrypted by default. Like Jon I do not
enable every one of my devices for receipt of encrypted mail. Until there
is a mechanism that makes that easy sending someone an encrypted email
message is going to be an inconvenience. I have a spec for such a scheme
but it isn't implemented in the prototype yet.
So I am thinking that there is going to have to be some kind of policy
glue. If we put that between the key hash and the public key data the user
can advertise statements such as "I support the PGP and S/MIME formats" and
telling different people to use different keys. I might have my mail
configured so that encrypted mail from people I don't know goes through the
spam content filter which has one key and encrypted mail from people I do
know is encrypted under my end-to-end key.
The code is still at the plumbing stage which means I spent today writing
an SMTP proxy in C using OpenSSL.

@_date: 2013-12-12 22:19:31
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
What has changed here is Snowdonia has arrived.
This may not be the most relevant response to Snowden but it is the crypto
deployment that gives the end user the most security for the least effort.
It is also a platform we can build other message level features on.
Particularly as patents begin to expire.
I am planning to do PGP eventually to provide backwards compatibility. I
have even reserved hex keys of the relevant length for PGP fingerprints.
But I want to move beyond the plumbing. The interesting part is in the
trust model and that is where I want to be able to combine CA and peer
endorsements. They both bring different things to the table.
To make use of keys opportunistically I have to know if the recipient
prefers encrypted mail.
You can use the convention we did of keys.* to be a domain name for a
I will cite that in the longer version of the workshop paper where I am not
limited to 5 pages (!).
There are some differences between my approach and yours. But the big
difference is context. I think that we have a lot more people willing to
make the effort right now.

@_date: 2013-12-13 12:18:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
These schemes are starting to create hardware devices that remind me of
I have half a mind to build a full scale model and use it as an RNG.
Of course the original was supposed to have the ability to hack into any
computer with a Tariel cell in it. So it is actually quite close in

@_date: 2013-12-13 13:45:23
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] International Cryptography Day 
If we are going to get people to take notice of what we are doing we need
to think about outreach. Which got me thinking about International
Cryptography Day.
In one version of events this would be the day that people descend on Apple
and Microsoft stores in the shopping malls to demand security that actually
works. In an alternative version Microsoft, Apple, etc. get on board and
hold workshops on how to use their security features. Either way consumers
get the chance to demonstrate that they do care about cryptography.
It would also be a deadline by which people who are going to build
something for the show and tell they will need to have built it by then.
As far as timing goes I think we need at least six months to get ready and
get people on board which puts us in the summer vacation period. Putting
the event at the start of the vacation gives college students a chance to
show off the projects they built during the previous semester or get fired
up and spend the rest of the summer writing something.
The main date to avoid would be July 4th (for obvious reasons) and it would
have to be well before the Black Hat PR machine starts.
I would prefer it to be a weekday over a weekend because media events work
better on a weekday.
If nobody has a better suggestion, I am thinking July 18th (my birthday) or
July 11th. I am thinking of a Friday because Friday prayers is a time when
a lot of people are meeting in the populations we need to reach.

@_date: 2013-12-13 14:14:33
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
I think we should go quantum.
Take a really low intensity light source, stop it down to a single photon
and put it through a mirror with 50% transmission with detectors for both
transmission and reflection.
To remove bias from the mirror, read pairs of bits and reject them if they
are the same and take the first bit as the output bit otherwise.
There is a literature on this stuff.

@_date: 2013-12-13 15:56:47
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
Understood. Unlike the DANE WG I prefer to know the problems.
The thing that does irritate me is when I am five words into my explanation
before they come out with 'won't work, SPAM!' which is stupid at so many
levels, not least the idea that I haven't thought of spam as a problem.
One of the reasons I use gmail is that it is one of the few platforms that
can cope with my spam load. I receive more spam that gets through my
filters than most people get mail. I don't know how much is rejected
outright but when I was at VeriSign I was getting a quarter of the mail
sent to the company because of the spam.
E2E email does not prevent spam filtering, it only affects one technique
and one that is not very effective at that. Content filtering is not a good
spam reduction technique but it does kill viruses. There are several
approaches that can be used. One of them is to only accept E2E encrypted
mail from people who are known and trusted and give everyone else the key
for the spam/virus filter.
Another would be to modify the S/MIME protocol so that a mail gateway can
add in a header with 'prohibited content types' (or acceptable ones). then
modify the protocol slightly so that only clients that understand the
restriction can decrypt.
The model I am taking here is that
1) The user never has to make any more effort to perform a task securely
2) The apparent model of how the system works is a useful approximation to
the actual way it works.
So a strong email address is not actually the encryption key or even a hash
of the  encryption key. It is the hash of the signing key of the personal
master key (aka CA) which signs the encryption key. But it looks close
enough to being the thing you encrypt to for the user to think they
understand how it works.
The nuts on your car wheels don't actually hold the wheel to the car
either. They hold the rim to the hub. The hub is held on by one little nut.
The NSA is after you and me but probably not Joe Bloggs.
But the Russian mafia is after Boggs, or at least his money. And because of
that his electricity utility won't send him his bill in email, it has to go
to a web site where he keeps an account and is required to remember a
piddly username and password he uses once a year. And it is all a waste of
time security wise because the reset on the username password is his email.
I think that my strong email addresses will help corporate mail users move
a lot of their customer communications from the Web to mail. So rather than
sending a message to the user to tell them to go to the web site to find
out something the company wants to tell them, they can just tell them
straight out.

@_date: 2013-12-14 11:44:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Size of the PGP userbase? 
I don't think the use figures are a good basis for choosing formats.
The objective is to go from two separate userbases that can't communicate
without adopting the other's code to a single infrastructure. So during the
transition period PGP users will acquire the ability to send in S/MIME
format and vice versa.
If people want to make use of some of the new features I am adding for
frictionless usability then they are going to have either generate a new
key set or run the key generation tool to generate a policy file using the
old one.
One of the things I am doing to make crypto simpler is to remove
unnecessary options. I don't give the option of using separate keys for
encryption and signature, it is a requirement. I don't give people a choice
of RSA key size, it is 2048 bits.
I also require key signing and end-entity keys to be separate. So the way I
would support someone's legacy PGP key is that if it is RSA2048 then they
could generate encryption and signature keys, otherwise they could use it
to create a 'self-endorsement' assertion accrediting a newly generated key
In the transition period we will need to support both formats and any
successor trust infrastructure must support all the use cases of the old
one. The US DoD is not going to switch to a Web of Trust scheme, neither is
any enterprise user. An RA model makes best sense for those applications
just as Web o' Trust makes better sense than CA for many individual users.
But when it comes to formats, I can't see the value of keeping Blu-Ray and
HD-DVD. S/MIME is the better packaging format for a lot of reasons.
1) It is the format supported by the legacy email infrastructure.
1a) Existing mail clients can send and receive S/MIME mail without
1b) Every MTA provider of consequence has made sure that their equipment
works with S/MIME and supports use.
2) It is the format that the IETF has maintained.
PGP/MIME never got the same attention as S/MIME and has not been made to
work as well.
3) Adding the necessary support for S/MIME format to use PGP keys is much
easier than adding the necessary support for PKIX certs in PGP format.
PGP is a key centric PKI and the format makes use of this. Adding the
necessary support for PGP keys in CMS/PKCS is trivial. All that you need
to do is to put the key identifier of the recipient into the message.
4) We can use the WebPKI for authenticating corporate mail.
I think that a mail message that comes from Bank of America should come on
an impossible to forge Bank of America letterhead.
The technical infrastructure to support this already exists and so does
much of the policy infrastructure. The signature would have to be backed by
a certificate that is issued by a CA and the EV validation criteria would
apply and the holder would have to prove that they had obtained the
corresponding trademark and we would probably have some transparency
5) PGP users are more likely to move than corporate S/MIME users.
If we are going to merge formats then one group has to move. Enterprises
move much slower than real users. The only reason you can still buy a copy
of Windows XP is that there are corporate users who demand it. This despite
the fact that XP is the last of the legacy MSDOS versions of Windows and is
intrinsically insecure.
People who use PGP are using it to protect their privacy, not to follow
some ideological crusade that requires that particular message encoding.
I can't see any long term value to the legacy PGP format and I can see
plenty of problems with the authentication format. Only the parts of the
message inside the boundaries are authenticated.
The heart of PGP is the peer endorsement model. Anyone can endorse a key
for someone else, everyone can be a trust provider. The critical failure in
S/MIME is that it only supported the CA trust provider model.

@_date: 2013-12-14 14:02:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Kindle as crypto hardware 
That is rather an amusing claim given what Greenwald and I got up to:
I am the person who wrote to Boylan immediately after Greenwald published
his piece. We then compared the SMTP message header sequences to establish
that the messages had been sent from the same server.
I sent a mail to Petraeus informing him of the alleged attempt to
impersonate his communications staff, giving my credentials. Since there
was never any attempt to contact me in response it is clear that there was
never an investigation of what would have been a very serious attack. Ergo,
checking the log files on the mail server showed that Boylan was lying.
The Bush Administration asked the NSA for information on their opponents,
Juan Cole currently has a lawsuit against the CIA and NSA. But it appears
that the NSA actually leaked more information that damaged Republicans.
corrupt group of defense contractors and Republican politicians. Two of the
pols went to prison for taking bribes. But they pled guilty and so the more
lurid parts of the allegations that are detailed in the indictments never
got a proper airing. Such as the prostitutes and poker parties in the
watergate and the connection to a limo company that also somehow got a
contract to drive people between the pentagon, NSA and CIA. The second in
command at the CIA was involved in the scandal and this is what led to the
downfall of Porter-Goss, a republican politician who had been installed as
CIA director.
We don't know who leaked or why. What we do know is that Hayden was moved
sideways from the NSA to deputy DNI and then again to Director CIA and this
coincided with Alexander being put in charge at NSA and the leaks

@_date: 2013-12-17 08:12:32
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Preimage Attacks on 41-Step SHA-256 and 46-Step 
This is not particularly impressive or worrisome. The attack is on a reduce
strength version of the algorithm and the time complexity is 2^253.5 for
If this is the best that can be done, we are in good shape.

@_date: 2013-12-17 11:46:56
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The next generation secure email solution 
There are different use cases that need to be solved.
How much security we provide is going to be limited by the amount of effort
a user can or will put in.
Trust infrastructure is an obvious potential point of failure. But in the
real world none of the models that are used has a significant failure rate.
I can even pick up a key from a PGP server that does nothing to
authenticate the published keys and it is most probably the right one.
So while developing a better trust infrastructure that offers more security
and is easier to use is certainly a worthwhile goal, it is much better for
people to use any of the existing end-to-end email schemes with their
possible flaws than to send the message without encryption.
If everyone was using strong end to end encryption and we discovered that
the trust model was being exploited then we could fix the trust model much
more easily than the problem we currently face which is getting people to
use strong crypto at all.
I don't specify what the trust infrastructure is or how it works in PPE. I
have a proposal that merges OpenPGP and S/MIME but other people might have
other ideas. And even if my proposal was right for 99% of users, the
remaining 1% might have a different set of requirements.
In PPE I have three types of email address:
Mail with the to address alice at example.com will be encrypted on an
opportunistic basis. That is, it will only be sent encrypted if the trust
infrastructure returns an assertion bound to that address that says '
alice at example.com prefers email encrypted under key X'.
Mail with the to address ?alice at example.com will only be sent if it can be
sent with end-to-end encryption with a key that meets the sender's trust
Mail with the to address
AALNAT-USYLKI-WT5OKIK-QFPDQ2-PDA at alice.example.comwill only be set if
the senders trust criteria and the criteria specified
by an email sending policy signed by a key authenticated under the
specified hash are both met.
The first two approaches are easy to use but require reliance on a trust
infrastructure. The last is a direct trust model that does not require any
third party to be trusted.
We can certainly get a lot of users using the first approach because they
don't even need to know that they are using it. I think we can even get
people to use the third if it is sufficiently sugar coated.
The gap between strong email addresses and using a regular email address
plus a PGP fingerprint might seem small but so are the differences in
usability between Windows CE, PalmOS and the iPhone 1.
Combining the fingerprint into the address makes it possible to exchange a
strong email address in (almost) every venue that supports regular emails.

@_date: 2013-12-17 16:55:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Preimage Attacks on 41-Step SHA-256 and 46-Step 
The full algorithms are 64 rounds for sha256 and 80 for sha512.
These attacks only reduce the time complexity by 2.5 bits over exhaustive
brute force. So given that brute force will typically return a result at
the 50% point means we are talking about an improvement factor of 3.
If you look at the best attacks on SHA-1 to date, in and of themselves they
We started getting worried about SHA-1 when Dobbertin published the attacks
on MD5. We are a long way from having a usable attack on SHA-1 but we are
currently in the phaseout stage. SHA-1 will stop being acceptable for SSL
certificates in the near future.
If we were talking about any weakness in 64 round SHA256 then I think you
would be seeing a movement to switch away from it. A really significant
improvement against a reduced strength version of the algorithm might also
be a concern. But these are neither.
I don't consider the result unexpected. We know that the strength of SHA-1
is less than 160 bits and that SHA-2 is very close in structure and
I don't expect to use SHA-2 forever. We are getting to a point where
deployment of SHA3 alongside SHA-2 as a backup algorithm should really be
expected. But I can't see this result being significant in itself.

@_date: 2013-12-18 15:22:47
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The next generation secure email solution 
Guido's scheme is very similar to the one I am planning to build in stage 2
of my system. In fact if he wants to build his system he could use stage 1
of mine as a development platform.
But I do think it is important to acknowledge one of the lessons we learned
in spam control: the bad guys will exploit every feedback channel.
So when we are dealing with reports of violations of protocols we have to
be ready for attackers making false complaints. Many early spam blacklists
that had 'zero tolerance' policies lost credibility very quickly as people
would sign up for mailing lists for the purpose of reporting the source as
a spammer. Some of the malicious reports were intended to sabotage
political or commercial rivals. But quite a few were made to discredit the
lists themselves.
The trust problem is very easy if you have some form of ground truth to
reference. But no such ground truth exists.

@_date: 2013-12-18 19:50:11
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA Key Extraction via Low-Bandwidth Acoustic 
I would expect it to work against any crypto code that has not been
designed to avoid power or RF analysis.
Although the vector is acoustic here the acoustic signal is effectively
parasitic to the electrical signals going through the wires. So any code
that does not have code level protection against power analysis etc is
going to be vulnerable to this attack (and vice versa).
Randomizing the process so that there is no correlation between each run
seems to be the best available defense right now. But check the Kocher
patents, RAMBUS paid a fair bit for them so they are probably keen on
getting a return on their investment.
Some high end crypto devices have had acoustic shielding for quite a while.
It is not unusual to find that they are potted in some sort of expoxy gunk
inside. Nico Van Sommeren at n-Cipher was excited about acoustic as a side
channel at one point. I remember acoustic being raised as a possible vector
when Kocher published his power analysis paper in 1998 (possibly even by
Adi Shamir who was at MIT frequently while I was there).
What has changed here is that someone has found a way to exploit this
channel. We definitely need to check with the vendors to see if their
current products are vulnerable. But they should not have needed someone to
demonstrate the exploit before taking action.

@_date: 2013-12-18 21:46:14
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA Key Extraction via Low-Bandwidth Acoustic 
I was thinking about the randomization defenses.
RF shielding etc is going to be borked, yep.

@_date: 2013-12-19 12:41:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA Key Extraction via Low-Bandwidth Acoustic 
Ben Laurie said that OpenSSL should be OK provided that the blinding flag
is used.
But OpenSSL has practically no documentation on such things other than the
source. So I would not trust that without looking at the source.

@_date: 2013-12-19 17:39:31
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Some chips have microcode on microcode. The DEC Alpha even let the
operating system write instructions into the microcode (this allowed the
chip to emulate the VAX four ring security system).
It would be difficult to smuggle code to bork the RNG into the microcode.
But getting a backdoor in there that could allow the O/S to do the borking
might be rather easier. In fact that is the only way that a mass
manufactured chip could credibly bork a RNG whose design might be changed
after the chip went to fab.
It is a long time since I read an instruction set for a CPU and I suspect I
am not alone in that. Auditing a system down to the bare metal would be a
big challenge.
Do we know this?
I thought that the evidence we had was an elliptic comment in a powerpoint
slide that we have interpreted as being a smoking gun for the already
suspect DUAL_EC_NRRNG (Not Really Random Number Generator)
The code word is NOBUS 'Nobody But US'.
But I have it on authority that Snowden has changed the calculation. The
insider risk means that the risk of disclosure is now very high, possibly
as high as 1.0. That means far fewer NOBUS plans can be approved.
And might not have found out at all if the Israelis had not relaunched
STUXNET with their own payload, or at least that is one story that NSA
sources have tried to push.
Talking of which, one of the more surprising disclosures is that the NSA
handed raw intercept traffic to Israel. So does this mean that US political
organizations that are opposed to the Likud party policies have been spied
on by their own government and the intelligence passed to their political
It certainly seems that the NSA didn't consider that possibility when they
handed over the data.
It seems very likely to me that the NSA has been effectively intervening in
domestic US politics and sabotaging the efforts of the Boycott, Sanctions
and Disinvestment movement.
What other political causes are they using their powers to tip the scales?
Mexico is pretty unhappy with the lack of US gun control laws, is the NSA
sharing raw intelligence with Mexico to help fight the war on drugs? Which
other countries are in the 'swapsies' club.
During the Reagan administration the CIA handed Saddam Hussein a list of
the major opposition leaders who were promptly murdered. Handing over raw
intelligence seems to me to be a way to achieve the same effect with more
plausible deniability.

@_date: 2013-12-19 17:51:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Chinese Cryptography 
They do have an impressive research and teaching program.
But the habit of stealing technology means that it is very hard for their
native talent to find employment at home to do anything other than
reverse-engineer western designs. They are really shooting themselves in
the feet.
Good call though. Wonder if they have their own cyphers. Maybe their
Don't need to wonder, yes they do.
In fact every single WiFi router on the market today has code to implement
a bunch of Chinese ciphers. It is impossible to sell the routers in China
without the code being in the router. But the code does not have to
actually be active. So many (all?) of the routers have code of the form
if (false) { register_Chinese_crypto_suite () }
This is not as daft as it seems. One of the things that China is really
peeved about is having to pay for US patent licenses. They are just as
upset when they have to pay for a legit patent of course but the fact that
many of the patents are obviously spurious and should never have been
issued they see them as being a non tariff trade barrier.
Requiring the use of their own patented technology is a way to repatriate
some of the money to China in a way that makes it easy to ensure that a
good portion of it flows into the pockets of party chiefs or their families.
Graft is a much bigger driver of international diplomacy than most people

@_date: 2013-12-21 12:59:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
In fairness to Art et al, I very much doubt the NSA came along and said,
'here is $10 to drop a back door into BSafe'.
The deal was reported at the time, I heard it as 'NSA pays RSA $10 million
to make ECC available in BSafe'. Which was not at all surprising given that
we know RSA2048 (maybe RSA4096) is the end of the line for practical RSA.
At any rate, I very much doubt the impact was very large. Once the patents
expired there was very little reason to use it in place of the open source
But the point I want to make here is we need to avoid accusing people of
being in league with the devil when all they actually did was not ask the
right questions or enough questions.
NSA recruitment is already down by a third. I suspect their technical
recruitment is down to zero. Pre Snowden a spell at the NSA was a good
thing to have on your resume. After Snowden it is like haveing a conviction
for hacking.

@_date: 2013-12-21 22:06:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
I don't remember the source, sorry. It was quite likely verbal.
They put out this PR when the SuiteB came out.
But I remembered the deal when the first round of the DUAL_EC thing came
out. And no, it was not the certicom deal which was completely different.
The NSA was pushing ECC at the time hard. They didn't have any reason to
hide the deal.
What would be new data is confirming that the DUAL_EC RNG was the
compromise hinted at in the previous report. We had a report that the NSA
had bongoed some standard and there was consensus that the DUAL_EC was

@_date: 2013-12-22 08:52:18
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA is dead. 
The job of the NSA was to make America safe. They have not been doing that
job at all.
Over the past twenty years the industrialized world has become dependent on
the net as a critical infrastructure. Without power and water it is not
possible to live in the urban population densities we live in today.
Without the net there is no food on the shelves of the supermarkets.
Instead of eliminating the vulnerabilities in the critical infrastructure,
the NSA has worked to make them bigger and create new ones.
The civil industry can't work with the agency that is meant to be working
on the same problems. The NSA has completely destroyed the trust that was
I find it very hard to see who is going to be joining the NSA now. It used
to be that they were the only game in town if you wanted to do crypto. Then
they became a place where you would get paid rather less than in industry
but get to work with the best people and emerge with a stellar resume. Now
they are a place where you will be paid less than in the commercial crypto
world, you will be considered a pariah in your local community and your
resume will be toxic afterwards.
The NSA has become the crypto world equivalent of Fox 'News': once you work
there you can't work anywhere else in the industry.
And, don't blame me for this rationale.  The NSA must be taught that if
No the lesson is that nobody works with the NSA.
If the US government wants to do anything to protect the country against
cyber attack they are going to have to set up a civil run, civil led
organization to do the work

@_date: 2013-12-23 08:21:39
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA is dead. 
The problem is that after someone has worked for the NSA there will be
under the suspicion that they might be following orders from the agency to
put backdoors into product after they leave.
What the recent revelations really show is that the NSA is abjectly
incompetent at its real job which is making America safe. They have failed
to protect the confidentiality of US government secrets. They were pwned by
a 29 year old contractor.

@_date: 2013-12-23 08:30:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
I remember RSA getting money from the NSA to put ECC into BSafe.
It may not have been very public, but it was known in the industry. I was
working for a competitor and the natural question was 'hey can we get some
of that'. But they did put out a press release that I cited earlier.
The significant part here is that it moves the start of the DUAL_EC program
back to 2004. Which was when Hayden, not Alexander was running the show.

@_date: 2013-12-23 17:42:31
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA is dead. 
I think they miss the real risks even worse than our own generals.
Disclosure is not the risk to worry about. The problem is integrity
attacks. Specifically the fact that we are all operating pre-cryptographic
critical infrastructures.
My bank just called to tell me that my bank card spending limit is reduced
to $200 and they are sending me a card because another vendor (not target)
has been compromised. We have known how to block card present fraud
completely for two decades and Europe has had Chip and Pin for getting on
for a decade.
Cyber is not a new and exciting domain for warfare any more than terrorism
was in the 1970s. Cyber-attack is terrorism. Stuxnet was terrorism.
Cyber-attack has a low barrier to entry and is inherently unattributable.
The risk of misattribution or a false flag attack are inescapable. The
likely impact of a cyber attack is low but the fear factor is very high.
Cyber lowers the threshold for deciding to use force. This encourages the
use of force in place of diplomacy. The alternative to Stuxnet was not war,
it was to make a serious effort at finding a diplomatic solution. Olympic
games were politically expedient in that they were much cheaper politically
than the diplomatic route. But the cost is that the US has set a precedent
in which civil nuclear facilities under IAEA inspection are fair game for
cyber attack.
Every major power should have as its primary foreign policy objectives to
avoid the use of nuclear weapons or any major conflict between the great
powers. The risk of a war between the US and Russia or China is negligible.
But the risk of a war between Russia and China is quite significant. Russia
has a weak, corrupt government with a loss of empire complex. China is
emerging as a successful, confident economic power. Between the two
countries lies a half dozen failed states with corrupt, unstable
governments and large ethnic Russian and Han populations.
Breshinsky calls the area the global Balkans because of the risk of a war
in or between the states in the region leading to a Russian/Chinese war.
Scenarios in which the Russians attempt a pre-emptive attack on the Chinese
cyber infrastructure to keep them busy while they invade seem quite
plausible to me. Since 1979 Russia has had an unbroken record of backing
the losing side in every major international conflict in which they have
taken a stand.
Security is not the zero sum game Hayden and Alexander believe it to be. It
is in the UK and US interest to ensure that every country has a solid and
trustworthy critical infrastructure.
I think it is possible to go further and argue for mutual assistance in
putting diplomatic and government communications beyond attack but that is
a longer argument.
Going to typewriters is not going to help much, there were acoustic attacks
against typewriters in the 1970s. And any modern process that is based on
typewriters is also going to have a lot of photocopiers involved. And those
all have CPUs these days and many have hard drives and all are hideously

@_date: 2013-12-24 16:45:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Serious paranoia... 
Well my main machine suddenly shut down last night right after I managed to
successfully send an encrypted mail and read it at the other end.
The water cooling loop was out of coolant despite the claim of the
manufacturer that it was 'maintenance free'. So obviously a STUXNET type
In reference to the other thread on how to check code. My approach is to
use a code synthesizer and bring the code as close as possible to the spec.
If someone wants to audit the code they can read the generated code which
is very plain vanilla and exceptionally regular. They can also look at the
generator and see if it has special case handling for particular specs.
Sure someone could bongo the synthesizer, just like they could bongo the
compiler. But that would be very hard to sustain.

@_date: 2013-12-25 12:09:43
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
But that type of code review is only possible for closed source where
someone is being paid or in an exceptionally highly motivated open source
I can't slap the authors of OpenSSL and tell them to document their stuff,
let alone force a rewrite

@_date: 2013-12-26 10:12:43
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
Stability of installation packages is a lot more important than many
developers imagine.
Mosaic was not the first Web browser. It wasn't even one of the first
dozen. What set it apart was that it worked without the need for the
installer to spend hours fixing it first. Today that is reasonably common
but in 1992 it was revolutionary.
I recently tried to install the IETF tool for writing in their stupid
documentation format and found that the code would not run because it
needed another package. Python suffers from the same dll hell idiocy as
Windows used to before people started to get a clue and realize that shared
object libraries are not your friend.
If a program links to 3 packages that each have three versions then you
have 27 variations of the package to regression test. Setting up a machine
so that it could do the tests is a major undertaking that is error prone in
This is why I don't like plug-ins. Nobody is going to test the interactions
between them all. Besides which, UI code is nearly impossible to automate
testing on.
The way I avoid these issues is that I always static link to non-platform
code, always. The idea that shared object is useful on a machine with 4Gb
of memory is just stupid. And the machine would have a lot more memory if
it was not due for replacement and the motherboard won't actually accept
any more (waiting for the next Intel release).
.NET addresses the problem with strong assemblies. You can link against a
very specific version of the code.
Which is why I plan to eventually spend some time cutting OpenSSL down to
only provide the algorithms I actually use and support. The only symmetric
algorithms I plan to use are AES and 3DES so the rest all go. I don't plan
to use ECC so that goes.

@_date: 2013-12-27 13:25:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What are other related forums already setup?, 
A forum is a good format for a discussion that is not time sensitive and is
arranged by separate topics. So it is a good format for having a discussion
about a specific code project.
Mailing lists work a lot better than forums for news driven discussions.
But like many of the features of the Internet, these are essentially
historical distinctions rather than necessary ones. If we had better
clients and protocols we could support forum, comment, mailing list and
mail in a single asynchronous messaging format. We could even bridge
synchronous and asynchronous.
One of the features of my email security proposal is that it has a policy
layer where a receiver can say 'please send encrypted mail in S/MIME or PGP
formats' or whatever their receipt policy is.
A lot of people have realized that this could also be used to say 'please
use the JSON-B based messaging protocol'. Where JSON-B is simply standard
JSON with some code points added for inserting length delimited binary or
unicode blobs without the need for BASE64 or escaping mechanisms.

@_date: 2013-12-28 11:49:39
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
A while back I had an idea for a scheme I was calling the free flowing
flux. Part of the idea was that a conversation should not be artificially
restricted to a particular medium or a particular set of correspondents.
So for example, we are currently having an asynchronous discussion with the
whole list which is a public list and likely archived by the NSA (among
others). So maybe confidentiality does not matter a lot. But now imagine
that I want to make an off-list comment to three or four list members about
something on the list. That is a conversation that I probably do want to be
If I am in an important conference call with other companies and there are
three people from my company, it is quite likely we are chatting in a
separate chat room to coordinate our position. The conference call might be
public but only some of the conversation is.
At the moment we have quite a few chat like protocols, we have twitter,
sms, chat, video and VOIP. They are all essentially the same thing and we
should be able to secure them all end to end with the same set of
Let us accept for the sake of argument that my email security scheme takes
off and we get folk creating credentials and publishing them for email.
Wouldn't we want to add the same capabilities to jabber like protocols?
At some point it is going to be easier to design one protocol that supports
all the different messaging modes with security built in rather than
working out how to back-fit security into each legacy protocol separately.
Moving to a forum does not interest me very much, there are some things
that might be moved to a wiki though. What would interest me would be a
remote conference or meetup. Particularly if we could get some tools that
help moderate the discussion better.
Five years ago the idea of doing that would be madness. But we are starting
to get pieces in place with HTML5 that could make it quite feasible.

@_date: 2013-12-28 12:39:11
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
Except that the main customer base for BSafe these days is government
suppliers and what most of us assumed the point of the contract to be was
making a FIPS certified Suite B implementation available to government
It was a social engineering attack and they got pwned. I don't think it is
appropriate to allege collusion or malice to RSA or EMC.
What is rather telling is that the NSA is apparently subverting US
government crypto and nobody seems to be asking if the motivation was not
to spy on other parts of USG and if so to what end.
toppling inconvenient governments. The military command is full of Fox News
watching Tea Party types. There are a lot of US politicians who openly
mouth treason and their are military officers who refer to those comments
'in jest'.
I don't think their coup attempt will amount to anything more than the
recent Spanish farce but they are certainly talking themselves up to
something of the sort and they will probably get a lot of people killed in
the process.

@_date: 2013-12-28 12:59:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
On Sat, Dec 28, 2013 at 12:42 PM, Thierry Moreau <
I think the mistake was a little subtler.
Back in the 1990s when we were fighting the cryptowars, the pass/fail
criteria for any crypto proposal was whether it resisted Louis Freeh's
wiretap ambitions. Which given that Freeh went on to facilitate a GOP coup
d'etat impeaching a President over a blow job was pretty damn important.
As a result we failed a lot of approaches that have delivered much more
real world security than the IETF projects ever have. The IETF was pushing
end-to-end solutions and rejecting anything that fell short of that ideal.
Meanwhile we passed a lot of security protocols that were unusable in the
real world. The IPSEC standard does not support NAT to this day. Only
implementations support NAT passthrough and they do so in ways that require
a huge amount of folklore to be known in order to make an interoperable
We accepted a situation where we had two separate email specifications with
a disjoint set of features and called them both standards.
Meanwhile every power plant, water plant and chemical plant in the country
runs with no authentication on any of the sensor or control buses.

@_date: 2013-12-29 08:42:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
You don't have to, most developers don't.
My garbage collection routine is 50 lines of code that cause collection of
all child objects when a parent object is destroyed. That is adequate for
most server type applications.
But what I see as typical C application behavior is that they grab chunks
of memory and never release any.
So even though I have 4Gb of RAM on my 7 year old machine, I am having to
consider upgrading because Chrome will grab 2Gb all by itself.
Oh they do things like ensure that they never take more than 50% of the
memory on the machine but that strategy does not work when you have three
programs running.
This should not be a problem on an O/S with fine grained security. But it
is because the security mechanisms don't actually control what I want them
to control. I want to give memory quotas to particular applications. I want
to limit access to data in ways that ACLs don't support.
Butler Lampson says there should only be one copy of an ACL and everything
should link to it. I think that is part of what is needed. Security
policies should be named first class objects in the system and it should be
possible to apply them to applications
Instead of a process running under the privileges of its account owner, it
should run under privileges specified by a named policy and the account
owner should have a choice between a manageably small number of policies.
So for most of my machines I would only have the 'game' and 'document'
policies in use. A game does not need to connect to any data it did not
create itself, it does not need access to my documents in particular. The
document policy would be for office, openoffice, etc. On some machines I
would have code running under the developer policy.
My experience is that code written in a managed language tends to be a lot
better behaved than code written in C. Which is one of the reasons I use C
It might be possible for a first class programmer to do better themselves.
But most code is written by third or fourth class programmers and the
machine can code much better than they can.
C++ has the wonderfully powerful template system.  Unfortunately, the
C# has generic types and lambda expressions these days. The implementation
is a lot more stable than my experience of C++ (which was admittedly a
decade ago)
Microsoft acquired much of the old Digital engineering staff and they
brought all the tools that had their origin in Genera, the LISP Machine

@_date: 2013-12-30 09:59:21
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
A while back Bruce told me that the Snowden docs show NSA uses every attack
Here is the latest installment. The phrase that comes to mind is 'have they
no decency?'

@_date: 2013-12-30 19:17:23
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
Sent from my iPad
The technology is somewhat meh afaik. It's the cool code names that we discover with each dump that I live for.

@_date: 2013-12-30 23:12:56
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Krugman has some examples of deflation observed in very similar systems,
babysitting vouchers is one example.
As with the dotcom bubble it is easy to see that it will pop but impossible
to know when it will pop and so there is no way to make money from the
BitCoin is currently indistinguishable from a penny stock in a company that
has no revenues, no assets or expectations of any in the future. The only
reason to hold bitcoin in preference to another currency are the belief
that Bitcoin will appreciate relative to other currencies or the perception
that transactions are effectively anonymous.
The second belief is essential to support the first. If the belief that
Bitcoin is anonymous is ever seriously challenged then the drug dealers
will dump their coins as quickly as possible and the value of Bitcoin will
drop as fast as cowrie shells did back in the day
Depending on the legal theory the FBI wants to concoct, shutting down
BItCoin could be as simple as asserting that everyone who participates in
BitCoin in any way is a co-conspirator with the cryptolocker/silk road/etc.
organized crime gangs and thus subject to civil forfeiture and criminal
prosecution under RICO. Sure, the theory might well stink but after the US
courts have spent ten years desperately looking the other way to avoid
recognizing torture, murder and imprisonment without trial, anyone who want
to rely on the US courts protecting their civil rights is suffering from
sillier delusions than their belief in the inevitability of BitCoin.
As with Sealand and the rest of libertopia, it all looks so much better
when you can pretend that the authorities are taking no action because they
can't or are too clueless to make a move rather than because they might
have some tactical reason not to.

@_date: 2013-07-04 14:47:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Snowden "fabricated digital keys" to get access 
I read an article today that claims one and a half million people have a
Top Secret clearance.
That kind of demonstrates how little Top Secret now means.

@_date: 2013-06-29 17:19:45
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Snowden "fabricated digital keys" to get access 
I think that fabricating a key here is more likely to mean fabricating an
authentication 'key' rather than an encryption key. Alexander is talking to
Congress and is deliberately being less than precise.
So I would think in terms of application level vulnerabilities in Web based
document servers.
One of the things that I have thought weak in our current approach to use
of crypto is the way that we divide up access control into authentication
and authorization. So basically if Bradley had a possible need to see a
file then he has an authorization letting him see it. Using access control
alone encourages permissions to be given out promiscuously.
The Snowden situation sounds like something slightly different. Alexander
says he was not authorized but he was able to get access. The common way
that happens on the Web is that Alice has account number 1234 and
authenticates herself to the server and gets back a URI ending something
like ?acct=1234&.... To get access to Bob's account she simply changes that
to ?acct=1235&...
It should not work, but it works very often in the real world. Having
worked with contractors I have seen people hired out as 'programers' at
$1500 per day whose only coding experience was hacking Dephi databases. No
C, C++, Java or C Not even a scripting language.
So it would not shock me to find out that their document security comes
undone in the same way that it does in commercial systems.
Heads should be rolling on this one. But they won't.

@_date: 2013-11-02 12:28:13
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PGP Key Signing parties (Trust Link Grid) 
I think that is an unhelpful way to approach the problem. Peer endorsements
and CA endorsements have different effects. Limiting the design to one or
the other is unnecessary.
We don't need to limit ourselves to one approach. A pure peer endorsement
scheme has the problem that none of the links are grounded. I can generate
a large web of trust with 10,000 users in a few hours on one PC.
A pure CA endorsement scheme has the problem that the CA has no personal
knowledge of the subject and can only attest to a process, usually limited
to checking government issued documents.
Combine the two and you can create a mechanism that has a higher work
factor for an attacker than either model on its own. And it then becomes
possible to trust keys from the other side of the world or people that you
have never met.

@_date: 2013-11-03 11:40:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Please when we are having this discussion, distinguish the two cases:
1) Generating public keypairs
2) Generating session keys
The security concerns for the two cases are completely different yet we
have seen the issue of ssh keys being used.
There is no excuse for not generating public key pairs on a machine that is
completely trusted and trustworthy and has a strong random seed and
effective means of capturing additional random input.
For session keys, I suggest that any device that is not capable of
generating a good public key pair should not be relying on its own random
seed either. So for that I would suggest that whatever process provisions
the public key or shared session key to the device also provision a random
seed to it.
When generating random numbers the device should always use multiple
sources and compliment the randomness from the random seed with other
sources. So the final random seed would be something like
R = R_1 XOR R_2 XOR R_3
R_1 = randomness captured from environment
R_2 = randomness from seed embedded by manufacturer
R_3 = randomness from seed provided during provisioning.
Devices that can't generated good random keys are almost always going to be
devices that are slave to some other machine. So lets not get hung up about
how to generate good random seeds in my toaster or kettle or fridge. They
are only going to be on the net at all because I have provisioned them into
my network and granted them an access priv. I can easily provision in a
backup random seed at the same time.

@_date: 2013-11-05 15:26:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Bitcoin attack 
This will give the NSA something to do with all that crypto hardware they
bought with taxpayer money that is going to be rendered useless by
deployment of pervasive crypto. They can mine the bitcoin pool to
exhaustion and shut the system down or they could attempt to preserve the
viability of the system by limiting their extraction rate to a 'tax'.

@_date: 2013-11-08 10:16:13
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] How Snowden broke the NSA, how to break them again. 
Instructive reading:
It occurs to me that Snowden could have done the same thing armed with just
a smartphone.
1) He loads a custom app onto the phone that monitors the microphone in the
background and performs acoustic keystroke snooping.
2) Goes over to the person and asks them to type a bunch of text while he
is close, at some point they are asked to enter their password while he
looks the other way.
3) Profit!

@_date: 2013-11-12 15:18:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Secure random mixing 
Along the way someone suggested
R1(t) | R2(t) | R3(t)
As a randomness function that prevents bias. But actually it does not since
it allows any of the parties to control the output provided that they are
the last to vote. This is a better approach:
H(R1(t) + R2(t) + R3(t))
There is always going to be a byzantine generals issue here. What you need
to avoid that is to have the parties commit to a value before the other
parties reveal their value.
So a better protocol would be to construct the beacon as a Lamport hash
chain and reveal the previous value on each tick. We can now eliminate the
possibility of collusion as the beacon is committed to its value before the
other parties.
The only way to cause a default is if there is a three way collusion and
the beacon provider reveals the source value in advance.
This does sound to me like a function that NIST and its equivalents in
other countries could usefully provide. It would be a 'starter function'
for a state cryptographic bureau (along with a signed time function).
I would imagine the following functions to be potentially useful:
1) A year chain with a time increment of minutes
2) A ten year chain with a time increment of hours
3) A hundred year chain with a time increment of days
A new chain would be started each month for 1, each year for 2 and each
decade for 3 so that there is overlap.
This is a pretty modest outlay. The biggest CP commitment is for chain 1
which is only half a million operations.
The providers could sign all the overlapping chains at the same time as
there are at most 32*8 bytes of chain data.
(Thinking about my trust model, chain 2 is probably redundant)
Governments could have fun promoting their domestic provider of trusted
And I am sure someone has thought of this before, possibly Lamport himself.

@_date: 2013-11-14 20:04:20
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
How does CT prevent coding errors in browsers? in Adobe Flash?
How does CT prevent network managers losing their keys or exporting the
private component and sending it to someone as an attachment?
How does CT shut down a party that legitimately obtains a certificate and
then acts maliciously?
There are many issues with the Web PKI. The biggest one is actually the
fact that most of the browsers make reducing connection latency a higher
priority than processing certificate revocation properly.

@_date: 2013-11-18 20:08:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
That is what I think the big idea in CT is, the principle of Transparency.
 I am not that much into the details of the protocol itself, its the
transparency that maters.
I would like to see transparency in crypto hardware too. There was a side
meeting on this in Vancouver. But it is a very hard problem.
Yes we can take a Raspberry Pi and run Linux on it from a distribution with
a known fingerprint. But that still leaves us with a half million lines of
code to wade through.
While I was thinking about the problem it suddenly hit me that the NIST/NSA
backdoor DRNG might have been originally designed to meet this problem.
Let us imagine that the NSA wants to check to see if a piece of crypto
hardware has a backdoor inserted by Belgium. They take samples of the
device, they generate a bunch of random numbers, use their backdoor to pull
the seed out and at this point the device is completely deterministic. They
can audit it. If the device passes they throw the samples in the grinder
and approve the device for government use. Otherwise they give it a NIST
certification but mark it as 'unacceptable' in some registry kept in the
bowels of Fort Meade.
It makes perfect sense that the NSA would have such a program. But it only
really works for them well if they keep the fact that it exists secret.
Otherwise an attacker might play some game where they only defect sometimes
or for some particular domains.
Now imagine that we take the same idea, a DRNG with a backdoor and we use
it to create an auditable crypto module. We can run the device on the bench
with curves and points that we know aren't jiggered and see if it gives the
same output as our reference implementations.
Then for production use we use a set of curves, points, whatever we
generate in a controlled manner after the hardware was validated, on
machines that we physically destroy (belt sander plus thermite) afterward.
The backdoor is sealed shut.
Open source is a good foundation but it only enables open review and there
is going to be too much code to review fully for quite a while and it does
not extend to the machine code running on the hardware. There are many
points of vulnerability that are not covered. Being able to test the whole
crypto system is very useful.

@_date: 2013-11-22 11:30:08
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Does anyone know what the Dark Mail Alliance specs might look like?
I have been trying to contact the principals with no success.
Its no secret that I am working on a secure email scheme based on PGP and
S/MIME and have proposed a series of drafts to the IETF. It would be nice
if this time round we could end up with one specification rather than two.
The rest of this message is a heads up for folk who are interested in
working on this problem that there are code projects they might consider.
Given all the crap I have done with government agencies over the past two
decades I don't think people would be well advised to place absolute
reliance on my code even if it is open source. I also don't want to rely on
just one crypto stack.
The good news for folk wanting to do that is that since I work in C
anyone else who wants to write an implementation of one or both of the
tools has a free choice of any of the crypto libraries in C or even
The scheme I am working on allows people to exchange secure email using 99%
of the already deployed MUAs without any updates, plug ins or patches. The
basic idea is to decouple the problem of trust management from the question
of message formats.
As far as message formats go, S/MIME has completely won the battle.
Virtually every mail client that supports secure mail supports S/MIME and
virtually everyone who uses an MUA rather than WebMail uses one that has
S/MIME built in.
On the trust side I think that people have been thinking about the problem
in a very unhelpful way. PGP's Web of trust is better for some groups of
people and S/MIME's CA managed trust is better for other groups of people.
If I am sending mail on Comodo business people are going to be asking 'is
this mail from Comodo' at least as often as 'is this mail from PHB'.
Why did we get the idea that there was no room for both approaches? Why not
have a scheme that allows people to make use of any trust model they find
works for them?
I see a need for at least three distinct trust exchange models:
1) Direct out of band trust.
2) Peer-to-Peer Endorsement
3) CA managed certificates
Plus (0) Self endorsement of short term keys from longer term keys.
The way I support direct out of band trust is through a 'Strong email
address' which is essentially just a fingerprint prepended to a
To enable backward compatibility with PGP there is a convention that if the
KeyID has a : separator in it then it is a PGP fingerprint in hex. So
people will be able to use the toolset to send mail to PGP users in PGP
The first tool is almost working now and generates a keypair for the user:
Generating Key
Writing to file Alice.keyfile
Key Identifier is ADAEXA-G4UKAN-UADASXA-JQAGBS-XAA
Strong email address is ADAEXA-G4UKAN-UADASXA-JQAGBS-XAA?
alice at hallambaker.com
Right now I only generate one keypair for encryption use only. But the long
term plan is to generate a master keypair that has no predefined expiry
time and then use that to sign temporary keys for signing and encryption.
The second tool is a simple mail proxy which I am working on. The idea is
that you redirect your outbound mail through the proxy. The idea is that
this will perform enhancement of mail as follows:
1) If the email address contains a ?, the mail will not be sent unless it
can be sent under a security policy acceptable to the sender. This
typically means end to end encryption using S/MIME or PGP.
2) If the email address contains a ? and has a fingerprint, the mail will
only be sent if it can ALSO be encrypted under a policy that has been
signed under a key that is accredited under the specified public key.
3) Otherwise the proxy will attempt to locate a sending policy for the
domain from a policy service. If the receiver has specified a receiving
policy with a preference for end to end encryption, the message is
encrypted opportunistically.
Based on my calculations and simulations, I believe that the design using
only direct out of band trust can scale to an arbitrary number of users but
it only provides reliable trust when there is an opportunity for direct
Which is a lot better than where we are now but not where I want to stop.
But that is the fun part of the project where there is room for lots of
ideas and innovation. Right now we have to fix the plumbing.
As far as wireline formats go my approach is:
1) All the crypto in ASN.1 (yuk!)
2) Everything else in JSON or simple extensions thereof (e.g. JSON-ABC)
3) All Web services over HTTPS with client authentication at the HTTP layer.
A bare bones scheme does not require any web service at all. The public key
and policy files can be fetched from a HTTP URL places at a .well-known
The next step up requires two very simple Web services to support key
publication to the trust infrastructure(s) and key discovery/validation.
Allowing people to use multiple devices requires another two Web services,
an account establishment/management service and the confirmation service I
proposed earlier.
Once we get beyond that into trust infrastructures it will get a lot more
complicated as what we are doing is essentially research. But I don't think
we will need to be there for 18 months even by a very optimistic adoption

@_date: 2013-11-23 16:24:08
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Dark Mail Alliance specs? 
The last thing we need now is a second VHS vs Betamax standards war on
secure mail formats.
But, the format is locked into the MUA.  So it has no bearing to anyone, as
Only the message formats are baked in. The trust model does not need to be
managed in the MUA
I could care less what the format of the messages is. The S/MIME folk have
done a very good job of working out how to get their messages through the
SMTP infrastructure. Why duplicate that.
I'm obviously missing something here -- what can be done with S/MIME that
No user interface is needed to receive encrypted S/MIME messages. All that
is required is that the MUA is configured so it can find the decryption
key. That does not need to be done by the MUA, another program can
configure that. Alternatively a one time effort to get the cert and key
into the MUA.
No user interface is needed to send encrypted messages. That can be done by
redirecting outbound mail through a proxy.
To force encryption of an outbound mail, put a ? in front of the mail
address. Let the proxy work out how to send it encrypted.
I don't have quite that model. But I do have the idea of combining peer
endorsements and CA endorsements. So the idea is that if I get my key
endorsed by a CA, that endorsement is fixed in tie and has a certain work
factor associated with it. If I get my key endorsed by a peer, that also
has a work factor.
The problem with the peer only model is that it is easy to generate a web
of a million keys all purportedly endorsing each other. The trust needs to
be grounded. Sprinkling in some CA endorsements into a Web grounds the Web.
So in the CA only model the work factor for a distant key is $100 (say)
That is the cost of forging the documents.
In the Web of trust only model, the work factor for the same key would be
$0 because the pat is a friend of a friend of a friend ^n.
In the Web of trust plus 100 CA endorsements model, the cost is $10,000/p
where p is some factor to represent fading due to the distance from the
endorsements. I think p can be kept to a low number.
I think it is viable to establish a Web of trust with some millions of CA
endorsements in it. Which brings the work factor into the hundreds of
millions plus a high risk of being caught.
But in terms of the marketplace and the way users think, what they want is
Actually I am not that bothered about who the CAs are as the CA
endorsements would be fixed in time. The attacker does not have a TARDIS to
go back in time and forge a key.
We can build a timestamp notary that does not need to be trustworthy.
I wouldn't suggest using that old stuff.  Pick something well
Then I lose the chance to use that deployed code.
Plus I have an ASN.1 compiler and a JSON compiler that can target the
language(s) of my choice.
I would rather stick to JSON and add in a length delimited binary chunk

@_date: 2013-11-25 10:15:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Email is unsecurable 
The problem isn't the certificates, it is leaving the management to end
S/MIME was designed for enterprise use based on experience of the military
email system. And as we know from Snowdonia, it did not even work there.
A while back Glen Greenwald and I caught Petraeus' PR guy, a Col. Boylan
lying about having sent Greenwald an email. What happened was Boylan sent
Greenwald an email which he promptly published, Boylan is a public official
it is a public document. I then wrote to Boylan pointing out that if he was
in the UK army rather than the US he would be court martialled for writing
a letter like that. Boylan then wrote to me denying having sent the
message, a lie quickly exposed by examining the sequence numbers in the
The point here being that accountability is also important. If someone had
been impersonating Boylan in that message it should have been a major
concern. There should have been an enquiry because it is rather significant
when the enemy might be injecting false communications as Boylan alleged.
If S/MIME worked right then all the messages would have been signed and
Boylan would have known that he was always accountable for what he wrote.
But that said, the idea of authenticating the sender (i.e. the US military)
is a good one. But the execution is pretty much healthcare.gov. It is a
boondoggle for contractors who are never held accountable for their stuff
not really working right.
And the other part of the problem was that PGP does not meet the trust
needs of the enterprises.
There just wasn't the perceived market demand to fix the system.
Hence, I've concluded that email is unsecurable.  Obviously Jon and PHB and
I really think that Snowdonia has opened an opportunity here. I have a
three phase plan
1) Use direct trust exchange (strong email addresses ?
alice at example.com) to allow people to exchange secure mail with people they
know using existing, unmodified mail clients without plug-ins and without
2) Build out infrastructure to support other trust models including CA
trust and peer-endorsements and trust discovery. Can now send email to
people without knowing their trust credentials in advance.
3) Transition to mail clients with native support that also support a new
messaging protocol that cleans up the insanities of SMTP and supports other
modes (dropbox, chat, voice, video) and always goes over TLS.
We won't get to protect meta data properly till we get to step 3 but each
step is manageable, has significant demand and builds out the
infrastructure for the next step.
The technical hook is designed into step 1. The mechanism requires a
mapping of the fingerprint to a key that is to be used to encrypt the email
under. That mapping can be to a permanent signing key that signs limited
lifespan encryption keys. The same mechanism can produce other signed
statements such as to redirect the mail to another address or to use a
particular format (PGP, SMIME) or a completely new protocol.

@_date: 2013-11-26 08:08:19
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
I think everyone is barking up the wrong tree here.
How do you explain how the car works to gran? You don't because she never
thought to ask. All she knows is that you stick liquid in one end and it
burns to make the engine go. Gran probably thinks that it is some sort of
steam engine in there.
These explanations are tantamount to explaining to her how a carburetor
works or the details of the differential.
Thats why I am hiding the details of my scheme:
gran at example.com
"It tries to send the message encrypted."
G: "What if I want to be sure?"
?gran at example.com
"It won't go unless it can be encrypted."
G: "How does it know how to do that."
"It asks the broker how to do it."
G: "I don't trust my broker, that stock he sold me was a dud"
ADAEXA-G4UKAN-UADASXA-JQAGBS-XAA?gran at example.com
"This tells the system how to check the instructions."
G: "Looks complicated."
"You just cut and paste."
G: "How do I do that again?"

@_date: 2013-11-26 13:00:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Lawyer: "Are you familiar with public key 
On Tue, Nov 26, 2013 at 1:49 AM, Viktor Dukhovni
I did rather worry when I saw the Plaintiff's counsel's line of
questioning. It is the sort of thing that can be really damaging.
I have been in the same situation (only over the invention of Web Mail
which is rather less consequential than Public Key Crypto). And the result
was I ended up as a paid witness for that very specific part of the case
rather than an expert witness explaining email technology in general.

@_date: 2013-11-26 14:52:26
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
Unless they don't.
But that is OK, no really. What we are trying to do for gran and what we
are trying to do for a political dissident is completely different.
Different consequences, different risks.
It is OK for a solution we provide to Gran to sometimes fail.It is better
to give gran a solution that we can trust her to always use that might be
compromised with a lot of effort than something that could be more secure
but won't be because she will never use it.
Jon and I are working at opposite ends of the spectrum and that is OK.
What I learned from Tim B-L vs Ted Nelson is that Ted Nelson invented 120%
of the Web while Tim only invented 90%. But Tim is the guy who made the
difference because deciding to leave the parts out is what made the Web
possible. Tim's genius was to work out the right parts to leave out.
The sad part about Ted vs Tim is not that Tim worked out how to deploy
Ted's life's work, its the fact that Ted can never accept the fact or
understand what Tim did for his work.
Before Snowdonia hit I was working on a History of the Web. One of the
patterns that pops up a lot of times is the visionary versus the
pragmatist. RMS was the visionary behind GNU but it took Eric Raymond and a
lot of other people to create the Open Source movement and kick out the
parts of the ideology that were broken.
It comes up before as well. Everyone here likely knows that Edison did not
invent the light bulb. Even his improved light bulb was an exaggeration of
his contribution there. But Edison really did invent electric light because
he invented the first complete infrastructure that could deliver it to the
house. Lots of people had the idea of electricity into light but Edison
made it practical.
One challenge we face today is to make Phil Zimmerman's vision of 1991
practical. We can work on that while Phil and Jon continue to work on
pushing forward the frontiers.
This gets me to the key insight I came to about the Web, 1+1 = 6 billion
Tim plus Ted was more productive than either could have been alone. There
are different styles of thought and very often the reason that a problem
has been unsolved for too long is that it needs a different style of
thought. Ted is a very powerful, very charismatic speaker and he managed to
pull several hundred people into his style of thought. Before that Freud,
Yung, Marx and many others did the same sort of thing.
The Web changes that. It brings together all the styles of thought to work
on a problem whether the principals want that or no.

@_date: 2013-11-27 12:10:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Microsoft announces new email encryption 
That is not how I would see it. I think this is more the result of
designing the security down to meet a set of usability goals that say the
user must not take any effort ever. So they decide to deny the user any
ability to know what is happening.
To make a car analogy, if the typical usability crew were given the task of
designing a car they would remove all the gauges from the dashboard and
replace them with a soft red light to tell you everything is OK. So you
would have to drive without fuel, oil or water temperatures, no tachometer,
no speedo either. Just a big red light that is 'easy to use'.
And its not just Mr Softy that does that. Apple is just a guilty and some
of the Linux Distos are going the same way.

@_date: 2013-11-27 14:35:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Microsoft announces new email encryption 
Is this the result of that Data Layer security some Microsoft people have
been pushing in IETF as PLASMA?
That was some pretty high grade security going on. But the problem is one
of usability. I would certainly NOT describe PLASMA as epic fail security
wise. But making it work securely would likely require a lot of
infrastructure and I am not sure all of that really exists yet.
The three most important security lessons from Snowdonia are:
1) The insider threat will get anyone.
2) Data level security is essential
3) Usability is critical, security people won't use is useless.

@_date: 2013-11-27 16:39:19
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
All the user needs to know is how to configure their email on a different
machine. If it takes more than giving the machine the address of the
account and authorizing the new machine to connect to it then it has failed.
that they must *not* give that key to anyone else.
No! No!
Make the scheme so that Grandma can't give her key to someone else without
a great deal of effort.
Imagine Granny has a little box next to her computer that does all
I think you are talking about the scheme that people like us might use.
Unless Grandma is running a revolutionary cell, I don't think we need to go
quite that far.
I certainly agree that for a particular class of user we want to lock it
down that well. But not for Grandma.
The NSA can't compromise every endpoint without being noticed. The more
times they get noticed, the more likelihood of an investigation eventually
taking place. And if that ever happens there is no knowing where it might
end up.

@_date: 2013-11-28 13:50:11
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
The difference is that the machine might not have the key ever, merely the
ability to decrypt.
I got beaten up by MEZ for saying similar stuff about padlocks and green
bars when we did the UI work in the W3C, it really does make a difference.
We have to think in terms of user objectives, not mechanism.
For example, I have an iPhone, how much can I trust it? Well only to a
degree because I could easily lose it or it could get taken off me at an
airport by the type of government that does this.
So maybe it is better to not put the decryption key on the end point device
at all. Maybe it is safer in the cloud. Or maybe we use some sort of key
splitting scheme.
Going straight to a mechanism forecloses options that might turn out to be
I can walk instead of ride in a car but I can't do RSA at more than 512
bits in my head and I am not sure if I can do 512 any more.
So there is going to be a machine involved or machines even but Alice is
not a machine. The trust end points are people and organizations but the
technical implementation is machines.

@_date: 2013-11-30 10:19:01
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Explaining PK to grandma 
That is a bit harder but not insurmountable provided that they are
sufficiently S/MIME aware to present the encrypted message as an attachment
so that the browser can pop up a reader for an encrypted message.
It will take work on the part of Webmail providers to make this possible.
But it is rather more possible than most people imagine.
I 'invented' WebMail back in 1993, to the extent that it needed to be
invented. It was one of my test applications for HTTP forms and it was how
I discovered that POST was broken. So I would really like to support
WebMail to the greatest extent we can.
End-to-end secure WebMail requires extensions to WebCrypto plus a suitable
editor widget. But it is quite possible.
You are showing your age by mentioning Facebook. The young people are
already walking away from it as uncool. It is uncool because Grandpa uses
Facebook is a band, not a label. The Rolling Stones were big once but they
were not the last band in history.

@_date: 2013-10-02 09:09:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] encoding formats should not be committee'ized 
Replying to James and John.
Yes, the early ARPANET protocols are much better than many that are in
binary formats. But the point where data encoding becomes an issue is where
you have nested structures. SMTP does not have nested structures or need
them. A lot of application protocols do.
I have seen a lot of alternatives to X.509 that don't use ASN.1 and are
better for it. But they all use nesting. And to get back on topic, the main
motive for adding binary to JSON is to support signed blobs and encrypted
blobs. Text encodings are easy to read but very difficult to specify
boundaries in without ambiguity.
Responding to James,
No, the reason for baring multiple inheritance is not that it is too
clever, it is that studies have shown that code using multiple inheritance
is much harder for other people to understand than code using single
The original reason multiple inheritance was added to C was to support
collections. So if you had a class A and a subclass B and wanted to have a
list of B then the way you would do it in the early versions of C++ was to
inherit from the 'list' class.
I think that approach is completely stupid, broken and wrong. It should be
possible for people to make lists or sets or bags of any class without the
author of the class providing support. Which is why C# has functional
types, List.
Not incidentally, C also has functional types (or at least the ability to
implement same easily). Which is why as a post doc, having studied program
language design (Tony Hoare was my college tutor), having written a thesis
on program language design, I came to the conclusion that C was a better
language base than C++ back in the early 1990s.
I can read C++ but it takes me far longer to work out how to do something
in C++ than to actually do it in C. So I can't see where C++ is helping. It
is reducing, not improving my productivity. I know that some features of
the language have been extended/fixed since but it is far too late.
At this point it is clear that C++ is a dead end and the future of
programming languages will be based on Java, C# (and to a lesser extent
Objective C) approaches. Direct multiple inheritance will go and be
replaced by interfaces. Though with functional types, use of interfaces is
very rarely necessary.
So no, I don't equate prohibiting multiple direct inheritance with 'too
clever code'. There are good reasons to avoid multiple inheritance, both
for code maintenance and to enable the code base to be ported to more
modern languages in the future.

@_date: 2013-10-03 19:33:12
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] encoding formats should not be committee'ised 
XML was not intended to be easy to read, it was designed to be less painful
to work with than SGML, that is all.
There are actually good reasons why a document markup format needs to have
more features than a protocol data encoding format. People tend to edit
documents and need continuous syntax checks for a start.
XML is actually a good document format and a lousy RPC encoding. Although
that is exactly what SOAP is designed to turn XML into. The design of WSDL
and SOAP is entirely due to the need to impedance match COM to HTTP.
What does work in my experience is to design a language that is highly
targeted at a particular problem set. Like building FSRs or LR(1) parsers
or encoding X.509 certificates (this week's work).
And no, an ASN1 compiler is not a particularly useful tool for encoding
X.509v3 certs as it turns out.

@_date: 2013-10-04 09:57:39
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
The case is described in Why Buildings Fall Down.
The original design was sound structurally but could not be built as it
would have required the entire length of the connection rod to be threaded.
There was no way to connect one structure to the other.
The modified design could be built but had a subtle flaw: the upper skyway
was now holding the entire weight of both The strength of the joint was
unaffected by the change but the load on the joint doubled.
We see very similar effects in cryptographic systems. But the main problem
is that our analysis apparatus focuses on the part of the problem we know
how to analyze rather than the part of the problem that fails most often.
Compare the treatment of coding errors in cryptographic software and the
treatment of CA mis-issue. Coding errors are much more likely to impact the
end user and much more likely to occur. But those get a free pass. Nobody
has ever suggested that the bugs in Sendmail in the early 1990s should have
stopped people using the product (OK apart from me). But seven mis-issued
certificates and there is a pitchfork wielding mob outside my house.
The fact that the Iranian Revolutionary Guard has a web site filled with
hijacked software that is larded up with backdoors completely missed the
attention of most of the people worrying about the seven certificates, all
of which were revoked within minutes and would be rejected by any browser
that implemented revocation checking like they should. But much easier to
flame on about the evils of CAs than ask why the browser providers prefer
shaving a few milliseconds off the latency of their browser response than
making their customers secure.
Oh and it seems that someone has murdered the head of the IRG cyber effort.
I condemn it without qualification. There are many people who have a vested
interest in keeping wars and confrontations going. There are many beltway
contractors who stand to make a lot of money if they can persuade the US
people to fund a fourth branch of the military to fight cyber wars and fund
it as lavishly as they have foolishly funded the existing three.
A trillion dollars a year spent on bombs bullets and death is no cause for
pride. Nobody should ever carry a gun or wear a military uniform with
anything other than shame for the fact that our inability to solve our
political issues without threat of violence makes it necessary. We do not
need to spend hundreds of billions more on a new form of warfare. But there
are many who would get a lot richer if we did.
As Eisenhower observed, spending too much on the military makes the country
less safe. If politicians believe their war machine is invincible, some
stupid fool is going to use it just because they can. Just like the last
President did. At the end of the cold war when the Soviet Union was on its
knees, so was Margaret Thatcher, begging Gorbachev to send the tanks into
East Berlin and stop the collapse of the enemy that her world was built in
opposition to. And Thatcher claimed to be speaking for the other Western
leaders as well. I have the transcript of the meeting if anyone is
While most of the information on the Comodo attack is in the public domain
there is some that was with-held. The reason was not to protect Comodo but
to protect the attacker in the unlikely event that they were actually
telling the truth and they were acting outside government direction. The
chance is very small but if they were acting on their own initiative and
had diverted the entire Iranian Internet they would risk a long prison
sentence, possibly a capital sentence if they were caught. I am not going
to provide the Iranian authorities with information that could assist them
in that even if the guy had attacked us.
One of the more ridiculous spectacles resulting from PRISM is the parade of
establishment worthies telling us that we don't need to be worried about
the government intercepts and we should not worry our silly heads about
matters that are too complex to understand. Well I knew quite a few members
of the British cabinet when they were up at Oxford, I have known
politicians all my life, my cousin was a cabinet member, I have met world
leaders and acknowledged leading foreign policy experts. That experience
gives me absolutely no confidence in the establishment worthies.

@_date: 2013-10-04 10:10:48
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I don't understand what you mean there. The actual history of SSL was that
SSL 1.0 was so bad that Alan Schiffman and myself broke it in ten minutes
when Marc Andressen presented it at the MIT meeting.
SSL 2.0 was a little better but none of the people who worked on it had any
formal background in security. During the design process Netscape finally
got a clue and hired some real security specialists but one of Andresen's
hiring criteria was not to hire anyone from CERN who might suggest that the
Web had been invented by Tim Berners-Lee rather than himself so it took
them a lot longer than it needed to.
During that time I told them about their random number generator design
being barfed and they told me they would fix it but they didn't.
SSL 3.0 was designed by Paul Kocher as we all know and he did a pretty good
job. But they only gave him two weeks to work on it.
I don't think Paul's design was very theoretical and Netscape didn't give
him anywhere near enough time to do a full formal analysis of the protocol,
even were that possible with the tools available at the time.
It is far better to select a target such as 128 bit security, and then
I don't like that approach to hash function design.
Yes, I know that the strength of a 256 bit hash against a birthday attack
is 2^128 but that is irrelevant to me as a protocol designer as there are
almost no circumstances where a birthday attack results in a major
compromise of my system.
Dobertin demonstrated a birthday attack on MD5 back in 1995 but it had no
impact on the security of certificates issued using MD5 until the attack
was dramatically improved and the second pre-image attack became feasible.
So I would rather that SHA3-256 provide a full 2^256 computational work
factor against pre-image attacks even if there is a birthday vulnerability.
Proofs are good for getting tenure. They produce papers that are very

@_date: 2013-10-04 13:06:52
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Well if we are going to get picky, yes it was a collision attack but the
paper he circulated in 1995 went beyond a collision from a known IV, he had
two messages that resulted in the same output when fed a version of MD5
where one of the constants had been modified in one bit position.
I find the preimage nomencalture unnecessarily confusing and have to look
up the distinction between first second and platform 9 3/4s each time I do
a paper.
Yes, that is what I would use them for. But I note that a very large
fraction of the field has studied formal methods, including myself and few
of us find them to be quite as useful as the academics think them to be.
The oracle model is informative but does not necessarily need to be reduced
to symbolic logic to make a point.
I think the main value of formal methods turns out to be pedagogical. When
you teach students formal methods they quickly discover that the best way
to deliver a proof is to refine out every bit of crud possible before
starting and arrive at an appropriate level of abstraction.
But oddly enough I am currently working on a paper that presents a
formalized approach.

@_date: 2013-10-04 13:23:12
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Sha3 
You mean Keccak is spongeworthy.
I do not accept the argument that the computational work factor should be
'balanced' in the way suggested.
The security of a system is almost always better measured by looking at the
work factor for breaking an individual message rather than the probability
that two messages might be generated in circumstances that cancel each
other out.
Given adequate cryptographic precautions (e.e. random serial), a
certificate authority can still use MD5 with an acceptable level of
security even with the current attacks. They would be blithering idiots to
do so of course but Flame could have been prevented with certain
If a hash has a 256 bit output I know that I cannot use it in a database if
the number of records approaches 2^128. But that isn't really a concern to
me. The reason I use a 256 bit hash is because I want a significant safety
margin on the pre-image work factor.
If I was really confident that the 2^128 work factor really is 2^128 then I
would be happy using a 128 bit hash for most designs. In fact in
PRISM-Proof Email I am currently using a 226 bit Subject Key Identifier
because I can encode that in BASE64 and the result is about the same length
as a PGP fingerprint. But I really do want that 2^256 work factor.
If Keccak was weakened in the manner proposed I would probably use the 512
bit version instead and truncate.

@_date: 2013-10-04 13:36:47
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
There is a back story to that. One of the reasons that Ayatolah Kohmenhi
knew about the CIA and embassy involvement in the 53 coup was that he was
one of the hired thugs who raised the demonstrations that toppled Mossadegh.
So the invasion of the embassy was in part motivated by a desire to burn
any evidence of that perfidy on the regimes part. It was also used to
obtain and likely forge evidence against opponents inside the regime. The
files were used as a pretext for the murder of many of the leftists who
were more moderate and western in their outlook.
On the cipher checksum operation, the construction that would immediately
occur to me would be the following:
k1 = R(s)
kv = k1 + E(k1, kd)    // the visible key sent over the wire, kd is a
device key
This approach allows the device to verify that the key is intended for that
device. A captured device cannot be used to decrypt arbitrary traffic even
if the visible key is known. The attacker has to reverse engineer the
device to make use of it, a task that is likely to take months if not
NATO likely does an audit of every cryptographic device every few months
and destroys the entire set if a single one ever goes missing.

@_date: 2013-10-05 11:27:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] A stealth redo on TLS with new encoding 
I think redoing TLS just to change the encoding format is to tilt at
windmills. Same for HTTP (not a fan of CORE over DTLS), same for PKIX.
But doing all three at once would actually make a lot of sense and I can
see something like that actually happen. But only if the incremental cost
of each change is negligible.
Web Services are moving towards JSON syntax. Other than legacy support I
can see no reason to use XML right now and the only reason to use
Assanine.1 other than legacy is to avoid Base64 encoding byte blobs and
escaping strings.
Adding these two features to JSON is very easy and does not require a whole
new encoding format, just add additional code points to the JSON encoding
for length encoded binary blobs. This approach means minimal changes to
JSON encoder code and allows a single decoder to be used for traditional
and binary forms:
Web services are typically layered over HTTP and there are a few facilities
that the HTTP layer provides that are useful in a Web Service. In
particular it is very convenient to allow multiple Web Services to share
the same IP address and port. Anyone who has used the Web Server in .NET
will know what I mean here.
Web Services use some features of HTTP but not very many. It would be very
convenient if we could replace the HTTP layer with something that provides
just the functionality we need but layers over UDP or TCP directly and uses
JSON-B encoding.
One of the features I use HTTP for is to carry authentication information
on the Web Service requests and responses. I have a Web Service to do a key
exchange using SSL for privacy (its a pro-tem solution though, will add in
a PFS exchange at some point).
The connect protocol produces a Kerberos like ticket which is then used to
authenticate subsequent HTTP messages using a MAC.
In my view, authentication at the transport layer is not a substitute for
authentication at the application layer. I want server authentication and
confidentiality at least at transport layer and in addition I want mutual
authentication at the application layer.
For efficiency, the authentication at the application layer uses symmetric
key (unless non-repudiation is required in which case digital signatures
would be indicated but in addition to MAC, not as a replacement).
Once a symmetric key is agreed for authentication, the use of the key for
application layer authentication is reasonably obvious.
OK, so far the scheme I describe is three independent schemes that are all
designed to work inside the existing HTTP-TLS-PKIX framework and they
provide value within that framework. But as I observed earlier, it is quite
possible to kick the framework away and replace HTTP with a JSON-B based
presentation layer framing.
This is what I do in the UDP transport for omnibroker as that is intended
to be a replacement for the DNS client-server interface.
So in summary, yes it is quite possible that TLS could be superseded by
something else, but that something else is not going to look like TLS and
it will be the result of a desire to build systems that use a single
consistent encoding at all layers in the stack (above the packet/session
Trying to reduce the complexity of TLS is plausible but all of that
complexity was added for a reason and those same reasons will dictate
similar features in TLS/2.0. The way to make a system simpler is not to
make each of the modules simpler but to make the modules fit together more
simply. Reducing the complexity of HTTP is hard, reducing the complexity of
TLS is hard. Reducing the complexity of HTTP+TLS is actually easier.
That said, I just wrote a spec for doing PGP key signing in Assanine.1.
Because even though it is the stupidest encoding imaginable, we need to
have a PKI that is capable of expressing every assertion type that people
have found a need for. That means either we add the functionality of PKIX
to the PGP world or vice versa.
The PKIX folk have a vast legacy code base and zero interest in compromise,
many are completely wedged on ASN.1. The PGP code base is much less
embedded than PKIX and PGP folk are highly ideologically motivated to bring
privacy to the masses rather than the specific PGP code formats.
So I have to write my key endorsement message format in Assanine.1. If I
can stomach that then so can everyone else.

@_date: 2013-10-06 11:18:57
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
If people who purport to be on our side go round murdering their people
then they are going to go round murdering people on ours. We already have
Putin's group of thugs murdering folk with Polonium laced teapots, just so
that there can be no doubt as to the identity of the perpetrators.
We are not at war with Iran. I am aware that there are people who would
like to start a war with Iran, the same ones who wanted to start the war
with Iraq which caused a half million deaths but no war crimes trials to
Iran used to have a democracy, remember what happened to it? It was people
like the brothers Dulles who preferred a convenient dictator to a
democratic government that overthrew it with the help of a rent-a-mob
supplied by one Ayatollah Khomenei.
I believe that it was the Ultra-class signals intelligence that made the
operation possible and the string of CIA inspired coups that installed
dictators or pre-empted the emergence of democratic regimes in many other
countries until the mid 1970s. Which not coincidentally is the time that
mechanical cipher machines were being replaced by electronic.
I have had a rather closer view of your establishment than most. You have
retired four star generals suggesting that in the case of a cyber-attack
against critical infrastructure, the government should declare martial law
within hours. It is not hard to see where that would lead there are plenty
of US military types who would dishonor their uniforms with a coup at home,
I have met them.
My view is that we would all be rather safer if the NSA went completely
dark for a while, at least until there has been some accountability for the
crimes of the '00s and a full account of which coups the CIA backed, who
authorized them and why.
I have lived with terrorism all my life. My family was targeted by
terrorists that Rep King and Rudy Giuliani profess to wholeheartedly
support to this day. I am not concerned about the terrorists because they
obviously can't win. It is like the current idiocy in Congress, the
Democrats are bound to win because at the end of the day the effects of the
recession that the Republicans threaten to cause will be temporary while
universal health care will be permanent. The threatened harm is not great
enough to cause a change in policy. The only cases where terrorist tactics
have worked is where a small minority have been trying to suppress the
majority, as in Rhodesia or French occupied Spain during the Napoleonic
But when I see politicians passing laws to stop people voting, judges
deciding that the votes in a Presidential election cannot be counted and
all the other right wing antics taking place in the US at the moment, the
risk of a right wing fascist coup has to be taken seriously.

@_date: 2013-10-06 21:10:21
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Shamir said that he would like to see AES detuned for speed and extra
rounds added during the RSA conf cryptographers panel a couple of years
That is the main incentive for using AES 256 over 128. Nobody is going to
be breaking AES 128 by brute force so key size above that is irrelevant but
you do get the extra rounds.
Saving symmetric key bits does not really bother me as pretty much any
mechanism I use to derive them is going to give me plenty. I am even
starting to think that maybe we should start using the NSA checksum
Incidentally, that checksum could be explained simply by padding prepping
an EC encrypted session key. PKCS has similar stuff to ensure that there
is no known plaintext in there. Using the encryption algorithm instead of
the OAEP hash function makes much better sense.

@_date: 2013-10-07 13:14:45
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Elliptic curve question 
Are you planning to publish your signing key or your decryption key?
Use of a key for one makes the other incompatible.

@_date: 2013-10-07 15:52:32
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I proposed a mechanism for that a long time back based on Rivest's notion
of a suicide note in SDSI.
The idea was that some group of cryptographers get together and create some
random numbers which they then keyshare amongst themselves so that there
are (say) 11 shares and a quorum of 5.
Let the key be k, if the algorithm being witnessed is AES then the value
AES(k) is published as the 'witness value for AES.
A device that ever sees the witness value for AES presented knows to stop
using it. It is in effect a 'suicide note' for AES.
Similar witness functions can be specified easily enough for hashes etc. We
already have the RSA factoring competition for RSA public key. In fact I
suggested to Burt Kaliski that they expand the program.
The cryptographic basis here is that there are only two cases where the
witness value will be released, either there is an expert consensus to stop
using AES (or whatever) or someone breaks AES.
The main downside is that there are many applications where you can't
tolerate fail-open. For example in the electricity and power system it is
more important to keep the system going than to preserve confidentiality.
An authenticity attack on the other hand might be cause...

@_date: 2013-10-08 16:07:17
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The cost of National Security Letters 
One of the biggest problems with the current situation is that US
technology companies have no ability to convince others that their
equipment has not been compromised by a government mandated backdoor.
This is imposing a significant and real cost on providers of outsourced Web
Services and is beginning to place costs on manufacturers. International
customers are learning to shop elsewhere for their IT needs.
While moving from the US to the UK might seem to leave the customer equally
vulnerable to warrant-less NSA/GCHQ snooping, there is a very important
difference. A US provider can be silenced using a National Security Letter
which is an administrative order issued by a government agency without any
court sanction. There is no equivalent capability in UK law.
A UK court can make an intercept order or authorize a search etc. but that
is by definition a Lawful Intercept and that capability exists regardless
of jurisdiction. What is unique in the US at the moment is the National
Security Letter.

@_date: 2013-10-09 08:44:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Iran and murder 
I said the same thing in the launch issue of cyber-defense. Unfortunately
the editor took it into his head to conflate inventing the HTTP referer
field etc. with rather more and so I can't point people at the article as
they refuse to correct it.
I see cyber-sabotage as being similar to use of chemical or biological
weapons: It is going to be banned because the military consequences fall
far short of being decisive, are unpredictable and the barriers to entry
are low.
STUXNET has been relaunched with different payloads countless times. So we
are throwing stones the other side can throw back with greater force.
We have a big problem in crypto because we cannot now be sure that the help
received from the US government in the past has been well intentioned or
not. And so a great deal of time is being wasted right now (though we will
waste orders of magnitude more of their time).
At the moment we have a bunch of generals and contractors telling us that
we must spend billions on the ability to attack China's power system in
case they attack ours. If we accept that project then we can't share
technology that might help them defend their power system which cripples
our ability to defend our own.
So a purely hypothetical attack promoted for the personal enrichment of a
few makes us less secure, not safer. And the power systems are open to
attack by sufficiently motivated individuals.
The sophistication of STUXNET lay in its ability to discriminate the
intended target from others. The opponents we face simply don't care about
collateral damage. So  I am not impressed by people boasting about the
ability of some country (not an ally of my country BTW) to perform targeted
murder overlooks the fact that they can and likely will retaliate with
indiscriminate murder in return.
I bet people are less fond of drones when they start to realize other
countries have them as well.
Lets just stick to defense and make the NATO civilian infrastructure secure
against cyber attack regardless of what making that technology public might
do for what some people insist we should consider enemies.

@_date: 2013-10-08 17:13:14
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PGP Key Signing parties 
Does PGP have any particular support for key signing parties built in or is
this just something that has grown up as a practice of use?
I am looking at different options for building a PKI for securing personal
communications and it seems to me that the Key Party model could be
improved on if there were some tweaks so that key party signing events were
a distinct part of the model.
I am specifically thinking of ways that key signing parties might be made
scalable so that it was possible for hundreds of thousands of people to
participate in an event and there were specific controls to ensure that the
use of the key party key was strictly bounded in space and time.
So for example, it costs $2K to go to RSA. So if there is a key signing
event associated that requires someone to be physically present then that
is a $2K cost factor that we can leverage right there.
Now we can all imagine ways in which folk on this list could avoid or evade
such controls but they all have costs. I think it rather unlikely that any
of you would want to be attempting to impersonate me at multiple cons.
If there is a CT infrastructure then we can ensure that the use of the key
party key is strictly limited to that one event and that even if the key is
not somehow destroyed after use that it is not going to be trusted.

@_date: 2013-10-09 21:01:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Elliptic curve question 
The original author was proposing to use the same key for encryption and
signature which is a rather bad idea.

@_date: 2013-10-10 13:29:08
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Other Backdoors? 
I sarcastically proposed the use of GOST as an alternative to NIST crypto.
Someone shot back a note saying the elliptic curves might be 'bent'.
Might be interesting for EC to take another look at GOST since it might be
the case that the GRU and the NSA both found a similar backdoor but one was
better at hiding it than the other.
On the NIST side, can anyone explain the reason for this mechanism for
truncating SHA512?
Denote H(0)?
to be the initial hash value of SHA-512 as specified in Section 5.3.5
Denote H(0)?? to be the initial hash value computed below.
H(0) is the IV for SHA-512/t.
For i = 0 to 7
(0)?? (0)? Hi = Hi ? a5a5a5a5a5a5a5a5(in hex).
H(0) = SHA-512 (?SHA-512/t?) using H(0)??
as the IV, where t is the specific truncation value.
[Can't link to FIPS180-4 right now as its down]
I really don't like the futzing with the IV like that, not least because a
lot of implementations don't give access to the IV. Certainly the object
oriented ones I tend to use don't.
But does it make the scheme weaker?
Is there anything wrong with just truncating the output?
The only advantage I can see to the idea is to stop the truncated digest
being used as leverage to reveal the full digest in a scheme where one was
public and the other was not.

@_date: 2013-10-11 09:30:39
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PGP Key Signing parties 
Reply to various,
Yes, the value in a given key signing is weak, in fact every link in the
web of trust is terribly weak.
However, if you notarize and publish the links in CT fashion then I can
show that they actually become very strong. I might not have good evidence
of John Gilmore's key at RSA 2001, but I could get very strong evidence
that someone signed a JG key at RSA 2001.
Which is actually quite a high bar since the attacker would haver to buy a
badge which is $2,000. Even if they were going to go anyway and it is a
sunk cost, they are rate limited.
The other attacks John raised are valid but I think they can be dealt with
by adequate design of the ceremony to ensure that it is transparent.
Now stack that information alongside other endorsements and we can arrive
at a pretty strong authentication mechanism.
The various mechanisms used to evaluate the trust can also be expressed in
the endorsement links.
What I am trying to solve here is the distance problem in Web o' trust. At
the moment it is pretty well impossible for me to have confidence in keys
for people who are ten degrees out. Yet I am pretty confident of the
accuracy of histories of what happened 300 years ago (within certain
It is pretty easy to fake a web of trust, I can do it on one computer, no
trouble. But if the web is grounded at just a few points to actual events
then it becomes very difficult to spoof.

@_date: 2013-10-11 11:26:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Key stretching 
Quick question, anyone got a good scheme for key stretching?
I have this scheme for managing private keys that involves storing them as
encrypted PKCS blobs in the cloud.
AES128 seems a little on the weak side for this but there are (rare)
circumstances where a user is going to need to type in the key for recovery
purposes so I don't want more than 128 bits of key to type in (I am betting
that 128 bits is going to be sufficient to the end of Moore's law).
So the answer is to use AES 256 and stretch the key, but how? I could just
repeat the key:
K = k + k
Related key attacks make me a little nervous though. Maybe:
K = (k + 01234567) XOR SHA512 (k)

@_date: 2013-10-15 13:43:53
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encoding Key Identifiers in email addresses 
I was noodling round with the problem of how to force an existing client to
do the right thing with respect to encryption. One option is to have an
email gateway do opportunistic encryption if it can find a key. Which is OK
but lacks user control.
I don't like the idea of user control coming from the UI because I have to
let the user make use of any email client they like, even ones that can't
cope with top posting. So I can't assume that there will be buttons to
press to say 'encrypt this message'. And that is not what I want in any
What I probably want is the ability to force use of end-to-end encryption
for a small number of users like my clients, the lawyers, various crypto
folk. Something like the https scheme for SSL.
This is what I came up with (cut and pasted from the manual):
Private Key Example
Alice uses a key generation tool to generate a public keypair. The public
parameters in hexadecimal are:
Modulus  :
 db 13 46 62 02 6d c3 4b 98 24 e1 f9 a8 ca 61 3a
 3f 95 f3 d6 c0 45 5a fe 2d be 1d d7 76 d5 95 02
 f4 f9 1b 42 b5 7f 3b 14 f5 79 4c 34 f3 9f 04 07
 ba d2 52 30 dd 61 b3 4a 56 db 4b 12 b7 8b 87 55
 23 39 3a f5 a1 f0 6d 10 4e e8 bb 08 9f b0 66 92
 20 47 20 b4 77 4d 89 a6 58 a2 01 da 05 54 36 1b
 47 3e e0 dc 0b 4e 53 c1 c3 7d cd cf f7 b3 bf 7e
 45 38 5c 0c 0c 13 33 bb c7 da e6 c1 7d 37 f3 99
Exponent :
 01 00 01
The Key Identifier is calculated using SHA512 and truncated to 224 bits to
produce the Key Identifier value. The Key Identifier in Base32 encoding is:
KeyIdentifier: ACACEA-H7MBAA-LAA2RMA-FUAAFQ-AADHAHS-KNAL3A-DPZJAJ-KAA
An email sender may send email to Alice through a compliant gateway as
follows: alice at example.com Send email to Alice using encryption if and only
if an encryption key for Alice can be found and Alice has published the
email encryption policy 'encryption preferred' or stronger. ?
alice at example.com Send email to Alice using encryption if and only if an
encryption key for Alice can be found, otherwise report an error.
ACACEA-H7MBAA-LAA2RMA-FUAAFQ-AADHAHS-KNAL3A-DPZJAJ-KAA?alice at example.com Send
email to Alice using encryption if and only if an encryption key for Alice
can be found that is directly endorsed under the specified key, otherwise
report an error. ACACEA-H7MBAA-LAA2RMA-FUAAFQ-AADHAHS-KNAL3A-DPZJAJ-KAA??
alice at example.com Send email to Alice using encryption if and only if an
encryption key for Alice can be found that is (directly or indierectly)
endorsed under the specified key, otherwise report an error.
We can reduce the length of the key identifier from the 224 bits above to
128 bits if it is a personal key identifier.
In the scheme I am thinking of, the key identifier would be either a PGP v4
key or the hash of the PKIX PublicKeyInfo blob in DER format with an
algorithm identifier plastered on the front.
I am trying to work out how to do the truncation securely using standard
crypto libraries that don't allow the initial IV to be set. (The NIST
approach is broken in that regard).

@_date: 2013-10-16 16:29:32
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PGP Key Signing parties 
The point I was trying to make when I started the thread was not so much to
ask whether they took place as to whether they could be improved and made
more useful if they were recognized as distinct events with a specific set
of endorsement attributes.
I agree with Jon and others that the existing practice is not exactly
ideal. But I think that we could do better.
In particular, I was thinking of key ceremony as being potentially a means
of spreading the use of strong crypto beyond ultra-techy communities. To
doctors, lawyers etc.
Such groups would probably require (and pay for) the services of a compere
to run the process. If it was appropriately designed it can become part of
the social mingling.

@_date: 2013-10-16 22:33:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Trust model for PRISM Hardening Email 
The conclusion I come to is that CA and Web O'Trust both serve different
communities well. However combining both has advantages to both.
CA + Web of Trust + CT is best of all.
Please excuse the haste, I am writing code and doing the design at the same
time and also developing the theory. I will go back and put in references
etc. after the ID cutoff on Monday.

@_date: 2013-10-17 16:40:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encoding Key Identifiers in email addresses 
We could reverse the order if people prefer, that would make it easier for
autocomplete to function since someone is going to start typing 'alice' It
is arguably tidier too since all the user related stuff is at one end and
all the plumbing at the other.
I'll wait for more comments before making any changes though.
The pity is that different systems use a different character: plus (gmail,
That is one of the reasons I chose ?, it is not already in use for other
schemes that might conflict.
Just to make clear, this is the option for forcing use of encryption that
is designed for backwards compatibility sending email from existing clients
with the encryption taking place on a local submit gateway on the same
If someone was to write a new client or plug in that was aware of this
stuff, they would hopefully take the opportunity to do a better job.

@_date: 2013-10-17 18:12:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proof Email, 
I have produced a first draft of the specification for the Key Publication
service and key management tool that talks to it.
The code being documented is rough. Not least because the ASN.1 encoder I
wrote does not know about ASN.1 inanities like OPTIONAL, IMPLICIT or such
yet so the certs are not DER encoded.
This specification represents one of the two interfaces to the blob in the
cloud that I call 'research'. We don't yet know the best approach to trust
management but it is going to be a lot easier to find out if we separate
that hard research problem from the 'plumbing' required to make secure
email work.
The other interface is the Omnibroker specification I wrote earlier this
I believe that between these specifications we have a fairly complete idea
of what the 'plumbing' side of 'Privacy Protected' Email should look like.
The Strong Email Addresses shown earlier provide a demonstration that we
can solve this problem for at least some class of email user using stock
email clients (OK plus a proxy gateway to send the mail).
If people would like to write code, we are at the point where that is now
practical. In addition it would be very useful if people could find out
information such as how various commonly used email clients store S/MIMe
keys and how might a program do the user's job of configuration for them.

@_date: 2013-10-22 15:34:49
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] A different explanation of the Snowden documents 
We have all seen what happens when an organization have a clear set of
priorities, a set of aggressive metrics used to evaluate progress and an
'up or out' culture: The middle managers massage the figures to meet the
So China might be going through an economic boom or a bust but the official
figures won't show the difference because they bear no relation to reality.
Are the leaked NSA documents possibly the result of the same cultural
I am specifically thinking of claims like the purported vulnerabilities
introduced into security specs. So far we have detected the NIST random
number generator but that was spotted at the time. There are a few areas
where DoD contractors have dominated IETF process but the result has not
been to block changes to the standard, the standards have instead been set
outside IETF process.
So I see the following possibilities
1) The NSA documents are genuine
2) The NSA documents are a hoax
3)  The NSA documents are the result of structural self delusion.
I discount 2 and at least some documents are describing real programs. But
I am starting to think that some of the programs maybe work about as well
as that missile defense scheme they have never tested without fudging the
result so it succeeds.
Imagine you are a Major in the NSA and Alexander has taken over and the
only way he knows to win a war is to destroy the opposition (rather than
not start it). You have ten years of working constructively with the IETF
etc. to improve the security of Internet standards. How do you present your

@_date: 2013-10-23 09:53:49
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] programable computers inside our computers 
All I want from a trusted computing base is the ability to store private
keys and make use of them in a way that prevents them being extracted by an
attacker without physical access to the machine and reasonably advanced
The ability to tell a server to reboot remotely and to only load the O/S
image that I authorized would also be useful. In a very large installation
I might want the ability to drop ship the machine and have it boot from the
network the first time.
Instead we get...
None of these functions would be useful to me unless they were part of an
open standard because however good one vendor's tools are, the cost of lock
in is never acceptable.

@_date: 2013-10-24 09:39:06
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] "Death Note" elimination for hashes 
Cybercash went out of business before the rise of PayPal. PayPal did not do
a merchant gateway product that would have competed with Cybercash until
they acquired the Signio business from VeriSign which included the
Cybercash assets that VeriSign bought out of bankruptcy.
What did for Cybercash was the fact that the merchant software kept falling
over and causing double charging of accounts. That and their business model
was a fee per service rather than the flat rate fee for connection of

@_date: 2013-10-24 11:16:52
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] /dev/random is not robust 
And I have not seen any proposal that is really going to solve this
particular problem in the thread since.
If I was asked three months ago my position would be 'generate the keys on
the device that is going to use them and they never leave unless it is a
really constrained device like a credit card.'
I have completely changed my mind on this. I now think public keys should
be generated in device adapted for that purpose and migrated out using some
form of secure protocol that ensures only the intended device can use them.
Further, the scheme used should provide the devices with a unique random
seed that can be used as a backstop against compromise or failure of other
RNGs. Using a stream cipher is not a very good RNG but nothing bad can
happen by XORing a good but brittle RNG against the output of a completely
independent cipherstream.

@_date: 2013-09-02 14:45:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] NSA and cryptanalysis 
Given the reaction from Microsoft, yes.
The Microsoft public affairs people have been demonstrating real anger at
the Flame attack in many forums.

@_date: 2013-09-02 16:04:10
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] NSA and cryptanalysis 
You know, if there was a completely ironclad legal opinion that made use of
ECC possible without the risk of a lawsuit costing over $2 million from
Certicom then I would be happy to endorse a switch to ECC like the NSA is
pushing for as well.
I would not therefore draw the conclusion that NSA advice to move to ECC is
motivated by knowledge of a crack of RSA, if anything that would argue
against moving from ECC. It is merely a consequence of the US government
having a license which we don't have.

@_date: 2013-09-03 11:02:55
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Keeping backups (was Re: Separating concerns 
Want to collaborate on an Internet Draft?
This is obviously useful but it can only be made useful if everyone does it
in the same way.

@_date: 2013-09-03 11:13:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] NSA and cryptanalysis 
What is the state of prior art for the P-384? When was it first published?
Given that RIM is trying to sell itself right now and the patents are the
only asset worth having, I don't have good feelings on this. Well apart
from the business opportunities for expert witnesses specializing in crypto.
The problem is that to make the market move we need everyone to decide to
go in the same direction. So even though my employer can afford a license,
there is no commercial value to that license unless everyone else has
Do we have an ECC curve that is (1) secure and (2) has a written
description prior to 1 Sept 1993?
Due to submarine patent potential, even that is not necessarily enough but
it would be a start.

@_date: 2013-09-03 12:30:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Backup is completely separate 
<521CE337.6030706
To avoid mandated/coerced release substitute 'keep at bank' with 'bury at
undisclosed location'.
There is really no 100% reliable way to make things available to your heirs
while avoiding government coercion. Particularly since the government
issues the documents saying that you are dead.

@_date: 2013-09-04 07:32:35
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Three kinds of hash: Two are still under ITAR. 
While doing some research on the history of hashing for a client I
discovered that it is described in the very first edition of the ACM
journal and the paper is a translation of a Russian paper.
One of the many problems with the ITAR mindset is the assumption that all
real ideas are invented inside the US by white men wearing white lab coats
and that the rest of the undeserving world is stealing them.
Anyone with any grasp of history recognizes that the industrial scale
industrial espionage practiced by China on the industrial powers is merely
DIY reparations for the 19th century and the first half of the 20th.

@_date: 2013-09-04 11:00:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Hashes into Ciphers 
On a more theoretical basis, Phil Rogaway gave a presentation at MIT many
years ago where he showed the use of a one-way function as the construction
primitive for every other type of symmetric algorithm.

@_date: 2013-09-05 14:26:23
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Google's Public Key Size (was Re: NSA and 
Which is rather easier to effect since the browser providers have no
longstanding contractual agreements made prior to the BRs being adopted.

@_date: 2013-09-05 16:11:57
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
OK how about this:
If a person at Snowden's level in the NSA had any access to information
that indicated the existence of any program which involved the successful
cryptanalysis of any cipher regarded as 'strong' by this community then the
Director of National Intelligence, the Director of the NSA and everyone
involved in those decisions should be fired immediately and lose their
What was important in Ultra was the fact that the Germans never discovered
they were being intercepted and decrypted. They would have strengthened
their cipher immediately if they had known it was broken.
So either the NSA has committed an unpardonable act of carelessness (beyond
the stupidity of giving 50,000 people like Snowden access to information
that should not have been shared beyond 500) or the program involves lower
strength ciphers that we would not recommend the use of but are still there
in the cipher suites.
I keep telling people that you do not make a system more secure by adding
the choice of a stronger cipher into the application. You make the system
more secure by REMOVING the choice of the weak ciphers.
I would bet that there is more than enough DES traffic to be worth attack
and probably quite a bit on IDEA as well. There is probably even some 40
and 64 bit crypto in use.
Before we assume that the NSA is robbing banks by using an invisibility
cloak lets consider the likelihood that they are beating up old ladies and
taking their handbags.

@_date: 2013-09-05 16:33:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
? The NSA spends $250m a year on a program which, among other goals, works
with technology companies to "covertly influence" their product designs.
I believe this confirms my theory that the NSA has plants in the IETF to
discourage moves to strong crypto.

@_date: 2013-09-05 16:58:07
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Or they contacted the NSA alumni working in the industry.
I think it is subtler that that. Trying to block a strong cipher is too
obvious. Much better to push for something that is overly complicated or
too difficult for end users to make use of.
* The bizare complexity of IPSEC.
* Allowing deployment of DNSSEC to be blocked in 2002 by blocking a
technical change that made it possible to deploy in .com.
* Proposals to deploy security policy information (always send me data
encrypted) have been consistently filibustered by people making nonsensical
3) I would not be surprised if random number generator problems in a
Agreed, the PRNG is the easiest thing to futz with.
It would not surprise me if we discovered kleptography at work as well.
I think the thing that discouraged all that was the decision to make end
user certificates hard to obtain (still no automatic spec) and expire after
a year.

@_date: 2013-09-05 21:29:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] tamper-evident crypto? (was: BULLRUN) 
Sent from my difference engine
Not necessarily
Anyone who raised a suspicion was risking their life.

@_date: 2013-09-07 16:20:18
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Before you make silly accusations go read the VeriSign Certificate
Practices Statement and then work out how many people it takes to gain
access to one of the roots.
The Key Ceremonies are all videotaped from start to finish and the auditors
have reviewed at least some of the ceremonies. So while it is not beyond
the realms of possibility that such a large number of people were suborned,
I think it drastically unlikely.
Add to which Jim Bizdos is not exactly known for being well disposed to the
NSA or key escrow.
Hacking CAs is a poor approach because it is a very visible attack.
Certificate Transparency is merely automating and generalizing controls
that already exist.
But we can certainly add them to S/MIME, why not.

@_date: 2013-09-07 16:30:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Protecting Private Keys 
And this is why I have been so peeved at the chorus of attack against
trustworthy computing.
All I have ever really wanted from Trustworthy computing is to be sure that
my private keys can't be copied off a server.
And private keys should never be in more than one place unless they are
either an offline Certificate Signing Key for a PKI system or a decryption
key for stored data.

@_date: 2013-09-07 16:12:54
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Good theory only the CA industry tried very hard to deploy and was
prevented from doing so because Randy Bush abused his position as DNSEXT
chair to prevent modification of the spec to meet the deployment
requirements in .com.
DNSSEC would have deployed in 2003 with the DNS ATLAS upgrade had the IETF
followed the clear consensus of the DNSEXT working group and approved the
OPT-IN proposal. The code was written and ready to deploy.
I told the IESG and the IAB that the VeriSign position was no bluff and
that if OPT-IN did not get approved there would be no deployment in .com. A
business is not going to spend $100million on deployment of a feature that
has no proven market demand when the same job can be done for $5 million
with only minor changes.
CAs do not make their money in the ways you imagine. If there was any
business case for DNSSEC I will have no problem at all finding people
willing to pay $50-100 to have a CA run their DNSSEC for them because that
is going to be a lot cheaper than finding a geek with the skills needed to
do the configuration let alone do the work.
One reason that PGP has not spread very far is that there is no group that
has a commercial interest in marketing it.
At the moment revenues from S/MIME are insignificant for all the CAs.
Comodo gives away S/MIME certs for free. Its just not worth enough to try
to charge for right now.
If we can get people using secure email or DNSSEC on a large scale then CAs
will figure out how to make money from it. But right now nobody is making a
profit from either.

@_date: 2013-09-07 22:11:17
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
No they do not. There is W3C and OASIS both of which are larger now. And
there has always been IEEE.
And they have no power to mandate anything. In fact one of the things I
have been trying to do is to persuade people that the Canute act commanding
the tides to turn is futile. People need to understand that the IETF does
not have any power to mandate anything and that stakeholders will only
follow standards proposals if they see a value in doing so.
Like STARTTLS which has been in the standards and deployed for a decade now?
What on earth is that? DNS is a directory so anything that authenticates
directory attributes is going to be capable of being used as a PKI.
The value of all the gold in the world ever mined is $8.2 trillion. The
NASDAQ alone traded $46 trillion last Friday.
There are problems with bitcoin but I would worry rather more about the
fact that the Feds have had no trouble at all shutting down every prior
attempt at establishing a currency of that type and the fact that there is
no anonymity whatsoever.
Umm I would suggest that it has more to do with supply and demand and the
fact that there is a large amount of economic activity that is locked out
of the formal banking system (including the entire nation of Iran) that is
willing to pay a significant premium for access to a secondary.
RFC 3702 published in 2002.

@_date: 2013-09-07 22:56:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
On Sat, Sep 7, 2013 at 10:35 PM, Gregory Perry
This is seriously off topic here but the idea that the indictment of Phil
Zimmerman was a token effort is nonsense. I was not accusing Phil Z. of
being a plant.
Not only was Louis Freeh going after Zimmerman for real, he went against
Clinton in revenge for the Clipper chip program being junked. He spent much
of Clinton's second term conspiring with Republicans in Congress to get
Clinton impeached.
Clipper was an NSA initiative that began under Bush or probably even
earlier. They got the incoming administration to endorse it as a fait
Snowden and Manning on the other hand... Well I do wonder if this is all
some mind game to get people to secure the Internet against cyberattacks.
But the reason I discount that as a possibility is that what has been
revealed has completely destroyed trust. We can't work with the Federal
Government on information security the way that we did in the past any more.
I think the administration needs to make a downpayment on restoring trust.
They could begin by closing the gulag in Guantanamo.

@_date: 2013-09-08 08:28:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] MITM source patching [was Schneier got spooked] 
But is the source compromised in the archive?
It think we need a different approach to source code management. Get rid of
user authentication completely, passwords and SSH are both a fragile
approach. Instead every code update to the repository should be signed and
recorded in an append only log and the log should be public and enable any
party to audit the set of updates at any time.
This would be 'Code Transparency'.
Problem is we would need to modify GIT to implement.

@_date: 2013-09-08 08:40:38
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
No, that part is untrue. I sat at the table with Jeff Schiller and Burt
Kaliski when Burt pitched S/MIME at the IETF. He was Chief Scientist of RSA
Labs at the time.
Jim did go after Phil Z. over PGP initially. But Phil Z. was violating the
patent at the time. That led to RSAREF and the MIT version of PGP.
DNSSEC was (and is) a mess as a standard because it is an attempt to
retrofit a directory designed around some very tight network constraints
and with a very poor architecture to make it into a PKI.
PS: My long-standing domain registrar (enom.com) STILL doesn't support
The Registrars are pure marketing operations. Other than GoDaddy which
implemented DNSSEC because they are trying to sell the business and more
tech looks kewl during due diligence, there is not a market demand for
One problem is that the Registrars almost invariably sell DNS registrations
at cost or at a loss and make the money up on value added products. In
particular SSL certificates.

@_date: 2013-09-08 09:42:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Trapdoor symmetric key 
Two caveats on the commentary about a symmetric key algorithm with a
trapdoor being a public key algorithm.
1) The trapdoor need not be a good public key algorithm, it can be flawed
in ways that would make it unsuited for use as a public key algorithm. For
instance being able to compute the private key from the public or deduce
the private key from multiple messages.
2) The trapdoor need not be a perfect decrypt. A trapdoor that reduced the
search space for brute force search from 128 bits to 64 or only worked on
some messages would be enough leverage for intercept purposes but make it
useless as a public key system.

@_date: 2013-09-08 13:18:53
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Trapdoor symmetric key 
But the compromise may only be visible if you have access to some
cryptographic technique which we don't currently have.
The point I am making is that a backdoor in a symmetric function need not
be a secure public key system, it could be a breakable one. And that is a
much wider class of function than public key cryptosystems. There are many
approaches that were tried before RSA and ECC were settled on.
2^128 is still beyond the reach of brute force.
2^64 and a 128 bit key which is the one we usually use on the other hand...
Perhaps we should do a test, move to 256 bits on a specific date across the
net and see if the power consumption rises near the NSA data centers.

@_date: 2013-09-08 13:53:49
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Points of compromise 
I was asked to provide a list of potential points of compromise by a
concerned party. I list the following so far as possible/likely:
1) Certificate Authorities
Traditionally the major concern (perhaps to the point of distraction from
other more serious ones). Main caveat, CA compromises leave permanent
visible traces as recent experience shows and there are many eyes looking.
Even if Google was compromised I can't believe Ben Laurie and Adam Langley
are proposing CT in bad faith.
2) Covert channel in Cryptographic accelerator hardware.
It is possible that cryptographic accelerators have covert channels leaking
the private key through TLS (packet alignment, field ordering, timing,
etc.) or in key generation (kleptography of the RSA modulus a la Motti
3) Cryptanalytic attack on one or more symmetric algorithms.
I can well believe that RC4 is bust and that there is enough RC4 activity
going on to make cryptanalysis worth while. The idea that AES is
compromised seems very less likely to me.
4) Protocol vulnerability introduced intentionally through IETF
I find this rather unlikely to be a direct action since there are few
places where the spec could be changed to advantage an attacker and only
the editors would have the control necessary to introduce text and there
are many eyes.
5) Protocol vulnerability that IETF might have fixed but was discouraged
from fixing.
Oh more times than I can count. And I would not discount the possibility
that there would be strategies based exploiting on the natural suspicion
surrounding security matters. It would have been easy for a faction to
derail DNSSEC by feeding the WG chair's existing hostility to CAs telling
him to stand firm.
One concern here is that this will fuel the attempt to bring IETF under
control of the ITU and Russia, China, etc.

@_date: 2013-09-08 18:49:56
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
There is a market demand for security. But it is always item  on the list
of priorities and the top two get done.
I have sold seven figure crypto installations that have remained shelfware.
The moral is that we have to find other market reasons to use security. For
example simplifying administration of endpoints. I do not argue like some
do that there is no market for security so we should give up, I argue that
there is little market for something that only provides security and so to
sell security we have to attach it to something they want.
People buy guns despite statistics that show that they are orders of
magnitude more likely to be shot with the gun themselves rather than by an
However, if you told consumers "did you know that food manufacturer
Yes, but most cases the telco will only buy a fix after they have been
To sell DNSSEC we should provide a benefit to the people who need to do the
deployment. Problem is that the perceived benefit is to the people going to
the site which is different...
It is fixable, people just need to understand that the stuff does not sell

@_date: 2013-09-09 12:00:54
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The One True Cipher Suite 
Steve Bellovin has made the same argument and I agree with it.
Proliferation of cipher suites is not helpful.
The point I make is that adding a strong cipher does not make you more
secure. Only removing the option of using weak ciphers makes you more
There are good reasons to avoid MD5 and IDEA but at this point we are very
confident of AES and SHA3 and reasonably confident of RSA.
We will need to move away from RSA at some point in the future. But ECC is
a mess right now. We can't trust the NIST curves any more and the IPR
status is prohibitively expensive to clarify.

@_date: 2013-09-10 18:20:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The One True Cipher Suite 
I really hate the monoculture argument. It misses the fact that evolution
of Internet applications and attack strategies is not according to
Darwinian evolution.
Diversity is only a successful strategy against Darwinian evolution. It
does not work against intelligent design and malware is a product of
intelligent design.
Whether it is better to put all your eggs in one basket or in many baskets
depends on the consequences of compromise.
If the loss of one egg is acceptable then many baskets is the way to go. If
on the other hand they are dragon eggs and the loss of just one is a
catastrophe then putting them all in one basket is the lowest risk strategy.
1.  If everyone uses the same cipher, the attacker need only attack that
But on the flip side the cost of developing ciphers is large and the
vulnerabilities introduced into a protocol through support for algorithm
negotiation are significant.
Moreover as Newt Gingrich discovered, it only takes one party to your
conversation to be using an old AMPS analog line for your conspiracy to be
I would rather choose one algorithm and one additional strong algorithm as
a backup than have the hundreds of algorithms.

@_date: 2013-09-11 12:11:52
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Defenses against pervasive versus targeted intercept 
I have spent most of yesterday writing up much of the traffic on the list
so far in the form of an Internet Draft.
I am now at the section on controls and it occurs to me that the controls
relevant to preventing PRISM-like pervasive intercept capabilities are not
necessarily restricted to controls that protect against targeted intercept.
The problem I have with PRISM is that it is a group of people whose
politics I probably find repellent performing a dragnet search that may
later be used for McCarthyite/Hooverite inquisitions. So I am much more
concerned about the pervasive part than the ability to perform targeted
attacks on a few individuals who have come to notice. If the NSA wanted my
help intercepting Al Zawahiri's private emails then sign me up. My problem
is that they are intercepting far too much an lying about what they are
Let us imagine for the sake of argument that the NSA has cracked 1024 bit
RSA using some behemoth computer at a cost of roughly $1 million per key
and taking a day to do so. Given such a capability it would be logical for
them to attack high traffic/high priority 1024 bit keys. I have not looked
into the dates when the 2048 bit roll out began (seems to me we have been
talking about it ten years) but that might be consistent with that 2010
If people are using plain TLS without perfect forward secrecy, that crack
gives the NSA access to potentially millions of messages an hour. If the
web browsers are all using PFS then the best they can do is one message a
PFS provides security even when the public keys used in the conversation
are compromised before the conversation takes place. It does not prevent
attack but it reduces the capacity of the attacker.
Similar arguments can be made for other less-than-perfect key exchange
schemes. It is not necessary for a key exchange scheme to be absolutely
secure against all possible attack for it to be considered PRISM-Proof.
So the key distribution scheme I am looking at does have potential points
of compromise because I want it to be something millions could use rather
than just a few thousand geeks who will install but never use. But the
objective is to make those points of compromise uneconomic to exploit on
the scale of PRISM.
The NSA should have accepted court oversight of their activities. If they
had strictly limited their use of the cryptanalytic capabilities then the
existence would not have been known to low level grunts like Snowden and we
probably would not have found out.
Use of techniques like PFS restores balance.

@_date: 2013-09-11 13:39:40
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
One advantage of this approach is that we could use RSA for one and ECC for
the other and thus avoid most consequences of an RSA2048 break (if that is
The problem I see reviewing the list is that ECC has suddenly become
suspect and we still have doubts about the long term use of RSA.
It also have the effect of pushing the ECC IPR concerns off the CA and onto
the browser/server providers. I understand that many have already got
licenses that allow them to do what they need in that respect.
Perfect Forward Secrecy is not perfect. In fact it is no better than
regular public key. The only difference is that if the public key system is
cracked then with PFS the attacker has to break every single key exchange
and not just the keys in the certificates and if you use an RSA outer with
an ECC inner then you double the cryptanalytic cost of the attack (theory
as well as computation).
I think this is the way forward.

@_date: 2013-09-11 15:44:49
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
My point was that the name is misleading and causes people to look for more
than is there. It took me a long time to work out how PFS worked till I
suddenly realized that it does not deliver what is advertised.
That is my point precisely.
Though the way you put it, I have to ask if PFS deserves higher priority
than Certificate Transparency. As in something we can deploy in weeks
rather than years.
I have no problem with Certificate Transparency. What I do have trouble
with is Ben L.'s notion of Certificate Transparency and Automatic Audit in
the End Client which I imposes a lot more in the way of costs than just
transparency and moreover he wants to push out the costs to the CAs so he
can hyper-tune the performance of his browser.

@_date: 2013-09-11 16:30:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Summary of the discussion so far 
I have attempted to produce a summary of the discussion so far for use as a
requirements document for the PRISM-PROOF email scheme. This is now
available as an Internet draft.
I have left out acknowledgements and references at the moment. That is
likely to take a whole day going back through the list and I wanted to get
this out.
If anyone wants to claim responsibility for any part of the doc then drop
me a line and I will have the black helicopter sent round.

@_date: 2013-09-16 13:49:55
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] End to end 
Just writing document two in the PRISM-Proof series. I probably have to
change the name before November. Thinking about 'Privacy Protected' which
has the same initials.
People talk about end-to-end without talking about what they are. In most
cases at least one end is a person or an organization, not a machine. So
when we look at the security of the whole system people security issues
like the fact they forget private key passphrases and lose machines matter.
Which ends you are talking about depends on what the context is. If we are
talking about message formats then the ends are machines. If we are talking
about trust then the ends are people and organizations.
End to end has a lot of costs. Deploying certificates to end users is
expensive in an enterprise and often unnecessary. If people are sending
email through the corporate email system then in many cases the corporation
has a need/right to see what they are sending/receiving.
So one conclusion about S/MIME and PGP is that they should support domain
level confidentiality and confidentiality, not just account level.
Another conclusion is that end-to-end security is orthogonal to transport.
In particular there are good use cases for the following configuration:
Mail sent from alice at example.com to bob at example.net
* DKIM signature on message from example.com as outbound MTA 'From'.
* S/MIME Signature on message from example.com with embedded logotype
* TLS Transport Layer Security with Forward Secrecy to example.net mail
server using DNSSEC and DANE to authenticate the IP address and certificate.
* S/MIME encryption under example.net EV certificate
* S/MIME encryption under bob at example.net personal certificate.
[Hold onto flames about key validation and web of trust for the time being.
Accepting the fact that S/MIME has won the message format deployment battle
does not mean we are obliged to use the S/MIME PKI unmodified or require
use of CA validated certificates.]
Looking at the Certificate Transparency work, I see a big problem with
getting the transparency to be 'end-to-end', particularly with Google's
insistence on no side channels and ultra-low latency.
To me the important thing about transparency is that it is possible for
anyone to audit the key signing process from publicly available
information. Doing the audit at the relying party end prior to every
reliance seems a lower priority.
In particular, there are some type of audit that I don't think it is
feasible to do in the endpoint. The validity of a CT audit is only as good
as your newest notary timestamp value. It is really hard to guarantee that
the endpoint is not being spoofed by a PRISM capable adversary without
going to techniques like quorate checking which I think are completely
practical in a specialized tracker but impractical to do in an iPhone or
any other device likely to spend much time turned off or otherwise
disconnected from the network.

@_date: 2013-09-16 15:27:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] MITM source patching [was Schneier got spooked] 
Well people bandwidth is always a problem.
But what I want is not just the ability to sign, I want to have a mechanism
to support verification and checking of the log etc. etc.
Where I am headed is to first divide up the space for PRISM-PROOF email
between parts that are solved and only need good execution (message
formats, mail integration, etc) and parts that are or may be regarded as
research (key distribution, key signing, PKI).
Once that is done I am going to be building myself a very lightweight
development testbed built on a SMTP/SUBMIT + IMAP proxy.
But hopefully other people will see that there is general value to such a
scheme and work on:
[1] Enabling MUAs to make use of research built on the testbed.
[2] Enabling legacy PKI to make use of the testbed.
[3] Research schemes
Different people have different skills and different interests. My interest
is on the research side but other folk just want to write code to a clear
spec. Anyone going for [3] has to understand at the outset that whatever
they do is almost certain to end up being blended with other work before a
final standard is arrived at. We cannot afford another PGP/SMIME debacle.
On the research side, I am looking at something like Certificate
Transparency but with a two layer notary scheme. Instead of the basic
infrastructure unit being a CA, the basic infrastructure unit is a Tier 2
append only log. To get people to trust your key you get it signed by a
trust provider. Anyone can be a trust provider but not every trust provider
is trusted by everyone. A CA is merely a trust provider that issues policy
and practices statements and is subject to third party audit.
The Tier 2 notaries get their logs timestamped by at least one Tier 1
notary and the Tier 1 notaries cross notarize.
So plugging code signing projects into a Tier 2 notary would make a lot of
We could also look at getting Sourceforge and GITHub to provide support

@_date: 2013-09-16 15:58:15
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] End to end 
Yeah, not trying to attack you or anything. Just trying to work out exactly
what the security guarantees provided are.
I doubt it is necessary to go very far to deter PRISM type surveillance. If
that continues very long at all. The knives are out for Alexander, hence
the story about his Enterprise bridge operations room.
Now the Russians...
Do we need to be able to detect PRISM type surveillance in the infrequently
connected device or is is sufficient to be able to detect it somewhere?
One way to get as good timestamp into a phone might be to use a QR code:
This is I think as large as would be needed:
[image: Inline image 1]

@_date: 2013-09-17 14:43:35
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
My phrase PRISM-Proofing seems to have created some interest in the press.
PRISM-Hardening might be more important, especially in the short term. The
objective of PRISM-hardening is not to prevent an attack absolutely, it is
to increase the work factor for the attacker attempting ubiquitous
Examples include:
Forward Secrecy: Increases work factor from one public key per host to one
public key per TLS session.
Smart Cookies: Using cookies as authentication secrets and passing them as
plaintext bearer tokens is stupid. It means that all an attacker needs to
do is to compromise TLS once and they have the authentication secret. The
HTTP Session-ID draft I proposed a while back reduces the window of
compromise to the first attack.
I am sure there are other ways to increase the work factor.

@_date: 2013-09-18 09:08:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
A few clarifications
1) PRISM-Proof is a marketing term
I have not spent a great deal of time looking at the exact capabilities of
PRISM vs the other programs involved because from a design point they are
irrelevant. The objective is to harden/protect the infrastructure from any
ubiquitous, indiscriminate intercept capability like the one Gen Alexander
appears to have constructed.
PRISM-class here is merely a handy label for a class of attack where the
attacker can spend upwards of $100 million to perform an attack which
potentially affects every Internet user. PRISM-class is a superset of
PRISM, BULLRUN, MANASAS, etc. etc.
2) SSL is not designed to resist government intercept
Back in 1993-6 when I was working on Internet security and payments at CERN
and the Web Consortium the priority was to make payments on the Web, not
make it resistant to government intercept. The next priority was to
establish the authenticity of news Web sites. There were several reasons
for that set of priorities, one of which was that the technology we had
available was limited and it was impractical to do more than one public key
operation per session and it was only practical to use public key some of
the time. Severs of the day simply could not handle the load otherwise.
Twenty years later, much has changed and we can do much more. The designs
do not need to be constrained in the way they were then.
It is not a question of whether email is encrypted in transport OR at rest,
we need both. There are different security concerns at each layer.
3) We need more than one PKI for Web and email security.
PGP and S/MIME have different key distribution models. Rather than decide
which is 'better' we need to accept that we need both approaches and in
fact need more.
If I am trying to work out if an email was really sent by my bank then I
want a CA type security model because less than 0.1% of customers are ever
going to understand a PGP type web of trust for that particular purpose.
But its the bank sending the mail, not an individual at the bank.

@_date: 2013-09-18 10:44:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] An NSA mathematician shares his 
As someone who has met Hayden, I do not think his words are necessarily
untrue, they may be out of date. It appears that there was a major change
at the NSA after his departure. In particular the number of external
contractors seems to have increased markedly (based on the number and type
of job adverts from SAIC, Booz-Allen, Van Dyke, etc.)
The enterprise bridge control center certainly does not seem to be Hayden's
style either. Hayden is not the type to build a showboat like that.
After 9/11 we discovered that our view of the cryptowars was completely
false in one respect. Louis Freeh wasn't building a panopticon, he simply
had no comprehension of the power of the information he was demanding the
ability to collect. The FBI computer systems were antiquated, lacking the
ability to do keyword search on two terms.
I rather suspect that Alexander is similarly blind to the value of the
information the system is collecting. They might well be telling the truth
when they told the court that the system was so compartmentalized and
segregated nobody knew what it was doing.
For example, did the NSA people who thought it a good wheeze to trade raw
SIGINT on US citizens to the Israelis understand what they were passing on?
They certainly don't seem to know the past history of US-Israeli
'cooperation' only last year an Israeli firm was trying to sell intercept
equipment to Iran through an intermediary and the story of how the Chinese
got an example of the Stinger missile to copy is well known. My country has
had an arms embargo on Israel for quite a while due to breach of Israeli
undertakings not to use military weapons against civilians.
That does not make the situation any less dangerous, it makes it more so.
What Barkan does not mention is that we know that the NSA internal controls
have collapsed completely, Snowdens disclosure proves that. Snowden should
never have had access to the information he has disclosed.
As with gwbush53.com, the intelligence gathered through PRISM-class
intercepts will undoubtedly be spread far and wide. Anything Snowden knows,
China and Russia will know.
The fact that nothing has been said on that publicly by the NSA
spokespeople is something of a concern. They have a big big problem and
heads should be rolling. I can't see how Clapper and Alexander can remain
given the biggest security breach in NSA history on their watch.

@_date: 2013-09-18 20:36:46
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
On Wed, Sep 18, 2013 at 5:50 PM, Viktor Dukhovni
This is no longer the case. Best Practice is now considered to be to use
name constraints but not mark them critical.
This is explicitly a violation of PKIX which insists that a name constraint
extension be marked critical. Which makes it impossible to use name
constraints as they will break in Safari and a few other browsers.
The refusal to make the obvious change is either because people do not
understand the meaning of the critical bit or the result of some of that
$250 million being felt in the PKIX group. As I pointed out at RSA, the use
of name constraints might well have prevented the FLAME attack working.

@_date: 2013-09-19 09:54:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA equivalent key length/strength 
The most corrosive thing about the whole affair is the distrust it has sewn.
I know a lot of ex-NSA folk and none of them has ever once asked me to drop
a backdoor. And I have worked very closely with a lot of government
Your model is probably wrong. Rather than going out to a certain crypto
vendor and asking them to drop a backdoor, I think they choose the vendor
on the basis that they have a disposition to a certain approach and then
they point out that given that they have a whole crypto suite based on EC
wouldn't it be cool to have an EC based random number generator.
I think that the same happens in IETF. I don't think it very likely Randy
Bush was bought off by the NSA when he blocked deployment of DNSSEC for ten
years by killing OPT-IN. But I suspect that a bunch of folk were whispering
in his ear that he needed to be strong and resist what was obviously a
blatant attempt at commercial sabotage etc. etc.
I certainly think that the NSA is behind the attempt to keep the Internet
under US control via ICANN which is to all intents a quango controlled by
the US government. For example, ensuring that the US has the ability to
impose a digital blockade by dropping a country code TLD out of the root.
Right now that is a feeble threat because ICANN would be over in a minute
if they tried. But deployment of DNSSEC will give them the power to do that
and make it stick (and no, the key share holders cannot override the veto,
the shares don't work without the key hardware).
A while back I proposed a scheme based on a quorum signing proposal that
would give countries like China and Brazil the ability to assure themselves
that they were not subjected to the threat of future US capture. I have
also proposed that countries have a block of IPv6 and BGP-AS space assigned
as a 'Sovereign Reserve'. Each country would get a /32 which is more than
enough to allow them to ensure that an artificial shortage of IPv6
addresses can't be used as a blockade. If there are government folk reading
this list who are interested I can show them how to do it without waiting
on permission from anyone.

@_date: 2013-09-19 13:15:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Cryptographic mailto: URI 
I am in mid design here but I think I might have something of interest.
Let us say I want to send an email to alice at example.com securely.
Now obviously (to me anyway) we can't teach more than a small fraction of
the net to use any identifier other than the traditional email address.
So we need some sort of directory infrastructure to allow discovery of
those email addresses and it would be good to be able to reuse existing
directories if at all possible.
But how do we insert the email addresses into a directory like LinkedIn?
Well you can add a URI into your account. So what if the URI is of the form:
ppid:alice at example.com:example.net:
alice at example.com is Alice's email address for secure communications
example.net is a server which will resolve the reference by means of a
simple HTTP query using the pattern "Syd...NWM" is the Base64 hash of OID-SHA256 + SHA256(X)
X is a public key that signs a document (probably JSON) that specifies:
* X
* Alice's certificate(s)
* Alice's email receipt policy whether to always encrypt, what message
formats are supported
* links to whatever additional advice information might help convince a
relying party the key is genuine like a CT log.
* reliance policy (is this key for public use or restricted)
* reporting policy (for future changes)
So to use this as a mechanism for ghetto key distribution receivers would
add the URI into their account. Or let their PKI discovery agent do it for
Senders would enable their PKI discovery agent to access their LinkedIn
It would slurp down the data once a day (say) and keep it in a cache for
use by that user alone unless it is marked public when any user of the PKI
discovery agent can make use of it.
It would attempt to validate the information obtained, possibly resulting
in a report if it detected a change in a previously registered key that had
not been properly countersigned by the old.
The validated info would then be used to encrypt the outbound mail
according to the specified policy.
1) This is only about key discovery, not validation.
2) Better to send email encrypted under a key that is not validated than in
the clear.
3) A MUA should offer the option 'force encryption' however. And in that
case it would barf if the key provided didn't meet the validation criteria.

@_date: 2013-09-19 18:09:25
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
The key to make them work is to NOT follow the IETF standard and to NOT
mark the extension critical.
If the extension is marked critical as RFC 5280 demands then the
certificates will break in Safari (and very old versions of some other top
tier browsers).
If the extension is not marked critical as CABForum and Mozilla recommend
then nothing breaks and the certificate chain will be correctly processed
by every current edition of every top tier browser apart from Safari.
The peculiar insistence that the extension be marked critical despite the
obvious fact that it breaks stuff is one of the areas where I suspect NSA

@_date: 2013-09-19 18:21:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Bear in mind that securing financial transactions is exactly what we
designed the WebPKI to do and it works very well at that.
Criminals circumvent the WebPKI rather than trying to defeat it. If they
did start breaking the WebPKI then we can change it and do something
But financial transactions are easier than protecting the privacy of
political speech because it is only money that is at stake. The criminals
are not interested in spending $X to steal $0.5X. We can do other stuff to
raise the cost of attack if it turns out we need to do that.
So I think what we are going to want is more than one trust model depending
on the context and an email security scheme has to support several.
If we want this to be a global infrastructure we have 2.4 billion users to
support. If we spend $0.01 per user on support, that is $24 million. It is
likely to be a lot more than that per user.
Enabling commercial applications of the security infrastructure is
essential if we are to achieve deployment. If the commercial users of email
can make a profit from it then we have at least a chance to co-opt them to
encourage their customers to get securely connected.
One of the reasons the Web took off like it did in 1995 was that Microsoft
and AOL were both spending hundreds of millions of dollars advertising the
benefits to potential users. Bank America, PayPal etc are potential allies

@_date: 2013-09-20 08:55:48
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Cryptographic mailto: URI 
Interesting, though I suspect this is attempting to meet different trust
requirements than I am.
A couple of days ago I spoke with someone well known here who has seen the
Snowden files. His take was that when the NSA has a choice of doing A or B
it always does both.
I think we need to adopt the same approach but in a way that lets all the
various approaches work together. It should not be necessary for me to
install five plug ins into Thunderbird to support five different flavors of
researchy trust infrastructure.
A better approach is to have one plug-in, or better native support for a
connector to a Web Service that can then perform all the researchy trust
infrastructure navigation on my behalf. The Web service can be shared
between users and when there is a new researchy trust infrastructure
proposed, all that is necessary to add it into the mix is to extend the Web
Service and everyone using it can try out the new scheme and see if it is
This approach does introduce the risk that the web service might be
compromised. Particularly if the client is unable to perform at least some
degree of local validation on the keys. But the long term expectation would
be that support for trust infrastructures that prove to be stable, widely
used, and effective will eventually migrate into the client.
At this point the experimental research question should be 'is this trust
infrastructure practical'. We can get a very good idea of the security
properties of the system by looking at how people use it and doing math.

@_date: 2013-09-20 14:59:46
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Specification: Prism Proof Email 
We need an email security infrastructure and recent events demonstrate that
the infrastructure we develop needs to be proof against PRISM-class attacks.
By PRISM-class I mean an attack that attempts pervasive surveillance with
budgets in excess of $100 million rather than the PRISM program in
Neither OpenPGP nor S/MIME is capable of providing protection against this
class of attack because they are not widely enough used. We can only hope
for these to be useful if at least 5% of Internet users start sending mail
But while the legacy protocols are not sufficient, 95% of the existing work
is fine and does not need to be repeated although there may be some details
of execution that can be improved.
The part that is going to need new research is in the area of trust models.
As someone who has seen the documents said to me this week, given a choice
between A and B, the NSA does both. We have to do the same. Rather than
have a pointless argument about whether Web 'o Trust or PKIX is the way to
go, let everyone do both. Let people get a certificate from a CA and then
get it endorsed by their peers: belt and braces.
The idea in this draft is to split up the problem space so that people who
know email clients can write code to support any of the research ideas that
might be proposed and any of the research ideas can be used with any of the
mail clients that have been enabled.
The draft is to be found at:

@_date: 2013-09-20 17:09:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] InfoRequest: How to configure email clients to 
Working on Prism Proof email, I could use information on how to configure
various email clients to support S/MIME decryption using a previously
generated key package.
While descriptions of how the user can configure S/MIME would be nice, what
I am really after is information on the internals so that it would be
possible for a tool to do this configuration for the user automatically.
Info on where the account configuration data is stored would also be very
The end goal here is a tool that will generate and manage private keys and
configure their email clients so that they can read mail encrypted under
If we have the 'how to read encrypted mail well' side of things sorted
using this tool that leaves only the 'how to send encrypted mail well' as a
research problem.

@_date: 2013-09-22 20:09:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
So we think there is 'some kind' of backdoor in a random number generator.
One question is how the EC math might make that possible. Another is how
might the door be opened.
I was thinking about this and it occurred to me that it is fairly easy to
get a public SSL server to provide a client with a session key - just ask
to start a session.
Which suggests that maybe the backdoor is of the form that if you know
nonce i, and the private key to the backdoor, that reduces the search space
for finding nonce i+1.
Or maybe there is some sort of scheme where you get a lot of nonces from
the random number generator, tens of thousands and that allows the seed to
be unearthed.
Either way, the question is how to stop this side channel attack. One
simple way would be to encrypt the nonces from the RNG under a secret key
generated in some other fashion.
nonce = E (R, k)
Or hashing the RNG output and XORing with it
nonce = r  XOR H (r)
Either way, there is an extra crypto system in the way that has to be
broken if a random number generator turns out to have some sort of
relationship between sequential outputs.

@_date: 2013-09-24 19:53:34
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
There are three ways a RNG can fail
1) Insufficient randomness in the input
2) Losing randomness as a result of the random transformation
3) Leaking bits through an intentional or unintentional side channel
What I was concerned about in the above was (3).
I prefer the hashing approaches. While it is possible that there is a
matched set of weaknesses, I find that implausible.

@_date: 2013-09-24 19:58:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA equivalent key length/strength 
On Sun, Sep 22, 2013 at 2:00 PM, Stephen Farrell
And the problem appears to be compounded by dofus legacy implementations
that don't support PFS greater than 1024 bits. This comes from a
misunderstanding that DH keysizes only need to be half the RSA length.
So to go above 1024 bits PFS we have to either
1) Wait for all the servers to upgrade (i.e. never do it because the won't
2) Introduce a new cipher suite ID for 'yes we really do PFS at 2048 bits
or above'.
I suggest (2)

@_date: 2013-09-26 19:54:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Quite, who on earth thought DER encoding was necessary or anything other
than incredible stupidity?
I have yet to see an example of code in the wild that takes a binary data
structure, strips it apart and then attempts to reassemble it to pass to
another program to perform a signature check. Yet every time we go through
a signature format development exercise the folk who demand
canonicalization always seem to win.
DER is particularly evil as it requires either the data structures to be
assembled in the reverse order or a very complex tracking of the sizes of
the data objects or horribly inefficient code. But XML signature just ended
up broken.
[Just found your ASN.1 dump tool and using it to debug my C# ASN.1 encoder,
OK so maybe ASN.1 is not terrible if I can put together a compiler in four
days but I am not using the Assanine 1 schema syntax and I am using my
personal toolchain]
I have a theory that the NSA stooges are not the technical folk. Why on
earth would a world class expert want to spend their time playing silly
games sabotaging specs when they could have much more fun working inside
the NSA at Fort Meade or building stuff.
What I would do is to take a person who is a technical wannabe and provide
him with technical support and tell him to try to wheedle positions as a
document editor. Extra points if they manage to discourage participation by
folk with solid technical chops.
We saw something of the sort during the anti-spam efforts. I was sure at
the time that the spammers had folk paid to make the discussions as
acrimonious as possible.

@_date: 2013-09-27 11:23:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA equivalent key length/strength 
Actually, it turns out that the problem is that the client croaks if the
server tries to use a key size that is bigger than it can handle. Which
means that there is no practical way to address it server side within the
current specs.
There is no reason to use DH longer than the key size in the certificate
and no reason to use a shorter DH size either.
Most cryptolibraries have a hard coded limit at 4096 bits and there are
diminishing returns to going above 2048. Going from 4096 to 8192 bits only
increases the work factor by a very small amount and they are really slow
which means we end up with DoS considerations.
We really need to move to EC above RSA. Only it is going to be a little
while before we work out which parts have been contaminated by NSA
interference and which parts are safe from patent litigation. RIM looks set
to collapse with or without the private equity move. The company will be
bought with borrowed money and the buyers will use the remaining cash to
pay themselves a dividend. Mitt Romney showed us how that works.
We might possibly get lucky and the patents get bought out by a white
knight. But all the mobile platform providers are in patent disputes right
now and I can't see it likely someone will plonk down $200 million for a
bunch of patents and then make the crown jewels open.
Problem with the NSA is that its Jekyll and Hyde. There is the good side
trying to improve security and the dark side trying to break it. Which side
did the push for EC come from?

@_date: 2014-04-13 08:53:01
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I stopped using Java when Sun sued Microsoft. Microsoft does actually
do the right thing in .NET:
The data is encrypted and is disposed of properly.
In C my coding rule is invariably, that I declare the only memory
allocator and this zeros all memory on acquisition and release. I
don't allow calls to malloc, strncmp and a bunch of other library
functions by disabling them with macros setting them to make a compile
 restricted_calls
 malloc(x) malloc_not_permitted()
 free(x) free_not_permitted()
This approach does allow the functions to be used via pointers. But
code that does that is rarely a problem since the main reason to use
the pointers would be to enable the memory manager to be dropped out.

@_date: 2014-04-16 00:37:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] I don't get it. 
Human or machine?
Certainly some good coding standards would help. Like always zero all
memory before returning it in malloc, always zero memory with critical
data before freeing it.
But we have a big problem with C and it is not going to go away with
any of the current upgrade paths people are looking at. Java and C#
are much better but only when all the code is managed code. And they
both require runtime baggage that comes with a truckload of IP issues.
Microsoft .NET is OK but I can't really ship code that depends on
C++ is utterly ludicrous, it is vastly worse than C in every respect.
It comes with a large amount of baggage, the code is incomprehensible
when people actually try to use the botched object system.
Which is why I have been working in C and C# for quite a while. But I
still really want a language that allows me to make unencumbered
standalone executable files. One option that might actually be viable
as a consensus for that specific purpose would be Objective C. It adds
an object model to C without the nonsense and incomprehensibility of

@_date: 2014-04-16 07:00:18
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] I don't get it. 
Seems like the incentives for writing OpenSource crypto are rather
poor. If you do good work few people know, if you make one slip people
think you might be an NSA spy.
Also rather poor that people don't realize that its not just the NSA
who operate large scale PRISM like programs. Other governments had
much better motives than the US.

@_date: 2014-04-16 18:24:55
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
There is a C# class that does the same thing.
I am not sure what the story is for keys, I think the assumption is
that they will be implemented in a CAPI library. But there is a
similar problem in C, unless you really understand what is going on,
keys get passed on the stack all the time.
But I need to find out soon...

@_date: 2014-04-17 20:10:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Cue the blamestorming 
There but for the grace of ...
Remember DigiNotar and the proposals made then for what to do to the
next CA to screw up? The plot writers for Game of Thrones could have
got some ideas there.
Hasn't taken long for people to start in on the same type of cheap
talk on OpenSSL.
We have some big problems here. And the fact that the US govt. which
we thought was making a significant contribution to COMSEC through the
NSA turns out to have spent less than 0.5% of its budget on COMSEC
standards related activities and most of that went into sabotage.
So I have been looking into some structural alternatives. We need
resources. But more importantly we need to know how to apply them.
Right now I have no doubt that we can work out a solution for OpenSSL.
But that is not the only underfunded software project that has a major
impact on a critical resource.
We have to look at all the points where we might be vulnerable and fix them.
We also need to bring government resources to bear because there are
some things that are really hard to achieve in either a commercial or
a volunteer model.
The WebPKI was designed to support multiple CAs for a reason. Having
multiple CAs does create an incentive for each to keep their
competitors honest. So now we play that game with governments. We
don't need 50 but we need more than the US or the US plus UK and

@_date: 2014-04-18 14:28:52
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Cue the blamestorming 
We also need a documentation fix. Bad documentation is a canary for bad code.
This might have been fixed but we all assumed that there was no
problem. Nobody in OpenSSL was telling us they needed help.
Problem is that it is really impossible for an organization to address
two incompatible goals. We now know that SIGINT was the overwhelming
priority to the effective exclusion of all COMSEC.
I was just in Turkey for the opening of the new CyberDefense program
at METU in Ankara. Comodo is backing the program but commercial
entities can't back such programs without government support. It has
to be a partnership.
I can back individual projects but they will inevitably be seen as
backing one particular commercial position. We need government funding
as well.
If I don't like what is on offer from one, I'll pick another.

@_date: 2014-04-18 18:09:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Just turn off C-optimization? 
[Merging the C threads]
Perhaps we could reduce errors by simply turning of optimization in
the C compilers? If the optimizer does not do the right thing then I
don't want it used. And the semantics of C make it next to impossible
to get the optimizer right.
Since we are doing security code I would much rather the code was
right than fast.
Alternatively writing a set of string handling, memory management etc
routines that can be compiled without the optimizer and linked from
the rest of the crypto code.
I am trying to work out how we get to somewhere we can work
productively in a language that does not have a run time dependency.
Right now that means C (and possibly Objective C?). I can't use a
language like Erlang or C# to produce code that I am going to link as
a library in other languages like Perl, Erlang, etc.

@_date: 2014-04-20 21:27:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Cue the blamestorming 
The US is not the only government I talk to.
It is quite possible for a US general to claim that they are going to
create a force of 'cyber-warriors' who will beat the rest of the
world. They are talking nonsense but nobody is going to tell them that
they are talking nonsense.
Thats the problem with being a superpower: Hubris doesn't look like hubris.

@_date: 2014-04-25 08:34:55
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I have always considered it rather odd that
1) ASN.1 BER has an inherently unsafe, difficult to implement encoding option
2) X.509v3 requires use of the unsafe option
3) ASN.1 is 1.NSA backwards
What I mean by unsafe is the following, X.509 DER requires the use of
definite length encodings so that if I have a sequence nested inside a
sequence the bytes on the wire will be something like:
where  =   result:     Which all looks very sensible until we start on the fact that the
length encodings in Assanine One are themselves variable length, so
you can't calculate the  without having first finalized
. It is not possible to emit ASN.1 using a simple recursive
descent scheme unless you construct the output values in reverse, from
the last item to the first.
There is also a subtle opportunity here for an interesting type of
bug, just like the heartbleed bug:
What if Length2 is given as greater than the length of value2?
Unless the decoding logic is just right, the decoder can end up with a
buffer overrun. And getting the logic right requires a lot of
discipline. A lot of decoders will blindly accept the input as valid
and the world is lost...

@_date: 2014-04-26 11:31:34
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
That is what I thought was the reason, but its actually subtly
different. JSON-B is almost as dense as ASN.1 BER. JSON-C is actually
better and unlike ASN.1 it is a regular encoding that does not depend
on the schema.
The problem with ASN.1 is that the schema is designed to describe the
encoding of the bits on the wire rather than the bits on the wire
being a representation of the schema.
That is why they have the weird stuff like implicit and explicit
tagging, they make sense only if you are thinking about how to lay out
bits on the wire.
So does IP and that is where we had a whole slew of issues for IPv4
and will no doubt have a whole series of issues for IPv6 which has a
huge number of options
ASN.1 does have a safe way to represent structures, the indefinite
length encoding. Instead of giving the length of a nested structure in
bytes, the end of the structure is specified as a terminal marker.
As for skipping ahead in a structure using those length markers, I
have seen lots of code like that. I generally scrub it as soon as
possible because I know that it is never going to be correct and even
if it is correct it is going to be vulnerable to illegal inputs
because the act of skipping means that the data isn't being checked.
After a lot of bad experiences, the only approach to parsing data I
accept is to take the input data, perform authentication checks,
reduce the data set to exclude all the data that has not been
authenticated, parse the input stream in its entirety and return the
parsed structure to the dispatcher. As you point out, CPU is cheap.
It means I have to pretty much encode the data just to find out the
length. I can't take shortcuts.
Looks like you are going to spend all your time copying data from one
buffer to another.
The simple way to encode ASN.1 is to perform a simple recursive
descent but emit the data values on the way out rather than the way
So if you have A ( B (C, D))), the first data that you emit is D, then
C, then B, then A. The advantage of this is that you can traverse the
data in one pass with a simple stack and the data length you need is
always on the top of the stack.
[This actually reminds me of another good practice for coding
encoders: don't use recursion. Or rather code the recursion explicitly
rather than using subroutine calls. The reason for this being that use
of subroutine calls lays you open to attacks. Though this is something
I haven't applied in my own code yet.]
The number of programmers who understand the value of generators is
very small. Maybe 5% and the number who would think to write one, very
One of the things a lot of people don't get about UNIX is that the
fundamental approach was toolbuilding, see Software Tools by Kernighan
and Plaugher to see the real heart of the UNIX architecture.
Funny you should mention that...

@_date: 2014-04-27 18:31:48
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I can see something like that going on right now with the DNS Privacy
work. This starts from a requirement for privacy and so one approach
is likely to be encrypting the connection between the client and the
There are various approaches that could be used there but these boil down to:
1) Use TLS (and thus DNS over TCP)
2) Use DTLS (and stick to UDP)
3) Develop a new security enhancement.
Now the fact that TLS has been going on for 20 years leads to the
assertion that TLS is the simplest approach. BUT TLS is optimized for
a totally different application and performance is going to be poor
because every TCP connection is now going to require two TCP startup
sequences, one for the DNS transaction and another for the connection
to the discovered endpoint.
OK so what about DTLS, it is built on TLS right? Well sorta. It has
all the complexity of TLS and 20 years of barnacles clinging to the
hull. It is built on a PKIX trust mechanism with DNS grafted in badly.
And it is actually designed around applications like chat and gaming
where people really want TCP connectivity but are using UDP to work
round firewalls.
work for an application where they are not suited is going to be
vastly more complex than designing something that is purpose built.
But you can imagine what the arguments are going to be like 'go with
what we understand'. Which actually means 'I don't understand anything
about this stuff so I want to stick with what exists'.
Its only the message packing mechanism that is a poor match to DNS. We
can easily establish something that is essentially a shared secret
bound to a kerberos ticket like identifier and use that to enhance the
But it gets worse. The original framing for the problem is encryption.
But my objective is actually authentication. Because for me
authentication is worth ten times encryption. I am far more concerned
about making sure that Erdogan in Turkey can't block Twitter than
stopping him seeing the DNS traffic as the IP traffic that follows
will show who is going to Twitter. Being able to guarantee a link to a
resolution and discovery service you trust should be the foundation to
all other security, including the ability to hook in censorship
busting transports like TOR.
If we are going to encrypt packets then we need to do a key exchange
and establish a shared secret. Once we have a shared secret, encrypt
and authenticate is pretty much the same complexity and difficulty as
encrypt only. Adding a MAC to a packet is not a lot of effort.
But I know what the pushback is going to be; MAKE IT SIMPLE! Because
the last problem that arose in the USA was privacy that is the only
problem we are going to allow to be discussed. Any proposals to do any
other security will be rejected as OUT OF SCOPE. Because stopping what
the NSA might do is much more important than stopping what we know is
going on in Iran etc.
And then when the fact that we need authentication becomes obvious we
will go through a process of defining extensions to the core to allow
for authentication. And so what should have been mandatory to
implement is now an option. And options never work as well as
something that is required.
One of the reasons that PKIX is so complicated is that at every step
of the way there was someone insisting that the spec be made simpler
than the problem. And so we ended up with CRLs and OCSP rather than
one mechanism. And later SCVP was added on top.
With SAML we got to put together a core that was vastly more flexible
and capable than was necessary to solve authentication and
authorization. As a result SAML has remained pretty simple because
there was no need to invent ad-hoc work arounds for stuff.

@_date: 2014-12-16 09:10:00
@_author: Phill 
@_subject: [Cryptography] Sony "root" certificates exposed 
We don?t know if there was a breach yet.
The certificates aren?t published but they are not secret. They are used on the public network and were visible before the attack.
If the private key was compromised or the CA was breached and caused to mis-issue, that is a completely different ball game.
It isn?t unusual for hackers to tell reporters that they have committed more serious attacks than they have. We once had a hardware failure at  I know it was a hardware failure because I had the sysop call me to ask if we had a spare router he could borrow (this was back in the days before you could walk into a store and buy one). In the meantime a gang of pro-Serb hactivists knocked over a few other sites then called a journalist at wired claiming they had hacked the whitehouse.

@_date: 2014-02-01 20:33:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer 
Why assume that its the NSA/GCHQ?
When I got stopped three times by UK customs on one trip during the crypto
wars it was pretty obvious what was going on. But this incident could have
been due to Iran, Israel, Russia, China (in no particular order) and there
might well be more countries getting in on the pervasive intercept party.
This is not about stopping the NSA. The NSA wanabees are far more numerous
and likely just as well resourced. They won't have as much cash but they
will use what they have at least ten times more effectively.
We do have a model for protecting Web sites that works pretty well called
PCI. That is the scheme that the credit card companies developed to protect
their assets when they are exposed online. PCI is supported by numerous
tools and services that provide compliance checking. It isn't perfect but
it is a known starting point.
What we need is PCI for social media sites and for email providers. It does
not have to be perfect and it won't be. But it will be a start. And unlike
the credit card companies we have a lot more ability to change our

@_date: 2014-02-07 11:14:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
I have a solution to the random number generator problem that can be built
for about $50 and is completely verifiable.
1 Web Cam capable of focusing at a distance of 6" or less (can use a
correcting lens if necessary) neither resolution nor quality is critical.
1 Conical flask as used in chemistry, about 6" high.
1 packet multicolored candies (smarties, skittles)
1) Dump the candies in the flask, shake, take a photograph with the camera,
take the SHA256 hash of the photograph to acquire the 256 bit seed.
2) Shake flask vigorously, repeat step 1 as needed
3) Eat the candies
I'll do an instructable video maybe.

@_date: 2014-02-10 08:03:41
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] BitCoin bug reported 
The MtGox people are claiming that the reason they have been offline is a
bug in the BitCoin protocol:
Does anyone with deep knowledge of the protocol know if this is a credible
I am getting mighty fed up of a group of people who mouth off constantly
that we can't trust the government but take great offense and try to bully
anyone who asks questions about the scheme.
There are some people in the community that I trust completely. But some of
the names involved in the BitCoin world are people that I know I can;t
trust and neither can anyone else. They have lied to and cheated me, they
have lied to and cheated others.
They are currently using as much electricity as the nation of Cyprus. This
is way beyond a science project. If it continues then in a few years
BitCoin will be taking all the electricity generated by the Three Gorges
Dam project. Which is why I suspect China will soon be introducing condign
punishments for mining.
Its not a Ponzi scheme, but the design of the blockchain it pretty
interesting. Since the upper limit on the value of a bitcoin is set by the
cost of electricity to mine it, the value increases as the difficulty of
mining increases. Since that is exponential, there is a built in bubble

@_date: 2014-02-13 09:04:12
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
The latest attempt to plant a replacement for Silk road has lasted only 9
days. It seems that the authorities are now looking to stamp out any
copycats before they get a toe hold.
What I find rather confusing is the idea that hiding a service rather than
a client is feasible. Tor is vulnerable to traffic analysis as the Harvard
bomb threat proved. The student responsible was discovered because his
IP/MAC address was one of only five using Tor on Harvard campus at the
Tor is very good at preventing the authorities from seeing which sites a
person in Iran is contacting outside Iran. So it is a very powerful
anti-censorship tool. But use of Tor for criminal purposes is an obvious
concern for the authorities and it is fairly easy for them to set up Tor
nodes. So I have always assumed that at least 50% of the nodes in Tor are
operated by LE and intel agencies. They may not be able to see the actual
traffic but they can certainly see IP addresses and an IP address only has
meaning if there are BGP routes pointing packets towards it.
So from a technical point of view it seems to me that the 'dark net' cannot
possibly exist but there seem to be many people betting they can stay out
of jail on the belief it does.
Is this just an example of wishful thinking or is there something else at

@_date: 2014-02-13 22:35:11
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
1) The attacker controls multiple rendezvous points and
2) The attacker controls multiple clients, using them to make contact
attempts and
3) Traffic to the hidden service with the drug site is an appreciable
proportion of the total hidden service traffic
Then, a timing attack seems very likely to reveal the IP address of the
exit node for the hidden service which can then be unrolled in turn.

@_date: 2014-02-14 08:52:52
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
There is a similar risk in that the mining guilds have a network effect and
it is better to be part of the biggest guild with the best tools. So the
Ukrainian guild recently had to voluntarily shed members to avoid getting
up to 51%.
But there is nothing to stop a collusion under the table between the
miners. And given the interest organized crime has taken in mining, that
collusion can be coerced. We lost a couple of guys who are presumed
murdered by the Russian mob a couple of years back and Ulrich is on charges
of attempted murder. So a scenario in which the mob works out who controls
the machines coordinating the mining rigs and literally puts a gun to their
kids heads if they don't help them steal a few tens of millions seems very
likely to me.
Another area where cheating looks possible is in these 'proven secure'
bitcoin gambling sites.
Most of the sites tell you that they aren't cheating and for most
Bitcoiners, that is enough. Though some are careful enough to look at the
'I'm not cheating page' where you can press buttons that tell you the site
isn't cheating, honestly. Or if you are really paranoid you can download an
open source program provided by the site owner and run it. And that will
tell you that the site owner isn't cheating.
Hows that for confidence building?
I can't see any specifications or explanations on the sites I have visited
so I can't see if the protocols are vulnerable to other forms of attack. I
am pretty sure that there are attacks that are going to be possible if the
site owner colludes with the miners. One easy way to cheat would be to only
include losing bets in the blockchain. Which would be visible in the
results of course. Unless the site owner made sure to only cash out by
making an equal number of known winning bets.
The idea of getting close to a currency being used by people with lots of
guns and a history of using them on other people seems like a very bad plan
to me...

@_date: 2014-02-14 11:19:05
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
Again this is raising the cost of the attack, not preventing the really
determined attacker.
Operating a low latency hidden service means allowing other people to
direct packets in your direction. There are certainly ways of mitigating
the risk of dox-ing the hidden server. But the effectiveness of those
controls are going to depend on how visible the service is and how
determined an attacker is to disclose its location.
Underestimating the determination of the authorities to locate and destroy
online drugs marketplaces seems to be a habit of these people. At this
point there are more markets on the list of 'failed/scam' sites than are
operating on the  list.
It could be that they are all being found because they are making stupid
mistakes like sending email which has to go outside the Tor system
becauseof the spam controls. But it wouldn't surprise me if we later find
that there are tens of thousands of NSA/GCHQ run nodes. The Snowden papers
we have that express concern at the difficulty of Tor intercepts are rather
old. Knowing that the NSA could not solve a problem due to lack of
resources three years ago would make me conclude they now have the
necessary resources rather than it isn't a problem.

@_date: 2014-02-14 11:33:20
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] BitCoin bug reported 
The price of something is the amount someone will charge, the value is what
someone is willing to pay.
Yes, but the speculators are not going to pay significantly more than the
expected cost of mining the coins in the future.
So there is a positive feedback circuit there, but a gradual one.
And wishful thinking is a large part in every bubble.
Ponzi and Madoff didn't actually sell their schemes themselves. They let
everyone else sell themselves on it. BitCoin is not a Ponzi scheme but it
does seem to be a Ponzi delusion.  The price is going up for no other
reason than the 'investors' think it will continue to go up and so they
BitCoin is super deflationary, the value of goods relative to BitCoin has
dropped to 1% of their original value in a year. In an economy like that,
anyone spending money is an idiot because their money will be worth much
more in a short period of time.
But for the same reason, nobody is going to hire anyone to work in BitCoin
because the cost of their labor will go up exponentially over the course of
the work. Instead of a contract being the price of a new car as expected
when it was signed, it is the price of a house or a private jet by the time
payment is due.
That just isn't a viable economy.

@_date: 2014-02-15 13:02:19
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
I was certainly planning to use post processing!
Basically thats what shoving the data through SHA512 is about. But my
interest here is in having a system that is verifiable end-to-end.
Relying on circuits etc has the problem that it is very hard to know is one
is measuring noise or bias. I prefer to be measuring macro values that I
can check independently and verify are part of the source.
If I take a 5MP image and squirt it through SHA2-512, I am going to take in
randomness at both the macro and the micro levels. Instead of measuring the
last bit on individual pixels, if the resolution of the camera is fine, it
is essentially going to be unguessable as to whether the pixel is red or
green or some other color.
Out of 5MP there are going to be plenty of sites that might be one side of
an edge or the other. There are going to be many edge sites that are
512 bits is 64 bytes, which is a tiny fraction of a 5MP JPEG size. I don't
think it very likely that there is an insufficient amount of randomness in
the image. We can at any rate measure and see...
The sort of thing I was thinking of was using a camera on a Raspberry Pi
($40) to capture two frames of video during the randomness acquisition. It
would store the data for each and provide a HMAC of both images under a
specified key and a thumbprint of both images. The auditor could then
request that the device reveal either the first or the second image. It
would then provide the input to the hash function of the chosen image and
delete the other.
The problem then becomes how to prove that the random number is calculated
from both inputs... which is actually quite hard to do. A verifiably read
once device would be nice.

@_date: 2014-02-15 18:27:34
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
Let us assume for the sake of argument that BitCoin survives to the no more
mining phase.
At this point there is only an incentive to mine if the return justifies
the cost of mining. Which means that miners have to be making more money in
transaction fees than they are paying for in electricity. Which means that
the difficulty of mining may go up or down depending on how many miners
stay in the game.
This then creates a situation where it is very likely that there is a large
amount of mining hardware sitting idle because it isn't economic to run it
for the transaction fees on offer. If only 10% of the mining rigs built are
running it becomes quite easy for a 51% attack to work because it is now
only an 11% attack.
Similar problems follow any large scale slide in the price of BitCoin. In
the short run the price of coin is set by supply and demand but in the
longer term people will buy rigs if they think they will make money (a much
weaker condition than actually making money). Once the rigs are bought they
will run if they are profitable.
So if we run a simulation based on these assumptions it is easy to put the
system into a positive feedback situation. The price of coin goes up, this
creates an incentive to mine, more rigs are bought, the difficulty of
mining goes up.
But if the price of bitcoin falls sharply we end up with something that
looks very similar to the zero lower bound problem. Once a mining rig is
bought, the capacity of the BitCoin network is permanently increased but
the capacity is only used if it is profitable. So we can come into a
situation where 51% type attack becomes feasible because it is only
necessary to have 51% of the active capacity and that becomes much easier
when there is idle capacity.
What I have never worked out is why join the blockchain to the mining at
all. As Ben Laurie points out, it is wading through treacle.
The blockchain is just an adaptation of the Surety/Harber/Stornetta scheme.
Forget the mining part for a moment, lets imagine that there are 100
independent notaries and every ten minutes they each produce an output
value that is based on local inputs plus the outputs of all the other
It is not possible for a notary to defect in such a scheme unless every
other notary also defects.
There are no 51% attacks possible, the cost of running the network is
acceptable. It is possible for someone to check if they have finality in a
The key difference would be that the transaction chain would have to be
maintained by parties that form a consortium for that purpose.
If there was going to be a PoW component then it would be the blocks go to
whoever gets the lowest hash output in a given computation chunk and has
the hash notarized in time.

@_date: 2014-02-15 23:46:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
That was my conclusion.
I actually started with rather more elaborate steampunk devices that had
dice with different colored faces. These would be machine read somehow.
Then in the next iteration I had a grid of the dice being read at once.
Then I realized that I could read them with a camera.
Then I replaced the dice with colored balls.
I hadn't seen lavarand back in the day, but I had the idea after finding
one while looking for the flask to put the m&ms in...
What got me started on this course is that I don't like trusting the
assumed lack of bias in very small measurements. That is not repeatable or
measurable or auditable. I want to see devices that I can check and know
are functioning right.
Adding a camera to a Raspberry Pi is $40 which is a lot less than almost
any custom circuit is going to be.

@_date: 2014-02-16 09:23:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
Some MIT students have been targetted by a NJ Prosecutor over Tidbit, a
mine bitcoin for content scheme...
Of course it is easier to go after a bunch of students than the Winkelvoss
twins and their heavily lawyered-up BitCoin fund. But not if the students
are from MIT.
Only reason to choose MIT is if someone wants to get embroiled in a messy
and very public lawsuit.
You are underestimating the amount of electricity required. BitCoin is
already using vast amounts of electricity. According to one site as much as
the Island of Cyprus. People could spend $10,000 of their money in cash to
defend the stability of BitCoin but now you are relying on a social compact.
If I am going to sell a house for BitCoin, I want to have more certainty
that the money has been transferred than some folk might do some mining out
of the goodness of their hearts to stop an attack.
Blockchain is the new bearer.

@_date: 2014-02-17 13:42:30
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encodings for crypto 
Responding to Ted on ASN.1 PER...
Yes, ASN.1 is horribly complex and I say this as someone who just
implemented a JSON encoder and an ASN.1 encoder in the same month. The JSON
encoder and decoder took me only a few hours in C less than the time it
would take me to learn someone's API. The ASN.1 encoder took three weeks.
And that was just for DER.
My view is that we should try to converge on one data model for encoding
data and the model that the industry has chosen is JSON. We can use JSON as
an interchange format for any data that we can encode in ASN.1 or XML but
it is a lot simpler.
There are however some real problems with using a text based format for
crypto. In particular:
* BASE64 encoding of binary data increases the size by 33% per pass. Which
is a pain.
* Escaped encoding of text strings is inefficient.
* Decimal encoding of binary floating point values introduces conversion
* Parsing of text based tags is inefficient.
There was some effort to rectify this but it was a private effort, not an
IETF effort. So the principals felt free to only consider their
requirements telling other people to shove off. So an opportunity was
I think we can do better:
There are three encodings:
JSON-B 'Binary' Adds binary encodings for numbers and length delineated
encodings for strings and binary data
JSON-C 'Compression' Adds tag compression to JSON-B
JSON-D 'Data' Adds in additional data formats (e.g. 128 bit and 80 bit
floats) that are used in scientific calculations.
The basic idea here is that instead of an alternative encoding to JSON, we
make use of code points that are unused in JSON (i.e. all the code points
above 127) to introduce binary sequences. This has a number of important
1) It is only necessary to implement one decoder. JSON is a strict subset
of JSON-B which is a strict subset of JSON-C which is a strict subset of
2) It is possible to insert JSON encoded data into a JSON-B sequence
without needing to re-encode.
Encoding formats can be a security nightmare. Especially when there are
nested structures specified by length. I don't consider ASN.1 BER to be
safe. Poor implementation leads to smashing the stack.
Unfortunately JSON supporters tend to be folk with a scripting mentality
rather than a structured type checking mentality so at this point we don't
even have a schema language for JSON (although to be fair this is in part
because ASN.1 schema and XML Schema are so very awful).
As I see it, there is only one way to parse a security sensitive data
1) Read in the bytes, abort if there are too many
2) Authenticate the bytes (Nope, I don't believe in canonicalization.)
3) Parse the data structure to internal representation, discarding all data
that is not understood.
4) Validate the data for consistency (if possible)
5) Pass to the security sensitive module to process.
The BitCoin bug looks to me to be a bug caused by misplaced faith in the
power of canonicalization and assuming a structure to be in canonical form
when it isn't rather than a lack of c18n.
Doing authentication at the byte level before parsing is robust and
insulates the code from parser errors. It is also a good defense against
injection errors if we enforce a rule that every command is independently
authenticated and validated.

@_date: 2014-02-17 17:06:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encodings for crypto 
That is in part what motivated the initial work. The problem with the JOSE
encryption work is that if you have nested encryption/signature sections
you suffer a cumulative 33% inflation per pass.
So what I started with was 'I just want to be able to do normal JSON
framing but be able to write out a binary chunk efficiently as a length
encoded blob'.
One consequence of my approach is that it is very easy to switch between
the text and binary encodings. Which means that you can use the text
version in the documentation knowing that it is semantically equivalent to
the binary. And you can have tools that interpret the binary and dump out
Yes, it is not a good piece of work. Unfortunately Paul was far more
interested in ramming it through the process than listening to anyone's
requirements other than his own. Each time a requirement was raised, his
response was 'this is a private matter, none of your business, go write
your own'. Which is why I appealed the standards track designation.
CBOR is not closely aligned with JSON, it is a new data model which means
that it is rather less interesting than BJSON which does at least have
millions of MongoDB users and is much closer to the JSON model.
The problem here is that a new encoding only has value if it has a chance
of becoming ubiquitous which means meeting all the requirements that are
regularly raised for encodings. So going off into a private 'members only'
club to write the spec removed the main potential benefit. As did moving it
through the process so fast that there was no time to get buy in from the
people who might adopt it in significant platforms as the default.
So instead of one definitive binary encoding we have another binary
encoding to add to the five that existed and were in use before the work
started and one that is not even based on the JSON data model.
For a new encoding to become a natural extension to JSON it has to be easy
for someone to take their existing JSON encoder/decoder and adapt it to
support the new encoding types. And it has to be backwards compatible.
CBOR requires a whole new encoder which is a very different model.
I did manage to convince them that it had to be possible to write data out
in chunks but they punted on the hard problems by making it extensible. One
of the main lessons of JSON is that a data encoding should not be
extensible or have versions.
JSON-B is not an extension to JSON, it is a new encoding that is a superset
of JSON - a different thing entirely. What this means is that when doing
content negotiation, the features are all or nothing. Either a decoder
understands the encoding or it does not. Having optional extensions that
might or might not be understood is a recipe for disaster.

@_date: 2014-02-17 17:18:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encodings for crypto 
On second thoughts, maybe I should explain why signature in a text based
encoding up being ugly. The encryption is just some gratuitous BASE64. Its
the signature that is a problem.
The problem is that when you have a signature you have to know exactly what
is in the signature scope. If the scope is out by one byte then
verification will fail. Text based encodings invariably end up with added
or subtracted whitespace being possible. This results in two approaches:
1) Convert the data to binary blobs with Base64.
2) Convert the text to canonical form
The second is only possible if you have a data model underlying everything
which of course JSON does not have. Well not a model that could be
considered canonicalizable. Which pushes us to the first approach.
Its not that jwe or jws are baddly designed, they are just having to solve
an ugly problem.
One simple solution to this is to say that the authentication info is
actually metadata and this belongs outside the signed object itself. So
instead of trying to cram the signature into the application/json content
of a web service request, put it in the HTTP header.

@_date: 2014-02-17 18:43:33
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
Well one thing to watch out for, there is an SGI patent on lavarnd and the
SGI patents were bought by a well known troll operation.
But the general point about these being pretty, I was thinking of making
electro-mechanical sculptures for sale. A twist on kinetic art.

@_date: 2014-02-18 00:40:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] BitCoin bug reported 
Snowdonia does prove the need for pervasive data level security at Fort
Meade. They don't seem to be having much luck.
I suggest that we help them out so that we don't have more Snowden stories.
Build some good specs for end-to-end and data level security and test them
out in the real world before they start using them.
A beta test of two billion people or so should easily be sufficient.

@_date: 2014-02-18 11:28:33
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Encodings for crypto 
But part of the exercise here is to try to get to convergence on one single
encoding by meeting all needs. I don't see a lot of need for compression or
for 128 bit floats. But some people need them for their data work. Hence
JSON-C and JSON-D.
I understand the temptation to go optimizing bits, but I don't think there
is any point in my current applications which are JSON Web Services over
HTTP. However, right at the end of this over-long message I show a
situation where bit grinding does provide real value.
What we could do is to either define a profile that drops unneeded features
or have applications state which features they actually use.
If we were doing a crypto framing scheme then it would make sense to say
that the bits on the wire must only use the binary tags and that only
positive integers, strings and binary data chunks are used. But the
documentation could still use the JSON representation for illustrative
The problem with the 7 bit trick is that it requires the following bytes to
be shifted about, same with the similar UTF8 scheme. The JSON data model is
simple enough not to need more than one byte for the tag and length of data
to follow combined.
Allowing any number of bytes in an integer from 1 to 8 requires 3 bits
which is a lot. Restricting the number of bytes to 1,2,4,8 saves a bit. But
that is all that is going on. Using a 16 bit representation on the wire
does not mean that the corresponding data model representation is 16 bit.
It is just that the number either does or does not fit into 16 bits.
Since the JSON data model has negative integers it is necessary to deal
with signs. Which means either twos compliment or a sign bit. I prefer a
sign bit because it is a lot simpler to code and avoids confusion between
So I would argue that there are actually only two number representations in
JSON-B, Integer and float.
If a protocol involves integers bigger than 32 bits then ultra-compact
representations are probably not a priority. That is why a 16 bit length is
used for bigints, while it is virtually certain that a bigint will fit into
a 8 bit length, saving the extra byte is not worth the extra code point.
I agree that floating point values are not necessary for protocols and
arguably neither are negative integers. But they are part of the JSON data
model which means they have to be supported.
Well the code is in C# so that is how the encoder/decoder works. Not sure
what you are getting at here.
Sounds like you are doing a compact binary version of LISP S-Expressions.
Which is another totally valid approach.
Yep, regression testing. Have not got round to writing this yet but I plan
to. I wanted to re-implement the scheme in C first. Then I can generate the
same data inputs for C and C# and check that they produce the same outputs
and decoding produces the original data.
Probably the key point here is that if you are sill thinking about
I am not sure about the OO thing since I was doing OO back in the days when
it really was message passing and then C++ came along and made a mess of
the ideas.
What we are really doing with a protocol compiler is writing the on the
wire format for messages passed between concurrent network objects.
Further, in a message framework, those messages correspond to method
invocations on the objects. So the (abreviated) signature of a Transaction
in the Confirmation protocol is:
    Transaction Enquirer Confirmation ConfirmationRequest
        Description
Post a request for confirmation to a user.
        Status Success
        Status Refused
        Status UnknownUser
    Message ConfirmationRequest
        Description
Request a confirmation from a specified user.
        String Account
            Required
            Description
The user being asked to provide confirmation.
            Description
The format of the account identifier is the same as for
i.e. <username>
        String Text
            Required
        String Option
            Multiple
When this is translated into C# we get the following class (abreviated)
public partial class ConfirmationRequest : CNF {
public string Account;
public string Text;
public List Option;
Or in C, the following structure:
typedef struct _CNF_ConfirmationRequest {
struct _CNF_ConfirmationRequest *_Next;
int _Type;
JSON_String Account;
JSON_String Text;
JSON_Group Option;
} CNF_ConfirmationRequest;
It turns out that most IETF protocols can be implemented as a sequence of
RPC calls between a client and a server. The main exceptions being Jabber
(XMPP) and IRC which are essentially reverse RPC calls. The client opens up
a context and then receives a series of RPC calls from the server. I have
looked into that but to support that communication pattern in a web service
I either need to use an extended HTTP that supports multiple streams (e.g.
HTTP/2.0) or write my own.
Although PROTOGEN was originally designed to be a protocol synthesizer just
for JSON, writing a C library means that I need to be able to make HTTP
calls out which in turn means either calling someone else's API and deal
with a different implementation of strings etc. etc. and a lot of code I
don't need or writing my own very simple HTTP implementation.
So last night I wrote a PROTOGEN schema for HTTP (or at least the parts I
care about) and I hope to have it done this week.
Protocol Goedel.HTTP HTTPG
Structure Common
String Content_Type
Integer Content_Length
DateTime Date
Structure Request
Inherits Common
String Accept
String Authorization
One side product of this work is that I can now encode HTTP messages in
JSON syntax rather than the traditional RFC822 style headers. Which in turn
means that I can write a Web service that only uses one encoding for both
the wrapper and the contents and since the compiler supports JSON-B (or
soon will) I can optimize a protocol for space by simply setting the
encoding flag to JSON-B rather than JSON.
If we went a step further and encoded the SSL layer in the same encoding,
ditching the ad-hoc SSL/TLS encodings this would provide consistency all
the way down to the IP layer.
At this point Ian's proposal for a totally minimal binary encoding with no
frills suddenly looks very attractive. The HTTP layers don't need floats or
the like so ditch them.

@_date: 2014-02-18 11:45:58
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
One important boundary condition is that I want the operation of the random
number generator to be completely auditable so that we can tell with a very
high degree of confidence that no strange business has taken place.
So for example, lets take a cut and choose type protocol.
What I want is a machine that I can cause to perform the random number
acquisition process repeatedly without knowing whether the machine itself
is being audited or not. So we have the dice roll in a transparent box 256
times and one one occasion chosen using a process that could not be
predicted when the machine is configured we put a cover over the camera so
the dice rolls are not observed.
We check that the results are consistent with the observations from the
second camera in the other 255 cases. Thus an occasional defection attack
has only a 1 in 256 chance (i.e. 8 bits) of success.
Now imagine we repeat that process 16 times and XOR the outputs. We now
have a random number such that an attacker trying to trick us would only
have a 1 in 2^128 chance of success.
We still don't have a completely auditable system but we are getting
closer. The problem is that I can't eliminate the 'first public key
creation' problem. I can show that a system is effectively unbeatable if we
can trust an initial public key. But so far I haven't been able to get past
that point.
The objective here is to end up with a device with a public keypair that we
are assured is only in that device alone and could not be affected by the
original coding or hardware construction.

@_date: 2014-02-18 13:06:50
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The ultimate random source 
On Tue, Feb 18, 2014 at 12:42 PM, Philip Gladstone <
That can just be any dice roll that is not observable by the recorder.
There are two separate requirements here
1) A randomly assigned output.
2) A randomly assigned output that is not known to any other party.
What does come under the rubric of 'just moving the randomness about' at
the moment is the combination of the partial random inputs into a single
But I can generate a public/private keypair from each 128 bits of random
output. And that is probably enough leverage to solve the rest of the

@_date: 2014-03-01 00:48:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] GOTO Considered Harmful 
I don't write code in C. I have my own personal language that is
implemented as C macros
 int PUBLIC_ENTRY;
 PRIVATE_ENTRY static int
 BEGIN  << stuff >>
 END  return 0; _fail:  << stuff >> return -1;
 CHECK_STATUS(x) if (x != 0) goto _fail;
So all my functions look like
PUBLIC_ENTRY function () {
CHECK_STATUS (some_function);
If I had other people coding I would check their stuff with a preprocessor
that rejects any code that has the string goto in as a code smell.
(thats not the actual code, from memory as i don't take any source with me
when crossing national boundaries but you get the idea).
I don't trust ||= either...
The reason for PUBLIC_ENTRY is because I have a tool that looks for that
string to generate the function declaration headers from it.
Exception handling in C....

@_date: 2014-01-01 11:47:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
The nuclear codes leaked long ago. I know the navy one from the 60s
In deference to the needs of national security, I will give it you off list Sent from my iPad

@_date: 2014-01-01 19:32:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Look up GEANT.
The necessary tools have been open source since before the term was coined.

@_date: 2014-01-01 21:40:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
There is an important difference: I don't have the slightest doubt as to
the trustworthiness of GCHQ when it comes to the defense of democracy in
the UK. I certainly cannot say that of the US military or the NSA.
The political situation in the US does worry me for several reasons. First
the NSA and CIA were in the business of toppling inconvenient governments
for over thirty years. Even Eisenhower wondered about the possibility that
it might become a habit that they would eventually practice on their own
Then there is the fact that after Louis Freeh lost the crypto wars, he
conspired with the Republican party in Congress to entrap the President and
enable his impeachment. That was in my view an attempt at a constitutional
coup. That failed attempt was followed by a successful one when the
Republican party partisans on the Supreme court conspired to prevent the
counting of the votes in the 2000 Presidential elections.
The gulag in Guantanamo and the fact that none of the people responsible
for ordering the use of torture in Guantanamo or Abu Ghraib have been
prosecuted is a demonstration that there is no accountability in the US
political system.
And finally here is a political culture in which it is considered
acceptable for one party to peddle myths about the elected President's
birth certificate being somehow defective. It is obvious what the real
purpose of these claims is, in the UK we would call it mouthing treason.
So no, I don't think the issues are equivalent.

@_date: 2014-01-02 08:16:26
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
Looking through the 'mail order catalogue', are we sure that all these
capabilities are actual capabilities rather than projects attempting to
create them?
They have no limits in their ambitions to spend public money. Whether the
results perform as advertised is another matter. Wouldn't the war on terror
be over by now?
I suspect it works about as well as that start wars 'defense' system that
Reagan proposed. Decades later they still haven't had a single test that
wasn't faked. They certainly could not prevent an attack with Chevalene
type warheads which the UK withdrew from service before the project started.

@_date: 2014-01-05 09:14:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Just don't, we used to laugh at UNIX security back in the days when VMS was
the only secure OS. Security is often used as ammo in standards wars, the
comparisons are rarely accurate.
I believe the point of OpenBSD is that it is not a kitchen sink O/S which
ships everything someone might want by default. That is certainly going to
offer more security if you use it for a single purpose with a stripped down
build. It also means that the O/S is not going to report a vulnerability
each time sendmail gets rolled.
But take OpenBSD and lard it up with the thirty packages that are written
by the usual C-crew and the advantage is lost. Very few Microsoft or OSX or
Linux security reports are for code in the O/S core. It is usually the
support apps that cause the issues.
The most significant differentiator in security has actually been whether
accounts have a mandatory separation of superuser privs from regular
accounts. Windows XP does not have that and so every app that runs in an
account with admin privs can bongo the machine without any trouble.
One of the reasons for that is I believe that all modern O/S have
essentially the same approach to access control which is essentially
broken. Butler Lampson thinks it is broken as well, but even he can't
change it.
The problem is that access control attributes are not attached to either
files or to the applications that run them. They are ledger entries in the
file system and grant access to users. Which makes them essentially useless
for modern uses where each machine has between zero and one user and files
move from machine to machine without the security controls being carried
with them.
I don't think the firmware on my printers has ever been updated. An the
routers were never updated till I moved from the cheap linux based ones
that last 6 months to Apple airports.
Windows XP still accounts for the majority of rooted systems. I find it
quite astounding that there are companies who still insist on using it. I
started buying my own machines when I was at VeriSign precisely because the
IT dept refused to let me run Vista.
I think the Vista hatred was mostly driven by lazyness on the part of the
IT staff who wanted to avoid having to make new builds for their machines.
The same companies that would allow their IT departments to continue to run
an operating system the provider was warning was defective would go out and
buy a million dollar firewall.
It will be interesting to see what happens when XP goes EOL in April. A lot
of IT staff are likely to find themselves looking for new jobs as they
discover that they can't get to grips with the new Windows that other
people have been working on for 7 years. Some voluntarily, quite a few not.

@_date: 2014-01-05 16:47:37
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Well as Jerry quoted my old college tutor earlier in this thread, you can
either make something so simple it is obviously correct or so complex that
there aren't any obvious errors.
Every O/S has a broken privilege system in my view. Instead of system
privileges being monolithic as they have become defacto in every O/S, they
should be mutually exclusive.
A user can have multiple privs but a particular application should not be
able to claim 'modify executable code on disk' and 'modify application
Only Microsoft should be able to patch my copy of Microsoft Office without
some very explicit overrides on my part. Same for Adobe.
When a program is installed, the installer should only see the default O/S
environment. It should not be able to modify any part of the O/S or install
any dll or .so that any other package can see the change.
We need to get back to the idea of least privilege but apply it to
applications and daemons rather than users.

@_date: 2014-01-05 18:52:43
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] defaults, black boxes, APIs, 
You mentioned Tony Hoare earlier, he didn't use his Turing Award lecture to
point out that lack of array bounds checking was going to bit on a whim. He
knew that it was going to be a disaster.
The CERNLIB code for the Web was actually pretty robust as all the string
handling was performed by macros with built in bounds checking.

@_date: 2014-01-05 20:03:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Using Raspberry Pis 
That looks more than fast enough for my needs since I can't get more than
75 Mb/sec broadband anyway and there will be a faster model sooner than
that changes.
For me a gateway box should not be a positively trusted system. It should
be trusted in the sense of depending on it to keep the bad stuff out but
compromise of the box should not cause other boxen to fail.
At that price though, I might just go with a two more boxes the same to be
my positively trusted service (fault tolerant).
Is it possible to expand the file store beyond 4Gb or is that a hard limit?
My Google mail file is 8Gb already so I would really need more...

@_date: 2014-01-13 10:23:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Yes, we all know that RSA got punked by the NSA. But this boycott some folk
are pushing seems like a terrible idea to me.
What is important now is to get people to deploy and use defenses. Which
means making the most of the communication platforms we already have.
Even if a boycott of RSA was to work the best it could achieve is to damage
the main industry trade show. It would take any replacement several years
to recover. In the meantime we have no venue to sell security product.
Black Hat is not RSA and does not want to turn into RSA. They have
deliberately created a totally different value proposition.
The boycott proposal is purely punitive. They have no demand. Which is a
very weak position. We did not boycott Bigot-fil-a to punish them. We were
demanding that they stopped being bigoted. What are the boycotters
demanding of RSA?
Unless someone shows evidence that RSA actually knew they were being
punked, the boycott makes no sense. And I can't believe that evidence
exists because there was absolutely no need to tell RSA they were being
punked to get the outcome they wanted.

@_date: 2014-01-13 14:35:06
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Absolutely right. But how should we respond?
There should be a penalty, no question. But what should the penalty be?
We should not choose a penalty that causes collateral damage on our side. A
much more effective response would be to gut the RSA token business. That
hurts EMC's bottom line directly. Changing the speaker lineup at the show
does not.
If the RSA token business is gutted there will be no reason for EMC to keep
RSA Labs or the name.
Lets pick out battles here.

@_date: 2014-01-13 15:33:03
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
It is the only international trade show for the IT security industry.
Try 10,000 people coming to buy products to protect themselves from the NSA
This is not acceptable collateral damage.

@_date: 2014-01-14 08:16:54
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
NIST does have a conference in April and we can boycott that by setting up
a parallel conference with bigger names very easily.
That does at least mean that we are likely to send the right message (i.e.
boycott successful) and send it to an organization that can relay it to the
political entities.
One of the questions raised by Flame is how the US government can hope to
have public-private partnerships when the US government is attacking US
companies. Flame involved an attack on Microsoft, remember.
I'd also boycott companies doing business with the NSA.  And USG.  If
That particular outcome is practically self-enforcing. Everyone is going to
be very suspicious of NSA proposals now.
The claim that the NSA simply bribed RSA makes it sound as if all companies
need to do is refuse obvious bribery attempts.
Looking at the attack as sophisticated social engineering makes for a much
stronger warning: If you deal with the NSA they will betray you and then
the scheme will come out in an insider attack.
The attack on the RSA conference is an attack on the brand of RSA.  This
Attacking the brand through the conference is difficult because I don't see
any name pulling out so far that is big enough to have effect.
The fact that the main trade show is joined to one company is a very long
standing irritation for all of us in the industry. It would be better if
the RSAConference was owned by a conferencing company that didn't have a
business competing with the rest of us. I don't think that conflict of
interest has helped RSA the company either. Their strategy has been
constrained by needing to avoid compromising the conference too badly.
That is an idea.  If one is in the business of sanctions and one is
Well we have been facing the same problem with the boycott of Sochii. Its
going to be another Nuremberg. And I don't mean that figuratively, Putin
has been copying the institutions of fascism. But we did get the Pussy Riot
girls and the Greenpeace protestors out of jail which isn't nothing.
The conference made the company business a target in the past. At VeriSign
we did an open standard version of the SecureID token, OATH and launched it
at the conference. It does not take a genius to work out what the objective
was there. It was the conference we were after, trying to commoditize the
token business was an attempt to buy it cheap.
Without the RSA tokens biz, there would be no real business reason binding
RSA to EMC. That is the pressure point I would attack. But given that I
have proposed a second alternative to number based tokens that uses the
capabilities of smart phones, that would be a somewhat self-interested
If there was a free alternative that people could use to turn their smart
phone into a token, people could press for it as an alternative. The IT
desk would almost certainly like to be rid of the stupid tokens, they are
very expensive and the preprogramed expiry date creates a constant admin
This is pushing at an open door. Replacing the tokens is something almost
all CISOs would like to do. Especially after the 2011 fiasco. But it just
hasn't been a priority. This set of circumstances can make it a priority.
We would need more that speaks directly against the tokens to spread the
No, I don't think we do. There are solid business reasons for abandoning
the tokens already.
The fact that RSA has dual control over your authentication infrastructure
is the issue I would point to. RSA could be subpoenaed to give the feds
access to the whole token database. It would be very easy to match token
codes to tokens given their intercept capabilities.
We don't need to allege collusion in the past. All we need to do is to
point out that the scheme lacks transparency.
In the contrary, do we do more damage to companies by tricking them into
Unlike the NSA, I do not give people advice knowing it is false.
The tokens do rely on the token provider being trustworthy. The token
database allows backdoor access.
I feel like we should also boycott the IETF.  They have truly not served
No, leave them out of this. Don't turn off my damn waster supply while I am
trying to fight a fire.

@_date: 2014-01-14 12:35:31
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
I hadn't thought of it that way before.
but yes, I think that the way the NSA worked to disrupt certain IETF and
industry programs was to play that type of game.
"Lets put a TPM chip in the machines to keep private keys safe"
DRM! DRM! Antichrist! DRM! Don't you use treacherous computing. DEEEE
ARRRRRR EEEMMMMM!
"Lets turn on this S/MIME crypto in a billion email clients"
Don't you know that PGP is the only way to go!!!
"Lets add PGP features to S/MIME"
And have the smelly hippies come round?
I am sure that a lot of arguments were played on both sides with wormtongue
types telling both sides to stick to their guns. And too much of it has
That is the real crime here. Instead of protecting the country and its
allies against the threat of attack against an infrastructure that is
brittle, they were actively sabotaging efforts to make it robust.

@_date: 2014-01-14 14:01:36
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
I am really reluctant to set that type of precedent. I think that we are
not finished with the disclosures yet. If we take everyone out to the
woodshed and hack their heads off each time we find out about the next NSA
hack, well we might find that none of us are left to do the work that needs
to be done.
That is my problem. Hurt RSA if you like, fine. But boycotting the show is
like boycotting the village fete because the Lord of the Manor let a cow
get loose and trample everyone's begonias.

@_date: 2014-01-15 15:48:59
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
What then should we do about all the folk clinging to 3DES? How about the
people who stuck with MD5? How about the people who have not junked SHA-1?
Rather than compiling lists of people who should be drummed out of the
industry for bad decisions their companies made in the past, how about
compiling a list of proposals for things that you think people should get
drummed out for in the future?
I remember back in the day when I was having a USENET flame war with Dennis
Richie over the then UNIX policy of keeping the password file world
readable. It didn't take them very long to change in the wake of crack
(which arrived a few months later). But boy did they cling to their
religion hard. I should have taken a drive down to the Vatican and got John
Paul II to change his policy on abortion and birth control. It would have
been easier and more chance of success.

@_date: 2014-01-15 17:41:51
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Tweet from Art Coviello saying "Time for some NSA backdoors in Fort Lee"
No they didn't. I knew about the deal at the time and I was a competitor.
RSA made no secret about getting the NSA gig. The only thing they didn't
publish was the amount and my sales guys knew that. It is probably in the
federal register.
No, I don't have much faith in them anyway so nothing to lose there.
The people who made the decisions are likely long gone.
Corporations are not people.

@_date: 2014-01-16 07:58:15
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
The problem is the same problem as usual with DES: Adi Shamir. Remember
when he got bounced from that NSA conference? He gave a talk at MIT
instead. And what he showed generalizes the meet in the middle approach.
It isn't a break of 3DES but the approach does show how the construction
approach is weak.
The reason I point it out is that what we had in 2007 was very similar.
There was no proof that the algorithm is backdoored. I am not aware that we
have an actual smoking gun in the Snowden docs even today. No 'Time for
some backdoor in DUAL_EC_DRNG'.
But what there is today on 3DES is certainly enough for those of us who
went to the right talks to be able to say in 5 or 6 years, 'told you 3DES
was vulnerable'. It wouldn't really be fair, it would be using a huge slice
of hindsight. But so are the people complaining about RSA.

@_date: 2014-01-17 07:57:37
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] [cryptography] Boing Boing pushing an RSA 
The criteria for a one time pad is that the entropy in matches the
ciphertext length so there is an equal probability of any possible
plaintext mapping to any possible ciphertext.
Since every physical implementation of a random number generator has bias,
it is necessary to perform conditioning of the random seed before use and
this may be either a hash or a PRNG.
So using a OTP in practice does involve a PRNG which in turn means that the
practical system is not theoretically secure. Not using a PRNG makes the
system theoretically secure but insecure in practice.
Theoretically secure but insecure in practice also applies to quantum

@_date: 2014-01-19 23:09:28
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
There are more people working on the defense certainly. And there are more
parts to the defense than just what they are doing.
The biggest roadblocks we can put up in front of programs like PRISM have
to work at the packet and link layers. Which means that only a few
companies can make the decision to deploy. There aren't many of us able to
have much impact there, its pretty much up to Bruce.
If people are getting in the way of getting those messages out they are
hurting, not helping.
We need tools for the general public to use and tools for the people
building the tools and other high profile targets. So its not even a
question of one solution for email, we are likely to need multiple schemes.

@_date: 2014-01-20 17:10:46
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA is dead. 
They were a little subtler.
The NIST standard permits the use of user defined curves. They didn't trust
the Fort Meade folk either. The scheme is secure if you choose your own
curves but most people don't.
In fact the use of a deterministic RNG with that type of trapdoor is
arguably a best practice. It provides a way to audit the operation of a
manufactured device.
The behavior of the device is transparent and deterministic if the backdoor
constants are known and pseudo random and non predictable otherwise.
The device itself has no way to tell if it is being fed trapdoor constants
or not and thus no way to tell if is being audited or not.

@_date: 2014-01-21 17:28:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
I think we need to consider the whole email infrastructure these days. In
particular we have DKIM now which we didn't before.
So my preference would be,
Let m be the initial message, ks be the personal signature key of the
sender, kr be the personal encryption key of the receiver, kd the dkim
server signature key
DKIM:  Sign (body, kd)
body = E ( m + Sign (m, ks), kr )
The DKIM signature should be sufficient for anti-spam control purposes
which should be all the receiver requires in order to decide whether it is
worth spending effort to decrypt.
Of course the scheme can be improved considerably if the encryption format
allows the content and the signature(s) to be encrypted separately. In that
case we can construct a signature over the encrypted and unencrypted data
in one go.

@_date: 2014-01-21 18:33:32
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Timing of cyberattacks -- is this a joke? 
Since the paper is by Robert Axelrod, yes THAT Axelrod and it is an
application of game theory to timing of uses of attacks I would say it is
I don't see the paper as predicting when the attacks would happen, rather
it is considering what the optimum time to use an attack is for an
attacker. The model is only predictive if it is assumed that the attackers
adopt an optimal strategy.

@_date: 2014-01-22 12:23:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] RSA is dead. 
This is why I think it is so important to think rather harder about the
NSA/RSA issue than assume that the $10 million was paid as a direct pro quo
to a company that knew they were taking a bribe.
If defeating the NSA was so easy, all we would need to do is to be on the
look out for people taking huge bribes. But that is not the simplest or the
safest way for the NSA to have achieved their objective so it is almost
certainly not the tactic they used.
I think we also need to have the right mode for the people running the NSA.
I don't doubt their patriotism or their devotion to duty. But I have met
ex-NSA directors and they all suffer from the limitations of the military
mind. They have a particular comfort zone which is based on their
experience of what they call 'kinetic' warfare. They don't like facing up
to the realities of what they are doing so they talk in jargon designed to
insulate themselves from the moral consequences of their decisions.
When they are outside their comfort zone they are liable to panic.
We saw this happen on a massive scale in the 1950s and 60s. The series of
coups in emerging democracies were all engineered by elites panicking at
the prospect of popular government. In many cases the US elites that
panicked first.
I do not believe that it is safe to let these people loose with such power.
Hayden in particular strikes me as the type of person who might well panic
and declare martial law if he thought the republic under threat. The
problem is that I strongly suspect that the sort of things he considers a
threat are rather different to the ones I consider threats.
There is a large portion of the power elite in the US that spends its time
talking about the President being born in Kenya and equating Obamacare to
communism and talking about traitors. When people do that sort of thing I
suspect them of being the traitors themselves and working up a pretext.

@_date: 2014-01-27 15:04:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The crypto behind the blackphone 
Yes they can.
I have no direct knowledge of the specs other than brief hints from Jon.
But it is pretty clear that this phone is going to offer application layer
encryption like the silent circle products do.
The big difference is that on this phone you can compile the code from
source and be sure there is no backdoor. Which is not really possible on
the iPhone version (though I guess someone could compile the source and
check that the deployed app matches if they provide source for that.)
There are some attacks that no application layer scheme can protect you
against. In particular, traffic analysis and metadata can't be fully
controlled, particularly for a system with a low user volume. Say there are
a million users of the phone and a thousand calls in progress at a time. if
the Feds are watching two people and one dials and the other picks up at
that very moment, they have a data point. If they do it a second time then
they have two data points. Three data points are enough to put the match
beyond reasonable doubt.
This is the attack that caught the jackass who tried to avoid a finals exam
with a bomb threat at Harvard last term. The police found that only five
people were using Tor on campus at the time. Now if the guy had been at
I am pretty certain Jon and co have the confidentiality pretty well locked
down so that is an advance.

@_date: 2014-01-27 17:40:00
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The crypto behind the blackphone 
As Robert Morris used to say, the three laws of computer security
1) Don't have a computer
2) If you have a computer, don't turn it on
3) If you turn it on, don't use it

@_date: 2014-01-28 14:56:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Angry Cryptographers... 
This latest NSA atrocity is the last straw.
The military can spin this any way they like but the way I see it is that a
group of officers of questionable loyalty have been spying on everyone and
everything for their own gratification. This isn't about stopping
terrorism, its just because they can.
The sheer scope of the spying and the Abu Ghraib photographs call the
officers loyalty into question. They didn't raise any questions when the
previous President ordered the use of torture on prisoners of war. They
didn't raise any questions about the reclassification of prisoners of war
as 'illegal combatants' or torture as 'enhanced interrogation'. So now they
are not due any benefit of the doubt when they try to explain their
motivations or demonstrate their loyalty.

@_date: 2014-01-28 19:45:35
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Feds make Bitcoin arrests 
The feds have taken down Bitinstant and arrested the CEO on charges of
money laundering...
I did predict this some time ago. After all its what happened to all the
gold exchanges.

@_date: 2014-01-31 09:49:42
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] New head of the NSA, 
His deputy is, as is traditional, a civilian. Richard Ledgett is a senior
NSA analyst and the man in charge of the investigation into the Snowden
affair. So far his investigation has revealed that Snowden went on the run
with over a million classified documents, a tiny fraction of which have
since been released.
So, looks like the disclosures are going to continue a while.
I am thinking that after I complete my email scheme, I'm going to look at
CRM. There is a large government agency out in Fort Meade could use some...

@_date: 2014-03-02 23:15:23
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
My take "BitCoin is Stupid" More precisely it is the Bitcoiners thinking they are going to get rich
quick here because BitCoin is certain to replace money.
12 million bicoins x the price someone would pay for one bitcoin = $7
Seems to me that a better way to estimate the market cap of BitCoin is to
look at what 1% of the float would fetch if it was offered for sale over a
24 hour period. I don't think that it would fetch anything like $70
million. Maybe $7.
But more importantly, currencies are a medium of exchange and I see no
evidence that is happening with BitCoin except on a proof of concept basis
or buying drugs. And why would anyone spend a BitCoin when they believe it
will be worth a million dollars in a few years time??

@_date: 2014-03-04 09:38:40
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
So let us count up the arguments as opposed to straight contradictions here.
This might be an argument, but not one that makes sense to me. I don't
think it parses for a start.
Here at the IETF we just had a rather remarkable presentation on BitCoin by
a CEO who clearly sees buying BitCoin as an 'investment'. He has tripled
his million bucks since the start. But even as an investment, he is in big
trouble as the market for BitCoin is thin. It is easy to buy BitCoin in
bulk as $2 million of new currency is minted every day. Selling BitCoin in
bulk is far harder. There isn't enough liquidity to sell $3 million BTC
except veeery slowly.
Extraordinary claims demand extraordinary evidence. Here we have a group of
people who claim that BitCoin will replace the US Dollar, the Euro and the
Pound as global reserve currencies. There is a group claiming that
AuroraCoin is going to 'free Iceland from the shackles of fiat currency'.
I am not demanding extraordinary evidence, I am demanding some evidence
that people ARE using BTC to buy stuff rather than the abstract possibility
that they could do so if they chose.
What I see in BitCoin is a classic bubble mentality. Nonsensical
projections are made and accepted as incontrovertible fact, except when
they are being challenged when they are immediately abandoned rather than
risk a defense.
Nobody in the BitCoin community is arguing against the notion that BitCoin
will 'certainly' replace the dollar when that is proposed. At best the
response is 'that is probably over-enthusiastic, it will be a long time
before that happens'.
There is a principle called projection.

@_date: 2014-03-06 08:12:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
People can write good code using gotos... but most do not
People can write code without memory leaks in C or C++... but most don't
People can write comprehensive test suites for certificate checking... but
so far none has been mentioned so I don't think they did.
Coding risk is just another risk and there are different ways to control.
We have just seen the failure of the open source security argument. There
are many good reasons to use open source licenses and public domain. But
publishing the source code does not sprinkle magic security dust over it.
What it does mean is that we have the chance to learn from the error should
we choose to..

@_date: 2014-03-08 01:24:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] See??? Satoshi Nakamoto Smeared 
Actually RAH did invent BitCoin and was feeling the heat a little and so
used some of the $0.5 Billion he stashed away in BTC to hire an actor to
pretend to be Satoshi. To act as a distraction...
The disclosure of Satoshi might be the start of the end for BTC. Before
BitCoin was a tabula risa on which everyone could write their own goals for
a perfect currency.
It also means that his family are at risk of being kidnapped by the seedy
types that BTC has attracted.

@_date: 2014-03-16 08:40:22
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
One of the side discussions that came up at the STRINT workshop was on the
need for better management of crypto algorithms in standards.
[I am working on the draft I agreed to write some time ago but hadn't
finished. It is almost done this time.]
Right now we have a fairly well established mandatory to implement set:
AES-128, SHA2-256, HMAC-SHA2-256, [DH & RSA-Encrypt], RSA-Signature
The use of RSA-Encrypt turns out to be redundant in Internet protocol use
as we never actually need public key encryption, what we actually use is
Key Exchange. Which is good since we don't have a EC version of RSA :-( so
we are going to need to to EC-DH anyway.
Although that set is pretty well established it is starting to look a
little on the old side. We have a consensus replacement for SHA2 but right
now we don't have solid consensus on replacements for anything else.
It seems that we are pretty close to a consensus on the use of EC as the
next generation digital signature algorithm. But there are a few more
degrees of freedom there which need to be nailed down. There is the choice
of curve in particular which is now subject to a lot of possibly justified
So we can hypothesize a backup set of algorithms:
??, SHA3, HMAC-SHA3 / ??-CCM, ECDH, ECDSA
Spot the problem? We currently have no backup for an encryption algorithm.
We really do need a backup for that slot and I don't think we can just take
one of the AES runners up. The criteria for a reserve algorithm are not the
same as for the default. Since the idea is that you can depend on the
reserve algorithm even if the default is broken, it has to be tuned for
security rather than performance.
I can't see how to get there unless we run a whole new crypto algorithm
Maybe that is something the Brazilians might want to take on.

@_date: 2014-03-16 19:45:04
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Actually that was the case for TLS which required MD5 and SHA1 in the first
incarnation. So when MD5 was found to be faulty, the switch to SHA1 was
Every IETF crypto protocol has specified a mandatory to implement algorithm
to ensure a minimal level of interoperability. If the spec designers don't
choose one cipher the market will - and we may not like the choice.
IETF already requires algorithm agility so there is no more complexity.
Today there is no backup algorithm specified so the tendency is that
implementations offer the kitchen sink. Which does not work because
security is determined by the least secure algorithm.
Specifying one required and one recommended algorithm as backup means there
is a chance that might be all the implementations support unless there is
good reason.
No, we have had pretty good notice of past issues.
I don't think the IETF are fans of stream ciphers or anything that looks
like one.
So, in a sense, there is an emerging consensus that competitions are
Well, they take five years for a start.
relationship of
It isn't that EC is stronger the reason it is interesting is that RSA key
size stops delivering interesting improvements in strength after about 2048
Not any more, its now a cloud thing. If more computing grunt is required
they will up their server farm size as necessary to deploy it.

@_date: 2014-03-18 09:38:14
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
That is the aim. Only for obscure reasons I am this morning working on
transport layer encryption for DNS.
One of the modules on which I am building my messaging proposal is a scheme
called Omnibroker. The idea of Omnibroker is that in certain situations you
are better off putting complex trust decisions in a cloud based service
rather than embedding them into the end points.
For example, if you want to do research on new trust models, you probably
don't want to start off embedding that code in the end points. Otherwise
you have to change every client every time you tweak the trust model. That
is no way to run an experiment.
Alternatively, consider the day you want to run PKI to your SousVide,
coffee pot and telly. You probably don't want to have to manage PKI on a
device that has no user interface. So the trust configuration is going to
come from outside. Better to keep it there.
Now before people start getting hot under the collar, remember that 'the
cloud' simply means 'some place I haven't decided yet'. The cloud could be
off in an Amazon data center, a Comodo SOC or it might be on a RaPi that
you configured yourself using an SD card with a golden distribution of
ubuntu having personally checked the hash code with Mark Shuttleworth,
Linus Torvald and Rebecca Watson.
So to be completely general the Omnibroker service answers questions such
* What is the trusted value of current time
* How does X connect to Y to perform protocol Z
* Is this credential valid for Y
The first only needs to be accurate to a second or a minute to be a vast
improvement on current security systems. I don't so much care about the
time being right to the milisecond as that an attacker can't unwind me by
more than a few minuted.
The second gives rise to
* How does alice at example.com send to bob at example.net using mail
* How does alice at example.com send to bob at example.net using chat
* How does * connect to  using HTTP
These are very abstract questions. The idea of having a trust service
answer them is that it can say things like
* Use SMTP to 10.1.2.3 port 25, expect STARTTLS, cert hash is X
* Use HTTPS to 10.3.4.3 port 443, cert hash is Y ignore the subject data,
don't put up UI chrome signaling security
And those answers can come in two flavors:
* Just the conclusions (a trusted query service)
* The conclusions with proof (DNSSEC, cert chains, etc)
So anyway, the protocol consists of two parts, a key exchange part to
establish a kerberos like ticket and a query/response part. The
query/response part is very simple, very easy to do. The key exchange part
is a lot more complicated because there are different use cases in which
the client must be authenticated using a lightweight scheme, a strong
scheme, not authenticated at all or possibly with some form of anonymity.
All the complexity goes into that key exchange. Which for crypto purposes I
am actually punting on to rely on TLS right now for the crypto part.
So having developed a scheme that does the very abstract protocol, it seems
completely sensible to then reuse the difficult and complex part to also
support the 'DNS Privacy' requirement that was raised at the last IETF.
Basically when someone binds to an Omnibroker service they get back a list
of service hosts which are IP address, port, protocol and credential
tuples. To support the DNSE use case, all I need to do is to add in a
transport binding for DNS query/response messages. I already separated out
the Omnibroker query scheme from the OmniConnect scheme.
My particular key exchange may not be the best way to do it. In fact I
deliberately stripped down the protocol to the minimum so as to minimize
possibilities for objection. It is easier to add bits in than take them
out. But if we divide up the DNS privacy problem in my particular way we
can then reuse the same mechanism for the messaging layer problems.

@_date: 2014-03-25 14:28:09
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
If people don't like what the IETF does then make your own. Its rather
easier than people imagine. I was part of the W3C breakaway from IETF
and then the OASIS breakaway from W3C.
People have the standards model completely wrong. Writing a standard
is the easy part. The hard part is getting together a coalition to
deploy. I don't go to the IETF because I enjoy doing engineering work
by committee with 200 people. I go there because I need certain
stakeholders to buy-in.
The IETF problem tends to be that it is rather too easily swayed by
the lone wolf who has a crazy idea. So instead of having one way to do
a thing we have a framework like EAP or SASL or GSSAPI that each offer
It would be helpful if more people joined in. Right now we are having
a discussion driven by the Snowden papers. Which is important. But I
don't see why we should only build infrastructure to limit NSA abuse.
Turkey is blocking Twitter via the DNS, shouldn't we be looking at
that problem as well? DNS Privacy would be nice but I want a
Private-DNS service that protects my privacy and neutralizes
government censorship attempts.

@_date: 2014-03-25 19:20:16
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Dark Mail Alliance specs? 
On Tue, Mar 25, 2014 at 6:26 PM, Peter Fairbrother
I have a series of Internet Drafts, Code and podcasts that propose a
mechanism that does pretty much what you suggest building on the
S/MIME message format and ideas from PGP.
Since neither camp is willing to give in to the other and one has the
mindshare, the other deployment, the only real way forward is to
provide a mechanism that is a worthy successor to both.
90% of the work that went into the S/MIME message scheme was actually
all the backwards compatibility with email while supporting MIME
Key servers are one approach, and a good one. There are others.
The podcasts:
The code
The docs
The proposal
Stuff I am building on

@_date: 2014-03-27 11:17:27
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Security is not a product, it is the mitigation of risks.
There is a risk that an outsourcer will attack you or be suborned by an
attacker. But that is only one of the risks to consider. The risk that they
screw up accidentally is probably larger. If you are deciding to insource
or outsource you have to consider both sets of risks.
The risks for insourcing include:
* Untrustworthy employees (insider risk)
* Incompetent employees
* Can't hire any employees at all
* Insufficient resources to do the job right
An outsource supplier has to convince customers to trust her. So most
invest heavily in security and process and audits. Those are not a
guarantee that the right thing is done but they have economies of scale on
their side and they do at least have the resources to do the right thing.
Inhouse teams usually don't face anything like the same scrutiny as the
outsource providers.
The main downside to the cloud security wise is that they are a bigger
target. So a successful attack is likely to be more profitable.

@_date: 2014-05-09 07:30:56
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Heartbleed and malloc 
I am not too worried about exploits before the bug was found. Its the
exploits that came afterwards that worry me. Those will continue for a
long time.
Getting OpenSSL to compile and run is not so easy. It is in fact a
PITA and the documentation is of the chocolate teapot variety.
As for complexity, that is itself a problem. A system that is too
complex is a problem in itself.

@_date: 2014-05-13 19:23:13
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
Yes and no.
In general any proposal of the form 'lets replace X with something 10%
'better'' is a losing proposition. Particularly when we are talking
about systems where network effects dominate such as protocols, APIs
and keyboard layouts[1].
And the proposals that are utterly doomed are the proposals of the
form 'lets add this one feature to X that I am obsessive about while
ignoring the fact that the alternative proposed does not support
features A, B, C that experience has proved to be essential'. So
proposals for new PKIs that assume admins never screw up are kinda
However there are occasions where such changes do happen. For example
QWERTY will survive until handwriting or voice recognition becomes
good enough that they become more convenient modes of text entry.
To replace a protocol such as TLS it is futile to propose a
replacement that does the same job in the same way but with slightly
fewer bits used. But a protocol that is useful in an application that
is not part of the core TLS use cases and TLS does not support well
could be a different matter.
For example, in Web Services we frequently require message layer
security in addition to transport layer security because a Web Service
transaction might involve more than two endpoints and messages that
are stored and forwarded etc. This is why WS-* is not TLS. (It is
unfortunately horribly baroque but that was not my doing).
So while I don't see a value in a replacement for TLS that is just a
better SSL. But there are areas that are currently poorly served by a
protocol where the key exchange part is welded to the transport part.
For example:
Note that in this particular case SXS-Connect is actually built on top
of the TLS key exchange because all we are looking to do is to
establish a ticket and shared secret that can be used in later
transactions. The value comes from the separation of the two parts
rather than having it as an 'all in one'. But having separated out the
two parts we can vary each independently. For example, I am already
using two separate transport packaging schemes, one layered on HTTP
and the other layered on UDP.
The key exchange part could also be modified. Right now it relies on
the authentication and confidentiality provided by TLS but it would be
better if it only relied on the authentication. It would be easy to
introduce a lightweight DH exchange to achieve that. We could even
introduce a new binding mechanism that dispenses with TLS entirely.
The point I am trying to make here is that looking for a 10% better
version of TLS is probably not the best use of time because any TLS
2.0 library will inevitably have to be backwards compatible with
legacy TLS and so any libraries will only be larger [2].
The improvements I look for are ones that make the code base a lot
smaller. And that requires rather more radical thinking than a 2.0
protocol. Refactoring the protocol so that the interfaces between the
abstraction layers expose different information for example.
[1] Yes I am aware that a farcical 'study' was purchased from a
K-street outfit which purports to tell 'the fable of the keys' which
disputes the Dvorak keyboard story. The study didn't even come close
to justifying its conclusions, the strongest claim it could make was
that nobody properly researched whether Dvorak was better. Well what a
surprise, because it was clear to everyone that no speed advantage
would be sufficient to justify retraining the typists.
[2] Yes I know that in theory a 2.0 version could be backwards
incompatible. But life does not work that way. What you would have to
do is support both versions of the protocol. All 2.0 means is that you
don't necessarily need to do both at once so you sitll have to do the
old junk and the new junk but not both at the same time.

@_date: 2014-05-13 21:35:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
So PKI is a failure for not succeeding in implementing a stupid,
dangerous model?
The lack of a single root isn't a failure. It was a very deliberate
design decision one that I see no reason to revisit.
A single root PKI means that whoever controls the root controls
everything. EVERYTHING. To quote Davros 'that power would set me
amongst the Gods'.
The attempt to set up a single root in DNSSEC is the reason I consider
the project dangerous and foolhardy. I have been speaking out against
it now for ten years. That is not because I want to kill DNSSEC, it is
because it is a fixable error and one that I want to see fixed.
Its not just me who has this problem. Two gentlemen, one of who I know
to be ex-KGB (now GRU) I presume tried to explain the problem to Steve
Crocker some years ago to no effect. Crocker isn't at all worried
about the possibility he might do something others disagreed with
after they have no opportunity to change service providers.
Its the world wide WEB, not the world wide top down hierarchy with the
US government at the top of the pyramid.

@_date: 2014-05-14 12:56:53
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
This is the main reason I am interested in Certificate Transparency
for DNSSEC. Otherwise it is not much point.

@_date: 2014-05-15 16:26:24
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
The question actually makes my point: We very rarely rewrite an old
protocol just to change the syntax.
JSON has become established as the way to write new protocols. We are
a long way from the point where people are going to want to redo DNS
in JSON just to have JSON syntax.
And writing a completely new protocol, JSON is a lot more than 10%
better than ASN.1 or XML because both of the latter are bjorked. XML
prefixes are insane, it is a document markup language, not a protocol
data encoding layer.
One of the epiphanies that came to me in the design of a JSON-schema
tool was that in a protocol design schema validation is utterly
useless. The reason we need schema validation in XML is that in a
document editor it is useful to be able to tell the user if their
markup is valid or not.
XML schema allows a designer to require elements to come in a
particular order. That is utterly pointless in a protocol unless the
order is indicating the slot to fill.
So if you take a look at the reasons why JSON is succeeding, I think
you will find it is much more than a 10% improvement. And in any case,
XML never achieved ubiquity anyway. JSON is replacing RFC822 headers
rather than XML. And it is better because it supports structure and is
more consistent.

@_date: 2015-12-09 01:07:02
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Who needs NSA implants? 
Sent from Outlook Mobile
    _____________________________
Sent: Tuesday, December 8, 2015 5:11 PM
Because diverting them will let NSA flash BIOS trojans (or hard drive
firmware trojans).  All three of the issues that you mentioned are
resolved if you merely wipe the hard drive upon reciept.  NSA prefers
exploits that survive hard drive erasure and installation of a fresh
OS of your choice.
The cryptography mailing list
cryptography at metzdowd.com
You have to divert the dells.etc or else folk could use Diversion as an oracle for has vulnerabilitySeems qnap is secure..,

@_date: 2015-06-17 10:40:26
@_author: Phill 
@_subject: [Cryptography] password fatigue;  was: Lastpass 
I consider the cost of remembering a password to be $100 each. So unless the asset being secured belongs to me and is worth more than $100, I use the same password.
If the site requires capitals and a number: Useless1
I dont see a real need for zero knowledge. That might be nice to have but public key is OK. All that matters is that the authentication process does not disclose the authentication credential.
But to be useful, a system has to do more than allow passwords to be replaced, it has to service the users need to remember all the stupid passwords. Here is where I think we have been going wrong, we have seen these as alternatives:
1) A password management system for storing passwords in the cloud
2) A public key based auth scheme that eliminates the need for passwords.
Any system that gets adopted by users has to be both.
As part of my prismproof project I have been writing a service to manage private keys. I want these to be very easy to use, completely transparent. Which forces me to use a cloud service. But I am not forced to use a trusted service. In fact the degree of trust in the service is very limited. The service never gets any confidential data, all the data is encrypted. It occurs to me that I can use exactly the same infrastructure for passwords as for private keys.
The basic idea is as follows:
0) The user never needs to be an expert.
1) Every device has its own private keys, every independent purpose (authentication, code signing, encryption, etc) gets its own keys. Unless these are email decryption keys, they are per-device.
2) Each user has a personal virtual network consisting of the devices they own. Devices can be added or removed at any time. To add a device, the user tells it which personal network to join, a join request appears on their authorized admin devices, they approve it. 3) Recovery is supported through a master key signing key and an encryption escrow key. By default, each time an encryption keypair is created, the decryption key is escrowed.
A number of keys have to be migrated from one device to another. In particular, end to end email encryption is only practical if the user can share their decryption key on all the machines they read email on. [Right now I am moving the key itself, when we move to ECC based algorithms it becomes possible for split key schemes to be employed. One half of the key residing at a quasi trusted service, the other on the device that reads the mail.]
So to address this use case, I store the encrypted private key in the cloud and upload decryption blobs for every device that is authorized to access it. This same structure can be used to store passwords. When I tell my browser to remember a password, it encrypts it under the set of keys for the currently authorized devices. When a browser needs to access a site, it decrypts the username/password combo for that site.
This provides the same convenience as lastpass without the need to trust the service. There is even a mechanism that bounds the ability of the service to perform a DoS attack - the encrypted blobs are shared across a surface of independent providers and a blockchain type mechanism used to prevent default. If sites prefer to abandon the username/password combo, the same infrastructure supports public key based authentication. However this may not be essential if we can get the scheme widely supported since if a user can guarantee that their username/password will be accessible from every site they use, they can use a randomly generated password with a very high work factor comparable to a 128 bit key as there is no need to remember it.

@_date: 2015-10-05 08:57:40
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Spec for SSLv1 
Basically SSLv2 but without any authentication checks.
"He's talking about an authentication attack, Marc"
On Mon, Oct 5, 2015 at 1:04 AM, Yuhong Bao

@_date: 2015-10-05 08:59:20
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] Spec for SSLv1 
Basically SSLv2 but without any authentication checks.
"He's talking about an authentication attack, Marc" - comment from the only
time it was shown in public

@_date: 2016-11-14 09:30:59
@_author: Phillip Hallam-Baker 
@_subject: [Cryptography] October 28th is now National Cryptography Day 
There can be an international cryptography day as well. Preferably in the
But there is a tactical advantage. Comey is going to be trying to push his
anti-crypto policies, delegitimizing the man, the office, the institution
he has soiled is the best way to push back.
Rather than having a conversation about what controls should be placed on
cryptography, I want a discussion on how the FBI is going to be broken up
and reformed.
Taking J Edgar Hoover's name off the FBI HQ isn't going to change anything.
Putting that issue on the agenda will cripple attempts to extend FBI power.

@_date: 2016-09-07 10:01:21
@_author: Phill 
@_subject: [Cryptography] Strong DNS Names 
A while ago, I proposed a new form of strong email address that combined a PGP fingerprint like identifier with an email address:
MB2GK-6DUF5-YGYYL-JNY5E?alice at example.com The idea of this scheme is that MB2GK-6DUF5-YGYYL-JNY5E is the fingerprint of a key under which a policy for sending mail to alice at example.com  is signed. That might have statements like emails must be signed, use PGP, etc.
Note that as with Tor, merely having the address does not mean that you can use it. But that is what we want.
The Tor scheme is better in some ways.
alice at example.com.MB2GK-6DUF5-YGYYL-JNY5E.onion But the .onion means Tor and I dont want to have .openpgp, .smime etc. It would not work. I have a better idea:
alice at example.com.MB2GK-6DUF5-YGYYL-JNY5E
There is no need for a suffix at all. The probability of an accidental collision here is 2^92 and we can use a variety of techniques (e.g. work hardening) to increase the work factor. We can even pile on more characters if need be.
No need for permission from ICANN either. Their US government mandate expires in a few months and I dont recognize their new one.
From a deployment perspective, we can (and should) allow clients to retrieve policies from their trusted DNS server by simply adding a line to the effect that if the TLD is more than 24 characters long, interpret it as a UDF key fingerprint.
The new strong DNS addresses are compatible with pretty much every existing email client. Just route the inbound and outbound email through a proxy that strips off fingerprints from strong email addresses, fetches the policy and acts accordingly. Users can compose and read email just like normal. The only difference being that their email is now encrypted end to end (but not in client storage).
Yes, a proper client that does this native is better but only if the product is done really-right rather than botched and bungled user experience that Microsoft, Apple and Thunderbird offer. I greatly appreciate the heroic efforts of people writing plug ins but having written some myself I dont think that is a viable model to produce world class user experience. The base applications are just too buggy.
The chief security issue here is that if we are talking about alice at eop.gov  or alice at microsoft.com  or the like, we want to make sure that we are talking to the implied domain if it matters. Which is something that enterprises can do with an appropriate CAA record requiring messages to that domain be supported by a suitably validated cert.
