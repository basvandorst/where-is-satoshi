
@_date: 2013-12-15 08:16:20
@_author: Nemo 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and 
Linux /dev/urandom already XORs against RDRAND, but using its own
homegrown hand-waving entropy collector instead of Yarrow.
"Guarantees" is perhaps too strong a word should Intel turn out to be an
 - Nemo

@_date: 2013-12-30 16:33:25
@_author: Nemo 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
Close, but not quite...
C99 and C11 restrict signed integer representations to ones-complement,
twos-complement, and sign/magnitude. Also some of the bits can be
"padding" and ignored.
The same phrasing never made it into any C++ spec, including C++11. So
in C++, integer representations are considerably less constrained, at
least in theory (see e.g. However, none of this has anything to do with overflow of signed integer
arithmetic. Signed integer overflow is simply undefined behavior,
period. (What you call "quality of implementation" some would call
"non-portable vendor-specific extensions".)
Undefined behavior does not mean "something strange happens"; it means
the compiler can assume you don't do that and optimize accordingly.
My favorite example:
    int foo(int n)
    {
        return n < n+1;
    }
If you compile this with any modern compiler, enable optimization, and
call it with INT_MAX as argument, it will return *true* (i.e. 1). (Even
though if you print INT_MAX+1 you will probably see a large negative
power of two.)
The compiler's reasoning goes like this. Signed integer overflow is
undefined behavior, which I can assume never happens. If n equals
INT_MAX, this function will perform signed integer overflow. Therefore,
n cannot equal INT_MAX and this function may be optimized to return the
constant 1. (Check the assembly code if you doubt this.)
Whether such a language is suitable for writing secure code is certainly
a matter of opinion. But before having such an opinion it helps to know
what the language acutally *is*. :-)
[Aside: For what it's worth, I found the bug in
 in less than two
minutes. But then, I have read and written an awful lot of C and C++
code, and the fact that something "obfuscated" is hiding there gives a
huge hint to stare at the macro.
The two classic macro bugs are (1) variable capture (as illustrated) and
(2) this sort of thing:
     SQUARE(x) ((x)*(x))
    return SQUARE(random() % 16); // probably not what you intended
End aside.]
 - Nemo

@_date: 2013-12-31 11:16:05
@_author: Nemo 
@_subject: [Cryptography] QUANTUM, QFIRE, etc. 
I saw some discussion of the Q Division -- er, I mean "ANT Division" --
gadgetry, but I have not seen mention of these new QUANTUM slides on
this list:
Sure, physical bugs in your monitor/keyboard/USB cable are neat, but the
Internet-wide surveillance is a lot more disturbing, in my opinion. Or,
as I like to phrase it, there is a huge and important difference
betweent the power to spy on *anybody* and the power to spy on
*everybody*. (Admittedly, the line blurs when systems subject to
"targeted" attacks run into the hundreds of thousands...)
Anyway, the QFIRE slides are perhaps the most interesting. I
particularly liked the bit about using "Non-cooperative Wireless Access
Points" to achieve low-latency payload injection on hijacked TCP
 - Nemo

@_date: 2013-11-12 10:27:55
@_author: Nemo 
@_subject: [Cryptography] /dev/random has always been a poor design [was Re: 
Well now, let's see. The stated intent of /dev/random is for people who
"don't trust the crypto", whatever that means. (I would say another word
for such people is "morons", but never mind.)
For any non-cryptographic purpose, a cryptographically strong
pseudo-random generator is just as good as truly random
data. "Indistinguishable", one might say.
For any cryptographic purpose, you either "trust the crypto" (for some
value of "crypto"), or you are using a one-time pad.
Therefore, there is exactly one application where /dev/random is needed,
and that is for generating one-time pads. Used any lately?
On the other hand, /dev/urandom is never useful for cryptography, since
there is no way to know whether the state has been properly seeded.
This total disaster of a design has been pointed out repeatedly for 15+
years. It is completely obvious, but it has not been fixed because the
Linux /dev/random maintainer(s) *do not understand cryptography*.
For example, when someone in academia identifies some powerful attack
model, shows a PRNG design that thwarts it, and then publishes a paper,
the /dev/random maintainer(s) reaction is always of the form "but if
they could do that, they must have root access on the system blah blah
blah". They will go off on some tangent about the uselessness of formal
software verification or the irrelevance of "academic" proofs or
whatever. Basically, they will demonstrate for the N_th time why they
should never be allowed near any critical code, never mind cryptographic
Then there is the /dev/random implementation: That utterly ad-hoc,
unanalyzable moving target. "NO THAT'S FIXED IN MY TREE! THE AUTHOR
ISN'T USING THE LATEST GIT!!!1!" Seriously?
If organizations like NSA are the adversary, simple designs with
provable attributes are an absolute necessity, because quite frankly
they are smarter than all of us combined. Linux /dev/random is a
But hey, thanks for trying once more, John. Based on history, there is
zero reason to be optimistic, but I would love to be wrong.
 - Nemo

@_date: 2013-10-20 10:56:56
@_author: Nemo 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
What would break is the /dev/random maintainers' brains.
This exact same discussion comes up every 5-10 years. At some point, one
or more people who actually know something about cryptography give a
critique of the unanalyzable continuously moving target that is Linux
For example:
And every time, the Linux /dev/random maintainer(s) demonstrate why
he/they should never be allowed anywhere near cryptographic code.
Good luck fixing this.
 - Nemo

@_date: 2013-09-09 10:37:09
@_author: Nemo 
@_subject: [Cryptography] Seed values for NIST curves 
I have been reading FIPS 186-3 (
 and 186-4 (
 particularly
Appendix A describing the procedure for generating elliptic curves and
Appendix D specifying NIST's recommended curves.
The approach appears to be an attempt at a "nothing up my sleeve"
construction. Appendix A says how to start with a seed value and use SHA-1
as a psuedo-random generator to produce candidate curves until a suitable
one is found. Appendix D includes the seed value for each curve so that
anyone can verify they were generated according to the pseudo-random
process described in Appendix A.
Unless NSA can invert SHA-1, the argument goes, they cannot control the
final curves.
To my knowledge, most "nothing up my sleeve" constructions use clearly
non-random seed values. For example, MD5 uses the sines of consecutive
integers. SHA-1 uses sqrt(2), sqrt(3), and similar.
Using random seeds just makes it look like you wanted to try a few -- or
possibly a great many -- until the result had some undisclosed property you
Question: Who chose the seeds for the NIST curves, and how do they claim
those seeds were chosen, exactly?
 - Nemo

@_date: 2013-09-10 09:43:58
@_author: Nemo 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was Re: 
(I apologize this message is not correctly threaded. I only subscribed
to the list recently, and I have found no way to cannot construct a
proper In-reply-to header from messages in the archive.)
I agree with everything you have said, except for this.
"GET / HTTP/1.1\r\n" is exactly 16 bytes, or one AES block. If the IV is
sent in the clear -- which it is -- that is one plaintext-ciphertext
pair right there for every HTTPS connection.
In fact, _any_ aligned 16 bytes of plaintext in the conversation that
are known, or that are in a guessable range, represent a
plaintext/ciphertext pair if either of the following are true:
    1) You sent the IV in the clear
    2) You used CBC mode
Of the modes I know (CBC, CTR, GCM, et. al.), the only one that does not
freely give up such plaintext/ciphertext pairs is OCB.
Of course, we assume our block ciphers are secure against even
astronomical numbers plaintext/ciphertext pairs, because any evidence to
the contrary would represent a publication-worthy if not Ph.D.-worthy
But is it really such a good assumption against _this_ adversary?
It seems to me that the IV could easily be negotiated at the same time
as the session key. 2048-bit or 3072-bit RSA or DH already provide
enough bits, so you can have a secret IV, independent of the session
key, "for free". ECDH provides enough negotiated bits, too, once you are
in the 256+ bit range.
So, "avoid CBC" plus "negotiate the IV" seems like a simple way to stir
extra entropy into the encryption. It does nothing for any security
proofs, since those assume perfectly secure block ciphers... But it
might make somebody's job just a little bit harder in practice. And
since it would cost nothing, why not?
 - Nemo

@_date: 2013-09-10 16:27:16
@_author: Nemo 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
Not sure what "not transmitted" means here. In typical CBC
implementations, the IV is certainly transmitted...
As written, this does nothing to deny plaintext/ciphertext pairs further
along in the stream. Typical encrypted streams have lots of
mostly-predictable data (think headers), not just the first 16 bytes.
I agree with Perry; a reference to a paper would be nice.
I think you mean BEAST.
The security proof of CBC against Chosen Plaintext Attack requires that
the IV be unpredictable to the attacker. (I am working my way through
Dan Boneh's lectures on Coursera. Great stuff.) This was a "purely
academic" consideration, until BEAST came along.
Which leads to a personal pet peeve... If NSA is your adversary, then
**there is no such thing as a "purely academic" attack**. Any weakness,
no matter how theoretical, is worth avoiding if feasible. Implementors
keep making this mistake again and again -- "it's a purely academic
attack because blah blah blah so relax" -- and then something bad
happens years later. It would be nice if we could all finally learn this
Back to CBC mode and secret IVs. I do not think we will too find much
guidance from the academic side on this, because they tend to "assume a
can opener"... Er, I mean a "secure block cipher"... And given that
assumption, all of the usual modes are provably secure with cleartext
IVs. Nonetheless, there is no danger in keeping IVs secret, so why not?
Negotiating 512 bits of secret costs little more than 256. So just
negotiate the IVs. Or, more plausibly, negotiate a second key to encrypt
the IVs. (Since you never reuse an IV anyway, ECB mode for the IVs is
All of this is secondary to securing the key exchange, of course. That
part is much more scary because NSA's math skills are scary. In my
opinion, it is virtually certain NSA knows something about integer
factoring and/or integer discrete log and/or elliptic curves that we do
not. So I would build in some margin.  I would start with 3072 bits for
RSA/DH and 384 bits for ECC and only go up from there...
 - Nemo

@_date: 2013-09-11 14:57:16
@_author: Nemo 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
Wrong, according to the Rogaway paper you cited.  Pull up
 and read the last
paragraph of section I.6 (pages 20-21).  Excerpt:
    We concur, without trying to formally show theorems, that all of the
    SP 800-38A modes that are secure as probabilistic encryption schemes
    -- namely, CBC, CFB, and OFB -- will remain secure if the IV is not
    perfectly random, but only unguessable.
Thank you for the reference, by the way; it is an excellent paper.
If you are going to call me wrong in a public forum, please have the
courtesy to be specific. My statement was, in fact, correct in every
To rephrase:
Security proofs for block cipher modes never depend on keeping the IV
confidential from the attacker. Standard practice (e.g. TLS, SSH) is to
send it in the clear, and this is fine as far as "provable security" is
Rogaway's paper does point out, among other things, that naive handling
of the IV can break the security proofs; e.g., for the scheme you
described earlier in this thread and incorrectly attributed to Rogaway.
My point is that if the IV can be kept confidential cheaply, why not? (I
am particularly thinking of CTR mode and its relatives.)
 - Nemo

@_date: 2013-09-11 16:03:44
@_author: Nemo 
@_subject: [Cryptography] Summary of the discussion so far 
First, I suggest removing all remotely political commentary and sticking
to technical facts.  Phrases like "questionable constitutional validity"
have no place in an Internet draft and harm the document, in my opinion.
Second, your section on Perfect Forward Secrecy ignores the purpose of
PFS, which has nothing to do with defense against cryptanalytic attacks.
The purpose of PFS is this: Should an attacker compel you to disclose
your private key, or should they compromise or confiscate the system
where your private key is stored, they could then decrypt all of your
earlier communications...  unless you used PFS.
 - Nemo

@_date: 2013-09-11 16:34:06
@_author: Nemo 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
Rogaway provides the definition in the paragraph we are discussing...
That "e(q,t) value defined above" is the probability that the attacker
can predict the IV after q samples given time t. That appears to be a
very precise definition of "predictability", and the smaller it gets,
the closer you get to random-IV security.
But enough of this particular rat hole.
Fair enough; I apologize for my flippancy. Of course the assumption of a
"strong block cipher" is justified by massive amounts of painstaking
effort expended in attempts to crack them.
Nonetheless, I think it would be wise to build in additional margin
anywhere we can get it cheaply.
I doubt we will have provable complexity lower bounds for useful
cryptographic algorithms until well after P vs. NP is resolved.  That
is, not soon.
Until then, provable security is purely about reductions. There is
nothing wrong with that. And as I said before, I believe we should worry
greatly about theoretical attacks that invalidate those reductions,
regardless of how "purely academic" they may seem to an engineer.
Yes, obviously... which is why I wrote "I am particularly thinking of
CTR mode and its relatives".
It's a pity OCB mode is patented.
 - Nemo

@_date: 2014-04-18 22:21:47
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
(1) C++11 intptr_t is adopted from C99 ( and headers, respectively)
(2) It is not mandatory; it is optional, both in C99 and C++11. But if
the typedef exists at all, it must have the semantics you describe.
Re: A C compiler is a compiler for the C language. The C language is, by
definition, the language described by the C specification. You should
expect compiler authors to do everything allowed by that specification
to generate faster code. If you cannot handle the semantics of the C
language, perhaps C is the wrong language for your task, or for you. Do
not blame the compiler writers if you don't know the language.
At any rate, this is the attitude of the GCC developers. It is also the
attitude of the Clang developers:
Re: bounds checking
In C++, it is trivial to create "SafeInt" and "SafeArray" classes that
overload addition and dereference, respectively, so that expressions
like "x+y" check for overflow and expressions like "a[n]" check for
out-of-bounds access. That so many programmers write security-sensitive
code without using such simple idioms is hardly the fault of the
 - Nemo

@_date: 2014-04-19 10:54:25
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
Of course you can have SafeInt constants, just like you can have any
other type of const object:
static const SafeInt twelve(12);
You can also provide an implicit cast-to-int operator that will let you
use your SafeInt in a "switch" statement.
Live example: Since int and SafeInt can cast to/from each other transparently, it is
easy to imagine converting an existing code base from to the other
Perhaps there are useful properties of "int" that "SafeInt" cannot
imitate, but your examples so far do not demonstrate any.
This is not just easier in C++, but trivial. C++11 introduces
"initializer list constructors", so filling any container (array,
vector, set, etc.) with constants is expressed as easily as built-in
array/struct initialization in legacy C/C++.
Good workmen never quarrel with their tools.
 - Nemo

@_date: 2014-04-19 19:49:29
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
Oh, I see... You meant "literal", not "constant".
You might be interested in C++11's user-defined literals, which let you
define "_safe" as a literal suffix so that you can write:
  s = 12_safe * s;
(Live example again: I concede the equivalent is messier in C++03. Although not by much; is
writing "SafeInt(12)" really so hard?
 - Nemo

@_date: 2014-04-20 09:54:15
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
Perhaps. But in the case of OpenSSL, the problem quite unambiguously
lies in the authors, not the language.
The OpenBSD folks -- who, unlike OpenSSL, actually know what they are
doing -- are taking a scalpel to the code base. Well, more like a
chainsaw. You can follow the most egregious / hilarious bits here:
  This is really not complicated. OpenSSL is buggy and insecure because
the code is garbage, plain and simple. Anyone for whom this is not
completely obvious should do everyone a favor and not attempt to write
cryptographic code.
 - Nemo

@_date: 2014-04-20 18:38:57
@_author: Nemo 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers in C) 
Some years ago, I put a sign above my monitor that reads, in large block
letters, "DO NOT CORRECT PEOPLE ON THE INTERNET".
But what the heck, it's a holiday.
Guess what? _Every_ modern C compiler will "remove" the same "safety
See for yourself. Browse to here:
...and select a compiler, any compiler, from the drop-down menu at the
upper right. Stare at the assembly and see if you can find the
Every modern compiler does this because IT IS NOT A VALID SAFETY
CHECK. It has never been a valid check, in any version of C, since at
least 1989. If you do not know this, then you do not know C.
As an aside, one compiler is kind enough to warn about this invalid code
if you ask it to:
Developers who cannot handle the semantics of the language can paper
over their (broken code / lack of understanding) with "-fwrapv" and
"-fno-delete-null-pointer-checks" and "-fno-strict-alias" and so forth,
but honestly, it is probably better for all concerned if they simply
switch to a "safety-scissors and sippy-cups" language like C# or Java or
Have no fear; I am quite confident the GCC developers will give your
opinion all of the consideration it is due.
 - Nemo

@_date: 2014-04-20 18:51:08
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
Please, please, please say you are joking.
Have you actually LOOKED at the commits? "Garbage" is too kind a word
for the OpenSSL source. That fetid pile of bug-ridden, unauditable
excrement could be in "real-world use" for 50 years, and it would still
be a fetid pile of bug-ridden, unauditable excrement.
There is a lot of bad code in the "real world", because there are a lot
of bad programmers in the world. Regrettably, some have roles like
"Principal Engineer" or "Linux /dev/random maintainer".
But reality has a way of asserting itself in the end. Heartbleed is just
the latest and greatest in a long line of disasters in a laughable
implementation of a poorly-conceived protocol. More will surely follow.
It need not be like this. Some developers really are capable of writing
solid code, while others are not. Far too few understand this.
If you can look at the OpenSSL source and not feel a bit of vomit in
your mouth, then I do not know what to say to you. But I do know I want
you nowhere near any system I rely upon for my privacy.
 - Nemo

@_date: 2014-04-21 09:46:56
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
This is getting increasingly off-topic, and I need to get back to
work... But this is simply not true.
"A" for effort, but that is not the C standard.
"This manual contains a summary of the syntax and semantics of the C
programming language as implemented on SGI workstations."
So you are not reading the standard. You are reading the manual for a
particular C compiler on a particular system that no longer exists.
The C standard looks more like this:
See section 6.3.2.3 paragraphs (5) and (6) and section 7.20.1.4.
As a general statement, your conclusion is incorrect.
"Learn your tools" is a good general principle, in my ever-humble
 - Nemo

@_date: 2014-04-21 21:56:19
@_author: Nemo 
@_subject: [Cryptography] LibreSSL (was Re:  bounded pointers in C) 
Yup, still funny even a few days later.
Anyway, the project to end the OpenSSL joke has a name:
  OpenSSH worked out pretty well, I think. Time will tell about this one.
 - Nemo

@_date: 2014-04-22 18:32:30
@_author: Nemo 
@_subject: [Cryptography] bounded pointers in C 
(DO NOT CORRECT PEOPLE ON THE INTERNET... I think I need to increase the
font size)
Only the last of those is guaranteed by the C specification. If you ever
see someone write "sizeof(char)", you can go ahead and fail their
interview because they don't know C (or C++).
First, I do not need a counter-example because I HAVE A SPEC. The
language is defined independently of any implementation; this is one of
the strongest arguments for using C at all, actually. If you code to the
spec, you can write routines that are guaranteed to run correctly both
on today's systems and tomorrow's.
But I realize there are lots and lots of bad programmers in the world
who will not find this argument convincing. So fine, here:
"There are more things in heaven and earth" etc. That is why we CREATE
specs and DESIGN TO them and... oh never mind
 - Nemo

@_date: 2014-04-24 09:34:55
@_author: Nemo 
@_subject: [Cryptography] Swap space (Re:  It's all K&R's fault) 
Well, Windows does not use fork()+exec(); it uses spawn() and its
variants. Hence it avoids the whole vfork() / "memory overcommit" mess.
As a quick reminder, the mess is that fork() clones the entire virtual
address space. So if a huge process is trying to start a tiny
sub-process, the system will need twice the VM (following the fork()),
but only momentarily (until the exec()). A spawn() interface, combining
fork()+exec() into on fell swoop, does not suffer from this problem.
POSIX actually added a spawn function
but too late and with a bloated interface. I am pretty sure I have
literally never seen it used.
Also, a little bit of swap is nice if you have lots of little processes
in mostly doing nothing in the background, which you almost certainly
Encrypted swap is great for servers and always-on desktops, and
obviously it should be the default in such environments. But it wreaks
havoc on suspend/resume, which is built on top of swap (at least on
Linux the last time I checked). This strikes me as tricky, though not
impossible, to solve... I do not know whether anybody has.
 - Nemo

@_date: 2014-04-24 14:13:24
@_author: Nemo 
@_subject: [Cryptography] Swap space (Re:  It's all K&R's fault) 
Both Jerry and Tom have given good replies. Here is mine.
As Jerry mentioned, the issue is *virtual* memory (VM). Both the
original process and the fork-ed child have a full copy of the virtual
address space. You either (a) have sufficient RAM+swap to back all of
the writable VM or (b) fork() will fail or (c) you are Linux and have
vm.overcommit_memory enabled.
That last is the default on at least some Linux distributions, and it
means a process can get killed simply because an unrelated process
decides to write to its own _aleady-allocated_ memory. A truly lovely
failure mode, as a quick search for "oom-killer" will demonstrate.
As for vfork()... Yes, on Linux, it is no faster than fork(). But its
original purpose was not just speed.
(I say "was" because vfork() has since been removed from POSIX, probably
at the same time posix_spawn() appeared.)
vfork() traditionally suspends the parent process until the child calls
exec(), and the child is supposed to do pretty much nothing other than
call exec().
That is, the purpose of vfork() was to let you implement spawn(). (Prior
to Linux, no O/S even considered the "overcommit_memory" approach
because, let's face it, it's idiotic.)
 - Nemo

@_date: 2014-04-25 09:17:52
@_author: Nemo 
@_subject: [Cryptography] GCC bug 30475 
What does that do if al is -1?
The usual approaches are more complex. See:
  ...and everything it references, especially:
  On the bright side, you only have to solve this problem once.
 - Nemo

@_date: 2014-04-25 09:24:40
@_author: Nemo 
@_subject: [Cryptography] GCC bug 30475 
GCC and Clang have done some work along these lines with
  But (a) it will never be complete and (b) if it ever were complete, the
performance cost would be so high that it would not be C anymore.
 - Nemo

@_date: 2014-04-26 21:44:02
@_author: Nemo 
@_subject: [Cryptography] GCC bug 30475 
C99 introduced the "pinning down" to twos-complement, ones-complement,
and sign-magnitude representations (plus padding bits).
No C++ standard ever adopted any similar language, by the way, leading
to some truly obscure possibilities:
        UnsignedType n = -1;
...is guaranteed to work, for every unsigned integral type, by every C
and C++ standard ever.
C has  which is mandated to provide SHORT_MAX, INT_MAX,
LONG_MAX, etc. This does not help if it's an unknown typedef, but it is
The C++ standards all specify  which provides the
std::numeric_limits type traits, allowing:
    MySignedType n = std::numeric_limits::max();
Computations based on types are usually easier in C++. Whether that is a
strength or a weakness depends on your point of view.
 - Nemo

@_date: 2014-08-01 09:53:28
@_author: Nemo 
@_subject: [Cryptography] You can't trust any of your hardware 
DFU has been part of the USB protocol from the beginning:
Have a nice day.
 - Nemo

@_date: 2014-08-02 17:54:11
@_author: Nemo 
@_subject: [Cryptography] You can't trust any of your hardware 
Yes, that is one idea on the right track. Another is to have the device
only accept firmware updates with an appropriate cryptographic
The iPhone uses both of these approaches.
    Plug your device into your computer with a USB cable.
    Turn off the device.
    Hold the Power button for 3 seconds.
    Hold the Home and Power buttons for 10 seconds.
    Release the Power button but keep holding the Home button.
    After about 15 seconds you will be alerted by iTunes saying that it
    has detected a device in Recovery Mode.
Granted, firmware signature schemes do have this funny tendency to get
Still, unless it was designed by idiots, any firmware update procedure
obviously must require the user to diddle with the physical controls in
some way. Regrettably few devices meet the precondition.
I dunno... How many iPhones are there?
I own one of these:
"Software updates are available"
Does my USB printer count as a "USB device"?
I am fairly certain updatable USB devices are the norm, not the
 - Nemo

@_date: 2014-02-01 09:48:08
@_author: Nemo 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 
If the function is many-to-one (like, say, a hash function), then your
definition of "invert" is pointless because it is vacuously
impossible. For example, the function "x modulo 12" is non-invertible by
your definition. This has nothing to do with cryptography.
If, on the other hand, the function is one-to-one (like, say, a block
cipher with a fixed key), then your definition of "invert" is equivalent
to your definition of "finding a preimage". Again, this has nothing to
do with cryptography.
Either way, it seems totally pointless to distinguish the concept of
"invert" from "find a preimage" in cryptography. And indeed I have not
seen this distinction in practice (see
e.g. Or am I missing something?
 - Nemo

@_date: 2014-02-03 09:40:28
@_author: Nemo 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 
I disagree; I think the distinction is irrelevant. Let me try once more.
You seem to be missing my point, which is this: *It is impossible to
invert any hash function, ever, by your definition*. In general, for any
output y, there are many (or an infinity) of possible inputs x such that
f(x) = y. If all you know is y, even if you can determine *every*
possible x, you have no way to know which was the actual input.
To make your definition of "invert" meaningful, you need to incorporate
a distribution of possible inputs, and then say something like "to
invert a hash function at y means to determine the _most likely_ input x
such that f(x) = y". But I have never seen such an elaborate definition,
either in a theoretical context or a practical attack. Have you?
What I have seen is the simple definition "To invert a function f at y
is to find any x such that f(x) = y", followed by the simple definition
"A one-way function is easy to compute but hard to invert". (The only
subtle part being the definitions of "easy" and "hard".)
I do not see how adding additional complexity to these definitions
contributes anything other than confusion.
 - Nemo

@_date: 2014-06-02 10:57:41
@_author: Nemo 
@_subject: [Cryptography] Is it mathematically provably impossible to 
"Sun, 01 Jun 2014 10:34:22 +0200")
This assumes your program can feed _itself_ to a function it is
calling. Which is true, of course, but usually requires a bit of clever
This is a generally under-appreciated point worth repeating (and
Although it is impossible to build a function that correctly answers
"yes" or "no" to "is this program safe?", it is very possible to build
functions that answer "yes" or "I don't know".
The same principle applies during code review. Your code needs to be not
only correct, but simple enough to be "obviously correct". Otherwise it
will fail my review and I will ask you to rewrite it.
To build secure systems, we do not need to be able to detect all back
doors; we just need to be able to write code that provably does not have
back doors. The former is impossible; the latter is not.
 - Nemo

@_date: 2014-06-02 11:11:05
@_author: Nemo 
@_subject: [Cryptography] What is going on with TrueCrypt? 
(I am not a lawyer, so what follows is based on my non-expert
What would happen if someone just cut&pasted the TrueCrypt source
wholesale and put his/her own name, copyright, and license on it?
The only ones with legal standing to sue for infringement are the
authors, who are anonymous. They (he?) would have to give up that
anonymity just to initiate legal action. Not to mention engaging a bunch
of lawyers for a settlement worth zero.
Then there is the little detail that even the GPL has only ever been
deemed enforceable by a couple of German courts... What would they make
of some poorly-phrased non-expert trash license, I wonder?
Would hijacking the TrueCrypt source like this be unethical?
Perhaps. But I would still love to see someone give it a whirl.
 - Nemo

@_date: 2014-06-09 17:14:33
@_author: Nemo 
@_subject: [Cryptography] Swift and cryptography 
"What we need most, at this juncture, is another programming language"
...said nobody over the age of 30, ever.
Why does every fresh CS grad feel the need to invent his own programming
language (and/or "make" replacement)?
I guess Apple has hired a few, so this one may take longer than usual to
fade away, unfortunately. On the bright side, I have had a recent debate
with some colleagues over whether Apple has jumped the shark, and this
should settle it.
Awesome! If only the C++ committee had thought of this and incorporated
it into the ISO standard back in 1998.
Oh wait, they did:
int n = std::numeric_limits::max();
This gadget behaves like a compile-time constant for most purposes in
C++98, and for all purposes in C++11.
Formally meaningless. Optimizers do not "remove" anything; they
transform code into semantically equivalent code, where "semantically
equivalent" is defined by the language specification. Concepts outside
of the language -- like "leaking data to the operating system" -- do not
generally appear in the language spec, and for good reason.
"Guaranteed erasing of data" is and always will be a platform-specific
concept, regardless of language.
I spent some time reading the Swift documentation today and -- when I
could see through my tears of laughter -- I saw nothing that might
contribute to any application domain, least of all cryptography.
But perhaps I am being too cynical. It has happened once or twice.
 - Nemo

@_date: 2014-06-10 16:06:47
@_author: Nemo 
@_subject: [Cryptography] Subject: Re:  Swift and cryptography 
An earlier reply of mine was rejected (go figure), so this time I will
try to make my points consisely and more-or-less impersonally. A couple
might even be on-topic.
Amazing what people will spend money on, isn't it?
Re: Dylan. The people, code, and ideas from Dylan have no relationship
to Swift. Browse to  and search for "drawing
ideas from" for initial evidence. Learn anything about the two languages
for additional evidence.
Re: Syntax. Programming language syntax is irrelevant and boring. (Also
the example I gave is quite easy to read for anyone with more than a
cursory knowledge of C++.) If your awesome new language's chief benefit
is its syntax, you are solving the wrong problem.
Re: Securely wiping data. This is a subtle problem, but also a solved
one. The solutions are platform-dependent -- e.g. OpenBSD has
"explicit_bzero" -- but they do exist. Examine the source for any decent
crypto library for evidence. Obviously, re-solving solved problems is a
waste of time. Indeed this is my basic point about Swift in general.
Swift appears to be little more than syntactic sugar over
C++/Objective-C, which is not surprising given its provenance. As far as
I can tell, every interesting feature is available in C++ -- especially
C++11 -- as a built-in feature (e.g. lambdas, shared_ptr) or an add-on
library (e.g. SafeInt).
Finally, and most on-topic: If the choice is between (1) a language with
an international standard and multiple competing world-class
implementations, and (2) a proprietary language with its associated
vendor lock-in (to a vendor known to collaborate with NSA), which should
we depend upon for our privacy?
 - Nemo
   P.S. I do admit to a secret fondness for Rust. Every cynic is a romantic
at heart.

@_date: 2014-06-11 09:40:59
@_author: Nemo 
@_subject: [Cryptography] Bitcoin compute power 
He mined all of $8000 on his "few supercomputers".
The aggregate power of the Bitcoin miners is approaching 100 petahashes
per second. Note "petahashes", not "petaflops". And it appears to be
growing steadily at something like 1000x / year:
    Given that most of the ASIC mining rigs are vaporware and/or scams (*),
how it is even possible is not too far from being an interesting
 - Nemo
   (*) for a fun waste of time, try a search for "minerscube" and "scam"

@_date: 2014-06-15 14:36:23
@_author: Nemo 
@_subject: [Cryptography] ghash.io hits 50% of the Bitcoin compute power 
Before answering questions about the Bitcoin software, may I suggest
learning something about it?
The Bitcoin software has a concept called a "checkpoint lockin":
  The 0.9.1 software includes a checkpoint at block 279,000:
  Whether checkpoints are a good idea -- in particular, whether they imply
that Bitcoin is not nearly as decentralized as advertised -- is a
legitimate matter for debate. But on a purely technical basis, the
current block number is 306,006, so the correct answer to John's
question is "nothing would happen because his software would reject any
attempt to rewrite that much history".
As for "fail massively in some spectacular way", there is less than zero
evidence to support that claim even for shorter-term blockchain
forks. First, spinning up a test Bitcoin network is trivial, and the
developers have done quite a lot of simulation of potential failure
modes, including forks. Second, the live network experienced a real fork
in 2013 (search for "blockchain fork"), during which all versions of the
software kept chugging along fine; they just disagreed about which
version of the blockchain represented reality...
 - Nemo

@_date: 2014-06-15 18:47:01
@_author: Nemo 
@_subject: [Cryptography] ghash.io hits 50% of the Bitcoin compute power 
Then it would follow its Prime Directive and accept as Truth whichever
fork represented the greatest total sum of work, where "work" is defined
per block as the expected number of hashes a miner would have to try to
generate the block (see Such temporary forks are not unexpected and are, in fact, inevitable
when two miners happen to a valid block sufficiently close together in
time. That is why the more confident you want to be about a Bitcoin
transaction having actually happened, the longer you have to wait.
 - Nemo

@_date: 2014-03-09 09:31:57
@_author: Nemo 
@_subject: [Cryptography] RC4 again (actual security, 
This entire discussion is premised on falsehoods.
The people attacking our systems, now and (especially) in the future,
are *smarter than we are*. That means they can and will imagine things
that you and I cannot.
There are two possible ways to deal with this fact: (1) Keep adding
complexity to your design until you do not see how to break it; or (2)
_simplify_ your design untl it is provably secure, based on minimal
assumptions, against "unrealistically" powerful attackers.
Academic cryptographers work on (2) because (1) has failed over and
over and over and over again.
So the question is not: "How can the adversary break our system?" The
question is: "How much power can we assume the adversary has and still
prove that we can win?"
Academic cryptography has discovered lots of concepts -- PRFs, PRPs,
IND-CPA, IND-CCA, etc. -- and proven that if you start with a
primitive satisfying one concept, and then you build a protocol around
that primitive like so-and-so, then you obtain a system that provably
satisfies some other concept.
There simply is no other rational approach to thwart people who are
smarter than you.
Now, RC4 has been known for at least 15 years not to satisfy *any*
relevant concepts as a cryptographic primitive. So it makes less than
no sense to use it in any design for the past 15 years, never mind the
In my experience, there are only two kinds of engineers: Those who get
all of this right away, and those who never will. (The Linux
practical standpoint, one core goal should be to keep the second kind
of engineer far away from all discussions, designs, and
implementations of anything remotely related to security.
 - Nemo

@_date: 2014-05-02 15:02:10
@_author: Nemo 
@_subject: [Cryptography] GCC bug 30475 
Only if your compiler actually emits x87 instructions, which it probably
will not unless you ask for a "long double" and/or target very old CPUs.
Modern x86/x86_64 processors have SSE (or AVX) instructions, which
operate on 2 (or 4) 64-bit doubles at a time with very low latency and
proper IEEE-754 behavior. Modern compilers tend to emit such
instructions, which are both faster and avoid the "extra precision".
Still, C and C++ are arguably too under-specified for serious numerical
I believe you botched this example. Consider the sequence:
  Thread 2 prints x ("0")
  Thread 1 assigns to x
  Thread 1 assigns to y
  Thread 2 prints y ("1")
As written, your example might output any of the four possible
combinations, even with no reordering of loads and stores.
The corrected version of this example is one I like to use myself; see
In another message, you write:
C++11 provides std::numeric_limits::is_modulo with exactly these
  ...but I know of no optimizing C++11 compiler for which it is "true".
 - Nemo

@_date: 2014-05-09 09:29:20
@_author: Nemo 
@_subject: [Cryptography] How to lock registers with GCC? 
GCC supports global register variables:
"Defining a global register variable in a certain register reserves that
register entirely for this use, at least within the current
compilation. The register is not allocated for any other purpose in the
functions in the current compilation, and is not saved and restored by
these functions."
So this is a step towards what you want, provided you compile the entire
application (including all libraries) with the same global declaration
However, as others have pointed out, your next problem is the operating
system. You would need to recompile the kernel with the same global
declaration visible there, too, so that the kernel's own code would not
use it. You would need to audit all of the assembly in the kernel to
eliminate any uses there, too.
Obviously, you would need to modify the context switch machinery not to
save and restore the register.
Finally, any other application running on the same core following a
context switch might clobber the register... So you would either need to
perform the same exercise for all applications on the system, or you
would need to use the "CPU binding" machinery (see "numactl
--physcpubind ..." for a starting point) to ensure your process and only
your process runs on a certain core.
All of this just to ensure that someone with total access to your
physical memory cannot steal your password. Is there anything else in
memory that might be of interest to such an attacker?
I wonder, though... How many registers would it take, in principle, to
encrypt/decrypt every memory access?
 - Nemo

@_date: 2015-01-13 14:23:52
@_author: Nemo 
@_subject: [Cryptography] Summary: compression before encryption 
Nice summary, with one nitpick...
Compression after encryption is nonsense. Under any modern definition of
"secure", a secure cipher's output is computationally indistinguishable
from random noise, which is not compressible.
Put another way, to compress the long-run output of a cipher is to break
it, by definition.
Excellent summary otherwise.
 - Nemo

@_date: 2015-01-25 14:47:30
@_author: Nemo 
@_subject: [Cryptography] Summary: compression before encryption 
CBC mode is provably secure against chosen-plaintext attack assuming the
underlying block cipher is. Pull up
 and search for
"CBC" for a proof.
CBC mode obtains the requisite non-determinism from its random IV. It is
a perfectly good mode, superior to many defined later (e.g. the
laughable "PCBC mode" created by overconfident M.I.T. undergrads). Lots
of good lessons in this particular history.
Anyway, this is extremely elementary stuff, so I am guessing you made a
typo and meant "ECB mode" (?)
 - Nemo

@_date: 2015-11-02 09:47:02
@_author: Nemo 
@_subject: [Cryptography] How programming language design can help us 
Or pass the "-fwrapv" flag to GCC or Clang
Your overall point is correct, of course. People should either actually
learn C -- it was standardized in 1989, for crying out loud -- or they
should stick to child-friendly languages.
Whines about compiler authors who implement the spec reveals much about
the whiner and nothing about the compiler authors.
 - Nemo

@_date: 2015-10-24 11:15:16
@_author: Nemo 
@_subject: [Cryptography] cryptography Digest, Vol 30, Issue 25 
C11 is current, but it doesn't matter; integer overflow is undefined in
every version of standard C and C++, ever.
This section is talking about integral conversions, which do have
implementation-defined (not undefined) behavior.
You asked for chapter and verse of the C99 spec, so let's start with
section 3.4.3:
    3.4.3
    1 undefined behavior
    behavior, upon use of a nonportable or erroneous program construct
    or of erroneous data, for which this International Standard imposes
    no requirements
    ...
    3 EXAMPLE An example of undefined behavior is the behavior on
    integer overflow.
So not only is integer overflow an example of undefined behavior, it is
the textbook example.
The formal language appears in section 6.5:
    6.5 Expressions
    ...
    5 If an _exceptional condition_ occurs during the evaluation of an
    expression (that is, if the result is not mathematically defined or
    not in the range of representable values for its type), the behavior
    is undefined.
And of course, as some have been trying to explain, optimizing compilers
routinely take advantage of this. Trivial example:
    int test(int x)
    {
        return x + 1 > x;
    }
If you compile this with optimization enabled on any modern C or C++
compiler, you will find the resulting code returns 1 (true)
unconditionally (live example:  ). For example, even
though you might print out INT_MAX+1 and see a negative number,
test(INT_MAX) will still return true.
This sort of internal inconsistency is to be expected when you engage in
undefined behavior. The compiler assumes you don't, so when you do it
anyway, you introduce a falsehood into the compiler's reasoning. And
logic tells us that from falsehood, anything follows ("ex falso
Undefined behavior is always a bug. Always.
 - Nemo

@_date: 2016-04-12 10:10:29
@_author: Nemo 
@_subject: [Cryptography] Is storing a hash of a private key a security 
I do not see how this follows. "Identifying" a key can be as simple as
saying "Key  That is completely different than knowing the actual
bits of the key.
More relevantly, an attacker who knows the hash can try candidate keys
offline without ever talking to the enclave. This matters if the private
key was produced by a low-entropy process (e.g. derived from a
password). Also he can tell whether two keys are (probably) identical,
which could matter depending on how the keys are used.
That said, crypgotraphic hash functions behave like random functions,
which means you cannot learn anything significant about the input from
the output. You can guess the input, and you can tell when two inputs
were (probably) the same, but that's it. So assuming a strong hash
function and private keys generated with sufficient entropy, there is
nothing wrong with this scheme.
 - Nemo

@_date: 2016-03-27 11:57:45
@_author: Nemo 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Actually, no, it is nothing like "a publicly validate-able digital
signature". And "Encrypt-then-Sign" is nothing like authenticated
In fact, this is the entire lesson of the recent JHU attack on
iMessage. See  and
Even Apple's engineers could not get this right. And some wonder why we
say, "For crying out loud, just use Bernstein's code"?
 - Nemo

@_date: 2016-03-27 14:30:16
@_author: Nemo 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Re-trying :-)
poly1305-AES is a Carter-Wegman MAC provably as strong as AES itself.
Are you saying you are not happy with AES?
 - Nemo

@_date: 2016-03-27 17:42:54
@_author: Nemo 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Thank you for being gracious even though I was not. I just get tired of
software engineers trying to design their own cryptography. How many
breaks do we have to endure because of this arrogance? The answer
appears to be "infinity".
My advice is more fundamental, which is "leave the cryptography to the
cryptographers". I am not one of them. I am not even sure there are any
on this list.
If your cryptographic protocol does not come with a security proof, then
you are doing it wrong. Note that a "security proof" is not really a
proof of security, because there is no such thing. Rather, it is a
collection of provable statements of the form "if an attacker could do
Y, they could also do X", where X is something everyone agrees looks
hard, like "violate IND-CPA for AES".
If you are not comfortable reading and creating such proofs, then my
advice is to obtain your protocol (and preferably your code) from
someone who is.
Or, more briefly: The first rule of cryptography is to use someone
else's design. The second rule of cryptography is to use someone else's
Unfortunately, in my experience, there are only two kinds of software
engineers in the world: Those who do not need to be told any of this,
and those who will not listen when told.
 - Nemo

@_date: 2017-07-05 17:42:16
@_author: Nemo 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Wait, so you are adding a function with the same (ill-chosen) name as
the BSDs, but giving it subtly different and less secure semantics?
That sounds like a significantly worse plan than not adding it at all.
If you really want the non-blocking behavior for ASLR or whatever, why
not give that API a different name? Call it arc4random_nb() or something
if you really like the ill-chosen naming scheme. Better yet, give it
some internal name and do not expose it at all.
If you insist on adding yet another cryptographic randomness API and
implementation -- a bad idea in general, although I guess the C library
is a beter place than most -- please try to do it right. The
never-blocking /dev/urandom on Linux has been a serious design flaw from
the beginning. Repeating that mistake today would be... Well, a step
backward, to put it mildly.
 - Nemo

@_date: 2017-06-12 15:37:01
@_author: Nemo 
@_subject: [Cryptography] Crypto Books, 2017 
Wow, thank you for this.
Boneh's Coursera class ( is
excellent, and this book appears to be based on the class notes.
(Incidentally, the online class just restarted today. It is free and
very, very good. If you decide to give it a whirl, I recommend making
time for the optional programming exercises. Implementation is the best
way to double-check and firm up your understanding.)
It does require some modest math+CS sophistication, but that is a
prerequisite anyway for any treatment beyond the trivial.
 - Nemo

@_date: 2017-06-27 10:42:09
@_author: Nemo 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Have you read "Cryptographic Right Answers" from Thomas Ptacek, who
actually knows what he is talking about?
    Use /dev/urandom.
    Avoid: userspace random number generators, havaged, prngd, egd,
    /dev/random
Why on earth would you put something as critical as random number
generation into some complex userspace monstrosity?
I am fairly confident you will ignore this advice, but could you please
do me one small favor? Go ask on crypto.stackexchange.com and see what
kind of response you get
 - Nemo

@_date: 2017-06-28 15:55:14
@_author: Nemo 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Actually, it is...
Nonsense. Default behavior of getrandom() is to pull from the
deny service to other processes reading /dev/urandom.
Seeding the generation of session keys and the like is precisely what
it is not useful, period.
Rich Salz says he is worried about system call overhead. Modern systems
are much, much better about this than your typical 1985 SunOS box; I
doubt it would even be measurable relative to the overhead of using
OpenSSL in the first place. But if you are really worried, read blocks
of 256 or 1024 or 4096 bytes from getrandom() and parcel them out in
pieces. (Yes above 256 you would have to handle EINTR and loop, just
like every call to read() in every production Unix application ever.)
Of course, the only reason even to use getrandom() instead of
the generator was properly seeded. But, you know, here we are.
I do not much care what you do for older platforms, because the set of
systems running 10-year-old OSes that are going to install the
next-generation OpenSSL is empty.
Anyway... Whatever clever user-space machinery you come up with, can you
please just disable it completely by default on any system with
getrandom() / getentropy()? TIA
 - Nemo

@_date: 2017-06-30 10:02:36
@_author: Nemo 
@_subject: [Cryptography] OpenSSL CSPRNG work 
That depends... Might any of those sources know something about your
internal state?
    But really this is just a detail. In the bigger picture, your question
itself is wrong.
For cryptographic code, every additional piece of machinery makes the
design harder to analyze rigorously. Every extra line of implementation
is just another opportunity to make a subtle mistake.
Unnecessary mechanism does not create "defense in depth"; it merely
increases the attack surface.
Since every piece of complexity is a liability, the right question is
not "Does this do any harm?", but rather "Is this necessary?" Each
additional mechanism should contribute in a clear (ideally: provable)
and meaningful (ideally: quantifiable) way to the security of the
system. Otherwise, leave it out.
Again, I humbly request that, whatever clever userspace machinery you
devise, please disable it completely by default on any system with
getrandom() / getentropy() / etc. (Key words are "by default". Sure,
provide APIs for enabling whatever you want... But by default, please
just use the system's provided mechanisms.)
 - Nemo

@_date: 2017-03-01 09:55:31
@_author: Nemo 
@_subject: [Cryptography] cryptography Digest, Vol 47, Issue 1 
No, you cannot, because of Rule 4:
    Rule 4 (recursion)     There shall be no direct or indirect use of recursive function
    calls. [MISRA-C:2004 Rule 16.2; Power of Ten Rule 1]
    The presence of statically verifiable loop bounds and the absence of
    recursion prevent runaway code, and help to secure predictable
    performance fo r all tasks.  The absence of recursion also
    simplifies the task of deriving reliable bounds on stack use. The
    two rules combined secure a strictly acyclic function call graph and
    control-flow structure, which in turn enhances the capabilities for
    static checking tools to catch a broad range of coding defects.
In the absence of recursion, any decent static checker will compute a
bound on the stack use. I know Coverity does.
Well, technically you do also need to avoid alloca() and variable-length
arrays (VLAs).
alloca() is non-standard and is anyway called out as verboten in Rule 5.
VLAs are part of the C99 standard, which Rule 1 identifies as the source
language. It would be consistent with this spec to forbid them, but at
first glance I do not see it. This might be an oversight.
Anyway, I would say the authors of these rules have thought things
through pretty carefully. I would not want to code under these
restrictions, but then my code does not need to run a billion miles out
of my reach for several years.
 - Nemo

@_date: 2017-03-01 15:30:20
@_author: Nemo 
@_subject: [Cryptography] formal verification +- resource exhaustion 
In other words, it's an oversight.
I actually did search for a newer version of the JPL spec, but all I
found was the same one referenced in this thread
( Do you have a
link to a more current version?
Or are you citing MISRA? That is a different spec. As far as I can tell,
the JPL coding guidelines were never updated beyond 1.0.
Somewhat interesting Hacker News thread on these guidelines:
It sounds like they were designed with static verification in mind by
people who actually know something about the field.
JPL treats their multi-billion-dollar one-shot toys pretty seriously
 - Nemo

@_date: 2017-03-01 17:15:09
@_author: Nemo 
@_subject: [Cryptography] formal verification +- resource exhaustion 
Well, the JPL Coding Standard is dated 2009. It names C99 (ISO/IEC
9899-1999(E)) as the target language. In C99, VLAs are a mandatory
feature. Since they name a decade-old spec, I will stick with
But possibly a self-correcting one: C11 makes VLAs an optional feature,
probably to cater to embedded applications. And VLAs have never been
incorporated into any C++ spec, probably because somebody thinks they
are a terrible idea.
Could you be more specific about what you recommend MISRA for? If it's
resource-constrained environments where you cannot crash, sure. For
secure code, though, it is debatable. Resource exhaustion is not
necessarily a security hole, especially when the system is designed for
it. Controlling resources globally strikes me as both more flexible and
more reliable than trying to bound every individual use.
For example, heap-allocated self-resizing buffers are invulnerable to
buffer overruns; their failure mode is heap exhaustion. So maybe
self-resizing buffers combined with a process-wide hard memory limit is
a safer design pattern than "never call malloc()". Let the user/attacker
flood their session with data and crash it; just make sure it only
affects themselves.
Also, as mentioned elsewhere on this thread, higher-level constructs
(e.g. smart pointers) can make it easier to reason about code and prove
invariants. Unless you are talking about full formal verification of
security properties, I would trust code written for human reviewers over
code written for static analyzers any day.
I wonder whether MISRA-C code is more secure in general than ordinary
"well written" C or C++. Unfortunately I do not know how to quantify
this question.
 - Nemo

@_date: 2017-11-30 09:52:30
@_author: Nemo 
@_subject: [Cryptography] Intel Management Engine pwnd 
Not necessarily...
    MCTP is the universal back door. If someone controls your management
engine, they also control any NIC whose "feature" set includes MCTP.
This has received far less attention than it deserves, in my humble
 - Nemo

@_date: 2018-08-02 09:44:01
@_author: Nemo 
@_subject: [Cryptography] Krugman blockchain currency skepticism 
Sure, assuming nobody legitimately values privacy for its own sake.
What happens, exactly, when you want to move those wads of cash or piles
of ingots across borders?
Drifting on-topic... The "crypto" in crypto-currencies enables new
things. Nobody can seize them from you via a physical search, or even
determine how much you have. They travel anywhere in an instant,
undetectably, at zero cost. They can be exchanged between people who
never come anywhere near each other.
The exact combination of features offered by cryptocurrencies has never
existed before. This does not mean they are worth anything, necessarily,
but it does make Krugman's arguments sound pretty stupid.
Thought experiment: Would drug dealers use credit cards if the
transactions were untraceable?
 - Nemo

@_date: 2018-02-07 09:43:04
@_author: Nemo 
@_subject: [Cryptography] RISC-V branch predicting 
Wrong on every count. For example:
    Granted, that is just a 20K-upvote StackOverflow question. But it is
extremely common for branch patterns not to determined until
runtime. Also note that the performance delta in this example is 6x.
Polymorphic method invocations (e.g. C++ "virtual functions") are
similar. Where it is possible for the compiler to make a determination
("de-virtualization") or a good guess about the target, they already do
so. But it is still very common for patterns to exist only at runtime.
Eliminating speculative execution would be a disaster for
performance. It would also be stupid, because the real problem is not
speculation per se, but covert channels between privilege domains
(e.g. cache timing attacks).
 - Nemo

@_date: 2018-02-09 14:15:09
@_author: Nemo 
@_subject: [Cryptography] RISC-V branch predicting 
This is the third time I have seen this misconception repeated on this
list. The first was from John Gilmore himself.
Once again: This characterization is accurate for Meltdown, but not at
all for Spectre. If you want to comment on this topic, please learn the
details of BOTH attacks first.
All CPUs other than Intel's already do this correctly, which is why none
of them were vulnerable to Meltdown in the first place.
Spectre is a completely different beast. It has nothing to do with using
speculative accesses bypassing memory protections.
Spectre works by poisoning the branch prediction tables, allowing an
attacker to trick "privileged" code into speculatively executing a path
that would never occur, speculatively or otherwise, during normal
execution. This allows the attacker to pump a firehose of information
through the already-existing (and still-existing) covert channel of
cache timing.
One of the original proof-of-concept attacks for Spectre was a
Javascript applet that could read arbitrary memory in the browser. Both
the applet and the Javascript runtime execute in a single process; there
are no memory access controls between the two in the first place.
Right now, countermeasures against Spectre fall into two categories:
Compiler options to eliminate indirect branches entirely (search for
"retpoline"), and instructions to flush the branch prediction tables
(used on entry to privileged code). Oh, and also disabling
high-precision timers in Javascript LOL
But these countermeasures are just squashing the particular attacks that
have been demonstrated. None of them address the underlying covert
channel(s), which appears to be a genuinely hard problem to tackle
 - Nemo

@_date: 2018-02-12 10:48:25
@_author: Nemo 
@_subject: [Cryptography] Spectre again (was Re: RISC-V branch predicting) 
Yes, I know. I was just giving an example of how Spectre has nothing to
do with speculative code paths bypassing hardware access controls.
Except Spectre also crosses hardware privilege domains: The other
proof-of-concept was reading kernel memory from unprivileged user
code. True, they had to "cheat" by using eBPF... But someone with more
resources than a few motivated grad students could probably do
interesting things without cheating.
Chrome already supports "site isolation"
( which is a big
step along these lines.
But Spectre also crosses the kernel protection boundary, and almost all
of the mitigations for that could also be applied to a sandboxed app /
runtime boundary within a single process (e.g. retpolines, IBRS,
IBPB). So while I do not necessarily disagree with the principle you and
Jerry are advocating, I am not sure Spectre is a good example.
Quick list of search terms if you are curious about Spectre mitigations:
      retpoline
      Indirect Branch Restricted Speculation (IBRS)
      Indirect Branch Prediction Barrier (IBPB)
      Single Thread Indirect Branch Predictors (STIBP)
If you find (or author) a single document describing each of these in
detail, please send it along, because I have not found one
yet. Everything I know is just snippets.
 - Nemo

@_date: 2018-02-13 10:03:52
@_author: Nemo 
@_subject: [Cryptography] Spectre again (was Re: RISC-V branch predicting) 
Yes, I know what eBPF is, and that is precisely why it is cheating.
A non-cheating Spectre attack would look like this:
  1) Disassemble megabytes of operating system code looking for a
     sequence of instructions that leaves an interesting footprint in
     the cache
  2) Poison the BTB to convince the O/S to execute that sequence
  3) Use cache timing to sniff the footprints
With eBPF or any similar mechanism, you can replace step (1) with:
  1') Generate your own sequence of instructions that leaves
      dinosaur-sized footprints in the cache
Not surprisingly, the researchers who found Spectre went with the latter
approach for their proof-of-concept. But blaming eBPF is completely
missing the point of the attack.
Maybe you should write to the linux-kernel mailing list and tell them
they are being stupid for implementing all of these mitigations --
retpolines, IBRS/IBPB/STIBP, etc -- when all they need to do is fix (or
disable) eBPF. I wonder what kind of response that would get.
"Proof by Lack of Counterexample"? Good enough if your adversary's
resources are limited to a few hungry grad students, I guess. But
bounding what a serious attacker might do with Spectre is effectively
impossible. Which is why all of the mitigations are focused on the BTB,
not the in-kernel JITs and interpreters.
 - Nemo

@_date: 2018-01-16 17:59:01
@_author: Nemo 
@_subject: [Cryptography] Speculation considered harmful? 
Good question.
True for Meltdown; not so true for Spectre.
The core of the problem is not really speculative execution... The core
of the problem is implicit CPU state leading to covert
channels. Speculative execution just made the attacks more real, like
allowing a proof-of-concept JavaScript program to read arbitrary memory
in your browser process.
Meltdown works like this:
Step 1) Access mapped but inaccessible memory in a speculative execution path
Step 2) Use timing to extract information about the contents of the cache
Spectre works like this:
Step 1) Poison the branch prediction buffers
Step 2) Convince privileged code (kernel, JavaScript runtime, etc.) to
execute an indirect jump, speculatively executing code of your choice
Step 3) Use timing to extract information about the contents of the cache
In both cases, the speculative execution leaves "footprints" in the
cache that can be detected via timing, which opens a huge gaping covert
channel across protection domains. For Meltdown, it is just an Intel bug
that speculative accesses can bypass protections. For Spectre,
it's... Well, something weirder than just a "bug". And it affects nearly
evey high-performance CPU.
Compiler writers are arranging to emit code avoiding all indirect jumps
(try a search for "retpoline"). Intel is adding instructions to flush
the branch prediction buffers. This is a bit beyond "fixing a few
implementation bugs", and all of it just to shut the door on Spectre.
But here is the problem: Even if you eliminate speculative execution
entirely, the cache still holds "footprints" of the execution of your
privileged code. And it is hard to prove exactly what information that
conveys (or does not convey).
There are two kinds of security. One is where you say "I do not see how
an attacker can do X". The other is where you say "I can prove the
attacker cannot do X, assuming Y and Z". The former leaves you
vulnerable to people smarter and/or more motivated than you. The latter
is what you want.
Cache timing attacks, given the implicit management of the cache by the
CPU hardware, means that I do not know exactly what information I am
leaking from privileged code *no matter how I write that code*.
And it is not just the cache. Consider performance counters, debug
registers, register renaming (i.e. vastly more physical registers than
architectural registers), etc. All of this implicitly-managed state
might carry who-knows-what information across protection domains.
So, yeah, plugging Meltdown is just a bug fix. Plugging Spectre is a
large but tractable task.
So, congratulations, you stopped the proof-of-concept code from
working. But being able to prove that nothing interesting passes across
protection domains, given all those megabytes of implicitly managed
state. That is going to require some serious rethinking of CPU
architecture. Just for starters, not sharing cache between privileged
and unprivileged code...
 - Nemo

@_date: 2018-01-19 09:14:15
@_author: Nemo 
@_subject: [Cryptography] Speculation considered harmful? 
The x86 SSE instructions actually provide this. Try a search for
Using such instructions to avoid covert channels via cache is an
interesting thought... But ultimately I do not think it would work
unless maybe you used them exclusively, which would be murderous for
Not sharing caches across protection domains looks like the only robust
fix. Others are thinking along lines; see e.g.  - Nemo

@_date: 2018-07-27 09:41:33
@_author: Nemo 
@_subject: [Cryptography] Signal double-ratchet vs. future breaks in ECC? 
I asked this on the Crypto StackExchange but got no replies:
    I understand how a "double ratchet" protects both future and past
messages from one-time key compromise.
My question is what happens if the public key (ECC) algorithm is broken,
while the hash function(s), symmetric algorithm(s), and keys themselves
remain secure. (Consider a hypothetical mathematical breakthrough on
elliptic curves, or quantum computers becoming practical...)
Perhaps somebody here knows the answer or can explain why it is a dumb
 - Nemo

@_date: 2018-07-31 10:54:17
@_author: Nemo 
@_subject: [Cryptography] Signal double-ratchet vs. future breaks in ECC? 
Hi Jon, and thank you for your reply.
I actually do understand that part; I should have phrased my question
better. I am wondering what happens if the attacker does *not* intercept
all communications from the beginning of time, but only some subset of
It looks to me like each step of the ratchet stirs together both the
current agreed key material *and* some new material agreed via ECDH. So
even an attacker who can break the key exchange would need to see *all*
of your key agreement traffic back to the beginning of time in order to
"replay the ratchet" and know your current key. So perhaps not
necessarily game over (?)
This is very informal, and I am not sure whether it would hold up
formally or even practically (e.g. does Signal ever fall back to a
completely fresh key agreement)?
 - Nemo

@_date: 2018-11-21 16:45:00
@_author: Nemo 
@_subject: [Cryptography] Buffer Overflows & Spectre 
A little bit afield, but I believe discussions of secure coding
w.r.t. programming languages is on-topic.
This is close to "not even wrong" territory.
The C language is defined by a precise specification ratified as an
International Standard in 1990. (And then extended in 1999 and 2011, but
C90 remains the most popular flavor.)
Everything that was defined in 1990 is still defined today. Everything
that is undefined today was already undefined in 1990. Neither "compiler
people" nor anyone else has added any undefined behavior in 30 years,
nor could they.
You are complaining that a piece of code from 1990, whose behavior was
definitely never defined by the language, acts differently under
different compilers over the years.
Have you tried taking your complaint to someone who works on compilers?
You would not be the first. The interaction invariably goes like this:
Inexperienced C or C++ programmer is surprised by some optimization and
petitions compiler authors that said optimization should be
disallowed. Compiler authors explain they have a decades-old spec
clearly stating exactly what is and is not allowed, and asks petitioner
what they would like to change, exactly.
Petitioner either turns out not to know except in the vaguest possible
way (80%), or to be asking for something that would harm performance for
everyone everywhere (20%). Also every petitioner has a different answer.
Perhaps the most deeply-embedded philosophy in the DNA of the C/C++
language designs is "you do not pay for what you do not use". Security
concerns, like everything else, fall under this rubric. If you cannot
handle this, you are using the wrong language.
There is significant empirical evidence that the security of any
software has everything to do with who wrote it and almost nothing to do
with the implementation language. In the case of C/C++, it is certainly
possible to adopt a style that enables secure code. See OpenBSD for a
real-world example.
You did not give even one example of what you mean, so I will pick a
typical one: Integer overflow.
What should happen if the mathematical sum of two positive signed
integers does not fit in an int? (BTW, how many bits should that be,
exactly?) If you care about correctness -- and thus security -- there
are only three possible answers:
  1) Don't do that.
  2) An exception should be raised.
  3) The width of the result should grow to hold the sum.
There are approximately zero cases where "produce a negative result" is
what the programmer actually wants.
The C/C++ language spec goes with option (1). If you want (2) or (3),
you can use a library to get them (see Microsoft's SafeInt and
Boost.Multiprecision, respectively).
You will find the same is true for all forms of undefined behavior:
Defining it in a sane way would not be performant, and defining it in a
performant way would not be sane. Thus "don't do that or use a library".
Or use a language that takes a performance hit everywhere to give
defined behavior everywhere. There are plenty of them.
 - Nemo

@_date: 2018-11-22 11:26:39
@_author: Nemo 
@_subject: [Cryptography] Hohha quantum resistant end-to-end encryption 
Mollinier Toublet's message of "Wed, 21 Nov 2018 14:08:59 -0800")
Yes, this is a good idea.
And it is not original. ZRTP supports an "auxsecret" to stir some
out-of-band shared key material into the "fancy" Diffie-Hellman
negotiation. See I made a feature request to add something similar to Signal
( but nobody cared
 - Nemo

@_date: 2019-01-30 14:14:25
@_author: Nemo 
@_subject: [Cryptography] Introducing the world's worst hash function 
Linux still does this. Or did prior to the era of processor frequency
See  - Nemo

@_date: 2020-06-06 10:34:30
@_author: Nemo 
@_subject: [Cryptography] [FORGED]  Cubbit 
We have a RAID6 at work that uses 12 1TB drives. It provides a full 10TB
of usable storage, and any two of those 12 drives can fail without
causing us any data loss whatsoever.
How does this fit with your analysis?
 - Nemo

@_date: 2020-06-08 11:09:28
@_author: Nemo 
@_subject: [Cryptography] Cubbit 
RAIDx write performance is fine if you work with files in the hundreds of
gigabytes and write to them in large sequential chunks, which we do. I will
stop here before I dox myself.
The point is that 2x, 3x, 4x, etc. redundancy is quite possible without
doubling, tripling, or quadrupling your storage, contra Jerry's analysis. I
have not studied Cubbit, but it sounds like you can lose a third of your 36
"partners" and still recover all of your data at a storage increase of 1.5x.
 - Nemo

@_date: 2020-10-01 10:21:32
@_author: Nemo 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
Ever try left-shifting an int by 32 on x86? (Hint: x86 only uses the low
5 bits of the shift count.)
This is why left-shift by the word size is undefined behavior in C, and
thus ((x)<<(b)|(x>>(32-b))) is technically only valid for 0 < b < 32.
I suppose whether this is "subtle" is a matter of opinion.
 - Nemo
