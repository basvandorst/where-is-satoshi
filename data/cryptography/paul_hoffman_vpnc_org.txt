
@_date: 2005-12-03 13:07:13
@_author: Paul Hoffman 
@_subject: [Clips] Banks Seek Better Online-Security Tools 
I have, and it's nice for making Quicken data entry faster, but that's about all. The rest gives me the willies when I see the security clue of the folks running the site.
FWIW, I have never had a problem changing my password to something very long and all-alphabetic, even if I don't include "at least one capital letter and one digit" or whatever the CYA rules for passwords are these days.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-12-04 16:37:37
@_author: Paul Hoffman 
@_subject: RNG implementations and their problems 
Depends on what you mean by "Unix". FreeBSD 5 and 6 have much of what you want.
 From 'man 4 random':
      If the device has is using the software generator, writing data to random
      would perturb the internal state.  This perturbation of the internal
      state is the only userland method of introducing extra entropy into the
      device.  If the writer has superuser privilege, then closing the device
      after writing will make the software generator reseed itself.  This can
      be used for extra security, as it immediately introduces any/all new
      entropy into the PRNG.
      The software random device may be controlled with sysctl(8).
      To see the devices' current settings, use the command line:
            sysctl kern.random
      which results in something like:
            kern.random.sys.seeded: 1
            kern.random.sys.burst: 20
            kern.random.sys.harvest.ethernet: 0
            kern.random.sys.harvest.point_to_point: 0
            kern.random.sys.harvest.interrupt: 0
            kern.random.yarrow.gengateinterval: 10
            kern.random.yarrow.bins: 10
            kern.random.yarrow.fastthresh: 100
            kern.random.yarrow.slowthresh: 160
            kern.random.yarrow.slowoverthresh: 2
      (These would not be seen if a hardware generator is present.)
      All settings are read/write.
Thus, you can do your own calculations and change the paramters to your heart's content (assuming you have root privs).
(...Other Linux-specific complaints elided...)
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-12-12 09:08:21
@_author: Paul Hoffman 
@_subject: crypto wiki -- good idea, bad idea? 
I cannot answer the first question: I am leery of wikis that have open posting rights, and I am leery of trusting anyone to maintain the posting rights and document quality of a wiki without being paid (either in money or whuffie) to do so.
The second and third questions can be answered with "no". Someone wrote a fairly poor article on VPNs. That article has been flagged as needing a lot of work. I proposed a few weeks ago (in the meta-discussion) to do it, but was concerned that doing so would step on toes and seem invasive. No one has responded to that, not even the people who flagged the article as needing work.
In other words, Wikipedia is trying to correct problems, but doesn't have the personpower to do so in a predictable fashion.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-11-15 09:46:16
@_author: Paul Hoffman 
@_subject: "ISAKMP" flaws? 
The advisory itself is at . Note that the abstract is "Multiple Vulnerability Issues in Implementation of ISAKMP Protocol", with emphasis on "Implementation of". It appears that this is *not* a problem with ISAKMP or IKE, but instead only a problem with some implementations. A summary would be "when some IKEv1 implementations are sent certain malformed messages, they stop, reboot, or possibly do other bad things".
Given that they started this research with sending malformed SNMP packets to SNMP-aware systems (with similar results), it is safe to extrapolate the results to implementations of nearly any protocol to varying extents. It is likely that this applies to IKEv2 as well, but using differently-malformed packets. It is also likely that it applies to some SSL/TLS implementations, of course using very different malformed packets.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-11-15 13:06:12
@_author: Paul Hoffman 
@_subject: "ISAKMP" flaws? 
Well, then we fully agree with each other. Look at the message formats used in the protocols they have attacked successfully so far.
Humorously, security folks seem to have ignored this when designing our protocols.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-11-17 09:01:22
@_author: Paul Hoffman 
@_subject: "ISAKMP" flaws? 
Which "proper programming tools" would check for a logic path failure when a crafted packet includes Subpacket A that is only supposed to be there when Subpacket B is there, but the packet doesn't include Subpacket B? There are no programming tools that check for this, or for related issues: it has to be the implementer who has enough understanding of the protocol and enough time (and program space) to code against such issues.
Throw in PKIX certificates in certificate chains, and it gets much worse.
IKE is a very complicated protocol with many within-packet and within-stream dependencies. These cannot be resolved by "proper programming tools" unless those tools are specifically crafted for IKE. SSL/TLS probably suffers the same fate.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-11-30 09:14:23
@_author: Paul Hoffman 
@_subject: "ISAKMP" flaws? 
Exactly. It prevents organizations who want security but cannot afford someone who understands it well from using IPsec. Optimally, someone should be able to say little more than "I want to do strong crypto and make a network with that guy over there; he will trust this ID and I will trust that ID; do it". That is not possible now. It is arguable that it isn't doable with SSL/TLS, either, but it's a heck of a lot closer there than in IPsec.
Correct. When the IETF was designing IKEv2 after seeing what real-world deployments of IKEv1 were causing, it was pointed out that this is not a "negotiation" but really "the responder always picks". Therefore, there was a suggestion that instead of having all this pre-arranged setup, we do "ask the responder what he wants", which is much simpler. We rejected that idea early on for (IMHO) bad reasons.
On the other hand, no other widely-deployed security protocol seems to have made this leap of understanding either.
Exactly. You can always tell a user "pick crypto suite A" and they can figure it out. Imagine telling them "figure out the network topology you want the other side to see, then figure out the network topology you expect to see, then write them down exactly".
It is easier than that: just use Ethereal. It decodes the first four packets just fine.
How does he fit his sneakers over the phone? :-)
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-11-30 09:47:49
@_author: Paul Hoffman 
@_subject: from the bad idea department 
A different approach would be for him to write an open-source program that generates the passwords on your local machine. Of course, if it is distributed as an executable, you don't know if the executable is the same as the source, but you are already trusting him now on the program on his web site.
Given that most users of this would be Windows folks, one could possibly write a really creative batch program to do this, thus eliminating the worry about the difference in executable. It would be mostrously ugly, but a nice hack.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-01 09:07:11
@_author: Paul Hoffman 
@_subject: Another entry in the internet security hall of shame.... 
s/happen/happen in a widely useful fashion/
s/Web/Web and email/
Self-signed certificates that are fingerprinted out-of-band are better than PSKs in some situations, worse in others.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-12 09:52:09
@_author: Paul Hoffman 
@_subject: Another entry in the internet security hall of shame.... 
In many deployments of "SSL first, then authenticate the user with a password", the "site" consists of two or more machines. Many or most high-traffic secure sites use SSL front-end systems to terminate the SSL connection, then pass the raw HTTP back to one or more web servers inside the network.
The reason I bring this up is that the SSL server generally does not have access to the users' credentials. It could, of course, but in today's environments, it doesn't. Changing to TLS-PSK involves not only changing all the client SSL software and server SSL software, but also the what the SSL server's role in the transaction is.
Exactly. So far, the banks have not found it that painful. If they had, they would be spending much more money on reducing the problem. Banks are extremely good at measuring risks and costs, and then counterbalancing them. Banks do not feel like the costs are that high yet. They haven't even started any significant anti-phishing efforts. Said another way, the anti-phishing efforts so far have been cheap and mostly ineffective.
Even though pretty much all of our user security training efforts have been a dismal failure so far, you assume that we'll get this one right? If we don't, then the large cost of upgrading everyone's SSL, and the banks' SSL processes, is wasted. That's a interesting risk.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-13 07:54:01
@_author: Paul Hoffman 
@_subject: ECC patents? 
That's pretty bold statement that folks at Certicom might disagree with, even before --Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-14 08:56:38
@_author: Paul Hoffman 
@_subject: ECC patents? 
It's not just curves. Certicom has patents for some optimizations and methods for validating the strength of some uses of ECC.
Both are probably true. Why would anybody be interested in curves that do not support their minimum strength ciphers?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-25 14:26:55
@_author: Paul Hoffman 
@_subject: PKI too confusing to prevent phishing, part 28 
Summary: some phishes are going to SSL-secured sites that offer up their own self-signed cert. Users see the warning and say "I've seen that dialog box before, no problem", and accept the cert. From that point on, the all-important lock is showing so they feel safe.
Although the company reporting this, SurfControl, is known for alarmism, this is a completely predictable situation. If users can hold one bit and the bit is "look for the lock", then phishers will do anything to get the lock up there.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-26 08:30:15
@_author: Paul Hoffman 
@_subject: PKI too confusing to prevent phishing, part 28 
To me, the first paragraph contradicts the second paragraph. Actually, the third sentence of the first paragraph contradicts the first two sentences of that paragraph.
A technology that cannot be made usable, but is widely used anyway, is the cause of its own problems.
Looking at decades of experience with PC software, it seems unlikely that TrustBar or anything like it will be deployed and understood by typical users. It is fine to help increase the security for a small (possibly tiny) audience, but please do not conflate that with making the whole market more noticeably secure.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2005-09-26 15:13:46
@_author: Paul Hoffman 
@_subject: PKI too confusing to prevent phishing, part 28 
If it is an inherent usability problem (users not understanding why there are trust anchors they have never heard of in their browser, users not understanding what a hierarchy is, users not understanding revocation, users not understanding the difference between a hierarchy and self-signed certs, and so on), then PKI is the cause of the problem.
Correct, although for more reasons than just because it is an extension. Mostly, they will be suspicious of it unless it comes from the browser maker, and in fact because it comes from someone they have never heard of. Why should they trust their security to anyone other than Microsoft?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-12-21 16:27:11
@_author: Paul Hoffman 
@_subject: How important is FIPS 140-2 Level 1 cert? 
US federal agencies are supposed to require that certification for any system they buy that uses crypto.
Sometimes, US state agencies require it as well.
Sometimes, clueless corporations require it because it has the word "certification" in it and, well, if it's good enough for the feds, it should be good enough for everyone.
Assuming that the two products use Internet protocols (as compared to proprietary protocols): no. Probably the only thing that could differentiate the two is if the cheaper one has a crappy random number generator, the more expensive one will have a good one.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-12-21 17:38:04
@_author: Paul Hoffman 
@_subject: How important is FIPS 140-2 Level 1 cert? 
Now seeing what your company does, I can see where you might have that question. An overly-simple but sufficient answer comes from whether or not you need to be able to interoperate with other vendors over a non-secured network. If so, call it an "internet protocol". In your case (local disk encryption), it is fine to be proprietary.
... and essentially all of those mistakes are caught by even mild interop testing. Again, this is not valid in your case. You could completely mis-implement AES and never know it, but a FIPS 140-2 test would find that.
You can catch such mistakes for a lot less money than it will cost for a FIPS certificate. Assuming that you are using a standard encryption algorithm like AES, there are probably a dozen people on this mailing list who could sanity check your product's implementation of AES (and probably even of key storage) in less than 50 hours of consulting time,
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-02-12 16:53:41
@_author: Paul Hoffman 
@_subject: general defensive crypto coding principles 
The IETF has not recommended any "solutions to hash problems". The sense of the room at the Hash BOF and the SAAG discussion at the Paris IETF meeting was that the IETF should *not* propose solutions to the problem. That is why the BOF did not turn into a Working Group and why there has been little discussion of the proposed solutions in the relevant IETF working groups.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-02-23 15:41:18
@_author: Paul Hoffman 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
That's an incorrect assessment of the short piece. The story says that it does actually work but no one uses it. They briefly say why: key management. Not being easy enough to use is quite different than "NOT actually working".
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-02-23 16:46:36
@_author: Paul Hoffman 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Fully agree.
We disagree on the meaning of the phrase "actually work".
Phil *does* have a problem with key management. He knows how to do it, but his communications partners are not as good as he is.
Yes, I could. But I won't bother. :-)
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-02-24 08:30:16
@_author: Paul Hoffman 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
It is a bit harsh to equate "doesn't want to do it because of the hassle" with "doesn't know how to do it".
This is my original disagreement with Ed's message. It can be done, and when you do it it works, but it is too difficult for most people to bother with. I think we all agree on those three facts, just not on what to label the last one.
Fully agree, and I would certainly extend that to S/MIME as well.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-02-26 11:12:35
@_author: Paul Hoffman 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
We have the preamble and (a) already; the problem is that the preamble is insufficient. What we ultimately need is encryption and authentication *and validation of the authentication* that match at least (a).
Currently, it is the validation of the authentication that makes most users uninterested. When you get a message from Bob that comes with a warning that says "I cannot tell whether or not Bob really sent this", but you are sure that Bob actually sent that (due to some out-of-band knowledge), you lose faith in the system. When Bob has the same problem with your messages, you give up.
For signed personal mail, (b) and (c) may be mutually exclusive. Why sign your messages if you don't want to be held liable for their contents? How can you get the reward of integrity without the cost of Given those two hurdles, my hopes for authenticated mail near zero. I have some hopes for authenticated syndicated messages through Atom or RSS, but not this year. The hardest part there will be (c), but there are many environments where signing one-way mail is quite appropriate, particularly in replacing paper messages.
The demand for encryption of personal email is perpetually low. Without a legal requirement, it will probably always be a small niche --Paul Hoffman, Director
--VPN Consortium

@_date: 2006-05-22 08:18:31
@_author: Paul Hoffman 
@_subject: Phil Zimmerman and voice encryption; a Skype problem? 
Please don't forget that the VeriSign spokesperson may be mistaken, or purposely lying (possibly in order to drum up business for the company). Neither would be a first for VeriSign.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-09-05 08:50:45
@_author: Paul Hoffman 
@_subject: signing all outbound email 
No, it's not. The receiving MTA *and/or* MUA can verify signatures. That is clearly covered in the protocol document.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2006-09-08 08:52:34
@_author: Paul Hoffman 
@_subject: signing all outbound email 
Correct. It is for white-listing. It tells the recipient (MTA or MUA) that the message received was sent from the domain name it says it was, and that parts of the message were not altered.
Good; that's what the protocol is designed to do.
Yes, if you are willing to throw out messages whose signatures are broken during transit. (This is the same risk that others face with insisting on valid S/MIME or OpenPGP signatures be on every message from particular parties.)
And there is no disadvantage either. There is advantages for sending signed mail to users who have a different threat model than you have, and there are certainly administrative advantages to signing all outgoing mail, not looking to see "oh, if it is James, don't sign it because he won't like it".
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-04-05 07:32:09
@_author: Paul Hoffman 
@_subject: DNSSEC to be strangled at birth. 
But how does having the root signing key allow those?
Control: The root signing key only controls the contents of the root, not any level below the root.
Surveillance: Signing keys don't permit any surveillance.
Falsification: This is possible but completely trivially detected (it is obvious if the zone for furble.net is signed by . instead of .net). Doing any falsification will cause the entire net to start ignoring the signature of the root and going to direct trust of the signed TLDs.
More than it is now?
Fully disagree. Many ISPs and individuals will be happy to do direct trust of the significant zones (com/net/org plus maybe their local ccTLD) and simply ignore signatures on the rest. This has already been well-discussed in the ISP community even before this event: many are not sure they trust ICANN itself, much less its current "sponsor".
Note that I'm not supporting the US signing the root in the least. I'm just saying that predicting doom is grossly premature.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-04-05 16:49:33
@_author: Paul Hoffman 
@_subject: DNSSEC to be strangled at birth. 
This is, of course false. In order to control the contents of the second level of the DNS, they have to either change the control of the first level (it's kinda obvious when they take .net away from VeriSign) or they have to sign across the hierarchy (it's kinda obvious when furble.net is signed by someone other than .net).
Um, since when are you (or I) so good at figuring out what DHS wants? For that matter, assuming that a massive bureaucracy like DHS has one thing that it wants also seems silly. For all we know, this could be one clue-deprived dork who can write press releases after not really listening to the one technical person whom he asked. Or it could be a conspiracy to take over the Department of Commerce. Or ...
If the owner of any key signs below their level, it is immediately visible to anyone doing active checking. The root signing furble.net instead of .net signing furble.net is a complete giveaway to a violation of the hierarchy and an invitation for everyone to call bullshit on the signer. Doing so would completely negate the value of owning the root-signing key.
...again assuming that the users of those keys don't bother to look who signed them. Given that this thread is about an entity whom almost no one trusts being the key holder, that scenario seems Because I believe that ISPs, not just security geeks, will be vigilant in watching whether there is any layer-hopping signing and will scream loudly when they see it. AOL and MSN have much more to lose if DHS decides to screw with the DNS than anyone on this list does. Having said that, it is likely that we will be the ones to shoot the signal flares if DHS (or ICANN, for that matter) misuses the root signing key. But it won't be us that causes DHS to stand down or, more likely, get thrown off the root: it's the companies who have billions of dollars to lose if the DNS becomes untrusted.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-04-05 17:30:53
@_author: Paul Hoffman 
@_subject: DNSSEC to be strangled at birth. 
And you seem to be missing my point. If the root signs itself a new .net key, it will be completely visible to the entire community using DNSSEC. The benefit of doing so in order to forge the key for furble.net (or microsoft.com) will be short-lived, as will the benefit of owning the root key.
The only reason for concern is if the top of the hierarchy can forge without people noticing, or if people notice that they won't care. I claim that that isn't possible, particularly if the root owner is someone as unloved as USDHS.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-04-06 10:07:27
@_author: Paul Hoffman 
@_subject: DNSSEC to be strangled at birth. 
[[ Agree with Nico's MITM arguments; different point below ]]
Whoever owns the root key would only get to veto the inclusion of new or current TLDs in the DNSSEC-protected namespace, not in the root itself. No one expects that ICANN will be signing the zone keys for most of the TLDs for many, many years, if for no other reason than those TLDs don't even want to be responsible for protecting their zone key.
Fully agree. It also means that, if there is a breach, the first few days / months will be spent finger-pointing instead of fixing.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-04-22 11:53:46
@_author: Paul Hoffman 
@_subject: More info in my AES128-CBC question 
In a typical standards-setting environment, non-security people are usually only willing to listen to security people up to a certain threshold. There are three normal scenarios:
- A security person proposes a good way to do security for the proposed protocol. A non-security person says (incorrectly) "I heard that doesn't work". The security person argues that it does work here, and the non-security person, not wanting to look foolish, digs in his heels. People get bored of hearing an argument they don't understand and make an arbitrary decision.
- A non-security person proposes a bad way to do security for the proposed protocol. A security person explains why that is insecure. The non-security person argues (sometimes correctly) that they did it in this other protocol so we should copy that, and the security person tries to explain why this is bad security. People get bored of hearing an argument they don't understand and make an arbitrary - A security person proposes two different ways to do security for the proposed protocol. The second is significantly faster than the first, but has worse security properties. People say "the first is good enough for our scenario" and pick it, often not even bothering to document the diminished security properties.
FWIW, this can happen when designing pure security protocols, swapping "non-security person" with "security novice" or "security tourist" or "security hobbiest" or "security poser".
Because doing so can get things finished earlier and/or make a more efficient protocol.
Same as it ever was.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-08-14 12:21:52
@_author: Paul Hoffman 
@_subject: Fwd: Potential SHA 1 Hack Using Distributed Computing - Near 
I have the same question. I could not find any description of *why* they think that finding near-misses is going to help the research. It's not clear if they are taking their own path, or trying to improve Wang's path, or what.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-08-14 13:42:08
@_author: Paul Hoffman 
@_subject: Fwd: Potential SHA 1 Hack Using Distributed Computing - Near  
Did that, in specific Note the lack of information about what they are actually doing. "We developed new cryptanalytic methods..." sounds great, but is meaningless without specifics.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-08-14 15:47:06
@_author: Paul Hoffman 
@_subject: Fwd: Potential SHA 1 Hack Using Distributed Computing - 
Welcome to the world of public cryptography! :-) At least I haven't seen anyone so far suggest that you will find pre-images.
Is there any estimation of how high? Specifically, do you believe there is a good chance of having less work effort than the current Wang strategy? For example, if you are sure that your result will be around 2^70, well that is interesting in theory but probably not worth any publicity you have gotten so far. If you are sure it will be around 2^55, I'll certainly give you some of my spare CPU cycles.
That's good to hear. It would also be interesting if you could keep a running meter of approximately how much work you are getting from the participants. This isn't nearly as "sexy" as finding ETs or even protein folding...
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-08-22 08:30:02
@_author: Paul Hoffman 
@_subject: more SHA-1 progress? 
This is a continuation of the thread on this list from last week.
I watched the webcast of the rump session, and Christian Rechberger said that they think they will get 2^60ish with a new technique. He did not describe the technique in any detail. Offline, he has told me that there will be papers published.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-12-03 07:51:47
@_author: Paul Hoffman 
@_subject: Flaws in OpenSSL FIPS Object Module 
Another interesting part is that open-source systems are much more susceptible to being attacked by competitors (that is, having their validation suspended) than are closed-source systems.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-01-05 07:53:02
@_author: Paul Hoffman 
@_subject: SC-based link encryption 
You could take an IPsec stack and repurpose it down one layer in the stack. At least that way you'll know the security properties of what you create.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-01-24 14:26:34
@_author: Paul Hoffman 
@_subject: more on NIST hash competition 
At the Second Hash Workshop this summer, NIST explained this a bit. (There were a bunch of regulars from this list there who can correct me if I'm wrong.)
First, there is SHA-2 (SHA-256, -384, and -512). Nearly everyone thinks they are good enough unless there is an unexpected attack. So NIST was not hot to create something that competes with this.
More important, however, is the lack of sureness in the community that we know what will make a good hash function, much less one that is better than SHA-2. See  for much more on that.
Also, remember that we don't know much about the design of SHA-2. In fact, unless the NSA tells the world a whole lot more, it will not be able to compete in the NIST competition due to requirement B1 in the At the end of the workshop, there were at least two camps: those who wanted a competition in case Wang-esque attacks degrade SHA-2, and those who didn't want a competition until we knew more about how to judge it because we don't know enough now. Some of the Big Names In Crypto are in the second group. It looks like NIST sided with the first group, but it will be interesting if the folks in the second group are vocal during the coming few years.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-01-26 07:54:11
@_author: Paul Hoffman 
@_subject: more on NIST hash competition 
This is an incorrect interpretation, I believe. The NIST folks at the workshop said a few times that they were not worried about SHA-1 because they have already deprecated it beginning at the end of 2010. That leaves only SHA-2, in which they said they had sufficient confidence. Further, no one publicly expressed worry at the workshop that SHA-2 would have any significant breaks in the near future.
The dates on the competition timeline shows that AHS (cute name, Peter!) is not meant as a replacement for SHA-2, given that it won't be selected until after SHA-1 needs to stop being used.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-07-03 09:13:10
@_author: Paul Hoffman 
@_subject: Quantum Cryptography 
This is a nice, calm analogy, and I think it is useful. But it misses the point of the snake oil entirely.
The fact that there is some good quantum crypto theory doesn't mean that there is any application in the real world. For the real world, you need key distribution. For the cost of a quantum crypto box (even after cost reductions after years of successful deployment), you could put a hardware crypto accelerator that could do 10,000-bit DH.
Going back to the theory, the only way that quantum crypto will be more valuable than DH (much less ECDH!) is if DH is broken *at all key lengths*. If it is not, then the balance point for cost will be when the end boxes for quantum crypto equals the cost of the end boxes for still-useful DH.
Oh, and all the above is ignoring that DH works over multiple hops of different media, and quantum crypto doesn't (yet, maybe ever).
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-07-18 12:23:36
@_author: Paul Hoffman 
@_subject: New article on root certificate problems with Windows 
Hi again. I posted a new security research article at . It is not directly related to crypto (although not so much of the traffic on this list is...), it does relate to some PKI topics that are favorites of this --Paul Hoffman, Director
--VPN Consortium

@_date: 2007-07-19 08:07:47
@_author: Paul Hoffman 
@_subject: New article on root certificate problems with Windows 
As you can see from my list of proposed solutions, I disagree. I see no reason not to to alert a user *who has removed a root* that you are about to put it back in.
Note that I did not criticize the practice of starting with a zillion roots that Microsoft trusts.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-07-20 07:04:02
@_author: Paul Hoffman 
@_subject: New article on root certificate problems with Windows 
Correct, I was.
Very good point.
Bigger picture takeaway: when both a user and an application can change a crypto setting in an application (or OS), any later messages relating to that event are likely to be confusing because they can't be directly linked to the action. This applies to all of our crypto-in-the-real-world, not just the trust anchor issue at hand.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-06-22 08:41:27
@_author: Paul Hoffman 
@_subject: Quantum Cryptography 
...whereas the key distribution systems we have aren't affected by eavesdropping unless the attacker has the ability to perform 2^128 or more operations, which he doesn't.
Which part of the word "useless" is not apparent here?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-06-22 10:43:16
@_author: Paul Hoffman 
@_subject: ad hoc IPsec or similiar 
Note that that RFC is Informational only. There were a bunch of perceived issues with it, although I think they were more purity disagreements than anything.
FWIW, if you do *not* care about man-in-the-middle attacks (called active attacks in RFC 4322), the solution is much, much simpler than what is given in RFC 4322: everyone on the Internet agrees on a single pre-shared secret and uses it. You lose any authentication from IPsec, but if all you want is an encrypted tunnel that you will authenticate all or parts of later, you don't need RFC 4322.
This was discussed many times, and always rejected as "not good enough" by the purists. Then the IETF created the BTNS Working Group which is spending huge amounts of time getting close to purity again.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-06-22 10:47:23
@_author: Paul Hoffman 
@_subject: Quantum Cryptography 
No, I'm not. I am talking about protocols that do their own key exchange. IPsec. SSL/TLS. Kerberos. Etc.
No, requiring that the two ends have a fixed connection which QKD works over is far tougher than using a proven protocol that works over any connection.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-06-26 13:20:41
@_author: Paul Hoffman 
@_subject: ad hoc IPsec or similiar 
Why, are you some sort or purist? :-) (For those outside the IETF, Nico is one of the main movers and shakers in BTNS, and is probably one of the main reasons it looks like it will actually finish its Well, almost suffice. You also need a way of signalling before the Diffie-Hellman that this is what you are doing, but that's a trivial extension to both IKEv1 and IKEv2.
Fully agree. BTNS will definitely give you more than just one-off encrypted tunnels, once the work is finished. But then, it should probably be called MMTBTNS (Much More Than...).
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-06-26 13:30:51
@_author: Paul Hoffman 
@_subject: ad hoc IPsec or similiar 
Whereas I was in the camp of liking the name very much for the very reason that this thread was started: "because it lets you encrypt an arbitrary conversation with essentially no startup cost".
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-05-01 18:48:30
@_author: Paul Hoffman 
@_subject: 128 bit number T-shirt? 
This would be a lot more popular if the t-shirt and mug said something a bit more fetching above the hex such as "Ask me about

@_date: 2007-05-21 07:22:30
@_author: Paul Hoffman 
@_subject: 0wned .gov machines (was Re: Russian cyberwar against 
Oh, goodie. I get to the same source to show the opposite. At Rob's talk at the AOTA summit, he talked about someone offering some botted machines in a particular US government subnet at a normal prices and someone quickly over-bid by a suspiciously high amount. The assumption is that it was for the possible data on those machines.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-05-21 20:07:24
@_author: Paul Hoffman 
@_subject: 307 digit number factored 
FWIW, according to Arjen Lenstra, there should be a better paper than the physorg.com article on the eprint.iacr.org site next week, Which ones? They have many. Using EC depends on how brave you are and which country you are in.
Because I agree with the latter, I disagree with the former, at least for a few more years and until a few people are braver than I am.
That's good of you not to expect it, given that zero of the major CAs seem to support ECC certs today, and even if they did, those certs would not work in IE on XP.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-05-22 15:32:35
@_author: Paul Hoffman 
@_subject: 307 digit number factored 
For the math weenies on the list, see the full announcement here: --Paul Hoffman, Director
--VPN Consortium

@_date: 2007-05-23 12:07:12
@_author: Paul Hoffman 
@_subject: SSL certificates for SMTP 
No one? I thought that VeriSign and others did, at least a few years ago.
Why would you need to? SMTP-over-TLS only identifies the system to whom you are speaking. No routing inforation is needed or wanted.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2007-10-09 10:20:14
@_author: Paul Hoffman 
@_subject: Fixing the current process 
Highly doubtful. The institutional inertia at DoD/NIST is too great. It has been suggested numerous times by numerous concerned parties for at least a decade.
Yes. That part is easy, and some people in the system admit designing a much better system is quite tractable, as long as it is done in a vacuum. However, bureaucracy abhors a vacuum.
My feeling is that the only way that we can overturn the silliness of FIPS-140 / CC is for a major defense ally to implement a sane system. Five years later, and with a lot of vendor push, it could become a third process and the other two could wither over the ensuing decades. If we're lucky.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-08-08 12:35:43
@_author: Paul Hoffman 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
...and only a tiny number of CAs do so.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-08-18 10:16:02
@_author: Paul Hoffman 
@_subject: Voting machine security 
This is quite disputable. Further, hand vs. machine counting is core to the way we think about the security of the voting system.
On a "complex" ballot, there are maybe 20 races or propositions, some of which may allow multiple votes per race. The pre-electronic method for hand-counting these was to start with race  have one person reading each vote out load from a large stack of ballots, and another person tabulating. In most districts, this is done twice with different people doing the counting and, often, those people coming from the "opposite party" in our wonderful two-party system.
The numbers I saw in the late 1970's said that each vote took 2.5 seconds per ballot per race when done slowly; so that's 5 seconds when run twice. Per "complex" ballot, that's about 100 seconds, or roughly 2 minutes, or roughly 1/30 of an hour. At current labor rates of $12/hour for this type of work (that's high, but we want qualified people to count), that means it costs about US$0.40 per ballot for a complex ballot.
Essentially no one would argue that is is "quite expensive". I suspect that nearly everyone in the country would be happy to pay an additional $1/election for more reliable results.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-01-22 19:57:59
@_author: Paul Hoffman 
@_subject: SSL/TLS and port 587 
Can you point to some sources of this "often expressed idea"? It seems like a pretty flimsy straw man.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-01-23 07:49:12
@_author: Paul Hoffman 
@_subject: SSL/TLS and port 587 
I'll take that as a "no".
That's not what I asked, of course.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-07-01 18:56:11
@_author: Paul Hoffman 
@_subject: Strength in Complexity? 
The quote from the article was:
"There are, of course, obstacles that must still be overcome by EKMI proponents. For example, the proposed components are somewhat simple by design, which concerns some encryption purists who prefer more complex protocols, on the logic that they're more difficult to break It jumps from "components" to "protocols". In general, "encryption purists" like simpler algorithms. OTOH, when "encryption purists" get involved in protocol design, the protocols usually become complex to the point of opacity.
So, I agree with Peter that that article is probably correct about protocols.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-07-05 10:42:13
@_author: Paul Hoffman 
@_subject: Strength in Complexity? 
Peter: please turn down the hyperbole a bit. You forgot the word "may" between "and" and "then".
Wrong. There is no requirement to "ignore everything else in the cert". There is simply no requirement to use that material.
No offense, but if I wanted to know the intent of a group of people who make hard-to-read-and-harder-to-impelemnt crypto standards, I would not ask you. You don't know their intent, Peter: you only know the output of the sausage-making process.
If I haven't made it clear: I agree with Peter that the spec should have clearly stated what one was supposed to do with the extensions. Where I think we disagree is that I would rather that the spec said explicitly to throw them away. I would rather have the semantics of "this is what I say my name is and this is what I say my public key is" quite separate from "this is what I think you should trust me to authenticate". This adds complexity (it takes two certs), but it also reduces complexity (it doesn't mandate binding policy to We might agree here because I don't think there are many sane implementations of X.509.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-07-05 10:45:55
@_author: Paul Hoffman 
@_subject: Upper limit? 
On a related note: why did you skip 1536 bits? There is nothing special about key lengths being an integral power of 2 bits long.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-07-09 08:20:33
@_author: Paul Hoffman 
@_subject: Kaminsky finds DNS exploit 
First off, big props to Dan for getting this problem fixed in a responsible manner. If there were widespread real attacks first, it would take forever to get fixes out into the field.
However, we in the security circles don't need to spread the "Kaminsky finds" meme. Take a look at . The first draft of this openly-published document was in January 2007. It is now in WG last call.
The take-away here is not that "Dan didn't discover the problem", but "Dan got it fixed". An alternate take-away is that IETF BCPs don't make nearly as much difference as a diligent security expert with a good name.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-07-14 09:06:57
@_author: Paul Hoffman 
@_subject: Kaminsky finds DNS exploit 
That whole paragraph, taken together, makes no sense.
There is a difference between code changes in the kernel for some systems (which you allude to above), code changes and a universal rollout in all DNS software (which you allude to at the end), and stable rollout of the DNSSEC trust anchor system in every significant zone and all resolvers.
FWIW, only the latter has anything to do with this mailing list...
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-06-01 14:56:46
@_author: Paul Hoffman 
@_subject: Protection mail at rest 
Fully agree. You're in control, all the way to root of the box.
And, if you want to host on FreeBSD instead of Linux, see . Same price, good service.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-05-13 08:10:12
@_author: Paul Hoffman 
@_subject: The perils of security tools 
I'm confused about two statements here:
. . .
The second bit makes it sound like the stuff that the Debian folks blindly removed was one, possibly-useful addition to the entropy pool. The first bit makes it sound like the stuff was absolutely critical to the entropy of produced keys. Which one is correct?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-05-15 07:33:02
@_author: Paul Hoffman 
@_subject: The perils of security tools 
I take it that these are not 128-bit, non-monotonic PIDs. :-)
The bigger picture is that distributions who are doing local mods should really have an ongoing conversation with the software's developers. Even if the developers don't want to talk to you, a one-way conversation of "we're doing this, we're doing that" could be --Paul Hoffman, Director
--VPN Consortium

@_date: 2008-05-19 09:29:01
@_author: Paul Hoffman 
@_subject: The perils of security tools 
More interesting threadage about the issue here: , particularly in the --Paul Hoffman, Director
--VPN Consortium

@_date: 2008-09-08 10:33:11
@_author: Paul Hoffman 
@_subject: once more, with feeling. 
Ditto. :-)
It depends on how we think change can be achieved. Until now, people designing pages using bad security practices balanced their laziness with the fact that their content would be displayed anyway so whatever. You are proposing moving to the other extreme. Given how easy your solution would be for browser vendors to implement, we have to assume that they have considered it and rejected it.
A less extreme solution would be to make the warning the user sees on a mixed-content page more insulting to the bank. "This page contains both encrypted and non-encrypted content and is inherently insecure. The owner of this web site has clearly made a very poor security decision in showing this page to you. It is likely that other pages on this site also have similarly poor security. Knowing this, do you wish to continue anyway?"
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-09-10 08:22:40
@_author: Paul Hoffman 
@_subject: once more, with feeling. 
That may concern you, but I consider it a feature. Instead of teaching users to "always click through the damn dialog boxes", FF3 says "if you fell for it once, you're going to always fall for it so we won't teach you bad habits". There are arguments for either Given that few or none of us on this list are actually trained interface experts, I'm sure we could debate this until Perry pulls the moderator switch again. The salient point is that people who have more stake in the game (Mozilla Inc.) have spent longer thinking about this than we give them credit for and come to the design decisions that they have.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2008-09-14 11:09:08
@_author: Paul Hoffman 
@_subject: "Cube" cryptanalysis? 
There now: --Paul Hoffman, Director
--VPN Consortium

@_date: 2008-09-29 16:21:04
@_author: Paul Hoffman 
@_subject: TLS Server Name Indication and IDNA? 
============================== START ==============================
RFC 4366 is somewhat of a mess. I do not remember the authors asking the authors of IDNA (of which I am one) about what they should do.
FWIW, I'm not sure why this would be on the cryptography list, but I'm not sure of that for most of the "we can design a better UI" threads either.
Hopefully, the former. But if that doesn't work, try the latter.
Hopefully, neither: leave it as an ACE.
Hopefully, neither: leave it as an ACE.
Yes+. That's why we designed IDNA that way.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-08-19 15:28:22
@_author: Paul Hoffman 
@_subject: Certainty 
I understand that "creaking" is not a technical cryptography term, but "certainly" is. When do we become "certain" that devastating attacks on one feature of hash functions (collision resistance) have any effect at all on even weak attacks on a different feature (either first or second preimages)?
This is a serious question. Has anyone seen any research that took some of the excellent research on collision resistance and used it directly for preimage attacks, even with greatly reduced rounds?
The longer that MD5 goes without any hint of preimage attacks, the less "certain" I am that collision attacks are even related to preimage attacks.
Of course, I still believe in hash algorithm agility: regardless of how preimage attacks will be found, we need to be able to deal with them immediately.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-08-19 15:31:27
@_author: Paul Hoffman 
@_subject: Crypto '09 rump session summary? 
...some summaries of some of the presentations...
More like this, please! The rump sessions have a lot of value (beyond the often-strained attempts at humor).
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-08-23 12:00:03
@_author: Paul Hoffman 
@_subject: Certainty 
Getting a straight answer on whether or not the recent preimage work is actually related to the earlier collision work would be useful.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-08-25 09:15:33
@_author: Paul Hoffman 
@_subject: SHA-1 and Git 
Perry is completely correct on the two previous paragraphs. Hard-coded algorithms, particularly asymmetric encryption/signing and hash algorithms, will lead to later scrapping and a transition for the entire protocol. But it is quite easy to build a protocol with too many knobs, and IPsec is living proof of that. TLS's fixed, registered ciphersuites are far from perfect, but in retropsect, they seem a lot better from an operations / deployment standpoint than IPsec.
A different answer to Ben would be "because not planning sooner causes your software users more grief". If you build in both algorithm agility and a few probably-good algorithms, the operational changes needed when one algorithm fails is low. Later software updates that contain other changes can also include new algorithms that are suspected to be good even if all of the original ones fail.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-01-01 13:37:56
@_author: Paul Hoffman 
@_subject: Security by asking the drunk whether he's drunk 
That part is true.
That part, I think, is wrong. I looked into this a bit earlier this month and found that most of the ones I looked at are still using sequential numbers.
If by "arbitrary text" you mean "a non-negative integer".
True as well.
The attack is on end users who trust a root store that has a trust anchor from *any single* CA that uses MD5 and has predictable sequence numbers. The attack lets the attacker become a subordinate CA for that CA. At that point, the attacker can issue their own certs for any purpose.
It never does. That's why it is the sky.
There are not CRLs for CAs. That's why is it is a root store.
Oh, and how do you create a definitive list of CAs that use MD5 in their signatures?
Quite right.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-01-17 12:03:57
@_author: Paul Hoffman 
@_subject: MD5 considered harmful today, SHA-1 considered harmful 
No offense, Benne, but are serious? Why would "everybody" even consider it? Give what we know about the design of SHA-2 (too little), how would we know whether SHA-3 is any better than SHA-2 for applications such as digital certificates?
In specific, if most systems have implemented the whole SHA-2 family by the time SHA-3 is settled, and then there is a problem found in SHA-2/256, I would argue that it is probably much more prudent to change to SHA-2/384 than to SHA-3/256. SHA-2/384 will most likely be much than to SHA-3/256, but it will have had significantly more study.
It all depends on who you trust and why.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-01-19 08:33:26
@_author: Paul Hoffman 
@_subject: MD5 considered harmful today, SHA-1 considered harmful 
Sure. I need 128 bits of pre-image protection for, say, a digital signature. SHA2/256 is giving me that. Then, due to some weakness, it is only giving me 112 bits of protection. The weakness is understood in the crypto community, and it's a straight-line loss of bits of protection.
SHA2/384 would then give me 168 bits of protection, which is more than the 128 what I need.
Even if you don't trust that there is a straight-line loss of bits, you would have to be believing that the attack is much worse for SHA2/384 than it was for SHA2/256 in order to bring the output down to the level that I need.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-01-26 08:26:24
@_author: Paul Hoffman 
@_subject: Obama's secure PDA 
Government Computer News says it is definitely not a BlackBerry. However, GCN's reporters aren't always as good as they should be (or even as good as the regular IT press) on getting their facts straight on security issues.
I too would like to hear more information on this, particularly the crypto that is known to be used on the Edge.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-07-04 11:59:01
@_author: Paul Hoffman 
@_subject: MD6 withdrawn from SHA-3 competition 
I agree more with Brandon than with Steve, but who knows. I read Ron's message as a challenge to NIST about whether or not NIST would really rely on the proofs. It was clear they didn't want to withdraw MD6, but that they felt like they had to because of the speed requirement.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-07-05 07:07:50
@_author: Paul Hoffman 
@_subject: MD6 withdrawn from SHA-3 competition 
The more important question, and one that I hope gets dealt with, is what is a sufficient proof. We know what proofs are, but we don't have a precise definition. We know what a proof should look like, sort of. Ron and his crew have their own definition, and they can't make MD6 work within that definition. But that doesn't mean that NIST wouldn't have accepted the fast-enough MD6 with a proof from someone else.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-07-14 14:38:37
@_author: Paul Hoffman 
@_subject: HSM outage causes root CA key loss 
Bingo. Key rollover has been thinly tested in relying parties.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-07-19 12:24:34
@_author: Paul Hoffman 
@_subject: 112-bit prime ECDLP solved 
Why not just go with 256-bit EC (128-bit symmetric strength)? Is the 8 bytes per signature the issue, or the extra compute time?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-06-06 17:16:30
@_author: Paul Hoffman 
@_subject: Factoring attack against RSA based on Pollard's Rho 
Is there any practical value to this work? That's a serious question. The main statement about the value is "This is a factoring attack against RSA with an up to 80% reduction in the search candidates required for a conventional brute force key attack." Does that mean that it reduces the search space for a 1024-bit RSA key to, at best 205 bits (0.2 * 1024) of brute force? That is a silly reduction; reducing it to anything less than the estimate for NFS (about 80 bits) is not useful. Or, can this attack be combined with NFS? Or...?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-05 10:17:00
@_author: Paul Hoffman 
@_subject: Has any public CA ever had their certificate revoked? 
Peter, you really need more detents on the knob for your hyperbole setting. "nothing happened" is flat-out wrong: the CA fixed the problem and researched all related problems that it could find. Perhaps you meant "the CA was not punished": that would be correct in this case.
This leads to the question: if a CA in a trust anchor pile does something wrong (terribly wrong, in this case) and fixes it, should they be punished? If you say "yes", you should be ready to answer "who will benefit from the punishment" and "in what way should the CA be punished". (You don't have to answer these, of course: you can just mete out punishment because it makes you feel good and powerful. There is lots of history of that.)
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-05 16:06:04
@_author: Paul Hoffman 
@_subject: Has any public CA ever had their certificate revoked? 
Tautologically so.
Even with this definition, there was no significant punishment in this case. I'm not saying there should be, particularly because the CA cleaned things up fairly rapidly, but only a few people probably have reduced their trust of the CA in question.
That has never been shown in a case of CAs not following their stated procedures.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-06 06:53:07
@_author: Paul Hoffman 
@_subject: Has any public CA ever had their certificate revoked? 
We agree fully, then.
This assertion is probably, but unprovably, wrong. I suspect the CA now has better mechanisms in place to check for the problem in the future, and I suspect that a few other CAs seeing the kerfuffle probably added their own automated checks. Note that these are checks that should have been in place before the error was found.
...because not only did no one die, but also the CAs were able to fix the problem.
Slight worry about making a more serious mistake than happened here.
Fully agree.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-07 20:08:44
@_author: Paul Hoffman 
@_subject: 80-bit security? (Was: Re: SHA-1 collisions now at 2^{52}?) 
That's an oddly-worded question.
It is true that NIST has specified that algorithms with 80 bits of effective strength should stop being used in US government systems after the end of 2010.
It is not true that the accepted wisdom is 80-bit crypto "is to be retired" by the end of 2010.
It is true that some uses of SHA-1 have 80 (now many fewer) bits of effective strength.
It is not true that SHA-1 gives 80-bit security; many uses of a hash rely on the preimage resistance, not the collision resistance.
It may be true that 1024-bit RSA and DSA gives 80 bits of effective strength, and it is true that this is the accepted wisdom. This is based on some wild hand-waving and scaling assumptions with very few data points, and particularly few in the past five years since that number became oft-repeated accepted wisdom.
Bupkis. The best asymmetric attack published so far is about 700 bits. No one has produced a SHA-1 collision at 62 bits of effort (the previous estimated work). Our ability to extrapolate work effort to 80 bits is questionable indeed.
How could we tell? The whole point of the paper was estimating the strength needed to keep a secret *for a long time*. We are only 13 years into the 20 years that they used as a basis for the estimate of 90 bits.
Asking that question six years into the 30 years (if those were the numbers they used) is begging to make approximations on insufficient data.
That's one problem. Another is that because it can also be used in environments where 160ish bits of security are needed and it's still believed to be fine there, people on this list and in the press are sloppy when they speak about its use. Another is that people on this list and in the press are sloppy about security decisions that involve periods of time longer than about a year.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-08 09:08:03
@_author: Paul Hoffman 
@_subject: Has any public CA ever had their certificate revoked? 
Comodo publicly said they did. That's why I said "researched all related problems that it could find".
And yet they appear to have done it.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-05-22 07:34:57
@_author: Paul Hoffman 
@_subject: End-of-chapter questions for "Practical Cryptography"? 
Greetings again. I'm helping someone new to the field learn cryptography. He's a book-learner, and is starting with Ferguson & Schneier "Practical Cryptography". I would love to give him some things to think about after each chapter to make sure he's thinking about the big picture.
Has anyone on this list used the book to teach a class? If so, did you create a list of discussion questions? Or, do people know profs who have used the book to teach? Any pointers are appreciated.
--Paul Hoffman

@_date: 2009-10-14 17:02:34
@_author: Paul Hoffman 
@_subject: Possibly questionable security decisions in DNS root 
What part of owning a temporary private key for the root zone would be worth even 10% of that much? There are attacks, and there are motivations. Until we know the latter, we cannot put a price on the former.
Related question: if all the root keys were 2048 bits, who do you think would change the way they rely on DNSSEC?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-10-14 17:02:34
@_author: Paul Hoffman 
@_subject: Possibly questionable security decisions in DNS root 
What part of owning a temporary private key for the root zone would be worth even 10% of that much? There are attacks, and there are motivations. Until we know the latter, we cannot put a price on the former.
Related question: if all the root keys were 2048 bits, who do you think would change the way they rely on DNSSEC?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2009-10-29 07:15:53
@_author: Paul Hoffman 
@_subject: AES-CBC + Elephant diffuser 
Yeah, we all know what a light-weight and careless person Neils Ferguson is. Who would listen to him?
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-04-20 09:07:06
@_author: Paul Hoffman 
@_subject: Quantum Key Distribution: the bad idea that won't die... 
You hit it: "almost". As long as a few researchers are interested, and there is money to be thrown down the drain^w^w^wat them, there will be active development.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-04-21 16:17:25
@_author: Paul Hoffman 
@_subject: The EC patent issues discussion 
This is starting to turn around. More vendors are questioning the murk. Please see .
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-08-02 15:53:34
@_author: Paul Hoffman 
@_subject: /dev/random and virtual systems 
It is certainly doable: put a "file" on the host whose contents are random and change every second. On the VM, read that file on wakeup or boot and mix it into /dev/random. This guarantees a different value for each wakeup/boot, but not that every cloned machine that starts will have a unique state (because they might start within the same refresh. If you need that, you probably want to automatically mix a microsecond-accurate time at the same time.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-08-15 17:27:14
@_author: Paul Hoffman 
@_subject: 2048-bit RSA keys 
You are under the wrong impression, unless you are reading vastly different crypto literature than the rest of us are. RSA-1024 *might* be possible to break in public at some point in the next decade, and RSA-2048 is a few orders of magnitude harder than that.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-08-16 12:42:41
@_author: Paul Hoffman 
@_subject: 2048-bit RSA keys 
We have no idea. The methods used to factor number continue to slowly get better, and it has been quite a while since there was a single large improvement. That could mean that there are no more improvements to be made, or that some smart cryptographer who isn't focused on the SHA-3 competition is about to make another big improvement, or something in between.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-07-17 08:23:01
@_author: Paul Hoffman 
@_subject: Root Zone DNSSEC Deployment Technical Status Update 
Thierry, can you say how using one of those alternatives would look different than the DURZ that they used? Should they all be marked as "unverfied" in a compliant DNSSEC resolver? If not, I am interested in how you think the differences would have manifest in the real world.
FWIW, this was *widely* publicized, particularly on mailing lists that Thierry is on. It even made the IT trade press.
As a side note, I find the IT press part disturbing, given that the key signing ceremonies were just as much security theater as many of the things we like to chide on this list.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-09-14 08:14:59
@_author: Paul Hoffman 
@_subject: Folly of looking at CA cert lifetimes 
At 10:57 AM -0400 9/14/10, Perry E. Metzger did not write, but passed on for someone else:
No, no, a hundred times no. (Well, about 250 times, or however many CAs are in the current OS trust anchor piles.) The "lifetime" of a "CA key" is exactly as long as the OS or browser vendor keeps that key, usually in cert form, in its trust anchor pile. You should not extrapolate *anything* from the contents of the CA cert except the key itself and the proclaimed name associated with it.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2010-09-14 14:54:12
@_author: Paul Hoffman 
@_subject: Folly of looking at CA cert lifetimes 
Ah, I see what you are saying, and what Perry's anonymous forwarder meant. That is, if the "CA keys have lifetimes of 10 years or more" means "because that's how long OSs and browsers leave them in the trust anchor pile", then it has nothing to do with the built-in notAfter dates in the server certificates.
--Paul Hoffman, Director
--VPN Consortium

@_date: 2013-11-01 08:10:57
@_author: Paul Hoffman 
@_subject: [Cryptography] FIPS 140 testing hurting secure random bit 
Actually, I meant both the CMVP staff at NIST *and* some of the labs. Both have that reputation (that may or may not be deserved).
+1 to all that. When I interviewed about a dozen vendors about CMVP a few years ago, the overall themes were:
a) The way this program is run hurts security, and it would be easy to change it
b) But NIST won't change it because they have no idea about operational security
c) You must not quote me because my lab might rat me out to NIST
There was no differentiation between NIST CMVP and NIST CSD except for maybe two vendors.
--Paul Hoffman

@_date: 2013-10-10 16:17:31
@_author: Paul Hoffman 
@_subject: [Cryptography] PGP Key Signing parties 
Phil Zimmerman and Jon Callas had started to work on that around 1998, they might still have some of that design around.
--Paul Hoffman

@_date: 2013-10-30 07:20:11
@_author: Paul Hoffman 
@_subject: [Cryptography] FIPS 140 testing hurting secure random bit generation 
From what multiple implementers (not just Peter) have said: yes.
This is a somewhat absurd suggestion for two reasons:
- The NIST CMVP people have a reputation (that may or may not be deserved) for taking much longer to validate systems from boat-rockers. I have been told by implementers that their labs explicitly told them not to complain about anything during the 140-3 development process because of this.
- The folks in NIST Computer Security Division are down the hall from these people. They are writing rules for the documents generated by CSD. The people in CSD need to lead the charge for fixing the broken testing, not asking people who are already paying a hundreds of thousands of dollars, and losing even more of that in delayed sales, to do the work of fixing CMVP.
This problem has been known by the CSD and CMVP people for many years. The other deep problems with the CMVP has been known for many years. Everyone looks at NIST as NIST, not as two departments. You can fix this, but we can't.
--Paul Hoffman

@_date: 2013-10-30 19:07:51
@_author: Paul Hoffman 
@_subject: [Cryptography] FIPS 140 testing hurting secure random bit 
As you know, I was a consultant for NIST CSD for a while.
There was no "automatically" above for the obvious reason. However, CSD can still fix this in a way that no vendor or commenter can hope to do. CSD has the crypto knowledge *and the position in the NIST org chart*; none of us have the latter.
Then maybe the FIPS 140 evaluations are inconsistent; that's clearly worse.
That seems incorrect. NIST 800-90 A/B/C are open for comments; FIPS 140-2 and the still-delayed FIPS 140-3 do not appear to be. The problems in this thread are with the FIPS testing, not with 800-90 (other than that B and C are about five years late; they clearly should have come out with the first version of A, as many people said at the time). Outside people commenting on the new drafts will not fix the FIPS 140 evaluation procedure; inside people who wrote the specs on which FIPS 140 testing is based can do so.
--Paul Hoffman

@_date: 2013-09-14 09:31:22
@_author: Paul Hoffman 
@_subject: [Cryptography] RSA equivalent key length/strength 
Also see RFC 3766 from almost a decade ago; it has stood up fairly well.
--Paul Hoffman

@_date: 2014-04-01 14:49:57
@_author: Paul Hoffman 
@_subject: [Cryptography] TLS/DTLS Use Cases 
SIP is a big one.
But, to me, "every layer 7 protocol that runs over TCP" is a valid response. TLS' security properties add assurance to a client that they are reaching the intended server (based on IP address or DNS name) and encryption of the layer 7 protocol. Every other security property is just gravy and should probably be ignored.
WORM CAN ON A SLIPPERY SLOPE! Being actively discussed in the IETF. Opinions here, strong as they might be, probably won't help one way or the other. (My opinion is "yes" with at least two sentences after it.)
--Paul Hoffman

@_date: 2014-12-12 13:54:32
@_author: Paul Hoffman 
@_subject: [Cryptography] Sony finding SHA1 collisions? 
I had not heard that any progress had been made on reducing the preimage strength of SHA-1, or even that of the otherwise-derided MD5. Pointers to such progress would be greatly appreciated.
--Paul Hoffman

@_date: 2014-12-16 08:19:47
@_author: Paul Hoffman 
@_subject: [Cryptography] Any opinions on keybase.io? 
Not at all. You can use their web UI without doing anything from the command line. This brings in some completely terrible features involving your private key, but no one has proposed any other way of doing what they do in a browser context with less terrible things.
Do note that that article does not give any actual solutions for people who do not completely trust their enterprise or service provider. A better description of the article is "we can and should make life much easier for those who trust others with their keys and identity". However, many of us tell our friends not to do that, particularly with high-value keys or identities.
Keybase for the command line is open source. They have said they would like to federate, but I don't think anyone else has stepped up to do the work they have done, so there isn't anyone to federate with.
--Paul Hoffman

@_date: 2014-12-17 07:50:26
@_author: Paul Hoffman 
@_subject: [Cryptography] Any opinions on keybase.io? 
You say that as if you have proposed a design that allows people with only web browsers, not control of their command line, to securely share their identities. I don't see that in your linked article. Or are you saying that participation in this type of identity federation should only be allowed to those of us with those capabilities, or people who are willing to run some "trusted" binary executable for our platform?
--Paul Hoffman

@_date: 2014-12-17 07:53:49
@_author: Paul Hoffman 
@_subject: [Cryptography] Any opinions on keybase.io? 
Yep, and AFAICT, it is equally terrible to keybase.io. (More or less depending on whether you trust Google and Yahoo...)
That doesn't help Johnny encrypt his personal communications. It's good stuff, but orthogonal to this thread.
--Paul Hoffman

@_date: 2014-12-22 08:10:40
@_author: Paul Hoffman 
@_subject: [Cryptography] Certificates and PKI 
Not "magically", but systematically. Every domain owner has chosen a registry, and possibly a registrar (some registries do not have registrars). There is an established business relationship with the registry that means that you trust them with serving your name correctly. There is fate-sharing between the name you registered and the contents that the registry advertises. Given that fate-sharing, they are in a position to attach any DNS-specific semantics to your name, such as the DS records.
--Paul Hoffman

@_date: 2014-12-23 08:12:22
@_author: Paul Hoffman 
@_subject: [Cryptography] Certificates and PKI 
Not at all. As a bunch of other folks indicated on this thread:
- In the DNS model, the registry completely controls the domain by controlling the NS records, and even the existence of the domain. Thus, it saying "and here's the signing key that the domain will use" is no more control than it already has.
- In the CA model, the CA has no control over the domain itself. If a CA wants to "take control away" from a domain owner, it can issue a cert to someone else, but the domain still belongs to the original owner. All that CA can do is to say "if you got here through bad information, I can pretend that this domain belongs to these other folks".
Many of us believe that this is a strong distinction.
--Paul Hoffman

@_date: 2014-12-23 10:12:32
@_author: Paul Hoffman 
@_subject: [Cryptography] Certificates and PKI 
It depends on who you mean by "we".
We domain owners trust registries more than CAs *because we have to*. In a hierarchically-allocated structure like the DNS, owning a node always means trusting everyone up the tree from you to not remove your name and to enter bad data for your name (such as transferring ownership of your name to someone else). That is inherent in the tree structure.
We relying parties don't have to trust anyone more than anyone else, but given that we don't really understand trust at all and just in a muddle about it, we end up trusting the DNS hierarchy just as much as name owners are forced to trust it. In other words, we relying parties don't even know that we are trusting the registries unless we are in the 5%* of Internet users who understand name hierarchies, but we are trusting them nonetheless.
Lest this feels like it is not about crypto, it is. The crypto piece to all of this is not the keys or signatures on the data, but the discovery thereof. A CA creates assurances that are discovered and used in TLS; DNSSEC creates assurances that are discovered and used in DNS; DANE creates assurances that are discovered in DNS and used in TLS. If you don't discover the key or signature, you can't use it.
--Paul Hoffman
* A precise-sounding but completely-made-up number

@_date: 2014-02-04 08:06:12
@_author: Paul Hoffman 
@_subject: [Cryptography] Random numbers only once 
Many others have pointed this out for over a decade as well.
See my message to this list on January 28 with the subject "Not everything is Linux (was: Re: [Cryptography] cheap sources of entropy)"
You are correct that it hat may or may not be a good idea for various reasons, but it also might not be such a good idea. Different OSs have very different views of what is and is not a good idea.
Fortunately, the most significant OSs have people who have thought about random numbers at least as much as the folks on this list.
--Paul Hoffman

@_date: 2014-01-28 18:35:40
@_author: Paul Hoffman 
@_subject: [Cryptography] Not everything is Linux (was: Re: cheap sources of 
One of the threads earlier on this meta-topic pointed out that this does not need to be true: keying sshd during the first boot of a system is handy but often completely unnecessary; it obviously can be dangerous.
This is part of the reasoning for the design of some non-Linux systems to not assume that application developers understand this. On all recent FreeBSDs:
# dir /dev | grep random
crw-rw-rw-   1 root  wheel     0x14 Oct  7 07:01 random
lrwxr-xr-x   1 root  wheel        6 Oct  7 14:00 urandom -> random
--Paul Hoffman

@_date: 2014-03-17 15:17:29
@_author: Paul Hoffman 
@_subject: [Cryptography] Better than passwords and cookies 
The OBC work kind of died, but we have many of the ideas in

@_date: 2015-03-19 10:20:17
@_author: Paul Hoffman 
@_subject: [Cryptography] TB2F CAs as (un)official browser policy 
Careful there. The quoted paragraph was not about number of certs issued, but about number of validations using however many issued certs there were.
--Paul Hoffman

@_date: 2015-05-04 14:13:06
@_author: Paul Hoffman 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Because registries (not registrars) controls the ownership already. It's not a matter of they "verifying" ownership: they control all the delegation in their namespace.
Note how this is completely different than CAs who don't own the namespace at all.
--Paul Hoffman

@_date: 2015-05-14 09:24:57
@_author: Paul Hoffman 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
6. Breaking the collision resistance for older widely-deployed hash functions is possible; breaking the preimage resistance so far is not
