
@_date: 2007-04-05 08:27:20
@_author: Joe St Sauver 
@_subject: DNSSEC to be strangled at birth. 
Dave mentioned:
#  Can anyone seriously imagine countries like Iran or China signing up to a
 that places complete control, surveillance and falsification
 in the hands of the US' military intelligence?  I'm not sure having control of the keys for the root zone would give you
all that. #  Surely if this goes ahead, it will mean that DNSSEC is doomed to widespread
  And unless it's used everywhere, there's very little point
 it at all.
This issue came up on Dave Farber's [IP] list; my comments to him (which
never appeared, perhaps because Dave was already sick of hearing about it,
or simply because my comments were boring :-)) are included below, for what they may be worth:
Three points to consider about the current DNSSEC "who should signs the root?" issue...
  1) While DNS is a critical core protocol, and one which has garnered      substantial miscreant attention, deployment of DNSSEC to fix some      of DNS' current weaknesses is still only embryonic. Most sites on      the Internet today neither sign their own zones nor have
     configured their name servers to cryptographically validate others'      domains.
     Numerical estimates for DNSSEC penetration range from just 0.001% to      0.0015% (see slides 74-75 in my "Port 53 Wars" talk, available at
      (or .pdf)),
     and the domains that *are* getting secured by DNSSEC are generally
     not the most popular domains, nor the ones which are being used for      critical online banking or electronic commerce, nor even those which      belong to market-leading (or thought-leading) technology companies.
     When DNSSEC is more broadly deployed it will be more practically
     useful; when it is more practically useful, it will be more broadly
     deployed. I'm sure it is no surprise to anyone that Internet      bootstrapping can be tough, whether we're talking about IP multicast,
     IPv6, jumbo frames, or, in this case, DNSSEC...
     Until substantial adoption does occur, we're largely arguing about      a theoretical issue of limited *practical* import.      If you want to help make DNSSEC (and the issue of who signs the root!)      one which *is* practically important, then folks need to *use* DNSSEC:
     -- if you operate name servers, configure the name servers you         administer to check the DNSSEC signatures of other zones,
     -- if you control one or more domains, sign your *own* zones, and
     -- talk to critical Internet partners you work with about DNSSEC         and the status of *their* name servers and *their* zones         (can you imagine the impact if even some of the giants such as         Google, Yahoo, CNN, the BBC, Amazon, AOL, IBM, Microsoft, Cisco,         WalMart, Citibank, etc., began to actually use -- and actively         encourage *others* to use -- DNSSEC?)
     DNS server admins who'd like to try DNSSEC can find pointers to      recipes for signing their own zones, and recipes for configuring      their name servers to check the signatures of others' zones, in my      talk at slide 76.
  2) So when *will* the question of *who* signs the root become technically
     important? Well, at the risk of offering a semi-tautological answer
     to a semi-rhetorical question, that will probably be when the root
     actually gets signed.
     The root zone is NOT signed today, and depending on your perspective,      signing of the root is either (a) imminent, or (b) something which may      *perpetually* remain at least six months away (see slides 55-58 from      my talk).
     If I were reading the tea leaves which are currently visible, I      think the indicator with the highest predictive value is likely      Verisign's February 2007 announcement of Project Titan, a three year      (and hundred million dollar) DNS upgrade initiative (see       ).
     I believe their completion of Project Titan may be a defacto      precondition for the potential signing of the root, although signing      of the root may still not occur even once Project Titan has been      completed (DNSSEC is clearly an after thought when it comes to that      expansion effort, not the central operational/business driver).
  3) Does this mean the whole matter of who signs the root is a complete
     non-issue? Most emphatically no.
     The issue of who signs the root is one which may be trivial as a      *practical* *technical* matter *today*, but it is one which is      potentially *huge* as a matter of policy and precedent, and as a      *longer term* practical technical issue, and as an issue which      has the potential to halt, slow, or potentially fragment DNSSEC's      actual deployment.
     If the issue of who signs the root cannot be consensually resolved,
     the most likely impact will be for DNSSEC adopters to move from
     a trust model rooted at "." to a trust model rooted at the TLD level.      Now, instead of having a minimal number of keys to juggle, sites      would be facing a far larger number of islands of trust, each
     with their own keys.
     Even with just DNSSEC's limited deployment to date, we already know      that when faced with the prospect of managing a large number of keys,
     adopters will turn to trusted third party brokers who *are* willing      to cryptographically vouch for multiple keys (for example see the      discussion of islands of trust and Domain Lookaside Validation (DLV)      at slides 59-61).
Bottom line, my belief is that ultimately the root *will* end up being signed. If the community viscerally or intellectually doesn't like the party providing that signature, the unhappy parts of the community have a number of options, including:
   -- they can ignore DNSSEC, not checking DNS signatures on their name       servers and not signing their own zones (remember that this is the
      default option selected by 99.999% of the online world right now,       including virtually everyone who may be reading this note)... but
      I think that would be... unfortunate.
   -- they can "hold their nose" and proceed (even if they're uncomfortable),
      using the default signed root unless/until some abuse of trust occurs       (and presumably everyone would be watching quite closely for any
      sign of inappropriate behavior, and presumably the party that       ultimately signs the root would know that and hopefully behave       accordingly)
   -- they can deploy a DLV-like solution, trusting a third party commercial
      or non-profit entity (or even some other government) to act as what       amounts to an alternative DNSSEC root-like trust anchor, or
   -- they can devote a tremendous amount of time and effort to arguing a       battle about who signs the root, potentially ultimately achieving a       Pyrrhic victory.
Given those options, and the current realities of DNSSEC deployment today, I'd suggest that people not devote their primary attention and energy to worries about whether or not a disliked or liked national authority ultimately signs the root, but rather I'd suggest that folks focus on whether or not DNSSEC ends up taking off at all. If you want DNSSEC to succeed, use it, talk about it, and write code to take advantage of its capabilities. Ultimately I believe the turf wars which may come up can be settled one way or another.
Joe St Sauver (joe at oregon.uoregon.edu)
Disclaimer: all opinions strictly my own

@_date: 2013-12-12 17:53:24
@_author: Joe St Sauver 
@_subject: [Cryptography] Size of the PGP userbase? 
Christian commented:
 is probably more than one gap. In fact, I see five fairly big issues:
 your own certificate, For technical users, one-off, if you aren't concerned about higher LOAs,
this is very doable (e.g., see the free Comodo client certs available at
 )
 the right software in your mail client, Most POP/IMAP email clients (e.g., Thunderbird, etc.) come with integrated
support for S/MIME
 the certificates of your peers, In the traditional S/MIME model, those certs are automatically bootstrapped
when you receive an S/MIME-signed email message.
 multiple computers,
It is admittedly a pain to export and reimport the same cert to multiple
hosts (or even multiple trust stores on the same host); I think there's a lot to be said for USB-format PKI hard tokens or smart cards for that sort
of scenario (it also handles the case where multiple people share a single
system, as they might in a computer lab)
 working with web applications.
Are you thinking of web email, or something else? If you're thinking of
web email, Penango does an amazing job of integrating with Gmail, for
All that said, I do think that there ARE some real operational issues
when it comes to using client certs:

@_date: 2013-12-20 09:03:17
@_author: Joe St Sauver 
@_subject: [Cryptography] The next generation secure email solution 
Ralf commented:
 user finds some website interesting and trustworthy and
 on. He gets a client-side X509 cert from the site  is stored on his local computer (including unprotected
 key) under a globally unique name, that can be but
 be his own.
Solutions that rely on client X509 certs inevitably run into
a fairly common set of issues, a subset of which include:

@_date: 2013-12-23 07:48:35
@_author: Joe St Sauver 
@_subject: [Cryptography] Passwords are dying - get over it 
Kent commented:
 love to say passwords are dead, but any alternate proposals they  suggest always seem worse to me.
And that's certainly consistent with the adoption rate we see for alternatives to passwords, at least when users are choosing based on ease-of-use/convenience, rather than the fully panopoly of considerations (including things like the security of the non-password-based authentication technology).
 seems to have the biggest head of steam by trying to become the  sign-in for everything else, Among OpenID providers, Google's certainly the leader (Janrain quotes their market share at 38%), but Facebook is non-negligible at 27% and Yahoo's still in the race at 14%, see (as quoted by me on slide 53 of  )
Of course, there are alternatives to OpenID, including CAS and Shibboleth, both popular in higher ed and some government contexts (e.g., the Federal GFIPM project,  , for example).
 then, because they are so important they can force you to carry a  fob, or something like that.  Actually, they probably won't go  a fob...
In Google's case, it's pretty clear that they're putting their bet on smart phones as their 2nd factor/2nd channel of choice. (See  )
I've got a page that lists a variety of phone-based two factor
authentication options at (if I've inadvertently overlooked anyone, please let me know and
I'd be glad to add them to that page)
 Google is working hard to know everything about me, and that is  to their security solution: they will know I am legit when I log in  they will know it is me because they have been following me.  A consideration to be aware of with OpenID is that not only will the OpenID provider know "everything" about you, the relying parties will ALSO get to know a surprising amount about you, courtesy of what gets shared by default with the relying party (although this can vary from identity provider to identity provider, see slides 56-59 of the slide deck mentioned above)
SAML-based solutions (like Shibboleth), on the other hand, support fine grained attribute release policies, but in that case, bilateral negotiation of attribute release policies (beyond minimal default attribute release
policies) may hinder global scaling properties.
 something like that, they don't exactly know how it will work, but they  getting good at recognizing login patterns and being confident I am  based how and where I login.
You're really talking about risk-based authentication.
It may be key to keeping folks from going nuts as a result of better-than-password technologies: if someone's doing something they always do, from where they always do it, when they always do it, and the trasaction's low risk anyhow, let them. On the other hand, if they're doing something unusual, from somewhere odd, uncommon time, and the transaction is "significant" (typically high dollar value, or security-sensitive), be careful and require stronger authentication.
The down side of that approach, from my POV, is that
it adds unpredictability to my logins -- maybe I'm
prone to forgeting my smart phone at home, and normally I can get away with it because I'm rarely
asked to do step-up authentication, but then bang,
once in a blue moon I might need it to login... ugh!
Regarding use of a simple notebook as a password cache, Kent observed:
 don't bring the whole notebook when  internationally, maybe leave it with someone trusted whom you  phone.)
That's one good recommendation, but not the only one
you should be thinking about if you're traveling
The Higher Education Information Security Council (HEISC) has collected a variety of recommendations related to international travel and cyber security; if interested, see: "Security Tips for Traveling Abroad",
Merry Christmas/Happy New Year,

@_date: 2013-11-04 08:17:30
@_author: Joe St Sauver 
@_subject: [Cryptography] DNSSEC = completely unnecessary? 
Greg commented:
 all my readings on it I kept walking away thinking that I understood
 purpose, but I'd then come back at myself with the same question:
 does it give us over HTTPS?
Consider the IETF DANE work. Currently it is hypothetically possible for any globally trusted CA to issue an SSL/TLS cert for any given domain. If you do DNSSEC, you now have a framework that will allow you to definitively assert that the cert for your domain should be *this* one and not some other one. I consider that to be a worthwhile improvement, in and of itself.
 poisoning isn't a serious threat if SSL/TLS is working correctly. 1) Not all network traffic (whether web or otherwise) is secured with SSL/TLS, on the other most network traffic does employ/rely on DNS.
2) I'd also note that some operationally critical bits and pieces get shared via DNS. For example, if you do SPF, you're making decisions about acceptable email sources for a given domain based on information published via DNS. It would be terrific if that data was secured against
cache poisoning. Ditto DNS-based blocklist results.

@_date: 2013-10-11 15:04:07
@_author: Joe St Sauver 
@_subject: [Cryptography] prism-proof email in the degenerate case 
commented:
 alternative I've been considering is having e-mail clients support  messages if they are received for an incorrect envelope  So you can have an envelope address and a PGP encrypted blob,  when you decrypt that blob there's a new RFC822 with a new envelope  and another PGP encrypted blob. If e-mail clients honor a  agreement on this kind of message, it will be practically  to tell who sent the original message and who is the final  really hard bit about this is that there are a lot of e-mail clients  there, and getting them all to support this - even optionally - is  take some doing.

@_date: 2014-08-01 10:11:14
@_author: Joe St Sauver 
@_subject: [Cryptography] You can't trust any of your hardware 
Pete Todd commented:
 dunno how to fix this. The best I can come up with is to make more of
 exploits happen - post anonymous bounties to get firmware and
 leaked?
Use of non-rewritable ROMs would obviously "fix" the vulnerability (for some definition of "fix," but that assumes you can produce flawless code that might never need to be patched or updated (or that you can live with pitch-and-replace (rather than patch ) as an update/remediate strategy).
The best practical solution I can think of to fix this would be to interpose
a manually operated physical switch on each device that would need to be
intentionally closed by the user to update the firmware on the device. [This sort of scheme would effectively be the firmware equivalent of a "write protect tab" on old floppy disks (man, sometimes I feel really old even *mentioning* stuff like that :-))]
After completing the intentional firmware update the physical switch would then be reset to its normal open state, thereby preventing involuntary.
programmatic updates of the device's firmware. That would, I think,
*largely* eliminate the issue of firmware being tampered with by malware, while still allowing occaisionally needed updates to be intentionally applied (albeit only by someone physically in contact with the device).
Corner cases? Presumably social engineering around activating the switch when you shouldn't be, or efforts to piggy back malicious updates on top of legitimate patches. I could also see people with large
centrally managed farms of systems being grumpy about having to manually physically switch-on hundreds or thousands of devices to
push firmware updates (but presumably this wouldn't be happening on
a daily (or even quarterly) basis).

@_date: 2014-01-03 11:08:15
@_author: Joe St Sauver 
@_subject: [Cryptography] nuclear arming codes 
Peter Fairbrother commented:
 technique I'm told is still in use is to affix glitter in random  in clear epoxy to missile and/or warhead parts, then shine  on it from variable positions and compare the return sparkle  to a known set for that patch. Very hard to forge.
And that technique was even covered by Wired, albeit with a different context:
"Don't Want Your Laptop Tampered With? Just Add Glitter Nail Polish"

@_date: 2014-01-05 15:07:00
@_author: Joe St Sauver 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Jerry commented:
 what it's worth, I think Chrome is probably, across time, the most  because Google puts a huge amount of effort involving a really  team into making it so.
The Educause Technologies, Operations and Practicies (formerly Educause
Security Effective Practices) working group started an effort to make
recommendations that would help higher ed people improve the security and privacy of their browser configs. I'd naively assumed that would be a relatively straightforward task, but I've increasingly come to appreciate just how subtle that objective actually was, even when it comes to something as seemingly straightforward as choice of browser.
For example, you mentioned Chrome, a very popular browser. There's a lot
to like about Chrome, including the fact that it support TLS 1.2, and the way it supports IPv6 (by way of contrast, Firefox still is stalled at
TLS 1.1, and even when network connectivity is dual stack, Firefox still prefers IPv4 over IPv6). And there are many more features in Chrome that
are really great, too,
On the other hand, Chrome is produced by the Internet's largest and most
successful online marketing enterprise. Perhaps not surprisingly, at least some have been critical of its user tracking provisions, and how Chrome handles privacy issues (e.g., see for example  , although I give Google credit for doing a good Chrome Privacy white paper, *if* people bother to read it, see
 ).
If you are ever feeling bored and/or paranoid, install Little Snitch on a Mac and fire up Chrome. You'll be surprised at the amount of outbound
traffic you'll see that you didn't explicitly originate when you're running Chrome in its default configuration.
What does this mean? Well, fundamentally, there may be tensions between browser security and privacy, where emphasizing one may require compromises when it comes to the other.  place some amount of trust in Safari, but that's a matter of  not anything special about the code:  People aren't  it as much.  (Apple seems to have been getting ever  serious, but how far they've come is hard to judge.)
My concern with Safari starts with the fact that Safari on at least
some operating systems has been "orphaned" -- for example, Safari
for Windows has been frozen at 5.1.7, which is distressing if you
believe Safari releases since then have fixed important security-related bugs. (See  to see what
versions are available for various platforms)
We could also talk about IE, but in that case, there's no version for Mac OS/X (which makes that a moot option when it comes to using
it in my case, unless I want to do something like run a Windows VM
for browsing purposes).
There is also Opera, but the scarcity of its adoption makes any user using it "stand out from the crowd," which is the antithesis
of what a privacy and security-concerned user may want. (The same
can be said for all the other uncommon browser options)
Like I say -- choice of browser for the security *and* privacy concerned users can be tough.
 just don't see how they can possibly be made secure.
You need to break a *lot* of functionality, particularly if you want
a browser that is both secure *and* private. It's unclear to me that
anyone can produce a web browser configuration that is secure, AND
privacy preserving, AND still usable with modern/popular Alexa 100
class web sites. And if you do manage to do so, you'll be distinctly ususual, and as such, you'll stand out from the normally-insecure and normally-heavily-tracked average user.
(And if you look at  , it quickly becomes
apparent that even if you block everything except things like your routinely-reported system font string and other routinely
reported-by-default bits, you're still going to be all-too-easily  do find fascinating the reaction to the never-ending series of  issues in Flash and Java.  What people have learned from  is:  Plugins are bad; Flash itself is bad.  Plugins are another example of a time when you need to make tough
choices. For example, in Firefox, there are terrific plugins that
do a nice job of blocking advertising (including potentially malvertising), and others that do a nice job of blocking trackers,
and still others that reduce the risks associated with scripting,
Deciding that you're going to run zero plugins may thus (at least in some cases) *decrease* your security and/or *increase* your privacy exposure.
And when it comes to Flash, things like the integrated Chrome Pepper Plugin architecture complicate Flash usage management
(  )
 * Same question, but for pdf files?
 think we have the makings of an excellent context here:  Pick  of these - PDF is probably the best choice - and ask for a  implementation.  Again, decisions in some browsers (such as Chrome) to include an
integrated copy of Adobe PDF Reader (see  ) complicates any effort to manage PDF content processing, including deploying an alternative PDF reader (such as Foxit Reader,  )
Trying to secure the web browser, and attempting to increase user privacy on the web, too, is a fascinating/challenging exercise.

@_date: 2014-07-08 08:05:55
@_author: Joe St Sauver 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Ian commented:
# There aren't specific restrictions as such with security clearances [1]
# but there are conflicts of interest.  If a person has a security
# clearance, then they have a master or power.  If they are devoted to
# your project, then this means they serve two masters, the best you can
# hope for is that the other master is dormant.
# That power can be used at will.  There are a range of pressures that can
# be put on a person to assist the power.
I think that's well put. For what it may be worth, I've enclosed below what I shipped to Bill directly on the 4th of July (I'd assumed that there wouldn't be much general interest in this topic, so I just replied to him directly, but given the discussion that's taken place, I'll throw this in for what it may be worth)...
Forwarded message follows:

@_date: 2014-05-20 10:38:24
@_author: Joe St Sauver 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
Eric asked:
 you explain why CA certs are futile for SMTP? It's not immediately
 to me. (I'm new to STARTTLS, have never configured it.)
I wouldn't say "futile," but I would say that there are some challenges.
Some of the challenges I see include:

@_date: 2014-05-27 08:04:05
@_author: Joe St Sauver 
@_subject: [Cryptography] client certificates ... as opposed to password 
Tony commented:
 great... so what's the problem? User experience
 certificate UX is terrible in all browsers. Worse, it's inconsistent
 browsers. Managing certificates is terrible. Hell, browsers can't
 decide whether or not they should use the system trust store or their
I'd distinguish between two phases when it comes to client certs:
provisioning, and routine use.
No question about it, provisioning is currently painful, although there are some commercial tools (Cloudpath ExpressConnect and SecureW2, for example) that have been putting a lot of effort into making cert installation a lot less painful, and the Internet2 community has also has been working on InCert, an open source option (see  and Or, for that matter, a determined individual can just do it the "hard way,"
as described in The key point about provisioning is that it's a comparatively rare event for
most users (e.g., annual, worst case) and if you only use a single device (e.g., one laptop you use for everything), it isn't too bad. If you have multiple accounts and multiple devices, it gets harder, due to
the need to move more certs around to keep everything sync'd, or the need
to use a different deployment model (e.g., instead of one cert everywhere,
perhaps a different cert on each device, a model that obviously falls apart
for encryption rather than just signing and authentication)
But what about routine use? *If* all the user is doing is S/MIME, and everyone uses the same key for signing as for encryption, key exchange via signed messages works okay, and most popular email IMAP clients support S/MIME and you can even use something like Penango for web email (free for free Gmail account users). That largely just works.
HOWEVER, routine use gets harder when:

@_date: 2014-09-12 14:08:25
@_author: Joe St Sauver 
@_subject: [Cryptography] HR 5099 
Regarding House Bill 5099, John had commented:
# I'm not an expert, but I'd wager that this bill isn't # going anywhere. Govtrack suggests (literally) a 7% chance of the bill getting
past committee, and a 2% chance of the bill actually being enacted.
In other words, I think that Govtrack thinks that you're right.
See For comparison, 11% of all bills made it out of committee, and
about 3% were enacted in general during 2011-2013. For those curious about how they model/estimate this, see

@_date: 2014-09-21 12:30:04
@_author: Joe St Sauver 
@_subject: [Cryptography] [cryptography] Email encryption for the wider 
Hope everyone's having a nice Sunday. John commented:
 sounds just like S/MIME, with the minor exception that S/MIME
 the key in the MIME body.  Once I send you a S/MIME signed
 your MUA can put my key in your address book, and you can send
 encrypted mail.  This has worked in MUAs since forever.
It works, but it's not a complete solution. For example, assume I want to use an escrowed encryption key, and a non-escrowed/non-repudiable signing
key -- that breaks this model, and users need to resort to an online directory to get the keys they need.
Online directories can be fine at enterprise scale, but fall apart at
Internet scale.   Anybody can send her email like this:
  S/MIME's solution was to require keys to be signed by a well
 CA, but we know how well that works in practice.
The biggest issue is not the fact that S/MIME involves use of a well
known CA, but that the binding is normally to an email address rather
than to a real identity (the personal cert version of domain validation
SSL/TLS certs).
Rigorous identity proofing is always a pain, and given that most folks
don't care (unless we're talking about something like a CAC or PIV card),
most client certs just treat your email as your "identity"
