
@_date: 2007-09-07 12:15:29
@_author: Dan Walker 
@_subject: In all the talk of super computers there is not... 
This is an interesting analysis, and the right way to proceed, I think,
when dealing with passphrases that contain sequences of natural language
words. I I think the 2.5 bits per word estimate, however, is a massive
The problem is that, when it comes to word sequences, the perplexity of
each contiguous word is much higher than for each contiguous letter in the
same text.  That is, there are many more possible symbols that can follow
the current one.  You identified several alternatives that have the "had
a" prefix, and they are are fairly likely.  Given the "had a" bigram, it
might be the case the the conditional entropy of the following token is
fairly small, compared to the general entropy of English unigrams.  If
"had a little" isn't right, though, the number of possible words that
might come next is massive.
I think the proper way to continue with this analysis would be to look
into  the research that has been done on n-gram language models.  I think
you'll find that even the best models will never get the conditional
entropy of an arbitrary word down to 2.5 bits.  That would mean that
basically had the next word narrowed down to less than 8 choices!  This
may occur in some very  common combinations, but in general the
conditional entropy will be much higher.  In addition, the phrase-initial
word will always have a fairly high perplexity, because the only thing to
condition the distribution over possible words for this case on is the
fact that it is phrase-initial.
That being said, it seems like the idea that the passprases are much less
secure than traditional character-lever analysis would suggest is spot on.
 Google recently published DVDs with their counts of the frequencies of
all n-grams up to 5-grams in their web corpus
Armed with that data and enough resources, one could build a language
model that would make passphrase guessing much more principled and could
reduce the amount of conditional entropy in a passphrase significantly. In fact, for passphrases up to 5 words in length, the entire phrase is
probably already in the Google data, it's just a matter of having the
resources to be able to get through them all.

@_date: 2007-09-10 13:26:45
@_author: Dan Walker 
@_subject: In all the talk of super computers there is not... 
True, the contestants are given extra information, though.  They know
ahead of time that the words make up the name of a place, or a common
saying, for example.  That helps decrease the entropy considerably.  They
also know the exact number of characters in the final answer and are able
to probe multiple characters in the phrase simultaneously.
If a system is setup correctly, you should never be able to get a hint as
to whether you have guessed any portion of a password correctly, and you
probably don't know what sort of phrase the target has chosen, so it would
seem like most of the entropy-reducing information the Wheel of Fortune
contestant is able to take advantage of are not available to a password
cracking algorithm.
