
@_date: 2001-04-18 01:09:38
@_author: dmolnar 
@_subject: New Discrete Log Record due to Joux and Lercier 
Tuesday, April 17 2001.
We are very pleased to announce a new record for the general discrete
logarithm problem. We were able to compute discrete logarithms modulo
a 120 digits prime. This was done in 10 weeks, on a unique 525MHz
quadri-processors Digital Alpha Server 8400 computer.
The approach we followed for this computation is close to the approach
we used for computing discrete logarithms modulo a 110 digits prime
[JoLe01]. It is mostly based on algorithms described in [JoLe00].
Precisely, let
  p = \lfloor 10^{119} \pi \rfloor+ 207819,
    = 314159265358979323846264338327950288419716939937510582097494\
      459230781640628620899862803482534211706798214808651328438483,
  g = 2,
  y = \lfloor 10^{119} e \rfloor,
    = 271828182845904523536028747135266249775724709369995957496696\
      762772407663035354759457138217852516642742746639193200305992.
  y   = g^262112280685811387636008622038191827370390768520656974243035\
          380382193478767436018681449804940840373741641452864730765082,
and, similarly,
  y+1 = g^39657965519539238631090956325038481900751981791165229696297\
          421520645832904710912189562251329527994908449750607046857937.
This result was obtained using a now classical algebraic sieve as
explained, from a theoretical point of view, by Schirokauer
[Schi93]. So, it was done in three steps:
  _ the sieving step,
  _ the linear algebra,
  _ the final computation of individual logarithms.
The sieving step consisted in finding couples (a,b) such that the
principal ideal (a+bt) is of smooth norm in the so-called ``right''
number field defined by
                        t^3-9*t^2-9*t+9
and such that (ac+bs) is of smooth norm in the so called ``left''
number field defined by
                        s^2
                        -7374389167922711279538633461199308033087*s
                        -333238556260219119547406855509826713348*c
                        c = 1201639291188427271122019272295979872125.
The Galois group of the degree 3 polynomial is of order 3. The
corresponding number field has 2 fundamental units, 1/12*t^2-t+1/4 and
-1/12*t^2+1/2*t+5/4, its class group is of order 1 and its index is
equal to 2^6*3^2.
Nothing is really known about the degree 2 polynomial except, of
course, that the corresponding number field is a real quadratic field.
The sieving was done efficiently following a now traditional ``sieving
by vector'' with ``special-q'' technique.  It yields many linear
equations between ``logarithms of ideals'' of norms smaller than
15485870 (1000000-th prime) in the left number field and of norms
smaller than 3497870 (250000-th prime) in the right number field.
After a 40 days computation on a quadri-processors alpha server 8400
computer, we obtained 2685597 equations with 1242551 unknowns.
At this point, it usually remains to add 5 Schirokauer maps to these
equations. Here, we added only 2 maps (for the left number field) and
prefer for the right number field, instead of what was done in
[JoLe01], to add explicitly fundamental units contribution taking
advantage that any ideal on the right number field is principal.
This step (maps+units) took one day.
Linear algebra:
We take advantage of the Galois group of t^3-12*t^2-9*t+12 [JoLe00].
Precisely, we found in less than 6 hours, using our very crude
implementation, two algebraic integers
      num = 136919628471533453465*t^2
          - 109185518772042207040*t
          - 218010383119442982304
      den = 90752177247861263294*t^2
          + 5976502381138861785*t
          + 161979899979266169279
such that
                         19^2*y = num/den modulo p
and such that,
        in GP-PARI notation, the principal ideal (num) is equal to
[[53, [7, 2, 0]~, 1, 1, [-18, 19, 12]~] 1] *
[[431, [-20, 2, 0]~, 1, 1, [168, 20, 12]~] 1] *
[[2179, [140, 2, 0]~, 1, 1, [-502, -300, 12]~] 1] *
[[16831, [-3928, 2, 0]~, 1, 1, [-1478, 7836, 12]~] 1] *
[[156781, [-45467, 2, 0]~, 1, 1, [15351, -65867, 12]~] 1] *
[[7691507, [-3118090, 2, 0]~, 1, 1,
 [-1592322, -1455347, 12]~] 1] *
[[5847120361, [-944554227, 2, 0]~, 1, 1,
 [-2613536996, 1889108434, 12]~] 1] *
[[7689099923, [-1979641955, 2, 0]~, 1, 1,
 [-1763503409, -3729816033, 12]~] 1] *
[[14023824873312563677, [2910448673841826685, 2, 0]~, 1, 1,
 [-4872413772263364932, -5820897347683653390, 12]~] 1]
        and the principal ideal (den) is equal to
[[2, [2, 0, 0]~, 1, 3, [1, 0, 0]~] 1] *
[[3, [1, -1, 1]~, 3, 1, [2, 2, 0]~] 3] *
[[19, [8, 2, 0]~, 1, 1, [-3, 2, -7]~] 1] *
[[1873, [-237, 2, 0]~, 1, 1, [889, 454, 12]~] 1] *
[[110359, [36889, 2, 0]~, 1, 1, [-37268, 36561, 12]~] 1] *
[[2672473789, [-932319595, 2, 0]~, 1, 1,
 [1125968488, -807834619, 12]~] 1] *
[[626844366559, [-255501920253, 2, 0]~, 1, 1,
 [-211702090482, -115840526073, 12]~] 1] *
[[685495972547, [-197652881054, 2, 0]~, 1, 1,
 [-276404069769, -290190210459, 12]~] 1] *
[[641614040507139551, [33835176915624305, 2, 0]~, 1, 1,
 [159262188369356649, -67670353831248630, 12]~] 1]
Then, using special-q descents, computing discrete ``logarithms for
the ideals'' of norms 7691507, 5847120361, 7689099923,
14023824873312563677 and 25465743776843, 160516256694037129 in the
right number field was (thanks to one hour computation for each ideal,
on a unique processor) equivalent to compute discrete ``logarithms for
 ideals'' of norms larger than those of the factor basis in the
left number field. Time needed for computing the 31 corresponding
discrete logarithms was at most one hour for each on a unique
So, as a conclusion, time that we need for computing discrete
logarithms modulo a 120 digit prime on a 525 MHz quadri-processor
alpha server 8400 computer is approximatively 12 hours for each, once
the sieving step (42 days) and the linear algebra steps (30 days) is
Antoine JOUX    (DCSSI, Issy les Moulineaux, France, Antoine.Joux at ens.fr),
Reynald LERCIER (CELAR, Rennes, France, lercier at celar.fr).
[JoLe01] A. Joux and R. Lercier, ``Discrete logarithms in GF(p)'',
         January 19 2001. Announce on the NMBRTHRY Mailing List.
         Available at [JoLe00] A. Joux and R. Lercier, Improvements to the general Number
         Field Sieve for discrete logarithms in prime fields, acceped
         for publication at Math. of Comp., 2000. Preprint
         available at [Schi93] O. Schirokauer, Discrete Logarithms and local units. Phil.
         Trans. R. Soc Lond. A 345, pages 409-423, 1993.
[LaOd91] B.A. Lamacchia and A.M. Odlyzko, Computation of discrete
         logarithm in prime fields, Designs, Codes and Cryptography,
         1991, volume 1, pages 47-62.
[GoLo89] G.H. Golub and C.F. van Loan, Matrix computations, chapter 9,
         The John Hopkins University Press, 1989, Mathematical
         Sciences.
End of NMBRTHRY Digest - 16 Apr 2001 to 17 Apr 2001 (

@_date: 2001-07-18 17:19:04
@_author: dmolnar 
@_subject: I'm looking for FSE2001 proceedings 
It also doesn't seem to be on Springer LINK yet.
For what it's worth, this is not my experience. While not every paper is
online, a large number of people make papers available from their web
Plus I'm very lucky in that my library subscribes to the online Springer
LINK service.
It's been a while since looking at the Springer-Verlag copyright notice. I
remember that it allows authors to publish papers on their web pages. Is
this correct?
What are the ACM and IEEE copyright terms like? Do they also allow
publication on the web?
Computer science might have it better than the life sciences in this
regard. de facto if not de jure.

@_date: 2001-06-22 16:08:31
@_author: dmolnar 
@_subject: crypto flaw in secure mail standards 
One way to do this is called a "designated verifier signature,"
originally AFAIK discussed by Jakobsson, Impagliazzo, and Sako. Rivest
has a paper up on his web page right now giving a particularly nice way of
implementing it by means of "ring signatures." In a ring signature, you
can determine that the message was signed by a member of a set S, but
not who exactly that member is.
So Alice signs document D as being from the set {Alice, Bob} and sends it
to Bob. Now Bob knows he didn't write D, so he believes it's from Alice.
If he passes D along to Charlene, she can't determine whether Alice
wrote D or Bob came up with it himself.
In fact, IIRC, the paper suggests the sorts of scenarios discussed in this
thread explicitly as the motivation for this use of ring signatures. The
paper then goes on to argue for the practicality of implementing ring sigs
in mail clients.

@_date: 2001-06-26 14:56:45
@_author: dmolnar 
@_subject: crypto flaw in secure mail standards 
Good point. There's always the approach of Alice sending each of
Bob,Charlie,Daniel,Eve,Fred,Greg a separate individual DVP, but this
incurs undesirable overhead. Plus Alice could send them all slightly
different messages and they'd be unable to prove to each other that she
was trying to trick them.
The original Impagliazzo, Jakobsson, Sako paper partially adresses this.
Their method of implementing designated verifier proofs is to perform
proofs of the form
In the peer-to-peer case, A is "document signed by Alice" and K is Bob's
private key. So Bob knows the document is real (assuming he hasn't lost
K), but no one else can be sure that he has not faked the proof/forged a
Their suggestion for multiple recipients is to have all the recipients
share a key K. Following your example, Bob,Charlie,Daniel,Eve,Fred, and
Greg generate a secret key using some distributed algorithm such that they
each have a share of the key and all shares are required to reconstruct.
Now when Alice sends the document to them, each one knows, given that
their share is safe, that the document really did come from Alice. An
outside observer, on the other hand, has to consider the possibility that
the group got together to forge a signature with their knowledge of K.
(This forgery can also be done in a distributed manner, so faking one
signature doesn't reveal the full key K to anyone).
There are at least two flaws with this approach. The first is that it
seems to require a new key K for each group of receipients. This may be
adressable through the use of group signatures. I haven't thought about it
much. I suspect this flaw is closest to the objection you raised.
The second flaw is tangential, but I think worth noting. The flaw is that
the members of the group must collaborate in order to forge a signature.
Whether or not the designation "works" depends on whether or not it is
plausible that the group members would collaborate.
For instance, suppose that a message is designated to Bob and Carol, who
are the heads of state of two warring countries? (The scene from "13 Days"
with its "these assurances can never be made public" comes to mind, but
isn't quite right).
It is not plausible that Bob and Carol would create a shared key K in the
first place, never mind collaborate to forge a signature. If the message
leaks and is published, Alice cannot convincingly argue that Bob and
Carol signed the message instead of her. This might be bad. So the issue
becomes not so much one of efficiency, but repudiation.
So how do you do designated verifier proofs with multiple
non-collaborating verifiers?
I think this may be partially adressed as follows:
Suppose Alice wants to designate a message to Bob and Carol. Alice sends
to Bob
and to Carol sends
Here M is the message, and by "X :=" I am giving a definition of X.
DVS(E,F) stands for "A non-interactive designated verifier proof of E
which is designated to F." E_B and E_C are public-key encryption using Bob
and Carol's public keys respectively. The notation is slightly
inconsistent as written, because by "DVS(M, Carol)" I really mean a
signature of M designated to Carol, while by "DVS( "X := E_C(DVS(M,
Carol))", Bob) I mean a proof that X is a valid encryption of a designated
verifier signature.
I claim that this gives us a signature in which
  	signature of M. This is required to prevent Alice from sending
and the above is similar for Bob.
For three designees, Bob, Carol, and David, Alice prepares
For four or more the construction is similar.
Note that the length of the signature grows linearly in the number of
designees. So if there are n designees, Alice has the pleasurable job of
computing n^2 messages, none of which are nice by themselves. My guess is
that there's some cute combinatorial result I don't know which will make
this easier or show it optimal.
The issue now is proving that a string X is a valid encryption of a
designated verifier signature. Note that this is a statement with a
succinct polynomial time certificate, namely the transcript of the program
used to create these messages. So by the generic GMW theorem that all NP
statements have ZKPs, plus random oracles to make them noninteractive, we
know this can be done. by foolhardy masochists, anyway.
A better solution seems to be to note that the IJS paper implements
designated verifier signatures as ordinary signature schemes + a chameleon
commitment function. I will consider chameleon hash functions (introduced
by Rabin and Krawczyk) instead. A chameleon hash function is a public hash
function with a secret key; knowing the key allows you to produce
arbitrary hash collisions easily, while finding collisions remains
intractable without the key.
Chameleon hash functions are randomized, so they take the form
CHAM-HASH(Key,M,r), where "Key" is the public description of a particular
entity's hash. For example, CHAM-HASH(Alice,M,r1) means "the string
resulting from a hash of M with Alice's chameleon hash and random value
r1". Alice would be able to find a collision for this value, but no one
else. A designated verifier signature from Bob to Alice then consists of
SIG(CHAM-HASH(Alice,M,r1)), where SIG() is any ordinary signature scheme.
Because Alice can create arbitrary collisions in CHAM-HASH, Alice cannot
credibly claim that this signature refers to a particular M.
(it occurs to me that this still shows that Bob sent Alice *a* message).
The idea is that we can leverage this to use existing protocols for
verifiable encryption of signatures. Alice tells Carol how to obtain the
same hash string S for M as the one Alice used for her DVS for Bob. Then
Alice just needs to show that a string X is a proper encryption of an
*ordinary* signature.
More formally, if Alice wishes to construct
X_C := E_B(DVS(M,Bob)), DVS(X_C := E_B(DVS(M,Bob)),Carol)
then she does the following
Note that under the assumption that Alice does not know Bob's private key,
she must send Carol the correct r1 for M in step 4. Otherwise Alice could
compute a collision on S.
Now for step 5, we need a protocol for verifiable encryption of
signatures, i.e. a way to show X_C := E_B(SIG(S)) without needing D_B().
Reuben Sumner pointed out Asokan, Shoup, and Waidner made use of these
protocols in their fair exchange protocol. Giuseppe Atienese has a few of
them as well.
I think that may do it. Now the members of the group can all verify that
they received the same message M from Alice by exchanging their X values,
yet none of them can claim to outside parties that Alice signed M. I'm
sure there's a better way; this feels a little bit inelegant/obvious...

@_date: 2001-05-25 04:23:00
@_author: dmolnar 
@_subject: Tamperproof devices and backdoors 
Skimming the papers on his web page, I would guess it's related to
M. Blaze, J. Feigenbaum and F.T. Leighton, "Master-Key Cryptosytems."
Abstract presented at Crypto '95 (rump session), Santa Barbara, CA, August
which opens by defining a "Master-Key Cryptosystem" and then goes on to
show that a MKCS implies a PKCS. The public key is the cryptosystem with a
back door. The private key is the back door/master key.

@_date: 2001-05-25 05:17:45
@_author: dmolnar 
@_subject: Tamperproof devices and backdoors 
This is true - but I think we could require the device design to be
engineered in a certain way to make proving compliance easier. After all,
we do prove programs (and floating point hardware) correct (sometimes),
even though the halting problem is in general unsolvable and Rice's
theorem obtains.
I think there may be several separate questions here. I might have ideas
for some of them and no idea about others. So let me try to restate the
problem in an overly verbose manner and see if I can make these
distinctions any clearer.
We have (at best)
There are at least four questions involved with back doors
For 1) Rice's theorem holds for arbitrary designs, but we can hold out
hope for open source or formal methods or code reviews or whatever. No
more and no less of a problem than the usual security issues. Perhaps in
this context we'd have to replace the design with "zero-knowledge proofs
that the design does not have undesirable properties", since the
manufacturer might not want to release the entire design - and only a
commitment to the design is known.
For 2), it seems there are at least two more questions
For 3), maybe some of the work on "subliminal free channels" would come in
handy. Another model I've been playing with is this: you have a box B that
implements a randomized algorithm A. The box B has an input tape for
random bits, which it is "supposed" to use for its random coin flips.
However, B might choose to flip its *own* coins and ignore your coins.
You tell B to compute algorithm A on input x. How do you tell whether B
used *your* coins or *its* coins or *both*? (do you even care? weaker
question - can you force B to use a particular distribution, though not
your exact coins).
If you evaluate A yourself with your coins you can catch B. That's
slow and so you can't do it every time (or even most times). Is there a
general way to do better? I ask because of a comment of Adam Young that
subliminal channels occur in protocols where random choices occur.
The subliminal channel occurs when the choice isn't actually "random" but
instead conditioned on an event. So my goal would be a way to force a
black box to use a particular string or distribution of coins (or be
caught immediately), given only control over the coins tape and the input
tape and assuming the box can flip its own coins.
For 4), that seems to be an issue of physical design. Don't build
tamperproof devices big enough to hold a nuclear bomb. More than that,
I don't know.
There are also some audit issues which come to mind
Oh and finally

@_date: 2001-05-25 11:02:30
@_author: dmolnar 
@_subject: Tamperproof devices and backdoors  
That is likely
V. Rijmen, B. Preneel, "A family of trapdoor ciphers," Fast Software
Encryption, LNCS 1267, E. Biham, Ed., Springer-Verlag, 1997, pp. 139-148

@_date: 2001-05-25 14:10:59
@_author: dmolnar 
@_subject: Tamperproof devices and backdoors 
You're absolutely right. This fact became apparent to me partway through
writing. I tried to work it in but wasn't nearly explicit enough. Thanks
for pointing it out.
That's why there seems to be a distinction between
For 1), the only way out I see is if the "something awful" requires some
other system to accept the device's output (e.g. login program returns
"true") - maybe you can force the device to prove its output matches the
spec. If the "something awful" is completely internal to the device
For 2), maybe you have more chance - it seems plausible that you could
limit the number of bad operations of the box before it must be caught.
An interesting recent paper, by the way, focuses on something related
"Funkspiel Schemes: An Alternative to Conventional Tamper Resistance"
Their idea is that a device should notice when it is being tampered with.
Before the adversary breaks in entirely, it should subtly alter
functioning. The adversary does not observe the state info of the device
as it alters. After this alteration, the adversary will not be able to
distinguish the device from an ordinary unaltered device, even given full
acess to the stateof the device. The legitimate owner of the device,
however, can tell from the output alone that something is wrong.
In this tamperproof testing situation, the "legitimate owner" is the back
door owner. The adversary is the box tester, except more powerful, since
we aren't allowing the box tester to open the box at *all*. The
"device notices it is being tampered with" is just triggering the back
Someone - Ian Goldberg, maybe? - once mentioned the "wireless remotely
programmable pacemaker." There's a movie scene waiting to be scripted for
that one...
-David Molnar

@_date: 2001-11-13 17:30:13
@_author: dmolnar 
@_subject: Thai Pirates Crack Microsoft's New Windows System 
On a separate note, what does WU do to prevent replays of bad bugfixes?
This article
mentions a patch which closed down Win2K Terminal Services. What prevents
someone from causing WU to accept this patch (perhaps in conjunction
with a compromise of DNS) in order to mount a denial of service attack?
I poked around the Microsoft site a bit, but did not find many details;
then again I haven't looked particularly hard yet.
While we're at it, does anyone know whether a good treatment of "issues in
secure upgrades" exists?

@_date: 2001-11-27 12:45:58
@_author: dmolnar 
@_subject: standardization of client puzzles for TLS? 
Drew Dean and Adam Stubblefield had a paper at this year's USENIX Security
on "Using Client Puzzles to Protect TLS." Does anyone know if this idea is
being considered for standardization yet?
-David Molnar

@_date: 2002-08-16 04:21:22
@_author: dmolnar 
@_subject: employment market for applied cryptographers? 
Don't forget schedule pressure, the overhead of bringing in a contractor
to do crypto protocol design, and the not-invented-here syndrome. I think
all of these contribute to keeping protocol design in-house, regardless of
the technical skill of the parties involved. It takes a serious investment
in time to qualify a consultant. If having the protocol right isn't a top
priority, that investment won't be made...and I'd guess that designing a
new protocol isn't common enough to merit a separate job/new hire in most

@_date: 2002-08-18 01:46:09
@_author: dmolnar 
@_subject: employment market for applied cryptographers? 
I agree with this as far as "crypto" protocols go. But one thing to keep
in mind is that almost all protocols impact security, whether their
dsigners realize it or not. Especially protocols for file transfer, print
spooling, or reservation of resources. most of these are designed without
people identifying them as "crypto protocols."
Another thing that makes it worse -- composition of protocols. You can do
an authentication protocol and prove you're "you." Then what? Does that
confer security properties upon following protocols, and if so what?

@_date: 2002-08-22 13:25:47
@_author: David Molnar 
@_subject: Mixes and CCA 
In fairness, it wasn't just me! Roger Dingledine, Mike Freedman, and David
Hopwood all did amazing work on that paper. You should also look at
Dingledine & Syverson's paper on reputation in MIX cascades from the last
Right now, there is work underway on "Mixminion," a type III remailer
protocol and implementation. Roger Dingledine has a slide show on it
available at
(this was shown at DEF CON). For more information, you can check out
 for draft paper, current code, and spec. Then hop
over to the mixminion-dev archives for in-depth information.
-David Molnar

@_date: 2002-01-29 12:11:38
@_author: dmolnar 
@_subject: Limitations of limitations on RE/tampering (was: Re: biometrics) 
Could you be a bit more specific? There are at least two distinct
scenarios which come to mind here. In both cases, it's not clear that
the non-obfuscation result is a problem.
1) Secure multiparty computation. "How to play any mental game" and
   following. In this scenario, Alice has a private input a and Bob
   has a private input b. Both have agreed on a public function f.
   The goal is for both to learn f(a,b) but neither learns anything
   "more" about the others' input. Generalize to multiple parties.
   The question is whether some variant of multiparty computation
   implies obfuscation (and is therefore impossible). At first that
   seems unlikely, since in this scenario we don't care about hiding the
   function f.
   But maybe there's a way around that. For instance, set f to be a
   universal TM and then smuggle in the function to "obfuscate" as the
   private input a. Let the other private input b be the input to
   the function a. Then the output f(a,b) is the result of running program
   a on input b, but by secure multiparty computation, Bob learns
   nothing about the program a.
   The other main difference is that all the protocols I can think of
   in this scenario are interactive. Obfuscation is not interactive;
   once you've released the supposedly obfuscated program, anyone can
   come along and query it to his/her heart's conent.
   Now if you could obfuscate "self-modifying code" then maybe you could
   get closer to this by having obfuscated functions which refuse to
   answer after a small number of queries..? I'd have to check the
   paper carefully to see how many queries are sufficient to reveal
   one of the unobfuscatable functions - maybe as little as one.
2) CryptoComputing. "Noninteractive CryptoComputing for NC^1,"
   Sander, Young, and Yung FOCS '99.
   Alice has a function f and Bob has a private input b.
   The goal is for Alice to learn f(b), but not b. At the same
   time, Bob must learn nothing about f or f(b).
   Here we *do* care about hiding the function f, so this is
   closer in spirit to obfuscation. The difference is, however,
   that Bob does not get to see f(b). So he can query the function
   all he wants, but the answers will be of no help.
   In the paper, Bob ends up with the value f(b) encrypted with Alice's
   public key, which he then sends to Alice. If you know Alice's private
   key, then you can decrypt the circuit description as well. So it seems
   difficult/impossible to create an obfuscator from their CryptoComputing
   construction, because either you don't know the output or you know
   the circuit description. So I'm not sure what bearing the
   impossibility result has here.

@_date: 2002-03-21 00:33:54
@_author: dmolnar 
@_subject: crypto question 
There are several different possible scenarios which fit this description.
My message will overlap a little with the other reply I've seen, for which
I apologize. Here they are in rough order of what I think you're asking.
1) You are trying to distribute an obfuscated binary which
encrypts/decrypts using a secret key, with the goal that the key resist
reverse engineering. The usual application for this is DRM, but you can
also use this to do "public-key encryption" from any symmetric algorithm
(obfuscate the encryption function!).
(disclaimer: I work for ShieldIP, which is a DRM company. All statements
and opinions here are my own.)
There's a recent result showing that there exist some functions which
*cannot* be obfuscated, for several technical formalizations of the notion
"obfuscated." That result is available as:
On the (Im)possibility of Obfuscating Programs
Boaz Barak Oded Goldreich, Russell Impagliazzo, Steven Rudich, Amit Sahai,
Salil Vadhan, Ke Yang
It is important to note that this result doesn't necessarily apply to the
kinds of programs we want to obfuscate in practice. Rather it shows that
there is a large class of "unobfuscatable functions" and builds such
functions through clever means. At least that's my current take; I should
hedge here and say I haven't gone through it thoroughly -- I'd welcome
correction from anyone who's taken more time to map out the practical
implications (for instance, "is it possible that a block cipher could be
Naturally this result hasn't stopped people from trying practical
techniques for code obfuscation. Cloakware ( is just one
of the companies pursuing research into software obfuscation. Doing a
google search for "code obfuscation" provides many links. I don't know
enough to say which of them are any good.
People have also tried to obtain a similar level of protection by
embedding code in tamper-resistant hardware. IBM's ABYSS project was an
early example of this aimed specifically at copy protection. That begat
Citadel which begat 4758 and thus was the begatting begun. As another
message mentions, Atallah/Compaq/HP and Wave Systems today do similar
things. I note that the Intertrust web page mentions a Rights|Chip which
may or may not do similar things. Bennet Yee's thesis, among other places,
is a good place to learn about secure coprocessors.
2) You have an application which uses private keys and you are worried
about writing them to disk. Your adversary is not the user, but someone
who may gain "lunch-time" access to the machine and not plant keyloggers,
bugs, etc, but only transfers files or swap to a diskette. This is kind of
a weak adversary, but it's also about what most co-workers or kid sisters
can mount, and hey we have to protect at least against them...
The best practice here, AFAIK, is to do what PGP does. Encrypt the private
key while it's on disk using some key not on the machine. Then use a
kernel driver to obtain memory which is guaranteed not to be paged to disk
and use that memory for all sensitive operations. Get yourself a copy of
the WinPGP source code and take a look.
3) You are worried about an adversary breaking in and stealing your own
signing or decryption key from your computer. You also just happen to have
a bunch of other computers lying around that are not running the same OS
or same version (so they are unlikely to be cracked at the same time as
your first machine).
Now you're in the territory of "threshold cryptography" and "proactive
security." The MIT Threshold Cryptography page explains it better than I
Dan Boneh's group has put some of these ideas into code:
With "proactive security," you "refresh" machines from time to time so as
to limit damage from machines which are compromised and then renewed.
Here's the abstract from the paper reporting on the IBM implementation.
that paper citation is
B. Barak, A. Herzberg, D. Naor, and E. Shai. The proactive security
toolkit and applications. In Proceedings of the 6th ACM Conference on
Computer and Communications Security (CCS'99), pages 18--27, Kent Ridge
Digital Labs, Singapore, November 1999. ACM SIGSAC, ACM
There used to be an IBM page specifically on the topic of "proactive
security" and they were even going to let people download the toolkit! I
don't think that actually happened. If it did, dude, I'd like to know.
-David Molnar

@_date: 2003-08-11 16:45:21
@_author: dmolnar 
@_subject: Crypto Hygiene? 
(also posted to sci.crypt in modified form)
At Usenix Security, Eric Rescorla pointed out that some of the
cryptographic flaws we have seen can be prevented by applying good
"crypto hygiene." My questions for the floor --
-David Molnar

@_date: 2003-12-14 16:25:23
@_author: David M 
@_subject: Postgraduate programs 
Thanks for mentioning my list! I've recently moved to UC-Berkeley, so the
most current version will be here:
I should caution, however, that I make no attempt to rank or order
programs (except to put Berkeley and Harvard first since I went there).
Also, I only update the list when people send me e-mail corrections, so it
is possible that it's out of date. Any corrections or updates are welcome.
-David Molnar
