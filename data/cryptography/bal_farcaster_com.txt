
@_date: 2002-08-12 00:27:22
@_author: Brian A. LaMacchia 
@_subject: Challenge to David Wagner on TCPA 
I just want to point out that, as far as Palladium is concerned, we really
don't care how the keys got onto the machine. Certain *applications* written
on top of Palladium will probably care, but all the hardware & the security
kernel really care about is making sure that secrets are only divulged to
the code that had them encrypted in the first place.  It's all a big trust
management problem (or a series of trust management problems) --
applications that are going to rely on SCP keys to protect secrets for them
are going to want some assurances about where the keys live and whether
there's a copy outside the SCP.  I can certainly envision potential
applications that would want guarantees that the key was generated on the
SCP & never left, and I can see other applications that want guarantees that
the key has a copy sitting on another SCP on the other side of the building.
So the complexity isn't in how the keys get initialized on the SCP (hey, it
could be some crazy little hobbit named Mel who runs around to every machine
and puts them in with a magic wand).  The complexity is in the keying
infrastructure and the set of signed statements (certificates, for lack of a
better word) that convey information about how the keys were generated &
stored.  Those statements need to be able to represent to other applications
what protocols were followed and precautions taken to protect the private
key.  Assuming that there's something like a cert chain here, the root of
this chain chould be an OEM, an IHV, a user, a federal agency, your company,
etc. Whatever that root is, the application that's going to divulge secrets
to the SCP needs to be convinced that the key can be trusted (in the
security sense) not to divulge data encrypted to it to third parties.
Palladium needs to look at the hardware certificates and reliably tell
(under user control) what they are. Anyone can decide if they trust the
system based on the information given; Palladium simply guarantees that it
won't tell anyone your secrets without your explicit request..
                    --bal
P.S. I'm not sure that I actually *want* the ability to extract the private
key from an SCP after it's been loaded, because presumably if I could ask
for the private key then a third party doing a black-bag job on my PC could
also ask for it.  I think what I want is the ability to zeroize the SCP,
remove all state stored within it, and cause new keys to be generated
on-chip.  So long as I can zero the chip whenever I want (or zero part of
it, or whatever) I can eliminate the threat posed by the manufacturer who
initialized the SCP in the first place.

@_date: 2002-08-12 00:38:42
@_author: Brian A. LaMacchia 
@_subject: dangers of TCPA/palladium 
We are talking internally about doing exact what is suggested here. We have
thought about working with someone to offer a free compilation service in
conjunction with the compiler, so you could compare your results on your
compiler with someone elses results on theirs - they could provide you with
a benchmark, so to speak. This directly bears on how people assert the
trustworthiness of their applications, so whether or not we do it, for
certain applications some kind of system is bound to spring up.
There are two parts to answering the first question:
1) People (many people, the more the merrier) need to understand the code
and what it does, and thus be in a position to be able to make an informed
decision about whether or not they trust it.
2) People reviewing the code, finding security flaws, and then reporting
them so that we can fix them
These are two very different things.  I don't think that anyone should count
on the goodwill of the general populace to make their code proveably secure.
I think that paying people who are experts at securing code to find exploits
in it must be part of the development process.
We're also making some changes to our internal engineering processed that we
hope will help us identify security bugs earlier in the development cycle.
For example, we are generating our header files from specs (yes, actualy
specs!). Any change to an interface must first be made to the spec, the spec
must be reviewed, and only then will the change be propogated into the code,
at which point the code will be code reviewed. This doesn't fix coding bugs;
it does however aim
squarely at fixing design and architectural flaws.  We will make the specs &
software widely available.  (That doesn't make the code more secure, of
course, but it does provide a basis for analysis & comparison.)
                    --bal

@_date: 2002-08-13 23:55:24
@_author: Brian A. LaMacchia 
@_subject: dangers of TCPA/palladium 
Yeah, I wasn't very clear here, was I?  What I was trying to say was that
there's a difference between understanding how a system behaves technically
(and deciding whether that behavior is correct from a technical perspective)
and understanding how a system behaves from a policy perspective (e.g.
social process & impact).  Those are two completely different questions.  2)
is all about verifying that Palladium hardware and software components
technically operates as it is spec'd to.  1) is about the larger issue of
how Palladium systems interact with service providers (CAs, TTPs), what
processes one goes through to secure PII, etc.  The two groups of people
looking at 1) and 2) have non-zero intersection but are not equal.  And,
just to be clear, 2) is *not* done only by internal folks, but I expect that
the size of the set of people competent to do 2) is significantly smaller
than the size of the set of people who need to think about 1). :-)
                    --bal
