
@_date: 2001-09-19 17:17:18
@_author: Theodore Tso 
@_subject: chip-level randomness? 
It's definitely the case that feeding extra bits of randomness into
the /dev/random entropy pool is a good idea.  Whether or not to give
any entropy credit for this is another question.
One of the things which I've always been worried about with the 810
hardware random number generators in general is how to protect against
their failing silently.  My original design intention here was that
this be done in a user-mode process that could run FFT's, and do other
kinds of analysis on the output of the hardware random number
generator, and then if it passed, it could use an already-existing
interface to atomically add the random bytes to the entropy pool and
give credit to the entropy counter.
It turns out that with the Intel 810 RNG, it's even worse because
there's no way to bypass the hardware "whitening" which the 810 chip
uses.  Hence, if the 810 random number generator fails, and starts
sending something that's close to a pure 60 HZ sine wave to the
whitening circuitry, it may be very difficult to detect that this has
In addition, depending on how paranoid you are, your threat model
might encompass the secenario where an NSA "black bag" job which
punches the 810 rng into a mode where its output is the sequence 1, 2,
3, 4, ... encrypted by some secret SkipJack key.  And there isn't much
that could be done to protect or even detect that something like this
is going on.  (Yes, this scenario requires the cooperation of Intel
with the NSA when doing the chip design --- I *said* it was a highly
paranoid scenario....)
On the other hand, for most people, on balance it's probably better
for the kernel to just blindly trust the 810 random number generator
to be free from faults (either deliberate or accidentally induced),
since the alternative (an incompletely seeded RNG) is probably worst
for most folks.  (Unless, of course, your threat model has a very
heavy bias towards national security and law enforcement agencies.  :-)
So probably what makes sense is to make this be configurable.... and
probably at run-time.  So what probably what makes sense is two /proc
control parameters.  One controls whether or not excess entropy is fed
from the 810 RNG to the /dev/random pool and the other
controls (from 0 to 100 percent) how many bits of "entropy" is
credited for each bit read out from the 810 RNG.  I expect for the common case, the /dev/random pool will just blindly
trust the 810 RNG, and so entropy will be siphoned over at 75 to 100%.
Hopefully, even if the 810 RNG is completely compromised, there will
be enough other sources of randomness being drawn from the general
system operation that it will make the job of the attacker somewhat
more difficult.
However, I *do* want to preserve the original design goal of allowing
the transfer of entropy from hardware random number generators to
arbitrarily paranoid about trying to do quality checks on the output
of the hardware random number generator before feeding it to
compromised RNG (since an encrypted stream will be indisguishable from
noise unless you no the crypto key), but it does protect against
random hardware failures.
Does this seem reasonable?
If the PRNG in /dev/random can be broken by analytical techniques,
this means that someone was able to find potential inputs (or at least
significant information about potential inputs) to a SHA-1 hash given
the SHA-1 hash.  I'm not going to say that this is impossible, but if
it is, I suspect the analytical breakthrough(s) necessary to make such
a feat impossible is going to make life interesting for a rather large
number of people.....  :-)
But yes, your general point stands; if we only pull less entropy from
compromised, the resulting random stream should be secure; that's why
pool, and limits how much randomness it will emit based on that
entropy estimation.  However, given the use of /dev/urandom, being able to feed more
possible randomness into the entropy pool, even if we don't bump the
entropy estimator, can only be a good thing.

@_date: 2013-12-04 11:02:42
@_author: Theodore Ts'o 
@_subject: [Cryptography] Kindle as crypto hardware 
It is possible to add a display shield for an Arduino:
Arduino Uno R3:   $28.90
LCD Keypad Sheid: $13.90
Total cost:       $42.80
Sure, althouh the above pricing is for a single unit (and there are
cheaper Arduino's out there, but this one is one of the more
convenient ones to use; if you were creating a large numbers you could
use much cheaper components, but might require more work to assemble
the kit).  Even if you aren't doing a mass production run, you can get
cheaper pricing if you're buying 10+ units.
(BTW, my quick pricing of a Rasberry Pi with a display is not cheaper
than an Arduino, but your milage may vary.)
One other nice thing about using your own kit version is that it's
simpler to do certified distruction of only the components that might
contain keying information, and be able to reuse the rest.  It's also
probably easier to create a tamper-proof enclusure with an Arduino
style device compared to using a Kindle.

@_date: 2013-12-05 10:10:33
@_author: Theodore Ts'o 
@_subject: [Cryptography] Kindle as crypto hardware 
Citation and more detail about this accusation, please?

@_date: 2013-12-05 16:23:39
@_author: Theodore Ts'o 
@_subject: [Cryptography] Kindle as crypto hardware 
A few years ago, people who suggested that NIST might issue a standard
sabotaged by the NSA would be a joke and/or the paranoid ravings of
the tin foil hat crowd...
More seriously, if your threat environment includes a tempest truck
parked outside of your house, I doubt tamper-evident evidence bags
would be a sufficient defense against a state-funded black bag job team....
      	   	      	      	      	- Ted

@_date: 2013-12-13 18:07:27
@_author: Theodore Ts'o 
@_subject: [Cryptography] Size of the PGP userbase? 
You could always try filing a FOIA request with the NSA.  :-P
    	  	     	      	   - Ted

@_date: 2013-12-17 20:47:21
@_author: Theodore Ts'o 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
The problem is that I've actually taken a graduate class in CPU
architecture and design at MIT, so I understand the difference between
a gimmicked RDRAND functional unit, and something which requires
making changes to the instruction decode logic, the register renaming
logic, and all of the other bits of complexity that make a modern CPU
go really, really fast by doing out-of-order instruction execution.
Yes --- it could be done.  But it would require having at least an
order of mangitude or two more people inside Intel that would have to
know about it.  It's not something you could just hide away inside the
RDRAND unit, which among other things, is documented as having AES as
its final "whitening" step.  So if you can control what key it uses
and what input it's fed, it's much easier to gimmick RDRAND in a way
where it would be very hard for someone to notice without doing
extensive electron microscope and circuit analysis.
However, if you are going to posit that the CPU is so badly subverted
that the instructure decode and execution units can analyze the
machine instructions to figure which register is being used as the
pointer to the entropy pool or being used for the output, then you
could just as easily imagine the CPU going into System Management Mode
to scan for AES keys and then ship them off the system either using
the network card, either by modulating the timing of packets or by
sneaking bits into unexamined portions of the network packets.
Hacking SMM would be much easier, and allow a much larger and wider
range of attacks.  Or you could hack the keyboard controller to
capture packets: Ultimately, if you need to live at that level of paranoia, you'll need
to build your own CPU out of TTL logic chips --- something which I
learned how to do when I was a freshman at MIT.  It won't be a
terribly fast computer, though....

@_date: 2013-12-19 09:49:24
@_author: Theodore Ts'o 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
You have to do something very close to this already given how modern
CPU's handle register renaming, out of order execution, speculative
execution, etc.  You can do anything at all, including scanning the
instruction sequences for crypto algorithms and then finding secret
ways to exfiltrate the data via various covert channels (and since
modern computers tend to use chipsets where the wired and wireless
ethernet also come from Intel, if you assume that level of complete
control, you really, REALLY, should be building your own computer out
of TTL chips).
What I've done instead will hopefully finally put this issue to bed,
except for those people who belive that since the NSA authored SHA-1,
they clearly would have anticipated the use of SHA-1 with RDRAND two
decades ago, when SHA-1 has been authored, and put a backdoor into
commit a9f069e38cc36d6c4ab3c831bc4bef2ae1a16e96
Author: Theodore Ts'o     random: use the architectural HWRNG for the SHA's IV in extract_buf()
    To help assuage the fears of those who think the NSA can introduce a
    massive hack into the instruction decode and out of order execution
    engine in the CPU without hundreds of Intel engineers knowing about
    it (only one of which woud need to have the conscience and courage of
    Edward Snowden to spill the beans to the public), use the HWRNG to
    initialize the SHA starting value, instead of xor'ing it in
    afterwards.
    Signed-off-by: "Theodore Ts'o" diff --git a/drivers/char/random.c b/drivers/char/random.c
index 8cc7d65..d07575c 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
 -1012,23 +1012,23  static void extract_buf(struct entropy_store *r, __u8 *out)
 	__u8 extract[64];
 	unsigned long flags;
-	/* Generate a hash across the pool, 16 words (512 bits) at a time */
-	sha_init(hash.w);
-	spin_lock_irqsave(&r->lock, flags);
-	for (i = 0; i < r->poolinfo->poolwords; i += 16)
-		sha_transform(hash.w, (__u8 *)(r->pool + i), workspace);
 	/*
 	 * If we have an architectural hardware random number
-	 * generator, mix that in, too.
+	 * generator, use it for SHA's initial vector
 	 */
+	sha_init(hash.w);
 	for (i = 0; i < LONGS(20); i++) {
 		unsigned long v;
 		if (!arch_get_random_long(&v))
 			break;
-		hash.l[i] ^= v;
+		hash.l[i] = v;
 	}
+	/* Generate a hash across the pool, 16 words (512 bits) at a time */
+	spin_lock_irqsave(&r->lock, flags);
+	for (i = 0; i < r->poolinfo->poolwords; i += 16)
+		sha_transform(hash.w, (__u8 *)(r->pool + i), workspace);
 	/*
 	 * We mix the hash back into the pool to prevent backtracking
 	 * attacks (where the attacker knows the state of the pool

@_date: 2013-12-23 21:29:39
@_author: Theodore Ts'o 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
Actually, I believe this.  Never attribute to malice what can be what
can adequately explained by incompetence.
That might not change my opinion, though, if someone asked me for
advice about whether to buy products from RSA --- would *you* want to
buy products from a company that (a) allowed to have their SecureID
tokens get compromised[1], and (b) allowed themselves to be suckered
by the NSA?
[1] As for the rest, the lesson we should take from this is, moving
forward, if any company in the future hears the words, "I'm from the
NSA and I'm here to help", they should run away, as fast their legs
can carry them.
       	   	    	       	    - Ted

@_date: 2013-12-26 14:33:50
@_author: Theodore Ts'o 
@_subject: [Cryptography] Serious paranoia... 
I don't think discussions of "serious paranoia" are particularly
useful, and can be counter-productive, if they don't also take into
account the question of usability.  Sure, you can talk about how we
need to use a key stretching function that takes minutes to transfer
the a 160 character (which will be completely random alphanumerics
with special characters), but if it is too difficult to use, it won't
get used.
It's also important to consider the threat environment, and what the
adversary might or might not be willing to do in order to get at your
precious bodily fluids^H^H^H^H^H^H^H data.  Are they going to be
willing to carry out a black bag job, where they may physically invade
your home and install hidden cameras and keyboard bugs?  Are they
willing to park a van outside of your house and try grab tempest
emissions from your computer or laptop?
Personally, I'm much more inclined to keep very tight control on those
systems where my ssh private key might reside, and also to keep very
tight control of my laptop which contains the ssh private key.  After
all, you could use a fancy, non-standard key stretching KDF to encrypt
your ssh private key, but what about the rest of data on your laptop?
And what if they just install a trojan'ed ssh client that simply
captures the private key once it has been decrypted.
So the bottom line is that it's imporant to take a holistic view of
security, and not focus in on the threats which key-stretching is
designed to protect to the exclusion of all else.  So what if the key
is splattered all over RAM?  If the attacker is able to grab arbitrary
contents from your system memory, your OS has been subverted so badly
that there are million other, more simpler ways that you can get
I've been kill-threading most of these non-technical discussions,
including the lame one about moving to a web forum, but the signal to
noise ratio of this list has dropped through the floor lately, and if
it doesn't improve, it may be that many experts may decide to
unsubscribe.  Indeed, the strongest argument against moving to a web
forum is that it is likely to make the signal to noise drop even lower.
Others can judge whether or not I'm an expert or not, but I will say
that the noise has gotten bad enough that I did briefly consider
declaring this list devoid of intelligent life, and to say, "beam me
up Scotty", and unsubscribe.
                                                - Ted

@_date: 2013-12-26 19:02:51
@_author: Theodore Ts'o 
@_subject: [Cryptography] Serious paranoia... 
I think the creation of such an algorithm would make for a wonderful
academic publication, and I would think it meets the "minimal
publishable unit" that academic publishing venues and tenure boards
care about.  So certainly, having an algorithm which has certain
superior characteristics, such as being hard to brute force back to
text password even in the face of hardware acceleration, protection
from cache attacks and swap attacks, etc., would certainly be better
than a KDF that doesn't have these properties.
There is a separate question, though, which is whether this threat is
significant enough that it is changing the current deployed base to
use it.  But I don't think this is a discussion that can be made
generally, which is how the discussion has been running on the list as
of late --- for example, the subject line "Why don't we protect
passwords properly?"
I suspct the discussion would be more productive if it were more
tightly focused --- for example, changing the string-to-key function
used by ssh to protect its private key file, versus changing the
string-to-key function for PPP CHAP authentication, etc.  The cost and
the benefits for making this change are quite different.
But first, if you think it's easy, sure, propose a new KDF that you
think is superior.  Get it peer reviewed, and you'll probably even get
a paper out of it.  The next question is whether people will consider
it worthwhile to transition to it, given all of the other threats they
need to worry about, and how much time and energy have to devote to
this threat (as compared to other threats or other features that they
would like to implement).
P.S.  I was re-reading one of Jerry Leichter's recent messages:
And I think this really strikes at the heart of the problem.  His
concluding paragraph:
    So I expect to see many more discussions about security wandering, as
    we're no longer certain about what security means.  Yes, worthwhile
    security debates start with a definition of the attacks to be defended
    against; or, even better, of the risks and costs associated with
    different attacks and defenses.  But given the huge spectrum of
    entirely different classes of risks, and the very different
    likelihoods and costs different people will assign to them ... to
    accept agreement on what are, at base, the *goals* is increasingly
    folly.

@_date: 2013-12-27 13:36:26
@_author: Theodore Ts'o 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Yes, but *so* *what*?  How could an attacker to achieve some goal that
he or she might want to achieve?
It's not enough to say things like "an attacker could do XXX".  If
we're going to do a credible analysis, this is critical.  Why is this
important, and how much are we willing to pay (in terms of
inconvenience, extra hardware, etc.) to avoid this potential "attack"?
I don't know about other people, but I don't consider this list
critical infrastructure.  If I were to not get some number of the
messages, it wouldn't necessarily impact my life or my work in any
significant way.

@_date: 2013-12-27 14:12:18
@_author: Theodore Ts'o 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
I think we should do both steps at the same time.  But if you want to
separate them out, that's fine --- but then we shouldn't start
proposing using 4GB worth of memory whenever we need to execute a
string-to-key algorithm, or pursuing other solutions, until *after*
we've done this risk analysis.
Personally, part of talking about listing the threat also includes
doing the risk analysis, because otherwise the list can easily become
unbounded, and because there are people who are overly inclined to
paranoia will start pursuing solutions and demanding that we make
changes to mailing lists, protocols, open source software, etc.,
prematurely.  (Having gotten all sorts of demands from really clueless
people about changes that they think I should make to the Linux
I'm reminded, though, of the theory that one of the reasons why former
Vice President Cheney got so enthusiastic about waterboarding and
torture, and other forms of overkill in the "war on terror" (including
warrantless wiretapping) was because he got unfiltered access to the
list of all "potential threats", before it "is this really a credible
threat" filter had been applied, and this caused his paranoia to race
out of control.
Which is why I'm not all that enthusiastic about people making lists
of random threats, and then seeing people proposing algorithms and
changes, with apparently *no* serious risk analysis taking place.

@_date: 2013-12-29 13:19:29
@_author: Theodore Ts'o 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
It is possible to use Linux containers (via the memory cgroup) to
control how much memory is used by a particular task.  Some of the
newer Linux distributions will be shipping user-friendly access to
this technology (which has been in use for a while to control memory
usage in Google's production servers and at most web hosting services
which use containers to efficiently run many virtual machines on a
single host).
That being said, Chrome is using that memory to cache graphics and to
prefetch web pages and other web assets.  And you will really notice
if you try to constrain Chrome to use less than 2GB of memory,
especially if you like to keep a large number of tabs open.
Personally, I don't see that as a security problem, at least not on a
single user workstation, but rather as a system
adminsitration/management.  It certainly can be seen as a security
problem, but the problem is that this would require a fundamental
change in how we specify ACL's.  It's not enough to name a userid for
the ACL, but the combination of a userid and some application (i.e.,
chrome), for example.  And the instead of the traditional read, write,
execute, etc. permissions, we would now need to add resource
constraints for memory, disk, CPU, network quotas, etc.
Worse yet, ACL's are a usability nightmare.  Users have consistently
been shown to be unable to properly manage a simple set of Unix
permissions, never mind ACL's --- and adding resource management as
being within the scope of ACL's would increase their complexity by at
least tenfold.
So I'm not convinced that ACL's are the right tool of choice for
managing file access, let alone the resource management problem.  It's
great to have low-level controls for people who really know what they
are doing, but we need something simpler for civilians to use.
    	       	       		 	 - Ted

@_date: 2013-12-30 11:06:11
@_author: Theodore Ts'o 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Even in the domain of the attacker, the attacker is attacking a
*specific* protocol, being used in a *specific user case*, and to
achieve a *specific* goal.  If we are going to accurately model a
threat, we need to understand these specifics.  Unfortunately, I don't
see that happening in many of these discussions on the mailing list.
For example, the people worrying about cache timing attacks for a KDF.
OK, but in what protocol or application is the KDF being used?  What
most of the cache timing attacks require an oracle where the attacker
can feed many different versions of the plaintext and ciphertext, and
then can measure the difference in timings for different
paintexts/ciphertext being encrypted/encrypted.  Given that many KDF's
are used to transform a password typed by the user into a key, what's
the scenario where a cache timing attack on KDF be applicable?
And yet, there is a whole thread, and people proposing changes to
algorithsm, to address this threat, without any kind of filtering
about whether it is a credible threat.
Great.  But that is not what is happening for most of the discussions
on this mailing list.  There is no attempt to make sure that the list
is complete, and there is no attempt to filter the threats for
credibiity.  Instead, people immediately start leaping to proposed
solutions and wringing their hands that all of our crypto applications
are broken....
I'm not sure that this is ever going to be productive.  Sure, it's
possible, but the solution is open proposals, open source, and open
peer review.  Otherwise, I could start complaining that people who are
ranting and raving about threats without doing any kind of filtering
are NSA plants who are trying to waste/disapate our energy, and you
could claim that I'm a NSA shill for refusing to pay attention to your
favorite pet threat.  And in the end, it doesn't further the
discussion, but in act, degrades it.
Who is "the business" and why do they get to decide who to appoint?
How does this apply to all of our open source technologies, such as
OpenSSH, OpenSSL, the Linux /dev/random driver, etc?  In the case of
the RSA business, they chose Bart Harman as their CTO, who is
presumably "the decider".  Given his recent statements, does that make
you feel any more comfortable?
Committees do have their downsides, but at least we're not depending
on a single person.  Annointing a single person means that there is a
single point of failure where that person might make be bribed or
otherwise corrupted, or just make a single mistake.
And that person may make take on all of the successes and failures,
but to the extent that we depend on that technology, and we don't have
the time to audit all possible security technologies, or to write all
of the securiy technologies (and all of their dependencies, including
the Intel CPU for those people who believe that the NSA could subvert
the instruction pipelining and execution engine), we don't have a
choice but to delegate our security to some set of people, or a single
person.  Personally, I'd much rather delegate my security to an open
committee using an open process.

@_date: 2013-12-30 18:14:02
@_author: Theodore Ts'o 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
I suspect a number of us have the use case (I certainly do) where I
invest my laptop with far more trust than my SMTP/IMAP server.  So in
that sense, my Linux laptop is "mobile", although it's not a mobile
handset in the way most people use the word "mobile.
My laptop is certainly not on the internet all of the time, and when
it is on the network, it may be behind some NAT box, hotel network,
etc.  So for me, using a PGP mail paradigm for my secure e-mail makes
a lot of sense.  Granted, this may not be a model that works for most
consumers, especially those who are using things like Chromebooks,
Windows 8 tablets, or other "thin clients".  (And apparently the NSA
is planning on moving strongly to thin clients to make it a lot harder
for a future insider leaker.  :-P)
That's certainly an interesting idea.  Obviously we'd want to have
some kind of sequence number embedded in the cleartext to make it
easier to handle the case where the messages get received out of
order, but that's an implementation detail.
If we assume that the NSA can't vaccuum up all of the e-mail messages,
then this could very well frustrate them if they lose an intervening
message.  Of course, this isn't something that the participants can
count upon.  This might not provide additional security in the case of
a user which is keeping local archives of sent messages (and the NSA
manages to seize or otherwise compromise one of the communicating
party's MUA), or if the e-mail quoting contains portions of previous
mail messages.
But if this could be made sufficiently easy to use, the benefits could
easily outweigh the costs of such a scheme.

@_date: 2013-11-02 21:43:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
You didn't say which Linux distribution you measured this on, but you
later on you mentioned, "upstart".  So I suspect you ran this on an
Ubuntu system.  On a Debian system, which still uses sysvinit (as God
intended :-), the ordering is quite different.  The urandom script is
run before networking is enabled, and in fact this is enforced by the
init script's dependencies:
 BEGIN INIT INFO
# Provides:          networking ifupdown
# Required-Start:    mountkernfs $local_fs urandom
# Required-Stop:     $local_fs
# Default-Start:     S
# Default-Stop:      0 6
# Short-Description: Raise network interfaces.
# Description:       Prepare /run/network directory, ifstate file and raise network interfaces, or take them down.
 END INIT INFO
So your observation the urandom init script is being late is
apparently configuration bug for Ubuntu.  Debian sysvinit's has a
dependency which requires that urandom be run before we bring up the
networking stack.  Clearly that dependency is missing for Ubuntu's
The other thing to note about the "number of bits being extracted from
the ASLR for the processes being executed out of the various init
scripts.  For example, the fact that the bits used to initialize the
ASLR for processes such as "mount" and "fsck" were before the RNG was
fully initialized is not as important compared to the ASLR for
processes such as inetd and sshd, which are (a) long-running, and (b)
listening to network connections, where they might be more likely to
encouter external inputs that might trigger a potential buffer-overrun
So the important here is to make sure that "/etc/init.d/urandom start"
is run before networking and the networking daemons are started up.
This is definitely true for Debian, and I suspect it's true for Red
Hat Enterprise Linux, since it also uses sysvinit.  I don't know about
Fedora systems using systemd; they might have a similar problem, but
this is fundamentally an boot services configuration problem.
     		      	      	       		     - Ted

@_date: 2013-11-04 13:26:53
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
If we have the random secure seed built into each device, it's
certainly better than nothing.  But if we started building systems
that depended only on the secure seed, then how long would it take
before the NSA started leaning on manufacturers to make that "secure
random seed" be AES_ENCRYPT(NSA_KEY, DEVICE_SERIAL_NUMBER)?
I'd much rather try leaning on the ARM cpu vendors include a CPU cycle
counter, since it's much easier to audit that the CPU cycle counter is
doing what you think it is doing, and then you can use that to create
a better entropy-gathering RNG in the OS.  (Having them add a hardware
RNG is also good, but that might require more silicon and validation
than simply adding a cycle counter register.)

@_date: 2013-11-04 19:58:03
@_author: Theodore Ts'o 
@_subject: [Cryptography] HTTP should be deprecated. 
I'm in the anti-"pay CA's for their crappy job" school.  So my web
site uses a CACert certificate, which most browsers don't accept,
which is why I default to http.  If people want to access my web site
via https, they certainly can --- and that I's how I access it when I
need to send my password to the administrative interface for my site.
I just don't force via a redirect that users use https for thunk.org.
If deprecating http means that I have to pay $$$ to Verisign or
GoDaddy, I'm personally not excited about funding elephant hunters or
a company that is probably deep in the pockets of the US Government.
     		      	     	     	    - Ted

@_date: 2013-11-04 20:16:29
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
One of the reasons why we don't attempt to extract "true random bits"
and save them across a reboot is that even we had such bits that were
secure even if the underlying crypto primitives were compromised to a
fare-thee-well, once you write them to the file on the hard drive and
the OS gets shut down, there's no guarantee that an adversary might
not be able to read the bits while the OS is shut down.  Even if you
don't do something truly stupid (such as leaving your laptop
unattended in a hotel room while visiting China), the risk of having
your "true random bits" stolen is probably higher than the
cryptographic primitives getting compromised.
That's probably one of the reasons why people tend to not necessarily
worry about the difference between a CSRNG and a TRNG in practice.
For example, these are the people who believe that we should just
replace Linux's /dev/random with a Fortuna RNG which doesn't even
pretend to try to track entropy estimates, and which fundamentally
assumes that the underlying crypto algorithms are secure, or at least,
not the weakest link to worry about.  (Again, realistically, the
chances that your OS kernel has some 0-day vulnerability that the
NSA's Tailored Access Operations folks have purchased from some black
hat is probably a bigger risk than there being a cryptographic
weakness in AES or SHA that is exploitable given the how we are using
the encryption or crypto hash in Yarrow, Fortuna or Linux's
I still think it's worth it to have a /dev/random where we attempt to
make an estimate of the entropy that we've collected and then later
dispensed.  But I recognize that from a engineering perspective, the
distinction is not going to be that important for many people who are
interested in practical security issues.

@_date: 2013-11-05 00:01:00
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Sure, we need enough entropy to seed the /dev/urandom device.  And
there's been quite a lot of work to improve things since the P's and
Q's paper.  The distinction that I was talking about is whether every
single bit that is returned from /dev/random should correspond to bits
of entropy gathered from the system (and where you block until the
system has been able to gather enoguh entropy to satisfy the request),
or whether you depend on the cryptographic algorithms for your
security once the CSRNG has been sufficiently well seeded (which is
what /dev/urandom in Linux is intended to do, as contrasted with the
On x86 class machines, both servers and desktop, this is pretty much a
solved problems.  The real challenge is with ARM and MIPS systems,
where the CPU's not only don't have a HWRNG, but they don't even have
a CPU counter register, so it's hard to get the timer resolution to
needed to use the timing of events from the hardware for the entropy.
       	      	  	    	   	- Ted

@_date: 2013-11-05 18:09:49
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Currently, there isn't.  We could add it, but most programs aren't
going to check for such a flag.  So one of the things which I've
recently added:     are changes so that when the pool is initialized, you get a kernel
printk in dmesg:
random: nonblocking pool is initialized
Also, if a process tries to access /dev/urandom before it's
initialized we do this:
Which will result in something like:
random: ssh-keygen: urandom read with 52 bits of entropy available
My current thinking is that whether or not urandom is fully
initialized by the time should really be more of an attribute of the
overall system design rather than the application program.  Hence my
emphasis on having kernel printk's so we can understand whether or not
we have a problem, and if so, how bad is it.  My expectation is that
we're probably fine for most x86 desktops and servers (so most
developers who use those as development machines don't have a strong
incentive to do much on those platforms), but we have a much bigger
problem on ARM and MIPS embedded/consumer electronics devices.
I could add an ioctl which returns the state of the pool initialized
flag, or which blocked until the pool is considered initialized, but
I'm not convinced that enough programs would really use it.  And if I
made /dev/urandom reads block until the pool was initialized, I
suspect that product managers would just tell the engineers to patch
out the check, as opposed to doing something intelligent, such as
demanding that ARM vendors including a HWRNG in their SOC, or at least
include a CPU cycle counter register --- since that might increase the
BOM cost by a few pennies, and that would be considered unacceptable.
(You think I'm kidding --- I recently learned that hard drive
manufacturers measure cost in millicents.  Think about that, and
Still, I suppose it wouldn't hurt to add such an ioctl interface, even
if it never, or rarely, gets used.

@_date: 2013-11-05 22:14:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
What, you mean like this?
Actually, things aren't too bad.  The primary problematical caller
that I noted was:
random: rc80211_minstrel_ht_init+0x2b/0x6a get_random_bytes called with 23 bits of entropy available
... however, this looks like it's not a security problem, since as
near as I can tell the code in question doesn't actually need
cryptographic randomness.  It simply dates back to before
prandum_u32() existed in the kernel.  (We have a similar use case in
ext4, where we're we only need a PRNG, and not a CSRNG.  Although
fortunately, by the time the file system is remounted r/w, urandom is
typically already initialized, so we're not actually triggering this

@_date: 2013-11-05 22:18:50
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
gpg is already using /dev/random, which blocks.  So it's not a problem
ssh-keygen is using /dev/urandom, which can be problematic since host
keys tend to get generated way too early.
Well, with the printk, the engineers will know that there's a problem.
More importantly, end users who get access to the dmesg logs will
know, and thus apply pressure (or fix the problem in Cyanogenmod :-).
I'm not against providing a programtic way for programs to determine
whether /dev/urandom has been initialized, or even blocking until it's
been initialized.  I just disbelieve that the critical applications
will use such an interface.  Maybe I'm being too pessimistic about the
fundamental laziness of most product engineers.  But in general, it's
hard to overestimate people being lazy...

@_date: 2013-11-06 07:41:08
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
of a networking expert to be authoratative), it appears that it just
needs some random retry times for its learning algorithm.  It appears
that it might be better if the random retry times chosen unique per
host[1], but it didn't appear to have any security significance that I
could see.
[1] That's the one problem with prandom_init(); before it tries to
reseed using get_random_bytes() as a late_initcall(), the initial
state used for the prng doesn't appear to be very host-unique.
It would be great to have a networking person take a closer look at
this.  It's been on my todo list to send patch to net-dev, but
November has been crazy for me.

@_date: 2013-11-07 14:50:23
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Yes, we do.  The minstrel driver is using get_random_bytes(), which
does decrement the entropy.
The bigger problem is that it doesn't call it once --- it calls it
several dozens times, so it basically drains the entropy all the way
down to zero.  So if it doesn't need security random numbers, I'd much
rather get it using prng so we don't waste the entropy, so that
urandom can get fully initialized more quickly.

@_date: 2013-11-08 16:12:54
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
(1b) is true for all RNG's which has any kind of batching in their
design, and that tends to be all RNG's.  Even a design which is using
a noise diode and generating bits on demand, still has to batch bits
until it sends back a byte or a set of bytes to the requester.
Also note that you may not know when you've suffered from a
compromise; so part of a good design is to have ways to limit damage
after a state compromise.
2b) was historically true, but after the 2.13 kernel, it will be
moving much more towards the PRNG.  The main reason for this is that
if you have processes such as the Chrome browser which is using
input pool for use by /dev/random.  The reason why I made this change
was a user complaining to me that generating a new 2048 GPG key was
taking over ten minutes on his desktop, and I started taking a closer
look at how the entropy was getting used.
It's my (the /dev/random maintainer's) development tree, and so it's
already in test integration builds in the linux-next tree, and it's
scheduled to go into the mainline linux when the merge window opens
next week.  Linus and I will be both be in Korea giving at the
LinuxCon Korea conference (my talk is going to be about security
requirements for Linux, and it's going to include discussion about
while I am in Seoul, and I expect that he'll pull it in, do his test
build, and push it to the official mainline tree, while he is in
That's what we are doing *today*.  I'm explaining why it doesn't
really make sense to extract out however many bits of entropy we have
out of /dev/random, and then push it into the input pool.  This is
something that is within my power to change, but as I've described
above, I don't really think it's that great of an idea.
With the changes that are in the random.git tree, which are in the
linux-next tree, and will probably be in Linus's tree by the end of
the next week, if there are heavy users of /dev/urandom, the amount of
entropy consumed out of the input pool has been significantly reduced.
There are some further changes that could be made, and which I am
thinking about.  Part of this includes using AES for /dev/urandom,
since we now have CPU's with AES acceleration, and we no longer need
to worry as much about export control laws (the current design was
implemented in 1994, back when crypto export was a real issue).  One
of the things that is holding me back is that currently the Crypto
layer in Linux is optional, and can be compiled as a module, and I've
always wanted to make sure /dev/random was something user progams
could always count on being there.  So there are some negotiations I
need to make with the maintainers of the Crypto subsystem about how to
make this all work, since it would require making such changes in how
the Crypto layer is configured.
It is true that part of the current design relies on the fact we can
sample interrupt timings and storage timing issues, so we do get a
continuous stream of incoming entropy.  We want to make sure that this
entropy is used wisely, but not using it all is, at the end of the
day, just as wasteful as using it too profligately.  One of the ways
that we do use these bits is to limit the damage in the unlikely case
of internal state compromise.

@_date: 2013-11-09 08:59:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
The flip side of this is that there are multiple implementations of
AES, optimized for x86, x86_32, x86 w/ the AES-NI instruction,
Sparc64, and ARM.  Plus the generic AES implementation, of course.
Replicating all of this for the /dev/random driver would be a bit
One option would be to find a generic AES algorithm which is optimized
for size, as opposed to speed (so it doesn't have any assembly
instructions or pre-generated tables which would bloat the kernel text
size).  For example, Ilya Levin has one[1] which compiles down to about
5k.  Presumably it would be faster than the SHA-based generation
code that I'm currently using, but I haven't tried measuring its speed
on other platforms.
[1]

@_date: 2013-11-10 09:27:25
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Well random(3) is not a cryptographically secure PRNG.  So it's fine
for Monte Carlo algorithms, but not for cryptographic applications.  I
used to think that if all you wanted was a pure CSRNG, you should do
it all in userspace.  But after various implementation disasters where
pools ended up getting shared after fork(2) calls, I've decided I
placed too much confidence in the competence of application-level
programmers, and so I've re-evaluated my position on that front..
That is still what /dev/random is.  If you don't have enough entopy,
you will block.  The main concern with /dev/random is whether or not
the entropy estimators, which were designed to be conservative, are
really conservative enough.  The work done by Lacharme, Roeck,
Strubel, and Videau in their paper, "The Linux Pseudorandom Number
Generator Revisited"[1] seems to indicate that we are being
sufficiently conservative, but this is always an area were I've been a
little nervous, especially since the work to indicate that there is
hard entropy coming from the chaotic air patterns in HDD's was over
two decades ago.
It's also why I've been a bit resistant towards CPU jitter designs,
which basically use the argument "there sooooo much internal state and
it's soooooo complicated how the L1, L2, TLB cache works, and *I*
can't figure out any kind of correlation, surely it *must* be
entropic".  Of course, the Soviets didn't think the NSA could figure
out the Venona ciphers, either....
The 'hybrid' nature was always for /dev/urandom (and not /dev/random),
and as I've said, it's going to be evolving towards a CSRNG which gets
periodically reseeded from the entropy pool.
The main reason why most application programs tend not to want to use
hard randomness, and the system doesn't have any, what else can you
My suggestion would be to develop some tools that would help us give a
"confidence score" for each entropy source that gets feed into an
random number generator.  This may be a multi-dimensional score,
though, since for some people, it might be enough to assume that the
adversary can't monitor some kind of internal state (i.e., the network
timing on your LAN), but for others, they don't want to make that
assumption (after all, the NSA might have somehow figured out how to
trojan your network switch, for sufficiently large values of tin foil)
and so they want to the confidence score to be tied to physical
quantuum or chaotic effects.  For yet others, they will be willing to
trust that Intel, or some other hardware vendor, is honest in
designing what they claimed, and for others, they aren't so sure.
So the if we can't figure out how to award a "confidence score" to
RDRAND, and some people are using rngd to feed fully credited entropy
into /dev/random, how are you going award a objectively agreed upon
"confidence score" to a system configuration which uses /dev/random
and rngd to feed physical entropy from RDRAND?
If you have more confidence in Intel, and less in the amount of
entropy you can get from measuring HDD or SSD timings, the score might
be very different than if you don't want to include Intel in your TCB.
P.S.  The reason why I say the confidence score has to be based on a
specific system configuration is yet another variable might be if you
need to trust that Red Hat or SuSE wasn't approached by the NSA to put
something special in their kernels, versus if you built the kernel
from scratch and someone _you_ trusted to audit all of the
security-relevant code, which would include the /dev/random driver,
the crypto code, critical userspace daemons that listen to the
network, etc.  (And if you do care about such things, that's when you
hire someone like Kees to work on improving the security of ChromeOS.  :-)

@_date: 2013-11-10 09:44:39
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Some people have whined at me about the speed of /dev/urandom, and how
"wasteful" it is in CPU utilization.  In my original conception of the
world, something like the Open VPN server or the Kerveros KDC server
would periodically grab randomness from /dev/random, and then run
their own CSRNG in userspace.  Originally back in 1994, /dev/urandom
was almost an afterthought; I thought the more popular interface would
be /dev/random.  Which shows you how much I knew back then.  :-)
Over time, it's been clear that people don't want to implement their
own CSRNG in userspace, and they would much rather use /dev/urandom
for everything --- session keys, random padding, even to wipe a hard
drive ("dd if=/dev/urandom of=/dev/hdc bs=8k").  The last is probably
ludicrous, but if you have a really prolific user of /dev/urandom,
this can be a measurable amount of CPU time (and battery consumed).
For example, if you set up the kernel tracepoint
"extract_entropy_user", and then try reading e-mail using the Chrome
browser and gmail, you will probably be quite astounded how reads from
This is also a great way to find bugs such as the one in libnss which
opens /dev/urandom using fopen() in buffered mode, which means it
pulls in 4k out of /dev/urandom as soon as libnss is intialized.  (And
apparently Chrome runs libnss in at least two sandboxes, so that's
responsible for two 4k reads from /dev/urandom at Chrome startup;
there's a bug filed already for that problem.)
Yes, that's probably worth looking at as well.

@_date: 2013-11-12 17:52:09
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
We're doing this already.  See the call to arch_get_random_long()
(which abstracts RDRAND) in extract_buf() in drivers/char/random.c:
We also use RDRAND to initialize the entropy pools, if it is available:
So on modern x86 systems, using a reasonable recent kernel, we're in
pretty good shape.  My big concerns at the moment are on ARM and MIPS
platforms, which do not have RDRAND, and worse yet, which do not have
a fine-grained CPU counter, so get_cycles() doesn't work.  And using a
coarse grained timer, such as the jiffies lock, which typically has a
1ms or so resolution, is not going to produce quality entropy.  What
we have is good enough to avoid the embarassing "Mining your P's and
Q's" problem.  What I am not as sure about is whether we have enough
entropy to resist a determined attack by someone who is willing to
purchase specified popular handsets (i.e., the Samsung Galaxy S4, the
Nexus 4, etc.) and try to characterize the interrupt patterns for
specific hardware models, given the lack of a CPU cycle counter
register and the lack of a CPU-level HWRNG.

@_date: 2013-11-13 18:57:34
@_author: Theodore Ts'o 
@_subject: [Cryptography] randomness +- entropy 
Relying to a number of messages on this thread, not Dan's post in
For x86 desktops and laptops, I'm not that concerned about
v3.12 with the random.git changes that will be merged into mainline in
the next week or so, for the v3.13 release, running on my T430s latop:
[    4.446047] random: nonblocking pool is initialized
[    4.542119] usb 3-1.6: New USB device found, idVendor=04f2, idProduct=b2da
[    4.542124] usb 3-1.6: New USB device strings: Mfr=1, Product=2, SerialNumber=0
[    4.542128] usb 3-1.6: Product: Integrated Camera
[    4.542131] usb 3-1.6: Manufacturer: Chicony Electronics Co., Ltd.
[    4.575753] SELinux: initialized (dev tmpfs, type tmpfs), uses transition SIDs
[    4.653338] udevd[462]: starting version 175
[    6.253131] EXT4-fs (sdc3): re-mounted. Opts: (null)
So even without adding device attach times, and without fixing the
rc80211 minstrel initialization code, which is wasting two dozen bits
of entropy (it only needs non-cryptographic PRNG numbers for their
machine learning algorithm) --- both of which are on the todo list ---
the /dev/urandom pool is getting initialized with at least 128 bits of
(estimated) entropy almost two seconds *before* the root file system
is remouted read/write.
What I am more woried about is ARM and MIPS platforms, where we don't
have the high-resolution CPU cycle counter (I *am* working on pushing
on ARM CPU vendors such as Samsung and Qualcomm to try to get this
addressed for future products).
My current thinking about adding a boot-time blocking to /dev/urandom
reads for a maximum of N seconds (where N would be something like five
minutes by default).  If the timeout occurs without /dev/urandom
getting initialized with M bits (where M would probably be 128 bits by
default), then after N seconds, we would log a kern.crit message and
then let the /dev/urandom read succeed.  The idea behind this is that
if there is a system configuration issue such that /dev/urandom isn't
getting initialized, we don't want to completely break a user's
system, especially when previously with older kernels their boot
wouldn't have, perhaps inexplicably (especially in the case of a
cloud-hosted VM where ssh is the primary way sysadmins can get into
their virtual machine) hang during the middle of the boot sequence.
People who don't like the defaults will be able to specify overrides
on the boot command-line, so people who want /dev/urandom to block
forever in the absense of sufficient amounts of entropy can get that
behaviour --- it just won't be the default.  So, too, can people who
don't like the five minute hang will be able to configure their system
to not do this.  But hopefully, with a five minute delay plus the
kern.crit log message, system administrators and product engineers
will get nudged to do the right thing.  It was similar thinking which
has led me to introduce this printk for 3.13:
[    4.446047] random: nonblocking pool is initialized
also, if a process does try to read from /dev/urandom before it is
fully initialized, they will get a warning like this:
random: ssh-keygen: urandom read with 56 bits of entropy available
The main thing to remember here is that I don't get to dictate to
system administrators and software engineers at HTC, Samsung, Red Hat,
SuSE, etc., how they configure and patch their kernels, and how they
set up their boot-time init scripts.  Nor can I force application
programmers to change how they write their programs.  What I can do is
to try to "nudge" them to try to do the right thing, by making it be
obvious when they are reading from /dev/urandom too early, instead of
lazily generating ssh host keys as late as possible.
P.S.  Yes, there is a separte question about how accurate the entropy
estimation algorithms are, especially on ARM and MIPS platforms, which
haven't been as well studied as on x86.  But that's a separable
problem, and one which I hope to get some embedded engineers to work
on.  One of the things which I'm doing at the Korea Linux Forum this
week is to try to influence various Linux industry engineering groups
to be interested in tackling this problem, both by putting RFP
requirements on the CPU/SOC vendors, and by doing some studies about
how well entropy can be collected on mobile/embedded linux platforms.

@_date: 2013-10-16 15:11:12
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
Actually, the scenario you describe above is called "forward security"
in the referenced paper.  That's granted.  It's also not a problem
with Linux's /dev/random, so what you are complaining about is a straw
man argument.
What I think folks (including myself) are much convinced by is the
importance of worrying about the other attacks detailed in the paper,
where the attacker is presumed to be able to control the entropy
sources to some arbitrary extent.  In the case of Linux's /dev/random,
that means the the adversary would be able to control the exact timing
of interrupts in the system in such a way that the entropy estimators
would be fooled.
So if someone would like to make a concrete suggestion, I'm certainly
willing to entertain a specific proposal.  I'll note first of all that
FreeBSD's use of Yarrow still uses an entropy estimator, and so it
doesn't answer the paper's complaint that "all entropy estimators are
crap, and we shouldn't trust any RNG that uses an entropy estimator".
I've also said that I might be willing to add some arbitrary threshold
were reads to /dev/urandom will block until we estimate that we've
received a certain amount of entropy --- although I'd first like to
make sure we get as much entropy as possible, so we don't block for
too long first.  There are some real practical problems on certain
embedded platforms where we don't have access to high resolution CPU
counters which as far as I'm concerned is a highr priority.
This however, is still going to cause academics to kvetch about how we
are hopeless, since they disbelieve in entropy estimators and insist
on an attack model where the attacker can arbitrarily influence
interrupt timing.
Furthermore, in the area of the cold start problem, which is the much
more real and more practical problem, Fortuna doesn't help, since it
in a cold start scenario, it has no idea when it's safe to allow
someone to draw from Fortuna --- since in order to do that would rely
on an entropy estimator which the academics disbelieve in!
I prefer to call it "healthy skepticism".  As I've said elsewhere, I
recall the huge focus on formal verification of computer programs, and
the very large number of trees that were killed over that particular
academic fad.  If we consider the use of techniques that have
_actually_ improved security: valgrind, ASLR, static code analysis,
how many of them have actually come from academics?  The only one I
can think of is Coverity, and even there, most of the work was done in
a commercial setting, *not* an academic one.
(In another area, the academic focus on real-time scheduling (at least
five years ago, when I was focusing on that as the technical lead of a
Real-Time Linux effort at IBM in support of the US Navy's DDX-1000
next generation destroyer), was completely divergent from what we were
actually using in industry.  Why?  Because by the time the perfect
real-time scheduling algorithms were finished running, especially in a
dynamic environment, you were pretty much guaranteed to miss *all* of
your deadlines and the missle which had popped above the radar horizen
unexpectedly would have long ago exploded amidships!)
Going back to the random number generators, there are a lot of
practical issues, such as making sure the entropy collection is fast
enough that downstream kernel engineers and device driver maintainers
don't just turn off the entropy collection.  This can sometimes happen
in embedded kernels, and you might not ever know that this has
happened.  This is not an academic concern, but it's a real one.
On another front, I recently noticed that on my Debian Testing box,
the openssl librcrypto library is apparently not using /dev/urandom or
public/private key pairs that you might generate would have no entropy
at all!  How did I notice this?  Because I added a kernel trace point
so I could monitor how much use of /dev/random was being used by
various userspace progam.  I was originally concerned by overuse of
"openssl genrsa" and "ssh-keygen" is apparently not using /dev/urandom
or /dev/random at all(!!!).
(Fortunatly this does not appear to be the case on Debian Stable, so
it looks like a recent regression.  Or maybe it's a misconfiguration
on my end, but (a) I'm getting lost trying to figure out the mazy of
twisty compile-time and run-time configuration options of OpenSSL,
complicated by the Debian packaging system, and (b) even if it is
somehow "my fault", it shouldn't be that easy to have things silently
fail to have no entropy at all.)
So quite frankly, I have much bigger fish to fry, and I have to
prioritize my time, since I don't get paid to maintain the Linux
random number generator and so my time is not unlimited.
  I've read through the paper, and the there is nothing new in the
     paper that I consider as something I'd like in the RNG.  I do
     care about what they call "forwards security", where compromise
     of the random state pool does not compromise past results.
     That's not a new requirement, and it's one which I'm satisfied
     that we meet.
  I'm not obligated to prove to *you* anything.  I don't get paid
     or my prospects for promotion do not go up by spending hours
     writing an peer-reviewed paper.  So if you are demanding a formal
     proof, in the form of an academic paper, instead of "mere
     handwaving", you can demand anything you want.  I get demands for
     free programming efforts for pet features to my OSS code all the
     time, and I know how to handle such requests/demands.
  Let me turn this around, and ask *you* to give me concrete
     suggestions about changes you'd like to make, preferably in the
     form of a patch.  I'll tell you right away that both Fortuna and
     Yarrow, which use crypto hashing in the entropy mixing step, is
     going to be a non-starter from a performance point of view.  It's
     not hypothetical when I talk about embedded shops demanding of
     their Linux kernel developers That they disable entropy
     collection.  That has actually happened, and I've engaged said
     embedded kernel developer who got pressure from his management to
     do this on LKML to try to address that particular concern.  To
     that end, I've made the entropy collection even lighter-weight
     that will be merged into the next kernel merge window, and I
     believe I've done it in a way that preserves our security
     properties.
So when you ask me to worry about a hypothetical attack where an
adversary might be able to control all interrupt timing, and I'm
dealing with an actual attack where the adversary (also known as the
product manager :-) demanding that entropy collection be disabled,
please don't be offended when I don't take you all that seriously.
Especially when the "academically approved" RNG's don't fare all that
well in a world where all entropy estimators are f*cked and interrupt
timing and other entropy sources are subject to fine-grained control
by the remote attacker.  (Even Fortuna, if you are worried about the
cold start problem.)
P.S.  If you actually read the /dev/random source code, and take a
look at the git commit logs, you'll see that I have made changes in
response to academic papers, and I make sure to give them full credit
in the comments.  My bias only comes because I've seen so much
academic work which has very little relationship to the problems that
I need to worry about as a practicing engineer.

@_date: 2013-10-16 22:12:14
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
The answer is, " the paper's claim that the Linux generator fails
if the entropy sources are under the control of the adversary relies
on the fact that it stops collecting entropy when it thinks the
entropy pool is full, which is NOT TRUE, and  it's really, REALLY
stupid to assume the adversary has complete control of the interrupt
timing on your system."  I think you have missed the first part.
I'm not sure that's the best analogy, because there are known attack
scenarios where someone might have some plaintext/ciphertext pairs and
might be interested getting the key.
I haven't seen an even half-way reasonable attack scenario where the
attacker can control all of the entropy sources in the system --- not
just know the interrupt timings, but to *control* the interrupt
timings, in a very fine-grained way.  (So it's not enough to just to
know roughly when a packet gets sent to the machine, but to be able to
send the packet such that you can control the exact value of the CPU
counter, so you can fool the entropy estimator.  And the attacker has
to be able to do this not just for network interrupts, but also for
disk, keynoard, and mouse interrupts, all at the same time.
That's only true if they have fairly privileged access to the
hypervisor.  And while it's barely possible to imagine scenarios where
an adversary would have read access to hypervisor memory, but not
write access, that is actually pretty far-fetched.  Feel free to
construct a scenario....
Um, if you read the paper, its claim that /dev/random is not robust by
their definition relied fundamentally about the entropy estimator
being "wrong" because the adversary could control the inputs to the
entropy pool, and thus construct inputs that would fool the entropy
estimator.  So it feeds into the discussion in a rather fundamental
In the Linux Pseudo Random Number Generator Revisited paper
( the authors sampled and
analyzed the various real-life entropy sources, and found the entropy
estimation to be pretty good, and if it erred, it erred on the side of
convervatism, which is as designed.  In case you were wondering, I'll
consider this "good" academoc research --- not because I like the
result, but because they actually carried out research instead of
relying only on articially created attacks dressed up in the language
of mathematical formalism.
Formal proofs may be impressive, but it's nice if the formalism is
actually tied to reality, instead of tenuously based on some
fantastical assumptions, e.g., "The US Naval aircraft carrier is not
robust against photon torpedoes".  You can do lots of formal
mathematics involving weapons yield to "prove" such a result, but it
begs the question of whether photon torpedos exist in the real world.

@_date: 2013-10-17 09:08:00
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
... and Linux's /dev/random driver does this.
Post July 2012, most of the entropy is gathered via a per-CPU (to a
avoid cache line bouncing effects and so it can be lockless) entropy
pool, where we sample the high resolution cycle counter (or whatever
the highest granularity clock / memory refresh control register /
etc. we have access to on the archtecture) and the interrupted IP, and
mix that into the per-CPU fast mix pool on every interrupt.  We do
*not* use an entropy estimator for this interrupt fast mix pool.
Instead, we sample Every 64 interrupts, we transfer entropy from the
fast mix pool to the input pool, and we credit the input pool with a
single bit of entropy.  (There is very likely much more than a single
bit of entropy that has gotten accumulated during those 64 interrupts,
but out of an abundance of caution, we're using a very conservative
estimate for administrative concerns.)
In both the pre and post July 2012 designs, using a Yarrow-like
approach, we only transfer entropy from the input pool to the output
pool when there is sufficient entropy estimated to be in the input
pool so that we can do a "catastrophic ressed".  The "/dev/random is
not robust paper" assumed that the attacker could control the
interrupt timings such that estimate of entropy in the input pool was
incorrect, and thus the catastrophic reseed aspect of the design could
be bypassed.
I've already discussed why I don't believe that the assumption that
the attacker could control the interrupt timings to such an extent is
not realistic, and analysis of the entropy estimator (as used in the
pre-July 2012 design) showed that in fact, it was quite good.  But in
the post July 2012 design, we no longer use an interrupt estimator for
the interrupt fast mix pool.  We abandoned it for efficiency concerns,
since we wanted to make the cpu count on the global interrupt fast
path as low overhead as possible; instead, we traded this off by a
brute force quantity argument --- if we can collect the timing for
every single interrupt we're much better off than collecting it only
for some interrupts, especially when in the old design (which involved
CPU cache line bouncing and potential lock contention) device driver
authors were disabling the entropy collection more often than not.
So in the new design, we aren't using an dynamic entropy estimator ---
instead, we're assuming that after collecting the timings for 64
interrupts, we've collecting a single bit of entropy, which is really
a static entropy measure.  Could this be spoofed if the attacker has
control of the interrupt timings of the system?
Sure, but if the attacker has that level of control on the system,
then then pretty much all generators would be seriously compromised as
well.  The only way the paper could show that their proposed generator
was "robust" was based on the assumption that it would be possible for
the attacker to control the entropy inputs in such a way that entropy
estimator would be spoofed, but the attacker might still not know some
of the bits of the entropy inputs.
After all, if the attacker knows all of the bits, then by definition
all generators would be screwed.  However, what has not been
demonstrated in the paper is a real life scenario where the attacker
would have that level of control over the entropy inputs --- enough
that entrpoy estimators would be fooled, but not enough control that
their constuction could be considered robust.

@_date: 2013-10-17 17:07:10
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
The major problem which could be considered a "Linux distribution
issue" is ssh host key generation, which is done by the boot scripts
if the ssh host keys do not exist.  It would be much better if this
was done on the first TCP connection to the ssh server, but that would
require changes to sshd.
The other places where there are problems will be creation of
public/private key pairs when a printer first boots up, but that's not
really "Linux distribution code".  I have seriously been thinking about adding that.  The main thing
which is causing me pause is I need to make sure that on low entropy
devices, that we don't end up stalling processes which require
were broken already, but it might be our fault because we're using too
conservative of an entropy estimate.  The issue is if it becomes too
onerous, people will just simply "fix" the kernel regression by
commenting out the check, and then it will be harder for me to get
that functionality re-added in the future.
So there's some measurements that need to be done before I'd be
comfortable turning this on, at least as the default.  What I might do
is to add some tuning parameters that could be passed on the kernel
boot command-line which forces /dev/urandom accesses to block until N
bits of entropy have been accumulated or M seconds, whichever comes
first, and we would log a kernel message as soon as /dev/urandom is
considered initialized.  It least initially, N would probably be
defaulted to zero, which would disable this feature, and M would be
300 seconds or some such.  Users could then experiment on various
different devices before we made a final decision about whether this
feature should be enabled by default.
When the init scripts restore the seed file, there is no entropy
credits given.  So if the seed file has been compromised, it's not a
disaster because we don't assume the seed file is guaranteed to have
any entropy.  OTOH, if the seed file is good, then we'll obviously be
much more secure.

@_date: 2013-10-17 17:26:22
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random has issues 
This is a known problem, and I have a patch pending for the next merge
window to address this.
The Chrome browser in particular is a very heavy /dev/urandom user,
and this is causing the problem you describe below:
With my recent change, /dev/urandom becomes much more like a
periodically seeded CRNG, where we aren't even pretending to extract a
bit of entropy from the input pool for each bit sent to userspace.  If
you want that, then you should use /dev/random.
There are some good questions here.  Some of them have been recently
addressed, others have not been yet.  I don't have time right now to
go through them all in detail, but I will put this on my reading list.
Some quick notes: I have considered the possibility of replacing the
output pools with something that uses AES instead, which would be
especially useful for those architectures which have an AESNI-like
instruction.  That's obviously something that would require a lot of
thinking and prototyping before making such a major change, though.
As far as your comments about /proc/sys/kernel/random/entropy_avail
usually being close to zero, I'm currently running an upstream kernel
with the dev branch of the random.git tree merged in, and things are
significantly improved on that score:
% cat /proc/sys/kernel/random/entropy_avail On a process note, there is a huge amount of interest about
some seem to be from people who haven't necessarily looked at the
actual drivers/char/random.c source code, nor are interested in
proposing specific changes, your comments above indicate that you have
done this, and I very much appreciate your thoughts.
Is the cryptography mailing list the best place to be having these
discussions?  There is the moderation delay, and I'm also not sure how
eager the moderators are about having the mailing list taken over by
people talking about code patches, etc., on this list.  I wonder if we
should create a separate mailing list, perhaps a
linux-random at vger.kernel.org, and take the more technical discussions
to that mailing list.
P.S.  If there are folks who will be at the LISA Conference in
Washington, D.C, I'm hoping to meet with Matthew Green and try to
interest him into doing a detailed look into at the random driver, and
perhaps dragoon some of his students into evaluating entropy sources
on various embedded Linux platforms.  If there are other people who
are interested in talking /dev/random while I'm in DC, I've
tentatively blocked off the afternoon of Tuesday, November 5th for
that purpose.  Let me know off-line....

@_date: 2013-10-17 17:29:52
@_author: Theodore Ts'o 
@_subject: [Cryptography] /dev/random is not robust 
Yeah, there are some people (including Dustin Kirkland at Canonical)
working on automated provisioning of random seeds from the hypervisor
to the guest kernels.
If you are compiling your own guest kernel, and the hypervisor
supports it, using virtio-rng which allows the guest to use the host
OS's /dev/random to bootstrap its local entropy pool is almost
certainly the Right Thing.

@_date: 2013-10-19 10:33:34
@_author: Theodore Ts'o 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
At the risk of repeating myself, we made a lot of changes to the
P's and Q's paper (patches went into mainline the first week in July,
and got propagated to older kernels via the stable kernel trees about
2 weeks later; the paper was published at Usenix Security in August.)
One of them was to do precisely this --- /dev/urandom now mixes in
salting information (ethernet MAC addresses, etc, via the new
interface add_device_randomness).  Zero entropy is indeed assessed,
and the main goal is to avoid the trivially easy case of shared primes
in the case where we fail to gather enough entropy.
The other change we made was to gather entropy on every single
interrupt, instead of only on those device drivers where the device
driver authors gave us permission to collect entropy.  That was a
mistake, because device driver maintainers care about performance and
CPU efficiency at all costs, and they don't particularly care about
entropy collection.  So we made entropy collection fast, and
As I've already said, I'm open to adding code that blocks /dev/urandom
until "enough" entropy has been collected.  But that's an
interface-visible change, and it could break things.  So there is due
diligence that will need to be done first, because the reality is if
it breaks things, people will just comment it out, and it will be
harder to propagate the change in the future.  In the real world, the
product manager is often as much a security engineer's adversary as
Boris and Natasha.  :-(

@_date: 2013-10-26 12:58:28
@_author: Theodore Ts'o 
@_subject: [Cryptography] provisioning a seed for /dev/urandom 
virtio-rng has been around for over 5 years, and it specifically
provides access to the host's /dev/random and makes it available via
virtio-rng.  I'm not sure about Xen, but if it doesn't, boo, hiss to
the Xen folks, especially since the paravirtualized interface has been
around for so long.
There is typically plenty of interrupts from your network and storage
devices which should provide plenty of entropy for the hypervisor.

@_date: 2013-10-27 04:15:36
@_author: Theodore Ts'o 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Yes, absolutely.  For example, if you assume that the attacker has
network taps at Fort Meade and in a phone closets of companies like
AT&T, they are very likely not going to be able to watch your LAN
traffic.  OTOH, if they have physical access to your LAN such that
they can drop an agent close to your computer that can monitor all of
the packets hitting your computer, we have to ask how are they doing
this?  If they can someone break into your local ethernet switch
remotely, then you might be in a world of hurt (although usually
switches generally don't have enough of general purpose CPU that this
is likely).
If you posit a "black bag" job where they physically break into your
house, and replace your ethernet switch, then they could presumably
place a keyboard bug on your keyboard, or otherwise physically tamper
with your computer, install audio/video surveillance equipment in an
HVAC duct, etc. --- and then you're either doing something really
black hatish, or I have a tin foil hat to sell to you, or possibly
both.  :-)
My challenge as someone who is designing things like a general purpose
about the threat environment might make sense in a large set of
hypothetical scenarios, and which do not.  I can imagine scenarios
where the adversary is on a public network --- say, at a University
dorm network --- who might be able to watch interpacket network
arrival times, but who probably can't make a lot of assumptions about
HDD completion drive times --- and the user might want to generate a
securely long-term public key for their ssh host key or for GPG.
I'm less willing to accept as a valid threat model one where the
adversary has near-total control over _all_ entropy sources, *and* can
divine the state of the prng, but has no other access to the system so
they can't break root in other ways, *and* where if you can't prove
that you can make the prng secure again, it's somehow horrible and
your rng is not robust (and that the authors of said paper should
deserve lots of citations so they can get a suitably high impact score
on their way to achieving tenure :-).
But maybe there are scenarios where such a threat environment is
actually realistic.  I'm certainly willing to hear someone try to give
me an example of such a threat environment; it would probably be quite

@_date: 2013-10-28 18:04:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
And this is *already* in Linux's /dev/random code since July 2012.
(There is another problem caused by proprietary binary drivers from
!@ proprietary chips from a company whose name shall remain
starting with the letter Q, forcing some home routers to still be
using ancient, years-old 2.6 kernels, but that's not a problem under
my control.  Said 2.6 kernels probably have huge numbers of zero-day
flaws, some of which might allow a remote adversary to be able to
execute a buffer overflow attack in kernel space, making flaws in the
random number generator somewhat irrelevant...)
There is no "solve".  In all of these cases, it's going to be very
dependent on the hardware involved.  Many home routers might only have
two or three devices, so even if you get four bits per device, that's
only going to be 12 bits of extra entropy.  In combination with the
entropy that we get from sampling interrupts, and if the device holds
off on generation of public/private keypairs until they are actually
needed, hopefully it will be enough.  In other cases, if there are
almost no init scripts configured, and SSH keys generated as the very
thing after a cold power up, even adding device attach times might not
be sufficient to defeat an adversary who is doing deep analysis about
your particular home router's hardware and software setup.
We are already adding the CPU time elapsed (userspace plus kernel
time) for each process when it exits.  The theory behind this is that
we have this information anyway, and process exits are a place where
adding a call to add_device_randomness() isn't going to hurt system
performance.  We don't credit the entropy counter with any additional
randomness, but the theory is that it can't hurt, and it might help.
It's hopeful that for a system that is running shell scripts as part
of boot, there will be at least some entropy added by sampling these
values, even if it is not a huge amount.
In theory this could be done for other system timings, but we need to
chose things that minimize overhead imposed on the system.       	     	  	   	    	       - Ted

@_date: 2013-10-29 08:07:28
@_author: tytso@mit.edu 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
One addendum.  Some insecurities may be brought to you by the letter
'B', and not just 'Q'....

@_date: 2013-10-28 23:03:43
@_author: tytso@mit.edu 
@_subject: [Cryptography] My comments regarding using CPU jitter for random 
There is a thread on LKML which I commend to people people interested
in this subject.  The thread starts here:
... and my comments can be found here:
I'm going to take the liberty of reproducing my comments here, because
I think it's especially relevant to some of the discussions we've been
having on the Cryptography list vis-a-vis random number genreation.
Fundamentally, what worries me about this scheme (actually, causes the
hair on the back of my neck to rise up on end) is this statement in
your documentation[1]:
   When looking at the sequence of time deltas gathered
   during testing?[D]?, no pattern can be detected. Therefore, the
   fluctuation and the resulting distribution are not based on a
   repeating pattern and must be considered random.
[1] Just because we can't detect a pattern does **not** mean that it is
not based on a repeating pattern, and therefore must be considered
random.  We can't detect a pattern in RDRAND, so does that mean it's
automatically random?  Why, no.
If all you have is the output of "AES_ENCRPYT(NSA_KEY, i++)". and
NSA_KEY is not known to you, you won't be able to detect a pattern,
either.  But I can guarantee to you that it's not random...
It may be that there is some very complex state which is hidden inside
the the CPU execution pipeline, the L1 cache, etc., etc.  But just
because *you* can't figure it out, and just because *I* can't figure
it out doesn't mean that it is ipso facto something which a really
bright NSA analyst working in Fort Meade can't figure out.  (Or heck,
a really clever Intel engineer who has full visibility into the
internal design of an Intel CPU....)
Now, it may be that in practice, an adversary won't be able to carry
out a practical attack because there will be external interrupts that
the adversary won't be able to put into his or her model of your CPU
--- for example, from network interrupts or keyboard interrupts.  But
in that case, it's to measure just the interrupt, because it may be
that the 32 interrupts that you got while extracting 128 bits of
entropy from your jitter engine was only 32 bits of entropy, and the
rest could be determined by someone with sufficient knowledge and
understanding of the internal guts of the CPU.  (Treating this
obscurity as security is probably not a good idea; we have to assume
the NSA can get its hands on anything it wants, even internal,
super-secret, "black cover" Intel documents.  :-)
To be honest, I have exactly the same worry about relying on HDD
interrupts.  The theoretical basis of this resulting in true
randomness is based on a 1994 paper by Don Davis: "Cryptographic
randomness from air turbulence in disk drives"[2]:
[2] The problem is that almost two decades later, the technology of HDD's,
and certainly SSD (which didn't exist back then) have changed quite a
lot.  It is not obvious to me how much entropy you can really get from
observing the disk completion times if you assume that the adversary
has complete knowledge to the relative timing and block numbers of the
disk accesses from the OS (for example, if we boot multiple mobile
phone from flash for the first time, how many bits of entropy are
there really?)
But at least back in 1994, there was an attempt to come up with a
physical theory as to where the entropy was coming from, and then as
much work as possible to rule out other possible causes of the
So if you want to really convince the world that CPU jitter is random,
it's not enough to claim that it you can't see a pattern.  What you
need to do is to remove all possible sources of the uncertainty, and
show that there is still no discernable pattern after you do things
like (a) run in kernel space, on an otherwise quiscent computer, (b)
disable interrupts, so that any uncertainty can't be coming from
interrupts, etc., Try to rule it all out, and then see if you still
get uncertainty.
If you think it is from DRAM timing, first try accessing the same
memory location in kernel code with the interrupts off, over and over
again, so that the memory is pinned into L1 cache.  You should be able
to get consistent results.  If you can, then if you then try to read
from DRAM with the L1 and L2 caches disabled, and with interrupts
turned off, etc, and see if you get consistent results or inconsistent
results.  If you get consistent results in both cases, then your
hypothesis is disproven.  If you get consistent results with the
memory pinned in L1 cache, and inconsistent results when the L1 and L2
cache are disabled, then maybe the timing of DRAM reads really are
introducing entropy.  But the point is you need to test each part of
the system in isolation, so you can point at a specific part of the
system and say, *that*'s where at least some uncertainty which an
adversary can not reverse engineer, and here is the physical process
from which the choatic air patterns, or quantum effects, etc., which
is hypothesized to cause the uncertainty.
And note that when you do this, you can't use any unbiasing or
whitening techniques --- you want to use the raw timings, and then do
things like look very hard for any kind of patterns; Don Davis used
FFT's because he wanted to look for any patterns that might be
introduced by the rotating plattern, which would presumably would show
up in a frequency domain analysis even if it was invisible in the time
If you don't do all of this work, there is no way to know for sure
where the entropy is coming from.  And if you don't know, that's when
you have to be very, very conservative, and use a very large
engineering safety margin.  Currently we use the high resolution CPU
counter, plus the interrupted IP, and we mix all of this together from
64 interrupts, and we count this as a single bit of entropy.  I *hope*
that at least one of those interrupts has sufficient unpredictably,
perhaps because the remote attacker can't know when a LAN interrupt
has happened, such that have a single bit of entropy.
Maybe someone can prove that there is more entropy because of some
instability between the oscillator used by the CPU clock and the one
used by the ethernet NIC, and so I'm being hopelessly
over-conservative.  Perhaps; but until we know for sure, using a
similar analysis to what I described above, I'd much rather be slow
than be potentially insecure.
The jitter "entropy collector" may be able to generate more
"randomness" much more quickly, but is the resulting numbers really
more secure?  Other people will have to judge for themselves, but this
is why I'm not convinced.
Best regards,
                                        - Ted

@_date: 2014-04-01 10:16:26
@_author: Theodore Ts'o 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Apparently the obsoleted by tag is in the HTML version, but not the
text version.  (I thought the old text rfc's were updated to include
obsoleted-by information, but apparently not any more.)  Unfortunately, Google gave me the text version first, probably because
historically more people have clicked on it:
instead of the HTML version:
Hmm.... maybe I should ask the Search folks about adding Knowledge
Panels for RFC's.  :-)

@_date: 2014-04-18 07:52:25
@_author: Theodore Ts'o 
@_subject: [Cryptography] Cue the blamestorming 
It would be nice if we could start instituting an industry standard
that when a company hires a "core" developer for an open source
project, they get to spend 50% of their time working on "community
stuff".  That way, in addition to adding features that their company
wants (i.e., smart cards, GOST algorithms so they can sell into the
Russian market, yadda yadda yadda), there is also some time that
people can spend doing the necessary code cleanup that a company might
not otherwise be willing to invest engineering resources to do.
I negotiated such a deal when I joined Google, and the Samsung Open
Source folks have made that a general policy, since they're trying to
rapidly build up a team of senior open source folks.  So there
precedence for something like this; the trick is to see if we can get
this to be more widely adopted, and turn it into an expectation that
the open source leaders of various projects, whether they be crypto or
security related or not, have when it comes time to hiring

@_date: 2014-04-20 20:01:27
@_author: Theodore Ts'o 
@_subject: [Cryptography] Apple and OpenSSL 
I remember participating in discussions with OpenSSL in the context of
the Linux Standard Base (LSB) which is very much about stable ABI's.
At least at one point, the commitment that the OpenSSL folks were
willing to make is that the ABI would be stable within a micro release
level.  That is, 0.9.N and 1.0.N would remain ABI stable so long as N
didn't increment.
But historically, every two years or so, N would bump, and that point,
you would need to bump the major SOVERSION of the shared library,
because there would be ABI breakage.
Many other packages try much harder to only add new functions, but to
not break any existing functions once they are added to the shared
library.  However, this is much more difficult if the library has
exposed many structures as part of the ABI, such that it is incredibly
difficult to change the data structures without breaking the ABI.
If you design your interfaces to maximize ABI stability (and you don't
use C++, because every single time you add even a private variable to
the base class, it breaks ABI compatibility for all of the
subclasses), it is possible to make this kind of ABI stability for far
longer than "we'll break the world every year or two".
So when you say "stable ABI", it's important that you set expectations
for what you mean by that.  Some people are willing to make much more
stringent promises of ABI stability than others.  I've run Linux
binaries dating back from the late nineties on modern kernels without
a problem.  That's a much different level of ABI stability than what
OpenSSL has promised or has delivered in the past.  (Although to be
fair, they have a legacy code base and a legacy set of interfaces that
make life very hard for them.)

@_date: 2014-04-21 09:27:19
@_author: Theodore Ts'o 
@_subject: [Cryptography] Apple and OpenSSL 
You can solve this problem two different ways.  You can either put a
type and length values at the beginning of each object, so you can
always dynamically find the end of the structure (this is what
Microsoft COM did decades ago), or you can use pointers everywhere, so
struct B doesn't contain struct A, but contains a pointer to struct A.
The latter is how most systems that use "object orientated principles"
in C do things.
So this is of course not a fundamental fault in the language.  It
would be possible to create an C++ calling convention/runtime
architecture where you could change the base class without breaking
ABI compatibility of its subclasses.  But this would require breaking
the existing ABI, and it would also require that C++ classes not use
inline functions for any of their method functions, and do a number of
other things to provide a stable ABI --- and most of the C++ code out
there don't bother to do any of these things, because culturally, C++
and "stable ABI" have been incompatible for a long, long time.
It's not impossible; the Qt and KDE folks have figured out ways of
making this happen.  But you have to do a lot of unnatural things to
hack around deficiencies in C++ and its runtime bindings:

@_date: 2014-04-22 17:16:45
@_author: Theodore Ts'o 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
On modern systems the amount of bloat is truly astonishing.  Consider
that libxml2 is almost 1.5 megabytes of compiled object code.
libcrypto is almost 2 megabytes, the libQt libraries is 62 megabytes,
and so on.  So if you have a dozen or so windows open (and in a GNOME
or KDE system, many of the applications will all be using the same
shared libraries), the amount of memory which is saved can be quite
Of course, the fact that we have so much bloat is bad from security
perspective as well.  How many zero-days do *you* think is hiding in 2
megabytes of compiled crypto code?  :-)
P.S.  Yes, if you statically linked, you might not need to drag in the
entire text of the library.  But unfortunately, because everyone uses
shared libraries, the discipline to keep library functions well
isolated and in separate small .o units so that pulling in a few
functions doesn't end up dragging in the entire library anyway is much
less rigorous than there used to be.  (``It my day, we had to make our
programs fit in 64k.   And get off my lawn!''  :-)

@_date: 2014-08-18 08:33:45
@_author: Theodore Ts'o 
@_subject: [Cryptography] [cryptography] Question About Best Practices for 
That may be true --- although decompilation doesn't give you any code
comments and/or code history, which significantly eases the work
needed to review a portion of code.  Even if we accept those figures, you are assuming that the only choice
is "no review at all" and a "thorough review".  It may only be 10-20%
of a very large amount of effort to do a total, thorough review.  But
it is enough work that it prevents anyone from doing a casual review.
A casual review won't find all bugs, but if enough people look at
different parts of the code, the probability that a vulnerability will
be noticed increases significantly.  This is especially true if people
are doing not-so-casual reviews of the code in preparation for making
Of course, the changes might introduce additional vulnerabilities ---
but not all vulnerabilities will be catastrophic vulnerabilities of
the sort that a backdoor might introduce.
So yes, granted, open source doesn't solve the backdoor problem; but
by your own admission, it makes the problem at least 20% easier, and I
suspect the situation is much better than that.

@_date: 2014-02-01 18:07:51
@_author: Theodore Ts'o 
@_subject: [Cryptography] cheap sources of entropy 
*Maybe*.  There could be enough quantization errors such that you're
not really measuring this.  Consider what might happen if the VMs are
being scheduled by the host OS with a scheduling quantum measured in
10's of milliseconds (servers generally get configured with a clock
tick of 100HZ), and suppose the variability caused by air turbulence
is measured in hundreds of microseconds.  By the time the host OS has
has done the I/O on behalf of the VM, and then scheduled the VM to
deliver the virtual disk's interrupt, in this case you almost
certainly won't be measuring variations which can be attributable to
the noise on the disk.  Now, there may be enough unpredictability
caused by whatever the *other* VM's on the host OS are doing that a
remote attacker might (or might not) have access to.  But the bottom
line is that you may not be measuring what you think you are
measuring, and so how you reason about how much entropy is in that
particular source might be very much at odds with reality.
Is it better than nothing?  Sure, although I think using virt-rng is
actually a better choice.  Yes, you are now dependent on whoever is
running the host OS server not having received a National Security
Letter from the FBI, but that's always been the case --- and it's much
easier for them to simply grab snapshots of your virtual hard disk,
and to go pawing through the memory of the running VM to get the
encryption key if you are using dm-crypt.

@_date: 2014-02-02 08:35:10
@_author: Theodore Ts'o 
@_subject: [Cryptography] cheap sources of entropy 
Jerry Leichter has already explained why with track buffers this isn't
a concern.  In addition to that, modern file systems don't send read
requests one sector at a time, because of controller overhead; we will
try to read, and write large extents of data at a time --- preferably,
64k, or even several megabytes, at a time.
Your conception how disk drivers had to optimize for sectors going
under the disk head was true back in the era of the Apple II and the
TRS-80's (and in my first computer, which was a PDP-8/i with a DF32
fixed-head disk drive which stored 32k 12-bit words on a single-sided
12" platter).  But that hasn't been true for quite some time.
In *addition*, once the host OS gets the data requested by the guest
OS, the "wakeup()" function does *not*, in general, immediately wake
up the guest OS.
Most VM providers, not being Mafia fronts trying to launder money,
prefer not to lose money and go out of business, which means that
their machines are not idle, but in fact there are dozens, if not
hundreds of VM's running on each physical host.  That means on a disk
interrupt, the wakeup() function puts the process (such as the guest
VM) on a linked list of processes that should be run the next time
there is a free CPU and their turn is up.  This *can* happen due to
all other guest OS's getting blocked on I/O, but if you have
processes/VM's which are CPU bound, they will get to run until their
scheduling quanta runs out, which is measured in units of 100HZ clock
ticks.  Which is why you can see quantization effects.
The bottom line is that when the guest OS gets the virtualized disk
interrupt delivered to it is going to be based not on "disk
turbulence" (the theoretical underpinning of which was a paper written
in '94, and disk drive technologies have changed a wee bit since
then), but based on when other VM's might be blocking, or when some
other cpu-bound guest OS gives up its CPU on a clock tick boundary.
Now, all of this might not be predictable to an outside observer who
doesn't have full information about the internal state of the Host OS,
and all of the guest OS's running on it.  But it's not based on
chaotic air patterns, but rather something else.
Is it good enough?  Maybe.  My preference, as others have suggested,
is to mix in something purpose built, such as RDRAND, and if that has
been backdoored somehow, either by the NSA or the MSS, to also mix in
as much environmental noise as you can get.  Belt and suspenders....
   		      	       	       - Ted

@_date: 2014-02-02 15:38:53
@_author: Theodore Ts'o 
@_subject: [Cryptography] cheap sources of entropy 
*Your* VM may be I/O bound, but other guest OS's, running on other
VM's, may be CPU bound at the same time.  In fact, if the hosting
service is being smart, they might try to start, and possibly migrate,
VM's which are mostly CPU bound to be on the same physical machine as
other VM's which are mostly I/O bound, so they can more efficiently
use 100% of all of the host OS's resources.  In the ideal world, you
want each physical machine to be using close to 100% of all available
memory, CPU, disk time, and networking bandwidth.  Companies that can
do this will can afford to sell VM's cheaper than their competition,
and guess what, will tend to be the ones that survive and propser.
And therefore, you may find that even though you are I/O bound, your
disk interrupts in your guest OS are being delivered to you subject to
certain quantization effects.  And even if it isn't doing so today,
you might find that tomorrow, your hosting service may change how they
do things to more efficiently use their physical servers...
    	  	     	     	- Ted

@_date: 2014-02-03 19:23:38
@_author: Theodore Ts'o 
@_subject: [Cryptography] request for consideration: VM guest entropy: 
Linux kernels newer than 3.3 (released about two years ago) will use
arch_get_random_long/int() in some way.  The
arch_get_random_long/int() functions are wired up to RDRAND on x86
systems that have it.  We use a generic name so we can support other
CPU's that might have a purpose built hwrng.
Yes; that's the job of userspace rngd daemon.  The idea is that if you
are using some random home-built which you've wired up into a usb
drive, it might have some catastrophic failure mode, and it's better
to have a userspace daemon which can do some sanity checks on the
entropy source before we use it.
It's possible that we could have the kernel automatically use the
entropy source from /dev/hwrng but not give any entropy credit.  My
only concern is that sometimes the /dev/hwrng device may use extra
battery when you access it, so whether we did this would have to be
configurable, and perhaps the default should be keyed on whether we
are running on battery or not.
(Basically, there's an assumption that arch_get_random_long/int() is
essentially free from a power perspective, and it's competently
implemented such that we don't need to worry about doing any sanity
checking.  OTOH, we also aren't giving any entropy credit for RDRAND,
either.  People who want the RDRAND source to be counted as entropy
credit will need to use rngd, since that's a policy decision I didn't
want to leave hard-wired into the kernel.  Also, I'd note that vast majority of the deployed Linux systems don't
have an extrnal entropy source which can be accessed via /dev/hwrng.)
No.  Bad idea.  Programs should use /dev/urandom for session keys or
if they can not afford to block, and /dev/random if they are
generating a high value or long-term private key.
This is why it's actually better to tell programs to use
mixed in.  Using /dev/[u]random also makes your program portable to
other operating systems and othe platforms that may not have RDRAND,
but may have some other high frequency counter we can use in
combination with external events.  (For example, on MIPS systems there
is a DRAM refresh counter we can use, etc.)

@_date: 2014-02-04 11:05:54
@_author: Theodore Ts'o 
@_subject: [Cryptography] request for consideration: VM guest entropy: 
It already exists, and all/most distributions has had it for years.
It's called rngd.
Making it be the default is up to whoever is creating the base images
for various hosting providers.  The bigger problem is that not all
cloud hosting providers are providing virtio-rng.  But that code
exists today, so it's a matter of lobbying the hosting providers to
make it available.  It appears Rackspace does support virtio-rng.  As
others have mentioned Amazond doesn't appear to support virtio-rng.
       	    	      	      	      - Ted

@_date: 2014-02-05 10:46:57
@_author: Theodore Ts'o 
@_subject: [Cryptography] Random numbers only once 
Linux distributions extract some number of bytes from /dev/urandom and
then use this to seed the entropy pool for the next boot cycle, and
have since the very beginning[1].  This is done at shutdown, and also
right after the entropy pool is seeded at startup (so the value used
to seed the system at boot can not be exposed, and so we don't use the
same seed value after an unclean shutdown).
[1] I have been amused for a while now how many people have been
making assumptions without understanding how things actually work.
However, because the state file could get compromised (i.e., the disk
image gets copied/ghosted or accessed directly while the system is
shut down), we don't make any assumptions about the state file being
secure, and hence give no entropy credit.

@_date: 2014-02-06 14:22:20
@_author: Theodore Ts'o 
@_subject: [Cryptography] Random numbers only once 
I agree that for server systems, saving a few hundred bytes of data
from /dev/random and restoring it on reboot is unlikely to help,
but... (a) it's more useful for laptop systems that reboot more often,
and (b) for those virtualization systems that for whatever reason
can't or don't want to support virtio-rng, it's another way for the
host OS to provision randomness to a guest OS at startup.  Personally,
I prefer using virtio-rng, but any kind of seeding from the host OS to
the guest OS is a gift horse which I won't complain too much about.
    	     	       	     	   - Ted

@_date: 2014-02-14 00:27:22
@_author: Theodore Ts'o 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
Well, the maximum block size is 1MB, and the smallest transaction size
is aronud 300, plus or minus.  So that means you could pack a maximum
of around 3500 transactions, although looking at blockchain.info, the
number of transactions per block is currently around 350, with peaks
up to around 600 transactions/per block.
If we do have 3000 transactions per block (which is the absolute best
case), times a transaction fee of 0.0001, then the reward per block
becomes $200 USD given today's bitcoin prices; if the number of
transactions per block remain at today's levels, then it's more like
$20 to $40 USD per block.
Of course, by the time block rewards go away, we will have a fixed
supply of bitcoins, and if bitcoin becomes super popular, then
increasing demand and a fixed supply will cause deflation, and the
value of the transaction fees for an entire block BTC --- 0.3 BTC in
the best case --- might become much more valuable.
(OTOH, if the hijinks with Mt. Gox and Bitstamp shake the the general
public's faith in bit coins, or more countries outlaw bitcoins and/or
indict Bitcoin Foundation board members, such that the demand of
bitcoin doesn't take off, who knows where the value of an BTC will end
up.  It seems to me quite unknowable at this point where the value of
a BTC will be by 2140.)
That's a really interesting question.  Suppose the total rewards
generated by the entire network from transaction fees is 43 BTC/day
(0.3 BTC/block times 6 blocks/hour times 24 hours/day), shared across
the entire set of miners.  How valuable does a BTC need to be such
that 43 BTC/day will motivate enough miners to stay in the game to
make it not worthwhile for an attacker to try to buy or create enough
mining hardware to control 51% of the mining resources in the network?
              		   	       	      - Ted
P.S.  Is discussions of bitcoin in or out of scope of this mailing
list?  Apologies in advance if the moderators consider it too far
afield of the ML's charter.

@_date: 2014-02-15 13:56:21
@_author: Theodore Ts'o 
@_subject: [Cryptography] RAM memories as one source of entropy 
IBM Power servers may have already beaten you to this.  Indeed, with
the publically available interfaces, it is impossible to get access to
the raw CPU; the BIOS is always running as a hypervisor.
    	     	      	 		   - Ted

@_date: 2014-02-16 08:18:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] BitCoin bug reported 
I think you are engaging in pedantry.  The problem was that the
defined the abstract transaction in terms of something like an ASN.1
structure, and failed to specify the use of a canonical encoding for
that structure when it is passed on the wire.  I.e., the difference
between DER and BER.  This had been a known problem, and for
compatibility reasons, most implementations (including the reference
implementation) were sending the DER encoding, but accepting a more
liberal encoding of tha transaction, and matching against the actual
structure elements of the structure.
Unfortunately, there were older implementations that were sending
non-canonical encodings, and worse, trying to rely on the hash of the
encoding without verifying that the encoding was in fact the canonical
Furthermore, there are in fact many definitions of transactions, even
with the computer industry, and context matters.  There is the
accounting deifnition of the word "transaction", which for financial
applications, is just a perfectly valid system.  You would agree that
if you complained that a double-entry accouting system, even if it
were computerized, used "transaction" in the way any accountant would
accept and understand it, and complained that program was not using
the word "transaction" in the database sense (which is what I think
you mean by "well established in computer science"), that "words
matter" and clearly the accounting program was using the terminology
wrongly, that this would be pedantry, I hope.
I will note that the question of multiple encodings is a very old
problem, and we've seen it with x.509 certificates, and with Kerberos
tickets, and many others.  It's one of the reasons why I am not very
fond of complex encoding schemes, such as ASN.1, when used in complex
cryptographic protocols.  Yes, such protocols are extensible, which is
wonderful from a protocol author's point of view.  From the point of
view of an attacker looking for mistakes engendered by all that
complexity, it is even more wonderful....

@_date: 2014-02-17 12:52:23
@_author: Theodore Ts'o 
@_subject: [Cryptography] BitCoin bug reported 
I agree that PER is probably better of a sea of bad choices.  However,
I still have a philosophical dislike of complex encoding schemes such
as ASN.1 which allow for arbitrary optional fields and optional data
structures, because it encourages protocol complexity.  If you have a
protocol message with a dozen different optional fields, the
combinatorics mean that you have at least 4096 test cases to write if
you are doing black box testing, and that's just testing the presence
or absence of the optional fields, ignoring tests based on the value
of those fields if they are present.
There is nothing in ASN.1 which requires such complexity --- you could
just use a very simple Tag-Length-Value (TLV) encoding scheme using
ASN.1, true.  But in that case, why not use a much simpler encoding
scheme to begin with?
In some sense, my dislike of ASN.1 is the same reason why I am
cautious about C++.  Unless you have a very strong C++ coding
guidelines which disables huge portions of the C++ language, the
temptation to use every last language feature, whether it is defining
the comma operator, or function overloading combined with large
numbers of class conversion functions where it can be verify unclear
which variant of the overloaded function the compiler will pick, is so
huge that C++ without a rigorously enforced C++ coding style guide is
a minefield.
Of course, Bitcoin managed to create an overly complex encoding scheme
without using ASN.1, which just goes to show that you can write
Fortran in any language.  :-/

@_date: 2014-02-17 16:46:56
@_author: Theodore Ts'o 
@_subject: [Cryptography] Encodings for crypto 
Have you looked at RFC 7049?  Paul Hoffman pointed me to it off-list,
like your I-D, it's closely aligned to the JSON model.  There is also
an Apache 2.0 licensed implementation available:
Sure, as long as people don't assume that (2) doesn't obviate the need
for (4).  Fuzz testing is a really good idea for any input data which
is from a potentially untrusted source.
No, it does **not** insulate the code from parser errors.  It just
means that assuming the attacker hasn't scrubbed your logs after
carrying out a stack overflow attack, you might be able to figure out
from the authenticated identity who was able to carry out an attack
against your parser.  :-)

@_date: 2014-02-21 12:48:44
@_author: Theodore Ts'o 
@_subject: [Cryptography] Random numbers only once 
Huh?  What are you including your sample set "uses of random numbers"?
And when you mean "crypto key", are you making any distinctions
between some random session key used for some bullshit TCP session
(which might or might not have any real security impact if it leaks,
other than a "let's encrypt everything to give the NSA a hard time)?
When you say "crypto key", are you including other cryptographic use
cases, such as IV's?
Given that most users aren't running monte carlo simulations, I'd
argue that 99% of the users of /dev/urandom on Linux *are* using it
for cryptographic purposes for one sort or another.  And if we move to
a world where we encrypt everything, then if we arbitrarily block in
early boot, users will *not* be happy, and device vendors will patch
out the code.
So from a practical engineering perspective, I'd much rather grab as
much stuff as possible which hopefully won't be easily monitored by a
remote attacker in Fort Meade (i.e., grab radio signal strength info
from the wifi and cell radios, etc.) and make the random pool be
seeded with as much stuff as possible which is hard for attackers to
guess (it would be nice to be able to prove with 100% certainty that
it is impossible for an attacker to compromise or guess, but I'd also
like to be the sole winner of the $440 million dollar powerball
loterry --- and I think the latter is more likely).
My compromise to this is to make /dev/random to be as conservative as
possible, and let it block, on the theory that there will be cases
where you are willing to block if the system doesn't think the random
poll is sufficiently well seeded.  However, I'm well aware that I will
be attacked from both sides --- on the one side, having something that
will block at all, and on the other, for having something which
application authors and device vendors will actually be willing to
And if anyone has any suggestions about how to influece or force ARM
SOC vendors to include a hardware entropy source, I'd love to hear
them.  I even have some friends at Linaro who might be willing to help
to bring some of these suggestions into practice.  Anyone have any
ideas short of compromising photographs of CEO's of companies like
Qualcomm and Samsung?  :-)

@_date: 2014-02-21 13:37:13
@_author: Theodore Ts'o 
@_subject: [Cryptography] Entropy Attacks! 
I'm willing to believe that the Intel RNG might be evil, since it's a
standalone component inside the CPU.  However, compromising the
execution engine such that you can snoop on memory, etc., would
require at least 10x to 100x more engineers to be able to figure out
that the CPU was doing something evil --- only one of which has to be
have the courage and bravery of Snowden to leak the information to the
whole world.
The other thing I'd note about "The Bernstein Attack" is that it
requires O(2**N) work to reduce the key strength by N bits.  That is,
suppose you have a 256 bit AES random session key.  Using his algorithm:
1. Generate a random r.
2. Try computing H(x,y,r).
3. If H(x,y,r) doesn't start with bits 0000, go back to step 1.
4. Output r as z.
Reduces the effective strength of the key to 252 bits, at the cost of
taking 16 times as long to generate the session key.  In order to
reduce the strength of the random session key to be 128 bits, the work
factor would be 2**128!
In other words, this is a straightforward brute force attack, where
you can shift some amount of the work from Fort Meade to the victim
CPU.  This is not what I would call exciting, since given the amount
of effort and the number of engineers you would have to suborn inside
Intel, and the corresponding risk of destroying one of US's larger
tech companies, and compare it to the possible benefit to the NSA, it
just isn't worth it.
The NSA could get much better bang for its buck by suborning someone
inside a certifying authority.  Give what we know of some of the
"mistakes" made by CA's, perhaps they may have done so already, and
those were really failed operations that couldn't be kept secret.

@_date: 2014-01-03 10:57:40
@_author: Theodore Ts'o 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Um, where are you going to get the true entropy from?  If you are
willing to assume that you can seed the device with true entropy, then
you can just use an AES-based CRNG.  Sure, it's not "provably secure",
but (a) it's not like we've proven that factorization is hard, only
mathematicians can't crack it to date, and if it doesn't, we're kind
of toast anyway since we assymetric crypto algorthms based on this
assumption all over the place, and (b) the exact same argument holds
for AES.  We use AES all over the place for our symmetric crypto, and
if AES is broken, then sure, the RNG is broken, but so is everything
The hard part is "seed the device with true entropy".   where does
that come from, and  do you trust the entity who seeded the
entropy, either against maliciousness (did the Apple really cooperate
with the NSA, or not), or simple outright incompetence (consider how
many manufacturers can't even get unique ethernet MAC addresses
I recognize that there are limits to software-based entropy collection
systems, such as that which is used in Linux's /dev/random.  But at
least the algorithm is one which can be openly analyzed and peer
reviewed.  How do you peer review whether or not Apple (or more
specifically, Foxconn) seeded your iphone with "true entropy".  And
even if you reviewed their processes at one point in time, who's to
say whether either the Chinese MSS or the US's NSA hasn't managed to
bribe an assembly line worker after your review of their manufacturing

@_date: 2014-01-03 13:01:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Sure, but a prng is not the only tool in the toolbox.  You can also
try to grab entropy from hardware, and from OS-level events.  Whether
you use Fortuna or Linux's /dev/random, or some other system which
tries to distill entropy from hardware level events, these are all
things which you can do instead of, or better yet, in addition to,
relying on some initial stored secret.
So for example, it's a really good idea to seed Linux's /dev/random
with some unpredictable randomness.  We save some before system
shutdown, and we reinitialize it with it on system startup.  But if
you are starting up a VM from scratch, initializing the seed file with
a secret is a useful thing to do.
It has a security proof *if* the primes chosen in an honest fashion.
What's not proven is whether the prims were chosen in an honest
There are ways you can do that; for example, it used to be the case
that the IETF Nomcom chair would select the people on the committee by
publically announcing the list of volunteers, and a list of ten stock
symbols in advance, and then use an PRNG based on the stock volumes
and/or stock prices on a pre-announced date in advance.  The
presumption was that IETF members didn't have the ability to
manipulate the stock market to a fine enough degree to force a
particular outcome, and since it was public, it meant that the Nomcom
chair could demonstrate to the world that he or she didn't pack the
committee with people of his choosing.
If the adversary can control the US stock market, or there is the
belief that there that the adversary could control the US stock
market, maybe you would need to use multiple markets.  Of course, this
would still be subject to manipulation by world-wide secret society of
the Illmuinati....
       	 	  	     	 	- Ted

@_date: 2014-01-03 14:49:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] Timing of saving RNG state 
It's such a good idea I recommened it almost a decade ago in the Linux
kernel sources.  :-)
 * When any operating system starts up, it will go through a sequence
 * of actions that are fairly predictable by an adversary, especially
 * if the start-up does not involve interaction with a human operator.
 * This reduces the actual number of bits of unpredictability in the
 * entropy pool below the value in entropy_count.  In order to
 * counteract this effect, it helps to carry information in the
 * entropy pool across shut-downs and start-ups.  To do this, put the
 * following lines an appropriate script which is run during the boot
 * sequence:
 *
 *      echo "Initializing random number generator..."
 *      random_seed=/var/run/random-seed
 *      # Carry a random seed from start-up to start-up
 *      # Load and then save the whole entropy pool
 *      if [ -f $random_seed ]; then
 *              cat $random_seed >/dev/urandom
 *      else
 *              touch $random_seed
 *      fi
 *      chmod 600 $random_seed
 *      dd if=/dev/urandom of=$random_seed count=1 bs=512
And it's such a good idea Debian and Ubuntu's /etc/init.d/urandom also
does this.

@_date: 2014-01-21 10:10:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] cheap sources of entropy 
My answer to this is to mix from as many sources as possible, in the
hopes that one or more of them can not be predicted by the attacker.
Yes, this may be less efficient, but that's engineering tradeoffs for
you --- and how many applications really *do* need 3 gigabits per
second of cryptographic grade random numbers?  :-)
The other thing I'd note is that I fear people are focusing more
attention on the random number generator and less on other parts of
the entire solution.  Maybe it's because of Parkinson's Law of Triviality[1]
[1] I'm not sure whether the RNG is better characterized as the "bite
shed" or the "coffee for refreshments" as described in Parkinson's 3rd
chapter, "High Finance, or the Point of Vanishing Interest", but I
know one thing for sure.  It's not the 10 million pound nuclear
Remember, the system is always going to be secure as its weakest link,
and having the most wonderful RNG in the world isn't going to help you
if the NSA has diverted your hardware and installed a miniature radio
transmitter into the guts of your system.  Or if you aren't using the
latest security updates, and worse yet, using PHP, and there's flaw in
your web framework that hasn't been patched or you don't know about.
(Some people have talked about using Own Cloud as being more secure
than cloud services from companies like Amazon or FaceBook.  Now, the
founder of OwnCloud the startup is a friend of mine, and I wish him
all of the best success in the world.  But the fact it uses PHP for
its web front end makes me shudder with fear.)
And of course, it might not be PHP; it might be security weakness with
your web server, or with your security assumptions that everything
befind your firewall is secure --- but then it turns out that your
access point is running a 2.4 based kernel to support an ancient
legacy binary-only blob, and it's been cracked to a fair-thee-well,
and then the attacker has used that to establish a beachhead from your
"smart refridgerator", and is then attacking your internal
infrastructure from there.
So don't get me wrong; having a good RNG is important.  But I do find
it interesting the volume of attention it is getting on this mailing
list compared to all of the other things that we have to get right in
order for the entire solution to be secure.  It may be my own personal
area of interest, but let's be realistic here.  If it's a hundred
times easier to break into a firmware update system and hide something
in your printer, or your BIOS, or your router, then maybe that's the
path which various criminal groups or other foreign intelligence
services (especially including those who are most likely much less
well resourced than the nSA) will use to screw you over.

@_date: 2014-01-30 20:09:31
@_author: Theodore Ts'o 
@_subject: [Cryptography] cheap sources of entropy 
Sure, the problem comes when you don't have any "truly unpredictable
sources".  Sure, if you have a "truly unpredictable source", you're
golden, but is it really unpredictable?  Maybe the NSA has leaned on
the sound board manufacturer which John Denker's Turbid generator is
relying on, such that even though you *think* you're getting Johnson
Noise, you're really getting something that has been very cleverly
gimmicked to pass all the statistical tests, but in fact can be
predicted by the NSA.  Or maybe the sound card has just failed in some
interesting way that the author(s) of Turbid hasn't anticipated.
And then in real life, if you have mass produced consumer electronics
devices, how can you be certain that the manufacturing tolerances of
the hand-assembled units made in the US will be the same as the
mass-produced units made in Shenzhen?
This is why there are those of us who believe that it is useful to
pull in sources which might not be "truly unpredictable", but
"unpredictable to an outside, remote attacker" according to some
threat model.
For example, I might have the threat model that says even though the
relative strength of all of the access points visible to my wireless
handset might not be "truly unpredictable", the remote attacker will
not know be able to (a) get a detailed enough survey of my real-time
radio environment, and (b) get a precise enough location within said
radio environment, that given the effects of radio path bouncing,
blocking to the absorbtion of the wooden table (ie., is the cell phone
on top of the table, or below the table in my knap sack), that there
will be some amount of uncertainty that will not be known to the
remote attacker.
This is the "sow's ear" which John Denker has so disparagingly
referred to.  It is _not_ truly unpredictable; instead, it's that it's
probably not be predictable to an attacker given a certain threat
model, and if you have enough of these sources, it hopefully adds
enough uncertainty such that the attacker needs to do more work than
just simply doing a brute force search of the key space.
The same argument holds true of using keyboard or mouse event timing;
again, it's _not_ completely unpredictable, but unless you believe the
NSA might have a camera trained on your keyboard while you are
generating your GPG key, hopefully (in combination with other
environmental sources being gathered by the OS) maybe it's "good
At the end of the day it's all about engineering considerations.  My
arguments that you might not be able to trust the sound card to be
producing "true" Johnson noise is also about questioning certain
engineering assumptions.  The concern that the NSA could really
reverse engineer all of the "sow's ears" that might go into a random
number generator which is using cryptographic mixing of "hopefully
unpredictable" sources, is also a question of what assumptions you
might or might not be willing to make.
The devil really is in the details, which is why debates like this
tend to go on.... and on.... and on....
I'll also note that we also don't need perfection; we just need to
make it harder than other attack vectors.  The old joke of "I don't
need to run faster than the bear, I just have to run faster than
*you*" applies here.
If we make the random number generator sufficiently hard to attack
that it becomes easier to carry out other attacks (i.e., attacking the
BIOS or embedded controller in your laptop, etc.) then (a) the
rational, determined attacker will probably turn their attention to
the easier attack vectors, and (b) it the rational defender should
adjust their engineering resources to defending against those other
attacks, instead of being so fixed on random number generation to the
exclusion of all else that we end up constructing the cryptographic
equivalent of the Maginot Line.  :-)

@_date: 2014-07-17 07:59:58
@_author: Theodore Ts'o 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
This is not just true for open source projects, but also for for many
major technology companies.  Companies like Google, Facebook, Intel,
IBM, etc., all are eagerly accepting interns from China, and/or have
development centers in China, despite the fact that it's well known
that the Ministry of State Security is trying their hardest to try to
penetrate into American data centers.  Despite this, it's just not
practical for companies to turn their backs on that much raw
engineering talent, even if some of them could potentially be plants
from the MSS.
The goal is to design systems so that even if you have some malcious
actors, that you have enough auditing and multi-person control systems
so that a single bad apple isn't going to be able to compromise
whatever you consider to be most critical data (i.e., PII data,
authentication/encryption related subsystems, etc.), and that any
misuse can be discovered during or after a compromise.  And then you
do lots of internal orange/tiger team attacks to make sure your
policies and procedures are as airtight as possible, and that any
holes that the tiger teams find are fixed.
After all, even if you do try to do all of the screening in the world,
do you really think you can stop all potential bad actors?  The NSA
wasn't able to prevent Snowden from being hired and given keys to the

@_date: 2014-07-26 14:57:28
@_author: Theodore Ts'o 
@_subject: [Cryptography] Browser JS (client side) crypto FUD 
I think it's a bit more complicated than you're making it out to be.
Ultimately, the nearly all of the software that people run come from
the network, at one time or another.  Even if you are using gpg
running on your linux laptop, where did you get your copy of gpg and
the Linux OS?  Odds are, you got it over the network.  But I don't
think that's necessarily a reason to throw up our hands and give up,
saying all is lost.  If your packaging system has reproducible builds,
then it becomes possible for independent audits to check out various
critical bits of code, and it becomes possible for people to determine
that the binary package which they installed corresponded with the
version that was audited.  And even if not everyone does this, the
fact that it is possible for spot checks to reveal a rat significantly
decreases the chances that an APT such as the NSA would be willing
potentially try the attack, lest they get caught red handed.
How does this apply to javascript cryptography?  Well, if you are
using something like a Chrome Extension, or a Firefox plugin, where
the crypto is executing in the browser context, but it is downloaded
once and then used multiple times, this becomes very similar the
version of a gpg getting downloaded once onto your laptop and then
used multiple times.  It's no more or less secure than a version of
gpg downloaded from your network and installed on your Mac or Linux
Like it or not, the vast majority of people are using some kind of web
based e-mail, whether it's GMail or Yahoo Mail or Hotmail, or
something else.  And if the crypto is being done in a browser
plugin/extension, the advantage is that if you downloaded the
plugin/extension *before* Google or Yahoo or Microsoft is served with
a search warrant or FISA order, you are far more secure than if your
data is stored on third party server for which you have no control and
for which if the search warrant is served against the third party,
they can be also served with a gag order not to tell you.
Could this be hacked?  Sure, just as Red Hat or Debian could be forced
to give you (and only you) an updated gpg package that had a backdoor.
But this is far more observable, and I suspect that Red Hat and Debian
would have for more legal ground to contest, and far more interest in
contesting in court, a order to knowingly install malware on a
customer's machine.  That is significantly different from simply
turning over information, and it also signicantly increases the
chances that the installation of the malware would be noticed and
traced back.

@_date: 2014-06-03 17:04:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
There's actually a bigger problem, which is users can't tell whether
or not a company has good security or not.  (Sure, they can tell after
a massive security failure, ala Target, but that's not the same
thing.)  So from a economic signalling perspective, which makes more
sense?  (a) investing extra money to improve the company's security
(whether this is by paying for more power and CPU cores to do
encrpytion even for internal RPC's, or to spend lots of software
engineering time doing code audits and redesigning vulnerable
systems), or (b) employing marketing specialists to make it _appear_
that your company has really good security, for those few customers
that will actually pay the switching cost and/or pay more for
additional security?
There's a reason why there is so much snake oil being sold by security
companies in the field, and it also explains the race to the bottom by
most CA providers.
Obviously, a company will want to invest just enough in security to
hopefully prevent the really embarassing security breach, but when you
consider how little the market has punished various CA's after some
truly spectacular failures, the economic choices that they have made
seem perfectly rational from a free market point of view --- just as
the long delay in implementing chip and pin for credit cards in the
United States was a perfectly rational choice from an economic point
of view.

@_date: 2014-06-03 18:14:17
@_author: Theodore Ts'o 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
Ross references the paper and mentions the problem in passing, but the
citation he gives goes into much more detail, and is worth reading:

@_date: 2014-06-06 10:15:36
@_author: Theodore Ts'o 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
The important thing to keep in mind is that most users are, in
practice, not willing to trade the prospect of a potential avoidance
of future pain due to a security exposure, with the imminent decrease
in functionality.  The reason why Noscript has adoption is that you
can whitelist sites you *want* to use that happen to require
The assumption that because Noscript has some amount of usage (but
mostly by more technical people who tend to care more about security)
that therefore people would be willing to deal with a wholesale
removeable of Javascript functionality, no matter that it might things
that sites that they *want* to use is not, I suspect, one that will
turn out to be a well-founded one.
If you at the same time can propose some addition a *functional*
extensions to substitute for desirable functionality that would
otherwise be curtailed by castrating some "dangerous" Javascript
feature, and those extensions would allow some highly desirable
functionality to be achievable, then maybe people would be more likely
to embrace it.  Otherwise, it will have as much mass adoption as, say,

@_date: 2014-06-17 19:14:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] Help please, 
The question of how the lifelong key will be (a) protected so an
adversary can't obtain the private components, but (b) available so
Alice and resign the subkeys periodically is to perhaps the trickiest
part of the design IMO.  If the private components are encrypted, then
the risk is that Alice will forget the password, especially if it is
but rarely used.  And if they aren't encrypted, then they will be
subject to being disclosed to an authorized party.  Sure, you can
break it apart using some secret sharing scheme, but how does that
scale?  And it doesn't solve the problem of how the individual
components are protected --- they are either encrypted, in which case
the problems are whether or not the passwords are strong, and whether
or not the passwords get forgotten, or they aren't encrypted, in which
case how do you protect them from being stolen?
Even if they are written on a piece of paper using a QR code, or some
such, the piece of paper still has to be protected somehow.  Do you
trust putting it in a safe deposit box?  Does your threat environment
includes the possibility of a court order demanding access to said
safe deposit box?
There are solutions, but they all involve tradeoffs.

@_date: 2014-06-20 12:41:05
@_author: Theodore Ts'o 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
The eMMC specification does have something called "secure discard"
which is supposed to guarantee the data does get erased --- that is,
those sectors not effected by the erase block get moved elsewhere, and
then the entire erase block is erased.
Of course, you have to trust that the eMMC device correctly
implemented secure erase, but that's true in general whenever you
delegate to a closed source implementation.  There is always the risk
that some security function will be incomptently or maliciously

@_date: 2014-06-20 15:05:02
@_author: Theodore Ts'o 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
I think you're beggin the question here.  I don't think there's any
good proof that FIPS certification is indeed better than nothing.
When you look examples such as the Taiwan's "Citizen Digital
Certificate" which used FIPS certified smartcards, but which *still*
had a crap random number generator, and Apple's "do not modifiy a
single line of this file, not even a comment or else we will need to
pay hundreds of thousands of dollars of certification fees", those are
two strong suggestions that in fact, FIPS may be worse than nothing.
The first showed that it FIPS certification did not have the benefits
you would think, and the second shows active harm done by FIPS
The net result of that is when someone complained to me that LibreSSL
removed FIPS compliance as a feature, my response was, "if this means
they are able to clean up their code faster, 'good riddance to bad

@_date: 2014-06-23 22:40:15
@_author: Theodore Ts'o 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
So if the audit is **perfect** then it doesn't matter.  The library
with the "goto fail" bug also passed the audit (just as the Taiwan
Personal Certificate hardware token had passed the FIPS certification
audit), and then the problem is the "don't even touch a line of this
code, not even a comment", pretty much guarantees that no developer
will look at the code after it has been audited, lest they result in
the company getting charged hundreds of thousands of dollars.
Of course, this doesn't stop the bad guys from looking at the code,
and finding entertaining problems like the "goto fail" bug.
So the question is does the audit actually do enough good that it's
worth freezing all further development activity on the library?  It's
true that further development could introduce bugs ---- but code clean
ups can actually find and fix problems, too.
There are no easy answers here, agreed.  But one would think that if
you've paid hundreds of thousands of dollars, and the code gets a
"pass", you should have some assurance that the code doesn't have
horrendous bugs in it.  If not, is it worth paying that money and
freezing any cleanup activity?  (Other than so you can sell into the
US Government market, that is....)
Well, the ideal situation is that the software validation would cover
a specific git commit.  That way futher validations could look at the
code changes since that particular git commit, instead of starting
from scratch.  And ideally, if the FIPS labs are going to charge
hundreds of thousands of dollars, they should be willing to pay a
bounty of say, $50,000 per security bug that they didn't catch, with a
trusted third party validating whether a security bug report was valid
or not.  And of course, they shouldn't charge to validate the fix.
And maybe the certification should be paid for by the insurance
company, with the companies paying insurance to cover the economic
damages for any missed security vulnerability.
But as it stands, the FIPS labs are basically a tax on companies that
want to sell to the US government, and presumably that means the
prices for selling into the US government market are jacked up
accordingly.  Which means the economic incentives are all broken, and
the people who end up getting fleeced are the US Taxpayers....
    	       	      	      	      	  - Ted

@_date: 2014-06-25 07:06:46
@_author: Theodore Ts'o 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
I have fairly reliable information that the answer to this question is
"yes".  What I am not sure about is whether any of these users are
doing so on public facing hosts that are exposed to the public
internet or not, and how bug-ridden those ancient FIPS-certified
versions might be.
If any one of them were public-facing, and some critical government
agency were to suffer a highly public security incident that was
directly traceable to a well-known OpenSSL bug that has since been
fixed in a mainstream, non-FIPS version of OpenSSL, maybe that would
help be a final nail in the coffin of FIPS certification..... but
probably not.
Unfortunately, I have my doubts that even that would be enough, even
if it resulted in the head of NIST getting dragged in front of the
House or Senate Intelligence Committees....

@_date: 2014-03-01 00:09:29
@_author: Theodore Ts'o 
@_subject: [Cryptography] GOTO Considered Harmful 
Git is actually quite paranoid if there is any changes in the
surrounding code.  It's not at all obvious to me that you could
construct a situation where an automated merge resolution would result
in the double "goto fail;" situation.
I could imagine that git flagged a merge conflict, and a human failed
to delete the extra line while fixing up a merge conflict, but in that
case, the changes needed to clean up the merge conflict, and any
deltas from the automated merge resolution, *do* show up if you use a
code review tool such as Gerrit (which is what the Android and
Chromium developers use; the production kernel team inside Google uses
Gerrit as well, so that all changes get reviewed by a second
I can't speak for other tools (does Apple use Git, or does it use some
other SCCS?), but it's hard to imagine how the use of git and Gerrit
for code review could allow an error like this to slip through.
    	 	      	       	     	  - Ted

@_date: 2014-03-02 20:50:42
@_author: Theodore Ts'o 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
You're begging the question about whether the "goto cleanup handler"
coding style is a bad coding style.  Yes, it was present for this
particular failure, but so was the fact that the author was right (or
left) handed.  Does that mean being right handed was a contributor?
I'll also note that Openssh uses the "goto cleanup handler" coding
style, and most people I think would consider Openssh to be (a)
cleaner code, and (b) less likely to contain security bugs than
OpenSSL.  If you plan to completely refactor openssh to use some other
coding style --- let me know.  I want to be in the bleacher seats,
eating popcorn, while watching Theo respond to your bright idea.
If someone would like to gather some hard data about whether
hopelessly deeply nested if statements is less or more error-prone
than using the "goto cleanup handler" style, that would be great.  But
until that work is done, what I see is a lot of assertions and assumptions.
There are plenty of things wrong with OpenSSL besides the use of goto.
For one thing, its API design is execretable, such that pretty much
any functional change is almost guaranteed to break ABI compatibility,
which gets quite painful if you are trying to use shared libaries.
Also, if you want to make the code to be more easily testable using
unit tests, this has to be taken account when you are designing the
So I'd argue that it wouldn't be a good use of programming effort to
refactor OpenSSL.  Probably better to take off, nuke it from orbit,
start from scratch, and design the API from the beginning to be easily
extensible and testable, and most importantly, write the tests while
you are reimplementing the code, from the ground up.
Testing is like security.  You can't just sprinkle the magic testing
pixie dust after you are done implementing, just as you can't (at
least not practically) try to add security after the fact, if it
wasn't designed in from the beginning.  (Or if you can, it requires
infinite budgets....)

@_date: 2014-03-03 10:18:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Those who can't do, teach.  :-)
If open source maintainers accepted every single patch that the
senders were convinced were great ideas, I assure the code would be
far buggier, and far less secure.  Trust me, I've seen the bright
ideas and (to their minds) brilliant patches people send; the vast
majority of them are terrible, and the one of the big reasons why
Linux's random driver and ext4 code is better is because I reject (or
ignore) the vast majority of crappy patches.
But hey, if you can convince everyone else to follow your rewrite of a
project, then it is open.  The problem is that most people aren't
willing to trust random people who claim that they can improve code,
or worse, they aren't willing to stand behind the code base, and send
"drive by patches" and aren't willing maintain the code over the long
term.  So the fact that most attempted code forks never gain traction
is a not surprising --- and, and I claim, it's a good thing.
And Peter Gutman's magic 30 pass secure HDD rewrite is still
applicable today.  Seriously, the work done in the 1970's and 1980's
is not necessarily valid for today's coding styles.  More importantly,
it's not enough to do work just saying that  is bad.  All coding
techniques have downsides.  This is why I said that the only way that
you're not doing speculation or argument by assertion is to take a
look at the number of bugs (security and otherwise) with alternate
techniques, since it may be that an alternate technique avoids one
class of bugs, but ends up opening the code to other classes of bugs.
It certainly wouldn't be the first time that conventional wisdom on
things like "Microkernels is the only way to write Operating Systems"
would turn out to be disastrously, terribly, wrong.  Sometimes when
academic theorizing runs into the concrete wall of actual pratice,
theory doesn't necessarily survive the impact with actual engineering
reality terribly well.
Nope, it just requires someone to start writing code, and
demonstrating to others that their code base is better.  Not just in
theory, but in actual fact.  The IETF mantra of "rough consensus and
running code" is really important here.  Especially the last bit of
"running code".
So this is the big difference the two links you pointed out.  Your
second one:
Was basically saying, Someone Else should do something!
And this is running code (both client and server), released under open
source, and there are even servers on the web which are usign this new
See the difference?
"You can't change people; you can only change yourself".
What's *your* strategy for making *your* life easier?  And if it so
happens that your strategy can also make the rest of our lives easier,
and you are willing to contribute some effort to make that happen,
even better.
One of the things which demotivates volunteers (and open source
projects are fundamentally very much like any other volunteer
effort), is other people telling them that their work suck, and
other people demanding that they be able to dictate how they spend
their volunteer time.
So if you tell me how I should spend my time, I'll tell you to f*ck
off.  If you send me small, easily reviewable patches, each one which
improves the code base, I'm much more likely to respond positively, or
at least tell you why in my opinion, it might not be considered an
improvement, and whether the patch can be changed so that it would be
acceptable.  (But note that I'm not going to demand that you change
the patch; only that this is the work that would be needed before I am
willing to accept the patch; the difference is subtle, but important.)

@_date: 2014-03-09 17:11:49
@_author: Theodore Ts'o 
@_subject: [Cryptography] RC4 again (actual security, 
That's not really a fair summary of Matthew's blog entry.  To quote
from his summary:
   "I realize none of the above actually tells you which AES
   alternative to use, and that's mostly because I don't want to
   legitimize the question. Unless your adversary is the NSA or you
   have some serious performance constraints that AES can't satisfy,
   my recommendation is to stick with AES -- it's the one standard
   cipher that nobody gets fired for using."
He was recommending salsa20 only if you have performance requirements
that can't be met by AES.  And given that many modern CPU chips have
hardware support for AES, including Intel, Arm, and Power chipsets,
presumably this mostly applies to people who need to implement
software on legacy CPU's.
And if you read Adam's blog post carefully, he added chacha20 as a
_fallback_ cipher.  Since it is different from RC4 and AES, that's
useful if you want something that will hopefully survive some new
cryptographic attack that is able to make RC4 or AES fall.  But that's
__not__ the same as saying that it's "the way to go".

@_date: 2014-03-14 10:10:51
@_author: Theodore Ts'o 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
But who is responsible for doing the update?  And what if the entity
which is capable of providing security updates is out of business, and
the source code for the firmware is not available?
For certain high security applications, you might also want a third
choice, which is a local management interface which is guaranteed to
always be able to reset the firmware to the uploaded state (which
means that the firmware loader must not be modifiable).
This presumes that access to the local management port is available,
and that someone is on the hook to (a) provide security updates, and
(b) apply security updates, of course.  But these are issues that are
of concern for the first two choices as well.
   	       	   	     	     - Ted

@_date: 2014-03-17 01:43:37
@_author: tytso@mit.edu 
@_subject: [Cryptography] Apple's Early Random PRNG 
ASLR of the kernel during early boot.  Sure, you could boot the
kernel, gather enough entropy, and then kexec boot again with a
fully-seeded RNG to do ASLR of the kernel text segment, but that gets

@_date: 2014-03-17 17:16:03
@_author: tytso@mit.edu 
@_subject: [Cryptography] Apple's Early Random PRNG 
If anyone has any suggestions about how to influence ARM SOC vendors
to provide something liek RDRAND, short of compromising photos from
web cams of company execs provided courtesy of GCHQ :-), I'm sure lots
of people would appreciate any ideas....

@_date: 2014-03-31 22:06:14
@_author: Theodore Ts'o 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
============================== START ==============================
There is a detailed design, code, and process hints available in
RFC 2777, "Publicly Verifiable Nomcom Random Selection", written by
Donald Eastlake which decribes how to do exactly this:

@_date: 2014-05-24 10:18:07
@_author: Theodore Ts'o 
@_subject: [Cryptography] The proper way to hash password files 
And until we can get something like the Fido initiative to create
something better than passwords, we're toast.  Because people aren't
going to memorize hundreds of passwords, and password managers such as
LastPass aren't used by enough people, and some bank web sites
actively try to make things password managers impossible to use by
disabling cut and paste and other ways that a password manager might
try to use to fill in the password field.  Thus encouraging users to
use the same password for E-bay and their checking/brokerage

@_date: 2014-05-27 11:42:40
@_author: Theodore Ts'o 
@_subject: [Cryptography] Langsec & authentication 
Furthermore, competent JSON readers do not parse JSON by exec'ing the
JSON as javascript, but rather by explicitly parsing it.  It's true
that a config language such as this:
can be parsed by a shell script using ". config", but that doesn't
mean that this is the only way, or even the advisable way, to parse
such an encoding!

@_date: 2014-10-11 20:38:55
@_author: Theodore Ts'o 
@_subject: [Cryptography] HP accidentally signs malware, 
Ah, I didn't realize that they didn't catch they had signed the
malware for a long period of time.  You're right though, having an
SSRL only stops new installations of the malware.  I was thinking of
the sort of "packagekit" situation where you might send someone
software with a particular MIME type that automatically trigges an
offer to download software to handle that MIME type, where you might
really want to block new installations of said malware.
This is all in the definition of what it means for a software to be on
the SSRL list.  Does it mean, "you're not allowed to run it" (a DRM
mechanism), or does it mean, "I strongly recommend you stop using this
code; upgrade NOW"?  One could imagine that if the software was
already installed, that on the periodic check, the checker would
display a pop box that printed some explanatory string that was
included in the SSRL, and then asked the user if they wanted to
continue using the software.
This would be much like the "Danger Will Robisnon" warning which
Chrome pops up when you visit a web site that is on the "is known to
try to download malware" list.  You can still continue on if you
__really__ want to visit that site, but there is a clear explanation
of why it is not a good idea, and what to do if you are the owner of
said web site.  I could imagine a similar dialog box which would
explain how to upgrade the software component, or perhaps offers to
automatically upgrade the software after the user gives permission.
So whether or not this makes headway is all in the UX design, I would

@_date: 2014-10-25 15:53:56
@_author: Theodore Ts'o 
@_subject: [Cryptography] Uncorrelated sequence length, 
If you have that much randomness, why do you need a cryptographic hash
to do the mixing?   Pretty much any mixing algorithm will do.
Note that even if the randomness isn't evenly distributed across the
4096 bits of the input entropy pool, we do use a secure cryptographic
hash to generate the output, so if you've added 256 bits worth of
uncertainty in the pool, it doesn't really matter whether it is
concentrated in one part of the pool or not.
      	  	    	   	    	   - Ted

@_date: 2014-10-25 17:18:52
@_author: Theodore Ts'o 
@_subject: [Cryptography] In search of random numbers 
You wait until the first time someone tries to connect to the ssh
port, and generate the ssh key in a just-in-time fashion.
So the thing about aslr and stack canaries is that if they aren't
perfectly random for the first boot, it isn't as catastrophic,
especially if you end up rebooting shortly after the initial setup.
But if you generate a bad SSH or SSL key, that tends to last for a
much longer period of time.
BTW, mixing in device personalization information (i.e., MAC
addresses) is useful for making it harder to prevent embarassingly
easy demonstrations that your system is insecure (because it prevents
using the GCD to find common factors after scanning for all certs from
various printers on the internet, for example).  But it shouldn't be
mistaken for truly fixing the problem.

@_date: 2014-09-13 20:40:52
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
That's simple.  The patch should be split up so that each patch
changes *one* thing.  Think of the Japanese concept of Kaizen, where
each change has to be independent, small, auditable, and individually
justifiable.  And note that whitespace and formatting is considered one
thing.  *Please* don't change whitespace on unrelated lines at the
same time when you are making other changes.
The worst thing in the world is when I get a 1000+ line change, which
changes lots of things all at once, such that resulting patch is
impossible to reason about or audit.  If it takes me more than a few
minutes to review it in an email window, it will generally get put on
a backlog queue, and it's no longer on the fastpath.  So keep the each
individual patches as small as possible in terms of lines of change,
and do the simple bits first.  If you can convince me to merge in the
first few patches, it reduces the total number changes that still
needs to make, and it tends to be good the contributor's morale.
Where as if I get a 1000+ line patch, I might finally get a chance to
look at it several weeks or months later.  And when I do, I will
generally look at them for a few design inspirations, and then pull
them out, and make those changes one at a time, as I have time.
(Basically, if you don't do this patch refactoring, I will; at my own
copious spare time.  :-)   And if it's not obvious why the change is
made, and why, I will simply discard them out of hand.
This has happened more than once in the past, and usually people get
hurt/angry/upset and challenge my intelligence, my integrity, and my
paternity.  That's OK.  But the change is still not going in.  :-)
P.S.  It may help if you think of what hints you might give an NSA
agent who is trying to contribute in a Kleptographic trojan horse into
an open source project.  How would you do that?  Bury the change in a
thousand line patch.  Change whitespace all over the space to further
obfuscate the deadly change.  Etc.  Then do the exact opposite.  :-)

@_date: 2014-09-13 21:03:10
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Funny thing; when many academics write papers criticize the Linux
random driver, it's often because they don't think we *still* don't do
a good enough job by their lights.
And then there is the camp who believes that once you gather 256 bits
or so of "true randomness", all you need to do is just crank a DRBG,
and the rest is all just mummery, and I get mocked/criticized/told I'm
stupid by that crowd too.
Which is OK.  Over time, you learn how to have a thick skin when
people deliver mutually conflicting criticisms.  :-)
But this is one additional reason why I tell people that every single
change should be broken apart, and individually justified.  Because
there are many different religions about generating random numbers,
and they often hold conflicting viewpoints about what is important and
what is not important.  So by splitting out each change individually,
and making the rule be that each change should not make the system any
worse --- understanding fully that different people who are reviewing
the code will have their own definition of when a tradeoff is "worse",
means that if we can satisfy everyone, then it's probably a good
change.  :-)
I'll also add that improving boot-time entropy is the hard part; and
it has nothing to do with changing the crypto.  It has to do much more
with passing good seed information from the bootloader, so we can get
high-grade entropy at the time when we need to do the kernel ASLR ---
which means the bootloader needs to read the entropy state out of some
NVRAM, or out of the file system, and pass it to the kernel early boot
code.  It means working with individual device drivers and hardware
manufacturers to be able to harvest data values which are
unpredictable to different classes of attackers.  (i.e., getting
signal strength and bit error rates from the various radios in a
handset; which might not be *secret*, but given that someone in Fort
Meade might know whether the cell phone in someone's knapsack is
located below the desk or on top of the desk, might give a one or two
bits of extra entropy, etc.)
This is all hard stuff, and it's a lot easier than replacing the hash
algorithm used by the output function, etc.  It's also much less
glamorous, and very architecture specific.  P.S.  Oh, and if anyone can get the ARM architecture folks to specify
a cycle counter which is guaranteed to be there, or at the very least,
won't crash the SOC if you try to use it and it's not there --- which
is why Linux doesn't try to use the cycle counter at all on most ARM
platforms, even though it's often present (it's just that it's not
guaranteed by the ARM architecture, and if you guess wrong, the
results are catastrophic) --- that would be awfully nice.  But again,
you'll see that all of the really important stuff has a lot less to do
about cryptography, and a lot more about engineering and politics and
negotiating with hardware vendors who are worried about shaving
millicents off of their BOM cost....

@_date: 2014-09-15 15:20:45
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Something to think about in terms of doing this as a very simple
change.  I've considered for a while the thought of using a
per-process key, lazily generated the first time a process tries to
read from /dev/urandom, and which hangs off the task struct, and which
gets released on task exit or on an exec, and which is *not* inherited
across a fork.
That way, there's absolutely no question that a heavy entropy user
from one process would influence the random number stream that would
be made available to another process.
We can keep the urandom pool for now for kernel-calls to
get_random_bytes.  In fact, it might be used to generate the keys for
the per-process entropy state.  In the future, we can potentially
change this to use something else; but this would be very simple to
reason about, and it's in fact what I think all users of /dev/urandom
should do in userspace, except for the problem of what happens in a
library when the application forks.

@_date: 2014-09-15 23:31:03
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
What I have in mind is much simpler.  The executive summary:
When a process tries to open /dev/urandom or tries to use the new
getrandom(2) system call for the first time, if the AES cipher is
enabled, grab a key and use it set up crypt_tfm structure which would
be hanging off the Linux's current (task_struct) struxture, using the
ctr(aes) cipher.  If this is successful, in order to generate N bytes
of randomness, encrypt using ctr(aes) a buffer filled with RDRAND (if
available) or all zero's (if not).
If the aes ctr crypto_blkcipher couldn't be set up, either because of
some kind of failure (most commonly ENOMEM), or because the AES cipher
is not enabled, fall back to using the existing urandom pool.
On a process, exit, exec, or fork, crypto_free_blkcipher(current->tfm)
gets called to zap the crypto context.
This has a couple of advantages.
(1) it will be much easier to audit than your thousands of lines of
(2) we can guarantee that it will be no worse than what we currently
have, if in the worst case we fall back to old scheme.
(3) we can take advantage of all of the hardware acceleration of AES
(i.e., using AESNI for x86, etc.) wihtout having to drag a huge amount
of complexity into drivers/char/random.c.

@_date: 2014-09-16 11:42:34
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
In all cases, if anything fails (including ENOMEM, the user not
compiling AES into the kernel, the AES crypto module failing to load,
etc., etc., etc.,) we fall back to the existing methods of using the
urandom pool.  I think I mentioned this in one portion of the
proposal, but this was supposed to be a universal fallback.

@_date: 2014-09-16 11:45:56
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
My thinking was to reseed the PRNG by regenerating the key
periodically.  How often is "periodically" is an open question.
This is hanging off the per-thread task structure, so there is no
synchronization problems.  This is the one advantage of doing this in
the kernel....

@_date: 2014-09-16 12:02:08
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
We need to be even more careful about the attacks.  The vulnerability
of using cipher-based DRBG is quite different from using cipher using
a data encryption system.  So just because a cipher is vulnerable to
various chosen plaintext attacks, differential cryptanalysis, etc., it
doesn't follow that a cipher which is being used as a DRBG would also
be vulnerable.
It's also worth noting that even NDRBG's use some kind of cipher as
part of the whitning process.  For example Fortuna uses some kind of
cipher: AES, Serpent, Twofish, etc.  RDRAND uses AES as a whitener.
The only difference is how much entropy is harvested from hardware
sources.  If ratio of entropy harvested hardware sources is >= output
of the system, then it's considered a NDRBG.  If the amount of entropy
is < the output, then it's considered a DRBG.  The only question then
is how often the DRBG is reseeded from hardware-derived entropy.
Currently we are using the folded output of SHA-1 of the urandom pool
(with the pool mixed between each read).  For all of the weaknesses
that people have of SHA-1, the mode that we are use it is not one
where the currently known attacks are applicable.  But for those
people who are nevertheless paranoid, would anyone like to seriously
argue that using an AES-CTR based DRBG would be significantly
cryptographically weaker?
(Not that I'm all that worried about the details of crypto-level
attacks, to be honest.  I'm much more worried about making sure we can
reliably harvest hardware-derived entropy, especially on embedded
Linux platforms such as the MIPS-based systems used in most home
routers, the ARM processors used on most handsets and tablets, etc.
People who are focused too much on the cryptography are missing the
forest for the trees, IMHO.)
And the processes that happen to use /dev/urandom today are generally
incredibly profligate already.  For example, until recently NSS used
fopen() on /dev/urandom, which immediately pulled 4k into a memory
buffer (and anyone want to guess whether the stdio buffer was properly
cleared afterwards?  I'll give you three guesses, and the first two
don't count.)
That seems reasonable.
That was in the original proposal.  The PRNG state would be cleared on
exit, exec, and fork (in the child thread/process).

@_date: 2014-09-16 13:15:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
I would think that in most cases, for each per-client request, they
would want to consume at least a session key's worth entropy.  In
which case it would be no worse than what we have today, at least with
respect to entropy consumption.

@_date: 2014-09-16 13:17:39
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Basically, because if I use ctr(aes) mode, then the Linux crypto layer
takes care of all of the deblocking issues when the userspace reads
something which isn't an exact multiple of the AES block size.  This
is me being lazy --- in the ctr(aes) mode, encrypting zero bytes is a
quick and dirty way of giving me a aes-ctr DRBG without having to do
extra implementation work.

@_date: 2014-09-16 21:48:09
@_author: Theodore Ts'o 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Sure, that would work too; and that would certainly be simpler on a
number of levels.  In that case I *would* have to worry about what
happens if the file descriptor is inherited across exec's, although so
long as there is proper locking, it wouldn't be catastrophic for
multiple processes to share the same DRBG.  But I suppose we could
argue that it should be CLOEXEC enabled by default.
We don't limit the caller to 256 bytes.  The OpenBSD getentropy(2)
system call does do that, and I was considering adding that at one
point, but after some discussion, we decided against following that
design for Linux.
This is a design question of how much should be done in userspace,
versus what should be done in the kernel.  One could argue that a DRBG
should be done 100% in userspace, and there is no need to implement
one in the kernel at all.  While I see the argument for why it might
be better to do it in the kernel (userspace applications will screw up
handling what happens across process or thread forks) --- I'm not
convinced the balance of arguments is in favor of adding that level of
creaping featurism, since 99.9% of the applications out there simply
only need a DRBG, and aren't interested (or are competent to specify,
in the case of the application programmers) how many bits of entropy
they need when they are filling a 4kb page --- which isn't a common
use case anyway.
Programmers tend to assume that all problems can be solved by giving
the user more knobs to twiddle.  But I think there is a real danger in
terms of the paradox of choice, combined with the "would *you* trust
the average application programmer to make intelligent decisions
vis-a-vis cryptographic security"?
I see this as a completely orthogonal system interface.  We could
arguement about adding a new madvise(2) flag that might have these
semantics, but it's quite separate from random number generation.  It
also has its own pitfalls; what happens if the user tries to mark all
pages in this fashion.  Since the functionality is a superset of
mlock(2)/mlockall(2), this can be quite problematic from a denial of
service perspective.

@_date: 2014-09-24 15:12:03
@_author: Theodore Ts'o 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
Are you asserting that politicians never ask for ridiculous things?
I'm not sure whether it was Dan Geer or Bruce Schneier who first made
the point that the Intelligence Community has been given the
impossible task, "Never Again" (in reference to 9/11).  But it's
fairly consistent with some of arguments put forward by General Keith
Alexander, who has justified pretty much everything using the "but
think of 9/11" argument.  Or General Hayden's "Give me the box you
will allow me to operate in. I?m going to play to the very edges of
that box."
Again, you seem to be assuming that the people who make the laws are
fundamentally interested in "good government", instead of say, only
worried about playing to whatever they need to do, including fear, in
order to get re-elected.

@_date: 2014-09-26 08:19:45
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
Indeed; the dispute is really about which problem you are trying to
solve.  It's much like the argument about opportunistic encryption (or
security, or whatever term people seem to like these days).
Both won't protect you against a targetted, determined MITM attack.
But both significantly discourage the use of bulk pervasive
monitoring.  If that is your threat model, then CT is certainly
useful.  In fact, CT is more likely to be more costly, at least from a
public relations / public exposure perspective than the use of a
targetted attack against opportunistic encryption, as CT is more
likely to result in the disclosure of an MITM attack after the fact
compared to merely using an unauthenticated diffie-helman exchange.

@_date: 2014-09-27 20:23:00
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
I've looked at the blog entry, and compared it to the Certificate
Transparency descriptions, and I believe that CT does add value.  Is
it a silver bullet that solves all problems?  Of course not.
The fact that anyone can run a log, and a certificate (or TLS session)
should include multiple SCT's, including some from trusted third-party
log services, means that if a government agency wants to order a CA to
issue a bogus cert via a NSL order, it would now need to send NSL's to
N log services so they could also issue SCT's and create forked Merkle
Hash Trees.  That right there increases the costs to the government
agency; and if the browser (client) policy included some rule that at
least one SCT must come from a log service has to be run in some
country that is outside of Five Eyes and NATO, that right there can
significantly increase the costs to the FBI.  (One could imagine a
system the client might require that for a particular server, it would
require SCT's from log services running in China, Russia, and the US.
If some organization like ISIS manages to piss off the US, China and
Russia, such that all three countries issue NSL's to the log services
running in their territories, one might argue that this organization
deserves everything that is coming to it.  :-)
Another thing which a laptop client could do is save all of the
certificates it has ever received, and periodically try to submit all
of them to one or more log servers whenever it comes up on some new
network.  Remember, the log servers will accept certificates from
anyone, so long as they properly chain up to a trusted set of root
CA's.  If someone tries to MITM a laptop user with a forged cert, it
will chain up to the root CA, so if the laptop even once manages to
come up on a network not controlled by the MITM attacker, and the
laptop manages to squirt certificates which it has seen out to one or
more log servers, then the covert MITM attack will no longer be
Even if not every client does this, if enough clients start playing
this game, it makes it very hard to do pervasive monitoring using MITM
certs.  Yes, it might not help against a targetted attack, unless that
laptop happens to be using a browser which is caching all of its certs
that it has seen and sending them up to the log server, but if someone
knows they might be subject to a targetting attack, they can use
client software which does this.
Note too that that the log sevices used by the client don't have to be
the same as the log services used by the CA's or by the web servers to
issue SCT's which are either embedded in the certificate, stapled to
the OCSP, or sent via a TLS extension.  These could log services run
by the EFF, and FSF/E in Germany, and so on.  Perhaps something more
sophisticated and/or efficient can be done by the
not-yet-very-well-defined gossip protocols referenced in the slides,
but something fairly basic could be done using the existing defined CT
Ultimately, if you are really concerned about a very sophisticated
attacker who is able to MITM every connection, it's unlikely such an
attacker would have the resources to do that with everyone, but this
is now really much more of a targetted attack.  And IMHO it's not that
important to make such attacks impossible.  If the attacks become more
expensive than someone simply performing a black bag job and bugging
the hardware to a fare-thee-well, then that's quite sufficient.
Because that's exactly what the FBI will do, instead of trying subvert
the CA and multiple logging services located in different countries
with National Security Letters.  :-)

@_date: 2014-09-27 21:33:08
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
If the client has a policy which requires one or more SCT's from
entities that come from legal jourisdictions that aren't subject to
NSL's, then in order for the MITM to impersonate those SCT's, it will
need to either gimmick those entities's certificate, or compromise
their private keys.  Either way, it makes the job of the MITM more

@_date: 2014-09-27 22:31:31
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
I don't see why using multiple SCT's from different legal
jurisdictions is a terribly onerous thing.  But I'm glad you agree
that it does clearly make life harder for the MITM attacker.
But if we set aside NSL's for a moment, consider the scenario of what
happened with Diginotar, where either Iran or the NSA (depending which
theory you subscribe to) hacked into Dutch CA --- no NSL was used, nor
would an NSL have worked, since Diginotar was based in Holland.
If we were using CT, and required SCT's from multiple entities, then
Iran (or the NSA) would have had to compromise multiple log service
entities in order to surreptitiously carry out a pervasive monitoring
attack, instead of just needing to compromise a single, apparently
badly run, CA.
P.S.  I am making no claims about how well or poorly other CA's might
be run by that last statement.  :-)

@_date: 2014-09-28 20:03:13
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
If that *everyone* is out to get you, there's very little CT or
anything based on traditional x.509 certificates can do, short of
exchanging keys PGP style in person, and maybe using PGP certificates
in a very tiny web of trust.
So if your threat model is one where everyone is out to get you, yes,
it's hopeless.
The whole point of something like CT is to expand the number of actors
that have to be malicious, and hopefully expanding to required set of
actors that have to be subverted by a National Security Letter to such
a point where the pervasive monitoring can no longer be a secret
(because someone will leak it), or hopefully, that at least some of
the required actors will be beyond the reach of a NSL.
And yes, it will be much harder to stop the US Government than the
Iranian Secret Police.  But CT deployed correctly can almost certainly
stop the latter trying to perpetrate a Diginotar-style attack, and
hopefully slow down the former so that the difficulty is at least as
hard as doing a targetted black bag job on the target's hardware.

@_date: 2015-08-28 13:04:40
@_author: Theodore Ts'o 
@_subject: [Cryptography] A thought about backdoors and quantuum-resistant 
I don't know if this is possible, because I don't know enough about
quantuum computing, and I don't know enough a about "quantuum
resistant encryption".
Suppose quantuum computing is a thing, and suppose NSA^H^H^H NIST
supplies us with a quantuum-resistant encryption algorithm.  Would it
be possible to create an encryption algorithm which is resistant to
quantuum computing --- except for someone with a quantuum computer
*AND* knowledge of some secret quantuum state stored in a quantuum
computer only available to the NSA.
Even more, would it be possible to create such a thing in such a way
that NSA^H^H^H NIST could introduce non-transparently in such a way
that the public world *thinks* that that the encryption algorithm
against all quantuum computers, but in fact there is a trapdoor that
only the NSA could utilize --- but no one knows this?
Of course, people wouldn't have to use the new quantuum resistant
encryption algorithms, but if quantuum computer were a thing, they
would be screwed if they kept on using AES, so the NSA would be quite
happy with that outcome.
And of course, if it was introduced non-transparently, then China and
Russia and Iran would be able to demand that a backdoor be engineered
for them, because no one would know that the backdoor existed.  And if
someone future Snowden leaks this, all of the current fear-mongering
from James Comey and Keith Alexander would help prepare the ground in
case it does leak.  (Or maybe they plan to introduce this
transparently, if they've learned their lesson from the Snowden
All of this is premised by the hypothesis that it is possible to
create quantuum-resistant encryption system for everyone but NSA, and
preferably (for the NSA) in such a way that it's not possible to
modify the encryption system so that backdoor can't be removed or
changed so that China and Russia could have their own
quantuum-backdoored encryption algorithm, and force companies who want
to do business in those countries to use their alternate-backdoored
encryption.   Is this possible?

@_date: 2015-08-28 23:14:17
@_author: Theodore Ts'o 
@_subject: [Cryptography] AES Broken? 
This would be a great paper to submit to one of those SPAM Conferences
in China --- to see if a paper that contains the following would get
accepted by their program committee:
    We see that the algebraic degree of the underlying transform is
    deg S  = 1.  This degree is very low, in fact lower than zero,
    the degree recommended by the Chinese Government (sinkhole
    transform). For reference, the U.S. National Security Agency
    recommends degree 1 (the identity transform) for all but the
    most confidential data. AES has been clearly designed to offer
    even lower security than these proposals against Algebraic Attacks
    of Courtois.

@_date: 2015-12-29 10:01:03
@_author: Theodore Ts'o 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
This isn't quite right, actually.
Most journalling file systems only journal *metadata*.  Now, we all
know the "it's only metadata" b.s. promulgated by the US National
Intelligence establishment is simply is not true.  For example, if you
have a file named "Death_to_America.txt", it will show up in the
journal since directory entries are metadata.  But journalling file
systems such as ext3, ext4, and xfs normally do not journal data
blocks.  (Ext3 and ext4 have a data journalling mode, but (a) this is
unusual; I'm not aware of any other file systems have this feature,
and (b) it's not normally turned on.)
Now, if you are using a copy-on-write file system, such as btrfs or
ZFS, then most data blocks are not overwritten, but are written to new
locations on disk.  (Both btrfs and ZFS do have a way of disabling
this feature because otherwise it would be a performance nightmare for
things like databases, but it defaults to on, and many people aren't
aware that they should really disable COW for things like mysql or
postgres or Oracle files).
P.S. The i_size and file access times can also leak a fair amount of
information, but realistically, if someone has block access to your
hard drive, and is doing that level of forensics on your file system
blocks, given that most people aren't encrypting data at rest, you
have much bigger problems.  Just doing secure delete on a few files
means that you're leaking all of the data in your _unencrypted_ files,
including your system log files, etc.

@_date: 2015-12-29 11:22:13
@_author: Theodore Ts'o 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Actually, ext2 in Linux has erase on delete.  It was disabled in ext3
because it's was more trouble to implement when journalling was
enabled, and no one cared enough to implement it.  Also getting it
right so that blocks get erased when only some of the blocks in the
file are released (e.g., after truncate or punch hole operation) is
With ext4 recently gaining the optional data block encryption feature,
it would be at lot easier to solve the problem for "erase on delete"
if this feature is enabled, by simply making sure the per-file key is
zapped on delete.  Of couse we still have to worry about a copy of the
per-file key still persisting in the journal and the somewhere in the
SSD's not-yet erased free erase pages.  And the caveats about blocks
still being recoverable after a partial truncate or punch hole
operation would still be available.
On the upside, given that blocks coming from different files are
encrypted with different per-file keys, it makes life harder for
people trawling deleted files and free pages in an SSD.  Also, if all
files belonging to one user (say the enterprise user account) use a
different wrapping key than another user (say, the user's personal
account), it means that if one temporarily loses control of a mobile
device (or one leaves their employer for another job), different
policies can be enforced such that only one of the users is remotely
erased using a remote mobile management tool.

@_date: 2015-12-29 11:54:36
@_author: Theodore Ts'o 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
John, can you expand how you think this behavior is *very* likely, and
how it can be induced?
Flash cells essentially hold a charge surrounded by a dielectric
material; it is programmed by applying a large enough voltage
potential to overcome the dieletric meterial, and it is read by
reading the voltage level in the charge bit.  This is why flash cells
have a limited number of write cycles.  As you write to flash by
applying a large voltage potential to force charge through the
dieletric material, this degrades the material so that the charge can
no longer be reliably held.
This is why TLC flash has much lower write endurance than SLC flash.
With SLC flash, you are storing a single bit of information per flash
cell, so the flash controller has to distinguish between '0' and '1'.
With TLC flash, you are storing three bits of information per flash
cell, which means the flash controller has to distinguish between
eight possible voltage levels.  Since over time, the charge will
dissipate due to flaws in the dieletric, and this increases as the
temperature increases, as the feature size of the flash becomes
smaller, and as the number of write cycles increases, it follows that
TLC flash is much less durable than SLC flash --- to the point that
flash manufacturers are starting to use ECC and "smart" flash cells
that try to take into account time elapsed and flash aging make a
better chance of predicting how the voltage curves of the charge will
change over time.
(Flash failure is defined by the probability reaching some threshold
value after a threshold amount of time when the cell is held at a
specific temperature; and SSD manufacturers refuse to publicize what
test values they are using when they make write endurace claims with
their products.  This is why I don't consider flash appropriate for
long-term storage of precious data.  Flash is only useful as cache as
far as I'm concerned; data on my computers isn't really safely stored
until it is backed up on spinning platters, or replicated on git
servers all around the world.  :-)
All of this being said, most ways of influencing flash externally will
more likely decrease the flash lifetime, so the data isn't reliably
stored for as long of a time.  Also, because reads are much faster
than time to program a flash cell, most flash controllers will do a
trial read after a write.  So if you can somehow cause a flash cell to
be "stuck" at a certain voltage level, the flash controller will
notice when it tries to program a new value.
If you can hack the flash controller, of course then you can do
anything you want.  This is the same problem as alleged ability of the
NSA to compromise the controller of hard drives and can thus introduce
malware into the controller hardware.  But in that case, you're not
really hacking the flash cell, you're hacking the controller, and
that's a different unsolved problem.  (Since it requires embedded
programmers to be competent at writing digital signature checks to
protect their firmware upgrade process, and controller programs are
generally considered trade secrets so it's very hard to audit it, it
really is going to be a very hard problem to solve indeed.)

@_date: 2015-02-02 10:02:19
@_author: Theodore Ts'o 
@_subject: [Cryptography] best practices considered bad term 
There seems to be a war of analogies going here.  Sure, sometimes
"best practices" are really good ideas are like Aviation Checklists
--- which, as the old saying goes, are written in blood, unlike
Aviation Manual, which are merely written ink.
But there are also those "best practice" for which we don't have
strong evidence to back up the author's opinion, and where following
the "best practice" leads to tradeoffs --- and the net effect of the
compromises is sometimes not clear.
Forcing users to change passwords every six months, and requiring at
least one number and one symbol, and to be at least 10 characters ---
a good idea, or a bad idea?  Telling everyone to use different
passwords for every single website inevitably leads them to use
password managers such as LastPass --- is that a net win or net loss
in security?
These sorts of "best practices" are very different from things like
"always use random IV's", or "don't reuse IV's in GCM mode".
So I think part of the problem is that people are talking about very
different things.

@_date: 2015-01-04 14:36:42
@_author: Theodore Ts'o 
@_subject: [Cryptography] 
=?utf-8?q?ything=3F?=
So I don't actually verify key _fingerprints_, but....
ssh-keygen -h -s ~/.ssh/cert_signer -I thunk.org -V +52w -n thunk,thunk.org -z 8 ~/.ssh/host-keys/thunk.pub
And in the sshd_config file:
HostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub
Works quite nicely, since I only have to install a single
~/.ssh/known_hosts_cert file in each of my laptops/desktop/ChromeOS

@_date: 2015-01-26 14:29:22
@_author: Theodore Ts'o 
@_subject: [Cryptography] The Crypto Pi 
At this point, the main justification I see for counting entropy is
(a) as a hueristic to decide when the entropy pool has been
sufficiently initialized in a cold boot / "fresh from the factory"
situation, and you don't have access to a trusted hw random generator
(where I'll ignore for now the question of how to define "trusted" ---
how much *should* you trust Intel's RDRAND or any random number
generator coming out of NIST/NSA after DUAL-EC, etc.?), and (b) as a
rate limiter when deciding how often to pull from a hw random number
generator, since there may be costs in terms of battery life, etc., to
pull from a hw random generator.
For (b) you could talk about this in terms of, "I'll fire up the lava
light generator after extracting N bytes from the RNG" if you want to
avoid using the word "entropy accounting", I suppose.  But the only
real difference is a question of how much you pull from your various
hardware entropy sources after you hit a particuar threshold.  Is it
one byte for every byte extracted?  Is it sufficient to simply pull
256 bits every five minutes?  Regardless of how much rng output you
are exposing to a potential attacker?
The other thing I'll note is that these days, it's considered
fashionable to look down on people who want to do designs based on
various crypto algorithms having unknown weaknesses.  After all,
someone like Snowden has promised us that used correctly, we can trust
the algorithms.  And *surely* that's not a false flag operation.  :-)
But in the early days of the PGP and Linux random number generators,
recall that SHA-1 was a gift from NIST/NSA --- just like DUAL EC was
--- and there was some doubt about what cryptographic guarantees
really could be assumed with those algorithms.  And back then, in a
world where the cryptographic core might not be 100% considered to be
trustworthy, and might be subject to engineered "NOBUS" attacks, using
entropy accounting really isn't that insane of an idea.

@_date: 2015-06-16 23:39:33
@_author: Theodore Ts'o 
@_subject: [Cryptography] Lastpass hacked. 
At least in theory, LastPass can be configured so this is true.  It is
*not* the default, since apparently users forget passwords and they
get seriously pissed when all of their encrypted data suddenly becomes
worthless.  LastPass can store the password recovery information on
their servers (in which case you are trusting them, seriously), and/or
in a hidden file on your local system (how many bets NSA has a module
that will steal said recovery secret from the user's Windows?), but at
least in theory you can tell them to configure all of the password
recovery features off.
Of course, then we are left with the problem that no one has audited
the LastPass extension to show that things really do work the way they
claim, such that if configured appropriately, the encryption key never
leaves your system.  Of course, if you haven't audited every line of
FireFox, and confirmed that the FireFox package which you downloaded
from your distribution matches the sources that hopefully *someone*
has bothered to audit, you're kind of screwed anyway....
(And I don't mean just audit for backdoors, but audit for stupid-sh*t
security bugs, as well, for which we've had entirely too many examples
lately.  So you need to keep a certain amount of context here.)
Personally, I **really** don't care if a hacker finds my New York
Times password, so such passwords I'm happy to store in LastPass.  But
for a much smaller set of critical passwords, I'll either memorize
them or use a manually encrypted file from which I'll refer to.

@_date: 2016-12-01 11:14:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] Is Ron right on randomness 
Huh?  It most certainly is in 4.2.0.  What were you saying about noise
on these threads being aggravating?
% git show v4.2:drivers/char/random.c  | grep getrandom
SYSCALL_DEFINE3(getrandom, char __user *, buf, size_t, count,

@_date: 2016-12-01 21:42:59
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL and random 
We do give such a an indication.   For example:
random: systemd: uninitialized urandom read (16 bytes read, 3 bits of entropy available)
It was a reported as a bug, and closed by Leonard Poettering:

@_date: 2016-12-02 12:40:43
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL and random 
The application can do this already using the existing API's.
int get_pseudorandom_hash(char *buf, int buflen)

@_date: 2016-12-06 22:24:08
@_author: Theodore Ts'o 
@_subject: [Cryptography] Is Ron right on randomness 
Yes, there is no getrandom in libc.  That's because glibc developers
are being.... glibc, as I said before.  There has been discussion
about maybe the kernel developers should support a libinux.a library
that would allow us to bypass glibc when they are being non-helpful.
In any case, your file compiles just fine for me:
% gcc -Wall -o /tmp/foo /tmp/foo.c
% /tmp/foo
bytes read = 32
% strace /tmp/foo 2>&1 | grep getrandom
getrandom("\310Ag\363\315\4\340\252\244\302\305\252\0319\230\3432\243\256\21\304\f\305\204\220\347\266e\324\243T\235", 32, 0) = 32
% find /usr/include  -type f | xargs grep _getrandom
% gcc -E /tmp/foo.c | grep bits/syscall
# 1 "/usr/include/x86_64-linux-gnu/bits/syscall.h" 1 3 4
% gcc -E /tmp/foo.c | grep unistd_
# 1 "/usr/include/x86_64-linux-gnu/asm/unistd_64.h" 1 3 4
% dpkg -S /usr/include/x86_64-linux-gnu/asm/unistd_64.h
linux-libc-dev:amd64: /usr/include/x86_64-linux-gnu/asm/unistd_64.h
% lsb_release -a
No LSB modules are available.
Distributor ID:	   Debian
Description:	   Debian GNU/Linux testing (stretch)
Release:	   testing
Codename:	   stretch
Maybe an Ubuntu bug; maybe Canonical was too lazy to update their
header files since Linux 3.17?   I dunno....

@_date: 2016-12-27 12:08:32
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
Technically, it's /var/lib/systemd/random-seed for systems using
systemd, and /var/lib/urandom/random-seed for older systems.  I think
you were generalizing based on a small sample set.
This is only good enough if you only care about user-space
applications that start up after the system is fully initialized.
It's not good enough for people who are worried about KASLR
(Kernel-level Address Space Layout Randomization), which happens in
very early boot, before the root file system or initrd is
mounted/loaded.  There are kernel people who are looking at this,
though, and the solutions tend to require architecture-specific
solutions (e.g., involving the bootloader, or trying to leverage the
UEFI BIOS, or device tree on embedded ARM systems, etc.).
Certainly during the installation process there is a lot of I/O, and
if you believe that this will result in uncertainty that won't be
known by an external attacker that can be harvested while copying
files from the CD and the network onto the disk drive or SSD, that may
be good enough in most cases.
This is *not* true.  Certianly not for debian, where the Debian
installer and Debian Live CD are two different things.  You can
install using the Debian Live CD, but there is also the Debian
Installer image which is faster and more flexible in terms of
installation options compared to the Devian Live CD.
If you are dependant on an installation image which you are
downloading from some central source, you might as well trust it to
generate a random key which can be appended to the end of the
top-level ISO-9660 file system for you.  And the reason why I suggest
appending it onto the *end* of the top-level ISO-9660 file system is
because that way an appropriately suspicious user can still easily run
a crypto checksum on the entire ISO-9660 file system image.  The .iso
file might contain an extra bit of random key that can be used to
encrypt a combination of the RTC plus the MAC address of the system to
be installed, so the program which calculates the crypto checksum
would have to understand the difference between the ISO-9660 file
system image versus the the .iso file, but that's achievable.
The rest is all engineering details.
Nah, just have the installer script look to see if there is a random
key at the end of the ISO-9660 file system, and if so, use it plus the
time from the RTC plus the MAC address(es) to create a random-seed
file file in the appropriate place.
In practice, getrandom(2) is fully initialized before the root file
system is mounted.  For example, on my laptop:
Dec 19 14:46:34 callcc kernel: [    0.000000] Linux version 4.9.0-00090-g3a45c5c (tytso at tytso-ssd) (gcc version 4.9.2 (Debian 4.9.2-10) )  SMP Tue Dec 13 04:59:15 UTC 2016
Dec 19 14:46:34 callcc kernel: [    2.563665] random: fast init done
Dec 19 14:46:34 callcc kernel: [   12.693192] random: crng init done
Dec 19 14:46:34 callcc kernel: [   13.039601] EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts: (null)
OK, so this is a little unfair since my root file system uses LVM on
top of dm-crypt, so I had to type my passphrase before the root file
system could be checked, let alone mounted.  But in that scenario, in
practice getrandom(2) *never* blocks for normal userspace programs.
So as a more fair demonstration, let's use a KVM system (which has
even less entropy available to be collected than a physical laptop):
[    0.000000] Linux version 4.10.0-rc1-12302-g7ce7d89f4883 (tytso at callcc) (gcc version 5.4.1 20161202 (Debian 5.4.1-4) )  SMP Mon Dec 26 09:54:17 EST 2016
[    4.363243] random: fast init done
[    4.366114] random: crng init done
[    5.073506] EXT4-fs (vda): mounted filesystem with ordered data mode. Opts: (null)
So in practice, non-boot-time callers of getrandom(2) can and should
assume that it will never block.  If it does block, it means something
has gone terribly wrong.  During boot-time, some care may be needed,
on some classes of hardware.  But to the extent that your analysis
assumes that the random number generator will be initialized from the
random-seed file during the boot sequence, that assumes the root file
system is mounted and before that point, the random state can't be
guaranteed.  So even in your design, you are assuming that some care
needs to be taken for programs running during the boot sequence, some
of which may be happening before, or in parallel with, the root file
system being mounted, and the random-seed file being used to
initialized the random pool.
The design goal of for /dev/urandom and getrandom is to make sure that
by the time the boot sequence is complete, in practice both will be
fully initialized, and for the most common VM and PC class hardware,
that they be initialized before the root file system is mounted.
This doesn't completely solve the problem for KASLR, but for the class
of problems most people have been wringing their hands over, it does
take care of most of the cryptographic concerns that people might
The only reason why getrandom(2) blocks is as belt and suspenders for
those people who are concerned about what happens in the case when
over.  (And for insane designs that generate long-term public keys
during the boot sequence, of course.  Ssh init scripts as commonly
deployed on many systems, I'm looking at *you*.)  But on most systems,
we try to make sure we can grab enough uncertainty that won't be known
by an external attacker so that we are reasonably proof against these
sorts of attacks by the time the root file system is mounted.
I certainly agree with this.

@_date: 2016-12-27 19:18:31
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
It's a lot more complicated than that.  For one thing, the complex
internal structural blocks includes an initial decompressor and the
offsets in System.map refer to text and data segments inside the
compressed portion of the kernel image.  So you can't use System.map
to find the offset.
The bigger issue is that modifying the kernel image will cause a
number of problems.  For one thing, it would break the checksum used
by package managers (e.g., "rpm -V kernel-core").  This will also
break signed kernels used by UEFI secure boot, and it will also break
remote attestation using TPM (which has been proposed for to prove
that software in voting machines hasn't been tampered with).
So while it would simplify certain things to store the random seed in
the kernel image, it would also break a number of things.  The fact
that it would break booting on systems that enforcing UEFI secure boot
probably means that Linux distribution wouldn't find this approach to
be particularly attractive.

@_date: 2016-12-28 17:30:39
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
Historically the boot-time argument isn't considered a secret.  So
it's easily visible in grub (since you are allowed to edit the kernel
command-line arguments when you interrupt the grub boot sequence ---
so if you have physical access to the console, it's trivial to get
access during the boot sequence).  It's also stored in
that could be changed).  And finally, the command-line argument for
the kernel is available in /boot/cmdline (which is world-readable by
default).  It might also be available in /sys somewhere if you are
using kexec, etc.
So it's doable, but this is why I've been favored changing the
bootloader and passing randomness via an extension to the Linux/x86
boot protocol:
The other thought was to use a UEFI Boot variable to store a key
(which could then be used to encrypt the RTC clock, if that's
considered trusted and valid), since UEFI Boot variables become
inaccessible after the UEFI boot mode is exited in the kernel (which
happens very early in the kernel startup process).
Both of these are potential solutions to having secure randomness
early enough in the Linux kernel boot sequence that it could be used
for KASLR.
The disadvatnage of these solutions is that we would have to create
another scheme for use in Arm64 processors used in Android mobile
handsets (for example), since Android doesn't use UEFI or grub.  But
with Android, you can't change the kernel command-line unless you are
willing to unlock the bootloader, and that opens your phone to an
entire different set of attacks if your phone is seized as you cross a
As with many engineering problems, life is full of tradeoffs.  And how
you value the benefits and evaluate the costs of the tradeoffs may
cause you to prefer one solution over another.

@_date: 2016-12-28 19:39:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
Not true; this is not a problem.  Since the very beginning, this is
what I recommended for system init scripts.  From
 *	echo "Initializing random number generator..."
 *	random_seed=/var/run/random-seed
 *	# Carry a random seed from start-up to start-up
 *	# Load and then save the whole entropy pool
 *	if [ -f $random_seed ]; then
 *		cat $random_seed >/dev/urandom
 *	else
 *		touch $random_seed
 *	fi
 *	chmod 600 $random_seed
 *	dd if=/dev/urandom of=$random_seed count=1 bs=512
What this means is that the random_seed file is rewritten during the
boot sequence.  It won't necessarily have much in the way of new
entropy, but it is cryptographically secure in that if you believe the
attacker didn't have access to the prior contents of the random_seed
file, and if you believe the cryptographic algorithm used in the crng
(SHA1 or ChaCha20, depending on the version of the kernel), the
attacker won't be able to predict the new contents of the random_seed
As long as you had one clean shutdown --- for example, if you take a
Nexus or Pixel handset fresh out of the box, it almost certainly will
be taking a security update and then rebooting, that will be a clean
shutdown, and there should hopefully be uncertainty that will be hard
for an attacker to reverse engineer accumulating in the entropy pool
while downloading the update and then installing to the flash device,
and then the clean shutdown will initialize random-seed.
Another useful thing you could do in userspace is to use the username
and password of the user signing into the device, and mixing that into
the entropy pool.  And certainly userspace can decide to update the
random_seed file at times other than at clean shutdowns.  So having
userspace initialize random_seed after communicating with the user,
which hopefully will have some uncertainty available during setup
process, is certainly quite possible.  On a Debian system, you can do
this very easily simply by running the command "/etc/init.d/urandom
It's true that for specific embedded systems, you may need to do
something special, but that's always been true.  Especially an IOT
where there is no user I/O and where it blindly believes any initial
connection over the network as coming from the owner.....  but I'd
argue that cryptography and random entropy initialization is the
_least_ of your problems.  The bigger problem is that by that between
the time that the IOT firmware was flashed, and when the device was
delievered to the user, there are probably *already* publically known
security vulernabilities, and if the IOT device manufacturer isn't
providing monthly security updates, why worry about random entropy
initialization?  That's not how the hackers are going to break into
your Smart Home....

@_date: 2016-02-19 09:32:54
@_author: Theodore Ts'o 
@_subject: [Cryptography] Thoughts on the Apple iPhone fiasco 
Even for a system where the door is "slammed shut", what if in the
next case, the FBI serves Apple with a warrant in a secret court
demanding that the All Writs Act gets used to record the passcode when
the (alive) user enters it, then squirts all of the data over the
network to the FBI?
Resisting in this case would be good, even if all it does is that it
forces to FBI stipulates that it only applies for this very narrow
fact pattern --- sort of like when the Supreme Court stole the
presidential election, they were so embarrassed by what they did that
they had to explicitly disclaim that this should be considered a
binding precedent for future cases.

@_date: 2016-02-21 13:32:40
@_author: Theodore Ts'o 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
The biggest problem with separated metadata is the atomic update
problem.  What if you've updated the disk block, but not the
authentication/integrity metadata?  Or vice versa?
The challenge here is economic.  You can get disk drives that have
enough space for checksums; there is a SCSI standard for adding a Data
Integrity Field.  This was intended for end-to-end checksums, so the
disk will verify the checksum from the DIF field when the block is
read from spinning rust.  But one could imagine doing something very
much like this for crypto checksums where the disk drive would keep
its grubby little paws off of the authentication tag field.
The problem is $$$.  Drives with DIF/DIX are only available on
enterprise-grade SAS drives.  Which are so expensive that even the
major cloud companies tend not to use SAS drives, because they're
priced rapaciously.  If the HDD vendors had been smart enough to price
DIF/DIX as a only a tiny price premium then OS's would have started
using DIF/DIX, and then there would have been demand for such a
feature.  But without the demand, it's only a speciality / niche
feature, and it gets priced accordingly.
So not only do we have to consider cryptographic support from the
ground up, we have to figure ways of solving the demand side of the
equation.  I consider the lack of DIF/DIX in SATA grade drives to be a
market failure, and I can forsee the same sort of dynamic making
non-power-of-two storage devices as also a niche feature.  So short of
some kind of government mandate, or big-company mandate (we are
willing to purchase gazillion eMMC flash drives that have a block size
of 4160) I'm a bit pessimistic that this kind of redesign from the
ground up will happen.

@_date: 2016-02-22 20:42:16
@_author: Theodore Ts'o 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
It's not obvious that the equivalent of CALEA would pass today's
congress.  If we have another 9/11 event, it might pass in the
immeidate aftermath of such an event.  But there have been attempts to
propose such a law already, and it's getting some surprising
resistance from various congress-critters.  I'm sure there are various
silicon valley companies that have been priming allies for this
specific eventuality, which was going to come up sonner or later even
with out this particular All Writs Act based court fight.
     	      		     	       - Ted

@_date: 2016-02-23 19:50:21
@_author: Theodore Ts'o 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
For update-in-place file systems, you can update the data block
without updating the metadata.  For copy-on-write file systems, yes,
you can only update the data block by allocating a new data block,
writing the new data block, and then updating the extent map.  But
then you have to allocate a new block for the extent map, etc., all
the way up to the root.  Excamples of such file systems include ZFS,
btrfs, log-structed file systems, etc.
However, the performance of such file systems on random write
workloads, such as a typical enterprise database, are disastrous.  So
in practice, these file systems generally have a mode where you can
disable COW and you can update database's tablespace files using an
update-in-place write, without updating the metadata.  (ZFS is owned
by Oracle, and has this feature.  Coincidence?  I think not.  :-)
This is called torn writes, and it's a very well known problem.  It
happens when the disk sector size is 512 bytes, and the block size is
4k.  In this case, yes, you can have torn writes on a crash.  To
protect against this, enterprise databases have per-block checksums
built into each page (so a 4k page might only have 4088 bytes of real
data, with 4 bytes of CRC and 4 bytes of blocok number -- to protect
against blocks written to the wrong place on disk).
Journalling file systems will often use checksums in the commit block,
and will discard the last commit as being incomplete if the checksum
doesn't check out.
With advanced format disks where the physical size is 4k, and the unit
of ECC is 4k, the disk drive *does* guarantee that each 4k write will
be atomic.  The bigger problem is in order to have more efficient
writes, the HDD vendors are proposing going to 32k or 64k physical
block sizes, where the unit of atomic write is 32k or 64k.  This means
that 4k writes will now require a read-modify-write cycle, or the
operating systems will have to go through significant changes so that
they can support VM page sizes smaller than the file system block
One observation I recently made just today (I'm at the FAST
conference), is that one of the problems with trying to do file
systems with encryption or cryptographic data integrity is that many
storage engineers don't understand cryptography, and many crypto
experts don't understand file systems and storage issues.  Fortunately
with ext4 encryption I've been collaborating with Michael Halcrow (who
designed bitlocker for Microsoft) who does really understand the
crypto stuff, and we have access to some of really high-powered
cryptographic experts who happen to work at Google....

@_date: 2016-01-01 12:40:57
@_author: Theodore Ts'o 
@_subject: [Cryptography] Write-protect switches, etc. 
DVD-R drives work fairly well as WORM drives.  Certainly better than
paper printers.  Yes, there is the block size chunking issue, but this
can be solved by using a separate logging server located in a DMZ
which is connected using either a dedicated network link, or, if
you're really paranoid, an serial line.
This strategy is suitable for both logs and things like git
repositories, and is being used in production for certain critical
open source infrastructure sites on the net.

@_date: 2016-01-20 16:05:50
@_author: Theodore Ts'o 
@_subject: [Cryptography] TRNG related review: rngd and /dev/random 
Sure, but note the caveat that the attack has to see the results of
"every read to /dev/urandom".  This might be true for IV's, but it
would not be true for session keys, unless you have much, much larger
problems.  The moment something unknown to the attacker is pulled ---
say, a 128 bit session key the number of possible entropy pool states
that the attacker would have to validate explodes, and it becomes
roughly equivalent to a brute force attack of said 128-bit session
key.  (Worse, actually, since we throw away half of the bits so the
trying to maintain knowledge of the entropy pool sate after extracting
128-bits not known to the attacker becomes equivalent to a 256 bit
brute force attack.)
       	       		       	   - Ted

@_date: 2016-01-21 20:41:22
@_author: Theodore Ts'o 
@_subject: [Cryptography] TRNG related review: rngd and /dev/random 
The real solution for Linux is to just user getrandom(2) and be happy.
With the flags set to zero, it works just like OpenBSD's getentropy(2).

@_date: 2016-05-23 11:35:56
@_author: Theodore Ts'o 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
I agree with this, and there are ways in which this can be useful ---
for example, using the relative strength from multiple access points
to seed a random number generator may be useful because the NSA
analyst sittiing in Fort Meade might not know whether the mobile phone
in your knapsack is sitting on top of the desk or below it, and this
would change the RSSI numbers that you might get.
However, I do worry about this a bit to the extent that sometimes
don't know what we don't know, or more importantly, we don't know what
the adversary might be able to find out.  For example if you read the
claims made by the CPU Jitter "True Random Number Generator", it
essentially (albeit perhaps slightly unfairly) boils down to "The
algorithms L1/L2 cache of an Intel CPU are horribly complex, and no
one can figure them out, so we can treat the timing results as true
Well, maybe you and I can't figure them out, but maybe someone with a
more detailed understanding of the implementation details of the Intel
CPU could do a better tjob.
So while I think it is a useful engineering tool, and it's something
I've relied upon myself, to use it as a fundamental design principle
could be dangerous.

@_date: 2016-05-25 22:25:26
@_author: Theodore Ts'o 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
Right but what are you measuring that CPU clock against?  In the
absence of interrupts if you are running something in a tight loop,
and then periodically sampling the TSC, then if there is no jitter,
the only thing which is unknown is the starting offset of the TSC.  So
maybe that's ten bits of entropy.  But that's *all* which is
unknowable.  Running the jitter "true random number generator"
continuously isn't going to change how bits of initial uncertainty ---
just how many bits you've extracted out.
Keep in mind that on many hardware implementations, there is only a
single crystal-controlled oscillator, and all of the clocks are
generated by using various divide by N circuits.  So you own't even
get any uncertainty caused by two different osclliators beating
against one another.
Now, if you have interrupts, then you may have additional bits of
uncertainty.  But that's not the claim of the jitter true random
number generator.  The claim is that you can run in a tight loop, and
continuously generate lots of high-quality, "true" random numbers.

@_date: 2016-11-22 22:55:49
@_author: Theodore Ts'o 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
Another way about thinking of it about it is that we're not worried
about random (so to speak) failures, but rather about the concern that
some untrustworthy advisary has intrudced a NOBUS back door into the
component.  Let's assume that the NSA has introduced a backdoor into
DUAL-EC.  And let's posit the possibility the KGB has a deep
penetration agent at Intel, who has introduced a NOBUS backdoor into
RDRAND.  And let's assume that my laptop has a TPM chip, made in
China, has a NOBUS into my TPM module courtesy of the MSS.
So the question is what do you get if calculate
Well, it's certainly not "random", in the formal information
theoretical sense.  But from a _practical_ sense, if you assume that
the NSA, KGB, and MSS will never collude and/or admit to each other
that they introduced a NOBUS vulernabiliy into DUAL_EC, RDRAND, and a
TPM module, it might be _practically_ random.
So this is a different type of anti-corrleation protection, since for
cryptographic systems, we have to worry about not only failure in the
engineering sense, but also deliberately introduced NOBUS
vunerabilities --- which if they exist, aren't necessarily "failures"
from the intelligence agency's perspective...  :-)
   	     	       	       			- Ted

@_date: 2016-11-23 22:28:38
@_author: Theodore Ts'o 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
Well, yes.  I was trying to keep things simple.  The other thing trick
is that there may be things like inter-packet arrival times on the
local ethernet, which may be observable to someone on the local
segment, might not be observable to an analyst sitting in Fort Meade
who hasn't managed to install a device on your local network.  And
even if it is, the local Russian Mafia hacker might not have access to
it.  So mixing in things like Squish_network_timing doesn't hurt, and
it might help.
If you are designing the whole product, and you have complete control
over the supply chain and manufacturing process, *and* you are sure
that you have control over the completed product from the assembly
plant to where it is deployed (so a friendly neighborhood NSA TAO team
hasn't intercepted the product while it is being shipped), then John's
approach can work.
But otherwise, ***any*** hardware random number generator is almost by
definition "squish".  This includes things like Altus Metrum's
"ChaosKey" product[1], for example --- how do you know that you can
trust the ARM SOC on the sucker?  And I say this while having the
utmost respect for Keith Packard and Bdale Garbee.
[1]

@_date: 2016-11-23 22:58:05
@_author: Theodore Ts'o 
@_subject: [Cryptography] RNG design principles 
I'm not going to be vehement about it, but there is the question of
whether or not you believe the system should be able to recover after
partial or complete exposire of the internal state of your CSPRNG or
not.  One approach, which is the purist's approach, is to say if
someone can compromise your system even a tiny bit, all bets are off,
and you might as well go home.
Another school of thought is to say that building a resilient system
is good, and so periodic reseeding from hardware sources of randomness
is a good thing.  Whether or not this is a denial-of-service attack
depends on whether or not you believe it's worthwhile to have such
self-healing properties or not.

@_date: 2016-11-24 11:01:25
@_author: Theodore Ts'o 
@_subject: [Cryptography] RNG design principles 
My attitude is that is that we need to consider all of these things as
*possible*, and then weigh the costs and benefits of the attacker
trying to use a particular vector, versus the costs and benefits of
trying to defend against that particular vector.  So for example,
mixing multiple sources that may introduce difficulty to potential
attackers from being able to predict the numbers generated from those
sources doesn't hurt, and might help, and is relatively cheap.  Yes,
we need to mix them such that if there is unanticipated correlation,
that it won't make things worse, but that's not that hard -- for
example, you can just use a cryptographic hash (which you would be
relying upon for your whitener, anyway) to do the mixing.
Does this slow down the random number generator, potentially?  Sure!
But I seriously question the claim that you *need* gigabytes of
cryptographically secure random numbers per second in the first place.
No, but we *would* consider nesting them with different keys.  e.g.,
Why don't we do this?  Because it's slow, and unlike a CSRNG, where we
don't need bulk random number generation, we *do* want to be able to
do bulk data encryption.  So it's a question of whether the costs are
worth the potential benefits, and how much trust do we put in a
particular cryptographic algorithm that *wasn't* designed by the NSA
using secret criteria.
Does that mean that we need to potentially worry about backdoors in
keyboards?  Sure --- see the Jitterbug[1] paper.  Do we need to worry
about backdoored HDD firmware?  Yes[2].  And there are countermeasures
that can be used by both a careful cloud provider and by HDD
manufacturers to protect against this, and trust me, work *is* being
done to try to defend against these sorts of things.
[1] [2] But again, it's a matter of costs and benefits, both for the attacker
and the defender.  Things like keyboard bugs and HDD firmware attacks
are fine for an attacker interested in doing a targetted attack.  But
there are some real limits as to the practicality for using such
attacks for bulk trawling expeditions, including the fact that
keyboards and HDD's don't have network interfaces and thus can't
"phone home" for instructions or assistance --- and the SOC's used in
HDD's are relatively weak.  But yes, we *should* consider this, and
use layered models of security to try to make life as hard as possible
for the attacker.
And that's really the key for why mixing from multiple sources is a
good thing.  Think of it as layered security; defense in depth; belt
and suspenders.  So you use FDE on a hard drive, *and* you encrypt at
the cluster file system level, *and* you use application specific
encryption, *and* you encourage the user to use end-to-end encryption,
precisely so that if one layer has a weakness, you can hopefully limit
or eliminate the exposure caused by that weakness or security breach.
RNG design should be no different.

@_date: 2016-11-27 12:29:57
@_author: Theodore Ts'o 
@_subject: [Cryptography] RNG design principles 
One approach which might work is if you have some way of storing a
static secret --- for example, UEFI has a way you can store a UEFI
variable where access is removed after boot services are teriminated
--- and a reliable realtime clock, or some kind of guaranteed
monotonically increasing counter.
If you had that, then you could encrypt the clock or the timer by the
secret key in early boot, before the Linux kernel has terminated the
UEFI boot services, and use that to seed the CSPRNG.  This could be
done before KASLR kicks in, and once the early boot services are
terminated, even if ring 0 protections get violated, you won't be able
to get access to the secret key.
The secret key could be regenerated each time the system is installed
or the kernel gets upgraded --- the UEFI variables are typically
stored in NOR flash or some other non-volatile storage that doesn't
allow for a large number of write cycles.  That's why we can't store
the counter using a pre-boot UEFI variable.  But it should be good
enough that we can regenerate the key each time the kernel gets
installed, even if there is a security upgrade every month, ala the
Android security release model.
This doesn't completely solve the "out of the box problem", of course
--- but past a certain point, you really do want an on-chip RNG
peripheral, and fortunately, most x86 systems will have a TPM chip.
The trick is what to do with those ARM systems, and there we're at the
mercy of the SOC manufacturers.
The kernel command line is available at /proc/cmdline, so it becomes
visible.  Also, /boot/grub/grub.cfg is also normally a publically
readable file (although that could be fixed).
Since you have to change the kernel to prevent /proc/cmdline form
leaking the key, you might as well pass it as a binary extension to
the boot-time protocol, which is how the boot loader passes the
command line and initrd information to the kernel.  For an example of
the x86 boot command loader, please see:
This is something that I've thought about quite a lot, but I haven't
had time to really try and execute on it.  The bottom line is that the
step requires changes in a multiple components --- in the kernel, the
boot loaders, the distribution installers, etc. etc.  The kernel piece
isn't too hard (at least for me).  Making changes to grub, the distro
bootloaders, etc. --- that's what makes this problem more complicated.

@_date: 2016-11-29 20:09:11
@_author: Theodore Ts'o 
@_subject: [Cryptography] [FORGED] Re:  OpenSSL and random 
On Linux, my recommendation is to use getrandom(2) on Linux; you'll
have to use syscall to access it because glibc developers are being,
well.... very glibc.  If it doesn't exist, fall back to /dev/urandom.
In practice this should be good enough for most OpenSSL users, since
by the time Apache fires up, in practice /dev/urandom will be seeded.
(Within 3-5 seconds after boot you should see kernel message to that
I am worried about silly distro init scripts that create random host
ssh keys in very early boot, but that's generally not an issue for
most OpenSSL uesrs.

@_date: 2016-11-30 16:17:29
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL and random 
More than that, the 0 day kernel test system (many thanks to Intel)
verified that such a kernel change (to cause /dev/urandom to block
until there was enough entropy) would break at least one fairly modern
Ubuntu setup as well as OpenWRT.  What OpenWRT was trying to do to
read from /dev/urandom that early in the boot sequence was something I
never bothered to figure out, since I use Google onTAP for my routers
these days, and I've been too busy.  Someone who does use OpenWRT
might want to take a look, though.
If you are using a systemd system that uses Python scripts to generate
systemd config files dynamically at boot time from //etc/fstab,
et. al, if you are using the wrong version of Python (newer versions
use getrandom0, making /dev/urandom block until there is enough
entropy will brick your System/VM.
(In case people had been wondering why I didn't just make this change
years ago, despite lots of ranting and raving on lists like this, it's
because I *worry* about things like this.)
P.S.  Silly bit of trivia: the reason why Python was reading fom
when Python was being used from a CGI script.  Not an issue when it is
being used as a system generator, but....  and that's why you
shouldn't panic when there are reports that system'd udev and Openwrt
are reading from /dev/urandom during early boot.  In some cases, it's
for completely innocuous things. Like lint warnings, though, it's good
to check them and if possible silence them so when someone is creating
long-term public keys immediately after a device is first powered up,
people are more likely to notice...

@_date: 2016-10-04 00:19:07
@_author: Theodore Ts'o 
@_subject: [Cryptography] French credit card has time-varying PIN 
I suspect that even changing the CVV once a day (generated from the
hash of a secret plus the date) would be useful from an anti-fraud
      	 	    		     - Ted

@_date: 2016-10-15 13:36:38
@_author: Theodore Ts'o 
@_subject: [Cryptography] Announcement: the SC4-HSM is now a FIDO U2F token 
This may be an FAQ, but it's not on the FAQ list for your web site
yet.  :-)
Given the concerns folks have had that the latest Yubikey's (which are
capable of supporting 4k RSA keys) are no longer using open source
firmware, is the hardware capable, and how hard would it be, to create
a OpenPGP smartcard compatible (using the CCID interface) with the
SCM4-HSM token?

@_date: 2016-09-29 22:41:19
@_author: Theodore Ts'o 
@_subject: [Cryptography] Use Linux for its security 
Indeed, Kees made a very similar presentation at the Kernel Summit
last year[1] and it was quite well received; no one argued with his
premise and everyone was very glad that he was proposing spearheading
this effort.  He got a large number of kernel engineers who were
willing to work with him --- and kudos to their employers for letting
them work on it.  (Especially Google, who has let Kees spend a lot of
time on this.)
After his presentation, the kernel-hardening mailing list was revived,
and new features to add additional hardening to the kernel has been
rolling into Linux for the last couple of releases.
There are engineers from multiple vendors collaborating on this
project, including Google, Red Hat, Intel, and others.  They have have
been using the kernel-hardening list for traffic control and for an
initial review of the patches before sending them out for wider review
and inclusion. The progress section of this LWN article[3] shows that
they've been quite successful so far --- or perhaps, it would be more
accurate to say they've made a good start.
[1] [2] [3] Yeah, these days I assume most web articles are click bait until
proven otherwise.  :-/

@_date: 2016-09-30 17:04:58
@_author: Theodore Ts'o 
@_subject: [Cryptography] Use Linux for its security 
It's a lot bigger than just political.  After all, if someone wants an
operating system that prioritizes security above all else, there are
plenty of other alternatives.  OpenBSD for example.  Or Brad
Spengler's patch set.
That fact that many people have chosen Linux is for a very wide
variety of reasons, but many of these people care about many things
beyond Security, and might not be happy of we (for example) traded
away a factor of ten of performance in the name of security.  Or if we
broke backwards compatibiity with Unix/POSIX progams in the name of
Linux and the other kernel maintainers are stewards for all
stakeholders who care and who use Linux, and that means we won't take
hacks to satisfy just one narrow special interest, whether that is
support for large enterprise databases, or mobile handsets.  That
means that we ask people proposing patches to go back and make it
better and more general, so it can support a wide variety of use
cases, or doesn't compromise scalability, or performance, etc.  Brad
Spengler wasn't willing to make those changes, and so his changes
didn't go in.
The difference with the current kernel-hardening effort is that they
they are willing to see if there are ways to provide the security but
with a much smaller impact (whether that be performance, or not
breaking compatibility, etc.).  They are also willing to break the
work about into small chunks, and to approach the problem as one of
continuous, incremental improvement.  And so the various security
features *have* been entering into the mainline Linux kernel over the
past year.
But hey, if you think you can do better, the whole *point* of Open
Source is that you are always free to fork the kernel, and make your
own version of Linux.  But then it's up to you to convince other
people to contribute to your fork, and to convince users, companies,
etc., to use your fork.  Or you can try to get everyone to use Open
BSD.  Or Minix3.  And you may find that not everyone shares your
"Security uber alles" philosophy.

@_date: 2017-02-08 10:40:20
@_author: Theodore Ts'o 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
My bank is doing that already, and has been doing it for, oh, two or
three years?  So it's hardly a new or novel technique.
      	      	      	       	      	    - Ted

@_date: 2017-02-09 13:07:33
@_author: Theodore Ts'o 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Or you set a domain-level policy that says Google will only accept the
user's password in combination with a tap on a FIDO Universal 2nd
Factor device which just barely juts out of the user's USB port.  If
John Podesta had one of those, maybe we wouldn't be enjoying spoofs of
Press Secretary Spicer being played by Mellissa McCarthy in drag....
Best of all, this is available today.  Unfortunately it requires that
users pay anywhere from $10[1] to $50[2] dollars for a U2F key, which is why
it probably really only works at companies who can set a security
policy requiring users to use it.  (And then the companies can pay the
cost of supplying all of their employees with the U2F security key.)
[1] [2]      	    	  	    	     	      - Ted

@_date: 2017-02-10 10:34:28
@_author: Theodore Ts'o 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
There is open source code available so it can be made to work with any
e-mail client and server.  And given that it doesn't depend on
civilians recognizing the presence (or absence) of the correct image
on one of hundreds of web sites that they need to log into (and for
the high value accounts, probably only once a month or); and given
that the user experience is *way* less complicated than Joseph
Kilcullen's Rube Goldberg machine (apologies to Rube Goldberg; but
*not* to Joseph Kilcullen); and given that:
(note that Joseph paper's claims some number of patents and patents
.... why are we wasting time discussing his solution (other than he's
acting like an obnoxious boor?)
It could be made to work with your server and your own e-mail client
today, if you're willing to take the effort.
       	  	 	    	     - Ted

@_date: 2017-02-10 15:31:35
@_author: Theodore Ts'o 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Actually, there are U2F security keys which are Bluethooth Low Energy
enabled, as well as U2F keys that support NFC, both of which can work
with Android phones.  Both of which are available today on Amazon.
I'm less sure about iOS support, since I don't track this area super
closely.  I'm a user of this technology, not a developer.
The BLE U2F devices do require charging every few months, but despite
that, in my experience they are far more convenient than the NFC
variants, since you don't need to carefully place the key at the right
place at the back of the phone; you just have to push a button
So this is not vaporware, in that there *are* multiple sites/services
which are using U2F.  Maybe not the one you are interested in, but see
also "there exists open source code".  That's not far *less* vaporware
than Joseph Kilcullen's proposed solution, for which as far as I know
there is *no* deployed code or sites/services using it.  And who knows
what the patent licensing fees will be....
     	    	       	    	 - Ted

@_date: 2017-02-13 10:29:17
@_author: Theodore Ts'o 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Which users have to verify.  And for which they have been repeated
demonstrated they aren't able to do reliably.
The above is something which *is* applicable to your solution.  If you
don't believe it, or believe that your solution is somehow special,
you are welcome to bankroll some human factors lab to do a study
specific to your design...

@_date: 2017-02-24 09:54:49
@_author: Theodore Ts'o 
@_subject: [Cryptography] SHA1 collisions make Git vulnerable to attakcs 
There are patches being discussed on the git mailing list to integrate
in the collision detector code which Google made available in the blog
post[1].  That will protect against this specific attack in question,
although granted there may be other attacks that have been or will be
developed against SHA1.
Work to allow git to support an alternate crypto checksum is on-going.
Work to convert to use "struct object_id" everywhree is ongoing, and
is about 40% done[2].
[1]  at sigill.intra.peff.net/
[2]  at genre.crustytoothpaste.net/

@_date: 2017-02-28 09:18:36
@_author: Theodore Ts'o 
@_subject: [Cryptography] Google announces practical SHA-1 collision attack 
If I'm not mistaken, those are the costs for the *second* phase of the
attack (110 GPU years).  However, you have to first carry out the
*first* phase of the attack, which takes 6200 CPU years.
Aside from throwing out numbers which are much scarier, which make for
good headlines and scaring clients to score more consulting time, is
there a reason why people are fixated on the 110 GPU year "second
phase" number, and not the 6200 GPU years "first phase" number?

@_date: 2017-01-02 18:09:36
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
It's not a matter of giving up; I'm just questioning your assumption
that there won't be regular clean shutdowns.  If the embedded device
is getting regular security updates, it will very likely be doing
clean shutdowns every month or so.  And if it's not getting regular
security updates, you might as well give the Russian hackers free rein
over the US Power Grid....  (Oh wait, they've already hacked computers
on the US Power Grid.  Maybe fixing the gaping wide security holes
should be higher priority, you think?)

@_date: 2017-01-04 18:44:48
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
We can do that, but we want to rewrite the random file right after we
use it anyway.  The reason is to deny attackers who manage to penetrate root
from have access to the state of the random used to initialize the
pool.  It's a minor point, since it only really helps in the case
where the privilege escalation attack happens soon after the boot
(when access to the data dumpted into the pool might help), but
rewriting the random state file is cheap.
One caution of using using the KOBJ_UEVENT_CHANGE idea --- very often
the entropy pool is initialized before the root file system is
mounted, so the event may never trigger before the userspace daemon is
started.  Of course, in that case, rewriting the random state file by
the init script or systemd should serve the purpose nicely.  We just
need to make sure that nothing bad happens if the userspace daemon
ends up waiting for Godot....

@_date: 2017-01-06 17:18:19
@_author: Theodore Ts'o 
@_subject: [Cryptography] where shall we put the random-seed? 
The basic thinking is that if the attacker manages to get root access,
it is almost game over going *forward*, but random session keys that
were generated *before* the attacker manages to gain root access
should protected as much as possible.  For that reason, we shouldn't
make life easier by making the state of the random-seed file avalable
to the attacker.
I agree with you that once privilege escalation has happened, it
is.... unlikely that it will be possible to resecure the system short
of doing a complete reinstall of the system (and that's assuming the
attacker hasn't manage to compromise, say, the HDD firmware).
It is *possible* that there could be an attack that allows the
internal state of the crypto random number generator to be exposed,
but for the attacker to not get any other access, so various
constructions that are designed to allow the system to "recover" as
quickly as possible after an internal state exposure aren't
*completely* pointless, but yes, if the attacker really has gained
full root access, it's game over.

@_date: 2017-07-01 20:40:11
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
If you mean DoS concerns against the kernel, on any non-prehistoric
(3.13+) Linux kernel, you're not going to "drain the entropy" on the
should be using /dev/urandom, where entropy accounting doesn't matter,
and secondly because /dev/urandom is only periodically reseeding the
CRNG from the entropy pool.)
As I mentioned previously, using a n1-standard-1 GCE VM, it takes 2.9
microseconds to generate 32 bytes worth of randomness (executing
getrandom in a tight loop), and on pre-ChaCha20 CRNG kernels
(pre-4.9), it takes 5.5 microseconds.
On my T470 laptop (with a 2.8GHz Core i7-7600U Kirby Lake CPU),
running Linux 4.11.1 and libbsd 0.8.3, getrandom(2) takes 0.379
microseconds, versus 0.137 microseconds for arc4random_buf(3).
Arc4random_buf() is is a userspace ChaCha20 RNG originally from
OpenBSD.  Of the 0.379 vs 0.137 microsecond difference, from measuring
the time to execute getpid(2), only 0.042 microseconds can be
attributable to the system call overhead.  What's the rest of the
difference?  I suspect it's either locking overhead or a more
optimized Chacha20 implementation.
So yes, a userspace CRNG can certainly be faster, and if you don't
mind the hair of dealing with process-level state (e.g., dealing
fork(2), making sure it doesn't get written out to swap, or written
out to a core dump, etc.), there are certainly valid reasons to use a
userspace RNG.  But the cost of using getrandom(2) directly really
isn't as great as some people are making it out to be.
For these reasons, for a less-than-clueful applications programmer,
I'd personally recommend getrandom(2) over trying to implement their
own CRNG in userspace, and probably getting wrong.  I'm sure, though,
the OpenSSL devs are far more talented than your average application
programmer!  :-)
 * bench-getrandom.c
 *
 * Copyright (C) 1993, 1994, 1995, 1996, 1997 Theodore Ts'o.
 *
 * This file may be redistributed under the terms of the GNU Public
 * License Version 2.
 *
 * The init_resource_track() and print_resource_track() functions are
 * adapted from in e2fsck/util.c from e2fsprogs.
 *
 * Compile via: (do NOT enable any optimizations)
 *
 * cc -o /tmp/bench-getrandom /home/tytso/src/bench-getrandom.c -lbsd -Wall
 *  */
         __NR_getrandom
 __i386__
 __NR_getrandom	355
 __x86_64__
 __NR_getrandom 318
 No getrandom syscall number!
struct resource_track {
struct resource_track	global_rtrack;
void init_resource_track(struct resource_track *track)
static __inline__ float timeval_subtract(struct timeval *tv1,
static void print_resource_track(struct resource_track *track)
int main(int argc, char **argv)

@_date: 2017-07-02 14:48:01
@_author: Theodore Ts'o 
@_subject: [Cryptography] Depending on Google to protect your anonymity?! 
And that's just the beginning.  State can be stored via HTML5 and
Flash interfaces, and you can correlate based on all sorts of things
including IP address, screen size and resolution, etc., etc.
The other thing that's important to note that there are plenty of ad
networks beyond just Google and many of them are far less ethical (and
for that matter, competently implemented).  So just because you are
still seeing ads for restaurants in Honolulu weeks after your Hawaiian
vacation, don't necessarily be quick to jump up and blame Google as
the automatic boogieman.
I normally give permission to Google to know my location via my
browser's HTML5 facilities, because I *like* location-aware
advertising.  It's actually annoying to get ads about restaurants
which I can no longer enjoy.  So when I get ads for those restaurants
(which no doubt was coming from tracking cookies established while I
was researching fine dining experiences from my hotel room in Oahu),
I'm pretty confident that they aren't coming from Google's ad network
--- Google does a much better job targetting me, and for me that's a
feature, not a bug.  Relevant ads are actually *way* less annoying
than rubbing my nose in the fact that my vacation only for a week.  :-/

@_date: 2017-07-03 11:44:14
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Yes, *please* look at the man pages.
     These functions first appeared in OpenBSD 2.1.
     The original version of this random number generator used
     the RC4 (also known as ARC4) algorithm.  In OpenBSD 5.5 it
     was replaced with the ChaCha20 cipher, and it may be
     replaced again in the future as cryptographic techniques
     advance.  A good mnemonic is A Replacement Call for
     Random.

@_date: 2017-07-03 11:50:26
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
If the kernel pool is not initialized, what hope does glibc have of
getting good seed entropy for its CRNG?  What will you plan to do in
that case?  Please don't say, "oh, we'll just seed from the time and
process id."  :-)
The only thing you can do is what getrandom(2) does, which is block.
If you return an error, you'll run into the problem that most
application programs don't bother checking for error returns.  You can
try to use /dev/urandom, which will provide best efforts randomness,
and probably better than what you can do purely in userspace ---
except you seem to be worried about what to do in chroots.  And so if
you are in a chroot w/o /dev/urandom and someone calls arc4random(),
is it OK if you return crap randomness?
OTOH, I'm not sure it's valid to worry about improperly formed
chroots.  Do you worry about how to run an ELF binary in a chroot is
missing /dev/null?  Or if /dev/tty is missing?  And similarly, what
does "support" for kernels older than 3.17/3.18 mean from a security
perspective when those kernels are probably filled with security
So if /dev/urandom is missing the only question is does arc4random
fail securely (abort the process with an error message or block), or
fail insecurely (return numbers which appear to be random, but aren't

@_date: 2017-07-05 17:43:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
You do know that AT_RANDOM is only present in 2.6.29+ kernels, right?
Are you planning on supporting kernels older than that?  What happens
if a user tries calling arc4random() and they are running on a kernel
which predates AT_RANDOM?
Super-early in the boot process, there may not be a whole lot that we
can do on crappy hardware.  In those cases depending on AT_RANDOM is
not a whole lot different from using /dev/urandom, so from that
perspective I get why you are arguing that blocking is something you
can't do.
I would counter though that at least arc4random() in glibc is a new
interface, and if the new interface blocks, that is much different
than changing an existing interface (e.g., reading from /dev/urandom).
So if ssh tries to generate long-term private RSA keys (for the host
ssh keys) using arc4random during early boot, it is really a hard
question whether you should block or not.  And I can understand why
glibc developers would prefer never to block.  At some level it
becomes a blame game.
If you have a cheap-sh*t router using crappy hardware with no hardware
RNG, no high resolution timer, and no environmental noise (because the
user might plug in the device before plugging in the network, so there
really isn't a lot even the kernel can work with), and then the
cheap-sh*t router generates a crappy RSA host key, and said cheap-sh*t
router is being used in Trump's private offices in Trump tower and the
Russians use it to break into his system.
Who's to blame?  Trump, for buying a cheap-sh*t router?  The router
manufacturer, for setting up a configuration which is impossible to
secure?  SSH, for insisting that host keys be generated within seconds
of first boot, instead of "on-demand" the first time someone tries
connecting to the host?  Glibc for providing an interface which can be
used to get insecure randomness, despite the claims on the man page?
The kernel, because glibc got the randomness from the kernel, and it's
always easier to blame the kernel devs?  etc.
It's really a systems failure, and at the end of the day, there is
very little that can be done by the kernel and by glibc.  All we can
do is make choices that hopefully lead to a more secure outcome, while
not screwing over existing users and hopefully not make life too hard
for new users....

@_date: 2017-07-06 19:38:48
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
If we can build a way of getting that seed passed into the kernel from
the bootloader, that will probably go a long way towards solving much
of the problem.  We still have to store that seed where the bootloader
can get to it, regenerate the seed at boot and shutdown, but the real
hard problem is that each architecture will need a at least one
solution for its bootloader.  (For some architectures you might need
to support multiple boot loaders.)
You still have to solve the problem of how do you reliable and secure
the seed when the system is booted for the first time right after it
has been unpacked from the box.  That can potentially be quite
difficult, since it's likely going to be different for each consumer
electronics device that is using Linux.
And trusting consumer electronics manufacturers to be able to
correctly and securely generate random seeds for all of their devices
when they can't even manage to assign unique ethernet MAC address is a
very challenging problem!

@_date: 2017-07-08 11:38:17
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
That was on an Intel architecture.  The architectures I'm most
concerned about are ARM and MIPS, some of which don't have a
high-resolution timer, nor a cycle counter, nor RDRAND.
And cheap-sh*t rounters tend not be to be using x86.  Nor do the $40
Android tablets you can pick up at Shenzhen market.  I'm not sure how
many of the cheap-sh*t routers are using glibc, though. They may all
be using some other C library in which case maybe it doesn't matter to

@_date: 2017-06-30 11:45:02
@_author: Theodore Ts'o 
@_subject: [Cryptography] OpenSSL CSPRNG work 
As I've told Rich, if he wants to focus narrowly on cryptographic
security issues and not worrying about whether the OS is so badly
riddled with countless known security bugs and even more zero-day
attacks that haven't been made known yet, that's __totally__ up to him
and the OpenSSL team.
Personally, I think thats a silly thing to worry about, and I wouldn't
consider it a great use of resources to support.  Even if there are
crazy people who do that.
Also, I personally don't care about FIPS as I view that as a hopeless
waste of taxpayer dollars to enrich FIPS certification labs.  I know
that people feed at the US Government Trough need to worry about it.
I'm just thankful I'm not part of that racket.  How much effort
OpenSSL volunteers want to spend working on that use case, is again,
their decision.  If *I* were on the core team, I'd probably say that
patches would be accepted, but FIPS certification support is not
something I'd ask volunteers to work of their own free will.  But
again, that's up to other people to decide.
       	      	    	  	    - Ted

@_date: 2017-03-25 08:33:08
@_author: Theodore Ts'o 
@_subject: [Cryptography] Google distrusts Symantec for mis-issuing  30, 
Fix them *how*?  Why don't you give a concrete proposal, which can be
evaluated on its merits?
Do you want to let US Government issue the certs?  Now we have to
trust the US intelligence and law enforcement community not to
secretly screw with the certification system.  Feel free to ask the
Germans how they would feel about that.
Do you want the US government to be dictating to Google and Mozilla,
and all other browsers suppliers, which certs should be trusted by
default in the US?  See previous concern.  Also in that case, what
stops Iran and China and Russian from dictating to browser suppliers
what certs must be trusted by default to citizens of their country?
It's really easy to complain about the current system.  But coming up
with a better solution can often be harder.  To update a certain quote
from the musican Hamilton (and which might also be easily applicable
to a certain political party these days):

@_date: 2017-11-28 12:29:37
@_author: Theodore Ts'o 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
It's not clear whether you are assuming that there should be a
per-user key.  So I'll describe the solution used in Android's File
Based Encryption (FBE) which uses ext4 encryption as implemented by
Michael Halcrow (who also was the architect of Bitlocker, and Linux's
ecryptfs --- for which he considers ext4 encryption to be his apology
:-) and I.
The user key is a random key which is either stored in a secure
element, or encrypted using the user's passphrase or Android PIN.
(Yes, PIN's are crappy, so for devices with secure elements which zap
the PIN after so many bad tries, that's the solution to *that*
Each file is encrypted in a per-file key which is either encrypted in
one or more user keys (the original V1 format, which also was spec'ed
supported symmetric key encrpytion, but that was never implemented) or
which is derived from a nonce and the user key (what we now use
because we wanted the encryption and SELinux information to fit in the
inode table and not in an external block, and there was no product
need for multi-user/multi-key access).  The nonce or the wrapped key
is stored in an extended attribute for the file.
Directories and symlinks are similarly protected.  Because we assume
that applications are encryption oblivious, we have a security policy
which requires that if the parent directory is encrypted, its
subdirectories and files must be protected using the same key as the
parent directory.  The top-level per-user encrypted directory
hierarchies have the key identifier used to protected verified at boot
time from trusted storage to make sure they are encrypted using the
expected per-user key.  All of this is to protect against certain
chip-off attacks that might be carried out by an evil maid.
The original devices that featured FBE didn't have a secure element,
so we weren't proof evil maid attacks, but the architecture was
designed to take advantage of the appropriate hardware features to
provide protection against such attacks when available.
The result of *all* of this is precisely such that if you have a
device which has enterprise management, and the user loses their
Android or Chrome device, or the user leaves the company, the
enterprise manager can arrange to have all of the corp data wiped by
destruction of the per-user key stored on the device, without having
to do a full secure erase of the flash storage, which would wipe out
the personal data stored on the phone.
I don't know if you consider this to be a "simple" solution, but it's
a real-world deployed solution used to protect corporate data at work
today, and it's available wherever fine Android / ChromeOS devices
with FBE are for sale, and the enterprise management features are part
of Google's G Suite for Work.  :-)

@_date: 2017-10-17 13:55:57
@_author: Theodore Ts'o 
@_subject: [Cryptography] Millions of high-security crypto keys crippled 
I'm not aware of any DRM schemes which use the TPM's.  That was a
hope, but it turned out to be a support nightmare for general consumer
use.  So Netflix, Hulu, etc., all use embedded keys in applications,
and they do not use TPM technology.
The primary use of TPM's has been limited to a few, highly
sophisticated enterprise use cases for controlling access to their
Intranet.  And it's mostly involved custom software, both on the
client and management end of things.  Some of the enabling changes to
open source software has been contributed back upstream, such as
wpa_supplicant, but not in any coherent way that someone could use it
in a turn-key fashion for securing, say, the internal I/T
infrastructure for a NGO or US political party to protect it from
infiltration by hostile nation-state entities...
All or most of what you've listed above doesn't involve TPM's.  For
example, the requirement that some modern x86 tablets can only boot
Microsoft signed OS's (which can include some enterprise Linux distro
kernels, but yes, only on Microsoft's sufferance) is based on UEFI's
Trusted Boot feature, but that doesn't involve use of the TPM on the
client machine.  It's _possible_ that the signing key used to sign the
OS kernels which a locked-down x86 UEFI system is allowed to boot was
generated on a Infineon TPM module, but in general the hardware
signing modules used for these sorts of applications are purpose built
devices, such as this:
... and since the the market for such HSM's includes government/miltary
users that require high levels of FIPS certification, it's unlikely
that the be-as-cheap-as-possible-so-you-dont-inflate-the-BOM-cost
units such as the Infineon chip would get used.  These are specialized
units which cost $$$ so you can afford to use something better than a
TPM chip which is slow has heck and which main feature is that it is
priced out in millicents.
Bottom line, you can probably use the TPM hack to be able to break
into various company's WiFi networks (if KRACK didn't leave the door
wide open anyway).  And it's certainly interesting to look into what
else might be jepordized by the TPM failure.  But the reality is that
TPM's have never been all that widely used, and certainly not in the
DRM arena.

@_date: 2018-01-03 22:30:46
@_author: Theodore Ts'o 
@_subject: [Cryptography] NSA seeing large exodus? 
If I had to guess that percentage of crypto specialists at NSA
relative to its entire employee base is relatively small.  I suspect
there is a much larger percentage of NSA employees who are computer
engineers, especially in the computer security specialists.
And there is certainly a significant demand in the private sector for
cyber security specialists.  After all, security bugs like this:
don't discover themselves, you know!

@_date: 2018-01-09 17:46:07
@_author: Theodore Ts'o 
@_subject: [Cryptography] Speculation considered harmful? 
Yet another problem with VLIW is that the optimizations tend to be
horribly architecture specific.  If you change the number of various
functional units, but you want to keep the instruction set backwards
compatibility, that's possible --- but if the compiler has to know how
the instructions will be scheduled, not only is it slow to compile,
but what is optimized for a particular version of Itanic might have be
terribe performance on a different version of the Itanic.

@_date: 2018-01-10 15:24:49
@_author: Theodore Ts'o 
@_subject: [Cryptography] Speculation considered harmful? 
That alone would probably doom an architecture with VLIW from a
commercial perspective.  Think about what this means from a software
vendor's perspective.  Or from a Linux Distribution's perspective.
The CPU vendor will be pushing out --- at minimum --- 4 CPU models per
release cycle, which is every 12 months, plus or minus.  One for
"super low-power mobile devices or netbooks", one for laptops, yet
another for servers, and yet another for super-high-end servers (aka
"Xeon Processors").  Over 5 years, that's twenty different CPU models
the binary provider would have to reoptimize and support --- for every
softare release.  And in practice, if the binary is different, it will
have to go through its own QA cycle.  So we've just increased the
overhead of supporting this architecture by an order of magnitude over
a traditional non-VILW architecture.
(Yes, I know that the Itanic would be laughable in the low power
domains; so even if you think an architecture that neglected the
laptop market would be successful, it's still a factor of 10 more
binaries that you have to produce and QA.)
And this is ignoring the fact that these compilers tend to take a
factor of three to five longer to actually compile, which is really
annoying from a developer's edit, compile, debug cycle.

@_date: 2018-09-02 18:14:46
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] WireGuard 
... but minimizing dependencies on third parties is part of minimizing
risk.  Yes, of course on a typical laptop, you'll have trust some set
of Google, Mozilla, Apple, Microsoft, etc.  But just because we have
to trust *some* third parties, that doesn't mean that current scheme
where there are hundreds of CA's (with many worked examples of
spectacular failures, such as Diginotar) that are trusted to verify
certificates for *any* hostname, including *.google.com,
*.microsoft.com, etc.  is sane.
And a design brief where a random nation state can create a
certificate for microsoft.com is a good one?!?
Sure, but if we look at where the failures caused by malicious actors
have been in the past, they have been far more often by CA's as
opposed to browser or platform providers.

@_date: 2020-02-14 09:12:49
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] 'The intelligence coup of the century' 
This wasn't just IBM mainframes.  There was a program (I think from
DECUS, but I could be wrong) that would broadcast music from a PDP-8/i
to an AM radio.  As I recall one of songs that it would play was
"Flight of the Bumblebee".
In this case, though, I don't think we could call it a "side channel"
since the whole *point* of the program was to force radio
"interference" (I'm pretty sure the PDP-8/i wasn't certified by the
FCC as a radio transmitter).  :-)

@_date: 2020-02-26 19:43:26
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] Well, that showed them! 
Well, speaking as someone who is not a trustee of the Internet
Society, the transparency of the Ethos Capital deal, IMHO, makes a
FISA wiretap warrant look open and transparent.  As a result... it's
hard for me to trust what was going on, much like it's hard for as
it's hard for many to trust when the NSA says, "hi, we're from the
government and we're here to help".

@_date: 2020-01-21 20:20:15
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] Proper Entropy Source 
The original Djksta quote: "Testing shows the presence, not the
absence of bugs" was paraphrased by John as:
  testing can show the absence of randomness;
    but it can never show the presence of randomness.
Your reference of the McNamara fallacy[1]
[1] wasn't want John was going for, but it's also somewhat applicable.
My usual example is using any AES key as "NSA_KEY", try feeding the
following to a statistical test:
   AES(NSA_KEY, SEQ++)
It will look perfect, but for anyone in possession of NSA_KEY and a
sequence of numbers generated from that formula, they can decrypt the
sequence to find SEQ, and then predict all future numbers of said
"random number generator".
This is why I generally consider statistical testing for randomness to
be worse than useless, since it causes people to get a false sense of
security.  Sure, if you use it to test the raw, non-whitened sequence

@_date: 2020-01-23 11:25:55
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] Proper Entropy Source 
This is true only *if* the 100 sources are uncorrelated with each
other.  But if they are based on timing, and all of the "clocks" on a
particular "system on a chip" are driven by a single master oscillator
(got to reduce cost, you know), and it's a very simple RISC-V CPU
which didn't have any Spectre or Meltdown vulnerabilities because it
wasn't doing any kind of speculative execution or register
renaming.... then it could very well be that all of your timing events
are highly correlated.
If you can use packet arrival times on the local area network, and
perhaps radio signal strength information from the WiFi, perhaps
that's not as easily guessable by a remote attacker in Fort Meade.
But take for example a IOT device that attempts to generate a
public/private keypair the first time it is plugged into AC mains,
perhaps before it has attempted to bring up the network....
Agreed that this is the best we can do.  Now, if only all
optimized-for-BOM-cost consumer grade devices had hardware random
number sources....

@_date: 2020-11-28 22:40:19
@_author: Theodore Y. Ts'o 
@_subject: [Cryptography] A Scheme for Verifiable Lottery 
For a more detailed analysis of this idea, see section 3 of RFC
3797[1], "Publicly Verifiable Nominations Committee (NomCom) Random
Selection", written by Donald Eastlake in 2004.
[1]
