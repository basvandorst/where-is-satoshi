
@_date: 2013-08-26 18:44:13
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Good private email 
This is everything *but* PRISM-proof : it doesn t solve the metadata issue
and your directory server containing public keys could very well be forced
by a law enforcement agency ( in the best case scenario because it could
also be the mafia) to answer the fbi/mafia public key on any request made
to it.

@_date: 2013-12-19 01:19:13
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] RSA Key Extraction via Low-Bandwidth Acoustic 
As a student I was fortunate enough to attend one of Adi Shamir's lectures
at the university of Waterloo where he talked specifically about this
problem. It stuck with me and I'm glad to see that an actual key recovery
attack came out of it.
Have you trief this out against openssl ? How succesful do you think it
would be ?

@_date: 2013-10-22 00:50:06
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] [RNG]   on RNGs, VM state, rollback, etc. 
A cursory look at the HAVEGE papers tells the following story : as a user i don't have access to the entire internal state of my cpu and since the time needed to execute a given instruction is a function of both the instruction and the cpu state measuring the time gives me entropy.
I might have a few issues with that. First the fact that intel didn't give me access to the entire cpu state does not mean that they haven't given the NSA the opcode to do just that. Then, since we never get access to the cpu state we don't actually have a formal model guaranteeing that the lsb of the tick counter are actually random. I generally feel uneasy calling unpredictable a deterministic process and then imposing that the adversary is stupider than me. That being said i wouldn't have a problem feeding all that into /dev/random while crediting 0 entropy.
As far as your idea of feeding ram into the entropy pool I fear that because of the remanence  effect you could actually be feeding adversarially controlled input into the entropy pool. Also if the idea is to resolve the "first-boot" issue then it assumes the RAM has uncertainty which I m not sure is true ( i would be even less confident of that in VM)

@_date: 2013-09-11 12:00:40
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Evaluating draft-agl-tls-chacha20poly1305 
2013/9/11 William Allen Simpson Chacha20 being  a stream cipher, the only requirement we have on the ICV is
that it doesn't repeat isn't ?
This means that if there's a problem with setting 'mostly zeroed out' ICV
for Chacha20 we shouldn't use it at all period.
As far as your proposition is concerned, the performance penalty seems to
largely depend on the target platform. Wouldn't using the same set of
operations as Chacha prevent an unexpected performance drop in case of lots
of short messages ?

@_date: 2014-08-18 17:56:07
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Is this (
supposed to be considered atypical ?
The model described here models what you call forward secrecy (which is
rarely perfect).

@_date: 2014-01-23 00:05:31
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
I think signing ciphertexts is generally a best practice, and
In the public key world, signing ciphertexts not only reveals the identity
of the sender but also allow relay attacks where a guy intercepts a signed
message, strips it from his signature and replaces it with its own.
Depending on the protocol it can be a problem.
I think the encrypt-sign-encrypt solution solves both of those problems

@_date: 2014-03-20 10:32:44
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Modern definitions of security for asymmetric ciphers *do* provide an
encryption/decryption oracle to the attacker which mean that PK crypto is
not brittle in general, some algorithm are extremely fragile and must be
used with caution (RSA PKCSv1.5 as an example) while others are better
since we can prove they achieve a more demanding form of security

@_date: 2014-03-20 14:41:00
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
What would be your pick for a non-brittle modern asymmetric cipher?
As far as I know, a careful, constant-time implementation of RSA-OAEP can
withstand chosen ciphertext attacks in the random oracle model.
In the EC world ECIES is quite common but it's basically asynchronous DH +
symmetric encryption so it might not fit your requirements.

@_date: 2014-10-23 14:00:13
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] In search of random numbers 
Using emails/ spams seems like it gives a networked attacker
read/write access to your entropy source (I can always send you more
spam) which is probably not the best idea.
Anyway hanno is right : if it doesn't solve the "startup problem" then
it probably isn't much better than what we already have

@_date: 2014-10-24 14:18:39
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] In search of random numbers 
Recent research has shown that numerous devices (headless servers for
example) generate their long lived cryptographic keys upon their first
In that case there is no "last time" that can be reliably trusted.
Unless I misunderstood your point I don't clearly see the engineering option.
Alexandre Anzala-Yamajako

@_date: 2014-09-22 11:09:47
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] The world's most secure TRNG 
I don't think a simulation is "good enough" for the tin foil hats
among us : in a perfect world a TRNG would come with a statistical
model that is falsifiable, that is I want to be able to design health
test for that particular TRNG that would tell me if it diverges from
its expected behaviour. That is no easy task I admit but until we have
that it's hard to trust any TRNG over another IMO

@_date: 2015-01-14 00:33:45
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] simple codebook for passwords 
The idea is good and user friendly. I slightly dislike the "chose a
single secret word" part since users are probably going to pick short
words that are easy to bruteforce.
Also you still need to trust them to erase your qwertycard info
because they already own your first name, last name and email to send
you the card so the only thing standing between you and your
facebook/gmail/twitter account is the secret word

@_date: 2015-07-18 01:57:40
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Nasruddin Cryptographic Function (99% finalized) 
What problem are you trying to solve ?
This is both heavier and slower than regular AES while the security section
is headlined by a worrisome
"I imagine this is more secure than many other ciphers from Buffer
over-read, Cache timing [...], Differential Power, as well as other
potential attacks." why is any of that true ?

@_date: 2015-06-06 21:48:11
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography]  let's kill md5sum! 
Just a thought...
If we re going to kill of md5sum and break user's habits and scripts we
might as well do it once and for all.
Why not build a tool called hashsum whose options are md5 sha2 sha3 and
blake2 ? This tool could be transparently updated wo breaking compatibility
in the future and the man page would explain the rationale for each option
(md5 would be indicated as deprecated but there for verifying old file
hashes for example)
Going further we could include an option "security level" to give users who
want to have 128 bits of security against (second-) preimage a better
option than md5.
Alexandre Anzala-Yamajako

@_date: 2015-05-31 03:17:17
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Why is ECC secure? 
2015-05-30 20:08 GMT+02:00 Ryan Carboni :
This is simply not true !
To use ECC one needs to share curves and while this is true that
there's suspicion growing about the NIST generated curves because of
the obscure nonces they have used for generation there are today
several several serious alternatives to those NIST curves see
 for example.
It is also true that there might be an unknow class of curves for
which solving DLP is easier but there might also be a class of modulus
for which breaking RSA is trivial.
And as far as picking parameters go, once you agree on a trusworthy
curve (using for example Curve25519) it is much easier to generate
keypairs safely using ECC than it is for RSA : see the work of Nadia
Heninger presented at Crypto 2012 where it is shown that a large
number of device share only one prime factor of their public modulus
which makes it trivial for anyone to break their security
Alexandre Anzala-Yamajako

@_date: 2015-11-12 11:52:23
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Post Quantum Crypto 
We can't deny that you have a sense of timing,
see this posting on the eprint archive concerning Post Quantum Key exchange
Last week I also read a position paper from Michele Mosca about crypto in a
post quantum world (
While I think work in post quantum crypto is valuable I take issue with the
estimation of how close we are from de crypotapocalypse I don't really see
any substanciated arguments in your or Michele's statements.
According to you we're are 3 away, while Michele's horizon is around 2030.
Who's right matter because the policy we chose are not going to be the same.

@_date: 2015-10-10 19:22:10
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Collisions w/SHA-1 ~$100,000 TODAY 
If you know about the Merkle Damgard paradigm to build hash functions then
page 2 of the article linked in the original email explains very clearly
what those are

@_date: 2016-04-15 18:04:32
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] At what point should people not use TLS? 
Don't you end up with a single global trusted third party ?

@_date: 2017-12-05 10:00:26
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Always keep a second browser installed and up to 
While I think it is a good advice for browser I don't know that it is valid
for keys. Key management is hard. There is no reason to keep 2 sets of keys
if you manage them similarly. So basically while we already suck at
handling one set of keys you re advocating handling 2 sets of keys
separetaly. And then use the second set while it might have very well been
Not a good advice IMHO

@_date: 2017-12-13 09:52:02
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
IMO a statistical approach based on taking a bunch of data a saying
essentially "I don t see any signs that it s not random" is not a good
approach for entropy seeding.
The example is old but I could give you the output of an AES in counter
mode with a null key and a null iv and no standard statistical test woud
ever show you any defects while you have absolutely no entropy.
You case is particularely worrisome for several reasons
1) you use a von neuman like extractor but you have also shown that your
data is not only biased but also correlated
2) you don t seem have a model of your hardware source from which you could
derive the output distribution
3) you do some wizardry to remove some correlation but nowhere show or
prove that there isn t more corrolation to be taken care of or how
4) I didn t see in your document a justification of the fact that the
manufacturer of the camera (soft and hardware) doesn t have more
information than you and could therefore target defects in your entropy
management procedure.
You should have a look at the work of Viktor Fischer, David Lubicz, Florent
Bernard and patrick Haddad.
They invested quite a bit of effort to give entropy guarantees when using
very specific hardware device.
Best regards

@_date: 2017-12-14 11:28:47
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
Most
It would but, don't you need to be able to lower bound the min-entropy of
the source data to use a Universal Hash extractor ?
I actually think that from my points earlier point 4 is the most pertinent
: in your case not only are you neither the designer nor the manufacturer
but you also do not control the software updated that get delivered to you.

@_date: 2017-12-14 11:38:15
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
Actually it's generally a tell of a bad design when there are sets of "weak
keys" and the larger the set, the worse the design. Since you're going to
pick your keys uniformly at random you don't want to have to go and check
AES doesn't have any known weak keys AFAIK. My point was not that AES or
Crypto are bad but rather that stastistical tests suck at evaluating
entropy. The only thing they are good for is to detect catastrophic
failures (you forgot to plug the output and it s constant for example).

@_date: 2017-12-16 07:38:44
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
I can sort of get behind this if you *add* the sources of entropy instead
of *replacing* them. The one nice thing about entropy is that it sums
easily, and you need to spectacularely mess up to have a bad source
actually hurt you.
If you have n adversaries that fully control a source of entropy but you
have n+1 sources. Xoring those sources together to seed a DRBG is safe and
doesn t require you to distinguish between the "good"
And "bad" ones.

@_date: 2017-12-16 08:29:08
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
1. you don't need Apple to target people you just need to impersonate the
appstore to them (so basically we re back to the PKI as your lifeline. That
does *not* make me feel safe). Probably there needs to be some nifty tricks
to pass the code signing check but that's their job and the payoff woulf be
2. there is already a ton of software running around your actual hardware
camera and i would guess a significant part of this software has a direct
impact on what you would get as the entropy of you raw image.
3. monitoring the output to try to detect suspicious variations of
"entropy" is extremely hard here because you have 3 non independent
distributions :
-the distribution of harware camera, each of them having their own specific
- the distribution of scenary (my office, my desk, la tour eiffel)
- and when the first 2 are fixed you have the pattern distribution of your
specific camera.
I could also add the distribution of effect of the software stack and/or
the process variations in the production of cameras which makes it that no
two of them are actually identical.
Basically from all of those complexed distributions I give you one (or 10
or 1000) data points and I ask you "Is it normal ?". I don't know how you
could ever answer yes in a foolproof way.
Sorry but I very much dislike those kind of arguments because I think that
it encourages people to actually *use* the half baked crypto they wipped in
their garage.
Also even for the people you are talking about it is counter productive.
1. the cases where you know from the get go that you will never have to
interoperate with anyone are very seldom.
2. Even if you do, and you ve got your  AESbis, you re still a long way
away from a working application level secure communication protocol that
could replace say TLS for your secret organization. The issue is with every
layer you add, the probability of making a huge mistake grows exponetially.
OK you got AESbis but on top of it you need a mode. You use CBC ? Then I
bet you re vulnerable to those padding attacks (vaudenay, lucky13). Have
you thought about the authenticity ? On the protocol level have you
protected about replay, downgrade, cross-protocol attacks ? Do you get PFS
from you algorithms ? How do you do key distribution ?
That s a lot of things to get right for a group of guys.
I seriously believe that the reason we think those half baked algo are safe
is because nobody in the public sector has any incentive to break those...
3. Finally let s assume for a minute that you got 2. right (BIG
assumption). Then how do you communicate over the network ? If you use a
custom protocol you are pretty much sure that you are going to stick out
like crazy in any traffic analysis. If I worked at the NSA/CIA I wouldn t
make sure that I decrypted all the traffic, that s probably too hard. I
would make sure that I at least know how to classify traffic. If I see
something weird and unknown and clearly made not to interoperate I ll make
sure that I ll metadata the hell out lf it to know who is communicating
with whom when and so on. If they re not a group of students in Comp.Sec. I
ll just use one my many exploits to attack directly the endpoints before
the data is encrypted or after it s been decrypted and that whole thing
costed me a lot less than finding the outlier in a sea of TLS traffic.
The lesson for me is : If the starting point of your reasonning is "AES is
broken" then just go home and do something else anything that comes after
that has a 50% chance to be horribly broken and that s already way to high
if you want to protect valuable data.
Alexandre Anzala-Yamajako

@_date: 2017-12-17 10:57:01
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
I refer you to the work of Viktor Fischer, David Lubicz, Florent Bernard
and patrick Haddad.
You can make *reasonable* assumptions about hardware and come out with good

@_date: 2017-12-17 19:47:20
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
of nuclear decay. And we have the hardware to detect the phenomenon...

@_date: 2017-03-02 13:51:02
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] [FORGED] Re: Google announces practical SHA-1 
2017-03-01 23:55 GMT+01:00 Peter Gutmann :
Isn't what you describe a "second preimage" attack on SHA -1 rather than a

@_date: 2018-08-01 13:51:00
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Perfect Integrity? 
The short answer is : yes.
You can find some examples here
I believe that any Wegman-Carter MAC where the key is changed after every
use gives you information theoritic security.

@_date: 2018-08-18 12:19:49
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Rescuing Encrypt-then-Sig 
The introduction of Antoine Joux's book Algorithmic cryptanalysis spend
some time discussing those different paradigms. If I remember correctly in
public key crypto he also advocates for encrypt then sign then encrypt.

@_date: 2019-12-12 11:48:21
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Stupid movie encryption scenes 
Additionally this is how side channel cryptanalysis works by focusing of
measurements on different bytes of the key. That being said, it seems more
like it is sheer luck than them having been right all along

@_date: 2020-04-07 11:10:39
@_author: Alexandre Anzala-Yamajako 
@_subject: [Cryptography] Privacy post COVID 
If you really believe that it's where we are heading then we should all
make noise about privacy preserving alternatives such as CEN (Contact Event
Numbers) [1] and DP3T (Decentralized privacy-preserving proximity tracing)
[1] [2]
