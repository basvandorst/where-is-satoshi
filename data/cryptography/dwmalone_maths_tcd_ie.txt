
@_date: 2005-12-27 22:18:53
@_author: David Malone 
@_subject: another feature RNGs could provide 
S_n has size n!, so the size of the keyspace is (2^x)!. The thing
is that if you compose two of these the resulting key space is of
size (2^x)!, because you've already got all possible permutations,
so you gain nothing from it.
Usually a cypher is a small subset of the set of all possible
permutations, so composing the permutations may result in a bigger
subset. If the subset turns out to be a subgroup, then you gain
nothing, because a subgroup would be closed under composition.
In the case of having a plaintext/cyphertext pair in a cypher where
the key can be any possible permutation, knowing E(P) = C tells you
nothing except D(C) = P and E(X) != C for X != P, because the image
of one element tells you nothing about the others.

@_date: 2005-12-27 22:45:59
@_author: David Malone 
@_subject: another feature RNGs could provide 
I'm fairly sure knowing that E(P) = C reduces the key space from
(2^x)!  to (2^x - 1)!, because you've just got to choose images for
the remaining 2^x - 1 possible blocks.
I think a problem with Ben's arument is in assuming that knowing
E_A(P)=D_B(C) tells you that your key was A.B. For example, suppose
my key K is the permutation:
and my P = 2. Now we know E_K(P) = C = 3. Ben guesses A:
and B:
He sees that E_A(P) = E_A(2) = 3 = D_B(3), and so assumes that K =
A.B. But A.B = A != K.
(In this example, imagine x = 2, and we label the blocks 00 = 1,
01 = 2, 10 = 3, 11 = 4.)

@_date: 2005-12-28 10:46:59
@_author: David Malone 
@_subject: another feature RNGs could provide 
Yep - that's my point. The thing to note is that for an arbitrary
permutation, knowing the image of n plaintexts tells you (almost)
nothing else.  Usually for a block cipher with a smaller key space,
knowing a plaintext/ciphertext pair actually has a pretty big impact
on what you know about the key, and this is how people usually think
about block ciphers.
In AES with a 128 bit block and 256 bit key, if the images are
uniformly and independently distributed, then each pair known reduces
the possible amount of key space by about 128 bits, so 2 or 3 pairs
will nail the key down with reasonable probability. For good measure
we could say 20 or 30 would be sufficient, even if the images aren't
well distributed.
For S_(2^128) the original key space has (2^128)! keys so it is
about 128*(2^128) bits. Knowing 30 pairs here will reduce the key
space by about 128*30 bits, leaving us with 128*(2^128) - 128*30 =
128*(2^128-30) bits. We've barely had any impact at all, because
the key space was much bigger to begin with.
Of course, this also shows why using an arbitrary permutation in
S_(2^128) isn't very practical - you need to store 128*(2^128) bits
to remember which one you're using!

@_date: 2006-03-23 20:43:58
@_author: David Malone 
@_subject: Linux RNG paper 
The problem is that we have to decide what out metric is before we
can give it a name. Shannon entropy is about the asympitic amount
of data needed to encode an average message. Kolmorogrov's entropy
(which got a mention here today) is about the shortest program that
can produce this string. These things aren't often important for
a PRNG or /dev/random like device.
One metric might be guessability (mean number of guesses required
or moments there of).  As you point out, Arikan and Massey have
shown that Shannon entropy are not particularly good estimates of
guessability. Generalisations of entropy, like Reni entropy do seem
to have meaning. The min-entropy mentioned in RFC 4086 seems
reasonable, though I don't think the rational given in the RFC is
actually correct.

@_date: 2006-03-27 14:20:48
@_author: David Malone 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits of entropy) 
That's not the whole problem - you have to be looking at the right
"average" too.
For the long run encodability of a set of IID symbols produced with
probability p_i, then that average is the Shannon Entropy.  If
you're interested in the mean number of guesses (per symbol) required
to guess a long word formed from these symbols, then you should be
looking at (\sum_i \sqrt(p_i))^2. Other metrics (min entropy, work
factor, ...) require other "averages".
To see this behaviour, you both need a large sample and the right
type of average to match your problem (and I've assumed IID).

@_date: 2006-05-20 11:58:10
@_author: David Malone 
@_subject: statistical inferences and PRNG characterization 
You could do this with relatively simple Bayesian classification.
Start with a prior assumption like "As far as I know it is 50/50
that it is source A or B" and then for the output you see you
calculate P(A|output) and P(B|outout) using Bayes rule, your
probabilistic model for the source and P(A) = P(B) = 0.5.
A finite number of sources is not required here, as long as you're
willing to provide a prior distribution over all possible sources
that you can do calculations with.
I think you're still going to run into the problem of deciding what
is random, and that problem will be tied up in your choice of prior
distribution on the sources.
I guess the usual proviso: these sort of calculations require
assumptions to make them possible, and the results should not be
confidently applied outside situations where those assumptions are

@_date: 2007-04-26 09:18:47
@_author: David Malone 
@_subject: open source disk crypto update 
Marc Schiesser gave a tutorial at EuroBSDcon 2005 on encrypting the
whole hard drive on your laptop using FreeBSD. If I remember right,
He used the trick of booting from a USB drive. The notes from his
tutorial are here:

@_date: 2008-02-20 09:12:30
@_author: David Malone 
@_subject: Irish blood donor records 
It seems that disk containing records of the Irish Blood Transfusion
service seems to have been stolen in New York:
Thankfully, the data was encrypted. The head of the IBTS said on
the news that there was a remote possibility of access, roughly
equivelent to winning the Euromillions Lottery 10 times in a row.
Based on the jackpot odds from:
they are probably thinking around lg(76275360)*10 = 261.84, or 256
bits of key.

@_date: 2008-03-20 09:13:23
@_author: David Malone 
@_subject: Firewire threat to FDE 
[This has been thrashed out on other lists.]
The OS can program the Firewire controller not to allow DMA.
Isn't what you're describing here an IOMMU?

@_date: 2009-07-19 12:43:33
@_author: David Malone 
@_subject: work factor calculation for brute-forcing crypto 
This has been dubbed the "guesswork" of a distribution by some authors,
I think originating with:
It turns out that its asymoptic behaviour is bit like that of Renyi
entropy, see:
I did an explanitory writeup of this with some experiments at:
