
@_date: 2001-12-27 01:20:31
@_author: Nomen Nescio 
@_subject: CFP: PKI research workshop 
Could someone define PKI (beyond just what it stands for, Public Key
Infrastructure)?  It looks like you are all talking past each other by
using the term to mean different things.

@_date: 2001-09-20 07:00:25
@_author: Nomen Nescio 
@_subject: chip-level randomness? 
The "whitener" is just a slightly improved von Neumann bias remover.
The tradition vN state machine looks at pairs of bits and does something
like this:
0 0  ->  discard
0 1  ->  output 1
1 0  ->  output 0
1 1  ->  discard
This removes a static bias.  I.e. if you are producing say 55% 0's
and 45% 1's, after this whitener you will output 50% 0's and 1's.
However it is at the cost of discarding a considerable fraction of
the bits.
The improved version in the Intel RNG has a 3 bit window and this
lets it remove the bias just as well while discarding somewhat
fewer bits.
If the internal circuitry did output a 60Hz sine wave then regularities
would still be visible after this kind of whitener.  It is a rather
mild cleanup of the signal.
It doesn't seem right to object to them including a bias remover.
They have done other things to reduce bias.  For example they use a pair
of thermal resistors located next to each other on the chip and use the
difference of the values from each of them, to reduce sensitivity to
environmental influences.  This reduces bias, but should they have left
the differencing out so that you could more easily measure a possible
Suppose the voltage were to drop to this part of the chip; the
differencing will hide this fact and prevent you from detecting that maybe
some other parts aren't working well.  Here is an example of a possible
partial-failure mode which the chip internal design will tend to hide.
It should not be considered a design flaw for the chip to do this.
It improves the random numbers which the chip produces.  And similarly,
the digital bias remover does the same thing.
The bottom line is the quality of the random numbers produced by the
device.  It is designed internally to withstand various kinds of noise
and bias, so as to produce the best random numbers possible.
See  for information on the
design of the RNG.  See if you can identify a plausible failure mode
which could be detected if the whitener was not present, but which will
be undetectable with the vN whitener in place.

@_date: 2001-09-21 03:10:05
@_author: Nomen Nescio 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
No, Provos' own system, Outguess,  is secure in the
latest version.  At least, he can't break it.  It remains to be seen
whether anyone else can.  See the papers on that site.

@_date: 2001-09-24 19:30:35
@_author: Nomen Nescio 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
If you read the report at
 you
will find that the authors, Niels Provos and Peter Honeyman, you find
that they actually found a great many images with statistical indication
of steganographic content: "After processing the two million images
with Stegdetect, we find that over 1% of all images seem to contain
hidden content."  That is, these images seemed to depart from normal
statistics to a significant degree.
The question is whether these are random variations from the norm or
are they actual embedded content?  This is the factor which the analysis
above seems to neglect.  Any statistical test is going to have a certain
number of false positives.  This provides a background of "noise"
(that is, false positives) in which a true signal (a true positive,
an image with actual steganographic content) can hide.
The Stegdetect paper proceeded to further analyze the 20000+ images by
looking for passwords that would produce meaningful messages from the
hypothesized hidden content, via dictionary attack.  No valid passwords
were found, and the authors concluded therefore that these were all
false positives.  This does not seem to be a fully supported conclusion.

@_date: 2002-08-10 20:40:21
@_author: Nomen Nescio 
@_subject: adding noise blob to data before signing 
Eugen Leitl asked:
You shouldn't need to salt/pad with random data, fixed data should be
Derek Atkins replied:
Actually, depending on the data being signed, it can be important to
hash for RSA.  After all, RSA is existentially forgeable: anyone can
forge a signature on a *random* value (if C=M^e mod n, then M is a
signature on C).  They might be able to try some large number of sigs
until they got a random value which looked enough like legitimate data
to be accepted - especially possible if the 1kbit value being signed
holds dense, random-ish binary data.

@_date: 2002-08-21 01:00:06
@_author: Nomen Nescio 
@_subject: Chaum's unpatented ecash scheme 
David Chaum gave a talk at the Crypto 2002 conference recently in which
he briefly presented a number of interesting ideas, including an approach
to digital cash which he himself said would "avoid the ecash patents".
The diagram he showed was as follows:
        Optimistic Authenticator
                                     z = x^s
Payer         f(m)^a z^b             Bank
      ----------------------------->
            [f(m)^a z^b]^s
      <-----------------------------
               m, f(m)^s
      ----------------------------->
It's hard to figure out what this means, but it bears resemblance to a
scheme discussed on the Coderpunks list in 1999, a variant on a blinding
method developed by David Wagner.  See
 at toad.com/msg02323.html for a
description, with a sketch of a proof of blindness at
 at toad.com/msg02387.html and
 at toad.com/msg02388.html.
In Chaum's diagram it is not clear which parts of the key are private and
which public, although z is presumably public.  Since the bank's action
is apparently to raise to the s power, s must be secret.  That suggests
that x is public.  However Chaum's system seems to require dividing by
(z^b)^s in order to unblind the value, and if s is secret, that doesn't
seem possible.
In Wagner's scheme everything was like this except that the bank's key
would be expressed as x = z^s, again with x and z public and s secret.
f(m) would be a one-way function, which gets doubly-blinded by being
raised to the a power and multiplied by z^b, where a and b are randomly
chosen blinding factors.  The bank raises this to its secret power s,
and the user unblinds to form f(m)^s.  To later deposit the coin he does
as in the third step, sending m and f(m)^s to the bank.
For the unblinding, the user can divide by (z^b)^s, which equals z^(b*s),
which equals (z^s)^b, which equals x^b.  Since x is public and the user
chose b, he can unblind the value.  Maybe the transcription above of the
Chaum scheme had a typo and it was actually similar to the Wagner method.
Chaum commented that the payer does not receive a signature in this
system, and that he doesn't need one because he is protected against
misbehavior by the bank.  This is apparently where the scheme gets
its name.

@_date: 2002-08-22 08:50:08
@_author: Nomen Nescio 
@_subject: Chaum's unpatented ecash scheme 
Actually the scheme described based on Chaum's talk (corrected for
probable typos) is essentially what you describe in your paper as the
Type II Defence, in section 5.  Your analysis shows that it is not
vulnerable to marking and is anonymous.
Speaking of anonymous, you should give credit in your paper to Anonymous
for discovering the possibility of marking Lucre coins, in a coderpunks
posting at
 at toad.com/msg02186.html, and for
inventing the Type II Defence, both in the posting above and amplifed
at  at toad.com/msg02323.html.
It may seem pointless to credit anonymous postings, but it makes the
historical record more clear.

@_date: 2002-08-28 23:00:07
@_author: Nomen Nescio 
@_subject: Cryptographic privacy protection in TCPA 
Carl Ellison suggested an alternate way that TCPA could work to allow
for revoking virtualized TPMs without the privacy problems associated
with the present systems, and the technical problems of the elaborate
cryptographic methods.
Consider first the simplest possible method, which is just to put a
single signature key in each TPM and allow the TPM to use that to sign
its messages on the net.  This is reliable and allows TPM keys to be
revoked, but it obviously offers no privacy.  Every usage of a TPM key
can be correlated as coming from a single system.
TCPA fixed this by adding a trusted third party, the "Identity CA" who
would be the only one to see the TPM key.  But Carl offers a different
Instead of burning only one key into the TPM, burn several.  Maybe even
a hundred.  And let these keys be shared with other TPMs.  Each TPM has
many keys, and each key has copies in many TPMs.
Now let the TPMs use their various keys to identify themselves in
transactions on the net.  Because each key belongs to many different
TPMs, and the set of TPMs varies for each key, this protects privacy.
Any given usage of a key can be narrowed down only to a large set of
TPMs that possess that key.
If a key is misused, i.e. "scraped" out of the TPM and used to create a
virtualized, rule-breaking software TPM, it can be revoked.  This means
that all the TPMs that share that one key lose the use of that key.
But it doesn't matter much, because they each have many more they can use.
Since it is expected that only a small percentage of TPMs will ever need
their keys revoked, most TPMs should always have plenty of keys to use.
One problem is that a virtualized TPM which loses one of its keys will
still have others that it can use.  Eventually those keys will also be
recognized as being mis-used and be revoked as well.  But it may take
quite a while before all the keys on its list are exhausted.
To fix this, Carl suggests that the TPM manufacturer keep a list of all
the public keys that are in each TPM.  Then when a particular TPM has
some substantial fraction of its keys revoked, that would be a sign that
the TPM itself had been virtualized and all the rest of the keys could be
immediately revoked.  The precise threshold for this would depend on the
details of the system, the number of keys per TPM, the number of TPMs that
share a key, the percentage of revoked keys, etc.  But it should not be
necessary to allow each TPM to go through its entire repertoire of keys,
one at a time, before a virtualized TPM can be removed from the system.
Carl indicated that he suggested this alternative early in the TCPA
planning process, but it was not accepted.  It does seem that while
the system has advantages, in some ways it shares the problems of the
alternatives.  It provides privacy, but not complete privacy, not as
much as the cryptographic schemes.  And it provides security to the TPM
issuers, but not complete security, not as much as the Privacy CA method.
In this way it can be seen as a compromise.  Often, compromise solutions
are perceived more in terms of their disadvantages than their benefits.

@_date: 2002-08-30 06:40:07
@_author: Nomen Nescio 
@_subject: Palladium and malware 
Paul Crowley asks: > I'm informed that malware authors often go to some
lengths to prevent > their software from being disassembled.  Could they
use Palladium for > this end?  Are there any ways in which the facilities
that Palladium > and TCPA provide could be useful to a malware author
who wants to > frustrate legitimate attempts to understand and defeat
their software?
Palladium provides a "curtained" memory area where software is supposed to
be relatively free from being accessed or modified.  (Apparently it can be
configured to be debugged when in this area by the use of "test keys".)
If disassembling a program requires access to it in memory, then this
curtained memory region could present an obstacle to disassembling.
However in most cases the memory would be loaded from the disk so having
access to the program on disk would allow you to disassemble it just as
well as if it were in memory.  From what has been said about Palladium,
it will not support encrypted programs.
However that might not stop a determined malware author from having the
program encrypt itself when it first runs (using the "virtual vault"
concept) and store the encrypted version on the disk, deleting the
original plaintext version.  The Palladium vault technology would
then make it impossible for any other software to decrypt that data.
The malware might not be able to use the regular Palladium program loader
to run its encrypted portion, but conceivably it could "manually" load
that encrypted data into the curtained area, decrypt it using a key
accessible only to itself, and then run.
This would still leave the program vulnerable until the first time it
runs, but afterwards it would be impossible to get at the program text
in the clear.
A program could reduce its window of vulnerability if it were distributed
in encrypted form, using a key that would be provided by a remote server
- which could even be any other infected computer!  The program stub
would start up, use Palladium attestation to connect reliably to another
virus-corrupted instance of itself on the net, and receive a global,
system-wide key.  This key would be kept locked in the "virtual vault"
and would decrypt the remainder of the program, which could then run in
the curtained area.  Even if all instances of the virus shared the same
key or set of keys, it would be impossible for non-virus programs or
programmers to get access to the key except by hacking their hardware,
scraping out a key, and creating a virtual Palladium system.
So the main question at this point is how program code and/or data gets
loaded into the curtained area, and whether it would be possible to
load encrypted data into that area, decrypt it and then run it as code.
There is a computer design called the Harvard architecture which has a
strict separation between code and data space, and conceivably Palladium
could use a similar approach to make it impossible to run decrypted code.
Adopting this approach would add credibility to Microsoft's promises
not to use Palladium for software copy protection.  But if they don't
go to such an extreme, it is likely that Palladium would allow the use
of various techniques to help malware hide from its opponents.

@_date: 2002-08-31 06:00:27
@_author: Nomen Nescio 
@_subject: Palladium and malware 
Bill Frantz writes, regarding the possibility that the Palladium
architecture could be designed to resist the use of encrypted Well, this is usually done by storing the data to the disk, and
then later loading it as a program file.  It does not prevent data
and code memory from being distinct, which was the proposal for how
Palladium could reduce the risk of being used to run encrypted code.
If a Palladium program was forced to go through the disk, that is, to
load data, decrypt it, store it to the disk, and then load it as code,
then that would provide a means to get access to the unencrypted code,
defeating the goal of keeping the code within the "vault".
It's not clear why these languages would use the Palladium features and
run their scripts in the shielded mode.  But you're right that if they
did, this could provide a mechanism for disassembly-resistant code.

@_date: 2002-12-10 21:20:11
@_author: Nomen Nescio 
@_subject: Hooray for TIA 
[I'm not happy with the tone of this, but I'm forwarding it as privacy
 politics is pretty clearly on topic... --Perry]
For years we cypherpunks have been telling you people that you are
responsible for protecting your own privacy.  Use cash for purchases, look
into offshore accounts, protect your online privacy with cryptography
and anonymizing proxies.  But did you listen?  No.  You thought to
trust the government.  You believed in transparency.  You passed laws,
for Freedom of Information, and Protection of Privacy, and Insurance
Accountability, and Fair Lending Practices.
And now the government has turned against you.  It's Total Information
Awareness program is being set up to collect data from every database
possible.  Medical records, financial data, favorite web sites and email
addresses, all will be brought together into a centralized office where
every detail can be studied in order to build a profile about you.
All those laws you passed, those government regulations, are being
bypassed, ignored, flushed away, all in the name of National Security.
Well, we fucking told you so.
And don't try blaming the people in charge.  You liberals are cursing
Bush, and Ashcroft, and Poindexter.  These laws were passed by the entire
U.S. Congress, Republicans and Democrats alike.  Representatives have
the full support of the American people; most were re-elected with
large margins.  It's not Bush and company who are at fault, it's the
whole idea that you can trust government to protect your privacy.
All that data out there has been begging to be used.  It was only a
matter of time.
And you know what?  It's good that this has happened.  Not only has
it shown the intellectual bankruptcy of trust-the-government privacy
advocates, it proves what cypherpunks have been saying all along, that
people must protect their own privacy.  The only way to keep your privacy
safe is to keep the data from getting out there in the first place.
Cypherpunks have consistently promoted two seemingly contradictory
ideas.  The first is that people should protect data about themselves.
The second is that they should have full access and usability for
data they acquire about others.  Cypherpunks have supported ideas like
Blacknet, and offshore data havens, places where data could be collected,
consolidated and sold irrespective of government regulations.  The same
encryption technologies which help people protect their privacy can be
used to bypass attempts by government to control the flow of data.
This two-pronged approach to the problem produces a sort of Darwinian
competition between privacy protectors and data collectors.  It's not
unlike the competition between code makers and code breakers, which has
led to amazing enhancements in cryptography technology over the past
few decades.  There is every reason to expect that a similar level of
improvement and innovation can and will eventually develop in privacy
protection and data management as these technologies continue to be
But in the mean time, three cheers for TIA.  It's too bad that it's the
government doing it rather than a shadowy offshore agency with virtual
tentacles into the net, but the point is being made all the same.
Now more than ever, people need privacy technology.  Government is not
the answer.  It's time to start protecting ourselves, because nobody
else is going to do it for us.

@_date: 2002-07-04 17:50:33
@_author: Nomen Nescio 
@_subject: Montgomery Multiplication 
Bear replied:
That's not Montgomery multiplication; that's what Knuth called modular
arithmetic, described in section 4.3.2 of Seminumerical Methods.
Montgomery multiplication is well described at
 as Paul Crowley

@_date: 2002-06-22 20:40:13
@_author: Nomen Nescio 
@_subject: Shortcut digital signature verification failure 
David Wagner describes a trick from Dan Bernstein to speed up
RSA signature verification with e = 3:
So the signer supplies s and k.  And our first step is to verify that
0 <= s^3 - kn < n.  But given that we've computed S = s^3 - kn and it
is in this range, isn't it the case that S = s^3 mod n?  And so we can
test test that H(m) = S = s^3 mod n and that verifies the signature

@_date: 2002-06-24 00:20:08
@_author: Nomen Nescio 
@_subject: Ross's TCPA paper 
Lucky Green writes regarding Ross Anderson's paper at:
It's an interesting claim, but there is only one small problem.
Neither Ross Anderson nor Lucky Green offers any evidence that the TCPA
( is being designed for the support of
digital rights management (DRM) applications.
In fact if you look at the documents on the TCPA web site you see much
discussion of applications such as platform-based ecommerce (so that
even if a user's keys get stolen they can't be used on another PC),
securing corporate networks (assuring that each workstation is running
an IT-approved configuration), detecting viruses, and enhancing the
security of VPNs.
DRM is not mentioned.
Is the claim by Ross and Lucky that the TCPA is a fraud, secretly designed
for the purpose of supporting DRM while using the applications above
merely as a cover to hide their true purposes?  If so, shouldn't we expect
to see the media content companies as supporters of this effort?  But the
membership list at shows none of the usual suspects.  Disney's not there.  Sony's not there.
No Viacom, no AOL/Time/Warner, no News Corp.  The members are all
technology companies, including crypto companies like RSA, Verisign
and nCipher.
Contrast this for example with the Brodcast Protection Discussion
Group whose ongoing efforts are being monitored by the EFF at
  There you do find the big media
companies.  That effort is plainly aimed at protecting information and
supporting DRM, so it makes sense that the companies most interested in
those goals are involved.
But with the TCPA, the players are completely different.  And unlike
with the BPDG, the rationale being offered is not based on DRM but on
improving the trustworthiness of software for many applications.
Ross and Lucky should justify their claims to the community in general
and to the members of the TCPA in particular.  If you're going to make
accusations, you are obliged to offer evidence.  Is the TCPA really, as
they claim, a secretive effort to get DRM hardware into consumer PCs?
Or is it, as the documents on the web site claim, a general effort to
improve the security in systems and to provide new capabilities for
improving the trustworthiness of computing platforms?

@_date: 2002-06-24 21:10:13
@_author: Nomen Nescio 
@_subject: Ross's TCPA paper 
Hmmmm.... Not clear that this really works to make money.  The GPL
allows everyone to redistribute HP's software verbatim, right?  So a
cert on one copy of the software will work on everyone's.  How can HP
make money on a product that everyone can copy freely, when they can
all share the same cert?
It's true that modified versions of the software would not be able to
use that cert, and it would no doubt be expensive to get a new cert for
the modified software.  But that still gives HP no monopoly on selling
or supporting its own version.  Anyone can step in and do that.
Is the cert itself supposed to be somehow copyrighted?  Kept secret?
Will it be illegal to publish the cert, to share it with someone else?
This seems pretty questionable both in terms of copyright law (since
a cert is a functional component) and in terms of the GPL (which would
arguably cover the cert and forbid restrictively licensing it).
It seems more likely that the real purpose is to bring the benefits of
TCPA to the Linux world.  As an innovator in this technology HP will gain
in reputation and be the source that people turn to for development and
support in this growing area.  The key to making money from open source
is reputation.  Being first makes good economic sense.  You don't need
conspiracy theories.

@_date: 2002-03-02 21:20:26
@_author: Nomen Nescio 
@_subject: Bernstein's NFS machine 
More analysis of Dan Bernstein's factoring machine from
The NFS algorithm has two phases.  The first searches for coefficients
(a,b) from some interval which are relatively prime and which satisfy
two smoothness bounds.  The smoothness is with respect to a factor base
consisting of all primes less than some bound.  (Actually there may be
two or more bounds used for different tests, but that complication will
be ignored here.)  Each success produces one relation and will be one
row of a matrix whose columns correspond to the primes in the factor
Once enough relations are collected (more rows than columns) the second
phase begins.  This involves looking for linear dependencies among the
rows of the matrix, where the math is in GF(2).  Once a dependency is
found, the corresponding rows can be combined to produce a value with
two different square roots, which with good probability will lead to
a factorization of n.
Dan Bernstein proposes methods to significantly reduce the cost (defined
as space times time) of both phases.
L is defined as exp(log(n)^(1/3) * log(log(n))^(2/3)), where n is the
number to be factored.  It is a key parameter for the work cost of the
NFS class of algorithms.
    n      L
 2^512   2^33
 2^1024  2^45
 2^1536  2^54
 2^2048  2^61
Let E represent the bound on the (a,b) coefficient values which are
searched.  Then E^2 values will be checked for relative primality and
smoothness.  Let B represent the bounds on the factor base.  A smooth
value is one for which all of its factors are less than B.
In previous work, sieving is used to test for smoothness, which is in
part where the Number Field Sieve algorithm gets its name.  Sieving
E^2 values over a factor base of size B takes roughly E^2 log log B
time and B space.  We will ignore the log log B factor and just say
E^2 and B space, for a total work factor of B*E^2.
With a factor base of size B, roughly B relations have to be found.
Then a sparse B by B matrix must be solved in the second phase.
Using conventional algorithms this solution takes roughly B^2 work on
a machine that has B space.  The total work factor is B^3.
For optimum balance, the work factor of these two phases is set equal,
giving B^3 = B*E^2, or B=E.  It is also necessary that E be large
enough to produce at least B relations.  This leads to the solution
E = B = L^0.961.  The corresponding work factor is this value cubed,
or L^2.883.  (Dan Bernstein gives a slightly lower figure due to an
improvement by Don Coppersmith.)
Bernstein improves on both of these phases.  He points out that for
sufficiently large n, it will be less work to determine smoothness by
simply trying to factor each value using the Elliptic Curve factoring
method (ECM).  This is the best factoring method known for finding
relatively small factors, as is required here.  Attempting to factor
E^2 values over a factor base of size B takes time proportional to
E^2 times a function of B which is larger than that in the sieve
case, but still asymptotically small compared to E^2.  So the time
is basically proportional to E^2 as with the sieve.
The savings comes in the space.  The sieve requires an array to hold
the primes in the factor base, and an array to represent the values
being sieved.  The most efficient approach is to set these two arrays
to roughly the same size, breaking the input into pieces of size B.
This does not slow the algorithm but allows it to run in space B as
assumed above.
In contrast, the ECM algorithm requires no large storage spaces.
It does not have to store the factor base or any representation of a
large block of numbers to be checked.  It simply runs the ECM factoring
algorithm on each value to be checked until it either finds a factor or
times out.  If it finds one, it divides it out and repeats the process
with the residue.  Eventually the residue is small enough that we know
the number is smooth, or the algorithm fails indicating that the value
is not smooth.  Storage requirements are minimal.  Based on this, the ECM
approach takes time proportional to E^2 and space essentially a constant,
for a total work of E^2.  This is compared with B*E^2 for the sieve.
Bernstein also improves on the second phase, finding a dependency among
the rows of the matrix.  It still requires space of size B to hold the
sparse matrix of size B by B.  But by making the matrix elements be active
instead of dumb memory cells, he can accelerate the matrix operations
that are necessary.  The result is that finding the dependencies is
reduced to taking time B^1.5 rather than B^2.  The total work is then
For optimality these two phases are balanced with E^2 = B^2.5 or
E=B^1.25.  Incorporating the requirement that E be big enough to
generate the necessary B relations, the solution is E=L^0.988 and
B=L^0.790.  The total work is E^2 or L^1.976.  This is compared to the
L^2.883 value above, a significant savings.
It is interesting to compare the approaches used to accelerate the
two phases.  In each case we are essentially replacing memory with
computational elements, but the kinds of computation being done are very
The matrix solution phase requires a high degree of parallelism.
The matrix-storage elements need to be enhanced with high speed local
interconnects and rudimentary processing capable of simple comparison
and exchange operations.  This would be an attractive candidate for a
custom IC where the basic processing cell would be small and laid out
in many replicating units on a chip.
In the smoothness testing phase, on the other hand, the requirements for
the computing elements are much greater.  Each one needs to be capable
of running the ECM factoring algorithm on large values of hundreds of
bits in size.  This is a relatively complex algorithm and would probably
require what is essentially a CPU with some kind of firmware to hold
the program.  It would probably not have to be as large and complex as
modern CPUs, but would be much more than the simple augmented memory
cell needed in the matrix solution phase.
The smoothness testing phase has the advantage that the individual
tests can be run independently, without any need to interconnect the
processing units (for other than I/O).  There is more flexibility in
terms of time/space tradeoffs in this phase.  If we need to use fewer
units and take more time, that will not affect the total work involved.
(Possibly a good architecture for the smoothness phase would be similar
to the 1980s-era Connection Machine from Thinking Machines.  The CM-2
had up to 64K processors with up to 32K of memory per processor.  It used
a SIMD (single instruction multiple data) architecture which meant that
each processing node ran the same instruction at the same time.  However
there were condition flags which could be set to selectively disable
instructions on certain processors based on the result of earlier tests.
This allows different nodes to behave differently depending on the data.)
The question remains at what point these improvements become effective.
In the first phase, the elementary step of the sieving machine is to add
two numbers together.  For the ECM machine it is to run the entire ECM
algorithm on a test value.  Both of these take asymptotically constant
time, but obviously the ECM operation will be far slower than the
sieving one.  Only by replacing passive sieve memory with active custom
ECM engines will the potential performance advantage of the ECM machine
be realized.
Note that the improvements to the two phases can be applied independently.
Either the speedup of the factoring or the speedup of the linear algebra
could be used with the other phase being done in the conventional way.
Because the nature of the machines for the two phases is very different,
it is likely that they have different crossover points at which the new
techniques become advantageous.  So it is very likely that there are
key sizes where only one of the new techniques should be used.
A final point: the factoring phase gets a greater improvement (exponent
reduced from 3 to 2) than the linear algebra phase (exponent reduced
from 3 to 2.5).  This leads to the use of a smaller factor base than
in the conventional method.  A smaller factor base is helpful for the
linear algebra since it gives a smaller matrix, but it is harmful to
the smoothness test since fewer numbers will be smooth over the smaller
factor base.  The greater speedup of the smoothness phase accounts for
this shift in emphasis.

@_date: 2002-03-03 10:20:12
@_author: Nomen Nescio 
@_subject: Bernstein's NFS machine 
Bernstein basically treats memory and processing elements as
interchangeable to a first approximation.  After all, it's just different
ways of laying out silicon.  So assuming that space comes for free is
equivalent to assuming that you have an infinite number of processors
to bring to bear on the problem.
If you had an unlimited number of processors for the first phase, and
you have X number of values to check for smoothness, then you can use X
processors and run one test on each value, completing the whole thing in
time L^o(1).  This does not reduce Bernstein's cost metric but it would
reduce the time considerably.  I don't think such a speedup is possible
with the sieving approach.
However this is not physically reasonable, as the number of values to
be tested for smoothness is approximately L^2, which would be on the
order of 2^90 for a 1024 bit key.  If each cell were .01 microns on a
side you would still need 100,000 square kilometers of chip area.  So
this extrapolation is not very meaningful.

@_date: 2002-03-05 00:00:12
@_author: Nomen Nescio 
@_subject: Bernstein's NFS machine 
No, this is not quite right.  The 2^71 is the cost in terms of elementary
operations (add, xor, etc.).  This is based on 2^53 ECM factorizations
per machine times 2^18 operations per factorization.
James' quoted time would be correct if the machine could do 50 billion
*operations* per microsecond (a clock rate of 50,000,000 gigahertz
(50 petahertz), compared to 1-2 gigahertz available today).  This fast
machine would then take 100 million years to break a 1024 bit key.
Even though these are "back of the envelope" calculations which ignore
o(1) terms that could theoretically be negative, the basic principles seem
sound.  The need to do 2^89 tests to get enough relations for a factor base
of 2^36 is based on the known fraction of multi-hundred bit numbers that
are going to be smooth.  This value doesn't have much "wiggle room".  You
could try to use a smaller factor base, but the size of the values to be
tested for smoothness is going to be in the hundreds of bits, and reducing
the factor base will make it even harder to find smooth values.
The 2^18 estimate for the cost of the ECM smoothness test will likewise
be hard to improve on significantly.  In addition, with a factor base of
size 2^36 and testing numbers up to ten times longer, each ECM factorizer
will have to do approximately 10 factorizations before it can determine
Even with the uncertainties, this calculation seems to be strong enough
to say that this machine cannot pose a threat to 1024 bit keys using
near-term technology.  You can assume impossibly large numbers of an
impossibly fast machine, and it still takes an impossibly large time.

@_date: 2002-03-27 16:10:05
@_author: Nomen Nescio 
@_subject: ecash news: Brands & credentica.com 
For cpunxnews/cryptography:
Seems people missed this anonymous note about Dr Stefan Brands new
company  on cypherpunks -- interesting news

@_date: 2002-05-02 00:20:26
@_author: Nomen Nescio 
@_subject: Lucky's 1024-bit post [was: RE: objectivity and factoring analysis 
This is probably not the right way to approach the problem.  Bernstein's
relation-finding proposal to directly use ECM on each value, while
asymptotically superior to conventional sieving, is unlikely to be
cost-effective for 1024 bit keys.  Better to extrapolate from the recent
sieving results.
 is the paper
from Eurocrypt 2000 describing the first 512 bit RSA factorization.
The relation-finding phase took about 8000 MIPS years.  Based on the
conventional asymptotic formula, doing the work for a 1024 bit key
should take about 10^7 times as long or 80 billion MIPS years.
For about $200 you can buy a 1000 MIPS CPU, and the memory needed for
sieving is probably another couple of hundred dollars.  So call it $500
to get a computer that can sieve 1000 MIPS years in a year.
If we are willing to take one year to generate the relations then
($500 / 1000) x 8 x 10^10 is $40 billion dollars, used to buy
approximately 80 million cpu+memory combinations.  This will generate
the relations to break a 1024 bit key in a year.  If you need it in less
time you can spend proportionately more.  A $400 billion dollare machine
could generate the relations in about a month.  This would be about 20%
of the current annual U.S. federal government budget.
However if you were limited to a $1 billion budget as the matrix
solver estimate assumed, the machine would take 40 years to generate
the relations.
The $40 billion, 1-year sieving machine draws on the order of 10 watts
per CPU so would draw about 800 megawatts in total, adequately supplied
by a dedicated nuclear reactor.

@_date: 2002-11-05 23:30:06
@_author: Nomen Nescio 
@_subject: patent free(?) anonymous credential system pre-print 
Stefan Brands writes regarding And balanced against all these numerous shortcomings, there is one
inescapable, overwhelming fact:
THE AUTHORS ARE MAKING THE FRUITS OF THEIR LABOR AVAILABLE FREELY FOR
THE WORLD TO USE.
With all of your patents, and your writings, and your self-promotion,
how many people are using your certificates in the real world?  Think how
much you could have accomplished, how much of a difference you could have
made, if you had been willing to sacrifice the hope of great riches.
Instead you have followed in the footsteps of your mentor Chaum, and
both of you have withheld your talent from the world.
What is it about cash and credential systems that everyone who works
in the area thinks they should patent their results?  All you have
accomplished is to make sure that no implementations exist!  What good
are your great ideas if no one can use them?
Look at Chaum!  Is that where you want to be in 20 years?  Bitter and
barren?  Cut off from the cryptographic community?  Reduced to publishing
via the government patent office?
That's no life for a great mind.  Creativity demands interaction with
an active and vital intellectual community.  You have to give in order
to take.  Building walls around your intellectual property shuts others
out even as you shut yourself in.
If you really want to accomplish something meaningful, rather than
continuing to hype and shill for a system which no one can use without
entering into delicate financial negotiations, why not make it available
on some basis for people to experiment with?  Maybe a non-commercial,
open-source GPL implementation could be a starting point.  There is
considerable interest in reputation systems among the P2P community
and credentials could be a part of that.  You can still protect your
commercial interests while letting people get familiar with the technology
by making a non-commercial library available.
That's just one possibility.  The point is, your ideas are going nowhere
using your present strategy.  Either this technology won't be used at
all, or inferior but unrestricted implementations will be explored,
as in the recent work.  If you want things to happen differently, you
must change your strategy.

@_date: 2002-09-02 00:10:06
@_author: Nomen Nescio 
@_subject: Cryptographic privacy protection in TCPA 
It looks like Camenisch & Lysyanskaya are patenting their credential
system.  This is from the online patent applications database:
Some of the claims seem a little broad, like this first one:
Wouldn't this general description cover most proposed credential systems
in the past, such as those by Chaum or Brands?
Does anyone know how to contact the PTO regarding proposed patents,
perhaps to point out prior art?

@_date: 2002-09-19 02:10:06
@_author: Nomen Nescio 
@_subject: Cryptogram: Palladium Only for DRM 
Could you say something about the sense in which Palladium achieves
BORE ("break once run everywhere") resistance?  It seems that although
Palladium is supposed to be able to provide content security (among
other things), a broken Palladium implementation would allow extracting
the content from the "virtual vault" where it is kept sealed.  In that
case the now-decrypted content can indeed run everywhere.
This seems to present an inconsistency between the claimed strength of the
system and the description of its security behavior.  This discrepancy
may be why Palladium critics like Ross Anderson charge that Microsoft
intends to implement "document revocation lists" which would let Palladium
systems seek out and destroy illicitly shared documents and even programs.
Some have claimed that Microsoft is talking out of both sides of its
mouth, promising the content industry that it will be protected against
BORE attacks, while assuring the security/privacy community that the
system is limited in its capabilities.  If you could clear up this
discrepancy that would be helpful.  Thanks...

@_date: 2003-04-18 01:50:04
@_author: Nomen Nescio 
@_subject: DMCA Crypto Software 
Cryptography Research's content protection system is summarized at
and apparently described in more detail in the patent application at
  The basic idea is to include some
executable code with the content which would run in a VM in every content
player.  This code would then make use of pre-existing low-level crypto
functions and keys built into the players to decrypt and play the content.
The main advantage of this approach is that as various content protection
schemes are cracked, or keys extracted, future releases of content
can be updated to use new schemes and to invalidate the stolen keys.
Unlike current approaches which build the technology into firmware in
players, this method would be much more flexible and adaptable in the
face of ongoing attacks.
Note that there would still be limits on what could be done in terms of
revising protection schemes, based on the cryptographic primitives which
are built into players.  If some future design for content protection
were invented which relied on the Weil pairing, for example, it would
not be implementable if the necessary primitives are not supported on
widely used devices.
A disadvantage is that each player must be powerful enough to run this
VM program at speed.  It's not clear how complex the programs will be
or how fast they will have to run.  Providing general purpose computing
functionality may be more expensive and difficult than implementing a
special-purpose standard in firmware and hardware, especially for small
portable devices.  The CR report estimates that a 1 MIPS processor would
be adequate, but they don't offer any justification.
All cryptography-based protection schemes have a fundamental flaw, which
is that keys "scraped" out of authorized devices can be used to unlock
data and then release it in unprotected form.  CR proposes to address this
by watermarking the data.  For this to work, the watermark must inherently
reveal the key which was used for the decryption.  This involves the
technique broadly known as traitor tracing.  The problem is that if the
bad guys have access to a considerable number of stolen keys it may be
intractable to devise a traitor tracing scheme that can identify them.
CR advocates "forensic watermarking".  In the longer report (available by
email request) they describe this as a system where there are two versions
of selected portions of the content - for example, two alternate versions
of a particular movie frame.  There would be multiple such "polymorphs"
throughout the content, and each device would have keys such that for each
polymorph it would see only one version.  By randomizing and encrypting
the frames it can be arranged that the devices can't even tell which
frames are polymorphic.  The set of keys assigned to a playback device
implicitly identifies the device itself, so that if an unprotected version
of the movie is released, the specific versions of the polymorphs that
are present will reveal which device did the decryption.
The obvious attack is to combine the output from multiple devices
from which keys have been scraped, but this does not work (up to a
point) because even when multiple devices are used, there is still
enough information in the output to identify which specific devices
were involved.  CR gives an example of a 90 minute movie, 30 frames per
second, with 1% of the frames being polymorphic - 1620 frames.  Even if
an adversary breaks into 4 playback devices and gets their keys in order
to identify the polymorph frames, the manufacturer can identify those four
devices with an error probability, according to the formula derived by the
CR report, of less than 4 x 10^(-10), an extremely good detection rate.
But what happens if you use the CR formula with the assumption that
the attacker cracks one more device for a total of 5?  Suddenly the
system doesn't work so well, and there are over 10^20 possible sets
of 5 devices that could produce the combined output!  We go from
4 x 10^(-10) to 10^20 with just one more device.  This kind of exponential
explosion is common to many traitor tracing schemes.  The attackers
have an inherent mathematical advantage which is very hard to address.
All this is glossed over in the CR analysis.
And this is leaving aside the question of whether we can create
polymorphic frames at all - 2 different versions of a movie frame, either
of which is equally plausible as an intermediate frame among the others
in the sequence.  It must be done so that looking at one version of the
movie or the other will not call attention in any way to the frames which
have been altered like this.  And it has to be doable in a completely
(or at least largely) automated way, due to the volume of content that
would have to be protected.
It also would be desirable to create content that can run on all future
players, at a time when many of them may not yet have been built or even
designed.  For example, pressing a CD today, it should still play on CD
players built 10 years from now.  Could the content be unlocked by devices
whose keys have not yet been chosen, but still be watermarked so as to
reveal those keys?  Or would we need a centralized key-creation agency
which pre-creates all of the keys which will ever be used in a CD player
for the rest of time, and then assigns them to manufacturers who enter
the CD business?  Most of these traitor tracing and broadcast-encryption
schemes have been implemented in closed environments like satellite TV
broadcasts, but the complexity of a multi-vendor world and an open-ended
future may push this technology beyond its current limitations.
Even in the most optimistic assessment, what we would expect to see
is a constant struggle between "protectors" and "unlockers".  Each
success by the unlockers will be thwarted in the next generation of
content released by the protectors.  But the old content will still be
vulnerable.  So we would expect that this approach would yield a time
window for protection.  At the time it was released, new content would
use the latest techniques and be relatively secure.  Over time, breaks
would occur to which content released in the past would be vulnerable.
So content in general would have a limited time in which it was protected.
This would be the window in which profitability was possible.  Content
older than some threshold - weeks?  months?  years? - would be freely

@_date: 2003-04-18 05:10:07
@_author: Nomen Nescio 
@_subject: [IP] Data sent to Microsoft by "Windows Update" (fwd) 
That's not quite right.  It does not send "a registry subkey listing
the vendor of every software package installed on your computer."
Nothing like that is sent, according to the article.
The product code is not digitally signed, it is encrypted with XTEA.
The article didn't say how they found the XTEA decryption key, probably
more hooking.  It includes a hash of the full product key, the long
string printed on a sticker on the CD box.  The product key (which is
not sent, just its hash) supposedly does include a digital signature,
but the article didn't say anything about the algorithm or the keys used.

@_date: 2003-04-19 06:30:03
@_author: Nomen Nescio 
@_subject: DMCA Crypto Software 
You're forgetting that different subsets of users could share the same
polymorph bits.  From the Cryptography Research report:
   If k out of N decoders collude to try to remove a forensic mark,
   there are C(N,k) possible sets of colluders.  A set of colluders can
   be excluded if no set of members could decode the observed version of
   a polymorph.  If each version of each poymorph can be decrypted by
   an independent random 50% of decoders, each polymorph in the output
   excludes (1/2)^k = 2^(-k) of the collusion sets.  If a total of p
   polymorphs are present, the expected number of non-excluded collusion
   sets is (1 - 2^(-k))^p * C(N,k).
(Here I wrote C(N,k) for the choice function, the number of ways of
choosing k objects from a set of N.)
If you substitute in N = 1 billion, p = 1620 and k=4 or 5, you get the
results presented earlier.  As a trivial case, if you set k=1 you get
N / 2^p, i.e. if you have more than 2^p users, some of them will have
exactly the same set of polymorphs and you can't tell them apart.
One additional point: aside from the earlier difficulties mentioned in
creating these marks, there is also the possibility that they can be
located just by comparing the analog output from two different decoders.
This would allow identification of which are the polymarked frames
without breaking any of the security.  Then it might be possible with
a single broken system to corrupt the marks in those frames (say,
by replacing those frames by an interpolation of surrounding frames)
without significantly degrading the overall quality of the movie if only
1% of frames are marked.
If marking is so subtle that it is not visible in an analog recording,
then an alternative strategy of randomizing the watermark channel may
work.  For example if marks affected only the LSBs of a small portion of
the polymorph frames, then all frames could have their LSBs randomized,
eliminating all the marks with perhaps not much effect on the quality.
It doesn't sound easy to choose marks which can't be eliminated blindly
but also can't be identified by inspecting unbroken versions from
multiple players.  They'll have to walk a fine line to avoid either of
these errors.
By the way, could the moderator arrange for the list archive at
 to be updated for the new address?

@_date: 2003-04-22 01:30:02
@_author: Nomen Nescio 
@_subject: the futility of DRM (Re: DMCA Crypto Software) 
Their intention is apparently not to watermark the user's identity
into the output in the sense of his name or credit card info; but
rather, to watermark an identifier of the unique key corresponding to
the player device (and possibly the keys of other devices in the data
processing chain).  The idea is that if high-quality digitized content
starts appearing on the net with its protection broken, you can use the
forensic watermark to go back and figure out which machine got hacked
in order to extract this data.  Then, future releases can exclude that
machine from being able to play them, in much the same way that satellite
broadcasts today exclude crypto cards which are no longer valid.
Their system allows things like DVDs to be pressed, distributed,
sit on shelves, and finally sold, all identical, and to create the
player-specific watermarks on the fly.
I suspect you are right that DRM ultimately cannot work for content
like movies and music.  Anything the human eye can see can be recorded
and transmitted.  The only way to prevent this would be extremely strict
technological regulation like hypothetical A-to-D converter restrictions.
Even then people will bootleg unauthorized hardware.
However DRM could be more successful in principle with active content like
video games.  You could have some kind of encrypted program which only
executes on a "black box" processor.  This does not seem to contradict
the laws of physics, as DRM of music and movies does.  Perhaps in the
future, all content will be active like games are today.  Imagine a song
or movie which was different every time you played it.  Static recordings
like we are familiar with might seem cheap, 1-dimensional shadows of
the real thing.
The idea is that any such D->A->D conversion will introduce at least some
loss of quality.  Just as we see people being convinced today that super
audio CDs produce better sounding music than regular CDs (despite the
bandwidth limits of CDs being far higher than almost anyone can hear),
in the future people will be sold the idea that "pure" content is higher
quality than the free bootlegs on the net.  And technically the content
companies will be right, the original quality will be higher.  I agree
with you that with good enough technology, the difference can be made
arbitrarily small, but that may take some time.  Today it would probably
be difficult to reconstruct an HDTV quality picture by sticking a video
camera in front of a monitor, even with professional equipment.
I think the bottom line is that while the DRM strategy cannot work to
protect movies and music in the long run, it may have short term success
(the next few years, or even a decade or more).  And we should expect
to see a shift to active content over the next few decades, along with
more technology for black-box processors that execute encrypted code.

@_date: 2003-04-30 20:20:04
@_author: Nomen Nescio 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
Given that Kocher is one of the smartest and savviest security experts
out there, how can he make absurd statements like those above?  We've
discussed here how impractical these watermarking systems are, how easy
it is to identify and remove the watermarks, given just a few systems.
His "provably secure" example worked fine with four conspirators, but
totally fell apart with five, as we saw.  This is a general property of
traitor tracing type watermarking schemes.  The provable security is
meaningless in the real world, because the limitations assumed in the
proofs are too easy to beat.
Surely someone with Kocher's qualifications knows this.  Is he being
duplicitous, exaggerating the beneficial properties of his system in
the hopes of passing off shoddy work to his clients, or perhaps with
some political goal of misleading the content companies into using
worthless cryptography?  That seems highly unlikely, given his usual
personal integrity.
Or is it possible that our technical analyses are mistaken, and that
it is actually possible to program devices to watermark the content
they play in such a manner that the marks can't be practically removed?
If so, what is the secret?

@_date: 2003-01-08 01:00:01
@_author: Nomen Nescio 
@_subject: DeCSS, crypto, law, and economics 
I don't see much evidence for this.  As you go on to admit, multi-region
players are easily available overseas.  You seem to be claiming that the
industry's main goal was to protect zone locking when that is already
being widely defeated.
Isn't it about a million times more probable that the industry's main
concern was PEOPLE RIPPING DVDS AND TRADING THE FILES?  Movies are
freely available on the net, just like MP3s, and the DeCSS software was
the initial technology that made ripping DVD's possible.  Many people
would rather get something for free than to pay for it, and DVD ripping
allows that for movies.  The MPAA obviously is afraid of following the
RIAA into oblivion.

@_date: 2003-01-15 01:25:01
@_author: Nomen Nescio 
@_subject: RIAA turns against Hollings bill 
The New York Times is reporting at
 that
the Recording Industry Association of America, along with two computer
and technology industry trade groups, has agreed not to seek new
government regulations to mandate technological controls for copyright
protection.  This appears to refer primarily to the Hollings bill,
the CBDTPA, which had already been struck a blow when Hollings lost his
committee chairmanship due to the Democrats losing Senate leadership.
Most observers see this latest step as being the last nail in the coffin
for the CBDTPA.
Some months ago there were those who were predicting that Trusted
Computing technology, as embodied in the TCPA and Palladium proposals,
would be mandated by the Hollings bill.  They said that all this talk of
"voluntary" implementations was just a smoke screen while the players
worked behind the scenes to pass laws that would mandate TCPA and
Palladium in their most restrictive forms.  It was said that Linux would
be banned, that computers would no longer be able to run software that
we can use today.  We would cease to be the real owners of our computers,
others would be "root" on them.  A whole host of calamaties were forecast.
How does this latest development change the picture?  If there is no
Hollings bill, does this mean that Trusted Computing will be voluntary,
as its proponents have always claimed?  And if we no longer have such
a threat of a mandated Trusted Computing technology, how bad is it for
the system to be offered in a free market?
Let technology companies decide whether to offer Palladium technology
on their computers or not.  Let content producers decide whether to use
Palladium to protect their content or not.  Let consumers decide whether
to purchase and enable Palladium on their systems or not.
Why is it so bad for people to freely make their own decisions about
how best to live their lives?  Cypherpunks of all people should be the
last to advocate limiting the choices of others.  Thankfully, it looks
like freedom may win this round, despite the efforts of cypherpunks and
"online freedom" advocates to eliminate this new technology option.

@_date: 2003-07-08 23:30:06
@_author: Nomen Nescio 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect Online Messages 
One difference is that with the identity-based crypto, once a sender
has acquired the software and the CA's public key, he doesn't have to
contact the CA to get anyone's "certificate".  He can encrypt to anyone
without having to contact the CA, just based on the email address.
Your proposed substitute doesn't allow for this.
The Weil pairing hardly constitutes "goofy new crypto".  They are
doing all kinds of cool stuff with pairings these days, including
privacy-enhancing technology such as public keys with built-in forward

@_date: 2003-06-01 22:20:06
@_author: Nomen Nescio 
@_subject: web apps with large volumes of bidirectional http traffic 
It's not, really.  Maybe some kind of cover traffic for an underlying
hidden data stream?
Or maybe you will create a client which mimics a human person, and
sends information by choosing what to click on?  Sounds like a pretty
low-bandwidth channel.
These two don't make much sense together: someone is going to sit
there, interacting with a server for hours, clicking every second?
That sounds more like a job than something people would do voluntarily.
You're either going to need to accept a much lower click rate, or else
a much shorter connection time.
Okay, maybe one of the online fantasy games services like EverQuest?
I hear people get addicted to those and spend hours on them, but I
think they do a lot of typing rather than mouse clicks.  Or one of the
shoot-em-ups?  Those probably use the mouse a lot more.  They aren't
really "web applications" though.  Does it really have to be a web app?
Most of those won't satisfy your click rate and connect time goals.
Maybe you could give some more clues...

@_date: 2003-06-10 07:00:08
@_author: Nomen Nescio 
@_subject: An attack on paypal --> secure UI for browsers 
It was none other than Microsoft's NGSCB, nee Palladium.  See
   NEW ORLEANS--Microsoft is trying to make security obvious.
   The software giant plans to visually alter document or application
   windows that contain private information that's secured through
   Microsoft's Next-Generation Secure Computing Base (NGSCB), formerly
   known as Palladium. Secure windows will look different than regular,
   unsecured windows in order to remind users that they are looking
   at confidential material, Peter Biddle, product unit manager for
   Microsoft, said Thursday at the Windows Hardware Engineering Conference
   (WinHEC) here.
   ...
   The border of a secured page may contain information--such as the
   names of all the dogs that someone has ever owned--to make the data
   instantly recognizable as sound to the individual owner, as well as
   difficult to replicate. A hacker can create a spoof page with dogs'
   names running along the border but, in all likelihood, not one reading
   "Buffy, Skip and Jack Daniels--and in that order," Biddle said.
   ...
   Information on secured windows will vanish if another window is placed
   on top of it or shifted to the background. Erasing the information
   will prevent certain types of attacks and remind people that they're
   dealing with confidential material, Biddle said.
   When the secure window returns to the top of the stack, the information
   will reappear, he said.
I don't see how this is going to work.  The concept seems to assume
that there is a distinction between "trusted" and "untrusted" programs.
But in the NGSCB architecture, Nexus Computing Agents (NCAs) can be
written by anyone.  If you've loaded a Trojan application onto your
machine, it can create an NCA, which would presumably be eligible to
put up a "trusted" window.
So either you have to configure a different list of doggie names for
every NCA (one for your banking program, one for Media Player, one for
each online game you play, etc.), or else each NCA gets access to your
Secret Master List of Doggie Names.  The first possibility is unmanageable
and the second means that the trustedness of the window is meaningless.
So what good is this?  What problem does it solve?

@_date: 2003-06-10 22:30:08
@_author: Nomen Nescio 
@_subject: Keyservers and Spam 
The solution to this problem is simple.  We want to be able to look
up keys on the key servers by email address or user name or keyid.
But we don't want the system to be useful for spam harvesting.
Simply require that lookups be by valid email address or user name.
Eliminate the wildcard searching.  Then spammers won't be able to find
email addresses in a very efficient or useful way.
Now, it may be argued that this is too strict, that we do need some
wildcard searches because of slight variations in spelling of email
addresses and names.  Fine, we can allow this without allowing full
wildcarding.  Supporting a "loose search" mode where some letters are
different or some email hostname components vary will solve the problem
without letting spammers snarf the whole keyring.
Keep in mind, first, that there are many other sources of email
addresses on the net, and second, that many (or most!) of the keys on
the keyservers use obsolete email addresses.  Key servers are not a fat
target for spammers.  But the trivial measures above would go a long
way towards eliminating the problem.

@_date: 2003-06-12 10:50:10
@_author: Nomen Nescio 
@_subject: The real problem that https has conspicuously failed to fix 
Of course it would help.  Have you been following this discussion
at all?  The idea is to eliminate passwords as being of any value in
getting access to PayPal or other ecommerce sites, by replacing them
with client certificates.  This implies using https or something
cryptographically similar.

@_date: 2003-06-12 11:30:04
@_author: Nomen Nescio 
@_subject: An attack on paypal 
It may be that real authentication is hard, but the unbelievably sloppy
practices of domain name registrars doesn't prove the case.
Imagine if property ownership were recorded with the same degree of rigor.
"I'm sorry, sir, but you don't own your house any more.  We received a
typewritten letter with your name on it saying you were transferring
ownership to ShoppingMall Inc.  The demolition teams are moving in,
and I'm afraid you'll have to be out by Friday."
Domain names are handled carelessly while real estate is not, due to
many factors.  Probably one of the main ones is the relative immaturity
of the domain name system compared to the centuries of experience we
have evolving mechanisms to deal with real property.
Clearly the registrars are making little or no effort to authenticate
domain name transfers at present.  At one time you could specify that only
messages signed with a given PGP key would authorize a transfer, but that
precaution has apparently disappeared, no doubt due to lack of interest
and the costs of support.  Maybe this could be something that a registrar
could use to differentiate itself from the many otherwise-identical
competitors in the market: we won't let your domain names get stolen.
What a novel concept.

@_date: 2003-06-27 22:30:48
@_author: Nomen Nescio 
@_subject: pubkeys for p and g 
Do you have a reference to what exactly Check Point says about this?
Maybe you are misunderstanding or misinterpreting them.  If you could
quote it here verbatim (or provide a link if it is online) we might be
able to understand their claim better.  It would be wise to make sure
it is not a simple misunderstanding before you put something critical
about them in your book.

@_date: 2003-03-11 20:20:02
@_author: Nomen Nescio 
@_subject: Proven Primes 
The Handbook of Applied Cryptography, has a chapter on efficient implementations which might provide some
You can take advantage of the left FFF's by using the modular reduction
algorithm described in section 14.3.4 of the HAC.  This is good for the
case where the modulus is slightly less than a power of 2.  Or you can
take advantage of the right FFF's by using Montgomery exponentiation,
described in section 14.3.2 of the HAC and also in algorithm 14.94.
Montgomery multiplication uses a value m' = - m^(-1) mod b, where m is
the modulus and b is the bignum base, typically 2^32 or 2^64.  With these
moduli m' becomes 1, simplifying the calculations.

@_date: 2003-03-24 19:20:02
@_author: Nomen Nescio 
@_subject: Brumley & Boneh timing attack on OpenSSL 
Regarding using blinding to defend against timing attacks, and supposing
that a crypto library is going to have support for blinding:
 - Should it do blinding for RSA signatures as well as RSA decryption?
 - How about for ElGamal decryption?
 - Non-ephemeral (static) DH key exchange?
 - Ephemeral DH key exchange?
 - How about for DSS signatures?
In other words, what do we need as far as blinding support either in
developing a crypto library or in evaluating a crypto library for use?
Suppose we are running a non-SSL protocol but it is across a real-time
Internet or LAN connection where timing attacks are possible.  And suppose
our goal is not to see a paper and exploit published within the next
three years telling how to break the protocol's security with a few
hours of connect time.

@_date: 2003-05-02 20:40:03
@_author: Nomen Nescio 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
Actually, the Cryptography Research approach did not require identity
tracking or legal judgements.  Data is not watermarked to the user's
identity during distribution, as had been anticipated by previous
watermarking schemes.  Rather, the data can be distributed as identically
manufactured optical disks, just as is done today.
In the CR system, the user's content player would watermark its own
"identity" (ie. some digital identifier) into the output that it produces.
It would apply marks to its output based on its own identifier.  Then if
the output is redistributed, it is possible to determine which playback
device (e.g. which DVD player) created it.  This device would then be
blacklisted and future releases of content would not include the keys
necessary to play on that particular device.
In theory there is no need to get courts involved.  Everything happens
on a private basis.  But in practice, someone whose expensive DVD player
has been turned into a semi-useless piece of junk may not be too happy.
He might go to court to challenge the content companies on invalidating
his player.  He can deny that he participated in a redistribution scheme.
Maybe he could even get an anti-trust complaint going, if all the content
companies band together to share blackball information.  Then we are
back to your scenario, battling experts in court.
On the other hand, the CR protection concept is very similar to the
approach which has been used by satellite TV broadcasting, where we have
seen a similar battle of measures and counter-measures.  People who use
pirate cards sometimes find them invalidated after a while.  You'd think
some of them might try going to court to fight the measure, but in
practice what happens is that if a legitimate customer (i.e. someone
paying the subscription fee) complains that his crypto card has stopped
working, they'll ship him a new one.
The idea is that a pirate would go through so many broken devices
that it would be too suspicious for him to keep requesting new ones.
Plus he probably wants to keep a low profile and not expose his true
name to the authorities.  These kinds of measures seem adequate to keep
things out of the court system.

@_date: 2003-05-09 03:50:02
@_author: Nomen Nescio 
@_subject: A Trial Balloon to Ban Email? 
Lauren Weinstein, founder of People for Internet Responsibility, has
come out with a new spam solution at According to this proposal, the Internet email architecture would be
revamped.  Each piece of mail would include a PIT, a Payload Identity
Token, emphasis on Identity.  This would be a token certifying that you
were an Authorized Email User as judged by the authorities.  Based on
your PIT, the receiving email software could decide to reject your
   It is anticipated that all Pits considered acceptable by the vast
   majority of all Tripoli-compliant software user would be digitally
   signed by one or more designated, trustworthy, third-pary authorities
   who would be delegated the power to certify the validity of identity
   and other relevant information within Pits.
In other words, here comes Verisign again.
   It is anticipated that in most cases, in order for the sender of an
   e-mail message to become initially certified by a Pit Certification
   Authority (PCA), the sender would need to first formally accept
   Terms of Service (ToS) that may well prohibit the sending of spam,
   and equally importantly, would authorize the certification authority
   to "downgrade" the sender's authentication certification in the case
   of spam or other ToS violations.
Thus you have to be politically acceptable to the Powers That Be in
order to receive your license to email, aka your PIT.  And be careful
what you say or your PIT will be downgraded.
Unfortunately he doesn't discuss various crypto protocol issues:
If the PIT is just a datum, what keeps someone from stealing your PIT
and spams with it?
If the PIT is a cert on a key, what do you sign?  The message?  What if
it gets munged in transit, as messages do?  You've just lost most of
your email reliability.
Or maybe you sign the current date/time?  Then delayed mail is dead mail.
Or maybe you respond to a challenge and sign that?  That won't work if
relays are involved, because they can't sign for you.
Spam is a problem, but it's no excuse to add more centralized
administrative control to the Internet.  Far better to go with a
decentralized solution like camram.sourceforge.net, basically a matter
of looking for hashcash in the mail headers.  This raises the cost to
spammers without significantly impacting normal users.

@_date: 2003-05-13 05:40:04
@_author: Nomen Nescio 
@_subject: Trusted Computing at WEIS2003 
The 2nd annual workshop on "Economics and Information Security" will be
held May 29-30 at the University of Maryland.  Unfortunately the website
at  is woefully out of date.
At least two of the papers will focus on Trusted Computing as exemplified
in the TCG (formerly TCPA) and NGSCB (former Palladium) proposals.
Ross Anderson himself, co-chair and founder of the conference, has
a new paper at Another one, by Stuart Schechter et al, is discussed at an EWeek article,
  The Schechter paper
is online at EWeek talks about the role of TC in limiting which applications get access
to protected content: "This kind of protection is seen as central to the
types of advanced digital rights management systems sought by content
owners as a countermeasure against piracy. However, this chain of trust
can be turned around and used by the people doing the illegal copying
and distribution, according to the paper's authors."
The authors are quoted, "Though this technology was envisioned to thwart
pirates, it is exactly what a peer-to-peer system needs to ensure that
no client application can enter the network unless that application, and
the hardware and operating system it is running on, has been certified
by an authority trusted by the existing clients..."
A similar point was made here last summer during our extensive debate
about the potential threat of Trusted Computing.  It would be fair to
say that it was not well received, however.  Perhaps now that the ideas
are being aired in an academic environment, people will take a closer
look at TC and gain a fuller understanding of the technology.
Even Ross Anderson recognizes that TC can help the pirates as well as the
protectors: "There is also a significant risk - that if TC machines become
pervasive, they can be used by the other side just as easily. Users can
create `blacknets' for swapping prohibited material of various kinds,
and it will become easier to create peer-to-peer systems like gnutella
or mojonation but which are very much more resistant to attack by the
music industry - as only genuine clients will be able to participate. The
current methods used to attack such systems, involving service denial
attacks undertaken by Trojanned clients, will not work any more [23]. So
when TC is implemented, the law of unintended consequences could well
make the music industry a victim rather than a beneficiary."
Anderson's paper is a significant improvement on his bizarrely paranoid
and error-filled FAQ.  He's had to back down on a number of his claims.
For example, Windows Server 2003 implements some DRM and document-locking
features which he attributed to Palladium.  He also seems to back away
from claims that Microsoft will censor your data.  He has to squirm to
deal with the work on TC Linux and try to explain how this fits into
his model of the monopolizing influence of these technologies.
Anderson now has to admit that his claims of a software blacklist are
mistaken as well: "Among early TCPA developers, there was an assumption
that blacklist mechanisms would extend as far as disabling all documents
created using a machine whose software licence fees weren't paid. Having
strong mechanisms that embedded machine identifiers in all files they
had created or modified would create huge leverage. Following the initial
public outcry, Microsoft now denies that such blacklist mechanisms will
be introduced - at least at the NGSCB level [18]."
Notice the claim that Microsoft has perhaps removed this feature based
on public outcry - an outcry for which Ross Anderson can no doubt take
credit.  This fulfils a prediction made here last year, that when their
apocalyptic scenarios failed to arrive, the critics would take credit for
having prevented them!  What a racket - if you're right, you're right,
and if you're wrong, you're even more right.
While this newer paper is better than the abysmal FAQ (which unfortunately
is still spreading its lies and misinformation, even though Anderson
now admits that he knows it is wrong), it has significant flaws as well.
All the analysis is presented from the perspective that businesses can
do whatever they like and consumers have no choice but to go along
helplessly.  Not once does he consider that the discipline of the
marketplace applies to sellers as well as buyers.  Any paper claiming
to be relevant to the topic of "Economics and Information Security"
should not be content with such a one-sided view.
All too often the text degenerates into the kind of anti-Microsoft
conspiracy theories which can be found in the sleaziest corners of
the net.  He never really explains why Intel, IBM and HP are going
along with these nefarious schemes.  Intel, we are told is behaving
"strategically".  What is the strategy?  Why will TC help Intel?
Anderson mumbles something about "lock-in" but that doesn't apply to the
hardware vendors.  He doesn't want to admit the obvious, that Intel thinks
this will sell more computers, because people will like their computers
better when they can access more content.  This is what happens when
you ignore the demand side in your analysis.
Anderson also presents a number of scenarios of Microsoft dominance
in the application demain as if they are new.  Why, law firms might
feel obligated to buy Microsoft Office in order to communicate with
their clients!  Imagine that.  Who could conceive of such a twisted,
backwards, upside down world as one in which companies felt stuck with
buying Microsoft for compatibility?  If he really thinks this is a new
threat, I'd suggest Anderson visit the real world occasionally.  I dunno,
maybe things are different over there in the Unreal Kingdom.
Despite these problems, I do want to emphasize that Anderson's paper is
a step forward.  And the paper by Schechter is also encouraging in that
it is willing to reject the anti-TC paranoia and take a clear-eyed look
at the technology.  Still, both of these papers express their results in
somewhat negative terms: look, you guys at the RIAA and MPAA, you better
not push for TC because it might benefit the pirates too.
None of these authors has quite been able to take accept the logical
conclusion of their analysis, which is that this is a technology which
can enable a whole host of powerful new applications, many of which
have probably not even been invented yet.  Then it should be up to the
marketplace to decide which will succeed and which will fail.  Everyone
wants to short-circuit that messy final step and decide for themselves
which are the "good" applications and which are evil.  I suggest that we
not reject out of hand the principle of allowing people to make decisions
for themselves about what they want to do with their computers, and that
includes utilizing TC technology.

@_date: 2003-05-15 04:20:08
@_author: Nomen Nescio 
@_subject: Zero Knowledge in the Cave 
Zero Knowledge in the Cave
There is a cave with a large entry room.  From this room lead two
passageways, 1 and 2.  Each of 1 and 2 branches into a myriad of smaller
passages, twisting and turning through the massive rock formation.
The passageways go on for miles and have never been fully explored.
One of the big questions has been whether passageways 1 and 2 ever
connect up.  Is there a way of getting from 1 to 2?  Many have
searched, but none have ever succeeded.  Most people believe that
no connection will ever be found.
At least, no one used to.  Now an explorer comes to you and claims to
have found a passage from 1 to 2, not a very long one, either.  He will
prove it to you, but to you alone.  Being a secretive type, he wants no
one else to know.  If you accompany him to the cave, he will prove the
existence of the passageway to you.
But there's a problem.  You carry a video camera and record everything
that you see.  If he shows you the existence of the passage, you will
be able to show the video tape to others, and they will learn of its
existence as well.
Not to worry, he says.  Come with me.  So you enter the large entry room
of the cave together.
Now the simplest thing to do in order to demonstrate the existence of the
connection would be for him to leave through passage 1 and return through
passage 2.  He could easily do this.  However, your film record of the
event would prove to anyone else who saw it that there was a connection.
Another way must be found.  The explorer tells you what to do.
Following his instructions, you leave the entry room for a few minutes,
while the explorer enters one of the passageways.  You then re-enter the
room, and loudly call out one of the passageway numbers, either 1 or 2.
In a few minutes, the explorer comes out of the requested passageway.
You then leave the cave and repeat the process many times.  Each time, the
reporter enters one of the passageways unknown to you; when you return and
name one of them, he is able without fail to exit from the named passage.
You reason that if there were no connection between the passageways,
the only way the explorer could come out the passage that you named
would be if he had gone in that same one.  He would have to guess
which one you were going to choose, and if he were right, he could
come out that one.  But you have repeated the test dozens of times.
The chances that someone could guess right so often is infinitisimal.
The only logical explanation is that the passageway does exist.
Excited, you return to the tavern where you met the explorer and show
the other patrons your tape.  But to your surprise, they just laugh.
They don't deny that the tape is real, that the explorer did come out
of the passageway you named.  But they don't believe in the connection.
Instead, they claim you are in league with the explorer in an attempt to
perpetrate a fraud.  You have simply predetermined together the sequence
of numbers you would call out.  Each of you has memorized the sequence,
and so each time the explorer is able to anticipate the number you will
call next.  He enters that passage and is able, after a suitable pause,
to exit from that same one when you call its number.
You leave the bar, frustrated.  You are convinced that the connection
exists, but even though the tape shows all of the evidence that was so
convincing to you, no one else finds it persuasive.  The explorer has
achieved his goal of proving the existence of the connection to you and
you alone.
Questions for the student:
1. How could you have done things differently, to produce a tape that
would be convincing to others?
2. What counter-measures and conditions could the explorer have put in
place to prevent you from getting a convincing tape in this manner?
