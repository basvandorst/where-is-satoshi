
@_date: 2013-08-09 20:07:42
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] LeastAuthority.com announces PRISM-proof storage 
Dear people of cryptography at metzdowd.net:
For obvious reasons, now is the time to push forward on strong
encryption for everyone. Here is our first attempt from
 . (We have more projects in the works!)
One of our goals is to spread the idea of *verifiable* end-to-end
encryption. It is possible. It isn't easy, but we just might make it!
We welcome criticism, suggestions, and requests from you all.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.
 LeastAuthority.com Announces A PRISM-Proof Storage Service
Wednesday, July 31, 2013
`LeastAuthority.com`_ today announced ?Simple Secure Storage Service
(S4)?, a backup service that encrypts your files to protect them from
the prying eyes of spies and criminals.
.. _LeastAuthority.com: ?People deserve privacy and security in the digital data that make up
our daily lives.? said the company's founder and CEO, Zooko
Wilcox-O'Hearn. ?As an individual or a business, you shouldn't have to
give up control over your data in order to get the benefits of cloud
verifiable end-to-end security
The Simple Secure Storage Service is built on a technology named
?Least-Authority File System (LAFS)?. LAFS has been studied and used
by computer scientists, hackers, Free and Open Source software
developers, activists, the U.S. Defense Advanced Research Projects
Agency, and the U.S. National Security Agency.
The design has been published in a peer-reviewed scientific workshop:
*Wilcox-O'Hearn, Zooko, and Brian Warner. ?Tahoe: the least-authority
filesystem.? Proceedings of the 4th ACM international workshop on
Storage security and survivability. ACM, 2008.*
It has been cited in more than 50 scientific research papers, and has
received plaudits from the U.S. Comprehensive National Cybersecurity
Initiative, which stated: ?Systems like Least-Authority File System
are making these methods immediately usable for securely and availably
storing files at rest; we propose that the methods be further
reviewed, written up, and strongly evangelized as best practices in
both government and industry.?
Dr. Richard Stallman, President of the Free Software Foundation
( said ?Free/Libre software is software that the
users control. If you use only free/libre software, you control your
local computing ? but using the Internet raises other issues of
freedom and privacy, which many network services don't respect. The
Simple Secure Storage Service (S4) is an example of a network service
that does respect your freedom and privacy.?
Jacob Appelbaum, Tor project developer (
and WikiLeaks volunteer ( said ?LAFS's design
acknowledges the importance of verifiable end-to-end security through
cryptography, Free/Libre release of software and transparent
peer-reviewed system design.?
The LAFS software is already packaged in several widely-used operating
systems such as Debian GNU/Linux and Ubuntu.

@_date: 2013-08-14 16:48:16
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] =?utf-8?q?Open_Letter_to_Phil_Zimmermann_and_Jon_C?= 
[This Open Letter will be posted to  when we
get our new blog up and running there.]
This open letter is in response to the `recent shutdown of Lavabit`_ ,
the ensuing `shutdown of Silent Circle's ?Silent Mail? product`_, `Jon
Callas's posts about the topic on G+`_, and `Phil Zimmermann's
interview in Forbes`_. Also, of course, all of this is unfolding in
the context of the `2013 Mass Surveillance Scandal`_.
.. _recent shutdown of Lavabit:
.. _shutdown of Silent Circle's ?Silent Mail? product:
.. _Jon Callas's posts about the topic on G+:
.. _Phil Zimmermann's interview in Forbes:
.. _2013 Mass Surveillance Scandal:
Dear Phil and Jon: Hello there! It is good to have a chance to chat
with you in public.
Please accept the following in the spirit of constructive criticism in
which it is intended.
For those readers who don't know, I've known you both, personally and
professionally for decades. You've each written texts that I've
learned from, inspired me to follow your example, we've worked
together successfully, and you've mentored me. I have great respect
for your technical abilities, your integrity, and your general
reasonableness. Thank you for the all of that and for holding fast to
your principles today, when we need it more than ever.
Your job is not yet done. Your customers are currently vulnerable to
having all of their communications secretly monitored.
I just subscribed to the service at  and
after I paid $120 for one year of service, it directed me to install
the Silent Text app from Silent Circle on my android phone, which I
did. Now, when I use that Silent Circle app to send text messages to
other Silent Circle customers, I have no way of verifying whether it
is really encrypting my message on my own phone, and if it is really
keeping the encryption key only for me, or if it is leaking the
contents of my messages or my encryption keys to you or to others.
If some attacker, for example the U.S. Federal Government ? or to pick
a different example the Zetas Mexican drug cartel ? were to coerce
Silent Circle into cooperating with them, then that attacker would
simply require Silent Circle to distribute an update to the app,
containing a backdoor.
There is no way for me to verify that any given version of Silent
Text, including the one that I just installed, is correctly generating
strong encryption keys and is protecting those keys instead of leaking
Therefore, how are your current products any safer for your users that
the canceled Silent Mail product was? The only attacker against whom
your canceled Silent Mail product was vulnerable but your current
products are safe is the attacker who would require you to backdoor
your server software but who wouldn't require you to backdoor your
client software.
Does that constraint apply to the U.S. Federal Government entities who
are responsible for PRISM, for the shut-down of Lavabit, and so much
else? No, that constraint does not apply to them. This was
demonstrated in the Hushmail case in which the U.S. DEA asked Hushmail
(a Canadian company) to turn over the plaintext of the email of one of
its customers. Hushmail complied, shipping a set of CDs to the DEA
containing the customer's messages.
The President of Hushmail `emphasized`_ in interviews with journalists
at the time that Hushmail would be able to comply with such orders
regardless of whether the customer used Hushmail's ?client-to-server?
(SSL) encryption or its ?end-to-end? (Java applet) encryption.
.. _emphasized: Phil had been Chief Cryptographer of Hushmail years earlier, and was
still a member of the Advisory Board of Hushmail at the time of that
case. He commented commented about the case at that time, and he also
`stated`_, correctly, that the Hushmail model of *unverified*
end-to-end encryption was vulnerable to government coercion. That's
the same model that Silent Circle uses today.
.. _stated: You have just taken the courageous act of publicly shutting down the
Silent Mail product, and publicly stating your reasons for doing so.
This, then, is your opportunity to make your stance consistent by
informing your customers of the similar dangers posed by the software
distribution practices currently used by Silent Circle (along with
most of the rest of the industry).
I don't know the perfect solution to the problem of the
*unverifiability* of today's software. But being frank about the
current approach and the vulnerability that it imposes on users is the
first step. People will listen to you about this, now. Let's start
talking about it and we can start finding solutions.
Also, warn your users. Don't tell them the untruth that it is
impossible for you to eavesdrop on their communications even if you
try (as your company seems to be on the borderline of doing in public
statements like these: [ `?`_, `?`_]).
.. _?: .. _?: We're trying an approach to this problem, here at
 of ?*verifiable* end-to-end security?. For
our service, all of the software is Free and Open Source, and it is
distributed through channels which are out of our direct control, such
as Debian and Ubuntu. Of course this approach is not perfectly secure
? it doesn't guarantee that a state-level actor cannot backdoor our
customers. But it does guarantee that *we* cannot backdoor our
This currently imposes inconvenience on our customers, and I'm not
saying it is the perfect solution, but it shows that there is more
than one way to go at this problem.
Thank you for your attention to these important matter, and your
leadership in speaking out about them.
(By the way,  is not a competitor to Silent
Circle. We don't offer voice, text, video, or email services, like
Silent Circle does/did. What we offer is simply secure offsite
*backup*, and a secure cloud storage API that people use to build
other services. So we aren't competitors.)
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2013-08-20 17:47:21
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] What is the state of patents on elliptic curve 
Here's a nice resource: RFC 6090!
Also relevant:
I'd be keen to see a list of potentially-relevant patents which have
expired or are due to expire within the next 5 years.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2013-10-15 18:42:51
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] my comment to NIST about reducing capacity in SHA-3 
Here are my personal opinions about these issues. I'm not expert at
cryptanalysis. Disclosure: I'm one of the authors of BLAKE2 (but not
one of the authors of BLAKE).
I personally do not believe that there is any secret agenda behind
this proposal, even though I believe that there was a secret agenda
behind Dual EC DRBG.
One reason that I believe that the motivation behind this proposal is
the stated motivation of improving performance, is that Joan Daemen
told me in person in January of 2013 that the Keccak team had
considered defining a reduced Keccak to compete with BLAKE2, but had
decided against it because they didn't want to disrupt the SHA-3
standardization process.
Apparently they changed their minds, and apparently their fears of
disruption turned out to be prescient!
I also do not think that a "security level" of 2^256 is necessarily
better than a "security level" of 2^128. *Maybe* it is better, but I'm
not aware of any examples where that sort of distinction has turned
out to matter in practice, and I can't really judge if it is likely to
matter in the future (except, of course, if you forget to take into
account multi-target issues?). I suspect nobody else can, either.
However, even though I *personally* would have confidence that a
Keccak with a 256-bit capacity would be safe and would be free of
maliciously induced weakness, I want a standard to be widely accepted
in addition to being safe.
This is the "Caesar's wife must be above suspicion" argument. It isn't
enough to make a secure standard, but also we need other people to
have confidence in it.
And, I don't know if we can persuade people that "no it isn't actually
backdoored/weakened". It may be the kind of thing where if that's the
conversation we're having then we've already lost.
Would it make sense to go ahead and standardize
SHA3-as-a-replacement-for-SHA2 by standardizing the form of Keccak
which is most widely accepted by cryptographers and which is closest
to what was studied during the contest, and then separately offer
SHAKE and reduced-for-speed-Keccak as additional new things?
A lot of uses of secure hash functions don't need to be particularly
efficient. In my slides about BLAKE2
( I argue that there are use-cases
where efficiency is critical, but it is equally true that there are
common and important use cases where a 576-bit capacity Keccak would
be fine, e.g. public key certificates.
Joan Daemen, one of inventors of AES and one of the inventors of
Keccak (SHA-3), replied to my mailing list post as follows:
Hello all,
Yes, Zooko and I met at the end-of-Ecrypt II event on Tenerife early
2013 (24? C in January!).
I don't remember our conversation in detail, but I I'm sure Zooko is
citing me correctly because that is what we were thinking about at the
Actually, what we had in mind was to propose something like "Keccak2"
to compete with BLAKE2 by drastically cutting the number of rounds,
e.g., down to 12 rounds for Keccak-f[1600], but otherwise keeping the
algorithm as it is. That might have sent the wrong message indeed, but
we just didn't do it.
In contrast, the capacity is an integral parameter of the Keccak
family that we even proposed as user-tunable in our SHA-3 submission.
Matching the capacity to the security strength levels of [NIST SP
800-57] is simply exploiting that flexibility.
Kind regards,
Joan, also on behalf of my Keccak companions?
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2013-10-28 19:07:43
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] [zfs] [Review] 4185 New hash algorithm support 
On Mon, Oct 28, 2013 at 6:49 AM, Richard Elling
Hey, I don't want to waste anyone's time, including my own. If nobody
is interested in this ? possibly including the original author of the
patch, Saso Kiselkov, judging from ? ? then by all means let's drop
the subject.
? However, in case someone out there is reading this?
I'm not sure what you mean about the future intended write. The risk I
was talking about was that an attacker can cause two blocks (on
someone else's ZFS system) to hash to the same fingerprint.
Assuming that ?the DDT key? is the secret which is prefixed to the
block contents in the current patch, then I agree it is extremely
difficult to cause two blocks to hash to the same fingerprint. A way
to be more precise about how difficult it is, is to talk about what
property we depend on the hash function to have in order to prevent
this attack.
If the attacker steals the secret, or if there is some variant of ZFS
which shares that secret among multiple parties ?, then the property
that we rely on the hash function to have is ?collision-resistance?.
If the attacker doesn't have the secret, then the property that we
rely on the hash function to have one which is closely related to, and
even easier-to-achieve than, ?MAC?.
? Functions which, in my opinion, have this easier-to-achieve-than-MAC
property include SHA-256, HMAC-MD5, Skein, BLAKE2, and
BLAKE2-reduced-to-5-rounds. Almost all cryptographic hash functions
have this property! One of the few cryptographic hash functions which
I would be not so confident in is Edon-R. It *probably* still has this
property, but it might not, and cryptographers haven't studied it
Functions which, in my opinion, have the much harder-to-achieve
?collision-resistance? property include SHA-256, Skein, BLAKE2, and
*probably* BLAKE2-reduced-to-5-rounds.
I'm sorry if I've misunderstood; I'm not an expert on ZFS. If you'd
like to take some of your valuable time to explain it to me, I'll
spend some of my valuable time to learn, because I'm interested in
filesystems in general and ZFS in particular. If not, I'm pretty sure
everything I've written above is still true.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.
Archives: RSS Feed: Modify Your Subscription: Powered by Listbox:

@_date: 2014-04-02 01:48:29
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Fwd: [messaging] Announcing the EFF Crypto Usability 
Of possible interest to many on this list, the EFF is hoping to offer
a prize for the most usable end-to-end encrypted communication tool
this year. This is still in the early stage of planning but we have
set aside a one-day workshop on July 9 in Menlo Park, CA to discuss
and debate the process for awarding the prize. The workshop is
attached to SOUPS, the biggest event for academic usability
researchers interested in security, so for security-minded folks this
is a great way to get in touch with UI experts.
As a co-organizer I'd be interested to hear ideas on the best
structure to adopt for the workshop. I'd also encourage anybody
interested in participating more to submit an abstract on a talk
they'd like to give at the workshop (due date is May 15 so lots of
time to think about ideas until then).
Full details below.
2014 EFF Crypto Usability Prize (EFF CUP) Workshop
CALL FOR PAPERS
Submission Deadline: May 15, 2014, 5pm PDT
Notification Deadline: May 30, 2014 5pm PDT
Anonymization: Papers are NOT to be anonymized
Length: 500 words
Formatting: PDF
Submission site: email to effcup at eff.org
Workshop Date: Wednesday, July 9, 2014
SCOPE AND FOCUS
The Electronic Frontier Foundation is evaluating the feasibility of
offering a prize for the first secure, private end-to-end encrypted
communication tool. There is currently tremendous interest in this
area, with several dozen new projects trying to make encrypted email,
instant messaging, text messaging, VOIP and video chat a reality. It
is not yet clear which of these tools is best-suited to meet
real-world usability challenges.
We believe a prize based on objective usability metrics might be an
effective way to determine which project or projects are best
delivering communication security to vulnerable user communities; to
promote and energize those tools; and to encourage interaction between
developers, interaction designers and academics interested in this
The EFF CUP workshop aims both to establish suitable metrics and
criteria for the prize, and to introduce developers working on open
source encryption tools (likely contestants) to the privacy and
security research community. EFF CUP will be held in conjunction with
the Symposium on Usable Privacy and Security (SOUPS) in July 2014 in
Menlo Park, CA. We are seeking talk abstracts and position papers on
the following topics:
USABILITY AND SECURITY METRICS: Holding an open competition for secure
communication tools is a new undertaking and requires new thinking
about measuring security and usability tools. We are seeking position
papers on what metrics can be used to most objectively evaluate
quality, including:
*Security metrics: Identifying the types of attacks that at-risk
groups (journalists, activists, lawyers) are subject to, and how we
can reliably measure the resistance which cryptographic communications
tools provide.
*Indirect usability metrics: Metrics which can be evaluated
analytically, such as backwards compatibility with existing tools,
integration into existing tools, or demonstrated adoption by N million
*Direct usability metrics: Metrics which can be evaluated through user
studies, such as the percentage of users who can quickly start using a
tool and survive various classes of real-world attack.
CURRENT TOOL SUMMARIES: Developers of secure end-to-end communication
tools are invited to submit a short (100-500 word) abstract describing
their project. We aim to have a series of short presentations
(followed by discussion) on the state of various projects, including a
description of the project's security and usability goals, current
development status, installed user base and supported platforms, known
usability challenges and vulnerabilities, and experiences (if any)
with user testing.
EXPERIENCE FROM PAST CONTESTS: Organizers or competitors from other
technology contests, particularly but not exclusively in the areas of
security and/or usability, are invited to submit a short (500 word)
abstract describing lessons from those contests. We aim to have a
series of short presentations including a brief overview of past
contest's goals, setup and rules, and outcomes. Example competitions
may include cryptographic primitive competitions (eg. AES, ESTREAM,
SHA3, PHC), Darpa contests, Capture the Flag contests, Crack Me If You
Can, VoComp or the Netflix Prize.
Lorrie Faith Cranor,
Associate Professor of Computer Science and of Engineering and Public
Policy at Carnegie Mellon University, Director of the CyLab Usable
Privacy and Security Laboratory (CUPS). Member, Electronic Frontier
Foundation Board of Directors.
Peter Eckersley,
Technology Projects Director, Electronic Frontier Foundation.
Joseph Bonneau,
Postdoctoral Fellow, Center for Information Technology Policy,
Princeton University
Messaging mailing list
Messaging at moderncrypto.org

@_date: 2014-04-23 21:01:44
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] swap needed, or not 
The theory is that swap allows certain situations that would have
ended with a process crashing instead end with the process completely
successfully. In my experience that's not the main effect. Instead,
what happens is that the situation still ends with a process crashing,
but only after many minutes of disk grinding (vm thrashing). I've just
habitually disabled swap on all my linux systems for at least a decade
now, whether workstations, servers, or laptops. Works fine. ?Zooko

@_date: 2014-01-30 20:33:14
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 16 
Dear Sergio:
On Sun, Jan 19, 2014 at 1:37 PM, Sergio Lerner
I don't think this is why collision-resistance is unimportant for this
use case. Instead, if I understand correctly, collision-resistance is
unimportant because the attacker does not get to choose the inputs,
but instead the inputs are provided by the defender.
I don't know the answer to your question about SHA-256, but may I
suggest BLAKE2 ? instead of SHA-256? SHA-256 reduced to 16 instead of
64 rounds would take about 6 or 7 cycles per byte, on 64-byte inputs,
on Haswell, according to bench.cr.yp.to ?. Full-strength (10 round)
BLAKE2s would take 5.75 cycles per byte on Haswell (ibid), and provide
excellent strength against pre-image attack. You could even cut
BLAKE2s down to reduced-round in order to get more efficiency. Exactly
how deeply you could cut it and still retain sufficient
pre-image-resistance is unclear to me. Perhaps ? sheds some light on
that, but I'm not sure.
Another possible advantage to using BLAKE2 for this application is
that some other password-hashing and/or memory-hardness functions that
have recently been proposed use BLAKE2: ?, ?, ?. So maybe if you use
BLAKE2, your schemes can be more easily compared to those ones.
Disclosure: I'm an author of BLAKE2.
Zooko Wilcox-O'Hearn
Chief Publicity And Marketing Officer for the BLAKE2 Team
? ? ? ? ? ?

@_date: 2014-07-11 22:39:36
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] [cryptography] hashes based on lots of 
Dear Eugen:
There have been several experiments in this direction, using
"memory-hard" proofs-of-work. For example, this was the motivation for
Litecoin ( to use scrypt in its
Proof-of-Work. To my knowledge, the state-of-the-art design is John
Tromp's Cuckoo PoW: In my opinion, this is a promising direction to take. It might still
succumb to centralization-of-mining in the long-term, but maybe not.
There's a possibility it would settle into an economic equilibrium in
which independent/hobbyist/small-time mining is sufficiently
rewarding, but customized, large-scale, vertically-integrated mining
is not rewarding enough to justify its costs.
Among anti-mining-centralization techniques that I've studied, this is
the only one that is easy to implement in the near-term, and doesn't
come with too many complications and risks for near-term deployment.
For the contrarian view, arguing that ASIC-resistance is either
undesirable and/or impossible, see this whitepaper by andytoshi:
 . I disagree with
the conclusions, but it makes some good arguments.
For a survey of state-of-the-art ideas about Proof-of-Stake ? ideas
which *aren't* easily implementable and which *do* come with
complexity, uncertainty, and risk ? see Vitalik Buterin's latest opus:
 . That guy is a good
thinker and writer! And he appears to have been reading my mind. As
well as adding in a bunch of ideas that were not in my mind, from such
sources as  .

@_date: 2014-06-04 19:16:59
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] What has Bitcoin achieved? 
[disclosure: I'm gnawing on these ideas as a potential new business to launch?]
Have you looked into the "colored coins" and related ideas for using
an underlying cryptocurrency's anti-double-spending mechanism, and its
global public ledger, but creating tokens that you can issue as many
of as you like, and underwrite with an asset of your choice?
A lot of smart people, and also me, have been chewing on that one for
a few years now, and I still can't see how to do it. I'd be happy to
compare notes!
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-06-05 13:53:39
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Reset The Net day! And LeastAuthority.com update 
Dear Perry's Crypto Mailing List:
It is Reset The Net day!
It is the one year anniversary of the Snowden disclosures. A coalition
of many companies, organizations, and individuals is using this to
urge people to adopt and deploy encryption software:
At LeastAuthority.com, we're announcing an upgrade to our verifiably
end-to-end-encrypted cloud storage service:
This service is all Free and Open Source software, based on the
Tahoe-LAFS project:
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-06-20 13:55:09
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
Here are a couple of entries from my blog in which security pros speak
out and say the same thing:
"FIPS and Common Criteria: don't rely on them"
"FIPS and Common Criteria: don't rely on them, part 2"
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-06-25 18:50:02
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
I think you should be careful not to conflate cipher-agility with
protocol upgrade-ability. A false alternative would be to say that we
have to choose one of these two choices:
1. SSL-style cipher-agility
2. MyTransportProtocol circa 2014 will come with AES, and then it will
be impossible for any future deployment of MyTransportProtocol to use
any other cipher than AES.
Note that if (2) were true, that would also imply that it is
impossible for any future deployment of MyTransportProtocol to change
anything *else* about the MyTransportProtocol protocol, either.
Instead, I think the omitted third alternative is the best one:
3. MyTransportProtocol circa 2014 will come with AES, and AES alone,
and it will have sufficient unambiguous versioning indicators that it
will be possible to deploy new versions of MyTransportProtocol in the
future that may come with a different cipher.

@_date: 2014-03-22 04:41:43
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] =?utf-8?b?QkxBS0UyOiDigJxIYXJkZXIsIEJldHRlciwgRmFz?= 
Hello people of cryptography at metzdowd.com:
I just wrote a blog post advocating for the use of BLAKE2 instead of
MD5 or SHA1:
Part of the impetus for the invention of BLAKE2 was the observation
that waggling our fingers and telling engineers to "stop doing that!"
wasn't having that much effect ? see
 for a list of ongoing users of MD5
and SHA1, including NIST's computer forensics division.
Of course, *you* didn't need to be told not to use MD5 or SHA1. You
might be more interested in the part of my blog post where I compare
BLAKE2 to Keccak and Skein.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-03-24 18:35:51
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I'm compiling a history of such things, and I have SHA-2 as published
in 2002 in FIPS 180-2 ?. Is there an earlier publication of SHA-2 that
I could reference? Thanks.
And by the way, I'd name Tiger as the champion hash function:
published in 1996 ?, deployed in the real world ?, widely studied ?,
and we still don't know of any way to break it. On top of all that it
is almost twice as efficient (in software on 64-bit CPUs) as some
others such as SHA-2 or RIPEMD-160, which makes its longevity all the
more noteworthy. (Because there is a trade-off between CPU efficiency
and safety in hash functions.)
RIPEMD-160 is another candidate for champion: also published in 1996
?, widely studied ? , used in practice, and no known weaknesses ?
except that its output size is a little too short for the 21st
century. If only it had been RIPEMD-192 instead of RIPEMD-160 then it
would look at least as good as Tiger looks today.
This isn't to detract from SHA-2's greatness of course. As I wrote in
 SHA-2 has been the most
widely-recommended standard for more than half a decade, and it too
shows no sign of weakness.
Oh, and Tiger and RIPEMD-160 were both designed without help from NSA,
as far as I know.
? ? ? ? ? ?

@_date: 2014-03-25 04:43:21
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
The problem with the XOR combiner (i.e. H?(x) = H?(x) ? H?(x)) is that
we can't prove an attacker wouldn't be able to generate collisions in
H? ? i.e. find x1 and x2 (different from each other) such that H?(x1)
= H?(x2) ? even though he cannot generate collisions in either H? or
in H?.
On the other hand, it is obviously proven that any possible technique
that would result in collisions in the concatenation combiner (i.e.
H|(x) = H?(x) | H?(x)) would imply a collision in both H? *and* H?.
We explored this issues in the Tahoe-LAFS "One Hundred Year
Cryptography" project. Here are a few more of our notes:
Also keep in mind that historically hash functions failed a lot more
often at collision-resistance than at anything else they have been
called upon to do. So therefore if you have to just pick one combiner,
then pick one that is strong against collisions. H| and Comb4P are
collision-resistant if either of their substrate hash functions is.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-10-07 02:05:43
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Hello again, Jason Resch of cleversafe.
I'd like to emphasize what Philipp Jovanovic said, in case he didn't
express it strongly enough: just use BLAKE2!
BLAKE2 has been designed by skilled cryptographers (not meaning
myself) to be safe in the ways that you are wondering about, and it
has excellent performance. It is also being adopted by quite a few
other folks with similar needs (i.e. the combination of crypto and
large volumes of data).
You could also make a good case for using Keccak or Skein, which also
have parallel and/or tree modes, but I do not think you can make a
good case for hacking together some construct yourself.
Here's a presentation I gave at ACNS on "Why BLAKE2?":
Zooko ?disclosure: I'm one of the authors of BLAKE2. The other ones
are more skilled cryptographers than I am. :-)
CEO, ?Freedom matters.?

@_date: 2014-10-07 15:21:09
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
BLAKE2 (and Skein, and Keccak, etc.) do logarithmic-time updates using
tree hashing, which is efficient enough for all uses that I have
looked at, and which can be secure in the traditional senses of
collision-resistance, etc.
I agree that a constant-time variant would be potentially interesting,
and I'm not saying (as someone else on this thread was thought to have
said) that we shouldn't discuss such a thing. What I'm saying is that
Cleversafe, as a commercial concern working on actual products, should
not be planning to use such a novel construction when BLAKE2 (et al.)
would work fine.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.

@_date: 2014-10-07 15:29:48
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] SPHINCS: practical hash-based digital signatures 
Dear Crypto Folks:
I'd like to draw your attention to a new digital signature scheme, SPHINCS:
(? Disclosure and disclaimer: Like with the recently-mentioned BLAKE2,
I'm a co-author, and like with BLAKE2, my co-authors did more of the
heavy lifting intellectually than I did.)
But anyway, here's my pitch for why you might care about SPHINCS:
Every digital signature algorithm that you can think of could be
broken by an attacker who could exploit a flaw in its secure hash
algorithm. *Or* the attacker could exploit a flaw in the *other* part
? the signature scheme.
That's because every digital signature algorithm (e.g. RSA-PSS,
Ed25519, ECDSA, etc.) uses a secure hash function to generate a short
fixed-length message representative, and then uses the signature
scheme to sign the message representative.
So there are two ways that an attacker can break any of the digital
signature algorithms mentioned above (RSA, Ed25519, etc. etc.) ? by
breaking the hash function or by cracking the other part.
But there is only one way that an attacker can break SPHINCS: by
breaking the hash function.
I think that's pretty awesome.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
Freedom matters.
?Eliminate the state!?
?Use more hash!?

@_date: 2014-10-28 04:33:27
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Auditable logs? 
We have a fairly thorough design for extending the vocabulary of the
Tahoe-LAFS storage system for this. The added vocabulary item would be
an "add-only set", a set of items that I can authorize you to add
things into without authorizing you to remove or overwrite any of the
This would be straightforward if we would just rely on some third
party to run a server which will accept new ciphertexts from you but
will refuse to delete or overwrite any of your old ciphertexts. Then
the set would have the "add-only" property with respect to you, but
not with respect to that server! The server would have the power to
rollback to earlier versions of the set.
We weren't satisfied with this, because all of the current vocabulary
items in Tahoe-LAFS are enforced by end-to-end cryptography *without*
relying on any single server to enforce the properties and without
being vulnerable to any single server being able to violate the
(Those vocabulary items are: immutable things vs. mutable things,
files vs. directories, and read-only access vs. read-write access.)
So, we went pretty far in defining a data-structure/crypto-structure
that minimized the power of servers. The resulting design is still
vulnerable to rollback attack by a collusion of *all* of the servers,
but if the reader connects to at least one server who is not in the
collusion, then the add-only property holds.
Here's the resulting design:
If you want even more detail, but more telegraphic in style, read the
rest of the comments after comment 13, and follow the link in comment
16 back to a mailing list post.

@_date: 2015-12-09 07:09:21
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Opinions on signatures algorithms for 
Disclaimer  I'm one of the authors of SPHINCS: Disclaimer  I don't really deserve to be—the other authors are the
smart ones.
I don't disagree with what's already been written on this thread,
including about the efficiency issues of SPHINCS — it has large
signature sizes.
I just want to add that SPHINCS is a safer digsig algorithm than any
known alternative *even* if there will never be a large quantum
That's because all the alternatives can be broken by *either*
second-preimage attack on the hash function (message-representative),
or a mathematical breakthrough against the asymmetric primitive, but
SPHINCS can be broken *only* by a second-preimage attack on the hash
Here's my attempt at encoding a 2-D matrix into text suitable for this list:
digsig type: current mainstream (RSA, DSA, ECDSA, Ed25519, etc.)
today: safe
quantum computer: unsafe
asymmetric crypto breakthrough: unsafe
second-preimage attack: unsafe
digsig type: post-quantum (McEliece, NTRUsign, LWE, Ring-LWE,
Lattice-based signatures, code-based signatures, Rainbow,
multivariate-quadratic, etc.)
today: safe
quantum computer: safe
asymmetric crypto breakthrough: unsafe
second-preimage attack: unsafe
digsig type: SPHINCS
today: safe
quantum computer: safe
asymmetric crypto breakthrough: safe
second-preimage attack: unsafe
Note that no secure hash function has *ever* proven vulnerable to a
second-preimage attack, except Snefru, which was invented by Ralph
Merkle in 1990 and broken by Eli Biham and Adi Shamir's discovery of
differential cryptanalysis in 1991.
Every other secure hash function that has ever been seriously
proposed, even weak little old MD5, is still completely immune to
second-preimage attacks as far as we know.
So I feel pretty confident that SPHINCS is a safer digital signature
algorithm than any other alternative currently known.

@_date: 2015-01-25 06:13:47
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] 2008 revision of Bitcoin whitepaper 
I've sought that original version of bitcoin.pdf and not found it. If
you find a copy, please share.

@_date: 2015-07-07 19:17:14
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Zerocash DAP implementation 
Dear Jan Carlsson:
Way to go! I haven't looked at your code yet, but I'm really happy
that you are working on this.
Haha! By coincidence, we actually posted ours to github under the MIT
licence 12 days ago!
But we haven't told anyone (until now), so I think that technically
means yours was the first to be "released". ;-)
Ours is also not completely functional, although we *do* have some
rudimentary integration with a fork of Bitcoind, so that's a start.
Excellent reason. Way to go!

@_date: 2015-07-13 11:59:54
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Are Momentum and Cuckoo Cycle PoW algorithms 
One detail that bothers me is that SipHash is being used in Cuckoo PoW
in a way that the attacker gets to control all the inputs to SipHash,
and that is not what SipHash was designed to resist. SipHash was
designed to resist an attacker who doesn't control â and actually
doesn't even *know* â the key. There's a possibility (although it
seems unlikely to me) that an attacker could exploit something about
the way Cuckoo uses SipHash to find Cuckoo solutions faster than by
treating SipHash as a random oracle.
I expressed this concern of mine multiple times to John Tromp in
private communication, and he was not persuaded that it is a real
problem, and he said that the CPU performance is important. I can see
his point: I wasn't able to figure out how to exploit Cuckoo's use of
SipHash after spending a few minutes peering at it. But I'm not a good
cryptanalyst, and the people who are good cryptanalysts have never, to
my knowledge, evaluated SipHash's strength under such conditions.
I would suggest someone like Bill Cox who is experimenting with such
constructions could replace the use of SipHash in Cuckoo with a
reduced-round version of Blake2. By reducing the rounds, that means
you are no longer covered by the positive cryptanalytic results on
Blake2 ( but on the other hand those
cryptanalysts *have* studied reduced-round versions of Blake2 (and
Blake, and ChaCha, and Salsa20), and that they have published any
interesting weaknesses that they found in reduced-round versions of
So, I'd suggest that if full Blake2 is too CPU-intensive for this
usage, then reduce the rounds of Blake2, but keep as many rounds as
you can tolerate. But definitely not fewer than 3 rounds. Or 4. Maybe
5. âº Hopefully that would make it close enough in performance to
SipHash for this usage.

@_date: 2015-06-06 02:22:09
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
Dear Perry's Crypto List folks:
The time has come to kill off md5sum. Here's the letter I wrote to the
GNU coreutils project advocating replacing md5sum with b2sum (BLAKE2)
which they seem to be in favor of:
I did a quick and dirty benchmark (attached to that email as a
postscript) and was delighted that b2sum (in BLAKE2sp mode) was almost
twice as fast as m5sum on my Intel Core-i5 laptop!
Do you all know of other implementations of md5sum besides the GNU
coreutils one?
One snag that I've noticed is that some people tell me "Okay, we're
going to switch from MD5 to BLAKE2, but our hash values have to fit
into the fields where we used to store our MD5 hashes.". I tried my
hardest to explain that no matter how good the hash function is,
truncating the output to 128 bits is going to leave users potentially
vulnerable to collision attacks at some point down the road. The
response was "Well, we'll just take our chances, because we can't
change the schema.".
I've been thinking that I'm going to go back to those folks and ask if
their schema is hex-encoded MD5 hashes (32 chars), and if so if they
could change their schema to base-64 encoded BLAKE2 hashes. That would
give them 192 bits, which would be enough to make me stop worrying
about their users. :-)
Any suggestions?
Zooko, the BLAKE2 Publicity Officer

@_date: 2015-06-08 14:33:51
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
There's also
Which says it can find a second-pre-image in full (all rounds) Tiger
with 2^8 memory and 2^188.2 computation.
I must confess I too am fond of Tiger. I agree with Ryan Carboni that
it is the oldest widely-used secure hash function which hasn't been
However, BLAKE2 has a much better security margin than Tiger â see the
Cryptanalysis section here:  â in addition to
being substantially faster than Tiger in software.

@_date: 2015-06-08 14:44:21
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
Several people replied pointing out how sticky data integrity
algorithms and values are. I totally agree. I think the best shot we
have of stamping out MD5 needs to have the following properties:
1. It has to be faster than MD5. Maybe this is irrational of these
users, or maybe it isn't, but in any case I don't believe we're ever
going to get the world to upgrade from MD5 to a slower algorithm. My
evidence is that we've already tried this for the last 20 years with
SHA-1, SHA-2, and now SHA-3. Last time I checked, even NIST, the
sponsor of the SHA-3 competition, was still recommending MD5 for data
2. It has to be a "drop in" replacement.
3. There is only one successor. No alternatives, no optional features.
And it has to be spelled the same way on all systems. System
administrators and programmers who don't know anything about anything
need to be able to say to one another: "b2sum is the new successor to
md5sum.", and that's that.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
 â Freedom matters.

@_date: 2015-06-08 18:31:28
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
The version of Tiger benchmarked on
 is Wei Dai's implementation
from Crypto++ (
It has optimized assembly and SSE2. I don't know for sure if there
could be further optimizations to Tiger, but I would be surprised if
anyone could squeeze better than 10% more speed out of it.
Look into  for full details,
but on some machines and some variants of BLAKE2, it is 2X as
efficient as Tiger.
BLAKE (the immediate ancestor of BLAKE2) came out in 2008, and during
the SHA-3 competition BLAKE was probably subjected to more
cryptanalysis than Tiger has been in its entire 20-year life.
As you know, Keccak was the winner of the SHA-3 contest, and one
reason that it was the winner was that it was the target of extensive
cryptanalysis during the competition. But, in the opinion of NIST,
BLAKE was the target of even *more* cryptanalysis than Keccak:
âKeccak received a significant amount of cryptanalysis, although not
quite the depth of analysis applied to BLAKE.ââNIST's Third-Round
Report of the SHA-3 Cryptographic Hash Algorithm Competition
(If you're interested in my story about how in the SHA-3 contest,
BLAKE got higher marks from NIST than Keccak got, see my slides from
ACNS 2013 â  â and my blog post â
Subsequent cryptanalysis of BLAKE2 is consistent with the belief that
BLAKE2 succeeded in its goal of optimizing performance without
substantially reducing the security margin:  .
And, of course, the core of BLAKE and BLAKE2 is based on from ChaCha,
which has been extensively cryptanalyzed both by itself and in the
form of its immediate ancestor Salsa20 in the eStream competition
( and which is now being
standardized for TLS
The bottom line is that BLAKE2, BLAKE, ChaCha, and Salsa20 have been
quite extensively studied, and they all exhibit a very large security
margin. No cryptanalysis has come anywhere close to breaking more than
few rounds of any of them.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
 â Freedom matters.

@_date: 2015-06-08 21:05:34
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] MITM attacks on Tor exit nodes 
You might like this new tool Honey Badger by David Stainton:
"HoneyBadger is primarily a comprehensive TCP stream analysis tool for
detecting and recording TCP attacks. Perhaps it can assist in
discovering 0-days and botnets."
(Note: David is my employee at LeastAuthority.com, but Honey Badger is
not a LeastAuthority thing â it is David's 20% project.)

@_date: 2015-06-09 15:46:46
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
Thanks! Working on it:
For my answers to this, please see my slides from ACNS 2013:
The first slide hopefully answers your second question there.
Basically, cryptographers have adopted SHA-2 for their specific
purposes, but the world of big data has long since standardized on
MD5-or-SHA1 and shows no signs of budging. NIST itself, even while
sponsoring the SHA-3 contest, was also continuing to recommend MD5 as
a good tool for digital forensics (in NIST SP 800-86).
I've heard many stories from engineers of how MD5 is sometimes the
bottleneck in their operations. For example a friend who works at
revision control company Perforce, tells me that a single "verify the
integrity of this repo" operation once took more than a week, during
which time it continually maxed out the server. And Perforce uses the
standard secure hash function for big data: MD5! I don't think they
will ever be persuaded to upgrade to a slower function. But they
*might* be persuaded to upgrade to a faster.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
 â Freedom matters.

@_date: 2015-06-09 16:19:01
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] let's kill md5sum! 
I don't understand why this matters. BLAKE2 is faster than MD5 in
software in most cases, currently. Future CPUs will probably further
increase that.
No, there's no known weakness in BLAKE2. Please see the security
analysis in  and in
 .
BLAKE2 as currently specified already has a finalization step
(preventing length-extension attacks) and is already faster than MD5
(in most cases).
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
 â Freedom matters.

@_date: 2015-03-16 04:28:30
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Looking for sequential-memory-hard hashing 
Hi Bear:
Nice to correspond with you again. I've been reading your posts on
mailing lists for a long time now. ?
I don't have an opinion about your proposed algorithm, but I wanted to
point out some related work: the candidates from the Password Hashing
Competition. Many of them, e.g. Catena
( ), are
explicitly designed to be usable as proofs of work, and I kind of
suspect that *any* of them *could* serve as a proof-of-work, because
the Password Hashing Problem seems to be almost the same thing as the
Proof Of Work Problem. Many of them (again, including Catena) are
Another piece of related work is John Tromp's Cuckoo Hashing Proof Of
Work: I think there is a pretty close relationship between your proposal of
partially-colliding triplets which have a relationship between the
three pre-images and Cuckoo. To see what I mean, read the part of the
cuckoo PDF about "Momentum", which is a lot like your proposal and
which Cuckoo is a generalization of.

@_date: 2015-11-10 15:52:07
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Cryptogit 
I'll name names! Tahoe-LAFS, IPFS, git-annex, and probably a lot more.
The thought process that PHB is going through in his original post on
this thread is one that many people have gone through. (I'm one of
them, with Tahoe-LAFS.)
Not true! You are a big fan of Tahoe-LAFS.

@_date: 2015-10-17 17:36:02
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] How does the size of a set of target results 
I don't believe that forensics is really safe with
collision-vulnerable but pre-image-resistant hash functions.
What if a bad actor generates a malicious or illegal file with the
same MD5sum as an innocuous file and then submits the innocuous one to
the forensics databases? How do we know that isn't already happening?
That would be using collisions to do something bad in your use-case
1), but I suspect similar collision-based attacks could apply to the
other two use-cases.
Anyway, the reason we made sure BLAKE2 was faster than MD5 was so that
we wouldn't have to have this conversation. ;-) Just switch to BLAKE2.
It's faster *and* more secure.
In answer to your question, yes, I think the difficulty of finding a
2^M pre-image for any of 2^N different images is indeed 2^{M-N}.

@_date: 2015-10-26 17:42:05
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] How does the size of a set of target results 
Sorry, Bear, I think you got a couple of things wrong:
This isn't called "weak collision resistance", this is called "second
pre-image resistance". You're right that MD5 is still thought to have
No, MD5 doesn't have resistance against this. The basic Joux
multicollision attack works against MD5:
here's a student research paper saying that the author implemented
this and benchmarked it:
And I'd like to say unequivocally that trying to figure out how to use
MD5 safely is a fool's errand, and we've seen it blow up in people's
faces time and time again (e.g. ¹). Now that we have BLAKE2
( which is secure *and* is faster than MD5 (at
least in most uses), there is absolutely no reason to use MD5 for
anything, and anybody who voluntarily does so should be regarded as
either ignorant, or as the sort of engineer who takes unnecessary
¹

@_date: 2016-07-01 22:57:23
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] sha1sum speed 
Thanks for reporting these measurements!
If you build openssl-1.1.0 pre-release then you can also measure
openssl's new implementation of BLAKE2b and BLAKE2s. (But not, yet,
BLAKE2bp or BLAKE2sp.)
By the way, as Bill Cox mentioned up-thread, there's a new
implementation of BLAKE2bp in AVX2 from Samuel Neves that brings it
down to an amazing 1.45 cycles per bytes. I'm not sure how that

@_date: 2016-10-11 01:40:23
@_author: Zooko Wilcox-OHearn 
@_subject: [Cryptography] Zcash Open Source Miner Challenge 
Hi folks!
I've been quiet on this list for a while now. I've been hard at work
on creating a Bitcoin-like cryptocurrency with zero-knowledge-based
This is the most sophisticated crypto that I've ever seen someone
attempt to deploy at scale to the Internet. (By all means feel free to
reply and teach me about counter-examples to that generalization.)
There's a lot going on there. To jump into the technical side, I'd
suggest the Zcash protocol spec:
 . For
an introduction to the bigger picture, probably our blog
( and FAQ (
Okay the reason I'm writing today is to let you know about the Zcash
Open Source Miner Challenge:
The Zcash company has donated $30,000 for prize money to reward better
open-source implementations of Equihash by Biryukov & Khovratovich:
Jump in! The worst that can happen is that you get the fun and
education of implementing an interesting new proof-of-work algorithm.
