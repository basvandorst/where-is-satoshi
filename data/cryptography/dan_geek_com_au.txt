
@_date: 2003-02-09 12:19:41
@_author: Daniel Carosone 
@_subject: Columbia crypto box 
It's probably just every-day insitutionalised paranoia.  It doesn't
matter why they care, the sticker on the outside says they have

@_date: 2003-06-14 18:24:50
@_author: Daniel Carosone 
@_subject: Session Fixation Vulnerability in Web Based Apps 
The problem is that this is not especially useful in practice, if
your client is IE. Essentially, you can't rely on IE to keep ssl
sessions open from one request to the next, and thus it's not
practical to treat this as a significant authentication token.

@_date: 2004-08-25 18:10:57
@_author: Daniel Carosone 
@_subject: More problems with hash functions 
My immediate (and not yet further considered) reaction to the
description of Joux' method was that it might be defeated by something
as simple as adding a block counter to the input each time.
I any case, I see it as a form of dictionary attack, and wonder
whether the same kinds of techniques wouldn't help.

@_date: 2004-08-27 07:40:23
@_author: Daniel Carosone 
@_subject: How thorough are the hash breaks, anyway? 
Yep, so far.. but lets assume for the moment that's as far as they
will go, however nervous it makes us about future extension of the
True. Even if you know the plaintext, many of the messages you might
want to tamper with have some sort of internal consistency constraints
(structured file formats, executable code for a particular
architecture, etc) that limit the possibilities of a useful attack.
There is one application of hashes, however, that fits these
limitations very closely and has me particularly worried:
certificates.  The public key data is public, and it's a "random"
bitpattern where nobody would ever notice a few different bits.
If someone finds a collision for microsoft's windows update cert (or a
number of other possibilities), and the fan is well and truly buried
in it.

@_date: 2004-09-01 13:26:48
@_author: Daniel Carosone 
@_subject: Hashes, splints, and PRNGs 
============================== START ==============================
I'm really enjoying the current discussion about hash constructions
and splints for current algorithms.  I will make one observation in
that discussion, which is that the proposal for a Hnew (2n -> n) seems
a little beyond the scope of a "field splint" that can be done using
existing tools and implementations, and so I prefer some of the other
proposals that don't require this new element.
The discussion also gives me the excuse to ask a question I've been
wanting to pose to this list for a long time, regarding the techniques
used by the NetBSD /dev/random pseudo-device.
Although designed and used for different purposes, the discussion here
in the past few days has similarities to that construction.  I hope
that by raising it, I can both add something to the discussion, and
also get feedback from this esteemed company on its general worthiness
for original purposes.
I've attached the source code for the core part of the accumulator and
output function, which is reasonably well commented. (I hope; the
comments are largely mine, the code is very largely not).  The general construction is an entropy "pool" (equivalent to the state
passed from block to block in the diagrams we've been using to talk
about Joux' attack) into which data (input blocks of 32-bit words) are
mixed.  The first observation here is that the state path between
blocks is very much wider than the blocksize of either input or
output, as desired.
The input mixing is done my modelling the state array as 32 parallel
LFSR PRNG's, one per bit-plane, in combination with a rotation of the
input and output around each of the LFSR's (so that none of them runs
for many cycles and becomes vulnerable to the well-known limitations
of LFSR's run too long).
In the normal usage of this device, the input data is timing
information from various device drivers (disks, keyboards, etc). It
can also include user-supplied data written to /dev/random, and so
collision-resistance does become at least potentially an issue.
Output is done by taking a SHA-1 hash of the whole state array, and
then folding it in half (XOR) before giving it to the user. (The
unfolded SHA-1 result is also mixed back into the pool as above, so
that successive calls produce different results as a PRNG).  Again,
this is reminiscent of using a n-bit hash to claim only n/2 bits of
security, though obviously this usage originated out of different
So, looking at this in the context of present disussions, and
pretending this construct is used purely as a (hybrid) hash function,
it has the following characteristics:
 - 32-bit input block
 - 80-bit output block
 - wide path (4096 bits in default configuration) between blocks
 - Hnew = TT ( H2 (H1 (M) ))
 - probably only effective for messages longer than some multiple of
   the internal state size
Joux' arguments certainly apply to this construction (which was in any
case designed to preserve entropy rather than for collision resistance
against wholly-known different inputs).  If collisions can be found
for H1, then H2 adds nothing (its purpose is whitening of the internal
state, in the original usage).
Anyway, some questions for the floor:
 - How much relevance and similarity is there between PRNG style
   constructs used this way and hash compression functions?  Normally,
   a PRNG is designed to produce many bits from few, and a hash the
   reverse. However, using the PRNG this way tries to produce many
   state bits to carry forward, rather than output bits.
 - If contemplating a construct such as this as a hash-like function,
   any hope of Joux-resistance relies on the large internal state. How
   could this be made stronger? One thought would be to clock the
   output function (a SHA-1 of the pool) and mix that back in every N
   inputs - or is this just needless work?
 - Is the whole thing pointless (for this purpose) and does it just
   amount to a SHA-1 of TT(M) if M is wholly-known?
 - Finally (and of most interest to me practically), can anyone see a
   problem with the construct for its intended usage as a PRNG.  In
   particular, can anyone see a way to feed/flood it with known input
   such that the output becomes weaker (if M is partially known or
   partially chosen)?
PS: if looking at the code, please ignore the "entropy count" parts;
they work with an estimator that implements the (non-)blocking nature
of /dev/(u)random, but this is known to be highly arbitrary and
(hopefully) overly conservative.

@_date: 2004-06-19 10:13:34
@_author: Daniel Carosone 
@_subject: recommendations/evaluations of free / low-cost crypto libraries 
Taking "especially" to mean you may be interested in others as well,
in the Java world it's impossible to go past the Legion of the Bouncy
Castle (
Of course, unless you need to get at low-level stuff or do any other
non "consumer" crypto, in Java you generally use the abstract JCE and
JSSE api's and don't really care (in the programming sense) which
provider is plugged in underneath.
As well as the typical JCE stuff (ciphers and X.509), BC also includes
OpenPGP and S/MIME.

@_date: 2005-08-09 18:13:18
@_author: Daniel Carosone 
@_subject: solving the wrong problem 
Nice list, and terms for the remaining ??? cases would be nice, but
I'm not sure that any of these captures one essential aspect of the
problem Perry mentioned, at least as I see it.
One of the nice aspects of the "snake oil" description is the
implications it has about the dodgy seller, rather than the product.
To my view, much of the Quantum Cryptography (et al) discussion has
this aspect: potentially very cool and useful technology in other
circumstances, but being sold into a market not because they
particularly need it, but just because that's where the money is.
Certainly, that's the aspect I find most objectionable, and thus
deserving of a derogatory term, rather than just general frustration
at naive user stupidity.
None of the terms proposed so far capture this aspect.  The specific
example given doesn't quite fit anywhere on your list.  It's somewhere
between  and  perhaps it's a  with a dodgy salesman trying to
push it as a  until a better problem is found for it to solve?
I was going to suggest "porpoise oil" (from "not fit-for-purpose"),
but how about "unicorn oil" - something that may well have some
uncertain magical properties, but still sold under false pretenses,
and not really going to cure your ills?

@_date: 2005-02-02 08:44:00
@_author: Daniel Carosone 
@_subject: Is 3DES Broken? 
Notably for those encrypting data at rest, it's also rather smaller
than current hard disk sizes, which are much harder to re-key.
(Even for those only encrypting data in flight, it has practical
implications regarding the feasibility of capturing that data for later

@_date: 2005-02-03 11:00:36
@_author: Daniel Carosone 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
Other merits of the idea aside, if the user knows the CA is untrusted,
what's it doing in the browser's trust path?
If we're going to assume users are capable of making this decision, we
should make it easier for them to express that decision properly
within the existing mechanism.

@_date: 2005-02-03 16:45:13
@_author: Daniel Carosone 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
That was a very large part of my point.. :)
[As an aside, pruning the ca trust list is a common hardening
recommendation for those building corporate SOE lockdowns and similar
platforms, where the organisation is making a trust decision for the
user differently than the browser maker is.]
I appreciate what they're trying to do, and think it has merits I'm
not in any way trying to diminish.
I just don't see a great history of success with the general user
populace reading and thinking and reacting properly to security
popup warnings of any kind.
The smart, security-conscious and PKI-aware users who can recognise
good CA's from bad will not be falling for phishing scams in the first
place.  The user who's already some way down the path of falling for
one is unlikely to make a better choice even when you give them
another popup, though there's a chance it might help at least
somebody, and we should surely take that chance.
If the users could make appopriate CA trust choices, having the
browser manufacturers prepopulate a list of potentially-trusted CAs,
with a popup asking for a trust approval the first time a site
presents a cert in that path, might work. Likewise, something that
remembered cert fingerprints and CA path for "known trusted sites",
vaguely a'la ssh, and popped up an appropriate warning when something
changes, might work for such a smart user.  Even so, most of the
popups they see are going to be for legitimate cases of cert renewals
or ICA changes or server load-balancers or .. whatever else.
What's really needed is a way to help them make fewer, better
decisions, rather than more decisions.   Wish I knew how..

@_date: 2005-02-13 16:09:02
@_author: Daniel Carosone 
@_subject: banks and ssl fingerprints 
I spent quite some time and effort, on an early Internet Banking
project some years ago, convincing a bank to publish the SSL
fingerprint for the service via a number of out-of-band channels.
I suggested they print the details somewhere on their advertising for
the service (even amongst the rest of the inevitable small print), on
the terms and conditions paperwork, perhaps on people's bank
statements, add a menu item to the telephone voice-response system to
read the fpr, etc etc. There were also to be instructions and pointers
to this amongst the 'security information' help docs.  There was some
discussion about it all, especially around changing the printed
material if certs were renewed/replaced, but they eventually went for
a reference to the IVR key reading (which could be changed) from a
number of the other places.
A couple of years later, I asked them to go through IVR logs and find
out how many times the fingerprint had been read out: they figured,
discounting internal test calls, perhaps just over a dozen since the
project went live.
We never expected it to be used much. Even so, if this helped those
few people who wanted to check, I felt it was a worthwhile service.

@_date: 2005-01-06 10:45:28
@_author: Daniel Carosone 
@_subject: FreeBSD's urandom versus random 
I keep poking you-know-who to write up his gbde criticisms properly :)
I'd be very happy for any review from this group, as I've said
I have idly considered replacing the urandom device with something
like yarrow, re-seeded as appropriate from the random device.  This
would likely improve its speed, and (more importantly) reduce or
eliminate the times that urandom readers cause random readers to
block because the entropy estimator (however bogus) is low.
Recommending that urandom (in whatever form) is strong without
blocking and should be used ~always is one thing. Inverting the sense
of random, for those few cases where someone decides they're prepared
to block no matter what, as it seems FreeBSD has done, is another
thing entirely.

@_date: 2005-01-27 11:46:02
@_author: Daniel Carosone 
@_subject: entropy depletion 
The point is interesting and well made, but I'm not sure it was
intended as a concrete implementation proposal.  Still, as long as
it's floating by..
Rather than locking out other readers from the device (potential
denial-of-service, worse than can be done now by drawing down the
entropy estimator?), consider an implementation that made random a
cloning device.  Each open would create its own distinct instance,
with unique state history.
One might initialise the clone with a zero (fully decoupled, but
potentially weak as noted above) state, or with a copy of the base
system device state at the time of cloning, but with a zero'd
estimator. From there, you allow it to accumulate events until the
estimator is ready again.  Perhaps you clone with a copy of the state,
and allow an ioctl to zero the private copy.. perhaps you only allow a
clone open to complete once the base generator has been clocked a few
times with new events since the last clone.. many other ideas.  Most
implementations allow data to be written into the random device, too,
so a process with a cloner open could add some of its own 'entropy' to
the mix for its private pool, if it wanted additional seeding
(usually, such writes are not counted by the estimator).
The base system device accumulates events while no cloners are
open. You'd need some mechanism to distribute these events amongst
accumulating clones such that they'd remain decoupled.
It's an interesting thought diversion, at least - but it seems to have
as many potential ways to hurt as to help, especially if you're short
of good entropy events in the first place.
Like smb, frankly I'm more interested in first establishing good
behaviour when there's little or no good-quality input available, for
whatever reason.

@_date: 2005-06-01 14:49:27
@_author: Daniel Carosone 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Perhaps we do - not so much as a source of hard statistical data, but
as a source of hard pain.
People making (uninformed or ill-considered, despite our best efforts
to inform) business and risk decisions seemingly need concrete
examples to avoid.
Its depressing how much of what we actually achieve is determined by
primitive pain response reflexes - even when you're in the beneficial
position of having past insistences validated by the pain of others.

@_date: 2005-06-08 10:59:38
@_author: Daniel Carosone 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
It may well be that, rather than the fault of the web designer, an
explicit business requirement was presented to do this, because of
perceived "ease of use" for the customer...  and then the lack of
knowing any better to kill the idea before it takes hold.
At least, that's how it's happened in several places I've been
involved, and it took more than a little effort to make them
understand why it was a bad idea.  Thankfully, they were paying for my
time and advice, and *also* had the sense to listen to it when given -
the two don't always go together - but in this sense security did cost
them something.
The customer ease-of-use argument is quite easy to see - and also
quite easy to provide, if they want to, by putting the normal company
homepage that contains the form on SSL too.  I've had conversations
about the cost of doing that with knowledgable web designers (largely
centering on image caching concerns), and really, it isn't quite free,
even if the costs come from unreasonable and annoying places. Those
costs can have returns, even non-risk ones like being better able to
track users' browsing patterns and site navigation, too - but just by
virtue of having to have the conversation, its no longer free,
especially if people bring organisational politics to the table.
These are the things that make the difference between middling-fair
and somewhat-decent security (let alone good security, which requires
many other things to be Done Right, more operational than technical).
The irony is that bad IT security (just like any other bad IT) is
expensive, often much more expensive - even without considering the
potential costs involved if the risks are realised.
Worse than that: people at the giant multinational corporations who
provide the outsourced IT services to those other corporations - and
who sell their services based on 'economies of scale' and 'industry
expertise' and 'best practice', and who are thus charged with the
responsibility of knowing better - are too dumb to implement it.
And then they're too dumb to implement it at other customer sites even
when that one rare customer comes along who knows better (or at least
knows to ask for independent outside help), and has fought hard to
convinve their outsource provider of the need and economic sense of
not being dumb on their behalf.  Most organisations just let them get
away with it, and think they're getting a good deal, while the very
basis on which that deal was sold is defeated.
If it weren't for all the other people that get hurt by the schrapnel,
it would be very hard to continue trying to help people who seem to
like walking around with bullet holes in their shoes.

@_date: 2006-02-16 13:14:59
@_author: Daniel Carosone 
@_subject: GnuTLS (libgrypt really) and Postfix 
Or an even simpler example: maybe it will still be a fatal error, but
there's some important state outside the library being called that it
should clean up before exiting so abruptly.   Somehow, applications that are consumers of crypto libraries seem like
likely candidates for this sort of thing.

@_date: 2006-03-24 15:33:39
@_author: Daniel Carosone 
@_subject: Creativity and security 
Two observations:
 - your preferred solution to a problem of fraudulent cashier staff
   doing the wrong thing ... relies on the cashier staff doing the right
   thing.  Training fraudulent and creative cashiers on the importance
   of this action probably encourages them to come up with other ways
   to do the same thing.
 - even when they've handed you a receipt, on many systems there's a
   good chance they can get a reprint those same three seconds later.
   Paper jams or gets torn, ribbons run out, and sometimes you
   legitimately need a duplicate.

@_date: 2006-09-15 16:32:04
@_author: Daniel Carosone 
@_subject: RSA SecurID SID800 Token vulnerable by design 
Me too.  While less emphatic, my reaction to Vin's post was similar to
Thor's.. that it seemed to at least miss, if not bury, this point.
But let's not also forget that these criticisms apply approximately
equally to smart card deployments with readers that lack a dedicated
pinpad and signing display.  For better or worse, people use those to
unlock the token with a pin entered on the host keyboard, and allow
any authentication by any application during a 'session', too.
And further, a token that includes more buttons (specifically, a
pinpad for secure entry of the 'know' factor) is vulnerable to fewer
attacks, and one that permits a challenge-response factor for specific
transaction approval is vulnerable to fewer again (both at a cost).
Several vendors have useful offerings of these types.
The worst cost for these more advanced methods may be in user
acceptance: having to type one or more things into the token, and then
the response into the computer.  A USB connected token could improve
on this by transporting the challenge and response, displaying the
challenge while leaving the pinpad for authentication and approval.
But the best attribute of tokens is they neither use nor need *any*
interface other than a user interface. Therefore:
 * it works anywhere with any client device, like my phone. I choose
   my token model and balance user overhead according to my needs.
 * it is simple to analyse.  How would you ensure the above ideal
   hypothetical USB token really couldn't be subverted over the bus?
By the time you've given up that benefit, and done all that analysis,
and dealt with platform issues, perhaps you might as well get the
proper pinpad smartcard it's starting to sound like, and get proper
signatures as well, rather than using a shared-key with the server.

@_date: 2006-09-18 08:07:02
@_author: Daniel Carosone 
@_subject: RSA SecurID SID800 Token vulnerable by design 
Yes, a friend lent me one of these to play with a while ago, they're
really quite cool. Lots of interesting possibilities - which was
entirely the point of the original development version.
Yes. The usb client port starts up emulating a usb mass storage CDROM
device, and uses autorun to load a usb-ethernet driver and several
other bits of software onto the windows box to help it get further.
The irony is, of course, that this shouldn't work at all on a properly
secured machine (though you could still try and launch the program
manually if the lockdown had only gone as far as disabling autorun).
The very thing it relies on to work smoothly could also have been
abused to install keyloggers and other nasties on the desktop that
will sink the security of the device, or at least of the user session.
Pretty much; it offers a samba share to the host to run this and other
programs from.
It uses a cute covert-channel trick to switch to this mode.  It starts
up emulating a CD.  The autorun software on the CD (and the linux
hotplug script equivalent) have the task of passing some
configuration/environment information through to the blackdog.  The
first of these is a network address range for the private 'lan'
between them.  On the filesystem is a directory, with four
subdirectories, inside each of which are 256 files, 0-255.  The
autorun tool picks a network range, then reads one file from each
directory in order to 'tap out' the network address.  When this
happens, the blackdog disconnects from the usb and reconnects, this
time emulating a usb ethernet corresponding to the driver that was
prepared earlier, and things continue from there over this network.
Yeah, and physical keyloggers and similar tampering too, pretty much
The important thing, though, is to keep in mind the separation between
the device/platform, and the default application. Plenty of alternate
applications, including potentially malicious ones, might not be
bothered by these concerns.
There was a newer commercial variant due for imminent release about
the time I was looking at the device.  It had a more purpose-specific
software image, some extra flash/ram, and a small screen.  It had also
lost something in the process, I think the card reader for extra local
storage, in favour of a smaller case and a network storage concept.

@_date: 2007-10-03 14:15:38
@_author: Daniel Carosone 
@_subject: Seagate announces hardware FDE for laptop and desktop machines 
Assumption: clearing the password stores the key encrypted with
password "" or an all-zeros key, or some other similar construct,
effectively in plain text.
Speculation: the drive always encrypts the platters with a (fixed) AES
key, obviating the need to track which sectors are encrypted or
not. Setting the drive password simply changes the key-handling.
Implication: fixed keys may be known and data recoverable from factory
records, e.g. for law enforcement, even if this is not provided as an
end-user service.

@_date: 2008-08-30 07:20:08
@_author: Daniel Carosone 
@_subject: Generating AES key by hashing login password? 
You want to look at something like PKCS for generating keys from passphrases.

@_date: 2008-02-07 13:20:52
@_author: Daniel Carosone 
@_subject: Gutmann Soundwave Therapy 
Others have made similar points and suggestions, not picking on this
instance in particular:
This is ok, if you consider the only threat to be against the final
endpoint: a human listening to a short-term, disposable conversation.
I can think of some counter-examples where these assumptions don't
 - A data-driven exploit against an implementation vulnerability in
   your codec of choice.  Always a possibility, but a risk you might
   rate differently (or a patch you might deploy on a different
   schedule) for conversations with known and trusted peers than you
   would for arbitrary peers, let alone maliciously-inserted traffic.
   How many image decoding vulnerabilities have we seen lately, again?
 - People have invented and do use such horribly-wrong things as
   fax-over-voip; while they seem to have some belief in their own
   business case, I may not have as much faith in their implementation
   robustness.
 - Where it's audio, but the audience is different such that the
   impact of short bursts of malicious sound is different: larger
   teleconferences, live interviews or reporting by journalists, and
   other occasions, particularly where the credibility of the speaker
   is important.  Fractions of seconds of sound is all I might need to
   insert to .. er .. emulate tourette's syndrome. Fractions of
   seconds of soundwave therapy could still be highly unpleasant or
   embarassing.
Particularly for the first point, early validation for packet
integrity in general can be a useful defensive tool against unknown
potential implementation vulnerabilities.  I've used similar arguments
before around the use of keyed authentication of other protocols, such
as SNMPv3 and NTP.
It also reminds me of examples where cryptographic protections have
only covered certain fields in a header or message.  Attackers may
find novel ways to use the unprotected space, plus it just makes the
whole job of risk analysis at deployment orders of magnitude more
Without dismissing the rest of the economic arguments, when it comes
to these kinds of vulnerabilities, be very wary of giving an attacker
this inch, they may take a mile.

@_date: 2008-02-12 14:28:17
@_author: Daniel Carosone 
@_subject: Gutmann Soundwave Therapy 
Point taken, but I respectfully disagree with the relevance in the
present context, though of course I agree entirely in the wider
philosophical sense.
Remember that we're also talking about practical deployment decisions:
 - if someone steals credentials, they can do all sorts of mischief
   and damage; the incremental risk in the present discussion is that
   doing 'lossy'/partial validation may allow additional injection and
   MITM attacks beyond those.
 - especially for the other cases I gave (SNMP and NTP), the
   alternative mitigating controls are such strong things as IP
   address based ACLs (on UDP packets). I'll take the stronger tool if
   it's on offer.
The fact this kind of authentication is applied before the packet gets
to more complex and potentially vulnerable parsing and processing code
gives me a valuable opportunity to be defensive, especially as an end
customer deploying some random vendor's kit.
In those cases, I don't have visibility of the implementation, but I
do have some assurance about the order of operations and can put that
structural knowledge to good use.  Much the same is true in this
discussion about protocol design; we're making no specifications about
processing of the data once the transport hands it off, but we're
starting to make assumptions about the risks therein, and the reliance
those layers may be placing on the transport, for better or worse.
Your criticism would be fair if I was advocating blindly accepting the
data or not doing any checking after the initial handshake.  It would
be fair criticism of a codec vendor who took such a stance, relying
overly on transport authentication (or forcing me to). I am most
certainly not advocating that, merely recognising that sometimes such
checking may be deficient or vulnerable, or just simply uncertain. Good defensive protocol design lets me validate the blob before
inspecting the fields; poor defensive programming conflates frame
validation with more detailed syntactic and semantic validation later.
If there are authentication-hijacking vulnerabilities in the endpoints
(like your SIP gateway), sure, I'm screwed in a number of ways.
That's sad, but a given regardless of whatever variant and detail of
keying and MACing mechanism this discussion comes up with.
If cryptography can come up with some way to ensure robustness against
hostile data all the way down an implementation stack, regardless of
layering, we'll all be surprised.  Some of us might even be very rich.
Otherwise, it's a risk mitigation tool, subject to constraints we need
to understand.  If the constraints are ones of key management and
endpoint security, I can use the mechanism in my toolkit.  If the
constrains mean that every fourth SNMP request or routing update will
be unauthenticated, it's much less use to me as a structural security
layer; the bar isn't raised in any practical sense.
I attempted to illustrate, with some counterexamples, threat models
where even one unauthenticated packet could lose you more than "not
much" security.  Threats where detection of the attack wasn't enough,
or was too late, or was itself the point.
Robust implementation behind that MAC is essential, and helps realise
and provide assurance around "not much", as well as addressing broader
threats that are more likely in the overall economic argument we
acknowledged at the outset, but is outside what I saw as the OP's
scope, and not part of the incremental risk I was highlighting.
