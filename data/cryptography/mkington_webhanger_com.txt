
@_date: 2013-12-23 09:09:59
@_author: Max Kington 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
default in BSAFE?  You did not say so in your post.  So now RSA
"categorically denies" entering into a secret contract with the NSA.  If it
wasn't secret, why didn't I hear about it?  I'm pretty sure it would have
made the geek news, and I may not be a crypto expert, but I follow geek
news (slashdot would have burned RSA alive).
Unless I've missed it, what was the $10 million for?
Secret or not I can't see in that statement why they got it in the first
place. Is the implied suggestion really just to encourage RSA to be early
adopters of the EC based RNG? If so why? And why so much money. If it was
purely technically better surely that case could be demonstrably made to
RSA (look it's better) and in due course NIST?
It does kind of lead to the obvious reason being that that case couldn't be
made and so as to pre-seed the market place before it going to NIST.
The legitimate business purpose I can guess was to pay rsa to spent the
time and money reviewing it and NSA wanted people to be more secure. Still,
ten million is a lot of money.
I'm surprised that question wasn't asked at RSA at the time. Perhaps it
was. We're questions like the following asked?
1) Are we being duped?
2) If so how?
3) Why?
Maybe they went in having reviewed it and couldn't see what was wrong.
After all even nearly ten years later people still can't put their finger
on exactly what it is the advantage for the NSA.
I can imagine a world where RSA were suspicious but not being behind the
door concluded no foul play (they're not thick) or at the very least we're
extremely hesitant to decline ten million dollars without providing the NSA
with a plausible and convincing reason.
'sorry, we don't want your money because we think you're upto something,
our best and brightest can't work out what but we're unhappy enough to turn
down your money. Also please don't hold that against us when it comes to
all the other business we do'
As you say they might have been duped or they might have known or an
absolute myriad of circumstances in between.

@_date: 2013-11-13 07:31:27
@_author: Max Kington 
@_subject: [Cryptography] Looking for feedback on new Java crypto library 
has been maintaining it for use in some Android libraries:
InputStream and OutputStream interfaces and derives keys from user-entered
passwords. Keyczar works on ByteBuffers, byte[], and String inputs, and
handles key versioning and rotation. Plus, it has been ported to Python and
Why would you use this over bouncycastle? Which is well established and
examined and incredibly complete. Not to mention cross platform.
of files and streams in a way that tries to integrate current crypto
best-practices with a foolproof API, while building on the existing Java
Cryptography Extension (JCE).

@_date: 2013-11-22 11:37:44
@_author: Max Kington 
@_subject: [Cryptography] Cryptolocker 
And the maximum amount of data is 117 bytes using 2048bit rsa. Just about
nothing encrypts anything other than a symmetric key or other nonce with
rsa alone.
Even if you did it's the private key operations which are an order of
magnitude slower than the public key operations which are slower still than
symmetric key operations.

@_date: 2013-10-19 00:08:15
@_author: Max Kington 
@_subject: [Cryptography] funding Tor development 
It would be a guess but who might want tor to continue with additional
Silk road 2, next gen, velvet motorway etc.
So with high profile criminal proceedings the likes of which the FBI and
NCA are executing Bitcoin as fascinating as I find it technically and
socially it's getting bad press as an enabler.
Three things affect it presently.
1) legitimate users (read non criminal) are likely going to want to turn
bitcoin into real coins in jurisdictions where it might get banned in the
coming months or years. So your donations die.
2) bitcoin flows can be analysed if you don't engage in complex camouflage
techniques. The sorts of behaviour that is a hallmark of money laundering.
So you're either highly anonymous because you're able to engage in criminal
footprint like behaviour or you're a criminal. See what I did there?
3) you use bitcoin you are a criminal and you have something to hide.
Consumers of bitcoin are criminals. It's merely an extension of the 'if you
have nothing to hide' argument.
That's three for instances that occurred to me almost immediately. I'm sure
the sorts of lawyers that the tor.* folks employ are a shed load smarter
than I.

@_date: 2013-09-08 21:12:41
@_author: Max Kington 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
This space is of particular interest to me.  I implemented just one of
these and published the protocol (rather than pimp my blog if anyone wants
to read up on the protocol description feel free to email me and I'll send
you a link).
The system itself was built around a fairly simple PKI which then allowed
people to build end-to-end channels.  You hit the nail on the head though,
control of the keys.  If you can game the PKI you can replace someone's
public key and execute a MITM attack.  The approach I took to this was that
the PKI publishes peoples public keys but then allows other users to verify
your public key.  A MITM attack is possible but as soon as your public key
is rotated this is detected and the client itself asks if you'd like to
verify if out of band (this was for mobile devices so it lends itself to
having other channels to check keys via, like phone your friend and ask
them).  The much more likely thing is where someone tries to do a MITM
attack for just a particular user but as the channels are tunnelled end to
end they need to essentially ask the PKI to publish two duff keys, i.e. one
in each direction, Alice's key as far as Bob is concerned and Bob's key as
far as alice is concerned..  In turn the two people who's traffic the
attacker is trying to obtain can in turn ask someone else to double check
their.  It means that you need to publish an entirely fake PKI directory to
just two users.  The idea was the alarm bells go off when it transpires
that every person you want to get a proxy verification of a public key via
has 'all of a sudden' changed their public key too.  It's a hybrid model, a
PKI to make life easy for the users to bootstrap but which uses a web of
trust to detect when the PKI (or your local directory) has been attacked.
 Relationships become 'public' knowledge at least in so far as you ask
others in your address book to verify peoples public keys (all be it via
uuids, you could still find out if your mate Bill had 'John's' public key
in his address book because he's asked you to verify it for him).  So for
those who want to protect the conversational meta data it's already
orthogonal to that.
Group chat semantics are quite feasible in that all users are peers but you
run into difficulty when it comes to signing your own messages, not that
you can't sign them but that's computationally expensive and the eats
battery life.  Again, you are right though, what do you want to achieve?
I certainly built a protocol that answered the main questions I was asking!
As for multiple devices, the trick was always usability.  How do you
securely move an identity token of some description from one node to
another.  I settled on every device having its own key pair but you still
need an 'owning' identity and a way to 'enrol' a new key pair because if
that got broken the attacked just enrols their own 'device'
surreptitiously.  You then get into the realms of passwords through salted
hashing algorithms but then you're back to the security of a password being
brute forced.  If you were really paranoid I proposed a smart card
mechanism but I've yet to implement that (how closed a world are smart
cards with decent protection specifications?! but that's another
conversation), the idea being that you decrypt your device key pair using
the smart card and ditch the smart card if needs be, through a typical
office shredder.
Silent Circle was one of the most analogous systems but I'm an amateur
compared to those chaps.  As interesting as it was building, it kept
boiling down to one thing: Assuming I'd done a good job all I had done was
shift the target from the protocol to the device.
If I really wanted to get the data I'd attack the onscreen software
keyboard and leave everything else alone.

@_date: 2013-09-10 21:22:40
@_author: Max Kington 
@_subject: [Cryptography] Usage models (was Re: In the face of 
It's not a dumb idea at all but getting the 'introduction' mechanism right is tricky.  I've implemented something similar where we 'trust' the first endpoint and then allow that well verified device to be
bound to other entities.  The mechanism I built was that each one gets its own identity and can be
peer'ed into a conversation and that the group can in fact decide if it wants to allow
the arrival of 'Walter on his Tablet' or 'Walter on his phone' depending on your level of paranoia
or that of the group.  I don't mind Walter on his PC at work but I don't like sending these messages
to Walter on his phone especially when I know he has a habit of leaving his phone
on the bus.
 I've implemented the primary notion around a phone because it's a simple
identity, it doesn't change much and it has multiple an out-of-band delivery channels which lends itself  to multiple authentication factors.
It allows other possibilities too, for instance, you can nominate the in-use end point and temporarily
suspend others or even kick them out by refusing to complete an MPC protocol with them, exchange
new keys and carry on the discussion.
The idea of multiple sub-idenities lends itself well to keys which have different lifecycles. So chat
for example, where a transient set of keys which, if lost isn't a massive problem but email for instance
where loosing them and having stacks of encrypted email in your inbox which is now useless is unhelpful.
I'm literally just in the process of building the master entity as a smart card component which can be used to 'introduce' the first device on an NFC capable android platform.  From  there you can bootstrap the other devices.
What I don't know the answer to yet is if material changes need to be notionally signed by the master entity
or if introductions can be done by devices as peers.  Of course, compromise one device and it winds up
with peer introducing rights.  There is a usability trade off and I suspect until I've actually tried using it the answer is non obvious.

@_date: 2013-09-11 18:14:42
@_author: Max Kington 
@_subject: [Cryptography] SPDZ, 
A colleague is looking into this venture. I gave him a synopsis of their
additions to SPDZ. There is a white paper describing their technology at
their website which talks about the other two related protocols, Yao and
One interesting use that occurred to me was the ability to split the two
nodes in their implementation across jurisdictions. Especially those who
are unlikely to ever collaborate. That giving you an advantage over a
typical HSM which could live in a jurisdiction that could be seized.
The wp and associated bibliography is available at

@_date: 2013-09-11 18:50:12
@_author: Max Kington 
@_subject: [Cryptography] Books on modern cryptanalysis 
How modern is modern? :-)
I have modern cryptanalysys by Christopher Swenson (or at least did have
before it was loaned and I moved) and it was an excellent book and
crucially was very accessible. Also available in kindle format now. It is 5
years old now though.

@_date: 2013-09-14 10:16:18
@_author: Max Kington 
@_subject: [Cryptography] Summary of the discussion so far 
I do wonder what the problem with being observed using it is though.  I
understand the problem and the want to not have traffic analysis done
on your communications but what's the practical effect on your communications
if they are?
If I think about what I'm bothered about.  I do work part-time for an arm of government.  I don't like the idea that someone is out there with a big ear-trumpet
recording all my communications.  I like to be able to discuss the rights &
wrongs of mass surveillance but at the same time I don't want to be labelled
a dangerous subversive.  I like to have a moan about things I dislike but I don't want those to re-appear at some meeting where I'm called in for
a meeting, hat on, without coffee.  At least not where I've not been compelled
to produce them (at least I know what's coming!).  So privacy on the messages is important to me but not
necessarily is it of *equal* importance that my communications partners are hidden.  I might swap emails
with Ben, Ben likes a good moan too, we both work for the same branch. The fact that I work with Ben and talk to him is neither here nor there.
For example, Hemlis is taking on the problem of obscuring traffic with regards
to the 'who' you're talking to and not just the 'what'.  I wonder how important
that is, really, especially when they're talking about centralised control of user information to ensure security, but haven't addressed what happens when they're compelled to help people game their own system (the it's ok, we'll
go to prison before we help the spooks I always find a bit weak, what if they
turn up with a car battery and a pair of pliers?) It's not clear how they're going to do any of this yet.  All in all they seem to have good intentions but I fear they're falling
into the trap of trying to solve the 'interesting' problems as a priority without having
a consistant plan.
I'm sure they'll come up with some sort of mix network.
As Perry points out in his August posts, latency is less important although
for instant messaging traffic people do kind of want 'instant' for a low enough
value of latency.  The latency though is only of massive importance if it's critical
that who you talk to be obscured as well.  If you have *some* idea
of the people in a network who are communicating with each other there
also needs to be enough bandwidth to hide your messages in, as you're probably already observing the traffic close (or fairly close) to the endpoint
it's being delivered to.  I took an approach in the system that I built of batching messages together
inside an encrypted bundle and padding them with junk so that you got a message every x minutes or x seconds and it was always at least y size regardless of if there was anything in it for you of interest or not.  If messages were over y size, they split and queued up for the next interval.
Specifically on the jurisdictional point:
I've looked into this, I did some research into cloud providers in different jurisdictions.  After all if it's going to scale you're unlikely to be building data centres on the way to the system becoming successful.  It is possible that
you don't actually need to go to the extremes of routing stuff via Russia, China
Egypt and Pakistan.  I've got another discussion on another list about what
entities that are allowed to co-opoerate can actually do on behalf of each other. It turns out there is an interesting disconnect between Irish law and the UK law
(I picked Irish law because Amazon's european operations are in Ireland)
You have to decide if you are worried about co-operation as allowed by law or not for the jurisdiction you're in, i.e. are you going to go to prison or not.
The main instrument of cooperation here is a thing called an MLAT, a mutual
legal assistance treaty and they're signed with an awesome number of countries.
They only enable cooperation to the extent that local law allows and have different
rules about support that allows evidence that can be admissible in court and other
kinds of support.
So it comes back to what you're worried about, it doesn't have to be about absolutes

@_date: 2013-09-14 17:23:40
@_author: Max Kington 
@_subject: [Cryptography] prism proof email, namespaces, and anonymity 
The keys. This to me is the critical point for widespread adoption, key
management. How do you do this in a way that doesn't put people off
There are two new efforts I'm aware if trying to solve this in a user
friendly way are  and Parley's approach does at least deal with the longevity of the private key
although it does boil security down to a password, if I can obtain their
packet and the salt I can probably brute force the password.
I've exchanged mails with the mailpile.is guys and I think they're still
looking at the options.

@_date: 2013-09-17 18:03:26
@_author: Max Kington 
@_subject: [Cryptography] End to end 
many cases the corporation has a need/right to see what they are
is necessary to protect the communication on transport and storage. Of
course a certain way of key recovery has to be in place.
I intend to reply in more detail to the draft there's lots of very
interesting work there.
The most common approach to ILM for email in highly regulated sectors I've
seen is to divorce the storage and transport mechanism and associated
security from the long term storage. In a corporate environment the message
is captured pre encryption and transmission and stored.
Whilst key escrow mechanisms do exist the risk is that what gets escrowed
isn't what was sent if you maliciously want to tunnel data (imagine not
being able to decrypt a message at the request of the SEC or FSA because
the key you were sent is wrong by the desktop app, or conversely having to
decrypt everything first to check). You have the added issue of having to
store all the associated keys and in 7 years (the typical retention period
over here for business now regarded as complete, let alone long running
contracts still in play) still have software to decrypt it.
Hence, store in the clear, keep safe at rest using today's archival
mechanism and when that starts to get dated move onto the next one
en-masse, for all your media not just emails.
Hence for the purposes of your RFC perhaps view that as a problem that
doesn't require detailed specification.

@_date: 2013-09-18 09:23:34
@_author: Max Kington 
@_subject: [Cryptography] End to end 
mechanism and when that starts to get dated move onto the next one
en-masse, for all your media not just emails.
the need to comply with regulations is more important than the need to keep
data confidential.
disclosure is much higher if data is stored unprotected. Any admin with
access to the file system is able to read it.
regulative pressure in US is higher, in Europe the privacy is more
important or more protected.
organisation, but this has to be a management decision, balancing the needs.
I was referring to the UK :-)
I'm not saying it isn't important to consider how data is made available in
the cases where you have end to end security but a future standard wants to
be permissive of a solution even if it's out of scope for the RFC rather
than prohibitive by including it as mandatory, could/can vs should/must.
That said now there appears to be evidence that side channel attacks that
force lesser security where it's an option are being actively exploited.
Previously we'd have all assumed that the main benefit of those was in
interoperability but now not so much. So there is an argument to use 'must'
more in standards concerning security.
By making archival a separate concern you also reduce the complexity of
many deployments. As you say, for environments with very high regulation,
my personal mailbox, isn't, my work one, is.

@_date: 2013-09-19 22:11:23
@_author: Max Kington 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
against a piece of paper I
want to do things online because it is much easier and "gets rid of paper."
 Those are the systems we need to secure.  Perhaps another way to look at
it:  how can we make out-of-band verification simpler?
talking about money, not just social networks.
number from the bank paperwork you got when you signed up for an account,
or signed up for online banking, or got with your monthly statement, or got
as a special security mailing and enter it into your email client, I
suspect a reasonable percentage of people would do it. It is, after all a
one time operation.
As with other themes though, one size does not fit all. The funny thing
being that banks are actually extremely adept at doing out of band paper
verification. Secure printing is born out of financial transactions,
everything from cheques to cash to PIN notification.
I think it was Phillip who said that other trust models need to be
developed. I'm not as down on the Web of trust as others are but I strongly
believe that there has to be an ordered set of priorities. Usability has to
be right up there as a near-peer with overall system security. Otherwise as
we've seen a real attack in this context is simply to dissuade people to
use it and developers, especially of security oriented systems can do that
of their own accord.
If you want to get your systems users to help with out of band verification
get them 'talking' to each other. Perry said that our social networks are
great for keeping spam out of our mailboxes yet were busy trying to cut out
the technology that's driven all of this.
Out of band for your banking might mean security printing techniques and
securing your email, phoning your friends.

@_date: 2014-12-18 17:28:45
@_author: Max Kington 
@_subject: [Cryptography] Google E2E (was: Any opinions on keybase.io?) 
I designed something similar but with a difference. A point to point MitM
which does a back to back attack is still possible and much harder to
detect. I conceived the system to work within social networks. (or if you
prefer, phone book)
The tricky part here is that if you are able to discern who is making the
request for public key data then you could insert pairs of false public
keys AND ONLY to those clients you are trying to MitM. One for client A and
one for Client B.
That way your directory 'looks clean' to others. Worse still if the
subsequently negotiated session key is long lived you don't need to MitM it
all the time. You get around this by ALSO signing the messages but in my
world that was annoying because of the cycles that eats (or in my use case
the battery power that consumes) but I couldn't see a way around that. A
bit like you you want immediate detection not relying on your next session
key renegotiation to stand another chance picking it up.
The protocol I designed dealt with this attack by having the idea of
authenticated directory querying and anonymous directory querying before I
realised that actually 'anonymous' was pointless because that puts a
massive burden on the system to actually enable anonymous access.
The other component was to Co opt people to help you validate keys. So I do
the verification (much like you describe) and at negotiation time I also
(and I think this is probably where our schemes differ) I ask others to
tell me what their view of the key is. They can do this a number of ways.
Through asking the directory, or by prior knowledge or both or even copting
others into the question.
You can then build a mesh of 'viewpoints'.
That mesh is a really interesting beast. Pick other verifiers at random? Or
people you trust? Or people who might know the person already? I'm doing
*alot* of thinking about this property.
You seem to maintain a 'trust memory' a bit like I do.
The other thing was I introduced the idea that you put a 'watch' on the
public keys of people you know *and* the directory needs to publish a CT
like transparency chain. When you put a watch on you start your 'trust
memory' at that point and you add meta data to your own record when you
have your own conversations with these people.
When  you ask for someone's viewpoint on a public key. That can include the
CT chain, from when you remembered it, when you last used it and what the
trust mesh looks like.
What this does is make the global MitM directory attack (1 fake key
published to the world) easy to detect and the back to back pointed attack
hard to execute successfully. Because you have to try to back to back the
MitM attack the entire mesh and hope nobody has prior memory of the real
Cool! Similar but different ?

@_date: 2015-05-18 10:19:09
@_author: Max Kington 
@_subject: [Cryptography] Intel SGX: Augean stables piled higher & deeper? 
"SGX" looks like BS piled higher & deeper -- i.e., building better places
for hackers (including nation-states) to hide their malware, and still more
complexity that hasn't been (can't be ?) proven correct.
To be fair, it's not a bad idea and we'll need to wait and see about the
implementation. I hasten to add that VMM doesn't get this sort of press for
providing memory isolation in hardware and extensions like vt-d are
generally regarded as a good idea (people can moan about the way they do
More abstractly pushing more of the responsibility of software protection
into hardware could well make it harder to attack by the vast majority of
Sure it's an evolving landscape and the bias  may just shift but that
doesn't mean it should get written off.
As for a place to hide malware, well there are plenty of places inside a
modern  processor architecture to achieve that today. Creating more
mechanisms, painting a target on them called 'security stuff' on them and
using them as a new vector seems at the very least unnecessary.
SGX provides new tools and hardware facilities to software developers to
protect an application's secrets.  In today's computing environment the
ability to keep a secret requires the integrity of millions of line of
software in the OS, VMM, and application.  SGX creates a trusted
environment called an enclave inside the application.  An enclave provides
an ability to protect the secret without dependency on the integrity of any
other code.  The talk will describe the programming environment,
instruction set, and hardware facilities which make up the SGX architecture.
Microsoft Windows Media player.
after 8PM on the days of the lecture.
OR, USA.
architecture research team. He has previous experience in microprocessor
design, security concepts, and trusted computing. He received a BSEE from
Northeastern University and is a member of the IEEE.

@_date: 2015-05-20 14:41:03
@_author: Max Kington 
@_subject: [Cryptography] Intel SGX: Augean stables piled higher & deeper? 
rigorous mathematical proofs?
that are only appreciated in hindsight.
higher level of scrutiny than do other types of HW improvements.  In
particular, it deserves some sort of proof that this "improvement" didn't
inadvertently (or not: NSA) introduce additional bugs/insecurities.
I disagree. In the environment where it's all fair game, it shouldn't
matter that this is security directed. The risk in this kind of thinking is
that other HW changes should 'get a pass' because they're not security
related (noting the VMM was done to improve the efficiency of memory layout
and to provide large logic address spaces.
modifications like this get a pass.  How come?
Nobody is saying it gets a pass but the default stance of it being
"bullshit" in the absence of mathematical proof is incongruous.

@_date: 2015-11-06 14:01:59
@_author: Max Kington 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
Ultimately you could only be *sure *it's an unsafe construction but only
have some confidence in it being a safe construction.
I guess the real question is to what end?
As Ian says, are you reviewing an existing system?  If so are you trying to
assert that the application *needs* changing to better apply a separation
of use principle for the purpose of a key (namely that your encryption and
integrity keys should be different for all the key management reasons
that's a good idea)?  Or is is that you want to assert that in this case,
the risk is low and doesn't *have* to be changed but could be?
I think you need to be a bit careful about what is meant here by safe and
unsafe and think about it in the context of what you're concerned about.
Specific cryptographic attacks relating to using AES as the chosen block
cipher to underpin MAC OR the general principal of key separation.
 Key purpose reuse is what Weis goes on to talk about is the general idea
that every time you use a key you expose information about the key.  If you
are ever forced to have to change a key because it has been compromised
(either stolen or broken and recovered) then you are able to make
assertions about what you've lost, i.e. the data previously might not have
had integrity but it wasn't ever exposed.  Can I go back, check the
contents of my data and be satisfied that it is what was transmitted and
not interfered with but still be happy it's not been leaked.
The general perspective case to my mind probably doesn't warrant changing
an incumbent system (at least in any high priority) but would make me want
to encourage a change to the design of a system yet to be deployed if
that's an option.
Sorry if this is all a bit teaching granny to suck eggs but are you worried
about the general case or some specific attack? Because that matters a lot
in the context of your question.

@_date: 2015-11-09 13:48:16
@_author: Max Kington 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
with AES-CTR for encryption, and then later that same key is used for
signing the ciphertext with HMAC-SHA1.  I know that generally it's unsafe
for CBC-MAC (which I'm not familiar with) and RSA keys: I want to be able
to say AES / HMAC is a safe or unsafe construction, and so far I'm not sure
have some confidence in it being a safe construction.
to assert that the application *needs* changing to better apply a
separation of use principle for the purpose of a key (namely that your
encryption and integrity keys should be different for all the key
management reasons that's a good idea)?  Or is is that you want to assert
that in this case, the risk is low and doesn't *have* to be changed but
could be?
Idea.  I want to be able to assert that the reuse of keys here, while not
recommended, has not been actively broken (or at least, there are no
publications to that effect), and if they used AES-CTR and then immediately
MACed the result and didn't mix it with anything... then they're probably
fine.  Maybe.
I wouldn't have said homegrown use of crypto was necessarily a bad idea
(tm). In fact I'd go soc far as to say that we should be careful to not
make that an unqualified statement and say what we consider the risk there.
What we usually worried about is home spun algorithms (with poor
properties) and poor constructions of known ones. Assembling the tools in
the toolbox is in my book fine if you know what you're doing.
and unsafe and think about it in the context of what you're concerned
about.  Specific cryptographic attacks relating to using AES as the chosen
block cipher to underpin MAC OR the general principal of key separation.
Ways To Do That Wrong.  However, I can't know how users put them together.
I have a library that exposes an encrypt method, and a sign (HMAC-SHA1)
method -- no real way to know how users composed them, if they did at all.
I think on that basis you can't necessarily say this is bad.
idea that every time you use a key you expose information about the key.
If you are ever forced to have to change a key because it has been
compromised (either stolen or broken and recovered) then you are able to
make assertions about what you've lost, i.e. the data previously might not
have had integrity but it wasn't ever exposed.  Can I go back, check the
contents of my data and be satisfied that it is what was transmitted and
not interfered with but still be happy it's not been leaked.
changing an incumbent system (at least in any high priority) but would make
me want to encourage a change to the design of a system yet to be deployed
if that's an option.
worried about the general case or some specific attack? Because that
matters a lot in the context of your question.
the system itself, only signing is used.  Some end users found and started
using the encrypt / sign methods for their own crypto and so I'm
deprecating those methods and sticking large warnings over everything.
Then I'm going to point them at Kalium or Keyczar and wish them good luck.
If that's the case and that seems fair enough they still need to use the
pieces sensibly and using certainly NaCl still means you can shoot yourself
in the foot.  Keyczar I guess less so.
Two things, in my world cryptographic libraries need creating as special
kinds of assets so that if they later found to be flawed we can find them.
This may not be a worry for you but consider if you're introducing more
complexity which you don't need.
Change it when you need to which I guess you're kind of doing.
Secondly if people are going to do a rethink and think more broadly about
key management then that's a deeper bit of reengineering and if the
opportunity presents itself that might be a better (or additional) legacy
to your review.
Are you decommissioning it in whole *because* of the crypto? Or just these
libraries all be the quality of their use by the parent app is unknown? I
didn't quite follow that.

@_date: 2015-11-16 18:48:40
@_author: Max Kington 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
It's actually happening all over the place with IoT enabled home based
technology right now. Two of my neighbours have intelligent Internet
connected heating systems. Even as a home consumer, the upgrade path and
ongoing support is a major reason I've not taken the plunge.
As you say, I'm not even talking about a life critical safety system with a
thirty year lifespan
I think you're sort of right but playing devils advocate, does it matter?
We accept that when we build and deploy cryptosystems we may need to not
paint ourselves into a corner when selecting algorithms or key lengths. The
reality is these things change far more infrequently than we engineer for.
The most important thing is though, what is the expected lifespan of the
systems we engineer now? Once upon a time a closed embedded system would
never get upgraded because there were no apis with which it needed to stay
compatible with. An upgrade would involve a new one because the
microcontroller had actually died or a repair.
Where systems now have a range of dependencies on networks and external
systems to provide a significant chunk of their value that ability to stay
static has gone. We are, like it or not, in an environment where systems
will *have* to change.
Ideally secure systems engineering would deal with the need to fail away
from certain mechanisms by design accepting that you may lose some value
add to keep the system safe. Consumers I suspect won't be very accepting of
that and as such I can't see it being in the interests of systems engineers
to invest in that kind of thinking.
This intersection with cryptography only comes about because of the
ingrained attitude of cryptographers to have to think to the future in a
way systems developers never have*
*perhaps they used to but not anymore in any wide measure

@_date: 2015-10-24 07:10:10
@_author: Max Kington 
@_subject: [Cryptography] "We need crypto code training" and other 
I think it depends entirely on the nature of the crypto system you're
building and the risks the crypto is designed to mitigate. Formal academic
crypto courses (the one on the course I'm doing) is largely about the
mechanisms, the properties of the building blocks and then a little bit
about the practical problems like key management.
For the types of system I build this is might be about right but there's a
lot more to building good distributed systems than writing good code. It
takes experience and practice writing a range of types of systems to design
them well which by and large the cryptography experts don't have.
I see the same thing in my sector. For anyone who has worked in financial
services with quants for any length of time you may have experienced very
capable mathematicians who are convinced they're rock star programmers
because it has to be easier than their core skill.
If you're writing an open ssl like library your ratios probably change to
bias the crypto element more.
I agree, but significant improvement could be made by being better
developers of systems which focus on *all* of the issues.
Again, I think it depends on the nature of the application of the crypto.

@_date: 2016-03-19 17:41:04
@_author: Max Kington 
@_subject: [Cryptography] Unicity distance of Playfair 
was a good reference when I was asked the same question on my masters.
There are lots of academics on this list. Perhaps you even know some of
