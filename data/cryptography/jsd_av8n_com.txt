
@_date: 2003-08-22 08:42:45
@_author: John S. Denker 
@_subject: PRNG design document? 
>
 > I'm assuming a cryptographic PRNG of the type in OpenSSL, PGP, etc.,
 > where entropic seeding data is accumulated into a pool and output is
 > produced by operating on the pool with a secure hash or similar
 > cryptographic algorithm.
The statement contains two inequivalent ideas:
  -- some applications (OpenSSL, PGP, etc.) which
     imply certain requirements, and
  -- some technology for generating numbers which
     may or may not meet those requirements.
The mentioned technology is what I classify as a
_stretched_ random symbol generator, because it
outputs an entropy density greater than zero but
less than 100%.
For most of the things that OpenSSL and PGP do,
certainly certificate generation and almost
certainly session-key generation, I would *not*
recommend using a stretched random symbol
generator, but rather a full-blown True Random
Symbol Generator, i.e. 100% entropy density.
There are other situations (e.g. expunging a
multi-gigabyte disk) where you might really
need to do some stretching.
BTW I prefer to reserve the term PRNG to apply
to the extreme case of zero entropy density, but
there's not much to be gained by quibbling about
 > Is there a definitive or highly recommended paper or book on the
 > design of PRNGs?
How about this:
    > I'm interested in whether there's a strong source on what the design
 >  considerations for how to process the input into the pool, mix &
 > remix the pool, and generate output are.
The idea of a pool that needs mixing and remixing
is not the optimal design IMHO.

@_date: 2003-08-27 13:12:18
@_author: John S. Denker 
@_subject: traffic analysis (was: blackmail / stego) 
I changed the Subject: line because most of the
blackmail / stego thread was about traffic
analysis.  It's amusing that traffic analysis could
be used to defeat a steganographic blackmail
attempt, but there are larger issues involved.
It is not true, despite what some people recently
suggested, that traffic analysis destroys any
hope of real-time anonymous communication.
It is true that if you design an "anonymity" system
under the assumption that the opposition doesn't
have enough resources to perform traffic analysis,
you'll be taken to the cleaners if the opposition
does have such resources.
There exist well-known techniques for greatly
reducing the effectiveness of traffic analysis.
A scenario of relevance to the present discussion
goes like this:
  -- There exists a data haven.  (Reiter and Rubin
     called this a "crowd".)
  -- Many subscribers have connections to the haven.
  -- Each subscriber maintains a strictly scheduled
     flow of traffic to and from the haven, padding
     the channel with nulls if necessary.
  -- All the traffic is encrypted, obviously.
Then the opponent can put unlimited effort into
traffic analysis but won't get anything in return,
beyond the _a priori_ obvious fact that some pair
of subscribers *may* have communicated.
As an extension:
  -- The haven may fetch a lot of web pages, some of
them in response to requests from subscribers, and
some not.
Then the opponent can conclude that some subscriber(s)
*may* have looked at some of the fetched pages.
Remark:  I said that each channel must carry (and
only carry) strictly scheduled traffic.  It is
sufficient but not necessary to send a constant
rate.  More complicated schedules, possibly
incorporating a degree of randomness, are allowed.
The point is that the cryptotext traffic must be
independent of the amount (and other characteristics)
of the plaintext traffic.
Additional remarks, having little to do with traffic
analysis, except as a reminder that traffic analysis
isn't the only threat to be considered:
  *) Anonymity means They can't prove you're guilty.
But it also means you can't prove you're innocent.
A sufficiently totalitarian regime will require
everyone to be able to prove their innocence at all
times.  Subscribing to an anonymity service would
therefore be automatically illegal.
  *) Obviously the haven itself must be resistant
to penetration by the opposition.
  *) Obviously if you use this service (or any
other) to communicate with somebody at an endpoint
that is already under surveillance, you have no
privacy.  So you must to some extent trust the
endpoints, no matter how good the channel is.

@_date: 2003-08-28 08:06:07
@_author: John S. Denker 
@_subject: traffic analysis 
A couple of people wrote in to say that my remarks
about defending against traffic analysis are "not
As 'proof' they cite
    which proves nothing of the sort.
The conclusion of that paper correctly summarizes
the body of the paper;  it says they "examined" and
"compared" a few designs, and that they "pose the
question as to whether other interesting protocols
exist, with better trade-offs, that would be practical
to implement and deploy."
Posing the question is not the same as proving that
the answer is negative.
I am also reminded of the proverb:
      Persons saying it cannot be done should
      not interfere with persons doing it.
The solution I outlined is modelled after
procedures that governments have used for decades
to defend against traffic analysis threats to
their embassies and overseas military bases.
More specifically, anybody who thinks the scheme
I described is vulnerable to a timing attack isn't
paying attention.  I addressed this point several
times in my original note.  All transmissions
adhere to a schedule -- independent of the amount,
timing, meaning, and other characteristics of the
And this does not require wide-area synchronization.
If incoming packets are delayed or lost, outgoing
packets may have to include nulls (i.e. cover traffic).
This needn't make inefficient use of communication
resources.  The case of point-to-point links to a
single hub is particularly easy to analyze:  cover
traffic is sent when and only when the link would
otherwise be idle.
Similarly it needn't make inefficient use of
encryption/decryption resources.  This list is
devoted to cryptography, so I assume people can
afford 1 E and 1 D per message; the scheme I
outlined requires 2 E and 2 D per message, which
seems like a cheap price to pay if you need
protection against traffic analysis.  On top of
that, the processor doing the crypto will run
hotter because typical traffic will be identical
to peak traffic, but this also seems pretty cheap.

@_date: 2003-08-29 15:52:39
@_author: John S. Denker 
@_subject: traffic analysis 
>
 > Are you sure you understood the attack?
Are you sure you read my original note?
 > The attack assumes that communications links are insecure.
I explicitly hypothesized that the links were
encrypted. The cryptotext may be observed and
its timing may be tampered with, but I assumed
the attackers could not cut through the
encryption to get at the plaintext.
 > The *transmission* from Alice may adhere to a fixed schedule, but
 > that doesn't prevent the attacker from introducing delays into the
 > packets after transmission.
Fine. So far the timing doesn't tell us anything
about the behavior of Alice, just the behavior
of the attacker.
 > For instance, suppose I want to find out who is viewing my web site.
 > I have a hunch that Alice is visiting my web site right this instant,
 >  and I want to test that hunch.  I delay Alice's outgoing packets,
 > and I check whether the incoming traffic to my web contains matching
 > delays.
I explicitly said that if some endpoints are not
secure, Alice suffers some loss of privacy when
communicating with such an endpoint.  Here DAW is
playing the role of attacker, and is mounting an
attack that combined traffic analysis with much
more powerful techniques; he is assuming he "owns"
the endpoint or otherwise can see through the
crypto into the plaintext.
Let us not confuse "traffic analysis" issues with
"anonymity" issues.
I explicitly said that traffic analysis was not the
only threat to be considered.
To say it another way:  The US ambassador in Moscow
is not trying to remain anonymous from the US
ambassador in Riyadh;  they just don't want the
opposition to know if/when/how-often they talk.
I described a certain model based on certain hypotheses.
Many people have responded with attacks on different
models, based on different hypotheses.  Some have
frankly admitted contradicting me without having
bothered to read what I wrote.  I'm not going to
respond to any more of these ... except to say that
they do not, as far as I can see, detract in any
way from the points I was making.

@_date: 2003-08-29 17:11:05
@_author: John S. Denker 
@_subject: PRNG design document? 
> Allow me to clarify my problem a little. I'm commonly engaged to
 > review source code for a security audit, some such programs include a
 > random number generator, many of which are of ad-hoc design. The
 > nature of such audits is that it's much more appealing to be able to
 > say "here are three accepted guidelines that your generator violates"
 > rather than "I haven't seen that before and I don't like it, you
 > should replace it with something else".
That's a very helpful clarification.
 > So I'm interested in such design guidelines, if they're available,
 > which such a generator could be tested against. While the resources
 > provided have been useful, it's only led me to where I was: that the
 > only way to do so is to attempt to analyze the system for
 > vulnerability to a collection of known flaws.
That dissatisfaction is wise. Checking for
known flaws is far from sufficient. It is
possible to do much better.
 > I know a bunch of basic, obvious things that I can state (have a
 > large enough internal state, generate output with a secure hash,
 > etc.) and a bunch of other fuzzier notions that are harder to
 > concretize (output should be dependent on a sufficient quantity of
 > the internal pool, reseeding should affect a sufficent quantity of
 > the internal pool, etc.). But I don't have a resource which attempts
 > to canonically define minimal requirements for all these elements.
 > (If I have missed such a list in skimming the broad resources
 > available, I'd appreciate a note.)
First, as is so common in this business, a lot
depends on the threat model.

@_date: 2003-12-10 12:02:46
@_author: John S. Denker 
@_subject: example: secure computing kernel needed 
Previous discussions of secure computing technology have
been in some cases sidetracked and obscured by extraneous
notions such as
  -- Microsoft is involved, therefore it must be evil.
  -- The purpose of secure computing is DRM, which is
     intrinsically evil ... computers must be able to
     copy anything anytime.
Now, in contrast, here is an application that begs for
a secure computing kernel, but has nothing to do with
microsoft and nothing to do with copyrights.
Scenario:  You are teaching chemistry in a non-anglophone
country.  You are giving an exam to see how well the
students know the periodic table.
  -- You want to allow students to use their TI-83 calculators
     for *calculating* things.
  -- You want to allow the language-localization package.
  -- You want to disallow the app that stores the entire
     periodic table, and all other apps not explicitly
     approved.
The hardware manufacturer (TI) offers a little program
that purports to address this problem
   but it appears to be entirely non-cryptologic and therefore
easily spoofed.
I leave it as an exercise for the reader to design a
calculator with a secure kernel that is capable of
certifying something to the effect that "no apps and
no data tables (except for ones with the following
hashes) have been accessible during the last N hours."
Note that I am *not* proposing reducing the functionality
of the calculator in any way.  Rather I am proposing a
purely additional capability, namely the just-mentioned
certification capability.
I hope this example will advance the discussion of secure
computing.  Like almost any powerful technology, we need
to discuss
  -- the technology *and*
  -- the uses to which it will be put
... but we should not confuse the two.

@_date: 2003-12-25 21:40:39
@_author: John Denker 
@_subject: stego in the wild: bomb-making CDs 
] Thursday 25 December 2003, 17:13 Makka Time, 14:13 GMT
] Saudis swoop on DIY bomb guide
] ] Authorities in the kingdom have arrested five people after
] raiding computer shops selling compact disks containing
] hidden bomb-making instructions, a local newspaper reported
] on Thursday.
] ] Police were questioning four owners of computer shops in the
] southern Jazan region and a fifth person believed to have
] supplied the CDs to the shops, Al-Watan newspaper said.
] ] Officials were not immediately available for comment.
] ] The daily said some of the shop owners might not have known
] about the bomb-making tutorial files hidden on the CDs. Only
] someone with technical knowledge would be able to find the
] files.
That was quoted from:
and the same story, almost verbatim, was carried by Reuters.
 1) This is not entirely unprecedented.  Al Qaeda for years has
    been hiding recruitment and training footage in the middle
    of otherwise-innocuous video tape cassettes.   2) OTOH using a commercial distribution channel bespeaks a     certain boldness ... somebody is "thinking big".   3) Also: as a rule, anything you do with computers generates     more headlines than doing the same thing with lower-tech methods.
    This is significant to terrorists, who are always looking for
    headlines.  Conversely it is significant to us, who have much
    to lose when our not-so-fearless leaders over-react.
 4) One wonders how many CDs were distributed before the operation
    was terminated.
 5) I wonder how the authorities found out about it.
 6) The article speaks of technical skill ... I wonder how     much technical skill was required.  Probably not much.
 7) Did it rely entirely on security-by-obscurity, or was there     crypto involved also?
    (The latter is possible;  whatever leak told the authorities
    where to look could also have told them the passphrase...
    but the article didn't mention crypto.)
I suspect there is a lot more to this story......

@_date: 2003-11-20 22:04:52
@_author: John S. Denker 
@_subject: Gresham's Law? 
It's not hard to discover other cases.
At the philosophical level, one could argue that
protecting the weak is one of the most fundamental
raisons d'etre for a government.
If you don't like the effects of Gresham's law,
replacing it with the law of the jungle isn't
really an improvement.  Practicality lies in
the vast gray area in the middle.
As a specific example, consider the legal status
of the lock on my door.  Any burglar with even
rudimentary skills could pick the lock.  One with
even less skill could break the fancy glass
beside the door.  More-secure locks and more-secure
doors are readily available.  Yet the law takes
notice of the lock.  If I don't have a door, if
you waltz in it might be trespass or it might be
no offence at all.  But if you pick or smash your
way past a locked door, without permission or
some very special reason, it's likely to be
felony breaking and entering.
Maybe you think that the B&E laws should only
apply to state-of-the-art high security vaults.
I don't.  I think the existing lock performs a
useful symbolic role:  it puts you on notice
that you don't belong there.  You can't get
past it by accident.  The law takes over from
 > ..... in my talks and testimony about the DMCA.
 > I referred to Gresham's Law as it applies to security. I also have
 > called the DMCA "The Snake-Oil Protection Act."
A friend of mine once told me:  Never support a
strong argument with a weak one.
There exist strong arguments why DMCA is a bad
law.  Boldly asserting that the government has
never heretofore built laws around imperfect
technology is not going to impress any lawmakers.
Also, if you're going to argue against something,
it pays to know where the other side is coming
from.  In the areas where cypto works well, it
works so extraordinarily well that bad systems
can, over time, be drowned in their own snake-oil
and forgotten.  If protecting substandard crypto
were the only issue, I doubt anybody would have
gone to the trouble of passing a law.
The point of the law is elsewhere:  the proponents
are worried about what happens in the thousand and
one cases where strong crypto doesn't solve the
To repeat:  If you want to make an argument against
the other side, it's a bad strategy to start by
misjuding what they're arguing for.

@_date: 2003-10-01 12:16:40
@_author: John S. Denker 
@_subject: Monoculture 
>
 > there's another rationale my clients often give for
 > wanting a new security system, instead of the off-
 > the-shelf standbys:  IPSec, SSL, Kerberos, and the
 > XML security specs are seen as too heavyweight for
 > some applications.  the developer doesn't want to
 > shoehorn these systems' bulk and extra flexibility
 > into their applications, because most applications
 > don't need most of the flexibility offered by these
 > systems.
Is that a rationale, or an irrationale?
According to 'ps', an all-up ssh system is less
than 3 megabytes (sshd, ssh-agent, and the ssh
client).  At current memory prices, your clients
would save less than $1.50 per system even if
their custom software could reduce this "bulk"
to zero.
With the cost of writing custom software being
what it is, they would need to sell quite a
large number of systems before de-bulking began
to pay off.  And that's before accounting for
the cost of security risks.
 > some shops experiment with the idea of using only
 > part of OpenSSL, but stripping unused stuff out of
 > each new release of OpenSSL is a maintenance hassle.
1) Well, they could just ignore the new release
and stick with the old version.  Or, if they think
the new features are desirable, then they ought
to compare the cost of "re-stripping" against the
cost of implementing the new desirable features
in the custom code.
I'm just trying to inject some balance into the
balance sheet.
2) If you do a good job "stripping" the code, you
could ask the maintainers to put your  into
the mainline version.  Then you have no maintenance
hassle at all.
 > they want their crypto clothing
 > to fit well, but what's available off-the-rack is
 > a choice between frumpy....
Aha.  They want to make a fashion statement.
That at least is semi-understandable.  People do
expensive and risky things all the time in the name
of fashion.

@_date: 2003-10-03 14:32:49
@_author: John S. Denker 
@_subject: anonymity +- credentials 
>
 > It seems to me that perfect pseudonymity *is* anonymity.
They're not quite the same thing; see below.
 > Frankly, without the ability to monitor reputation, you don't have
 > ways of controlling things like transactions, for instance. It's just
 > that people are still mystified by the concept of biometric
 > is-a-person identity, which strong cryptography can completely
 > divorce from reputation.
We agree that identification is *not* the issue, and
that lots of people are confused about this.
I'm not sure "reputation" is exactly the right concept
either;  the notion of "credentials" is sometimes better,
and the operating-systems folks speak of "capabilities".
There are three main possibilities:
  -- named (unique static handle)
  -- pseudonymous (dynamic handles)
  -- anonymous (no handle all)
Sometimes pseudonyms are more convenient than having no
handle at all.  It saves you the trouble of having to
re-validate your credentials at every micro-step of the
process (whatever the process may be).
Oftentimes pseydonyms are vastly preferable to a static
name, because you can cobble up a new one whenever you
like, subject to the cost of (re)establishing your
credentials from scratch.
The idea of linking (bidirectionally) all credentials
with the static is-a-person identity is a truly terrible
idea.  It dramatically *reduces* security.  Suppose Jane
Doe happens to have the following credentials
  -- Old enough to buy cigarettes.
  -- Has credit-card limit > $300.00
  -- Has credit-card limit > $3000.00
  -- Has car-driving privileges.
  -- Has commercial pilot privileges.
  -- Holds US citizenship.
  -- Holds 'secret' clearance.
When Jane walks into a seedy bar, someone can reasonably
ask to verify her "old-enough" credential.  She might
not want this query to reveal her exact age, and she
might *really* not want it to reveal her home address (as
many forms of "ID" do), and she might *really* *really*
not want it to reveal all her other credentials and
*) There is an exploding epidemic of "ID" theft.
That is a sure sign that people keep confusing
capability --> identity and identity --> capabilities.
*) There are those who want us to have a national ID-checking
infrastructure as soon as possible.  They think this will
increase security.  I think it is a giant step in the wrong
*) Reputation (based on a string of past interactions) is
one way, but not the only way, to create a credential that
has some level of trust.
We need a practical system for anonymous/pseudonymous
credentials.  Can somebody tell us, what's the state of
the art?  What's currently deployed?  What's on the
drawing boards?

@_date: 2003-10-14 01:59:54
@_author: John S. Denker 
@_subject: cryptographic ergodic sequence generators? 
>>I've noted to others on this before that for an application like
 >>the IP fragmentation id, it might be even better if no repeats
 >>occurred in any block of 2^31 (n being 32) but the sequence did not
 >>repeat itself (or at least could be harmlessly reseeded at very very
 >>long intervals).
I assume the point of the reseeding is to make
the ID-values more unpredictable.
 >
 > Let E_k(.) be a secure block cipher on 31 bits with key k.
 > Pick an unending sequence of keys k0, k1, k2, ... for E.
 >
 > Then your desired sequence can be constructed by
 >   E_k0(0), E_k0(1), E_k0(2), ..., E_k0(2^31 - 1),
 >   2^31 + E_k1(0), 2^31 + E_k1(1), ..., 2^31 + E_k1(2^31 - 1),
 >   E_k2(0), E_k2(1), E_k2(2), ..., E_k2(2^31 - 1),
 >   2^31 + E_k3(0), 2^31 + E_k3(1), ..., 2^31 + E_k3(2^31 - 1),
Again if we assume the point is to make the values
unpredictable (not just ergodic), then there is
room for improvement.
To see what I mean, divide the values into generations
G=0,1,2,3... where each row in the tableau above is
one generation.
The problem is that at the end of each generation,
the values become highly predictable, ? la Blackjack.
David's proposal can be improved by the method used
by Blackjack dealers:  shuffle early.  In each
generation, let the argument of E_kG(.) max out at
some fraction (f) of 2^(n-1).  A limit of f=1/2 is
the obvious choice, although other f values e.g. f=2/3
work nicely too.  The domain and range of E_kG(.) are
still unrestricted (n-1)-bit numbers.
This gives us the following properties
  -- Guaranteed no repeats within the last f*2^(n-1) IDs.
  -- Probably no repeats in an even longer time.
  -- Even if the opponent is a hard-working Blackjack
     player, he has only one chance in (1-f)*2^(n-1)
     of guessing the next value.  To put this number in
     context, note that the opposition has one chance
     in 2^(n-1) of guessing the next value without any
     work at all, just by random guessing.
Setting f too near zero degrades the no-repeat guarantee.
Setting f too near unity leads to the Blackjack problem.
Setting f somewhere in the middle should be just fine.
Discussion of conceivable refinements:
A proposal that keeps coming up is to take the values
generated above and run them through an additional
encryption stage, with a key that is randomly chosen
at start-up time (then held fixed for all generations).
The domain and range of this post-processing stage
are n-bit numbers.
This makes the output seem more elegant, in that we
have unpredictability spread over the whole n-bit word,
rather than having n-1 hard-to-predict bits plus one
almost-constant bit.
Define the phase to be P := (G mod 2).
The opponent will have to collect roughly 2^n data
points before being able to figure out which values
belong to which phase, so initially his guess rate
will be closer to one in 2^n, which is a twofold
improvement ... temporarily.
This temporary improvement is not permanent, if we
allow the opponent to have on the order of 2^n
memory.  He will in the long run learn which values
belong to which phase.  I see no way to prevent this.
So as far as I can tell, the proposed post-processing
is more in the nature of a temporary annoyance to the
opposition, and should not be considered industrial-strength
Perhaps more to the point, if we are going to allow
the opposition to have 2^n memory, it would be only
fair to allow the good guys to have 2^n memory.  In
that case, all the schemes discussed above pale in
comparison to something I suggested previously, namely
generating an ID absolutely randomly, but using a
look-up table to check if it has been used recently,
in which case we veto it and generate another.  If
you can afford the look-up table, these randomly
generated IDs have the maximum possible unpredictability.

@_date: 2003-10-17 02:58:25
@_author: John S. Denker 
@_subject: WYTM? 
>
 > it would make sense for the original vendor website (eg Palm)
 > to have signed the "MITM" site's cert (palmorder.modusmedia.com),
 > not for Verisign to do so.  Even better, for Mastercard to have signed
 > both Palm and palmorder.modusmedia.com as well.  And Mastercard to
 > have printed its key's signature in my monthly paper bill.
Bravo.  Those are golden words.
Let me add my few coppers:
1) This makes contact with a previous thread wherein
the point was made that people often unwisely talk
about identities when they should be talking about
credentials aka capabilities.
I really don't care about the identity of the
order-taking agent (e.g. palmorder.modusmedia.com).
What I want to do is establish the *credentials*
of this *session*.  I want a session with the
certified capability to bind palm.com to a
contract, and the certified capability to handle
my credit-card details properly.
2) We see that threat models (as mentioned
in the Subject: line of this thread), while
an absolutely vital part of the story, are
not the whole story.  One always needs a
push-pull approach, documenting the good
things that are supposed to happen *and* the
bad things that are supposed to not happen
(i.e. threats).
3) To the extent that SSL focuses on IDs rather
than capabilities, IMHO the underlying model has
room for improvement.
4a) This raises some user-interface issues.  The
typical user is not a world-class cryptographer
and may not have a clear idea just what ensemble
of credentials a given session ought to have.
This is not a criticism of credentials;  the user
doesn't know what ID the session ought to have
under the current system, as illustrated by the
Palm example.  The point is that if we want
something better than what we have now, we have
a lot of work to do.
4b) As a half-baked thought:  One informal intuitive
notion that users have is that if a session displays
the MasterCard *logo* it must be authorized by
MasterCard.  This notion is enforceable by law
in the long run.  Can we make it enforceable
cryptographically in real time?  Perhaps the CAs
should pay attention not so much to signing domain
names (with some supposed responsibility to refrain
from signing abusively misspelled names e.g.
pa1m.com) but rather more to signing logos (with
some responsibility to not sign bogus ones).
Then the browser (or other user interface) should
to verify -- automatically -- that a session that
wishes to display certain logos can prove that
it is authorized to do so.  If the logos check
out, they should be displayed in some distinctive
way so that a cheap facsimile of a logo won't be
mistaken for a cryptologically verified logo.
Even if you don't like my half-baked proposal (4b)
I hope we can all agree that the current ID-based
system has room for improvement.
Tangentially-related point about credentials:
In a previous thread the point was made that
anonymous or pseudonymous credentials can only
say positive things.  That is, I cannot discredit
you by giving you a discredential.  You'll just
throw it away.  If I somehow discredit your
pseudonym, you'll just choose another and start
This problem can be alleviated to some extent
if you can post a fiduciary bond.  Then if you
do something bad, I can demand compensation from
the agency that issued your bond.  If this
happens a lot, they may revoke your bond.  That
is, you can be discredited by losing a credential.
This means I can do business with you without
knowing your name or how to find you.  I just
need to trust the agency that issued your bond.
The agency presumably needs to know a lot about
you, but I don't.

@_date: 2003-10-22 17:47:47
@_author: John S. Denker 
@_subject: SSL, client certs, and MITM (was WYTM?) 
>
 > The frequency of MITM attacks is very low, in the sense that there
 > are few or no reported occurrences.
We have a disagreement about the facts on this point.
See below for details.
 > This makes it a challenge to
 > respond to in any measured way.
We have a disagreement about the philosophy of how to
"measure" things.  One should not design a bridge according
to a simple "measurement" of the amount of cross-river
traffic in the absence of a bridge.  One should not approve
a launch based on the "observed fact" that previous instances
of O-ring failures were non-fatal.
Designers in general, and cryptographers in particular,
ought to be proactive.
But this philosophy discussion is a digression, because
we have immediate practical issues to deal with.
 > Nobody doubts that it can occur, and that it *can* occur in practice.
 > It is whether it *does* occur that is where the problem lies.
According to the definitions I find useful, MITM is
basically a double impersonation.  For example,
Mallory impersonates PayPal so as to get me to
divulge my credit-card details, and then impersonates
me so as to induce my bank to give him my money.
This threat is entirely within my threat model.  There
is nothing hypothetical about this threat.  I get 211,000
hits from
   SSL is distinctly less than 100% effective at defending
against this threat.  It is one finger in a dike with
multiple leaks.  Client certs arguably provide one
additional finger ... but still multiple leaks remain.
The expert reader may have noticed that there are
other elements to the threat scenario I outlined.
For instance, I interact with Mallory for one seemingly
trivial transaction, and then he turns around and
engages in numerous and/or large-scale transactions.
But this just means we have more than one problem.
A good system would be robust against all forms
of impersonation (including MITM) *and* would be
robust against replays *and* would ensure that
trivial things and large-scale things could not
easily be confused.  Et cetera.

@_date: 2003-09-06 14:30:36
@_author: John S. Denker 
@_subject: cryptographic ergodic sequence generators? 
> For making things like IP fragmentation ids and other similar
 > protocol elements unpredictable, it would be useful to have what I'll
 > call a cryptographic ergodic sequence generator -- that is, a
 > generator that will produce a sequence of n bit numbers such that
 > there are no repeats until you pass the 2^nth number in the sequence
 > (that is, the sequence is a permutation of all 2^n bit numbers) and
 > such that it is very difficult to predict what the next number in the
 > sequence might be beyond the fact that it will not be one of the
 > numbers seen earlier in the sequence. It is also rather important
 > that the generator be computationally inexpensive.
 >
 > Anyone know how to produce such a thing?
Encrypted counter.
The counter provably has a cycle of 2^n.
The encryption is provably 1-to-1.
Choose the encryption key randomly and keep it secret.

@_date: 2003-09-06 20:28:31
@_author: John S. Denker 
@_subject: lopsided Feistel (was: cryptographic ergodic sequence generators) 
> I'm sure that it would be possible to design a Feistel-based block
 > cipher with variable block size, supporting some range of even values
 > of n.
There's no need to exclude odd n.
I know the typical superficial textbook describes
the Feistel trick in terms of splitting each block
exactly in half, but if you understand the trick
you see that it works just fine for other splits.
It doesn't need to be anywhere near half.  It
doesn't even need to be a two-way split.
You could process a 21-bit word as:
  -- three groups of seven, or
  -- seven groups of three, or
  -- one group of twelve and one group of nine, or
  -- whatever.

@_date: 2003-09-07 09:32:23
@_author: John S. Denker 
@_subject: cryptographic ergodic sequence generators? 
> For making things like IP fragmentation ids and other similar
 > protocol elements unpredictable,
OK, that more-or-less defines an objective.
 > it would be useful to have what I'll call a cryptographic ergodic
 > sequence generator
I'm not at all sure that is the best way to approach
the stated objective.
As usual, we need a clear idea of what threats we
are facing; we need more detail than is provided by
the objective mentioned above.  Also, as always, we
want to impose large costs on the bad guys and
small costs on the good guys, so we need to have
some sort of cost model.  In this case, the main
issues appear to be:
  a) some cost if try to re-use an ID prematurely.
  b) some cost if the bad guy guesses what ID we are
     going to use in the near future.  (Presumably he
     can replay IDs from the recent past as much as
     he wants;  we can't stop that.)
  c) constraints on n, the number of bits in the ID.
  d) computation costs and communication costs.
  e) possibly it is important for our peer at the
     receiving end to be able to treat our IDs as
     being _sequential_ and well-ordered, as the
     Subject: of this thread suggests, as opposed
     to being merely distinct.
There's no point in designing something that works
well only when it is not under attack... so let's
assume it is under heavy attack.  The bad guy is
trying like crazy to guess IDs.  This means that
cost (b) must be taken very seriously.
In particular, suppose we have an ergodic design
with n=24.  Then the bad guy can just sit and watch
the first 16 million packets, and can then eat us
for lunch, predicting future IDs with 100% success.
In the long run (large numbers of packets), this is
outcome is as bad as anything could possibly be.
We can do better.
I recommend we consider schemes that are not
strictly ergodic, but instead incorporate some
degree of randomness.
To understand the effectiveness of my schemes, we
must understand item (c), the constraints on n.
Let us suppose that there can be at most 2^k IDs
"alive" in the system at any one time.
  -- Obviously we must have n>=k;  otherwise it
would be impossible to use the IDs reliably, even
in the absence of an attack.
  -- More interestingly, we need to assume n is
appreciably bigger than k, or the whole game is
not worth playing.  That's because the bad guy
has one chance in 2^(n-k) of guessing an ID that
corresponds to an "alive" ID, no matter how
cleverly we choose the IDs.
Therefore, without further ado:
Scheme   Construct a n-bit-long plain-ID using
a k-bit counter concatenated with (n-k) bits of
randomness.  Then form the final crypto-ID by
encrypting the plain-ID.  (The encryption key is
randomly chosen at system boot time, and remains
This guarantees that no ID will collide with any
of the 2^k most-recently-used IDs.  It also
guarantees that the bad guy's chance of guessing
the next ID is only 2^(n-k).  [After all, *we*
don't have any better chance of guessing our next
ID, because of the randomness.]
In the long run, this is better than the strictly
ergodic scheme by a factor of 2^(n-k).  This is
about as well as we can do, in a minimax sense,
based on the maximum success the bad guy can have.
NOTE:  If the IDs need to be sequential, as
mentioned in desideratum (e) above, then we need to
give the decryption key to our peer at the receiving
end.  The peer can recover the plain-ID, discard
the randomness, and use the counter for sequencing.
Scheme   We could go to the extreme of having
no counter at all, just n bits of randomness.  To
prevent collisions, we use a content-addressable
memory sufficient to remember the last 2^k IDs we
sent.  We choose candidate IDs at random; if a candidate
would cause a collision, we discard the candidate and
try again.  This anti-collision step increases
computational costs by roughly one part in 2^(n-k)
and increases communications costs not at all.
About the only advantage this has over scheme  has
to do with whether you think k is an upper bound or
a typical value.  If the typical number of "alive"
IDs in the system is less than the maximum number,
scheme  introduces more randomness and therefore
makes life harder for the bad guy.
This of course totally sacrifices any notion of
Scheme   This is a hybrid of scheme  and scheme
  Keep a c-bit counter, where we allow c to be
less than k.  This absolutely guarantees no collisions
with the last 2^c IDs, and makes it highly likely
that there will be no collisions stretching back much
farther than that.  We take our chances, without
bothering to have a content-addressable memory.
This scheme exploits the tradeoff between cost (a)
and cost (b).  In the likely case that cost (b) is
much larger, it might pay to reduce the chance of
incurring cost (b) even if this means a slight risk
of incurring cost (a).

@_date: 2003-09-13 17:43:00
@_author: John S. Denker 
@_subject: quantum hype 
> ... any observation of the quantum stream is immediately
 > detectable -- but at the recipient's side, and only if checksums are
 > being employed, which are not disturbed by continual or sporadic
 > photon flips.
 >
 > someone will have
 > access to the 20 bytes before the recipient can look at the 20
 > bytes, decide they have been "tampered" with, and alert the sender.
 > So I use symmetric encryption and quantum cryptography for the key
 > exchange... the same situation here. Maybe the recipient will be
 > able to tell the sender about the junk it receives, but Mallory
 > already has read some of the text being ciphered.
1) As the subject: line suggests, there is indeed a lot
of hype in the quantum crypto business.  But there is
also a kernel of reality behind it.
2) Typically people use a combination of quantum and non-quantum
3) Typically there is a multi-stage process:
  -- Exchange several blocks of keying material.
  -- Check for tampering;  reject blocks that show tampering.
  -- Do some post-processing to reduce vulerability
     to undetected tampering.
  -- Use the result to encrypt your actual data.  This
     is the first stage at which valuable data is exposed
     in any way.
Consider the possibilities:
   *) In each block, Mallory has a 50/50 chance of being able
   to copy a bit without being detected.
   *) More generally, Mallory has a 2^-C chance of being able
   to copy C bits without being detected.
As an easy-to-understand example:
You (Alice and Bob, the good guys) choose a C big enough
that 2^-C looks negligible to you.  Alice sends Bob a
bunch of bits (N>>2C).  Bob tells Alice (in the clear) what
receiver settings he used.  Alice then knows which bits
Bob should have been able to receive correctly.  Alice
tells Bob (in the clear) to check a randomly-chosen set
of C bits, checking that they have the values Alice
thinks they should have.  If this test is passed, it
puts an upper bound on how greedy Mallory has been.
Then Alice tells Bob (in the clear) to use another
(disjoint) set of C bits.  Bob XORs these bits together
and calls it one bit of key.  There is only one chance
in 2^-C that Mallory knows this bit.  The efficiency of the
key-exchange is roughly one part in 2C.  So there is an
exponential security/efficiency tradeoff.  Not too shabby.
The foregoing assumed an error-free channel.  Things get
much worse if the good guys need to do error correction.
There are snake-oily products out there that throw in
some "mild" cryptographic assumptions in order to increase
the efficiency.  So beware.
 >
 > Quantum cryptography *assumes* that you
 > have an authentic, untamperable channel between sender and receiver.
Not true.  The signal is continually checked for
tampering;  no assumption need be made.
Not all the world's oil comes from snakes.
Some does, some doesn't.
 > if we want end-to-end security, one can't
 > stick classical routers or other such equipment in the middle of the
 > connection between you and I.
That's true.  A classical router is indistinguishable
from a tap.

@_date: 2003-09-13 19:02:09
@_author: John S. Denker 
@_subject: quantum hype 
>
 > I believe the following is an accurate characterization:
 >  Quantum provides confidentiality (protection against eavesdropping),
 >  but only if you've already established authenticity (protection
 >  against man-in-the-middle attacks) some other way.
I wouldn't have put it quite that way.  Authenticity
doesn't need to come before confidentiality.
Let's consider various threats:
  1) passive eavesdropping.
  2) active eavesdropping including tampering.
  3) simple impersonation at the far end.
  4) MITM, which can be considered a form of
     active eavesdropping by means of a double
     impersonation.
Quantum key exchange provides end-to-end protection
against passive eavesdropping.  It plugs into the
block diagram in the same place as Diffie-Hellman
key exchange would plug in.  It's the same only a
little stronger (no assumptions about algorithmic
That means you can establish a confidential but
anonymous tunnel, and then send authentication
messages through the tunnel.
As far as I know, there are no quantum algorithms
that prevent impersonation.  Perhaps I'll learn of
some tomorrow, but I would be truly surprised.
Quantum mechanics isn't going to tell you that
John Doe  is a good guy while John Doe is a bad guy.
This is quite significant, because key exchange is
only one part of any practical system.  Quantum
mountebanks claim to have solved "the" key
distribution problem, but this is untrue.  They
have dealt with _exchange_ of session keys, but
they have not dealt with the _distribution_ of
authentication keys.
Distributing and securing any kind of keys under
(say) battlefield conditions is a nightmare.
Reducing the amount of keying material helps
only slightly, unless you can reduce it to zero,
which has not been achieved AFAIK.
Then you have to consider the cost of very special
endpoint equipment, the cost of a very special
communication channel, and the cost of using that
channel inefficiently.

@_date: 2003-09-18 18:38:19
@_author: John S. Denker 
@_subject: quantum hype 
>>
 >>  *) In each block, Mallory has a 50/50 chance of being able to
 >>  copy a bit without being detected.
 >
 > This is what I don't buy. If Mallory sees the data, it must be
 > detected, because otherwise the approach is flawed. But in any case
 > does Mallory have the means to completely DoS any attempt of
 > communication between the parties, simply by reading along, unless
 > there is a dedicated channel between Alice and Bob. In which case,
 > why is there a need for quantum cryptography in the first place?
Yes, Mallory can DoS the setup by reading (and thereby
trashing) every bit.  But Mallory can DoS the setup by
chopping out a piece of the cable.  The two are equally
effective and equally detectable.  Chopping is cheaper and
Other key-exchange methods such as DH are comparably
incapable of solving the DoS problem.  So why bring up
the issue?
 >>There is only one chance in 2^-C that Mallory knows this bit.
 > One chance in 2^C, otherwise it would be deadly, no? But in any
 > case, Reasonable keysized DH exchanges give me the same security
 > with a lot more flexibility, and a lot less chance for DoS. I still
 > don't buy it.
The claim that DH is "secure" rests on certain assumptions
about which computational operations are easy and which
are not.  These assumptions are open to question to some
degree.  Numbers that some people considered hopelessly
difficult to factor a few years ago have been factored.
One can imagine a world where factoring is computationally
easy;  it wouldn't be the end of the world.  If you can
_prove_ DH is secure, please let us know immediately.
The security of the quantum algorithms rests on entirely
different foundations.  Nobody has been able to even
imagine a world where quanta are copyable, without
contradicting well-observed physical facts.  People
have tried.  Seriously.  If you have a consistent theory
of physics that repeals the uncertainty principle, please
let us know immediately.
 > How can you check for tampering without reading the data off the
 > channel? Checksums?
I spelled this out in my previous email.  It's a
standard quality-assurance check using sampling.
 > why do I need QC then if I have
 > a dedicated channel anyhow?
Suppose I *wish* to set up a dedicated channel.  Dedicated
means nobody but me is using it.  Wishing doesn't suffice.
I went through the motions of setting it up, and maybe I
was the only person hooked onto it yesterday, but how do
I know it hasn't been tapped sometime since then?  Quantum
key-exchange provides powerful assurance that the wished-for
property is actually achieved.

@_date: 2003-09-19 14:46:56
@_author: John S. Denker 
@_subject: quantum hype 
That's a fair question.  Here's an outline of
the answer.
We choose an eps << 1.
We ask how many people accurately received a
fraction (1-eps) of the bits.
  -- perhaps nobody received that many.  This
     will be detected.  No key exchange will
     take place.  Start over.  Do not pass Go,
     do not collect $200.00.
  -- perhaps one person did.  In this case,
     without loss of generality, we call this
     person Bob.
  -- the laws of quantum mechanics assure us
     that not more than one person will receive
     that many bits.  Quanta cannot be copied.
Alice can then publish in the clear (e.g. on
netnews) what basis she used for transmitting.
This information is of little use to anyone
except Bob (exponentially little, as a function
of eps and other parameters).  Anyone who
tampers with this message can cause a DoS but
not a compromise of the data.
Alice and Bob proceed with the integrity checks
leading to the key exchange as previously described.
After the key exchange has taken place, Alice
and Bob can use the key to set up a tunnel to
keep their discussions private.  Probably one
of the first things they will do is exchange
authentication messages through the newly
created tunnel.  Thereby Alice can decide
whether this Bob is the Bob she wanted to
talk to, as opposed to an impersonator.
Similarly Bob ought to check Alice's creds.
 >  They need integrity
No, the authentication etc. can quite
nicely come after the quantum key exchange,
as I previously mentioned.
 >  Is the
We need a more specific question.
Does quantum key exchange solve all of the world's
problems?  Surely not.
Does quantum key exchange solve *any* of the world's
problems?  More specifically, is there any plausible
scenario where QKE is more cost-effective than
conventional modern crypto, within (say) the next
ten years?  I tend to doubt it, but it's hard to
be sure.  What is the chance of a treeemendous
cryptanalytic breakthrough that will defeat all or
most of the currently-used ciphers?  I'd say the
chance is less than 1%.  But is it less than one
in a million?  Or perhaps more relevantly, what
is the chance that an enemy black-bag artist or a
traitor or a bungler will compromise all my keys
and/or all my plaintext?  The latter is not to
be sneezed at, and puts an upper bound on what
I'm willing to pay for fancy crypto.
To calibrate the sincerity of my estimate:  I
walked away from a potential job managing some
major programs in this area.

@_date: 2004-08-07 16:31:27
@_author: John Denker 
@_subject: Al Qaeda crypto reportedly fails the test 
Perry wondered:
I believe it's real.  As for WWII, remember that Lt. John F.
Kennedy used a Playfair cipher to arrange the rescue of his
PT-109 crew.  That doesn't mean that Playfair is unbreakable
in general.
As Ian pointed out in his commentary:
there is such a thing as a "field cipher" and it is appropriate
to use it for *some* purposes.
It appears there are two different conversations going on
here.  It may be useful to distinguish between:
  -- a cryptosystem, writ small,
  -- a communication security system, writ large.
IF (!) a field cipher had been used for long-term storage
of lengthy al-Qaeda surveillance reports, it would have
been a gross misapplication of the field cipher.  But ...
   a) I don't think that's what happened in this case, and
   b) Even if it did happen, IMHO that sort of thing should
not be considered a failure of the cryptosystem _per se_.
Instead it should be considered a failure of the comsec
system in the larger sense.
By way of analogy: Students misapply Newton's 2nd law of
motion all the time, but that doesn't mean the laws of
motion are invalid.
It now appears that our current discussion of field ciphers
is moot.  It is now reported that al Qaeda's commsec system
succumbed to _practical cryptanalysis_.  That is, a communcation
officer in a highly responsible post was turned, i.e. arrested
then left in place as a double agent.  According to the news
The outing of Khan will presumably be richly covered in news/talk
shows in coming days.
If reports are true, then Tom Ridge has found an amazingly
unhappy medium:  divulging enough detail to burn an invaluable
source, but not enough detail to make ordinary citizens believe
the alerts are well-founded.
This would not be the first time that high government officials
have allegedly burned important sources.  It was widely reported
that in 1998 bin Laden stopped using his Inmarsat phone when US
officials boasted about their ability to track his position.
Here's a challenge directly relevant to this group:  Can you
design a comsec system so that pressure against a code clerk
will not do unbounded damage?  What about pressure against a
comsec system designer?

@_date: 2004-08-23 01:04:58
@_author: John Denker 
@_subject: First quantum crypto bank transfer 
1) There is a difference between
  -- rightfully blowing one's horn, when one has something to
   blow about, versus
  -- saying things that aren't true.
We must object when people make claims that aren't true.  This
includes claiming something has short-term relevance when it
doesn't.  Responsibility for this attaches primarily to the
workers who originated the claims.  Responsibility attaches
secondarily to the press for uncritically propagating the
claims, but that's another matter.
2) As for "how the world works" ... the real world has means
for sanctioning people who say things that aren't true.
 > But we aren't physicists. We're security people. To us, this is an
 > extremely expensive way of producing a system that is no more secure
 > (and sometimes even less secure) than simply running, say, TLS.
Some of us are physicists.  In my judgement, these demonstrations
are not good physics.  They don't shed light on any of the
fundamental issues.  Sometimes the work is merely unoriginal, and
sometimes it's just plain wrong physics.  The fundamental physics
needed for these demonstrations was done years ago by other folks.
These demonstrations may be best thought of as a combination of
engineering and hype.
Investigators in any field (crypto or physics or whatever) must
exercise judgement in choosing which problems to attack.  If
somebody chooses a project such that even if the project meets
all its goals, the result is worthless ... that's spectacularly
bad judgement.
Hint: As a general rule, if somebody has to lie about the
applicability of his research in order to get funding, it's
because the research won't stand on its own merits.
Snake oil plus hbar equals snake oil.
I once knew a guy who passed himself off as a professor of
mathematical biology.  He would go to math conferences and
mumbo about biology (very impressive).  He would go to biology
conferences and jumbo about math (very impressive).  In fact,
though, he didn't have much to contribute to either field.
Calling oneself a physicist does not give one a license to
do bad cryptography.  Or vice versa.

@_date: 2004-08-23 10:55:12
@_author: John Denker 
@_subject: First quantum crypto bank transfer 
Most of the comments on this list are more nuanced than that.
Examples of sensible comments include:
  -- We have seen claims that QM solves "the" key distribution
   problem.  These claims are false.
  -- _Commercialization_ of QM bit-exchange is dumb, for now
   and for the forseeable future.  I am reminded of a slide
   Whit Diffie showed (in a different context) of an attempt
   to build a picket fence consisting of a single narrow pale
   a mile high ... while the rest of the perimeter remains
   undefended.  That's a dumb allocation of resources.  The
   opposition aren't going to attack the mega-pale;  they are
   going to go around it.  QM doesn't solve the whole problem.
   Sensible research should not be directed toward making the
   tall pale taller;  instead it should be directed toward
   filling in the gaps in the fence.
 > Even if some snake-oil salesmen have attached themselves
 > to the field doesn't say research in the field is worthless.
Be that as it may, there are other grounds for judging the
commercialization projects to be near-worthless.
That's backwards.  Quantum crypto free in space is hard.  It's
much easier to use a single-mode fiber, over distances such
that there is little total attenuation (which can be a quite
macroscopic distance, since the attenuation is a fraction of
a db/km if you do it right).
Again, that diametrically misstates the physics.  Propagation
through a couple km of fiber shouldn't have surprised anybody.
Within a year of the invention of quantum computation,
people were working on quantum error correction.  This
is interesting work and has had spin-offs in the form
of changing how people think about error correction even
in non-quantum systems.  And it has had spin-offs
applicable to quantum cryptography, i.e. showing how it
is possible to survive a modest amount of attenuation.
Huh?  The world abounds in QM systems that produce classical
results, including e.g. transistors, lasers, practically all of
chemistry, etc. etc. etc.  Quantum computers produce classical
results because that is what is desired.
If the intent is to make quantum cryptography sound better
than quantum computation, the point is implausible and
If the intent it so make the best results in quantum crypto
sound better than the lamest parts of quantum computation,
then the comparision is (a) unfair and (b) hardly a ringing
endorsement of quantum crypto.
It's not true that transistors were invented solely for
application to phone lines.  Even if it were true, it would
be irrelevant for mulitple reasons.  For starters, keep
in mind that the big computers built during the 1940s
were built using vast amounts of telecom switch gear.
Bletchley Park relied on engineers from the Post Office
(which was the 'phone company' in those days).
And even if the facts had been otherwise, arguments about
the near-term applicability of one technology are largely
irrelevant to the near-term applicability of another

@_date: 2004-08-23 21:06:52
@_author: John Denker 
@_subject: The Call Is Cheap. The Wiretap Is Extra 
1) Here's an article from the New York Times.
The headline just about says it all.  Reportedly
THEY want voice-over-internet users to pay for
the privilege of having their calls tapped.
 > The Call Is Cheap. The Wiretap Is Extra.
(I cite the version online at The Ledger because
folks can read it there without registering, unlike
the nytimes.com site.)
2) A modest proposal:
I think we should set up the following system:
   a) Users certify to their ISP that they use end-to-end
strong crypto on all their voice-over-internet calls, using
tamper-resistant (or at least tamper-evident) hardware and
   b) The ISP demands such certification from all users.
   c) The ISP declines to install wiretap equipment, and
passes the savings on to the users.
   ... Who could possibly object?
Note that traffic-analysis is still possible, but the
equipment to do that is much cheaper.
Also note that if THEY are going to bugger my endpoints
to defeat the crypto, they might as well do the job right
and change the signalling so that the call goes directly
to THEIR premises ... That way the ISP doesn't need to get
involved, i.e. the ISP has no tap-related costs.

@_date: 2004-08-26 22:00:16
@_author: John Denker 
@_subject: ?splints for broken hash functions 
Hi --
I don't fully understand the new attacks on the hash
functions (which I suppose is forgiveable, since the
papers don't seem to be available).
But here's a shot in the dark that seems promising at
first glance:
Given a message m, pad it to a length of at least one
block to form M.  Then encrypt M with some standard
algorithm (e.g. AES) using some agreed-upon key to
form E(M).  Then concatenate.  Then calculate the hash,
   hash2(M) := hash1[M (+) E(M)].
The conjecture is that hash2() is stronger than hash1().
Note that encryption alone is not enough;  you could
just find a collision pair (X and Y) for the unadorned
hash1 function, then apply the decryption function D()
and then you've got a collision pair D(X) and D(Y) for
the encrypt-then-hash function.
But having both M and E(M) changes the story.  You have
to fiddle bits of M to get a collision in the first
block, and that will mess up whatever you're trying to
do to find collisions in the later block where E(M) is
Here's another splint using the same general idea, but
with less complexity:  calculate the hash once then
prepend that to the message and hash again, i.e.
    hash3(M) := hash1[hash1(M) (+) M]
The idea is that anything you do to M to arrange a
collision in the later blocks will severely mess up
whatever you wanted to do with the first block.
The work for the legit hash user is only slightly more
than doubled, while the work for the attacker is, I
conjecture, increased quite a bit more than that.
I find it ironic that only a few weeks ago in the thread
on "the future of security" there seemed to be a general
consensus that the future would not involve much interesting
work on the mathematics of cryptography or cryptographic
As Yogi Berra supposedly said, it's hard to make predictions,
especially about the future.

@_date: 2004-08-28 23:37:42
@_author: John Denker 
@_subject: ?splints for broken hash functions 
>> However ... *any* on-line algorithm falls to a Joux-style attack.
If you insist on being able to hash exceedingly long strings
(i.e. longer than your storage capacity) here is a modification
of the "splint" I proposed earlier.
Run two hash-machines in parallel.  The first one operates
normally.  The second one puts the first block of the string
into a buffer (bounded size!) and then proceeds to hash the
rest of the string, starting with the second block.  At the
end of the message, it appends the saved block to the tail of
the message and hashes it.
The two results are combined in some nonlinear way, perhaps
by appending the other machine's hash onto this machine's
input string.
The strength here comes from the idea the that you cannot
engineer a collision in the "saved" block in the second
machine until you have engineered the collision in the
first machine, and then if you change the saved block to
engineer a collision in the second machine you have to
redo everything you did in the first machine.
Of course it would be nice to have hash functions that were
strong on a block-by-block basis ... but still I think there
is value in destroying the prefix property without destroying
the online property, i.e. not destroying the ability to hash
strings that are too long to store.
   Small point: I'm not entirely convinced that *any* useful
   hash must have the online property.  That's not a defining
   property of hash functions according to most authorities.
   I've done lots of hashing in situations where I would have
   been happy to store the entire input-string.
   Still we can agree that the online property is *sometimes*
   nice, and I'm happy to be able to provide it.

@_date: 2004-08-29 12:42:51
@_author: John Denker 
@_subject: A splint for broken hash functions 
I think that was intended to be something like
      H2(M)  = H1( H1(M)  xor  H1( TT( M)))
         ^
OK, it looks like we are in agreement on the general
outline of a reasonable method for splinting broken hash
However I think the given example of a  "Trivial Transformation"
is _too_ trivial.  There are too many common strings for which
swapping halves of the first block leaves the string unchanged.
At the very (!) least, we should call the two parallel
hash operations H1 and H1prime, and give H1prime a
different initialization vector.  This has considerable
benefits at essentially zero cost, which makes for a
nice cost/benefit ratio.
Also, I don't like the XOR function suggested above.  It
is linear and totally lacking in one-wayness.  That means
that an attacker has the option of making equal-and-opposite
(or in the case of XOR, just equal :-) changes in the H1
output and the H1prime output, leaving the result of the
XOR unchanged.
Therefore I restate my preference for combining the H1 and
H1 prime results nonlinearly, perhaps like this:
      Hnew(M)  = H1(M  (+)  H1prime(TT(M)))
where (+) denotes the string-concatenation operator.
It should go without saying that if you start with two different
hash functions (H1 and H2) which are not assumed strong but
are expected (hoped?) to not have the same weaknesses, you
want to use one of each:
      Hnew(M)  = H1(M  (+)  H2(TT(M)))
In case anybody didn't notice, bear's idea of applying the TT
operator within a given block has some good properties.  In
particular it strengthens the hashing of short messages,
including one-block messages.
However IMHO using a truly "trivial" transformation is really
under-doing it.  Perhaps we should be thinking in terms of
Tasteful Transformations rather than Trivial Transformations.
Also I suggest the transformation should be applied to each
and every block of the second stream, not just the first
I reiterate my preference for having the transformation include
a _long-range_ permutation.  My previous suggestion of relocating
an entire block is sub-optimal;  bear's half-block suggestion is
better (since it works for short one-block messages) and indeed
you could pick a smaller chunk, maybe 24 bytes.  But do not restrict
it to a short-range permutation i.e. within a given block; cut a
chunk off the front of the second stream and paste it onto the
very end.  This works well enough for one-block strings, but works
even better for longer messages, by destroying the prefix property,
thereby increasing the attacker's work ... out of all proportion
to the legit user's work.
My guess is that using a TT function consisting of a permutation
plus a change in IV should be enough to keep the attackers at bay
for a few years at least (hopefully long enough for us to invent,
validate, and deploy some intrinsically non-broken hash functions).
However, in the spirit of belt-and-suspenders, we might consider
including a lightweight block cipher (with a widely-known key)
as part of the TT function.

@_date: 2004-08-29 14:25:13
@_author: John Denker 
@_subject: ?splints for broken hash functions 
I agree with 99% of what Hal Finney wrote.  I won't repeat
the parts we agree on.
But let's discuss these parts:
I'd say the answer is at most 2^160, possibly 2^160, but not provably
2^160.  Every amateur cryptographer who combines two methods expects
to get a multiplicative increase in strength ... but cryptanalysts
earn their living by finding ways to kill multiple birds with one
I don't think that last question is the right question.  Journeyman
cryptographers check that their work is resistant to *known* attacks.
Master cryptographers design stuff that is resistant to attacks that
are not known, and not really even conceived of.
I don't claim to be much of a cryptographer, so I suppose I'd better
answer the question :-).
*Ideally* just changing the IV should be sufficient.  But *ideally*
the basic hash function wouldn't be broken and wouldn't need splinting.
I strongly prefer my long-range permutation trick, for the simple(?)
reason that I just don't like the prefix property.  It's a gut reaction.
My intuition is screaming at me:  bad design, bad design, asking for
trouble, asking for trouble.
Here is a barely-conceivable attack, offered as an a-posteriori
parable in support of the just-mentioned intuition.  Suppose we
have a long input string.  And suppose there are some initialization
vectors that are somehow "weak".  Further suppose somebody jiggers
the first block to make a weak IV for the second block, then jiggers
the second block to make an even weaker IV for the third block, and
so on.  Finally the IV for the last block is so weak that any desired
final result can be achieved.  (I'm not saying that there's the
slightest reason to believe such an attack is possible ... remember
this is just a parable.)  The point of the story is that maybe long
messages are more vulnerable to attack than short messages.  They
offer a bigger target.  [End of parable.]
In any case, I reeeally don't like the prefix property.  And the
cost of eliminating it is incredibly small ... so why not?  Just
apply the long-range permutation, i.e. cut a few bytes off the
front of the second string and paste them onto the end
So I still prefer the scheme in my previous email.  In its
simplest version, the diagram is:
    IV1 (h) B1  (h) B2  (h) B3  (h) ... Bk  -> H1
    IV2 (h) B1' (h) B2' (h) B3' (h) ... Bk' (h) H1 -> H2 = final
where processing proceeds left-to-right, both rows can be
computed in parallel, (h) denotes the elementary hash-a-block
operation, and the primes on the second stream indicate that
it has been subjected to the long-range permutation operation.
For that matter, it wouldn't hurt my feelings if you applied
long-range permutations to *both* strings.  (Not the same
permutation, of course.)  Did I mention that I really don't
like the prefix property?

@_date: 2004-08-31 16:56:25
@_author: John Denker 
@_subject: Compression theory reference? 
There are two conficting requirements there.
  -- authoritative and correct, and
  -- understandable to a judge with no technical expertise.
As you know, it is easy to satisfy either requirement separately.
My suggestions:
  1) Get a few "expert witnesses" to certify that your position is
certainly not a personal fantasy.  (I assume German jurisprudence
has something akin to the US notion of expert witnesses, right?)
  2) Try to get the burden-of-proof reversed.  The opposition
are claiming that known algorithms do the job.  Get them to be
specific about which algorithms they are referring to.  Then
file with the court some disks, each containing 1.44 megabytes
of data.  If the opposition are telling the truth, they should
be able to compress these using one of their chosen algorithms.
Offer to concede the entire case if they can shorten the text
by more than, say, 0.1 percent (i.e. 1.44 kilobytes shorter).
Of course you fill your disks using a good hardware random
number generator, e.g.
   Be sure to arrange suitable precautions so that the opposition
doesn't get a chance to peek at your disks and build a
special-purpose "rumplestiltskin" encoder/decoder, i.e. one
that contains quotations of your data.  One precaution would
be to require that they use an open-source implementation, or
at least an impartial implementation, of a published algorithm.
  3) Diagram the pigeon-hole argument for the judge.  See
diagrams below.
  4) Don't forget the _recursion_ argument.  Take their favorite
algorithm (call it XX).  If their claims are correct, XX should
be able to compress _anything_.   That is, the output of XX
should _always_ be at least one bit shorter than the input.
Then the compound operation XX(XX(...)) should produce something
two bits shorter than the original input.  If you start with a
N-bit message and apply the XX function N-1 times, you should be
able to compress each and every message down to a single bit.
Here's how to diagram the pigeon-hole argument:
Write down all the three-bit numbers, and demonstrate a lossless
non-compressive encoding:
plaintext     codetext
   000 ---------- 000
   001 ---------- 001
   010 ---------- 010
   011 ---\  /=== 011
           \/
           /\
   100 ===/  \--- 100
   101 ---------- 101
   110 ---------- 110
   111 ---------- 111
Then give an example of a lossy compressive code.  Make
the point that the codetext words are shorter than the
plaintext words.  However, there are necessarily fewer
of them, so the coding scheme is necessarily lossy and
plaintext     codetext
   000 ---------- 00  "zero"
   001 ---------- 01  "one"
   010 ---------- 10  "two"
   011 -------=== 11  "many"
             /
   100 ----/|
   101 ----/|
   110 ----/|
   111 ----/
Then give an example of a code that might or might not
be compressive, depending on statistical assumptions.
The following code is compressive if zero, one, and two
are considerably more probable than the other three-bit
numbers, but does is anti-compressive if all eight
three-bit numbers are equally likely, or nearly equally
   000 ---------- 00  "zero"
   001 ---------- 01  "one"
   010 ---------- 10  "two"
   011 ---------- 11000
   100 ---------- 11001
   101 ---------- 11010
   110 ---------- 11011
   111 ---------- 11100
Average length, for equiprobable plaintext words:
    (2+2+2+5+5+5+5+5) / 8 = 3.875
which is an expansion, not a compression.
Judges like fairness.  Certainly an equiprobable distribution
of input words is fair.  It reflects the bit-strings that
would be generated by tossing fair coins (three at a time),
or tossing a fair eight-sided die.  This fairest of distributions
is incompressible in the usual sense.
Finally, offer a challenge.  Write down all eight three-bit
plaintext words, and all four two-bit codetext words.  Ask
the judge to conclude for himself that it is obviously
impossible to draw lines connecting corresponding words in
a one-to-one fashion.
   000           00
   001           01
   010           10
   011           11
   100
   101
   110
   111
Note that in the last example, the codetext words were only one bit
shorter than the plaintext words.  If you want to pound on the point,
present the samething with four-bit plaintext and two-bit codetext,
i.e. where the codetext is _two_ bits shorter than the plaintext,
and demonstrate that the problem is even more extreme.
It may be better to leave off the diagram for this one, so as to
not burden the judge ... i.e. just make the assertion and dare
the opposition to refute it.  Remember that the typical judge chose
to study law because he didn't *want* to learn about logarithms and
entropy and all that.
Important note:
The following may sound like something from the Scholasticism
of the dark ages (angels dancing on pins and all that) but it's
I personally have been shouted down at conferences for making
the points discussed above.  The shouters say that Lempel-Ziv
is "universal".  Indeed they point out the the title of the
seminal paper by Lempel & Ziv is
   "A Universal Algorithm for Sequential Data Compression"
And the funny thing is, in some narrow Scholastic sense they
are right ... LZ is "universal".  The fun comes from the fact
that they have _defined_ the word "universal" in a funny way.
According to their definitions, a "universal" compressor is
one that is no worse than any other by more than some
constant.  (The constant depends on which compressors are
being compared, but is independent of the choice of inputs.)
Notice that I am careful to write the term "universal" in
scare-quotes whenever this twisted technical meaning is
A "universal" compressor is not always compressive.
To summarize:  An algorithm that is "universal" in the
narrow technical Scholastic sense is not universal, or
all-purpose, or general-purpose in the ordinary vernacular
Therefore I recommend you choose your words very carefully,
so the bad guys don't get a chance to trip you up on a narrow

@_date: 2004-08-31 21:19:58
@_author: John Denker 
@_subject: Compression theory reference? 
Actually you don't need to adjoin log(N) bits.  But perhaps my
assertion would benefit from some clarification.
I emphasize that I am only discussing messages of length N,
where N is some pre-chosen number.  For concreteness, let's
choose N=10.
I repeat my assertion that _if_ XX can compress any string,
shortening it by one bit, and _if_ you know that the original
messages each have exactly 10 bits, _then_ any 10-bit message
can be compressed down to a single bit.
I have proved that XX is ridiculous in this one case.
My function YY := XX^9 is less general than XX.  XX works
on any input, whereas YY by its definition only applies to
10-bit messages.
The fact remains that we have a proof by contradiction.  We
assume by way of hypothesis that the bad-guys are right, namely
that XX exists and has the properties they assign to it.  Then
I can construct YY.  But YY is ridiculous, through no fault of
mine.  Ergo the bad guys are wrong, i.e. no such XX can exist.
With a little more work I could construct a more powerful and/or
more general version of YY ... but that would be doing more work
than is required.  Their XX stuck its neck out;  it is not
required for my YY to stick its neck out in the same way.  The
requirement, as I understood it, was to prove the bad guys
wrong.  Well, the bad guys have been proved wrong.  If something
more is required, please explain the requirements in more detail.
   (BTW I suppose it would be better to call this the 'iterated
   composition' argument rather than the recursion argument.)
Ask your legal advisor.
In the US, getting such emails admitted as evidence would be
problematic.  Each jurisdiction (I assume) has its own standards
for how affidavits should be prepared.  Figure out the rules,
and play by the rules.

@_date: 2004-12-01 13:45:15
@_author: John Denker 
@_subject: IPsec +- Perfect Forward Secrecy 
I'm mystified by the word "always" there, and/or perhaps by
the definition of Perfect Forward Secrecy.  Here's the dilemma:
On the one hand, it would seem to the extent that you use
ephemeral DH exponents, the very ephemerality should do most
(all?) of what PFS is supposed to do.  If not, why not?
And yes, IPsec always has ephemeral DH exponents lying around.
On the other hand, there are IPsec modes that are deemed to
not provide PFS.  See e.g. section 5.5 of
   Perhaps the resolution of the dilemma is to say that IPsec
"always" uses ephemeral DH for _some_ things, but it does not
"always" use ephemeral DH for some _other_ things.  Right?
Also note that 'ephemeral' is not a binary predicate.  Some
things are more ephemeral than others.  Can you also have
more-perfect PFS and less-perfect PFS?
There are plenty of things out there (including Cisco boxes,
in the default configuration) where the IPsec does not have
PFS turned on.

@_date: 2004-12-01 21:08:13
@_author: John Denker 
@_subject: IPsec +- Perfect Forward Secrecy 
OK, let me ask a more specific question.  Actually, let me
put forth some hypotheses about how I think it works, and
see if anyone has corrections or comments.
0) I'm not sure the words Perfect Forward Secrecy convey what
we mean when we talk about PFS.  Definition 12.16 in HAC suggests
_break-backward protection_ as an alternative, and I prefer that. Perhaps the complementary concept of break-back _exposure_ would
be even more useful.
      I think for today we don't have a simple yes/no question as
to whether the secrecy is "perfect";  instead we have multiple
quantitative questions as to which connections have how much
break-back exposure.
1) First an ISAKMP SA is set up, then it is used to negotiate
one or more IPsec SAs, which carry the traffic.
2) Ephmeral DH is always used on the ISAKMP SA, so the ISAKMP
session has no more than one ISAKMP session's worth of break-back
exposure.  That is, the attacker who steals an ISAKMP session
key can read that session, but (so far as we know :-) does not
thereby gain any head-start toward reading earlier ISAKMP sessions.
3) Each IPsec SA has its own session key.  The stated purpose of
Quick Mode is to provide "fresh" keying material.  "Nonces" are
used.  As I understand it, that means the IPsec session keys are
sufficiently ephemeral that each IPsec session has no more than
one IPsec session's worth of break-back exposure.  That is, the
attacker who steals an IPsec session key can read that session,
but does not (sfawk :-) gain any head-start toward reading
earlier IPsec sessions.
4) As far as I can tell, the only interesting question is whether
a break of the ISAKMP session is _inherited_ by the IPsec sessions
set up using that ISAKMP session.  The break of an IPsec session
will not spread at all.  The break of an ISAKMP session will not
spread beyond that ISAKMP session ... but what happens within that
ISAKMP session?  The answer, as I understand it, depends on the
setting of the misleadingly-named "IPsec PFS" option.  If the
option is set, there is an additional layer of opacity on a
per-IPsec-SA basis, so that a break of the ISAKMP session is not
inherited by its IPsec SAs.
Bottom line:
As I understand it, IPsec always has reasonably tight limit on
the amount of break-back exposure, but setting the so-called
"PFS" option reduces the exposure further ... roughly speaking,
by a factor of the number of IPsec SAs per ISAKMP SA.
Comments, anyone?

@_date: 2004-12-05 18:03:48
@_author: John Denker 
@_subject: 'Proving' the correctness of a network encryption system test 
This question is unanswerable.  The attempted answer by Iang
misses the point.
The point is that we haven't been told enough, not nearly
enough, about what the phrase "as it should" means.
Step Zero is for Bob to sit down with Alice and make a detailed
list of all the things Alice wants to happen, and an equally
detailed list of all the things Alice doesn't want to happen.
In my experience, it may be possible to meet Alice's objectives
with no encryption at all ... or it may be impossible to meet
Alices objectives using any known combination of techniques
(crypto included) ... or somewhere in between.
Most particularly, there is a huuuge difference between
  -- verifying that the "encrypted network stream" is correct, and
  -- ascertaining that the "system" is working "as it should".
To say that again in more standard terms, there is a huuge
difference between cryptography and security.
As a specific and not-very-far-fetched example, suppose the
"encrypted network streams" consist of SSH traffic flowing
to/from port 22 on each of Alice's machines, and are fully
ideal in the sense of being as secure as you could possibly
ask SSH to be.  Alas that does prevent *anybody* from
telnetting to port 23 and logging in as root.  Maybe Alice
wants to permit that, or maybe (probably) not.
The IPsec RFC, to its credit, addresses both edges of the
two-edged sword.  It specifies what the encryption should
look like, and it also requires the existence of an SPDB
(Security Policy DataBase) that has enough expressive
power to prevent a wide class of undesired traffic from
occuring.  Of course this leaves to the implementor the
question of what policy "should" be in the SPDB, but at
least the mechanism is there begging to be used.
This brings to mind Dykstra's dictum that testing can
prove the presence of bugs, but cannot prove the absence
of bugs.  You should not imagine even for a moment that
any amount of testing will demonstrate that the system
is working "as it should".
Therefore if the question is whether the test system is
proving the security of the main system, the meta-test
system called for in item 3 can be replaced by a simple
box that answers "no".
As a specific example, suppose a port-scan tells you that
port 23 is not open.  But things could be set up such that
the port is only open during certain hours of the day, and
you didn't happen to look at the right time.  (I've seen
stranger things.)
Bottom line:  Alice doesn't need just a test.  Alice needs
a whole lot of things, including good specs, good design,
design reviews, good implementation, implementation audits,
etc. etc. etc. ... and even then Alice will have to accept
a nonzero amount of residual risk.

@_date: 2004-12-22 11:49:18
@_author: John Denker 
@_subject: SSL/TLS passive sniffing 
No, I wouldn't.
 > or ...
I wouldn't do that, either.
If the problem is a shortage of random bits, get more random bits!
Almost every computer sold on the mass market these days has a sound
system built in. That can be used to generate industrial-strength
randomness at rates more than sufficient for the applications we're
talking about.  (And if you can afford to buy a non-mass-market
machine, you can afford to plug a sound-card into it.)
For a discussion of the principles of how to get arbitrarily close
to 100% entropy density, plus working code, see:

@_date: 2004-12-22 17:20:48
@_author: John Denker 
@_subject: SSL/TLS passive sniffing 
>>If the problem is a shortage of random bits, get more random bits!
Florian Weimer responded:
Not very special, as I explained:
1) You read it correctly.
   2) The exact number depends on details of your soundcard.  14kbits/sec
was obtained from a plain-vanilla commercial-off-the-shelf desktop
system with AC'97 audio.  You can of course do worse if you try (e.g.
Creative Labs products) but it is easy to do quite a bit better.
I obtained in excess of 70kbits/sec using an IBM laptop mgfd in
3) Why should this be surprising?
Let's see....
  -- Cost = zero.
  -- Quality = more than enough.
  -- Throughput = more than enough.
I see no reason why I should apologize for that.
As the proverb says, no matter what you're trying to do, you can always
do it wrong.  If you go looking for potholes, you can always find a
pothole to fall into if you want.
But if you're serious about solving the problem, just go solve the
problem.  It is eminently solvable;  no sacrifices required.

@_date: 2004-07-01 14:26:46
@_author: John Denker 
@_subject: authentication and authorization (was: Question on the state 
> .... For the first
The object of phishing is to perpetrate so-called "identity
theft", so I must begin by objecting to that concept on two
different grounds.
1) For starters, "identity theft" is a misnomer.  My identity
is my identity, and cannot be stolen.  The current epidemic
involves something else, namely theft of an authenticator ...
or, rather, breakage of a lame attempt at an authentication
and/or authorization scheme.  See definitions and discusions
in e.g. _Handbook of Applied Cryptography_
   I don't know of any "security experts" who would think for a
moment that a reusable sixteen-digit number and nine-digit
number (i.e. credit-card and SSN) could constitute a sensible
authentication or authorization scheme.
2) Even more importantly, the whole focus on _identity_ is
pernicious.  For the vast majority of cases in which people
claim to want ID, the purpose would be better served by
something else, such as _authorization_.  For example,
when I walk into a seedy bar in a foreign country, they can
reasonably ask for proof that I am authorized to do so,
which in most cases boils down to proof of age.  They do
*not* need proof of my car-driving privileges, they do not
need my real name, they do not need my home address, and
they really, really, don't need some "ID" number that some
foolish bank might mistake for sufficient authorization to
withdraw large sums of money from my account.  They really,
really, reeeally don't need other information such as what
SCI clearances I hold, what third-country visas I hold, my
medical history, et cetera.  I could cite many additional
colorful examples, but you get the idea:  The more info is
linked to my "ID" (either by writing it on the "ID" card or
by linking databases via "ID" number) the _less_ secure
everything becomes.  Power-hungry governments and power-
hungry corporations desire such linkage, because it makes
me easier to exploit ... but any claim that such linkable
"ID" is needed for _security_ is diametrically untrue.
Returning to:
 > .... For the first
 > time we are facing a real, difficult security
 > problem.  And the security experts have shot
 > their wad.
I think a better description is that banks long ago
deployed a system that was laughably insecure.  (They got
away with it for years ... but that's irrelevant.)  Now
that there is widespread breakage, they act surprised, but
none of this should have come as a surprise to anybody,
expert or otherwise.
Now banks and their customers are paying the price.  As
soon as the price to the banks gets a little higher, they
will deploy a more-secure payment authorization scheme,
and the problem will go away.
(Note that I didn't say "ID" scheme.  I don't care who
knows my SSN and other "ID" numbers ... so long as they
cannot use them to steal stuff.  And as soon as there
is no value in knowing "ID" numbers, people will stop
phishing for them.)

@_date: 2004-07-05 18:28:01
@_author: John Denker 
@_subject: authentication and authorization 
That's true but unhelpful.  In a typical dictionary you will
find that words such as
  -- heat
  -- elastic
  -- blue
  -- etc. etc.
have many, many non-technical meanings that are radically
divergent from the technical meanings.
We should assume that the participants on this list have a
goodly amount of technical expertise.  We should use the
established technical definitions, unless there is a good
reason not to.
   Note that terminology has at best secondary importance.
   Concepts are primary.  Terminology is important only to
   the extent that it helps us think clearly and speak
   clearly about the concepts.
That is very unhelpful, because it lumps together two types
of things that really ought to be treated differently.
  -- I want my email address to be widely known.  I want my
   public keys to be widely known.
  -- I want my password to be secret.  I want my private keys
   to be secret.
Failure to make this distinction exacerbates the problem
significantly.  For example, originally a SSN was supposed to
be a database-key, used for indexing into databases, and as
such it needed to be unique but it didn't need to be secret.
Then some bozos started using it as if it were a password.
Just because it is "something you know" doesn't make it
useful as a password;  a password ought to be something you
know that nobody else knows!
Nevermind the terminology; we've got to start thinking
clearly about this concept:  Is your SSN merely a database-key,
or is it a password?  How about your credit-card number?
Again that (including the reference) misses the point and
blurs things that really need to be kept distinct.
The US government makes available to the public databases
that contain my SSN, height, weight, home address, and other
_identifying_ information.
I really don't care if everybody knows how to _identify_ me,
so long as they don't _impersonate_ me.  Maybe you know my
address, but that doesn't mean you live in my house.  Maybe
you know my height and weight, but that doesn't mean you
look like me (and even if you look sorta like me, that
doesn't mean you _are_ me).
We are talking about the uttermost foundations of cryptology
here.  Yes, ID information can be copied.  Virtually all of
cryptology starts from the assumption that at the transport
layer, everything is subject to passive attacks (copying) and
perhaps active attacks (tampering).  Crypto is something we
do at higher layers to make sure such attacks don't pay off.
As a corollary, this means good crypto imposes a high cost on
the bad guys but only a small cost on the good guys.
Ian G. put his finger on the problem when he spoke of
Anybody who knows anything about security knows that relying
on an all-powerful root privilege is the path to perdition.
But I don't approve of the rest of his paragraph:
 >>> So the reality of it is, the predeliction with
 >>> identity being the root key to all power is the
 >>> way society is heading. I don't like it, but
 >>> I'm not in a position to stop the world turning.
First of all, not everything is heading the wrong way.
The Apache server has for eons had privilege separation
features.  The openssh daemon acquired such features
recently.  As far as I can see, the trend (in the open
software world at least) is in the right direction.
Resignation and fatalism isn't going to get us anywhere.
We ought to take the lead in making sure that ID does
*not* become the root key to all power.  Contact your
elected representatives and explain to them that an
ID-based system is a baaad idea.  It is not even a
security-versus-liberty tradeoff;  it is bad for
security and bad for liberty both.
The focus _must_ be on the transaction, not on the ID.
Suppose I carry out a transaction with the jewellery
store.  Did I authorize a $3.00 payment for a new watch
battery, or a $30,000.00 payment for diamond necklace?
Collecting more and more ID information about me is at
best marginally helpful to the relying party;  "ID" might
tell the RP whether I *could* have authorized a particular
transaction (was it within my account limit?) but "ID"
cannot possibly tell the RP whether I *did* authorize a
particular transaction.  And (!!) don't forget the
converse:  If the transaction is legit, there is no
reason why my ID needs to be involved.  Cash transactions
are still legal!
The proper use of _identification_ is obvious:  In some
exceptional circumstances it is important to be able to
connect a real meat-space _identity_ with a particular
event.  For instance, if there is a hit-and-run accident,
it really helps if a witness notes the license number of
the car.  (Been there, done that.)
I don't know whether to laugh or cry when I think about how
phishing works, e.g.
The so-called "ID" is doing all sorts of things it shouldn't
and not doing the things it should.  The attacker has to
prove he knows my home address, but does not have to prove
he is physically at that address (or any other physical place)
... so he doesn't risk arrest.
 >>> the security experts have shot their wad.
I do not see any basis for such an assertion.  Look again
at the exploits described in
After a few minutes thought, I can see at least three ways
to defend against these particular exploits, including one
way of proactively making the crime uncommittable, plus
two ways of stinging anybody who dares commit the crime.
I imagine there are people on this list who can do even
It doesn't even take a "security expert" to figure out easy
ways of making the current system less ridiculous.
Note that on the page
one of the crooks repeatedly characterizes
  -- the Feds as "lazy"
  -- AOL as "stupid"
which is consistent with what I've been saying.  I don't
think people have tried and failed to solve the phishing
problem --- au contraire, I think they've hardly tried.
I've seen estimates that the losses due to phishing are
roughly one billion dollars per year, and rapidly rising.
This can be compared to "Nigerian" 419 advance fee scams,
which reportedly run about two gigabucks per year.
If the industry devoted even a fraction of that sum to
anti-scam activities, they could greatly reduce the losses.
I've been to the Anti-Phishing Working Group site, e.g.
   They have nice charts on the amount of phishing observed
as a function of time.  But I haven't been able to find
any hard information about what they are actually doing
to address the problem.  The email forwarded by Dan Geer
was similarly vaporous.
Here's an interesting link, describing the application of
actual cryptology to the problem:
   IMHO it's at a remarkable place in the price/performance
space:  neither the cheapest quick&dirty solution, nor the
ultimate high performance solution.  At least it refutes
the assertion about security experts' wads having been
shot.  This is one of the first signs I've seen that real
security experts have even set foot in this theater of
operations, let alone shot anything.

@_date: 2004-07-11 14:22:01
@_author: John Denker 
@_subject: Hyperencryption by virtual satellite 
To my surprise, there has been no follow-up discussion
on this list.
   (Hint:  Most people on this list will want to skip
   the first four parts of the presentation, which are
   just an introduction to cryptography, and start with
   the part entitled "hyper-encryption and semantic
   security".)
We should not let Rabin's assertions to go unchallenged.
Here is a brief review:
In this presentation, there are some remarkable claims,
including claims of a method that produces "provably"
everlasting secrets.  Alas no such proof is provided;
the alleged proof is full of holes.
By way of example, a particularly glaring hole surrounds
the notion that a PSN ("page server node") should serve
a given page only twice.  That causes all sorts of
difficulties to the legitimate system users (see the
discussion of "page reconciliation") but poses no
particular burden on the attackers, who can passively
copy one of those two issuances.
As another example, the "proof" of correctness uses
randomness in invalid ways.  It asserts that the
system users will randomly select a subset of the
available pages of random numbers (which is OK as
far as it goes) but alas it further assumes that the
attackers will only be able to select pages by an
_independent_ random process.  Perhaps this comes
from an assumption that attacks will be directed
against the servers.  However, attacks can perfectly
well be directed against the system users' PCs.  The
attacker therefore only needs communication bandwidth
and storage capabilities comparable to the system
users who are under attack (!!not!! all users total).
As far as I can tell, the whole topic of "hyper-encryption"
would be more usefully discussed under the heading of
_privacy amplification_.  I get 2000 hits from
   It remains to be seen whether these authors will make
any useful contributions in this area.  To do so, they
will need to come out with some more modest claims
and/or some stronger proofs.

@_date: 2004-07-15 16:29:18
@_author: John Denker 
@_subject: Humorous anti-SSL PR 
To which Eric Rescorla replied:
The humor just keeps on coming.  It's always amusing to
see an invocation of the principle that "I've tried it
on several occasions and it seemed to work, therefore
it must be trustworthy."
What's wrong with this depends, as usual, on the threat
model.  Sometimes it is wise to consider other parts
of the system (not just the pipe) in the threat model.
If we set you up on a blind date with an underfed grizzly,
you might find that protecting your pipe with a condom
doesn't solve all your problems.

@_date: 2004-07-18 12:30:28
@_author: John Denker 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
Good question.
Actually there are two questions we should consider:
  a) What are the procedures phishermen are using today,
     procedures that they manifestly *can* get away with?
  b) Why why why are they allowed to get away with such
     procedures?
Here is something of an answer to question (a):
The details are a bit sketchy, and maybe not entirely to
be trusted since they come from self-described crooks,
but they are plausible.
Still question (b) remains.  The described procedures seem
to be the e-commerce analog of parking your car in a bad
neighborhood with the windows rolled down and the keys in
the ignition.  That is, I expect that most people on this
list could easily think of several things the card-issuers
could do that would shut down these attack-procedures,
significantly raising the phishermen's work-factor and risk
of arrest -- without significantly burdening legitimate
merchands or cardholders.

@_date: 2004-06-10 14:21:38
@_author: John Denker 
@_subject: TIA Offices Discovered 
>
That's the main DARPA building.  For driving directions,
see the DARPA web site:
   It's not very surprising that folks there wear DARPA badges.
Should we congratulate Hampton and Thompson on their "discovery"?
Or should we chip in and buy them each a new tinfoil hat?
Their full article may be found at:
    I imagine parts of it are actually true.  But then again, a
stopped watch gives the correct time twice a day.
More-reliable accounts of TIA are readily available.  A
useful compendium is:
            et cetera.

@_date: 2004-09-01 21:24:34
@_author: John Denker 
@_subject: ?splints for broken hash functions 
I wrote
 >> the Bi are the input blocks:
 >> (IV) -> B1 -> B2 -> B3 -> ... Bk -> H1
 >> (IV) -> B2 -> B3 -> ... Bk -> B1 -> H2
 >>then we combine H1 and H2 nonlinearly.
   (Note that I have since proposed a couple of improvements,
    but I don't think they are relevant to the present remarks.)
OK so far.  I think this assumes a sufficiently long input string,
but I'm willing to make that assumption.
OK so far.
Whoa, lost me there.
I thought H1 was fixed, namely the ordinarly has of the original
message.  Two questions:
   1) If H1 is fixed, I don't understand how birthday arguments apply.
    Why will trying the bottom line 2^80 times find me H@ equal to a
    particular fixed H1?  There are a whooooole lot more that 2^80
    	possibilities.
   2) If H1 is not fixed, then this seems to be an unnecessarily
    difficult way of saying that a 160-bit hash can be broken using
    2^80 work.  But we knew that already.  We didn't need Joux or
    anybody else to tell us that.
    A proposal that uses 80*2^80 work is particularly confusing, if
    H1 is not fixed.
Still lost, for the same reasons.  Please explain.  Also if possible
please address the improved version, with different IVs and long-range
permutation of a partial block.

@_date: 2004-09-02 13:26:41
@_author: John Denker 
@_subject: Compression theory reference? 
>>Plus a string of log(N) bits telling you how many times to apply the
 >>decompression function!
 >>Uh-oh, now goes over the judge's head ...
That misses the point of the construction that was the subject of
Matt's remark.  The point was (and remains) that the compressed
output (whether it be 1 bit, or 1+log(N) bits, or 1+log^*(N) bits)
is ridiculous because it is manifestly undecodeable.  It is far, far
too good to be true.
The only question is whether the construction is simple enough
for the judge to understand.
There is no question whether the construction is a valid _reductio
ad absurdum_.
   While we are on the subject, I recommend the clean and elegant
   argument submitted by Victor Duchovni (08/31/2004 03:50 PM) and
   also in more detail by Matt Crawford (08/31/2004 06:04 PM).  It
   uses mathematical induction rather than proof-by-contradiction.
   It is a less showy argument, but probably it is understandable
   by a wider audience.  It proves a less-spectacular point, but it
   is quite sufficient to show that the we-can-compress-anything
   claim is false.  (Although with either approach, at least *some*
   mathematical sophistication is required.  Neither PbC nor MI will
   give you any traction with ten-year-olds.)
   So it appears we have many different ways of approaching things:
    1) The pigeon-hole argument.  (This disproves the claim that all
     N-bit strings are compressible ... even if the claim is restricted
     to a particular fixed N.)
    2) The mathematical induction argument.  (Requires the claimed
     algorithm to be non-expansive for a range of N.)
    3) The proof-by-contradiction.  (Requires the claimed algorithm
     to be compressive -- not just non-expansive -- for a range of N.)
    4) Readily-demonstrable failure of *any* particular claimed example,
     including Lempel-Ziv and all the rest.
    *) Others?
   Harumph.  That really ought to be enough.  Indeed *one* disproof
   should have been enough.
I don't see why the number of iterations should be exponential in
the length (N) of the input string.  A compression function is
supposed to decrease N.  It is not supposed to decrement the
number represented by some N-bit numeral .... after all, the string
might not represent a number at all.
Also I repeat that there exist special cases (e.g. inputs of
known fixed length) for which no extra bits need be represented,
as I explained previously.
I disagree, for the reasons given above.
In the worst case, you need log^*(N) extra bits, not N bits.  In
special cases, you don't need any extra bits at all.  The "win"
is very substantial.  The "win" is extreme.
Maybe.  But there's no need to make it more complicated than it
really is.
For general N, that's true.
 > So you'r counting register needs
Maybe.  For many practical purposes, the required number of bits
is considerably less than infinity.
 > When you're finished you know
We agree that there are many common mistakes.  We agree that
it is a mistake to have undelimited strings.  But it is also a
mistake to think that you need to reserve a special symbol to
mark the end of the string.  Yes, that is one option, but from a
data-compression point of view it is an inefficient option.
Anybody who is interested in this stuff reeeally ought to read
Chaitin's work.  He makes a big fuss about the existence of
self-delimiting strings and self-delimiting programs.  There
are many examples of such:
  -- The codewords of many commonly-used compression algorithms
   are self-delimiting.  This is related to the property of being
   "instantaneously decodable".
  -- As Chaitin points out, you can set up a dialect of Lisp such
   that Lisp programs are self-delimiting.
  -- If you want to be able to represent M, where M is *any* N-bit
   number, you need more than log(M) bits (i.e. more than N bits).
   That's because you need to specify how many bits are used to
   represent log(M), which adds another log(log(M)) bits.  On top
   of that, you need to specify how many bits are used to specify
   log(log(M)), which adds another ..... you get the idea.
   Fortunately the series converges verrry fast.  This series
   defines a new function, named log^*(), pronounced log-star.
   Constructive examples are known of representation schemes
   that give a self-delimiting representation of *any* integer,
   no matter how large.  These are called 'universal' representations.

@_date: 2005-08-06 16:27:51
@_author: John Denker 
@_subject: solving the wrong problem 
In a similar context, Whit Diffie once put up a nice
graphic:  A cozy little home protected by a picket fence.
he fence consisted of a single picket that was a mile
high ... while the rest of the perimeter went totally
So, unless/until somebody comes up with a better metaphor,
I'd vote for "one-picket fence".
I recognize that this metaphor is not sufficiently
pejorative, because a single picket is at least
arguably a step in the right direction, potentially
a small part of a real solution.

@_date: 2005-08-07 22:30:35
@_author: John Denker 
@_subject: solving the wrong problem 
That's an interesting topic for discussion, but I don't think
it answers Perry's original question, because there are plenty
of situations where the semblence of protection is actually a
cost-effective form of security.  It's an example of statistical
Look at it from the attacker's point of view:  If a fraction X
of the beware-of-dog signs really are associated with fierce dogs,
while (1-X) are not, *and* the attacker cannot tell which are which,
and there are plenty of softer targets available, the attacker
won't risk messing with places that have signs, because the
downside is just too large.  The fraction X doesn't need to be
100%;  even a smallish percentage may be a sufficient deterrent.
OTOH of course if the sign-trick catches on to the point where
everybody has a sign, the sign loses all value.
We can agree that the dog-sign is not a particularly good
application of the idea of statistical enforcement, because
there are too many ways for the attacker to detect the
absence of a real dog.
A better example of statistical deterrence is traffic law
enforcement.  The cops don't need to catch every speeder every
day;  they just need to catch enough speeders often enough,
and impose sufficiently unpleasant penalties.  The enforcement
needs to be random enough that would-be violators cannot
reliably identify times and places where there will be no
Statistical enforcement (if done right) is *not* the same as
"security by obscurity".
This is relevant to cryptography in the following sense:  I doubt
cryptological techniques alone will ever fully solve the phishing
problem.  A more well-rounded approach IMHO would include "sting"
operations against the phishers.  Even a smallish percentage
chance that using phished information would lead to being arrested
would reduce the prevalence of the problem by orders of magnitude.
Let me propose another answer to Perry's question:
   "Wearing a millstone around your neck to ward off vampires."
This expresses both ends of a lose/lose proposition:
   -- a burdensome solution
   -- to a fantastically unimportant problem.
This is related to the anklets on the White Knight's horse,
"to guard against the bites of sharks" ... with added emphasis
on the burdensomeness of the solution.

@_date: 2005-12-21 14:15:47
@_author: John Denker 
@_subject: permutations +- groups 
There are multiple misconceptions rolled together there.
1) All of the common block ciphers (good and otherwise) are permutations.
  To prove this, it suffices to require that ciphertext blocks be the
  same length as plaintext blocks, and that arbitrary text can be enciphered
  and (given the proper key) uniquely deciphered.  It's an elementary
  pigeon-hole argument:  each plaintext block must map to one and only one
  ciphertext block.
2) If you consider the set of all imaginable permutations, there will be
  ((2^B) factorial) elements, where B is the blocksize of the cipher.  Call
  this set U.  The set U is closed under composition, so in fact we necessarily
  have a group.  This is neither a good thing nor a bad thing;  it just is.
3) However, the group mentioned in item (2) is not what people mean when
  they talk about a cipher having "the group property".  Let me illustrate
  using plain old DES.  There are at most 2^56 keys.  Therefore let us
  consider the set of all 2^56 /keyable/ permutations;  call this set K.
  This is a verrry small subset of the ((2^64) factorial) imaginable
  permutations.
4) The interesting question is whether K is closed under composition.  This
  is of particular interest if you are trying to cobble up an improved cipher
  with an N-times longer key, by layering N copies of the original cipher.
  This is guaranteed to be a waste of effort if K is closed under composition,
  i.e. if K is in fact a group, i.e. a subgroup of U.
  The converse does not hold;  showing that K is not closed does not suffice
  to show that layering is a good idea.

@_date: 2005-02-09 12:02:21
@_author: John Denker 
@_subject: link-layer encryptors for Ethernet? 
Several people have made suggestions involving IPsec,
which were not immediately accepted.
Let me try to kick the discussion downfield a little ways.
I don't know what application Steve has in mind, but
there exists a wide range of applications for which I
would seriously consider the following approach:
   GRE over IPsec transport.
This would not in the narrowest sense be encrypting the
layer-2 traffic, but it would look like that to the
To say the same thing in more detail:  As you know, the
IPsec RFC covers two main subjects:
  A) the layout of the packets, and
  B) the SPDB (security policy database)
If we temporarily neglect part (B), to a decent approximation
we can describe IPsec tunnel mode in terms of IPsec transport
mode, as follows:
   IPIP over IPsec transport.
The analogy to GRE over IPsec transport should now be clear.
You can call it GREsec or L2sec.
The SPDB issues can now be stirred back in.  In the
implementation of IPsec that I am most famililar with
(OpenS/WAN, descended from FreeS/WAN) the SPDB was
   a) never 100% implemented, but
   b) what was implemented was pretty much orthogonal
    to the packet-formatting business,
so you don't AFAICT lose anything important by using GRE
instead of IPIP encapsulation.
I would argue that for most applications, GREsec is better
than genuine strict link-layer encryption, because it
can, if you want, be transported across routers, whereas
anything at the genuine link layer cannot.
I realize that there remain some issues that I have not
addressed, notably processes that live in the gray area
between layer two and layer three, such as arp, dhcp,
and neighbor discovery.
If this is barking up the wrong tree, please explain why.
Please tell us the real requirements in more detail.

@_date: 2005-01-04 15:41:12
@_author: John Denker 
@_subject: Conspiracy Theory O' The Day 
Another hypothesis:  Cover traffic, to defeat traffic analysis.
The procedure:  send N copies.  N-M of them are spam, sent to uninterested
parties.  The other M parties are the intended recipients.  Provided N>>M,
and other mild restrictions, they achieve plausible deniability.

@_date: 2005-01-05 14:06:29
@_author: John Denker 
@_subject: entropy depletion (was: SSL/TLS passive sniffing) 
>
 > This "entropy depletion" issue keeps coming up every now and then, but I
 > still don't understand how it is supposed to happen.
Then you're not paying attention.
 > If the PRNG uses a
 > really non-invertible algorithm (or one invertible only with intractable
 > complexity), its output gives no insight whatsoever on its internal state.
That is an invalid argument.  The output is not the only source of insight
as to the internal state.  As discussed at
    attacks against PRNGs can be classified as follows:
   1. Improper seeding, i.e. internal state never properly initialized.
   2. Leakage of the internal state over time. This rarely involves
     direct cryptanalytic attack on the one-way function, leading to
     leakage through the PRNG?s output channel.  More commonly it
     involves side-channels.
  3. Improper stretching of limited entropy supplies, i.e. improper
     reseeding of the PRNG, and other re-use of things that ought not
     be re-used.
  4. Bad side effects.
There is a long record of successful attacks against PRNGs (op cit.).
I'm not saying that the problems cannot be overcome, but the cost and bother
of overcoming them may be such that you decide it's easier (and better!) to
implement an industrial-strength high-entropy symbol generator.
 > As entropy is a measure of the information we don't have about the
 > internal state of a system,
That is the correct definition of entropy ... but it must be correctly
interpreted and correctly applied;  see below.
 > it seems to me that in a good PRNGD its value
 > cannot be reduced just by extracting output bits. If there is an entropy
 > estimator based on the number of bits extracted, that estimator must be
 > flawed.
You're letting your intuition about "usable randomness" run roughshod over
the formal definition of entropy.  Taking bits out of the PRNG *does*
reduce its entropy.  This may not (and in many applications does not)
reduce its ability to produce useful randomness.

@_date: 2005-01-06 04:48:24
@_author: John Denker 
@_subject: entropy depletion 
>>  Taking bits out of the PRNG *does* reduce its entropy.
By one bit per bit.
If you said that, you'd be wrong.
This is getting repetitious.  As I said before, this is an abuse of the
terminology.  If you want to quantify the goodness of your PRNG, go right
ahead, but please don't apply the word "entropy" to it.  The word is already
taken.  It means something very specific.
The linux /dev/random driver has lots of problems, but that's not one of them.
That makes perfect sense.  Anything else would not make sense.  That's what
entropy is.  If you're not interested in entropy, then go measure whatever
you are interested in, but don't call it entropy.
 > Perhaps, a great
 > deal of blockage problems when using /dev/random would go away with a more
 > realistic estimate.
100% of the blocking problems go away if you use /dev/urandom to the exclusion
of /dev/random.
For the Nth time:
   a) Most of modern cryptography is based on notions of computational intractability.
I'm not saying that's good or bad;  it is what it is.
   b) My point is that there is an entirely different set of notions, including the
notion of entropy and the related of unicity distance, which have got *nothing* to
do with computational intractability.
You can study (a) if you like, or (b) if you like, or both.  Maybe (a) is best
suited to your application, or maybe (b).  But whatever you do, please don't
mistake one for the other.
Lots of things have large amounts of usable randomness, with little or no
entropy.  Please don't mistake one for the other.

@_date: 2005-01-07 10:28:29
@_author: John Denker 
@_subject: entropy depletion 
Jerrold Leichter asked:
That's a good question.  I think there is a good answer.  It
sheds light on the distinction of pseudorandomness versus
   A long string produced by a good PRNG is conditionally
   compressible in the sense that we know there exists a shorter
   representation, but at the same time we believe it to be
   conditionally incompressible in the sense that the adversaries
   have no feasible way of finding a shorter representation.
In contrast,
   A long string produced by a HESG is unconditionally, absolutely
   incompressible. There does not exist a shorter representation.
   There cannot possibly exist a shorter representation.

@_date: 2005-01-07 13:29:55
@_author: John Denker 
@_subject: entropy depletion 
>>
It works for me.  It is in principle hard to apply exactly, but
in practice it's easy to apply to a good-enough approximation.
In particular, for any given PRNG I can readily exhibit the
compressed representation of its output, namely the PRNG
itself along with the initial internal state.
 > K/C complexity is robust
Yes, I stand corrected.  I sloppily wrote "string" when I should
have written "strings".  Specifically, the set of strings
collectively cannot be compressed at all. As for each string
individually, it is not compressible either, except possibly for
trivially rare cases and/or trivial amounts of compression.
Yes, for one particular string, or very small set of strings.
You get to choose the model, but you only get to choose once.
So you can only compress a trivially small subset of all the
output strings, unless we're talking about a trivial degree
of compression.  In any case, you derive no cryptanalytic
advantage from compression based on an ad-hoc model, so you
might as well not bother, and just stick to some convenient
standard model.
 > Turning that into a
Isn't that what I said, as quoted above?  I said it was
conditionally compressible.  It should go without saying
that knowing the trapdoor information, i.e. the internal
state, is the sufficient condition for feasible compressibility.
Well, I can agree that that's what we _should_ be talking about.
I like to speak of an SRNG (stretched random number generator)
as having a nonzero lower bound on the entropy density of the
output, as opposed to a traditional PRNG where the entropy
density is asymptotically zero.
 > There are
When I wanted to stretch entropy, I implemented the Yarrow
approach, i.e. encrypted counter plus systematic reseeding:
   It's not slow, and appears incomparably stronger than the
SHA^i example.  Indeed it does not have any serious weaknesses that I know of.  If anybody knows of any flaws, please explain!
We agree it's naive.  Indeed it's just wrong, as I've said N
times over the last couple of days.  And please let's not talk
about "effective" entropy.  I have no idea what "ineffective"
entropy is supposed to be, and I can't imagine any way that makes
sense to distinguish "effective" entropy from plain old entropy.

@_date: 2005-01-07 17:35:09
@_author: John Denker 
@_subject: Entropy and PRNGs 
I just took a look at the first couple of pages.
IMHO it has much room for improvement.
*) For instance, on page 2 it says
That's absurd.  If it has no unpredictability, it has no entropy,
according to any reasonable definition of entropy, including the
somewhat loose definition on page 1.
*) Later on page 2 it says:
First of all, you can't throw around the term "conditional
entropy" without saying what it's conditioned on.  For a PRNG,
unpredicatbility conditioned on knowing previous outputs is one
thing; unpredictability conditioned on knowing the entire
internal state is something else entirely.
More importantly, there are lots of cryptographers, and they
don't all want the same thing.  In particular, the entropy
of a good one-time pad is unconditional, in the sense that
you can condition it on anything you like and the entropy is
unchanged (with the trivial exception of 100% correlation to
the counterpart pad locked up in the safe at headquarters).
Also importanty, for UUIDs no entropy is required at all.
A globally-accessible counter has no entropy whatsoever, and
suffices to solve the UUID problem   OTOH if you demand an
offline, autonomous method for generating UUIDs, your
collision resistance depends on the number of different seeds
your PRNG can have ... which is plain old entropy, the same
sort needed when making one-time pads.
*) At the bottom of page 2 it says:
No, the system time has no entropy whatsoever, not by any
reasonable definition of entropy.
*) On page 4, I find the name "trueprng" to be an oxymoron.
The P in PRNG stands for Pseudo, which for thousands of
years has meant "false", i.e. the opposite of true.

@_date: 2005-01-08 11:29:33
@_author: John Denker 
@_subject: entropy depletion 
Yes, and I'd like my goldfish to ride a bicycle, but he can't.
The P in PRNG is for Pseudo, and means the PRNG is relying
on computational intractability, not entropy.
For PRNGs that's true ... which is why I have introduced the
terminology of SRNG, meaning Stretched RNG, which is a
hermaphrodite, being partly PRNG and partly HESG.  Linux
at a SRNG.  Yarrow is vastly better.
Well, that depends on details.  Suppose you seed your PRNG with
1000 bits of entropy, and then put out 50,000 bits of output.
For one typical class of PRNGs, which includes linux /dev/urandom
and excludes yarrow, the first 1000 bits of output will use up
all the entropy, in the sense that the mapping from seeds to
output-strings will be very nearly one-to-one.  To say the
same thing another way, the adversaries are trying to find the
seed by brute-force search of seed-space, if they've guessed
the wrong seed they'll know it with very high probability after
seeing the first 1000 bits of output.  (They could also wait
and know the same thing from the second 1000 bits of output,
so we can have metaphysical arguments about just where the
entropy resides ... but since the adversaries get to see the
output stream in order, our minimax strategy is to assume
they are not stupid and have used the first 1000 bits to
reduce *their* entropy.  The number that I'm decrementing by
one bit per bit is my minimax estimate of the adversaries'
entropy.  If you have some other number that you would like
to decrement by some other rule, that's fine ... but I think
I'm doing the right thing with my number.)
Yarrow, in contrast, might well be configured to use only
100 bits of entropy for the first 50,000 bits of output,
and then reseed itself with another 100 bits for the
second 50,000 bits of output.  This leads to a notion of
average entropy density of the output, which is greater than
zero but less than 100%.
That is not minimax, as explained above.
See above.
 > If that practice does indeed ever protect someone from a
I'm not sure I would have said it that way, but I might agree
with the sentiment.  If the PRNG were secure against
cryptanalysis, including "practical cryptanalysis" such as
peeking at the state vector, then you wouldn't need more
than an infinitesimal entropy density.
Meanwhile, implementing a High Energy Symbol Generator would
relieve you of worrying about those things ... other things
that you ought to be worried about besides.

@_date: 2005-01-10 00:21:49
@_author: John Denker 
@_subject: Entropy and PRNGs 
Referring to  >>I just took a look at the first couple of pages.
 >>IMHO it has much room for improvement.
David Wagner responded:
Or perhaps it was my critique that was not clear enough.
I am not motivated to look for subtle shades of meaning in a
paper that claims the system clock is a source of entropy.
 >>First of all, you can't throw around the term "conditional
 >>entropy" without saying what it's conditioned on.
 >
 > Conditioned on everything known to the attacker, of course.
Well, of course indeed!  That notion of entropy -- the entropy
in the adversary's frame of reference -- is precisely the
notion that is appropriate to any adversarial situation, as I
have consistently and clearly stated in my writings;  see
e.g. the end of the "definitions" section, i.e.
   Heed your own "of course" statement.  It is hard to imagine a
situation where my adversary has more "context" about my
generator than I have myself.
I am unable to imagine myself being so silly.
No.  There is only one entropy that matters in an adversarial
situation.  The so-called "unconditional entropy" H(X) is
merely a wild overestimate of the only thing that matters.
"There is no glory in outstripping donkeys."  (Martial)
There is no honor in introducing H(X) since it is irrelevant
in any adversarial situation.
I imagine a smart person such as DAW should be able to come
up with five schemes in five minutes whereby UUID generation
can be delegated to virtually any machine that wants it.
MAC(eth0) /concat/ local counter will do for scheme Horsefeathers.  For generating  UUIDs,  _zero_ entropy is
sufficient, and no positive amount of entropy (unconditional
or otherwise) can be called necessary.
I am not interested in ways of obtaining wild overestimates
of zero.
If you want people to believe that unconditional entropy is a
worthwhile concept, you'll have to come up with a nontrivial
application for it.

@_date: 2005-01-10 12:44:32
@_author: John Denker 
@_subject: Entropy and PRNGs 
I still cannot see how that can happen to anyone unless
they're being willfully stupid.  It's like something out
of Mad Magazine: White Spy accepts a cigar from Black Spy,
lights it, and is surprised when it explodes.  That's
funny when it happens to somebody else, but as for me,
I'm not going to accept alleged "entropy" from any source
such that my adversary might know more about it than I do.
I'm just not.
I *almost* agree with that, but my real argument is somewhat
more nuanced:
a) Certainly there is a very wide class of PRNGs that
have no entropy whatsoever, including many that Mr. Laurie
seems willing to attribute entropy to.
b) It is also possible, as I have repeatedly explained,
for an ordinary PRNG to have a modest amount of entropy
residing in its internal state.  This entropy must
have abeen obtained from elsewhere, from something
other than a PRNG, not produced _de novo_ by any PRNG.
Categories (a) and (b) share the property of having
no nonzero lower bound on the entropy _density_ of
the output stream;  the entropy density is either
strictly zero (case a) or asymptotically zero (case b).
c) At the opposite extreme, there exist things that
produce 100% entropy density.  These must not be
called PRNGs.  I like the name HESG -- High Entropy
Symbol Generator.
   d) Also as I have repeatedly explained, there exist
intermediate cases, where something that works like
a PRNG is coupled to something else that provides
real entropy.  I recommend calling this a SRSG,
i.e. Stretched Random Symbol Generator, since it
isn't just a PRNG and it isn't just a HESG either.
   Linux /dev/urandom was an early and unsatisfactory
attempt at an SRSG.  Yarrow, coupled to a good HESG,
is vastly better, and that's what I implemented.

@_date: 2005-01-10 14:28:01
@_author: John Denker 
@_subject: Entropy and PRNGs 
I disagree.  For the sources of entropy that I consider
real entropy, such as thermal noise, for a modest payoff I'd
be willing to bet my life -- and also the lives of millions
of innocent people -- on the proposition that no adversary,
no matter how far in the future and no matter how resourceful,
will ever find in my data less entropy than I say there is.
(For instance, under suitable conditions I might say that
there's 159.98 bits of entropy in a 160-bit word.)
Of course I'd want to make approriate checks for implementation
errors, but in principle the entropy is there and the adversary
can't make it go away.  Some guy named Shannon had something to
say about this.
Can you please exhibit a nonzero lower bound on the entropy
content of the windows registry?  If not, please don't call
it entropy.
In particular, I ask you to consider the case of mass-produced
network appliances as mentioned at
   and consider whether, registry or no registry, the boxes will
be waaay too much alike.
In ordinary situations, the windows registry is constructed
by strictly deterministic processes.  No entropy.  If your
adversaries are so lame that they cannot figure out how the
registry is constructed, you're living in some kind of paradise.
Let's stay on-topic.
There is only one measure of entropy appropriate to a random
symbol generator.  If I am unable in principle to predict
the output of my HESG, then under mild assumptions that's
what I call entropy.
   By mild assumptions I mean things like assuming my
   machine has not been taken over by the attacker.  This
   assumption is of course common to *all* discussions
   of security algorithms and principles.  I mention it
   only to fend off nit-picks.  There's not much point in
   discussing algorithm "A" if you're going to turn around
   and tell me your box might be implementing some other
   algorithm.

@_date: 2005-01-11 11:17:49
@_author: John Denker 
@_subject: Entropy and PRNGs 
When did you figure that out?  If you'd been paying attention,
you'd know that I figured that out a long time ago.
First of all, the phrase "not random" is ambiguous.  I said
   Thermal noise, as it comes off the hardware, has an entropy
density greater than zero and less than 100%, as I said at
   and elsewhere.
Stop wasting our time.  All that doesn't change the fact that
thermal noise, as it comes off the hardware, has a nonzero
entropy density.  And it is easy to arrange situations where
I can calculate a very useful lower bound on the entropy density.
That's tantamount to saying the second law of thermodynamics will
be repealed.  By that standard, it's "entirely possible" that the
sun will rise in the west tomorrow morning.  But I wouldn't bet
on it.
Nuclear decay processes are not in any practical sense safer than
thermal noise.  As I discuss at
   nuclear is in the same category as thermal:  entropy density
greater than zero and less than 100%.

@_date: 2005-07-01 17:28:58
@_author: John Denker 
@_subject: /dev/random is probably not 
So don't do it that way.
Vastly better methods are available:
   ABSTRACT: We discuss the principles of a High-Entropy Symbol Generator
(also called a Random Number Generator) that is suitable for a wide
range of applications, including cryptography and other high-stakes
adversarial applications. It harvests entropy from physical processes,
and uses that entropy efficiently. The hash saturation principle is used
to distill the data, resulting in virtually 100% entropy density. This
is calculated, *not* statistically estimated, and is provably correct
under mild assumptions. In contrast to a Pseudo-Random Number Generator,
it has no internal state to worry about, and does not depend on
unprovable assumptions about ``one-way functions''. We also describe a
low-cost high-performance implementation, using the computer's audio I/O

@_date: 2005-07-04 11:22:39
@_author: John Denker 
@_subject: /dev/random is probably not 
> So the funny thing about, say, SHA-1, is if you give it less than 160
 > bits of data, you end up expanding into 160 bits of data, but if you
 > give it more than 160 bits of data, you end up contracting into 160 bits
 > of data.  This works of course for any input data, entropic or not.
 > Hash saturation? ....
I don't know what it means to talk about "data, entropic
or not".  That's because for the purpose of analyzing
randomness genreators, nobody cares whether the input has
a large amount of "data"; what matters is _entropy_.
If you feed the hash function less than 160 bits of
entropy, it will not "end up expanding" it to 160 bits
of entropy.  No function can expand the amount of entropy.
For a hash function with output width W=160 bits, if the
input has 5 bits of entropy the output will have very nearly
5 bits of entropy.  The input/output relationship is very
nearly linear, with unit slope, until we get close to
 > Hash saturation?  Is not every modern hash saturated with as much
 > entropy it can assume came from the input data   (i.e. all input bits  > a 50% likelihood of changing all output bits)?
That's not what "saturation" means.  I introduced and defined
the term "hash saturation", so I ought to know.  Saturation
is what happens when the input entropy is large, such that
the output entropy smashes up against the horizontal asymptote.
This is discussed in detail at
  along with a tabulated numerical example.
 > Incidentally, that's a more than mild assumptoin that it's pure noise
 > coming off the sound card.
I have no idea what is meant by "pure noise", but I'm
pretty sure I didn't assume any such thing.
 >  It's not, necessarily, not even at the high
 > frequencies.  Consider for a moment the Sound Blaster Live's E10K chip,
I recommend against using any Creative Labs (aka SoundBlaster)
products for any serious or even halfway-serious purpose.  Far
too many of their products have deceptive specifications, and/or
don't meet specifications at all.
 > internally hard-clocked to 48khz.  This chip uses a fairly simple
 > algorithm to upsample or downsample all audio streams to 48,000 samples
 > per second.  It's well known that scaling algorithms exhibit noticable
 > properties -- this fact has been used to detect photoshopped works, for
 > instance.  Take a look how noise centered around 15khz gets represented
 > in a 48khz averaged domain.  Would your system detect this fault?
I'm not sure exactly what fault is being described, and I
don't know what "simple algorithm" is alluded to.  However,
my guess is that the "simple algorithm" is linear, and that
the gain contour is a fairly smooth function of frequency,
with no strong singularities at 15kHz.  And since my method
calls for _measuring_ the gain as part of the calibration
process, this so-called "fault" should not AFAICT be classified
as a fault;  most likely it is just one of the many factors
that affect the calibration constants.
 > Of course not.
Proof by bold assertion.  Unsubstantiated opinion.
 > No extant system can yet detect the difference between a
 > quantum entropy generator and an AES or 3DES stream.
First of all, there is nothing special about "quantum" entropy
that makes it better than other types of entropy (e.g. thermal
entropy), in any practical sense.  This point is discussed in
detail at
   Secondly, it simply is not correct to say that their is no
difference between a genuinely entropic randomness generator
and a pseudo-randomness generator.  Proof by construction:
a) On a system that relies on /dev/urandom in the absence of
sources of real entropy, capture a backup tape containing
the /var/lib/.../random-seed file.  Then provoke a crash and
restart, perhaps by interrupting the power.  Then you know
the output of /dev/urandom for all time thereafter.
To repeat:  One difference between a genuinely entropic
randomness generator and a pseudo-randomness generator is
whether I have to be paranoid about crash/restart scenarios,
and whether I have to be paranoid about protecting my backup
b) A related point:  Suppose we are running a high-stakes
lottery.  Who provided the _original_ seed for the randomness
generator we will use?  Even assuming (!) we can protect the
seed for all times greater than t=0, where did the t=0
seed come from?  If you provided it, how do I know you didn't
keep a copy?
This is important, because the historical record shows that
randomness generators are not necessarily broken by attacking
their cryptologic primitives, by direct cryptanalysis of
the PRNG output;  more commonly they are broken by attacking
the seed-generation and seed-storage.

@_date: 2005-07-12 18:52:17
@_author: John Denker 
@_subject: ID "theft" -- so what? 
I am reminded of a passage from Buffy the Vampire Slayer.
In the episode "Lie to Me":
   BILLY FORDHAM:  I know who you are.
           SPIKE:  I know who I am, too.  So what?
My point here is that knowing who I am shouldn't be a
crime, nor should it contribute to enabling any crime.
Suppose you know who I am.  Suppose you know my date of
birth, social security number, and great-great-grandmother's
maiden name.  As Spike said, so what?
It's only a problem if somebody uses that _identifying_
information to spoof the _authorization_ for some
And that is precisely where the problem lies.  Any
system that lets _identification_ serve as _authorization_
is so incredibly broken that it is hard to even discuss
it.  I don't know whether to laugh or cry.
Identifying information cannot be kept secret.  There's
no point in trying to keep it secret.  Getting a new
SSN because the old one is no longer secret is like
bleeding with leeches to cure scurvy ... it's completely
the wrong approach.  The only thing that makes any sense
is to make sure that all relevant systems recognize the
difference between identification and authorization.
Repeat after me:  identification is not authorization.
Identification is not authorization.
When people talk about authentication factors such as
   a) something I know
   b) something I have
   c) something I know
it is crucial to keep in mind that item (a) must be something
I know _that other people don't know_.  Identifying information
doesn't qualify, and cannot possibly qualify.  My SSN is not
a password.  It lacks many of the properties that a password
should have.
Credit-card numbers, in practice, do little more than
identify me and my account.  They are not handled the way
passwords should be handled.
Eliminating ludicrously broken authentication schemes is
something we should work on.  Password theft is something
we should try to prevent.  But when it comes to ID "theft",
we should say: So what?
I've been saying this for years, but it seems timely to say
it again.

@_date: 2005-07-13 16:08:20
@_author: John Denker 
@_subject: ID "theft" -- so what? 
Yes, there are annoying terminology issues here.
In the _Handbook of Applied Cryptography_ (_HAC_)
  -- on page 386 they say "The terms _identification_
   and _entity authentication_ are used synonymously
   throughout this book"  (which in fact they are :-)
  -- on page 24 they say the term _authentication_ is
   often abused.
It seems to me that the term _identification_ is even more
ambiguous and open to misunderstanding than "authentication"
is.  Overall, it's a quagmire.
In some circles, the notion of _identifying information_
is quite a weak notion, meaning sufficient information
to pick the desired entity out of a crowd.  More-or-less
synonymous notions include
  *) characterization
  *) description (sufficiently detailed to be unique)
  *) unverified _claim_ of identity
  *) pointer, indicator, index
  *) name, call-sign, handle
  *) record-locator, database-key
     -- used as an index into the database,
     -- *not* a key in the sense of lock-and-key
     -- example: LOGNAME i.e. the first field in each
        record in the /etc/passwd database
In other circles, _identification_ refers to a much stronger
notion.  When the military speaks of IFF (identification,
friend or foe) they don't consider it sufficient for you
to be able to _describe_ a friend, they actually want you
to _be_ a friend.
As to whether the word _identity_ should refer to full-blown
entity authentication, or to a mere characterization or
call-sign ... that seems like an unanswerable question.
==> My recommendation:  Avoid using terms like "identity"
     and "identification" entirely.
     -- If you mean "entity authentication", use that term
      in preference to "indentification".
     -- If you mean a mere description, handle, record-locator,
      etc. use those terms.
It would be nice to have a convenient catch-all term for
the whole category of notions like description, handle,
record-locator, et cetera.  I don't recommend calling them
"weak" identification, because the term "weak authentication"
has already been taken, and means something else, namely
passwords and the like (_HAC_ page 388).
The only time that you can even dream of using a detailed
description as a means of entity authentication is when
meeting face to face.  Somebody who fits my description
in full detail "probably" is me, although even that isn't
entirely certain.
On the other side of that coin, in a typical e-commerce
situation, allowing a description or a call-sign to serve
in place of entity authentication is ludicrous.  It means
that anybody who can describe me can impersonate me.  The
vulerability to replay attacks and MITM attacks is unlimited.
Typically a full-blown entity authentication starts with one
party making a _claim_ of identity which the other party
then _verifies_.   Unix login is a familiar example:  first
I give my LOGNAME and then I give the corresponding password.
The notion of "theft" of my LOGNAME is vacuuous.  My LOGNAME
(jsd) is known to everybody, good guys and bad guys alike.
As Spike said, so what?  My LOGNAME is nothing more than a
handle, a call-sign, a record-locator, used to look up the
appropriate record in places like /etc/passwd.
If you want to impersonate me, my computer requires you to
know not just my LOGNAME but also my password.  The way we
should treat passwords is verrry different from the way we
should treat call-signs.
Using this refined terminology, I can clarify what I was
trying to say yesterday:
  1) Being able to _describe_ me (height, weight, date of birth,
SSN, LOGNAME, and great-great-grandmother's maiden name) does
not mean you _are_ me.
  2) Even fully identifying and authenticating me as me doesn't
suffice to prove that wish to authorize this-or-that financial
transaction.  Who I *am* and what I wish to *happen* are
dramatically different notions.  Authenticating me as an entity
is not a proper substitute for authenticating the transaction
itself.  These notions are not unrelated, but they are not
identical, either.
In present-day practice, SSNs, credit card numbers, and
various bits of personal description are suspended in some
weird limbo: they are not nearly as secret as passwords
should be, yet they are still treated by some parties as
if they had magical entity-authenticating power and even
transaction-authenticating power.
So where do we go from here?  In general:
  -- When we think and when we speak, always distinguish
   handle versus entity authentication versus transaction
   authentication.
  -- Don't entrust our money to institutions that can't
   reliably make that distinction.
  -- Obtain legislation so that Muggles are protected, not
   just us.
Also:  A critical step that phishers must take in order to
exploit phished information is to check its validity.  Therefore
banks *must* be required to perform entity-authentication on
everyone who validates charging info.  This simple step would
AFAICT greatly suppress phishing, since phishers would become
super-vulnerable to sting operations.
The foregoing seems pretty much obvious and non-controversial.
Now let me start a new sheet of paper and discuss a half-baked
idea.  Refinements would be welcome.
Scenario:  I'm shopping online.  Using browser window  I
have found a merchant who sells what I want.   in the checkout
Now, in browser window  I open a secure connection to my
bank.  The bank and I authenticate each other.  (This is
two-way authentication;  one possible method is SSL certificate
on the bank's part, and a password or some such on my part.)
In the simplest case, I simply ask the bank to generate a
single-use "credit card" number.  I cut-and-paste this number
from the bank (window  to the merchant (window   The
merchant has an incentive to use this number properly, and
not mishandle it ... and as soon as they do use it, it
becomes worthless to anybody else.
As a refinement, I could ask the bank to issue a number that
was not only restricted to a single use, but also restricted
as to dollar amount.  As a further refinement, it could be
restricted to a particular merchant.
Everything to this point is upward-compatible with existing
systems.  The merchant doesn't even need to know there's
anything special about this card number;  the charge moves
through existing processing channels just like any other.
As a further refinement, once the system is established,
the merchant could provide, on the checkout page, a number
that encodes in some standard format various details of
the transaction:  merchant ID, date, dollar amount, and
maybe even a description of the goods sold.  I cut-and-paste
this code from the merchant to the bank, and let the bank
site decode it.  If it looks correct, I click OK and then
the bank generates a response that the merchant will
accept in payment.  If necessary I can cut-and-paste this
from the bank to the merchant ... but it would make more
sense for the bank to transmit it directly to the merchant.
This further increases security, and also saves me from
having to enter a lot of billing detail.
This doesn't solve all the world's problems.  As SMB pointed
out, if your PC has been trojanized you're in big trouble,
and this certainly won't help with that.
Browser  could be your cell phone.  To the extent that the
phone is "something you have", this further enhances security.
(Although I expect to see phone trojans become common pretty
This requires having the bank be online at all times when I
might be e-shopping, but the internet has become sufficiently
reliable that this shouldn't be much of a problem (unless you're
in Pakistan :-).

@_date: 2005-06-27 10:19:13
@_author: John Denker 
@_subject: WYTM - "but what if it was true?" 
Even more compelling is:
  -- obtain laptop hardware from a trusted source
  -- obtain software from a trusted source
  -- throw the entire laptop into a GSA-approved safe when
   not being used.
This is a widely-used procedure for dealing with classified

@_date: 2005-10-24 12:07:55
@_author: John Denker 
@_subject: EDP (entropy distribution protocol), userland PRNG design 
I've been following this thread for a couple of weeks now, and so
far virtually none of it makes any sense to me.
What evidence is there that HRNGs are expensive?  How many machines do
you have?  How many of them already have soundcards?  How much entropy
do they need (bits per second)?
The obvious solution is to put a high-performance low-cost HRNG in each machine.
   Is there some reason why this cannot be done?  If so, please explain.
Otherwise, this whole discussion seems like a futile exercise, i.e. trying
to find the optimal way of doing the wrong thing.
]] ABSTRACT: We discuss the principles of a High-Entropy Randomness Generator (also called a True
]] Random Number Generator) that is suitable for a wide range of applications, including
]] cryptography, high-stakes gaming, and other highly adversarial applications. It harvests entropy
]] from physical processes, and uses that entropy efficiently. The hash saturation principle is used
]] to distill the data, resulting in virtually 100% entropy density. This is calculated, not
]] statistically estimated, and is provably correct under mild assumptions. In contrast to a
]] Pseudo-Random Number Generator, it has no internal state to worry about, and does not depend on
]] unprovable assumptions about ?one-way functions?. We also describe a low-cost high-performance
]] implementation, using the computer?s audio I/O system.
For details, see
   OK so far.
If it were a decent HRNG it would have this built in.  XOR is not even
remotely sufficient.
What's an extractor?  What is needed is a compressor.
A decent HRNG is stateless and does not need any one-way functions.
A decent HRNG does not need any such encipherment.
Such assurances are discussed at:
   turbid is already written in C++ for this reason.  Strings and suchlike
are part of the language, defined in the Standard Template Library.
The semantics is just broken ... which is why turbid defines and
implements /dev/hrandom with its own semantics.  Optionally it can
feed entropy to /dev/[u]random for the benefit of legacy applications
under certain limited conditions.
Really not a problem with turbid.
Really not a problem with turbid.
Really not a problem with turbid.
Really not a problem with turbid.

@_date: 2005-10-26 15:14:27
@_author: John Denker 
@_subject: packet traffic analysis 
OK so far ...
This is a poor statement of the problem(s), followed by a "solution" that
is neither necessary nor sufficient.
1) Let's assume we are encrypting the messages.  If not, the adversary
can read the messages without bothering with traffic analysis, so the
whole discussion of traffic analysis is moot.
2) Let's assume enough randomness is available to permit encryption
of the traffic ... in particular, enough randomness is available
_steady-state_ (without stockpiling) to meet even the _peak_ demand.
This is readily achievable with available technology.
3) As a consequence of (1) and (2), we can perfectly well use _nonrandom_
chaff.  If the encryption (item 1) is working, the adversary cannot tell
constants from anything else.  If we use chaff so that the steady-state
traffic is indistinguishable from the peak traffic, then (item 2) we
have enough randomness available;  TA-thwarting doesn't require anything
4) Let's consider -- temporarily -- the scenario where the encryption is
being done using IPsec.  This will serve to establish terminology and
expose some problems heretofore not mentioned.
4a) IPsec tunnel mode has "inner headers" that are more than sufficient
to distinguish chaff from other traffic.  (Addressing the chaff to UDP
port 9 will do nicely.)
4b) What is not so good is that IPsec is notorious for "leaking" information
about packet-length.  Trying to make chaff with a distribution of packet
sizes indistinguishable from your regular traffic is rarely feasible, so
we must consider other scenarios, somewhat like IPsec but with improved
5) Recall that IPsec tunnel mode can be approximately described as IPIP
encapsulation carried by IPsec transport mode.  If we abstract away the
details, we are left with a packet (called an "envelope") that looks like
         ---------------++++++++++++++++++++++++++
 outer header | inner header | payload |              [1]
         ---------------++++++++++++++++++++++++++
where the inner header and payload (together called the "contents" of
the envelope) are encrypted.  (The "+" signs are meant to be opaque
to prying eyes.) The same picture can be used to describe not just
IPsec tunnel mode (i.e. IPIP over IPsec transport) but also GRE over
IPsec transport, and even PPPoE over IPsec transport.
     Note:  All the following statements apply *after* any necessary
     fragmentation has taken place.
The problem is that the size of the envelope (as described by the length
field in the outer header) is conventionally chosen to be /just/ big
enough to hold the contents.  This problem is quite fixable ... we just
need constant-sized envelopes!  The resulting picture is:
         ---------------++++++++++++++++++++++++++++++++++++
 outer header | inner header | payload | padding |    [2]
         ---------------++++++++++++++++++++++++++++++++++++
where padding is conceptually different from chaff:  chaff means packets
inserted where there would have been no packet, while padding adjusts the
length of a packet that would have been sent anyway.
The padding is not considered part of the contents.  The decoding is
unambiguous, because the size of the contents is specified by the length
field in the inner header, which is unaffected by the padding.
This is a really, really tiny hack on top of existing protocols.
If your plaintext consists primarily of small packets, you should set the MTU
of the transporter to be small.   This will cause fragmentation of the
large packets, which is the price you have to pay.  Conversely, if your
plaintext consists primarily of large packets, you should make the MTU large.
This means that a lot of bandwidth will be wasted on padding if/when there
are small packets (e.g. keystrokes, TCP acks, and voice cells) but that's
the price you have to pay to thwart traffic analysis.  (Sometimes you can
have two virtual circuits, one for big packets and one for small packets.
This degrades the max performance in both cases, but raises the minimum
performance in both cases.)
   Remark: FWIW, the MTU (max transmission unit) should just be called
   the TU in this case, because all transmissions have the same size now!

@_date: 2005-10-31 09:54:48
@_author: John Denker 
@_subject: packet traffic analysis 
In the context of:
 >>If your plaintext consists primarily of small packets, you should set the MTU
 >>of the transporter to be small.   This will cause fragmentation of the
 >>large packets, which is the price you have to pay.  Conversely, if your
 >>plaintext consists primarily of large packets, you should make the MTU large.
 >>This means that a lot of bandwidth will be wasted on padding if/when there
 >>are small packets (e.g. keystrokes, TCP acks, and voice cells) but that's
 >>the price you have to pay to thwart traffic analysis.
I very much doubt it.  Where did that factor of "half" come frome.
Ah, but if you generate unequal-length packets then they are
vulnerable to length-analysis, which is a form of traffic analysis.
I've seen analysis systems that do exactly this.  So the question is,
are you trying to thwart traffic analysis, or not?
*is* pointless, as previously discussed.
A better solution would be to leave the encryption on and use constants
(not PRNG output) for the chaff, as previously discussed.
The notion of synchronized PRNGs is IMHO crazy -- complicated as well as
utterly unnecessary.

@_date: 2005-09-17 13:33:12
@_author: John Denker 
@_subject: Clearing sensitive in-memory data in perl 
One could argue it has already been done.
There exists a widely available, freely available, well-implemented
example of something just like the STL for C++.  It is called
"the STL for C++".
a) Writing in C++ is easier than writing in C.
b) Porting legacy C to C++ isn't rocket surgery.  It can be done

@_date: 2005-09-27 16:05:42
@_author: John Denker 
@_subject: continuity of identity 
Jerrold Leichter mentioned that:
I agree there is considerable merit to a "continuity of identity"
But there are ways the idea can be improved.  So let's discuss it.
For starters, let me suggest that rather than having a self-signed
certificate of the type created more-or-less automatically when
you set up your Apache server or set up your SSH daemon, it makes
more sense to set up your own CA and issue your own certs from
there.  In some sense this is just a different type of self-signing,
but it adds a possibly useful layer of indirection.
One advantage is that one CA can issue lots of certs, so if you
are (say) a hosting service provider running N hosts, you only
need to have one CA.  The idea is that your users can have
a notion of continuity of your CA.  This notion is more powerful
than having N separate lines of continuity, one for each of your
hosts.  That's because each line of continuity is vulerable at
its beginning.  Having one CA reduces the vulnerabilty by a factor
of N, or at least a factor of M, where M tells us how many of
your hosts are likely to be visited by a typical visitor over
the course of time.
Another advantage is that you can keep your CA on a separate
machine, with a security policy radically stricter than the
policy on the N hosts.  That means if one of your hosts is
compromised, they get only the host key, not the CA private
key.  New host keys can be generates as soon as the compromise
is remedied, and assuming the old host keys have a reasonably
short expiration time, then your total vulnerability is
bounded, much more tightly bounded than in any commonly-used
scheme.  (I don't know of anybody who renews his Verisign-issued
certs every day, or even every week.)
In a similar vein, rapid expiration of host certs limits how
careful you have to be with backup takes, et cetera.
Another advantage is that it might shrink your known_hosts
file ... reducing it by a factor of M or thereabouts, since it
now becomes, in effect, a known_ca file.
This scheme is in some sense doable already, since modern browsers
have a way of importing new CA keys under user control.  If you
can persuade a user to go through those steps you're all set.
However, this process is sufficiently arcane and cumbersome that
it must be considered
  -- in the short term, a serious drawback to my continuity-of-CA
   scheme, or
  -- in the medium term, a tremendous opportunity for improvement.
In particular, it would be nice if both SSH and the web protocols
had a way of piggybacking a CA public key along with each cert
signed by that CA, so that SSH clients and web browsers could
give the users a relatively Muggle-friendly way of deciding
  *) to trust the CA henceforth
  *) to trust only this particular host henceforth
  *) neither.
By default, such a CA should be limited to signing only one
domain and its subdomains, such as *.jdenker.com (as opposed
to signing *.com).
Setting up a CA is no big deal.  There is a HOWTO by Ng Pheng Siong:
   I realize that such a scheme is open to abuse ... but on the whole,
I expect it would solve more problems than it creates.
Comments, anyone?

@_date: 2006-04-05 01:02:24
@_author: John Denker 
@_subject: Unforgeable Blinded Credentials 
Hal Finney wrote in part:
The phrase "there are no sensitive secrets today" sounds very strange
by itself, and doesn't sound much better in context.
I assume the intended meaning was more along the lines of:
==   The set of things you want to keep secret has zero overlap with
==   the set of things you might want to use as an identifier.
Let me just remark that there's nothing new about this.  The notion of
a secret identifier is very widespread, but if you think about it, it
is completely absurd, and always has been.  For a fuller discussion, see:
   which begins as follows:
]] I am reminded of a passage from /Buffy the Vampire Slayer/, in the episode "Lie to Me":
]]      BILLY FORDHAM:  I know who you are!
]]      SPIKE:          I know who I am, too.  So what?
]] My point here is that it shouldn?t matter if somebody knows who I am. Suppose somebody can
]] describe me -- so what? Suppose somebody knows my date of birth, social security number, and
]] great-great-grandmother?s maiden name -- so what?
]] It?s only a problem if somebody uses that identifying information to spoof the _authorization_
]] for some transaction.

@_date: 2006-04-26 15:20:38
@_author: John Denker 
@_subject: History and definition of the term 'principal'? 
Its use in security does not AFAICT differ from its use in
other contexts, notably law and finance.
I don't see why it is necessary to look beyond an ordinary dictionary.
See definition 5 here:
   That is an application or a corollary of the dictionary definition.
Ditto for the other examples mentioned.

@_date: 2006-08-30 13:07:40
@_author: John Denker 
@_subject: Impossible compression still not possible. [was RE: Debunking 
security bug in PGP products?)]]
Dave Korn asked:
Hint:  L'H?pital's rule.
That's not a polynomial.
x^Q is a polynomial.  Q^x is not.

@_date: 2006-12-22 16:23:19
@_author: John Denker 
@_subject: gang uses crypto to hide identity theft databases 
Because that would be a Bad Idea.  In a halfway-well-designed
system, cutting the power would just do the secret-keepers' job
for them.
That's why you don't do it that way.  If you want it to work, you
use an encrypting disk system so that everything on disk (including
swap) is encrypted all the time, and gets decrypted "as needed" when
it is read.
It should be in volatile unswappable RAM.  Cutting the power is one
way (among many) to obliterate it.  Overwriting it with randomness
suffices if there is any chance that the RAM might be non-volatile.
The time and cost of obliterating a key are negligible.
That's another reason why you don't do it that way.
Once something is gone from RAM, it's really, really gone.  The circuit
structure and the laws of thermodynamics ensure it.  No power on earth
can do anything about that.
There are, however, some things the cats can do to improve their chance of
success in this cat-and-mouse game.
  *) For starters, the cats must anticipate the possibility that the
   mice might try to secure their data.  The early-adopter mice benefit
   from a certain amount of security-through-obscurity, insofar as the
   cats have not heretofore fully appreciated the possibilities.
 *) The mice have a dilemma:  If they do not cache the passphrase somewhere,
  they will need to constantly re-enter it, which makes them vulnerable to
  shoulder-surfing, sophisticated key-loggers, unsophisticated rubber-hose
  methods, et cetera.  Conversely, if the mice do cache the passphrase for
  long periods of time, there is the possibility that the cats will capture
  the whole system intact, passphrase and all, and will be able to make a
  permanent copy of the passphrase before the system realizes that a compromise
  has occurred.  The cats can improve their chances by causing not-too-suspicious
  power failures and seeing how the mice handle the ensuing passphrase issues.
  The mice can improve their odds by ensuring good physical security, ensuring
  personnel reliability, providing easy-to-use panic buttons, rotating their
  passphrases, and so forth.

@_date: 2006-02-12 13:46:05
@_author: John Denker 
@_subject: GnuTLS (libgrypt really) and Postfix 
>>     Yet, I agree it is poor design to [exit] in a library.
I agree, it is a poor design.
Werner Koch retorted:
That is a remarkably unprofessional suggestion.  I hope the people
who write software for autopilots, pacemakers, antilock brakes,
etc. do not follow this suggestion.
First of all, "impossible" is the wrong word.  If the condition
were truly impossible, it would absolutely never happen, and
there would be no need even to check for it.  Instead, it would
be better to speak of conditions that "should" never happen, or
more precisely, conditions that are outside the range of what
the code can handle.
As an introductory example, let "you" be the computer.  Imagine you
are minding your own business, about 2500 feet above the surface
of the moon.  You detect a condition that "should" never happen;
namely, there is not enough time to execute the required tasks
in real time, i.e. the next clock tick occurs before all the
previous tick's tasks are finished.  What do you do, exit?  If
you exit, you kill everybody on board.  That seems suboptimal
IMHO.  Maybe it would be wiser to issue an alarm ("1201") and
keep going, leaving low-priority tasks unfinished, while still
dealing with high-priority tasks on schedule.
   Long, long afterward it is discovered that somebody inadvertently
   left the docking radar turned on during the descent.  That
   "should" have never happened.  But it did happen.  Deal with it.
There are other stories like this, including funny (?) stories of
what happens if exceptions are not handled so well.
More generally:  library routines should never exit.  They almost
by definition do not have a high-level view of what is going on,
whereas the decision to exit is quite a high-level decision.  It
is permissible for library routines to throw an exception;  then
the higher-level code can catch the exception if it wishes to.
If the language doesn't handle exceptions well, it is permissible
to return an exit status ("1201") that gets passed up through the
layers of callers.
   Nitpickers note:  I can imagine a situation where the stack is so
   messed up that you can't thrown an exception or even return from
   calls.  (Actually I don't need to imagine it;  I've dealt with
   such situations.)  However,
    a) That is not even remotely similar to the situation that led
     to the remarks quoted above, and
    b) Even then there are high-level things that you can do to
     recover from the situation, and there is no excuse for
     arrogant unprofessional low-level routines usurping high-level
     functions by halting or exiting.
Tangential philosophical note:  I have a saying that
   "All exceptions are caught and handled;  it's just a
   question of where and how."
Extreme examples:
  -- Under unix, exit() is caught by the atexit() routines.
  -- Similarly, _exit() is caught by the parent process.
  -- A hardware halt is caught by the operator and handled
   by pushing the reset button.
In many situations, these extreme handlers are far too drastic and
inflexible, so you should arrange for less-extreme handlers.

@_date: 2006-02-13 03:07:26
@_author: John Denker 
@_subject: GnuTLS (libgrypt really) and Postfix 
One could make an even stronger statement about the dangers of
making assumptions that are not provably correct.
That is a false dichotomy.
Again:  False dichotomy is a fallacy, and has been recognized for
 >2000 years as such.  Showing that one extreme is bad does not
prove that the opposite extreme is good.
The whole point of my previous note was to argue for more nuanced
handling.  Progressing from one knee-jerk handing to a choice
between two knee-jerk handlings is not much progress.
Again, that's the wrong question;  it's not an either/or proposition.
We can agree that letting the attacker take control of the situation
is a Bad Thing, but it is preposterous to think that exiting is
provably correct in all situations where the library might be put
to use.
It is just plain arrogant for low-level code to arrogate to itself
a decision that rightfully belongs to higher-level code.
Werner Koch wrote in part:
That is narrowly true as stated, but it does not prove that exiting
is the correct thing to do.
That might lead to an argument in favor of exceptions instead of error
codes, along the following lines:
  -- Naive code doesn't catch the exception.  However (unlike returned
   error codes) this does not cause the exception to be lost.
  -- The exception percolates up the call-tree until it is caught by
   some non-naive code (if any).
  -- If all the code is naive, then the uncaught exception terminates
   the process ... to the delight of the "exit on error" faction.
   However (!!!) unlike a plain old exit, throwing an exception leaves
   the door open for non-naive code to implement a nuanced response to
   the exceptional condition.
Again, enough false dichotomies already!  Just because error codes are
open to abuse doesn't mean exiting is the correct thing to do.

@_date: 2006-02-14 15:53:39
@_author: John Denker 
@_subject: GnuTLS (libgrypt really) and Postfix 
Yes, I reckon there is a pretty wide consensus that exceptions
provide a satisfactory solution to the sort of problems being
discussed in this thread.
OK ... although I wouldn't make a virtue of doing things ungracefully.
That raises the question of whether mission-critical applications
should be written in C.
It is straightforward but laborious to simulate exception-throwing
in C:
     extern int errno;
     /* try some stuff */
     if (errno) return;         /* return immediately on any error */
     /* try some more stuff */
     if (errno) return;         /* return immediately on any error */
     et cetera.........
This is laborious and inelegant, but no more so than the other gajillion
things you need to do to provide any semblance of security in C, such as
computing the (N) argument to strncat(,,N).
In particular, if-errno-return is often much preferable to some other
tricks that have recently been suggested, such as raising SIGABRT or
passing a pointer to a function to be called when exceptional conditions
are detected.  The reason is that throwing an exception _pops_ the
stack until a catcher if found, whereas signals and function-calls just
push deeper into the stack ... they allow you to regain some control,
but they don't make it easy to regain control _at the right place_.
For completeness, let me say again that _exit() is just an exception
that is caught by the parent process.  If you are ever forced to deal
with a package that exits when it shouldn't, it is straightforward to
create a wrapper that does a fork, calls the package, and collects
the exit status.  That is, rather than:
         rslt = nasty(a, b);           /* might trash the whole process */
we have:
         rslt = forku(nasty, a, b);    /* better */
         if (errno) return;
But of course I hope you never have to face such a problem.  If at
all possible, use a language that supports exceptions.
No, that is not the "best" way nor even a good way, let alone a standard.
Halting on every exceptional condition is like amputating to cure
every headache.
Keep in mind Dykstra's dictum:  testing can perhaps show the presence
of bugs, but testing can never show the absence of bugs.
Exit-on-error is a _problem amplifier_.   It guarantees that small
problems detected during testing will not go unnoticed.  But this is
a very long way from being a guarantee of error-free code.
   a) Remember Dykstra's dictum.
   b) If you knew the code was error free, then by the Red Queen's logic
    you wouldn't even need to check for errors, and there would be no
    need for exit statements.  The contrapositive is that if you put
    checks in the code, you are implicitly admitting that your code
    might not be error free.  And there's the rub:  you cannot possibly
    prove that during deployment (as opposed to during testing) using
    a _problem amplifier_ won't make things worse rather than better.
Whatever happened to doing what's best for the customer?  Doing what's
most convenient for the programmer during testing, while making things
worse for the customer during deployment ... that seems remarkably
Last but not least, I object (again!) to the false dichotomy, i.e.
the allegation that exceptional conditions must either
   a) result in an abort, or
   b) go undetected.

@_date: 2006-01-13 10:17:55
@_author: John Denker 
@_subject: quantum chip built 
My understanding is that quantum computers cannot "easily" do anything.
As the saying goes:
     "We can factor the number 15 with quantum computers. We can also
      factor the number 15 with a dog trained to bark three times."
                 --- Robert Harley, 5/12/01, Sci.crypt.
Scaling up a quantum computer to handle numbers much larger than 15 will
not be done "easily".
I can't say for sure.  There /might/ be a radical breakthrough in
quantum cryptanalysis tomorrow.  But I doubt it.  There is a comparably
small likelihood of a breakthrough in _classical_ (i.e. non-quantum)
cryptanalysis tomorrow.
To put this in context:  In the world there are incomparably more RSA
keys that are vulnerable to classical cryptanalytic attack than are
vulnerable to quantum attack.  As a specific example, a 30-digit RSA
key could be easily brute-forced by classical methods, but will not
be vulnerable to quantum-computer chips for many years.  (Of course
I exclude the case where you attach a quantum-computer chip to the
front of your PC using crazy glue and market the combination as a
quantum computer.)
To put cryptanalysis in context:  A person skilled in the art should
be able to create RSA keys and/or ECC keys with a 10-year lifetime
such that the risk of mathematical cryptanalysis is negligible compared
to the risk of "practical" cryptanalysis, e.g. bribery, rubber-hose
techniques, etc. applied to authorized keyholders.  I'm not saying
the risk is zero, just negligible compared to other risks.

@_date: 2006-01-27 14:31:39
@_author: John Denker 
@_subject: thoughts on one time pads 
Sure, you can.  It isn't soooo much different from rewriting any
other type of disk.
There are various versions of getting rid of a disk file.
  1) Deletion:  Throwing away the pointer and putting the blocks back
   on the free list.  This is well known to be grossly insecure.
  2) Zeroizing the blocks in place (followed by deletion).  This
   is vastly better, but still not entirely secure, because there
   are typically stray remnants of the pattern sitting "beside"
   the nominal track, and a sufficiently-determined adversary
   may be able to recover them.
  3) Trashing the blocks, i.e. overwriting them in place with
   crypto-grade random numbers (followed by optional zeroizing,
   followed by deletion).  This makes it harder for anyone to
   recover strays.
  4) Half-track trashing.  This requires wizardly disk hardware,
   which shifts the head half a track either side of nominal,
   and *then* writes random numbers.  I might be persuaded that
   this really gets rid of strays.
  5) Grinding the disk to dust.  AFAIK this is the only NSA-approved
   method.  A suitable grinder costs about $1400.00.
       One drawback with this is that you have to destroy a whole
   disk at a time.  That's a problem, because if you have a
   whole disk full of daily keys, you want to destroy each
   day's key as soon as you are through using it.  There
   are ways around this, such as reading the disk into volatile
   RAM and then grinding the disk ... then you just have to make
   sure the RAM is neither more volatile nor less volatile than
   you wanted it to be.  That is, you use the disk for *distribution*
   but not necessarily for intermediate-term storage.

@_date: 2006-01-28 13:39:14
@_author: John Denker 
@_subject: thoughts on one time pads 
That's quite an amusing turn of phrase.  There are two ways to
interpret it:
*) If taken literally, the idea of destroying a key _before_ it is
  used is truly an ingenious way to ensure security.  Alas there is
  some degradation of functionality, but isn't that always the case?
  Also the cost of key distribution goes way down once you decide you
  will only distribute already-destroyed keys.
*) Perhaps the intent was to speak about _protecting_ keys before and
  after use.  That's somewhat trickier to do securely, and is more
  dependent on the threat model ... but offers vastly greater functionality.
  -- The best way to _protect_ a key after it has been used is to destroy
   it.
  -- For keys that have yet been used, a sufficient scheme (not the only
   scheme) for many purposes is to package the keys in a way that is
   tamper-resistant and verrry tamper-evident.
   The package must be tamper-evident in order to be secure. If there are
   signs of tampering, don't use the keys.
   The package must be at least somewhat tamper-resistant in order to
   protect the functionality against a too-easy DoS attack, i.e.
   superficial tampering.
That indicates a gross lack of tamper-evident packaging, as discussed
above.  The store should never have activated a card that came from a
package that had been tampered with.
That's even funnier.  Most CDs and DVDs are totally non-magnetic to begin
with.  Degaussing them is not going to have much effect.
There are, of course, NSA-approved degaussers for magnetic media, but
heretofore this thread hasn't been about magnetic media.

@_date: 2006-01-28 19:37:23
@_author: John Denker 
@_subject: thoughts on one time pads 
I forgot to mention in my previous message:
It is worth your time to read _Between Silk and Cyanide_.
That contains an example of somebody who thought really
hard about what his threat was, and came up with a system
to deal with the threat ... a system that ran counter to
the previous conventional wisdom.  It involved protecting
keys before use and destroying them after use.

@_date: 2006-07-04 12:59:35
@_author: John Denker 
@_subject: Quantum RNG 
This is discussed at
  Quantum processes are in some very narrow theoretical sense more
"fundamentally" random than other sources of randomness, such as
thermal noise ... but they are not better in any practical sense.
The basic quantum process is less sensitive to temperature than a purely
thermal process ... but temperature dependence is easily accounted for
in any practical situation, and -- more importantly -- there are all
sorts of other practical considerations (such as detector dead-time
issues) that make real quantum detectors far from ideal.
The devil is in the details, and obtaining the raw data from a quantum
process is nowhere near necessary and nowhere near sufficient to make
a good randomness generator.
I have no idea whether the quantis generator got the devilish details right
... but in any case, there are easier ways to make a generator that is just
as good, or better.
For details, see

@_date: 2006-07-06 16:19:37
@_author: John Denker 
@_subject: Quantum RNG 
That's true as stated, and correctly reinforces the point that
lots of things are more convenient than the quantum mechanics
of photons.
However, it should not be taken so far as to become an endorsement
(in absolute terms) of shot noise as a convenient basis for a
practical HRNG.  A key element in the construction of a decent
HRNG (by my standards, at least) is to have a provable lower bound
on the amount of randomness in the raw data.  We agree that there
are many situations that have plenty of shot noise, but it is
relatively hard to get a provable lower bound on how much shot
noise there MUST be in any given situation.
 *) This applies to individual transistors and other devices;
  minimum shot noise is not one of the guaranteed specifications
  you see on the spec sheet.
 *) This applies even more strongly to larger systems with lots
  of components, such as a sound card treated as a black box.
In contrast, I can obtain a reliable lower bound for the thermal
noise in a sound card, based on black-box properties such as
impedance, bandwidth, and ambient temperature.
For details, see
  In summary, as things stand today, over a wide range of conditions
and requirements, the recently-mentioned sources can be ranked in
terms of practicality, as follows:
  photons << electronic shot noise << thermal noise
If somebody has a way of overcoming the limitations so as to change
the ranking, please tell us about it.
As I said in my previous note:  It's true that quantum processes are
in some very narrow theoretical sense "more fundamental" than other
processes, but this is nowhere near sufficient and nowhere near
necessary for building a decent HRNG.
As I should have said:  When vendors like idquantique emphasize the
quantum nature of their raw data source, it rubs me the wrong way.
  It indicates that either:
 -- they are clueless as to what's important and what's not, or
 -- they are operating on the assumption that their customers are
  clueless.
Either way, it doesn't make me want to be one of their customers.

@_date: 2006-03-22 21:11:02
@_author: John Denker 
@_subject: passphrases with more than 160 bits of entropy 
Yes, the term "entropy" is often misused ... and we have seen some
remarkably wacky misusage in this thread already.  However, physicists
do not have a monopoly on correct usage.  Claude S was not a physicist,
yet he definitely knew what he was talking about.  Conversely, I know
more than a few card-carrying physicists who have no real feel for what
entropy is.
I agree with all that, except for the "But".  Shannon well knew that
the entropy was zero in such a situation.
Yes.  Shannon called it the "source entropy", i.e. the entropy of
the source, i.e. the entropy of the generator.
Huh?  What's your metric for "usually"?  I'll agree as a matter of
principle that whatever you're doing, you can always do it wrong.
But that doesn't prevent me from doing it right.  I can use physics
to produce good bounds, that is,
   The problem posed by the OP is trivial, and good solutions have already
been posted.  To recap: SHA-512 exists, and if that isn't good enough,
you can concatenate the output of several different one-way functions.
You can create new hash functions at the drop of a hat by prepending
something (a counter suffices) to the input to the hash.
Example:  result = hash(1 || pw)  ||  hash(2 || pw)  ||  hash(3 || pw)

@_date: 2006-03-22 22:09:44
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
Entropy is defined in terms of probability.  It is a measure of
how much you don't know about the situation.  If by "message"
you mean a particular message that you are looking at, it has
zero entropy.  It is what it is;  no probability required.
If by "stream" you mean a somewhat abstract notion of an ensemble
of messages or symbols that you can only imperfectly predict, then
it has some nonzero entropy.
That is a tricky question for a couple of reasons.
  a) It will never be completely answerable because the question hinges
   on the word "random", which means different things to different people.
   Thoughtful experts use the word in multiple inconsistent ways.
  b) It also depends on just what you mean by "measure".  Often it
   is possible to _ascertain_ the entropy of a source ... but direct
   empirical measurements of the output are usually not the best way.
Yes.  It is naturally _dimensionless_, but dimensionless quantities
often have nontrivial units.  Commonly used units for entropy
  ++ bits
  ++ joules per kelvin.  One J/K equals 1.04?10^23 bits
For more on this, see
   They are neither exactly the same nor completely unrelated.
A pseudorandom sequence may be "random enough" for many
applications, yet has asymptotically zero entropy density.
A sequence of _odd_ bytes is obviously not entirely random,
yet may have considerable entropy density.
When you apply a mathematical function to an ensemble of inputs, it
is common to find that the ensemble of outputs has less entropy than
the ensemble of inputs. A simple pigeonhole argument suffices to show
that a function whose output is represented in 160 bits cannot possibly
represent more than 160 bits of entropy per output.  So if you want
the ensemble of outputs to have more than 160 bits of entropy, it is
necessary to do something fancier than a single instance of SHA-1.
Shuffling a deck of cards increases the entropy of the deck.
For more on this, see

@_date: 2006-03-23 13:44:35
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
I would have said almost the opposite.  The great power and beauty
of the entropy idea is that it has the *same* meaning across many
domains.  Entropy is defined in terms of probability, period.  Any
other definition is either
   a) exactly equivalent,
   b) an approximation, valid under this-or-that restrictive conditions, or
   c) wrong.
With some slight fiddling to get the normalization right, 1/2
raised to the power of (program length) defines a probability
measure.  This may not be "the" probability you want, but it
is "a" probability, and you can plug it into the entropy definition.
The problem is almost never understanding the definition of
entropy.  The problem is almost always ascertaining what is
the correct probability measure applicable to the problem
at hand.
Don't get me started about "universal" probability measures.
That has an arcane narrow technical meaning that is verrry widely
I think it is very close to 81 bits, not 81.5, but that is a minor
point that doesn't change the conclusion:
Yeah, but if I sample it N times, with high probability I can generate
a large number of very good keys.  This problem is faced by (and solved
by) any decent TRNG.

@_date: 2006-03-23 19:05:42
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
>>With some slight fiddling to get the normalization right, 1/2
 >>raised to the power of (program length) defines a probability
 >>measure.  This may not be "the" probability you want, but it
 >>is "a" probability, and you can plug it into the entropy definition.
OK, in a moment we will have gone through four plies of no-it-isn't
yes-it-is no-it-isn't yes-it-is.  Let's get serious.  The axiomatic
definition of a measure is
   -- a mapping from sets to numbers
   -- positive
   -- additive on the countable union of disjoint sets
And a probability measure has the further property of being
   -- bounded above
I have checked that -- with due attention to trivial details --
.5 ^ (program length) satisfies this definition.  If you wish to
renew the assertion that there is no such probability measure, please
explain which of the axiomatic requirements is not met.  Please be
 > For that measure, we can intelligently discuss the entropy of a
 > specific random string, without reference to a probability model.
That's like saying we can talk about three-dimensional force, velocity,
and acceleration "without reference" to vectors.
Measure theory is the tried-and-true formalism for dealing with random
strings.  It would be spectacularly contrary to ignore the formalism,
and just plain wrong to say the formalism is inapplicable.
Huh?  Omega is so obviously a probability that usually the word probability
is used in its definition.  See e.g.
   I suppose a masochistic nitpicker could demand to see a proof that this word
is justified, but I'm not going to bother, for reasons given above.

@_date: 2006-03-23 23:21:47
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
There is no such that as "the" universal measure;  rather
there are lots of "universal" measures.  Universality is a
rather arcane property of the measure, the term doesn't mean
what most people think it means.
   -- Universal does NOT mean all-purpose.
   -- Universal does NOT mean general-purpose.
   -- There are many inequivalent "universal" distributions, most
    of which are not what you want for any given application.
   -- Certain things that are true asymptotically are not true
    for _any_ practical application.
   -- Ratio converging to 1 does not imply difference converging to 0.
This is probably not the right forum for cleaning out this Augean stable.
Not really questionable.  If you have a probability, you have an
The entropy _per string_ is unbounded, as it jolly well should be for
random strings of unbounded length.  That's true but uninteresting.
A more interesting quantity is the _entropy density_ aka the entropy
_per symbol_ which for typical long random bit-strings is one bit of
entropy per bit of string-length.  Similarly if you have a string of
symbols over a 32-symbol alphabet, you would expect to see five bits
of entropy per symbol for a typical long random string.

@_date: 2006-03-24 11:57:47
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
This particular problem is contrived or at least exaggerated.  The
solution in this case is trivial:  Just run the raw data through
a compressor.  Huffman coding works fine.
     raw data                   cooked data
     zero word                  0             (one bit)
     other word                 1 || word     (161 bits)
The cooked data has 100% entropy density.  Not only does it have
162 bits of entropy for every 162 bits of string length _on average_,
every N-bits-long substring has N bits of entropy, for all values of
This version serves to illustrate, in an exaggerated way, the necessity
of not assuming that the raw data words are IID (independent and identically
Forsooth, in real life the raw data words are never exactly IID, but with
suitable engineering you can arrange that they are not terribly far from
IID, and in particular you can ascertain a useful _lower bound_ on the
entropy per raw data word.
You can then proceed to concentrate the entropy, so as to achieve something
approaching 100% entropy density at the output.  A method for doing this is
discussed at
   in particular

@_date: 2006-03-24 16:45:37
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
That's true in classical (19th-century) thermodynamics, but not
true in modern physics, including statistical mechanics.  The
existence of superconductors and superfluids removes all doubt
about the absolute zero of entropy.  For details, see
      Not true.
They are neither general nor relevant to crypto.

@_date: 2006-03-25 19:26:51
@_author: John Denker 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
In the context of
 >> 0 occurs with probability 1/2
 >> each other number from 1 to 2^{160}+1 happens with
 >> probability 2^{-161}.
 > This ... serves to illustrate, in an exaggerated way, the necessity
 > of not assuming that the raw data words are IID (independent and identically
 > distributed).
Correction:  IID isn't the issue here.  The raw data words described
above are IID.  That's not the problem.  The problem is that the
distribution is highly skewed.
Executive summary:  Small samples do not always exhibit "average" behavior.
Let me explain.  There is a very simple way to look at this.
Consider the expression for the entropy of the source,
        S := Sum_i P_i log(1/P_i)                      [1]
where i runs over all symbols in the alphabet.
One may, with a little care, attribute log(1/P_i) bits of unpredictability
to the (i)th symbol in the alphabet.  Then S can be interpreted as the
appropriately weighted _average_ unpredictability per symbol.
In the example quoted above, the minimum log(1/P_i) is vastly smaller than
the average log(1/P_i) -- namely 1 versus 161.  So focussing on the average
is unlikely to solve all the world's problems.
In crypto applications (including RNG construction), a crude yet provably
correct approach is to rely on the minimum (not the average) per-symbol
unpredictability.  Using this approach, it would require 80 samples of the
given distribution to produce an output with 80 bits of entropy.
Things can only get better from here:
  -- With full knowledge of the source statistics, one can distinguish the large
   log(1/Pi) words from the small log(1/Pi) words.  This would allow the number
   of required samples to be closer to the typical value (2) than to the worst-case
   value (80).  An example of this is the Huffman coding I discussed in my previous
   note.
  -- With even modest knowledge of the given source, one can (by histogramming
   if nothing else) estimate the probabilities well enough to yield worthwhile
   improvements in efficiency.
  -- If you want to produce a long sequence of output words, as you often do, a
   reasonable-sized buffer is all you really need to come fairly close to optimal
   efficiency, namely only about 1 sample of the distribution, on average, per
   80-bit output word.  The chance of getting fooled can be made verrry small,
   at a modest cost in terms of buffer-size and extra samples of the distribution.
   That is, the law of large numbers comes to your rescue sooner or later, usually
   rather soon.
  -- It may be possible to engineer a different source with larger minimum
   log(1/Pi).
Bottom line:  Setting up a highly-skewed distribution and then drawing from
it only a single sample is not guaranteed to produce "average" behavior.  Duh,
no surprise there!
The source entropy S in equation [1] is a single number that characterizes the
"average" behavior.  For a small sample from a highly-skewed distribution, S
doesn't tell you everything you need to know.  This has no bearing on the
definition of entropy;  entropy is defined by equation [1].  It just means that
equation [1] by itself doesn't solve all the world's problems.

@_date: 2007-12-06 11:46:21
@_author: John Denker 
@_subject: electoral security by obscurity on trial 
For years, the Election Integrity Committee of the Pima County
Democratic Party has been trying to improve the security of the elections systems used in local elections. The results include:
 -- a dozen or so suggestions that they made were actually accepted
  and implemented by the county.
 -- there was a criminal investigation by the Attorney General's   office into the actions of Pima County Division of Elections   personnel, but no charges were brought.
 -- last but not least, they wanted access (after each election) to   the record of votes cast, but the county refused, leading to a
  lawsuit that has just now come to trial.
Background info on the trial:
    Overview with links:
 Report on day one, with links to statements and testimony:
    I note that the plaintiffs' opening statement actually
  used the term "security through obscurity".
Report on day two, with links to testimony:
  "Election Security Report"
  From the County Administrator to the Board of Supervisors     The last 20 pages reproduce an article from     The Information Technology & Innovation Foundation
    "Stop the Presses:  How Paper Trails Fail to Secure e-Voting"
  which quite one-sidedly favors cryptologic solutions to all
  problems.  It suggests things like cut-and-choose and zero-
  knowledge proofs ... which makes for a dramatic contrast with
  the appalling unsophistication of the Diebold machines that
  are actually being used.
  Disclaimer:  One of the attorneys in the case is T. A. Denker.
  Yes, he is my brother.  No, I have not learned _anything_
  about the case from him.  I am not involved in this case ...
  except to the extent that as a voter I have a stake in the
  outcome.
This is not an issue for the trial, but I can't help noting
that for years the Australians have been using a Linux-based e-voting system, with all code open to public review:
    which makes another dramatic contrast with Diebold's stated
"need" for secrecy.

@_date: 2007-12-12 09:55:16
@_author: John Denker 
@_subject: PunchScan  voting protocol 
Hi Folks --
I was wondering to what extent the folks on this list have taken
a look the PunchScan voting scheme:
  The site makes the following claims:
Those seem at first glance to be a decent set of claims, from
a public-policy point of view.  If somebody would prefer a
different set of claims, please explain.
PunchScan contains some nifty crypto, but IMHO this looks like
a classic case of too much crypto and not enough real security.
I am particularly skeptical of one of the FAQ-answers
 Several important steps in the process must be carried out in
secret, and if there is any leakage, there is unbounded potential
for vote-buying and voter coercion.
  The Boss can go to each voter and make the usual silver-or-lead
  proposition:  Vote as I say, and then show me your voting receipt.
  I'll give you ten dollars.  But if I find out you voted against
  me, I'll kill you.
The voter cannot afford to take the chance that even a small
percentage of the ballot-keys leak out.
1) It would be nice to see some serious cryptological protection
of election processes and results.
2a) I don't think we're there yet.
2b) In particular I don't think PunchScan really solves "the"
whole problem.
3) I'd love to be wrong about item (2).  Does anybody see a way
to close the gaps?

@_date: 2007-12-15 10:30:08
@_author: John Denker 
@_subject: PunchScan voting protocol 
Well, that's the right question.  That's the sort of question
the punchscan team should be asking themselves, and answering
in more detail that I have heretofore seen.  What threats does
punchscan claim to defend against?  What threats does it leave
to be mitigated by other (non-punchscan) means?
As an example: Let's look at the plant where the ballots are
printed.  Suppose somebody attaches a tiny "spy camera" to
the frame of one of the printing presses, so as to obtain an
image of both parts of the two-part ballot (for some subset
of the ballots).
Obviously anybody who gets this information can defeat all the
cryptologic protections that the protocol is supposed to provide
(for that subset of the ballots).
  Note that the spy camera can be hiding in plain sight, in
  the guise of a "security camera".  Many election-related
  facilities are /required/ to have security cameras.
  There's a difference between mathematical cryptology and real-
  world security.
It's bad luck to prove things that aren't true.  I just gave an
example of a "partial exposure risk", since some of the ballots
were seen by the spy camera and some weren't.
Ah yes, but what is being assumed about the /properties/ of
this Election Authority?  Is the EA omnipresent and omnipotent,
like the FSM, or does it have boundaries and limitations?
For example, does it ever need to rely on employees or

@_date: 2007-12-25 11:19:14
@_author: John Denker 
@_subject: 2008: The year of hack the vote? 
Shouldn't that be:
  2008: Another year of hack the vote yet again?
  ......^^^^^^^.......................^^^^^^^^^
There is every reason to believe that the 2000 presidential
election was stolen.  A fair/honest/lawful election would
have made Al Gore the 43rd president.
There is every reason to believe the situation was even
worse in 2004.  If the election had been fair/honest/lawful
Kerry would have won be a wide margin.
Flipping Ohio's 20 electoral votes would have been sufficient
all by itself to flip the election from Kerry to Bush ... and there is plenty of evidence of widespread fraud in Ohio.  See e.g. the Conyers report,
  And Ohio was only the tip of the iceberg;  there was large-
scale hanky-panky in Florida and many other states.
I like the book by  Prof. Steven F. Freeman & Joel Bleifuss,
  _Was the 2004 Presidential Election Stolen_?
Most of the crucial information can also be found on Freeman's web site
  but the book is much better organized and easier to read.  The
book is dispassionate, scrupulous, and scientific ... which is
something you don't often see, especially in the political sphere.
Another book is by Mark Crispin Miller,
  _Fooled Again_
which is more passionate and less technical.  It takes a broader
view of the subject, and is far easier to read, especially for
readers who are not well-versed in statistics.

@_date: 2007-01-05 15:22:19
@_author: John Denker 
@_subject: SC-based link encryption 
That is a Good Idea that can be used in a wide range of
situations.  Here is some additional detail:
This can be understood as follows:  Half of IPsec "tunnel
mode" can be described as IPIP encapsulation layered on
top of "transport mode" which does the encryption and
arranges for transport of the encrypted packets.
   The other half of IPsec is the SPDB, which is an
   important part of IPsec but is often underappreciated
   by non-experts.
So ... one obvious way forward is to do what might be
called L2sec (layer 2 security) in analogy to IPsec.
That is, do layer-2-in-IP encapsulation using GRE or
the like, and then layer that on top of IPsec transport
  Then you make some straightforward tweaks to the
  SPDB and you've something pretty nice.  As PH
  said, the security properties will be well known.
This may sound like overkill, but it is likely to be
mention more secure and more richly featured).

@_date: 2007-01-18 13:03:38
@_author: John Denker 
@_subject: Private Key Generation from Passwords/phrases 
> The whole issue of entropy is a bit vague for me - I don't normally work
 > at that end of things - so could you point to a good tutorial on the
 > subject, or barring having a reference handy, could you give an overview?
Entropy is defined in terms of probability.
The /definition/ of entropy is
               sum_i  P_i log(1/P_i)             [1]
there the sum runs over all symbols (i) in the probability
distribution, i.e. over all symbols in the ensemble.
Equation [1] is the gold standard.  It is always correct.  Any
other expression for entropy is:
  a) equivalent to [1]
  b) a corollary, valid under some less-than-general conditions, or
  c) wrong.
We have not yet specified the _base_ of the log in equation [1].
If the log is base 2, the entropy is measured in bits.
If the log is base 3, the entropy is measured in nats.
For more on this, including the connection to macroscopic
thermodynamics, see
   One toss of a fair coin is 1 bit of entropy.
Two tosses of a fair coin is 2 bits of entropy.
Entropy can be measured in bits, but not everything measured
in bits is entropy (just as milk can be measured in gallons,
but also sewage can be measured in gallons).  In particular,
a 32 bit word may hold less than 32 bits of entropy, but it
cannot hold more.
A related notion is the /surprise value/ of a given symbol,
      $_i = log(1/P_i)
We immediately see that the entropy is the appropriately
weighted average of the surprise value.  It must be emphasize
that entropy is a property of the whole ensemble (in contrast
to the surprise value which is a property of an individual
symbol).  There is no such thing as the "average entropy";
entropy is already an average.
There are two branches of cryptography.
  -- One branch depends on entropy for security.
  -- The other branch depends on computational complexity for
   security.
The term /random/ is used in multiple inconsistent ways;  if
you ask 5 experts you can get 10 different definitions.  As
for me, I say something is random if it is
_random enough for the application_.  I am *not* correspondingly
tolerant about the term entropy.  Entropy is entropy.  There is
only one definition of entropy, used uniformly across a wide
range of disciplines including
    1. cryptography and cryptanalysis (secret codes)
    2. communications (error-correcting codes, as part of electronic
      engineering)
    3. computer science, including data-compression codes, machine
      learning, speech recognition, etc.
    4. librarianship
    5. the physics of computation
    6. the design of refrigerators, heat pumps, and engines (including
      piston, turbine, and rocket engines)
    7. nuclear engineering (reactors and weapons)
    8. fluid dynamics
    9. astrophysics and cosmology
   10. chemistry and chemical engineering
If you're not defining it in terms of equation [1] or something
provably equivalent, don't call it entropy.
In particular, 200 bits of pseudorandomness is not the same as 200
bits of entropy.  If you mean entropy, say entropy;  if you mean
randomness, say randomness.  Sometimes pseudorandomness based on
computational complexity is suitable for the application, and
sometimes it doesn't make sense to use anything less than genuine
entropy.  For more on how to generate usable entropy, see
   For more about the definition of entropy, including the connection
to thermodynamics, see

@_date: 2007-01-18 16:55:48
@_author: John Denker 
@_subject: Private Key Generation from Passwords/phrases 
> In article <45AFB67A.40300 at av8n.com> you write:
 >> The /definition/ of entropy is
 >>
 >>               sum_i  P_i log(1/P_i)             [1]
 >>
 >> there the sum runs over all symbols (i) in the probability
 >> distribution, i.e. over all symbols in the ensemble.
 >>
 >> Equation [1] is the gold standard.  It is always correct.  Any
 >> other expression for entropy is:
 >>  a) equivalent to [1]
 >>  b) a corollary, valid under some less-than-general conditions, or
 >>  c) wrong.
 >
 > I disagree.  In the context of Physics, Shannon entropy may well be
 > the end-all and be-all of entropy measures, but in the context of
 > Cryptography, the situation is a little different.  In Cryptography,
 > there are multiple notions of entropy, and they're each useful in
 > different situations.
There is only one technical definition of entropy, and
only one technical definition of force, and only one
technical definition definition of power.  In parallel,
there are nontechnical and metaphorical uses of the same
words, as in "military force" and "political power".
We would be better off maintaining just the one technical
definition of entropy, namely S = sum_i P_i log(1/P_i).
If you want to talk about something else, call it something
else ... or at least make it clear that you are using the
term in a nontechnical or metaphorical sense.
 > For this particular application, I would suspect that Pliam's workfactor
 > or Massey's "guessing entropy" could well be more accurate.  See, e.g.,
 > the following for a short summary and for references where you can learn
 > more:
 >   Pliam rightly points out the distinction between the "typical"
case and the "average" case.  Entropy is an /average/ property,
as I emphasized in my note this morning.
 > Shannon entropy is often a reasonable first approximation -- it's
 > usually good enough for practical purposes.  But it's just an approximation,
An approximation to what?
It's not an approximation to the entropy.
 > and in some cases it can be misleading.
As a general rule, /anything/ can be abused.
 > Example: Suppose I choose a random 256-bit AES key according to the
 > following distribution.  With probability 1/2, I use the all-zeros key.
 > Otherwise, I choose a random 256-bit key.
 > The Shannon entropy of this
 > distribution is approximately 129 bits.
 > However, it's a lousy way to
 > choose a key, because 50% of the time an adversary can break your
 > crypto immediately.  In other words, just because your crypto key has
 > 129 bits of Shannon entropy doesn't mean that exhaustive keysearch will
 > require at least 2^129 trial decryptions.  This is one (contrived)
 > example where the Shannon entropy can be misleading.
I never said the entropy was equal to the resistance to guessing.  The
crypto FAQ might have said that at one time ... but I am not responsible
for what other people say.  I am only responsible for what I say.
If you want to talk about work factor, call it work factor.
If you want to talk about entropy, call it entropy.

@_date: 2007-07-02 17:11:49
@_author: John Denker 
@_subject: Quantum Cryptography 
That's partly true, but there's more to the story.
Let's start by looking at the simple case, and then proceed to a more
sophisticated analysis:
By analogy:
  -- baseball pitchers should be evaluated on things like ERA, while
  -- football halfbacks should be evaluated on things like yard per carry,
  ... and not vice versa.
By that I mean:
  -- the integrity of DH depends fundamentally on the algorithm, so you
   should verify the algorithmic theory, and then verify that the box
   implements the algorithm correctly; while
  -- in the simple case, the integrity of quantum cryptography depends
   fundamentally on the physics, so you should verify the physics
   theoretically and then verify that the box implements the physics
   correctly,
  ... and not vice versa.
Don't complain that you cannot verify the physics the same way you
would verify the algorithm;  it's not a relevant complaint.
There are some beautiful operational checks that *can* be made on
a simple quantum crypto system.  For starters, you can insert a
smallish amount of attenuation in the link, as a model of attempted
eavesdropping.  The system should detect this, shut down, and raise
the red flag;  if it doesn't, you know it's broken.
A more sophisticated analysis takes into account the fact that in the
real world (as opposed to the ultra-specialized laboratory bench),
there is always some dissipation.  Therefore any attempt to do anything
resembling quantum crypto (or even quantum computing) in the real world
uses some sort of error correction.  (These error correction schemes are
some of the niftiest results in the whole quantum computation literature,
because they involve /analog/ error correction, whereas most previous
modern error-correcting codes had been very, very digital.)  So there is
some interesting genuine originality there, from a theory-of-computation
 From a security standpoint though, this raises all sorts of messy issues.
We now have a box that is neither a pitcher nor a fullback, but some
weird chimera.  To validate it you would need to verify the physics *and*
verify the algorithms *and* verify the interaction between the two.
Needless to say, an algorithm intended for crypto requires much stricter
scrutiny than the same algorithm intended for ordinary computation.
In particular, the oft-repeated claim that "quantum cryptography detects
eavesdropping" may be true on the lab bench, but it does _not_ follow in
any simple way that a usable long-haul system will have the same property.
I agree with Steve that there is a difference between bona-fide early-stage
research and snake oil.
I did research in "neural networks" at a time when 90% of the published
papers in the field were absolute garbage, such as claims of solving
NP-hard problems in P time.
  -- When there are people who respect the difference between garbage and
   non-garbage, and are doing serious research, we should support that.
  -- When people try to publish garbage, and/or package garbage in shiny
   boxes and sell it to the government, we should call it for what it is.

@_date: 2007-07-10 11:22:42
@_author: John Denker 
@_subject: How the Greek cellphone network was tapped. 
I agree.  It's a tricky question;  see below
JI responded:
We all agree we can make a distinction between telcos and phone HW
manufacturers.  But that may not be the relevant distinction.
I know in the US, and I imagine elsewhere, telcos buy phones from
the OEMs and then retail them to customers.  That makes them, in
the eyes of the law, both telecommunication carriers *and* device
vendors, even if they are not device OEMs.
Well, that's logical, but who said the law has to be logical?
IANAL but AFAICT the most sweeping parts of the CALEA law apply
to "telecommunication carriers" as defined in section 1001:
   Customer encryption is explicitly not included by the terms of
section 1002:
   "... unless the encryption was provided by the carrier and
the carrier possesses the information necessary to decrypt
the communication."
I repeat: "... unless the encryption was provided by the carrier
and the carrier possesses the information necessary to decrypt
the communication."
Following this line of thought leads to all sorts of illogical
conclusions, including:
  a) Arguably it might be OK to buy a backdoor-free crypto phone
   from the grocery store, but not OK to buy or lease it from
   the phone company.
  b) Arguably you could buy a phone from the telco with no
   crypto at all, and then take it to Orange County Choppers
   and have them install backdoor-free crypto.
  c) Arguably the OEM could have two product lines, one without
   backdoors, to be sold via telcos, and one without backdoors,
   to be sold otherwise.
  d) Arguably everybody is OK provided the telco doesn't have
   the keys.  Maybe you can use a crypto phone provided by a
   US telco if you have a high-assurance way of changing the
   keys to the back door as well as the front door.
  e) We all know the laws differ wildly from one jurisdiction
   to another ... and the laws can be changed at any time.
The cost of the second product line (item b) might not be too
much higher than the first product line (item a), since it
could be considered a /byproduct/, such that all the big
development costs are attributed to line (a) ... assuming
there is a market for crypto phones of any kind.
As to whether any such market will develop in the near future
is another interesting question.  The fact that only a tiny
fraction of present-day email is E2E encrypted is not an
encouraging sign.  (Email is easier to encrypt than voice.)

@_date: 2007-06-26 15:31:13
@_author: John Denker 
@_subject: Quantum Cryptography 
>  1) Do you believe the physics?  (Most people who know physics seem to.)
Well, I do happen to know a thing or two about physics.  I know
  -- there is quite a lot you can do with quantum physics, and
  -- there is quite a lot you cannot do with quantum physics.
I also know that snake-oil salesmen can lie about the physics
just as easily as they lie about anything else.
Since it's not clear what is meant by "THE" physics, it would
be more meaningful to ask more-specific questions, namely:
  -- Do I believe in real physics?  Yes.
  -- Do I believe in what Dr. Duck says about physics?  Usually not.
One commonly-made claim about quantum cryptography is that
"it can detect eavesdropping".  I reckon that's narrowly
true as stated.  The problem is, I don't know why I should
care.  The history of cryptography for most of the last 2000
years has been a cat and mouse game between the code makers
and the code breakers.  The consensus is that right now the
code makers have the upper hand.  As a result, Eve can eavesdrop
all she wants, and it won't do her a bit of good.
To say the same thing:  It appears that in this respect, quantum
cryptography takes a well-solved problem and solves it another
way at higher cost and lower throughput.  The cost/benefit ratio
is exceedingly unfavorable, and seems likely to remain so.
Meanwhile, it takes some less-well-solved problems and makes
them worse.  Consider for example traffic analysis.  Since
quantum encryption requires a dedicated hardware link from end
to end, there is no hope of disguising who is communicating
with whom.
I am reminded of a slide that Whit Diffie used in one of his
talks.  It showed a house that was supposed to be protected
by a picket fence.  The problem was that the so-called fence
consisted of a single picket, 4 inches wide and a mile high,
while the other 99.9% of the perimeter was unprotected.  Yes
sirree, no eavesdropper is going to hop over that picket!
One sometimes hears even stronger claims, but they are even
more easily refuted.  I've reviewed papers that claim quantum
mechanics "solves the key distribution problem" but in fact
they were using classical techniques to deal with all the
hard parts of the problem.  It reminds me of stone soup: if
the ingredients include broth, meat, vegetables, seasoning,
and a stone, I don't see why the stone should get credit for
the resulting soup.  Likewise, since a quantum key distribution
system is in no ways better and in some ways worse than a
classical system, I don't see why quantum cryptography
should get credit for solving the problem.

@_date: 2008-04-28 12:58:21
@_author: John Denker 
@_subject: defending against evil in all layers of hardware and software 
This is an important discussion  The threats are real, and we need to defend against them.
We need to consider the _whole_ problem, top to bottom.  The
layers that could be subverted include, at a minimum:
 -- The cpu chip itself (which set off the current flurry of
  interest).
 -- The boot rom.
 -- The BIOS code that lives on practically every card plugged
  into the PCI bus.
 -- Board-level stuff like memory controllers and I/O bridges.
 -- The operating system.
 -- Compilers, as Uncle Ken pointed out.
   -- Your "secure" application.
 -- Users.
As a particular example where PCs that we might wish to be secure are known to be under attack, consider electronic voting machines.  In most cases there's a PC in there, running Windows CE.  Some
application software was provided by people with felony fraud
convictions.  Means, motive, and opportunity are all present.
There is ample evidence that "problems" have occurred.  These
are not confined to the Florida fiasco in 2000.  An example from 2004 is the voting machine in Franklin County, Ohio that recorded 4,258 votes for Bush when only 638 voters showed up.
  This should not be an occasion for idly wringing our hands, nor sticking our head in the sand, nor looking under the lamp-post where the looking is easy.  We need to look at all of this stuff.  And we can.  We are not defenseless.
As in all security, we need not aim for absolute security.  An often-useful approach is to do things that raise the cost to the attacker out of proportion to the cost to the defender.
For software, for firmware, and to some extent even for silicon
masks, SCM (source code management) systems, if used wisely, can
help a lot.  Consider for example a policy every delta to the software to be submitted by one person and tested by another
before being committed to the main branch of the project.  Both
the submitter and the tester would digitally sign the delta.  This creates a long-tailed liability for anybody who tries to
sneak in a trojan  This is AFAIK the simplest defense against
high-grade attacks such as Ken's, which leave no long-term trace
in the source code (because the trojan is self-replicating).  The point is that there is a long-term trace in the SCM logs.
We can make the logs effectively permanent and immutable.
Of course we should insist on an open-source boot ROM code:
  The boot ROM should check the pgp signature of each PCI card's
BIOS code before letting it get control.  And then it should
check the pgp signature of the operating system before booting it.  I don't know of any machine that actually does this, but
it all seems perfectly doable.
Another line of defense involves closing the loop.  For example,
one could in principle find Ken's trojan by disassembling the
compiler and looking for code that doesn't seem to "belong".
I have personally disassembled a couple of operating systems
(although this was back when operating systems were smaller
than they are now).
We can similarly close the loop on chips.  As others have pointed
out, silicon has no secrets.  A cost-effective way to check for 	
trojans would be to buy more voting machines than necessary, and choose /at random/ a few of them to be torn down for testing.
(This has obvious analogies to sampling methods used in many
crypto algorithms.)  For starters, we grind open the CPU chips
and check that they are all the same.  That's much easier than
checking the detailed functionality of each one.  And we check
that the CPUs in the voting machines are the same as CPUs from another source, perhaps WAL*MART, on the theory that the attacker
finds it harder to subvert all of WAL*MART than to subvert just
a truckload of voting machines.
Checksumming the boot ROM in the torn-down machine is easy.  I
emphasize that we should *not* rely on asking a running machine
to checksum its own ROMs, because it is just too easy to subvert
the program that calculates the checksum.  To defend against
this, we tear down multiple machines, and give one randomly
selected ROM to the Democratic pollwatchers, one to the Republican pollwatchers, et cetera.  This way nobody needs to trusty anybody
else;  each guy is responsible for making sure _his_ checksummer
is OK.
All of this works hand-in-glove with old-fashioned procedural
security and physical security.  As the saying goes, if you
don't have physical security, you don't have security.  But
the converse is true, too:  Placing armed guards around a vault full of voting machines doesn't make the machines any less buggy than they were when they went into the vault. That's why we need a balanced approach that gets all the layers to work together.

@_date: 2008-02-07 12:18:53
@_author: John Denker 
@_subject: customs searching laptops, demanding passwords 
I quote from
    By Ellen Nakashima
  Washington Post Staff Writer    Thursday, February 7, 2008; A01
Most of the underlying issue is not new;  a Joe Sharkey article
about customs seizures of laptops appeared in the NY Times back on October 24, 2006.  And it has been discussed on this list.
(The news "hook" here is the filing of the lawsuit.)
One wrinkle that was not previously reported is the bit about
customs officers demanding passwords.  That is something I
have thought about, off and on, and the more I think about it the more worrisome it seems.
A) Here's one particularly nasty scenario:  Long ago, the traveler
experimented with using an encrypting filesystem, perhaps the dm-crypt feature of Linux.  However, he decided it wasn't worth the trouble and forgot about it.  This includes forgetting the passphrase.  Now he's at the border, and customs is demanding the passphrase.    -- Just tell us the password.
 -- I forgot.
 -- No you didn't.
 -- Yes I did.
 -- You're lying.
 -- No I'm not.
 -- Yes you are.
 -- No I'm not.
 -- Just tell us the password.
 -- et cetera.
B) Another scenario:  Your employer adopts a policy requiring
you to use a "blank" laptop when traveling, as mentioned in the news article.  They also require you to use an encrypting
filesystem, even when not traveling.  They discover that the
easiest way to "blankify" your laptop is to overwrite the IVs
of the encrypting filesystem.  Now any and all passphrases
will fail in the same way:  they all look like "wrong" pass-
phrases.  Now are back to scenario (A), because customs might assume you're just lying about the passphrase.
C) Another scenario:  Customs confiscates the laptop.  They say that you won't get it back unless/until you give up the D) Tangential observation:  If they were being reasonable, they would confiscate at most the disk drive, and let you keep the rest of the hardware.  But they're under no obligation to be E) Remark:  The fundamental problem underlying this whole discussion is that the traveler is in a position where he has to prove his innocence ... which may not be possible, even if he is innocent.
The doctrine of innocent-until-proven-guilty does *not* apply
to customs searches.  Ditto for the doctrine of requiring
probable cause, search warrants, et cetera.
F) A good way (not the easiest way) to "blankify" a laptop
is to remove the hard disk and replace it with a brand-new
obviously-innocuous disk.  (Small, slow disks are very cheap.)
When you get home from your travels, you can undo the switch.
G) It is fun to think about a steganographic filesystem, with
the property that if you mount it with one passphrase you see
one set of files, while if you mount it with another passphrase
you see another set of files.  The point here is that you give up one passphrase, they never
know if there is a second;  if you give up two passphrases,
they never know if there is a third, et cetera.
Note that we are talking about cryptologically-strong stego
here (as opposed to weak stego which falls into the category
of security-by-obscurity).
straightforward;  solutions have been worked out in connection with code division multiplexing.  However, I reckon it would
have serious performance problems when applied to a hard disk.  If anybody knows how to do this in practice, please speak up!

@_date: 2008-01-28 15:56:11
@_author: John Denker 
@_subject: two-person login? 
Hi Folks --
I have been asked to opine on a system that requires a "two-person login".  Some AIX documents refer to this as
a "common method of increasing login security"
  However, I don't think it is very common;  I get only five hits
  By way of not-very-apt analogy:  -- I am aware of business checks that require two signatures;
 -- I am aware that purchase orders are commonly signed by    two persons (the initiator and the approver);  -- I am aware of missile-launch procedures that require two
  persons using two keys;
 -- I am aware of software development procedures where a patch
  is submitted (and signed) by one person, tested (and signed
  off) by another, and committed to the official repository by   a third person.
 -- et cetera.
However, it is important to note that the aforementioned examples
all share an important property, as we see from the following:
  *) The parties give their approval _after_ the semantics has
   been fully determined.  It would defeat all security if two    signatures were attached to a blank check or a blank PO.
  *) As a related point, the approval is attached to a particular
   transaction.  The approver is not merely certifying that Joe
   is really Joe, and is generally allowed to write checks    (mere identification);  the approver is certifying that this    particular check is OK.
  *) To say the same thing another way, there is no pretexting.
   There is no pretext for turning the keys on the missile launcher
   unless you intend to launch a missile.  The semantics of the
   keys is clear.
We need to talk about threat models:
  a) The purveyors of the system in question don't have any clue
   as to what their threat model is.  I conjecture that they might
   be motivated by the non-apt analogies itemized above.
  b) In the system in question, there are myriad reasons why Joe
   would need to log in.  If Joe wanted to do something nefarious,
   it would take him less than a second to come up with a seemingly
   non-nefarious pretext.  When the approver approves Joe's login,
   the approver has no idea what the consequences of that approval
   will be.  The two-person login requires the approver to be
   present at login time, but does not require the approver to
   remain present, let alone take responsibility what Joe does    after login.
  c) The only threat model I can come up with is the case where
   Joe's password has been compromised, and nobody else's has.
   Two-person login would provide an extra layer of security
   in this case.  This threat is real, but there are other ways
   of dealing with this threat (e.g. two-factor authentication)
   ... so this seems like quite a lame justification for the
   two-person login.
  d) Or have I overlooked something?
system is worthless security theater ... but harmless security
theater, and therefore not worth worrying about either way.
But the plot thickens.  The purveyors have implemented two-person
login in a way that manifestly /reduces/ security.  Details available on request.
So now I throw it open for discussion.  Is there any significant
value in two-person login?  That is, can you identify any threat that is alleviated by two-person login, that is not more wisely alleviated in some other way?
If so, is there any advice you can give on how to do this right?  Any DOs and DON"Ts?

@_date: 2008-01-29 14:45:53
@_author: John Denker 
@_subject: two-person login? 
OK, that's clear and helpful.  Thanks.
The point I take away from this is that _procedure_ is primary
and fundamental.  Technology is secondary.  The two-person login is technology, and it is only icing on the procedural cake.
 -- If you have a good procedure, the two-person login might help   "remind" people to follow the procedure.
 -- If you don't have a good procedure, having a two-person login   will not magically create a good procedure.
This also gets back to what I said previously about semantics.  In
the situation The Fungi described, the semantics is clear.  A login
is tantamount to a commit, and the semantics of "commit" is clear.
Both parties make sure they understand the commit before they log
in.  Both parties log in, both parties hang around until the commit is complete, then both parties log out and leave.
  One question: If the goal is to have a two-person commit, why not
  implement a two-person commit, rather than using two-person login
  as a proxy?  For years there have been "paperless office" workflow
  systems that require two digital signatures on a purchase order.
  If you're going to use technology to support procedure, IMHO the
  technology should be tied as tightly as possible to the procedure.
  The semantics of login is IMHO very unclear, very open-ended, and
  it takes a lot of hand-waving to tie the "login" technology to the   "commit" semantics.
The foregoing makes sense, and is in extreme contrast to the situation
I am faced with, where Joe logs in with the help of Jane, and then
Jane leaves.  Jane has not the slightest control over what Joe does
while logged in.  I don't see a sane procedure here.  It seems Jane is signing a blank check.
It wouldn't be so bad if there were a development system separate
from the production system, but there isn't, so Joe spends all day
every day logged into the "high security" production system.  Joe
can commit anything he wishes.  There is no two-party review of the
commit, just two-party review of the login.
Just to rub salt in the wound, they've got it set up so that everybody
uses the "Admin" account.  There are N people who know the first half
of the Admin password, and M people who know the second half.  Besides
being an incredibly lame form of secret-splitting, this has the nasty
property that when Admin logs in, you don't even know who was involved.  There are M*N/2 possibilities.  There is no accountability anywhere.

@_date: 2008-07-23 08:04:32
@_author: John Denker 
@_subject: how to check if your ISP's DNS servers are safe 
Thanks, that's helpful.
Note that the command-line version accepts the " option,
which is useful if you have to deal with a mess of primaries, secondaries, forwarders, et cetera:
   dig  +short porttest.dns-oarc.net TXT
   dig  +short porttest.dns-oarc.net TXT
   dig  +short porttest.dns-oarc.net TXT

@_date: 2008-07-30 15:42:40
@_author: John Denker 
@_subject: On "randomness" 
That may or may not be an overstatement.
IMHO it all depends on what is meant by "random".  The only notion of randomness that I have found worthwhile is the notion of being _random enough for the purpose_.
  *) For some benign purposes, such as a Monte Carlo integration
   routine, a simple linear congruence generator might suffice.
   Such a generator would not withstand cryptanalysis for even
   a moment, but it will not be subjected to cryptanalysis, so
   we don't care.
  *) At the other extreme, there are many high-stakes business,
   military, and gambling applications where I would agree with    von Neumann, and would shun absolutely all PRNGs.  I would    rely exclusively on _hardware_ randomness generators, as
   detailed at:
         The /seeding/ of PRNGs is a notorious problem;  the idea of
   seeding one PRNG with another often reduces to the problem
   previously _not_ solved.  Sooner or later you need a source
   of high-grade randomness, not pseudo randomness, and sooner
   is better than later.
   For this reason, most so-called PRNGs are not really PRNGs
   after all, since their foundations are seen to rest on a    hardware randomness generator.  They are more usefully
   considered schemes for _stretching_ the randomness of the    underlying hardware.  I call them SRNGs, for "stretched    randomness generators".
How extensive?
To paraphrase Dykstra:  Testing may prove the absence of randomness,
but it cannot possibly prove the presence of randomness.
Testing for high-grade randomness is not just a "hard" problem; it is formally undecidable, in the same category as the halting problem.  Reference: Chaitin.  See also:
  On 07/30/2008 01:33 PM, Ben Laurie replied:
Quite so.
Sometimes that's good enough, but sometimes it's not.  Or more to the
point, often the cost of "thinking hard" enough exceeds the cost of implementing a _hardware_ randomness generator that has a _provable_
_lower bound_ on its entropy(*).
To paraphrase the previous paraphrase:  Testing may provide an upper bound on the randomness, but it cannot possibly provide a useful lower bound.  In contrast, physics can provide a useful lower bound.
Saying that this-or-that test "measures" the randomness is highly misleading if you don't distinguish measuring an upper bound from measuring a lower bound.  The test that judged a DNS server to be
"GREAT" was making precisely this mistake.
  *) NB: Whereas I mean something rather vague by "randomness" (i.e.   "random enough for the application") I mean something very specific   by "entropy".
For details on all this, see
  and in particular

@_date: 2008-10-24 05:29:36
@_author: John Denker 
@_subject: combining entropy 
The second assumption suffices to prove the result,
since (random bit) XOR (anything) is random.

@_date: 2008-10-24 15:20:24
@_author: John Denker 
@_subject: combining entropy 
The question, according to the original poster, is not whether it is "safe" to assume that one of the entropy
sources can be trusted.  Safe or not, the question explicitly assumed that one of the sources was trusted ... and asked what the consequences of that assumption would be.
In particular, evidently the scenario was that we started
with N high-entropy randomness generators, but N-1 of
them have failed.  One of them is still working, but we
don't know which one.
In that scenario, XOR is a good-enough combining function,
and nothing else would be any better.
If somebody wants to discuss a different scenario, please
clarify what the new scenario is.
Suggesting that the "trusted" source is correlated with one
of the other sources is quite contrary to the requirements
expressed in the original question.
That is to say, if the source is not independent, it was
never eligible to be a trusted entropy source.
If you want to quantify this, write down the _joint_ probability
distribution for all the sources, and calculate the entropy
of that distribution in the usual way.
1) There is _one_ very precise meaning for "entropy" that is well-established and conventional across a wide range of fields ... everything from kitchen appliances to cosmology.
  2) Authors are allowed to define and redefine terms however
they please ... _provided_ they define any nonstandard terms
that they use.  Anybody who takes a well-established standard
term and uses it in a nonstandard way has a double-extra-special
duty to explain what he's doing.
I assume the original poster was using the term "entropy"
in the conventional, precise sense ... and until I hear
otherwise I will continue to do so.

@_date: 2008-10-24 20:07:37
@_author: John Denker 
@_subject: combining entropy 
Thanks, that question advances the discussion.
The answer, however, is no, I did not assume 100% entropy
density.  Here is the critical assumption that I did make:
We consider the scenario where we started with N randomness
generators, but N-1 of them have failed.  One of them is
still working, but we don't know which one.
To say the same thing in more detail:  Suppose we start
with N generators, each of which puts out a 160 bit word
containing 80 bits of _trusted_ entropy.  That's a 50%
entropy density.
Here _trusted_ means we have a provable lower bound on the
entropy.  I assume this is the same as the aforementioned
"min-entropy above some security bound".
We next consider the case where N-1 of the generators have failed, or can no longer be trusted, which is essentially the
same thing for present purposes.  Now we have N-1 generators putting out zero bits of trusted entropy, plus one generator putting out 80 bits of trusted entropy.  I emphasize that
these 80 bits of trusted entropy are necessarily uncorrelated
with anything happening on the other N-1 machines, for the
simple reason that they are uncorrelated with anything happening anywhere else in the universe ... otherwise they
would not qualify as trusted entropy.
XORing together all N of the 160 bit output words produces
a single 160 bit word containing 80 bits of trusted entropy.
Therefore, unless there is some requirement or objective
that I don't know about, the previously-stated conclusion
XOR is provably correct because it is _reversible_ in the thermodynamic sense.  That means it cannot increase or decrease the entropy.
Obviously this numerical example generalizes to any entropy
density from zero to 100% inclusive.
To summarize:  The key assumptions are that we have N-1
broken generators and one working generator.  We don't
know which one is working, but we know that it is working For more about the theory and practice of high-entropy
randomness generators, see

@_date: 2008-10-25 13:40:58
@_author: John Denker 
@_subject: combining entropy 
Even so, it appears there is still some uncertainty as to
interpretation, i.e. some uncertainty as to the requirements
and objectives.
I hereby propose a new scenario.  It is detailed enough to
be amenable to formal analysis.  The hope is that it will
satisfy the requirements and objectives ... or at least
promote a more precise discussion thereof.
We start with a group comprising N members (machines or persons).  Each of them, on demand, puts out a 160 bit word, called a "member" word.  We wish to combine these to form a single word, the "group" word, also 160 bits in length.
We must find a combining function.  The primary objective
is to maximize the entropy of the group word.  A secondary
objective is computational simplicity.
The members can be categorized as follows:
 a) Some of them are abjectly broken.  Their outputs have
  zero entropy density.  Constant outputs and/or predictable
  outputs fall into this category.
 b) Some of them are malicious.  Their outputs may appear
  random, but are in fact predictable by our adversary.
 c) M of them have an entropy density greater than XX.
  As a concrete example, we consider the case where XX=50%,   i.e. 80 bits of entropy in a 160 bit word.
 d) Some of them could contain a high entropy density,
  very close to 100%.  For our example, we assume there
  are none of these;  otherwise the problem would be
  too easy.
If we do things properly, case (b) is no worse than case
(a), for reasons that will become apparent shortly, so
we can lump these cases together.
We don't know which generator falls into which category.
All we need to know is that M of the generators are
putting out useful entropy.
I recommend the following combining function:  concatenate
all N of the member words, and then feed that through a
hash function to produce the group word.  Since SHA-1 is
efficient and has a 160 bit output word, it will serve nicely in our example.
In the sub-case where M=1, the recommended hash-based
procedure produces a group word with 80 bits of entropy, i.e. a 50% entropy density, which is the best we can do.  In this sub-case, SHA-1 is no better than XOR.
As M increases, the entropy density of the output word
converges rather rapidly to 100%.  This is subject to
mild assumptions about the hash function actually working
as a hash function, i.e. not being grossly broken.
When M is greater than 1, the hash function approach
is much better than the XOR approach.  Here is an
easy proof:  Consider the case where each member in
category (c) puts out a 160 bit word consisting of 80 totally random bits in the left half and 80 constant
bits in the right half.  XORing these together only
gets you to 80 bits of entropy in the group word,
whereas hashing is better by a factor of 2.  Actually
(2 minus epsilon) if you want to be fussy about it.
In the case where the entropy is evenly distributed
within the member word, i.e. 160 bits each with a 50%
entropy density, the result is more subtle:  The group
word will converge to 100% entropy density, but the
hash version converges _faster_ than the XOR version.
Here "faster" means you can get by with a smaller M.
Considerably smaller.  Also (!) beware that to get XOR to converge at all, this paragraph depends on some properties of the members that may be hard to realize in practice ... whereas the hash approach has no
such dependence.
To summarize:  In the special sub-case where M=1, XOR
is as good as it gets.  In all other cases I can think
of, the hash approach is much better.
My analysis applies to a specific set of requirements.
If somebody wants to discuss other requirements, please
be specific about what the requirements are.  There
are innumerable creeping features that we could discuss.

@_date: 2008-10-25 13:53:55
@_author: John Denker 
@_subject: combining entropy 
I should have said that in the special sub-case where the member word has entropy density XX=100% _or_ in the special sub-case where M=1, XOR is as good as it
gets.  In all other cases I can think of, the hash approach is much better.
(I excluded the XX=100% case earlier in the note, but
I should have included it in the summary.  Sorry.)

@_date: 2008-10-28 12:09:04
@_author: John Denker 
@_subject: combining entropy 
Perhaps an example will make it clear where I am coming
from.  Suppose I start with a deck of cards that has been randomly shuffled.  It can provide log2(52!) bits of entropy.  That's a little more than 225 bits.  Now suppose I have ten decks of cards all arranged alike.  You could set this up by shuffling one of them and then stacking the others to match ... or by some more symmetric process. In any case the result is symmetric w.r.t interchange of decks.  In this situation, I can choose any one of the decks and obtain 225 bits of entropy.  The funny thing is that if I choose N of the decks, I still get only 225 bits of entropy, not N*225.
This can be summarized by saying that entropy is not an
extensive quantity in this situation.  The graph of
entropy versus N goes like this:
     225        *   *   *   *   *
       0    *
            0   1   2   3   4   5  (# of decks)
The spooky aspect of this situation is the whack-a-mole aspect:  You cannot decide in advance which one of the decks has entropy and which N-1 of them do not.  That's the wrong question.  The first deck we choose to look at has 225 bits of entropy, and only then can we say that the other N-1 decks have zero additional entropy.
The original question spoke of "trusted" sources of
entropy, and I answered accordingly.  To the extent
that the sources are correlated, they were never eligible to be considered trusted sources of entropy.  To say the same thing the other way around, to the extent
that each source can be trusted to provide a certain
amount of entropy, it must be to that extent independent of the others.
It is possible for a source to be partially dependent
and partially independent.  For example, if you take
each of the ten aforementioned decks and "cut the deck"
randomly and independently, that means the first deck
we look at will provide 225 bits of entropy, and each
one thereafter will provide 5.7 bits of additional
entropy, since log2(52)=5.7.  So in this situation,
each deck can be /trusted/ to provide 5.7 bits of
In this situation, requiring each deck to have "no
input" from the other decks would be an overly strict
requirement.  We do not need full independence;  we
just need some independence, as quantified by the
provable lower bound on the entropy.
If you wanted, you could do a deeper analysis of this example, taking into account the fact that 5.7 is not the whole story.  It is easy to use 5.7 bits as a valid and trustworthy lower bound, but under some conditions more entropy is available, and can be quantified by
considering the _joint_ probability distribution and
computing the entropy of that distribution.  Meanwhile
the fact remains that under a wide range of practical conditions, it makes sense to engineer a randomness generator based on provable lower bounds, since that is good enough to get the job done, and a deeper
analysis would not be worth the trouble.
  I'm going to ignore the "At least".  It is very hard to get out more than you put in.
On a less trivial note:  The original question did not
require getting out every last bit of available randomness.
In situations where the sources might be partially
independent and partially dependent, that would be a very hard challenge, and I do not wish to accept that Dealing with provable lower bounds on the entropy is
more tractable, and sufficient for a wide range of
practical purposes.

@_date: 2008-09-20 11:22:18
@_author: John Denker 
@_subject: Lava lamp random number generator made useful? 
Is $7.59 cheap enough?    For that you get a USB audio adapter with mike jack, and
then you can run turbid(tm) to produce high-quality randomness.
Reference, including analytical paper plus code:
  If the above is not good enough, please explain.
I think the turbid solution is much better than a disk.
 -- Unlimited long-term capacity.
 -- Perfect forward secrecy, unlike a disk, unless you do a   really good job of erasing each block after use.
 -- Perfect secrecy in the other direction, period.
If the above is not good enough, please explain.

@_date: 2009-08-03 08:40:54
@_author: John Denker 
@_subject: unattended reboot  (was: clouds ...) 
This problem is routinely solved in practice.
1a) Don't put the keys on the routine backups, and/or
1b) secure the backed-up keys as carefully as you secure the machine 2) If the machine itself is not secure, you have already lost the
game and there's no hope of securing any keys or certificates on
that machine.
I see no advantage in that.  The only halfway-useful property that such data has is that it is not backed up by ordinary off-the-shelf backup routines.  That's not an important advantage because it is easy to arrange for *any* data of your choosing to be not backed up.
 -- If you routinely back up files, put keys in a special file.   -- If you routinely back up entire partition, put keys in a special   partition (or outside any partition).   -- If you routinely mirror entire drives, put keys on a special drive.  This is all "stock technology".
Let's be clear:  If the attackers have penetrated the machine to the point where they can read the keys from a special file/partition/drive, they can read the hardware serial numbers etc. as well.
That makes life harder for the good guys, and makes life easier for
the bad guys.  Just putting the keys on disk is far more reliable
and practical, especially during hardware maintenance (scheduled or On top of all that, there is the very serious risk of a dictionary
attack against the hardware serial numbers.  There's nowhere near
enough entropy in the hardware serial numbers.  There is incomparably
more entropy in a special file/partition/drive.
That's yet another reason for not taking the hardware serial number
approach.  In contrast, a special file/partition/drive can be virtualized in a direct and natural way.
Bottom line:  Relying on hardware serial numbers etc. to defend keys is not recommended.  Vastly more practical approaches are available.

@_date: 2010-08-01 10:35:59
@_author: John Denker 
@_subject: init.d/urandom : saving random-seed 
Yes indeed!
I've been thinking about that.  That approach might be even *better*
than it first appears.
By way of background, recall that a good design for the central part
of a PRNG is:
   output = hash(key, counter)         [2]
  -- the "hash" is your favorite cryptographically strong hash function;
  -- the "counter" is just a plain old counter, with enough bits
   to ensure that it will never wrap around; and
  -- the "key" is unique to this instance of the PRNG, is unknown to
   the attackers, and has enough bits to rule out dictionary attacks.
  -- There should be some scheme for reseeding the key every so    often, using real entropy from somewhere. This is outside of what
   I call the "central" part of the PRNG, so let's defer discussion
   of this point for a few moments.
Note that this works even though the counter has no entropy at all.
It works even if the attacker knows the counter values exactly.
This is crucial to an analysis of idea [1], because I am not sure
that the date/time string has any useful amount of entropy.  Let's
be clear: if the attacker knows what time it is, the data/time
string contains no entropy at all.
Now, if all we need is a /counter/ then the merit of idea [1] goes
up dramatically.
I reckon "date +%s.%N" makes a fine counter.
Note that date is /bin/date (not /usr/bin/date) so it is usable
very early in the boot process, as desired.
This requires that all boxes have a working Real Time Clock, which
seems like a reasonable requirement.
Security demands that the key in equation [2] be unique on a
machine-by-machine  basis.  This means that if I want my "live CD" to be secure, I cannot simply download a standard .iso image and burn it to CD.  I need to
 -- download the .iso image
 -- give it a random-seed file with something unique, preferably   from the output of a good TRNG, and
 -- then burn it to disk.
I have very preliminary draft of a script to install a random-seed
file into an Ubunto live-CD image.    Suggestions for improvement would be welcome.
So, let's summarize the recommended procedure as I understand it.
There are two modes, which I call Mode A and Mode B.
In both modes:
 *) there needs to be a random-seed file.  The contents must be
  unique and unknown to the attackers.
 *) /dev/urandom should block or throw an error if it used before
  it is seeded
 *) early in the boot process, the PRNG should be seeded using
  the random-seed file _and_ "date +%s.%N".  This should happen
   -- after the random-seed file becomes readable
   -- after the Real Time Clock is available
   -- as soon thereafter as convenient, and
   -- before there is any need to use the output of /dev/urandom
 *) This is all that is necessary for Mode B, which provides a
  /modest/ level of security for a /modest/ level of exposure.    As a first rough guess I suggest limiting exposure to 1000   hours of operation or 1000 reboots, whichever comes first.
 *) Mode A is the same as Mode B, but has no exposure limits
  because the random-seed is replaced before the Mode-B limits
  are reached.   *) It is "nice" to update the  random-seed on every reboot.    This should happen
   -- after the random-seed file becomes writable
   -- as soon thereafter as convenient, to minimize the chance
    that the system will crash before the update occurs.
 *) The random-seed file should be updated again during shutdown.
  This allows recovery from a situation where the random-seed
  file might have been compromised.
 *) Updating fails if some wiseguy mounts a filesystem in such a
  way that the random-seed file that gets updated is not the one
  that will be used for seeding the PRNG.  AFAICT Mode A depends
  on having the random-seed file in local writeable persistent
  storage, not on (say) a networked remote file system.  In some
  cases the init.d/urandom script would have to be customized to   locate the random-seed file on a suitable storage medium.  It
  seems like a step in the wrong direction to use /etc or /lib
  as the routine/normal location for the random-seed file, since
  there are many machines that operate with permanently readonly
  /etc and /lib.  I recommend /var as being most likely to be
  routinely read/writable and persistent.
 *) If updating fails, it is not a tragedy, but it means we are
  operating in Mode B not Mode A.
 *) There should whenever possible be a way of introducing real
  entropy into the PRNG and into the random-seed file.  This is
  important, but the details are outside the scope of the present
  discussion.  Entropy can come from a local HRNG such as Turbid,
  or from a networked random-number oracle if necessary.  Note
  that Mode B is sufficient to allow a system to come up and   establish a secure connection to the oracle.
 *) Note for hard-core experts:  I can improve the security of   the "emergency boot CD" that I carry in my bag of tricks if I
  also carry a USB memory stick containing a seed file.  After   the machine is booted, I can feed that file into the PRNG, and   then update the file.  If I do this before exposing the machine   to attack, it minimizes the exposure of Mode B.  I call this   approach "Mode B+".  I don't expect this to appeal to the general
  public.
 *) Always remember that anybody who is serious about security
  should be using a TRNG not a PRNG anyway.
Also, to summarize earlier messages:
 -- when saving the random-seed file, it is appropriate to save
  the same number of bits as in the PRNG's poolsize
 -- the factor-of-8 bug in init.d/urandom ought to get fixed
 -- the code in init.d/urandom that tries to set the poolsize
  should be removed.

@_date: 2010-08-02 22:47:34
@_author: John Denker 
@_subject: customizing Live CD images (was: urandom etc.) 
We have been discussing the importance of a unique random-seed
file each system.  This is important even forsystems that boot from read-only media such as CD.
To make this somewhat more practical, I have written a script
to remix a .iso image so as to add one or more last-minute files.
The leading application (but probably not the only application)
is adding random-seed files.
The script can be found at
  This version is literally two orders of magnitude more efficient than the rough pre-alpha version that I put up
yesterday ... and it solves a more general problem, insofar
as random-seed files are not the only things it can handle.
Early-boot software is outside my zone of comfort, let
alone expertise, so I reckon somebody who is friends with
Casper could make further improvements ... but at least for now this script serves as an "existence proof" to show that  a) the PRNG situation is not hopeless, even for read-only
   media; and
 b) it is possible to remix Live CD images automatically
   and somewhat efficiently.
I think by taking two steps we can achieve a worthwhile
improvement in security:
 -- each system should have its own unique random-seed
  file, with contents not known to the attackers; and
 -- the init.d/urandom script should seed the PRNG   using "date +%s.%N"  (as well as the random-seed file).
Neither step is worth nearly as much without the other,
but the two of them together seem quite worthwhile.

@_date: 2010-08-27 10:05:50
@_author: John Denker 
@_subject: questions about RNGs and FIPS 140 
I would have said something different:  *IF* you are
lucky, then /dev/random gets reseeded during run time.
Depending on details of the system, there is no guarantee
that /dev/random gets reseeded at all, much less reseeded
There exist lots of small and/or embedded and/or virtual Linux systems that have no useful sources of entropy.
The kernel attempts to collect entropy, but there are no
positive lower bounds on the effectiveness of the built-in
measures.  You could always add a source, but that is a topic for a whole new discussion.  For more on this, see

@_date: 2010-07-29 14:25:19
@_author: John Denker 
@_subject: Persisting /dev/random state across reboots 
Actually it typically copies from /dev/urandom not /dev/random,
but we agree, the basic idea is to save a seed for use at the
next boot-up.
Before we can answer that, we must have a brief "what's your threat model" discussion.  As always, I define "random" to mean "random enough for the application".  The demands vary wildly from application to application.  Interesting use cases
 a) low-grade randomness: For non-adversarial applications
  such as Monte Carlo integration of a physics problem,   almost any RNG will do.
 b) high-grade randomness: For high-stakes adversarial
  applications, including crypto and gaming, I wouldn't trust   /dev/urandom at all, and details of how it gets seeded are   just re-arranging the deck chairs on the Titanic.
A) For low-grade applications, procedure [1] is well suited.
It is clearly better than nothing, although it could be
A conspicuous weakness of procedure [1] is that gets skipped
if the machine goes down due to a software fault, or power
failure, or really anything other than an orderly shutdown.
Rather than writing the file at the last possible opportunity,
it would make at least as much sense to write it much earlier, perhaps immediately after boot-up.
B)  At the other extreme, for high-grade applications, /dev/random
is not (by itself) good enough, and asking how it gets seeded is just re-arranging deck chairs on the Titanic.
Tangential remark:  It would be nice if the random-seed file could be written in a way that did not deplete the amount of
entropy stored in /dev/random ... but given the existing linkage between /dev/random and /dev/urandom it cannot.  On the other hand, if you have reason to worry about this issue, you shouldn't be using /dev/urandom at all anyway.  Remember what von Neuman said about living in sin.
Tangential remark:  You could worry about how carefully we
need to read-protect the random-seed file (and all backups
thereof).  But again, if you are worried at that level of
detail, you shouldn't be using a PRNG anyway.  If it needs a seed, you are living in sin.
Constructive suggestion:  Use something like Turbid:
  i.e. something that generates a steady stream of honest-to-
goodness entropy.
If you are not sure whether you need Turbid, you ought to use it.  It's cheap insurance.  The cost of implementing Turbid is very small compared to the cost of proving you don't need it.

@_date: 2010-07-31 04:55:18
@_author: John Denker 
@_subject: init.d/urandom : saving random-seed 
Hi Henrique --
This is to answer the excellent questions you asked at
  Since that bug is now closed (as it should be), and since these
questions are only tangentially related to that bug anyway, I am emailing you directly.  Feel free to forward this as appropriate.
There is no possibility of making things worse.  It is like shuffling
a deck of cards:  If it is already shuffled, shuffling it some more
is provably harmless.
This property is a core design requirement of the PRNG, and has been
for ages.
Note that writing to /dev/random requires no privileges, which makes
sense in light of this property.
As is so often the case in the security / crypto business, the answer
depends on your threat model.  The demands placed on a PRNG vary wildly
from one application to another.  Interesting use cases include:
 a) low-grade randomness: For non-adversarial applications such as Monte
  Carlo integration of a physics problem, almost any PRNG will do.  Even
  a LFSR would do, even though a LFSR can easily be cryptanalyzed.  The
  point is that nobody is going to bother attempting the cryptanalysis.
 b) current /dev/urandom: The consensus among experts is that /dev/urandom
  is routinely used in ways for which it is not suited.  See my previous
  email, or refer to
     c) high-grade randomness: For high-stakes adversarial applications,   including crypto and gaming, you really ought to use a TRNG not a
  PRNG.  In this case, no state is required and no seed is required,   so the question of how to preserve the state across reboots does not
  arise.  Constructive suggestion:  for high-grade applications, use
  Turbid:    To repeat:  For serious applications, I wouldn't trust /dev/urandom at all, and details of how it gets seeded are mostly just re-arranging the deck chairs on the Titanic.  The question is not whether this-or-that seed
preservation policy is "safe".  The most you can ask of a seed preservation
policy is that the PRNG after reboot will be _not worse_ than it was before.
Now, to answer the question:  A random-seed file should never be reused.
Never ever.
Reusing the random-seed file makes the PRNG very much worse than it would
otherwise be.  By way of illustration, suppose you are using the computer
to help you play "battleship" or "go fish" against a ten-year-old opponent.
If you use the same 'random' numbers after every reboot, the opponent is
going to notice.  You are going to lose.  In more-demanding situations,
against an opponent with more skill and more motivation, you are going to
lose even more miserably.
While we are on the subject, let me point out a bug in all recent versions
of init.d/urandom (including the current "sid" version as included in
initscripts_2.88dsf-11_amd64.deb) :   The poolsize as reported by /proc/sys/kernel/random/poolsize has units   of _bits_ whereas the random-seed filesize has units of _bytes_.  It is   a bug to directly compare these numbers, or to set one of them based on   the other.  There needs to be a conversion factor, perhaps something like
  this:
      (( DD_BYTES = ($POOLSIZE + 7)/8 ))
Now, to answer the question:  It suffices to make the random-seed file contain the same number of bits as the PRNG's internal state vector ("poolsize").  Call this the BBJR size (baby-bear-just-right).
On the other hand, it is harmless to make the random-seed file larger than
it needs to be.
In contrast, using the size of the random-seed file to reset the PRNG's poolsize is a bad idea, especially if the random-seed file is (intentionally or otherwise) bigger or smaller than the BBJR size.
Semi-constructive pseudo-suggestion:  *IF* we want to keep track of the poolsize, it might make more sense to store it separately and explicitly, in its own file.  This would make the code simpler and more rational.
On the other hand, I'm not sure why there is *any* code in init.d/urandom
for saving or setting the poolsize.  Chez moi /proc/sys/kernel/random/poolsize
is read-only.  Indeed I would expect it to be read-only, since changing it would have drastic consequences for the internal operation of the PRNG, and looking at random.c I don't see any code to handle such a change.
So the real suggestion is to eliminate from the Linux init.d/urandom all of the code that tries to ascertain the size of the random-seed file and/or tries to set the poolsize.  (For non-Linux systems, the
situation may or may not be different.  I have no comment on that.)
An unseeded PRNG is unsuited for any purpose except possibly the lowest
of low-grade non-adversarial applications.  It is grossly unsuited for any security-related application.
The security literature contains many examples of serious attacks based
on figuring out the state of the victim's PRNG.
Sometimes /dev/urandom is advertised as a "cryptographically secure PRNG".
If that's what it is supposed to be, it should block or throw an error if
it is used before it is seeded.  To say the same thing the other way: if it is meant to be used as an unseeded PRNG, it should be renamed to If the poolsize is too small, all sorts of bad things happen.
The built-in poolsize is 512 bytes i.e. 4096 bits which is plenty big
enough for PRNG purposes.  This size is not based on the needs of where the incoming supply of entropy accumulates slowly but the outgoing
demand is subject to sudden peaks.
I recommend not messing with the built-in poolsize.
Seeding should happen as soon as possible.  Seriously, any PRNG that makes
any pretense of security should block or throw an error if it is used before it is seeded.
I don't see how early seeding makes problem (2) any worse.  Late seeding
causes all sorts of problems.
Seeding should happen
 -- after the random-seed file becomes readable, i.e. after the
  relevant filesystem is mounted.
 -- as soon thereafter as possible
Note that the filesystem can be readonly at this point.  Read/write is OK too.
Note that it should be considered an error to mount something else on top of /var, using /var as a mountpoint.  Ditto for the other directories in the path leading to random-seed, typically /var/lib/urandom/random-seed.  Such a mount is bad because it makes it impossible to update the relevant
version of random-seed (unless somebody carefully unmounts the offending filesystem first, and there's no code to do that).
Updating the random-seed file should happen during boot
 -- after the random-seed file becomes writeable
 -- as soon thereafter as possible
Updating should happen again during shutdown, if possible.
Yes, the size of the random-seed file should be set according to the poolsize
(but not vice-versa).
This is a non-problem.  Just use the poolsize to determine the file size.
This will never cause a problem for the PRNG.
At present the filesize should be 512 bytes i.e. 4096 bits.  This is likely
to remain so for the foreseeable future.  If the filesystem cannot handle a
file of this size, then we are talking about a highly specialized application,
requiring detailed engineering, quite outside the scope of the standard
Embedded systems, if they want to have any pretense of security, need to
have either:
 a) enough persistent local storage to hold the random-seed, or
 b) a hardware TRNG that collects real entropy, so that no seed is needed.
For systems that boot from read-only media, such as diagnostic and emergency
maintenance CDs, there is a serious problem, with no entirely satisfactory
solutions that I know of.  Suggestions would be welcome.

@_date: 2010-07-31 13:36:53
@_author: John Denker 
@_subject: init.d/urandom : saving random-seed 
Yes indeed!
You can get a high-quality TRNG for free from
  OK.  Similarly let's differentiate the behavior of /dev/random from the
behavior of /dev/urandom.  My previous note was almost entirely about
does not apply to /dev/urandom.
Furthermore we must differentiate the various kinds of "randomness".
Even among experts there is some divergence of opinion about what the
word "random" should mean.  When I use the word, the meaning is intentionally somewhat vague, namely "random enough for the application" ... whatever that means.  In contrast, the word "entropy" has a very specific quantitative meaning rooted in mathematics and physics.
This allows us to understand that the "random" bits coming from /dev/urandom
may or may not contain any entropy.  If your application requires real entropy, you should not be using /dev/urandom.  Whether or not the "random"
bits coming from /dev/urandom are good for your application depends greatly
on the application.
That is strictly true, but perhaps misleading in some abnormal situations.
If you reboot and then reseed the PRNG using the random-seed file, then:
  a) No damage is ever caused by the random-seed file, strictly speaking.
  b) The damage was caused by the _reboot_, which caused the PRNG to lose    state.
  c) The question is whether the random-seed file is sufficient to undo the
   damage caused by the reboot.  Using the file a second time does *not*    undo the damage, and leaves the system in a much worse state than it    was before the reboot.  A random-seed file that has been used must be
   considered severely compromised and unsuitable for re-use.
There is never a problem with "dilution".  Shuffling (even lame shuffling)
can never undo the effect of previous shuffling.
As discussed above, the real damage is caused by the reboot, not by the random-seed file.  The random-seed file has the power to repair
this damage ... once ... but not more than once.  If the PRNG is rebooted
and then the seed is reused, the PRNG will replay the same outputs, which
is very bad.  The motto is:
          A  _used_  seed       is a  _used-up_  seed.
That's irrelevant.  The reboot CAN and DOES completely reset the state
of the PRNG.  This doesn't come from userspace, but it does happen.
The notion of "credit" does not apply to /dev/urandom.
OK, you have highlighted a point that was not clear in my previous note.  There are several possibilities on the table:
  a) a PRNG that has not been seeded at all,   b) a PRNG that has re-used a previous seed,
  c) a PRNG that has properly used a seed for the first time, and
  d) a PRNG that has been initialized from a TRNG.
Obviously we would prefer (d).  The next-best choice is (c).  If in some emergency situation (c) is not possible, then (b) is bad and (a)
is even worse.  In some impractical sense (b) is not as bad as (a),
but still (b) is so bad that I cannot recommend it.
There is a tricky tradeoff that needs to be made here.  The question is,
do you want the system to reboot into a state where /dev/urandom is
seemingly functional but insecure, or a state where it is not functional
at all?  This is a nontrivial question.
For more discussion of this point, see below.
We face this tradeoff directly when we consider whether we believe that
it good to reseed early, the earlier the better.  For best security, we
should wait until the random-seed file is available read/write, so that we can avoid bad situation (b).    Note that if the filesystem is not writeable, there is no way for   initscripts to tell whether the random-seed file has been previously
  used, so there is no way of distinguishing the bad situation (b) from
  the good situation (c).
For typical "live CD" systems and other _attended_ systems where the random-seed file is not readable, it is probably best to wait until the kernel has collected some real entropy (from keyboard events etc.) and then use that to seed the PRNG.  And of course if the system has a good hardware TRNG then we should rely on that.
If you want a decent level of security, it seems advisable to require
that any unattended system should have a hardware TRNG.  This is what I
have specified whenever I have faced this question in the past.
We must avoid situation (a).  One way to do this would be to make sure
I do not understand the distinction between "reseeded" and "shuffled".
The essential requirement is to put the PRNG into a state where the
attackers cannot possibly know the state.  If you are using a deck of cards as your PRNG, then shuffling is how this is accomplished.  I don't care whether you call this reseeding or shuffling or whatever.
If you are contemplating putting the /dev/urandom PRNG into some kind
of _known_ state, that is not what we mean by reseeding.  We agree that there is no way that userspace software -- by itself -- can do this.  But a reboot can do it.
For present purposes, shuffling is a metaphor for reseeding, or an example of reseeding.  There is no important distinction.
The fundamental problem we are trying to solve is this:  A reboot put the PRNG into a known state, a state vulnerable to attack.  The purpose of reseeding aka shuffling is to get it out of that state into a state that is unknown to the attackers.
It is easy to calculate the size:
    if ! DD_BYTES=$((        $(cat /proc/sys/kernel/random/poolsize 2>/dev/null) + 7 >> 3
    )) ; then       DD_BYTES=512
    fi
    dd if=/dev/urandom of=/var/lib/urandom/random-seed \
      bs=1 count=$DD_BYTES 2>/dev/null
This requires dd and some standard sh features, nothing more.
Failing that, using a hard-coded size of 512 is acceptable.  This value
is specified in the comments in drivers/char/random.c so we can at least say we are following the instructions.
I see no advantage to using preferring 2048 bytes to the calculated and/or
documented size, not at shutdown time or otherwise.  This would just be extra
complexity.  It would cause people to ask why it was done, and we would not
have a good answer for them.
The fact that reading from /dev/urandom depletes the supply of entropy used
by /dev/random is a longstanding weakness of /dev/urandom.  It allows a
nonprivileged user to create a denial-of-service -- intentionally or even accidentally -- against /dev/random.  This should have been fixed years ago.
Note that Turbid provides a yarrow-like "stretched RNG" that does not have
this problem.
Again, I would prefer to see it calculated, but if for any reason it
cannot be calculated, a parameterized "512" should be good enough for
all practical purposes.
"Considering" it so does not make it actually so.
For a diskless client, there many not be any entropy from disk activity or
any other sources.  Also note that solid-state disks are becoming more
popular, and they do not produce entropy the same way rotating disks do.
Alas, that is not true.  Certainly not "always".  It is a notorious weakness of the Linux RNG system that in the worst case there is no positive lower bound on the amount of entropy available to the kernel.
Also note that "a few bits" of entropy are not much better than nothing, because the attacker can just check all the possibilities.
Exponentials are tricky.  Exp(large number) is very large indeed, but exp(small number) is not large at all.
But not initialized in a securely usable way.  It will be usable only
for the lowest of low-grade applications.  It will not be safe to use
for any security-related applications.
Nobody is recommending any such increase.
That's irrelevant.  First of all, the concept of overestimation does not
apply to /dev/urandom.  (It might sometimes apply to /dev/random, but that
is not the topic of today's conversation.  And messing with the poolsize
is not a valid way of preventing overestimation.)
We agree that people would not accept such a change in the kernel.
Usually "get a real TRNG" is the only sensible answer.  If that is
not the answer, then pool-swapping is not the answer either.  I cannot imagine any scenario in which pool-swapping is the answer.
Good point.  I stand corrected.  I am no longer confident that "earlier
is better".  I am no longer confident that readable (but not writeable)
is acceptable.
There are nasty tradeoffs involved here.   -- Reseeding as early as possible is clearly better in the normal case,
  but I cannot in good conscience recommend an "earlier is better" policy
  since it causes security failures in abnormal cases.  We have to take
  this seriously because these cases are not rare.  We have to take this
  doubly seriously because an attacker could actively _cause_ such cases
  by forcing repeated reboots.
 -- It could be asked whether reseeding badly is better than not reseeding
  at all, but this is a question from hell.  The one case is like leaving
  the front door of your house standing open, and the other case is like
  leaving it seemingly closed but unlocked.  It is a security problem either
  way.  I cannot endorse either option.
Therefore, as a starting point for further discussion, I would propose
 *) /dev/urandom should block or throw an error if it is used before
  it is properly seeded.
 *) if a random-seed file is needed at all, reseeding should wait until   the file is readable and writable, since this is the only way AFAICT   that we can ensure that it will not be reused.
I leave this as a topic for discussion.  We foresee the possibility
that under some abnormal (but not rare) conditions, the system will come up in a state where /dev/urandom is unusable, because the filesystem holding the random-seed is not writable.  This is not ideal
but AFAICT it is preferable to the alternatives.  If anybody can think
of cases where this is not acceptable, please speak up.
Note that in the most common case, namely a single-user "maintenance"
shell provoked by fsck problems, a great many subsystems are offline,
and adding the /dev/urandom to the list of unusable subsystems would
not be completely out of character.
There are strong arguments for _not_ putting the random-seed in /etc
or /lib.  There are lots of systems out there which for security reasons and/or performance reasons have /etc and /lib on permanently
readonly partitions.
I think /var is as good a place as any.  More generally, if a random-seed
file is needed at all, it needs to be on a partition with the following
 -- local
 -- persistent
 -- readable and writable
 -- mounted read/write early enough so that /dev/urandom can be reseeded
  early enough.  That is, before any outputs from /dev/urandom are needed.
On a system with a good TRNG, the random-seed file is not needed at all.
Or use Turbid.  Most mainboards these days include enough built-in
audio to provide a source of real industrial-strength entropy, so
you don't need to spend even one cent on additional hardware, USB
or otherwise.
If by chance the board lacks an audio system, you can get USB audio devices for $5.00 or less, and connect Turbid to that.
Failing that, you could plug in a USB memory stick, and use that as a convenient place to store the random-seed file.  This is way better than nothing, but obviously not as good as a TRNG.
Yes, that is a reasonable tactic.  I've done similar things in the
past. Network entropy boxes give rise to an interesting (but manageable) chicken-and-egg problem:  If the client has enough stored randomness to
get started, it can establish a secure network connection and use that
to acquire more randomness from an entropy source box.  In contrast,
if the client starts out with no randomness, it cannot obtain any,
because it has no way of setting up a secure connection.  This applies
equally to a hypervisor and/or to its hosted VMs.  The design considerations
are the same either way.

@_date: 2010-10-08 19:14:12
@_author: John Denker 
@_subject: Disk encryption advice... 
============================== START ==============================
1) Thanks for being explicit about the threat model and objectives.
 As is so often the case, what the client wants is probably not
 exactly what the client is asking for.
2) In this case, I reckon the client would be content to encrypt
 _everything of value_ on the drive ... even if this is not quite
 the entire drive.
 In particular: On every normal boot, the machine boots into a
 preliminary kernel that uses key A to request key B over the
 network.  Key B is the disk key, which then gets stored in some  guaranteed-volatile place that will survive a chain-boot but
 not survive anything else.  Then the pre-kernel chain-boots
 Windows.
 To be clear:  The entire Windows partition is encrypted, but  the pre-kernel lives in a tiny partition of its own that is not
 encrypted.
 If the machine is stolen, it is immediately harmless because it  is no longer connected to the network.  If the machine is to be
 disposed of *or* if theft is detected or suspected, then the  keyserver that hands out disk-keys will stop serving the key  for that machine, so even if it is reconnected to the network  somehow it is still harmless.  For icing on the cake, the keyserver
 can check IP addresses, virtual circuits, etc. ... to check that  a machine that is supposed to be on the secure wired network has  not suddenly relocated to the insecure wireless network that serves
 the lobby and leaks out into the parking lot.
 If the network is down and somebody wants to boot his machine
 anyway, he can type in the key by hand.
3) The same effect can be achieved using a hypervisor / VM approach,
 rather than chain-booting.  The same effect can be achieved by
 messing with grub, although that would be more work.  The same
 effect can be achieved by messing with coreboot, but that would
 be even more work.
4) If the customer absolutely insists that "the entire Windows drive"
 be encrypted, just add another drive, perhaps a very small flash drive.

@_date: 2010-09-06 19:49:10
@_author: John Denker 
@_subject: Randomness, Quantum Mechanics - and Cryptography 
It's easy to pin down.  If it's unpredictable to the attacker,
it's unpredictable enough for all practical purposes.
You're working too hard.  QM is interesting, but it is overkill
for cryptography.  Plain old classical thermodynamical randomness
is plenty random enough.
FWIW, quantum noise is just the limiting case of thermal noise in
the limit of high frequency and/or low temperature.  There is no
dividing line between the two, by which I mean that the full range
of intermediate cases exists, and the same equation describes both
asymptotes and everything in between.  A graph of noise versus temperature for a simple circuit can be found at
  If anybody can think of a practical attack against the randomness
of a thermal noise source, please let us know.  By "practical" I
mean to exclude attacks that use such stupendous resources that
it would be far easier to attack other elements of the system.

@_date: 2010-09-07 10:58:59
@_author: John Denker 
@_subject: Randomness, Quantum Mechanics - and Cryptography 
1) This is not an argument in favor of quantum noise over
thermal noise, because the same attack would be at least
as effective against quantum noise.
2) You can shield things so as to make this attack very,
very difficult.
3) The attack is detectable long before it is effective,
whereupon you can shut down the RNG, so it is at best a
DoS attack.  And then you have to compare it against
other brute-force DoS attacks, such as shooting the
computer with an AK-47.
Even the cheapest of consumer-grade converters has 16 bits of
resolution, which is enough to resolve the thermal noise and
still have _two or three orders of magnitude_ of headroom.  If
you are really worried about this, studio-grade stuff is still
quite affordable, and has even more headroom and better shielding.
How much RF are we talking about here?  At some point you can
undoubtedly DoS the RNG ... but I suspect the same amount of
RF would fry most of the computers, phones, and ipods in the Is the RF attack in any way preferable to the AK-47 attack?

@_date: 2010-09-07 11:56:25
@_author: John Denker 
@_subject: Randomness, Quantum Mechanics - and Cryptography 
We should take the belt-and-suspenders approach:
 a) Do some reasonable amount of shielding, and
 b) detect the attack.
Detecting the attack is utterly straightforward.  The primary defense is to "close the loop" around the noise-generating element.  That is, we inject a known calibration signal on top of the noise ... and use that to constantly check that the input
channel gain and bandwidth are correct.
The true noise level depends only on gain, bandwidth,
temperature, and resistance.  Blasting the system
with RF will not lower the temperature, so that's
not a threat.  So unless you have a scenario where
the RF lowers the resistance, lowers the gain,
and/or lowers the bandwidth _in a way that the calibrator cannot detect_
then this attack does not rise above the level of
a brute-force DoS attack, in the same category as the AK-47 attack or the stomp-the-smart-card-to-dust
The calibrator idea relies on the fact that the
computer's i/o system has an o as well as an i.
Note that this defense is equally effective against
 *) Continuing attacks, where a continuing RF blast
  drives the first stage amplifier into saturation,   without necessarily doing irreversible damage, and
 *) One-shot attacks, where a super-large blast does
  irreversible damage to the amplifier.
Secondary defenses, if you want to go to the trouble,
include putting a canary in the coal mine, i.e.
implementing a second sensor with a different gain,
bandwidth, and resistance.  I reckon that attacking
one sensor and getting away with it is only possible
on a set of measure zero, but the chance of attacking
two non-identical sensors without either one of them noticing is a set of measure zero squared.

@_date: 2010-09-10 11:05:10
@_author: John Denker 
@_subject: customizing Live CD images 
I just now put up new-and-improved versions of these
software tools, and a web page summarizing the situation.
   *) Worst-case customization time:  about a minute on my laptop.
 *) Best case:  Less than 1 second.
 *) The best case is the typical case, if you are making many disks.
 *) The checksums in md5sum.txt are properly recomputed.
 *) Better usage messages, better comments.
 *) The supporting tools are now more robust and more generally useful:
    -- Hacking ISO 9660 images
    -- Hacking md5sum lists.
Comments and suggestions are welcome.

@_date: 2013-12-22 17:35:33
@_author: John Denker 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
That's diametrically the wrong way to think about it.
Every cryptographer knows about "cut and choose".  You don't
do both at the same time.
By the same token, the HRNG isn't used for production at the
same time it's being audited.  Just because they aren't done
at the same time doesn't mean they can't be done.
Moving now from analogy to technical detail:  Entropy is a property of the distribution, not of any particular data point drawn from the distribution.  This is not a deep concept;  the same goes for the mean, standard deviation, and innumerable other statistical properties:  they are properties of the ensemble.
In other words, there is no such thing as a random number.
You can have a random distribution over numbers, but then
the randomness is in the distribution, not in any particular
number drawn from the distribution.
Therefore one does not check to see whether the HRNG is
producing the so-called "correct" so-called "random numbers";
rather one audits the mechanism.  One audits the mechanism
by which the random distribution is produced.
  Very little of the code that ships with turbid is involved
  in producing the randomly-distributed output.  Most of it
  is for calibrating the mechanism.
As H.E. Fosdick put it:  Person saying it cannot be done is
liable to be interrupted by person doing it.
Turbid can be audited.  It is an interdisciplinary exercise,
requiring skill in electronics, computer science, cryptography,
and physics ... so if you walk into the National Zoo and ask
the first primate you find, he or she probably won't know how
to do it.  On the other hand, you should be able to find a
person -- or put together a small team -- with the requisite skills.  Compared to validating a new cryptologic primitive from scratch, it is trivial.  That's because it uses existing primitives in prosaic ways.
If anybody disagrees, please tell us what part of turbid cannot be audited.  Please be as specific as you can.

@_date: 2013-06-28 23:20:59
@_author: John Denker 
@_subject: [Cryptography] Snowden "fabricated digital keys" to get access 
Here's one hypothesis to consider.
 a) The so-called "digital key" was not any sort of decryption key.
 b) The files were available on the NSA machines in the clear.
 c) The files were protected only by something like the Unix file
  protection mechanism ... or the SELinux Mandatory Access Controls.
 d) The "digital key" might have been not much more than a userID
  and password, plus maybe a dongle, allowing him to log in as a   shadow member of some group that was supposed to have access to   the files.
Crypto is great for protecting stuff while it is being transmitted
or being stored offline ... but when the stuff is in active use, the temptation is to make a cleartext working copy.  Then anybody
who can attach a thumb drive and can get past the access controls
can grab whatever he wants.
It is against NSA policy to attach a thumb drive.  I betcha some
folks really want to know how he did that without getting caught.

@_date: 2013-11-01 06:25:57
@_author: John Denker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
I understand the question.  It's a perfectly fine question
... but it may not be answerable.  There is an important
category of stuff I call "squish" ... namely stuff that is
neither reliably predictable nor reliably unpredictable.
I suspect timing information is in this category.
Also note that good engineering -- and in particular engineering management -- requires looking not only at the strengths and
weaknesses of plan A, but also at the strengths and weaknesses
of all the plausible alternatives.
So, any analysis that artificially restricts attention to just network timing will never be a serious, professional-grade analysis.  If you ask the question too narrowly, the answer is guaranteed to be "no" ... for several reasons:
 -- Even if it works in a datacenter, network timing doesn't   work for a handheld device that powers up with no network   connectivity at all.
 -- In a datacenter or elsewhere, network timing will have a   very hard time competing with easier and better solutions,   such as a properly provisioned seed.  Let's be clear:  Even   if you could get it to work in some technical sense, it would   not be competitive.  It would not be worth bothering with,   because there are easier and better solutions.
Security generally requires a minimax approach, which is why
squish is not useful.  Squish would be great in a best-case
scenario, but there's no point in designing something on that
Really?  If you shop around you can find off-the-shelf server-class mainboards with built-in audio.  There are plenty that don't have
audio, but some that do ... and if the demand went up the supply
would go up pretty fast.  In the worst case, you can add a USB audio dongle for about $5.00.
They're hopeless if they have to rely on network timing, but not hopeless in general.
There are lots of ways that a host machine can provide randomness to its guests.  virtio-rng among many others.  It takes a bit of engineering and a bit of fussing to get this set up (both host and guest) and it helps if the host has a
copious supply of randomly-distributed numbers to draw on ... but if you have thousands of VMs the incremental cost per VM is essentially zero.  Certainly it's small compared to the cost of cleaning up after a security breach, which is the all-too-likely alternative.

@_date: 2013-11-01 13:45:06
@_author: John Denker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Alas, that leaves important parts of the problem unsolved.  We
cannot "forget it" until we solve the whole problem.
For example:  SSH has to cut host keys when it is first used (if not before).  This requires a lot of high-quality randomly-
distributed bits.  There are a gazillion scenarios where this has to happen /before/ the first DHCP happens.  For example, I might need to "ssh root at localhost" in order to configure DHCP.
Rather than writing repetitious email, I put together a little
document on the care and feeding of a secure PRNG, including proper provisioning:
  This is a first draft.  It is a work in progress.  Comments and
suggestions are welcome.

@_date: 2013-11-01 17:00:09
@_author: John Denker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
 Here's a hypothetical scenario for you to consider.:
 Some guy dresses up in sackcloth and ashes and goes to the SSH  developers and says:
   (a) We've done everything humanly possible, and there's just      no way the RNG can produce stuff good enough for cutting
     SSH host keys.
   (b) Therefore you need to treat "ssh localhost" as a special case.
   (c) This is a bug in SSH.
 At this point the SSH guys say    "We'd love to help you out.  Which way did you come in?"
  And then our guy goes around to the developers of every other
  app that uses random numbers.  And also to the kernel developers,
  since the kernel is a major consumer of PRNG bits ... some of   which go for trivial purposes, but some for utterly nontrivial
  purposes.
If *you* want to take that approach, be my guest ... but I'm not
gonna do it.  It's not my style to blame the user.  Also, I don't
believe a single thing (a) through (c) that the guy said.
That would hurt a lot.  I would consider it a crock.  It requires SSH to know waaay too much about the PRNG.  Throwing away host keys also means selectively editing who-knows-how-many known_host files.
Most of all, SSH is merely one representative of a large class of applications that use PRNG bits.  Messing with the applications is the hard way to solve the problem.
It would be far easier to construct a PRNG that just ... ahem ...
puts out randomly-distributed numbers.  Easier to build, easier to test, easier to trust, easier to document, easier to use, easier to maintain ......

@_date: 2013-11-02 16:36:13
@_author: John Denker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
As one possible answer to the question in the Subject: line of this thread:  The  all-time most-plausible method for
attacking a PRNG starts by finding out how badly initialized the thing is.
Some actual observed facts:
                                   prior
     startup script                   ---------------------           -----
   (mountall)                      18816
   (mounted-run)                   21888
   (sshd server)                   35616
   (network-interface : lo)        55968
   (network-interface : eth0)      68832
   (urandom)                       79168
In the left column, we have the description of a startup script,
as observed on an ordinary Linux system.  In the rightmost column we have the number of bits extracted from the kernel PRNG before said script gets invoked.
These are "typical" numbers, as discussed below.
1) The script that claims to be in charge of initializing the kernel PRNG is getting called late in the game.  This definitely a problem.
2) In general, it is not sufficient to initialize the PRNG from a external web server, DHCP server, or anything like that. That?s because network connections are likely to become available too late in the boot-up process. As you can see from the numbers tabulated above, a *lot* of water has flowed under the bridge before the network devices come up, and IMHO it would be a Bad Idea to assume (or require) that nobody can do anything random until this-or-that network device come up, let alone completed the DHCP process.
 -- The device might have a fixed address, or some other reason
  for not doing DHCP at all.
 -- The device might be a USB dongle that gets hotplugged long
  after the system has come up, if at all.
 -- etc. etc. etc.
It could be argued that it is "sometimes" OK for everybody to wait,
but that argument doesn't cut it.  The shoe is on the other foot.
Security requires evidence that it is /always/ OK, but this
remains unproven and IMHO implausible.
The upstart process is asynchronous, for good reason.  As a
consequence, the numbers tabulated above will exhibit some
variation.  Indeed, even the order in which things come up is not 100% reproducible.  However, it is not necessary for me to prove that the DHCP idea always fails.  That shoe is still on the other foot.  If anybody wants to "implement it, standardize it and forget it" they should please provide evidence that it always works.  The tabulated numbers suffice to prove that unless sweeping changes are made, it doesn't always work.
Of course the stuff deployed in the current Linux distros is also
several sandwiches short of a picnic.
The problem is entirely fixable, but it's a lot harder than most
people think.  There are a thousand ideas that look like quick
fixes at first glance, but do not withstand serious scrutiny.
I still think that proper care and feeding of a /seed file/ is
the way to go.  As you can see from the tabulated numbers, it
will be a challenge to get the file loaded early enough.

@_date: 2013-11-03 17:56:14
@_author: John Denker 
@_subject: [Cryptography] initializing kernel PRNG much much sooner on 
That's innnnnteresting.
In the current Ubuntu distro (raring),
  a) there is no factor of "urandom" in the upstart init/ssh.conf, and
  b) simply adding such a factor doesn't suffice, because nobody is
   emitting any such event, because
  c) the sysv init.d/urandom script hasn't been ported to upstart.
  d) There is at least one open bug on the subject.            The consensus seems to be     "we do want to translate /etc/init.d/urandom to an upstart job"
   but the guys are
    "frankly not sure at present how to write it correctly"
I took a stab at translating the thing.
  This is first-draft code that has been thought about for maybe 5 minutes
total, but it's better than nothing.  It seeds the PRNG much, much sooner.
It makes the ssh server dependent on the "urandom" event (although this
is now in the category of belt-and-suspenders).
I am under no illusions that the seed file is getting loaded early /enough/
in absolute terms.  It is, however, a whole lot earlier in relative terms.
The new data is tabulated here, along with more discussion:
   Comments?  Suggestions?  Better ideas?

@_date: 2013-11-04 12:21:00
@_author: John Denker 
@_subject: [Cryptography] randomness +- entropy 
Hash: SHA1
Hi Folks --
Some people have been throwing around the word "entropy" rather carelessly.
Entropy means something very special.
For a great many cryptological purposes, a high-quality PSEUDO-random distribution is good enough, even though its entropy density is very low.  Note the contrast:
  TRNG entropy density = 1 - epsilon
  PRNG entropy density =   epsilon
As another way of emphasizing the distinction:  a PRNG places orders-of-magnitude harsher demands on the strength of the cryptological primitives it uses.  This can be quantified in terms of classical cryptologic ideas such as unicity distance, but for present purposes I prefer the "entropy density" language. The rubber meets the road here:  Consider the contrast:
PRNG:  I am quite sure that on startup the machine needs to   have on board a crypographically strong, well-seeded PRNG.
  This needs to be up and running very, very early in the   boot-up process.  Some things that need the PRNG cannot
  wait.
TRNG:  At the moment I have no firm opinions as to how much   actual entropy the machine needs on start-up.  I look   forward to having a discussion on this topic, with use-case   scenarios et cetera.
  In particular, AFAICT it is not a settled question as   to whether the things that need a TRNG can wait, or how
  long they can wait.
Both of these are solvable problems.  They are not, however,
the same problem.    *) A reservoir of true-randomly distributed bits would,    as an immediate corollary, provide a seed that solves    the PRNG problem.
  *) The converse is spectacularly not true.
FWIW note that current Linux distros make no attempt to
provide a reservoir of true-randomly distributed bits for
use at the next startup.  There are some efforts toward storing a seed for the kernel PRNG, but the stored seed is itself pseudo-randomly generated, and the kernel correctly
attributes zero entropy to it.
Even more tangential remark:  Note that even if there were
a reservoir of true-randomly distributed bits, AFAICT ssh
would not use them.  Openssh is built on top of openssl,
which has its own internal PRNG, which it prefers to seed
using the kernel PRNG via /dev/urandom AFAICT.  I refuse
to get too excited about this, because obviously this is
not set in stone.  There is an engineering principle that
says we should "aim for the moving target" which in this
case means providing services to support the way apps /should/
work, even if some of them don't presently work that way.
By way of contrast, gnupg seems to be good about insisting
on true-randomly distributed bits for cutting its keys.
Bottom line:
  -- If you mean "randomness" please say "randomness"
  -- If you say "entropy", please be sure you really mean it.
  -- Please do not use "entropy" as a misnomer for "randomness",
   or even for "cryptologically strong randomness".

@_date: 2013-11-05 09:51:50
@_author: John Denker 
@_subject: [Cryptography] randomness +- entropy 
Are we not having a technical discussion?  If not, then what?
The physics entropy and the information-theory entropy are the
same thing.  This is not a mere "similarity in formalisms".
For example, it is a one-line calculation to find the entropy
of the nuclear spins in a sample of copper, starting from
statistical principles, namely
     S = R ln 4
and you can also measure S using a physical thermometer.
Mirabile dictu, you get the same answer either way.
Nevermind the word, ideas are what's important.  Terminology is
important only insofar as it helps formulate and communicate the
The /idea/ of entropy has tremendous significance.  If we didn't
call it "entropy" we would need to invent another name for it.  So
please let's save everybody a lot of trouble and call it "entropy".
Yes, "tied to" ... but that's not the same as "same as".  Simple
 -- If you mean "unpredictability", say "unpredictability".
 -- If you mean "entropy", say "entropy".
We have perfectly good words for each of these things.
Speak for yourself, Kemosabe.  As for me, I know of a systematic method for estimating the entropy, based on a series of intelligent guesses.  The method was described by some guy named Shannon, several decades ago.  It is consistent with everything else we know about Successful cryptography often depends on depth of understanding and
attention to detail.  The loosey-goosey approach is very likely to
get you into trouble.
The same idea applies to various other fields of endeavor
  There *is* a difference between a TRNG and a cryptographically-strong
  A) In /some/ situations, the difference doesn't matter very much.
  B) In other situations, it matters a great deal.
For starters,   A) It possible to imagine a TRNG independent of any PRNG.
  B) It is not possible to imagine a PRNG completely independent of
   any TRNG, because the PRNG needs a seed, and the seed has to
   come from somewhere.
Here's another contrast:
   A) It may be that under /normal/ operating conditions, the /user/
    does not care about TRNG versus PRNG.
   B) Anybody who is /designing/ any such thing needs to understand
    the distinction.  In particular, the /recovery from compromise/     is wildly different in the two cases.
Here's a highly-condensed tutorial:
Suppose we have N-bit codewords, and various statistical distributions
over codewords.
Entropy is a property of the ensemble (not of any particular codeword).
 1) If the ensemble consists of all 2^N possible codewords, evenly   distributed, the distribution has N bits of entropy.
 2) Now consider a different distribution.  If half the codewords are
  missing, and the rest are evenly distributed, the distribution has
  N-1 bits of entropy.
 2a) In the sub-case where you know exactly which codewords are missing,
  this distribution is noticeably less random, less predictable than
  distribution (1).
 2b) In the sub-case where it is computationally infeasible to figure
  out which codewords are missing, this distribution may be -- for a
  wide range of practical purposes -- just as unpredictable as
  distribution (1).  However, it still has less entropy.
 3) For a typical real-world PRNG, we are not talking about one bit of
  entropy going missing.  Almost all of the bits have gone missing!
  If we seed the PRNG with 100 bits and then use it to generate a
  billion bits, then there are 2^999999900 missing codes out of a
  possible 2^1000000000.  That's a lot of missing codes.  Well over
  99.99% of the codes are missing.
  With a TRNG, the situation is reversed.  Ideally there would be no
  missing codes whatsoever.  However, for reasons of computational
  efficiency, a practical TRNG will typically allow a few -- a very
  few -- codes to be missing or under-represented in the ensemble.
  The contrast is extreme:
   A) The cryptographic strength required to hide the missing codes
    when almost all codes are missing, versus
   B) The cryptographic strength required to hide the under-represented
    codes, when there are very few of them.
And on top of that, there is the issue or recovery from compromise.
There is an important idea here.  If you are not going to call this
idea "entropy", you need to come up with a different name for it.
Bear in mind that practically everybody in the world reserves the
word "entropy" to denote the genuine physical / statistical entropy.
Also note that we have perfectly good words like "randomness" and
"unpredictability" to cover the other cases.

@_date: 2013-11-05 11:57:24
@_author: John Denker 
@_subject: [Cryptography] /dev/random is not robust 
I wouldn't have said that.
Statement [1] is true enough as it stands ... but it's looking
through the wrong end of the telescope.  We should instead turn
it around.  Consider the contrapositive:
  Given that we don't want to be completely screwed, we MUST
  ensure that the device has enough randomness onboard, so   that it can generate secure session keys.
  This is an entirely /solvable/ chicken-and-egg problem.
  Proper provisioning is a big part of the solution.  As
  soon as you can establish a secure connection, you can
  download tons of exogenous randomness.
That is not the "only" way to solve the main problem(s).  There must be a hardware RNG somewhere in the mix, and I have devoted a lot of time and effort to this part of the problem.  However, the fact remains that a properly seeded cryptographically strong PRNG is good enough for many purposes, and indeed superior for many purposes, especially given that the demand for randomly-distributed bits tends to be very bursty.
Increasing the number of critical failures is not a good
way to win friends and influence people.  -- Converting an insidious critical failure to a manifest
  critical failure is a small step in the right direction;
  however ...
 -- The real goal should be to eliminate the failure mode
  entirely.  This appears to be entirely doable.
Those are wise words.
 -- There are a couple of different common cases that
  require slightly different answers.
 -- There are a couple of unusual cases that require
  some extra effort, but are still entirely doable.
 -- I'm sure there are cases that cannot be handled.  It
  will always be possible to design a device that cannot
  be secured, because fools are so ingenious.  However,
  this is not the typical case.  Not even close.
There are huge categories of devices out there that could
be made much more secure at essentially zero incremental
cost, and that should be the first place we focus our
Exactly so.  Those are important cases.  Along the same lines, it should go without saying that proper provisioning of NON-virtual machines is also required.
Meanwhile, as another part of the overall solution, a good
hardware RNG is also required.  This does not, however need
to exist on every machine, and it does not need to meet every
demand for randomly-distributed bits.
Note the contrast:  As a rule of thumb:
  A) Seed-files and PRNGs tend to be good for meeting    short-term peak demand.
  B) There is still a long-term need for a HRNG.  A PRNG
   cannot exist without a HRNG backing it up.  On the
   other hand, the coupling between the two does not
   need to be particularly tight.

@_date: 2013-11-06 12:40:28
@_author: John Denker 
@_subject: [Cryptography] randomness +- entropy 
I have a /constructive/ suggestion, followed by a /specific/
constructive suggestion.  But first, some background:
We have seen that even the /unfounded/ rumor that the PRNG
might block causes users to shun the PRNG, substituting
something that is no better and almost certainly worse.
If the rumor were true, the user behavior would be even
Here's a suggestion:
--> Make sure the kernel PRNG is initialized very, very early.
 *) Then the question of whether or not to block does not arise.
 *) Then the users have no temptation to evade the block.
In more detail:  What we have now is an insidious failure, i.e. a PRNG that sometimes provides an insufficiently-random distribution
of bits.  This is a problem.  This is a Bad Thing.  The main goal should be to fix the problem ... and to fix it such a way that it Some applications cannot afford to wait.  For these applications,
if the PRNG blocks, we have converted an insidious failure into
a manifest failure.  It could be argued that this is a step in
the right direction ... but it is only a /small/ step, and it
does not really fix the problem.  It could be argued in political
terms that this would make users so angry that they would demand a real fix, but creating demand for a fix without actually providing
a fix is bad politics, bad marketing, and bad engineering.
So this brings us back to the main point:  The main goal should be to implement a PRNG that is up and running very, very early.
In engineering-management terms:  this is in the critical path.
  By way of contrast, blocking is not in the critical path,
  because even if you implement blocking, you *still* need
  to fix the problem.
Here is a specific suggestion for how the problem could be --> Incorporate the stored seed into the kernel boot image  (zImage or bzImage).  That ensures that the PRNG is "born"  ready to go, just as a newborn dolphin knows how to swim,  and a newly-hatched rattlesnake is already venomous.
I have looked into this a little bit.  Although I don't yet understand all the details, it looks like there is a
straightforward path.
Actually there are two complementary requirements:
 -- the stored seed must be available very, very early
 -- it should not be unduly difficult to refresh the
  stored seed from time to time.
Refer to the following for a simplified view of the structure
of the boot image:
    Actually things are somewhat more complicated than item
12 in the faqs.org document suggests.  The actual lines
from the relevant x86 Makefile include:
VMLINUX_OBJS = $(obj)/vmlinux.lds $(obj)/head_$(BITS).o $(obj)/misc.o \
        $(obj)/string.o $(obj)/cmdline.o $(obj)/early_serial_console.o \
        $(obj)/piggy.o
$(obj)/vmlinux: $(VMLINUX_OBJS) FORCE
        $(call if_changed,ld)
(and even that is a simplification).  Unless I am missing
something, it should be No Big Deal to add $(obj)/urandom-seed.o
to the list, somewhere ahead of $(obj)/piggy.o.
Next step:  It should be straightforward to write a tool
that efficiently updates the stored seed within the boot
image.  Updating MUST occur during provisioning, before
the device gets booted for the first time ... and also
from time to time thereafter.  Updating the boot image
isn't be quite as simple as   dd of=/var/lib/urandom/random-seed
but neither is it rocket surgery.  The cost is utterly
negligible compared to the cost of a security breach, which is the relevant comparison.
I have considered a *lot* of alternatives.  This is the
one that gives the most bang for the buck.
If we do this, many of the issues that we have recently
been discussing just melt and disappear.
Sometimes the system is booted from read-only media.
This must be handled as a special case.  In the case
of a "live CD" or "install CD" I recommend passing a
seed via the kernel boot cmdline.  We could teach
grub to demand a key from the user.
  If the machine is air-gapped and will not have
  any persistent consequences, then this step can
  be skipped.
Suppose we have something that boots from read-only media

@_date: 2013-11-06 18:11:01
@_author: John Denker 
@_subject: [Cryptography] suggestions for very very early initialization 
That's the right general idea, but things are half a step trickier than that.
We agree there are steps that need to be taken at the time the "distribution" is shipped, but that's not the whole story.  A
great deal of responsibility falls on device manufacturers who must /provision/ a unique seed into each instance of the device, even though the "distribution" is the same across all devices.
Similarly, a VM operator much /provision/ each VM instance.
The folks who put together the "distribution" need to provide the
tools to make it easy to do the provisioning properly, but they
cannot do the whole job.  --> Provisioning is a reeaaaally big deal.
Again, that's the right general idea, but there are a few devilish details that must be attended to.
We need to think separately about
  a) Initializing the PRNG using the stored seed, and
  b) refreshing the stored seed.
Separation is necessary, because part (a) needs to be done so early that in all likelihood, the disk is still mounted read-only.  Part (b) absolutely needs to be done, but it will have to wait.
Now, this opens up a weird attack, if the attacker can force the
machine to reboot after it has started using the PRNG but before
it can refresh the stored seed.  To defend against this, it pays
to /stir/ the seed using the RTC.  The RTC contains no entropy,
and we assume the attacker knows what time it is, so the RTC
as the source of non-repeating values, different from boot to
boot, and when *combined* with a stored seed it can produce a different set of randomly-distributed numbers on each boot.
A non-volatile counter would work just as well, but the RTC
is nice because it is almost-universally available.  All this
assumes the stored seed is large enough, random enough, and secret enough,
Perhaps that meant to say /dev/random flag, rather than urandom?
If we take /dev/urandom literally, I don't understand the suggestion.
The /dev/urandom ready flag will be raised as soon as the PRNG is seeded, which under my proposal will happen "at birth" ... so there is nothing to wait for.  The newborn dophin knows how to swim, and the newly-hatched rattlesnake is already venomous.
We agree that it is HIGHLY desirable for the PRNG to eventually
recover from compromise, but as I understand it, that happens
when the PRNG is re-seeded from the HRNG.  Asking the PRNG to
reseed itself doesn't make sense to me.  I don't see how that
ever recovers from compromise.
As I understand it, a properly-seeded PRNG is great for meeting high demands, including sudden high peak demands.  The PRNG
cannot exist without a HRNG somewhere in the mix, to provide
the initial seed and to provide for recovery from compromise.
However, the coupling between the PRNG and the HRNG does not
need to be particularly close.
I've been gradually collecting a summary of the situation at
   Yup.  Storing the seed in the boot image requires teaching the checksum routine to skip the seed section.  This requires work,
but it is entirely doable work.
Almost every "read-only" system I've ever seen has some non-volatile memory somewhere.  So ... it boils down to a configuration problem, i.e. teaching the system how
to store the required seed in the right place ... or how
to get the hypervisor to provide a seed, or whatever.
There will not be a one-size-fits-all solution for these
"read-only" devices.
I see no reason why updating the software on such a system would "make it fail".  The worst that will happen is that some init script will try to refresh the seed and will be unable to do so.  This will leave the system insecure, i.e. just as insecure as it already is!

@_date: 2013-11-06 23:16:13
@_author: John Denker 
@_subject: [Cryptography] suggestions for very very early initialization of 
That's too contrived to hold my interest.  Here's why:
In most cases, the best advice is this:
        If you feel the urge to use
        read-only media and nothing else,
        lie down until the feeling goes away. In the vast majority of cases, anything the small business owner
could do with a "Live CD" could be done more conveniently ? and much more securely ? using a USB flash drive.  You can still boot from a read-only partition if you choose, while still having a read/write partition for storing seeds and other stuff that should persist from one boot to the next.
You should also consider running a ?host? system that in turn boots a ?guest? system in snapshot mode. The guest system has all the convenience of a read/write filesystem, together with the security of knowing that the image goes back to its previous state on the next reboot. (The host provides the randomness needed for seeding the PRNG and for other purposes.)
A further advantage is that the guest can be booted in non-snapshot mode on special occasions, for instance to install high-priority security-related software updates. That?s tough to do on read-only            This assumes the Bad Guys have not already pwned
           the signing keys used to distribute updates........
Compared to trying to solve the problem within the constraints of
a CD-only approach, the flash and/or VM solutions seem easier and in every way better.
I just now incorporated this point into my screed:

@_date: 2013-11-08 12:23:57
@_author: John Denker 
@_subject: [Cryptography] randomness +- entropy 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
That inequality is true and useful and well said.
In a rational world that would be all there was to
say about it.  However, alas, there is more to the
story, especially when we look at the context, which
has to do with the Linux /dev/random and /dev/urandom.
It turns out that:
 1a) /dev/random is "supposed" to be a TRNG.
 1b) However, it can operate as a PRNG in exceptional   circumstances, when it is recovering from a compromise.
 2a) /dev/urandom is some sort of hermaphroditic chimera.
  That is to say, I don't know what it is.  Under a
  wide range of "typical" conditions it functions as
  a PRNG.
 2b) However, it also tries to approximate a TRNG if   it can.
There are some heavy tradeoffs involved here, trading
possibly-better performance under exceptional conditions against waste of CPU cycles and waste of entropy under normal conditions.  This leaves us with more questions
than answers.  There are as-yet unanswered questions
about the threat model, the cost of CPU cycles, the
cost of obtaining raw entropy, etc. etc. etc.  The
answers will vary wildly from machine to machine.
Because /dev/urandom is a hermaphroditic chimera, when
somebody says they are reading thousands of bits with
only 23 bits of entropy, that is not quite as insane
as you might think.  It's not a normal TRNG and it's
not a normal PRNG, but it is what it is.  There are
millions upon millions of machines in the field that
depend on it.
It's difficult even to describe what's going on in
is painful, because it is aggressively misleading if
not outright erroneous.  There are places in the code
where it is appropriately meticulous about accounting for entropy -- i.e. the actual real entropy -- and other places where it throws the word around recklessly.
In particular the get_random_bytes() function that has been featured in this thread basically just calls extract_entropy() ... even though it is extracting bytes from a quasi-PRNG that under /normal/ conditions will not have any appreciable entropy.
  So the name "extract_entropy()" is quite misleading.
The text of the printk warning mentions entropy, but we
should not pay attention to that, because what it says
is not what it means.  It does not communicate the real
meaning of the warning.  The if-condition that governs the printk checks the initialization flag ... not the entropy content.
Also note that this printk warning is in one person's
"development" branch and has not been incorporated into
any released version of the kernel.
  That reason doesn't make much sense.  There is a better reason,
as discussed below, but first we should observe that at present
Linux stores only a pseudo-random seed and then relies entirely on the PRNG!  This is in no way more trustworthy than storing
some real entropy.  Any attacker who could steal the hypothetical
random-seed file can also steal the urandom-seed file.
Here's a better reason why at present it would make no sense to
take the obvious approach to storing real entropy across reboots:
The /dev/urandom quasi-PRNG would immediately waste the entropy.
The device apparently assumes that a steady supply of new raw entropy will always be available.  In situations where entropy is scarce -- e.g. when there is a finite stored supply -- normal operation of /dev/urandom is tantamount to a denial-of-service attack on the entropy supply.
Any application that wants to file away some entropy can perfectly well do so, provided it does not let /dev/urandom get its hands on it.

@_date: 2013-11-11 23:44:16
@_author: John Denker 
@_subject: [Cryptography] randomness +- entropy 
Sorry, the problem is muuuuch harder than that.  If the solution
were that simple and that obvious, it would have been implemented
a long time ago.
The fact is, there are some applications that cannot make do with
low-quality randomness *and* cannot afford to wait.
  -- A PRNG that puts out low-quality randomness causes insidious
   failures.
  -- A PRNG that blocks causes manifest failures.
It could be argued that trading an insidious failure for a manifest
failure is a step in the right direction, but it is only a small
step, and it does not solve the main problem.
We need a PRNG that /always/ puts out a cryptographically-strong
random distribution ... early in the boot-up process and at all
times thereafter.  Specific constructive suggestions for how to
do this have been put forward ... such as putting a seed in the
kernel boot image, and making sure the seed is properly provisioned.
If/when we have a PRNG that is always ready to use, the question of blocking does not arise, and there is no need to define a new

@_date: 2013-10-01 11:10:24
@_author: Isaac Bickerstaff 
@_subject: [Cryptography] Linux /dev/random and /dev/urandom 
That reminds me of the Linux device driver for /dev/random and We know it is highly reliable, because it is used for a wide range of critical applications, and nobody would use it if it
weren't reliable.  Users -- as well as kernel developers -- are all keenly aware of how much modern cryptography depends on random numbers ... and how much security depends on attention to detail.
We know it is a "strong" RNG, because it says so, right at the top of the file, the drivers/char/random.c file.  Therefore there
is no need for anybody to review the code, let alone measure its
performance under real-world conditions.
I'm sure the driver was written by highly proficient cryptographers,
and subjected to a meticulous code review.
There is no way the code could have bugs that waste entropy.  There
is no way the code could have bugs that waste buffer capacity,
degrading the response to peak demand.  There is no way a variable
could be used with one undocumented meaning and then used with a
different undocumented meaning a few lines later.  There is no way anybody would ever create a PRNG with no lower bound on how
often it gets reseeded.
I haven't looked at the code -- heaven forbid -- but it "must" be well commented, in accordance with the high standards found throughout the kernel.

@_date: 2013-10-10 17:13:07
@_author: John Denker 
@_subject: [Cryptography] prism-proof email in the degenerate case 
That's fine, in the case where the traffic is heavy.
We should also discuss the opposite case:
*) If the traffic is light, the servers should generate cover traffic.
*) Each server should publish a public key for "/dev/null" so that
 users can send cover traffic upstream to the server, without
 worrying that it might waste downstream bandwidth.
 This is crucial for deniabililty:  If the rubber-hose guy accuses
 me of replying to ABC during the XYZ crisis, I can just shrug and  say it was cover traffic.
*) Messages should be sent in standard-sized packets, so that the
 message-length doesn't give away the game.
*) If large messages are common, it might help to have two streams:
 -- the pointer stream, and
 -- the bulk stream.
It would be necessary to do a trial-decode on every message in the
pointer stream, but when that succeeds, it yields a "pilot message"
containing the fingerprints of the packets that should be pulled out of the bulk stream.  The first few bytes of the packet should be a sufficient fingerprint.  This reduces the number of trial-
decryptions by a factor of roughly sizeof(message) / sizeof(packet).
*) Forward Secrecy is important here.

@_date: 2013-10-17 09:12:48
@_author: John Denker 
@_subject: [Cryptography] /dev/random has issues 
Here is an experiment you can do, if you have a Linux system:
  cat /proc/sys/kernel/random/entropy_avail
I predict that it is likely to be a smallish number, less than 192
bits, not enough to cut a PGP key.  This seems to conflict with
the stated purpose of having /dev/random, and with the purpose
of having buffers within the device.
I find the current version of /dev/random to be partly yarrow-like and partly not.  It is yarrow-like in the sense that it performs updates in batches, with a substantial minimum batch-size. It is non-yarrow-like in that it presents far too much load on the upstream source of entropy.
I'm not at all convinced that hundreds of eyeballs have ever looked at the source code for Linux /dev/random.  In any case, a small number of careful eyeballs would be far more valuable than a huge number of cursory eyeballs.
Suppose we provide /dev/random with a good source of entropy,
including (!) a reliable estimate of the amount of entropy
(hint: turbid).  Even then, it is not at all obvious that the current version of the Linux /dev/random is a good custodian
of the entropy it is given.
I noticed this when working on the upcoming new version of
turbid.  It contains a subsystem that feeds entropy into
but eventually I had to, because I couldn't figure out a
way to feed it entropy without huge amounts of waste.
AFAICT that isn't possible in the current version, although
this is a fixable problem.
A non-exhaustive list of questions and issues -- some quite deep and some quite superficial -- can be found at
  I have a prototype ("alpha") version of random.c that addresses most of these issues.  If there are any
misunderstandings about what /dev/random is doing, it
would be good to clear them up sooner rather than later.
A word about the article by Dodis et al. claiming that It raises issues that have little direct importance.  For one thing, there is no consensus that their definition of "robust" is relevant in a practical engineering sense.
Perhaps more importantly, we must object to the assertions about ?how hard (or, perhaps, impossible?) it is to design a sound entropy estimation procedure?.
It is a truism in many fields, including sculpture as well as
programming, that it is easy to do things wrong and hard to do
things right.  However, that does not mean that things /cannot/
be done right.  In particular, it is definitely *not* impossible
to implement an entropy estimator based on the second law of
thermodynamics, which is far more reliable than several other
assumptions that form the basis of modern cryptography.  Such
a thing requires effort and depth of understanding and attention to detail, but it can be done.  Hint: turbid.
The existence of unimportant issues should not blind us to more-important issues.

@_date: 2013-10-18 11:54:56
@_author: John Denker 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Hash: SHA1
Agreed, this is a system issue, not so much a /dev/u?random issue.
Blocking /dev/urandom is a bad idea.  Providing a good seed is the key.
Well, for qemu there is something.
I quote from  ...
Also:  Almost every VM I've seen provides a mechanism for shared
access to files.  A simple standard solution is to have the host
write some randomly-generated bits into a file that the guest can read.
Thirdly:  Booting from a read-only CD or similar .iso image, there is a problem if lots of people have images with the
same initial seed for the PRNG.
Several years ago I wrote some code that can take apart a .iso image, replace the seed, and put everything back together again.
This allows one to rather cheaply make N images all different.
For details, see
  I tried to get this incorporated into the Ubuntu distribution,
to no avail.
Note that if the machine can be booted with *some* randomness,
it can be given more, via a securely encrypted link, using
simple userspace tools.  There is a tool distributed with turbid that reads a file (or stdin) and does the ioctl to
feed randomness to the kernel.

@_date: 2013-10-18 16:02:21
@_author: John Denker 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Could we please quote a little more of the context?
What I actually said on 10/18/2013 11:54 AM was:
On 10/18/2013 01:33 PM, Christoph Anton Mitterer continued:
1) As to the question of "why", here are some partial answers:
 a) If it doesn't block, it might not be secure.
    If it does block, it won't get used.  Application
    developers will roll their own PRNGs which leaves us
    in some ways worse off and in no ways better off.
 b) I've built plenty of systems where the only way in is
  via SSH.  If necessary, I can set up a one-foot-long
  network air-gapped from the rest of the world, and SSH
  in that way ... so long as the thing is not blocking.
2) Remember what I said originally:  Providing a good seed is the key.
 If you provide a good seed, it doesn't need to block.
3) You can run turbid, so there is always lots of entropy
 available, more than enough for reseeding your PRNGs.
4) In this business there is a proverb:  If you ask  whether the system is "secure", the answer is no.
 If you want any other answer, you need to specify
 your threat model in some detail, and then decide
 how much risk you can tolerate, and what kind(s)
 of risk.

@_date: 2013-10-19 08:59:40
@_author: John Denker 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
That the sort of thing that /might/ help.  It cannot hurt,
except insofar as people overestimate how effective it is.
What is the chance that the attacker can figure out the
MAC address of the box?
What is the chance that the attacker can figure out tight
uppper and lower bounds on the value of the real-time clock?
What is the chance that the attacker can figure out tight
uppper and lower bounds on the device serial number?
Constructive suggestion:  As I have been saying for years:
 a) Any device that wants to have any security whatsoever
  needs to be able to store a seed, even when powered off.
 b) The seed needs to be provisioned on a per-device
  basis, much like the MAC address is provisioned.
 c) The seed needs to be randomly-chosen and secret, very   unlike the MAC address, RTC, and device serial number.
Go ahead and mix in stuff likt he RTC and the MAC address if you want, but you'll have a hard time convincing anybody
that such things are sufficient.

@_date: 2013-10-19 09:48:42
@_author: John Denker 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Uhhh, that's the answer to a different question.  We
agree that the amount of available entropy is "small".
My point is that it is too small.  Furthermore, the
work imposed on the attacker is an /exponential/
function of this number, so the work is verrrry much
too small.
If the embedded device had a hundred different MAC
addresses, all unrelated, all unknown to the attacker,
each of which contributed a few bits to the available entropy, then there would be no problem ... but that's
not the situation we find ourselves in.  Not even close.
We do not get to make an argument about the perfect
being the enemy of the good in this case.  That's not
the choice we face.  Instead, there is a choice of
real security versus futile security-theater.
 a) Any device that wants to have any security whatsoever
  needs to be able to store a seed, even when powered off.
 b) The seed needs to be provisioned on a per-device
  basis, much like the MAC address is provisioned.
 c) The seed needs to be big enough, randomly-chosen, and   secret, very unlike the MAC address, RTC, and device   serial number.
Exactly so.

@_date: 2013-10-19 14:21:15
@_author: John Denker 
@_subject: [Cryptography] PRNG WYTM 
OK, the next step in any such discussion is to ask the
famous question, What's Your Threat Model (WYTM).
Several different reasonable answers are possible.
1) At one extreme, we have the "no threat at all" model,
aka the "non-adversarial" model.  Examples include
 *) Doing a Monte Carlo integral in the context of   a molecular dynamics calculation.  The molecules   are not going to attack our PRNG.  They are not   going to cryptanalyze it.  Almost any PRNG is
  "random enough" for this purpose.
 *) Cooperative situations, such as friendly computers
  on a LAN, doing random exponential backoff as part
  of the layer-1 Ethernet CSMA/CD.  Everybody has a
  shared interest in implementing the protocol properly,
  so even if they could break the PRNG they wouldn't
  want to.
 *) Et cetera.  There are tons of examples in this   category.
A PRNG in category (1) could be considered "random" but not "secure".  It is usually adequate to seed this type of PRNG with things like the MAC address, serial number, and time-of-day.
2) At the opposite extreme we have high-stakes adversarial
applications, including military cryptography, banking,
other high-value business communications, high-stakes
gaming, etc. etc. etc.  A PRNG in this category needs to
be *secure* against a wide range of threats.
For tasks in this category, seeding the PRNG with things like the MAC address, serial number, and time-of-day is
nowhere near good enough.  It is a band-aid or worse.
It is security theater.  It gives you "randomness" in some weak sense, but it does not give you security.
The typical modern PRNG in this category consists of a
seed, a counter, a hash function, and a reseeding
mechanism.  Sometimes there is a block cipher in there somewhere, but to a sufficient approximation this is the same basic architecture, just with a fancier hash So let's look in more detail at the threats against such
a PRNG.
*) For starters we have the threat of direct cryptanalysis
of the output.  If the preimage can be found, all further
outputs will be known to the attacker, and probably all past outputs as well, over the span bounded by the nearest past and present reseedings.
The feasibility of finding a preimage depends on the number of bits output by the PRNG.  Therefore there
should be a limit on the number of output bits between
*) Another type of threat is more indirect.  For example,
suppose the PRNG was seeded at boot time from the saved
random-seed file.  It may be possible for the attacker to find this, perhaps by sneaking a peek at an old backup tape or whatever.  Such a threat is independent of the number of bits emitted by the PRNG.  It is hard to say what it /does/ depend on, but in the absence of anything better, wall-clock time is a plausible proxy.  Therefore there should be a time limit on how long a seed file is allowed to remain on disk before it is regenerated, and a time limit between reseedings of the PRNG.
That might be a "solution" in certain favorable cases, but
it is nowhere near being a reliable, general solution.  I
can think of five failure modes in five minutes.  Perhaps
the most obvious is this:  Suppose my system is sitting in
a rack at some colocation provider.  All the attacker needs
to do is rent a box in the same rack, on the same LAN
segment.  Then he knows my MAC address, the time at which
I booted up, and (to a good approximation) the arrival time
of every network packet addressed to me.
Similarly for my laptop on the corporate wifi network.  The
bad guy in the loft across the street has a nontrivial
chance of figuring out everything he needs to know about my network traffic.  If anybody has a proof that this
cannot happen, please explain.
Here's the only thing that has ever made sense to me:
 a) Any device that wants to have any security whatsoever
  needs to be able to store a seed, even when powered off.
 b) The seed needs to be provisioned on a per-device
  basis, much like the MAC address is provisioned.
 c) The seed needs to be big enough, randomly-chosen, and   secret, very unlike the MAC address, RTC, and device   serial number.

@_date: 2013-10-20 06:30:14
@_author: John Denker 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Could we please dial back the ad_hominem nonsense?
If we can't be respectful, could we at least be factual?
It is kinda comical to see the one guy who actually /has/
discussed a range of threat models (e.g. 10/19/2013 02:21 PM)
patronized via sweeping value judgments with no discernible connection to any of said threat models.

@_date: 2013-10-24 13:14:25
@_author: John Denker 
@_subject: [Cryptography] provisioning a seed for /dev/urandom 
I reckon that in most cases, it would be better to obtain the seed from the host's /dev/urandom (rather than /dev/random) to avoid problems with blocking, as discussed below.
  The exception is in cases where you know you have a copious
  supply of high-quality randomness available via /dev/random,
  so that blocking is not an issue.  Hint: turbid.
I'm mystified by that.  We're talking about Linux /dev/urandom aren't we?  That never blocks, not for reseeding or for anything
else.  There have been proposals to change this, but that would
be a Bad Idea? and I've never seen any blocking urandom device
actually get distributed ... although perhaps I have overlooked That's a bizarre overreaction.   ++ It would make sense to switch from /dev/random to /dev/urandom,
  since the latter never blocks.
 -- It makes no sense to switch from /dev/random to a constant.
Note that the seed in question is being used to initialize the guest's PRNG, so there is no reason to go overboard with the quality of the seed.  If the guest wanted hard-core industrial-strength entropy, it wouldn't be using its PRNG for that anyway.
If you are provisioning large numbers of VMs and/or hardware
devices, the per-unit cost of provisioning a proper seed is essentially zero.
Tangential remark:  /dev/urandom never blocks.  Occasionally I see
proposals to make it "better" by allowing it to block under some
conditions, but IMHO this is a terrible idea, for reasons that should
be clear from the story related above.  If given a source that blocks,
users will just not use it.  They will do something else instead, and the results will almost certainly be worse, not better.
The key is to make sure that /dev/urandom is always sufficiently
well seeded, so that it is always ready to use (for the appropriate
class of purposes).
On 10/23/2013 10:06 PM, Alon Ziv asked:
ssh is built on openssl which prefers to initialize itself from
So once again the key to making the problem go away is to make sure that /dev/urandom is properly seeded and ready to use.
When provisioning VMs or anything else, this should not be hard
to do.  It is already necessary to provision things like the
hostname and MAC address, and it is not rocket science to provision the seed-file at the same time.
There are additional issues that arise when booting from a genuinely
read-only system image, such as a CDROM, but there are other ways of
addressing these issues.  Options include:
  -- mounting a second disk containing the seed file ... and teaching
   the guest system to find this and use it properly.
  -- passing a seed on the kernel cmdline.
Constructive suggestion:
It would help to have sort of best-practices document that explains to VM providers and device manufacturers the importance of provisioning a unique seed ... and tells them how to do it properly.

@_date: 2013-10-27 15:17:14
@_author: John Denker 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
[sources for a good high-entropy seed]
This is excellent.  This is exactly the sort of discussion we
ought to be having.
I agree with the sentiment, but having lots of entropy isn't the only way of solving the problem;  we can make do with a lot less entropy if it is /computationally infeasible/ for the attacker to untangle what we have done.
OK, good.
It may help to add a little bit more detail to that analysis.  There are two subcases:
  a) The PRNG starts out in a good state, with a seed that is
   large enough, random enough, and unknown to the attacker.  If    we now use the real-time clock (RTC) to /stir/ the seed, that
   should be sufficient to guarantee no collisions.  That should
   be enough to stop replay attacks.
  b) If the PRNG starts out in a bad state, because the seed has
   been compromised, then the PRNG cannot quickly recover, and in
   particular it cannot recover while it is under attack.  The RTC
   is nowhere near sufficient to /substitute/ for a proper seed.
To say the same thing in slightly different words:  There are two
separate objectives that we need to consider:
  A) Resistance to attack, and
  B) Recovery from compromise.
In some rock-candy world it would be nice to do both at the same time, but in this world, there are situations where a practical
PRNG cannot do both at the same time, and that's OK.
The moral of the story is that it is super-important to make sure
the PRNG is properly seeded.  The seed needs to be big enough,
random enough, and unknown to the attacker.
Note that the RTC does not add real entropy to the PRNG.  All it
does is stir the PRNG.  Assuming the PRNG's hash function is working
properly, (seed + RTC1) should be effectively very different from
(seed + RTC2).  The attacker could try to mount a related-plaintext
attack against the hash, but for any decent cryptologic hash this
should be computationally infeasible.
There is a fundamental conceptual distinction between (a) reseeding
the PRNG with honest-to-goodness entropy and (b) merely stirring
the existing seed.  However, there are some situations in which
stirring suffices.
It shouldn't take much stirring, assuming the hash function is working
properly.  We absolutely require that the RTC produce non-repeating values, but that's about all we really require.  Again, this is all predicated on starting with a high-quality seed.
I hate to sound like a broken record, but the solution is the same in
all cases:  You need a proper seed:  big enough, random enough, and
unknown to the attacker.
The manufacturer reeeeally needs to provision each instance of the
device with a proper seed.  The cost of doing this is ridiculously
small.  The manufacturer already needs to provision things like the
MAC address, hostname, IMEI, et cetera ... and the addition burden
of provisioning the seed for the PRNG is negligible.
Again (!) the solution is the same:  If there is a proper seed, you're
fine.  If there is not a proper seed, you're screwed.
Specific suggestion:
  a) Download the .iso image onto disk.
  b) Provision a proper seed onto the image.
  c) Burn the modified image to CD if desired.
A few years ago I cooked up some tools to do exactly this.  I tried to
get support for it installed into the distributions, but the proposal
was shot down by some "expert" who decided that the RTC was sufficient
security, even if every attacker on earth knew what was in the distributed
seed-file ... and even if every attacker on earth knew what time it was.
More general suggestion:  We ought to produce some sort of "Best Practices"
document that explains to manufacturers, software distributors, VM providers,
et cetera how important it is to provision a proper seed, and explains how
to do it.
We should also provide tools and infrastructure that makes it easy to do
the right thing and hard to do the wrong thing.  For example, in the
general case, unpacking and repacking an .iso image is a pain in the neck,
but if the image is engineered to facilitate rewriting the seed-file the
procedure is much, much simpler.
Following up on a proposal by Jerry Leichter, on 10/25/2013 05:15 AM, I like this idea so much that I implemented it many years ago.  The
command line looks roughly like:
lynx -auth=harpo:swordfish -source ' | randomize kernel from -
I disagree; see below.
Again (!) the key is to have a proper seed stored on the machine
in question.  It's like the proverbial seed-corn:  If you have
none, you're screwed.  If you have some, you can grow more.
Some comments:
a) If you have 256 bits of honest-to-goodness entropy from
 /any/ source, OS or otherwise, then that's more than enough
 to seed the PRNG, and nothing on the list matters.
b) Ditto.
c) Not so good. Ping times are not guaranteed to be always
 different, and not guaranteed to be unknown to the attacker.
 In the crypto business it is conventional to worry about
 MITM attacks.  A scenario where the MITM doesn't need to
 mess with the packet contents, just the packet timing,
 is extra-easy for the attacker.
d) Not so good.  All that stuff is constant, and relatively
 easy for the attacker to know or guess.  The RTC wasn't  mentioned, but it is better than the items that were
 mentioned, in that it is at least non-constant and non-
 repeating.  The RTC cannot /substitute/ for a proper
 stored seed, but it can /stir/ the seed.
e) That's basically the right idea, except that we should
 call it the "stored" seed rather than the "at-birth" seed.
 The stored seed should be changed at frequent intervals.
 This helps with the recovery from compromise.  Perhaps
 more importantly, it reduces the window of vulnerability
 and reduces the value the attacker gets from peeking at  the stored seed.

@_date: 2013-10-31 14:59:09
@_author: John Denker 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Thanks, this is exactly the sort of clarity and specificity we need.
OK, that's clear.  That's sound engineering.
Just to state the obvious, we are talking about a PRNG unless
otherwise specified.
It should also be noted that one can imagine a TRNG without
a PRNG but not vice versa.  That is, you have to *seed* the
PRNG from somewhere.  You could perhaps seed PRNG "A" from
PRNG "B" ... but that just creates a Cat-In-The-Hat-Comes-Back
problem.  At some point you have to seed something with real
honest-to-goodness entropy.
It should go without saying that "randomness" is not synonymous
with "entropy".  The output of a PRNG may be /random enough/
in the sense that breaking it is computationally infeasible.
In contrast, the output of a proper TRNG is unbreakable in
It should also go without saying that a PRNG is good enough
for some purposes but not others.  YMMV but I would normally
be OK with using a cryptologically strong PRNG for ephemeral session keys ... but I would insist on genuine a TRNG with
100% entropy density for cutting a long-term RSA key.
Last but not least, it should go without saying that there is a huge difference between
 a) a /calibrated/ source of entropy, and
 b) an uncalibrated source of alleged entropy.
The former is a lot harder to do ... and incomparably more
useful.  There is a category I call "squish" i.e. bits that are neither reliably predictable nor reliably unpredictable.
See the diagram at
  I get tired of people providing long lists of sources that
"might" be unknown to an attacker.  It is incomparably
better from an engineering point of view to have a short
list of sources that are absolutely, positively unknown
to the attacker.
I wouldn't do it that way.  The smart thing to do is to
"populate" (i.e. provision) the "pool" (aka seed) with good-
quality randomness.  It is always good and very often
necessary for this to come from outside the machine, before
it is booted for the first time. Provisioning such a thing is not a big deal.  Whether it is a handheld device or a VM or whatever, it needs to be provisioned with a MAC address,
IMEI number, hostname, /etc/passwd, et cetera ... so there
is /already/ a provisioning process.  We just need to add
the seed-file to the list of things that MUST be provisioned
if you want the machine to be secure.
That's not smart, unless the machine has an unusually
good /calibrated/ entropy source.  The smarter thing to do is to provision a proper seed, so that no blocking is necessary.  The seed needs to be big enough and random enough and not known to the attacker.
To guard against any possibility of replay, /stir/ the
seed using the RTC.  The RTC value is assumed known to
the attacker, but the seed is assumed unknown, and
hashing the RTC with the seed prevents replays and
sends the attacker back to square one.
  It must be emphasized that the RTC provides zero
  entropy, and by itself it would be useless, but
  it serves as a source of non-repeating values, which
  is plenty good enough to /stir/ the seed.  Other   things like MAC addresses and serial numbers are
  not worth fooling with, because they are the same
  every time you look at them, and because they are
  known to the attacker, or too-easily guessable.
In the case of a bootable DVD or CD-rom, download the .iso image, unpack it, provision a proper seed, pack it
back up again, and then burn the properly-provisioned
image onto the rom.
Actually, you shouldn't wait for shutdown.  Recompute a
new seed file at the /first/ opportunity, not the last.
Do not assume the machine will undergo an orderly shutdown.
I've seen machines that essentially never undergo an orderly
Not necessarily.
If you really are running a datacenter, you can well afford
to run turbid on one or more of the machines.  This will
provide copious amounts of hard-core industrial-strength
entropy.  This can be distributed to guest machines in
the usual variety of ways.
If I were doing it, I would be tempted to run turbid on
every host, and distribute the output to the guest VMs
via the virtio-rng interface.  This guarantees the guests
have plenty of TRNG entropy to use for seeding their PRNGs,
for cutting long-term keys, et cetera.
Well, there are lots of things that could go wrong.  Murphy
was an optimist.
Well, yes and no.
It is possible to have an in-bounds discussion of privilege
"too much" privilege ... but rather less than full permanent root access.
For starters, imagine somebody getting read-only privileges
for only a millisecond.  He swipes a copy of the random-seed
file and then forces a reboot.  This would be a Bad Thing.
One action item here is to make sure the file-access modes
are set properly on the seed file(s).
Other categories of attack are more cryptanalytic in nature.
For instance, the PRNG might be built using SHA-1 or AES.
The security of such a thing is very sensitive to the "mode"
plumbing, not just to the primitive hash or cipher itself.
Since the PRNG per se is deterministic, anybody who captures
the state of the PRNG can predict all future outputs, up to the next reseeding event.  In contrast, with a well-designed
PRNG, capturing the state does not allow prediction of past
traffic.  That is to say, proper "mode" plumbing provides
forward secrecy.  This does not prevent an attack, strictly
speaking, but it makes the attack less damaging.
[1] A reasonable place to start is
  John Kelsey, David Wagner, Bruce Schneier, and Chris Hall
  ?Cryptanalytic Attacks on Pseudorandom Number Generators?
  [2] An amusing exercise is to read
  Elaine Barker and John Kelsey,
  NIST Special Publication: ?Recommendation for Random Number       Generation Using Deterministic Random Bit Generators?
  For example, look at the plumbing on page 51.  There are some
pieces there that may seem non-obvious at first, but are
there to block specific attacks mentioned in reference [1].
Also look at the six references starting here:
  These are more than plausible, these are attacks that have
succeeded in the past.
That is a major issue.
The other way of dealing with it is to seed the kernel PRNG
as early as possible.  I have toyed with the idea of having
a random seed on the initrd (initial ramdisk) and making sure it gets loaded very very early.  It could be a plain file or
perhaps a random-seed.ko module.  Refreshing such a thing would require unpacking and repacking the initrd image, which is
work, but not unbearably much work.
Similarly one could imagine linking such a thing right into
the kernel image.  Refreshing would require unpacking and
repacking the bzImage, which is work, but not unbearably much work.

@_date: 2013-09-05 16:56:38
@_author: John Denker 
@_subject: [Cryptography] tamper-evident crypto?    (was: BULLRUN) 
I don't have any hard information or even any speculation about
BULLRUN, but I have an observation and a question:
Traditionally it has been very hard to exploit a break without giving away the fact that you've broken in.  So there are two fairly impressive parts to the recent reports:  (a) Breaking some modern, widely-used crypto, and (b) not getting caught for a rather long time.
To say the same thing the other way, I was always amazed that the
Nazis were unable to figure out that their crypto was broken during WWII.  There were experiments they could have done, such as sending
out a few U-boats under strict radio silence and comparing their longevity to others.
So my question is:  What would we have to do to produce /tamper-evident/
data security?
As a preliminary outline of the sort of thing I'm talking about, you
could send an encrypted message that says   "The people at 1313 Mockingbird Lane have an    enormous kiddie porn studio in their basement."
and then watch closely.  See how long it takes until they get raided.
Obviously I'm leaving out a lot of details here, but I hope the idea
is clear:  It's a type of honeypot, adapted to detecting whether the
crypto is broken.
Shouldn't something like this be part of the ongoing validation of any data security system?
You can get a lot more entropy than that from your sound card, a
lot more conveniently.
  I'm not so sure about that.  Typos are not random, and history proves that seemingly minor mistakes can be exploited.
I wouldn't have said that.
As Dykstra was fond of saying:
   Testing can show the presence of bugs;
   testing can never show the absence of bugs.

@_date: 2013-09-06 12:31:47
@_author: John Denker 
@_subject: [Cryptography] tamper-evident crypto? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Well, I'm sure /somebody/ on this list is clever enough to arrange countersurveillance and counterintrusion measures...
  a) especially given that detecting surveillance and/or
   intrusion is the whole point of the exercise;
  b) especially given that we have all the time in the world    to arrange boatloads of nanny-cams and silent alarms etc.,
   arranging everything in advance, before provoking the    opponent;
  c) especially given that we know it's a trap, and the
   opponent probably isn't expecting a trap;
  d) especially given that the opponent has a track record
   of being sometimes lazy ... for instance by swearing that    the fruits of illegal wiretaps came from a "confidential
   informant who has been reliable in the past" and using that
   as the basis for a search warrant, at which point you've
   got them for perjury as well as illegal wiretapping,
   *and* you know your information security is broken;
  e) especially given that we get to run this operation
   more than once.
  *) If they don't like that flavor of bait, we can give
   them something else.  For example, it is known that    there is a large-diameter pipeline from the NSA to the
   DEA.
      *) Again:  We get to run this operation more than once.  I repeat the question from the very beginning of this thread:
Shouldn't this be part of the /ongoing/ validation of any data security scheme?
There's a rule that says that you shouldn't claim a crypto
system is secure unless it has been subjected to serious
cryptanalysis.  I'm just taking the next step in this
direction.  If you want to know whether or not the system
is broken, /measure/ whether or not it is broken.
One of the rules in science, business, military planning,
et cetera is to consider /all/ the plausible hypotheses.
Once you consider the possibility that your data security
is broken, the obvious next step is to design an experiment
to /measure/ how much breakage there is.

@_date: 2013-09-08 13:29:56
@_author: John Denker 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
I wouldn't have said that.  It's a lot more complicated than
that.  For one thing, there are lots of different "people".
However, as a fairly-general rule, people definitely do consider safety as part of their purchasing decisions.
 -- Why do you think there are layers of tamper-evident
  packaging on Tylenol (and lots of other things)?  Note that
  I was not kidding when I suggested tamper-evident data
  security measures.  Not only do responsible vendors want
  the product to be safe when it leaves the factor, they want   to make sure it /stays/ safe.
 -- Any purchaser with an ounce of sense will hire an inspector
  to check over a house before putting down a deposit.  Sales
  contracts require the seller to disclose any known defects,
  and generally provide some sort of warranty.
 ++ Forsooth, if people bought crypto as carefully as they buy
   houses, we'd all be a lot better off.
 -- In many cases, consumers do not -- and cannot -- /directly/
  evaluate safety and quality, so they rely on third parties.
  One familiar example is the airline industry.  The airlines
  generally /like/ being regulated by the FAA because by and   large the good guys already exceed FAA safety standards, and   they don't want some bad guy coming in and giving the whole
  industry a bad name.
 -- I imagine food and drug safety is similar, although the
  medical industry complains about over-regulation more than
  I would have expected.
 -- There are also non-governmental evaluation agencies, such
  as Underwriters' Laboratories and Earth Island Institute.
 ** There are of course /some/ people who court disaster.  For
  example, there are folks who consider seatbelt laws and motorcycle
  helmet laws to be oppressive government regulation.  These are
  exceptions to the trends discussed above, but they do not   invalidate the overall trends.
 !! Note that even if you are doing everything you know how to do,
  you can still get sued on the grounds of negligence and deception
  if something goes wrong ... especially (but not only) if you said
  it was safer than it was.  Example:  Almost every plane crash ever.
  Let's be clear:  A lot of consumer "demands" for safety are made
  retroactively.  "Caveat emptor" has been replaced by /caveat vendor/.

@_date: 2013-09-09 07:40:21
@_author: John Denker 
@_subject: [Cryptography] auditing a hardware RNG 
Can you be more specific?  What flaws?
Yes, it's impossible, but that's the answer to the wrong
question.  See below.
Let's assume she actually said that.

@_date: 2013-09-12 10:04:50
@_author: John Denker 
@_subject: [Cryptography] real random numbers 
Executive summary:
The soundcard on one of my machines runs at 192000 Hz.  My beat-up old laptop runs at 96000.  An antique server runs at "only" 48000. There are two channels and several bits of entropy per sample.
That's /at least/ a hundred thousand bits per second of real industrial-strength entropy -- the kind that cannot be cracked, not by the NSA, not by anybody, ever.
Because of the recent surge in interest, I started working on a new version of turbid, the software than manages the soundcard and collects the entropy.  Please give me another week or so.
The interesting point is that you reeeeally want to rely on the
laws of physics.  Testing the output of a RNG can give an upper bound on the amount of entropy, but what we need is a lower bound, and only physics can provide that.  The physics only works if you /calibrate/ the noise source.  A major selling point of turbid
is the calibration procedure.  I'm working to make that easier for non-experts to use.
Concerning "radioactive" sources:
My friend Simplicio is an armchair cryptographer.  He has a proposal to replace triple-DES with quadruple-rot13.  He figures that since it
is more complicated and more esoteric, it must be better.
Simplicio uses physics ideas in the same way.  He thinks radioactivity is the "One True Source" of randomness.  He figures that since it is
more complicated and more esoteric, it must be better.
In fact, anybody who knows the first thing about the physics involved
knows that quantum noise and thermal noise are two parts of the same
elephant.  Specifically, there is only one physical process, as shown
by figure 1 here:
  Quantum noise is the low-temperature asymptote, and thermal noise is
the high-temperature asymptote of the /same/ physical process.
So ... could we please stop talking about "radioactive" random number
generators and "quantum" random number generators?  It's embarrassing.
It is true but irrelevant that somebody could attempt a denial-of-service
attack against a thermal-noise generator by pouring liquid nitrogen
over it.  This is irrelevant several times over because:
 a) Any decrease in temperature would be readily detectable, and the   RNG could continue to function.  Its productivity would go down by
  a factor of 4, but that's all.
 b) It would be far more effective to pour liquid nitrogen over other
  parts of the computer, leading to complete failure.
 c) It would be even more effective (and more permanent) to pour sulfuric   acid over the computer.
 d) Et cetera.
The point is, if the attacker can get that close to your computer, you have far more things to worry about than the temperature of your noise source.  Mathematical cryptographers should keep in mind the proverb that says: If you don't have physical security, you don't have security.
To say the same thing in more positive terms:  If you have any halfway-
reasonable physical security, a thermal noise source is just fine, guaranteed by the laws of physics.
In practice, the nonidealities associated with "radioactive" noise are far greater than with thermal noise sources ... not to mention the cost and convenience issues.
As I have been saying for more than 10 years, several hundred thousand bits per second of industrial-strength entropy is plenty for a wide
range of practical applications.  If anybody needs more than that, we
can discuss it ... but in any case, there are a *lot* of services out there that would overnight become much more secure if they started using a good source of truly random bits.
The main tricky case is a virtual private server hosted in the cloud.
You can't add a real soundcard to a virtual machine.  My recommendation for such a machine is to use a high-quality PRNG and re-seed it at frequent intervals.  This is a chicken-and-egg situation:
 a) If you have /enough/ randomness stored onboard the VPS, you can   set up a secure pipe to a trusted randomness server somewhere else,
  and get more randomness that way.
 b) OTOH if the VPS gets pwned once, it might be pwned forever, because   the bad guys can watch the new random bits coming in, at which point
  the bits are no longer random.
 c) On the third hand, if the bad guys drop even one packet, ever,
  you can recover at that point.
 d) I reckon none of this is worth worrying about too much, because
  at some point the bad guys just strong-arm the hosting provider
  and capture your entire virtual machine.

@_date: 2013-09-14 12:29:35
@_author: John Denker 
@_subject: [Cryptography] real random numbers 
This discussion will progress more smoothly and more rapidly
if we clarify some of the concepts and terminology.
There are at least four concepts on the table:
  1) At one extreme, there is 100% entropy density, for example
   32 bits of entropy in a 32-bit word.  I'm talking about real
   physics entropy, the kind of hard-core industrial-strength
   randomness that will never get cracked by anybody, ever.
  2) It is also possible to have lesser entropy density, such
   as 3 bits of entropy in a 32-bit word.  Such a word is
   partly predicable and partly not.  Remark: I emphatically    recommend that we pay attention to cases where we have a    provable lower bound on the entropy, even if the entropy    density starts out less than 100%, because the density can
   be increased to virtually 100% by hashing.  This is how
   turbid works.
  3) At the other extreme, there is complete determinism, such
   as the decimal digits of ?, or even the decimal digits of    1/3rd.
  4) Last but not least, there is the "squish" category.  This
   case is covered by Murphy's law.  That means:
   -- If you wanted the symbol to be predictable, it would not
    be reliably predictable.
   -- If you wanted the symbol to be unpredictable, it would
    not be reliably unpredictable.
Illustration:  Back when I was 17 years old, I needed a random
number generator.  The pointy-haired boss directed me to use
the value of the instruction pointer at each interrupt.  I had
to explain to him that it was an event-driven system, so with
high likelihood, the IP pointed to the idle loop.  It was clearly
a squish.  You couldn't rely on it being predictable, but you
also couldn't rely on it being unpredictable.
To say the same thing yet again:  For non-adversarial situations,
you can do pretty much whatever you like, and you might reasonably
care about the typical case ... but a great many applications of randomness, including gaming and cryptography, are highly adversarial.  That demands a minimax approach.  The best case doesn't much matter and the typical case doesn't much matter;
we need to focus attention on the worst case.
Absence of proof is not proof of absence.  For the important
applications, we need a provably-good RNG.  The second law of
thermodynamics provides the required guarantees.
Here's another way of looking at almost the same issues.  People
on this list have been talking about combining every possible source of "randomness" ... the more the merrier.  Again there is a crucial distinction to be made:
 a) In the linux "random" device, /any/ user can mix stuff into
  the driver's pool.  This is a non-privileged operation.  The
  idea is that it can't hurt and it might help.  So far so good.
 b) Contributions of the type just mentioned do *not* increase
  the driver's estimate of the entropy in the pool.  If you want
  to increase the entropy-estimate, you need to issue a privileged
  ioctl.
If we want to have a meaningful discussion, we must distinguish
between  a) mixing in all sorts of squish, and
 b) mixing in some real entropy /and taking credit for it/.
Again, step (a) cannot get anybody into trouble.  Step (b) gets
you into trouble if you claim credit for more entropy than was
actually contributed.
Let's be clear:  There is a huge difference between contributing worthless squish and contributing real entropy.  IMHO the key distinction is whether or not you have a hard /lower bound/ on the amount of entropy, so you can claim credit for it, and be
confident of the claim.  I strongly recommend not bothering with
things that /might/ be unpredictable, and focusing instead on
things that are absolutely guaranteed to be unpredictable, as
guaranteed by the laws of physics.
I wouldn't have said that.  In a multi-stage amplifier, the noise figure is set by the /first/ stage, not the "final stage".  More importantly, it hardly matters which stage contributes which part of the gain, so long as there is "enough" gain before the signal is digitized.  Most computer audio systems have enough gain to make the Johnson noise observable.  This gives us a source of hard-core, industrial-strength, guaranteed physics entropy.  In typical cases,
increasing the gain helps a tiny bit, but doubling the gain does it by one bit.
Muting the inputs cannot help and might hurt.  It might well drop
the entropy production to zero.
Things like clock skew are usually nothing but squish ... not
reliably predictable, but also not reliably unpredictable.
I'm not interested in squish, and I'm not interested in speculation
about things that "might" be random.  I'm interested in things that
provides such a guarantee.
The word "random" means different things to different people.  There
is nothing to be gained by arguing over the definition.  Feel free
to use this word however you like.
In contrast, please do not use the word "entropy" loosely.  There is a very well-defined and well-understood notion of physics entropy.
Tacking on additional metaphorical or just plain sloppy usages just
pollutes the discussion and serves no useful purpose.
There is a huuuuge difference between squishy randomness and real
physics entropy.

@_date: 2013-09-14 20:11:07
@_author: John Denker 
@_subject: [Cryptography] real random numbers 
Previously I said we need to speak more carefully about these
things.  Let me start by taking my own advice:
Actually it's one step more complicated than that.  Step (a) causes problems if you /underestimate/ the entropy content of
what you contributed.  The problem is that the end-user
application will try to read from the RNG and will stall
due to insufficient entropy available.
Step (b) has the opposite problem: You get into trouble if you /overestimate/ the entropy of what you have contributed.
This causes insidious security problems, because your allegedly random numbers are not as random as you think.
That very much depends on what you mean by "expected".
 -- An ill-founded expectation is little more than a wild guess,
  and it is not useful for critical applications.
 ++ OTOH a well-founded statistical "expectation value" is just
  what we need, and it moves the source firmly out of the
  squish category.
I say again, a squish is not reliably predictable /and/ not
reliably unpredictable.  If you have *any* trustworthy nonzero
lower bound on the entropy content, it's not a squish.
On the other hand, again and again people latch onto something
that is not reliably predictable, call it "random", and try
to do something with it without establishing any such lower
bound.  This has led to disaster again and again.
There is a ocean of difference between "not reliably predictable"
and "reliably unpredictable".
However, alas, the good guys don't know how much either, so they don't know much to take credit for.  An underestimate causes the RNG to stall, and an overestimate means the output is not as random
as it should be.  I vehemently recommend against risking either of
these failures.
I emphasize that there are two operations that must be considered
carefully:   1) Mixing "stuff" into the driver's pool, and
 2) taking credit for it ... the right amount of credit.
One without the other is strictly amateur hour.
You might, or you might not.  In an adversarial situation, this
is begging for trouble.  I vehemently recommend against this.
Hope is not an algorithm.
So quantify the thermal noise already.  It sounds like you are
using the oscillator as a crude digitizer, digitizing the thermal
noise, which is the first step in the right direction.  The next step to come up with a hard lower bound on the entropy density.
OTOH when you plug in the actual numbers, you will probably find that the oscillator is incredibly inefficient compared to a My main point is, there is a perfectly reasonable formalism for analyzing these things, so that "hope" is not required.
Secondarily, there is a huge industry mass-producing soundcards
at a very low price.  Very often, a soundcard is build into the
mainboard, whether you ask for it or not.  So in practical terms, any sort of custom thing you think of has a big hurdle to overcome.
I've run the numbers carefully enough to know that emanations
from the audio system are almost certainly negligible compared
to a gazillion other things going on in the system.  A few
kilohertz versus a few gigahertz.  Audio cables are routinely
If you think I'm wrong about this, please spell out a realistic
attack scenario.  Please be as specific as possible, with RF
field levels and such.
That's not how the attack works.  The attack on clock-skew sources
starts by considering the possibility that the two clocks are
correlated ... so that the skew is producing vastly less entropy
than you might have hoped.  The idea of clocks being correlated
falls into the "duh" category;  it's a big part of why clocks were invented, originally.
This stands in dramatic contrast to the Johnson noise.  The physics
says the raw data is uncorrelated.  The processing could introduce
a little bit of correlation, but only at the cost of reducing the
bandwidth and/or introducing gross audio distortions.  Crucially,
you can measure the bandwidth!  This is part of the calibration that turbid does.
If you have a principled, reliable, non-zero lower bound on the entropy density of a stream of clock-skew measurements, please share it with us.
Again:  If you have a reliable, nonzero lower bound on the entropy
density, it's not a squish!  Please, let's try to settle on some
clear terminology and stick with it. Conversely, you don't have a reliable nonzero lower bound, then you are at risk for producing allegedly random numbers that aren't as random as they should be.  The historical record is replete with examples of people who fell into this trap and paid dearly.
That's based on a diametrical misunderstanding of what I was talking about when I mentioned hashing ... so let me clarify:
The concentration process I was talking about is the opposite of stretching.  It takes a large number of symbols with a low entropy density and uses a hash to produce a smaller number of symbols with a high entropy density -- very nearly 100% if that's what you want.
Stretching also has its place for some applications, but it
is just the opposite of concentration.  It uses a smallish
supply of truly random numbers to seed a *pseudo* random
generator.  Note that without a truly random seed, any PRNG is a dead duck.  Again, the list of people who have screwed this up is verrry long.
A stretcher is bundled with turbid, but the stretcher and
the concentrator are wildly different.  They run in different
threads, and are only very loosely coupled.

@_date: 2013-09-15 06:06:44
@_author: John Denker 
@_subject: [Cryptography] real random numbers 
That's just completely backwards.  In the world I live in,
people get fooled because they /didn't/ do the analysis, not
because they did.
I very much doubt that Bruce concluded that accounting is "doomed".
If he did, it would mark a dramatic step backwards from his work on the commendable and influential Yarrow PRNG:
  J. Kelsey, B. Schneier, and N. Ferguson (1999)
  This revolves around a /two-stage/ design.  Entropy accumulates
in the first stage and is then transferred in /batches/ to the
second stage.  There must be a substantial amount of energy in
each batch, or the entire batch is wasted, in the sense that it
does not help the PRNG recover from compromise.  Let's be clear:
transferring "randomness" to the second stage before the accumulator
has accumulated enough entropy is demonstrably worse than nothing.
It wastes entropy that otherwise would have eventually accumulated
to a useful level.
This design makes sense if *and only if* you have a reliable non-zero lower bound on the entropy coming into the first stage.
A PRNG is like almost everything else in cryptography:  You can't build a good PRNG unless you know how to /attack/ a PRNG.
Dribbling small amounts of entropy into the final-stage pool
does *not* have acceptable resistance to attack.  Na?ve
intuition suggests it might be OK, but it's not.

@_date: 2013-09-27 09:43:30
@_author: John Denker 
@_subject: [Cryptography] heterotic authority + web-of-trust + pinning 
Hash: SHA1
There are other ways of thinking about it that makes it seem not quite so bad.
There are many approaches to establishing trust.  Familiar examples
 *) The top-down authoritarian X.509 approach, such as we see in SSL.
 *) The pinning-only approach, such as we see in SSH.
 *) The web-of-trust approach, such as we see in PGP.
Each of these has some security advantages and disadvantages.  Each has some convenience advantages and disadvantages.
My point for today is that one can combine these in ways that are heterotic, i.e. that show hybrid vigor.
 -- The example of combining the CA approach with pinning has already
  been mentioned.
 -- Let's now discuss how one might combine the CA approach with the   web-of-trust approach.  Here's one possible use-case:
  Suppose you have a HTTPS web site using a certificate that you bought
  from some godlike CA.  When it expires, you buy another to replace it.
  So far so good.
  However, it would be even better if you could use the old certificate
  to sign the new one.  This certifies that you /intend/ for there to be
  a continuation of the same security relationship.  [As a detail, in
  this approach, you want a certificate to have three stages of life:
  (1) active, in normal use, (2) retired, not in active use, but still
  valid for signing its successor, and (3) expired, not used at all.]
  This is like PGP in the sense that the new certificate has multiple
  signatures, one from the top-down CA and one from the predecessor.
  The idea of having multiple signatures is foreign to the heirarchical
  authoritarian X.509 way of thinking, but I don't see any reason why
  this would be hard to do.
 -- Similar heterotic thinking applies to SSH.  Suppose I want to replace   my old host key with another.  It would be nice to use the old one to
  /sign/ the new one, so that a legitimate replacement doesn't look like
  a MITM attack.  (In theory, you could validate a new SSH keypair by
  distributing the fingerprint via SSL or PGP, which reduces it to a
  problem previously "solved" ... but that's labor-intensive, and AFAICT
  hardly anybody but me ever bothers to do it.)
You could call it that, but you could just call it a /signer/.  PGP has already demonstrated that you can have millions upon millions of signers. In the use-case sketched above, we don't even need a keyserver.
The web site just offers its public key, plus a certifiate signed by the CA, plus another certificate signed by the predecessor key.
   For end-to-end security of email, where it may be that neither end
   is a server, some sort of keyserver is probably necessary.  This
   seems like a manageable problem.
We agree that half a billion CAs would be too many, if they all had the
power to sign anything and everything.  Forsooth, my system already has 321 certificates in /etc/ssl/certs, and that seems like waaay too many, IMHO.  That's because the adversay needs to subvert only one of them, and the adversary gets to pick and choose.
On the other hand, if we think in terms of a /signer/ with much more
limited power, perhaps only the power to countersign a successor cert
that has already been signed by a CA, that sounds to me like a good thing, not a bad thing.

@_date: 2014-04-23 12:41:27
@_author: John Denker 
@_subject: [Cryptography] swap needed, or not 
Swap is not generally needed, not for that reason or any other.
Proof by construction:  There are LOTS of embedded systems that run just fine with no swap.  Been there,
done that.
Additional proof by construction:  This email is being
composed on a laptop with no swap.  Bog-standard Ubuntu Linux.  Bog-standard x86_64 hardware.
You can always create situations where swap is needed:
 -- Maybe you want to run big programs in a small amount
  of physical memory.
 -- Maybe you want your laptop to hibernate.
However ... there remain LOTS of practical situations where swap is not needed.
It's not a Unix-specific thing.  It's not any kind of
thing at all.  Unix runs with zero swap quite happily.

@_date: 2014-08-16 10:27:11
@_author: John Denker 
@_subject: [Cryptography] "password manager" --> _authorization manager_ 
I do things the same way.
I appreciate the sentiment, but there may be better ways of
dealing with the details.
The objective makes sense, but there may be better ways of getting
The frequent discussions of "ideal" password schemes remind me of
the "optimal" way of eating vichyssoise with a fork.  There ain't
no such thing.  Spoons are more practical, and readily available.
In particular: a "password manager" is a small step in the right direction, but it is not a logical stopping point.  With the same
amount of workload on the user, the manager could be doing some sort of asymmetric crypto, so that there is never any need to transmit a password.
  a) Certificates are the most obvious way of doing this.
  b) If somebody absolutely needs compatibility with the idea of    "password", we could use some variant of PAKE;  for a list of
   variants, see
     It is important to have a migration path.
To those who think this cannot be done, or who think it would be
unduly burdensome, here is a counterargument by way of analogy:
I run very little risk that any online merchant will compromise my
credit-card account, because my bank provides an app that generates
ephemeral credit-card numbers, one per transaction.  The virtual
card has a credit limit that suffices to cover that one transaction
and nothing more, so the merchant cannot overcharge me.  Also, the
virtual card is locked to a particular merchant, so even if the
balance is not used up, stealing the card number would be of no
use to anybody else.  I could safely publish all my past card numbers
on my web site.
There are some important ideas here, including:
  a) ease of use, and
  b) seamless compatibility with the huge installed base of vendors
   who ask for a credit-card number.
Part of the ease-of-use comes from the fact that the virtual card
app uses "automatic form filling" to stuff the card number, date,
cardholder name, etc. into the merchant's form.  This makes it
actually /easier/ to use than a non-virtual plastic card.
Furthermore it is vastly /more secure/ than a plastic card,
because in addition to identification and authentication, it
provides /authorization/ for a particular dollar amount.
Migration away from the existing addiction to passwords will not
be quite so smooth, but there is no doubt that it can be done.
The "automatic form filling" features can be put to good use
So, can we please stop talking about the "password" manager?  Instead
let's call it an identification/authentication/authorization manager or something like that.  Let's keep eyes on the prize, the real prize.
The real objective is not the password.  The real objective is secure
identification, authentication, and /authorization/.
A password manager is a step in the right direction, but it is not
a logical stopping point.

@_date: 2014-12-02 09:15:24
@_author: John Denker 
@_subject: [Cryptography] squish*infinity = squish 
There's nothing like a good joke.
That is nothing like a good joke.
We are talking about some code that uses uninitialized memory
to seed the OpenSSL PRNG.  This is a problem, for reasons having
nothing to do with valgrind and/or purify.  OpenSSL could take that code out or leave that code in, and the real problem would
remain: The PRNG is not being properly seeded.
This can be understood as follows:  Every word contains three
types of data:
  -- determinism:  guaranteed predictable
  -- entropy:      guaranteed unpredictable
  -- squish:       not guaranteed predictable *or* unpredictable
If any given word contains zero real entropy, a million such words still contains zero entropy.
If you think uninitialized memory contains some amount of real entropy, give us a nonzero lower bound on the amount.  There
are reliable ways of providing entropy to a newly instantiated
PRNG, and uninitialized memory is not one of them.
I get tired of saying this.  Any self-respecting cryptographer
should know this already.  It has been in the literature since
1967 if not longer:  Knuth gives an example where combining a
bunch of crappy RNGs does *not* result in a good RNG.
I don't know how to say it any more clearly:  Just because
you can't guarantee it's predictable doesn't mean it is
guaranteed unpredictable.
Note that the OpenSSL code in question is still there;  see
  and search for "really bad randomness".
True story:  Once I had the proverbial pointy-haired boss
tell me to stop fussing with the RNG and just use the program
counter at the last interrupt, because that was obviously not
predictable.  Now it turns out that this was an event-driven
system, so the PC at interrupt was almost always a constant,
i.e. the address of the null job.
 -- Chance of PHB figuring this out:            0%
 -- Chance of adversaries figuring this out:  100%
They say a word to the wise suffices.  Evidently that is not
applicable here, so I'll say it a few more times:
Combining a large number of guaranteed sources will improve
the /rate/ of entropy delivery.  Combining a large number of probably-good sources may improve the reliability, provided
the probabilities are guaranteed uncorrelated ... which they
usually aren't.  One guaranteed source is worth more than an arbitrary number of bogus sources.  Feeding squish into the PRNG is not an acceptable seed.  Feeding in more and more squish will not solve the problem.
Tell me again why we need to have an underhanded crypto
contest?  It seems to me there is enough broken code out
there already;  why do we need to write more?  Why not
take whatever resources were going to be used to judge
the contest, and apply them to reviewing the code that
is already out there?  The top prize goes for catching
the /oldest/ serious bug, since that one was obviously
the hardest to catch.

@_date: 2014-12-03 11:32:31
@_author: John Denker 
@_subject: [Cryptography] Underhanded Crypto 
Hash: SHA1
On 12/03/2014 05:20 AM, Ben Laurie retorted:
How firmly has it been established that there is no craziness
is going on?  It seems to me that OpenSSL is a library.  The code in question
    is not called from within OpenSSL AFAICT, so presumably it gets called from some higher layer.  Has somebody checked all possible applications to verify that whenever purify complains about an uninitialized seed, the app is doing things correctly?  If so, please cite a reference so we can all read about it.
Not as a strict proof, but as a plausible inference, experience
suggests that folks who use an uninitialized seed are doing so
because they don't trust their other seed-sources.  So at least
sometimes, it is a blind man clutching at a straw that isn't there.
Conversely, if there is a proof that uninitialized seeds are used
only when they are not needed, please explain.
Two wrongs do not make a right ... and fixing one wrong does not
fix the other.  Reverting an iatrogenic error does not mean that
the patient is cured;  the presenting complaint is still there.

@_date: 2014-12-03 14:25:38
@_author: John Denker 
@_subject: [Cryptography] Construction of cryptographic software. 
Hash: SHA1
That's a very useful way to frame the discussion.
Good construction practices are an excellent idea.  They reduce the attack surface by orders of magnitude.  OTOH
I would like to argue that they don't reduce it to zero.
They don't reduce it by the factors of 2^80 that we are
accustomed to seeing in good crypto primitives.
The problem is, I've almost(*) never seen a compiler, library, operating system, or hardware platform that could /really/ be trusted to have no side-effects.  Some are muuuch worse than
others, but even the top of the pile is underwater, AFAICT.
Compiler developers don't even think about side-effects
of the kind we are talking about.  They have a model that says the variables that are seen by the program have to
conform to the spec, but everything else belongs to the implementation.  The implementation details are "unspecified"
which means the implementation can do whatever it wants.
Ditto for libraries.  C++ classes have private member
variables and methods, for the express purpose of hiding
implementation details.  Hiding is considered a virtue.
The cryptographer's notion of "no side effects" is not even on the radar.
The issue of paging has already been mentioned.  You can try
to work around that by locking a crypto-sensitive workspace in memory, but that is not the end of the story.  Even if the
compiler is not trying to do any super-clever optimizations,
it still has to to compute stuff one sub-expression at a time and store stuff in registers.  If there is an interrupt,
the registers get saved who-knows-where, not in the locked workspace, and might very well get paged out or hibernated.
And then you have really lost control of the situation.
  Zeroizing a disk is no longer possible AFAICT, because
  the disk controller is constantly moving stuff around,
  for error-concealment and wear-leveling.  Main memory
  is not far behind.
  Heating discarded magnetic media above the Curie point
  still works, but is not what you want for routine day-
  to-day erasure.
(*) For super-sensitive tasks, you can use a Faraday cage
with a single-user machine on the inside and armed guards on
the outside.  However, alas, that leaves the other 99.9999%
of the problem unsolved, for instance if you want to write a reasonably decent browser or MUA for Grandma to use.
Bottom line:  I don't think we have the primitives to really
do what we want.  This is fixable, but it would require changes at every layer from the hardware on up.  This is
serious business.  Try to imagine a hardware instruction
that says
  onInterruptStoreMyRegistersHereAndNowhereElseEvenIfItHurtsPerformance
and imagine support for that all the way up to the language
and library level.
For today, we should do what we can.  It's a tricky two-part
 a) It helps to lock the front door and the back door, even
  though the side window is standing open.
 b) OTOH we should not overestimate how much good it will do.

@_date: 2014-12-04 13:56:29
@_author: John Denker 
@_subject: [Cryptography] converting one base to another 
Hash: SHA1
That's not a correct solution.  As stated, it's both biased
and many-to-one.  The bug is where it says "...small enough..."
Suppose the base-5 input has a string of N zeros in a row.
When you go to put these into the register, if the value happens to be zero initially, it will stay zero for a long time, and the output won't tell you how big N was.
Here are one way of fixing it:
Rather than thinking in terms of integers, think in terms of
a fixed-point fractions with the radix point in front, spanning the half-open interval [0, 1).
  -- The input  is a base-5 fraction of this kind.
  -- The output is a base-2 fraction of this kind.
Now consider the fraction 1/2.    On input, 1/2 is a repeating numeral in base 5, namely 0.222222*.   On output, 1/2 is obviously 0.100000*.
Any input that starts with the digits 0 or 1 is less than 1/2, so we know the first bit of the output is 0.  Any input that
starts with 3 or 4 is greater than 1/2, so we know the first
bit of the output is 1.  The nasty case consists of an input
that starts with N 2s in a row.  We have to sit on the fence
until we see the 1+Nth input.  When we hop off the fence,
we get to put out a very long string of output bits, so no
information was wasted.  However, there was some latency.
The latency problem recurs.  There are infinitely many
numbers that have a non-repeating base-2 representation
but a repeating base-5 representation.
The chance of a long latency is exponentially small, but
nonzero, and intuition tells me it cannot be completely
avoided if the bases are incommensurable, not without making compromises on the bias and/or the bijectivity.
Note that a similar but slightly harder problem arises in connection with the construction of a HRNG.  Input entropy arrives in base who-knows-what.  We want the output to be unbiased.  We want the mapping to be very nearly bijective, because that is the only way to do the job without wasting very much entropy.
Turbid uses a hash for this.  The result is very very nearly
bijective (although the inverse mapping is computationally
intractable and completely uninteresting).   The result is very nearly unbiased, in the sense that if you compared the output stream to a truly unbiased stream, it would take you longer than the age of the universe to tell which is which.

@_date: 2014-12-11 06:01:40
@_author: John Denker 
@_subject: [Cryptography] the hierarchy of cluelessness 
Specifically, they are calling for a *government-industry* cyber war council
On 12/10/2014 08:49 AM, John Ioannidis asked:
Executive summary:
 a) Yes, the banks are infinitely clueless... but we knew that already.
 b) The government is infinitely clueless.   c) Most of the bank customers are infinitely clueless.
 d) Other industry sectors are just as clueless.
 e) The mainstream media are clueless.
 f) The bankers blame the security geeks (i.e. us) for not providing
  them with easy-to-use tools.
Evidence for point (b):
 The NSA knew, or should have known, that what they were
 doing was wrong, but they did it anyway.  Consider the
 contrast:
 ++ The secrecy should be in the keys.  In contrast, the
  method "should not require secrecy, and it should not
  be a problem if it falls into enemy hands."
                     -- Auguste Kerckhoffs
 ++ "The enemy knows the system."
                     -- Claude Shannon
 ++ "In the long run it is more important to secure one's own   communications than to exploit those of the enemy."
                     -- Frank Rowlett
 -- "Let's create a situation where our friends can be spied
  upon more easily than our enemies."
                     -- NSA policy for 40+ years
Evidence for point (a):
 The fact that the banks would turn to the clueless government
 for help proves the banks are infinitely clueless ... but we
 already had plenty of other ways of proving that.
 Example:  The following email recently came to my attention.
     I called the bank.  The conversation went something like this:
  jsd:  This looks like a phishing attack.    bank: No, it is completely secure, because we sent it, and     our IT department is super-careful about things like that.
  jsd:  The little old lady who received this email, how is
    she supposed to know you sent it?
  bank: Because it's from us.
  jsd:  But how is she supposed to know that?  By sending this
    email, you are training your customers to be victims.  What
    if she got two emails on the same day, one from you and one
    from North Korea, which one should she trust?
  bank: She should trust the one from us.
  jsd:  And how is she supposed to know which is which?
  bank: It says right at the top it's from us.
  jsd:  And how hard do you think it is for the bad guys to
    forge that?
  bank: But the point is, we would never direct you to an
    insecure website.  It says so right in the email, at
    the top and the bottom.  We have super-strict rules and
    procedures about that.
  jsd:  Well, this email violates your own rules bigtime.  It
    directs users to two different web sites not your own, a
    third party and a fourth party.  The mail itself emanated
    from a fifth party.  You keep saying you sent it, but in
    fact you didn't.
  bank: But those web guys are under contract to us.
  jsd:  You know that, but the little old lady doesn't.  The
    bad guys could be preparing almost-identical emails right
    now, and none of your customers would be able to tell the
    difference.  You're training your customers to be victims.
  bank: This email is secure, because we sent it.
  jsd:  Aaaaarrrrrgh!
 They guy said he would pass my comments on to management.  I
 know this had no effect, because a couple of weeks later the
 bank sent out another round of equally-phishy emails.
Further evidence for (a), (b), (c), and the interaction between them:   The government has imposed voluminous regulations on the banks.
 A lot of stuff that used to be punishable by revoking your  banking license is now a felony.  There are also regulations  that require elaborate encryption of customer data.  The regs
 are so cumbersome that bankers find them hard to handle, and  most customers find them impossible to handle.  The result is
 that bankers have completely-insecure personal hotmail accounts
 that they use to communicate with customers.  After the deal
 has been worked out, they cut-and-paste it into the official
 bank system.  Ta Da!  Problem "solved".
Example of point (e):
 Today the Gomorrah Post ran a story about the Sony hack:
    It says in part:
 It seems to me that the CIO's point is absolutely valid.  There
 is a dial to adjust how much risk you want to accept, and one
 could argue that Sony set the dial in the wrong place, but one
 should not "ridicule" the idea that such a dial exists.
 The hypocrisy is stunning, given that Post itself has been hacked:
   Remark on the interaction between (a), (b), (c), and (f):
 The US black budget is on the order of 50 billion dollars  per year.  AFAICT virtually all of it is spent on offense,
 i.e. weakening perceived enemies.   Department of "Defense"
 my ass.  The overall "defense" budget is on the order of a
 trillion dollars per year.  The /profit/ in the US financial
 sector alone is something like 1.7 trillion dollars per year
 ... roughly 1/10th of the GDP.
 Imagine what would happen if the feds, the banks, Sony,  and other players were to invest some of their money by
 paying a few geeks to develop more easily-usable tools.
 You know, actual /defense/ as opposed to offense.
One more point:  The idea that China might try to pillage US
trade secrets should come as a surprise to nobody.  The US
did exactly the same thing to Britain 200 years ago.  See e.g.

@_date: 2014-12-18 14:18:44
@_author: John Denker 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Hash: SHA1
That won't solve the problem.  It costs well over a billion
dollars to set up a modern chip fabrication line.  That by
itself is at least 0.25% of the annual GDP of Belgium, or at least 1% of the annual GDP of (say) New Zealand.  And chips
aren't the whole story.
  a) Suppose they collect the tariff and put it in the treasury.
   Then the users would just be getting foreign chips at a higher
   price.
  b) Suppose they actually spent the money to build their own
   fab line.  That would be a tremendously wasteful drain on
   the economy.
Free-and-fair trade is generally a good thing.  When trade is
disrupted by tariffs -- OR BY LACK OF TRUST -- everybody suffers.
A hypothesis to consider:  Perhaps other economic forces could be brought to bear.  The perception (whether entirely true or
not) that Cisco's revenue is suffering from post-Snowden
distrust might be enough to change behavior.  On the other
hand, if you're in Belgium, buying tech from China instead of the US doesn't seem like an improvement.  Commercial advantage would accrue to the first vendor who could offer products with a warranty, certified free of back doors.
I'm not sure exactly how it would work, but one could imagine
independent certification laboratories, perhaps like UL.com
or kaspersky.com.  The labs could test a random sample of the company's products.  Certification by labs in two different
countries would make life harder for the nation-state bad
actors.  On the other hand we should keep in mind that there
are huge trans-national enterprises such as Goldman Sachs and ExxonMobil that think nothing of subverting multiple
governments at the same time.
Here's another hypothesis to consider:  Maybe the NSA should
pay attention to what Frank Rowlett said:
   "In the long run it is more important to secure one's own     communications than to exploit those of the enemy."
That is, imagine what would happen if NSA put more resources
into fixing stuff than into breaking stuff.
Here's a third hypothesis:  At some point the various players could get together and decide not to attack each other.  Before
you cue the "impossible" music
  remember that treaties /sometimes/ help a little bit.
  ++ The Vienna Convention on Diplomatic Relations sorta works.
  ++ The chemical weapons treaty sorta worked in Syria.
  ++ The nuclear nonproliferation treaty sorta worked in Libya.
  -- Although not so much in Pakistan, North Korea, et cetera.
  -- et cetera.

@_date: 2014-12-23 20:03:40
@_author: John Denker 
@_subject: [Cryptography] floating point 
I have no idea what that is trying to say, but whatever it is,
that's not the right way to say it.
There are some things that you can do with floating point, and
something you cannot.
 -- On any machine that uses IEEE floating point, in any normal   language such as c or python, if one external representation is   "0.0" and another is "0.", they have the same internal representation.
  Exactly the same.  Exactly equal.  So count me as one of the "alleged
  programmers" who thinks zero is equal to zero.
 -- In fact, in IEEE double precision, any integer from zero up to   9007199254740992 is represented exactly.  Comparing such things   succeeds reliably.
 -- A lot of other things, include 0.5, 0.25, 0.75 are also represented
  exactly.
 -- Et cetera.
On the other hand,   python -c "print (.6 + .3 + .1 - 1)"
does not produce zero.
Floating point numbers are a subset of the rationals.  As such, the
numbers themselves are exact.  Common operations may or may not
produce exactly-representable results.  The programmer gets to
choose which operations to perform or not perform.
Sometimes I compare floating point numbers, and sometimes I don't.

@_date: 2014-12-23 23:39:34
@_author: John Denker 
@_subject: [Cryptography] floating point 
That's just completely backwards.  If you want to roundtrip a
number -- i.e. write it back out and read it back in, producing
the same internal representation -- it is quite likely that you
need to print /more/ digits than the internal representation
can handle.
Roundtripping IEEE double precision requires 17 decimal digits.
This is well known in numerical analysis circles.  See code below.
    Of course if you want to go the other direction, hairpinning
    an external number into the machine and back out again, the
    limiting number of digits is lower ... but that's the answer
    to a very different question.
Perhaps the people who don't care enough about the subject to
dig beyond one-sentence slogans should refrain from sneering at people who do.
 /usr/bin/perl -w
use POSIX qw/ceil/;
# Returns the lowest safe upper bound on the number of
# decimal digits to guarantee that writing a number and
# reading it back in produces the same internal
# representation.
# Obviously we can only provide an upper bound; /sometimes/
# you can get away with something much lower, perhaps a
# single digit.
sub roundtrip_decimal_digs{
  my ($internal_radix, $internal_digits) =   $pack = log($internal_radix) / log(10);       # packing factor
  return ceil($pack*$internal_digits) + ($pack==int($pack)? 0:1)
print roundtrip_decimal_digs(2, 53), "\n";     # for IEEE double precision

@_date: 2014-12-25 22:55:12
@_author: John Denker 
@_subject: [Cryptography] floating point 
Executive summary:
 *) There are things you an do with floating point, and things you can't.
 *) Recognizing the distinction does not make you an "alleged programmer".
 *) Name-calling is not an acceptable substitute for facts or logic.
 *) Name-dropping allusions to famous scientists is not an acceptable
  substitute for facts or logic.
Here is a /partial/ list of issues and non-issues:
1) There is a distinction between /numbers/ and /numerals/.  It
 is quite common to have multiple representations for the same
 number, such as seventeen, 17, XVII, 0x11, 17.0000, and literally
 infinitely many others.  This is something that people were  supposed to learn in third grade.
 The fact that IEEE floating point has two numerals that represent
 the number zero is not a serious problem.  It is a third-grade
 problem.  It is manageable with minimal effort.
2) IEEE floating point is not the only game in town.  Forsooth,
 it seems likely that most of the floating point done in the
 world today is /not/ IEEE compliant.  Hint:  GPUs.
 If you want IEEE floating point behavior, and it is not  supported in the hardware, it is painful to implement it
 in software.
3) There are many different equivalence relationships in the
 world.  A blue triangle is shapewise_equivalent to a red  triangle.  A blue triangle is colorwise_equivalent to a  blue square.  Take your pick.
 In the computer, we have arithmetical comparison as well as
 bitwise comparison, stringwise comparison, et cetera.  Take
 your pick.  Different ones are good for different purposes.
4) Restricting attention now to IEEE floating point default
 behavior, there are three types of entities to consider:
  -- rfloat: regular floating point numbers, representing a     subset of the rational numbers.
  -- xfloat:  all the above, extended to include +inf and -inf.
    I don't care whether you consider infinity to be a "number"
    or not.
  -- xxfloat: all of the above, extended to include NaN, which
    (as it says on the tin) is /not/ a number.  Even though it
    is not a number, it is an xxfloat entity.
 So, to answer a question that was previously asked:  In the
 domain of xxfloats, the arithmetic == operator is definitely
 /not/ an equivalence relation, because it fails the "reflexive"
 requirement.  Specifically, (NaN == NaN) is false.  If we  don't have a viable notion of equality, we can't even ask
 the question about whether something is a function or not.
 In the xxfloat domain, arithmetical == does not imply bitwise
 equality ... or vice versa:
  -- 0.0 == -0.0 but the representations are bitwise different.
  -- NaN != NaN but the representations are bitwise the same.
 Do not ask me to explain why NaN != NaN returns true.  I am  quite aware of the traditional and historical explanations.
 They just don't make any sense.
 If we focus attention on the xfloat domain, narrowly speaking,
 then arithmetic == is an equivalence relation as far as I can
 tell.  In particular, let         a = 0.0         b = -0.0
 Then a is == to b, and any xfloat that is equal a is also ==  to b.
 On the other hand, if we mix ints and xfloats, then arithmetic
 == is not an equivalence relation, because if fails the  "transitive" requirement.  Specifically, due to a peculiar  notion of /promotion to float/ you can have p == q and q == r  but p != r.  I have code that demonstrates this.
 On the third hand, if p == 0 and 0 == r then p == r, so  the example that provoked this discussion is still not a  good example.
5) Within the xfloat domain, atan2 is /not/ a function, but  that's overkill;  there are plenty of simpler examples, not  involving transcendental functions.  Indeed, xfloat divide  is not a function, because (a == b) is true whereas (1/a == 1/b)
 is false.  Recall that
        a = 0.0         b = -0.0
 Also the usual decimal formatting is not an injective mapping,
 because b is arithmetically == to a, but str(b) is not stringwise
 equal to str(a).  That is, "-0" is not stringwise equal to "0".
6) Even within IEEE floating point, default behavior is not
 the only allowed behavior.  If you are writing a low-level
 library, with no control over the big picture, it is difficult
 to find /any/ floating point operations that are safe.  Even
 simple operations such as add, subtract, multiply, and divide
 could trap out and die.
 Conversely, some things that you might want to trap out or
 return NaN -- such as atan2(0,0) -- do not.  OTOH you could
 write wrappers for such things.
7) Looking at the standards won't tell you what happens in the
 real world.
  -- Chez moi clang++ will not trap on floating divide by zero,
   no matter how politely you ask, even though other traps
   behave as expected.
  -- Chez moi g++ labels every FP trap as "divide by zero", even
   when something else (e.g. underflow) actually happened.
  -- et cetera.
8) The things I am calling FP "traps" are commonly called exceptions,
 but they are not to be confused with C++ exceptions.  With a bit
 of work you can convert an FP trap into a C++ exception.

@_date: 2014-12-27 23:42:02
@_author: John Denker 
@_subject: [Cryptography] on-chip crypto accelerators (was: floating point) 
Hash: SHA1
One big reason we've been discussing floating point is that FPUs are very powerful and very widely available.
A goodly fraction of the real estate on a typical
processor chip is taken up by the FPU.  So if you can get the FPU to do your bidding, it's a big win.
Meanwhile, we should keep in mind that there are other things in the same category, including MMX/SSE as well
as GPU.  There exist crypto algorithms implemented as GPU programs.
More importantly, some modern processors have onboard crypto accelerators
      It may be that the FPU is not optimized for our purposes,
but that stops being relevant when the crypto accelerator
does the job.

@_date: 2014-12-27 23:52:06
@_author: John Denker 
@_subject: [Cryptography] floating point 
Hash: SHA1
Note that if you want a logarithmic representation you can
have it.  Just take logarithms at an early stage and proceed
from there.  For example, scientists and engineers spend a lot of time doing "least squares" fits.  The quantity being minimized is actually the /log/ probability, and the
sum of squares represents the product of the probabilities.
Things get weird if you ever need to add probabilities, but
as long as you are just multiplying them, the logarithmic
representation works fine.  It is so convenient that folks
do it without even thinking about it.
Floating point is *not* trying to approximate a logarithmic
distribution.  Just look at the bit-counts:  In IEEE double precision, there are 53+1 bits of signed fixed point, plus 10 bits of exponent.  Mostly it's just fixed point.
Also note: Plain old fixed point is still a thing.  There
is a lot you can do with plain old fixed point.  64-bit
fixed point has a great deal of dynamic range.  You can
convert an integer math processor to fixed point by waving
a feathered stick over it and declaring where you want the implicit radix point to be.
Fixed point has a lot of advantages.  Unless there is overflow,
 ++ addition is associative
 ++ multiplication distributes over addition
 ++ addition, subtraction, and multiplication are exact
Each of those things is only approximately (not exactly)
true for floating point.  Also:  If you want to do a lot
of fixed-point stuff, you can use a DSP or MMX/SSE.
Maybe you choose to care only about relative error, but there are lots of people who choose differently ... which is kinda where this conversation started.  There are a lot
of people who make use of the 2^54 integers that can be
represented in IEEE floating point, represented exactly,
with no relative error, absolute error, or any other kind of error.
It's not helpful to think of FP as a logarithmic representation
with a little bit of fixed point pasted on;  mostly it's just
fixed point, with a few bells and whistles to help you keep
track of where the radix point is.
There are some things that floating point can represent
exactly, and some things it can't.  For example, IEEE
double precision cannot represent 0.1 or 0.01 exactly.
However, you can sometimes work around this by combining
floating point with fixed point techniques.  For example,
by computing in cents rather than dollars, FP can represent
$.01 exactly.
Usually, IEEE floating point gives you multiplicative inverses
only approximately, so usually division will be inexact.
(17/10) * (10/17) - 1. = 1.19209e-07 in single precision.
Fixed point doesn't solve the problem.  A logarithmic
representation would give you exact inverses and exact
division, but only at the cost of making addition and
subtraction much messier.  For 99.99% of the customers,
well-behaved addition and subtraction are more important than exact division.  Besides, there are other ways of
getting exact division, notably by using /two/ of something
to represent (or approximate) rationals:  Two floats, two
ints, two bignums, or something like that.  This doesn't
solve all the world's problems, but it solves some of
them.  Software packages that deal with rationals have
been around for decades.

@_date: 2014-02-02 16:04:01
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
There's a medical proverb that says:
  Suppose you're standing beside a racetrack in Kentucky.
  If you hear hoofbeats, don't assume it's zebras.
  It's probably just horses.
If a host system wanted to be malicious, it would not need
to do anything as zebra-like as messing with disk timing.
It could just snapshot the entire guest memory, including
codetext, plaintext, session keys, long-term keys, random
numbers, non-random numbers, recent keystrokes, and everything
Forsooth, I suggest we use the other edge of that sword:
If you're going to be a VM guest at all, you should arrange
by contract for the host to provide you with entropy ... via
a virtual /dev/hrng device or some such.
At this point technical issue reduces to the somewhat simpler
case of finding a good source of entropy for the host.  This
does not create any /new/ trust issues of any significance.

@_date: 2014-02-03 13:55:57
@_author: John Denker 
@_subject: [Cryptography] request for consideration: VM guest entropy: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
That is indeed an interesting thing to "worry" about.
The question arises, will the "worrying" be limited to feckless
hand-wringing, or are we "worried" enough to actually do something?
  Qemu already knows how to provide the guest with a virtual /dev/hwrng
   device ... it's just not the default.  References:
              Suggestion   Make it the default, for security reasons.
  There is code in the linux kernel to check for the availability of the
   RDRAND instruction
        and some versions of /dev/random seem to know about this:
        However, AFAICT there is no place in the kernel that makes use of the
   /dev/hwrng device, even if it is available.
   Suggestion   On hardware where a satisfactory RDRAND instruction is     not native, one could teach qemu to trap and emulate this instruction.
    This looks like far and away the path of least resistance for introducing     effective randomness into guest systems.
   Tangential remark:  There are some hard feelings about the native RDRAND     instruction, due to lack of transparency as to its implementation.  One     could argue that the emulated version might actually be /better/ in terms
    of trust and transparency.
So, here's the action item:
Do we have consensus on this list that the foregoing suggestions are reasonable?  They obviously don't solve all the world's problems, but
can they be considered cost-effective steps in the right direction?
Can anybody think of any improvements that would reduce the barrier to
  Actually, I can think of about a dozen improvements, but in the spirit
  of not making the perfect the enemy of the good, I hesitate to mention
  them.  It would be nice to get /something/ reasonable deployed, and
  then make incremental improvements.
The first step is to get consensus within the crypto/security community.
Then we can bring the qemu developers into the discussion.
Comments?  Questions?  Suggestions?

@_date: 2014-02-03 18:11:17
@_author: John Denker 
@_subject: [Cryptography] request for consideration: VM guest entropy: 
On 02/03/2014 05:23 PM, Theodore Ts'o wrote in part:
Better?  Better than what?  I'm pretty sure that nobody
suggested avoiding the /dev/u?random interface.  Instead, I rather explicitly pointed out that emulating the rdrand
instruction was the path of least resistance for getting
entropy *into* /dev/random.
  On the other side of the same coin, it is is not "better",
  it is not even good to tell people to obtain entropy   from /dev/u?random device in situations where there's
  no reliable way of getting entropy *into* the device.
Always?  This whole thread is predicated on the observation

@_date: 2014-02-04 00:17:05
@_author: John Denker 
@_subject: [Cryptography] Random numbers only once 
I mostly agree with the sentiment.
To say the same thing in my own words:  We can agree that "most" needs can be met with a PRNG.  However, every PRNG needs to be seeded, which
means that at /some/ point a good HRNG is needed.
Let's be clear:  One can imaging a HRNG without a PRNG, but not vice
versa.  Contrary to what iang keeps saying, at /some/ point /some/
actual entropy is needed.
This should be done as a routine part of the /provisioning/ process.
The problem is, a lot of people make incredibly foolish assumptions
about RNGs.  Here are a few of the "greatest hits":
1) Starting with the correct assumption that a high-entropy seed
 must be available at /some/ point, certain implementers leap to  the incorrect assumption that a steady supply of entropy will
 "always" be available.
 It sounds ridiculous when said that way, but certain well-known
 RNG implementations really do embody the assumption that entropy  will "always" be coming in at some rate, possibly a small rate,  but nonzero.
 Sometimes this assumption is implicit and deeply buried in the
 code ... but sometimes it is blurted out quite explicitly.
2) People sometimes do foolish things in the name of "recovery
 from compromise".  Apparently the argument is that the PRNG
 is "better" if it recovers more quickly from compromise.  When
 carried to the extreme, this means that routine, unavoidable,
 non-critical use of /dev/urandom is tantamount to a denial-of-
 service attack against /dev/random, and against the critical
 upstream entropy sources that are called upon to feed /dev/random.
 This is particularly foolish given that it is predicated on an
 artificial, stylized threat model, namely a one-time passive
 capture of the PRNG state and nothing else.  In the real world,  there are also active attacks.  There are also ongoing passive
 attacks.  There are also attacks that capture a whole lot more
 than the PRNG state, including passwords and keys, such that
 reseeding the PRNG is verrrry far from sufficient to recover
 from the real compromise.
3) There are RNG implementers running around who don't understand
 what entropy is, or don't care.  It is easy to find "extract_entropy()"
 routines that extract no entropy, when used in the normal way.
4) A lot of people wildly underestimate how hard it is to properly
 build and operate a RNG.  They think they understand the problem,
 when they really don't.  When you tell them the PRNG is not being
 properly seeded, they get the brilliant idea of using the host's
 MAC address as the seed, as if the attacker could not possibly
 figure out the MAC address.  If you argue with them, they come up  with the brilliant idea of using the real-time clock, as if the  attacker could not possibly figure out what time it is.
 These arguments sometimes come from "official" package maintainers
 who are in a position to block upgrades to the widely-used distros.
5) There is a lot of mental inertia.   People assume that just
 because a certain RNG has been in use for years, it must have
 been well designed.  They assume that "somebody else" must have  thoroughly reviewed the code at some point.  They assume that  whatever assumptions were made originally must remain valid when
 the code is ported to new platforms, even wildly dissimilar  platforms.
6) People outside the crypto community wildly underestimate the
 importance of having a good RNG.   They are unaware of the lurid
 history of RNG failures.  Bug reports against the RNG system are  given near-zero priority.
These are all fixable problems.  The need to get fixed, pronto.
I'm reasonably optimistic that they can be fixed.

@_date: 2014-02-04 20:53:43
@_author: John Denker 
@_subject: [Cryptography] Random numbers only once 
way around this.  See further discussion below.
No, that's not the right answer in general.
 -- We must distinguish between /dev/random from /dev/urandom.
 -- We must also distinguish between how things presently are
  from how they should be. There should be no excuse for /dev/urandom blocking, ever.
There should be no excuse for /dev/urandom doing anything except returning high-quality hard-to-predict pseudo-randomly
distributed numbers.
This should be adequate for almost all "ordinary" uses.
Note that the for all internal-to-the-kernel purposes, the
linux kernel depends on the PRNG provided by /dev/urandom.
  The problem is, due to bad design, bad implementation, and
  bad operation, there are lots of scenarios where /dev/urandom
  returns low-quality squish.  This needs to get fixed, pronto.
  Fixing it MUST NOT involve making it block.
  In particular, much more attention needs to be paid to the
  issue of per-instance /provisioning/.  Every piece of hardware
  and every instance of a VM guest needs to be provisioned with
  a high quality seed.  This needs to be done before the PRNG
  is used for the first time.  Provisioning is part of why I   can say that there should be no excuse for /dev/urandom to
  block, not ever, not even the first time.
In contrast, /dev/random should be for wizards only.
It might block.  It might block now or later.  It might stay
blocked for an arbitrarily long time.  Anybody who needs to
use /dev/random needs to understand this.
As we have discussed before:  "Most" applications should be
content with high-quality pseudo-randomly distributed numbers.
However, every PRNG needs to be seeded.  Therefore at /some/
point there needs to be /some/ actual entropy coming from
somewhere.  One can imagine a HRNG without a PRNG but not
vice versa.
This distinction is important and will always be important.
For many years, due to bad design, there has been an excessively
close coupling between /dev/random and /dev/urandom, leading to
needless waste of entropy, needless waste of CPU cycles, and needless confusion about what these devices are and how they should be used.
This bad design explains why linux kernels have traditionally stored a seed (relevant to /dev/urandom) but have not bothered
to store high-quality entropy (relevant to /dev/urandom).
I've been collecting a bunch of FAQs and best practices at

@_date: 2014-02-05 02:11:09
@_author: John Denker 
@_subject: [Cryptography] who cares about actual randomness? 
In the context of /dev/random versus /dev/urandom, How about you and me get together for a nice friendly
game of poker.
I'll bring the cards.  I'll bring several decks, so we can
use a fresh deck for each hand.  This saves time, because
they're already shuffled, using my favorite PRNG.  The PRNG
is strong enough to make it computationally infeasible for
you to find any non-random pattern in the cards.

@_date: 2014-02-05 13:35:34
@_author: John Denker 
@_subject: [Cryptography] who cares about actual randomness? 
Millions of people do play for high stakes, placing their trust
in entropy -- you know, real physics entropy -- and in procedural
safeguards such as clear plastic boxes.
  Can you convince these people to trust a PRNG instead?
Can you make even a plausible argument that they would be better off using a PRNG instead?
And, by the way, where did you get the /seed/ for your PRNG, for
this application or any other?
As I have said before:     I can imagine a HRNG without a PRNG,
   but not vice versa.

@_date: 2014-02-05 14:45:05
@_author: John Denker 
@_subject: [Cryptography] Random numbers only once 
Physician, heal thyself.
There is such a thing as entropy -- you know, real physics entropy.
If you want to talk about something else, that's fine, but please
don't call it entropy.  Call it something else.
There are parts of linux drivers/char/random.c that make a reasonable
effort to distinguish entropy from non-entropy ... yet other parts utterly drop the ball.
That's the wrong explanation.  Compromise has got nothing to do with it.
On linux systems, the state file in question gets written using bits from /dev/urandom (*not* /dev/random).  Reference:
   /etc/init.d/urandom line 76
As such, on typical systems over most of the last many years, the
state file contains zero entropy.  It would be quite ludicrous to reboot and load this state file and credit to the system more entropy afterward than before.
Let's be clear:  No entropy credit is the right answer because of no entropy ... for reasons having nothing to do with compromise.
On the other side of the same coin:  In a wide range of typical
systems, security depends on the state file being "secure" in
the sense of being computationally strong.  Therefore compromise
of that file is a disaster *whether or not* any entropy is
"credited" to it.
To summarize:  As applied to the state file, equating zero entropy to compromise is wrong coming and going:
 -- zero entropy "credit" doesn't mean you've been pwned
 -- zero entropy "credit" doesn't mean you can't be pwned
Currently typical systems make no attempt to store entropy.  This
would not be hard to do;  they just don't do it.
The usual advice is, if you store some entropy, make sure linux most of the last many years, linux /dev/random has been notorious for wasting entropy.
  This is a moving target, and I emphatically decline to comment   on what "the" code doing at the moment.  I keep hoping it will
  get fixed some day.

@_date: 2014-02-06 03:33:41
@_author: John Denker 
@_subject: [Cryptography] who cares about actual randomness? 
That is not "The Answer" in general.  It is partly sloppy and
partly just wrong.
First of all, the idea of "tiny" entropy is neither a necessity
nor a virtue.  There are plenty of real-world situations where
the supply of entropy is more than sufficient, not at all tiny.
Secondly, redundancy is not a virtue unto itself.  Reliability
is a virtue, but multiplicity by itself is neither necessary nor
sufficient for reliability.  For clarification on this point, see
    Thirdly, you don't necessarily need a cipher.  For most applications,
you need a PRNG.  You can turn a hash into a PRNG, and you can turn a
cipher into a hash, but strictly speaking a "stream cipher" is not
in the critical path.
Fourthly, it causes problems to say sloppy things like "turn a little drop of entropy into a firehose".  In accordance with the ordinary rules of English, people will take that to mean that a little drop of entropy is being turned into a firehose of entropy
... which is not what is going to happen.  The laws of physics forbid it.
If you mean to talk about a high-rate, computationally-strong random
distribution, please say so, using actual words.
Overall, I beg of you to say what you mean and mean what you say.
It causes real problems when people assert that XYZ is necessary and/or sufficient when in fact it is not.
I've been collecting FAQs and rules of thumb at

@_date: 2014-02-07 13:40:06
@_author: John Denker 
@_subject: [Cryptography] RAM memories as one source of entropy 
It is good to recognize these ideas as very loose.
They are at present far to loose to be usable for
any serious purpose.
That resolves to
  Keaton Mowery, Michael Wei, David Kohlbrenner, Hovav Shacham, and Steven Swanson
  "Welcome to the Entropics: Boot-Time Entropy in Embedded Devices"
  I do not consider that to be serious research.  For those who are looking for entropy (as mentioned in the Subject: of this thread), you need to look elsewhere.  The key statement in the paper is:
Then the paper proceeds to use statistical tests to measure some kind
of "unpredictability".  This whole approach -- statistical testing --
has long since been completely discredited.  To borrow a phrase from
Dykstra:  Testing can prove the absence of entropy;  it can never prove
the presence of entropy.
The stuff Mowery et al. obtain is not entropy;  it is /squish/ by which
I mean it is neither reliably predictable nor reliably unpredictable.
That resolves to
  Daniel E. Holcomb, Wayne P. Burleson, and Kevin Fu
  "Initial SRAM State as a Fingerprint and Source
     of True Random Numbers for RFID Tags"
The paper mentions "thermal noise".  I am willing to believe that thermal
noise plays /some/ role in SRAM initial state ... but this paper utterly
fails to calibrate or even estimate the size of the effect.  It cites
Nyquist and Johnson, but doesn't learn from them.  What is the source
impedance?  What is the gain?  What is the bandwidth?  I see nothing
resembling a lower bound on the entropy content;  instead I see non-
quantifiable statements like
Like reference [1], this paper [2] proceeds from the ludicrous Manichaean
assumption that anything that is not reliably predictable must be reliably
unpredictable.  They ignore the third possibility, namely squish.
There are all sorts of ways the FERNS approach can go wrong.  For starters,
it is sensitive to how long the device was powered down before powering it
up.  It is also sensitive to how quickly the supply voltage ramps up.  The
entropy production, if any, will vary from manufacturer to manufacturer and
from year to year.
Last but not least, there are intrinsic limitations associated with using
a one-bit digitizer (such as an SRAM cell) as opposed to a high-resolution
linear digitizer (such as a sound card).  This has to do with /headroom/.
Headroom allows you to detect attempted interference at a point /before/
it becomes an operational problem.  It also makes it easier to do a proper
That resolves to
   Anthony Van Herrewege, Vincent van der Leest, Andr? Schaller,
     Stefan Katzenbeisser, and Ingrid Verbauwhede
  "Secure PRNG Seeding on Commercial Off-the-Shelf Microcontrollers"
This work was carried out with a modicum of intelligence.  They tested two
families of chips, and noticed that one family exhibited a pattern that
made the whole approach obviously unworkable.
As for the other family, I say that just because there is no obviously
disastrous pattern does not mean there is no pattern.  There could easily
be a less-obvious pattern.
Approaches like this will not be taken seriously until somebody answers
the basic questions about source impedance, gain, and bandwidth.
ALSO:  From an engineering point of view, for a very wide range of
applications (albeit not all), you will be much better off using a
well-characterized PRNG, provisioned with an exogenous seed stored in NVRAM.
If you want to supplement this with some endogenous entropy, that's
fine ... but it can happen on a much slower timescale, using a
relatively low-rate HRNG if necessary.
Bottom line:  Security depends on attention to detail.  An ounce of
calibration is worth more than a ton of wishful thinking.  An ounce
of real entropy is worth more than a ton of squish.
It is true that /some/ people are unable to distinguish S from Shinola.
However, that is not a law that applies to the rest of us.  Most of the people on this list have progressed beyond the baby-talk stage.
I expect most people on this list to act like adults ... indeed like professionals.  I expect they will make some effort to think clearly and communicate clearly.

@_date: 2014-02-14 18:33:44
@_author: John Denker 
@_subject: [Cryptography] The ultimate random source 
Answer: Yes.
Entanglement has got nothing to do with it.
Quantum mechanics is relevant only in the smart-aleck sense that quantum mechanics explains and /includes/ all of classical physics as a special case.
   BTW FWIW the reverse is not true:  Classical physics
   cannot explain QM.
OTOH if you meant to exclude classical physics, that's a
mistake.  The second law of thermodynamics is just as well
"verified" as the "uncertainty principle" ... and in fact,
if you look at them closely enough, you find that the are
essentially synonymous.  There is no such thing as purely
zero-point fluctuations as distinct from purely thermal
fluctuations;  those are just two asymptotes on the *same*
  Bottom line:  Thermal noise is a random process.  The
principle of the thing is verrrry well verified.  Everything
after that is implementation details.  The details are
important, but they don't change the principle of the thing.
Quantum noise is no better than thermal noise.  It is no worse except insofar as the details are harder to manage, which they generally are.
Regarding the M&M RNG specifically:
Firstly:  I assume everybody knows that the Subject: line
is a joke.  This is not the ultimate random source.  It's a facetious random source.  There exist other sources that are cheaper, faster, and in every way better.
Secondly: The analyses that have been offered so far are in
the facetious and/or amateur category.  They are the sort of
analysis that even if done carefully would only result in
an estimated /upper bound/ on the entropy.
  Security depends on a reliable lower bound, and I haven't
  seen anybody even try to calculate that for this system.
Thirdly:  Fussing over "sensor noise" misses the point of
this example.  The M&M source is designed so that it does not depend on sensor noise.  In this sense, it belongs in the same general category as pointing a camera at a powerball machine, or pointing a camera at a coin toss.  For better or worse, the fundamental random source process is several stages upstream of the camera.
The problem with sensor noise is that a lot of people seem
unwilling to do the work required to calibrate (i.e. "validate")
the sensor in enough detail to obtain a reliable lower bound
on the entropy.
The M&M process gets around that problem, but AFAICT it
throws the baby out with the bathwater, insofar as it would be /at least/ as hard to validate this source,
i.e. to calculate a reliable lower bound on the entropy.
I'm not saying it couldn't be done ... but I am saying it would be a lot of work.
Fourthly:  It is a mistake to think that the fundamental
process here has anything to do with capturing the randomness
provided by the human being who shakes the beaker of M&Ms.
If this source works at all, it works on another principle.
You could use a non-human robot to stir the M&Ms.
There is such a thing as /chaotic dynamics/.  In a chaotic
system, the state at some large time T is exponentially
sensitive to initial conditions.  In other words, it is a
noise amplifier.  The so-called "butterfly effect" is a
familiar metonym.
If the M&M source works at all, to validate it you would
need to demonstrate that there is chaotic dynamics at work.
I reckon it is, but you would need to demonstrate it and
quantify it, not just assume it.  You would need to show that it outruns various segregation processes.  There is serous, non-facetious research that is relevant, in the materials literature:
   Williams, J. C.    "The Segregation of Particulate Materials. A Review"
   Powder Technology 15, 245-251 (1976).
and in the physics literature:
   Rosato, A., Strandburg, K. J., Prinz, F. & Swendsen, R. H.
   "Why the Brazil Nuts are On Top: Size Segregation of Particulate Matter by Shaking"
   Physical Review Letters 58, 1038-1040 (1987).
   By way of cautionary tale, pointing a camera at an ordinary
coin toss would be a terrible idea.  If you were to assume
the coin exhibits chaotic dynamics, you would be in for a
nasty surprise.
   Persi Diaconis, Susan Holmes, and Richard Montgomery
   "Dynamical Bias in the Coin Toss"
   SIAM Review, 49(2):211-235 (2007)
   I suspect M&Ms are better, but you would need to demonstrate
that, not just assume it.
An ounce of calibration is worth more than a ton of wishful

@_date: 2014-02-21 03:03:24
@_author: John Denker 
@_subject: [Cryptography] The ultimate random source 
That's very much more sensible than packing them into a beaker,
which was the original proposal.
Even if that were the right number, the only thing that would
matter would be the logarithm thereof.  For independent events,
the entropy would be 600*log(6), which is about 1551 bits.
This does not overflow even the humblest scientific calculator.
In fact, you can calculate it using a slide rule.
Not really.
That's not the right thing to be calculating.  It's off by a
factor of several million.  Even if it were the right question,
it would be pointless to grind out the 466-digit answer.
Now log(6^600) would not be a bad guess, and it serves as an upper bound to this component of the entropy, but as a matter of principle it's not the right thing.  It assumes that the occupation of each
site is independent, which surely isn't the case.  Look at it this way:  The color of the final M&M is completely determined by the state of the previous 599 M&Ms, so it contributes exactly zero entropy.
A better estimate of the multiplicity is 600! / 100!^6 and upon
taking the logarithm we find the entropy is approximately 1529 bits
... about 22 bits less than the original guess.
So this particularly obvious type of correlation makes only a 1.4% correction to the entropy.  That seems small in relative terms, but
on the other hand, if you sold this thing as a stream cipher and it
put out 1529 random bits followed by 22 completely predictable bits,
your customers would have you tarred and feathered.
  On the third hand, if all you needed was 150 bits for seeding a
  CSPRNG, then 1529 is plenty, even if there are a few nonidealities
On the fourth hand, there will be other types of correlation as well, depending on as-yet unspecified details of the mixing process.  You
need to get a handle on this, to demonstrate that the fundamental
front-end physics really is giving you some entropy.
To summarize the main points:
  a) Cryptography requires attention to detail.
  b) There are a lot of things that can make the entropy less than    you guessed it was.
  c) The security of a RNG depends on having a reliable lower bound    on the entropy.  Guesses, estimates, and upper bounds do not get
   the job done.
  d) Entropy is super-important, but it does not tell you everything
   you need to know.  It does not by itself solve all the world's
   problems.  Much depends on how the entropy is used.

@_date: 2014-01-20 11:59:47
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
That is not the smart way to think about it.
Let's consider a few basic types of signal:
1) At one extreme there is perfect randomness, by which I mean  100% entropy density, such as a source that produces 8 bits of
 entropy in each 8-bit byte.  There is no pattern whatsoever.
2) There is also pseudo-randomness, which means that there is  a pattern, but it is computationally infeasible for the adversary  to discover the pattern. For a wide range of purposes, the  adversary finds this to be just as unpredictable as real entropy.
3) At the opposite extreme, there is perfect determinism, i.e.  no randomness at all, such as source the produces an endless  string of bytes all the same, or perhaps successive digits of ?.
4) There is also something I call squish, which is neither reliably
 predictable nor reliably unpredictable.
5) Combinations of the above.  For example, there could be a  distribution over 32-bit words having a few bits of entropy in  each word, with the other bits being deterministic and/or squish
For details, including diagrams and additional discussion, see
  In computer science and engineering there is the notion of
"proof of correctness".  Can you /prove/ that a human waving
his arms around is random?  More specifically, can you establish
a provable lower bound on the entropy-content?
All the literature I've seen says that humans are /not/ in
fact very good randomness generators.
Until there is a proof to the contrary, we should assume that
a human waving his hands around is squish, not high-quality
randomness.  It is squish, not entropy.  It is not reliably
predictable, but alas it is not reliably unpredictable.
When we want real entropy, as opposed to squish, we should
not depend on human behavior.  It is much better to depend on the second law of thermodynamics.
Any sensor will have a certain amount of Johnson noise.  This
applies to audio inputs, video inputs, accelerometers,
thermometers, et cetera.  This applies whether or not anybody
is talking to the device or waving it around.
An adversary could inject additional noise, but
 a) In the linear regime, that would do him no good, because
  the Johnson noise would still be there, and
 b) Long before the system went nonlinear the attack would
  be easily detectable.  It would be nothing more than a   crude denial-of-service attack.
Any attempt to /remove/ the Johnson noise or replace it with
something else would either  a) violate the basic laws of physics,
 b) render the sensor unusable for its nominal purpose,
 c) be ridiculously easy to detect, and/or
 d) be completely impractical, in the sense that it would be
  so difficult to carry out that other, more direct attacks
  would be a far better use of resources.
The proof of correctness is not easy, but it is entirely doable.
It requires an understanding of physics, electrical engineering,
computer science, and cryptography.
Any such device needs to be /calibrated/.  That includes ascertaining
the input impedance and the gain*bandwidth product.  This is not
trivial.  Sometimes it is not doable ... but sometimes it is.  Audio inputs have the advantage of being relatively easy to calibrate.
To say the same thing the other way:  When I see a discussion
of "high-quality randomness" that does not include a calibrated,
provable lower bound on the entropy content, I have a hard time taking it seriously.

@_date: 2014-01-20 14:38:19
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
I still claim it is worth quantifying things as much as you
can.  The calculation is not that hard to do, and it tells you
things you wouldn't otherwise know. It tells you that if you are using an accelerometer to capture
the human interaction, the physics of the sensor is a better source of entropy than the human is.  More specifically, it
tells you that choosing a sensor with more gain and/or more bandwidth buys you a lot more than any amount of wild arm-waving.
The arm-waving increases the amount of squish, but it does not
appreciably increase the amount of hard-core entropy.
Given a high-precision microphone preamp, it provides better randomness if the input is open-circuited, rather than attached
to an actual microphone, no matter how "complex" the acoustic
environment is ... and it continues to work even in non-complex That suffers from the fact that the bandwidth is not large.
The mass of the arm produces a low-pass filter, in accordance
with Newton's second law of motion.  If you did the calculation, you would notice this immediately.
If you don't know how to do the calculation, that does not
give you a license to just give up.  If you don't know how to do the calculation, collaborate with somebody who does.
Speak for yourself, Kemosabe.
The fact is:
  a) Statistical tests provide only upper bounds.
  b) Security depends on establishing a lower bound.
  c) Under reasonable conditions, physics can provide    the required lower bound.
Petitio principii is not an acceptable substitute for actual
The world I live in is not deterministic.  This is required
by the laws of thermodynamics, not to mention quantum mechanics.
Computers are deterministic, to a good-enough approximation,
because engineers go to a lot of trouble to make them so.
Everyone should please refrain from assuming that the real
world works the same way a computer does.  Specifically:
Any Turing-computable model of the universe would be at best
a hidden-variable theory.  It appears that any such theory
that is subject to relativistic causality would violate the laws of quantum mechanics in general, and the Bell inequalities
in particular.
So what's the deal?  Are you assuming everything we know about QM is wrong, or everything we know about relativity
is wrong?  Do you have any actual evidence?
That would be the /conditional entropy/.  That is related
to conditional probability in the same way that entropy
is related to probability.
A good TRNG produces unconditional entropy.  I could not
predict the output of my own TRNG even if I wanted to.
Statistical tests on the RNG output (or the raw sensor
data) can provide upper bounds on the entropy.  This is sometimes interesting, but it is nowhere near sufficient for building a secure system.
Not necessarily, unless you plan on violating basic, long-
established laws of physics.
As James Randi is fond of saying:  Extraordinary claims require extraordinary proof.  I haven't seen any proof -- or indeed any
evidence -- that any "better model" can remove Johnson noise
from a signal.

@_date: 2014-01-21 13:18:42
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
Very nice.
Conversely:  Demented Squirrel Fail:
  The claim that you have 5000 sources of food buried in the forest,
  even though you can't be bothered to defend them or even check on   them.
Similarly:  Knuth's chapter on random number generators starts with an example where combining a whole bunch of lousy RNGs
does not make the result better.  It makes it worse.
My point is that it makes more sense to have one or two
properly-calibrated well-defended entropy sources than
some vast number of "sources" that might produce entropy
or might not.

@_date: 2014-01-29 15:34:42
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
Yes sir.
That's what we call wishful thinking.  It is *not* a good
Anybody who believes that argument must think quadruple-rot13 is a good cipher.
My point is:  Combining a bunch of sucky crypto primitives is *not* a good practice.
One well-calibrated well-defended well-monitored entropy source
makes incomparably more sense than an arbitrarily complicated
conglomeration of sucky sources.
To those who say calibration is hard:  Yeah, a lot of things in
cryptography are hard.  We're supposed to be grown-ups here.  We
don't just give up when we see something that requires a bit of
If you want us to use the thermostat A/D that's fine ... provided
you tell us how to calibrate it.
To those who say different platforms will have different entropy
sources:  Yeah, they also have different graphics hardware,
different networking hardware, different disk hardware, et cetera.
We deal with that by loading the appropriate drivers.

@_date: 2014-01-29 17:38:46
@_author: John Denker 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
One reason there is a permathread is that people keep saying
wrong things.
That's a red herring.  The attacker can also bugger the BIOS,
bugger the keyboard scanner, et cetera.  Supply-chain trust
is a problem.  This problem does not affect entropy-sources
any more than it affects a hundred other things.  If you're
going to give up on entropy because of supply-chain trust
issues, you ought to give up on everything.
Also:  One point that the web page doesn't mention:  It helps
to use general-purpose components.  Using special-purpose crypto chips (including RNG chips) is like putting a "kick me" sign on your own back.  In contrast, a sound card can be put to lots of different uses, and it is relatively hard for the bad guys to mess with it in a way that subverts the crypto without making the device unusable for other purposes.
This doesn't solve all the world's problems (angry birds) but it helps.
That's imprecise and overstated.  Not all measurements are
created equal.
  a) We agree that statistical tests on the output are mostly
   window-dressing.  As Dykstra said, testing can show the    presence of bugs, but it can never show the absence of bugs.
  b) On the other hand, there are some things that do need to
   be measured, such as the impedance, gain, and bandwidth
   of the source.  These physical measurements are not even    remotely in the same category as statistical tests on the
   outputs.
Statistical tests on the output are mostly pointless but harmless.
The real point should be twofold:
 a) Do not rely on statistical tests on the output.
 b) Do the calibration.  Do the /right kind/ of measurements.
  An ounce of calibration is worth more than a ton of   wishful thinking.
Yeah, different machines will have different peripherals.
So what?  We have for decades been dealing with different
graphics hardware, different disk hardware, different
networking hardware, even different math coprocessors.
We deal with this by loading different drivers and different When faced with the equivalent problem in RNG space, it
would be ridiculous to just give up.
Nonsense.  It is definitely reachable.  Yeah, it's hard, but
lots of things in cryptography are hard.  We don't just give
up when we see something that requires a bit of work.
I assume that refers to something involving a PRNG.  The problem
with all such things is that they require a seed ... whereupon
we need a HRNG anyway.
Let's be clear:  You can have a HRNG without a PRNG but not
vice versa.
At present there are plenty of environments that are critically lacking in trusted sources of entropy.  They will remain so
unless and until somebody takes pains to remedy the situation.
A server farm with lots of VMs is an obvious and important example.
One critical requirement is proper /provisioning/ of each new
machine with its own initial endowment of entropy.  Later, during
routine operations, there are ongoing problems.  If the main
array of servers lacks well-calibrated well-trusted sources of
entropy, one way to alleviate the problem is to buy a couple of special machines, special enough to have sound cards.  These
distribute random outputs around the room via secure links.  The VM guest machines can read from a virtio-rng device, or from a To summarize:  Giving up on the HRNG is neither necessary nor

@_date: 2014-01-29 22:36:39
@_author: John Denker 
@_subject: [Cryptography] cheap sources of entropy 
Well, that's basically the right idea.  I will assume(*) that
by "mix" you mean something like a good cryptologic hash.
Let's explore the consequences:
As a corollary, if you have one truly completely unpredictable
input, the others don't help.  This is consistent with my
statement [A].
  If you want to talk about redundancy, we need to have a muuuuch
  more detailed discussion.  If you're serious, we would have
  to work out a full fault tree to check for correlated failures.
Conversely, there are a lot of people -- in this forum and
elsewhere -- who seem to think they can make a silk purse out of a sow's ear, if only they can get their hands on "enough" sow's ears.  There is nothing in statement [B] to support this approach.  Basic engineering principles and experience indicate that this is not, in fact, a viable approach.  This is consistent
with my statement [A].
  If you want to talk about combining multiple *good* entropy
  sources, we can do that.  However, any one of them would   serve as the basis for a proper HRNG.  Combining them just   improves the output rate, without changing the principle of
  the thing.  We are talking about multiple good, well-calibrated
  well-defended well-monitored sources.  All this is consistent
  with my statement [A].
(*) Not meaning to derail the conversation, but there are lots
of "mix" functions, not all of which are suitable for this
application.  For example, if you /collate/ the inputs, the
output could very well be far from unpredictable, even if one
of the inputs is truly unpredictable.
Also, if you take two inputs, each of which /by itself/ is
completely unpredictable, bad things might happen if you "mix"
them using XOR, since they might be correlated.  Let's not even discuss foolish "mix" functions such as Boolean AND.
The thing I don't understand is why any of this should be
considered controversial.

@_date: 2014-01-30 19:26:29
@_author: John Denker 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
Oh, you mean the way the current generation of electronic
voting machines were "validated" by "independent" testing
labs as required by law?  It's a travesty.
   What's to keep the TLA that subverted the design of the crypto
chip from subverting the validation procedure?
The only procedure I've heard of that makes any sense is based
on cut-and-choose.
 -- Somebody makes a million sound cards, intended for the genuine
  audio market.
 -- I buy a bunch of them.  I select a subset at random and tear
  them down.  Anything that does not conform to the sound-card
  blueprint is disqualifying.
 -- If I don't like what I see, I can return the whole batch to
  the sound-card market ... which is something I could not do   with purpose-built crypto products.
 -- No, I'm not panicked about side-channel attacks.  The sound
  card is already well shielded, for ordinary audiophile reasons.
  There is no reason to think that the sound subsystem is more   vulnerable than the networking subsystem or the memory subsystem
  or anything else.  Furthermore I can deliberately transmit a
  jamming signal that swamps whatever is leaking out of the sound
  card, with orders of magnitude to spare.
 -- Similarly I'm not panicked about incoming interference.  The
  soundcard is already well shielded, and furthermore any such
  interference would be detectable long before it caused real
  degradation, with orders of magnitude to spare.  So the best
  an attacker could hope for would be a preposterously expensive
  denial-of-service attack that called attention to the attacker.
To be clear:  At the moment I have not seen any off-the-shelf
soundcards with a validation I trust, but on the other hand
I haven't seen anything else I trust, be it purpose-built or otherwise.  So you can argue the comparison either way.  It's an indeterminate form, travesty divided by zero.
The same applies to software:  Open-source software "could" be
reviewed by anyone, which is a lovely theory, but in practice
a lot of the stuff we rely on has never been subjected to anything remotely resembling a rigorous code review. A thousand
cursory checks are nowhere near as useful as one thorough,
professional review.
Face it, the community has not learned to take security seriously.
There is a treeeeemendous seriousness gap, because the attackers
do take their job seriously.  If we spent anywhere near as much
securing Android as They-Who-Shall-Not-Be-Named have spent subverting it, the world would be a far different place.

@_date: 2014-07-04 12:55:33
@_author: John Denker 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Generally not directly, not in so many words, but indirectly yes, in some scenarios ... and obviously so.
 -- Scenario   Your code contains a longstanding bug ? la
  heartbleed.  The NSA knows of the bug, and has classified   this knowledge.  Your buddy with the clearance is most   definitely "restricted" from telling you about it.  So
  you continue shipping the buggy code.
 -- Et cetera.  IANAL, but obviously there is a very wide
  range of possibilities to consider.
Also note that "security clearances" come in many different
flavors and colors.  Everyone -- whether *OR NOT* they hold
a security clearance -- is forbidden by law from disclosing certain types of classified information;  as one example, see
  In theory, they could convict you of disclosing information
that you invented on your own, even if you didn't learn of
it through classified channels.  To do that, they would have
to show that you knew it was classified.  That is a tall but
not impossible burden for the prosecution, especially given
that any discussion of whether such-and-such is classified is
itself classified.  In your defense you could argue that the
information is "obvious" and therefore not properly classified,
but you're not guaranteed to win that argument.
On top of the requirements embodied in black-letter law, agencies can impose a wide array of additional requirements,
by demanding a solemn "agreement" as a precondition for issuing a clearance.  As one example, see
  In particular, 18 USC 798 forbids "knowingly and willfully" disclosing classified information ... whereas anybody who signs the 312 agreement can be sanctioned for /any/ disclosure, even if not knowing and/or willful.  Congress has repeatedly and emphatically declined to write such a sweeping restriction
into law, but that doesn't stop the agencies from writing it into the agreement.
I'm not convinced they can impose criminal penalties on the basis of such an agreement, but they can certainly impose civil penalties.  This will further "restrict" your buddy.
This is relevant to the question that was asked, because unless you know exactly what agreements he has signed, the fact that he has a generic "security clearance" doesn't tell you anywhere near the whole story.

@_date: 2014-07-10 15:45:30
@_author: John Denker 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Lost me already.
The problem is, the clearance system has never worked very well AFAICT, and I've never seen a plausible
proposal for fixing it.
For starters, the concept of "need to know" is a central
pillar of the system, but if you take it literally, bad things happen.  For instance, politicians complain that in the run-up to 9/11, the law enforcement and intelligence communities failed to "connect the dots".  The dots could
not be shared, because there was not a demonstrated "need
to know".
In the usual bureaucratic way, after 9/11 the pendulum swung in the other direction.  People were encouraged to share more.  As a result, people like Pvt. Manning,
Edward Snowden, and others got access to huge amounts of stuff.
This is relevant to crypto, insofar as the better your cryptography is, the more obvious it becomes that human weakness (not code-breaking) is the dominant threat to your encrypted secrets.
There are fundamental reasons why there is no happy
medium, no way to make "need to know" actually work.
Consider the whole idea of /research/ on classified
topics.  Good research depends on knowing what other
people are up to.  It always has, at least since the
1600s.  Even Newton needed to "stand on the shoulders of giants."  The problem is, you don't know until
afterwards what it was that you needed to know.
There are fundamental dilemmas even in non-research
settings.  Consider the lowly airport security screeners.
On the one hand, you would like them to know exactly what to look for.  On the other hand, it would be a disaster if the bad guys found out exactly what the screeners were looking for.  There is a huge number of such screeners.  Do you trust them all?  In particular,
what about the screeners who serve United Airlines
flight 802, departing Dubai direct Dulles?  Logic says
those guys /especially/ need to know.  OTOH, how much do you trust them?
There are also long-standing and never-ending problems with how clearances are granted.  Once upon a time, clearances were given to proper gentlemen with the correct "old school tie" ... which led to the Cambridge Five and other fiascos.  More recently, as we know
from news stories, corners were cut on the background checks for people in fairly responsible positions.
Now extrapolate:  Imagine how many corners get cut in connection with low-level positions, such as
airport screeners.
(A) Publicly-available evidence suggests that almost everybody with a clearance fudges the rules, even while
exercising good judgment and complete loyalty.  Without
fudging, nothing would ever get done.  On the other hand, it opens up endless opportunities for blackmail ... from the inside.  Suppose you are an engineer doing fluid dynamics at a top-secret government lab.  If some coworker doesn't like you, he can have you arrested and threatened with life imprisonment under the 1917 espionage act.  After a year or so in solitary confinement without bond, the charges might (if you're lucky) get bargained down to misdemeanor mishandling of classified information ... which you probably /are/ guilty of, along with everybody else.  The fact that
you exercised good judgment and that everybody else is doing the same thing is not a defense under the law.
  (B) This is not the only law that works this way.  I   reckon everybody on K street routinely violates the
  bribery statute, 18 USC 201, not to mention the Hobbs   act, 18 USC 1951, but they almost never get prosecuted
  for it.  Jack Abramoff understands that he broke the
  law, but he doesn't understand why he got prosecuted
  while so many others continue with monkey business   as usual.
There is a partial analogy here, but *not* a complete equivalence.  I'm unhappy with bad laws that go mostly
unenforced, and also with good laws that go mostly unenforced, but the cure is different in the two cases.
I have sympathy for the routine law-breakers in group (A) but not group (B).  IMHO many of the former are just trying to do the right thing, while the latter are selfish bastards who pose a grave threat to democracy.
That's bureaucratic approach   We agree, it doesn't
work too well.  There's nothing wrong with trying to identify bad guys and keep them outside, but it doesn't solve the whole problem.
The whole idea that there exist good guys as distinct
from bad guys reflects a Manichaean world-view that
has little connection to reality.  In the real world,
there do exist adversaries and enemies who *must* be
kept outside ... but that does not make everybody else
a reliably good guy.
That's bureaucratic approach   There's nothing wrong with it as far as it goes, and indeed sometimes it solves a great many small sub-problems, but it does not solve the big overall problem, not by a long shot.
The word "instead" does not belong here.  The two approaches
are not mutually exclusive.  Indeed, they work better together than separately.
Alas, crucially, even the combination of the two doesn't work very well.  In particular, if the amount of bad behavior on the inside
exceeds critical mass, exceeds the percolation threshold,
then the system fails spectacularly.  Well-known examples
  -- LAPD Rampart scandal
  -- peer reviewing for Chinese scientific journals
  -- various emperors overthrown and/or assassinated    by the praetorian guard
  -- politicians in thrall to the aforementioned
   selfish bastards
  -- state secrets privilege used to conceal government
   wrongdoing (e.g. Reynolds among many others)
  -- et cetera
As a separate matter, even if everybody on the inside
is a good guy, the claim that "given enough eyeballs, all bugs are shallow" still doesn't work all that well.
Maybe a lot of bugs, but not "all" bugs.
The fundamental issues of trust, trustworthiness, integrity,
patriotism, loyalty, good judgment, et cetera are beyond what bureaucracy can understand, much less regulate.

@_date: 2014-07-15 14:03:45
@_author: John Denker 
@_subject: [Cryptography] multi-key encryption of "meta" data 
It seems to me that the binary distinction between "metadata" and
other data is a crock.  As a glaring example of the problem, common protocols for encrypted email encrypt only the main body of the message, leaving /all/ the headers unencrypted.  This is a serious
security breach, as discussed below [*].
We can do better than this.  We need to do better than this.
At first glance, one might think that "data is data" and we should
not distinguish "metadata" from other data, but actually I wish to
go in the other direction, and distinguish /more/ classes of data.
Use-case scenario:    a) I send an email from my desktop.  My mailer needs to know
   the IP address of my mail relay server.
  b) My relay server wants to know my ID and password.  It is not
   an open relay, so it needs to know I am not a spammer.
  c) The relay server also needs to know the IP address of the
   mailhost serving the addressee.
  d) The destination mailhost needs to know the userID or alias
   of the recipient, so that it can deliver the mail to the    correct mailbox.
  e) The recipient fetches the mail to his desktop.
  f) The recipient's mailer wants to know things like sender,
   date-and-time, subject, et cetera, since these are displayed
   in the mailbox summary.  The user wants to see these things,
   if only to decide if-and-when he wants to open the mail.
  g) When the recipient decides to read the mail, he needs to
   decrypt the main body of the message.
You can imagine all sorts of more-detailed scenarios -- and also
less-detailed scenarios -- but this suffices to illustrate my point.  The point is that at different stages along the way, different bits of data need to be known.  It's not even monotonic, i.e. not like the layers of an onion or a Russian doll.  The authorization needed at step (b) is not needed later, at least not in the same form.
To begin to address this, we don't need new cryptological primitives,
just smarter protocols for using the existing primitives.  This
needs more thought, but to get the discussion rolling, here is the outline of a simple scheme that might be a step in the right The destination organization publishes a public key.  At the outermost layer, I send a plain brown envelope addressed to
"somebody at destination.com".  When this arrives, the mailhost
opens the plain brown envelope and finds that it contains another envelope, addressed to a particular person within the organization, or perhaps a generic delivery point such as "Room 40".
Meanwhile, the recipient has published two public keys, an outer key and an inner key.  The secret half of the outer key is known to the recipient's mail reader, and is used to decrypt relatively
routine things like sender, subject, date-and-time, et cetera.
The secret half of the inner key is more closely held, and is
only used if-and-when the recipient wishes to decode the main
body of the message.
Let's be clear:  A great deal of the stuff that appears in RFC822 headers is not needed for delivery of the message, and MUST NOT be sent in the clear.
A) It seems to me that STARTTLS operates at not quite the right
level.  For one thing, it only applies to mail traffic.  So at the get-go we are surrendering more than we should to traffic
analysis.  It would be better to have something more like IPsec
(but perhaps easier to use) where even the TCP port numbers are
concealed.  Onion routing helps here.  Systematic sending of
B) In case it wasn't obvious:  When I say we should distinguish "metadata" from other data, this is not based on US constitutional
law;  I am talking about technology including cryptology.  This
has multiple advantages, including being applicable internationally,
and being more reliable, given a history of (shall we say) spotty
adherence to fourth-amendment principles even within the US.
[*]  As Glenn Greenwald recently noted:
  The US government, when it responds to FOIA requests, generally
blacks out large amounts of metadata.  Quote:
IANAL, and this is not the proper forum to make legal arguments, but
it seems that the USG has well and truly forked itself.  I can just
see the opposing lawyer asking, "Are you lying now, or were you
lying then?  Are you violating the FOIA law, and lying about the
reasons, or were you lying back when you said that hoovering up unlimited amounts of metadata was not a violation of the 4th The use of the word "unwarranted" is particularly ironic.  Not
undue, not excessive, not improper, but unWARRANTed.
It is amusing to think about the legal argument, but that is not really my point.  The point is that  a) Leaving so-called "metadata" in the clear ?would constitute a   clearly unwarranted invasion of privacy?, and
 b) If we solve the problem technologically we don't need to worry
  so much about the chicanery and law-breaking.
To say the same thing another way:  Assume the law of the jungle.
The only privacy rights you have are the ones you can enforce on
your own, using the strength of your cryptography.

@_date: 2014-07-19 13:43:54
@_author: John Denker 
@_subject: [Cryptography] multi-key encryption of "meta" data 
I appreciate the sentiment, but it's not that simple.  One
of the first rules of sound reasoning is to consider /all/
the possibilities.
In this case, the four most obvious possibilities form a
2x2 matrix:
      [ A  C ]
      [ B  D ]
  A) Some problems allow alternative solutions at different
   layers.  Some problems /require/ multi-level solutions.
  B) We agree that /some/ problems exist at the transport
   layer, and ought to be solved at that layer.
          C) OTOH, some problems are email-specific, and             demand email-specific solutions.
          D) Some problems cannot be solved either way,
           and will therefore persist.
At the next level of detail, in a different order:
   B) Defeating traffic analysis starts with concealing     the very fact that email has been sent.  This requires     encryption at a very low level -- OSI transport layer
    or below -- so that everything that gets transmitted
    is an opaque blob.
    Ditto for everything that gets stored.  Transmission
    carries data from one place to another, while storage     carries data from one time to another, so we should
    see the two as very similar.
    Also:  Defeating traffic analysis requires cover traffic.
    I have seen appallingly little discussion of this.  I
    get less than 50 relevant hits from
                C) The email client needs to show some of the
             metadata, such as sender, date, subject, etc.
             This is email-specific.  This is different              from decrypting the main body of the email,
             which is why a /layered/ approach has been
             suggested.  We do not need new cryptologic
             primitives, but we need better protocols for
             using the existing primitives.
             This need is not merely "historical".  If email
             did not exist, we would have to invent it.
    A) If you don't have good encryption at a lower level,
     then email-specific encryption such as STARTTLS is better
     than nothing.  Even if you do have lower-level encryption,
     additional layers of superencryption are still needed.
     1) Traffic analysis needs to be dealt with at a low level.
      OTOH, traffic analysis is not the only threat, not by a
      long shot.
     2) MITM attacks are a documented problem in the wild.  See       companion message:
      Subject: hard to trust all those root CAs
     3) Unencrypted email headers pose far more of a threat
      than low-level traffic analysis.  This is a huge burning       issue that needs to be dealt with ASAP if not sooner.
      If you don't believe me, ask Ed Snowden:
                  D) One of the virtues of email is that it is easy
             to use.  OTOH real security will never be easy.
             Sending email to  is              easy, but it is not quite so easy to recognize
             that you should not have done that, and should              have said  instead.
We all agree with that, as far as it goes.  Einstein said every
theory should be as simple as possible, but no simpler.
Sometimes cleverness plus hard work makes extreme simplicity possible, but sometimes you have to accept some complexity
as the cost of doing business.

@_date: 2014-07-19 14:03:24
@_author: John Denker 
@_subject: [Cryptography] hard to trust all those root CAs 
AFAICT, a lot of existing protocols were designed to resist
passive eavesdropping.  In contrast, the idea of large-scale MITM attacks was sometimes considered tin-foil-hat paranoia.
To this day, standard Ubuntu Firefox trusts 162 different
authorities (including the Hong Kong Post Office) to certify
In the /usr/share/ca-certificates/mozilla directory, only one of 163 root certificates has any v3 Name Constraints at all.
Why Ubuntu and Firefox tolerate this is beyond me; I can understand trusting Microsoft to sign Microsoft-related stuff, but allowing them to sign /anything and everything/ ?!????!!
     Actually it's even worse than that, because people like
     Microsoft have been issuing subsidiary certificates with      unlimited power, so you don't even need to capture a root      CA;  all you need is one of the subsidiary certs.
Forsooth, one would think that if these Authorities had any sense at all, they would voluntarily put constraints on their own certificates, just to make themselves less of a target.
Issuing an all-powerful cert is like walking through a bad neighborhood pushing a wheelbarrow full of cash.  If you carried less cash, you'd be less of a target.
Forged certs are a documented problem in the wild.  No tin-foil hat required:
     SSL "packet inspection" is an article of commerce.  The fact that
this is even remotely possible tells me that SSL fails to provide
the thing I most want it to provide.
  That crunching noise you hear is the sound of dead canaries
underfoot.  We really need to take action to reduce exposure
on this issue.

@_date: 2014-07-23 05:32:39
@_author: John Denker 
@_subject: [Cryptography] hard to trust all those root CAs 
Hash: SHA1
The moderators wisely insist on brevity.  However, we have to give up something in return.  You don't get to
snip out the threat model and then complain that no
threat model was specified.
I hate to belabor the obvious, but on 07/19/2014 02:03 PM, the OP in this thread did mention MITM attacks and
did cite data on forged certificates in the wild.
If you want the next level of detail, it is known that
the NSA acts as a MITM at the /hardware/ layer:  they
intercept and tamper with shipments after they leave
the manufacturer and before they reach the end-user.
They can insert back doors in everything from consumer-
grade stuff like cable modems, to corporate firewalls,
to carrier-grade backbone routers.  This meddle-in-the-
middle approach saves them the trouble of suborning a
whole bunch of manufacturers directly;  all they need to do is suborn a handful of shipping companies.  This is documented in the Snowden files; no tin-foil hat is
If the Chinese PLA Third Department is not installing
their own back doors, I'd be shocked.  If they weren't
doing it a year ago, they must have read the Snowden
files as a how-to manual.  For equipment made in China, they can demand direct cooperation from the manufacturers.
Couple that with a rogue CA.  Now you're drowning in Note that back doors are notoriously hard to secure.
A third party gets to choose the NSA back door, or the Third Department back door, or some generic stack-
overflow bug, or whatever.
A question for each person on this list:  Are you sure
that all of your communications with your banker, doctor, lawyer, mistresses, etc. move over networks that are immune to MITM attacks?  If so, please raise your hand.
It can't be a very successful solution, if people refer
to it in the past tense, and can't remember the name.
Note the contrast:  As currently deployed:
  SSL relies on authority, with no pinning or notary.
  SSH relies on pinning, with no authority or notary.
  PGP relies on web-of-trust, which usually boils down
   to little more than a labor-intensive form of pinning.
As discussed on 09/27/2013 09:43 AM, I reckon a heterotic approach would greatly increase security in all three cases.
I use the term "pinning" to refer to local approaches, and "notary" to refer to network-based online approaches.
AFAICT no "perfect" solution is possible.  If somebody wants to make a Truman Show / Matrix fake universe for you to live in, they can do so -- in principle.  However,
I reckon that good crypto engineering can make this much more expensive to do, and much easier to detect.
Evidently there are no widely-deployed solutions;  otherwise we wouldn't be seeing forged certificates in the wild.  Is there anything on the horizon?
If not, why not?

@_date: 2014-07-23 14:48:27
@_author: John Denker 
@_subject: [Cryptography] hard to trust all those root CAs 
Under that assumption, there is no need for cryptography.
We should shut down this list.  We should all go do something else.  Orwell was an optimist.
Also under that assumption, there is no such thing as a
trade secret, and therefore no such thing as industrial
research and development.
To say the same thing in less sarcastic terms:  We had better do whatever it takes to make sure that assumption does not become true.
This affects many different aspects of life.   -- Baseball would be a very different game if the batter   could crack the communication between catcher and pitcher,
  and if the pitcher could crack the "bunt" and "steal"
  signs, et cetera.
 -- Poker would a verrrry different game if all the cards
  were transparent.
 -- I take this personally, because most of my adult life
  has been spent doing R&D.  Almost every dollar I ever
  earned was predicated on the idea that my work conferred
  some competitive advantage to the company that I owned
  and/or worked for.  It would be hard to have any kind of   intellectual property, or any kind of competition at all,
  if everything becomes an open book.

@_date: 2014-07-25 14:24:32
@_author: John Denker 
@_subject: [Cryptography] propaganda on "hurdles for law enforcement" 
Hash: SHA1
Today's Gomorrah Post has a long article in the "National Security"
  Ellen Nakashima
  "Proliferation of new online communications services    poses hurdles for law enforcement"
  I see no particular reason to believe a single word of what
it says.  Virtually all of the evidence supporting the main
conclusion is "according FBI officials and others" ... which puts it in the same category as the "stories" Judy Miller wrote for the New Ys Times in the runup to the Irag war.
  Positive reasons for disbelieving the main thrust of that
story is that if the authorities want to search somebody's
bedroom, they can still do it;  they just (sometimes!) can't do it quite so cheaply.  That can't do it without getting up from their comfy armchairs.
The article contradicts Bill Frantz's assumption that all
present-day crypto is ineffective.  I tend to disbelieve
both extremes.  I reckon any lock can be picked or drilled
out /if somebody wants to badly enough/ ... but this does
not mean that all locks are completely useless.
There is a companion article that lets the cat out of the
  Ellen Nakashima
  "The government wants to wiretap online communications
  ? or in some cases hack them"
  Both articles appear to be part of a PR campaign to lay
the groundwork for a new CALEA-on-steroids law that would
reportedly require every ISP and every app developer to provide hooks to enable armchair/pushbutton wiretapping.
Before you say that such a law is impossible, especially
in the context of open-source software, let me point out
that most people on earth /already/ live under regimes
where use (or even possession) of an unregistered encryption
device is a serious crime.
I don't see any technical/cryptological way to defeat the
proposed US law;  it looks like a political problem that needs to be dealt with by political means.
Tangentially related: On 07/24/2014 09:13 PM, Peter Gutmann I am certainly not an international lawyer, but we can all read the plain language of the law.  Under the otherwise-Draconian
UK RIP law, the Rumpelstiltskin defense is explicitly allowed:
  Also note that if such a defense is not possible, you are already a criminal, because of the encrypted "message" below, which you have already received.  a) You don't know the decryption key, although nobody can prove   that you don't.
 b) You cannot obtain the key from me or anyone else, because   I destroyed the public key /before/ encrypting the message,   although nobody can prove that I did.
 c) Furthermore I can tell you that the plaintext consisted of   512 bytes of high-grade randomness that wasn't seen or recorded,   although nobody can prove that either.
I encourage you to forward my "message" to all your legislators,
along with lots of similar messages.
To say the same thing in more constructive terms: This serves as an example of /cover traffic/.  It allows you to say with complete sincerity that at least "some" of the data you hold is undecryptable.
   Adversaries will have to consider the hypothesis that I'm    engaging in some bizarre yet effective steganography, hiding    a tree in the front row of the forest.  Nobody can prove /or/    disprove this hypothesis.

@_date: 2014-06-19 06:43:55
@_author: John Denker 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
Almost any crypto-related assumption about flash-based file
systems is not reasonable.  Ditto for many other modern
hardware systems.  They do too much behind your back,
including moving data from place to place.
The only defense I've seen that makes any sense is to
do full-disk encryption or something similar.  Sometimes
file-by-file encryption suffices.  Then the problem of erasing the disk (or file) reduces to the problem of
zeroizing the key.  This is not necessarily trivial in
absolute terms, but it is often easier in relative terms.

@_date: 2014-03-18 13:38:48
@_author: John Denker 
@_subject: [Cryptography] Use process ID in mixing? 
Yes, it can ... but the time-of-day clock has all the
same strengths and fewer weaknesses.
You can use concat(clock,PID) if you feel like it.
The word "random" means different things, depending on
 -- At one extreme, in non-adversarial situations, the PRNG   does /not/ need to be resistant to cryptanalytic attack.
Specific example:  When I am doing physics, e.g. molecular
dynamics simulations, I use one of the C (or C++) library
randomness functions, and seed it from the clock.  The
upside is that this is more widely portable than trying
to read from /dev/u?random.  I'm pretty sure the molecules are not going to mount a cryptanalytic attack against the  -- At the other extreme, in high-stakes adversarial situations,   there are really no options other than seeding the CSPRNG
  with genuine entropy from a high-quality HRNG ... or just   using the HRNG directly.
  Still, though, there are situations where a well-seeded
  PRNG can benefit from salting or stirring.
The PID idea does not apply to the kernel itself, especially
during the critical start-up phase.  However, the time-of-day
clock can be -- and should be -- used to salt or stir the
state of the PRNG.  This provides some less-than-ideal but
still valuable protection against replay attacks, especially
in situations where the machine has not had a chance to
update the PRNG seed and write the seed to persistent
storage somewhere.
The previous paragraph is dependent on having each machine be /provisioned/ with a unique, secret seed for the PRNG.
This is not always done.  IMHO there reeeeeally needs to
be a best-practices document that emphasizes the importance
of proper provisioning.
This is discussed in more detail at

@_date: 2014-03-19 10:51:50
@_author: John Denker 
@_subject: [Cryptography] Use process ID in mixing? 
WYTM?  What's the problem you're trying to solve?
I can think of three cases, from worst to best:
1) If the PRNG has never been properly seeded, that's a big
problem, and mixing in the PID and/or RTC is nowhere near sufficient to solve the problem.
2) If the PRNG was properly seeded once upon a time, by far the most serious threat is a replay.  Stirring in the RTC
provides considerable protection against this, although it is far from ideal.  Stirring in the PID does not impress me, because there are too many ways that the PID could be replayed along with everything else.
3) If the PRNG is not being replayed, i.e. if the PRNG state
has not been reset to a previous value, then normal operation
of the PRNG should be very very strong, and no amount of
mixing in other stuff (RTC or PID or whatever) will make it
any stronger.
  If you are worried that the PRNG is not strong in normal
  operation, throw it out and get a better one.  We know
  how to make a PRNG that is /at least/ as strong as other
  crypto primitives that we utterly depend on (i.e. hashes
  and block ciphers).

@_date: 2014-03-30 14:26:41
@_author: John Denker 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Bravo.  That is very true and very important.
Let me repeat:  There is no such thing as a random number.
If it's a number, it's not random.
If it's random, it's not a number.
You can have a random distribution over numbers, but then
the randomness is in the distribution, not in any particular
number that may have been drawn from the distribution.
A so-called RNG is a random generator of numbers, not a
generator of random numbers.
Now, to answer the question that was originally asked:  Here
is how to collect N bits with the nothing-up-my-sleeve property:
Organize several players who will each contribute a string
of N bits:  One string from the NSA, one from the Chinese
equivalent, one from the Russians, one from Google, one from
EFF, et cetera.  We are going to XOR the strings, so you
don't need to trust all of these players;  it suffices to
convince yourself that at least /one/ of them will see it
in his best interest to contribute a good number.
During phase 1, each player selects a N-bit number.  IMHO the minimax strategy is to draw the number from a random
distribution.  During this phase, the number is secret.
However, each player publicly commits to the number, using
some agreed-upon cryptologic commitment scheme.  Phase 1
ends when all the commitments have been received by all of
the players.
During phase 2, all the strings are revealed.  Each of the
players can attest that he received all of the commitments
before revealing his string, so again you don't need to
trust every player, so you as you trust at least one.  The final answer is the XOR of all the contributed strings.
Anybody on earth can verify the commitments and verify the correctness of the XOR computation ... ???? ???? ??????.
This is just a sketch; you can add various bells and whistles
if you like.
By way of contrast, using digits from some mathematical constant is not entirely safe, because there are a lot of mathematical constants, and each one has a lot of digits,
so a wise guy can just search through them to find something
with the desired backdoor property.

@_date: 2014-05-07 12:16:15
@_author: John Denker 
@_subject: [Cryptography] high-school crypto project 
Well, first of all, it is not true that "everything else has
already been done in standard cryptography".  The fact that
you would say this suggests that your background in cryptography
is not as strong as you think it is.
Secondly, quantum cryptography is the most-challenging area of
a challenging subject.  It requires an understanding of physics
waaaay beyond anything that is normally covered in high school.
Quantum cryptography would make more sense as a graduate thesis
of above-average difficulty, rather than as a high-school project.
In any case, the effect on current crypto standards is nil.  As was said more than a decade ago:
     "We can factor the number 15 with quantum computers. We can also
      factor the number 15 with a dog trained to bark three times."
                 --- Robert Harley, 5/12/01, Sci.crypt.
If quantum crypto worked in practice, it would require throwing out many of the current standards entirely.  Meanwhile, now and
for the foreseeable future, it has no effect.  There is no middle
  As a general rule, the absence of evidence is not evidence of
  absence, but we can note that the Snowden revelations contain
  not the slightest hint that the NSA can break the fundamental
  cryptologic math.  Snowden said, quote, "the math works".  The
  breaks come from tampering with the code and from working   /around/ the crypto.
There are some folks who claim there are real-world applications
of quantum crypto, but in all cases I've seen, it is nothing but snake oil.  For example, it's OK if somebody talks about quantum-
mechanical resistance to wiretapping, but when they claim this solves "the key distribution problem" it means they have not the slightest clue what the real problem is.  Either they are very stupid, or they cynically believe their customers are very stupid ... and in either case, you don't want to be their customer.
As an almost-reasonable compromise, one might consider writing
a paper debunking the claims on this page:
  perhaps by showing that each of the four applications could be
handled more cheaply /and better/ using classical non-quantum However, even that would require a lot of work, to do it properly.
Remember that there is a lot more to security than just crypto.
For example, securing a set of election results requires verrry
much more than resistance to wiretapping.
Perhaps an even better option would be to take one of the open
questions discussed on this list, and write a review.  Pick
something that doesn't require graduate-level physics.  For
example, a few weeks ago there was a discussion of sending
encrypted email and (!) cover traffic via a netnews stream,
thereby providing a certain amount of anonymity and deniability.
A number of suggestions were made a different times.  It might
be worth writing a review that collects, evaluates, and harmonizes
the various suggestions.

@_date: 2014-05-13 13:01:40
@_author: John Denker 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
Hash: SHA1
This thread has generated a lot of good discussion.  In contrast, to my surprise, the recent thread on "forged SSL certificates" was only one message long.
In the same vein, the previous MITM thread was only one message long:
1) There is a connection between these three threads.  It seems to me that 6845 forged certificates is 6845 too many.  It is proof that TLS has failed in its primary mission.
In the words of John Oliver, "Why is this still a thing?"  Why is
it even remotely possible for forged certificates to be found in
the wild?
2) IMHO the first step should be to ask, what would it take to
create some semblance of security?  Preventing MITM attacks is high on the list of requirements.
*) If this can be achieved by non-revolutionary fixes to TLS, then
 the answer to the question posed by the Subject: of this thread
 is no, we don't really need a revolution.  We can make repairs in
 the short run, and re-implement whatever we like in the long run.
*) OTOH if security cannot be achieved within TLS, that proves we
 do need a revolution.
3) Following up on the previous point:  For many many years we have
known how to do zero-knowledge password checking ... but the browsers
don't do it.  This is relevant because it would make life much
harder for the certificate forgers.  Apache supports "basic auth"
and "digest auth" which have no resistance to MITM attacks.  Why not support something that actually resists attack?  This would not require a revolution.
As mentioned in the Huang paper cited above, javascript doesn't have access to TLS keying information.  This means that if somebody wanted
to implement password-authenticated key exchange in javascript, they
couldn't do it.  This is a fixable problem.  It would not require a
revolution.  I suppose it would be considered a "layer violation" which is inelegant, but IMHO having passwords stolen by the MITM is a lot worse than inelegant.
This doesn't solve all the world's problems, because the MITM
could be fiendish enough to send bogus .js code ... but it would
be a step in the right direction.
4) One the other (server) end of the connection, /pinning/ would
make things a lot harder for the forgers.  Right now SSL depends
on authorities with no pinning, whereas SSH depends on pinning with no authorities.  It seems cloyingly obvious that things would be
more secure if we insisted on at least two forms of authentication. For example, if the First National Bank of Dogpatch wants to start using a new certificate, it should be signed by the old certificate
5) That's a nice list.  We agree it doesn't make sense to insist on a one-size-fits-all solution.
In particular, I can imagine multiple security schemes within a single connection ... with varying /degrees/ of trust.
For example:  I contact port 443 on the non-virtual server at a certain IPv4 address.  This connection needs to have "some" degree
of security at this point, before I even send the URL ... because I would prefer not to send the URL in the clear.  As a second stage,
the URL refers to a virtual server, and after I have sent the URL
the security should pivot to something specific to virtual server.
Thirdly, the virtual server asks for my password.  At this point
we can do some sort of zero-knowledge password-authenticated key
exchange, and the degree of security is now higher.  Some of this
could be done by layers of superencryption, but some of it could
be done more efficiently by a succession of hand-offs.
6) Has anybody seriously checked the design and/or implementation of
the convergence plugin?
  The .xpi download is not signed in any way AFAICT.  That forces one
to wonder whether they are serious about security.
How do we know the convergence.io site is not a wholly-pwned subsidiary
of the FSB or NSA or whatever?

@_date: 2014-05-19 10:26:22
@_author: John Denker 
@_subject: [Cryptography] updating a counter 
My favorite thing is to use a collection of subcounters.
Most of the subcounters are LFSRs.  Some of them shift left, and some of them shift right.  Between the shifting and the XORing, a lot of bits get changed.
Each subcounter gets updated every time.  I arrange for the periods to be relatively prime, so that the period of the collection is the product of the periods of the individual subcounters.
One of the subcounters can be a plain arithmetical
adder.  We agree that using a random (nonzero) addend flips a lot more bits than a simple increment, and costs nothing extra compared to a simple increment (i.e. addend=1).
One advantage of this approach is that you can test
each subcounter, to make sure that it is implemented
correctly.  Counting to 2^32 doesn't take too long.
You can then verify that the periods are relatively
prime.  Then you know that the collection has a very
very long period.
Last but not least, there is the issue of block cipher
(in counter mode) versus hash function (in counter mode).
The cipher has the advantage that it can be re-keyed.
Rekeying AES is remarkably fast.  There is some value
in doing that, not on every count, but maybe every 1000 counts or so.
I consider all this to be in the category of belt plus suspenders plus crazy glue.  If the block cipher is any good, a simple increment is entirely sufficient.
To say the same thing the other way, if a simple increment is not sufficient, it constitutes a grave known-plaintext break of the cipher.  There are probably
other parts of the overall system that depend on the
cipher being not broken, so it's not worth spending
toooo much effort fussing with the counter.

@_date: 2014-05-19 13:39:32
@_author: John Denker 
@_subject: [Cryptography] updating a counter 
Actually it *is* sufficient to ensure that the overall
counter does not "repeat values" until its overall period
expires.  Consider a collection of four subcounters, with   2^32
  2^32 - 1
  2^31 - 1
  2^29 - 1
The overall period is very nearly 2^124.  Proof:  Immediate
consequence of the fundamental theorem of arithmetic.
  plus the fact that the block cipher is invertible.
On a verrry fast machine that will "repeat" once in 10^9
times the age of the universe.  So yeah, it "repeats".
Please explain in more detail why that matters.
If you don't believe the theorem, please provide a counterexample.

@_date: 2014-05-26 16:14:49
@_author: John Denker 
@_subject: [Cryptography] client certificates ... as opposed to password 
Imagine a far-away culture where there is a recent fad that involves putting lipstick on pigs.  This is a hard thing to do.  Lots of things can go wrong.  More recently, somebody decided to have a contest to find the absolutely optimal way of doing it.  A bunch of smart
people took it as a challenge.  They discussed it at great length.  They even organized a pig-makeup /contest/ to see who was the smartest of them all.
Then one day one of the children asked, why are you trying
so hard to optimize something that you shouldn't be doing
at all?
Ideally, there are several properties we would like a
password (and the surrounding system) to have:
  a) Easy to remember over long times.
  b) Easy to type in.
  c) Sufficient entropy content.
  d) Not shared between sites.
  e) Resistant to passive eavesdropping.
  f) Resistant to active MITM attacks.
  g) Resistant to catastrophe, such as when Target or
   eBay loses a huge database.
The problem is, (a) and (b) conflict with (c) and (d).
This is a problem.  Hashing helps with (g).  SSL helps
with (e) ... but as it is currently deployed, it doesn't really resist MITM attacks (f).  In any case, you're still stuck with (a), (b), (c), and (d).  You're still putting lipstick on a stinking pig.
Right now the "tradition" for doing e-commerce relies
on x.509 certificates to authenticate the server, plus login/password to authenticate the client.  It's hard
to decide which end is more problematic.
There are various cases to consider:
1)  Let's quickly deal with a trivial case:
 Suppose I don't control the client machine, for instance  when I am visiting an internet caf? in Lagos.  Then I'm  not going to use a password at all.  I'm not going to  do anything that requires security, authentication, or  authorization of any kind.
 In other words, if you can't secure the endpoint, you
 can't secure the connection.
 Therefore, we now focus attention on the case where I  /do/ control the client endpoint.
2) Assuming I do control the machine, I can install a
 password manager.  In fact this is what I've been doing
 for years.
 That means I can use really strong high-entropy passwords.
 I can use a different password for every site.  I don't
 need to remember any of them.  All I need to do is  remember the key to the keys, and since I use that often,
 there's no problem.
3) HOWEVER >>>>> None of that really makes sense.  For the
 same amount of work ... actually *LESS* work ... we could
 get a better result using client certificates.
 I'm not talking about the existing client certificates,  which are a horror show:
   I'm talking about how it could be done, how it should be done.
 It doesn't even need to be fancy.  Suppose the Acme Anvil
 company starts a "secure" session with me.  Rather than
 sending them my ID and password, I send them the following:
  -- My claimed ID, plus
  -- The Diffie-Hellman secret that they sent me,
    signed with my PGP private key.
  -- The first time, I send them my PGP key fingerprint,    so they can look up the public key.
Note that I could cut-and-paste this into the "user" and
"password" fields of an existing form, just as easily as I cut-and-paste a longish password today.
This is tantamount to a zero-knowledge proof that I hold the private key.  The PGP web-of-trust is not required
here.  They have /at least/ as much reason to rely on my
PGP key as they ever had to rely on my password.
Also, suppose security has been lost at /their/ end.  This
could easily happen if there is a MITM attack, using a
forged version of their SSL server certificate.  This is not
the least bit implausible.
  Any decent client-certificate scheme would instantly detect
this.  The MITM fails unless he can compromise /both/ ends in this situation.  This stands in contrast to anything that sends passwords over the link, where as soon as the server gets compromised, the passwords get stolen.
Let's be clear:  The hassle required for setting up and
operating a password manager is strictly greater than the
hassle required for doing something with certificates.
A password is in some hypothetical sense easier *IF* you
assume you are the first and only e-commerce site in the
world, and you are only doing low-value transactions, and
you are never going to be attacked by anybody with serious
resources.  However, that just doesn't scale.  In the real, non-hypothetical world, there are too many e-commerce sites, and the dollar volume is large enough to make them attractive targets for serious attacks.  Whatever lame excuses there were for using passwords 30 years ago, those excuses have long since lost any semblance of validity.
My suggestion:  Shut down the password hashing contest.
Take half of the people who would have worked on that, and
have them work on writing a browser plugin to deal with
client certificates in a nice way.  Have the other half
write an Apache plugin to handle the other end.
This is more work than password hashing, but it is entirely
doable.  And unlike password hashing, it accomplishes
something worthwhile.
4) Tangents and creeping features:
 There are issues with syncing the certificate manager if
 there are multiple client machines, but this is manageable.
 It's certainly no worse than having a password manager.
 There are good things you can do with phone+computer, but
 let's not go into that right now.

@_date: 2014-05-27 15:04:20
@_author: John Denker 
@_subject: [Cryptography] client certificates ... as opposed to password 
There have been a number of good comments in this thread.
I won't go into details about the stuff we agree on.
OTOH, there are a couple of points that might beneifit
from correction or clarification:
The client certificate does not need to be signed ...  not
signed before use, anyway.  By way of analogy, note that in the present e-commerce scheme, my password is not signed!
So an unsigned certificate can do everything a password can
do, and more.
Doing without the third-party signature greatly simplifies
the process of creating the certificate.  This improves the
user experience.
Note that Joe User can easily have a separate certificate-
pair for each merchant.  The certificate manager can handle
this with ease.  (I already use a different email and different
password for each merchant, and have done for many years.)
As a related point, dealing with certificates does create
a burden on the user ... but dealing with passwords (with
any semblance of security) also creates a burden.  Also,
having your account information compromised every few
months (Target, eBay, ...) also creates a burden!
I reckon Citibank should sponsor something like this.  The
US e-commerce sector is on the order of 350 gigabucks per
year.  Insecurity is already making people less willing to
shop online.  You could make a pretty strong business case
that improving security and usability are worth the cost.

@_date: 2014-11-17 13:27:14
@_author: John Denker 
@_subject: [Cryptography] FW: IAB Statement on Internet Confidentiality 
Thanks to RS for forwarding the IAB statement.
Then on 17/11/2014 04:54 am, alex at alten.org wrote in part:
On 11/17/2014 02:00 AM, ianG disagreed, saying:
I vote with Alex on this one.
a) Obviously, unencrypted communication is like walking
 around in a bad neighborhood pushing a wheelbarrow full
 of cash.  You are going to get mugged.
b) Encryption without authentication is like putting a
 tarp over the wheelbarrow, with a big sign that says  "please do not passively eavesdrop on all this cash".
To repeat:
a) Obviously, the era is over when we can leave the IP
 networks and (!) the phone networks unprotected.
b) Evidently less obvious, and in need of emphasis:
 The era is *also* over when we can pretend that high-
 power active attacks are infeasible or even rare.
 Defending the Home Despot PoS terminals against the
 script-kiddie-with-a-laptop-in-the-parking-lot is
 nice, but nowhere near sufficient.
c) Furthermore, we must add traffic analysis to the
 list of things to worry about.  The IAB statement
 did not even hint at this.
 For example, traffic to  is encrypted, but even a passive observer can tell
 what articles I've read, just by looking at the file
 sizes.
When the bad guys read "unauthenticated encryption, first and foremost" they start joyfully singing to   M-T-M all night,
  M-T-M all day,
  Traff'c analysis five miles long,
  Oh, de doo dah day.
Did you see the recent expos? in the WSJ about the
dirtbox aka DRT Box?  Here's a rehash:
  I'm not surprised to hear that most of the population
is subject to more-or-less continual tracking.  Rather,
I was surprised that they bothered with aircraft and
dirtboxes rather than just vampiring the data out of
the phone companies.  Note the contrast:
 -- I can understand that a Stingray confers an
  advantage, namely pinpoint accuracy;
 -- but dirtboxes on aircraft?  Huh?
As a first guess, maybe they didn't want to risk the ever-so-slight chance that somebody at the phone company would notice that it's illegal and unconstitutional, and perhaps pull a Snowden on them.  I dunno, what am I missing here?

@_date: 2014-10-03 11:43:12
@_author: John Denker 
@_subject: [Cryptography] cryptologic proof-of-life ... was: crypto clock 
I am not ready to presume that.  None of the messages in the "Best Internet crypto clock" thread have addressed this use-case.
As others have noted, there are any number of ways of applying a cryptologic time-stamp to a message (such
as an image of the hostage) ... but none of them offer any assurance that the message itself is not deceptive.
Post-dating is only one of innumerable possible deceptions.
Specifically, I could prepare an image of Elvis holding
today's New York Times in one hand and the latest NIST
beacon number plus a bunch of Pick Six lottery numbers in the other hand.  Alas this does not prove that Elvis is alive.
As a tangentially related matter:  Duress codes have
been part of cryptology for centuries.  Reference:
Excellent book:
   Leo Marks
   _Between Silk and Cyanide_
Proof-of-life falls into a weird intermediate category:
"I'm under duress so you can't trust what I say, except
for my proof-of-life claim."  This is a subset of the
infinitely-tricky "double agent" problem:  you know
your guy has been captured, but you are trying to double him, and you think/wish/hope he can tell you
which of his messages are believable and which not.
To answer a question that wasn't asked:  The /opposite/
functionality exists:  It is possible to use crypto to
prove that a certain message was prepared /before/ a
certain date and has not been tampered with since.
I've been using this idea for decades.  As a particularly
simple example:  Write up a description of an invention.
Compute a HMAC.  Send it to your patent attorney, with
instructions to date-stamp it and save it in the files.
This creates zero incremental risk of exposure, but can be used later to prove that your invention existed on or before the date of the email.
Fancy online services along this line exist.  See e.g.

@_date: 2014-10-03 12:48:43
@_author: John Denker 
@_subject: [Cryptography] NSA versus DES etc.... 
OK, that's a balanced view.
On 10/02/2014 08:35 AM, Jerry Leichter replied:
Defense in depth ("belt and suspenders") makes sense sometimes
... but if you go to far ("belt and suspenders and crazy glue") it can defeat important parts of your core mission.  As a good rule of thumb, I tell my customers /not/ to pour crazy glue into their pants.
As for crypto in particular:  The opposite of Kerckhoffs's
principle is called "security by obscurity" and is held in
contempt by serious cryptographers and security experts.
You don't need to make a virtue of disclosing the algorithm, but if you /rely/ on non-disclosure you are doing something wrong.  Among other things, you will hesitate to put your best crypto into the field, for fear that the algorithm will
be captured.
More generally, I am astonished by the amount of traffic on this list attempting to justify NSA actions that are by any objective standard unwise or illegal or both.
Of course we want to /understand/ where the NSA is coming
from, but that does not require rationalizing or justifying
There are actually some rather simple ways of understanding
the observed behavior.  For starters:  Follow the money.
The US "black budget" is on the order of 50 billion dollars per year.  Over the course of ten years, that starts to add up to real money, something like half a trillion dollars.
    That can be compared to the "nominal" cost of the Iraq     war, namely a couple trillion expended so far (not counting     various accrued liabilities).
In any case, it stands to reason that bureaucrats will fight over the money, and fight intensely.  I've seen people go nuts
over a lot less than that.
a) Of that, the amount spent on code /breaking/ completely dwarfs the amount spent on code /making/ i.e. information assurance (IA).  So it stands to reason that in any bureaucratic
knife-fight the IA guys are going to lose.
b) As a related point, they guy who /benefits/ from codebreaking
knows where the benefits are coming from, and is willing to pay.
In contrast, the guy who /suffers/ from codebreaking is usually
slow to find out what the problem is, and therefore unwilling to pay for security (until it's too late).  So this is another reason
why in any bureaucratic knife-fight, the IA guys are going to lose.
I emphasize again:  These explanations are /not/ justifications.
The fact that the NSA feels obliged to lie to Congress about
what they are doing indicates that even they know it is wrong.
    It is bizarre for the US taxpayers to be paying the NSA to spy
on them and (!) to leave them open to spying by foreign powers.
The NSA has repeatedly taken actions that are self-defeating in
terms of their stated mission.  Their actions are unconstitutional.
Even if they were constitutional they would be illegal.  Even if
they were legal they would be bad public policy.
 ++ [The method] should not require secrecy, and it should not
  be a problem if it falls into enemy hands.
                     -- Auguste Kerckhoffs
 ++ The enemy knows the system.
                     -- Claude Shannon
 ++ In the long run it is more important to secure one's own   communications than to exploit those of the enemy.
                     -- Frank Rowlett
 -- Let's create a situation where our friends can be spied
  upon more easily than our enemies."
                     -- NSA policy for 40+ years

@_date: 2014-10-07 12:21:29
@_author: John Denker 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
I wouldn't have said that.
Actually      ?(N) > N / (2 + ln N)                        [1]
is a hard lower bound for all N ? 2, i.e. for all nontrivial N.
Even tighter bounds exist, but [1] is more than good enough
for present purposes.

@_date: 2014-10-08 18:14:41
@_author: John Denker 
@_subject: [Cryptography] The world's most secure TRNG 
How about $1.08 for the whole thing, for a finished
product (not just the of materials), including labor
and including shipping?
  Or pay $0.00 if your machine comes with a built-in
audio subsystem.
It's an audio device, so it comes with a documented standard interface.  Also the fact that it has an output comes in handy for calibration and for life-long quality-assurance checks.
It's as secure as anything you could build yourself.
The entropy delivery rate is high enough for all ordinary purposes.

@_date: 2014-10-16 20:53:37
@_author: John Denker 
@_subject: [Cryptography] =?utf-8?q?Whisper_app_tracks_=E2=80=98anonymous?= 
In case you missed it:
The Whisper app is a product of WhisperText LLC.
On the GooglePlay store it claims:
Now there is a long, detailed article:
  Paul Lewis and Dominic Rushe
  "Revealed: how Whisper app tracks ?anonymous? users"
  The Guardian, 16 October 2014 11.35 EDT
  Apparently Whisper's notion of "anonymous" means they don't overtly ask for your name.  They do however capture and
store all of your messages.  By default they also capture
and store your GPS location.  If you opt out of "location
tracking" they track your location anyway, using IP
location and perhaps other means;  I don't know if they
use cell tower triangulation.
If you post something sufficiently provocative, they have
staffers who undertake to track you "for the rest of your
Also, the firm has cozy relationships with FBI, DoD, and news organizations.
You can read the story:
  My thoughts:
Evidently "anonymity" is not the same as privacy.  AFAICT
using Whisper is like running around in public all day every day, naked except for a tag that says "Hello, my name is I wonder how many of the folks who signed up for the service
were aware of this.
I wonder how many other "free" apps play by the same rules.
Does anybody have any suggestions for how to prevent this
sort of thing?

@_date: 2014-10-17 18:32:48
@_author: John Denker 
@_subject: [Cryptography] Best internet crypto clock 
so far so good ....
Resetting the local clock hardware is not necessary, not desirable, and not implied by anything that was said.
When you grab the official time from NIST or wherever, you should use that to write a calibration certificate, which you keep in a file along with all the previous calibration certificates.
The local clock hardware continues to be free-running and imperturbable.
Using the calibration certificates, you can define a as a function of the local clock hardware reading.
This function is   a) one-to-one,
  b) continuous,
  c) differentiable [except on a set of measure zero, at worst],
  d) very nearly unit slope, and
  e) highly overconstrained.
Because it is overconstrained, you can perform jackknife
resampling and claim that the calibrated time was never
off by more than XYZ milliseconds during the times of
interest.  There is strong evidence in support of this
claim and no evidence against it.
Courts have seen this sort of calibration a gazillion
times, e.g. for the speedometers and radars in police
If you do it right, the evidence is so overwhelming that
the adversary will not seriously consider challenging it.
A challenge would look like a crackpot move, and would just be an admission of weakness and desperation.

@_date: 2014-10-24 10:02:59
@_author: John Denker 
@_subject: [Cryptography] In search of random numbers 
Hash: SHA1
That's a good description of the problem.
However, there is a distinction between realism, fatalism, and defeatism.  We must be realistic about the problem as it exists, but we should not fatalistically accept it as a permanent state
of affairs.  We need to fix this problem.
We require the RNG system to work correctly at all times, even
early in the startup process, even during the very first boot.
To make this possible, any device, no matter how large or small, MUST be *provisioned* with some entropy.  We must train people
to do this, as a basic element of sound engineering practice.
The idea of provisioning is discussed at
  "Security Recommendations for Any Device
     that Depends on Randomly-Generated Numbers"
    Provisioning is an option ... AFAICT the only option.

@_date: 2014-10-27 13:14:05
@_author: John Denker 
@_subject: [Cryptography] Auditable logs? 
Let me start out ultra-simple and work up from there.
Here is a technique that applies to any file, not just
a log file.  I've used this for decades.  When I invent
something, I type up a description.  I compute a HMAC
and send it to my lawyer, with instructions to date-stamp
it and put it in the files.
This compares very favorably to the usual practice of
having a colleague countersign my lab book.  Among
other things
 -- It means there can be no suggestion that I altered
  the lab book after it was signed.
 -- It means there is no possibility of a leak;  the
  HMAC is a one-way function and cannot be used to   reconstruct the meaning of the document.
 -- I expect the timestamped page to be admissible under
  the "business records exception"
       which might not apply to my colleague since he was
  not necessarily required to sign my book as a matter
  of routine.
This suffices to prove that something was invented /before/
a certain date.
    In contrast, proving that something happened /after/ a
    certain date -- e.g. hostage proof-of-life -- is a whole
    different ballgame, as discussed in a previous thread.
    This is a subset of the infinitely-tricky double-agent
    triple-agent problem.
The foregoing is really bare bones, not even involving
a digital signature, but it gets the job done at two
  1) I trust it.
  2) The adversaries seem to trust it.  IANAL and my
   experience with this is limited ... but in a situation
   where the adversaries were spending millions of dollars
   to discredit everything and everybody associated with
   me, they didn't bother to challenge this.
Starting from that bare-bones baseline, you can make a
number of improvements.
One possible embellishment is to publish the HMAC in
a newspaper somewhere.  There are small-circulation
newspapers that specialize in publishing "legal notices"
that nobody will ever see, yet meet the legal definition
of publication.  This is a crude form of date-stamping.
A better option is to send the HMAC to a "notary service"
who adds a timestamp, digitally signs it, and sends it back.  That gives you something you can keep in your own files, without relying on the lawyer's files.
For belt-and-suspenders protection, do both.  Have it notarized /and/ filed by a third party.
The foregoing applies to loose documents.  In the case
of a log file, you can do something even stronger. Every time you add something important, and also at
scheduled intervals (daily, weekly, or whatever),
hash the new material /along with the previous hash/.
(This is basically how the git commit logs work.)  Have the new hash signed and/or filed as above.
This creates a /chain/ that is hard to hack.
That should suffice for any application I can imagine
at the moment.  If there is something else that needs
doing, please explain.
PS:  Note that much harder problems than this have been solved.  In particular, there is an extensive literature on zero-knowledge proofs.  This involves some elegant

@_date: 2014-10-29 12:38:55
@_author: John Denker 
@_subject: [Cryptography] SSLv3 in the wild 
As John Oliver might say:
  SSLv3 -- How is that still a thing?
SSLv3 was deprecated and superseded by TLS1.0 in 1999
  I was disappointed to find large SSLv3-only servers existing in the wild, 15 years post TLS, and two weeks post-POODLE.
I was expecting a few small clients, but I'm not sure I was expecting large servers.
Here is an example that you may find useful, as a test-target
or perhaps a talking point.  Canadian tax dollars at work:
  Note that there is no "http" access to the navcanada site.
This is relevant because it removes a possible workaround,
and violates the dictum that says if you can't encrypt
properly you shouldn't encrypt at all.
The overall situation is a pain in the neck because it means I can't just eradicate all traces of SSLv3 and forget about it.
Firefox says:
Nmap seems to have an overoptimistic notion of "strong":
nmap --script ssl-enum-ciphers -p 443 flightplanning.navcanada.ca

@_date: 2014-09-03 12:06:30
@_author: John Denker 
@_subject: [Cryptography] stories from the real life MITM book 
Hash: SHA1
Some all-too-rare good news on this front:
  Richard Chirgwin
  "Firefox 32 moves to kill MITM attacks"
  Sounds like an important step in the right direction.
Thanks for the pointers to recent articles.  We agree that
reported /evidence/ of MITM attacks is somewhat rare.  However, the attacks themselves are not particularly rare.
Actually the evidence is starting to get some play.  You
can find interesting stuff by googling:
  For example:
   Adrian Mettler, Vishwanath Raman and Yulong Zhang
   "SSL Vulnerabilities: Who listens when Android applications talk?"
   Here's a particularly detailed, interesting report:
  Lin-Shung Huang, Alex Rice, Erling Ellingsen, and Collin Jackson
  "Analyzing Forged SSL Certi?cates in the Wild"
  And another:
  Ryan Gallagher   "New Snowden Documents Show NSA Deemed Google Networks a 'Target'"
  It must be emphasized that NSA is *not* the only player in this
arena.  Maybe one of these days Ed Snowden's Chinese cousin,
Sou-Den E-Duan, will leak a few gigabytes telling us what the
Third Directorate has been up to.
There was a time when networks had no security at all, not even
against passive snooping.  You could login using telnet, sending your password in the clear.  That would be considered gross malpractice today.  SSH has been around for 20 years.
Until recently, in some circles, worrying about active MITM attacks
was considered tin-foil-hat paranoia.  Not anymore.  Nowadays we need strong protection against such attacks ... much stronger than what is presently deployed.  IMHO, continuing with things as they are would constitute gross malpractice.

@_date: 2014-09-05 11:27:46
@_author: John Denker 
@_subject: [Cryptography] What is the difference between a code and a 
In classical cryptology, "code" more-or-less refers to looking stuff up in a codebook.
 -- This is not necessarily a 1-to-1 mapping;  good-quality
  codebooks have had homophones for hundreds of years.
 -- This is not limited /solely/ to encoding of words, phrases,
  and other high-level linguistic and semantic units.  Any
  decent codebook will have codes for individual letters,   although these should be used sparingly.
 -- Codewords (codegroups) need not have fixed length.
 -- Not every code is a secret code.  A good codebook can
  provide treeeemendous compression, which is a sufficient
  reason to use it, even if secrecy is not required.
The classical definitions are set forth in detail starting on
page xv in the preface to David Kahn's book _The Codebreakers_
Etymologically speaking, the term codebook is redundant;  originally to codify something meant to write it into a book, i.e. a codex.
Example:  United States Code i.e. the book of laws.
Meanwhile, if we widen the focus to include things other than
cryptology, the word "code" has a much broader meaning.  Almost
any system whereby one thing symbolizes another is called a code.
Various fields of endeavor have codes including
  data compression:     Lempel-Ziv code;  Huffman code; ....
  computing:            hash code;  PDP-8 machine code;  c++ code; ....
  error correction:     phonetic alphabet (alpha bravo charlie ...);
                        Reed-Solomon code; ....
  data representation:  NTSC;  ATSC;  GSM half-rate voice codec;
                        QAM-64 code;  FM;  NRZ code;  ASCII code;
                        Morse code;  Braille;  semaphore flags ....
These things can be layered:  A cable operator might send ATSC
over QAM-64.
More specialized fields can have their own codes:
 -- "Lifeguard seven seven charlie, Seattle Center roger, Boeing
  altimeter two niner niner zero, squawk four zero zero one,
  cruise seven thousand."
  If you don't know the code, you might have a hard time guessing
  what that means.
 -- Q-codes.  European aviation uses QNH and QNE.  The US uses
  the same concepts, but names them differently.
 -- The APCO 10-codes are deprecated, but are still used.
Note that in popular culture, many codewords including "10-4" and "roger" are very often used incorrectly.  People think they know what these codewords mean, but they get it wrong.
Also in popular culture, the word "code" sometimes connotes
"secret code".  This is something to beware of when speaking
to non-experts.
Some codes provide compression, while others actually expand the length of the message (typically to provide error
correction).  The compression factor is well-defined when the
input and output media are the same, such as compressing one
file to make another.  However, when switching media, all you
can do is compare one code to another.  If you didn't encode
the bits in QAM-64 or something similar, you wouldn't be able
to put them on the wire at all, so the notion of compression
ratio fails the dimensional-analysis test.  However you can
say that QAM-64 gives considerable compression relative to FSK.
A one-time codebook is as secure as secure can be, just like
any other one-time pad.
Enciphered code is typically more secure than either straight
code or straight cipher.  That's because there is less entropy density in the codetext than in the original plaintext.
To answer the original question:  The definition of cipher
overlaps with the definition of code, insofar as a simple
substitution cipher (such as "the dancing men") is both a
code and a cipher.

@_date: 2014-09-08 10:03:03
@_author: John Denker 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
I wouldn't have said that.
I've seen any number of hand-wavy arguments that "P = NP" would spell the end of public-key crypto, or worse.  However, all of these arguments seem seriously flawed to me.  I reckon that even if it turns out that P=NP, we can still do crypto, including public-key crypto.  This is true twice over, both at the low end and the high end.
At the low end:  Consider the first known asymmetric crypto algorithm, Merkle's puzzles
  The work factor is only polynomial (indeed quadratic, which is not a particularly fancy polynomial) in the sense that the bad-guy work is only the square of the good-guy work.  If you square a sufficiently large number, you win.  Higher-order polynomials work even To say the same thing another way:  We need breakage
to be hard.  It doesn't need to be infinitely hard,
or even exponentially hard.
At the high end:  If P=NP, you can always choose something that is not in NP, i.e. harder than NP.
I haven't seen much work done in this direction, which is understandable given that most people are betting that P ? NP, but it remains a possibility in principle.
By way of background:  P and NP are highly specific
technical terms.  Unless there is a compelling reason
to redefine them, please let's stick to the established
 *) P is a category of easiness.
 *) NP is also a category of easiness.  It means if
  you have a guess as to the answer, you can check
  it in polynomial time.  This implies that if you
  are given unlimited hardware running in parallel,
  you can /find/ the answer in polynomial time,
  because you can just check all possibilities.
  Obviously P is contained within NP.
 *) P_HARD is a category of hardness.  It means a   problem is at least as hard as the hardest thing
  in P.
 *) NP_HARD is a category of hardness.  It means a
  problem is at least as hard as the hardest thing
  in NP.
 *) NP_COMPLETE means a problem is both NP_HARD
  and NP.  In other words, it is exactly as hard
  as the hardest thing in NP (within a polynomial
  scale factor).
  Please do not say NP when you mean NP_HARD.
  Please do not say NP when you mean NP_COMPLETE.
 *) There are lots of things that are not in NP,
  meaning they are harder than NP, and therefore
  harder than NP_COMPLETE.
  ++ Some things take exponential time -- worse    than any polynomial -- even if you have unlimited
   hardware.
  ++ Some things take combinatorial time -- worse
   even than exponential.
  ++ Some things are just plain undecidable, such
   as the infamous halting problem.
A decent discussion with inclusion diagrams and examples of harder-than-NP problems can be found at

@_date: 2014-09-08 13:27:29
@_author: John Denker 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
Lately we've been discussing some cerebral, sophisticated things;
stuff that's hard because it's supposed to be hard.
Let's spend a moment on some low-level tricks of the trade;
stuff that is not supposed to be hard, but nevertheless wastes your time if you have to rediscover the tricks.
Let's talk about that.  Yes, it is possible to put contact
information and PGP information in QR codes on a business
card ... just not in the /same/ QR code AFAICT.  It ends up being slightly non-obvious and slightly inelegant, but it
is doable.
Details on one way of doing it can be found here:
   That includes some examples and some discussion of why the more obvious approaches don't work.
The same document includes some hints on how to set the
mime types when distributing keys by http.
Additional examples of QR codes and key files can be found
  If anybody has more elegant ways of doing this, please let
us know.

@_date: 2014-09-10 12:19:01
@_author: John Denker 
@_subject: [Cryptography] email from strangers --> 
Agreed!  For details see below.
+1 to that.
I get a lot of email from unintroduced strangers.  In particular,
I solicit comments about my web site, and 99% of the comments
have value.  Even the ones that might seem like dumb questions
I take seriously, because they indicate that my writing is not
as clear as it should be.  Far less than 1% of the comments come from out-and-out cranks.
If I didn't have a spam filter I would be deluged with spam,
but in fact verrry little spam makes it past the filter.
If spam ever did overwhelm filter technology of the existing
kind, there is other technology that could be applied.
For one thing, I could insist that all unsolicited email
bear a payment of 49 cents (equivalent to US first-class
postage) payable to me via some cryptologically-secure e-payment scheme.  I promise not to collect the payment unless I actually
read the message.  I furthermore promise not to collect
the payment if I consider the message to be interesting
or otherwise non-obnoxious.  Anybody who thinks my time is not worth 49 cents should please not send me email.
There is some risk that I could collect the payment
without reading the email, but that just means that
when we remove risk from the recipient (the unbounded
risk of spam) we are shifting some risk onto the sender (the bounded risk of poor return on the investment of
A check for 49 cents would be more than sufficient to stop spammers cold.
  Note that electronic /currency/ is not the right
  model here, because if you send a stranger some
  currency, you don't know whether it got lost or
  is just being hoarded.  You need a /check/ model
  with an expiration date, such that if the recipient
  does not cash the check by a certain date you get
  your money back.
  In some vague sense, PayPal is close to the right   model, if we ignore details such as being insecure
  as well as generally obnoxious.

@_date: 2014-09-10 13:27:48
@_author: John Denker 
@_subject: [Cryptography] keys, signatures, trust, identification, badges, 
Indeed!  That's the crucial question.
AFAICT the question is unanswerable within PGP's conceptual
framework, because the framework is too unsophisticated and
Indeed!  AFAICT the whole idea behind the usual key-signing party is predicated on confusing the concepts of identification and trust ... which IMHO ought to be kept well separate.
On top of that, the PGP notion of "trust" is far too
simple to be useful.
In the real world, I sign /documents/ where the body of
the document spells out what my signature means in that
particular context.  On another document, my signature
might mean something else entirely.
You can use PGP to sign documents, which is fine ... but in contrast, the idea of a "keys signing keys" is inherently ridiculous.  There is no way to assign a reasonable semantics.
In the real world trust is highly multi-dimensional.  I
might trust you with one set of things but not another.
Possibly-constructive suggestion:  In the social-media
world we have /badges/.  The more formal name for such things is /credentials/.  A related concept is /certificates/, if we use the word in its broad vernacular sense, *not* limiting it to x509 certificates.
Examples include driver's licenses, teaching certificates, workplace ID badges, et cetera.
These serve to split the difference between the highly
detailed notion of a signed document and the highly
non-detailed notion of "trust" versus "non-trust" as
conceived by PGP.
For example, so-and-so might earn a Cryptography List
badge, which we define to mean that they are a known,
established contributor to this list.  It does *not*
mean that we have identified the person in any physical
sense;  the contributor could be a sock puppet controlled
by some unknown person ... or could be the proverbial
dog typing at the keyboard.  The badge -- securely
associated with a particular PGP public key -- serves only to indicate that a message comes from the same person (or dog) as last time.  This usefully solves a
a /subset/ of the general trust problem.
In general, people rely far too much on credentials.
A credential is just a symbol.  Never confuse a symbol with the thing symbolized.  However, my main point remains:  For every problem that credentials have, the PGP "trust" model has the same problem only orders of magnitude worse.
All this is discussed in more detail, along with some
related issues, at
  I have no idea how hard it would be to create a
PGP-like system that supports badges.
There's an app for that.
I have not carefully researched the issue, but the android app "APG" seems to work fine for that.  It can
put a QR-encoded fingerprint on the screen for others
to scan, and it can scan QR codes from others and interpret them correctly.  Details on this and other options are discussed at

@_date: 2014-09-12 10:23:22
@_author: John Denker 
@_subject: [Cryptography] HR 5099 
Reference:  I'm not an expert, but I'd wager that this bill isn't going anywhere.  The sponsor (Grayson) is more of a show-horse than a work-horse.  He hasn't lined up any co-sponsors as far as I can tell.  He doesn't sit on any of the committees that would hear the bill.  The
main effect of the bill is to allow Grayson to send letters to constituents and (ahem) "donors" saying he submitted an amendment to do such-and-such.
If any such provision does become law, it will be due
to somebody working behind the scenes, not due to
action on HR 5099.

@_date: 2014-09-13 15:22:07
@_author: John Denker 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Those are all noble goals.
Note that I've never been able to get a straight answer
about the goals or requirements for the legacy "standard"
random.c, so to the extent that Sandy can address /any/
of the listed items, that's valuable progress.
I agree with Sandy that item (3) is "critically important".
It should be moved up the priority list.  It's doable, but
not easy.  About 100 wrong ways of doing it have been
suggested, in this forum and elsewhere.
Other items such as (2) are equally critical, but more
straightforward to accomplish.
I would suggest one more high-priority item for the list:
8) be more frugal with the available real entropy, in
 situations where entropy input is limited yet a large
 amount of PSEUDOrandom output is required.
 Such situations are critical, and not particularly rare.
In the spirit of covering all the bases, I would like
to mention but /not/ advocate one further item:
x) rapidly recover from compromise of the PRNG internal
 state.
For years, the legacy "standard" random.c has been
obsessed with this goal, out of proportion to its
actual practical importance, to the great detriment of other goals including item (8).

@_date: 2014-09-15 08:01:26
@_author: John Denker 
@_subject: [Cryptography] feature-test info (or lack thereof) 
That's technically true ... but this is a crypto list,
and if you look at the Venn diagram, the intersection between systems that need cryptologic security and the
ones that lack persistent and/or configurable memory
is pretty small.
Almost anything that connects to a network is going to
need a hostname, MAC address, and/or similar things.
Whatever memory is used to hold such things can hold
a few feature-test bits and other configuration data.
I know some people enjoy cursing the ARM designers
  but I don't understand what the fuss is about.
In an embedded system, almost by definition, the guy who embeds the software knows what the hardware looks like, and can configure the software accordingly.  It's not like
he is embedding an unmodified piece of software that is
supposed to work cross-platform (such as an Ubuntu Live
.iso image).
Conversely, there are some guys who like to load software
onto ill-understood hardware, but you can't play that game at all unless you have modifiable memory.  So once again, I don't see any part of the Venn diagram where feature-test info (or the lack thereof) is a real problem.
If somebody has a large-scale use-case that is a problem in practice, please explain why the obvious solutions don't work.

@_date: 2014-09-16 03:41:04
@_author: John Denker 
@_subject: [Cryptography] Simple non-invertible function? 
That's a stronger property than mere non-invertibility.
SP800-90A calls that "backtrack resistance".
SP800-90A recommends schemes for achieving this.
  At least in the short term, I would recommend using one of the block-cipher approaches.  There are some remarkably
efficient block ciphers available, with well-established
security properties.
Later, if we decide the non-invertible function is the
rate-limiting step, and if somebody comes up with something just as secure and more efficient, it can be be dropped in at any time, as a plug-in replacement.

@_date: 2014-09-16 03:43:30
@_author: John Denker 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Executive summary:  In any PRNG, it is necessary to be
fastidious about the distinction between entropy on
the one hand and pseudo-randomness on the other hand.
The idea of having a wasteful PRNG /per process/ is very much open to question.
I don't see how it solves the main problem.
One problem I see is that /dev/urandom wastes entropy,
by which I mean real entropy.
 -- If the problem gets solved, it can perfectly well   be solved on a per-host basis.  Solving it on a   per-process basis doesn't help.
 -- If the problem remains unsolved, it is at least   as bad on a per-process basis as on a per-host   basis.  In fact it could be worse, if we have a   lot of entropy-wasters running in parallel.
When entropy is scarce, as it often is, deciding who gets
how much becomes a policy issue, essentially an economics
issue.  Doing things on a per-process basis is neither
necessary nor sufficient.  For one thing, even within a single process there can be multiple randomness-consumers, each with different needs, each subject to different
That's not the right way to frame the discussion.  The
statement refers to the output of the PRNG, which is not properly called entropy.  It contains a lot of randomness, but very little entropy.
Note:  If I thought the word "entropy" in statement [1]
was merely a typo I wouldn't be mentioning it.
Similarly:  If I thought it were merely a misnomer I
wouldn't be mentioning it.  Terminology is not very
important ... except insofar as it affects how we formulate and communicate ideas.
I mention it because it seems to be a misconception,
not just a misnomer.
There is a crucial distinction here:
 *) The output of a TRNG has an entropy density of
  (100% minus epsilon).
 *) The output of a PRNG has an entropy density of
  (0% plus epsilon).
In any PRNG it is necessary to be fastidious about
this distinction, and to manage the entropy carefully.
The existing random.c fails to do this.  In the past
I have made specific constructive suggestions about this off-list, to no effect AFAICT.
Bottom line:  ++ Please let's be fastidious about the distinction   between entropy on the one hand and pseudo-randomness
  on the other hand.
 ++ The idea of having an wasteful PRNG /per process/
  is very much open to question.

@_date: 2014-09-16 10:20:00
@_author: John Denker 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Those are good questions.  Professionalism demands that we ask such questions.  However the answer IMHO is not quite that scary, for multiple reasons.
1) The block cipher inside a RNG is under less stress than many others.  A lot of attacks rely on known plaintexts, chosen plaintexts, and/or huge amounts of ciphertexts, whereas in the RNG the plaintext is unknown /and/ the cipher gets re-keyed at not-very-long intervals.
I'm not rash enough to say this makes the RNG unbreakable;
I'm just saying it pushes the break rather far down the
list of things to worry about.
2) As with other block ciphers, you can (for a reasonable
price) buy additional safety margin by superencryption,  putting one cipher in series with a dissimilar cipher.
3) See below:
Right.  This is the poster child for "cipher agility"
that actually works, actually makes things better not
There is a space on the block diagram labeled "cipher
goes here" and you can drop in a new cipher any time you want.  If it's an embedded system that cannot be
recompiled, you can (for verrrry small cost) compile
a bunch of stuff in advance and make it switchable at Bottom line on this topic:  Professionalism demands that we ask such questions.  However, it seems to me that other things are far, far higher on the list of things to worry about.
Segueing now to the larger list:
That's strictly true if we assume the word "entropy" is
being used correctly.
In my very strong opinion, a kernel PRNG should be secure
from the first moment of the boot process ... just as a baby rattlesnake is venomous from the moment of birth.  The PRNG should be able to produce computationally-strong pseudo-randomness, even if it cannot produce very much actual entropy.
I appreciate the sentiment, but that is not a particularly
strong example.  A CSPRNG is perfectly adequate for syn-
cookies, for ASLR, and a wide range of other things that
happen early in the boot sequence.
Giving these consumers what they want -- high-grade
randomness -- is not super-easy, but it is eaiser than
Please, folks:  If we're going to make progress in this
field, we need to be really fastidious about the distinction
between pseudo-randomness and actual entropy.

@_date: 2014-09-17 22:30:44
@_author: John Denker 
@_subject: [Cryptography] new wiretap resistance in iOS 8 
Quoting from the new iOS 8 privacy policy announced tonight Wed Sep 17.
  This is new.
This is widely being interpreted as payback for  a) NSA attacks against Apple,
 b) wiretap orders that Apple considers overly broad, and
 c) lack of adequate legal process to challenge wiretap orders.
I reckon we are now all set for a reprise of the Clipper-chip
wars of the mid-1990s.  Legislation will be introduced to compel manufacturers to cripple their crypto.  Opponents will argue that when privacy is outlawed, only outlaws will have privacy.  They will also bring up international competitiveness issues.  And superencryption.  And so on.

@_date: 2014-09-18 06:17:26
@_author: John Denker 
@_subject: [Cryptography] RFC possible changes for Linux random device 
AFAICT, locking pages in memory to make them non-
swappable is a solved problem lo these many years.
  It has to be done by the app, not by the kernel RNG.
Here's why:  Suppose you get some entropy from the RNG and use it to cut a long-term key.  You need to protect that key for years and years, until it expires.
You need to protect it wherever it resides, in memory
and elsewhere.  The kernel RNG knows nothing of this.
Similarly if the key goes into a password-protected file, the password needs to be protected.  Again, the kernel in general and the RNG system in particular have no traction on this.
Similarly, the app has to take responsibility for
zeroing sensitive data as soon as it is no longer
What brought that on?
That's how I do it.  That's what I recommend.  (*)
Here's why: Cutting long-term keys is a low-bandwidth high-value adversarial application.  In such cases, you can surely afford to use a HRNG. (*)
Also because the number of things that can go wrong with a HRNG is a very small subset of the things that can go wrong with a PRNG.
In contrast, for high-bandwidth low-value applications, a properly-designed properly-seeded PRNG works fine.
Here the HRNG is needed so that the PRNG can be properly seeded. (*)
The third possibility is that a PRNG is adequate to the
task but plenty of HRNG entropy is available, so that either one will do.
The fourth possibility is that entropy is scarce but
critically needed, in which case exceedingly careful engineering is needed.  Seeding the PRNG on a newly
booted system is a conspicuous example.  For the next
level of detail on this, see
  *) Note:  I have gone to a lot of trouble to spell out
what is needed and provide tools to help make it doable.
That's not the sort of activity that is usually referred to as "handwaving".
Yes.  Some of the key RNG issues are discussed at
  More generally, a good RNG and good system interfaces
are necessary but not sufficient for overall system There's 5000 words of discussion at
  If anybody requires more detail and specificity, please ask a more detailed specific question.

@_date: 2014-09-20 13:18:35
@_author: John Denker 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
1) As the proverb says, don't let the perfect be the  enemy of the good.
 There will never be perfect security.  The measure  of good security is that it imposes a cost on the  attacker, out of proportion to the cost borne by
 the user.
 The new practice of /not/ escrowing the keys to iOS
 user data does not make the device attack-proof,  but it does raise the cost of the attack.
   Forsooth, if this initiative fails, it will not    be because it didn't sufficiently raise the cost
   to the attackers, but rather because it imposed
   too much burden on the rightful users.
2) Another proverb goes even farther in the same
 general direction:  A journey of 100 miles begins
 with a single step.
 Suppose there is a weakest-link situation, e.g.
 where locking the front door has no measurable  benefit until you also lock the back door, side
 door, windows, et cetera.  You still ought to lock  the front door!  Even if you can't do everything
 at once, take the first step and then proceed  from there.
3) It is a mistake to focus too directly on the
 threat from the NSA.
 Not escrowing the keys makes Apple somewhat less  of a target for the FSB, Third Directorate, etc.
 etc. etc. etc.  Not zero target, but less of a
 target.
 If you're worried about Apple Headquarters being  compelled to subvert your phone, you should also  be worried about a Clipper-like back door in the
 hardware, which is made in China.  Ditto for HTC
 and other brands.
 Probably the biggest threat from the NSA is more
 /indirect/.  I am referring to weakening crypto  standards and products, again and again over the  years, thereby creating conditions for a Hobbesian  war of all against all.  For example, IMHO it was  both arrogant and stupid for the NSA to think they  would be the only ones who could break 56-bit DES.
 Tangential remark: Interesting reference:
   Michael Schwartzbeck
   "The Evolution of US Government Restrictions on
    Using and Exporting Encryption Technologies"
   From "Studies in Intelligence"  (the secret internal CIA magazine)
   (date not obvious;  circa 1998)
    (prettier)
    (same, but uglier)
 Also:  CIA FOIA homepage (with search feature)
   Hundreds of "Studies in Intelligence" articles
 were released last week.
4) It's bad practice to support a strong argument
 with a weak one, but since the topic has already
 been brought up, let me address it.
 For the /subset/ of the problem that concerns
 NSA versus Apple, laws matter ... somewhat.  Yes,  there is a long track record of violations, but
 in the spirit of item (1) above, forcing the NSA
 to resort to lawless and unconstitutional methods
 raises the cost to them.
 In particular, if I have information about you,  I can be subpoenaed to produce it.  However, if
 I don't have the information, I cannot easily
 be compelled to break into your house to collect  it.  If somebody wants to break into your house  badly enough they can do it, but we can take  steps to raise the cost.
5) We agree that illusory security is worse than
 none.  Tom Mitchell pointed out yesterday that
 Apple does not want to be "directly" complicit  in pillaging your data.  However ... if pillage
 is still going on, a big pretense of security
 would be worse than nothing.  It would reflect  a "Not My Job" attitude:
    So I say let's take a step in the right direction
 today ... and then take whatever additional steps
 are necessary.

@_date: 2014-09-20 13:44:26
@_author: John Denker 
@_subject: [Cryptography] Simple non-invertible function? 
There are at least four different concepts on the table.
 a) Non-invertible function.
 b) One-way function.
 c) One-way permutation.
 d) Backtrack resistance.
 *) various less-relevant possibilities
In cryptography, there is a rather detailed technical
definition of "one-way function" ... and the details
are important.
One-way /permutations/ are of particular importance,
because they preserve entropy.
I'll let Sandy speak for himself, but given the context
I suspect that the application calls for a backtrack
resistant one-way permutation ... not just any old
non-invertible function.
As others have pointed out, multiplication by zero is
a simple (!) non-invertible function according to the usual definition, but not very useful in the context of RNGs or crypto in general.
Yes, but is it backtrack resistant?  Is it a permutation?
Does it preserve entropy?
---> What is the intended application?
As Henny Youngman would say:  So don't do it that way.
Choose a bigger blocksize, so that brute-force lookup tables are infeasible.  Ditto for rainbow tables.

@_date: 2014-09-23 13:47:18
@_author: John Denker 
@_subject: [Cryptography] encryption is /normal/ : iOS 8 and Google and ... 
One more thing I forgot to mention about the new iOS 8: The crypto is
turned on by default.  In an indirect way, that is more important than
it might sound.  There are (or soon will be) millions of people
running around with encrypted data.  That means that those of us who
use crypto no longer stick out like a sore thumb glued to a lightning
In a related matter:  Google has announced that when both http: and
https: versions of the same document exist, the secure version will be
favored in search results.  This policy has not yet taken effect
AFAICT, but it sounds promising.  It helps in the obvious direct way,
and also in the indirect way mentioned in the previous paragraph.
We seriously need to create an ecosystem where the use of crypto
cannot result in "probable cause" or even "reasonable suspicion".

@_date: 2014-09-23 14:36:27
@_author: John Denker 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
Agreed.  See further "mining" below.
I never said NSA "designed" DES.  I said they weakened it.
FWIW, they're not even pretending otherwise anymore.
See e.g. page 232 of reference [1].
Isn't that enough?
IBM wanted a longer key.  NSA wanted a much shorter key.
They compromised on 56 bits.  Reference [1].  Also implied
by reference [2].
  Very hypothetically and temporarily *IF* we compare DES
  to a 64-bit cipher with random S-boxes, DES is stronger   with respect to differential cryptanalysis but weaker   with respect to brute force.  Indeed according to Adi   Shamir, DES is about as strong as 128-bit Lucifer.
Non-hypothetically, I don't care.  That's not the right comparison to be making.  One of the most fundamental principles of reasoning is to consider /all/ of the
plausible options.  It would have been straightforward to strengthen Lucifer against differential cryptanalysis without shortening the key.
As it says in reference [1], quoting none other than Frank
  "in the long run it is more important to secure one's own   communications than to exploit those of the enemy."
Alas the NSA seems to get this wrong again and again and Well executed?  I very much doubt it.  It sounds like an awfully foolish gambit to me.  I attribute to the NSA an immense budget and some highly skilled cryptologists, but
I don't give them credit for being able to predict the actions of other people.
At the time, any sane person would have expected such a gambit to backfire ... and all available evidence suggests that it did backfire.  The ones who were most hurt by weakening DES were outfits like US banks who felt constrained by regulation to use DES, who trusted NSA to get it right, and were too clueless to superencrypt.
In contrast, when playing chess or doing high-stakes crypto, you should not assume that your main adversary is clueless.
Specifically, at the time (mid 1970s) the microelectronics
revolution was in full swing.  DES was allegedly constrained
to "just barely" fit on a single chip.  So in accordance with
Moore's law, all you needed to do is wait a couple of years and then implement a scaled-up version on a single chip ... or implement the algorithm in software on a microprocessor.
This is more-or-less what happened.  Hint: GOST.  Soviet chip fabrication was years behind the US, but there was nothing to prevent them from buying microprocessors by
the bagful.  The last time I checked, GOST (very unlike DES) was unbreakable in practice even today.
  Then superencrypt with bog-standard DES on the off chance
that the NSA was actually adhering to Rowlett's dictum
for once ... and so you can say to the banking regulators yeah, sure, I used the approved DES.
Also superencrypt with whatever you were using before,
be it a fancy rotor machine or whatever, on the off chance that there might be a systematic weakness in all Feistel ciphers.
To summarize:  The claim that DES was superior to this-
or-that straw man is irrelevant and deceptive.  Better ciphers were available at the time.  Proof by construction.
Useful references:
[1]   Thomas R. Johnson
      "American Cryptology during the Cold War; 1945-1989"
      Center For Cryptologic History / National Security Agency (1998)
      [2]   Michael Schwartzbeck
     "The Evolution of US Government Restrictions on
      Using and Exporting Encryption Technologies"
      From "Studies in Intelligence"  (the secret internal CIA magazine)
      (date not obvious;  circa 1998)

@_date: 2014-09-23 23:09:17
@_author: John Denker 
@_subject: [Cryptography] NSA versus DES etc.... 
On 09/23/2014 05:35 PM, Richard Outerbridge replied:
I hope we don't need to quibble over the definition
of what "was" was.
There is a big distinction between
 -- /was/ in existence, and
 -- was /visible/ in the published literature.
Note that Soviet spies were not obliged to publish
their crypto designs.
Really?  Nothing in existence?  How do you know?  How sure are you?
I have in mind GOST, at some point in the 1970s, for multiple reasons.
For starters, it is reported by Schneier that the GOST cipher was developed and deployed in the 1970s.  I'm
not sure what is source is on that.  See also:
  Also, it's obvious from the design that GOST was optimized
to run on 1970s-style hardware.
Furthermore ... even in the absence of direct evidence I would be almost certain that the Soviets had something like GOST.  Any self-respecting cryptographer who looked
at DES and the public discussions thereof would say
  a) That's a nifty architecture.
  b) The key is too short.
  c) There's something sketchy about the S-boxes.
  d) There's barely enough rounds.
About 60 seconds later the guy would come to some
  a) Let's keep the Feistel architecture.  It    generalizes to any blocksize and any keysize.
  b) Let's use a much longer key.
  c) Let's change the S-boxes.
  d) Let's use more rounds.
I very much doubt that the GOST 28147-89 cipher was the best they came up with.  It's just the best they deigned to publish.
In particular, GOST has an astonishingly small hardware
footprint.  It stands to reason that they came up with other versions optimized for board-level and/or software implementation.  For starters, adding more rounds increases the cost only linearly in software, but increases the attacker's cost exponentially.  Using 128-bit block size costs practically nothing, except insofar as it might require more rounds to ensure full diffusion, yet it makes things very much more unpleasant for the adversary.
Similarly, expanding the S-boxes to 8 bits in and 16 or even 64 bits out costs practically nothing in software, increases diffusion, yet raises the workload enormously
for the adversary.
If nothing else, the triple DES idea has been around
since the late 1970s, as a way of getting around the
short-key problem.
I know first-hand that people in the US were playing
with scaled-up DES variants in the 1970s.  They didn't
talk much about it.  If the Soviets didn't do something
similar, I would consider it inexplicable, to say the
I stand by the point of my previous message:  The idea
that the NSA could talk about DES in a way that tricked
the Soviets into making a cryptographic mistake does
not withstand scrutiny.
The entirely foreseeable result of putting out a weakened cipher standard was that friends would use
the weakened version and enemies would very rapidly
come up with a non-weakened version.
If the NSA couldn't foresee this, they were really,
really dumb.  Assuming they did foresee it, it tells
you a lot about their priorities.

@_date: 2014-09-29 10:09:31
@_author: John Denker 
@_subject: [Cryptography] new wiretap resistance in iOS 8 
In the context of:
Subtle?  Not entirely subtle so far.  "What if Bin Laden kidnapped
your child?  What then??????!!!!!!!"
            etc. etc. etc. etc.
This kind of PR operation creates political support (or at least
political cover) for legislation.
It looks like they want to fight this on all fronts.  They can do subtle stuff and non-subtle stuff at the same time.  They can play good cop / bad cop.
  "In the long run it is more important to secure one's own   communications than to exploit those of the enemy."
                                  -- Frank Rowlett
  "Let's create a situation where are friends can be spied
  upon more easily than our enemies."
                                  -- NSA policy for 40+ years

@_date: 2015-04-07 07:16:50
@_author: John Denker 
@_subject: [Cryptography] upgrade mechanisms and policies 
Multiple recent threads have been very interesting, but
incomplete and unbalanced.  It is like the old parable:
Each person is correctly describing their favorite /part/
of the elephant without acknowledging its relationship
to the other guy's part ... and to other parts heretofore
not mentioned.
For crypto primitives as well as higher-level protocols,
all-too-often we lack both /mechanism/ and /policy/ for outroduction (at the end of the life cycle).
 *) It could be argued that in the absence of a decent
  policy, there's no point in having a mechanism.
 *) It could be argued that in the absence of a decent
  mechanism, there's no point in worrying about policy.
Each of those arguments is technically true, but irrelevant.
The point is, we will have to /solve the whole problem/ eventually, including both mechanism and policy.
  Suggestion   This is a discussion group, not the
  patent office.  Brainstorming is allowed.  Incomplete
  ideas are allowed.  People are not required to   /solve the whole problem/ on Day One.
  On the other hand:  Suggestion   Those who are   suggesting a mechanism should please acknowledge the
  need for a policy.  If they haven't got a policy, they
  should at least point out where a policy could be   plugged into their mechanism.
As another way of saying the same thing:  If we don't
like the mechanism, we should criticize the mechanism on its own terms, or suggest a better mechanism;  we
should not blame the policy (or lack thereof).  Conversely, if we don't like the policy, we should criticize the policy on its own terms, or suggest a better policy; we should not blame the mechanism (or lack thereof).
The overall job's not done until we have both mechanism
and policy.
This elephant has been seen before.  It is a classic
fallacy in business management.  The business plan for
some new widget always includes an introduction strategy,
but all-too-often neglects the outroduction strategy.
The guys promoting the new widget assume a ridiculously
overoptimistic estimate of its lifespan.  The wise
manager will slap those guys upside the head and tell
them not to come back until they have a more grown-up
business plan.
This is related to the sunk-cost dilemma, which is the
mirror image of the sunk-cost fallacy.  It is best
understood in terms of game theory, rather than classical
There exist workable models that we can look to for
inspiration.  For example, several categories of
aeronautical charts expire every 56 days.  The reason
is that the effective date of the new chart serves
as "flag day" for anything that needs to be changed,
such as airway alignment, communication frequencies,
et cetera.  Everybody knows in advance that the charts
have to be replaced every 56 days, even if the only
change is the expiration date.
Crypto itself provides some good examples:  We put
expiration dates on our session keys, PGP keys, x.509
certificates, et cetera.  Everybody knows in advance
that such things will have to be replaced.
The same approach could be applied to crypto primitives
and higher-level protocols.  It could be applied to the
Internet of Things and everything else.  We should plan
for the *fact* that such things have a finite useful
lifespan.  Life-cycle costs should be reflected in the purchase price.  We can only estimate the lifespan, but we should not assume a ridiculous overestimate.  An imperfect estimate is better than no estimate.  Most importantly:  Once you have a mechanism for updating
the widget, you can do that ... even if the only thing
that changes is the expiration date!
Don't tell me that the IoT can't be upgraded.  The very
fact that the widgets are connected to the internet
makes them easy to upgrade, incomparably easy compared
to other things that we manage to renew every so often,
such as car tires, brake pads, and engine oil.  Doing
the IoT upgrade securely requires a bit of crypto, but
we are supposed to know how to do that.
Do we need a plan for dealing with emergencies?  Yes,
absolutely.  That's an important part of the elephant.
The only thing better than that is /preventing/ emergencies,
i.e. staying far far away from anything that could lead to an emergency.  In particular:  We should plan on replacing primitives and protocols /before/ we know for sure that they are broken.
As a corollary:  We can avoid "flag day" problems by
introducing the new thing on cycle N, then deprecating
the old thing on cycle N+2 and outlawing it on cycle
N+4.  This sort of well-planned transition works a lot better in non-emergency situations.

@_date: 2015-04-14 13:12:32
@_author: John Denker 
@_subject: [Cryptography] ToFU +- SaFU 
Hash: SHA1
By way of background:  There is a distinction between
  *) ToFU == Trust on First Use
  *) SaFU == Same as First Use
... among many other important distinctions to be made.
I mention this because on 04/13/2015 12:35 PM, Christoph OK.  That's the ridiculous system we are stuck with at
the moment.
That's not a fully accurate description of PGP.  PGP is often
nothing more than SaFU.  That is:  I have a lot of PGP keys for people who aren't entirely trustworthy.  All the PGP signature tells me is that it is the /same/ sketchy character as last time.
SSH ought to be in its own category.  I reckon it is mostly
used in ToFU or SaFU mode ... but it can be configured to
respect authorities also.
* More importantly, these schemes are not mutually exclusive.
* A combination of authority plus pinning is incomparably
* more secure than either one separately.
That's an important point.
To expand upon that point:  There are crucial distinctions
between identification, authorization, recognition, and trust.
Recognition comes before trust.  If I can recognize the guy reliably and repeatedly, then over time I might develop "some" level of trust.
It must also be emphasized that "trust" is not a Manichaean
black-or-white proposition.  It's not even one-dimensional.
I might trust somebody with $10 but not $10,000.  I might trust somebody's judgment in one area but not another.
  Maybe in a small village identification+authentication
  could result in a measure of trust, insofar as if the
  guy did something nasty you could find him and punish
  him.  In contrast:  On the internet, identification+
  authentication is almost completely decoupled from trust.
The semantics of signing a /key/ seems IMHO undefined and undefinable.  In the real world we have ways of specifying
what a signature means.  A contract spells out in detail
what its signatures mean.  A signature on the front of a check means one thing, while a signature on the back means something else.  A signature on a candidate's nominating petition means something else yet again.
It makes sense to PGP-sign a contract (or an email) ...
but signing a "key" is highly problematic, because there
is so little control over the semantics.
It cracks me up when people complain about "identity theft".
There is no such thing as ID theft;  there is only FRAP:
Failure of Ridiculous Authorization Protocols.
    Ford:   I know who you are.
    Spike:  Yeah, I know who I am, too. So what?
                    Knowing how to identify me (in the sense of being able to pick me out of a crowd) does not mean you "are" me, and it absolutely doesn't mean I authorized such-and-such financial transaction.  A authorization protocol that relies on individual traits and not-very-private factoids (such as
SSN) is just ridiculous.
Person-to-person fingerprint exchange is almost irrelevant
to me.  There are lots of people I trust /with certain things/
based on reputation and/or longstanding collaboration.  In
some cases I have met these guys in person ... and in other
cases not.  In any case, meeting the guy has got virtually
nothing to do with my decisions about trust.
Looking at the guy's driver's license is even less relevant.
OK, so the guy has a license that says John Smith.  Is that
John Smith the cryptographer, John Smith the pornographer,
John Smith the spy with a high-quality fake license, or
what?  And why should I trust that guy more than John Smith
the sock puppet or John Smith the dog?
   Seriously, I would be more inclined to trust a sock puppet
with no license and a good reputation than an in-person
person with a license and no reputation.
I don't pretend to be an expert on such things, but it's
obvious even to me that we need to take a step back and
get a better handle on what we are trying to do.  Too often,
crucial distinctions are getting trampled on.
At the very least, we need a layered approach:
  *) At some level, we need a secure channel, resistant
   to tampering and to eavesdropping.
  *) Given such a channel, we can carry out identification,
   authentication and (more importantly) recognition.
  *) Given recognition, we can over time develop various
   degrees and various kinds of trust.
Also:  We need to stop signing "keys" and instead sign
things where the semantics is clearly specified.
  At this point the question arises, how do you get a   computer to check the semantics.  I have no idea how   to do that in the general case, but I'm not sure it   matters, so long as "somebody" can look at it and   understand the semantics.
  Common special cases can be handled.  For example,   there are big companies that will sell you a   paperless-office system whereby you can sign a   travel voucher and your department head can
  counter-sign it.  The semantics is reasonably
  clear.
My PGP signature on this email indicates that I sent it, and that it represents a snapshot of my opinions.

@_date: 2015-04-17 11:59:26
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
Agreed, the question makes no sense.
Agreed.  It is a fool's errand to assess the randomness of a "random number".  For starters, there is no such
thing as a "random number".
  -- If it's a number, it's not random.
  -- If it's random, it's not a number.
  -- You can have a random /distribution/ over numbers,
   but then the randomness is in the distribution, not
   in any particular number that may have been drawn
   from such a distribution.
It is a well-known issue.
    Anybody with any sense would assess the /mechanism/
whereby the allegedly random distribution is generated.
No, because they are not /independent/.  (Sometimes it may be computationally infeasible to exploit the dependence, but that's a separate question.)
Obviously, entropy is computationally indistinguishable from entropy ... but the converse does not hold:  Just because you can't tell whether something is entropic or not doesn't mean it /is/ entropic.
Suppose I prepare a one-time pad consisting of a sample
of 2000 random bits.  I print it once on red paper, and print the same thing on blue paper.  I give you one of
them.  That gives you 2000 bits of information you wouldn't otherwise have.  Then if I give you the other one, that gives you zero additional information.  The amusing thing
is that it doesn't matter which one I give you first, blue or red ... the first one is informative and the second one is not.  Either one separately has 2000 bits of information ... but the information is not additive.
In the language of chemistry: entropy is not an extensive
This is another manifestation of the principle set forth
above:  Entropy is a property of the ensemble, i.e. a
property of the distribution ... not a property of any
particular sample that may have been drawn from such
a distribution.
That's not true.  That's not even the right way to
frame the issue.
The fact is, the formalism is a tool.
 -- Some questions are easy to answer using entropy.
 -- For that matter, some questions are easy to answer
  without using entropy.
 -- Some questions are hard to answer, whether you
  use entropy or not.
 -- In addition, there is a provably infinite number
  of ways of abusing the tool, to get all sorts of
  wrong answers.  You can't make anything foolproof,
  because fools are so ingenious.
Also not true.  Information theory can be used to
provide some very strong secrecy guarantees.  This
is often sufficient but not necessary, insofar as
there may be more convenient ways of meeting the secrecy requirements ... but information theory is by no means silent on the issue.
Any tool can be abused.  There is however a community
of people who know what they are doing, who use the
terminology and (!) the underlying concepts correctly.
Linux /dev/random has lots of problems, but underestimating
the entropy -- by overestimating the effect of depletion -- is not one of them.
A leak changes the ensemble.  Entropy is a property of
the ensemble.

@_date: 2015-04-17 18:47:35
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
Sorry, no, there is not any notion of complexity or entropy that is inherent to an individual string.
This is fundamental to cryptography, although oddly enough
your average cryptographer doesn't need to worry about it on
a day-to-day basis.  By way of analogy, atomic physics is
fundamental to chemistry, but your average chemist doesn't
need to worry about it on a day-to-day basis.
You can define complexity that way, but statement [1] does not follow from [2], not even close.
First of all, calling it a "universal" compressor in the narrow ultra-technical sense required by statement [2]
does not make it a good compressor, or an all-purpose compressor ... and it certainly doesn't make it unique.
The complexity is as much a function of the compressor [C] as it is of the string (str):
     K = K[C](str)
For any two compressors [C1] and [C2] there is a uniform
bound on the difference between K[C1](...) and K[C2](...).
Sometimes the difference doesn't matter, but sometimes it
matters a great deal.
Also, there is not the slightest reason to expect that
the complexity is additive.  That is, in all probability:
    K[C](str1 + str2) != K[C](str1) + K[C](str2)
which is another reason why na?ve notions of complexity
"inherent" in the string fail miserably, even for some
fixed compressor [C].
The formal relationship is simple and easily stated.
Entropy is defined in terms of probability.  If you
have a probability distribution over programs, it
induces a probability distribution over strings ...
or it would, if it didn't run afoul of the halting
If you ignore the halting problem you can get lower
bounds on the probability of each string.
Whether the probability constructed in this way bears
any relationship to the real-world probability of the
relevant strings is another question entirely.  There
are lots of probability measures in this world.  Just
because you have found "some" probability measure does
not mean it is the right one.  Calling it a "universal" probability measure doesn't make it an all-purpose probability measure, or even a decent approximation to the correct probability measure.

@_date: 2015-04-19 00:18:14
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
In the context of:
Well, as it turns out, I have read those guys, et al., and I find
that they recognize that the complexity of a string is not inherent; it depends on how it is measured.  They don't always make a big fuss about the dependence, but they know it exists.
That's a common misconception ... but it remains a misconception
nonetheless.  As I said before, for *some* purposes it doesn't matter, but for other purposes it matters a great deal.  In
more detail:
 -- If you are a common carrier selling bits in bulk, a bounded
  number of "extra" bits does not affect the price per bit that
  you offer to your largest customers.  Alas, common-carrier
  quantity discount issues are off-topic for this list.
 ++ Probability is exponential in complexity.  An additive
  shift in some of the complexities makes an exponentially
  large /multiplicative/ shift in some of the probabilities.
  This multiplicative factor does *not* wash out as the
  strings become longer.  For crypto, this is crucial.
We agree that the ratio
         K[C1](str)
       --------------                       [1]
         K[C2](str) converges to 1 as the length of str becomes large.  However, it is a gross fallacy to imagine that the difference
     K[C1](str) -  K[C2](str)               [2]
converges to zero.  Convergence of the ratio does not imply convergence of the difference.  In all probability, the sequence of differences [2] doesn't converge at all, although it remains bounded.  If the emulator program that allows C2 to emulate C1 is only ten bytes long, it means your probability estimates are off by "only" a factor of 2^80.
In the cryptography business, factors of 2^80 are often
significant in practice.  Also, most emulator programs are
more than ten bytes long.
Well, on the philosophy list or even the "pure mathematics"
list you are free to hypothesize any probability measure you
like, no problem there.  However, crypto depends on real-world engineering.  It depends on math also, but not math alone.
By way of analogy:  In pure mathematics, the axioms of Euclidean geometry assume a flat space.  If you want to assume that, go
ahead ... but remember that's just a mathematical fantasy. Don't turn around and pretend that you have proved that the earth is flat, or that spacetime is flat.
In the world where I live, the metric (i.e. the curvature) is not determined by pulling axioms out of some bodily orifice;
it has to be measured.  By the same token, you cannot assign
entropy to this-or-that string by cranking up the first Turing
machine you find lying around.  The entropy is a property of the ensemble, not a property of the string.  In fact it is an ensemble average, the expectation value of the surprisal.
It has to be based on measurements, not on philosophy, not on arbitrary hypotheses.

@_date: 2015-04-19 15:22:12
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
By way of background: It is a bedrock principle of sound reasoning and sound planning that one should    /Consider all the plausible scenarios./
This principle is well known in farming, small business,
big business, and even the Boy Scouts.  If you are
planning a campout, consider the possibility that it
will rain, even if you hope it doesn't.
I mention this because on 04/19/2015 04:27 AM, Jerry We should not make any such assumptions.
We should not make that assumption, or the opposite,
or anything like that.  Instead, we should make sure
that our methods work for *any* distribution of inputs.
That includes
 -- known plaintext (0% entropy density)
 -- completely random plaintext (100% entropy density)
 -- everything in between
The in-between case is incomparably more important than either extreme.  In the case of completely known plaintext, the sender has nothing to lose from bad
crypto.  The case of completely random, meaningless, unauthenticated plaintext is almost as trivial; the
attacker can replace the message with random garbage
and nobody will know the difference.
So, again, the bedrock principle is:
   Consider all the plausible scenarios.
   Consider all the plausible hypotheses.
The only people who don't recognize this principle
are the high-school science fair people.  They seem
to think that science is some sort of occult guessing
game.  They seem to think that scientists are not as
smart as Boy Scouts.  This is a travesty of science, and a safety problem also.
    Please don't presume that sort of thing when we are
talking about entropy.
Both of the following are interesting:
 -- The computationally-feasible one-way property
 -- Entropy.
They are both interesting, but they are *not* the same.
thing.  To have an intelligent conversation we must
respect this distinction.

@_date: 2015-04-20 13:58:18
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
Clarification: The full, correct statement is:
  For any given distribution,                        [a]
  the entropy is a property of the distribution      [b]
  ... not of any particular string that may have   been drawn from such a distribution.               [c]
When people are in a hurry, common practice (but not good
practice) is to assert part [b] without being explicit about part [a].  An example is my message from 04/17/2015 11:59 AM.
Beware that in part [b], the word ?the? must not be given
undue emphasis, because it is heavily modified by part [a],
and must not be taken out of context.  That is, we are
*not* talking about ?the? one true distribution.  In
cryptography and many other situations, it is common to
have more than one distribution in use at the same time.
Even if you know the string, you don't know ?the? distribution
from which it was drawn.  In a card game, the microstate
(i.e. the objective state of the cards) is the same for
everybody, but different players see the probabilities
differently, i.e. they are working with different
distributions.  For further discussion including a
diagram, see
  In crypto, it is virtually always the case that the sender
and the attacker are using wildly different distributions over plaintexts and keys.  The microstate is known to the
sender, whereas the attacker presumably has to guess.  The
microstate is the same for both, but the distributions are
Therefore it is madness to speak of ?the? entropy inherent
in a string.  It is not possible to ascertain or even define
?the? one true distribution, not using Turing machines, not using philosophy, not by any means whatsoever.
In the crypto business, more often than not, when people
talk about ?the? distribution they are talking about the
distribution /as seen by the attacker/ ... but I am not
touting this as a reliable rule.  The smart approach is to identify explicitly the distribution you are talking
about ... or (!) to carry the distribution as an unbound
  Yes, I know one can find lots of references to the
  idea of ?the? ?inherent? entropy.  By the same token,
   -- Climate change is a hoax:            -- Dinosaurs coexisted with humans:
       -- The Apollo moon landings were fake:
       -- Cigarettes are not addictive:
       -- Elvis is not dead:
       -- et cetera..............
  Similarly, I am quite aware that wikipedia says that
  both energy and entropy are extensive variables:
      However, it's just not true (not for energy or entropy),
  especially for smallish systems, where surface states   make a nontrivial contribution.  I hate to belabor the
  obvious, but wikipedia is not a reliable authority for
  the foundations of physics or information theory.  It
  doesn't try to be.  It explicitly doesn't want to be.

@_date: 2015-04-21 04:12:26
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
"When it was announced that the Library contained
   all books, the first reaction was unbounded joy."
Please don't abuse the terminology of "entropy" in this
way.   ++ There is such a thing as a computationally-strong
  pseudo-RNG (CSPRNG).
 ++ There is also such a thing as a true-RNG (TRNG).  A   good hardware-RNG (HRNG) closely approximates a TRNG.
A CSPRNG and a TRNG are both interesting, but they are not the same thing.  Really not.
Suggestion:  If you mean randomness, please say "randomness".
Entropy is a very special kind of randomness;  it is not an
all-purpose synonym for randomness.
Sorry, no.  The entropy density of a PRNG is a lot closer
to zero percent than it is to 100 percent ... and is not
the basis for judgment.  That's how you know it's a pseudo-RNG not a true-RNG.
A PRNG is judged on (among other things) whether breaking
it is /computationally feasible/.  This stands in contrast
to the definition of entropy, which has got nothing to do with computational feasibility.  In particular, breaking *any* PRNG will always be easy in the NP sense;  you just have to figure out the internal state.  In contrast, a TRNG has no seed and no long-term state ... and the amount
of entropy cannot be reduced with NP work or any other amount of work.
It may be that for this-or-that practical purpose, you
don't care whether something is impossible in principle
or merely "hard enough" ... but still, you've got nothing
to gain by trampling on the distinction.  If you don't
really mean entropy, please don't call it entropy.  Call
it "randomness" or whatever.
There is no "also".  Entropy is a function -- technically
a /functional/ -- of the distribution.  If you know the
distribution (aka ensemble aka macrostate), you don't care
who or what the observer is.
Sometimes when I am observing things ... e.g. when playing
poker ... I change my mind about what distribution to use.
This changes the entropy, even though the observed string
and the generation process remain the same.  The observer
remains nominally the same.  I suppose you could say the
state of the observer changed, but in any case, the only
thing that matters is the /distribution/.  Entropy is
*defined* to be a property of the distribution.  It is
an ensemble average, specifically the expectation value
of the surprisal.
I have no idea what that's talking about, but it's not
talking about entropy.
First of all, not all real sources have a relevant
feedback variable.  It is entirely possible to build
a HRNG that produces 99.99999% entropy density without
using any long-term memory.  One batch is completely
independent of previous batches.  In digital-filter
terms, it's FIR, not IIR.  No feedback.
  It's fairly common practice to stick in some
  feedback anyway, even though it falls into the
  category of belt+suspenders.  If done halfway
  competently, such feedback does not weaken the
  HRNG, not even a little bit.  Random XOR random
  is still random.
Secondly, entropy is properly defined in terms of an ensemble average.  Averaging over the ensemble
washes out most of the time dependence.  There is
such a thing as "entropy fluctuations" but they're
not good for anything;  the cost of identifying a
fluctuation exceeds any value you could hope to
get from it.  There is an extensive literature on
the topic of Maxwell demons.
  Maybe this-or-that screwed-up implementation has
  a "window in time" where things go wrong, but that
  does not "always" happen.
If we now assume a decent HRNG implementation, the
output entropy density never goes to zero, not even
close, not asymptotically or otherwise.  If at some
point the output string looks non-random to you, that's not a reflection of the actual entropy, as you can verify by looking at other members of the Suggestion:  Even though you can "usually" replace
the ensemble average by a time average, if you are
in a situation where you think fluctuations might
be important, you can save yourself a *lot* of
trouble by not doing that.  Rely on the ensemble

@_date: 2015-04-21 13:36:22
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
The quantity calculated there is called the /surprisal/.
It is not properly called the entropy, or the min-entropy, or anything like that.
Here's how you know:  The surprisal is a very simple function
of the sample.  For any given distribution P, the surprisal
of the (i)th sample is       $(i) := log(1 / P(i))
The entropy is a functional of the distribution as a whole.
It is an ensemble average, i.e. the expectation value of
the surprisal:
      S[P] := ? P(i) log(1 / P(i))
Again:  For any given distribution, the entropy is a property
of the given distribution ... not a property of any particular
sample that may have been drawn from such a distribution.
To say the same thing the other way:  If it's a property of the sample, it cannot possibly be the entropy.
You have to decide what you're trying to do.  a) If you're building a PRNG, the objective was never to   achieve 100% entropy density anyway.  The output of a   PRNG is a lot closer to 0% than to 100% entropy density.
  That's how you know it's a PRNG.
 b) If you're trying to build some sort of whitener to
  hang on the output of a HRNG, that's something else
  entirely.  Then you care about the approach to 100%
  entropy density.
Both of these are interesting tasks, but they are not the
same thing.  Not even close.
We can agree that it appears(*) impossible to build a practical HRNG (or whitener) that emits exactly 100% entropy density ... although a good HRNG comes very
very very close.
(*) I can't prove it's impossible, but I've never seen
it done, and I can't imagine how it could be done.  It
doesn't really matter, because (with a lot of effort)
one can come close enough for any practical purpose.
The artificial example given above is a mild case of
something that is absolutely primary in the design of any practical HRNG ... in some sense the definition
of the task.  The core task of any HRNG is to start with a screwy distribution and transform it into a nice
distribution.  You need a thousand pieces of tricky
infrastructure to support that core, but still it's the core and the raison d'?tre.
That's not the right way to think about it.  Although
entropy has many important uses, it is *not* the answer
to all the world's questions.  In fact, in modern
cryptology, entropy is the answer to a verrry tiny
subset of questions.  The strength of a PRNG depends on (among other things) the computational difficulty of breaking it;  it is *not* measured in terms of entropy.
Any PRNG is easy to break in the NP sense.  That's what "pseudo" means.  That's what makes a PRNG different from a TRNG.  In contrast, the definition of entropy makes absolutely no mention of computational feasibility.
Secondly, the NSA backdoor in the Dual_EC_DRBG PRNG does
not merely remove "a few bits" from a large search space;
it makes the search trivial.
Thirdly, more than a few people knew about the backdoor,
years before the Snowden revelations.  Snowden made it
more widely known, confirmed it was intentional, and
showed that it was part of a much larger program.  Still,
the people who noticed it earlier are not "silly" people.

@_date: 2015-04-22 21:20:01
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
Hash: SHA1
I reckon that's true in context, but to be more general
and more methodical we ought to specify what "we" are
using the word for.
Surely entropy is not the answer to all the world's
questions ... but it is the answer to some questions,
including some ultra-fundamental ultra-practical
The operation of every cryptosystem I know of depends
critically on randomly-distributed numbers.  If we get that from a CSPRNG, we care about the computational
strength of the algorithm, but we also care a great
deal about the seed.  If the seed comes from another
CSPRNG, it reduces to the problem previously not
solved:  where does the seed come from?  The only
way to escape the loop is to obtain a seed with real
entropy.  Physics is required.  You can't do it with
algorithms alone, as von Neumann pointed out in 1947
... but you can do it with physics.
The average cryptographer doesn't need to worry about
it on a day-to-day basis, but the fact remains:   If you follow the chain far enough, every cryptosystem I can imagine depends on a number of things, one of which is physics, more specifically thermodynamics.
   (Thermo includes quantum statistical mechanics.
   Often the classical limit is more than good    enough, by a wide margin, but we can handle
   the general case if we want.)
Do not take your random numbers for granted!  It is
not by accident that the NSA decided to subvert
random number generation via NIST SP 800-90.  If
you can break the RNG, you can break everything.
I vehemently disagree.  Entropy is entropy.  It is
not the answer to all the world's questions;  in
many cases it is not even the right way to frame the question.  So use a different term already!
Something can be 100% random in the sense of a
CSPRNG yet still have very nearly 0% entropy
density.  The physics entropy *is* the crypto
entropy;  it's just not (usually) the relevant
measure of randomness.
That's fine, but how do we convince ourselves that
the state is unknown and unpredictable?  I strongly
recommend obtaining seeds from a good HRNG.
Entropy is one thing.
Surprisal is another.
Randomness is yet another.
The only rule is, say what you mean, and mean what
you say.  If you mean randomness, say "randomness",
not entropy.

@_date: 2015-04-24 11:49:51
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
Actually the physics entropy /is/ the computational entropy,
without limitation.  Checking the equivalence is simple in some cases and complicated in other cases, but it has been   Nitpickers note:
  a) The classical information (Shannon) corresponds to the
   classical physics (Boltzmann).
  b) In the exceedingly unlikely event that you are dealing
   with entangled Schr?dinger cat states, you need to use a
   more sophisticated formula for the entropy ... same
   concept, fancier formula ... but even then the physics    entropy /is/ the computational entropy.  Unsurprisingly,
   you need to upgrade both sides of the equivalence.
If you want to break it down into two steps, and delegate the first step to somebody else, that's
entirely reasonable.
However, it is not reasonable to prejudge the outcome of the first step.  In particular, it is not reasonable to squander the results of the first step by:
 a) Assuming differences with no basis in reality
  (Shannon versus Boltzmann), or
 b) Assuming equivalences with no basis in reality
  (entropy versus computational feasibility).

@_date: 2015-04-24 22:07:45
@_author: John Denker 
@_subject: [Cryptography] Entropy is forever ... 
OTOH we ought not run around making the problem worse.
That's true. Absolutely not end of story, for multiple reasons:
 *) We can quite reasonably discuss the fact that the
  entropy is not "in" the pool, but rather in the ensemble
  of similarly-prepared pools.
 *) We can quite reasonably ask whether the code keeps   track of the entropy correctly.
 *) We can discuss the fact that the driver is completely   at the mercy of upstream entropy providers. It depends
  on them to provide entropy, and to correctly say   /how much/ entropy is being provided.
 *) We can also discuss the fact that what the code
  actually /does/ is different from what the comments
  say it does ... and what the symbol-names imply it
  does.
 *) We can also quite reasonably ask whether entropy   is the right thing to be keeping track of.
You could define your terms as you go along.
If you say entropy when you mean surprisal, it may be easier for you to say, but it's eeeeeenormously harder for everybody else to figure out what you're talking about.  If you want to
be understood, it helps to use the terminology correctly.
Yes, there ... along with lots of other places.  The term "surprise value" has been in the communication-theory literature for well over 50 years that I know of, possibly "The" normal metric?  I think that's overstating it.
For starters, here's a whole family of R?nyi functionals.  If
you just say "R?nyi" without qualification, people will assume
H[2], which has to do with collisions.  It might be relevant
or it might not.  If you mean H[1] that's just the plain old
Shannon entropy.  If you mean H[?] you should please say so. It has some relevance directly, and also indirectly insofar as it provides a lower bound on the plain old entropy.
Secondly, it depends on what you're doing.  As usual, we
must ask What's Your Threat Model and by the same token
What's Your Use Case.  It also depends on whether we are
optimizing things from the message-sender's point of view
or the attacker's point of view.
Surely there are /some/ quite-reasonable threat models where
we care about the min-entropy H[?].  I'm 100% supportive right up to the point where it is claimed to be "the normal metric". necessary and sufficient, but sometimes it is merely sufficient.
In the parts of the world where I live, there is rather little
one can do with Shannon entropy alone, or with algorithms alone.
The combination of the two is a lot more interesting.  The same
goes for min-entropy plus algorithms:  The combination is more
useful than either one alone.  In particular, the min-entropy
might tell us that one of the output codes is at risk of being
enormously over-represented.  That doesn't matter, if the attacker has to wait 10^10 times the age of the universe before seeing that code.
A large hash buffer size ("pool" size) covers a multitude of sins.

@_date: 2015-12-01 10:03:13
@_author: John Denker 
@_subject: [Cryptography] Security of a permute-only system? 
The question is interesting ... but unanswerable.  It is ill-posed coming and going, i.e. simultaneously overspecified and underspecified.
To save time, I will offer some guesses as to what was intended.
If these guesses are close to correct, fine;  otherwise they will
serve to indicate what sort of clarification is needed to make
the question answerable.
*) The Subject line is contradicted by the body of the message.
The Subject line says "permute-only" but the body specifies
a whitener.  The whitener is doing a lot of the heavy lifting.
Depending on unspecified details of the whitener, the system could be reasonably secure with or without the permutation ... or it could be hopelessly insecure with or without the permutation.
Therefore assumption [A]: We should ignore the Subject line,
and assume some sort of whitener is present.
*) Similarly the body says the whitener and permutation "might
be part of a larger system".  Again depending on unspecified
details, the larger system might be reasonably secure with or
without the permutation ... or it might be hopelessly insecure
with or without the permutation.
Therefore assumption [B]:  We should ignore this part of the
body, and assume the larger system is absent, and focus on
the whitener and permutation.
*) A strong cipher is in some sense an ideal whitener.  Such
a thing makes the question trivial.  The answer is that the
permutation adds nothing;  the system is as secure as it's
going to get, with or without the permutation.
Therefore, to avoid triviality, assumption [C]:  We assume the whitener is an open code, perhaps like the famous mercantile
telegraph codes e.g.  If the whitener is keyed in any way, we assume the key is
known to the attacker (Eve).  If the whitener is context-
dependent (as any decent whitener must be), we assume the
context is known to Eve, or guessable with high confidence.
*) It is not specified whether the same permutation is used
for a single block or re-used for multiple blocks.  Since
re-use is a dumpster fire, assumption [D]:  All messages
(or all sessions) are one block long.
Under assumptions A,B,C,D we can get an upper bound on how
well the system can perform.  After whitening we have a 171
bit block, and all 2^171 possible patterns are more-or-less
equally probable.
Now consider an implementation [E1] where those 171 bits are
re-encoded in 176 bit words, such that each word has 88
ones and 88 zeros.  That is, each word has a Hamming weight
of 88.  That is relevant, because Hamming weight is a
conserved quantity with respect to permutations.  There
are (176 choose 88) such words, which is more than 2^171.
The re-encoding scheme is known to Eve.
This is an interesting representation, because now a
permutation can change any codeword into any other.  It
takes log2(176!) = 1064 bits to specify such a permutation.
That's a lot ... but you should not imagine that it is
all-powerful;  to specify an arbitrary invertible function
would require log2((2^171)!) bits, which is a muuuuch bigger number.
Now consider a different implementation [E2].  To save money sending the bits over the wire, we encode the blocks using "only" 175 bits.  Half of the codes have a Hamming weight of 87, and half have 88.  We call these the "light" codes and the "heavy" codes respectively.  All together, there are (175 choose 87) plus (175 choose 88) such words,
which is more than 2^171.
The problem is, the set of light codes is closed under
permutations, and the set of heavy codes is also closed.
That means that by looking at the ciphertext, Eve instantly
gets one bit of information about the plaintext.
For many applications, this would be a disastrous amount
of leakage.
Now consider an implementation [E3] where we use the
raw 171-bit blocks, without re-encoding.  There are now
172 different Hamming weights, which Eve can observe and
exploit.  The leakage is an order of magnitude worse than
in scenario E2.
Summary:  A,B,C,D,E2 is insecure, and A,B,C,D,E3 is worse.
A,B,C,D,E1 "might" be better.  It is not quite as obviously
broken as the others, but I offer no guarantees, only an upper bound on the strength.  There could be innumerable less-obvious weaknesses.

@_date: 2015-12-04 15:23:00
@_author: John Denker 
@_subject: [Cryptography] =?utf-8?q?The_attack_that_broke_the_Dark_Web?= 
I can think of a dozen reasons why people /should/ want and expect
privacy when browsing public sites.  These days shopping for a pressure cooker can get you into trouble.
That is "supposed" to provide privacy ... but how sure are we that
the Tor network is not a wholly-pwned subsidiary of CMU / FBI / NSA
Here's an interesting article on the subject:
 [1] Kashmir Hill   "The attack that broke the Dark Weband how Tor plans to fix it"
  The basic story has been floating around for a while, but that is the most detailed account I've seen of how the Tor guys detected the attack.  Among other things, it quotes the Black Hat abstract
that was taken down:
  A less-detailed article on the same subject is:
 [2] Andy Greenberg
  "Tor Says Feds Paid Carnegie Mellon $1M to Help Unmask Users"
  I doubt the details of that incident will remain secret much longer.
Looking forward:  It is reported [1] that Tor ...
That doesn't impress me.  It would be poor tradecraft to repeat
the tactic of inserting a "bunch of servers" into the Tor network all at once.  One must assume that a slightly less oafish M.O. would be used for subsequent attacks.  One wonders whether more a gradual infiltration would be detected.

@_date: 2015-12-12 12:06:55
@_author: John Denker 
@_subject: [Cryptography] crypto hygiene for keys, pads, et cetera 
In a sense that's entirely true, but in another sense it is profoundly That's an important proviso, but not the only proviso.  Much depends
on details of the threat model.
 -- Stand-off attacks against the communication channel only?
 -- Attacks that capture the pad (and the user)?
Here's problem  If you were to use that program "as-is", it would flunk the silk-or-cyanide test.  It implements the encode and decode
functionality, but fails to implement the /one-time/ property.
Here's problem  It is quite nontrivial to fix problem As the saying goes, encryption is easy, but security is hard.  The
XOR program is fine if all you need is encryption/decryption, but
it is vastly harder to implement a true OTP system that ensures
that the pad is used only once.
Here is a discussion of what can go wrong ... plus some possibly-
constructive suggestions on how to obliterate information stored
on flash memory chips.
  I would start with the point mentioned above:  Encryption is easy, but security is hard.  Real security depends on mathematics, physics, electrical engineering, computer programming, human factors, et cetera.
This should be flattering to the multidisciplinary audience.
For a general audience, one might consider presenting Merkle Puzzles
rather than full-blown DH.  It requires a lot less mathematics.

@_date: 2015-12-13 17:44:23
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Very true.
Very untrue.
In reality, every PRNG requires a seed ... whereupon you have reduced
the task to the problem previously *NOT* solved:  where are you going to get the seed????
There is a long and sordid history of problems with random number
The physics doesn't have to be complicated, but it is absolutely
necessary for generating true randomness ... necessary for seeds and for other high-grade applications.  You can't do it with physics
alone or with algorithms alone;  you need both.
BTW, as I have said before:
  There is no such thing as a "random number".
  If it's random, it's not a number.
  If it's a number, it's not random.
  You can have a random distribution over numbers, but then
  the randomness is in the distribution, not any any particular
  number that might have been drawn from such a distribution.
Or as John von Neumann put it:
     Anyone who considers arithmetical methods of producing
     random digits is, of course, in a state of sin.  For, as has been
     pointed out several times, there is no such thing as a random
     number -- there are only methods to produce random numbers, and a
     strict arithmetic procedure of course is not such a method.

@_date: 2015-12-22 09:36:00
@_author: John Denker 
@_subject: [Cryptography] Questions about crypto that lay people want to 
between classical crypto and voting:
The requirements are different yet again for DRM.  In classical
crypto, both endpoints share an interest in keeping the traffic
secret from third parties, but in DRM the endpoints have wildly
different interests.
Like e-voting, some of the goals that you might "want" to achieve
with DRM are simply impossible.
Changing gears:  Another point to be made to the nonspecialists is:
  Metadata is data.
  Stealing metadata is stealing.
  A cryptosystem that leaks metadata is a cryptosystem that leaks.
At the next level of detail:  The whole idea that metadata is somehow
"different" is a legal fiction, arising in the US as a way of getting
around the fourth amendment.  Once upon a time, the kernel of the idea
was:  If you share your data with a third party, it's no longer "your"
data, and it can be seized without a warrant.  This reasoning was applied
to so-called "dialed number recorders".  You lost control at this point,
since you had to give the dialed number to a third party (the telco) to
complete the call.  This was implemented via a "pen register" that was
supposed to be capable of recording the dialed number and nothing else.
IANAL, but despite the foregoing, the plain language of US law, as I
read it, does "generally" require a warrant for pen-register type    That law makes a certain amount of sense, as a matter of public policy.
It reflects the idea that you "should not" lose all rights to your data
(including metadata!) when it is temporarily entrusted to a third party.
Then a miracle occurred:  legal fiction turned into pharisaical pettifoggery.
By "creative" interpretation of the patriot act, and with the help of
the marsupial FISA court, the NSA decided that they could hoover up
everybody's so-called metadata /without/ a warrant.  In other words, they
gave themselves a "general warrant" which is something the Framers knew
all about.  This is *exactly* what the fourth amendment forbids, and was
intended to forbid.
   Furthermore, by additional casuistic chicanery, they gave themselves
a super-expansive definition of "metadata" ... far beyond anything
that was needed for delivery of the message, and far far beyond the
sort of things that could be picked up by a traditional pen register.
This is relevant to crypto because 95% of the world never had fourth
amendment protections to begin with, and in the US none of the three
branches seem interested in upholding the fourth amendment ... so the
only way to obtain even a modicum of privacy is to use encryption.

@_date: 2015-12-24 10:22:31
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
That is true at the moment, in practice, AFAIK ... but it doesn't
have to be that way.  In particular, the controller on the SSD
could provide a verrrry effective crypto-erase function if it
wanted to.
This is a fixable problem!
Compared to some of the hard problems routinely discussed on this
list, this problem is very easy to fix.  Almost all of the pieces
are already in place.  For example, there is already some open-
source SSD firmware.
At the next level of detail, the problems *and* a sketch of a
solution are discussed at:
  Seriously:  The following efforts could be carried out in parallel,
and then joined:
   a) Work with openssd-project.org to implement secure-erase
    functionality in the firmware.
   b) Prevail upon some SD card and/or SSD vendor(s) to offer
    products that are open-source-friendly ... in analogy to
    the wireless vendors that are DD-WRT-friendly.
This is an eminently fixable problem!

@_date: 2015-12-26 02:53:52
@_author: John Denker 
@_subject: [Cryptography] Godwin's law +- actual facts 
There were no free elections in Germany in 1933.
*) In 1932, Hitler ran for President and lost.
*) In the last pre-1933 elections, the Nazi share of Reichstag
  seats /declined/ to about 1/3rd.
  The centrist parties "should" have been able to form a coalition
  government devoid of Nazis, but they could not get their act
  together.  Part of the problem was Nazi intimidation and coercion.
*) On 30 January 1933 Hitler was appointed (not elected) Chancellor.
  The Nazis immediately moved to take over the media.
  On 28 February 1933, civil liberties were formally and explicitly
  suspended.  Thousands of opposition leaders were arrested.
*) Elections were held on 5 March 1933 but these were definitely
  not free and fair.  Some opposition parties had been outlawed.
  The Nazis made heavy use of radio, which they controlled.
  Even so, the Nazis got less than 45% of the vote.  Even if you   include their nationalistic and antisemitic allies in the DNVP,
  the two parties together got less than 53% of the vote.  That is
  a sufficiently slim margin that one cannot imagine that in theory
  they "would have" won a fair election.
  In practice it was enough to allow Hitler to virtually abolish   the Reichstag, and to rule by decree from then on.

@_date: 2015-12-27 11:31:14
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
material   MP / C
--------   -------
Aluminum:    660
Silicon:   1,414
SiO2:      1,600
Al2O3:     2,072
Gold:      1,064  (irrelevant, but since you asked)
chemistry      T / C
---------      -------
Air-MAPP:       2,020
Oxy-propane:    2,253
Oxy-MAPP:       2,925
Oxy-acetylene:  3,500
One amusing low-temperature option is to drop the chip into a pot
of molten aluminum, Terminator-style.  Silicon will /dissolve/
into molten aluminum, much as sugar dissolves into water, at
temperatures well below the MP of the solute.
*** Discussion:
I suggest that a belt sander or even a simple disk sander makes a
more convenient solution.
In any case, the physics problem is relatively easy to solve ...
but that leaves us with other problems.
For starters, a chip can hold many gigabytes of data.  Most crypto
operations, even one-time-pad operations, don't need that much, so
it is wasteful to destroy one chip per operation.
Even more serious is the user-interface problem.  In this forum
we bemoan the fact that users all-too-often choose a low-entropy
password, and re-use the password across multiple sites, because
it is "more convenient".  Therefore it strikes me as unlikely that
ordinary users can be trusted to annihilate one micro-SD card per
message, or one USB stick per message.  It's just too inconvenient.
It seems to me that for user-interface reasons alone, we really
need a flash memory with good crypto-erase performance.  Features
should include:
  -- Can erase small chunks (not just the whole drive).
  -- "Spare" copies of the data are never left lying around on
   the device.  If data is moved for wear-leveling, the old version
   is immediately obliterated.  If blocks need to be moved to the    bad-block list, they are immediately obliterated.
  -- Simple high-level interface.  In particular, overwriting a
   logical block should suffice to obliterate the previous contents.
  -- Good efficiency for normal operations.
This is all eminently doable!
A lot of the required pieces are already lying around.  See
      and references therein.

@_date: 2015-12-28 20:54:57
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
It is easy to put the flash chip beyond normal use ... but that
is not the same as being fully secure, because the attacker might use non-normal methods, including things like electron
microscopes.  The real question is how fast the electrons diffuse
out of the floating gates.
Note that when GCHQ wanted to destroy The Guardian's computers
they did not consider it sufficient to bake the circuits.  They
seems to have a pretty good idea of where bits might be hidden.
  On the other edge of the same sword, I mention yet again that baking is not very nice from a human-factors point of view.
I have zero confidence in them.  People have recovered data
from supposedly-erased flash chips without much effort.
     Of the remaining seven, only four executed the ERASE
  UNIT command reliably.
That's known to be not good enough.  The device could quite
plausibly have twice as much storage as you think it does,
and who knows what is lurking in the hidden half.
Not sure what is the question there.  See previous references
to various open-firmware efforts.
Not sure what is the question there.  The problem is that modern flash chips are highly complex and highly variable.
"Generic" drivers cannot keep up.  For practical purposes,
you need the chip vendor to ship a suitable driver along
with the chip.  OTOH chip vendors have no clue about crypto.
One way out of the dilemma would be to have a sufficiently flexible driver license, either open source or a not-too-
nasty NDA, so that somebody competent can add the required
crypto-friendly features.
Some manufacturers (e.g. Micron) make nice noises about
supporting open-source efforts.
To answer a question that other people have been asking, one
can mitigate the turtles-all-the-way-down problem by using cut-and-choose.  You order a bunch of chips.  You tear down
some random subset, and verify that the gate layout and
firmware conform to specifications.  This, in conjunction
with sufficiently harsh penalties for noncompliance, provides
some modest level of confidence.
That's called a hypothesis, or perhaps a question; not an In any case, the answer depends on your threat model.  It
depends on what problem you're trying to solve.  If you are trying to build a one-time pad, as various people including
Henry Baker have suggested (with varying levels of seriousness)
encrypting it is nowhere near good enough.  It requires you
to trust the crypto, rather than trusting the intrinsic
one-time-pad properties.
There are other scenarios where it might OR MIGHT NOT be
good enough to encrypt the data and throw away the key.
First of all you have to trust the crypto, and secondly you
have to trust that the key has been thoroughly obliterated,
which just gets us back to the problem previously not solved.
Typically crypto changes the /magnitude/ of the problem by
protecting a big file with a small key ... but it does not
reduce the problem to zero.  Beware that if you screw this
up, you are giving the storm troopers a reason to beat the
key out of you.
Also, full-disk encryption imposes a cost in terms of access time and battery life.  For most applications it's worth the
cost, but not everybody is 100% happy with it.
Bottom line:  There are always going to be "some" things that need to be erased.

@_date: 2015-12-29 05:54:03
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
1) Yes, such a failure is verrry unlikely.  It's getting into tin-foil
hat territory.  It would cost the adversary a billion dollars per bit,
on average, to get information this way.
2) It's more unlikely than that, if bits can be read but not erased,
 since it is trivial for the controller chip to notice the problem  and alert the user, whereupon he immediately takes the memory chip  to the belt sander.  More specifically, a successful attack would
 require:
  a) failure to write (i.e. preconditioning, which suffices for obliteration)
  b) failure to erase (which also suffices for obliteration)
  c) failure to read, in such a way that neither (a) nor (b) gets noticed.
I'm not saying correlated failures are impossible (hint: United 232)
but there are ways of reducing them to a very low level (fault tree This decreases the window of vulnerability by additional orders
of magnitude.  This gives the hat an extra layer of tin foil.
3) It is uneconomical for the attacker to "tailor" the flash-chip
physics.  That's because there are incomparably easier ways of
compromising the overall device, e.g. "tailoring" the controller
chip, which they have already been doing, e.g. Stuxnet.
There are things we can do to make the controller chip more secure
than it is today, but even so, it will remain a bigger target than
the memory array.  So this gives the hat a third layer of tin foil.

@_date: 2015-12-30 00:56:12
@_author: John Denker 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Hash: SHA1
We are having two separate conversations here.  We need to distinguish
  a) the limitations of today's firmware, versus
  b) the fundamental limitations of the technology.
Realism is good, but fatalism and defeatism are bad.
 a) As of today, bad things happen in firmware all the time, on
  *every* write to flash, whether there is an error or not.   b) Looking into the future, most of the problems are eminently
  fixable.
That highlights one of the fundamental (but fixable) problems.  As
of today, the computer industry considers "reliability" to mean
protecting user data against erasure.  In contrast, the crypto
community thinks "reliability" includes protecting user data against eavesdropping.  The two requirements are diametrically
opposite;  one seeks to preserve bits, while the other seeks to obliterate bits.
In normal routine operation, SSD firmware could securely obliterate
bits if it wanted to.  The problem is, as of today, it doesn't even
try.  It leaves extraneous copies lying all over the place.
That's true, the charge pumps are fragile.  And in all likelihood
the same charge pump is used for programming (writing zeros) and for
erasing (writing ones).  So if the charge pump fails, neither the
erase cycle nor the programming cycle is available for obliterating
sensitive data.  The chip becomes read-only, whether you like it
or not.
On the other hand, there should be no way this can happen without
being noticed.  The user's write() operation should return an error
code.  Let's assume there is private data on the chip at this point.
  -- If in response, the user throws the memory card in the trash,
   and the attackers scoop it up, there is gross data leakage.
  ++ If in response, the user incinerates the chip, or grinds it to    dust, there is no data leakage.
To summarize:  There is no point in talking about failure modes in
present-day chips, because the chips are disastrously insecure even
when they are working as designed.  In contrast, in the future we
could write security-conscious firmware.  It would still be easy for the attackers to cause gross failures, but it would become
hard to cause subtle failures.  Maybe I'm overlooking something,
but if the user practices decent tradecraft, I don't see the gross
failures as being much of a problem.  I don't mind annihilating a
chip, so long as I don't have to do it very often.

@_date: 2015-12-30 10:38:24
@_author: John Denker 
@_subject: [Cryptography] Unbreakable crypto 
Hash: SHA1
On 12/30/2015 04:50 AM, Jeremy French asked:
Answer:  Cover traffic.
I'm serious.  Send lots of cover traffic.  This point was also made in the message by Henry Baker on 12/30/2015 06:14 AM.  That
message was sarcastic as to tone, yet correct as to principle.
In particular, here is a not-very-laborious way of sending some
cover traffic:
  *) Configure your mailer to include a little bit of cover traffic
   in every email you send.  I've been doing this for a year or so.
   Look at the Quilt: header in this message.
Here is a more elaborate way.  This is not as sarcastic as it
might seem on first reading:
  1) Choose some prominent recipients in your jurisdiction.  For
   example, you might choose Richard Cheney, Donald Rumsfeld, and
   Jay Bybee.
  2) Cut some PGP keypairs, one for each recipient, and immediately
   throw away the private keys.
  3) Obtain a couple kbytes from the best RNG you have.  Encrypt
   it with the aforementioned keys.
  4) Email the encrypted data to the chosen recipients.
  5) Repeat steps 3 and 4 every so often.
  6) When asked by your "paranoid government department" you can
   say there is a conspiracy to commit war crimes, torture, murder,
   perjury, illegal wiretapping, and seditious overthrow of the    US constitution.  By way of corroboration, Cheney has admitted
   as much during TV interviews ... but emphasize that the proof
   is in those encrypted emails.  Tell them that they should arrest
   those guys and torture them until they divulge the keys.
HB sent cover traffic to this list, which is better than nothing,
but maybe not sufficient if you think the aforementioned paranoid
government department would just round up everybody on this list.
The technique of saying "I AM SPARTACUS" has a mixed track record.
So choose a wider distribution.
While we're on the subject, each person on this list is invited
to send some modest amount of email (up to 1 MB per day) to
.  PGP key 1449C7F7 should be available on all
the usual keyservers.  All mail received at that address is immediately discarded, whether encrypted or not.  All data
encrypted with that key is unreadable, because the private
key was destroyed as soon as it was created.
More generally, *all* network protocols should be redesigned to
include a goodly amount of cover traffic.  It will take some
time to implement this, but in the meantime we should not let
the perfect be the enemy of the good.
Bottom line:  Cover traffic.  Send a bunch of cover traffic already.
If we get to a point where enough people send enough cover traffic,
the bad guys will face a much harder problem, namely searching for
a particular needle in a stack of needles.

@_date: 2015-02-03 15:46:23
@_author: John Denker 
@_subject: [Cryptography] crypto standards and principles 
That's been the rule since before the NSA was the NSA.
 ++ The secrecy should be in the keys.  In contrast, the
  method "should not require secrecy, and it should not
  be a problem if it falls into enemy hands."
                     -- Auguste Kerckhoffs (1883)
 ++ "In the long run it was more important to secure one's own   communications than to exploit those of the enemy."
                     -- Frank Rowlett (1942)
 ++ "The enemy knows the system."
                     -- Claude Shannon (1949)
The NSA was not created until late 1952.  I believe Rowlett
was its Technical Director on Day One.
  Be that as it may ... the NSA still keeps secret the workings
of its own most-advanced systems.  AFAICT there are only two
possible explanations for this:
 1) They think their best system might get broken if the
  adversaries found out how it worked.
 2) Many other systems are already broken, and they don't   want unbreakable crypto to fall into the hands of others.   This surely leaves friendly non-top-secret communications   vulnerable, in violation of Rowlett's maxim.
Either way, it's not very flattering to the NSA.

@_date: 2015-02-09 01:24:27
@_author: John Denker 
@_subject: [Cryptography] Question on crypto implementation in existing 
That's good advice.  That's what I do.
Agreed.  The "etc." includes memory locking, facilities
for reading a password from the terminal, and a bunch
of other housekeeping stuff that would be virtually impossible to do with any semblance of security in
python ... for reasons having little to do with the
cryptological mathematics.

@_date: 2015-02-11 20:47:59
@_author: John Denker 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Sometimes that "has to be possible" ... and sometimes
not.  It is not desirable to allow somebody on a whim
to ask the general question of "what is this person authorized to do?"
In particular, suppose I walk into a sleazy bar and wish
to open a tab.  It is reasonable for the barkeep to ask
a couple of capability-related questions:
 1) Is this guy of legal age?
 2) Does this guy have a line of credit with the bank,
  good for $50.00 or so?
It is *not* reasonable for him to ask other questions:
 3) Does this this guy have a line of credit with the
  bank, good for $500,000.00 or so?
 4) Does this guy hold a NATO nuclear security clearance?
 5) Does this guy hold a commercial pilot certificate?
 6) Is this guy authorized to transport a box full of   ballots from point A to point B on election day?
 *) et cetera......
Questions in the latter group might be perfectly reasonable
in some other context, but not here.  In general, we do not
want some random person going fishing through the whole list of capabilities.

@_date: 2015-02-16 20:06:52
@_author: John Denker 
@_subject: [Cryptography] trojans in the firmware 
Those are well worth reading.
There are two possible interpretations of the following passage, where Goodin quotes Costin Raiu, director of Kaspersky Lab's
global research and analysis team:
It may be not possible /at the moment/ ... but in context
Raiu seems to suggest it is not possible in principle, which is crazy wrong.  I say instead:
a) The disk manufacturer could allow you to read the firmware
 easily.  It's always going to be readable if somebody wants
 to go to enough trouble;  we're only arguing about the price.
b) If they don't want to make reading easy, they should
 provide /at least/ the following:
  -- the total number of times the firmware has been modified, and
  -- the current hash.  Cryptologically strong hash.
===>  Heretofore there hasn't been much market demand for such a feature, but that is something that can be
changed.  In particular, the Kaspersky guys, rather
than giving up, should be collaborating with the disk
vendors to come up with a workable solution.
Obviously the checksum reporting routine needs to be in a non-flashable part of the firmware.
OTOH, equally obviously, this may not solve the problem
if your threat model includes "interdictions", where
the opposition trojanizes your equipment while it is in transit from the vendor to you.
On the third hand, there are ways of defending against even that.  It's not particularly hard to build your own
computer from components;  Gazillions of hobbyists have
done so.  If you think you are under threat, buy the
components and test them at the component level.  For
instance, read the BIOS ROM using a ROM reader, i.e.
in a way that is not dependent on booting from that
ROM.  Note that there exist open-source BIOSes.
Ditto for the boot blocks on the disk.  It doesn't take a very fancy disk diagnostic to discover that what you are reading from block 0 isn't what you
Then put the components together in some slightly
eccentric way.  For example, if you're doing software
RAID, it becomes very much harder for the firmware in
the disk drive to screw you over.  That's because the
drive doesn't know what you're going to do with the
We need to exert serious pressure in this direction.
This isn't just about Iranian weaponeers versus the NSA, which is complicated by political questions
about who you think the are the bad guys.  It is also about bankers trying to fend off advanced persistent threats from bandits.  Even if you think bankers are
bad guys, the bandits are worse.
And then there is election machinery.  Vote-counting
is heavily computerized, and we reeeeally need more transparency and reliability at every level, from BIOS and disk firmware on up.

@_date: 2015-02-17 12:58:41
@_author: John Denker 
@_subject: [Cryptography] What do we mean by ... ??? 
I've been enjoying this voluminous thread ... and also
the one about "best practices".
Here's my two millicents worth:
Ideas are primary and fundamental.  Terminology is tertiary.
Terminology is important only insofar as it helps us formulate
and communicate the ideas.
1) It is nice to see that despite the title, the "best practices considered bad term" discussion was almost entirely about ideas, not terminology.
Small constructive suggestion:  Insofar as BCP (best current
practices) is a term open to abuse, we can use a different
term, perhaps BPP (baseline prudent practices) ... with the
understanding that any particular instance ought to exceed
the baseline by a wide margin.
2) As for the terminology of "security", in my book that's
asking the wrong question.  Some people use the term
"reliability" to cover a combination of security and
availability.  For example:
  -- The proverbial air-gapped abacus in a vault inside    a Faraday cage surrounded by armed guards is very high
   on security, but low on availability and usability.
  -- The converse is more complicated.  Availability    (especially in the long term) requires security against
   intrusion;  otherwise hackers will take down your
   system whereupon you have neither availability nor
   security, much less reliability.  Still, though,
   availability is not the only requirement, especially
   if you have secrets that you need to keep.  You    don't want your secrets to be available to everybody.
   So reliability really is the better idea, comprising    both security and availability.
3) There was a tangential mention of baseline prudent
practices concerning the choice of passwords.  As I
see it, that's another example of asking the wrong
questions.  IMHO almost all of what we now do with
passwords should be protected by some sort of zero-
knowledge crypto.  This would decrease the attack
surface by orders of magnitude.  When I do business
with Achmed's Recycled Body Parts, he should have no
clue what password I am using, only that it is the
same password that I used last time.  Obviously if Achmed gets hacked, the hackers can impersonate Achmed
to me ... but they cannot get my password from his files, not even if I log into the hacked site.  In
particular, the hackers cannot impersonate me to other sites, not even to Achmed's sister (Kitty's Kosher Pork and Porno Emporium).
To say the same thing the other way, if I am running
a web site, I do *not* want to know anybody's password!
I don't want their password on my site, even momentarily.
That's because knowing it would make me a target for
bad guys who want to steal the password.  Why be
responsible for protecting something if I don't
need to?
only better, so there is no barrier to adoption
that I can see.  It's better in the sense that
the user needs to remember only one password,
with improved security.
This reduces the number of things the user has
to trust, but of course does not reduce it to
zero.  The local agent that carries out the
algorithm can still be subverted, perhaps by
a keylogger, but that is a much smaller target
than before.
  By way of contrast, using a unique password
  per site is more secure than using one password
  for all, but it is less convenient, and also
  carries some unnecessary risks, e.g. replay
  et cetera.
There exist implementations of zero-knowledge
password systems, e.g.
  and probably others.   Does anybody here have any
experience using such things?  Any useful pointers?
Why are such things not more widely used?  Why are
they not already fully integrated into servers and

@_date: 2015-02-18 14:55:35
@_author: John Denker 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
I would have said it differently:  Passwords are deeply flawed in principle, but convenient in practice.  An OTP is almost the opposite:  it is perfect in principle, but
inconvenient in practice -- often fatally so.
Agreed.  A password, like a chainsaw, can be /used/ in lots of ways ... some good, some bad.  One has to think very carefully about how to use it.  Many things that look like they might be an improvement end up being unusable or worse.
It is better to light a candle
than to curse the damn darkness.
   For one thing, merely /telling/ users what to do is
not a solution.  That experiment has been done.  They cannot be relied upon to actually do it.  We need mechanisms that make it easier to do the right thing,
and harder to do the wrong thing.
More generally, we don't face a choice between "fragile
replacements" and doing nothing.  There are a lot of
ways to preserve a /user experience/ that looks like
a simple password, yet improves the results.
Back in the stone age, computers stored the passwords
directly in a file.  Then somebody got the bright idea
of storing a salted and hashed version.  This was an
improvement of the type I am talking about.  The change
was completely transparent to the users ... except that
they could no longer ask the sysadmin to remind them of their password.
Nowadays, we have the option of going to some sort
of zero-knowledge password authentication, which is
another step in the aforementioned direction.  It
means the user password is never on the vendor's site, not even momentarily.  This can be done in a way that is completely transparent to the user.
Seriously, it's a fairly simple syllogism:  Once you require people to have a whole slew of passwords,
it becomes advantageous to have a password manager,
and if you are going to do that, you might as well
do the full zero-knowledge dance.
Alas, this would require the vendors to have a clue,
so I don't expect it to happen anytime soon.  See
below for a stopgap.
Again, there are devilish details.  Note that it is a serious burden to have users choosing N passwords, keeping them from being lost, keeping them from being stolen, finding the right one when needed, not sending them to not-quite-right
sites, et cetera.  I consider that to be on the list of ideas that make things in some ways better but in some ways worse.
Personally, my approach is not to write them down,
but rather to compute them on the fly.  This is not as good as SRP, but it's way better than most of the lower-tech alternatives.  It is less convenient
but more secure than just using the same dumb password
everywhere.  Meanwhile it is more secure and tremendously more convenient than looking up the proper password in some file.
  See also the limitations listed there.
As usual, the program is a few lines of crypto and
500 lines of user interface.  However, the interface
(crude as it is) is important.  Among other things, it makes it virtually impossible to send any given password to the wrong site.

@_date: 2015-02-19 12:07:49
@_author: John Denker 
@_subject: [Cryptography] Lenovo laptops with preloaded adware and an evil 
Thanks for the heads-up.
Attached are:
 -- The highly-compromised private key for "superfish" rogue CA.
 -- The self-signed CA certificate.
 ++ A /revocation/ for the aforementioned certificate.
I reckon the revocation needs to be distributed widely, the sooner the better.  Perhaps some folks on this list can help
with that.

@_date: 2015-02-23 07:30:08
@_author: John Denker 
@_subject: [Cryptography] Information.  it's the real thing 
It is always important to think about the /information/ carried by our messages.  This includes the information
available to the intended recipient, as well as the info
available to the adversaries.  This is relevant to what John Young wrote on 02/16/2015 10:34 AM:
... and relevant to a lot of other recent discussions,
and to crypto and security in general.
Information.  It's the real thing.
Regrettably, in dozens upon dozens of messages recently,
people have tossed around phrases like "indistinguishable
from random" (I-F-R).
Although I-F-R is "close" to the right idea, it is not
exactly the criterion we should be using.  To my astonishment, on 01/21/2015 09:17 PM, Jerry Leichter contradicted me on this point:
The nice thing about formal definitions is that there
are so many to choose from.  Anyone is free to define terms however they like, but the result is a choice,
not a law of nature.  There is no guarantee that the results will apply to the real world.  Cryptology lives within the intersection of fancy math *and* down-to-earth engineering.  You need both.
I choose to formalize many things in terms of information.
Often it is appropriate to exclude information that would
be computationally infeasible to obtain.
I do not choose the I-F-R criterion, because it is not viable in the real world, and I can prove it.  An example suffices to make the point:
  _Message Length_ is one of the many things that can be
  used as a covert channel.  It has been used this way   for a long time.  It has been used by both teams, i.e.
  when the sender wants it to be used that way (for
  exfiltration) and when the sender doesn't (but the   opponent uses it anyway).
So .... does anybody really think that the message length
needs to be indistinguishable from random?  If so, we are
in big trouble, because message length could be anything
from zero on up, and there does not exist any uniform
random distribution on such a range.  The formal laws of probability forbid it.  This is an example of what formality can do for you.  Sometimes it tells you that your formalism
is broken.
In the real world, people sometimes do send traffic that
resists traffic analysis.  One option is to send a lot of messages, all with the same length.  Consider for example ATM cells.  Many trillions of them are sent, all with the same size.  You will not have much luck traffic-
analyzing those lengths!  Arguably the first one told you something, namely the length, but it didn't tell you anything you didn't already know, given that it was an
ATM cell to begin with.  Even if you didn't know a_priori
that it was an ATM cell, after a while the leakage (on a per-message basis) goes asymptotically to zero anyway.  As long as the traffic is indistinguishable from business-
as-usual, the nobody gains any information from it.
There is an extensive literature on this, including
contributions from guys like Shannon, Kullback, and
Leiber, who were not exactly clueless about crypto.
If you think your formal methods are better than theirs, that's an extraordinary claim, and will require some
extraordinary proof.  My suggestion: if you want to formalize something, in all likelihood your time would
be well spent formulating it in terms of information gain and things like that, rather than some notion of
"indistinguishable from random" ... which is no more formal, and a lot less viable in the real world.
Speaking of cover traffic:  If you look at the header
of this message, you will probably find a field called
"Quilt" which contains 64 symbols that could convey
6 bits apiece, for a total of 384 bits.  All of my outgoing mail has had such a header for a while now.
Maybe it's just random cover traffic ... or maybe it is a cleverly-encoded message.  By sending such fields,
I create a forest.  That's a good place to hide a tree,
if I ever need to.
If you are wondering about the name: There is such a
thing as a crazy quilt, in contrast to a patterned

@_date: 2015-02-23 17:59:12
@_author: John Denker 
@_subject: [Cryptography] information, Shannon, and quantum mechanics 
I have no idea what that's trying to say.  Queen V.
died 15 years before Shannon was born.  He was not
a steampunk, not even remotely.  Bell Labs in the
1940s and 1950s was not known for doing 1800s-stye
Shannon's ideas of information and entropy have long
since been reconciled with quantum mechanics.
  Shannon:     1948
  von Neumann: 1955
The fully-general expression is known.  Reference:
  However, the answer is not very widely known.  Perhaps
that is as it should be, since Von Neumann's formula reduces to Shannon's formula in the correspondence limit.  The classical version is good enough for all applications you are likely to encounter, unless you go to a lot of trouble to set up a non-classical situation ... or you are answering trick questions.
  There is no doubt that such questions exist;
  see puzzle below.
Actually, Kemosabe, we do know.  The short answer is
zero.  In more detail:  there is no positive lower bound on the mass of a bit, or on the energy.  If you use a sufficiently sparse N-way coding, you can represent log(N) bits per photon (or per anything else).  This brute-force encoding suffices to prove
the point.
Those are questions from 50 or 60 years ago.  We have lots of other questions to keep us busy now, and 50 years from now.
Here's a seemingly-simple puzzle:
Suppose we have a system that can be in either of two
microstates.  Actually we have an ensemble of such
systems, and repeated measurements have observed the
system to be in microstate "A" with probability 1/3rd
and microstate "B" with probability 2/3rds.
Two questions:
  1) What is the entropy of the system?
  2) How sure are you that your answer is correct?
Hint:  It's easy to get the right answer, but it's also easy
to get the wrong answer, depending on how you approach the problem.  Lots of "authoritative" references will lead you to the wrong answer.
The usual jsd puzzle rules apply:  Everything I've said here
is true and helpful, to the best of my knowledge.  However,
obviously I haven't told you everything;  notably, I haven't told you the answer.  This isn't a word game;  solving the puzzle requires understanding the physics, not quibbling about words.

@_date: 2015-02-26 17:32:46
@_author: John Denker 
@_subject: [Cryptography] trojans in your printers 
I don't think that's good enough nowadays.
Bear said it's not to be "fully" trusted.  Forsooth, I don't trust an ordinary firewall at all, for the following
Let's think about where the threat is coming from.  On
a scale from ScriptKiddies to EquationGroup, the kiddies are not the ones trojanizing your printer, BIOS, and disk It seems prudent to assume that anybody who is badass enough to hack your printer will not hesitate to use a stolen IP address ... and MAC address.  The printer or toaster or whatever (i.e. Edith) can just passively listen to local traffic until it sees an address that seems to be working (i.e. Margaret) and use that.
  For purely outbound traffic, Edith can send pretty much whatever it likes.  Inbound traffic is almost as easy.
The remote party injects some packets into one of Margaret's traffic streams.  They fly right through
the firewall, and Edith just scoops them up.  If Margaret's connection is encrypted, so much the better.
The injected packets will be malformed, so Margaret will blissfully ignore them.  Edith however decodes them just fine, using a different algorithm.
Using current technology, the firewall has no way of detecting this, let alone preventing it.  The only box that has a chance of detecting it is Margaret.  It "might" notice its MAC address being abused, but what does it do then?  Write about it in some log file that never gets looked at?
If Edith waits until Margaret goes offline, any chance of
detection goes away.
I can imagine some sort of ?ber-firewall that might be
able to deal with this, authenticating each host separately,
in effect creating a virtual circuit, perhaps using IPsec or the like.  Thinking out loud here, it appears that odd
as it may sound, wireless might be more secure than wired ethernet in this regard:  Some base stations are smart enough to handle multiple ESSIDs.  If you give each device own ESSID and password, it would make the Dead Ringer scheme more difficult.
One could imagine revising the WPA specs to make this a lot better and a lot more user-friendly.  One could
imagine doing something similar with wired networks,
at layer 2.  Start from the assumption that the wired electromagnetic field is just as vulnerable as the radiated field, and proceed from there.
By the way, just for fun, why do we call them trojans?
   Sir Humphrey Appleby: I put it to you, Minister, that you are looking a Trojan horse in the mouth!
James Hacker: If we look closely at this gift horse, we'll find it full of Trojans?
Bernard Woolley: If you had looked a Trojan horse in the mouth, Minister, you'd have found Greeks inside.

@_date: 2015-02-28 14:46:18
@_author: John Denker 
@_subject: [Cryptography] Cheap forensic recorder 
There exist keyloggers for X-windows.
  As for the video, note that "screencasting" is super
trendy these days.  YMMV, depending on exactly what
effect you are trying to achieve, but here's what I
avconv                                  \
        -ar 44100                       \
        -f alsa                         \
        -i hw:0,0                       \
        -f x11grab -r 30                \
        -show_region 1                  \
        -s 854x480  -i :0.0+85,218      \
        -vcodec libx264                 \
        -preset ultrafast               \
        -crf 0                          \
        -ac 1                           \
        -y $ofile
You can look through the manual to figure out what
all those options do.
The idea of "ultrafast" is to get the stuff onto
disk as quickly as possible, with minimum realtime
CPU load.  I compress it later.
I had to jump through all sorts of hoops to make
that work.  In particular I had to compile my own
copy of avconv, since my distro didn't enable the
libx264 driver.
There is a lot more to the story.  I tried about
a hundred things that didn't work before settling
on something that pretty much does what I want.
If the question didn't apply to linux/X-windows,
then I have no firsthand knowledge, but I've seen
people using Camtasia for screencasting.  It seems
to know how to capture keystrokes as well as video.

@_date: 2015-01-05 08:20:17
@_author: John Denker 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
I suggest that any such analysis should proceed in two
stages:   A) synchronistic (i.e. "fair") criticism, and
 B) anachronistic (i.e. "unfair") criticism.
In category (A), it is entirely fair to compare Enigma to
the contemporary US M209 /and/ SIGABA.  The first thing you
notice is that it is a one-to-two comparison.   -- M209 was more portable, more reliable, less complicated,
  and less secure than Enigma.  It was used as a low-level
  field cipher.  By 1943, the Germans could break M209, but
  it took several hours.  This meant that the M209 was good
  enough for its intended purpose.
 -- SIGABA was larger and more complicated than Enigma.
  It was compact enough to be usable in a submarine, embassy,
  or intermediate HQ.  It was not broken during the war,
  and probably not for many years after that.
The first thing we learn from this is that you really need
to make the distinction between field cipher and HQ cipher.
Enigma was too big for one purpose and not big enough for
the other.
Also in category (A), Enigma used a "reflector".  This was
clever and stupid:  It was a cheap way to double the number of rounds, but it it meant that no letter could map to
itself, which was a devastating weakness.
Also in category (A): Parts of the Enigma system (notably
the initial rotor positions) provided what was essentially
an initialization vector.  This IV was not long enough and (in practice) not random enough.
This brings up a point that transcends categories:  Code
clerks make mistakes!  For example, German code clerks
tended to use an IV that was not much different from the
previous IV.  If they had been provided a pair of dice
and required on pain of death to choose truly random IVs,
cryptanalysis would have been more difficult.
Similarly, German code clerks were fond of sending test
messages consisting of proverbs or common jokes.  This
provided lots of almost-known-plaintext cribs.  Given a
truly strong crypto system this wouldn't have mattered, but given a system with weaknesses this added to the
stress on the system.
Also in category (A):  It helps to add more rounds.  SIGABA
had 15 rotors against 4 or 5 for Enigma.
Steckering made Enigma considerably stronger.
I'm not sure what category this is in, but one can imagine
something not much more complicated than Enigma where a
random Steckerverbindung became part of the per-message IV (rather than part of the daily key).  This would have made life very unpleasant for the codebreakers.  It would
have significantly reduced the volume of traffic they could afford to break.
Moving now to category (B):  Rotor machines as a class
have weaknesses.  Even SIGABA is breakable using modern
methods.  For starters, it has a key length of at most
72 bits, which can be brute-forced nowadays.  OTOH SIGABA could be scaled up to more rotors nowadays, using reliable
lightweight electronics, so the anachronism cuts both ways.
For some discussion of this, and pointers to other work,
  Also in category (B):  Any letter-by-letter system will
have weaknesses compared to a block cipher with a reasonably
large block.
Similarly, German messages tended to have formulaic headers
and prolix salutations.  It would have helped to remove these,
or at least compress them using a codebook.  Preprocessing messages with a lossless compressor is always a good idea, since it increases the entropy-density of the plaintext.
  This weakness remains relevant even today, e.g. as applied
  to full-disk encryption where there are many /blocks/ all
  the same, e.g. blocks of all zeros.
Compression followed by blocking is better than either one Just adding a bunch of random nulls at the front would have helped alleviate the known-header problem.  This would
have effectively increased the length of the IV, although there are other ways of achieving the same effect more That's true but misleading.  The M209 was broken by the
Germans as early as 1943.  The Ritchie/Reeds/Morris analysis
was suppressed for other reasons, presumably because the
systems that were still in use at the time.  The R/R/M paper remains unpublished to this day AFAIK.  See e.g.
    In any case, SIGABA remains a more worthy topic of "modern"

@_date: 2015-01-05 08:58:36
@_author: John Denker 
@_subject: [Cryptography] 
=?utf-8?q?ything=3F?=
I wouldn't have said it that way.  It would be somewhat better
to say ESP protects the transport.  IPsec is more than just
crypto, more than just ESP bits-on-the-wire.
  By way of background:  Security always has two parts:
  a) A list of good things to be allowed and supported, and
  b) A list of bad things to be prevented.
The IPsec RFC, to its credit, addresses both issues.
   a) The crypto allows you to do things you want to do.   b) One major function of the SPD is to disallow things   you want to prevent.
The point could be made that much of what the SPD seeks
to prevent is stuff you should have been preventing all
along, even if you had never heard of IPsec.  That's a
valid point.  However, it is also valid to observe that
IPsec, strictly speaking, grabs all of that and incorporates
it into itself.
It has been part of the freeswan / libreswan way of thinking
since Day One to concentrate on part (a) i.e. crypto, and leave part (b) i.e. firewalling to somebody else.  That's a reasonable management decision as far as it goes, but it should not be taken as a redefinition of what IPsec "is".
Especially in the present context, where people are ragging
on the IPsec SPD in particular, it pays to think and speak very clearly about such things.

@_date: 2015-01-08 00:47:14
@_author: John Denker 
@_subject: [Cryptography] lessons learned -- or not learned -- from Enigma 
It's not that simple.  Some people were non-metaphorically
trying to stab Hitler in the back and/or blow him to smithereens, for reasons having nothing to do with ULTRA or Enigma.  It's not a binary choice.  Weak crypto can coexist with subversion and treason.
Furthermore, when you have a little bit of each, the
combined effect can be strongly nonlinear, strongly
synergistic.  There is such a thing as multi-factor causation.  Competent crypto designers MUST take this
into account.
For example, over an 18-year period (1967--1985), the USSR
was able to break many millions of highly classified US
messages.  Broadly speaking, there were two causal factors:
 a) Chief Warrant Officer John Walker was a spy.
 b) The system was so badly designed that it was
  defenseless against a low-level spy.
For details on the many, many things that were done wrong:
  I hardly need remind this group that one of the selling
points for public key crypto is that it greatly simplifies
key distribution and key security.  This is just one of the ways in which a well-designed system can demagnify
the effects of a local breach.
  By way of contrast, having more than 150 trusted root
  CAs, any of which can sign any domain whatsoever, is
  an example of a bad system.  Any breach is a fiasco.
  This needs to get fixed, yesterday if not sooner.
Note that not every subversive is a traitor.  Before and
during WWII, it was certainly possible for Canaris to be
pro-Germany and anti-Hitler.  Similarly Snowden can be a
patriot and opposed to NSA abuses.  Furthermore, not every
screw-up is subversion or treason.  The root-CA situation
is an example.  I would call it a plain old lousy design,
except that I don't think it was designed at all.  It was
The Walker fiasco was extensively analyzed in the 1980s
and again by Heath (2005).  However, the Manning leak (2010) showed that the NSA had forgotten whatever lessons they had learned.  Furthermore, the Snowden leak (2013) showed that they STILL hadn't gotten their act together.
Obviously there is no such thing as perfect security,
but you can rig it up so that you don't bleed to death
from a shaving cut.
As an almost-separate issue, here's another lesson that
should have been learned over and over again, yet people
seem to keep forgetting:  These things need to be tested!
This is something you were supposed to learn early in
grade school:  CHECK THE WORK.
The Abwehr should have been able to figure out that the
allies had broken Enigma.  The US should have been able
to figure out that the USSR was reading virtually all of
the signals sent to US ships at sea.
For starters, you can send a steady trickle of false but
tempting information via encrypted channels.  Sooner or
later the opposition is going to take one of the baits. If they check to see whether the information is true or not, the act of checking gives away the game.
  It will always be a chore to untangle cryptanalysis
  from direction-finding, traffic analysis, overhead
  surveillance, plain old espionage, et cetera ... but
  you still have to do it.
On an even more basic level, you need to CHECK THE WORK
of the code clerks.  Check it in the field, under less
than ideal conditions.  You didn't really just send a
test message consisting of all Xs -- again -- did you?
You didn't really just use your girlfriend's name as
the passphrase, did you?
This is partially a discipline issue, but also partly
a system design issue, directly relevant to this group:
Make it easy to use the system properly, and hard to
use it improperly.  Don't just guess, OBSERVE how non-
experts use it in the field, and then revise the design
  One good clue is the size of the instruction manual:
  The more instructions there are, the more ways of
  screwing up there are.
One more thing about Enigma, and Fialka as well:  The
rotors were hand-wired and hand-soldered.  This strikes
me as weird, given that printed-circuit-board technology
was available even before WWII.  Millions of boards were
made during the war.  They were used in proximity fuses
for anti-aircraft artillery shells, primarily because they were extremely robust, not to mention lightweight and cheap.
If you miniaturize the rotors you can have more of them,
dramatically increasing the security.  Maybe I'm missing something, but it is hard to imagine any good reason for using hand-wired rotors, not even in the earliest days,
and certainly not later on.  Reportedly Fialka remained in use into the 1990s.
  Of course if you have toooo many rotors you get into
  reliability problems, but still, six rotors straight
  through is in many ways better and in no ways worse
  than three rotors and a reflector.
Cryptology is always interesting because it lives at the
intersection of super-abstract mathematics and super-
down-to-earth engineering.  Some of these rotor machines
suffered from dubious engineering as well as dubious math.

@_date: 2015-01-09 03:56:13
@_author: John Denker 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
Hash: SHA1
That's an excellent suggestion.  Anything that gets rid of the can't-map-to-itself bug is a huuuuge win.
The fact that it doubles the size of the IV and doubles
the number of rounds makes it even better.
IMHO this provides a clear answer to the question that
started this thread:  Using the technology of the time,
yes, one could encrypt in a way that was not breakable at the time.
Actually, superencrypting Enigma with "almost" anything
would have been a huge win.  The stecker board, as implemented, was one of the few things that /didn't/
solve the problem.  A manual Playfair cipher would
have been better.
People blame the reflector, but IMHO would be better to blame the idea of having encrypt mode be identical
to decrypt mode.  The reflector was a consequence of this bad idea, and the excessive symmetry of the
steckering was another consequence.  The steckering
affected the input and output in the same way, so
it was still impossible for any letter to map to itself.
Anything that affected input during encryption and
output during decryption would have been better. For example, the Fialka had a huge Frankenswitch to select encode mode versus decode mode.  This increased
the complexity, but solved the problem of excessive Well, let's see if we can alleviate that problem.
Here's a hypothesis to consider:   With only a very slight increase in complexity, construct an Enigma with /two/ paper tape punches and /two/ readers.
During pass 1.0, the machine punches a copy of the plaintext /and/ the intermediate codetext.
During pass 1.1, the two tapes are moved to the two readers.  The machine decrypts the intermediate codetext
and compares it against the plaintext.  This catches
virtually all machine errors, along with a wide class of operator errors (such as mis-setting the message Before pass 2.0, the message indicators are changed.
During pass 2.0, the machine reads the intermediate codetext, encrypts it, and punches out the final codetext.
During pass 2.1, the final codetext is decoded and
verified against the intermediate codetext.
Now the final codetext is ready to be transmitted.
Note:  The three different tapes use different colors
of paper, to help keep them separate.  You don't want
to accidentally transmit the plaintext.  Possible
refinement:  one could imagine making them physically
incompatible, e.g. different drive-sprocket holes.
This doesn't even add all that much to the workload,
since the added steps are few in number and are highly
automated.  The added steps are well worth it in terms of reliability and cryptologic strength.
I wouldn't want to carry out this procedure in a muddy
foxhole, but on a naval vessel or in a divisional HQ
caravan it shouldn't be a problem.
Obviously I haven't tried this, so it is just a hypothesis
at this point, but it seems worth a try.
At no time during the war was there any shortage of paper tape.
It is straightforward to generalize to N rounds,
using no additional hardware, using time proportional
to N (or better), and tape proportional to N+1.
Even better than chaining N machines all alike is chaining N machines all different.
As a separate matter:  The message indicator was
sent using the key of the day.  It was sent twice,
which improved reliability ... but created a weakness. To alleviate the weakness, send the second copy using
a different key!  Have two separate keys of the day.
The added workload is infinitesimal.
Also, as previously mentioned, roll the dice and choose
a /random/ steckering, which becomes part of the message
indicator.  This is incomparably stronger than leaving the steckering constant for an entire day and constant across all stations.
  Of course this implies adding stecker boards to Navy
  and Abwehr machines (not just Army machines).
Roll the dice and send a bunch of random null letters
at the head of the message.  Keep rolling until you get two letters the same, so it randomizes the length of the message as well.  Ditto at the end.  Sprinkle random null codewords throughout the message.  Take null codewords from a codebook, or make up hamXster your own pupXpy nonsense words kitXten as you go along.
Bottom line:  There was a lot they could have done
with 1930s technology.  Minor tweaks to the procedures
would have made a huge difference.
Ditto for non-military organizations.
Actually it is even worse than that.  There are structural reasons why code-breaking is rewarded more than code-making.
As we have discussed previously, the guy who benefits from
cryptanalysis /knows/ how much he is benefiting.  They guy who benefits from having strong crypto is never quite sure how much good it is doing.
Note that the Manning, Snowden, and Sony incidents are
remarkable for being not nearly as severe as they could
have been, because the leakage was well publicized.  This
is very different from the Enigma/ULTRA situation, where
the break was exploited for years without the broken side
finding out about it.
I mention this because a manifest break is one of the few
things that will get people to stop sucking their thumbs
and start paying for strong security.
Ditto for non-military clients.

@_date: 2015-01-09 13:09:47
@_author: John Denker 
@_subject: [Cryptography] Compression before encryption? 
OK.  Sometimes it helps, sometimes it doesn't.
Let's assume the plaintext /is compressible/. Otherwise there's nothing to discuss.  If the data is already random, or already
well compressed, further compression is obviously not going to help.
It does make frequency analysis much harder, although
that's not necessarily the only way to think about it.
a) That's true only if it is a lousy compression algorithm.
 If the headers are predictable, the headers themselves can
 be compressed.
b) A /small/ amount of predictability is more survivable than
 a large amount.
Sometimes it increases security, and sometimes it doesn't.
 -- If you are breaking RSA by factoring, you derive no
  advantage from a known plaintext.
 -- For a fully-known plaintext, the compressed version is
  also fully known, just smaller.  This may or may not be
  significant to the attacker.  For super-long *or* super-
  short messages it's probably not going to matter.  There   is a Goldilocks zone in between.
 -- At one opposite extreme, if you are breaking Enigma, or   breaking WEP, /partially/ known i.e. partially predictable
  plaintexts are a big deal.  Compression ideally removes   the predictable stuff, making cryptanalysis very much
  harder.
 -- As an intermediate case, compression might create a
  situation where the message was not breakable by itself,
  but breakable if the same message were transmitted to
  multiple stations under different keys.  This exploits
  /same/ plaintext, not "known" plaintext.  Compression
  doesn't change the sameness.  This is why you need
  session keys.
 -- At the other opposite extreme, if chosen plaintext can
  be concatenated to the unknown plaintext, compression
  might make things very much worse:
      Note that this magnifies and highlights the problem with
  traffic analysis, which is an oft-underappreciated problem
  whether or not there is compression involved.

@_date: 2015-01-10 01:57:46
@_author: John Denker 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
That sounds like fiction to me.
Cables don't have any appreciable acoustic signature.
Repeater boxes with relays?  More fiction.
Undersea telegraph cables didn't use repeaters of any kind.  Some used lumped loading coils, but mostly they
used continuous loading.  No acoustical signal either
Undersea telephone cables require repeaters.  The first
such cables were laid in the 1950s.
  a) They didn't use relays, obviously.
  b) They weren't bothered by Nazi U-boats, obviously.
A list of cables, comprehensive or nearly so:
    It shows no undersea repeaters of any kind before 1950.

@_date: 2015-01-16 12:02:27
@_author: John Denker 
@_subject: [Cryptography] coding for compression or secrecy or both or 
People have been doing that for centuries.  Some
codes provide compression or secrecy or both or
A Lempel-Ziv code provides compression but not secrecy.
At one opposite extreme, a code such as Reed-Solomon results in expansion, not compression, but provides for error detection and/or correction.
The Bentley Phrase Code provides compression, but not
secrecy, since the key is widely known.  Back in the
day, clerks could recognize common codewords without looking them up.  The code also provides a tiny bit
of resistance against transmission errors, although it also magnifies some errors.
Encoding a 64-bit float as 22 characters of ascii
decimal results in enormous expansion, but provides
for legibility and portability.
In yet another category, for centuries embassy codebooks
provided both a modicum of compression and a modicum of It is certainly possible to subject the output of such
an encoding to further compression.  Compressing a code
is *not* equivalent to breaking it.
It is also possible to subject the output of such a
code to further encryption.  Enciphered code was the
state of the art for a long, long time.  If Enigma
has been superenciphered by something even modestly
secure. the combination would have been unbreakable.
There is no particular reason to think such a case
will never arise again.
It may be that as of today, every state-of-the-art electronic crypto system produces incompressible output, but there is no deep reason why that has to be so.
It may be that the length of a compressed message leaks
information about the contents ... but so does the length
of an uncompressed message!  Consider for example the
website    This is
one of the few media web sites that offers https.  On
the other hand, the length of each article is known, so
any eavesdropper with an IQ greater than 37 can figure out what article you are reading.  AFAICT in such a situation, https without tor is mostly(*) security theater
... for reasons having nothing to do with compression.
Yes, CRIME makes things /worse/ in some cases, but things are already so bad that it's hard to notice.
The claim that hoovering up "only" the metadata is
somehow not spying is beyond ludicrous.
(*) The "https everywhere" campaign is still a good idea, even if most people derive no benefit from it,
because it creates a useful ecosystem and creates
useful cover traffic that may come in handy in
special circumstances.  Still, though, it's a long
way from doing what needs to be done ... for reasons
having nothing to do with compression.

@_date: 2015-01-18 15:21:29
@_author: John Denker 
@_subject: [Cryptography] information gain; 
That's not the correct criterion.  It would be better to
say the cryptotext does not convey any /information/ about
the plaintext or the keys.  Here's an example:
00010100 10110000 11111110 00110110 10000100 01010111 10001100 10111110
01011111 00101101 10010110 00000101 10100101 00000001 11011001 00111011
01010101 11110111 11100010 11011001 11101000 10100001 00011100 11100010
00001001 10111010 11001101 11111101 10101000 01100000 00011010 11011001
11001010 10110110 10001001 10001100 10001010 10001010 11111101 01101011
00101010 10101111 00110010 01110010 10011101 11001001 10101010 10001000
11110110 10110110 10010101 11000001 11011011 00001100 10101111 10011111
00100101 10011011 01010100 01101010 10000101 11100110 01011110 10001111
That's the beginning of a file that contains my Cayman
Islands bank account number, all of my passwords, et cetera.
However, it has been encrypted using a one-time pad, so I
don't mind publishing the cryptotext.
However, this cryptotext is highly compressible.  You can gain:
 -- a factor of 4 by using hex ascii instead of base-2 ascii.
 -- a factor of 6 using printable ascii chars.
 -- a factor of 5 or so (depending on file size) by using gzip.
 -- a factor of 9 by using raw format, 8 bits per byte.
My point is that -- compressible or not -- the example cryptotext provides zero information gain.  It doesn't tell you anything you didn't already know.(*)
The concept of /information gain/(**) can be formulated very precisely.
Executive summary:  Calculate the entropy of all possible plaintexts
(before seeing the cryptotext) and subtract off the row entropy or the conditional entropy (conditioned on the cryptotext).  The math is spelled out here:
  I say again that the randomness of the cryptotext is absolutely
not a good criterion.  Suppose I send three copies of the example
cryptotext.  The receiver attributes a randomness of one bit per
byte to the first copy.  The receiver attributes no randomness
whatsoever to the second and third copies.  More importantly,
none of that matters.  Information gain is what matters, and
that is zero for all three copies.
(*) We still need to account for traffic analysis.  This a super-important issue, but it affects all cryptosystems, whether compressible or not, so it does not affect the issues discussed (**) The information gain I am talking about here is not quite
the same as the Kullback-Leibler divergence ... although the name "information gain" is sometimes applied to both.  They
are related to each other, and to the conditional entropy, but
they are not quite the same.

@_date: 2015-01-26 14:43:22
@_author: John Denker 
@_subject: [Cryptography] traffic analysis 
Hash: SHA1
OK.... but the same can be said for lots of other
things, as we now discuss.
Every so often the pointy-haired boss asks "Is it secure if we do ...." and the answer is always "No."  It is not necessary to wait for the end of the question.
Still not secure.  If you want to argue that it is
somewhat less insecure, the devil is in the details.
Suppose a typical HTTP session lasts half a minute.
There are 1440 minutes a day.  Other things being
equal, remailing degrades the adversary's signal-
to-noise ratio by a factor of less than 1^12.  If you think the number of people using the anonymous remailer network that day is less than the number of people using tor, the gain is even less.
With modern ciphers we are accustomed to seeing
work factors on the order of 2^81.  At that point
I reckon the adversaries are not going to bother with a direct attack.  Instead they will use other
tricks, such as traffic analysis.  A work factor
of 2^11 is not enough to stop the attack.  Mostly
you're just calling attention to yourself.
This is not fixed by using tor.  It is also not fixed by using HTTP GET instead of HTTP POST.
It is not fixed by using remailers.  Fixing this
is really, really hard ... but it has to be done.
The only defense I know of against traffic analysis
is cover traffic, and lots of it.  Suppose every
hour on the hour I transmit a 1000-byte encrypted
message.  Most of them say "Wait."  Once in a very long while, one of them says "Barrage at 04:00,
paratroops land at 05:00, amphibious landings at 06:00, capture the bridge, hold until relieved."
Without a rigorous regimen of cover traffic, the special message would stick out like a sore thumb.
Things like remailers and tor rely on hiding a
tree in a forest.  The works best if the trees
are indistinguishable.  Otherwise it becomes (at most) a statistics problem.  The NSA is reeeally
good at statistics.  Quoting Ed Snowden:  Assume your adversary is capable of a trillion guesses per second.

@_date: 2015-01-26 12:47:25
@_author: John Denker 
@_subject: [Cryptography] entropy +- /dev/random 
1) The following are both true, and both irrelevant:
 -- Entropy is not a "base" for computing.
 -- Computing is not a "base" for entropy, as von
  Neumann famously pointed out.
2) Entropy *is* required for practical security, for
the following reasons:
That's because any PRNG must be initialized.  If you
initialize it from some other PRNG, you have just
reduced it to the problem previously *not* solved.
The Cat in the Hat Comes Back.  To avoid infinite regress, at some point in the chain you need Little Cat Z to provide some actual entropy.
The /density/ of entropy in the output of the PRNG
might be exceedingly low, but the entropy must not be zero.
3) As an almost-separate issue, the fact that in terms of entropy does not change fact (2) above.
It is axiomatic that no matter what you are doing,
you can always do it wrong.  This however does not
mean that it is impossible to do things right.  Any tool can be abused.  You are not however obliged
to abuse it.
"Touching" is not the right concept.  The relevant operations are properly called /copying/ and /erasing/.
The second law of thermodynamics -- the law about entropy -- imposes strict limits on the erasure
process.  If you have N copies of a bit, you can
erase N-1 of them for free.  To erase the last
copy, you have to get rid of the entropy somehow,
and that's going to cost you.  These are serious limitations, imposed by the laws
of physics, whether you like it or not, whether you
understand it or not.  If you build a mathematical
theory of computing that does not include these
phenomena, it does not correctly model the real
If you copy a bit, it could be argued that one or
the other of them has entropy, but not both.  This
is sometimes (albeit not always) a useful way of thinking about it.  This can be quantified in terms
of information gain and divergence.
There is an extensive literature on this subject,
featuring contributions from von Neumann, Shannon, Kullback, Leibler, Landauer, Bennett, and others.

@_date: 2015-01-29 12:35:19
@_author: John Denker 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
1) Owning the physical link is unnecessary and irrelevant.
 In the question, let's replace the concept of physical
 "link" with /virtual circuit/.  Assume the carrier who
 owns the physical link is untrustworthy.  You can still
 buy a virtual circuit with X amount of bandwidth for a  finite price.  You can fill that bandwidth with traffic  -- cover traffic or otherwise -- and secure the traffic
 using crypto.
2) It may help to distinguish "traffic analysis" from  "traffic engineering".  Imagine a cat-and-mouse game,
 where the mice are trying to communicate, and the cats
 are attacking by means of traffic analysis.  The mice
 need to do traffic engineering, so as to maximize the
 amount of traffic they can move, maximize the security,
 and minimize the cost.
 Traffic engineering starts from the observation that
 peak demand is eeeenormously larger than average demand.
 Therefore there is /traffic gain/ to be had by sharing.
 Under "some" favorable conditions, this can be done
 while still defending against traffic analysis.
 Here is a scenario.  This does not solve all the world's
 problems, but it has some value.  If nothing else, it
 proves we should not give up.
 Suppose there exist cheap links and expensive links,
 e.g. local versus transatlantic.  Suppose you have
 a group of N friends whom you trust.  The group is
 connected by cheap links.  All traffic is secured
 by crypto.  Each member of the group puts X amount
 of steady traffic on the expensive link, for a total  of N*X.  When you have a burst of traffic that
 exceeds X, you distribute it (cheaply!) to your
 friends, and ask them to forward it.
        _______
       |
   A   |                _________
_______|---------------|         |
          +   +           -----| carrier |---(ocean)---
          +   +_______   |   --|_________|
          +   |       |  |  |
          +   |   B   |--   |
          +   |_______|     |
          +    +            |
        __+____+            |
       |            |
   C   |------------
  "++" = cheap link
  "--" = expensive link
 In this scenario, you can achieve traffic gain in
 proportion to the number of people you can trust.
 Increasing N increases your peak bandwidth, at the
 cost of a proportional increase in attack surface.
 One must also analyze this from the cats' point of
 view.  Assume the cats' goal is to spy on /everybody/.
 Suppose M of the N sites can be compromised, meaning
 that the cat can now tell the difference between null
 and non-null traffic at that site.  It then becomes  a statistic problem to figure out where the non-null
 traffic originated.  As a function of M:
  -- how good are the statistics?
  -- what is the direct cost?
  -- what are the indirect costs, such as the chance
   that some mouse will notice that he's been pwned?
This is obviously just the sketch of an outline, but
it shows there are some things that can be done,
without requiring us to redesign all protocols from
layer 1 upward.

@_date: 2015-07-18 09:39:40
@_author: John Denker 
@_subject: [Cryptography] Hypothetical WWII cipher machine. 
Hash: SHA1
Do you want good fiction, or good cryptography?  The
two are not entirely aligned.    a) For the story to work, most authors would want    the MacGuffin to be revolutionary and weird.
  b) For good practical cryptography, you probably want
   a machine that is an incremental improvement over
   the existing rotor machines.
If I were writing the story, I would defy the first
assumption and lampshade it:  The most remarkable
thing about my Mark-II Plot Device is that there is
nothing very remarkable about it.  It's a bundle of
relatively prosaic refinements that work well together.
Practicality is relevant here.  Cryptography exists at the intersection of esoteric mathematics and down-to-earth engineering.  The mechanical details
of the proposed Plot Device are unexplained, and seem implausible.  Unless you are going to send Ern Rubik back in time to design the thing, it seems unlikely that the mechanics would operate
Slowly-changing state is "one" of the problems, but nowhere near the worst of the problems.
Yes, related-key attacks are a Big Deal.  Reference: excellent post by Ray Dillinger on 01/13/2015 02:28 PM.
I don't see that as the essence of the problem.  The
Plot Device has a tremendously long cycle time ... but cycle time isn't the problem that most needed solving.  If the messages aren't hugely long, a modest cycle time suffices.  Conversely, it doesn't
matter how long the cycle time is, if the IVs span
only a small part of the state-space.  A longer and more secure IV is much closer to the heart of the matter.  That's a major contribution to the size of the search space the cryptanalyst must deal with.
To be specific:  Here's my Mark-II Plot Device:  Let's
drop requirement (B).  It's still a rotor machine, just a much-improved rotor machine.  My objectives are:
 *) The machine is mechanically reliable.
 *) The machine is electrically reliable.
 *) The machine is readily manufacturable using
  contemporary technology.
 *) The operating procedures don't place undue burdens
  on the ordinary code clerk.
 *) The hardware and procedures are cryptologically sound.
  1) Use 6 rotors straight-through (as in the M-209).
   That's better than using 3 rotors and a reflector.
   For starters, it gives a larger key-space.  Also it
   gets rid of the reciprocity bug.  If you want to get
   fancy, use 8 rotors.
  2) Choose those rotors from a set of 20. That's much
   better than choosing 3 from a set of 5, or even 4    from a set of 8.
  3) Make frequent changes to the Ringstellungen.
  4) Have a more sophisticated way of deciding which
   wheels advance (again, as in the M-209).  In my    Mark-II Plot Device, the wheels are advanced by
   solenoids (not by a motor).  Steppers of this
   kind were well-developed technology at the time,
   used in e.g. telephone switchgear.  Advancement
   is controlled by a some wheels in the "back    stage" area (taking the place of the drum+lugs    of the M-209).  Advancement depends in part on
   the plaintext and ciphertext, so there is an
   element of cipher chaining.  Each wheel has    roughly a 50% chance of advancement per character.
  5) Implement the permutation circuits (i.e. the
   inner wheels) using printed circuit boards.
   This was well-developed technology at the time.
   Millions of PCBs were made for e.g. proximity    fuses for anti-aircraft shells.  This means the
   wheels are cheap, lightweight, thin, strong, and
   reliable. This makes it feasible to ship 20 or
   more inner wheels with each machine.  The PCBs
   snap into the rings.
  6) Provide a set of dice, and require operators
   on pain of death to use the dice to select the
   message indicator (which nowadays would be called
   the initialization vector) ... and also the nulls.
  7) At the beginning of each message, and the clerk
   MUST insert a random number of random null words.
   Among other things, this means that no two cipher
   texts have the same length, even if a message
   has to be re-sent.
  8) There is a strict limit on the size of each message.
   If the payload is longer, break it into multiple
   messages, each with its own IV.  Also note that
   shorter messages are better in case retransmission
   becomes necessary.
  9) When generating the IV, whenever one character
   is needed, use the dice to generate a /pair/ of
   characters.  The IV as transmitted contains the
   pairs (duly encrypted), but when the IV is used
   for initializing the machine, each pair is collapsed
   to a single character using a lookup table.  Here
   is what the lookup table might look like for a    five-letter alphabet;  you can readily generalize
   it to the full alphabet:
           A  B  C  D  E
       A:  e  b  b  d  d
       B:  a  d  e  c  e
       C:  a  a  d  c  a
       D:  b  c  c  d  e
       E:  c  a  b  b  e
   That is, the bigraph (row,column) = (A, E) maps to
   the single character "d" in the upper-right corner
   of the table.  This makes it very much harder for
   the cryptanalyst to recognize related keys.  (This
   is an elaboration of the way the Naval enigma IVs    were handled.)
 10) The IV is encrypted /once/ using the key of the    day.  Then for redundancy, the encrypted version is
   transmitted twice.  The *exact same* ciphertext is    transmitted twice.  Let's be clear: the IV is not    encrypted twice;  it is encrypted only once and    transmitted twice.       (Otherwise the first encryption would serve as
      depth for cracking the second.)
 11) Test messages MUST consist solely of nulls;  not
  German proverbs;  not dirty jokes;  not 200 copies
  of the letter "L".  Validity is ensured by re-using
  the first null as the last;  due to chaining, this
  won't come through correctly unless all the rest
  is correct.
Although the Allies could afford to build Bletchley Park,
they could not afford to build a million Bletchley Parks,
or even a thousand.
Expanding the key space by a factor of a million, and
hardening the IVs by even more than that, would make
the rotor machine secure for the duration of the war,
and for several years more than that.
This becomes a spy thriller as follows:  A couple of
Abwehr cryptanalysts are given a captured M-209 and
discover that it is (just barely) breakable.  They
put 2 and 2 together and figure that the Enigma must
be far more breakable.  They come up with a proposal
for a much stronger machine.
The Abwehr is thoroughly penetrated by people who
hate Hitler.  They want to suppress the Plot Device
proposal.  They can't just kill the analysts, because
that would just attract attention.  They have to
discredit them first........

@_date: 2015-07-23 13:22:53
@_author: John Denker 
@_subject: [Cryptography] Whitening Algorithm 
This is the proverbial elephant.  It can be approached
from a number of different directions.  It's a lot easier to grasp part of the problem than to understand
the whole thing.
Here's one leg of the elephant.  IF (big IF) you look
at it as a purely mathematical cryptography question,
a whitener is virtually indistinguishable from a hash
function.  So the basic question becomes finding a
cryptologically strong hash function that doesn't
burden the CPU.
The best you can hope to achieve by such a process
is a PRNG.  Given a hash function, using it as the
basis for a decent PRNG is non-obvious.  I recommend
reading something like
   Elaine Barker and John Kelsey,
   NIST Special Publication:    Recommendation for Random Number Generation Using Deterministic Random Bit Generators
      There's a lot of complexity there, and at first glance
you might think you can come up with something simpler
... but there are very specific attacks that those
complicated features are meant to defend against.  So
it quickly comes down to the old question:  WYTM?
What's your threat model?  If you're defending against
only one particular narrow threat, it's easy to come
up with something that works.  OTOH if you're trying
to defend against an advanced persistent threat, you need to do a lot of detailed analysis.  Even if you
stick to using tried-and-true crypto primitives, the
way in which you combine the primitives matters a
great deal.
Here's a completely different way of approaching the
elephant.  It's impossible in principle to design a
decent RNG unless you know a lot about the hardware
and about the underlying physics.  This includes the
physics of normal operation, the physics of plausible
natural degradation, and the physics of possible
attacks.  So for sake of argument, let's suppose you
know absolutely that the transistor puts out 1/f noise.  Then you can run that through a digital filter and get out nice white noise ... no crypto required.  The required filter requires very little
Let's be clear:  There are approaches that rely on
crypto with almost no physics ... and approaches
that rely on physics with almost no crypto.  I
tend to prefer designs that get the physics right
On 07/23/2015 07:07 AM, dj at deadhat.com made a number
of good points.
Well, yes, no, and maybe.  It depends on what you
mean by entropy ... and it depends on your threat
model.  There are plenty of situations where the
honest-to-goodness physics entropy is not what you
want.  You might need something more along the lines of the H_ Rnyi functional.  Some folks
consider that some kind "generalized" entropy but I'm not at all convinced that's a good idea.  Most people who throw around the word "entropy" have no idea what it means.
Calibrating a piece of hardware to ascertain how
much entropy (or whatever) it produces is a lot of work.
Last but not least, what's really going on here?
What's the motivation?  What's the objective?  TRNG
or CSPRNG or ....?   What are the real constraints? What's the threat model?  Unless you have some very specialized high-volume low-cost application in mind,
there are other solutions that cost less and work better (compared to custom hardware).

@_date: 2015-06-04 15:00:41
@_author: John Denker 
@_subject: [Cryptography] DEA bulk spying, FBI Spy Planes, IMEI Catchers, 
This is an important subject.  We need to discus
this more than we have.  People talk endlessly about
the insecurity of basic internet protocols ... but
the /telephone/ network is in some ways even less secure, and in some ways scarier in terms of its impact on personal safety and privacy.
That's nice as far as it goes, but I would not
put all of my eggs in that basket, for multiple
 a) The bill might get watered down before passage in
  the House.
 b) It might not get passed at all in the House.
 c) It will certainly face diehard opposition in the
  Senate.  (Without the patriot-act sunset, nothing
  remotely resembling this week's "reforms" would
  have made it through the Senate.)
 d) It would most likely get vetoed.
 e) The NSA, DEA, FBI, etc. have a loooong track   record of doing things that are expressly illegal.
  Even if they were legal they would be unconstitutional,
  and even if they were constitutional they would be
  bad policy ... but none of that stops them.
 f) There are lots and lots of phones outside the US,
  where US law does not apply.
I suggest that it is useful, and in keeping with the traditions of this list, to look for technical
  --> We need legislation also, if only to make sure
   that proper technical solutions aren't outlawed.
   When I argue against one extreme it does NOT mean
   I am in favor of the opposite extreme.  As a good
   rule of thumb, all the extremes are wrong.
As a small step in the technical direction, let me
point out that there exist IMSI-catcher-catchers, e.g.
    I don't know much about either of those examples.  Do we
know anybody who has taken a serious look?  In particular,
snoopsnitch has both a front-end client and a back-end server.  The client code is open-source, which is nice ... but I see no way to rule out the possibility that
the server is a wholly-pwned subsidiary of the BND (or The SecUpwN site has a wiki with links to some videos
on the subject:
  Alas, their code seems to be in a pre-alpha state right now.  Here are some desiderata and topics for further discussion:
 -- IWBNI we had a defensive system that was local and   self-contained, not dependent on communication with   any back-end server.
 -- OTOH it would be OK to make use of databases.  In
  particular, the latitude and longitude of all legitimate
  FCC-licensed cell towers are well known,
      so anything that pops up and/or moves will stick out like
  a sore thumb.
 -- IWBNI there was an optional, private, and secure   way to share statistics with somebody we trust, to get
  an idea of how many stingrays are swimming around in   the wild.
 -- IWBNI the detector was well integrated with the rest
  of the phone OS.
 -- In particular, IWBNI there was a mode that allowed for
  /passive/ monitoring, i.e. with the phone's RF Tx section
  shut down.  This would result in the catcher-catcher   knowing more than the would-be catcher.
 -- For that matter, IWBNI the underlying firmware had
  some semblance of security.  There are more than a few   "alternative" phone firmware packages:
     -- Keep in mind that stingrays are nowhere near the only
  threat out there;  SS7 is full of security holes.
    It seems to me this gives us what we euphemistically call
"an opportunity for improvement".  And (non-euphemistically)
there seems to be a little bit of recent motion in the right direction.

@_date: 2015-06-08 17:07:34
@_author: John Denker 
@_subject: [Cryptography] reality +- mathematical guarantees 
In the real world, there are no "hard" guarantees, not
for any form of error correction ... nor for anything else.
Specifically:  There are lots of ways in which a channel can fail that will not reliably be detected using CRC32.
Sure, one can create /mathematical models/ for which such-and-such method provides a hard guarantee ... but
more generally we would do well to remember the dictum:
   "Insofar as the propositions of mathematics refer to reality,
    they are not certain;     and insofar they are certain,
    they do not refer to reality.

@_date: 2015-06-13 14:18:36
@_author: John Denker 
@_subject: [Cryptography] let's do something intelligent about md5sum! 
Well, fixing "first things first" is nice, but
sometimes the first thing we see is a tree that obscures our view of the forest.
The fact is, introducing b2sum is not sufficient
to "kill md5sum".  It might not even be a step
in the right direction.
This gets back to our previous discussion of
life-cycle strategy:  Even a neophyte designer
realizes his pet project needs an introduction
  In contrast it takes more sophistication to
  appreciate the importance of the corresponding
  outroduction strategy later in the cycle.
We have a problem because the md5sum command
never had an outroduction strategy.  Introducing
a new b2sum command will not solve this problem,
and indeed could easily make it worse, insofar
as b2sum doesn't have an outroduction strategy
This is due partly to the nature of the commands
themselves, and partly to the way they are used
within larger systems ... but the point remains,
we need to take a step back and look at the whole
forest.  We need to /solve the whole problem/.
As numerous people have pointed out, it is not
practical to "kill md5sum", and will not be practical anytime soon.  A simplified view of
the life cycle should look something like this:
   first-generation thing
   transition / overlap phase
   second-generation thing
   transition / overlap phase
   third-generation thing
   et cetera.
There has been some talk of "agility".  That's
too vague to be useful.
  -- Agility is bad if it leads to promiscuity.
   It leads to downgrade attacks, i.e. the worst
   of all worlds.
  ++ Agility is good if it allows us to construct
   a well-behaved upgrade path.
Therefore I suggest that rather than focusing on
b2sum, we should be thinking in terms of a
   chksum
package that takes a grown-up approach to the life-cycle issue:
  -- It should know what to do with a legacy md5sum.
  -- It should know how to generate something better
   if-and-when possible.
  -- It should function correctly during the
   transition periods between generations.
  -- That includes recognizing that it, too, will    have to be phased out at some point!
  -- It may have to be a package of things, not
   a just a single program.
True story about quick-and-dirty solutions:  The first
program I ever wrote was a solution to a very specific problem.  It was written between dinner and midnight one evening.  It wasn't designed;  it was hatched.
The funny thing was, the program was still in heavy
use several years later.  People complained that it
had some nasty limitations.
  jsd:        If you don't like it, why don't you improve it?
  other guy:  It's too badly designed, and there's not a
              a single comment in the whole source file.
  jsd:        So why don't you write something better,
              starting from scratch?
  other guy:  Not worth the trouble.  The thing sorta
              works well enough at the moment.....
The 500th time I had this conversation, I got really
tired of it.  I learned my lesson.  The cost of a program
has to include the cost of the whole life-cycle, including
support, evolution, and outroduction.  There's always a lot of pressure to get "something" out the door, but sometimes that leads to disastrously false economies.
As a tiny but specific example of the sort of thing I'm
talking about, consider a manifest containing filenames
and checksums.  I say the manifest ought to contain some
sort of version marker that indicates which checksum
algorithm was used (md5sum or whatever).  The counter-
argument is that the file is shorter and easier to parse
if it does not contain the version marker ... but still
I insist that omitting the marker is a false economy,
because it makes it harder to blaze a migration path
later in the life cycle.
  "Premature optimization is the root of all evil."
                -- Knuth
  "Solve the /whole problem/."
  "That includes support, evolution, and outroduction."

@_date: 2015-06-16 19:37:10
@_author: John Denker 
@_subject: [Cryptography] password fatigue;  was: Lastpass 
There are several ideas there.  My comments:
1) You have to secure your own system FIRST.  To say
   the same thing the other way:  If you enter your    password via a platform that has been pwned, then ....
  -- It doesn't matter how good your master pw is.
  -- Also it doesn't matter whether or not you use    lastpass or anything like that, and it doesn't
   matter whether you consider lastpass to be better
   than nothing or worse than nothing.
  -- Also it doesn't matter whether you use zero-
   knowledge authentication or anything like that.
2) Password fatigue is a problem.  We need to focus
 on this.  Lastpass gets "some" brownie points for
 attempting to solve the problem, even if you think  their attempt is less than 100% elegant and/or
 less than 100% successful.
3a) Snickering at lastpass does not solve the problem,
 not even a little bit.
3b) Telling users to solve the problem on their own
 does not solve the problem.
3c) The only way I can see to solve the password fatigue
 problem is to get web services to stop asking for a
 per-site password and instead use some sort of zero-
 knowledge authentication.  Schemes for doing this have
 been known for a long time.
   3d) If anybody knows of a better solution, please let  us know.
4) Yes, securing your system is seriously hard.  Of
 course that includes securing the subsystem that
 handles your zero-knowledge authentication.
5) This supports my contention that the NSA is not very
 good at their job.  Note that Information Assurance is  part of their mission, if you believe their own website:
    If they had any sense, they would roll out some sort  of zero-knowledge authentication scheme and require  government sites to use it, e.g. when accessing the  security-clearance background info database, just to  pick a random example.
6) Similar words apply to google.  If they had any sense,
 they would embed some sort of zero-knowledge thing into
 android and chrome, and use it when connecting to
 *.google.com.
a) One thing that Snowden made clear is that essentially
 everybody on earth is a target.  One big hardened target
 is not necessarily more problematic than millions of small,
 soft, juicy targets.
b) If the intent was to say "Don't put all your eggs in
 one basket" I'm not sure that's uniformly good advice.   Mommy birds generally do put all their eggs in one nest.
 Then they go to some trouble to defend that nest.  The
 one-basket strategy is not always right, but it's not
 always wrong, and should not be dismissed out of hand.
 Thoughtful analysis is required.
Those are specific constructive suggestions.  Always
KeePass is clearly better than nothing.  It reduces the
attack surface, and /somewhat/ lessens the cost of a
breach.  As I see it, from the user-interface point of
view, getting the users to employ a password manager is no easier than getting them to use a zero-knowledge
authentication agent ... so why not go all the way?
  FWIW personally I use a password manager ... but
  I consider it little more than a finger in the
  dike.  It's not the Right Thing.
This gets back to the previous discussion of "BCP".
We need to get out the message that transmitting
passwords in any form is not best current practice;
it's not even Baseline Competent and Prudent.
Unless I'm missing something, syncthing and LAFS do
not claim to solve the general password-fatigue problem.

@_date: 2015-03-02 13:09:57
@_author: John Denker 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
The "cheap forensic recorder" thread is far more
important than the subject line would suggest.
Let's not sell ourselves short.  About 98% of what has been suggested so far applies just fine a wide range of systems, forensic and otherwise, cheap and otherwise.
The general question is, what can we do to facilitate
verifying the integrity of the system -- hardware,
firmware, software, etc. -- even in situations that
are somewhat adversarial.  Litigation and forensics
are familiar adversarial situations, but certainly not the only ones.
Here's another example dear to my heart:  Vote-counting
equipment.  Consider a setup where each voter goes to
the polling place, marks a paper ballot, and feeds it into a scanner right there at the polling place.  At the end of the day, the scanner prints a tape with the tally for that polling place.  Then the tape, and
a duplicate tape, and the original ballots are sent
downtown.  I'm leaving out a lot of details, but if
done right, this setup is vastly more secure than
an all-paper scheme or an all-electronic scheme.
This is an unabashedly adversarial situation ... and
existing practices use that to their advantage.
They don't even try to find nonpartisan poll workers; instead they rely on bipartisan teams of poll workers.
Ideally each critical step gets vetted and signed off
by somebody from each major party.
Validating the scanner system is an important part
of the story.  Many of the tactics suggested in the
"forensics" thread apply here also.
Ken Thompson wasn't wrong; he was only a few years
ahead of his time.
  The central message is that people can mess with your supply chain.  To some people, that sounded like
tin-foil-hattery in 1984, but nowadays there is tons
of evidence that people *are* messing with your
supply chain.
 -- NSA TAO intercepting and "tailoring" your hardware,
  as documented by Citizen Snowden.
 -- Trojans in the disk firwmare, observed in the wild.
 -- Multiple examples of people tampering with voting
  machines:   -- Superfish tampering with the TLS root of trust.
The Superfish example suggests that the amount of money you need to offer a hardware vendor to get them
to betray their customers is very small.  More than
30 pieces of silver, but not much more.
It is better to talk about /practical/ verifiable
systems, not just "cheap".  Cheapness is a relative
thing.  I have no doubt that seat belts and airbags
raise the price of a car.  They don't solve all the
world's problems, but they solve some problems and
mitigate others.  So, I don't want the cheapest
possible car;  I am willing to pay a modest premium
for safety and security.
Again, cheapness is a relative thing.  The threats
against the chain of evidence are different in the
case of a $13.00 shoplifting case and a $13 billion
oil-spill case.
Election security is a big deal.  In the 2012 election,
each party spent more than a billion dollars.  It is
common for candidates to spend more than $100.00 per vote cast ... even on down-ballot races.  Given that a number of races were close, the dollar value of flipping a few votes is just enormous ... far exceeding the cost of the vote-counting machinery.  So anybody in their right mind would happily pay a substantial premium for verifiable hardware, firmware, and software.
IMHO the zeroth order of business is securing the hardware.  One of my favorite sayings is
    If you don't have physical security,
    you don't have security.
For something like a forensic machine or a vote-
scanning machine, this starts with tamper-resistant
and tamper-evident enclosures.  This is more of a
challenge than you might think.  Seals don't do it.
At Argonne National Lab, the group that worries
about security of nuclear materials ran some tests
that showed the mean time to plan an attack to
bypass a security seal was 45 minutes, and the
mean time to carry out the attack was 5 minutes.
And that's if the security seals are properly used.
I've seen election materials "sealed" in such a
way that it was easier to bypass the seal than
not.  I returned to the downtown office some used materials, using the same /sealed/ containers that the original materials came in, with the same seals.  Nobody noticed.  I called their
attention to it.  Nobody cared.
This also calls for storing stuff in vaults with
guards and video surveillance.
There may be a role for cut-and-choose protocols
here.  If you need N machines, buy N+M machines,
and tear down some of them.  Let the adversarial
parties decide which ones they want to tear down.
This creates some nonzero probability that tampering
will be detected.  If you couple that with strict
penalties for getting caught (a bit of wishful thinking), then you might accomplish something IMHO the next step is securing the BIOS.  Again
the logic is simple:  If you can't trust the BIOS,
you can't trust anything else.  Conversely, a
trusted BIOS can vet the other components.  For
starters, it can demand a valid cryptologic signature
for BIOS updates.  Similarly it can demand a valid
crypto sig on the software it reads from disk at
boot time.  These things can be signed multiple
times, once by each of the interested parties.
This would make life noticeably more difficult for
anybody who wants to bugger the firmware in your disk drives.
Open-source auditable BIOS implementations exist.
    Some HP models already demand a signature for BIOS
updates ... to the annoyance of modders.
Software RAID may help here.  It makes it much
harder for the disk firmware to know what the bits
VM technology helps a lot here.  Each new VM guest
starts with a fresh copy of the software, from a
read-only file on the host.  It's not hard to break
the MS Windows software running on the guest, but
it's a lot harder to break the VM model in such a
way that the guest can have /any/ lasting effect
on the host.  The guest can't get anywhere near the host BIOS, disk drivers, or anything like that.
This doesn't solve all the world's problems, but it does dramatically reduce the attack surface ... and adds a layer of hardnessto the remaining surface.
As for printers, that's actually a harder problem,
but there are things we can do.  Suppose the mfgr
doesn't want to release the firmware code.  OK,
it's proprietary.  Even so, they should publishe
the HMAC of the firmware.  There should be a printer
command to return the HMAC of the installed firmware,
for comparison to the published value.  And then if I tear down the machine and hash the firmware, I should get the same answer, without relying on the firmware to fink on itself.
Also a teardown "might" detect the secret wifi
interface that I didn't pay for.
Last but not least, there are plennnty of ways of
detecting radio transmissions.  The hard part is
distinguishing unauthorized from authorized
transmissions.  Still, if a printer that's not supposed to have any wireless at all starts
transmitting, that is detectable.
See previous discussion of encrypted point-to-point
connections, basically virtual circuits.  See also
previous discussion about stolen MAC addresses.
Hugh Daniel always said we should never rely on
firewalls.  You can have one if you want, but think
of it like velvet ropes for a waiting line:  It is
a guideline for people who are trying to cooperate,
but it is no barrier at all against any determined
attacker.  We need point-to-point security.  For
a multiuser machine, host-to-host is not even good
enough;  it needs to be process-to-process.
Bottom line:  Stuff that passed as "best current
practices" a couple of years ago must be considered
security malpractice today.  We seriously need to
raise our game, not just for specialized forensic
systems, but for basically everything.

@_date: 2015-03-20 17:39:15
@_author: John Denker 
@_subject: [Cryptography] forward secrecy 
Hash: SHA1
In the context of Forward Secrecy:
+1 to that.  Lots of people say it that way:  Forward Secrecy.
Occasionally, Forward Secrecy must be distinguished from something
similar, such as LAaFS i.e. Lame Attempt at Forward Secrecy ...
in which case qualifiers should be placed on the other thing, not on Forward Secrecy itself.
An example of LAaFS is the way the Germans used Enigma.
There was, in effect, an ephemeral session key.  It was
the right general idea;  it just wasn't long enough or
random enough.

@_date: 2015-03-24 11:20:01
@_author: John Denker 
@_subject: [Cryptography] certification by N=0,1,2... authorities 
Hash: SHA1
In the context of "TB2F CAs as (un)official browser policy",
There are about five ways of interpreting that question;
see below for some interpretations and some partial answers.
I don't think the odds are remote at all.  In the recent
"superfish" fiasco, a bogus CA was inserted into the list
of "trusted" CAs.  How much extra work do you think it would have been to insert two (or more) bogus CAs?
More is not always better;  I am reminded of the guy who
thought quadruple-rot13 would be better than triple-DES.
  OTOH an N=2 authority scheme, if done right, could   have some value.  The devil is in the details.
Returning to the first question:  Broadly speaking, such
things have been designed, and even deployed.  The PGP web-of-trust (WoT) is a familiar example.
More narrowly speaking, given the context I assume the question intended to focus on browsers in particular
or x.509 in general.  Even in that case, the mechanism
for such a thing has been "designed";  see the discussion
of "mesh structures" at
  However, no browser implements such a thing, and before
implementing it one would need to come up with a policy
(not just a mechanism) ... and it's not clear what a
sensible policy would be.  We need to have a discussion
of What's Your Threat Model and conversely What's Your Use Case.
Here's one use case that pops to mind:  Suppose that
country X has been at war with country Y for the last
50 years or so.  Such a thing is not particularly hard
to imagine.  Now suppose that the root of trust in the
major browsers includes a lot of CAs that are domiciled
in or otherwise dependent on country Y, and that many
sites across the world depend on such CAs.  This puts
X in a bad situation;  they can't trust those CAs, but
they also can't just throw them out, if they want to get any work done.
It would be nice to have some means whereby country X
could double-check the work of various foreign CAs.
The idea is simple enough, but we are a long way from being able to do this.  Mozilla, for instance, doesn't
even have a way to revoke a certificate that is in
its root-of-trust list!  And they have no intention
of fixing it.  They say this "doesn't make sense" ... even in connection with an example (i.e. superfish)
where it would make a great deal of sense.  Amazing.
  I'll tell you what doesn't make sense:  Having more
than 150 "roots" doesn't make sense.  It defies the
definition of "root", in botany as well as in computer
science.  A browser should have *one* root, representing
the Lord High Certzilla.  If it wants to trust some
other CAs, it should /sign/ their certificates. (Such a signature would of course be revocable.)  This is the logical x.509 way to represent what is actually going on.
  The fact that the Mozilla Foundation pays Digicert
  Inc. to certify the mozilla.org domain is comical,
  given that for practical purposes most people rely   on firefox tell them whether or not to trust Digicert.
  Similar words apply to other browsers.
Continuing the discussion of N-way certification, we
have just consider the possibility of N greater than
one, but we should also consider the case of N less
than one.
A familiar example of this is ssh.  Although it knows
about authorities, most people don't use it that way.
Instead they rely on a ToFU model:  Trust on First Use.
A better word for this is pinning, since really it
signifies /same/ as first use, whereas the degree of "trust" is another matter entirely.
Here's a use-case where N=0 authorities makes sense:
Suppose Joe User is dealing with a wireless base station. It needs to be configured using a web interface.  How
do we secure that?  x.509 was designed with e-commerce
in mind, but the base station doesn't fit that model
at all.  It doesn't come with a domain name, nor should it.  Joe may not give it a name at all, and even if he did, the name is not certified.  Obtaining and installing a certificate is orders of magnitude more work than Joe will do, or should do.
In contrast, the SSH pinning model works reasonably well
in this case.  Joe resets the device to some standard
state, and then connects to it on a secure air-gapped
wired network.  He makes an ssh connection.  He can
trust this (for the appropriately limited purposes) on first use, because of the air gap.  From then on,
he can ssh to the thing in confidence.  The ssh key does not certify the domain name or even the IP address;
it merely signifies that the box is the same box as
last time.
So now we see what the real issue is:  What is the
signature?  This issue affects PGP, x.509, ssh, and
all the rest.  When I sign John Smith's PGP key, what
does that mean?  Does it mean the guy's name really
is John Smith?  Does it mean I would trust the guy
with ten dollars?  Does it mean I would trust the
guy with ten thousand dollars?  Very often I would
like to signify nothing other than the fact that this
is the same John Smith that I've dealt with before,
i.e. pinning.
  In the real world, there are ways of figuring out
  what a signature means.  Signing the front of a
  check means one thing.  Signing the back means
  another.  Signing a petition to recall the governor
  means yet another.  Et cetera.
For configuring the wireless base station, pinning
alone is just fine.  For e-commerce, we really ought
to be using two factors:  authority plus pinning.
Two factors all different works better than two
factors all alike.  Firefox claims to do pinning, but if you look into it you discover they only pin a handful of sites.  There exist add-ins such as
  but I find it hard to take that one seriously;  -- At the height of irony, they don't bother to   secure their own site with https (with or without   pinning).  Evidently they don't really think
  https is important.
 -- They don't bother to sign commits to their   git repo.  -- If you clone the code in their repo, the "make"   command fails.
 -- No work has been done on the project in 3+ years.
There's also another form of pinning, for sites that
want to be pinned:
  but I want to pin the other 99.999% of the sites.
Does anybody know of a pinning add-in that they
trust to do something useful?  Perhaps some smart person on this list could write something using certpatrol as a starting point or otherwise.  (I'm busy at the moment.)
  For completeness, we should note that the same
  guys who subverted the root-of-trust in the
  superfish case could almost as easily have
  defeated any pre-installed pinning mechanism.
  As Ken Thompson pointed out, once people start
  messing with your supply chain, you're screwed,
  and it is hard to get unscrewed.
Still, I do not understand why browsers make it so hard to use authority+pinning ... or pinning alone (as in the case of Joe's wireless base station).
There is a big gap between algorithms and implementations.
I reckon we have all the algorithms we need to make
stuff secure if we wanted ... but sometimes I think
we're not even trying.  The spies and the gangsters
of the world must be laughing themselves silly:  More
than 150 "roots" with no way to revoke them, and not
the slightest interest in fixing it?  Bwa-ha-ha-ha-ha.
Talk-talk-talk about cover traffic as a defense against
traffic analysis, talk about it for years and years but don't do anything about it.  Bwa-ha-ha-ha-ha.
Still no serious pinning.  Bwa-ha-ha-ha-ha.
Also, if you need help meeting your recommended daily allowance of ridiculousness:  The cobblers have holes
in their shoes:
  --  is still offering export-grade ciphers
  -- csail.mit.edu is still offering SSLv3
  -- cybersecurity.csail.mit.edu doesn't offer HTTPS at all

@_date: 2015-05-12 13:35:56
@_author: John Denker 
@_subject: [Cryptography] crypto goals and criteria 
That makes sense.
Here's another way of saying the same thing:
  *) Metadata is data.
  *) A cryptosystem that leaks metadata    is a cryptosystem that leaks.
  *) A cryptosystem that leaks when compression is applied
   is a cryptosystem that leaks.
  *) A cryptosystem that leaks when the attacker can    inject some known plaintext
   is a cryptosystem that leaks.
Traffic analysis is a Big Deal.  Cryptanalysts have been
using traffic analysis for as long as there's been crypto.
I reckon we will never be able to stop "all" leakage, but
still we have to recognize it for what it is:  leakage.
I see the distinction between metadata and data as (at
best) a legal fiction, created in the US as a way to
get around the 4th amendment (not to mention the 3rd,
9th, and 10th).
By way of contrast:
That strikes me as nave.
On a onesie-twosie basis, it would cost more than the price of a beer to figure that out.  However, the thought police in even a smallish police state are surveilling millions of people, and they get to /amortize/ the cost of indexing the
wikipedia.  The scaling behavior is similar to that of a dictionary attack.  The cost per victim is negligible.
Sure, /some/ of the articles have changed since yesterday,
but most of them haven't ... and the thought police do
not need to read all of your communications;  a sample
Since there are more articles than there are plausible
length values, there will be some collisions ... but the
ambiguities can be resolved by looking at additional information not provided in the example above, e.g. the pattern of included images, incoming links, outgoing links, et cetera ... and/or by statistical inference.  The NSA is reeeeally good at statistics.
A dissident could buy a measure of protection by downloading
the entire wikipedia and then referring to the local copy.
This is an example of what we call /cover traffic/.  The English-language part is on the order of 10 gigabytes, so this is not even particularly expensive:
  lynx -source -head   Content-Length: 11820881800
OTOH it remains a cat-and-mouse game;  cover traffic does
not defeat all avenues of attack.
Ed Snowden said:
  "Encryption works. Properly implemented strong crypto    systems are one of the few things that you can rely on.    Unfortunately, endpoint security is so terrifically weak    that NSA can frequently find ways around it."
I suggest we need to pay more attention to the last part.
Just to give you some idea how hard it will be to fix the
problem, consider the following use-case:
I google for "babe".  The query and the reply are secured
by https.  So far so good.
a) If, however, I click on one of the hits, google will know whether I am interested in
 -- mythical oxen
 -- mythical pigs
 -- legendary ballplayers
 -- damsels
 -- or whatever
And (!) if google has it, the government will grab it, without
even a warrant.  According to the 2nd circuit court of appeals,
this is illegal.  I say even if it were legal it would be unconstitutional, and even if it were constitutional it would be bad policy ... but none of that stops them from doing it.
Here's how google knows:  Even though the text and the
tooltip tell you that the link points to   en.wikipedia.org/wiki/Babe_(film)
it doesn't.  Instead it points to something at google.com
that will record your click and then redirect you to the
nominal destination.
b) Furthermore, after redirection the link points to an unencrypted http page, even though the corresponding https page also exists.  Many months ago google announced that they would fix this, i.e. that search results would favor the encrypted version when available ... but it hasn't actually happened.
On my system, I have workarounds for (a) and (b), but even
so, I don't imagine that my system is secure.  I assume my
machines (including phones) are compromised at every level from the firmware on up.  I assume the "Root CA" clown car
is compromised several times over.
Bottom line:
  *) Security requires a lot more than cryptography.
  *) Metadata is data.
  *) A cryptosystem that leaks metadata    is a cryptosystem that leaks.

@_date: 2015-05-19 21:29:54
@_author: John Denker 
@_subject: [Cryptography] cybersecurity letter 19-May-2015 
The latest President Barack Obama
The White House
1600 Pennsylvania Avenue NW
Washington, DC 20500
We urge you to reject any proposal that U.S. companies deliberately weaken the security of their products.  We request that the White House instead focus on developing policies that will promote rather than undermine the wide adoption of strong encryption technology.  Such policies will in turn help to promote and protect cybersecurity, economic growth, and human rights, both here and [approximately 150 signatories, including security experts and tech companies]
Full text at:
    Lots of news media have picked up on this, but distressingly few link
to the actual letter.  Maybe I'm old-fashioned, but I think primary
sources are important.

@_date: 2015-11-01 21:54:21
@_author: John Denker 
@_subject: [Cryptography] HTTPS usage at major media sites 
Here are some observations and remarks on the use of crypto at certain
high-volume media sites.
Sites that redirect HTTP to HTTPS:
    Sites that offer both HTTPS and HTTP, without redirecting:
  Sites that redirect HTTPS to HTTP:
     (note 3)
          Sites that present an invalid certificate (typically because the
CDN answers port 443, but the content originator never bothered
to obtain a certificate):
              Sites with a botched HTTPS implementation, e.g. mixed content:
    **** Remarks ****
1) Only a smallish minority of media sites offer any semblance of
 useful HTTPS.
2) A year ago there were none, so this is progress.
3) A year ago, the New York Times CTO co-authored a blog post listing
 all the reasons why a newspaper site should use HTTPS.
    I find it amusing that the Washington Post took his advice, but his
 own paper did not.
4) Contrary to what it says in that blog post, applying HTTPS to a
 newspaper site provides little if any privacy.  That's because
 traffic analysis is too easy.  The article lengths and the pattern  of image fetches give the game away.
 As I have said before:    Metadata is data.     A cryptosystem that leaks metadata is a cryptosystem that leaks.
 We seriously need to raise our game.  We need to come up with systems
 that do a much better job of protecting privacy.

@_date: 2015-11-04 13:08:20
@_author: John Denker 
@_subject: [Cryptography] observations: Let's Encrypt certificate authority: 
I suppose most folks on this list know about Let's Encrypt:
  The objective is to provide free DV ("domain validated") certificates, and to provide an easy-to-use method (ACME) for obtaining certificates.  The ACME objectives and methods
are described here:
  Executive summary:  I reckon letsencrypt will be quite valuable
eventually.  There has been some progress recently.  It is already
better than nothing, although it still has some ease-of-use issues.
Some informal observations:
1) The project /schedule/ has exhibited a lot of slippage.  For
 the last year or more, it has been slipping almost one month per
 month (i.e. almost no externally-discernible progress at all).
 HOWEVER recently there has been some discernible progress.
 The thing is now in "limited" beta status.  It is available
 by invitation only, but you can request an invitation via:
   I mention this because folks on this list might be interested
 in experimenting with it ... and because IMHO the system has
 quite a few rough edges and would benefit from some constructive
 feedback from people who know what they're talking about:
   -- checking the security of the protocol
   -- improving the usability of the UI
   -- improving the documentation
2) I have not examined the security properties and have nothing
 to say on the subject.  This note focuses on usability issues.
3) After a modest amount of fussing with it, I got it to work.
 Example:
   4) The command I ended up using was
Nuisances include:
 *) It is easily confused if you use apache "VirtualHost" "ServerName"
  features.  You have to edit your .conf files to work around this.
 *) The documentation is incomplete and not entirely consistent.
  -- Some of the documentation tells you about the the --server option,    and some of it doesn't.  If you leave it off, you get a certificate
   issued by an untrusted "fake CA".  If you include it, you get a
   for-real cert, trusted by typical browsers.
  -- Some of the documentation tells you about the -d option, and
   some of it doesn't.
 *) Once you get the certificates, you have to do some more editing
  to link them into your .conf files.  Again you have to read   disparate bits of documentation to figure out the details.  The
  command creates an example file but doesn't tell you about it,
  and it neither fully complete nor fully correct.  I reckon anybody
  on this list can figure it out ... but it's a long way from   meeting the project's stated objective of being "easy to use".
        ServerName xxx.av8n.com
        SSLCertificateFile      /etc/letsencrypt/live/xxx.av8n.com/cert.pem
        SSLCertificateKeyFile   /etc/letsencrypt/live/xxx.av8n.com/privkey.pem
        SSLCertificateChainFile /etc/letsencrypt/live/xxx.av8n.com/chain.pem
        DocumentRoot /var/www/xxx
        Include             /etc/apache2/sites-available/generic-ssl.conf

@_date: 2015-11-07 09:47:00
@_author: John Denker 
@_subject: [Cryptography] safety principles +- security principles 
So far so good...
I disagree almost completely.
I do not recognize any important distinction between safety and security.  There are minor differences of
connotation, but the basic idea is the same.
If you think there is "universal agreement" it just
means you haven't looked very closely.  For example, on the very same day (11/05/2015) a chemistry professor
was talking about safety.  He said:
Here's another example, from yet another field where
safety is constantly under consideration, namely aviation.  There is nowhere near "universal agreement"
about what's "safe" and what's not.  For example,
consider night VFR:
  a) In Canada night it's illegal.  It's considered
   too risky.
  b) In the US, it's perfectly legal.  "Live free or die."
  c) In the US, a lot of pilots won't do it.  "Fly at
   night, file the flight."  I've seen situations where
   this turned out to be spectacularly good policy.
More generally, each pilot has so-called "personal minimums" that are stricter than the legal minimums.
This explicitly enshrines the idea that there is no
"universal agreement" and never will be.
Also, there is a well-known story in the crypto
community:  The pointy-haired boss asks "Will it
be secure if we ...."  and the expert answers "No"
without waiting to hear the rest of the question.
The point is, nothing you do will ever by 100%
secure.  The real question is how much risk are
you willing to bear, and how much are you willing
to pay to reduce the risk.
The exact same story is told in every safety-related
field that I know of, including chemical safety,
nuclear safety, aviation safety, etc. etc. etc.
Last but not least, there are many ways in which communication security overlaps with operational  -- Huge numbers of lives depend on military crypto.
  This is obvious during wartime.  It is less obvious
  but no less true during times when you didn't think
  there was a war on, e.g.
       -- There are ways in which hacking aviation data   and/or voice communications could lead to tragedy.
  Some of this is obvious, some less so.  As usual,
  life is difficult for the white-hat whistleblower,
  because it is easier to find bugs than to fix them.
 ++ etc. etc. etc.
Bottom line: Trying to distinguish safety from security
is a fool's errand.

@_date: 2015-11-11 11:13:25
@_author: John Denker 
@_subject: [Cryptography] where is the weakness?  related-key, mac-then-encrypt, 
I mostly agree with the sentiment, although that seems
a bit overstated.
By way of analogy:  Suppose a drunk driver jumps the curb,
plows through the flowerbed, and smashes into my house.
I'm not sure it's fair to blame the flowers ... although technically it could be argued that if I had planted oak trees instead of daffodils, they would have defended against
this particular attack.
It seems to me that if a given cryptosystem (or subsystem)
must be followed by a MAC to make it secure, then the subsystem was no good to begin with.  To say the same thing the other way, if MAC-then-encrypt is not safe, then the encrypt step itself is unsafe (with or without
any earlier MAC) and it's not logical to blame the early Never confuse the presence of one thing with the absence
of another.
Here is perhaps a better way to think about it:  A block
cipher such as AES is just fine for a *single* block.
For a multi-block message, the block cipher would be
just fine if a different key were used for each block.
That's the logical approach, but almost nobody actually does that.  They're too lazy to do all that rekeying.
Instead they use "modes" which are IMHO a kludge that
gives an /approximation/ to rekeying.  ECB mode is the
trivial case of no rekeying and no concealment, leading
to disastrous results.  CBC is better under favorable
conditions, but in less-favorable situations it leaks
horribly.  This includes situations that I like to call
cowbird attacks, i.e. situations where inside your nest
you have a bad actor who can inject stuff into the channel.
BEAST is the poster child for this.
Let's be clear: In CBC mode, in some situations, the late
MAC is required for basic secrecy, not just for authentication.
The late MAC must be considered part of the cost of doing business in this mode.
So let's take a step back and ask what is the real problem,
and what solution(s) might there be?
I see this as fundamentally a related-key attack.  Not
just related, but identical.  The same exact key is being
used for every block of the message.  To fix this:
 a) we could change the block cipher to make it easier to rekey
 b) we could stop using CBC mode
 c) we could perhaps try to keep cowbirds out of our nest
 d) we could not leave padding oracles lying around
 e) we could encrypt-then-MAC
 f) we could move to purpose-built authenticated-encryption
 *) ????
As for (f), that seems to be a work-in-progress;  see e.g.
  As for (e), that seems like a kludge IMHO.  I'm not an
expert, but it would seem that (a) and/or (b) is better.
The latter are almost equivalent;  changing the IV is almost
the same as rekeying, if the new IV is based on something
on the previous Cipher Block.  Also note that the late MAC
comes at a cost, and the cost of rekeying (or computing a
new IV) can be weighed against this.  Similarly, transmitting
the late MAC consumes bandwidth, whereas rekeying does not.
This could be the deciding factor in some situations, especially if authentication is being performed at some other layer.
Specific constructive suggestion:  From now on, whenever
there is a cipher competition, the evaluation criteria should include the cost of rekeying.
Actually, the AES implementations I've seen in my limited
experience are rather cheap to re-key.  I could imagine
operating the thing in key-counter mode, where the key
gets changed for every block.
Here's another reason for moving away from CBC:  It only
works for sequential things like TCP file transfers.  -- It does not work in datagram situations.
 -- It does not work for random-access disk encryption.
I haven't looked into it in any detail, but I assume the
datagram guys and the disk-encryption guys have figured
out some decent solutions.
Bottom line:  I see this as a related-key attack.  The
real solution is to use a different key for every block.
CBC does not really solve the problem.  Encrypt-then-MAC
does not really solve the problem.

@_date: 2015-11-16 12:09:32
@_author: John Denker 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
I reckon we can.  I've seen good solutions to analogous problems
including much harder problems.
Suppose you buy a new car.  It has some life-limited parts.  The
original tires, filters, spark plugs, etc. are very unlikely to
last the life of the car.  Everybody knows this, and they plan
accordingly.  Installing a new air filter is not traumatic.
The same applies to bits of information.  In an airplane, there
are some charts and navigational databases that must be updated
every 56 days.  They must be updated, even if the only thing that
has changed is the expiration date.  Everybody knows this, and
they plan accordingly.  Installing an update is super-easy.
So, here's my suggestion for designing a crypto suite to last
forever:  Don't.
Instead, put an expiration date on it.  Demand that it be updated
every so often, even if the only thing that needs changing is the
expiration date.
Cars, airplanes, and elevators are already subject to periodic inspection in most jurisdictions.  They already have life-limited
parts that must be replaced every so often.  Software in general

@_date: 2015-11-18 17:25:14
@_author: John Denker 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
In the immortal words of Henny Youngman:  So don't do that then.
Let's stop the the straw-man arguments already.  People have been
solving problems like this, and indeed much harder problems, for
a long time.
For example, an airliner is required to undergo inspection every
100 hours.  However, the engines do not automagically shut down
at the end of the 99th hour.  It is mechanically possible to keep
going, and indeed it's even legal under mild restrictions, if
you read the fine print of the regulations.
Similarly, every 60,000 miles my car tells me "time for service"
which means I need new spark plugs.  However it's only a warning;
the engine does not automagically shut down.
As for the problem of "home routers" ... all my wireless base
stations run DD-WRT, which is open source.  I simply won't buy
hardware that is not DD-WRT-friendly.  So I have lots of options
for updating and/or reconfiguring.  I am not dependent on the whims of the original manufacturer.
Also FWIW all such things sit behind a firewall, which consists
of an old laptop running Linux.  So again I have lots of options
for updating and/or reconfiguring.
  I do not imagine that this is "secure" in any absolute sense.
  For starters, I assume that They-Who-Must-Not-Be-Named have   long-since stolen the Ubuntu release-signing key.
  Also I do not imagine that Joe Sixpack is interested in
  maintaining (much less setting up) his own firewall.  But
  so what?  Joe doesn't change his own spark plugs, either.
  The first couple of sets are covered under the warranty,
  and after that he can shop around for somebody to do it.
The drug industry is a very unhelpful analogy.  Along with the nuclear power industry, it is the poster child for "market
failure".  In any non-broken marketplace, customers can take
into account the long-term cost of ownership when deciding
what to buy.  The carmaker who offers free maintenance for
120,000 miles enjoys a marketing advantage over one who doesn't.
The EPA puts energy-related cost-of-ownership stickers on cars,
refrigerators, et cetera.  The wireless vendor who offers
DD-WRT compatibility and/or free crypto upgrades for N years
enjoys a marketing advantage over one who doesn't.
Dealing with life-limited components is not rocket surgery.
People have been dealing with this general class of problems
for a long, long time.

@_date: 2015-11-22 14:05:44
@_author: John Denker 
@_subject: [Cryptography] basic cryptography  ... was: key breaking 
Oh, it's muuuuuch worse than that.  Nowadays one needs to
worry about /chosen/ plaintext attacks.  BEAST is a recent
Isn't the whole purpose of a cryptosystem to "randomize the If the original system requires you to pre-randomize, it sounds to me like the original system wasn't worth much.
Specifically, the situation reminds me of stone soup.  If you pre-arrange enough yummy ingredients, the stone soup
recipe turns out great.  However, the stone per_se isn't doing what it claims to do.
In the context of cryptographic "modes", on 11/16/2015 One could argue that CBC is a nasty hack, and always has been.
Ditto for other chaining modes.
It seems obvious that if we wanted to do the Right Thing we
would use a separate key for each block of the message.  Then
some genius observed that keying was expensive, and one could
optimize for speed by re-using a key, block after block.  It
was argued that chaining was "almost" as good as rekeying.
It was never really as good, so this optimization came at a
 -- Less security, especially in the face of chosen-plaintext
  attacks.
 -- Increased cost, if you have to "pre-randomize" the data
  and/or encrypt-then-MAC in order to have a semblance of
  security.  Such increases are likely to nullify the alleged
  advantage that chaining was supposed to confer.
 -- Chaining doesn't work for datagrams, since it requires a
  well-defined notion of "previous" packet.
 -- Chaining doesn't work for random-access disk sectors.
 -- Chaining doesn't work if you want to encrypt a bunch of
  stuff using parallel hardware.
It doesn't have to be that way.  For example ChaCha does not
depend on chaining;  it can do a random-access "seek" to any
block number whatsoever.  So, even though it is classed as a
"stream cipher" it doesn't require an actual stream.
One can achieve a similar result using a block cipher as your
crypto primitive, as follows:
1) In some hypothetical ideal world, one could imagine the
following, which I call "plain key ringlet" mode:
     session key --\
                    \
     block # --->  XOR --\
                          \
                           \
                            \
     plaintext  -------->  block cipher -->  ciphertext
I don't trust the resistance of AES to related-key attacks, not nearly
enough to use it in this mode, but hypothetically if we could trust it, there would be numerous advantages:
 -- More resistance to guessable-plaintext attacks, known-plaintext
  attacks, and chosen-plaintext attacks.
 -- No "pre-randomization" required.  No encrypt-then-MAC required.
 -- Decent efficiency.  For AES the cost of rekeying is only 40% of   the cost of encrypting a block, so this has more than 70% of the   throughput of any chaining mode such as CBC.  It is out-and-out
  faster than disk-encryption modes such as CMC and EME.
 -- Unlike chaining modes, this works for datagrams.
 -- Also for random-access disk blocks.
 -- Also for parallel crypto processing.
2) Consider the following, which is meant as an existence proof,
 not as an optimized design.  I it call "turbo key ringlet" mode:
     session key ----\
                      \
                    preliminary
     block #  --->    cipher  ----\
                     (or hash)     \
                                    \
                                   main
     plaintext  ------------>  block cipher -->  ciphertext
One way of looking at this is as a version of the main block
cipher, with a vastly fancier key schedule, making it more
resistant to related-key attacks.  That is, we parenthesize
the diagram as   (SK + block  + (preliminary cipher + main cipher).
Another way of looking at this is as a stream cipher, which
is used to generate a stream of keys for the main cipher:
  (SK + block # + preliminary cipher) + (main cipher)
You could use any old block cipher in CTR mode to generate the
key stream.  AES is pretty fast.  Or you could use a so-called
stream cipher directly.  ChaCha is very fast.
3) This isn't new;  a while back Sandy Harris proposed "enchilada"
which can be seen as an optimized version of scheme (2):
   session key ----\
                    \
                 preliminary
   block # ------> ChaCha ------\ (round key array for AES)
                                 \
                                  \
   plaintext -----------------> guts of AES --> ciphertext
It bypasses all of the Rijndael key schedule, and instead relies
on ChaCha to directly generate all 11 (or 15) of the AES round
4) For completeness, we should mention using ChaCha directly as
a so-called stream cipher, in the usual way:
   session key ----\
                    \
   block # ------> ChaCha ------\                                  \
                                  \
   plaintext -------------------> XOR --> ciphertext
Compared to scheme (4), scheme (3) has better diffusion.  Compared
to scheme (1), scheme (3) has more convincing confusion.
Of course in a known- or chosen-plaintext attack, diffusion doesn't
do all that much good.  So maybe scheme (4), primitive as it may
look, might not be completely crazy.
In the spirit of the recent thread about "other obvious issues being
ignored", I turn to the mother lode of things that are "generally
believed" but not actually true:
Quoting from I say that's ludicrous.  As discussed above, there are exceedingly reasonable things one would like to do that involve related keys, which reportedly work with some ciphers (ChaCha) and not others (AES).  At a minimum, "concern" for related key attacks is a
prerequisite for "proper design".  If you tell me that "properly designed software" is obliged to pre-whiten the plaintext and/or pre-whiten the keys, I'm going to call it stone soup.
Quoting from Actually no.  If that were true, there would never be any such thing
as a related-key attack (since all cryptosystems are assumed secure with respect to related plaintexts).
In such a system, in a related-key + chosen-plaintext attack scenario,
you might end up with good security or no security whatsoever, depending
on how strong your key schedule is.  This is a big deal, because the AES key schedule is not very impressive.  I was astonished to hear that
related-key resistance was not a criterion and was not even evaluated during the AES competition.

@_date: 2015-11-23 10:22:55
@_author: John Denker 
@_subject: [Cryptography] crypto strength ... was: ratcheting DH strengths 
That reflects the attack_value_per_break relative to the attack_cost_per_break.  Note that we need to pay attention
to both the numerator and denominator.
Maybe it should go without saying, but let me say it anyway:
Ideally, this should be a /minimax/ proposition.  Specifically,
the design objective should be to maximize [over all designs] the minimum [over any batch] cost per key ... not the average
cost per key or typical cost per key.
Use case scenario: Suppose my friend in Ptomainia sends me
10,000 messages.  If any one of them gets broken he gets killed.
In this scenario, the attacker doesn't need to break the average
key, but only the weakest key.
Similarly:  Suppose I download 10,000 software packages from a
trusted distributor.  If the attacker can "taylor" any *one*
of them, I get pwned.
In such a scenario, we need:
 *) a large batch of keys, to minimize the attack_value_per_break
 *) a large *minimum* cost (not merely a large "average" cost)
  per broken key.
One can imagine other scenarios where the average cost is all
that matters, but we cannot always count on this.
FWIW, in cases where cost is related to some probability distribution
P, these ideas are related to things that can be quantified by using
various Rnyi functionals:
 -- average cost <-->  H1[P]  entropy
 -- minimax cost <-->  H[P]

@_date: 2015-11-23 18:04:39
@_author: John Denker 
@_subject: [Cryptography] basic cryptography  ... was: key breaking 
However, it is still true, as pointed out on Mon, Nov 23, 2015 at 12:30:08PM -0500 by Phillip Hallam-Baker:
Also with extra keying material required.
As with any cipher in this class, to have any semblance of security
for multi-block messages, you still need to do something like CBC.
So the parties need to agree on K_1, K_2, and IV.  Even then, we
still have all the nastiness associated with chaining modes:
 -- Doesn't work for datagrams.
 -- Doesn't work for random-access disk sectors.
 -- Doesn't parallelize.
 -- Doesn't solve all the security problems.
Contrast that with:
  i = block #
  (V_i, W_i) = ChaCha(K_1, i)
  ciphertext_i = V_i XOR AES(K_2, plaintext_i) XOR W_i
 ++ Works for datagrams.
 ++ Works for random-access disk sectors.
 ++ Parallelizes.
 ++ Does not require chaining.
 ++ Solves a bunch of problems that CBC doesn't.
 ++ Does not depend on encrypt-then-MAC.
 ++ Compatible with any imaginable authentication scheme
  (encrypt-then-MAC or otherwise) at this layer or higher or lower.
 ++ Does not require an IV.
 ++ The amount of keying material (K_1 and K_2) is not increased
  relative to vanilla AES/CBC.
 ++ Affordable.  Slightly more work than vanilla AES/CBC, but only
  very slightly.  Cheaper than AES/CBC/MAC.  Possibly slightly cheaper
  than enchilada.  Definitely cheaper than disk-encryption modes such
  as CMC and EME.
 ++ Hard to break.  Would require serious breakage of both AES and ChaCha.
 ++ Easy to analyze.  Even Mansour and all that.
This provides as much diffusion as AES/ECB (in contrast to ChaCha by
itself, which doesn't).  However, I don't want to emphasize diffusion.
It's overrated.  It asks the plaintext to do something it shouldn't
be asked to do, i.e. to provide randomness.  This can backfire bigtime
during chosen-plaintext attacks.  I would prefer to see the keys provide
sufficient randomness.  That's their job.
Did I mention that chaining modes are a nasty hack?
Indeed.  It's time to do something better.

@_date: 2015-11-25 10:18:28
@_author: John Denker 
@_subject: [Cryptography] basic cryptography ... was: key breaking 
Good catch.  Fixed in this version.
At a much deeper level, on 11/24/2015 09:16 AM, Sandy I should have explained this point earlier:
  When I am doing the encryption, I require that each   bit of generator output be used at most once.
Subject to that requirement, and assuming the generator
is cryptographically strong, the aforementioned known-
plaintext attack is non-issue.  The attacker already
has the plaintext, so he can't learn anything new about
that ... and learning about the generator output doesn't
do him any good.
In particular, I need to repair and explain the diagram
that I presented on 11/23/2015 06:04 PM.  I should have
  i = block #
  (V_i, W_i) = ChaCha(K_1, i, nonce)
  ciphertext_i = V_i xor AES(K_2, plaintext_i xor W_i)
Of course you can replace AES by some other block cipher.
Similarly you can replace ChaCha.  In this case almost
any cryptologically strong hash function would suffice.
To be clear:  I am assuming that ChaCha is so strong
that it would be secure even if the AES step were
replaced by the identity transformation.  The need for
this assumption becomes clear when you consider chosen-
plaintext attacks (e.g. BEAST).
Let's talk about the nonce.  It is a standard feature of
ChaCha, for good reason.
*) By way of background:  For TCP-like traffic, each  block is encrypted only once, so the sequence number
 is unique, and the nonce doesn't add much.
*) More interestingly, UDP doesn't have block numbers,
 so one would have to introduce a concept of block #
 and/or a concept of nonce.  (A lot of UDP traffic has
 a sequence number at some higher level, but this is
 not something we can rely on in the general case.)
*) Yet more interestingly, for random-access disk sectors,
 the usual block # is not good enough, because blocks can
 be rewritten.  Here is where the role of the nonce can
 be seen clearly.  We need a block number /and/ a nonce.
Adding a nonce to a stored disk block is like adding salt
to a stored password.  It increases the storage requirement,
but is necessary for security.
  Tangential remark:  The idea of encrypting disk blocks
  with zero added storage requirement was a non-starter to   begin with, assuming you wanted some sort of authentication.
Also:  For maximum security, I would prefer to see the AES
key change every time (as a function of block # and nonce).
The construction given above drops this feature as an economy measure, to save the cost of rekeying.
Also note that in contrast to the proverbial whole enchilada,
this can take advantage of standard AES hardware.  You just
wrap the AES in XORs.
The general, conceptual points are:
  1) Using the same key more than once, block after block,    is a bad habit.  It causes all sorts of problems, some
   of which are alleviated by chaining modes such as CBC,
   but some of which are not.  We reeeeally need to kick    this habit.
  2) As a corollary:  We need ciphers for which rekeying
   is not expensive.
  3) On the other side of the same coin, we need ciphers
   with a strong key schedule, so that they are as strong
   against related-key attacks as against any other kind
   of attack.  (AES lacks this feature.)

@_date: 2015-10-02 20:05:07
@_author: John Denker 
@_subject: [Cryptography] RFID Protocols 
I get 21,000 hits from
  and 37,000 hits from
  More specifically, we have standards including:
      Some of them yes, some of them no.
Some of them do, and some of them don't.
The term "RFID" covers a lot of territory.  Technically speaking,
at the high end it covers military IFF systems, which have every
advanced feature you can think of.  At the low end, they are
smaller than a dust mote, and have no more intelligence than
a bar code ... a completely open code, with no replay resistance
See also:

@_date: 2015-10-08 06:32:39
@_author: John Denker 
@_subject: [Cryptography] Reproducible results, scientific method, 
In the thread about "blockchain and trustworthy computing",
Replication /does/ make a difference.  It makes all the difference
in the world.  The fact that the performance claimed by VW could not be reproduced under real-world conditions is exactly what led to the discovery of the fraud.
  Also note that there is a big difference between inexactitude
  and fraud.  Making a mistake is not a crime.  Deliberate,
  systematic falsification of test results is a crime.
That has never been the criterion for publication in scientific
journals.  Anybody who has ever done scientific research would object to making it a criterion.
The actual "method" of doing research is a lot less methodical
than most people imagine it to be.  It does not even remotely
resemble the five-step "scientific method" that people are
taught in grade school.
For example, consider the Bohr model of the atom, i.e. electrons
going around the nucleus in "orbitals", like a miniature solar
system.  It was known at the time that it did *not* fit the
spectroscopic data for any element other than hydrogen.  It
was known at the time that it was inconsistent with the Maxwell
equations.  Now, apparently the suggestion is that Bohr should
have been prevented from publishing his model.  Maybe you don't
object to that suggestion, but you should.
In the experimental realm, consider the other Bob Wilson.  Suppose
you and Penzias have just discovered the microwave background
radiation.  Should you be prevented from announcing this, until
somebody else builds a fancy antenna and a super-sensitive
receiver and replicates the result?  Maybe you don't object
to that suggestion, but you should.
The fact is, *all* theories are imperfect and *all* experimental
data is imperfect.  Also, the first result in any area has to be
announced before it has been verified ... with the rarest of
exceptions, e.g. at CERN, where they intentionally did two
more-or-less independent Higgs searches in parallel.
Imperfect data and imperfect theories are stepping stones along
the road to better data and better theories.  This is how it
has always been, and how it will always be.
In science, reviewers are flatly forbidden from holding up
publication of a manuscript while they try to reproduce the
result.  Among other things, the manuscript is considered
confidential information, and using it as a guide to what
experiment to do would be highly unethical.
  Mathematics is an exception.  There is a weird process   whereby a novel result (especially if it is important)
  circulates informally for a time, while people look for
  errors in the proof, so that by the time the result is
  "officially" published it has a high degree of polish.
  This is in effect a two-tier publication system.  Everybody
  else uses a single tier, where the preliminary results are
  openly published.  Truly novel results are prized, not
  blocked.
There is an important difference between the /data/ and the
Fermi got the Nobel prize for some interesting experiments.
The experiments were OK, but essentially every word of his interpretation was wrong, and about half of the prize citation
was wrong.  At first he (and everybody else) thought he had discovered new transuranic elements, when in fact it was just fission, yielding isotopes of old elements.
There are also experimental mistakes, such as dirty cable connectors leading to erroneous observations of faster-than-
light propagation.
Again it must be emphasized that there is a huge difference
  1) Statistical fluctuations in the data, which are normal,
   not even a mistake.
  2a) Misinterpreting the data, which is a mistake.
  2b) Systematic error in the data, which is a mistake.
  3) Fraud, which is a crime.
This is important, because the mechanisms to detect and compensate
for mistakes are very different from the mechanisms to defend
against fraud.  Fraud, almost by definition, is /designed/ to
defeat the first line of defenses.  The penalty for making an honest mistake is near zero.  Nobody thought Fermi should return
his Nobel prize.  Indeed, when they wanted to build the first nuclear reactor, Fermi was the obvious guy to rely on to build it.  In contrast, if you get caught painting the mice, it's the end of your career.
In the VW case, several guys have already lost their jobs, and the company is on the hook for billions of dollars in fines.
Not to mention possible criminal sanctions.  Fraud, like arson and murder, is virtually impossible to /prevent/ completely.
Efforts to suppress fraud involve a large measure of /deterrence/.
That is, there are retrospective penalties.
James Randi, a respected expert in the art of fooling people,
famously said "scientists are easier to fool than children".
That's because they are not accustomed to being lied to.
Experiments on the natural world can be subtle and confusing, but the atoms are not trying to attack you.  It is not an
adversarial situation.  OTOH scientists are not stupid, and
when they detect fraud they respond with harsh penalties.
  This sets natural science apart from (say) law and politics,
  where everybody lies all the time and it's considered no   big deal.
  This also sets natural science apart from crypto in particular
  and security more generally, where we have to assume that the
  system is under attack from all directions.
The original network protocols were designed by scientists for
scientists.  This explains the enormous security holes.  They
assumed that good guys would voluntarily comply with ethernet
and TCP protocols, and anybody who got caught intentionally
cheating on (say) the backoff algorithms would get caught eventually ... and then fired.  This approach -- relying on retrospective enforcement -- is not necessarily a bad idea.
It can be made to work, and sometimes it is the only thing that works.
For example, in the late 1960s and early 1970s there was a
rash of people hijacking airliners to Cuba.  Airlines tried all sorts of defensive measures, but the thing that really worked was a US/Cuba extradition agreement.
  The same approach could be used in cyberspace.  For example,
  one could say to Nigeria:  We will cut you off from the
  internet backbone unless you catch and extradite the 419
  lads.
I do not understand why people get so worked up over the VW
situation.  I do not see it as a breakdown of The System.
It's a crime -- a rather banal crime.  Existing punishments
fit the crime:  We catch the guys, throw them in jail, levy a fine on the order of 10^10 dollars, and life goes on.  The next guy who wants to try something similar will think twice.
  OTOH if VW manages to weasel out of the sanctions, then
  *at that point* we will have a breakdown of The System,   and at that point we should get seriously up in arms.
In the meantime ... it's just a crime.  It's notable for
its magnitude ... but it's otherwise quite banal.  It's nothing we can't handle.
We should be infinitely more worried about attacks where
the perps are beyond the reach of the law.

@_date: 2015-10-20 11:47:17
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
That is an excellent question!  Contrary to what others have suggested,
it is not at all a rhetorical question.  Generally speaking, rhetorical
questions don't need to be answered, but this question needs serious
thought and serious answers.
I assume that means you can't come up with a /perfect/ checklist.
In any case, we should not let the perfect be the enemy of the good.
An incomplete checklist is better than no checklist.
Here is something to add to what others have said:
*) There is a fundamental principle that says
     "If you can't encrypt properly, don't encrypt at all.
      If you send in the clear, you endanger only yourself,
      in obvious ways, whereas if you misuse the cryptosystem
      you endanger everybody, in less-obvious ways."
 I forget who is credited with that idea.  It's discussed in
 Kahn somewhere.
 This stands in contrast to traditional internet policy,
 namely Postel's robustness principle:
     "be conservative in what you do, be liberal in what       you accept from others."
 In a friendly environment that leads to robustness, but in an
 adversarial environment it leads to catastrophe.  Basic security
 demands that clients must become much *less tolerant* of broken
 servers than they presently are.  Similarly, servers must become  much less tolerant of broken clients.
 This problem is not easy to fix, but it needs to be fixed anyway.
 Reasons why it is hard to fix include:
 1) There is the longstanding practice of shooting the messenger.
  Anecdote:
    One of the first business deals I ever did involved selling
    some software to JPL.  I wrote the software to be defensive.
    It detected deadlocks and printed warnings.  The customer
    complained about the deluge of warnings and threatened not     to pay me.  I had to explain, "Your code is broken, not mine.
    It's broken in mission-critical ways, and it's been broken for     years.  You should pay me extra for detecting and pinpointing
    the problem."  By way of compromise I offered to print less-
    verbose warnings.
  2) In typical client/server situations, the messenger is knocking
   on the wrong door.  For example, if a client app notices that the
   server is insecure, it does no good to print a warning, because
   the client (in all probability) is powerless to fix the problem.
   In fact, the warning is worse than nothing, because it just
   trains the user to habitually ignore warnings.  Even if the app    throws a fatal error, it does no good, because the user will just
   switch to a different app that "works better".
  3) People are really good at ignoring problems that they don't
   know how to fix, even if the problems are super-obvious.
 Possibly partially constructive suggestion:  there should be more
 in the way of white-hat scanner tools to check for known weaknesses  ... and people should get into the habit of using them.  Here's a
 small example of the sort of thing I'm talking about:
      It will tell you that  duats.com  is offering "export grade"
 ciphers, which is a Bad Thing, since the site is supposed to
 be providing life-and-death critical information.  Also, the
 following sites don't offer Forward Secrecy at all:
     citibank.com
     mastercard.com
     va.gov
     cisco.com
     microsoft.com
     icann.org
 Bottom line:  There is a *lot* of broken crypto out there, some
 of it affecting critical infrastructure.  Overall, this is a terrible
 problem.  The fact that I don't know how to fix it doesn't make it  any less obvious or any less terrible.  Theoretically the "boneheaded  obvious" stuff should be the easiest to fix.

@_date: 2015-10-20 12:16:01
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
Yet another item to add to the list:
As I like to say:
   * Metadata is data.
   * A cryptosystem that leaks metadata is a cryptosystem that leaks.
This is an obvious problem.  The fact that I don't entirely
know how to fix it doesn't make it any less obvious, or any
less problematic.
In particular, in email, a lot of stuff that should be
encrypted isn't.  For example, PGP doesn't even try to
encrypt the Date: and Subject: lines ... for no good reason.
Onion routing helps, but the TOR network seems to be rather
inefficient and rather low capacity at the moment.
There exist "secure messaging" apps, but they seem at the
moment limited to a rather small niche market ... which
might make them worse than nothing, insofar as using them
raises red flags.
One of the main tools for defeating traffic analysis is
discussion of that recently, in this forum and elsewhere,
but not much rubber is meeting the road AFAICT.  A search
  turns up almost nothing.
Another thing that might help is legislation:
   * Metadata is data.
   * You can't get the data without a warrant.
   * You can't get the metadata without a warrant.
In the US, the idea that metadata is somehow not covered
by the 4th amendment is a legal fiction.  It is a loophole
that could be closed, or at least narrowed to stop ginormous
trucks from driving through it.

@_date: 2015-10-20 12:29:32
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
Another item:
Although we are quite rightly worried about our privacy and security'
being taken away by governments and by criminal syndicates ... we
should also be worried about *corporations* doing stuff that is
nominally legal but nevertheless highly abusive and intrusive.
As it says in the Book of Revelations, it is a sign of the End Times:
  "no one was allowed to buy or sell things unless he bore
   the mark of the beast--that is, his name or his number."
I mention that because can't buy anything at Costco without having
your purchases tracked, even if you pay cash.  You can't buy anything at Fry's Foods unless you identify yourself, or pay a huge penalty.  You can't buy anything online without having your purchases tracked.
There are private firms setting up cameras to record license
plate numbers, so that they know where everybody is at all
The phone company tracks the location of your phone, in far
more detail than is needed for simply placing or receiving
These are obvious problems.  The fact that I don't know how to
fix them doesn't make them any less obvious, or any less terrible.

@_date: 2015-10-20 16:35:45
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
+1 To that.
Smart disk drives and smart flash drives have legitimate goals
of throughput and reliability that conflict with the goals of
security.  In particular, they cannot be zeroized in any nice
I reckon we can somewhat alleviate that by using full-disk
encryption.  We are then left with the problem of zeroizing
the keys in the coprocessor that does the encryption.
That's still hard, but overall it seems like a step in the
right direction, insofar as it is more localized and less likely to conflict with the drive's other goals.
Compared to some other stuff we've been discussing, that's
relatively easy to fix.  Open the case, unplug or cut the
wires leading to the external ports, then re-seal the case.
And/or fill the ports with a 50/50 mixture of industrial
epoxy and silicon carbide grit.

@_date: 2015-10-20 16:43:57
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
On 10/20/2015 03:04 PM, Ron Garret replied:
Irrelevant.  See below.
Irrelevant twice more.
When I am searching email, I very often want to do a full-text
search.  So I need to decrypt the entire message anyway.  As the intended recipient, I can do that.  Also if I want to
selectively decrypt some of the headers, I can do that.
The fact that I need to decrypt it in order to read and/or search
is irrelevant to the point I was making earlier, namely that there
is no good reason for it to be sent in the clear over the wire ...
especially when the wire belongs to a wholly-pwned subsidiary of Furthermore, as I have previously discussed in this forum, there
are reasons why some of the metadata should be encrypted with
separate keys, and made available on a need-to-know basis.  For
example, a forwarder needs to know the next hop, and perhaps
some evidence of authorization, but nothing else.
I still say:  Metadata is data.  A cryptosystem that leaks metadata
is a cryptosystem that leaks.  A lot of stuff is currently sent
in the clear for no good reason.
Secure search is not particularly harder than secure reading.
I copy the ciphertext onto a physically-secure machine.  If necessary I post a couple of armed guards outside the door and lock the door.
Then I decrypt the messages, search them, read them, et cetera.  The hard part is zeroizing the machine afterward.  (See previous
message.)  I zeroize it as best I can, and then lock the whole machine in a tamper-resistant safe for good measure.
That doesn't entirely solve the problem, but it moves it a good
ways down on the list, to the point where I've got more important things to worry about.

@_date: 2015-10-21 06:19:29
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
This is a fascinating, important thread.
Here's something to add to the list:
*) The fact that my operating system shipped with something like
 170 trusted "root" CAs is a problem.  When the attack surface is
 that large, it cannot be defended.  This is a profound, grotesque,
 obvious problem.
 It makes a mockery of the intended meaning of "root".
 It is a travesty that the Mozilla Foundation pays DigiCert to certify
 that mozilla.org is "trusted" ... when in effect it is Mozilla that
 decides whether DigiCert is trusted, not vice versa.
 Some small steps have been taken toward alleviating this problem  (pinning, transparency, cross-signing) but overall, the problem
 is nowhere near solved.
*) As an illustration of the aforementioned problem, and also
 as a problem unto itself, when the recent Superfish fiasco was
 exposed, Mozilla had no way to revoke the offending cert ...  and they didn't seem to think this was a problem.  They said:
 I'm not sure I understand what they're saying.  I asked for clarification
 and didn't get any replies.  Possible interpretations include:
  -- They can't revoke the CA cert because the software is profoundly
   screwed up and they don't feel like fixing it.
  -- They can't revoke the CA cert and they don't see why that might
   be a problem.
  *) As a related problem, it is ludicrous that each of those CAs
 has unlimited signing powers.  Obviously ludicrous.  For example,
 the Hong Kong Post Office could sign a certificate for irs.gov
 or microsoft.com or whatever.
 There are people on this list who think that SSLv3 signing
 constraints are not worth supporting, which I find bizarre,
 although it does make a self-fulfilling prophecy:  If the
 constraints are not enforced, nobody will bother using them.
 I'm not saying they would solve all the world's problems,
 but they could be used to reduce the attack surface somewhat.
Speaking of obvious, here's a super-obvious constructive suggestion:
There should be at most *one* all-powerful root CA.  If/when Mozilla decides to trust some CA, Mozilla should *sign* the CA, not simply
compile it into the list of trusted CAs.  This would regularize the
process of adding CAs to the list ... and revoking them when necessary.
I say "at most" one, because it would be even better to require
CAs such as DigiCert to be signed more than once.  For instance,
browsers used within the Taiwanese military might require CAs to
be signed by their own Information Assurance team, not just by
Mozilla.  They might decide they don't trust the Hong Kong Post
Office with unlimited signing powers, even though Mozilla does.

@_date: 2015-10-21 10:56:10
@_author: John Denker 
@_subject: [Cryptography] Other obvious issues being ignored? 
I disagree.
It's not the only issue, but it *is* an issue when the language, by definition, is not secure and not securable.  One of my favorite
sayings is,
  There is no benefit in getting the wrong answer quickly.
In other words, "optimization" for speed at the expense of security
is not really an optimization, and a language that permits this is
a bad language.  If the machine is not secure, you cannot assume
that the results of /any/ calculation are correct.
It's a criticism of C, and also of modern hardware.  A processor
nowadays is essentially a compiler unto itself;  it takes the
machine-language opcodes as rather loose hint as to you want
to do, and compiles that into a sequence of operations that
actually get performed.  Lots of pipelining, branch prediction,
conditional execution, et cetera.
As for the C language, let's consider a specific example, namely logical shift, according to section 5.8 of the language specification:
  ]] The behavior is undefined if the right operand is negative, or ]] greater than or equal to the length in bits of the promoted left
]] operand
The problem is that "undefined" is waaaaay too vague.  According
to this specification, 1<<-1 could print an ascii-art portrait of
Alfred E. Neuman on /dev/console ... and then publish all of your
private keys on facebook.
At the very least, this could be improved by saying that the result will be set to an undefined value.  This limits the scope
of the damage.
It would be even better to specify that if the right argument is out of range, the result will be set to zero.  This gets rid of
the idea of "undefined".  The behavior is the same across all hardware platforms and across all versions of the compiler.
It would be much, much better to specify that if the right argument
is out of range, the result will be set to zero and an exception
will be thrown.
Now it could be argued that range-checking the arguments introduces
run-time inefficiency.  Two responses:
 -- This could be handled in hardware at essentially zero cost.
  The original C compiler reflected the hardware of its day, but
  the tables have long since turned:  Hardware is designed to do
  what the language needs.
 -- A decent optimizing compiler should be able to optimize
  away the checks in speed-critical situations.  Non-critical
  situations are not worth worrying about.
I disagree.
Again I say, there is no advantage in getting the wrong answer quickly.
If the calculation can't be done securely, there is no point in doing
the calculation at all.
That ought to be a fixable problem.
It wasn't always a problem.  Back in the days of the PDP-11 and
6502, you could write in assembly language and have high confidence
that the machine would do as it was told.  If you told the machine to do an ASL it would actually do it, and it would do it in a reproducible amount of time.  There was no pipelining, no branch
prediction, et cetera.  If the registers got saved on interrupt,
you knew where they were saved, so there were no unaccounted-for
copies floating around.  There were of course /some/ side-effects,
e.g. TEMPEST, but the list of things you had to worry about was I'm vehemently not suggesting that we should write in assembly
language.  I am saying that a compiler should make things better,
not worse.
I don't have all the answers, but it seems like the list of what
we need is not very long:
  *) No UB!  No Undefined Behavior!  None!
  *) Constant time execution in some situations.  As mentioned
   above, this affects the hardware, not just the language    specification.
  *) Rigorous control of copied data, including zeroization
   when needed.
  *) Minimization of TEMPEST and other side effects.
  *) A few other things.
The computational model that we are asking for in connection with
copy-control is not particularly revolutionary.  On existing multi-user systems, the hardware and the kernel already have to
be scrupulous about zeroizing memory before handing it from one
process to another.  Security calls for a rather slight change,
i.e. zeroizing sooner rather than later.  A cold-boot attack is equivalent to a shared-memory coprocessor:  Eve is going to hook
up a hostile coprocessor and read all your memory.
To repeat: We are marking crypto-sensitive memory as /shared/ memory, shared with a hypothetical enemy process.  For example,
this means that a local variable is never "dead", because the
other process might be looking at it.  A language, a compiler, or a piece of hardware that cannot cope with shared memory is broken and needs to be fixed.

@_date: 2015-10-22 08:41:04
@_author: John Denker 
@_subject: [Cryptography] letter versus spirit of the law ... UB delenda est 
Executive summary:  UB delenda est.
Keywords: disingenuous pettifoggery, pharisaical legalism.
In the context of UB (i.e. Undefined Behavior) in the specification
for the C language, let's consider the /spirit/ of the law.  As a first example, let's consider signed integer overflow.  By all accounts, when the specification was being drafted, a handful of possibilities were considered.  These were based on the behavior
of the hardware of the day:
 1) Overflow could throw an exception.
   1a) It might be possible to recover from the exception.
   1b) Or it might not.
 2) It could saturate at INT_MAX or INT_MIN as appropriate.
 3) It could wrap around in a way that made sense in one of   the three allowable integer representations:
   3a) two's complement
   3b) one's complement
   3c) sign-and-magnitude
It must be emphasized that in all cases the intent was at worst an LSV (i.e. Loosely Specified Value) ... but it was
still a value.  In all cases the result was supposed to be a well-behaved signed int.  No other wild behavior was
contemplated.  In particular, wild behavior such as "having
a negative value yet passing a test for positive values" was
never contemplated.
However, this wild behavior is exactly what we see in the
incisive example attributed to Alexander Cherepanov and posted in this forum on 10/21/2015 07:42 PM by Peter Gutmann.
Let's be clear:  This wild behavior is exactly the sort of thing that gives disingenuous pettifoggery a bad name.  It is
the sort of thing that gives pharisaical legalism a bad name.
It tramples on the spirit of the law and exploits the letter
of the law to achieve a result that (at best) benefits some at the expense of others.
  Actually, it's worse than that.  It should be obvious that
  a so-called "optimization" that results in wild behavior   doesn't really benefit anyone.  There is no advantage   in getting the wrong answer quickly.  Bypassing security   checks is not really an optimization.  A machine that   is not secure cannot be relied upon for /any/ purpose.
  Requiring people to use unsigned ints to the exclusion
  of signed ints when writing reliable code is not really
  an optimization ... and isn't sufficient anyway.  Using
  unsigned ints gets rid of the UB with respect to overflow,
  but other UB remains, including e.g. x<<y.
  I am reminded of Knuth's dictum:
    Premature optimization is the root of all evil.
There are two distinct problems here:
 A) The fact that the C language specification is vulnerable
  to this sort of abuse is a bug in the spec.   B) The fact that some compiler writers would exploit this   vulnerability is a problem of a different kind.
The best solution is to rewrite the specification so that
no UB remains.  No Undefined Behavior!  None whatsoever!
As a first step, one could replace UB with LSV, in accordance
with the original intent of the spec and in accordance with longstanding practice.  As a second step, it would be nice to get rid of all LSV behavior also.  Note that the IEEE
floating-point spec got rid of UB and mostly got rid of
LSV with respect to floating point;  surely doing the same
for ints can't be that difficult.  In the interim we can live with LSV a lot easier than we can live with UB. (We've been living with LSV for years.)
  I don't care whether we call the rewritten thing a   "new language" or a "dialect" of the old language.
  Let's just fix the problem already.
UB delenda est.

@_date: 2015-10-26 04:54:04
@_author: John Denker 
@_subject: [Cryptography] letter versus spirit of the law ... Eventus 
That makes sense.
That's overstated.  We agree that /some/ types of strict
checking are good.  Bugs should be found and fixed.
However, "crashing" is not the right word.  Crashing is not "generally" good policy.
In the aerospace field, crashing is generally considered a Bad Thing.  Consider for example the Apollo 11 moon landing.  During the descent, the LM guidance computer started throwing "1201" and "1202" program alarms.  However,
steely-eyed mission controllers decided, correctly, that the alarms could be ignored.  The program did not crash,
and the spacecraft did not crash.
Note that the pilots did not screw up.  They followed the
checklist, but the checklist was wrong.  The simulations
during training were not sufficiently faithful to catch
the error.  The simulations were, however, good enough
to train the controllers to not over-react. The program was fault-tolerant;  it performed brilliantly
in the face of "impossible" inputs.
Here's a contrasting example from 2200 years ago:
  Chen Sheng:  "What's the penalty for being late?"
  Wu Guang:    "Death."
  Chen Sheng:  "What's the penalty for rebellion?"
  Wu Guang:    "Death."
  Chen Sheng:  "Well, guess what:  We're already late."
So began the Dazexiang Uprising, leading eventually to
the fall of the Qin dynasty.
Let's be clear:  All the extremes are wrong:
 -- Being overly tolerant of errors is wrong.  Ignoring   errors, especially during training, is bad policy.   -- Responding overly harshly to errors is also wrong.
   at 232828.htm
Again I say:  All the extremes are wrong.
 -- Sure, exceptions can be abused.  Any tool can be abused.
 -- Sometimes exceptions are a perfectly reasonable technique.
Example:  Suppose that N layers deep in a library, the
file-open routine throws a file-not-found exception.  A
much higher level knows that the lack of a configuration
file is harmless, indeed routine, and continues using the default configuration.  Using exceptions for things
like this is easier and generally more reliable than checking return status codes, layer upon layer upon layer.
That's the wrong solution anyway.  The 8086 was designed
in the 1970s.  The 1970s have been over for a while now.
Transistors are cheap nowadays, and parallelism is king.
The overflow check should be done in hardware, in parallel
with the arithmetic, without requiring a separate opcode
To say it another way, it's a vicious circle:  Integer overflow trap had a nonzero cost on a PDP11, so the C language didn't specify it, so programmers have no way
of asking for it, so there's no incentive for any
hardware to support it ... for the rest of eternity.
I say that's no excuse.  It should be added to the list of obvious stupidities that have gone unaddressed for far too   Note that it doesn't have to be that way.  By way of   analogy: Lots of PDP11s shipped with no hardware floating
  point, but the C language specifies floating point anyway.
  A decent language should implement things that people   need, even if the hardware imposes some nonzero cost.
Getting rid of the overflow check is an optimization.  Again
I am reminded of Knuth's dictum:   Premature optimization is the root of all evil.
Consider the phrase WYTM (What's Your Threat Model?).
That is often used in this forum, as it should be.
The same concept applies to optimizing compilers:  What's
Your Objective?  Hint: optimizing for speed is not the only possible objective.
It would be nice to have a compiler that optimizes for
reliability.  That includes things like trapping integer
overflow, which is vastly easier for the compiler than
for the ordinary programmer ... since it is usually supported at least partially in hardware, whereas it
is a nightmare to do it by hand, i.e. by performing
arithmetical checks on the arguments before each At the opposite extreme, some compiler-writers claim that the spec says that any misstep, however small, gives them a license to kill.  To them I say Wow, that
is amazingly arrogant.  You are not a team player.  You have not the slightest understanding of reliability.
You should not be allowed to come anywhere near a critical application.
I went into science because I didn't want to go to law school.  Sure, people should read and follow the instructions ... but they shouldn't be required to read the spec with lawyer's eyes, looking for some asinine legalistic pharisaical pettifogging chicanery
that tramples on the spirit of the law and tramples on longstanding tradition.
The rule should be, if you encounter a situation that
is not covered by the spec, or encounter an out-and-out
mistake, do something that makes the situation better
... not worse.
See also Eventus incertus delendus est.

@_date: 2015-10-27 21:49:41
@_author: John Denker 
@_subject: [Cryptography] portable, 
We agree that it is important to recognize the distinction between
development and deployment.
 -- During development, we want to maximize the visibility of
  any bugs.
 -- When the product is deployed, we want to minimize the
  visibility of whatever bugs remain.
On the other hand, I don't agree that compiling with gcc is
"important" as a means of reaching either goal.  As it stands,
we cannot rely on it to conceal bugs ... but neither can
we rely on it to make bugs manifest.  It does silly things not reliably friendly, but it's also not reliably adversarial.
We need a different approach entirely.  During development, we
need a compiler that provides maximally accurate and informative
warnings.  By "accurate" I mean that it should not generate false negatives *or* false positives.
Also, generating silly executable code is never the correct way to warn the user.  During development, the compiler
should emit clear, informative warnings.  If it can't generate good defensive code, it should generate no code
at all, and throw an error.
I'm not convinced that java is the solution.  If you want 2's
complement behavior, you can get it in C by using unsigned ints.
Meanwhile ... I'm not convinced that 2's complement is the solution
to all the world's problems.  The world is a complicated place.
For example, there are lots of GPUs where overflow results in GPUs as crypto accelerators.
More generally, I can imagine a /base language/ that leaves open
various details such as sizeof(int), overflow behavior, etc...
in combination with one or more /dialects/ that specify all the details.  The key here is that the base language should specify that every compiler MUST implement a specific dialect, so that when the rubber meets the road, nothing is left unspecified.
I can also imagine a language that is much more expressive, allowing
the user to specify that variable XXX should exhibit 2's complement
behavior, variable YYY should exhibit trichotomy and monotonic behavior, variable ZZZ should throw an exception on overflow, etc.
There are lots of properties that one might like to have, but
it's impossible to have them all at once.   -- The associative property is compatible with 2's complement   but not with saturation.
 -- The weak monotonic property is compatible with saturation
  but not with 2's complement.
 -- The trichotomy property is not compatible with floating-point NaN.
 -- et cetera.
Eventus incertus delendus est.

@_date: 2015-09-02 09:34:11
@_author: John Denker 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
Using the language of cryptanalysis, we can look at this as
a /dictionary attack/ ... in this case a white-hat dictionary
attack ... but in any case it boils down to populating the dictionary with the appropriate entries.
Existing password dictionaries were compiled largely by
observation, e.g. by stealing passwords from vulnerable
sites and adding them to the dictionary.
One could imagine doing something analogous for keys.  On
some live system(s), escrow each key for a while.  Any key that gets re-used within a week gets added to the dictionary;
all others get expunged.
Keep track of the hit-count for each item in the dictionary.
You could have regular expressions as entries in the dictionary,
perhaps in a separate volume of the dictionary.

@_date: 2015-09-24 12:36:42
@_author: John Denker 
@_subject: [Cryptography] VW/EPA tests as crypto protocols ? 
This topic touches on crypto only tangentially;  see last two
paragraphs below.
In the VW situation, the correct regulatory protocol is simple:
Stick a probe up the tailpipe and then go for a drive under real-world conditions.
The same idea applies to footballs:  Write the regulations in
such a way that the team is unconditionally responsible for
ensuring that the balls are inflated within the proper range
at all times /during the game/.
The general principle here is simple:     *Measure the thing you care about.*
To say the same thing the other way:  Avoid measuring something
that is only a proxy for the thing you care about.  As soon as
you start rewarding and/or regulating the proxy, it ceases to
be a reliable measure.  In this double-negative form it is known as Goodheart's law:
  To be sure, a proxy is often better than nothing, and may be a valuable hint, for instance when you are dealing with
potentially catastrophic but hopefully rare events.  Example:
you want to regulate the strength of nuclear power plant
components, airliner components, deep-sea oil well components,
et cetera.  However, you still *must* hold the operator accountable for bottom-line results including bottom-line safety.  Satisfying the proxy requirements is necessary
but never sufficient.
As for the VW story, so far as things stand today, it cannot
(yet) be considered a regulatory failure.  In theory, the
statutory penalties for non-compliance are large enough to ensure that VW will not profit from their wrongdoing.  About
1/3rd of VW's market cap was wiped out overnight, about 25
billion dollars.  OTOH if VW somehow manages to weasel out of the penalties, *then* it will be a regulatory failure.
  Example:  When a bank is "too big to fail" and gets bailed
  out by the US government, that's a gross regulatory failure.
  Example:  When TEPCO gets bailed out by the Japanese
  government, that's a gross regulatory failure.
  Example:  When the government enacts limits on liability
  for actual damages, you know it's a regulatory failure.
  Contrasting example:  When the government allows punitive
  damages out of all proportion to actual damages, its a
  failure in the other direction.
Companies -- and individuals -- will pull dirty tricks just
so long as they think they can benefit from doing so.  Crypto
can play a small role in /detecting/ dirty tricks, but that's
a relatively minor role, and we already have decent protocols
for that, e.g. source-code management with layers of sign-offs,
et cetera.  Nobody wants to be identified as the guy who wrote
or signed off on the air-quality-defeat code.
Insofar as VW didn't implement proper controls, and tolerated
(or rewarded) people who did things that were not in the long-
term best interests of the company, it's a management failure.
I don't see it as a crypto failure, just a plain old-fashioned
management failure.  Such failures are more common than they should be.

@_date: 2016-04-05 13:01:05
@_author: John Denker 
@_subject: [Cryptography] connectivity without complete loss of privacy 
That seems like it ought to be a fixable problem.
A lot of people seem to think that tor works, or could be made
to work somehow.  Let's accept that as a working hypothesis, at
least temporarily.
Then it would seem that voice over IP over tor should allow me
to move around and communicate without being tracked.
I'm not saying that's optimal, but it may serve as an existence
proof and a conversation starter.
Does that work?  Could it be made to work?  If not, why not?

@_date: 2016-04-18 11:48:15
@_author: John Denker 
@_subject: [Cryptography] How to get certificates on email server? 
:| openssl s_client -connect av8n.com:465
Or for even more detail:
 :| openssl s_client -debug -tlsextdebug -showcerts -connect av8n.com:465
On some old buggy versions the following works better:
 :| openssl s_client -CApath /etc/ssl/certs -connect av8n.com:465
Port numbers of interest include
  urd             465/tcp         ssmtp smtps     # URL Rendesvous Directory for SSM
  submission      587/tcp                         # Submission [RFC4409]
  https           443/tcp                         # http protocol over TLS/SSL

@_date: 2016-08-10 12:29:08
@_author: John Denker 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
In the context of:
Keep in mind that the defense is at best weak.  It makes things
tamper-evident, not tamper-proof.
Making things tamper-evident opens you up to a large class
of DoS attacks.  All somebody needs to do is break your
tamper-evident seals;  then what are you going to do?
  Specifically:  Suppose the voting machines arrive at the
  polling place with their seals broken.  Suppose this
  affects selected precincts but not others.  Then what
  are you going to do?  Cancel the election?  All these ideas are not new.
  For example:  In the years leading up to WWII, the Japanese
  government required all overseas telegrams to be delayed by
  arbitrary amounts, to help defeat timing-based defenses.
  Reference:  Kahn.

@_date: 2016-08-16 21:17:56
@_author: John Denker 
@_subject: [Cryptography] Shadow Brokers :: powerful NSA hacking tools leaked 
Ellen Nakashima
  Gomorrah Post, August 16, 2016
    The original instructions for obtaining the files are at
  but dereferencing the pointers leads to a lot of disabled pages.
That is IMHO unfortunate.  Surely quite a few bad guys have already
grabbed copies of the tools.  The next step is for the good guys to
figure out how to defend against the attacks.
IANAL, and the NSA folks consider themselves above the law anyway,
but I am reminded of the doctrine of /strict liability/.
  It looks to me like the NSA's pet lion got away.
Maybe they will exhibit some sense of responsibility and common
decency, and immediately release patches so everybody can defend
against the depredations of their escaped lion.

@_date: 2016-08-29 12:18:30
@_author: John Denker 
@_subject: [Cryptography] foreign hacks of state election systems 
The manure has already hit the ventilator.  This is not new, but
the public is becoming newly aware of it.
I reckon this is an important application for tamper-resistant
hardware ... and for high-quality top-to-bottom security, including
Quoting from:
  Ellen Nakashima,
  Gomorrah Post, August 29, 2016
  "FBI is investigating foreign hacks of state election systems"
  That refers to:
  Mary Jo Pitzl
  azcentral.com, June 30, 2016
  "FBI concerns shut down parts of secretary of state's elections site"

@_date: 2016-12-02 17:51:26
@_author: John Denker 
@_subject: [Cryptography] OpenSSL and random 
I stand by that.
Yes.  That's what users need.  We need to keep focused on that.
That's the right question.
That's mostly the same question.  Openssl and a bazillion other users
all face similar problems.
The answer has several ingredients:
In the short run, openssl and everybody else need to publish some
disclaimers, perhaps:
Like everybody else, openssl needs a RNG that never blocks and never
produces low-quality randomness.  That is an achievable goal, and
AFAICT the only reasonable goal.
In the short term, on linux platforms, that implies all of the
  a) use /dev/urandom, because it never blocks, *and*
  b) take strong measures to provide it with a proper seed, *and*
  c) keep in mind that sometimes proper seeding is impossible, and
   improper seeding is undetectable, and either way the system is
   not secure.
In the slightly-longer term, the work of Ard Biesheuvel is important,
since grub provides a way -- one of the few decent ways -- of providing
a sufficiently-early sufficiently-random seed.  This approach should
work on the vast, vast majority of laptops, desktops, and server-rack-
class machines.  Certain other high-volume platforms e.g. Mac and
Android are deployed by control freaks, who will have to take full
responsibility for providing suitable randomness.
Returning to the short term, if you can't for the desired grub + kernel
features, you can attempt to stir /dev/urandom yourself.  The two
easiest ways are
 a) Sneakernet.
 b) Using a hardware source.  In most cases the audio system is
  far and away the best quality and least hassle.
Both of these options are discussed in some detail near the top of
  That is so non-quantitative that there is no way of knowing whether it
is completely right or completely wrong.  Until somebody provides a
hard upper bound on how long initialization will take, and a hard lower
bound on how long users are willing to wait, statements of the form [1]
are useless for real-world engineering.
That is not a solution.  Users really need the randomness.  If they don't
get it from the platform, they will not wait around.  They just won't,
as Rich and Peter and others have pointed out.  Instead they will cobble
up some other source of randomness, with predictably terrible results.
People seem to be assuming, implicitly or sometimes explicitly, that
every problem in the world either has a software solution or has no
solution at all.  Well, that's just not true.  A random distribution
over numbers *can* be produced ... just not by software.  John von
Neumann was right.  The sooner everybody comes to terms with this,
the better off we will all be.

@_date: 2016-12-20 16:15:20
@_author: John Denker 
@_subject: [Cryptography] grand principles and niggling details 
I really don't think so.
Let me offer a different list of grand principles:
  Principle   In the security business, there are very few grand
   principles.  Mostly what we have is a boatload of niggling details.
  Principle  The details matter.  You have to get the details right.
In other words, it doesn't do much good to put four or five deadbolts
on the front door, if the back door and side window are standing open.
As a corollary:  Security is hard, and will always be hard.  The
defender needs to block every avenue of attack, whereas the attacker
only needs to succeed at one.
When viewed long-term, from the 100,000-foot level, that is arguably
kinda almost true.  Entropy is the wrong word and the wrong concept,
but let's not get into that right how.  More importantly, as Rich Salz
and others keep asking, what are we supposed to do when we don't have
both of those things?  Just give up?
I say that is an excellent question, and No, usually we should not
just give up.  There is no single "silver bullet" solution, but there
are ways of handling most of the cases that come up, case by case.
   On the other hand, here are /some/ platforms that are not secure
   and cannot be made secure.  We should not give up on the overall
   goal, namely security, but sometimes we have to give up on certain
   petty subgoals (such as certain brain-dead platforms).  This is
   basic maze-running strategy:  Back out of the dead end so you can
   make progress toward the overall goal.
              Here's a particular case:  Consider booting a system, called the
case where we want to install an operating system on a brand-new
target machine.  It also includes maintenance situations, and
other situations as well.
1)  Observe that nobody actually uses CDs for this purpose anymore.
 For obvious reasons, they use USB flash drives.  You can get a
 brand-name 16GB thumb drive for about 6 bucks (with free shipping).
 The .iso image takes up less than 2 GB.
2) When I am working from a "Live" .iso image, I need more than
 just that.  I need various public keys, at least one private key
 (possibly highly restricted), dozens of useful little scripts, and
 many pages of notes on what to do and how best to do it.
3) Combining the two previous ideas:  When creating the bootable drive,
 it makes sense to partition it.  This commonly requires munging the
 .iso image so it can handle booting from a partitioned drive.  This
 is just another of the niggling details.
    The .iso image occupies the first partition.  On the second partition
 I put a file called carepackage.tgz containing my keys, scripts, notes,
 et cetera.  In other words, I use sneakernet to schlep files from my
 normal well-configured machine to the target machine.
*At this point I have done zero extra work in the name of randomness.*
 This is all stuff I needed to do anyway.
4) The script that creates the carepackage also does this:
      :; ( echo -en "\037\037"          # magic number; see file(1)
        /bin/dd if=/dev/urandom bs=510 count=1 status=none
      ) > ${sneaker}/random.seed
 You can think of this as like passing sourdough starter from someone
 who already has it to someone who needs it.  You don't need to create
 it from scratch.
5) I boot up the system in "Live" mode (as opposed to installer mode).
 At this point it has already mounted the first and second partitions
 on the boot drive.  That means it is super-easy for me to cat the
 random.seed file into /dev/random.
 At this point the Live system has not installed anything.  It has
 not cut any cryptographic keys.  It has not made any HTTPS or SSH
 connections.  It has not even mounted the main disk on the target
 machine.
      It has done some ASLR using some very dubious randomness,
      but I'm not gonna worry about that too much.
 One of the first things I do with the newly-initialized RNG is to
 rewrite the random.seed file on the flash drive.  That way, even
 if I have to re-use the drive immediately, there will be less risk
 of replay attacks.  6) Now I can switch to "Installer" mode if desired.  When ssh and
 openssl are installed, the generated keys should be good enough ...
 certainly very much better than if the RNG had not been initialized.
 Similarly secure network connections can be established using decent
 nonces.
A) One weak point here is step 5.  If Joe Luser skips this step and
fires up the installer directly, the random.seed file is not put to
good use.
Suggestion:  Each distro should put a random.seed file in the .iso
image, near the top, outside of the squashfs system.  There is no
point in squashing random data anyway, and putting it outside makes
it easy to rewrite.  The isohybrid step already munges the .iso
image to make it bootable.  It would be very little extra work to
randomize the random.seed file, so that every "Live" drive on earth
has its own private, high-quality seed.
Another possibility:  The Live system could notice the random.seed
file sitting there on the second drive, and stir it into /dev/random,
without requiring any effort from the user.
B) The excellent grub-related ideas put forth by Ard Biesheuvel and
others don't solve the entire problem, because the "Live" flash drive
ordinarily doesn't use grub.  Instead there is a simple 440-byte
chainloader that gets stuffed into block 0.  This is another of
those niggling details.
C) Another weak point, related to item (A), is that the installer
does not bestow any randomness on the newly-installed system AFAICT.
This would be super-easy to fix.  The installer should use its own
RNG to randomize the /var/lib/urandom/random-seed file.
D) Another weak point is that you have to guard the flash drive from
now until the end of time, unless you destroy it by grinding or by
hot chemicals:
  That's because there is currently no such thing as "secure erase" on
typical flash memory chips.  Every past version of the random.seed
file is probably still there somewhere, and not overwhelmingly hard
for a determined attacker to find.  This is another of those niggling
details.  This is an entirely fixable problem.  One hopes that some
day soon the folks who write the drive firmware will start taking
security seriously.
Still, though, think about what we have accomplished with the sourdough
approach aka the sneakernet approach.  The bad guys can no longer sit
in their comfy chairs and succeed with a network-based attack on the
non-very-random number generator; instead they have to conduct a
black-bag job, to get the flash drive.  This greatly increases their
cost, including their risk of getting caught.  I reckon for a vast segment of the market, this is good enough.  Others
may need higher security, but they presumably have a bigger budget, and
can afford to keep the flash drive locked up when not in use, and grind
it or barbecue it before disposal.

@_date: 2016-12-20 21:20:48
@_author: John Denker 
@_subject: [Cryptography] grand principles and niggling details 
Here's another principle that all-too-often gets overlooked:
3) There is no such thing as a random number.
  You can have a random /distribution/ over numbers, but then the
  randomness is in the distribution, not in any number that may
  have been drawn from such a distribution.
  A distribution is different from a number in the way that an infinite-
  dimensional vector is different from a scalar.  It's a significant
  difference.
  You can also have a distribution over shapes, or colors, or other
  non-numerical things.
This has been well understood for more than 80 years\1,2,3\.  I trust
that most people on this list understand this, but more generally it
is astounding how many people didn't get the memo.
BTW:  I don't mind when people speak of a Random Number Generator, so
long as they diagram the sentence correctly:  It's a random generator
of numbers, not a generator of random numbers.  It might be better to
call it a Randomness Generator or Random Distribution Generator.
4) There are some problems that cannot be solved in software ... yet
 can be readily solved by other means.
 Randomness is the poster child for this category of problems.  State
 of sin and all that.\3\
Here's a more prosaic example:  According to the HD Audio specification,
the plug-sense circuitry is supposed to be independent of the audio
signal circuitry.  (This differs from the old AC'97 specification, where
plug-sense was accomplished by shorting the audio signal to ground.)
In particular, HD Audio is supposed to be highly software-configurable,
up to and including reconfiguring input jacks to become output jacks.
In the real world, however, when dealing with security and so many other
things, what *is* differs from what is supposed to be.  Apparently some
super-genius decided it would be fun to make a combination audio panel
module, such that pulling the plug activates the plug-sense wire *and*
shorts the signal wire to ground.  I kid thee not.  I have seen this on
laptops from otherwise-competent manufacturers.
pin is shorted to ground in hardware, there is nothing the software
can do about it.
The problem is however easy to solve at the hardware level.  My local
Ace Hardware store sells 1/8" diameter wooden dowel rods, 36" long, for
59 cents.  You can get the same thing at the hobby/model store, for a
slightly higher price.  One such rod is enough to make plugs that will
open-circuit the Mic input on several dozen computers.
Again the point is:  Sometimes a couple pennies' worth of hardware
will solve a problem that simply cannot be solved in software.
ON THE OTHER HAND ... these days, more and more machines do a decent
job of complying with the spirit of the HD Audio specification.  That
means if you ask nicely, you can record from the Mic input even when
nothing is plugged in.  So yes, /sometimes/ problems can be solved
in software.  You have to cook up a .fw file to put in /lib/firmware,
and cook up a .conf file to put in /etc/modprobe.d/, then you have
to unload and reload the hda-intel module.  It's kinda mysterious at
first, but it's permanent and works nicely.  You're aiming for this:
which gives you much finer control over the audio hardware, as
documented here:
  This means you might not need to spend 59 on a dowel rod, and
confers other benefits as well.
The key word there is provided.  That's quite some proviso.  Is
there an easy way to obtain an accurate lower bound?  If so,
please explain.
Turbid is something like 35,000 lines of code at the moment, of which
only a tiny percentage has to do with actually collecting randomness.
The rest has to do with characterizing the hardware, i.e. trying to
obtain a decent bound on the amount of adamance in the raw signal.  I
don't know how to do that automatically.  Turbid does however offer
some powerful tools that an expert can use.  The task is difficult for
an expert, and well-nigh impossible for a non-expert, although I'm
trying to cook up a GUI and a set of procedures that will make it not
quite so impossible.  As for accuracy, turbid doesn't need a tight
lower bound;  all it needs is a number that is low enough to be a
genuine lower bound yet high enough to provide decent throughput.
I don't think that should be the baseline recommendation.  That
might make sense in an unusual emergency situation.  Usually it
is better to _avoid_ emergency situations.  In particular, the
sneakernet / sourdough method mentioned on 12/20/2016 04:15 PM
is generally more convenient and more reliable, at least in the
situations I tend to run into ... and more suitable for becoming
a built-in feature on standard systems.
Also as mentioned on 12/20/2016 04:15 PM, the details matter.  In
particular, it matters what parameters you pass to the recording
Here's a constructive suggestion:  In an emergency, try either:
        :; sox -r 48000 -b 32 -t alsa hw:0  -c 1 noise.wav  trim .1 3
        :; arecord -D hw:0 --disable-softvol -f S32_LE -r 48000 -V mono -d 3 noise.wav
        :; aplay noise.wav
        :; od -t x4 noise.wav
You may be wondering, why so complicated?   Why not just
Well, as I said earlier, you have to get the details right.  A
list of pithy axioms is nice, but it diverts attention from the
other 99% of what you need to know.
 -- If you leave off the -r 48000, you are likely to get 8000
  frames per second, which costs you a factor of 6 in randomness
  productivity.  On the other hand, recording at a much higher
  rate wouldn't buy you much, because the noise bandwidth is
  limited by the $RC$ time constant of the input network.  I
  have a machine that will happily do 192000 frames per second.
  That's more samples, but it's not more randomness.  These are
  just a couple of the grotty details you need to know.
 -- If you leave off the -f S32_LE, you get 16-bit words, which
  is a disaster, because there is a great deal of randomness in
  the lower-order bits.  I have a desktop that gives 25 bits, and
  a laptop that gives 'only' 23 bits when digitizing the Mic-in
  signal.  You don't want to be throwing away 7 or 9 bits of
  randomness per sample.
 -- If you leave off the -D hw:0, it is quite possible that the
  'default' alsa device screws up the signal, e.g. by truncating
  it to 16 bits, even though the bits are delivered in a 32-bit
  word.  Don't ask me why it does this.  Turbid is careful to ask
  for the raw hardware device, but when you're using arecord from
  the command line, a lot of things can go wrong.
 -- If you leave off the --disable-softvol, you might get all zeros
  in the output file.  Don't ask me what arecord thinks it's doing.
 -- Overall sox is a better program, simultaneously easier to use
  (for me, anyway) and much more capable.  However I mention the
  corresponding arecord command also, because in a "Live CD"
  environment you might have arecord available but not sox.
 -- If you don't check your work, using at least od and aplay, all
  kinds of bad things could be happening and you wouldn't know.
  In a non-emergency situation, I would insist on doing a lot
  more checks than that, including impedance, gain, bandwidth,
  quantization, etc.
\1\  Andrey Nikolaevich Kolmogorov,
     _Grundbegriffe der Wahrscheinlichkeitsrechnung_ (1933)
     aka _Foundations of the Theory of Probability_
     \2\ For a relatively gentle, undergraduate-level introduction, see:
     Tom M. Apostol,
     _Calculus_
     2 volumes, Wiley (2nd. ed. 1962)
\3\  John von Neumann,
     Various Techniques Used in Connection With Random Digits
     page 36 in _Monte Carlo Method_
     proceedings of a symposium June 29  July, 1, 1949
     A.S. Householder, G.E. Forsythe, and H.H. Germond (eds.)
     Institute for Numerical Analysis (published 1951)

@_date: 2016-12-26 20:25:32
@_author: John Denker 
@_subject: [Cryptography] where shall we put the random-seed? 
This is in some sense a progress report, indicating how far I
have gotten toward answering the questions that Rich Salz and
others have been asking about how to provide practical, usable
AFAICT the only winning strategy is to store a random-seed file
somewhere, make sure it is properly initialized, make sure it
is properly protected, and make sure it gets loaded very, very
early.  This raises a number of questions, including:
   If it's not obvious why I'm asking, see the background <*>
   section below.
I think I know the right answer in some cases, but not in others.
Let's proceed with a case-by-case analysis:
1) On an ordinary full-featured desktop, laptop, or server system,
the obvious choice is
    /var/lib/systemd/random-seed  (for recent Ubuntu systems), and
    /var/lib/urandom/random-seed  (for everybody else)
That is where the system startup and shutdown scripts expect to find
it.  The plan is to teach grub to look for it there, and to pass it
to the kernel, so that it is available from time t=0 onwards during
the boot-up process.
Open issues include:
1a) Obviously it is imperative that the random-seed file be properly
initialized, even on newly-installed systems that are about to boot
up for the first time.
  ++ The Debian installer does initialize the random-seed.  Kudos!
  -- The Ubuntu installer does not.  I filed a bug report:
             It will be interesting to see what comes of this.
1b) Initialization of the newly-installed system assumes that the
installer system itself had some decent randomness to give out.
This is "slightly" less critical in the sense that the installer
might be able to gather some randomness from a HRNG, using time
and hardware resources that might not be available to the installee
at the time when its RNG is most needed.
2) Let's consider a typical "Live CD" setup.  Note that "Live"
image and "Installer" image are essentially synonymous these days.
In particular, let's start by imagining it embodied on an actual
CDrom.  In my part of the world, such things are about as scarce
and impractical as penny-farthing bicycles.  However, the major
distros seem committed to supporting this format, so let's play
along and see what happens.
For now we assume no persistent storage.  Persistence schemes
will be discussed later.
The distributed image actually contains three filesystems:  The
top-level ISO-9660 filesystem, the initrd.lz, and the squashfs
The only reasonable usage scenario I have been able to think of
goes like this:  The user
  a) Downloads the image from the distro site in the usual way.
  b) Runs a very simple script that randomizes the random-seed
   file in the image.
  c) Burns this one-of-a-kind image onto the physical CD.
It does no good to download a "Live CD" image if it has a built-in
secret that can't be modified.  As Ben Franklin pointed out, a
shared secret is no secret at all.  On the other hand, this scheme
requires "some" random-seed file to be present in the downloaded
image, as a plain file of the appropriate length (or longer) in
the top-level ISO-9660 filesystem.  Rationale: such a file can
easily be updated in place, in the image as it sits on the hard
disk, just prior to being burned to CD.  In contrast, a file
inside the squashfs would be prohibitively inconvenient for
Joe First-Time User to update.
To protect against replay attacks, the device driver stirs the
seed using the real-time clock.  The point is, a RTC (or even
a counter) stirred into a 512-byte random seed is a lot harder
to crack than either one separately.
2a) Open issue:  On hardware without a RTC, I'm not sure what
to do.  Some such systems may end up on the list of platforms
that are not secure and cannot be made secure.
2b) The system init scripts need to be taught to look for the
file on the outer ISO-9660 system.  It's readily accessible
at runtime via /cdrom.
2c) Or perhaps some more specialized script could append bits
from the CDrom file onto the end of the file normal
location.  There are two issues here: we need the specialized
append script itself, and we need the init script to be smart
enough to use the *entire* initialization file.  Right now
some versions only look at the first 512 bytes -- don't ask
me why -- so appending stuff to the end wouldn't do any good.
Question:  Does anybody have a preference for (2c) over (2b)?
My guess is that (2b) is cleaner;  fewer moving parts.
2d) One more issue:  You have to guard the CD from now until
the end of time.  You don't want the bad guys to learn the
random-seed that you used.
3) Now consider a "Live CD" image embodied on a flash drive.
This requires some sort of chain loader.
If we don't do anything fancy, such a setup has no built-in
notion of persistence, so it reduces to the case previously
considered, with one major exception:  The flash hardware does
actually permit persistence.  It is physically possible to
update the random-seed file from time to time, whether the
software realizes it or not.
  ++ The "Live" system could do this by writing to the device
   where the ISO image is sitting, updating the file in place.
  ++ If necessary, the flash drive can be carried to another
   machine, so that the random-seed can be updated that way.
In such a situation, putting the random-seed file in the
top-level ISO filesystem is the only thing that makes sense
Open issues include all the issues mentioned in item (2).
In addition, we need the distribution to include a script
that will update the random-seed.  The existing scripts
have no clue about this;  they tend to assume that anything
they need to do can be done within the normal filesystem.
On the other hand, it may be that some people actually want
(or think they want) absolutely no persistence, even on a
drive that would permit some.  Would anybody actually object
to a persistent random-seed?  Should we make it optional?
Note that it's hard to configure options on a read-only
4) It is possible to have a flash drive with a "Live CD" image
in one partition, plus an additional "persistence partition"
that gets mounted, as part of a unionfs, at the root of the
"Live" system.  This is supported by Ubuntu;  I don't know
about the other distros.  This is similar to the file structure
used in a lot of network appliances and IoT objects.
As far as the software is concerned, the result looks essentially
like a normal system [case (1) above].
One issue is that if you're not careful, such a setup could
cause a lot of wear and tear on the flash medium.   Other than
that, AFAICT there "should" be no new security issues.  Even
so, it's hard to make anything foolproof, because fools are so
5) Here's another scheme:  An ISO image on flash requires
a chain-loader.  It could be a very simple 440-byte thing
stuck into block 0 of the drive, or it could be something
very much fancier, such as grub.
If we assume grub is present, we can put the random-seed file
almost anywhere, e.g. in a tiny security-only persistence
partition.  This is in some ways simpler than imposing
persistence on the entire filesystem from the root on down
... and less likely to cause wear and tear.
Open issue:  It would be nice to standardize the location
for such a thing, and then teach the system init and shutdown
scripts to update the random-seed there.
Questions:  Does any of this make sense?  Does any of it
not make sense?  Does anybody have any better ideas?  Are
there any other use-cases that need special consideration?
<*> Background, Motivation, and Philosophy <*>
Here are some more principles that all-too-often get left off
the lists people make:
 *) Three may keep a Secret, if two of them are dead.    In other words, a shared secret is no secret at all.  If
   multiple system have the same random-seed, it just creates
   an incentive for the bad guys to steal one of the systems
   and lift the seed.
 *) Having a good source of randomly-distributed numbers is
   important.
   If you don't believe me, just look at the enormous efforts
   the NSA put into subverting the Dual_EC_DRBG.  They wouldn't
   have bothered, if it wasn't going to open up important lines
   of attack.
 *) The RNG that is offered to ordinary users must never block and
   must never return low-quality results.
   To say the same thing another way:  If you find yourself worrying
   over the design decision as to whether to block or whether to return
   not-very-random results, you have already lost the game.  Go back
   to the drawing board and fix it so that question never comes up.
   The typical linux /dev/random doesn't meet this criterion, because it
   very commonly blocks.  The typical linux /dev/urandom does not (at
   present) meet this criterion, because it is a PRNG and there is no
   guarantee that it has been properly initialized.  The new getrandom()
   function adopts the lose/lose strategy of requiring the user to answer
   a question that cannot be answered and should never have been asked.
And that leads us to the grandest principle of all.  As previously
 *) The grand principles don't do you a bit of good unless you take
   care of a whole lot of grotty details.
   I am reminded of the famous question:
     What's the difference between theory and practice?
   To which the answer is:
     In theory there is no difference, but in practice there is.
   In that spirit I would say that in theory it doesn't make much
   difference where we store the random-seed, but in practice it does.

@_date: 2016-12-27 15:32:16
@_author: John Denker 
@_subject: [Cryptography] where shall we put the random-seed? 
I have a suggestion that is architecture-independent and far simpler
than many of the other things that have been proposed.
The kernel image is already a fancy thing with internal structural
blocks.  So let's add another block, 512 bytes long, reserved for
a random seed.  Any platform that can read the image at all will
automatically bring the seed along.  The seed is available to the
RNG from time t=0 onwards.
To refresh the seed, read the System.map to see where it sits.  Or
add a pointer, at some fixed offset early in the file, pointing
to the seed.  (A few similar things already exist.)
The seed is excluded from the bzImage checksum computation.  It is
also excluded from the compression/decompression.
It will take some expertise to implement this, but even so, this
has a lot fewer moving parts than other schemes that have been
proposed.  It is more localized, with fewer dependencies and fewer
interfaces.  It does not require grub, or UEFI, or x86, or ARM,
or anything else beyond a BIOS that can read the boot image in
the usual way.
As a related benefit, the vmlinuz file is accessible at the top
level of a ISO-9660 image, without any additional layers of
compression or obfuscation, making it easy to refresh.
Yes.  For years.  Building the seed into the boot image will
put a stop to that, once and for all.
Given proper care and feeding of the seed, this upholds the
principle that the RNG must never block and must always return
good randomly-distributed results.
This is an industrial-strength solution to a serious problem.
I completely agree with the goal:  We want to encourage users
to be meticulous in general, and about checksums in particular.
However, when I do the analysis, I come to a slightly different
conclusion about how best to promote that goal.
AFAICT the simplest approach is to follow the sequence:
  a) Download the ISO image to hard disk.
  b) Verify the checksum.
  c) Refresh the seed.
     If the old (downloaded) value is nonzero, save it somewhere.
  d) Burn to CDrom (if that's what you want)
Verifying before reseeding solves the problem with zero additional
time or effort.
On rare occasions, if somebody wants to verify the checksum ex
postfacto, they can restore the seed to its as-downloaded value
and then compute the checksum in the usual way.
This applies to a seed stored anywhere within the ISO filesystem,
whether in its own file or tucked inside the kernel image.
  In contrast, sticking the seed outside the ISO-9660 filesystem
  is not super-terrible, but it has no real advantages.  The user
  cannot checksum the entire file containing the combination,
  but instead needs a tool that will delve into the ISO-9660
  structure to find its size, and checksum only that much.  The
  required tool is no simpler than the one that finds the seed
  file and updates it in place.
  What's worse, the bits appended to the end will need some kind
  of structure.  (An unstructured pile of bits is not the sort of
  thing we ought to encourage.)  Suppose some day we want to append
  /two/ things.  Which is which?  How big are they?  Very soon this
  turns into a way of /partitioning/ the medium, with an ISO-9660
  partition followed by something else.  If we're going to do that,
  we ought to do it right.
Using files /within/ the ISO system is simple, structured, standardized,
guaranteed to work, and easy to document.

@_date: 2016-12-28 12:13:30
@_author: John Denker 
@_subject: [Cryptography] where shall we put the random-seed? 
Excellent.  I completely agree.  Either in a file of its own in
What does it take to light a fire under this ice-tray?
That's irrelevant.  Of course you can't find it there /now/, because
it doesn't exist yet.  It hasn't been implemented yet.
The relevant point is, as part of the implementation, you write the
location to the System.map file.  Record multiple locations if need
be, since it has to be findable before /and/ after decompression.
But this is the second- or third-best scheme anyway, in the spirit
of belt and suspenders and crazy glue.  The primary scheme should
be to put the block (or a pointer thereto) in a standard location,
so that the image can be parsed even if it gets separated from the
System.map file.
Also irrelevant.  As previously discussed, the random-seed block is
exempt from compression.  It has to be, for multiple reasons.  Also
there is not the slightest advantage in trying to compress it.
I am confident that a programmer of ordinary ability can figure out
how to implement this.
So distribute a new version of the rpm thingy that knows how to
parse the new format.  There's a word for this, it's called "work".
I know that's a four-letter word, but it's not actually a cussword.
What's the alternative?  Are people going to keep sitting around,
wringing their hands and sucking their thumbs, waiting for a
solution that can be implemented with no work whatsoever?
That's also irrelevant.  Voting machines don't boot from a one-size-
fits-all Live CD.
Anybody who is building a voting machine that uses TPM can jolly well
use a processor that has a built-in RDRAND instruction that's secure
enough to use for early applications such as kernel ASLR.  Later, more
demanding applications of randomness can rely on a seed read from
disk in the usual way, plus some well-understood well-engineered
well-documented HRNG.  If they're not willing to do that, they are
dangerously unserious.
Defining a random-seed block within the kernel image doesn't mean
that everybody is required to use it to the exclusion of all else.
UEFI secure boot is a legal, technical, and ethical cesspool, for
reasons having nothing to do with me and nothing to do with where
we put the random-seed.  I did not create this cesspool and I will
not accept responsibility for cleaning it up.
I might also add that the idea of a seed within the kernel image
was proposed 6+ years ago.  That's a lot of dog years.  If the
idea had been adopted then, there would have been plenty of time
to incorporate it into the UEFI specs and the rpm code, including
the idea that some blocks are exempt from the checksums and

@_date: 2016-02-20 13:47:38
@_author: John Denker 
@_subject: [Cryptography] Apple 3rd Party dilemma 
Let me add another +1 to that.
Here is the analogy that I would "like" to hold in some ideal
1) Igor fills a scrapbook with photos and postcards. (first party)
2) The county government provided the scrapbook to Igor for
  him to use. (second party)
3) Joe owns a shop that specializes in selling custom trophies,
 T-shirts, postcards, and scrapbooks. (third party)  He sold
 the scrapbook in question to the county.
4) Ferris supplies Igor with his favorite thing to put into
 the scrapbook, namely pictures of goats.  (fourth party)
5) The FBI comes to Joe and says you are the third party.  You
 remember the scrapbook that you sold to the second party.
 We want you to seize it from the first party so we can  identify the fourth party, who is unknown to us but might
 be involved in kid porn.
 Joe replies:  You are out of your mind.  It's just a book.
 I'm in the printing business, not the search-and-seizure
 business.  I have never met this third party you speak of,
 and even if I had, I would have no control over what he  puts in his scrapbook.  It's not my book and it's not my
 information.
I'm not saying this model is relevant to the current situation,
but it seems like something we could strive towards, for next
time.  And there /will/ be a next time.
An excellent point.  So most people are in a situation where
they lose their basic rights because they want to live in the
modern world, and they can't do that without interacting with
third parties.
    "Thus no one was allowed to buy or sell things
     unless he bore the mark of the beast"

@_date: 2016-01-03 11:00:31
@_author: John Denker 
@_subject: [Cryptography] Alice, Bob, Eve, Mallory, Maxwell ??? 
This is a nice fun thread.
Works for me.
  The technical meaning of such names always has to be explained
  on first use.  The cultural reference exists at a second level,
  for style points and mnemonic value, so it's harmless if the   reader doesn't get it until later.
There are a number of possibilities in the traitor / collaborator /
turncoat category.  Some are open to multiple interpretations:
  Antenor
  Benedict
  Brutus
  Cain
  Delilah
  Ephialtes
  Jafar
  Judas
  Malinche
  Mata
  Vidkun
There are others who infiltrate from outside and only pretend to insider status, e.g. parasite / special operations / undercover /
double agent / naughty guest:
  Cowbird
  Donnie
  Judith
  Molothrus
There are also naughty hosts:
  Ptolemy
  Sweeney
  Yael
In the naughty vendor category we have:
  Mrs. Lovett
If we're going to allow last names, then counterfeiters with
distinctive names include:
  Mr. Arnaouti
  Mr. Bourassa
  Mr. DeBardeleben
  Mr. Talton

@_date: 2016-01-04 10:12:18
@_author: John Denker 
@_subject: [Cryptography] 256 bit key  +  12  digit PIN 
Actually, there is a way in which this "could" make a certain
amount of sense, hypothetically speaking.
Executive summary:  256-bit encryption "could" be used to address
the end-of-life disposal problem.  The PIN serves a different Key idea:  The threat model contains several different sub-models.
1) Consider the PIN that goes along with a debit card.  If you
 lose the card, the PIN provides some modest protection against
 casual unauthorized use.
 OTOH if you get mugged at the ATM, the mugger will beat the
 PIN out of you.  Making it longer would not help.
 So it is with the PIN on your encrypted disk.  The PIN protects
 against low-grade casual attacks.  It does not protect against
 an Advanced Persistent Threat.  The really bad guys are going
 to beat the PIN out of you, and making it longer would not help.
    2) Consider the end-of-life problem.  There are lots of ways the
 disk (SSD or otherwise) could fail in such a way as to make the
 mass storage medium unwriteable.  It might well remain readable,
 especially if the attacker is willing to apply some extra effort.
 If there is full-disk encryption, if you trust the implementation,
 it suffices to destroy the key.  I'm not saying that I trust any
 disk vendors to implement it properly, but let's temporarily and
 hypothetically suppose that they did, as follows:
  *) Each controller is provided with a good HRNG.
  *) On first use, the controller chip generates a unique 256-bit
   salt, and stores it on-chip.
  *) The salt value never leaves the chip.  There is no opportunity
   for the manufacturer to escrow the value.
     -- The chip will however tell you the SHA-256 of the salt,       so you can verify that you aren't getting stuck with some       standard value.
     -- Better yet, the device will allow you to read the raw
      undecrypted data from the disk, so you can verify that
      it was in fact encrypted.
  *) There is also a way to generate a new salt, in the field,
   using an on-chip HRNG and/or a long string of external data,
   much longer than the PIN.
  *) The salt is combined with the PIN to encrypt the disk.
  *) When it comes time to dispose of the disk, while maintaining
   privacy:
   -- In favorable cases, it suffices to electronically obliterate
    the salt stored on the controller chip.  Easy peasy.
   -- In the unlikely event that it was the controller that failed
   (rather than the mass storage medium), you have to grind away    the circuitry from the controller die.  This is easier than
   grinding up all of the mass storage.  Big win.
  *) At this point, the bad guys have nothing to gain by beating
   the PIN out of you.  They may torture you anyway, but you are
   free to lie about the contents of the disk.  When you give up
   the PIN it will not endanger your comrades.
Non-hypothetically speaking, I have zero confidence that disk vendors
(SSD or otherwise) have implemented such a thing correctly.  I can't
even get the Linux folks to implement a decent RNG that would allow
"live CD" images to operate securely.
Here's a bit more follow-up on that.  I experimented with various methods.
1) Grinding is vastly easier than melting.  Keep in mind that you only
 need to grind away a few microns from the surface;  you don't need to
 annihilate the whole die.
2) If you insist on reducing the whole die to slag, you are better off
 using a combination of modest chemicals plus relatively modest temperatures
 ... rather than exotic chemicals separately or exotic temperatures separately.
At the next level of detail, I discuss various sensible and not-so-sensible
methods of annihilating ICs at

@_date: 2016-01-08 11:57:51
@_author: John Denker 
@_subject: [Cryptography] Verisimilitrust 
1) Isn't this exactly the sort of problem that NameConstraints were
 supposed to alleviate, at least 15+ years ago?
     2) If not, can somebody explain why not?
Some people on this list are quite contemptuous of NameConstraints,
but I've never understood the argument.  The usual argument seems
to be:  "We refuse to implement them because they are useless because
we refuse to implement them."
To say the same thing the other way:  It seems like a suitably-constrained
.kz CA would give people an incentive to start respecting the constraints.
It not, why not?

@_date: 2016-01-09 03:18:17
@_author: John Denker 
@_subject: [Cryptography] Verisimilitrust 
Patient:             Doctor, doctor, it hurts when I do /THIS/.
  Dr. Henny Youngman:  So don't do that.
In other words, Francine has a perfectly usable lock on her door, but she refuses to use it, because she heard that Oscar,
who lives across town, has a broken lock on his door, which he refuses to fix, even though it would be easy to fix.
It still seems like a Henny Youngman problem.
Garbage in, garbage out.

@_date: 2016-01-09 03:23:45
@_author: John Denker 
@_subject: [Cryptography] Alice, Bob, Eve, Nessa, Twein ?? 
*) Minor point:  FWIW there's Iphigenia ... not a common name these days, but still recognizable as a name.
  *) Larger point:  Presumably the intent was to get all the way to five.  That is possible in Finnish.  Lots of vowels, and lots of hyphenated names.  There exist people named	
The initial V corresponds to the Roman numeral for five, which
is nice.  Furthermore, if you prefer an initial I, you can flip
things around:
There's nothing wrong with that, although it would be an unusual
name, because when forming hyphenated Finnish names it is more common to put the shorter name first.
These are not the sort of ethnic name that one most naturally
associates with a FVEY representative, but it could happen.

@_date: 2016-01-10 00:05:15
@_author: John Denker 
@_subject: [Cryptography] Verisimilitrust 
That's interesting, but I thought the original question that Peter Gutmann asked on 01/08/2016 02:09 AM was:
If e-commerce is where the rubber meets the road, this question is very nearby.  It is where the tires get attached the car.
So what's the answer?
  1) Add the CA to the list, because the WebPKI was only
   designed to facilitate e-commerce.
  2) Don't add the CA to the list, because the WebPKI was only
   designed to facilitate e-commerce.
  3) .....?????

@_date: 2016-01-12 12:28:34
@_author: John Denker 
@_subject: [Cryptography] improved CAs and CRLs ... was: Verisimilitrust 
You guys are needlessly talking past each other.  In general, there
is such thing as *multifactor causation*.  Factors can exist in
series and/or in parallel.  You can't explain the overall situation
by picking on one factor or the other.  Today we are considering
a situation where users are depending on the CAs *and* the browser
maker to do the right thing.  The CA _originates_ the revocations,
and then the maker inserts a _meddling proxy_ in series.
Note the contrast:
 *) If the CAs went away, then
   -- In the short term, users might coast along using old CRLs.
   -- There would be no _new_ revocations, since the maker doesn't
    even pretend to originate such things.
 *) If the makers' proxy went away, the user is screwed, short term
  and long term, because there is no direct access to the CAs.
The proxy messes with the CRLs to promote what the makers call
"efficiency".  This makes the normal, non-revoked case faster,
at the cost of making revocations slower and less reliable.
Oh, it's much worse than that.  To revoke a CA, you can rely on
neither the CA nor the browser.  When the "superfish" rogue CA was discovered, the mozilla guys had no idea what to do about it.
They have no mechanism to revoke it ... and they don't consider
this to be a problem.  To this day, the issue is classified as
"RESOLVED WONTFIX"
  In other words, "not my job".
  A perfect solution is provably impossible:  If somebody wants
to put you in a Truman Show Matrix, he can probably do it, at
least occasionally.  However, we should not let the perfect be
the enemy of the good.
I'm not sure "replacing" CAs is the correct focus ... but there are lots of things we could do to improve the situation.
 *) As previously discussed, properly-implemented NameConstraints
  would alleviate a number of problems ... not just the current
  Borat CA problem.
 *) Pinning would help.  Again we have the idea of multiple
  factors.  You can do better with pins *and* authorities than
  with either one separately.
  People have lots of experience with pinning in the context
  of SSH.  Note that SSH is *not* pinning-only;  it knows about
  authorities too.
  Mozilla has made some very tentative steps toward pinning.
 *) Redundant signatures.  People are familiar with multiple   signatures in the context of PGP ... and from long-established
  business practices:  When a salesman (Chico) takes a client
  to lunch, to get reimbursed he has to submit a voucher, signed
  by Chico and by his boss (Harpo) ... unless Harpo was at the
  lunch, in which case it has to be signed by the higher boss
  (Groucho).  This is not rocket surgery.  I would like to see
  high-value certs countersigned by a second CA.  This would   make it harder for a rogue CA to get traction.  Not impossible,
  but harder.
  Within X.509 there exists a mechanism for doing this sort of
  thing ... although as we would expect from X.509 it is a mess.
  I'm not sure how widely supported it is.
 *) Root control.
  Right now, e-commerce merchants are certified by the so-called
  CAs ... while the CAs themselves are, for practical purposes,
  pseudo-certified by the browser makers and/or software distro
  bundlers.  It doesn't have to be that way.  For starters,   there should be only *one* root.  That's what the word "root"
  is supposed to mean, as should be obvious from the botanical
  etymology if nothing else.  Rather than hard-coding the hundreds
  of CAs, the browser maker and/or bundler should *sign* the CA
  certificates.  This would make clear the chain of responsibility,
  and would establish a mechanism for revocation of rogue CAs
  (which is at present grievously and ludicrously lacking).
  It is trivial to take an existing self-signed "root" certificate,
  turn it into a signature request, and then sign it.
 *) More root control.
  Furthermore, imagine Zeppo works for the Freedonia Self-Defense
  Forces.  It would make sense for his browser to trust certain
  CAs not because they call themselves CAs, nor because they are   pseudo-certified by some foreign bundler ... but rather to
  trust only sites and CAs that have been approved and signed   by the FSDF Information Assurance department.
     Seriously, the last time I deployed a cryptosystem I cared
     about, this is how it worked.  It was PK without the PKI.
     The keys were trusted because and only because I issued
     them!  No gravy-training required.
  Again, there exist mechanisms for doing this.
 *) Signature chain continuity.
  When a merchant wants to start using a new key (because of
  impending expiration or whatever) the new key should be
  signed by some authority *and* by the old key!  This would
  greatly increase the attacker's cost in some situations.
  It's relatively easy for the Bad Guys to coerce one of the
  oh-so-very-many CAs to issue a certificate for MITMing my
  traffic ... but it greatly raises their cost and risk if they
  have to mount a black-bag job to get my old key.
 *) et cetera.  See this forum's previous nice, long-running
  discussion of pathetically obvious problems, many of which
  are fixable.

@_date: 2016-07-07 01:55:12
@_author: John Denker 
@_subject: [Cryptography] What to put in a new cryptography course 
.... and quite a few others seem to agree.
However, I'd say that's true except when it's not true.
Here is a sentiment that has been expressed in various ways by Aristotle, Ptolemy, Ockham, Einstein, and innumerable others:
  _Make things as simple as possible, but not simpler._
I suppose in theory, /ceteris paribus/ simpler is better ...
but in reality the ceteris are almost never paribus.
Here's an example I know a little something about, namely random
number generators:
    The simple ones are not good, and
    the good ones are not simple.
A good one requires:
 -- fundamental physics
 -- electrical engineering
 -- computer programming
 -- mathematical cryptography
 -- practical security
 -- user interfaces
 -- explanation and education
 -- et cetera.
And (!) each one of those ingredients by itself has a fair bit
of complexity.
Insofar as the product is complicated, there are lots of ways it
might fail if you make a mistake ... but if you oversimplify
it, that's a mistake unto itself, and the thing will fail for
Crypto in particular and security in general demand attention
to detail.  Up to a point it helps to reduce the number of
details ... but even so, and the end of the day there still
remain a treeeemendous number of details.
Eliminating complexity is nice work if you can get it, but
the other 99% of the job revolves around inventing systematic
ways of /managing/ the complexity that remains.

@_date: 2016-07-12 12:49:37
@_author: John Denker 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
That's wrong on multiple levels.
First of all, NP is a statement of how /easy/ a problem is.
If you mean NP-hard, please say NP-hard.
More importantly, as Arnold Reinhold explained with commendable clarity
on 07/12/2016 04:37 AM, we do not need breakage to be NP-hard.  All we need is hard.
This has been understood since Day One of public key cryptography (1974).
Merkle Puzzles are only polynomial hard.  In fact they are quadratic,
which is a remarkably wimpy polynomial.  However, depending on the
prefactor, and depending on how fast your machine is, this is good
enough for some applications.

@_date: 2016-07-12 23:47:14
@_author: John Denker 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Let's not forget the most important law of all.  There is one law to
rule them all, and in the darkness to bind them:  Murphy's law.
For example, we can use this to clarify the following:
Nothing every really goes away, unless you were trying to preserve it.
Seriously, on almost any given day, more people suffer from having not
enough copies of their data (e.g. disk crash) than from having too many
copies of their data (e.g. theft of private information).
Tangentially related:
... and the boolean value is always false.  When the pointy-haired boss
asks, "Will it be secure if we do ....." the security expert answers
"NO" without waiting to hear the rest of the question.  Nothing is
ever secure in the boolean sense.  Instead the expert asks, "How much
risk can you afford, and how much are you willing to spend to mitigate
the risk?"
To which the pointy-haired boss responds "zero, and zero."
Also related to all of the above:  Security (in the sense used in
this forum) is never the whole story.  I can secure my laptop by
putting it in a safe, welding the door shut, and storing it in a
locked room with armed guards posted outside.  The problem is,
that detracts from the usability.
The challenge is to provide security while also providing high
availability and high usability.
These are complicated issues.  For example:
  -- The 9/11 plotters go undetected, and people ask, why didn't
   you do a better job of sharing the data and connecting the dots?
  -- Then Pvt. Manning comes along, and people ask, why didn't
   you do a better job of compartmentalizing the data?
All the simple answers are wrong.
All the pithy laws are wrong.

@_date: 2016-07-16 09:40:24
@_author: John Denker 
@_subject: [Cryptography] hard natural problems 
They're out there, and they're a lot harder than n^3.  There is such
a thing as /deterministic chaos/ which is the physics equivalent of
an exponentially-hard algorithm.
 *) Weather forecasting certainly counts as a natural problem.
  It is subject to the famous /butterfly effect/.
 *) Even a system as simple as a double-jointed pendulum exhibits
  chaos.  That means it is exponentially sensitive to tiny details
  in the initial conditions.  Here's an animated gif, and some
  discussion:
      *) Et cetera.
Chaotic systems tend to be super-sensitive to things we can control
(which would be good for encryption/decryption) but are also super-
sensitive to things we cannot control (which is usually not so good).
That's why you don't hear about them much in discussions of codes
and ciphers.  Even so,
  -- Chaotic behavior helps if you're building a RNG.
  -- An example is using glitter to make tamper-resistant seals
   (but beware that people often ask seals to do more than they
   could possibly do).
  -- One may speculate about building an analog computer that
   uses chaos for encryption/decryption.  I reckon this would
   require noise control on a level comparable to a quantum
   computer, i.e. not practical, but it's amusing to think about.

@_date: 2016-07-21 09:00:55
@_author: John Denker 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Good point.
I would go farther and say that Morris analysis was nave
when first formulated and even worse now.  Most non-experts
seriously underestimate how important it is to protect
little details.
Let me suggest another aphorism for the collection.  As I've
been saying for years:
 137)  Metadata is data.
Corollaries include:
 137a)  Any system that leaks metadata is a system that leaks.
 137b)  Stealing metadata is stealing.
As another way of saying almost the same thing:
 *) Traffic analysis is a Big Deal.     Ignore it at your peril.
A great deal of what is nowadays called "big data" is traffic
analysis and variations on that theme.  It's been going on for
eons.  The main recent change is an increase in the number of people doing it.
It is embarrassing how badly this is handled by typical internet
protocols, especially considering how long the problem has been
allowed to fester.
It affects not just primeval protocols such as TCP but even
relatively recent things.  For example, SNI came out in 2003
with no provision for encrypting the virtual hosthame.
There's been some chit-chat about extending TCP to include
encrypted headers and cover traffic, but this doesn't seem
to have had much impact.

@_date: 2016-07-21 11:02:01
@_author: John Denker 
@_subject: [Cryptography] Entropy of a diode 
Background remark: noise in a Zener diode is basically shot noise.
Meanwhile, noise in an avalanche diode is basically shot noise,
amplified by (you guessed it) the avalanche effect.
Most importantly, if you pull random diodes off the shelf, you
have no idea what noise you're going to get, because they're not
designed for that. The manufacturer could "improve" the process
tomorrow in a way that reduces the noise. If you look at a typical
spec sheet, it doesn't mention noise at all:
OTOH if you are talking about a diode intended and optimized for
use as a noise source, you have to look at the spec sheet for
that particular diode.
Any book on semiconductor physics will talk about the avalanche
effect. Any book on low-noise circuit design will talk about
shot noise. I get 10,000 hits from
If that's not good enough, please ask a more specific question.

@_date: 2016-03-02 17:26:27
@_author: John Denker 
@_subject: [Cryptography] A question on Shannon's entropy 
Well, "cryptography" is a huge field, and "random" is not
strictly defined.  (This stands in contrast to Shannon entropy,
aka plain old entropy, which is verrry well defined.)
In cryptography, sometimes entropy is exactly what you want, and
sometimes not.  In particular, entropy measures a particular
average property of the distribution.  There are *lots* of other
properties that the distribution might have.  Depending on your
threat model, you might be much more interested in minimax properties as opposed to average properties.
Also it is crucial to note that strings do not have entropy,
in the same way that numbers do not have entropy.  Entropy is
a property of the /distribution/.  As some guy named John von Neumann famously said, there is no such thing as a random number.
 -- If it's random, it's not a number.
 -- If it's a number, it's not random.
You can have a random /distribution/ over numbers (or strings), but
then the randomness is in the distribution, not in any string
that may have been drawn from some such distribution.
The question is based on multiple misconceptions.  Entropy is
a property of the /process/ that generates the strings.  The
machine that generates sorted strings is different from the
machine that generates unsorted string.  You can calculate the
entropy from the single-character probabilities (or, loosely speaking, the frequencies) /provided/ the characters are IID (independent and identically distributed).  The characters in the sorted strings are nowhere near IID, so the assertion that
the entropy is unchanged is nowhere near correct.
Here's a paper that touches on some of these issues and can
serve as a good starting point:
  C.E. Shannon
  "Prediction and Entropy of Printed English"

@_date: 2016-03-03 16:48:31
@_author: John Denker 
@_subject: [Cryptography] A question on Shannon's entropy 
I wouldn't have said that.  A source with correlations
can have a perfectly well defined entropy.  You might
need a slightly more sophisticated formula for calculating
the entropy, but the concept is not open to question.
You can even calculate the entropy of a cat state,
i.e. an entangled quantum state.  That takes the
idea of "correlation" to a whole nother level.
  S = Trace  log 

@_date: 2016-03-25 09:21:06
@_author: John Denker 
@_subject: [Cryptography] Unicity distance of Playfair 
Not true.
Given N input letters, Playfair produces
M = 2 * ceil(N / 2)
output letters. M is always an even number.
The average M/N converges to 1 in the limit of very long
messages, but that's not the same thing.
That's categorically wrong in principle.  Among other things,
it is wrong as applied to block ciphers, of which Playfair
is a famous early example.
It might be sometimes kinda sorta useful as an approximation,
but elevating it to a categorical general proposition is just
If it were true, there would be no need to talk of both
"diffusion" and "confusion" in the context of a block cipher.
The more emphatically you state it, the more wrong it is.
Block ciphers exist for a reason.  The first half of the
block serves as an "autokey" for the second half (and vice
versa), which changes the unicity calculation.  The block
size in Playfair is not big enough to make a huge difference,
but it does make a difference ... which I assume was the
point of the original question in this thread.
Block ciphers exist for a reason.  Chaining modes exist for
a reason.  IMHO the reasoning is not as good as people tend
to assume, and people rely far too much on dubious assumptions,
but that's a topic for another day.
Here is a particularly simple counterexample, applicable
even in the absence of blocking.  Consider the contrast:
  Cipher  represents English using 5-bit Baudot code,    and enciphers it character by character, using a 40-bit
   key.  Then the unicity distance is
         U1 = 40 / (5 - 1.5) = 11.43
  Cipher  represents English using 8-bit bytes, and    enciphers it character by character, again using a 40-bit
   key.  Then the unicity distance is
         U2 = 40 / (8 - 1.5) = 6.15
Also not true.  Unicity is not the be-all and end-all, but it
is "a" measure.  In this example, it tells us that cipher  is
less secure than cipher   Enciphering those three unused
bits is a Bad Idea.

@_date: 2016-05-22 21:27:14
@_author: John Denker 
@_subject: [Cryptography] immortal quote about randomness 
``Anyone who considers arithmetical methods of producing
   random digits is, of course, in a state of sin.  For, as has been
   pointed out several times, there is no such thing as a random
   number -- there are only methods to produce random numbers, and a
   strict arithmetic procedure of course is not such a method.''
Actually it's a twofer.  I use the second sentence more
often than the first.  Here's my version:

@_date: 2016-11-09 11:07:40
@_author: John Denker 
@_subject: [Cryptography] SMoP versus wisdom and judgment 
That's quite an overstatement.  Sometimes users are instructed
to exercise judgment.  That cannot be reduced to code.
We agree it is a SMoP to get the computer to /implement/ policy ...
but some wise person has to /design/ the policy beforehand.
Once upon a time my research department brought in an expert from
one of the front-line business units, on temporary assignment,
to give some real-world grounding to our work.  I asked her what
was the right thing to do in such-and-such situation.  She replied
by quoting corporate policy.  I explained that I already knew
what the policy book said, but our job was to /design/ policy,
so the question remained:  what was the right thing to do.  She
repeated her previous woefully-unwise answer.
Here's a large family of additional examples:  Oftentimes you
can't use a secret weapon without revealing it, so there is a
tradeoff:  the value of the secret versus the value of actual
use.  The same idea extends to the metaphorical weapons of
cryptanalysis and espionage, where there is a tradeoff, namely
exploiting the information versus protecting the sources and
It is a SMoP to get a computer to make the decision, but this
does not solve the judgment problem;  it just throws it over the
fence into the domain of the guy who writes the specifications
and/or writes the code.
We agree that sometimes the machine is in complete control of
the process, and the user is nothing but a meat robot ("would
you like fries with that?") ... but OTOH there still remain a
lot of situations where the machine serves the user, not the
other way around.

@_date: 2016-11-09 14:41:54
@_author: John Denker 
@_subject: [Cryptography] protecting information ... was: we need to protect 
[640 words snipped]
Anybody who "sits down at some random computer" to do confidential
work is doomed.  2FA will not help.  492FA will not help.
Here "work" includes email and everything else.
Perhaps the intended scenario was something like this:
  Moving from machine to machine within some secure environment.
Relevant proverb:
  If you don't have physical security, you don't have security.
As part of asking
  What's Your Threat Model (WYTM)?
we need to ask
  What's Your Security Perimeter (WYSP)?
This is why security is hard, and will always be hard:  The
defender has to secure every door, every window, and every
keyboard ... while the attacker only needs to break one.
It would be a mistake to formulate the goal in terms of protecting
the nodes or the wires or even the dox.  Such things are
merely oversimplified corollaries of the larger goal, which is
to protect the /information/.
As mentioned back on 09/09/2016 10:47 AM, I recommend:
  Laura J. Heath,
  "An Analysis of the Systemic Security Weaknesses of
   the U.S. Navy Fleet Broadcasting System, 1967-1974,
   as Exploited by CWO John Walker"
  Master's Thesis, U.S. Army Command and General Staff College (2005)
Here's the nut graf:
Emphasis in the original:  /data/.
Protecting secret /data/ aka /information/ is a lot harder than protecting
wires, nodes, or documents.

@_date: 2016-11-10 12:14:53
@_author: John Denker 
@_subject: [Cryptography] protecting information ... was: we need to 
Excellent.  Thanks!
Continuing in that vein:
The US SECRET SERVICE would send everybody in the building a memo,
telling them to change their passwords.  They would hand-deliver the
memo, because they are not allowed to send or receive email.  They
would then depart, driving at 75 MPH enroute to a secure undisclosed
location (i.e. a brothel).
The county DEPARTMENT OF ELECTIONS wouldn't lock the doors, but they
would put tamper-evident seals across all the door/jamb joints.\1\
\1\ "We studied 198 different seals and demonstrated how all
 can be defeated quickly using low-tech methods available to
 almost anyone."
    Roger G. Johnston, Anthony R.E. Garcia, and Adam N. Pacheco
    "Efficacy of Tamper-Indicating Devices"

@_date: 2016-11-11 00:49:04
@_author: John Denker 
@_subject: [Cryptography] election security 
That's true ... and there are other risks besides.
 1) Oregon is essentially 100% vote-by-mail.
 2a) In Arizona some municipal elections are 100% vote-by-mail.
 2b) In other elections, it's optional, but even so, about 75%
 of Arizona ballots were cast by mail in this week's general
 election.
This may help put the crypto issues in perspective:  As a
matter of principle, the privacy horse escaped from the barn
a long time ago, because mail-in ballots are vulnerable to all
sorts of coercion, including bribery and extortion.  There's
little evidence of it happening on a large scale, but some
workplace overseer, union boss, ward heeler, or mobster could
demand that you fill out the ballot in his presence ... or
just hand over a blank ballot and a signed but unsealed
signature envelope.
Arizona encourages vote-by-mail, because it's cheaper ...
but I'm not the least bit convinced it's a good idea, because
of the downside risks.
A badly designed ballot-imaging system would make things worse,
permitting /wholesale/ coercion.  Ballot /imaging/ can permit
surreptitious ballot /signing/, which is very bad from an
anti-coercion point of view.
People who worry about corruption at polling places don't know
what they're talking about.  There are significant threats to
the system, but they're at other places and times, not at the
polling place on election day.
At my polling place we had an actual party observer, which
is something that I'd never seen before.  He came looking
for trouble, but soon came to realize that he was an amateur
watching a bunch of professionals.  The poll workers were
skillfully defending the integrity of the election, and there
was nothing the observer could do that would make anything
better or worse.
Even though we were short-handed, we ran 100 voters through
the polling place in the first hour, which is a nifty bit
of choreography.
The part of this that is particularly relevant in this forum
concerns the machines that scan the ballots after the bubbles
have been filled in.  This requires a platform that is resistant
to "tailoring" (in the NSA TAO sense of the word).  Everything
needs to be secured, including the BIOS, N stages of boot loader,
OS, applications, and peripherals.
This is a really hard problem.  It is an "advanced persistent
threat" situation.  One thing we learned from Snowden is that
the "tailors" are very advanced and very persistent.  Also
there are plain old bugs.  Open source is nicer than closed,
but it does /not/ automagically make all bugs shallow.
I reckon there is a role for cut-and-choose at some point:
pick some subset of the machines and tear them down to bare
metal and bare silicon.
I reckon there is also a role for good old-fashioned redundancy.
Count the ballots twice, with two dissimilar systems in tandem.
Arizona already does a bit of this, insofar as they routinely
hand-count 2 percent of the precincts.  I would prefer to see
two redundant machine-counts of everything, followed by a hand-
count of a smallish sample.
The power of this approach was well demonstrated in Humboldt
County in 2008.  Famous story:

@_date: 2016-11-12 12:46:27
@_author: John Denker 
@_subject: [Cryptography] securing the ballot scanners 
Then on 11/11/2016 12:12 PM, Arnold Reinhold asked:
Taking the last point first:  In each jurisdiction there are millions
of voters ... but you don't need millions of computer security
experts.  Each political party needs to find just one expert,
literally one in a million, to oversee the cut-and-choose and
other validation procedures.
As for redundancy, we all agree that is a good way to detect a
machine that is miscounting ballots.  However, alas, that is not
the only threat.
The following example is hypothetical, but it makes the point that
there are many possible threats, and you might not know what you're
looking for until you find it:
  Suppose we have paper ballots (as we should) and optical scanners
  in each precinct (as we should).  Now suppose the bad guys have
  hacked into the scanner in a way that doesn't change the count,
  but merely broadcasts (via WiFi or whatever) the results for each
  ballot as it comes in, and/or a running total.
  *) This causes a near-total breach of voter privacy.  It opens
  the door to voter coercion.
  The breach is even worse than you might think, because typically
  there are records showing who voted and the *order* in which they
  showed up at the polling place.  By law these records are routinely
  provided to the parties.  So the bad guys don't even need to watch
  the voters coming and going.
  *) Furthermore, the transmission provides real-time information
  as to which precincts are underperforming or overperforming.
    The least-intrusive consequence is that the underperforming party
    could redouble its GOTV efforts in that predinct.
    Or they could send observers and challengers to disrupt operations
    at that precinct, essentially mounting a denial-of-service attack.
    Or about ten other more brazen and/or more violent things they
    could do (which I'd rather not discuss).

@_date: 2016-11-13 18:28:20
@_author: John Denker 
@_subject: [Cryptography] highlights of crypto history 
Innnnteresting question.
Here are some candidates that spring to mind:
Security failures:
  Zimmermann Telegram : 11 January 1917 -- encrypted, but not quite well enough.
   From a US/UK point of view, this one is hard to beat.
  First Battle of the Masurian Lakes : 8-11 September 1914 -- the Russians had
   a bunch of these newfangled "radios" but they didn't have their cryptologic
   act together, so they transmitted in the clear.  The Germans picked up on
   this.  This could be considered the start of a years-long series of very
   unpleasant events for the Russians.  (David Kahn likes to cite this example.)
   The Bolsheviks learned from this, and have ever since devoted tremendous
   efforts to crypto, comsec, and opsec.
Security successes:
  Battle of the Bulge : 16 December 1944 -- Germans achieved essentially complete
   tactical surprise, effective comsec (not so much crypto, just low-tech comsec),
   effective deception.
  Operation Overlord : 6 June 1944 -- Allies achieved essentially complete tactical
   surprise, highly effective deception.
Other contenders:
  Washington Naval Treaty : signed 6 February 1922
    US codebreakers ate the Japanese diplomats' lunch.
    Nontrivial consequences for WWII.
    Consequences would have been greater if Yardley hadn't spilled the beans.
  So many aspects of WWII that one hardly knows where to begin:
    Battle of the Atlantic
    Battle of the Coral Sea
    Battle of Midway
    Assassination of Yamamoto
    US anti-shipping operations in the eastern Pacific, unheralded but very consequential,
      dependent on breakage of Japanese Maru code and non-breakage of US Navy codes.

@_date: 2016-11-14 11:48:37
@_author: John Denker 
@_subject: [Cryptography] highlights of crypto history 
An alternative suggestion for a date:  Perhaps crypto should be
celebrated on Cyber Monday, because without crypto there would
be no e-commerce.
In contrast, the idea of celebrating "cryptography day" based on this-or-that famous message seems misguided, because messages only
become famous when they're leaked.  Note the asymmetry:
  -- Secure encryption is ongoing and quiet, whereas
  -- cryptanalytic breakage leads to spectacular incidents
   on identifiable dates.
This asymmetry makes it hard to obtain balanced political support
and funding for crypto;  the code-makers generally lose out to
the code-breakers.  All too often people forget Rowlett's dictum:
  "In the long run it was more important to secure one's own
   communications than to exploit those of the enemy."
Yes.  Although it is not primarily a crypto issue, I apologize for
the idiocy of not mentioning it originally.  Pearl Harbor is in
fact an enduring metonym for infamous surprise attack, including:
 -- successful offensive secrecy, along with
 -- unsuccessful defense at many levels.
One could also add to the list any number of recent leaks:
 -- The Manning leaks.
 -- The Snowden leaks.
 -- The Sony hack.
 -- The OPM hack.
 -- The Mossack Fonseca leak.
 -- The DNC and Podesta hacks.
 -- etc. etc. etc.
To a degree, better crypto would have helped defend against some
of these, but only to a degree.
Like the Battle of the Bulge, Pearl Harbor involved precious little
crypto, but instead mostly old-fashioned low-tech comsec and opsec.
I'm not so sure of that.  The way I heard the story, OP-20-G was
for two reasons:
  a) Very little traffic was being sent.  The Pearl Harbor attack
   force was maintaining radio silence.  No attack orders were
   intercepted (much less decrypted) because no such orders were
   ever sent over the air.  Also there were no direction-finding
   (DF) fixes on the attacking force.  Intra-flotilla coordination
   was carried out by signal flags, semaphore, and Aldis signal
   lamps.
  b) OP-20-G could not break very much of the JN-25 code prior
   to December 1941.  They obtained a fair-to-good break early
   in 1942, based on increased level of cryptanalytic effort,
   plus greatly increased rate of traffic, magnified by the
   accumulation of traffic over time.
Evidence that the people who mattered were not biting their nails
comes from the fact that when the infamous 14-part diplomatic
message was decrypted, an alert was sent out to overseas bases,
but it was sent over prosaic low-priority channels.  High-priority
channels would have cost a few dollars more.  The alert was not
received until after the attack was well under way, and furthermore
it was so nonspecific that it wouldn't have had much effect even
if it had been received earlier.
So as I see it, this was not really a crypto issue.  In this forum
it may be worth a little bit of bandwidth to understand /why/ it
was not a crypto issue.  The intelligence failures were elsewhere.
For starters, there were search radars in operation in Hawaii.  The
one newly relocated to Opana Hill did see the incoming planes, but
this was not taken seriously and produced no defensive response.
Partly this was due to the lack of any semblance of decent IFF
Another explanation (not a good reason, just an explanation) of
why they weren't biting their nails goes like this:  Experience
with previous Japanese military operations (exercises and otherwise)
led to the following conditional:
   If the ships are at anchor in home waters,
   then they maintain radio silence.
That makes sense;  in home waters they can easily run telephone
and telegraph wires directly from ship to shore, and/or hand-
deliver written messages.
The truly terrible intelligence failure was a logical fallacy,
namely affirming the consequent, i.e. the fallacy of the converse.
The analysts assumed it was probable that:
  If they are maintaining radio silence,
  then the ships are at anchor in home waters.
This assumption had no basis in logic or evidence.  Also note that
"probability" is not what matters, but rather risk, i.e. probability
multiplied by the cost of being wrong.
And then there is old-fashioned complacency.  If you weren't attacked
yesterday, or the 5000 days in a row before that, it suggests you
probably won't be attacked tomorrow, but even so, you reeeally must
not ignore the tail risk.  It's a turkey problem:
  Another intelligence failure is that apparently the Western powers
didn't count the warships in Japanese harbors.  Remember, in Japan
in 1941 there were still active embassies with diplomats and long-
range radios.  Since the ships were not in harbor /and/ were radio
silent, people should have been biting their nails.  Nowadays, with
20/20 hindsight, major powers use satellites, spy planes, over-the-
horizon radar, picket ships, et cetera to keep track of each others'
forces.  There are gaps in this system.  This has been discussed in
the techno-thriller literature, but not all the gaps are fictional.
Last but not least there is the attacker's intrinsic advantage:
The defender has to secure all the doors and all the windows, all
the time ... whereas the attacker only needs to break one, at a
time of his choosing.  This applies equally to crypto along with
everything else.

@_date: 2016-11-15 12:24:23
@_author: John Denker 
@_subject: [Cryptography] October 28th is now National Cryptography Day 
Not quite "equivalent" : Crypto devices contribute verrry much less
to the likelihood that a member of the owner's family will be killed.
IMHO choosing that date is a step in the wrong direction.  It's a
tactic in support of a misguided strategy.
The point of National Cryptography Day is to get the public involved,
as a way to exert political pressure in some direction.  We need
to carefully choose the direction.
The public already labors under the misconception that bad guys make
codes and good guys break codes.  That misconception is reinforced
by movies about Turing, Enigma, U-571, Midway, Zimmermann, et cetera.
That is an extreme, unbalanced view.  I am *NOT* arguing for the
opposite extreme, since (as usual) all the extremes are wrong.  I am arguing for some semblance of balance.
Balance is difficult to achieve, because code-making in particular
and security in general tend to be slow, methodical, and quiet.
Code-breaking tends to produce much better screenplays.
I renew my suggestion to celebrate crypto on Cyber Monday.  E-commerce
is a 350 billion dollar industry (per annum).  Without crypto, there
would be no e-commerce.  Unlike Hollywood crypto:
 -- E-commerce is not restricted to armies, diplomats, or spies.
 -- You don't need to be a weirdo to appreciate it.
 -- The good guys encrypt stuff and the bad guys try to break it.
 -- The argument that if you have nothing to hide you don't need
  crypto is well refuted.
Last but not least:  The October 28th date was suggested as a means
to influence US elections.  That's a Bad Idea for multiple reasons.
It's way too late in the season.  A great many votes are already
cast by then, and more importantly, minds have been made up.  People
need time to process ideas.  (Last-minute Comey-style interventions
only work if they fit into a pre-established framework, and even
then the effect is limited.)
Also:  A US election costs a few billion dollars every four years.
it's a fact.  It should tell you that even though elections have
consequences, you also need to pay attention to what bureaucrats,
legislators, and lobbyists are doing /between/ elections.  Campaign
in poetry, govern in prose.  Crypto policy is so geeky and non-poetic
that it will never be much of a campaign issue, no matter how much
you might try to make it so.  It's the sort of thing that gets
hammered out /between/ elections.

@_date: 2016-11-15 12:39:09
@_author: John Denker 
@_subject: [Cryptography] client-side authorization 
I changed the Subject: line because AFAICT talking about certs
is the wrong way to frame the discussion.  Certificates as we know them leak all the wrong things and certify all the wrong things.
I am reminded of the quote from BtVS:
  	Ford:  "I know who you are."
The fact that somebody can /identify/ me (i.e. pick me out of a lineup)
does not prove that I authorized this-or-that transaction.  In the present
context, the goal should not be identification or authentication, but
know exactly what I am signing.
Perhaps rather than fussing with the details of x.509 certs, we should
be looking at zero-knowledge proofs.

@_date: 2016-11-15 23:44:12
@_author: John Denker 
@_subject: [Cryptography] Qubes 
Hi --
Is there anybody here who would care to comment on Qubes?
  "A reasonably secure operating system"
  It's kinda new, and I have zero first-hand experience with it,
but people seem to be saying nice things about it.
The basic idea seems to be to use modern VM features (and optionally
TPM) to tilt the playing field in favor of defenders.  Also they
seem to have a good attitude toward following through on the 10001
little things that are required.
The basic idea has been around for years, used in niche markets
such as kiosk machines.  Maybe the time has come for it to break
into the mainstream.
The downside is that it demands a lot of the hardware.
This is crypto-related insofar as it doesn't matter how good your
number-theoretical algorithms are if somebody has pwned your platform.
If it achieves critical mass it could be kind of a big deal.

@_date: 2016-11-19 21:58:48
@_author: John Denker 
@_subject: [Cryptography] Crypto and rustling 
1) No discussion of branding would be complete without this:
        i.e. Gary Larson : Far Side cartoon : The brand of Daryl Jones
2) Branding of cattle dates back to ancient Egypt, so restricting
 solutions to those "available at the time" is a bit vague.  I
 assume it means no electronics.
 Hallmarks have been used on pottery and metalwork since the
 dawn of time.
3) There are some problems in this space that cannot be solved
 by branding alone.
 For example, consider the extension to logging:  Somebody grows
 trees, fells them, brands the ends, sends them down the road
 or down the river by a complicated process, and then gets paid
 when the branded logs arrive at the mill.
 The obvious attack is to saw an inch off the end of the log
 and re-brand it.
4) There are problems that can be solved by paperwork + branding
 that cannot be solved by either one alone.
5) Positioning of the brand on the animal is part of the vocabulary.
 You can have two identical graphics, but if they are applied in
 different places they are considered different brands.  Usually.
6) Brand /registration/ helps a lot.   Theory says that a brand that is similar to another, or which
 can be easily created by alteration of another, will be rejected
 by the registrar.  However, this is not always true in practice.
 Counterexample:  In the California brand book:
  A- Avila	LR	earless
 -A- Anderson	LR	earless
   7) Brands can be used in conjunction with /earmarks/.  Altering
 both would be a lot harder than altering either one separately.
 However, sometimes there is no earmark, as in the previous
 counterexample.
8) The system is open to false-flag attacks and denial-of-service
 attacks.  If there is a brand on one side and some troublemaker
 applies a different brand on the other side, there might not be
 any obvious way to tell which is authentic, and the brand inspector
 will declare both invalid.
 Such an attack is not cheap, but somebody with a big enough
 grudge might try it.
9) It appears that most brand designers are more interested in
 cute symbols than in high-security symbols.
10) Applying the brand is not an exact science.  Brands can be
 smudged.  Therefore error-resistance is a consideration, not
 just forgery-resistance ... although there is a bit of overlap
 between the two.
 If you wanted to be systematic about this, you could use some
 sort of Latin square or magic square design.  I see no indication
 that anybody ever bothered to do this.  So this might count as
 something that could have been done, but wasn't.
 Designing one tamper-resistant brand isn't enough;  ideally we  would like a systematic way of generating a huge number of unique,
 readily-distinguishable, /and/ tamper-resistant brands.  There
 are hundreds of thousands of registered brands in the US alone.
 OTOH they don't have to be globally unique.  For most purposes
 it suffices to be unique on a county-by-county basis.
 A plain old numeral with a check digit would make sense.  It would
 certainly be easier to use, as compared to grubbing through the
 pictures in the brand book.  OTOH it fails the cuteness requirement.
 Use a forgery-resistant alphabet, e.g. using X instead of 8, so it's
 not quite so easy to turn other digits into 8s.
11) From a leather point of view, any brand on the hide is considered
 a defect.
Looking forward, it is easy to predict that electronic tagging will
make branding obsolete.
In cases of alleged brand forgery, DNA can help.  If Mr. Hatfield
owns both parents, Mr. McCoy might have a hard time explaining how
his brand came to be on the animal.

@_date: 2016-11-21 15:53:42
@_author: John Denker 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
On 11/21/2016 02:40 PM, Ray Dillinger replied:
As a *minor* point that's not strictly true.  In this world there
are many situations where it makes sense to put all your eggs in
one well-guarded basket.  Birds have been doing precisely that for
the last 50 million years.  That is quite often preferable to the
opposite extreme, where the eggs are spread over so many baskets
that you can't efficiently guard any of them (let alone all of
Meanwhile, the *major* point is that claim [1] all too often
morphs into a claim of the converse, namely that having more
sources is ipso facto better.
I disagree, for the following reasons:
As a general principle, RNGs are just as important as ciphers.
Consider superencryption using multiple broken ciphers, such as
rot13 combined with single DES.  You wouldn't advise people to
do that, would you?  By the same logic, you should not advise
people to XOR a pile of broken RNGs.
It helps to introduce the following contrasting concepts:
 -- reliably predictable
 -- random, i.e. reliably unpredictable
 -- squish, i.e. neither reliably predictable
   nor reliably unpredicable
Here are some useful equations:
  random XOR squish = random
  squish XOR squish = squish   (*not* random)
We can summarize by saying RDRAND is squish.  From our point of view,
it is neither reliably predictable nor reliably unpredictable.
Combining it with other sources of squish does not help!
The only solution is to find some source of randomness that *can*
be audited, and audit it.  The requires a boatload of work.  However,
just because the right thing is hard does not give us a license to
do the wrong thing.
That is true, if interpreted sufficiently narrowly.  So far so good.
No, that doesn't work, for the same reason that combining two ciphers
with NOBUS backdoors doesn't work.  It is only a matter of time before
the two attackers compare notes, whereupon both succeed in attacking us.
Seriously, folks:  The fundamental equation is:
  squish XOR squish = squish
And by induction, it doesn't matter how many lousy RNGs you combine:
  squish XOR squish XOR squish XOR squish XOR squish XOR squish = squish
Combining untrusted RNGs makes just as much sense as combining untrusted
ciphers, i.e. no sense at all.
Having two trusted RNGs would be great, but in the meantime, given the
choice between one trusted RNG and a whole steaming pile of untrusted
RNGs, you are much better off with the one good one.

@_date: 2016-11-22 12:04:07
@_author: John Denker 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
I have long emphasized the distinction:
  -- reliably predictable
  -- reliably unpredictable
  -- squish
No, it is neither new or vague.  It mathematical terms, it is well
defined.  Formal examples -- *constructively* provable examples --
have been known since Gdel (1931).  Intractable and undecidable
propositions are well known in computer science.  In practical
terms, squish is directly useful and well understood, and has
been for a very long time, so long that it has a Latin name:
   "argumentum ad ignorantiam"
and it is the subject of English maxims as well:
   "absence of evidence is not evidence of absence."
Application to RNGs in particular was discussed by Knuth (1969).
True story:  One of my earliest experiences in the workplace:  We
needed a PRNG, and I was instructed by the pointy-haired boss to
get rid of my LFSR and instead use the Program Counter as saved
by the last interrupt, because that was unpredictable.  I kid
thee not.  In fact this was a terrible RNG.  Successive calls
tended to return the same number, and even if you got two different
numbers, one of them was likely to be the PC of the null job.  I
tried to explain that even though it was not reliably predictable,
it was not reliably unpredictable.
That's like putting three locks on your door ... and then hiding the
first key under the mat, the second under a nearby rock, and the third
atop the lintel.
Of course there are protocols where the /interested parties/ get
together and XOR their various contributions.  However, neither the
NSA nor the Spetssvyaz nor the Third Department have my interests
at heart.  At such a meeting, any sane person shows up with a RNG
that *he* can trust.  [XX]
   At every poker table there is a fish.  If you look around the
   table and can't spot the fish, it's you.
Nit-pickers note:  There exist contrived cases where XOR actually
helps, for instance if one source is truly random in the bottom
half of the word (but zero in the top half), while the second
source is random in the top half of the word (but zero in the
bottom half).  However, such a situation does not naturally arise
in practice, and if it did there would be easier ways of dealing
with it.
There exist realistic cases where the combination of two things is
better than either of the components separately, for instance the
hardware part plus the software part of a well-designed HRNG.
The two components were never intended to work separately.  The
proof of correctness applies to the system as a whole.
Proper operation requires the two components to be very carefully
engineered to work together.  They are joined in an intricate way.
This does not even remotely resemble the bogus construction that
launched this thread, namely
     squish XOR squish = squish
On 11/22/2016 04:13 AM, Jerry Leichter gave a commendably clear
discussion of correlated failures.  He cited Knuth's example,
which is dead-center in the domain of RNGs.  Without proof of
decorrelation, simple multiplicity has at best an untrustworthy
upside, and on the downside it is a waste of resources.
  Tangential but more general remark:
  Correlations are not the whole story, and simply counting
  the multiplicity is not the whole story, for the following
  reason:  Extra chains in parallel might give you added
  strength ... whereas extra chains in series just exacerbate
  the weakest-link issue.
  Weak links in series are even worse than correlations, in the
  sense that failure of any one link takes out the whole chain,
  whether or not the other links have failed.
  random XOR squish = random			[1]
On 11/22/2016 08:13 AM, Phillip Hallam-Baker replied:
General point:  I'm not convinced that's a good way to think about
it.  If the first component can so easily be interfered with, I
would have classified it as non-random to begin with.  Chez moi
"random" means /random enough/, i.e. /reliably/ unpredictable.
See paragraph [XX] above.
Specialized point:  When we apply it to the topic of the day, insofar
as statement [2] is even partly true, it's yet more of a reason why
the idea of XORing a squishy source with something else should be

@_date: 2016-11-22 21:52:52
@_author: John Denker 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
I disagree.  It's an invalid argument leading to an unsound
Among other flaws, the argument depends on the unstated assumption
that a backdoor installed by this-or-that agency cannot possibly
be exploited by anybody else (without the installer's consent).
All evidence indicates that this is not a safe assumption.
 -- The Clipper Chip story is widely known.
 -- The Snowden documents indicate that many of NSA's favorite
  entry points are backdoors installed by *somebody else*.
 -- Do you really think that opening the "TSA-Approved" lock on
  your suitcase would require "collusion" by a TSA agent?
 -- etc. etc. etc.
This is a very serious public-policy issue.  Cryptographers (e.g.
Matt Blaze) have been called to testify before Congress.  The
point is, when this-or-that agency says something is NOBUS, you
really, really must not assume it is actually NOBUS.
Please do not assume any such thing.  It's bad engineering and
bad public policy.

@_date: 2016-11-23 17:19:10
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
I changed the Subject:, for the time-honored reason that we should
be discussing the ideas, not the person who put forth the ideas.
No.  It's quite a bit more complicated than that.
As always, I don't want to argue about terminology.  If the terminology
isn't helping, we should choose different terminology.  That's a problem
here, because in this context the word "entropy" has been thrown around
so carelessly that it's a struggle to figure out what anybody means by
it.  That's sad, because in other contexts, e.g. in physics, there is an
agreed-upon, very precise, extraordinarily useful meaning.
Indeed in kernel sources and elsewhere, one can find the word thrown
around as an all-purpose synonym for randomness, applied to things that
have no positive lower bound on the actual entropy content, according
to any definition that makes sense to me.
What's worse is that (depending on what you're trying to accomplish)
the physics entropy was probably never what you wanted anyway.
Entropy is just one property of the distribution, and there are other
properties that (usually) matter more directly. The Rnyi functionals
(H) come to mind, and H in particular.  I call H the /adamance/.  It
is possible to construct distributions where the physics entropy is
infinite, but the adamance is less than 2 bits.  I would not recommend
using such a distribution for crypto purposes.
Some people call the Rnyi functionals a form of generalized entropy,
but if you do that, especially if you plan to use it as the basis for
future design work, that's a problem, because nobody will know what you
mean by it.
Again:  I don't want to argue about terminology, but even so, it's not
safe to use unexplained and oft-abused terms as the basis for future
design work.
That's a separate assertion.  It should be enumerated separately.
It's not quite correct.  We agree that the detailed innards of the
whitener don't matter, but the gross black-box properties (such as
word width) do matter somewhat.
I agree with that in spirit.
Actually, not quite.  Whiteners aren't perfect either.  The laws of
statistics don't permit them to be.  To do things properly, you need
to feed slightly more than N bits in, to get N bits out.  Fortunately,
this is not much of a problem, because the excess is quite affordable,
especially when N is not very small.
This touches back to item (1), insofar as it depends on properties
of the whitener.
In any case, if we are going to write down a short list of pithy
commandments, and use them as the basis for future design work, we
ought to get the details right.
That's more-or-less kinda usually true, although once again I would
not promote it to the status of an axiom or commandment.
Again it touches back to item (1), insofar as if you are using SHA512,
then 512 bits of input is definitely not overkill.
More importantly, at some point, you have to address the re-seeding issue.
I'll let them speak for themselves, but there are people on this list
who would vehemently object to item (3).  They have spent years building
PRNGs that constantly re-seed themselves, to the point where the PRNG
becomes in effect a denial-of-service attack on the HRNG.
There is also a metric truckload of issues that have not been mentioned
in this thread heretofore, including:
*) What to do with a newly-booted (or worse, newly-created) system that
 needs a RNG right away but has not had time to accumulate the adamance
 called for in item (1).
*) How to calibrate that this-or-that piece of hardware, so as to obtain
 the required lower bound on the amount of unpredictability it produces.
*) et cetera.

@_date: 2016-11-23 23:15:21
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
That's not a very persuasive argument, for the simple reason
that anybody who believes that, even a little bit, has long
since given up on the field of cryptography and found another
line of work, perhaps as a landscape gardener or as a pizza
That's because the argument applies to everything, not just
the RNG.  If the NSA has pwned the keyboard on which you
type the passphrase, it doesn't matter what you use as your
RNG, or operating system, or browser, or hash function, or
cipher suite.
This leads to:
   RNG Design Principle   We should be just as fussy
     about our RNGs as we are about our ciphers.
I doubt many people on this list would say, "Golly, here
are three different ciphers we found lying around.  We
don't trust any of them, so let's just XOR them together.
That should be hunky dory."
We wouldn't tolerate that for an instant.  Why then should
we accept a similar approach to RNGs?
That's partly a rhetorical question, but partly not.  I
don't know the answer, but here are some possible partial
  a) Some people don't realize how important the RNG is.
  b) Some people think building a good RNG is easy.
   (It's not.)
  c) Some people think building a good RNG is impossible.
   (It's not.)
  d) Combining all the previous items, some people seem
   to think it "should" be trivial, and are not willing to
   do the work that is required.
Please let's stop with the fatalism and defeatism and start
doing what needs to be done.  If that requires a little bit
of support from farther down in the manufacturing chain,
then so be it.  If this requires declaring a few hardware
configurations to be hopelessly insecure, so be it.
As a separate matter, yes, manufacturers should make stuff
a lot more tamper-resistant.
OTOH before we use up all the world's tinfoil in our hats,
we should note that for every vulnerability implanted by
the NSA there are hundreds that result from plain old
mistakes in the programming.  First things first.
In the context of
On 11/23/2016 08:54 PM, Ron Garret asked:
As previously stated, it means before the system has had time
to accumulate the required adamance.  This could be quite a
long time, since the original item (2) asserted that
Well, since the time is unbounded, almost any system will do.
It's hard to think of any operating system where this _isn't_ an issue.  So I suggest the original item (2) is not the best
way to frame the discussion.  It would be better to recognize
the need for a modest lower bound on the productivity of the
Meanwhile, if you want a specific example, let's talk about
Linux.  The aforementioned tardy supply must be measured
against the demand, which includes the following:
Linux uses quite a bit of randomness so early in the boot
process that the root partition has not even been mounted
yet, using it for ASLR among other things.  (If that's not
early enough to make this issue seem relevant and interesting,
we can go into more detail.)
More generally, after / is mounted, systems booted from OEM
images need a substantial amount of high-grade randomness for
host keys, plus additional amounts of generic randomness for
ASLR and UIDLs and suchlike.  On top of that, they need session
keys and IVs before they can communicate with anybody else.
These problems are solvable, but it's a lot of work.  Generally
it is necessary to /store/ some randomness somewhere, which is
itself a tricky challenge.  A list of three or four brief items
gives a very misleading impression of how much work is required.

@_date: 2016-11-24 17:23:31
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
In the context of:
On 11/24/2016 09:01 AM, Theodore Ts'o wrote in part:
Not all of us, Kemosabe.  I would not do that, or recommend
it, or give it any kind of favorable consideration.  The maxim    "garbage in, garbage out"
has been around since the dawn of computing.  As a corollary:
   "garbage cubed in, garbage out"
Note the contrast:
 ++ Combining RNG seed sources makes sense /provided/ there is
  a trustworthy lower bound on the unpredictability of each,
  and source failures (if any) are known to be uncorrelated.
 -- It is not good practice to rely on a combination of sources
  without such guarantees.
Let's be clear:  It's not the idea of combination that is
objectionable.  It is the reliance (with or without combination)
on sources with no trustworthy good properties.
If we are talking about design principles, this seems like a
rather fundamental point.
That makes a good baseline, a good starting point.  It should work
in quite a few (but not all) situations.  As part of a discussion
of concepts and principles, it is a step in the right direction.
Constructive suggestions:
1) Add a validation step:  After recording the signal, play it back.
 If it doesn't sound like hiss, try again until it does.  Rationale:
 It is all too easy to screw up the mixer settings etc. the first
 time you make a recording.  Also there are lots of broken microphones
 out there.
2) Set the sample rate to "CD quality" (44,100 frames per second)
 or higher.
3) Record at least 1 second of data.
Analysis: If it sounds like hiss, it probably contains noise
components out to 4 kHz or higher ... possibly considerably
higher, but we don't need higher.  Also it probably contains
at least 4 bits per sample of raw unpredictability.  That
can be quantified in terms of adamance if you want to be
scientific about it.  If we derate that to 1 bit at 1 kHz,
that still leaves us with 1000 bits of unpredictability, which
should be plenty for seeding a well-designed CSPRNG.
Feed the recorded .wav file "as is" into your CSPRNG.  There
is no need to reformat the data.
One limitation of this method is that it is not suitable for
unattended operation, or for "kiosk" situations where the user
cannot be trusted to cooperate.
In terms of bang for the buck, you can improve things coming
and going, as follows:  Rely on the physics of Johnson noise
(instead of the physics of fluid dynamic turbulence in your
mouth).  On a typical computer you get *more* unpredictability
with the audio ADC open-circuited than you do with it connected
to a microphone.  Seriously, folks:
  A) Nyquist's theory says that Johnson noise power is proportional
   to the impedance, and the microphone lowers the impedance.
  B) I've done the experiment.  Nyquist and Johnson were right.
        It takes more cleverness to validate the input in this case,
because the guaranteed noise (guaranteed by the laws of
thermodynamics) might be buried under other stuff.  The other
stuff is harmless from the RNG's point of view, but it means
you can't validate the signal just by listening.  I have some
fancy tools to help with the validation, but you can get by
with prosaic tools e.g. sox + od + gnumeric.  Look at the FFT.
  It must be emphasized that the work mentioned in the previous
  paragraph needs to be done only once for each make&model of
  machine ... not once per instance, and certainly not once
  every time you need some randomness.  So it gets amortized
  pretty quickly ... unlike the audible noise approach, which
  requires work every time you need some randomness.
Getting rid of the microphone enlarges the number of customers
who can benefit, because although phones and laptops typically
have built-in microphones, desktop-class machines typically
have audio circuits but no built-in microphone.  In the latter
case, the open-circuit solution is cheaper and easier (as well
as better), since you don't need to procure a microphone.
For server-farm-class boards with no onboard audio at all,
you can buy USB audio dongles for a few dollars.  Anybody who
is not willing to do this is not serious about security.
For virtual machines, there are right ways and wrong ways
to proceed.
For IoT-class things that don't have any good way to obtain
randomness, again there are right ways and wrong ways to
proceed.  In particular, the claim that the same RNG software
has to run on all platforms, no matter what, is not reasonable.
Taking the next step down that road, we must face the fact that
there some devices out there that are not secure and cannot
be made secure.  These devices should not be used as a pretext
for giving up on the other 99% of the hardware, or for derailing
the conversation.
Returning to the original claim that Ron Garret put forth
on 11/22/2016 01:03 PM:
I really don't think so.  If you want to see what a RNG looks
like when designed by cryptographers, take a look at:
  Elaine Barker and John Kelsey,
  Recommendation for Random Number Generation Using Deterministic Random Bit Generators
  It's complicated ... even when using a cryptologically strong
hash function as a building block.  Every bit of that complexity
is there for a reason.
If at first you don't see the reasons, keep looking until you do.
Barker and Kelsey are not idiots.
And that's just a high-level architecture.  A real implementation
needs to deal with lots of grotty details, which we can discuss
if anybody's interested.  A list of three or four pithy axioms
tends to wildly underestimate the amount of effort that is needed.
Very specific constructive suggestion:
If somebody would like to do something that would help in a
big way, figure out how to pass a seed via the kernel command
line at boot time ... and then keep it secret thereafter.
Keeping it secret is harder than it might sound, because
command line processing differs from platform to platform,
and because the kernel copies the command line all over the
place and munges it in various ways.
  In general, when something was built without security in
  mind, trying to go back and add security is a big hassle.
This is however worth the trouble, because it is a systematic
way of solving a number of problems having to do with the
need for random numbers early in the boot process.
 -- Grub can pass a seed on the command line.
 -- A sufficiently meticulous user can roll some dice and
  pass a seed on the command line.
 -- A VM host can pass a seed on each guest's command line.
 -- et cetera.
We can discuss this in more detail if anybody is interested.
Bottom line:  It is much better to have a set T of trustworthy
sources, rather than a steaming pile U of untrustworthy sources,
even if the cardinality  is much less than

@_date: 2016-11-26 10:44:56
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
^^^^^^^^^^^^^^^^^^^^^^^
That's a good way of looking at it.
Yes, it would.  If I were in a big hurry, and stranded with
almost no tools, I would do something like that.
However, there are still lots of "plumbing" problems involved
in getting your favorite app(s) to actually use the randomness
you've generated.  Cryptography exists at the intersection of
fancy mathematics and down-to-earth engineering.
Then, alas, there is this:
It is better to /measure/ these things than to spread opinions,
humbly or otherwise.
As the saying goes, there is a difference between data and
information ... and on the other side of the same coin, there
is a difference between data and unpredictability.
In this case:  Almost all of those 500k bits are predictable.
Successive samples of the audio waveform are grossly correlated.
Throwing around that 500k number is like saying Enigma must be
unbreakable because the keyspace (including plugboard) has billions
upon billions of possibilities.  I created a waveform with only
one bit of randomness per sample, at 8k samples per second, and
the ear does not perceive it as being any less random than a
high-data-rate recording of "shhhh".  So the comfortable safety
margin is one or two orders of magnitude less comfortable than
it might appear.
And has been pointed out by several people, you can do better
(as well as better) because you don't need to scrounge up a
microphone.  On a laptop you might have to spend $0.001 for
a piece of wood or plastic to plug into the input jack, to
open-circuit the microphone.
I can undercut that.  For only $59.99 I will sell you a 3.5mm
diameter piece of plastic to plug into your laptop.  Free
shipping!  Act now while supplies last.
In the almost-worst case, if your machine has no audio circuits
at all, or if the existing audio is needed for other purposes,
for only $67.99 I will sell you a USB audio dongle that you can
dedicate to the HRNG task.
Or you could buy one at the corner store for a tenth of that.
Well, as long as people keep assuming that the hard things
are easy, and the easy things are hard, the surprises will
keep coming.

@_date: 2016-11-26 11:20:24
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
Thanks!  That sounds like a big step in the right direction.
This is really important, because it is one of precious few
ways of getting randomness into the system /early enough/.
There remain a couple of points where it might be nice to get
some clarification:
What happens if there is /not/ some on-chip RNG peripheral, or
not one that the firmware knows about?
In particular, is it possible for *grub* to scribble into the
configuration table?  This is important, because what we face
is partly an interface plumbing problem.
  1) There are situations where it would be a big win to have
   grub pass a seed, taken from the grub configuration file,
   updated between one boot and the next via:
  2) There are situations where the best option is to literally
   roll the dice and feed a seed to grub by hand.
    2a) Passing it on the kernel command line is one option,
     which has some charm because grub already fully understands
     the command line.
    2b) Passing it via the EFI configuration table is also fine
     but might require changes to grub.  Note that changing grub
     may be easier than changing the kernel, so as soon as we get
     a kernel that knows what to do with the configuration table,
     this option (2b) might be the way to go.
  3) Any VM host "should" provide both a virtual RDRAND instruction
   and a virtual /dev/hrng, but if it doesn't, a fallback option is
   to send a seed to the client via grub.  VMs already know how to
   talk to grub.  I don't care whether this is done via the kernel
   command line or via some other grub feature.
   Another possibility is for the VM host to munge /boot/grub/grubenv
   directly, before firing up the guest machine, in which case this
   reduces to scenario (1).
This is a multi-piece puzzle.  Having one critical piece in place
increases the motivation to come up with the remaining pieces.

@_date: 2016-11-27 07:08:38
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
I agree with that in spirit.  This seems like great progress.
Here are some possible refinements:
1) There should be a grub command, such as "randomize", on a
 par with other grub commands such as "initrd" or "uppermem" :
   2) GRUB needs to install the RNG seed config table if *either*
  a) a random seed is provided via the randomize command, or
  b) a random seed is available via EFI_RNG_PROTOCOL, or both.
 If both sources are available, grub should hash them together.
 The randomize command may be given more than once, with
 cumulative effect.
3) As a corollary of item (1), a grub environment variable can
 be used as the argument to the randomize command.  However
 the point is that the argument can be something else, perhaps
 randomness from dice rolls, typed in by the user, interactively.
4) We hope that the environment variable will get updated with
 new randomness before the next boot.  More specifically, it should
 be updated as soon as possible after booting up (to keep the old
 value away from prying eyes) and updated *again* as late as possible
 before shutdown (to keep the new value away from prying eyes).
5) Ordinarily the grub environment variable should *not* be cleared.
 Rationale:  Consider the case where the system crashes and reboots
 before the seed (stored in the grub environment) can be updated.
 Reusing a seed is suboptimal, but it's waaaay better than nothing.
 In particular, the hash of a stored seed with the real time clock
 is not too bad, and is waaaay better than either one separately.
6) In the case where the boot firmware does not know about EFI
 configuration tables, but the kernel does, grub can create a table
 ab_initio and pass it to the kernel.
7) A question:  In the EFI configuration table, is there any way
 to distinguish "hard" randomness (such as might come from a HRNG)
 versus pseudo randomness?  In other words, when the system makes
 use of this randomness, how much "credit" should be added to
 /dev/random?
That's fixable.  There's not much need for it to be world-readable,
since the contents are usually boring.  Most people don't even know
it exists.  Also, it would be easy to write an expurgated version,
world-readable, containing everything except the secrets, if there
were any need for it.  This is roughly analogous to /etc/passwd and
It needs to be secure.  Putting it outside the filesystem is an
odd way to make it secure.  If the existing permissions system
is not good enough, we have already forfeited the game, in ways
unrelated to randomness.  The permissions system is easier to
use than whatever tools would be used to store stuff outside
the filesystem.  In either case, attention to detail is required.
Similarly, the grub environment system already exists, with a
logical design, documentation, et cetera.
If/when anybody has a specific suggestion for where to store the
seed, we should definitely look at it.  But until then, the
grub environment seems good enough, and incomparably better
than what we have now.

@_date: 2016-11-27 11:30:21
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
Yes.  I'm convinced.  That's better in terms of security
and compatibility.
This means we need two versions of the command:  one to
read the file, and another to accept input interactively
from the user.  This should be easy enough.
Grub itself does not need such access.  Instead it can rely
on the kernel to mix in the RTC, along with the random seed,
before the first use of the RNG.
  This may impose some requirements on the kernel configuration,
  perhaps specifying "y" (for built-in driver) rather than "m"
  (for loadable module) when configuring the RTC ... but that's
  a price I'm willing to impose.
We agree we shouldn't pretend it's something it's not ... but
still I'm not ready to write off the entire non-UEFI segment of
the population.  In cases where we can't use the UEFI mechanism,
perhaps we can use an /analogous/ mechanism, just so long as it
lets us pass a high-quality seed to the kernel.  I haven't looked
at the code, but I reckon passing a not-quite-UEFI block can't
be too much harder than passing an actual-UEFI block.
Those who put the system "into the box" simply MUST bestow some
randomness on it.  Once the aforementioned mechanism is in place,
we must insist that manufacturers and packagers use it.
We agree, it's a multi-piece puzzle.  But I'm quite optimistic.
Some very large pieces are starting to fall into place.

@_date: 2016-11-27 11:51:54
@_author: John Denker 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
That's a good question.  I do not have a comparably good answer,
but there may be some partial answers.
It has been suggested that one should:
As a corollary:  We need to to inveigle the OS providers and
hardware providers to solve the problem.
Let's discuss the pros and cons of that:
  1) Pro:  It's cheaper.  Having everybody solve the problem
   on a per-app (or even per-library) basis leads to lots of
   duplication of effort.
  2) Pro:  It's more complete.  There are lots of random things
   the operating system has to do on its own, so an OpenSSL-only
   solution will not solve the whole problem.
  3) Pro:  It's safer in the long run.  Having everybody roll
   their own increases the chance that somebody will make a
   mistake.
  4) Pro: It is unreasonable to ask software authors to solve
   a problem that simply cannot be solved by software alone.
   As some guy named John von Neumann said in 1949:
  5) Con:  We have a situation where:
    a) the users want the app to "just work"
    b) the app authors what the library to "just work"
    c) the library authors want the OS to "just work"
    d) the OS authors want the hardware to provide what is needed
    e) but it often doesn't.
   So, part of the problem is that within the "huge uniformed
   community" the users are particularly uniformed about whom
   to blame.  They don't think about security at all until it
   fails, and then the blame the proximate party, hence the
   proliferation of per-app schemes.  (I call them schemes,
   not solutions, for a reason.)
   The "huge uninformed community" thinks the RNG problem is
   about 100 times easier than it really is.
  6) We don't even have a good specification for what the platform
   "should" provide.  Among other things:
     -- /dev/random offers no guarantees of availability
     -- /dev/urandom offers no guarantees of quality
   This is a terrible situation, but one can almost understand how
   it developed.  Given the state of the art until recently, if
   there had been any specified standards, the system would have
   been unable to meet them.  The growing demand for security in
   general and randomness in particular has outpaced the supply.
   By way of example, here is something that might go into such
   a specification:  There should be *one device* ... or if for
   back-compatibility there are two, they should behave the same.
   The device should guarantee 100% availability and 100% high
   quality, high enough for all practical purposes.
   Let's be clear:  a proper RNG device should never block, *and*
   there should never be any temptation -- or even possibility --
   of using the device (or the corresponding intra-kernel function
   call) before the RNG is well and truly initialized.
      Of course a PRNG can always be attacked, but we can make
      it just as hard as a dozen other things in the system.  If
      the attacker can read the PRNG state, he can also read
      your passphrases and certificates directly, which tells
      us we simply must prevent such attacks, at a point well
      upstream of the PRNG.
Bottom line:  There are big problems here that need to be fixed.
However, I don't think that fixing them within OpenSSL is the
right way to go.

@_date: 2016-11-28 10:47:07
@_author: John Denker 
@_subject: [Cryptography] unique system ID 
The standard approach is:
On the machine of interest (virtual or non-virtual)
  :; dmidecode -s system-uuid
On the host, when hosting a virtual machine:
  : host;  if ! test -r guest.uuid ; then uuidgen -r > guest.uuid ; fi
  : host;  qemu-system-x86_64 -uuid $(cat guest.uuid)
If that's not good enough, please clarify the question.

@_date: 2016-11-28 16:10:15
@_author: John Denker 
@_subject: [Cryptography] OpenSSL and random 
OK, that got my attention.  That seems to be an offer to
accept more than a fair share of the responsibility.  That's
nice, especially compared to the opposite:
    If somebody is eager to do the Right Thing we should all
encourage and support the effort, within reason, while
keeping an eye on what's possible and what's simply not
One tiny step in the right direction would be to upgrade the
openssl documentation, to include a warning.  Here's a first
draft, to start the discussion:
That leads to the question of what, if anything, openssl could
actually do that would help.
For starters, as some guy named Hippocrates pointed out, we should
try to not make things worse.  That's tricky, because some folks
might come to trust openssl and then be led down the garden path,
no matter how many warnings there are in the documentation,
especially if the warnings are non-specific and non-constructive,
and especially given that nobody ever reads the documentation.
Taking the next step down that road, we ought to ask whether it
is possible for library packages (or anybody else) to *detect*
the most prevalent security problems.
  Clearly the problem is not solvable in the general case.  If
  some genius has written the root password on the underside of
  the machine, no library package is going to detect that.
  Also if Alice has booted from a "Live CD" such that essentially
  everything is a replay of the previous boot, it is very hard for
  the system itself to detect that.  Eve knows, but Alice doesn't.
  Hypothetically we could detect that the system is booted from
  a Live CD and warn the user, but that would lead to a bunch of
  false positives, because not all Live CDs are insecure, and
  falsely crying wolf is terrible from a security point of view.
  All this is related to the fact that if we just analyze the
  outputs, a poorly seeded RNG looks an awful lot like a properly
  seeded RNG.  Eve knows the difference, but Alice quite plausibly
  might not.
To summarize this subsection:  This might be one of those odd situations
where it is easier to fix the problem than to detect it.  Also it's a
bit like a cancer vaccine:  By the time you have detected the problem
it might be too late, so it's really better to prevent it.
Here's a more-or-less constructive checklist.
1a) Do the ioctls to find out whether /dev/random thinks it has plenty
of so-called entropy.  Entropy isn't exactly the right concept,
and the kernel doesn't account for it properly, but even so, if
there is "plenty" of it, you have reduced the attack surface by
many orders of magnitude.
1b) Grab some bytes from /dev/random and stuff them into /dev/urandom.
This step is unnecessary in "most" cases, since "most" versions of
case, once you're sure urandom has been seeded, use it for whatever
you need to do.
2a) If the CPU supports RDRAND and/or RDSEED and/or a HWRNG device,
it is exceedingly likely that /dev/random is already using that,
so this reduces to case (1), but if not, you can grab some random
bytes and stuff them into /dev/urandom, then proceed as above.
Beware that /dev/hwrng can easily block, especially on a VM guest.
So if you just check that the device exists, you can't assume that
Beware that an unwise reseeding strategy is for all practical
purposes a denial-of-service attack on the upstream source(s)
of randomness.
3a) If neither (1) nor (2) is successful, and/or if you don't trust
whatever sources are being used, see if there is an audio system.
Maybe the simplest thing is to just bundle something like Turbid
with openssl, and tell people to use it.  I've been slowly working
on a second-generation Turbid, with the goal of making it easier for
Muggles to use.  For one thing, it uses a GUI, rather than relying
on a bunch of arcane cmdline commands.  Also I broke the documentation
into two parts: a set of down-and-dirty instructions for the user
in a hurry, plus a much deeper discussion for the aficionados. If you're running on a distro (perhaps a Live CD) that doesn't have
Turbid but does have arecord, you can check for a noise.wav file
in some path.  Make the path part of the openssl configuration.  If
the file exists, and has plausible contents, and has a trusted owner
(either root or the current user) and sensible permissions, and
isn't too old (certainly not before the current boot), then you
can stuff that into /dev/urandom and proceed as above.
If the platform doesn't have an audio system, tell the user to
spend a few dollars on a USB audio dongle.  A user who refuses
to do that is not serious about security.  At least when the
non-serious user gets pwned everybody will know it was his own
fault.  We gave him multiple opportunities to do things right.
3b) If the noise file does not exist, you can try to create it.  This
is gross hackery, but it's better than nothing.  Get the user to open-
circuit the audio input.  On a desktop this is trivial;  on a laptop
it might require a bit of wood or plastic(*) with a diameter of 3
or 3.5 mm to stuff into the mic port.  Then:
   arecord --disable-softvol -d 3 -f S32_LE -r 44100 /var/run/noise.wav
Play it back to make sure it sounds OK.  Then proceed as in (3a).
     (*) Consider using one tine of a plastic fork, or perhaps
     the ink tube from the inside of a cheap ballpoint pen.
     Seriously, folks, this is not tricky or expensive.
4) When booting from a Live CD, e.g. when bringing up a new system,
or bringing up a system with a crashed disk, instruct the user to
prepare a thumb drive.  The user ought to be doing that already,
to bring over personal keys et cetera, but make sure it also has
a random seed file.  Unless another trusted source is more easily
available, openssl should check for the existence this, then stuff
it into /dev/urandom and proceed as above.
This is yet another case were a one-dollar piece of hardware
solves a problem that is simply not solvable by software alone.
5) Ship a tool that allows people to download a Live CD image,
munge the image to add a unique random seed file, and only then
burn the image to CD.
I have some code that more-or-less does this, although I haven't
looked at it recently.
This works hand-in-hand with some of the grub enhancements that
Ard Biesheuvel has been talking about.
This doesn't seem particularly central in the set of things that
people expect of openssl ... but if you are serious about being
super-responsible and diligent and doing the Right Thing then
things like this will help.  Ship the necessary tools with openssl
in the short term and push them upstream in the slightly-longer

@_date: 2016-11-29 08:13:55
@_author: John Denker 
@_subject: [Cryptography] read-once file, outside the filesystem ... or not 
This is never going to be implemented on any significant scale,
for two reasons:
1) Read-once isn't the right semantics.  There are lots of cases
 where the seed would need to be read more than once.
2) Putting it outside the normal filesystem is nothing more than
 security by obscurity.  There have to be tools to install
 the thing.  The tools need to have permissions.  The existence
 of such tools is basically a big red arrow on the Death Star
 plans, saying "Attack Here".
3) To say the same thing in more constructive terms:  This is
 yet another of those cases where we really ought to rely on the
 standard mechanisms, e.g. file permissions ... and if that's
 not good enough, it's a serious bug, and needs to be reported
 upstream in the usual way.

@_date: 2016-11-29 13:19:18
@_author: John Denker 
@_subject: [Cryptography] RNG design principles 
There is no but.  It must be properly seeded, always.
Yes, that would be a step in the right direction, but it's
not the whole story.  We face a fundamental problem that
cannot be solved without coordination between the kernel
team, the grub team, the hardware team, and others.
Seriously, von Neumann was right:  It is absolutely not
possible to produce a random distribution using software
alone.  No amount of lobbying is going to change this.
No, it must never block.
Here is crypto design principle 137) The system must provide a unified, good RNG device,
 namely one that has 100% availability along with 100%
 high quality, high enough for any earthly purpose.
 Rationale and other observations:
 a) If /dev/urandom or /dev/random blocks, applications (and
  libraries such as openssl) will not rely on it.  Instead
  they will roll their own PRNG, with predictably terrible
  results.
 b) If /dev/random or /dev/urandom returns insufficiently
  unpredictable bits, applications (and libraries) will use
  it to produce insecure products.
 Other ways of expressing this principle include:
  -- Neither /dev/random nor /dev/urandom should be permitted
   to block.
  -- Neither /dev/random nor /dev/urandom should be permitted
   to emit untrustworthy bits.
  -- When designing the "main" RNG that is offered to users,
  if the designer ever needs to make the tradeoff, namely
  blocking versus producing untrustworthy bits, the game is
  already lost.
 c) If this requires storing some randomness so that it is
  available super-early during the boot process, so be it.
  If this requires improvements to the kernel, grub, userland
  tools, and (!) hardware, so be it.
 d) Minor additional point:  It might be nice to assign a proper
  name to the unified trustworthy device, perhaps /dev/jrandom
  (as in J. Random Hacker) or /dev/vrandom (as in Very random,
  also the lexical successor to urandom).
  That serves as a way of advertising its features to applications
  (and libraries).  Then the advice is simple:  If the good device
  exists, use it.  On systems where it exists, it can be aliased
  as /dev/random and /dev/urandom et cetera, so that legacy
  applications continue to work, and indeed work better than
  before.
  On systems where the good device does not exist, applications
  are stuck with unanswerable questions about whether to use
  /dev/random or /dev/urandom, neither of which reliably solves
  the problem.
Also:  Please let's not imagine that the right answer can be
expressed as a list of 3 or 4 pithy axioms.  It's very much
more complicated than that.
While we're in the neighborhood, here's a recommendation:
138) It is best to avoid the word "entropy" in this context.
 It almost never expresses the right idea.  If you don't
 need to be quantitative, you can use words such as
 "randomness" or "unpredictability" or "unguessability".
 At the next level of detail, you might choose to distinguish
 "pseudo randomness" from "hard randomness" (such as might
 come from a hardware RNG).
 If you wish to be quantitative, entropy is still almost
 never the right idea.  Depending somewhat on your threat
 model, you might want to quantify the /adamance/ which is
 a name for the Rnyi functional H[P], although there are
 innumerable other functionals that may be of interest
139) I'd like to re-clarify a previous point about combining
sources of randomness.  We must distinguish  (better than
or equal to) versus  (much better than):
 4 trustworthy sources
   3 trustworthy sources
     2 trustworthy sources
      1 trustworthy source
                                
                                  
                                     4 lousy sources
                                                   3 lousy sources
                                                    2 lousy sources
                                                     1 lousy source.
Sure, combining sources is better than not combining sources.
I don't have a problem with that.  The problem is that it is
*not enough better* to make up for the difference between a
good source and a lousy source.
I am not recommending a single source.  My point is that *any*
number of good sources is better than *any* number of lousy
The problem isn't the combining.  The problem is the reliance
(with or without combining) on sources that have no demonstrable
good properties.  Arguing that such-and-such source is "better
than nothing" is not acceptable, because lots of sources are
*not enough better* ... in contrast to actual well-engineered
good sources, which are available at modest cost.

@_date: 2016-11-30 12:43:30
@_author: John Denker 
@_subject: [Cryptography] OpenSSL and random 
If that suits _your_ purposes, congratulations ... but no, it is not
the right answer for openssl, and never will be, for reasons that
Rich Salz and others have pointed out:
  Given an RNG that blocks, a great many users won't tolerate it
  in any critical path.  Instead they will roll their own PRNG,
  with predictably terrible results.  A library must not foist a
  blocking RNG on its users.
  The Right Answer is to procure a RNG that just never blocks
  under any relevant conditions.  This is not easy, but it is
  doable on most platforms.
     There will always be some platforms that are not secure and
     cannot be made secure, but we must not let this derail our
     train of thought.
I say again:  /dev/random must learn to never block, and /dev/urandom
must learn to never emit untrustworthy bits, whereupon the specification
is the same for both.  The same goes for getrandom() and getentropy():
they must never block, and they must never emit untrustworthy bits.
I'm not saying this is easy, but it has to be done.  Anything short
of this fails to solve the problem faced by a great many apps and
libraries (e.g. openssl).
FreeBSD has been mentioned.  They got this _partly/ right.  Their
right direction, but both may block, rather like getrandom(), which
is just not good enough for critical applications.

@_date: 2016-10-01 10:08:45
@_author: John Denker 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct DSA 
The important thing is the notion of /repudiation/ as hypothesized
in the Subject: line.  Repudiation does not happen in a vacuum.
Consider what happens when you try to repudiate a document that has
been signed in a way that matches your public key.  You try to claim
that some bad guy forged your signature without using your private
key, using some "other" private key instead.  Everybody else says
no, you are the bad guy.  Your choice of keys proves you are the bad
guy.  In other words, the attempted repudiation is self-defeating,
at no cost to the other party.
  In contrast, good guys use valid keypairs.
Openssl generates valid keypairs, as far as we can tell, so there
is no openssl issue here.  Overall, this discussion is sound and
fury, signifying nothing.
  In contrast, if there were a way for innocent parties to accidentally
  generate invalid keypairs, that would would be worth discussing.

@_date: 2016-10-01 11:26:28
@_author: John Denker 
@_subject: [Cryptography] another security vulnerability / travesty 
Yes harder ... but wow, that's a really low bar.
Are you claiming it is "hard enough"?
Hard enough for what purpose?
If your main threat is script kiddies intercepting your email, then
OK, sure, faxing is somewhat harder to get at.
My point is that just because your desktop system didn't come preloaded
with tools to intercept faxes and voice calls doesn't mean the tools
cease to exist.
The topic hasn't been discussed much in this forum, but that doesn't
mean it ceases to exist.
Let's be clear:  As the proverb says, never confuse the absence of one
thing with the presence of another.  I said fax was not secure.  I
never said that email was better.  I asked an open-ended question:
   Suppose your aunt wanted to submit some medical documents
   to a clinic on the far side of town.  Short of hand-carrying    the documents, what would you recommend?
That's a sincere, non-rhetorical question.  I haven't heard an answer.
Since you ask:
 *) The motto of the NSA is collect it all, process it all, exploit it all.
  Their notion of "all" definitely includes faxes.  Reference: Snowden.
Fax intercepts have been going on for a long time:
 *)    "Jurors in Mobile steroids trial hear wiretapped phone calls, see intercepted faxes, e-mails"
  (2010)
 *)   "According to a European Parliament report, published in 2001, America's   National Security Agency (NSA) intercepted faxes and phone calls between
  Airbus, Saudi Arabian Airlines and the Saudi government in early 1994."
Perhaps more to the point, sometimes we see an attack against 100 million
low-value targets, but sometimes we see an attack against a much smaller
number of high-value targets.  NSA calls this "tailoring".
Also consider this:  Collect the faxes going to/from an AIDS clinic, or a
family planning clinic, or a civil-rights field office ... and then do a
"join" against the stolen OPM security-clearance database.  That tells
you exactly whom to blackmail.
The point is, you don't need to collect 100 million faxes to have an
impact, if people assume faxes are secure when they aren't.

@_date: 2016-10-01 11:58:26
@_author: John Denker 
@_subject: [Cryptography] distrusted root CA: WoSign 
I asked:
We have lots of choices short of dropping off the grid, some of
which are mentioned below.  Some of the worst abuses could be
greatly reduced by applying Crypto 101 principles.
Actually, for most lusers, it is the _distro_ packager who gets the
final say.  The packager decides what browser and what certificates
get bundled into the distro (and into the updates).
We saw the downside of this when Lenovo foisted the infamous "superfish"
onto their customers.
The browsers had a difficult time disabling the superfish certificate,
due to pathetically bad security design.  Crypto 101 (not to mention
etymology) suggests there should be only one true root.  The hundreds
of certs that currently claim to be "roots" should exist at the second
level, /signed/ by the one true root.  That would create a revocation
pathway that does not currently exist.
We could also implement widespread pinning and cross-signing.  This
would make it vastly easier to detect forged certificates.
Et cetera.
Bottom line:  We *do* have choices.  There are a *lot* of things that
could be done better, without dropping off the grid.
FWIW  is issuing free certificates, which
seems like a major departure from the status quo ante.  They are
at the "beta" stage.  The fat cats have not (yet) found a way to
shut them down.
I am a happy beta customer of letsencrypt.

@_date: 2016-10-02 13:03:11
@_author: John Denker 
@_subject: [Cryptography] identification / authentication / trust / 
I 99% agree with that in spirit.  There is however a counterargument
that should be given "some" consideration.  It starts with the following
vague mission statement:
  "We would like to conduct e-commerce on the scale of 300 billion dollars
  per year, so we need a setup that is (a) easy enough for consumers to use,
  (b) secure enough that consumers will type in the credit-card numbers,
  and (c) secure enough that merchants will want to play along."
In the context of that rather vague mission statement, one could argue that
PKI, as implemented by the current CA/browser scheme, has been a success.
One counter-counterargument is that what we have now is so poorly designed
that it could collapse at any moment.  The canaries in the coal mine died
a long time ago;  we are now seeing miners drop dead (e.g. WoSign/StartCom).
Steps that could be taken to help forestall an explosion are not being
A related argument is that the CA/browser model is being extended beyond
e-commerce, extended to more demanding applications where it cannot handle
the load.
Excellent.  That makes several important points.
When you say "we really need pinning and we really need certificate
transparency" how to they respond?
When you say "we need much wider use of X.509v3 Name Constraints" how
do they respond?
    I mean seriously, does the Hong Kong Post Office really need
    unrestricted authority over every domain-name in the world?
When you say "the browser should have one true root, and the existing
CAs should exist at the /second/ level, /signed/ by the one true root"
how do they respond?
   This would create a revocation mechanism that does not currently exist.
   Also, this wouldn't make superfish impossible, but it would make
   it much harder to pretend that it was an innocent mistake.  Bad
   guys would have to hack the browser in ways that would flagrantly
   violate Mozilla's trademark.
When you mention cross-signing, how do they respond?
  This does not solve all the world's problems, since an advanced
  persistent threat can subvert two CAs almost as easily as one
  ... but it does help with a few things, such as when the old CA
  signs off on the transfer to a new CA.
When you mention having the old leaf certificate cross-sign its
replacement, how do they respond?
  This is a Big Deal, because it means the attacker would need to
  subvert the key owner (not just the CA).
I would not have called that "the" key challenge.
Yes, people /want/ to solve that problem using PKI.  It could be argued
that e-commerce is a real thing, so "some" version of the problem has
been solved.
On the other hand, there are still lots of vulnerabilities.  So we
are faced with the challenge of redesigning and rebuilding an aircraft
on the fly.
There are at least three crucial concepts that must be distinguished:
 -- continuity;
 -- establishing identification/authentication; and
 -- establishing authorization and trust.
It is quite possible to have continuity in the absence of authentic
identification.  An example would be a long-running discussion with
some pseudonymous pen-pal.
It is also quite possible to have identification/authentication can
be used to establish continuity, but it's not the same thing.  Both
can exist without trust.  For example, I might know with absolute
certainty that a certain key belongs to a certain recently-paroled
arsonist, but that does not mean I would /trust/ him to serve as
night watchman at the lumber yard.
Notable efforts in the area of key management include:
  -- the CA/browser model
  -- the PGP model, with "web of trust" and "key signing parties"
  -- the SSH model
  -- various credit-card models (card + signature, card + PIN, etc.)
  -- etc.
Note that SSH actually does support the notion of CA, but typically
users just rely on the continuity (pinning) features, and establish
identification and authorization out-of-band.
I suggest that browsers should become more like SSH, relying much
more on pinning and much less on CAs.  This does not require giving
up CAs entirely.  However, if the CA says one thing and the pin
says another, that should set off alarm bells.  There is a distinct
possibility you are under attack.  I'm not sure what is the right
way to proceed in this case, but blindly deferring to the authority
of the CA is definitely not right.
I use PGP, but I have never used the "web of trust" features.  In this context, I have no idea what "trust" is supposed to mean.  I have no idea how to quantify or even define "trust" in the abstract.
In the context of e-commerce I might /authorize/ the merchant to
charge $14.99 against my account, but that does not mean I bestow
any other form of trust or authorization on the merchant.
Back when everybody lived in the same village, identification implied
a certain amount of trust, because if somebody breached a contract
your enforcers could easily get in touch with him.
  In contrast, being able to   does not establish trust, because the website might far away,
  beyond the reach of ordinary enforcement mechanisms.
The current CA/browser setup seriously screws up the distinction
between identification and trust.
Trust is a tricky thing.  Cryptography cannot create trust out of
nothing.  It can perhaps sometimes facilitate a discussion that
leads to a modicum of trust.  A typical CA certifies the /name/ of
the website but does not pretend to certify trust ... yet customers
are instructed to trust the connection.  It's insane.  Meanwhile,
on the web there are various sources of information about the reputed
trustworthiness of various actors.  At present there is no direct
way of binding this information to the certified names ... but there
could be.  This is a fixable problem.
These are just some of the many many reasons why I say that we in the
security community have lots of options.  There are lots of things
we could do, and lots of recommendations we could give to non-experts,
short of recommending that they drop off the grid.
For starters, let's recognize that continuity (aka pinning) is very,
very important.  We can make it very efficient to re-visit a site.
  In contrast, as J.L. pointed out, relying on a CA on the second visit
  is a step in the wrong direction.  It means that a hundred different
  CAs can get in on the action, and each one becomes a single point
  of failure.  This is insane.
The /first/ visit to a site should have much lower requirements for
efficiency, and much higher requirements for security.  Among other
things, the first visit should require vastly more authentication
of the client ... and of the server!
Let's be clear:  It is important to distinguish the first visit from
the typical visit.
My experience says otherwise.  I've been there and done that, using
standard software, e.g. apache and firefox.
So just set up your own CA.  I use a hacked-up version of the CA.pl
script that ships with openssl.  I wrote 12 pages of instructions to
guide me through it, which means it is more complicated than it ought
to be, but it's doable.  One of the steps is teaching the browser
about the new root certificate, which is not particularly hard.
Eradicating extraneous roots is even easier.
This lets me set things up the way I like.  A good maxim is:
   I /trust/ such-and-such certificate because I issued it!
I've used this maxim in the context of TLS, SSH, IPsec, et cetera.

@_date: 2016-10-02 13:46:07
@_author: John Denker 
@_subject: [Cryptography] phone firmware ... to mod, 
I agree, the Android update situation is underwhelming.
Would anybody care to comment on the pros and cons of replacing
Android with some aftermarket firmware, e.g. CyanogenMod?
Is this likely to close more security holes than it opens?
Some basic pros and cons can be found here:

@_date: 2016-10-06 12:26:16
@_author: John Denker 
@_subject: [Cryptography] Should NSA & Cyber Command Have Separate 
that can be reduced to:
  The arguments in that article make no sense.  They seem like
something out of a Dilbert cartoon.  The key claim is:
The specific conflict is:
The alleged distinction between offensive and defensive military
organizations is unusual, to say the least.  The Maginot line was
considered primarily defensive;  however:
  a) Even so, it had cannons and other firearms.
  b) It has become an infamous symbol of ineffectiveness
   (although this is not entirely fair).
At the opposite extreme, paratroops are thought of as optimized for
attack, yet the 101st Airborne was called upon to dig in and defend
The cryptography point of view is not the only point of view, but
it is certainly high on the list of relevant considerations.  In
crypto, one can distinguish between code-making and code-breaking.
However, it would be madness to separate these organizationally,
because if you don't know how to break codes you don't know how to
make them.
When applied to military operations, if you can break the enemy's
code it helps you both on offense and defense.  Indeed, almost any
battle between forces that are approximately evenly matched will
tend to be fluid, shifting from offense to defense from place to
place and from minute to minute.  The word "counterattack" exists
for a reason.  Sometimes it's impossible to draw the distinction;
for example, it hardly matters whether WWII antisubmarine warfare
was "defending" the convoys or "attacking" the U-boats.  So-called
stealth technology seems defensive, and indeed hardly seems like
a weapon at all ... but that doesn't mean that a stealth bomber
is defensive.  So we conclude, again, that splitting the NSA along
offensive versus defensive lines makes no sense.
One can also distinguish between passive crypto (eavesdropping),
active crypto (MITM), quasi-crypto (side-channel), and non-crypto
("tailoring" and black-bag jobs) ... but again it would be madness
to separate these organizationally.
Here's yet another reason why you wouldn't want to separate
offense from defense /or/ separate code-breaking from code-making,
even if you could:  The code-breakers will always have an unfair
advantage when it comes to budgeting and other forms of bureaucratic
infighting.  That's because they have spectacular successes to show
off.  In contrast, when the code-makers succeed, the best case is
that nothing exciting happens.  This is a perennial problem.  It
is not entirely solvable, but the only hope is to have a strong
and wise leader who has responsibility for the /whole problem/.
To summarize:  The arguments in the article make no sense.  So the
remaining task is to figure out what the stake-holders /really/ want.
One thing that must always be considered is that the folks
involved want to advance their own careers.  Splitting might
mean that the three-star in charge of Cyber Command gets to
move up to four stars, and everybody who reports to him gets
to move up one rank also.
One distinction that would make a certain amount of sense would
be military versus commercial.  The article does not address that.
The NSA is a wholly-owned subsidiary of the Department of Defense,
and splitting Cyber Command out of NSA wouldn't change that.
The civil/military distinction would make sense, because most of
the US economy does not (yet) belong to the military, and because:
  "In the long run it was more important to secure one's own   communications than to exploit those of the enemy."
                    	 -- Frank Rowlett (1942)
An example to illustrate the sort of thing I'm talking about
is NOAA, which makes weather information available to farmers,
sailors, fliers, hurricane-evacuation planners, et cetera.
We could apply that idea by having a trustworthy organization
that works with industry to improve their information security
practices.  There's a real problem here, because NSA has earned
a great deal of distrust.  Splitting would not suffice to fix
In any case, that's not the sort of split The Powers That Be
are pushing for, so we must continue to wonder what they're
really up to.
Management efficiency cannot be the real reason.  There are
already 16 large agencies that officially make up the US
Intelligence Community (and that is arguably an underestimate,
depending on how you count).  Increasing this by one isn't
going to be a game changer.
Splitting might result in a short-term improvement in performance
due to the Hawthorne effect.  Picking two agencies at random
and combining them would have a comparable effect.
On the other side of the ledger, splitting would cause short-
term turmoil, disruption, and distraction, as people run around
inventorying the paper clips etc. and deciding which resources
get assigned to which successor organization.
In the long run, splitting might increase tribalism and invidious
competition ... although there is already so much of this that
it's hard to imagine it getting much worse.
The article cites the example of the Air Force, which spun
off from the Army, 70 years ago.  However, for 55 of those
years, the trend has been toward /combined arms/ and away
from stovepiping ... for good reasons.
The NSA mission statement and the Cyber Command mission
statement are different in wording more than in meaning.
It would make just as much sense to have NSA report to
Cyber Command as vice versa.
    In my experience, when it comes to management structures,
there are an infinite number of ways of doing it wrong.
However, there are also a near-infinite number of ways of
doing it right, which means that most optimizations are
not worth the trouble.  People who want to get stuff done
will find ways to work across organizational lines to get
it done ... whereas people who don't care will find ways
to define everything as "not my job".
  By the time you get through fiddling with the org chart
to make it reflect the "current" reality, that reality
is no longer current.  A mostly-new set of ad-hoc teams
has been formed while you were fiddling.
Management should be judged on how quickly it can set up
ad-hoc teams to get stuff done, or at least how well it
can support worker-bees who set up such teams on their
own.  This requires /trust/ which comes from /integrity/.
Specifically, this means that if I was given a budget to
do X, I can reallocate it to do Y, if I think that's the
right thing to do.
  I've seen some organizations that worked this way, and
  some that didn't.  It's not an exact science.

@_date: 2016-10-30 14:56:30
@_author: John Denker 
@_subject: [Cryptography] Nundrum cipher = hypothetical WWII cipher machine = 
I prefer to look at a more challenging question, namely how
to create a more secure machine using technology that was
*available at the time*.
Yes indeed.  A related point is that the state vector is just
not large enough.
Cryptography exists at the intersection of fancy mathematics and
down-to-earth engineering.  Here is my attempt at a machine that
could be built using methods and materials of the time:  It's
called the Nundrum machine:
   The key idea is to use a large number of very simple rotors.
This stands in contrast to Enigma, which used a fatally-small
number of unnecessarily-complicated rotors.
It uses a large state vector, with relatively rapid changes
in state.
It comes with procedures to ensure a random session key, i.e.
no cillies.
I'm not claiming it's unbreakable in the modern sense, just
that it would have been ridiculously hard to break using the
methods of the time.
I would argue that codemakers of the time worried too much
about what happens when different messages are sent using
the same session key ... and didn't worry enough about making
sure you just never see a repeated key or a related key.
The design basically follows a checklist approach:  Look at
the weaknesses of Purple, Enigma, and Tunny, and don't make
those mistakes.  Those machines were just barely breakable at
the time, and relatively small improvements would have pushed
them beyond reach.  Nundrum avoids the worst features and
borrows the best features from each, and from NeMa and SIGABA.
It tries to uphold Kerckhoffs' principle:  Even if the adversary
knows everything about the machine, they still can't break it
without the key.
Some diagrams and 3500 words of discussion can be found at
  It might or might not be useful as a novelist's Plot Device
with a
The illustrations are readily understandable, but perhaps /too/
readily understandable, and not sufficiently flashy.  Perhaps
the novelist could lampshade this by saying the power isn't in
the complexity but in the simplicity ... and in the large number
of rotors.  It's an emergent property.  One quill taken from a
porcupine isn't very scary, but the living porcupine as a whole
is not to be trifled with.

@_date: 2016-09-07 17:24:34
@_author: John Denker 
@_subject: [Cryptography] Secure erasure 
AFAICT it is simply not possible to write a truly secure erase
routine in C.  The language is defined with respect to an abstract
machine that does not support the concept of erasure, secure or
  You can implement "some" measures that take away "some" of
  the attack surface, but that's far removed from a complete,
  provably-correct solution.
The language is guaranteed to produce "the" right answer and leave
it in "the" appropriate place.  It offers no guarantees as to how
many extraneous copies are left lying around.
For starters, on a typical system, you have no idea how many times
your program was interrupted.  Each context switch saves the
registers "somewhere".  You have no idea where.  This happens
at a layer several jumps removed from the language specification.
Similar words apply to swapping.  Sometimes you can lock your
variables in main memory ... but that is far outside the C
It gets worse.  If you are running on a virtual machine, which
is not particularly rare these days, the host can pillage your
memory and there's nothing the language can do about it.
"Volatile" means that the memory location could change behind
your back, as is typical of PDP-11 memory-mapped I/O registers,
and typical of memory shared between multiple processes.  It
offers not the slightest guarantee of security against copying
the value.  Volatile applies to the memory /location/; it does
not protect the /value/ that you tried to store there.
The guys who defined the language were not thinking like cryptographers.
This was in the early 70s, when telnet expected you to send your
password in the clear over the network.
Data security includes two things:
  a) secure against loss of information, and
  b) secure against unauthorized copying.
Objective (a) suggests making lots of backups;
Objective (b) suggests just the opposite.
To say almost the same thing another way:
The typical operating system clears stuff (memory, swap space, registers,
et cetera) as late as possible, right before giving it away to some new
process.  Type-(b) security would suggest clearing it as early as possible,
but that would incur a performance penalty, so they don't do it.
By not doing it, they leave a large attack surface, including cold boot,
reading /dev/kmem, reading /dev/swap, et cetera.
As long as the operating system is running correctly, you don't need to
worry.  Clearing stuff late is the same as clearing stuff early.  Erasure
is only important if you think the OS is going to lose control.  That
includes things like cold-boot attacks, attackers with root privilege,
et cetera.  All such attacks are waaaaay outside the specification of
the abstract machine used to define C.
AFAICT if you want to shrink the attack surface, it requires rethinking
and redesigning everything including hardware, operating system, and
language.  There needs to be a concept of protecting the /value/ from
undesired copying.  This is very, very different from protecting a
memory /location/ from undesired loss of information.
One approach is to define a smallish security perimeter, perhaps the
CPU chip itself, and require that everything that crosses the perimeter
must be encrypted.  Then "crypto erase" consists of throwing away the
session key, then zeroizing any copies inside the perimeter, including
L2 cache, general registers, MMX registers, et cetera.
Zeroizing flash memory requires changes to the flash controller, at a
level only one step above the raw memory transistors.  This is verrrry
far removed from the C language specification.  It would make a epic
Master of Engineering thesis project for some hotshot.

@_date: 2016-09-09 10:47:33
@_author: John Denker 
@_subject: [Cryptography] Secure erasure 
Non-peripheral non-parenthetical answer:  The issues here are far broader
than C in particular, and far deeper than computation in general.  I have
once again removed "C" from the Subject: line.
I recommend:
  Laura J. Heath,
  "An Analysis of the Systemic Security Weaknesses of
   the U.S. Navy Fleet Broadcasting System, 1967-1974,
   as Exploited by CWO John Walker"
  Master's Thesis, U.S. Army Command and General Staff College (2005)
Here's the nut graf:
Emphasis in the original:  /data/.
Protecting secret /data/ aka /information/ is a lot harder than protecting
secret documents.
As one small example of the larger problem, the "volatile" keyword is
supposed to tell the compiler something about the memory location where
you intended to store the information, but provides absolutely no
guarantees about the /information/ itself.
Furthermore, labeling the location as "volatile" does not make it so.
Context switching, caching, swapping, wear-leveling, etc. can make copies
of the /information/ at levels where your favorite programming language
has no control whatsoever.
All the secure erasure routines I've seen appear just as fatally flawed
as protecting the crypto key cards and other documents, but not protecting
the /information/ thereon.  They reduce the attack surface "some" but not
very much.  Small refinements are not going to help.
Here's the image that comes to mind:

@_date: 2016-09-09 17:28:07
@_author: John Denker 
@_subject: [Cryptography] Secure erasure 
That argument, oversimplified as it is, cuts both ways.
Back in the Olden Days some operating systems didn't bother to clear
memory before allocating it to a different process,  That wasn't even
particularly unreasonable on a single-owner single-user machine, but
it meant that you could spy on another process just by doing a bunch
of malloc()s.  In contrast, a modern operating system is "supposed"
to be more fastidious than that.
So the oversimplified argument reduces to this: Either you trust the
hardware and the OS to not give away your information, or you don't.
  -- If you don't, then what makes you think it is safe to do any
   crypto at all on this platform?   Your information could be given
   away shortly after it is acquired and before it is erased.
  -- If you do, then what is the added value of your so-called
   secure erase routine?
The way forward is to stop oversimplifying.  Start by asking What's
Your Threat Model.  The real threats come from verrrry far outside
the domain of specification for ordinary computer languages:  cold
boot, attackers with root privilege, NSA "tailoring" (i.e. subversion)
of the firmware in the flash controller, tempest, et cetera.
Let's be clear:  A zeroization routine that might have worked
just fine on an Apple ][e floppy is not good enough for a modern
hard disk, and is guaranteed to do almost nothing on a flash
drive except waste electricity and shorten the useful life of
the drive.
   There are things that could be done, but they involve a lot
   more than writing code with "volatile" keywords.  Support
   is needed at the OS level and the hardware level, down to
   a layer one step above the memory transistors.
This is one of the reasons for the existence of Hardware Security
Modules.  If the bad guys can't get inside, it doesn't matter
whether the software makes frequent calls to the "secure erase"
routine or not.
Here's a low-tech approximation that illustrates the idea:  Put
a single-user machine inside a Faraday cage with locks on the door,
with armed guards.  I'm skipping a lot of details, but the idea
is clear: If the bad guys can't get inside, it doesn't matter
whether the software makes frequent calls to the "secure erase"
routine or not.
That's an interesting way of formalizing the idea ... but to
implement it would require revolutionary changes, including
the hardware, not just the compiler and the run-time system.
Modern hardware makes copies all over the place without telling
you ... even if it hasn't been "tailored".
Under more than a few threat models, that's a non-solution, because
the same black-bag job that captures (R xor D) can also capture
R.  You'd be better off using ChaCha-style disk encryption, and
then protecting the key.  The key, being smaller than the one-
time pad, is easier to zeroize.  That's still a problem, still
not fully solved, but it's a much smaller problem.
OTOH in some specialized cases splitting the data does help.  For
example, on Tuesday you send a courier across the border.  If he
and his escort arrive safely home and swear that the information
was not compromised, then on Wednesday you send another courier
with the cipher key (or perhaps OTP).  If the bad guys capture the
first courier, you never send the second courier, and the bad guys
learn nothing.  This compares favorably to the insanely laborious
yet ultimately insecure chain of couriers described in
_Cardinal of the Kremlin_.
So do I, if/when swapping is needed.
Most of the time I run with no swap at all.  Usually when I run
out of main memory, it's because some application is grossly
broken, and I'd rather it fail sooner rather than later.

@_date: 2016-09-10 17:49:43
@_author: John Denker 
@_subject: [Cryptography] Secure erasure 
OK, good point, that's a useful addition to the list of threats
to worry about.
However, the style of solution proposed by the OP still doesn't work.
If we focus on heartbleed in particular, it depends on out-of-bounds
array access.  That produces explicitly /undefined behavior/ according
to the C standard.  As the saying goes, in such a case, the compiler
can order pizza.
Specifically:  If the only way the zeroized memory can affect the
operation of the program is via out-of-bounds array access, the
compiler is allowed to elide everything including the zeroization
... and indeed allowed to do much worse than that.
So we come by a different path to the same conclusion:  There is
nothing you can do using only native C-language constructs that
will reliably solve this problem.
  There may be ways of inveigling a particular version of a
  particular brand of compiler to do what you want, but this
  is not reproducible across other versions or other brands.
  Even if it works today it might fail tomorrow.
  Eventus incertus delendus est.
  That is to say:  any language that has "undefined behavior"
  in its specification is not well suited for writing critical
  applications.
Furthermore, previous remarks still apply:  Even if you don't get
screwed by the compiler, you might get screwed by the hardware.
Even if you manage to zeroize the copy in L2 cache, it's hard to
guarantee that the cache gets spilled to main memory, so you might
remain vulnerable to cold boot attacks and whatnot.
However, on a more constructive note:
1a) Consider munmap().  It is documented to remove the specified
 memory from your address space.  This solves the heartbleed
 problem and more than a few similar problems.  If you think
 that is the "most likely threat" you're in luck.
1b) Along the same lines, consider the "electric fence" version
 of malloc().
Both (1a) and (1b) depend on hardware features that are present on
the vast majority of platforms that we think of as "computers".
(In contrast, in the IoT controller world, life is more difficult.)
2a) Something to dream about:  Imagine reading from /dev/hardware/zero.
 That is a real piece of hardware that does DMA to main memory, and
 (in the usual way) invalidates all caches of the affected memory.
 I mention this because I/O is one of the hardest things for the
 compiler to be smart-alecky about.  I/O devices can have all
 sorts of internal state that the compiler doesn't know how to
 model.
 Back in the Olden Days some timesharing machines had such a
 device, so pages could be zeroized without gobbling up CPU
 cycles.
2b) Even in today's world, if your main worry is some smart-aleck
 compiler, reading from /dev/zero might be a reasonable option.
 This does not solve the fundamental problems, but it might
 provide some traction on the narrow question the OP asked.
2c) Another approach is to zeroize the buffer in software and
 then /write/ the buffer to some chosen FD.  Once again, the
 logic is that the compiler can't optimize out the zeroization,
 because it doesn't know at runtime whether you plan to connect
 the FD to /dev/null or to something else.  Writing to /dev/null
 is remarkably efficient.
3) If necessary, one could imagine negotiating a way to ask the
 compiler explicitly to not be so smart-alecky, e.g.
    -Wno-optimize-erasure-please-please-please  or
    -Wsecure-erasure-please-please-please
 That leads to obvious portability problems.  Still, I reckon
 obvious problems are preferable to insidious problems.
4) The combination of zeroizing and then munmapping is better than
 either one separately.

@_date: 2016-09-11 13:43:43
@_author: John Denker 
@_subject: [Cryptography] defending against common errors (was: secure erasure) 
That is a false premise leading to an incorrect conclusion.
A typical operating system such as Linux is not entirely written
in C, and the non-C stuff is highly significant to this discussion.
By way of analogy: a typical operating system nowadays supports
multiple processes running on a single processor ... and a single
process running on multiple processors.  Implementing this on an
x86 machine requires using features like MFENCE and LOCK.
    Such features do not exist in the C language spec.  To argue that
they therefore do not exist at all would be incorrect.
I would argue that any "secure erase" routine ought to do an MFENCE.
This concept is foreign to C, but it still needs to be done.
I have yet again changed the Subject: line.  If you are still
asking whether genuinely secure erase can be implemented entirely
in C, the answer is still NO.
As for the more interesting question of how to defend against
common errors, such as we saw in connection with heartbleed, I
suggest that getting the compiler to enforce runtime bounds-
checking is more useful than zeroization.
  I reckon that turning off bounds-checking is the sort of thing
Knuth was talking about when he said "premature optimization is
the root of all evil."  If you want to optimize things properly,
find the 1% of the code where it actually matters, seriously audit
that code, and then turn off bounds checking for that code only.
Also, as previously mentioned, there are things like munmap()
and electric fence.
When the code makes an out-of-bounds reference, if the buffer
has been zeroized it reduces the seriousness of the problem ...
but it will not detect the fundamental problem, i.e. the broken
code.  Furthermore, compile-time "optimizations" often magnify the
seriousness of broken code.  This includes defeating the attempted
zeroization.  In contrast, munmap(), efence, and bounds-checking
have a better chance of /detecting/ the error so you can fix it
properly.  Zeroizing the buffers is not an acceptable substitute
for fixing the code.
I agree with that.
This gets back to the point I've been making for a while:  Real
security needs support at every level from the transistors on up.
As a small step in that general direction, one could argue for
the opposite of MFENCE, perhaps an ENCLAVE directive, i.e. a way
to declare that certain information must spend its entire life
inside some well-specified enclave.
 *) Familiar example:  If you assume that main memory is secure
  but the swap device is not, then it makes sense to disable
  swapping.  This requires support from the OS.
 *) If you assume the L2 cache is secure but main memory is not,
  it would be nice to have a way to require that selected bits
  of cache never get spilled to main memory.  This requires
  support from the hardware at a rather low level.  This is
  not a completely crazy idea; note that there already exist
  "some" cache control instructions that do "almost" (but alas
  not quite) what you want:
        More generally:  As part of asking What's Your Threat Model, it
always pays to ask What's Your Security Perimeter.
Here's another tiny step in that general direction.  On 09/09/2016
09:10 AM, Henry Baker suggested "linear types" as a  style of
programming where the hardware is not allowed to make copies of
certain information.  That's tricky to implement.  Tricky, but
not impossible.  Consider the assembly code shown below.  Alas,
the hypothetical "shy" and "unshy" instructions do not exist in
present-day hardware.  The rest is perfectly ordinary.
The idea is that we don't trust anybody, not even the interrupt
handler, with the information in variables a,b,c,d,e, and f.
The specification goes like this:  The instruction "shy %eax"
means that from now on (until the next unshy), whenever there is
an interrupt, hardware will clear the %eax register before the
interrupt handler gets control.  Also, the return-from-interrupt
must return to the most recent "restartable point".
For this to make sense, the code between restartable points
must be idempotent.
        shy     %eax            /* hypothetical */
        /* shy restartable point */
        movl    a(%rip), %eax   /*   a *= b;     */
        imull   b(%rip), %eax
        movl    %eax, a(%rip)
        /* shy restartable point */
        movl    c(%rip), %eax   /*   c *= d;     */
        imull   d(%rip), %eax
        movl    %eax, c(%rip)
        /* shy restartable point */
        movl    e(%rip), %eax   /*   e *= f;     */
        imull   f(%rip), %eax
        movl    %eax, e(%rip)
        /* shy restartable point */
        xorl    %eax, %eax      /* for security */
        /* shy restartable point */
        unshy                   /* hypothetical */
        ret
The key idea here is that if you want to control the making
of extraneous copies, it's doable, but not easy.  As always,
security requires attention to detail, at every level, from
the fundamental physics on up.
Tangential technical points:
*) It should be obvious at runtime where the "restartable points"
 are.  This includes before loading the register.  It also includes
 before and after clearing the register.
*) All points outside the shy...unshy block are restartable, in
 the ordinary old-fashioned way.
*) In case you're wondering why we don't just turn off interrupts
 in the mul() routine (instead of backtracking over idempotent
 code):
 a) Some modes on the x86 have a non-maskable interrupt.
  Whether that is a good idea or not is debatable, but let's not
  go there.
 b) More to the point:  Suppose the user code has a low priority,
  and interrupts are rare but urgent.  That's a good reason to
  grant the interrupt immediately and backtrack over the user
  code when necessary.
 c) If you prefer an alternative implementation that disallows
  interrupts except at restartable points, that's OK with me.
  The key idea is the same either way.

@_date: 2016-09-12 12:37:49
@_author: John Denker 
@_subject: [Cryptography] defending against common errors (was: secure erasure) 
Well, since you asked ... heartbleed was /primarily/ an attack
on broken code, i.e. code that referenced out-of-bounds data.
At some secondary or tertiary level, zeroizing the buffer offers
a degree of error concealment.  It is does not correct the error
or even detect the error;  it only conceals it.
Agreed.  I hate bugs.  We need ways to defend against common
errors and common attacks.
HOWEVER that is several jumps removed from assuming that
"secure erasure" is the only means or even the best means
of defense.
Serious suggestion:  Let's not ask the compiler guys to do
something they don't want to do, such as turning off the
optimizations that make it hard to zeroize out-of-bounds
data.  Instead let's start by asking them to do something
that fits within their world-view *and* is more useful
anyway, namely bounds-checking.
For critical code including security-related code of the
kind I am familiar with, there should be verrry little
runtime penalty for bounds checking, because the code should
be manifestly correct at compile time.  The runtime checks
can be optimized out.
Furthermore, for code that I write, if the compiler cannot
prove at compile time that all the references are in bounds,
I would be happy to have that treated as a compile-time error.
Even if the code was technically correct before, I would be
happy to rewrite it to make it /manifestly/ correct.
I know some smart, security-conscious guys who write in C and
won't consider any other language, not even C++, because they
wouldn't feel "in control".  They worry that the language is
copying stuff behind their back, making it harder to zeroize
everything.  With all respect to those guys, zeroization is
not the primary goal.  Security is the primary goal.  If you
use a language that protects you against out-of-bounds access,
the most common mistakes go away, and zeroization becomes
much less of a priority.
Here's yet another reason why focusing on "secure erasure"
is not the whole story, or even the main part of the story.
You zeroize the buffer after you are "through" with it.
Alas that does not defend against broken code that leaks
information from the buffer /before/ you are through with
Key concept:  I hate bugs.
There is such a thing as fault tolerance.  If you do it right,
it's a life-saver ... but if you do it wrong, it is worse than
nothing.  Let me explain:
Suppose we have a system with N layers of fault tolerance.
Now suppose there are N-1 faults, which are fully concealed.
That's a problem, because as soon as the next fault occurs,
boom, you're dead, with no warning.
The better approach is to make sure that the first N-1
errors throw lurid warnings.  The mission can continue,
but the fault tolerance must be married to fault detection.
If the fault detection is not baked in from the beginning,
the system is both unsafe and undebuggable.
  I can provide a wide variety of real-world examples if
  anybody is interested.
Let's be clear:  I am not saying that zeroization is ipso
facto a bad thing.  Sometimes it is necessary, and sometimes
it is helpful as part of a belt /and/ suspenders /and/
crazy-glue approach.  If you want to use memset_s() /and/
munmap() /and/ bounds-checking that's fine with me.  OTOH
I am saying that zeroization by itself is not sufficient.
Overemphasis on zeroization is a bad thing, especially if
it leads to non-detection of broken code.

@_date: 2016-09-14 15:07:19
@_author: John Denker 
@_subject: [Cryptography] ?crypto in fully functional style 
To pursue that idea, I looked up some of implementations of AES
and ChaCha in lisp.  The first ones that I found (emacs and common
lisp) were underwhelming, with lots of setf and setcar statements
(like (fortran (with (parentheses)))).
The story is similar with Haskell:  It is touted as a "fully functional"
language but it still has ByteArrays.
Therefore, a question:  Can anybody point to an implementation
of some interesting crypto primitive, AES or ChaCha or whatever,
written in a fully functional style?
If not, is this a consequence of how the primitives are defined?
Do we even know how to define a natively, inherently functional-
style cipher?  Should we have a contest to create one?
For reference, some examples:

@_date: 2016-09-14 15:48:03
@_author: John Denker 
@_subject: [Cryptography] massively parallel processing 
You can get GPU cards with more than 2000 cores per card.  For
bitcoin mining, or for physics simulations -- or for "consumer"
applications such as fancy graphics -- you might plug in two or
more such cards, at which point you're already well over 4000
processors in a single "computer".
  It's also good for keeping you warm in the winter.
A state-of-the art brain has on the order of 10^11 neurons and
10^14 synapses, maybe more, operating in parallel, with a total
power dissipation on the order of 100 watts.  It runs at about
10^3 cycles per second, so the number of elementary operations
is rather large.
It's not particularly good at bignum arithmetic, but there are
other things it does rather well.  It has considerable fault
tolerance.  It can be programmed (i.e. taught) to a limited
extent, but this process is not well understood, not at all
similar to modern programming languages.
Looking forward, it's fun to imagine something with the best
of both worlds, including easily-programmable precision as
well as massive parallelism.

@_date: 2016-09-16 13:27:58
@_author: John Denker 
@_subject: [Cryptography] Recommendations in lieu of short AES passphrases 
Passwords should never be shared.  See below for details.  If sharing
the password is a requirement of the software, you need to get better
software, right now.
As for the length:  A short password is like having a weak lock on
your door.  As such, lawyers might consider it better than nothing,
insofar as there is a distinction between:
 a) trespassing (if you leave your door completely unlocked), and
 b) breaking-and-entering (if you have "some" kind of lock that
  the bad guys must defeat).
Therefore a trivial password, in conjunction with vigorous policing,
might deter some casual attackers ... but you shouldn't count on it.
In contrast, if somebody is the least bit serious about attacking
your system, a short password confers virtually no protection, and
no amount of recoding and/or hashing is going to change this.
Human-factors research has shown that a short random password is hard
for the good guys to remember, and easy for the bad guys to guess.  A
long passphrase is easier to remember and more secure.
  Passwords should never be shared.
Draw a matrix, with a row for each person and a column for each site
or function.  There should be a separate passphrase for each cell in
the matrix.  No sharing vertically (two people using the same password)
and no sharing horizontally (one person using the same password for
multiple sites).
    Alice  :	sesame		swordfish
    Bob    : 	00000000	Buddy
    Carol  :	asdf		12345
It imposes a burden on the users to have a separate passphrase for each
site, but this is necessary for security.
It helps to have a /password manager/ program to handle the details.
Then the user only needs to remember a single master passphrase.
You need to make sure the password manager is not broken, but that
is better than the alternative.  If you don't give users a password
manager program, either they will forget one or more passphrases,
or they will invent their own ad-hoc password manager, perhaps an
index card in their wallet (which puts them at risk from muggers at
pickpockets), or they will use short and/or recycled passwords.
We really ought not be sending passwords to online sites at all.
Once you accept the idea that a password manager is needed, it ought
to be doing /zero knowledge authentication/.  That way no password
is ever sent over the wire.  The password manager proves to the
site that you know the master password, but the site never sees
anything resembling a password.

@_date: 2016-09-17 06:00:59
@_author: John Denker 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Such things exist.
For starters, there is C++.  This provides an easy migration path,
since nearly any decent C program will compile just fine under C++.
The code can then be converted to C++ style incrementally.
Another line of attack is to add /annotations/ to make static
analysis more effective.
One way of thinking about this is to define a /dialect/, such that
stuff that is undefined behavior in the parent language is defined
in a particular way in the dialect.  For starters, you can implement
bounds checking.
  Another option is to use a language subset such as MISRA-C.  This
has the advantage of portability, insofar as anything that compiles
under the subset compiler will also compile under the parent-language
compiler.  However the MISRA-C subset is nowhere near a panacea.
For a critique, see:
  Les Hatton
  "Language subsetting in an industrial context: a comparison of MISRA C 1998 and MISRA C"
  If you don't like that particular subset, there are others.  You can
even implement your own ad-hoc subset.  For example, you can improve
portability by deprecating "int" and favoring "int32_t" instead,
which is easy to enforce at compile time.
  typedef int native_int;
   int bogus_int
More generally, for a discussion of the pros and cons of various
languages, including consideration of safety and practicality, see:
  David A. Wheeler
  "How to Prevent the next Heartbleed"
  IMHO it is an excellent exercise to examine languages and tools and ask
whether they would have prevented heartbleed, and at what cost.  As
one small example, I remark that 'cppcheck' would not have prevented
heartbleed.  That is, the bad old version of openssl d1_both.c *with*
the heartbleed bug passes all the static checks performed by
As for the cost, I find that cppcheck generates an awful lot of false
positives, falsely claiming that variables are uninitialized.
It should be noted that there exist C-to-Ada translators that do 80
or 90 percent of the work for you.  Also C-to-Rust translators.
That's sometimes true and sometimes not.  As a partial counterexample,
checking, without changing the representation.
Even if the representation has to change, the breakage isn't all that
terrible.  For example, in C++ you can convert from
This is absolutely not going to break the hardware.  Indeed, as previously
discussed, it is likely that future improvements in hardware will focus
more on increasing the parallelism rather than increasing raw clock speed.
Bounds checking can usually be performed in parallel with everything else.

@_date: 2016-09-18 13:26:20
@_author: John Denker 
@_subject: [Cryptography] Recommendations in lieu of short AES passphrases 
Opinions differ on that.  I would argue that they are the
worst imaginable idea, except for all the known alternatives.
Some of the smartest, most security-conscious folks I know
use password managers.
Sometimes it is a good idea to put all your eggs someplace
safe, and watch that place very carefully.  It decreases the
number of places you have to watch.
Why should we trust the users to remember a gazillion different
passphrases, when every study ever done indicates they are not
very good at that?
Nothing is perfect.
Then you're screwed anyway, with or without a password manager.
Lastpass and four others were found vulnerable in 2014:
  OpenSSL was also found vulnerable in 2014:
  I suggest that rather than giving up on SSL entirely, it makes
sense to fix the implementation and keep using it.  Ditto for
password managers.
You've just made yourself -- and all your advisees -- targets for
muggers, pickpockets, evil maids, shoulder surfers, et cetera.
Also the inconvenience of paper creates pressure to keep passphrases
short and/or cute, further reducing security.
IMHO it is security malpractice to transmit passwords *even* to
the account that is trying to authenticate you.  It would make
more sense to perform a zero-knowledge proof that you know your
master password.
Therefore a suggestion:  Don't give your master password to anyone
or anything other than your password manager (aka ZK proof manager).
For users who rely on present-generation password managers, the
incremental burden of a ZK proof manager would be zero, if the
infrastructure supported it properly.  This in itself is an
argument in favor of password managers, since it gets people
moving in the right general direction.
That's a corollary of the more general rule:
  If you don't have physical security, you don't have security.
Study after study has shown that most users don't exhibit that kind of discipline.
That's too short.
All evidence indicates that unaided users will either:
  a) reuse phassphrases,
  b) write down passphrases in some insecure way, or
  c) forget passphrases
Perhaps it would be worthwhile to work toward a consensus.

@_date: 2016-09-19 23:21:43
@_author: John Denker 
@_subject: [Cryptography] defending against common errors (was: secure 
Does it really?
It sounds nice in theory, but:
 -- Recent versions of gcc don't implement it.
 -- Recent versions of clang don't implement it.
 -- C++ inherits it from C, so using C++ doesn't help.
Is there a readily available compiler that actually supports
Does it really?
It sounds nice in theory, but on typical platforms,
including generic ubuntu linux, sodium_memzero() reduces
to a for-loop that simply assigns zero into each location.
It uses the "volatile" keyword, but (as usual) that provides
no guarantees.  Anybody on this list could have written this
code with near-zero effort.  This is an approach that was
considered and rejected, for reasonable reasons, in the
very first message in the "secure erasure" thread.
The libsodium documentation says it "tries to effectively
zero" the memory.  Trying is not good enough.  I wouldn't
go so far as to endorse the Yoda rule:
  Do. Or do not. There is no try.
but if something claims to reduce the attack surface I
would like to see a reasonably /quantitative/ claim.
I would care quite a bit if somebody could suggest a
solution that actually worked and was actually available.
If you need to erase things, here are some artful dodges
that may be helpful:
 -- compile the erasure routine separately from everything
  else.
 -- as previously mentioned, consider reading /dev/zero
 -- as previously mentioned, consider writing /dev/null
  or /dev/random
 -- consider keeping an _extern_ pointer to the memory
  you just erased.
All of these dodges are ways of defeating the infamous
"as-if rules" i.e. the rules about "observable" behavior.
They make it harder for the compiler to know who's observing
what.  In particular, as far as the compiler is concerned,
the extern pointer does a decent job of mimicking a possible
out-of-bounds reference coming from who-knows-where.
  To make sure that the extern pointer doesn't introduce
  a new type of vulnerability, don't set the pointer until
  after you have zeroized the memory.
It must be emphasized that these dodges do *not* solve the
whole problem.
Erasure may reduce the amount of time that critical data spends
lying around, which is nice, but it does *not* entirely close
the window.
Last but not least, it is worth saying again that it would
be a mistake to over-emphasize "erasure" per se.  Instead
we should focus on defending against common errors.
Erasure sometimes mitigates the damage caused by out-of-bounds
references ... but it is almost always easier and better to
get rid of the root cause, namely the out-of-bounds references.
In addition to straightforward bounds checking, there are also
techniques such as electric fence, mmap/munmap, aslr, et cetera
that make it much more likely that an out-of-bounds reference
will be detected before it can do widespread harm.  The notion
that bounds-checking would be unduly expensive is not supported
by the evidence.
A useful criterion is to ask whether such-and-such technique
would have prevented heartbleed, and at what cost.
Again:  we should focus on defending against common errors.
I've heard people complain that zeroization is harder in C++.
I'm not sure that's true, but even it were, bounds checking
is easier, so all-in-all C++ seems like the better choice.
Do not fixate on "erasure" per se!
There is a huge base of C code, but the migration path to C++
is really simple.  There are fancy C-to-Ada translators that
will do 80 or 90 percent of the work for you, whereas using
for you.
Also, there has been some talk of changing the language spec
to deal with integer overflow.  Often the right response to an
overflow is to throw an exception.   Ditto for out-of-bounds
references.  I mention this because C++ has exceptions whereas
C does not.
  There is a distinction between hardware exceptions and C++
  language exceptions, and getting the language to respond
  properly to the hardware is a work in progress ... but
  still it is a lot less of a stretch than getting C to do
  the right thing.
On 09/19/2016 10:27 PM, Florian Weimer described memset_s() as:
Yeah.  That's related to the general lack of exceptions.

@_date: 2016-09-21 15:22:03
@_author: John Denker 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Well, there is "some" data.  For example:
  Baishakhi Ray, Daryl Posnett, Vladimir Filkov, Premkumar T Devanbu
  "A Large Scale Study of Programming Languages and Code Quality in Github"
  Proceedings of the 22Nd ACM SIGSOFT International Symposium
    on Foundations of Software Engineering (2014)
  The observed effect is very modest indeed.
The article includes references to other work on the same topic.
Some consider the productivity issue, not just the incidence of
You can quibble with all this data, but I reckon imperfect data
is better than no data.
Presumably there are other parts of the software development process
that are more important than the choice of language.
Also NASA has coding rules for life-critical and mission-critical
  Most "modern" language architects would consider this to be
the Luddite approach:  No recursion, no dynamic allocation
of variables, et cetera.  However the NASA guys seem to think
this is the "only" way they can "guarantee" correct execution.
It's not some passing fad;  they've been doing it this way
for years.
It does not fully solve the problem.  For example:
  That was a loss on the order of 300 million dollars.
Probably the biggest issue is this:  Most developers don't care
very much about reliability or security.  The motto of Silicon
Valley is "ready, fire, aim".  That is, get /something/ out the
door fast, and (maybe) tune it up later.
This has got to change.  Maybe it will.  Sony says the 2011 PSN
hack cost them 175 million dollars.  At some point shareholders
are going to demand that people clean up their act.
The existence of botnets that could easily DDoS the entire internet
is just intolerable.  At some point somebody is going to impose a
duty of care on the vendors, to require them to make sure this
cannot happen.  By way of analogy:  To operate a car on public
roads, it must meet safety standards.  You also need insurance
and a licensed driver.  Now imagine enforcing similar standards on anything that connects to the public network.

@_date: 2016-09-24 00:41:21
@_author: John Denker 
@_subject: [Cryptography] Spooky quantum radar at a distance 
That article seems to be largely based on the following press release:
  Note that the 100 km detection claim appears in the CETC press release,
while the flamboyant references to Einstein and to stealth do not.
Perhaps they were added by the SCMP journalist.
The article is so deep in hogwash that one hardly knows where to begin.
 a) quantum mechanics does exhibit entanglement, and
 b) even though Einstein ridiculed the idea of action at a distance,
  quantum mechanics actually does exhibit a certain type of nonlocality.
However, those are not the same thing.  Really not.  Also, the nonlocality
cannot be used to transmit information.
 c) There is such a thing as "quantum illumination".  It makes use
  of entanglement.  It does not even attempt to exploit nonlocality,
  and any such attempt would fail anyway, because EPR-type nonlocality
  does not transmit information.  Instead, quantum illumination relies
  on a beam that goes out and comes back, whereupon it is compared
  against a local reference (the "ancilla") that has been retained,
  locally.
We can invoke Betteridge's law of headlines:  Any headline that ends
in a question mark can be answered "no".
The stealth idea has been circling the drain for quite a while now, for
reasons having nothing to do with quantum illumination.
  d) As Peter G. rightly pointed out, all existing stealth aircraft show
   up on S-band and L-band radar, and on any longer wavelength.
  e) To which one might add:  Any stealth aircraft will show up on    bistatic and multistatic radar.  The aircraft scattering function
   has a null in the direct backscatter direction, directly back toward
   the transmitter.  However, physics does not permit it to be null in
   every direction.  There *will* be a pattern of nodes and antinodes.
   If only they had an outpost in the South China Sea, it would make
   a great site for a multistatic radar installation.  Oh, wait, maybe
   they already thought of that.......
  f) Stealth aircraft also show up in the infrared.
This bag has been catless for many years.  I reckon stealth technology
is still effective against bush-league opponents, but I would be very
surprised if anybody could fly into Chinese airspace without being
picked up.
The article claims to be "news", but I'm not sure why.
  g) The idea of quantum illumination goes back to 2008.
   Lockheed has a patent on "quantum radar" from 2008.
   There are entire textbooks on the topic of quantum radar.
  h) If this were a real breakthrough, it would be highly classified.
   We would not be reading about on the CETG or SCMP web sites.
  i) Mentioning single photons in this context is misleading at best.
   Quantum illumination systems use (or should use) coherent states,
   aka Glauber states.  The Glauber-state basis is /incompatible/ with
   the photon-number basis.  They are incompatible observables, in the
   Heisenberg sense.   A Glauber state looks a whole lot more like a
   classical AC voltage than like a single photon.
  j) Counting photons would be insane.  The photon number operator is
   /quadratic/ in field strength.  In second-quantized language, we can
   write it as (aa).  For a small signal, the photon number is small
   squared.  Since the dawn of time, every radar ever built has instead
   measured the voltage, which is /linear/ in field strength, namely
   (a + a).
  k) There is no fundamental dividing line between quantum noise and
   classical thermal noise.  They are two limiting cases of the same
   fundamental physics.  The relevant scale-factor is Boltzmann's constant
   divided by Planck's constant, which comes out to 21 GHz per Kelvin.
   At the frequencies we are talking about (X-band and below) and at the
   temperatures we are talking about (300 kelvin or so), it's essentially
   all thermal.  Calling it "quantum" radar is mostly marketing hype.
   A careful classical analysis of the apparatus would give essentially
   the same answer.
  *) Et cetera.  I could go on, but why bother?  If you want to know
   about the interesting stuff going on in the radar world, this article
   is not a good place to start.

@_date: 2016-09-25 20:48:47
@_author: John Denker 
@_subject: [Cryptography] Spooky quantum radar at a distance 
Two words:  "Kill chain."
Initial detection is the first link in the chain.  Subsequent links
generally use technology that differs from the first.
That's fallacious.  Ex falso quodlibet.  The fact is, non-bush-league
operators have interceptors, with air-to-air missiles.  (Of course
they have surface-to-air missiles also.)
That's irrelevant.  In the security business, it is bad policy to
assume the other team hasn't done anything or learned anything in
the last 40 years.  This applies equally to cryptology and missilery.
That's false.  Assertions like this are easy to check.  I get 60,000
hits from:
   Even if such things were not already widely known, it would be prudent
to assume the opposition has such things in the closet, or under
development.  They would be stupid not to.
In missilery -- as in cryptology and a thousand other disciplines --
it is pointless to talk about "reducing" something without specifying
how much it is reduced, and at what cost.
If stealth "reduces" your chance of getting shot down from 100% to
99%, that's a "reduction" but it's not good enough, especially if
it comes at the cost of range, speed, rate of climb, maneuverability,
payload, and affordability.
Really?  What physics is that?  Please be specific.
Or don't bother, because even if it were true, it would be irrelevant.
Low frequencies aren't the only game in town.  Stealth aircraft still
show up on IR and on multistatic X-band.
That's untrue if the opponent also has stealth, and is combining long-
wave, multistatic, and IR to see you, while you can't see him because
you made a long string of false assumptions.
It's also untrue if the opponent is playing defense and you are intruding
on his turf, even if you have stealth and he doesn't.  He can have a
bazillion cheap unmanned transmitters in the rear echelon, which you can
see, plus a bazillion receivers and launchers along the front, which you
can't see.  By the time you get close enough to shoot at the transmitters,
the launchers can shoot you.
Even if you manage to get the shot off, you're using a 300,000 dollar
anti-radiation missile to take out a 10000 dollar unmanned transmitter.
Not a good exchange.  And then you discover that every transmitter
has several backups that you didn't know about, because they had never
previously been turned on.
That's partly untrue and entirely irrelevant.  If you don't need
monostatic directionality, you can use a very small antenna.
 *) The 700 MHz cell-phone band has a long wavelength, longer even
  than L-band, yet can be sent and received by cell phones that are
  quite a bit smaller than a missile.  That's relevant, because if
  you do multistatic interferometry, resolution depends not on the
  size of each antenna but rather the size of the array.
      This is related to the idea of "track-by-missile".
  That means you can launch 2N missiles to shoot down N supposedly
  stealthy intruders.  At $100,000 per missile and $100,000,000 per
  airplane, that's a good exchange.
 *) Instead, or in addition, you can fit a relatively prosaic L-band
  radar into an AWACS or even a fighter.
      You can use that to vector the missiles to the right general vicinity.
  Then, for the last links in the kill chain, the missiles can use their
  own monostatic IR and/or multistatic short-wavelength radar, which do
  fit quite nicely.
Here's a crypto-related tangent:  Reportedly, according to the Snowden
documents, the NSA believes a cyberattack from China purloined a great
many technical documents about the B-2, F-22, and F-35.
  One may conjecture that the Chinese were ROTFL when they saw the F-35.

@_date: 2016-09-30 11:23:17
@_author: John Denker 
@_subject: [Cryptography] distrusted root CA: WoSign 
In case you missed it:
In general, why do we put up with this?  Why, why, why?
In particular:
Mozilla reportedly intends to take action "in the near future" but
no specific date was announced.  Also it's not clear what the Beast
of Redmond plans to do.
Suggestion to speed things along:  Here's what I did on my Ubuntu systems:
  :; cd /usr/share/ca-certificates/mozilla
  :; mkdir deprecated
  :; mv *WoSign* deprecated/

@_date: 2016-09-30 13:35:29
@_author: John Denker 
@_subject: [Cryptography] another security vulnerability / travesty 
Simple question:  Suppose your aunt wanted to submit some medical documents
to a clinic on the far side of town.  Short of hand-carrying the documents,
what would you recommend?
Consider the contrast:
1) In the US, most physicians will not accept email, on the grounds that
 it provides insufficient privacy, and is therefore not HIPAA compliant.
 I reckon that's true as far as it goes.  One could imagine using PGP
 on top of Tor, but it's hard to imagine trusting typical patients to
 do that that properly.
2) The odd thing is that they consider _fax_ to be HIPAA compliant.  That
 seems quaint, like using an amulet to ward off disease.
 The latest version of this is to download the form, fill it out, and
 fax it back to the provider.  There's a lot of this, e.g.
    2a) Most people don't have fax machines, and even if they had the hardware
  they wouldn't be able to use it because they rely on cell phones and don't
  have a POTS line.  So at best they have to use somebody else's fax machine.
 2b) Even when using a plain old fax machine, the idea that the signal
  would be hard to intercept in transit is quaint, to say the least.
 2c) Some customers have the bright idea of emailing the document to a
  fax-gateway service.  Then they have the worst of all worlds, including
  the insecurity of email on top of everything else.
     2d) One could imagine uploading forms via SSL.  I've seen examples
  of this:
       However, I'm not sure I would trust a typical private-practice office
  to run a secure server properly.  Furthermore I don't trust the
  current Narrenschiff of root CAs.
 2e) There's also the risk that the data could be snatched while at
  rest at the destination, but that's a topic for another day.
HIPAA is not the only game in town.  There are analogous regulations that
apply to the banking industry.  Some bankers find the regulations to be
impossibly onerous, so they tell customers to send documents to a personal
gmail account, and then cut-and-paste from there into the bank systems.
IMHO we ought to take this seriously.  Based on the Snowden documents, the
OPM hack, the DNC hack, and a boatload of other evidence, it should be clear
that the advanced persistent threats are very advanced and very persistent.
If I worked for a local AIDS clinic, or a family planning clinic, or the
local office of a political party, or a high-tech startup company, I would
assume that I was under constant attack.  No tinfoil hat is required.
Cyber warfare has already begun, and we're losing the war.

@_date: 2017-04-26 20:23:08
@_author: John Denker 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
I agree with that 100%.
A cipher "mode" is a kludge that kinda maybe sorta allows people
to survive in situations where they can't (or won't) re-key ...
but you should always ask yourself, why not just re-key?  If
your cipher cannot be efficiently or securely re-keyed, maybe
you need a better cipher.
Constructive suggestion:  Use something like ChaCha20, which
is designed to do a good job with file encryption (and a lot
of other things).  It has an enormous keyspace, and can be
re-keyed efficiently.  Construct the key from at least:
  -- The master key.
  -- The block number.
  -- A sequence number, depending on how many times the
   block has been rewritten.  (This allows random access,
   as well as rewriting the whole file from the beginning.)
Good analogy.

@_date: 2017-08-18 10:23:16
@_author: John Denker 
@_subject: [Cryptography] securethe.news -- deployment of HTTPS and HSTS 
Hi Folks --
There are some interesting resources at:
  According to their report on 129 news sites:
 -- 46% "offer" HTTPS
 -- 36% default to HTTPS
The sites are graded and tabulated here:
  To get higher than a grade of "B" requires Strict Transport Security
(HSTS) -- a topic that has heretofore not been discussed very much
in this forum.
  The report doesn't explain it, but I surmise that the difference
between the first two columns is this:  A site is marked as "valid"
but not "available" if HTTPS redirects to plain old HTTP.
Beware that their data is not 100% reliable.  For example, I
observe  and to be fully available, even though they are not marked as such
in the report.
Perhaps most interesting of all is good bit of guidance on how
to secure a large site:
  Alas, as usual, they overstate the level of security that HTTPS
provides.  Note the contrast:
 ++ Yes, HTTPS is a good thing.  It protects against a fairly
  wide class of attacks.  It means the barista at your favorite
  internet caf cannot easily tamper with your traffic.
 -- It does not protect against traffic analysis.  People really
  need to stop underestimating the power of traffic analysis.
If you are relying on HTTPS without Tor, the barista can trivially
tell whether you are surfing buzzfeed.com or infowars.com, and can
infer quite a lot from that.
What's worse, there are thousands of organizations with enough
resources to carry out traffic analysis /at network scale/, which
means they capture you along with everybody else, and can identify
not only the sites you visit but also the individual pages within
each site.
Here's one of my favorite maxims:
   Metadata is data.
   Stealing metadata is stealing.
   A cryptosystem that leaks metadata is a cryptosystem that leaks.

@_date: 2017-12-21 07:07:41
@_author: John Denker 
@_subject: [Cryptography] paragraph with expected frequencies 
The question is both trivial to answer, and impossible.
It is trivial for linguistic and cryptological reasons:
Almost any reasonably large sample of English will
display characteristic English letter-frequencies.
This is not mathematically guaranteed;  it is just a
known property of natural language.
It is an important property.  Frequency analysis is
not a known-text or chosen-text attack, where you
know a_priori that the text has the exact "expected
frequencies".  It works for any halfway-reasonable
text.  This is the fatal weakness of any monoalphabetic
substitution cipher.
In contrast, there are good mathematical reasons why
no finite sample will display the "expected frequencies"
Frequency is a type of probability.  There are lots of
probabilities in this world, and lots of frequencies.
In this case we are particularly interested in the
effectively infinite set, and various finite /samples/
that might be drawn from the population.  Statisticians
give these terms technical meanings which unfortunately
diverge from the meanings in any other context, but
let's stick with the statistical definitions here.
The frequencies observed on any sample will converge
to the frequencies on the population in the limit
of large sample-sizes ... but we are talking about
convergence in the limit, not equality for any finite
For any finite sample, /statistical fluctuations/
guarantee that the sample frequencies are expected
to differ from the population frequencies.  You can
use properties of the population to predict the
distribution of fluctuations (as a function of
sample size) if you want.
The larger the number of observables (e.g. the 26
different letter frequencies) the smaller your
chance of seeing the "expected frequencies" exactly.
On the other hand, the point of the exercise is
statistical /inference/.  Frequency analysis allows
you to infer that the text is English, as opposed
to gibberish.  With a reasonable-sized sample, you
can infer this with high confidence _despite_ the
fluctuations.  The confidence will never be exactly
100%, because the tail of the English distribution
will overlap the tail of the gibberish distribution
"somewhat", but this is not a problem in practice.
Even if you could hunt up a sample that did have
the exact "expected frequencies", it would be very
unwise to use it as the basis of a lesson, because
it would teach a wrong lesson about statistical
fluctuations and statistical inference.
==> A much better lesson would be to repeat the
experiment with a few different sample-sizes from
the same source, to demonstrate the mathematical
point about fluctuations and convergence ... and
then compare a few disparate sources (e.g. Dickens
versus Rowling), to demonstrate the linguistic
point about near-invariance of the frequencies.
Thirdly, histogram a random process (diceware)
as a control.
Counting using tally-marks (a) is easier and (b)
constructs a histogram on the fly.  Plot a large
sample with N subsamples, using N colors of ink,
all on the same cumulative histogram, so you can
see the fluctuations and the convergence at a glance.
Digraphs converge 26 times more slowly, for obvious
reasons, and so require much larger samples.  This
should come several turns later on the pedagogical

@_date: 2017-12-22 11:49:25
@_author: John Denker 
@_subject: [Cryptography] paragraph with expected frequencies 
I suggest that "oh well" is not the message you should
be sending to your student.  A better message would be:
  Noise on the data is not the enemy.  Noisy data is
  not a mistake;  it is the nature of the beast.  It
  is natural and wholesome.
This concept is already understood by any kid old
enough to play Go Fish, or Clue, or any other game
of imperfect information.  You need to have a
strategy that is robust against statistical
fluctuations in how the cards turn up.
Similarly, any kid that is old enough to play
sports, or even rock-paper-scissors, knows that
you ought not run the same play every time.
There is value in /intentional/ randomness on
your side, i.e. a mixed strategy.  Also, once
again, you need a strategy that is robust
against random behavior by the other side.
Recommendation:  Play the hand you are dealt.  Take a
sample of generic text and do the frequency analysis.
  a) there will be noise, i.e. statistical fluctuations
  b) even so, the signal-to-noise ratio is quite high,
   for any reasonable sample-size.
Also:  Not the first lesson, but useful on a later
turn of the pedagogical spiral:  Don't pay attention
just to the high-frequency counts.  Sometimes the
low-frequency counts are quite informative also.
For example, if a hypothetical partial decrypt causes
the /qz/ bigram to crop up with high frequency, you
know something's wrong.
In the real world, there is very often more than one
right answer to any given question.  The idea that
there must be one almighty Right Answer is rather
widely taught in school these days, e.g. in connection
with high-stakes multiple-guess tests, which IMHO is
a scandal and an outrage.

@_date: 2017-12-28 18:27:28
@_author: John Denker 
@_subject: [Cryptography] Fast handling of IP Address changes for HTTPS 
This has been researched extensively in the context
of mobile devices.  That isn't a specific answer to
your question, but it tells you where to look, e.g.
  One specific thing that is not particularly lightweight,
but is guaranteed to work, is to have a box with a
static IP address, which you can consider a proxy or
simply a router.  It then forwards stuff to the
endpoint ... possibly using a succession of IPsec

@_date: 2017-02-15 07:53:05
@_author: John Denker 
@_subject: [Cryptography] detention and/or seizure if you don't give your 
Hi --
US Customs and Border Patrol (CBP) claims unlimited authority, not
restricted by the Constitution, at "points of entry".  They also
claim near-unlimited authority within a "reasonable distance" of
any land or sea boundary of the US.  They have unilaterally decided
that 100 miles sounds "reasonable" to them.  Two-thirds of all
people in the US live within this 100-mile zone.
This leads to problems with password-protected and/or encrypted
devices.  These problems are not hypothetical:
The rest of the story (1600 words) by Kaveh Waddell is at:    The story correctly quotes CBP policy:
  This leads to some hypothetical but entirely plausible situations
with considerable downside potential:
  1)  Suppose your battery dies while you're on a day trip to
   Tijuana.  Your device gets seized.
  2) Suppose you are on vacation. You give your phone and laptop
   to your family to take home while you go on ahead, or stay
   behind.  Your spouse and children miss their flight while
   CBP tries to extract the passphrase from them.  Then the
   devices get seized.
  3) Suppose your devices contains random numbers that you
   cannot possibly decode, such as are attached below.  You
   miss your flight, and your devices get seized.
The previous PGP block started out as 512 bytes from a high-grade
random number generator, although you could not possibly prove
that.  Also, it was public-key encoded to a recipient for whom
no private key has ever existed, although you could not possibly
prove that.

@_date: 2017-02-27 17:52:14
@_author: John Denker 
@_subject: [Cryptography] jammers, nor not 
Just in case it wasn't obvious, jammers are illegal in the US.
IANAL, but the last time I checked, there was an exception for military
and Federal law enforcement, but not state or local.  Don't bother
asking for an exemption; there is not even a process for granting one,
no matter how noble your purpose may be.
a) I reckon you can use a scanner so you can detect and confiscate any
 transmitters if/when they are brought in.
b) I reckon you are allowed to turn your home or office into a Faraday
 cage.
c) OTOH there is zero tolerance for active jammers.
See e.g.
  and references therein.

@_date: 2017-02-28 10:57:31
@_author: John Denker 
@_subject: [Cryptography] formal verification +- resource exhaustion 
Question:  How do formal methods handle the possibility of
resource exhaustion?
Related question:  What do reliability experts think of the
NASA / JPL coding standards?
  In particular, consider Rule 5, which forbids using the heap
(after the initialization phase).  I can understand where that
comes from, but it is remarkably restrictive and incurs a
terrible price in terms of efficient use of memory.
On the other edge of the same sword, one could argue that those
standards are not nearly restrictive enough.  They allow and
even require the use of local automatic variables (Rule 13),
but nowadays you can run out of room on the stack 1000 times
more easily than you can run out of room in the heap.  Typically
it's megabytes versus gigabytes, under the control of ulimit -s.
Just to rub salt into the wound, with gcc if you run out of
stack AFAICT the first and only symptom is a segfault;  g++
is no better.  The compiler's attitude seems to be:
   I cannot explain the inconsistency between local variables and
malloc.  The latter relatively gracefully returns a null pointer
if it runs out of resources.  C++ "new" throws a catchable
Stack exhaustion makes it appallingly easy to write seemingly
beautiful code -- with all the right loop invariants etc. --
that works fine for some parameter values yet fails for others.
The failures can involve insidious non-local interactions.
I'm a big fan of defensive programming, but it's not obvious
how to defend against this in any systematic way.
Any suggestions?
 Usage:      ./ulimit-s-test nnn
 Especially: ./ulimit-s-test $(ulimit -s)
 Segfaults for not-very-large values of nnn,
 depending on ulimit -s
 */
      /* for atoi() */
int main(int argc, char const * const * argv){
  int nnn = (argc > 1) ? atoi(argv[1]) : 8192;
  unsigned char big[1024][nnn];
  return big[0][0];

@_date: 2017-02-28 15:04:16
@_author: John Denker 
@_subject: [Cryptography] shielding 
I disagree.
Let me call attention to Murphy's Law.  One of the corollaries is:
 -- If you want the signal to be strong, it will fade.
 -- If you want the signal to be contained, it will leak.
Murphy's law is sometimes stated in facetious terms, but it has
a solid foundation in engineering principles.  The situation is
very asymmetric:
 -- When a wireless widget is supposed to work, you expect it
  to work everywhere in the home / office / whatever.  So you
  care about the weakest signal.
 -- The attacker, Eve, does not need to succeed everywhere; she
  only needs to succeed somewhere.  So she cares about the
  strongest leak.
Security is hard, and will always be hard.  It requires tremendous
attention to detail.
Try building one sometime.  One that works when there is a strong
source on one side and a delicate receiver on the other side.  One
that works across a wide range of frequencies.
Most people have no idea how to begin.  Even if you know, it's still
a lot of work.

@_date: 2017-01-10 19:18:00
@_author: John Denker 
@_subject: [Cryptography] nytimes.com switches to https 
Quoting from:
 This is new for them, and unusual in the industry.
++ washingtonpost.com enabled https a couple of years ago.
++ theintercept.com and its parent firstlook.org have used https
     since their inception a couple of years ago.
++ theguardian.com started using https within the past year
++ buzzfeed.com started using https within the past year
All of the above (including NYT) redirect http to https, so you get
it even if you don't explicitly ask for it.
More-or-less everybody else seems not to care.

@_date: 2017-07-06 16:19:09
@_author: John Denker 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Then on 07/06/2017 06:51 AM, Patrick asked:
There is a proverb that says:
  Never confuse the presence of one thing with the absence of another.
A non-blocking RNG is not the problem.  What we need is a RNG
that never blocks *And* never returns insecure results.
A RNG that blocks is a problem.
A RNG that produces insecure results is a problem.
A RNG system that foists on you the choice between blocking and
 insecurity is a problem.
That assumes away the root of the problem.  Any PRNG is at
the mercy of its seed.
On "most" systems proper seeding is entirely possible, although
the requirements differ from system to system.  Expertise and
attention to detail are required.
On some systems, proper seeding is simply not possible.  Such
systems are not secure and cannot be made secure.  However, this
should not be used as an excuse for doing a bad job on systems
where a good job is possible.
For example, consider a VM booted from the same snapshot
every time, so that it has no persistent writable storage.
That creates problems, but they can be overcome in various
ways that involve cooperation from the host, e.g. via a
virtual /dev/hwrng and/or a seed provided on the kernel
commandline and/or whatever.
  Conversely, you don't want to require every system to
  have a /dev/hwrng and you don't want to require every
  kernel commandline to have a seed, in cases where those
  are not needed.  So like I said, the requirements differ
  from system to system.
Stacking up a bunch of "possible" solutions without verifying
that one of them is an /actual/ solution in a given context
is the opposite of good engineering.

@_date: 2017-07-07 11:36:27
@_author: John Denker 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Let me clarify something I said earlier.  What I should have said was:
  There are some platforms that are not secure, and no amount of fiddling
  with the software will make them secure.
  As a corollary, installing a new&improved version of openssl has no
  chance of making them secure.
I still insist that the existence of such platforms must not be used
a pretext for doing a bad job on platforms where a good job would
have been possible.
Furthermore, the absence of an all-software solution does not mean
we just throw up our hands and walk away.  To the guy who owns a
bunch of embedded controllers, I would say this:
  You are a businessman.  You presumably know about scenario planning.
  Here are three scenarios to consider:
  In scenario A, you air-gap those things immediately.  You put them
  in a locked and guarded room, to make sure they stay air-gapped.
  You bear the cost of not being able to sit at home in your pajamas
  and monitor the production line from there, until such time as you
  bear the cost of installing some major upgrades.
  In scenario B, you wait until you discover that you can no longer
  sell anything, because some overseas competitor has ripped off all
  your intellectual property, and is offering the same product line
  at a fraction of the cost.  You declare bankruptcy, sell all your
  equipment for scrap, and lay off all your highly-trained workers.
  In scenario C, you actually win a sales contract, but this is even
  worse than the previous scenario.  You cannot fulfill the contract,
  because a stuxnet-style attack has done irreversible damage to all
  your equipment.  And you're being sued for negligence, for being
  an incubator and a vector whereby the malware attacked everybody
  else.
It's your choice.
That's a good question.  It depends.  It's hard to make anything
foolproof, because fools are so ingenious.
Even so, the hardware solution has one advantage:  It can make
randomness from scratch.  The list of things that can go wrong,
while not short, is at least finite.  That stands in contrast
to a PRNG which got its seed from a PRNG which got its seed
from .......
In theory the hardware solution "costs more" but in fact the hard
cost is so small that it's not worth worrying about.  The cost of
arguing about it exceeds the actual hard cost.
There is an attitude in the software community that everything
"should" have an all-software solution.  Well, randomness is an
exception.  This has been understood since even before there
was a software community, or even a computer industry.  John
von Neumann had something to say about this.
There are plenty of embedded whatsits out there where you cannot
upgrade the hardware *or* the firmware, and those are a security
problem ... but they are irrelevant to this discussion.  By
definition, you cannot install a new version of openssl on such
a system.

@_date: 2017-07-09 07:41:54
@_author: John Denker 
@_subject: [Cryptography] gobble, gobble, gobble, gobble 
"LinkedIn settled for $1.25 million in 2014."  Reference:
  Then I reply to that customer:
Gobble, gobble, gobble, gobble.  That's the noise a thanksgiving
turkey makes.  I find it odd that when you were in school they
never made you read Nassim Taleb.
  Oh well, I've given you my analysis, and you've given me yours.
Evidently they differ.  Like I said, it's your choice.
If you get hacked, I hope it's not expensive, Mr. Karpels.
I hope none of your subscribers commit suicide, Mr. Biderman.
I hope nothing bad happens, Ms. Clinton.

@_date: 2017-07-11 02:17:09
@_author: John Denker 
@_subject: [Cryptography] A software for combining text files to obtain 
Agreed.  Why are we even talking about this?
It reminds me of the facetious definition of "foolproof":
It's secure provided you are being attacked by fools only.
The actual number of random bits is roughly the log of the
size of the list of guesses the adversary has about where
you got your text.  (The famous Shannon guessing entropy,
about 1 bit per character, is for unknown text, not known
or knowable text.)
For the cases that people are worried about, e.g. the
proverbial embedded router or scada box or live-CD distro,
that number is not very large.
And once again, please let's not rely on /entropy/ to
characterize random number generators.  It's possible to cook up  a distribution that has infinite entropy, but
which is completely predictable more than 25% of the time.
It's not even particularly difficult, especially now that
you know it's possible.

@_date: 2017-07-14 17:53:09
@_author: John Denker 
@_subject: [Cryptography] Defeating timing attacks 
That's not practical.  Better solutions are readily available, as
discussed below.
Not good enough.
Not good enough.
Not good enough.
Randomizing the timing just turns the attack into a statistics
problem.  The NSA is reeeeeally good at statistics.  You can
"somewhat" slow down the leak, but it's still a leak.
Furthermore, a much better way to defeat timing attacks is already
 a) use a dedicated machine,
 b) inside a Faraday cage, and
 c) emit the results at some pre-arranged time.
That's because
 a) in a multi-tasking environment, one task can spy on another
 b) timing isn't the only issue;  there are other side-channels
 c) you don't need to defeat the timing attack on an instruction-
  by-instruction basis, just on a message-by-message basis.
Things get a little trickier in interactive situations (as opposed
to file-sized messages) but still manageable.
This approach means you don't need to argue with the hardware
designers and compiler designers.  They do their thing, and you
do yours.
As will all of cryptography, and security in general, this still
requires tremendous attention to detail.  There is a rather long
list of ways bad guys could exfiltrate information about your
message, and you have to stop them all.  Even so, the point
remains, the timing issue is manageable.

@_date: 2017-06-06 14:59:15
@_author: John Denker 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
In case you missed it:
Quoting from:
  This seems like a pretty good explanation for the rapid arrest of
NSA contractor Reality Winner.
When it comes to journalistic tradecraft, the picture is a bit
 -- In the last few years, there have been a number of high-grade
  leaks from sources whose identities have remained concealed.
 -- OTOH there are more than a few cases of sources whose identities
  did not remain concealed for long.
There are lots of forensic marking techniques.  Now that the yellow
tracking dots are well known to leakers and would-be leakers, I
reckon there will be a push to deploy other techniques.
  All this casts /communication/ security in a different light.
It does no good to encrypt the communication channel, if the
payload itself is compromising.

@_date: 2017-06-27 11:16:59
@_author: John Denker 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Current reality ain't that simple.  Not even close.
The xenial 16.04 LTS manpage for getrandom(2) says quite explicitly:
And that's an understatement.  Whether unnecessary or not, reading
not-particularly-large quantities of data is tantamount to a
denial of service attack against /dev/random and against its
upstream sources of randomness.
No later LTS is available.  Reference:
  Recently there has been some progress on this, as reflected in in
the zesty 17.04 manpage:
  However, in the meantime openssl needs to run on the platforms that
are out there, which includes a very wide range of platforms.
It could be argued that the best long-term strategy is to file a
flurry of bug reports against the various kernel RNGs, and then
at some *later* date rely on whatever the kernel provides ... but
still, in the meantime openssl needs to run on the platforms that
are out there.

@_date: 2017-03-04 19:15:40
@_author: John Denker 
@_subject: [Cryptography] Secret Handshake problem. 
There is an eeeenormous literature on this under the heading of IFF --
Identification Friend or Foe.
Not just the membership status, but the very existence of C is concealed
unless D is a member of the clube.

@_date: 2017-03-05 15:41:24
@_author: John Denker 
@_subject: [Cryptography] Mark XIIa Mode 5 IFF 
Some interesting bits are explicitly known and/or easy to figure
out, even by someone (like me) who has no special knowledge.
Two crucial elements of the threat model are the active attacks,
namely exploitation and spoofing.
It is often emphatically alleged that IFF is a misnomer.  A positive
response indicates a friend, but foes are not explicitly identified,
and some subset of the friends might go unidentified.
However, that is only true /when IFF is working properly/.  Beware
that sometimes team A can exploit team B's IFF and thereby locate
and identify the foes of A.  For example, the British were able to
exploit the German FuG 25a during WWII.
To say almost the same thing in crypto terminology:  an IFF system
needs to worry about replay attacks.  A replay of the interrogation
pulse could result in exploitation;  a replay of the reply could
result in spoofing.
The zeroth-order WWII-era defense is to send a coded interrogation
pulse, based on a code that changes daily.  However, you still need
to worry about intra-day replay attacks.  The current Mark XIIa Mode
5 uses a very short-term key based on the time of day (in conjunction
with the longer-term key).
This by itself makes it difficult to replay the interrogation pulse.
The interrogation contains a nonce that makes it doubly difficult
to replay the reply.  That is, replies are not portable from one
interrogator to another.
Another wrinkle is that the pulses can encode GPS location info.
A pulse where the encoded info is inconsistent with the primary
radar fix is probably either an echo or a replay attack.
A major complication is that the pulses are rather short.  There
are not enough bits to do everything you might like to do using
standard crypto primitives.  I suspect there is some bespoke
crypto involved.
The following argument is not entirely convincing, but I will
mention it anyway, as a possible topic of discussion:
 -- If the bad guys can break into your IFF reply and read the
  current X,Y,Z position of your stealth aircraft, that is a
  very very bad thing.
 -- If they can only read it 40 hours later, that's bad but not
  quite so bad.  So maybe the crypto doesn't need to be absolutely,
  formally unbreakable.
There are also passive attacks.  The pulses in both directions can
be picked up by the adversary and subjected to direction-finding.
To alleviate (but not eliminate) this threat, spread spectrum is
used.  This is analogous to the CDMA (code-division multiple access)
used in cell phones, with one exception.  CDMA and the bar codes on
retail products are examples of non-secret coding that can be used
to illustrate the concept to laypersons.  Military spread spectrum
uses secret codewords.  If you don't know the code, the pulse will
look like random noise, and from far away you won't even recognize
it as a pulse.  Mark XIIa changes the codewords on a very short
noise.  There's not much anybody can do to prevent that.
Crypto has never fitted very well into the OSI layer model.  For
example, PGP misfits on top of layer 7 email, and IPsec misfits
slightly above and slightly below layer 3.  Meanwhile, spread
spectrum coding misfits somewhere around layer 2.
Of course the IFF system has nonces and message authenticators
on top of that, at higher layers.

@_date: 2017-03-08 11:52:07
@_author: John Denker 
@_subject: [Cryptography] encrypting bcrypt hashes 
This is nonsense, as I presume you know.
First of all, encryption in the usual sense of the word doesn't
help.  OTOH there are standard ways of turning any block cipher
into a hash function, but at the end of the day it's just a
hash, so why not call it that.
As well they should be.
Pouring a gallon of snake oil onto the PINs will not help.
That won't help either.  If it's quick enough to be acceptable
to ordinary users, it's quick enough to be brute forcible.  The
key space is just too small.
In such a situation, the only thing that has a ghost of a chance
of making a material difference is some sort of PAKE i.e. some
sort of zero-knowledge proof that the user knows the PIN.  That
way there is nothing that could be stolen from the server that
would permit an offline brute-force attack.  The server never
sees the PIN, not even briefly.
  The remaining threats include stealing the user's endpoint
  and trying various PINs, but now this has to be done online,
  so the server can detect it.  Attempts can be rate-limited,
  without even relying on work factor;  a simple sleep() will
  suffice.
  A worse threat is subversion of the user's endpoint, as
  discussed in yesterday's headlines.  The leaks discuss CIA
  activities, but if the FSB / Third Directorate / MI6 / DSG
  / etc. are not doing the exact same thing on a massive scale
  I would be very surprised.
    It pains me to write the previous paragraph, because in
    the US there is a pettifogging legalistic doctrine of
    "reasonable expectation of privacy".  This is the sort of
    thing that makes people despise lawyers.  It means that
    abuse can be used to legalize abuse.  If people expect
    their homes and phones to be insecure, it becomes legal
    to bug them without a warrant.
    To that I say, we /demand/ that the government uphold the
    law.  We /demand/ that they vigorously and proactively
    defend the 4th amendment.  The right of the people to be
    secure in their persons, houses, papers, and effects,
    against unreasonable searches and seizures, shall not be
    violated ... by the government or anybody else.  Among
    other things, this means the CIA must tell vendors about
    whatever zero-days they discover.
    Whether or not this meets some disingenuous definition
    of "expectation" is not my problem.  It is a demand,
    and will remain so.

@_date: 2017-03-10 16:21:14
@_author: John Denker 
@_subject: [Cryptography] Crypto best practices 
The insight department is out of the office at the moment, but
here are a few comments from the keen-grasp-of-the-obvious
1) The quote "re-keying is not recommended" must not be taken
 out of context.  It applies in a very special case where the
 session is so short that rekeying is not worth the trouble.
 There will be a new key for the next session, and that provides
 sufficient forward secrecy in this special case.
2) AES has related-key weaknesses that you could drive a truck
 through ... but we knew that already.  BS claims that this is
 not a weakness in AES, because only an idiot would choose a
 related key.  However, I disagree.  There are plenty of real-world
 situations where you might want related keys, e.g. random-access
 disk encryption, real-time interaction, etc.  So either hash
 your keys before letting AES get its mitts on them (which means
 the allegedly fast AES key scheduling is not so fast), or go
 straight to ChaCha20.
3) I find cipher chaining modes to be creepy as a matter of
 principle.  You would really rather have a new key per block.
 However, many ciphers are weak and/or slow when it comes to
 changing keys.  The idea of changing the IV is a kludgey
 workaround for this problem.  It fails in lots of situations,
 including random access, real-time interaction, et cetera.
4) The quoted passage is only one of several hints that they
 have something that exploits weaknesses in AES.  I'll bet
 the CIA and NSA are really steamed about this leak, because
 now every OS and every app is going to upgrade to AES-256
 and hash all their keys.
5) One of the big problems with tapping and bugging is always
 backhaul, i.e. getting the harvest back to headquarters.  I
 assume when the CIA snatches data from somebody's phone they
 make it look like something innocuous, perhaps a TLS or Signal
 connection.  Otherwise it would be too easy to detect.  Also
 the fact that they are defending against against weaknesses
 in common protocols (not just exploiting them) lends support
 to this idea.
6) The question is not whether they have a related key attack;
 we can take that as a given and move on.  The more interesting
 question is whether they have something else /also/.  Some
 sort of protocol failure during key exchange, in some widely
 used protocol(s).
It is remarkable that they consider such weaknesses to be super-
highly classified.  As it says in reference [1], quoting none
other than Frank Rowlett,       "in the long run it is more important to secure one's own        communications than to exploit those of the enemy."
Alas the NSA and CIA seem to get this wrong again and again
and again.
[1] Thomas R. Johnson
    "American Cryptology during the Cold War; 1945-1989"
     Center For Cryptologic History / National Security Agency (1998)
That document used to be available on the NSA web site:
     but they took it down.  Have they never heard of the WayBack Machine?

@_date: 2017-03-23 15:06:24
@_author: John Denker 
@_subject: [Cryptography] encryption + authentication - waiting - chaining 
There's no need to rely on hearsay.  In telephony, very little
latency can be tolerated, and there is no relevant bound on the
length of a message.  The same idea in a slightly weaker form
applies to random-access storage devices:  You have to deal with
smallish blocks, not entire messages.
Blocking until a first pass has been made over the entire message
would be even worse than the usual schemes for authenticating at
the "end" of the "message".
For real-time interactive services and for random-access storage,
there is no such thing as the "end" of a message, and indeed no
such thing as a "message".  You have to encrypt and authenticate
on a block-by-block basis.
Also, in the real world there will be dropouts and there will
not be time for retransmissions.  Therefore chaining modes are
not usable.
Inefficiency is an issue.  Authentication is always going to cost
"something", so the question is how much.
I offer no opinions about the strengths or weaknesses of the following
example, but at the very least it serves as a stake in the ground,
i.e. as the starting point for discussions and comparisons:
  "ChaCha20 and Poly1305 for IETF Protocols"
  This uses a different key for each block.  Therefore  no chaining is
required, and random access is as easy as any other kind of access.
Poly1305 provides AEAD.
By way of contrast, AES is slower than ChaCha20 ... double-especially
if you include the cost of block-by-block rekeying, triple-especially
if you want to make it resistant to related-key attacks.
The ideal, almost always, is a new key per block.  A chaining mode is
just a kludge, just a fig leaf to conceal that fact that the underlying
block cipher cannot be rekeyed quickly enough and/or securely enough.
The services where encryption and authentication cannot wait until
the "end" of the "message" tend to be high-value services.  The
following story, albeit unverified, illustrates the sort of value
I'm talking about:
  If you design a cryptosystem that works for real-time and/or random
access services, it will work OK for big static messages also.  However,
the converse is not true.
  I say "OK" rather than "perfectly" because I suspect that the
  real-time / interactive version may require ciphertext expansion
  beyond what would be required for a big static message.
  If so, then "acceptable authentication latency" ought to be treated
  as a dialable parameter.  Setting it to infinity at the start of the
  design process is a Bad Idea.
Encryption and authentication have different requirements.  For ordinary
telephony, the encryption blocks have to be quite small, to keep the
latency under control.  Stream ciphers, if carefully used, work fine.
The authentication blocks can be somewhat larger.  Millisecond-by-millisecond
authentication would be overkill.  Second-by-second authentication should
suffice.  Waiting until the end of the call is not acceptable.
As for making things resistant to misuse:  I am reminded of the following:
 ++ For every proverb, there is an equal and opposite proverb.
In this case, there is a proverb that says we should make it easy to
do the right thing, and hard to do the wrong thing.  However, there is
another proverb that says garbage in, garbage out.
For example, an ordinary lawnmower comes with a number of safety features.
Even so, it remains open to abuse in innumerable ways, and even if used
carefully there is some irreducible risk.  (Not to mention the urban legends
about the dangers of picking up a lawnmower and using it to trim hedges.)
There is a nontrivial issue here.  It is *much* more nuanced than saying:
 -- "it's unsafe to use a stream cipher",
 -- "it's safe to use AES, because only an idiot would use related keys", or
 -- "use XXX because it retains some modicum of security even if the IV is reused".
Instead I would say:
 ++ Sometimes there are good reasons for a stream cipher, but you have
  to be careful.
 ++ Sometimes there are good reasons for using a bunch of related keys,
  but you have to be careful.
 ++ Sometimes it's safe to use AES, but you have to be careful.
 ++ Sometimes it's OK to mow the lawn, but you have to be careful.
 ++ You can't make anything foolproof, because fools are so ingenious.
One can fantasize about a formal language that would specify what each
module requires and what it provides (in loose analogy to a package
management system like dpkg).
Right now I would settle for an informal language, perhaps a checklist.
For example, a certain mode:
 -- requires an IV that is unguessable
 -- requires a generic block cipher
     (which will bring in its own requirements)
 -- requires absence of transmission dropouts
     (this is because of chaining)
 -- provides message encryption
 -- does not provide authentication
 -- does not provide random access
 -- does not provide real-time service
 -- does not provide protection against traffic analysis

@_date: 2017-03-26 17:08:08
@_author: John Denker 
@_subject: [Cryptography] escalating threats to privacy 
1) Quoting from:
    Hypothetically, if one were being logical, one might observe that
  the recent attacker, acting alone, killed three people using an
  SUV and killed one person using a knife ... yet there are no calls
  to outlaw SUVs or knives.
Non-hypothetically, I am aware that logic plays remarkably little
role in political discussions of this issue.
The political situation is not entirely one-sided:
  2) Quoting from:
  Bought and paid for by:
Technical details:
  For "some" purposes one might distinguish between unchecked corporate
surveillance and unchecked government surveillance, but personally I'd
rather have none of the above.  And I worry that one leads to the other
I like the farmer's market because I can buy stuff without having
my every choice recorded and analyzed.
I'm not so much worried by how stores track their tomatoes.
I'm much more worried by how they track me.
Quoting from:

@_date: 2017-03-29 08:22:31
@_author: John Denker 
@_subject: [Cryptography] AES related issue 
AES is known to have horrible related-key weaknesses.
To verify that the problem is here (rather than, say, in the
measurement methods) repeat the experiment, but hash each key
before feeding it to AES.
Or (!) use something like ChaCha20 that doesn't screw up when
given related keys.

@_date: 2017-03-29 12:30:31
@_author: John Denker 
@_subject: [Cryptography] escalating threats to privacy 
Update:  S.J.Res 34 has now passed both houses of congress, and is
expected to be signed into law.  This is home-page news on typical
news sites:
        I'm wondering what the remedies might be:
 -- Negotiate with the ISP to pretty please not do that?
 -- Everybody use Tor for everything -- despite the inefficiencies?
 -- Is there anything we can do to make Tor more efficient?
 -- ????????
See also next message.

@_date: 2017-03-29 12:39:45
@_author: John Denker 
@_subject: [Cryptography] stegophone (was: escalating threats to privacy) 
Here's yet another major threat to privacy:  In the context of
warrantless and suspicionless searches, on 02/25/2017 08:26 AM,
A *lot* of people are upset.  For example:
  which says in part:
See also:
  Where an I get a 'duress' device?  Or duress app?  When I google for
"duress app" I mostly see things that would more appropriately be
called /distress/ apps, e.g. "help I've fallen and can't get up".
I can find all sorts of duress-related proposals, but they seem to
have withered on the vine.
What I would like to see is something very simple, which I call a
stegophone, although the idea applies to all devices, including
laptops, not just phones.  The specifications are as follows:
 *) There are two passcodes:  one for normal use, and one for duress.
 *) Unlocking the phone using the normal passcode results in a completely
  normal phone.
 *) Unlocking it with the other passcode results in a sanitized phone:
   -- a short innocuous contact list;
   -- no call logs;
   -- no stored messages;
   -- an app that immediately deletes any logs, messages, etc.,
    so there is an innocent explanation for their absence, even if
    the adversary gets your actual call history from communication
    intercepts.
   -- a feature that instantly zeroizes the key used for full-image
    encryption of the normal phone, so that even they beat the normal
    passcode out of you it wouldn't be valid, and they wouldn't even
    know for sure that the phone had ever had any features or any
    information other than what the sanitized phone offers.
 *) The device is protected against disassembly and against cold-boot
  attacks.
In particular, the sanitized phone does *not* give any indication that
the duress app was ever installed.  More generally:
 *) It does not do anything to call attention to itself, or to you.
 *) It does not give anybody any reason to suspect you have anything
  to hide, or that you are being less than 1000% cooperative.
 *) It is not a distress app.  It does not send messages calling for
  help.
 *) It does not play an audio file reciting the fourth amendment at
  high volume.
The objective is to make finding the device totally unremarkable and
unexciting, rather like finding a pair of socks on your feet.
Usage note:
Consider the following threat scenario:
  Suppose the adversary grabs you by the neck and takes your phone
  by force, then demands that you hand over the passphrase.  Further
  suppose that your normal passcode is constructed using diceware:
      and your duress passcode is constructed using the Blackberry method,
  i.e. a full-length cyclic permutation of the normal passcode.  Now
  you are screwed, because it will stick out like a sore thumb:
     orrectHorseBatteryStapleC
Therefore the duress passcode MUST be constructed independently.  It
MUST NOT follow the Blackberry example.
Also, it must be easy to remember, even in a panic situation.  It
does not need to be very secure;  it could be your middle name, or
your date of birth, or Sw0rdf1sh.
  Avoid the temptation to be clever or political, e.g. 1776 or 1789.

@_date: 2017-03-30 14:59:34
@_author: John Denker 
@_subject: [Cryptography] stegophone 
The blackphone offers, as an advertised feature, the ability to
switch from one "space" to another.  This is advertised as a way
to keep your work life separate from your personal life:
  So it seems some existing platforms do what is needed, or pretty
close.  This includes the hardware and the open-source OS.
That's fine if you know you are going to the airport.  But here's
a riddle:
Q:  What's the difference between the Border Patrol and the Boy Scouts?
A:  The Boy Scouts have adult supervision.
I mention this because although not all ICE agents are alike, some of
them seem to think they have unlimited authority in their self-defined
"border zone" extending 100 miles from any land or sea border of the US.
They have been repeatedly slapped down by the courts, but they don't
care ... and in the last couple of months they have grown markedly
bolder.  Note that 2/3rds of the US population lives within this 100
mile zone.
Therefore it is an essential part of the stegophone specification that
it can enter "sanitized" mode at any moment, even under duress, with
no detectable action required of the user.  I'm sure the situation in
other parts of the world is much, much worse.

@_date: 2017-05-21 07:56:05
@_author: John Denker 
@_subject: [Cryptography] Password rules and salt ... or not 
Responding first to the Subject line:
 *) It seems to me that there are fundamental problems with how
  passphrases are used, problems that cannot be solved by the use
  of salt, traditional password rules, and/or nontraditional
  password rules.
  -- Defending the traditional approach by saying "all the kids are
   doing it" is not acceptable.  I tried that on my mother when I
   was five years old and it didn't work.
  -- Zero-knowledge passphrase proofs mean that the server never
   sees the passphrase at all.  Implementations are available:
                -- If you need temporary compatibility with an existing passphrase
   database, it is trivial to interpose an agent that uses the
   passphrase to create a zero-knowledge verifier.  This should be
   very temporary.  It should be the first step on a mandatory
   migration path.
In addition to being impossible to implement, it does not do
the job, not even in principle.  There are lots of passphrases
that have never been used before that are nevertheless insecure.
As Snowden put it:  Assume your adversary is capable of one
trillion guesses per second.
Let's be clear:  The  necessary condition is being hard to
guess.  That is not the same as being previously unused.
Any thought that begins with "Aside from the cost" is not
going to end well.
That's equivalent to saying that it's a security problem.
It's one more thing to go wrong.  That's the last thing
we need.
Passphrase construction is as much a human factors issue as
it is a mathematical cryptography issue ... and should be
treated as such.
Being hard to guess is one requirement.  Being easy to
remember is another.  This creates a dilemma, because
a passphrase that is hard to guess will likely be hard
to remember, especially if it is infrequently used.
This dilemma is exacerbated by the fact that users visit
multiple sites.  Re-using the same passphrase is a disaster
waiting to happen, if the sites store things in the
old-fashioned way.
Zero-knowledge passphrase proofs escape this dilemma.  You
can safely have one master passphrase.  This has several
advantages, including the fact that the passphrase is more
frequently used, and therefore less likely to be forgotten.
This is a situation where the best strategy is to put multiple
eggs in one basket, and then vigorously defend that basket.
Birds have been following this strategy for more than a
hundred million years.
In this forum there have been several long discussions of
how to manage and/or migrate password databases, none of
which make sense to me.  I leave it as a question:
  Why not just migrate to zero-knowledge password
  proofs and be done with it?

@_date: 2017-10-12 11:01:15
@_author: John Denker 
@_subject: [Cryptography] ? recommendations for secure communications 
Hi --
Suppose you were a reporter at a small local newspaper.
No great tech skills and no great resources.  Still,
you want people to be able to send you confidential
tips and leaked documents.
The standard answer is SecureDrop.  It seems pretty
decent to me.  It's a complicated system, but I
don't see any way to simplify it without sacrificing
a significant amount of security.
I suppose Signal makes sense for simple messages, but
even so there is a record of who called whom.  And
AFAICT there is no good way to attach typical documents.
Can email be secured in a reasonable way for ordinary
non-wizard users, or has everybody given up on this?
More narrowly, is it worth trying to provide some security
for the average non-techie gmail user?  In particular, has
anybody we know evaluated this plugin for adding PGP to
the gmail web interface?
  Anything else you would recommend?
Considerations include:
 -- ease of installation
 -- general ease of use
 -- attachments
 -- security w.r.t message body
 -- security w.r.t metadata
Favorite maxim:
  Metadata is data.
  A cryptosystem that leaks metadata is a cryptosystem that leaks.

@_date: 2017-10-18 12:45:47
@_author: John Denker 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce reuse 
I hate to ask silly questions, but is there any cryptosystem or any
mode whatsoever where key/nonce reuse is acceptable?
It seems to me that chaining modes depend on the randomness of the
plaintext.  Unless you can reliably establish a hefty lower bound
the amount of such randomness --  which seems hard to do since the
plaintext is not known in advance -- using it as a substitute for
a random key/nonce seems exceedingly unsafe.

@_date: 2017-10-19 05:55:21
@_author: John Denker 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
AFAICT all of the following are incompatible with all chaining modes,
including (but not limited to) fancy wraparound modes.  These categories
are not mutually exclusive:
 -- streaming services (since users commonly join in mid-stream)
 -- interactive services (including telephony, distributed gaming, etc.)
 -- random access (including disk encryption)
 -- anything that encrypts at OSI layer 2 or 3 (since lost packets are not retransmitted)
That notion of "most cases" seems to exclude quite a wide range of
services, as itemized above.  This started as a discussion of WPA2.
There's a reason why WPA2 doesn't use CBC or anything like that.
So what exactly should WPA2 have done?
Memo from the keen-grasp-of-the-obvious department:  My advice:

@_date: 2017-09-05 10:06:37
@_author: John Denker 
@_subject: [Cryptography] Finding Nemo's random seed 
This has been understood in the physics community since the
The root cause is ambiguity in the concept of "randomness" itself.
The type of "randomness" you want depends greatly on the application:
 -- Sometimes you want something that is entirely deterministic and
  predictable, just complicated-looking, e.g. sea grass.
 -- Sometimes you want to guarantee that Thing A is uncorrelated with
  Thing B.  Randomization is one way of doing this, but not the only
  way or even the best way.  Applications include spread-spectrum
  communication codewords, radar pulses, Monte Carlo integration,
  et cetera.
 -- Sometimes you want cryptographically-strong completely-unpredictable
  randomness.

@_date: 2017-09-11 03:48:27
@_author: John Denker 
@_subject: [Cryptography] Zero Knowledge: Have I Been Pwned? 
Thanks for calling attention to that.
That's a fascinating question.  I reckon Bill Frantz gave the right
answer:  Download a Bloom index rather than the whole corpus.  Also,
have one or more trusted third parties sign off that (a) the corpus
was constructed in a reasonable way, and (b) the Bloom index was
constructed correctly.
As a tangentially-related issue:  to avoid /future/ compromises, we
should insist my password never be sent from my machine to anywhere
else.  Instead, as the Subject: line suggests, use a zero-knowledge
proof that I know the password (i.e. that I am using the /same/
password as the one previously set up and validated).
The password issue is as much a user-interface problem as anything
else.  Passwords do not scale well when the user interacts with N
servers.  This can be alleviated by using a password manager, but
from the user's point of view a password manager is indistinguishable
from a zero-knowledge proof manager.
There is a range of possibilities:
  *) Entirely avoid all forms of online activity that require passwords.
  *) Use one password for everything.
  *) Try to remember N different passwords.
  *) Use a password manager.
  *) Use zero-knowledge proofs.
Of these, only the first and last offer reasonable security
without being Pareto-inferior to other items on the list.  In
other words, if you are going to authenticate at all, there
is AFAICT no excuse for not using zero-knowledge methods.
Well, here are some scenarios of concern.  These are off-the-cuff
thoughts;  I'm sure a determined adversary could come up with even
nastier attacks:
1) Suppose  a) the bad guys own (or pwn) troyhunt.com
   or cloudflare (where troyhunt.com is hosted);
 b) collect all the queries (sha1 and otherwise); and
 c) test them against some corpus with more than 306 million entries.
If your password is in the big corpus but not the small one, you
get back a negative result, and you cannot detect any wrongdoing,
but the bad guys now have your IP address to go along with your
password, so you are now much more vulnerable than before.
2) Suppose you get back a positive result.  You are now in a race against
the owner (or pwner) of the testing site, to see whether you can change
the password faster than they can exploit it.
The bad guys can give themselves a head start by delaying the positive
3) The bad guys can return a false negative result.  This could in
principle be detected by comparing online to offline results, but
I doubt anybody has checked for this.  And the bad guys could do
this selectively, making it even harder to check for.

@_date: 2017-09-13 12:30:31
@_author: John Denker 
@_subject: [Cryptography] letsencrypt.org 
Executive summary:  I'm a happy customer.
I have used the service for quite a while, since early beta days.
I haven't cryptanalyzed it, but it seems quite solidly built.
They're not aiming for top-level NATO-nuclear security, but
within the bounds of what they claim to do, they seem to take
security very seriously.
I contributed a few very minor suggestions concerning the UI.
The UI has slowly but steadily gotten better over time.  It
is still not super-easy to use;  I would classify it as more
wizard-friendly than muggle-friendly.  The results require
some post-processing to make them fit into my not-very-complex
That is to say, there was an initial learning curve, but by
now renewals have become easy and completely routine for me.
I have about 800 lines of localized instructions and scripts
to help me with this.
Bottom line:  YMMV, but for my purposes, it works great.  Using
it is verrrrry much better than not using it.

@_date: 2018-08-05 15:32:53
@_author: John Denker 
@_subject: [Cryptography] what application creates single-use coded email 
The idea has been around for decades.
      Bob Hall at Bell Labs developed rather fancy "channels"
    and references therein.

@_date: 2018-08-09 10:48:45
@_author: John Denker 
@_subject: [Cryptography] understanding PGP etc. -- best public 
Rather than "no", a more precise answer would be "it's
infeasible -- so far as we know".
1) We do *not* have any formal proof that the crypto used
 by PGP is unbreakable.
2) In fact, all crypto of the kind we are considering
 can be defeated "in principle" given unlimited computing
 resources.  Just do a brute-force search of the key
 space.
 Beware that the keyspace is *very* large.  We are not talking
 about two-digit numbers, as in the example above, but rather
 100-digit numbers.  There are a *lot* of those.
3) The strongest statement that can be made goes something
 like this:  Using publicly-known methods and present-day
 (or reasonably foreseeable) hardware, nobody has enough
 resources to crack your message on any relevant timescale.
If you want to google something, you might start here:

@_date: 2018-08-13 09:48:44
@_author: John Denker 
@_subject: [Cryptography] Throwing dice for "random" numbers 
*) Is this meant to be a conceptual question?  And
are we supposed to take the word "sum" literally?
If so, then we proceed as follows:
Twelve is big enough that we can treat the distribution
of sums as nearly Gaussian.  For a single die, the distro
is nowhere near Gaussian, but we know the mean is 3.5 and
the variance is 2.92, as you can easily verify directly
from the definition.  When you sum multiple dice, the mean
and variance grow linearly, so for 12 dice we get a mean of 42 and a variance of 35.  So the stdev is =5.92.
It is well known that a Gaussian can be approximated well
by a triangle with a HWHM equal to the stdev.  So the full
width at baseline (FWBL) is 4.  It can be approximated
less well but still decently by a rectangle with the same
FWBL.  That is to say, all values within 2 of the middle
are equally likely and all others are irrelevant.  So we
estimate the entropy as log(4).
If you want to do a bunch of calculus, the fancy "official"
result is log((4e) ) ... where the prefactor is (4e)
= 4.13 ... so our intuitive value of 4 was not off by much.
All together you get log(4.13) + log(5.92) = 4.61 bits
per roll.
*) Or is this supposed to be crypto engineering, as suggested
by the Subject: line?
In that case, you shouldn't be computing the sum at all.
You can get log(6) = 2.58 bits of entropy per die per roll,
and if you consider them all separately you get 12*log(6)
= 31 bits per roll.  That's a huge improvement in
throughput.  There's no need to distinctively mark the
dice.  We assume each die is statistically independent
of the others.
*) Or was the question meant to ask how to handle the
systematic imperfections in the dice?
In that case, I recommend using the appropriate Rnyi
H_ tells us the worst-case resistance to attack, which
is appropriate for engineering a defensive cryptosystem.
So we look at the probabilities.  The ideal case is
which gives you -log(1/6) = 2.58 bits of adamance per
die per roll.
Now suppose that on the basis of careful measurements
you know that the following is a *bound* on how badly
non-ideal things can be:
which gives you -log(1.2/6) = 2.32 bits of adamance per
die per roll.
This is what I would recommend (unless I have guessed
wrong about the intended application).  This is what
Turbid does.
Engineering philosophy:  It is neither necessary nor
possible to measure the exact amount of "randomness"
(whatever that means) in the dice.  It suffices to
obtain an estimate that is
 -- small enough to be a provable lower bound, and
 -- large enough to be useful.
The bound must be calculated from the physics of the
sourc.  It cannot be measured a_posteriori by looking
at the statistics of the downstream output. As Dykstra
famously said:
  Testing can be used to show the presence of bugs,
  but never to show their absence.
Here the critical "bug" is hidden nonrandomness.

@_date: 2018-08-18 02:42:28
@_author: John Denker 
@_subject: [Cryptography] Throwing dice for "random" numbers 
0) Worrying about biasing the order is a very minor optimization.
1) Marking the dice and then reading them out in marked
 order is a pain in the neck.
2) If you are really worried about the order, it would
 be better to separate them mechanically in advance,
 using something like an egg carton or a spice rack
 with one die per cell.
   Suppose you want decimal digits.  Twelve 6-sided dice can
be treated as a 12-digit base-6 number.  In decimal that
is a number from 0 to 2.18e9.  Roll the dice.  Whenever
there is an outcome greater than or equal to 2e9, throw
it away and re-roll.  Take the result modulo 1e9.  That
gives you a uniform distribution over 9-digit decimal
numbers, limited only by imperfections in the dice, with
no non-uniformity introduced by the modulo operation.
The same procedure generalizes to bases other than 10,
including 2, 8, 16, and 36 or 62 (i.e. alphanumeric).
Discarding out-of-range outcomes has been standard
practice since the 1940s (which is kind of a long
time in the computer business).
Also, for what it's worth (which is not much):  8-sided
dice are cheap and widely available (more so than
16-sided dice).  You can use those to generate bytes
or words having any integer number of bits, with no
need to re-roll.  This is not worth much because the
inefficiency introduced by re-rolling is very small.
It's at most a cosmetic issue, not a serious issue.

@_date: 2018-08-27 08:16:42
@_author: John Denker 
@_subject: [Cryptography] the crypto wars never end 
In case you missed it:
  The U.S. government is trying to force Facebook Inc (FB.O) to break the encryption in its popular Messenger app so law enforcement may listen to a suspects voice conversations in a criminal probe, three people briefed on the case said, resurrecting the issue of whether companies can be compelled to alter their products to enable surveillance. 
If the government prevails in the Facebook Messenger case, it could make similar arguments to force companies to rewrite other popular encrypted services such as Signal and Facebooks billion-user WhatsApp, which include both voice and text functions, some legal experts said.
Additional discussion:

@_date: 2018-02-08 12:22:41
@_author: John Denker 
@_subject: [Cryptography] Quantum computers will never overcome noise 
It is an observable fact that the quantum computing
industry has overpromised and underdelivered for years.
The rest of the magazine article is opinions.  Everybody
is entitled to their own opinion, but such opinions have
zero probative value.  FWIW I happen to agree with most
of Kalai's opinions, but 2x zero is still zero.
AFAICT the mathematical analysis to which he refers is
      	(abstract)
  	(article)
Alas, that doesn't prove much of anything either.
It shows that a particular mathematical model of a
particular machine design doesn't work, but that's
not enough to support any wide-ranging conclusions.
He concludes there is reason to be skeptical of
quantum computing, but I was mighty skeptical
The idea that you might be able to simulate some
quantum computer algorithms on a /classical/ analog
computer has been around for a while, and has some
merit.  It might eventually form the basis of a
robust proof that quantum computing is not worth
the trouble.

@_date: 2018-01-01 12:08:44
@_author: John Denker 
@_subject: [Cryptography] Fast handling of IP Address changes for HTTPS 
If you are running a server (at home or otherwise),
then in many cases the static IP is a feature not
a bug.
If you want people to be able to find your server,
then you have to have a DNS entry that points to
it ... and the DNS name has to be reasonably stable
over time.
 -- If reverse-DNS is working, every script kiddie
  can reverse your IP to find the DNS name, archive
  the name, and use that to identify you forever.
 -- Even if you think you have turned off reverse
  DNS, if you utter the server name in public,
  the big data companies will do a forward DNS
  lookup, match your IP address (whether static
  or not) and boom, all your traffic is identified.
 -- et cetera.
If you are serious about hiding, you need something
sure how much I trust TOR).  Rotating your IP address
is barely even security theater.
The same issue arises with mobile phones, only
much worse.
The word "supercookie" is sometimes applied to this
If you want to have a server and some measure of
anonymity at the same time, that's called "dark
web" and requires advanced, specialized techniques.

@_date: 2018-07-15 09:28:06
@_author: John Denker 
@_subject: [Cryptography] storage encryption 
Hi Folks:
Suppose there was a low-budget or no-budget activist group
that wanted to share documents via one of the cloud storage
services.  Obviously one wants to add a layer of encryption.
Question:  Has anybody we trust evaluated cryptomator for
security and/or ease-of-use?
  Anything else?
Are any of the closed-source solutions any good?  There
are plenty of pricey solutions.  Also quite a few that are
free but closed source, which remind me of the proverb:
You're not the customer, you're the product.
Is there some better way of approaching the whole group
collaboration issue?
I reckon there are extreme human-factors issues here,
because security requires getting everybody (and I do
mean everybody) on the team to follow the rules.  It's
not very effective to threaten volunteers that they'll
be fired if they break the rules.
I am also aware of the zeroth law, which says:
If you don't have physical security, you don't have security.
So the whole security problem might be impossible even
before we get to the crypto.  Sigh.

@_date: 2018-07-21 16:51:30
@_author: John Denker 
@_subject: [Cryptography] storage encryption 
Their goals are the same as everybody else's:
  confidentiality + integrity + availability
If you want an example, here's an obvious use-case:
  Each of N people are told to write their chapter of
  a report, then put a draft on the cloud drive where
  all N can see it.  There is no need for the other
  M-N members of the organization to read the draft,
  or even know that it exists.
An easy-to-read overview of small-organization security
issues, including some useful checklists, is here:
  There are too many threats to mention, even if I knew
what they all are, which I don't.
If you want an example, start with a Podesta-style
spearphishing attack.  That worked in 2016.  Attackers
are going to keep using it until it stops working.
  Reportedly, the Hillary campaign was advised to use
2fa (which might have blunted the spearfishing attack),
but decided that would be too burdensome.
True but not the whole story, I would hope.  Methods
for dealing with multi-recipient messages have been
around for eons.  See e.g. PGP.  Single session key
versus multiple access keys.
I would hope not.  Neither same session key nor
same access keys.
I would hope not.  Need-to-know reduces the attack
surface by a factor of N/M, ideally.

@_date: 2018-03-25 19:35:16
@_author: John Denker 
@_subject: [Cryptography] Justice Dept. Revives Push to Mandate a Way to 
0) Thanks for mentioning this.  It's something we need
 to stay on top of ... even though it has been discussed
 before.
1) The short answer is that there is no good technological
 fix.  This is a political problem, not a technical problem,
 and must be recognized as such.  We must defend against it
 using political means, not technical means.
 They assert, based on no evidence, that there will be
 a win/win outcome, whereby they can snoop on the bad
 guys and everybody else will be more secure.  In all
 likelihood, any strong action they take will lead to
 a lose/lose outcome.
2) One thing to consider is superencryption.  If the
 data has one layer of "regulated" encryption, no
 attacker has any idea whether it is superencrypted
 unless and until they strip the first layer.  They
 can pass a law against superencryption, but they
 can't enforce it without revealing that they've
 been stripping the first layer.
 If the second layer involves stego as well as
 encryption, it would be particularly hard to
 prevent.
 This leads to the maxim:  When strong crypto is
 outlawed, only outlaws will have strong crypto.
3) At the opposite extreme, we should consider the
 possibility that somebody will install (additional)
 bugs that bypass all security and exfiltrate your
 keys, upstream of any and all encryption.  This
 makes all ordinary citizens very much less secure.
4) Any strong immune response runs the risk of
 provoking autoimmune disease.  By that I mean it
 becomes possible to provoke all sorts of DoS
 attacks, using the government as a weapon against
 innocent persons.  In particular, you can send
 "illegally" encrypted messages to some innocent
 schmuck, and he will be unable to prove that he
 doesn't know how to decrypt them.  Put the words
 "ISIS kiddie porn" in the headers.
5) These are actually *not* the scariest scenarios
 I know of.  I mention these because they should be
 scary enough to persuade any halfway-reasonable
 person that security needs to be strengthened not
 in any way weakened or bypassed.
 Nastier scenarios can be discussed on a need-to-know
 basis.

@_date: 2018-05-07 12:39:50
@_author: John Denker 
@_subject: [Cryptography] secure authentication ... as opposed to passwords 
Executive summary:  Zero knowledge proofs!
Don't frame it as a password problem;  frame it as
an authentication problem, then solve it properly.
Nowadays there is no excuse for requiring passwords
to be sent over the wire to be checked at the server,
much less stored on the server in any form.
Consider the progression:
1) A single password.  Makes "some" sense if there
 is only one server you interact with.  Bad idea
 if shared across multiple servers.
2) Multiple passwords, committed to memory.  Bad
 idea from a usability point of view.  Passwords
 are always a tradeoff between too easily broken
 by the bad guy versus to easily forgotten by the
 good guy.
3) Password manager.  Of some /limited/ value as
 a stepping stone, in the sense that it is easy
 for users to understand, and gets them accustomed
 to using procedures that can evolve into something
 sensible.  See next item.  Uses a master password
 to unlock a "wallet" or "keyring".
4) Password generator aka password mangler. Generates
 a password for each site, guaranteed to be unique,
 guaranteed to be very long, based on a master
 password plus site ID plus other stuff.  Should
 use browser automation to fill in the "password"
 field in html forms.  Failing that, may use cut
 and paste for special applications.  Just as
 easy to use and in all ways better than (3).
 See also next item.
5) Zero-knowledge authentication.  Don't frame
 it as a password problem!  Frame it as an
 authentication problem, then do it properly.
 Just as easy to use and in all ways better
 than (3) or (4).
 If the server never sees the password, even
 temporarily, then it can't compromise the
 password.
 Code to do this sort of thing already exists.

@_date: 2018-11-09 18:18:19
@_author: John Denker 
@_subject: [Cryptography] Seeking recommendations for a dedicated 
For many years I've been using:
  On the two or three occasions over the years when I
had questions or suggestions, I got prompt competent
answers and cooperation.
Many years ago I selected these guys because at the time
they were almost the only provider with decent IPv6.  You
can take that as a sign of of being forward-leaning, as
opposed to just letting a sleeping cash-cow lie.

@_date: 2018-11-13 23:06:40
@_author: John Denker 
@_subject: [Cryptography] Brute force circa 1939 
This is clearly just Hollywood being lazy.
The Enigma machine entered the commercial market in 1923.
Non-military versions (without a plugboard) have a keyspace of
712,882,560 if I've done the math correctly.  Anybody who knew
anything about the unclassified state of the art would have
known this in 1939.  Censors don't care about stuff that has
been widely known for years.
I am reminded of the Batman movie where people watch a
thermonuclear bomb go off a few miles away.  Even though
it is a few miles away, the sound and the light arrive
at the same time.  And even though it is only a few
miles away, the city is not incinerated.  The filmmakers
weren't trying to protect top-secret weapon info.
There are plennnty of additional examples:

@_date: 2018-11-22 11:01:25
@_author: John Denker 
@_subject: [Cryptography] Buffer Overflows & Spectre 
That example supports Henry's point that we have been sold
down the river by HW manufacturers.  The VAX was designed
in the mid-1970s and included a bit in the status register
to turn on trapping on integer overflow.
To this day, x86 processors do not have such a bit.  There
is an INTO instruction that allows you to check for overflow,
but you have to use the instruction following every operation
that might overflow, which means there is a measurable cost
to using it, which means there is temptation to not use it.
Yet manufacturers have the audacity to say that even though
it is economical to put more and more transistors on a chip,
they can't figure out what to do with them, other than to
run multiple cores in parallel.  Give me a break.
That is a very dangerous philosophy -- especially when it
comes to security.  It supports Henry's point that language
designers have shown a lack of wisdom in deciding who pays
for what, when.
People *always* pay, either for security or for the lack
thereof.  For example, in 2016 the Democratic National
Committee chose not to pay for two-factor authentication
for their remotely-accessible servers.  They wound up paying
a bitter price for the lack thereof.
That's literally true but highly misleading.  We all know
that it is possible to write bad code in any language.  It
is also "possible" in principle to write good code in any
language, if you are skillful enough and work hard enough.
HOWEVER, some languages make it a lot easier than others.
No, not everywhere.  As Knuth put it, premature optimization
is the root of all evil.  The smart approach is to take the
performance hit in the 99% of the code where it absolutely
does not matter.  Then hire an expert coder and a language
lawyer to deal with the 1% where low-level pounding on the
bare metal might actually matter.
No, not invariably.  Some language designers do not assume
that everyone who doesn't share their background must be
stupid.  Some of them listen respectfully.  Some of them
proactively solicit suggestions.  Some of them systematically
study the errors that show up in production code, and then
brainstorm ways to improve the language.

@_date: 2018-10-15 11:00:40
@_author: John Denker 
@_subject: [Cryptography] Random permutation model for encryption as a 
Just as a point of terminology, I suggest not talking about
"permutations of b-bit blocks".  I know what you mean, but
some students will hear that as a permutation of the bits
within the block.  It might work better to speak explicitly
of permutation of blocks within the space of all possible
b-bit blocks.
At a deeper level, I think you're partially barking up the
wrong tree.  A permutation with a fixed point is harmless,
so long as the adversary doesn't know what the fixed point
is.  Forsooth, the fact that Enigma could never have a
fixed point was a terrible weakness.
Also the fact that the key-space is tiny compared to the
permutation-space is irrelevant, so long as the key-space
is large enough.
Any crypto *system* worthy of the name doesn't just substitute
blocks according to a fixed permutation, but uses a different
permutation for each block in the message.  This is where IVs
and chaining modes come in.  If you didn't do that, and used
ECB mode instead, AES *could* be partially broken by frequency
analysis.  So your students aren't entirely wrong.  For an
unforgettable reminder of this fact, see the penguin diagram
in the wikipedia article:
  I've often said you really want a new key for each block.
ChaCha can be very cheaply rekeyed from scratch, which is a
virtue.  Typical chaining modes are a quick-and-dirty way
of getting approximately what you want, without the cost of
fully rekeying the underlying block cipher component, but
I've never been completely happy with this.
Modern block ciphers are bijective mappings, which is not
surprising if you want the ciphertext to be uniquely
decipherable.  Whether you want to sell the bijective mapping
as a "permutation" is a judgment call, based on the level of
mathematical sophistication of your students.  You know your
students, and I don't, so I leave that call up to you.
FWIW there is no law that says a block cipher has to be a
permutation.  If code-block-space is larger than plain-block-
space, you can have one-to-many encryption and many-to-one
decryption.  To one way of looking at it, this is what an
IV or session key does for you.  Back in the olden days,
random nulls were used to accomplish more-or-less the same

@_date: 2018-09-09 00:57:07
@_author: John Denker 
@_subject: [Cryptography] WireGuard 
Agreed.  That's all true and important.
Taking another step down the same road, I would add that
 1) IPsec /requires/ a Security Policy Database (SPDB).
 2) RFCs don't say much about how to implement it.
  (Typically it gets implemented as some sort of firewall
  policy.)
This results in yet more matchstickery.
The SPDB very greatly affects security.  That's because
security requires both:
 a) making sure certain good things are possible, and
 b) making sure certain bad things are not possible.
The SPDB is necessary for (b).  Interoperation testing often
deals mainly with (a).
Unfortunately, I've seen implementations that gave the SPDB
very short shrift.  As soon as they achieved interoperation
they declared victory and forgot about the SPDB.
Tangentially related:  About three weeks ago The Intercept
dumped a batch of Snowden documents that talk about breaching
VPNs (including TLS and IPsec).
  There is not much there in the way of details on what was broken,
how it was broken, or how to make things secure ... but it's
a reminder that a very high degree of professionalism and
fastidiousness is required;  otherwise we're delivering snake
oil or worse.

@_date: 2019-04-07 06:09:36
@_author: John Denker 
@_subject: [Cryptography] Name for three key ECDH 
If I may be permitted to ignore the complex questions in the
body of the email and just answer the simple question posed
by the subject line, the obvious answer is:

@_date: 2019-04-14 23:54:36
@_author: John Denker 
@_subject: [Cryptography] Making models + scenarios realistic 
On 4/13/19 8:02 AM, Phillip Hallam-Baker wrote in part:
But is she /exactly/ 4'11" ????
Yet the scenario is ludicrous.  The rococo details tell us that
this scenario covers a set of measure zero in a very large space.
Scenarios are infinitely important, but other considerations
are also infinitely important.  In particular, you have to have
some sort of /model/.  It's like high-school chemistry, where
you collected some data points and then fitted a straight line
to them:
 -- The line is the model, with some adjustable parameters.  It
  allows you to interpolate between the data points, and to
  extrapolate.
 -- Scenarios play the role of data points.  They allow you to
  pin down the adjustable parameters in the model.
A good model incorporates your /understanding/ of the situation.
This understanding allows you to limit the number of parameters
in the model, which in turn reduces the number of scenarios that
are needed to constrain and test the model.
Yes.  Data points by themselves are infinitely brittle.  They
have measure zero in a very large space.
Three of the smartest guys I know just got the Turing prize for
work that revolves around this principle:
 *Without the model the scenarios are useless ... and vice versa.*
      Not only have they not asked;  they wouldn't have had a
language to use for asking the question even if they wanted
to, even if they realized it was an important question.
They would have had no way to specify which details of the
situation are important to them and which are not.
Again:  The scenarios are absolutely necessary but they are
not the only hard part, or even the hardest part.  One must
also have a sharable understanding of what the security
model is /supposed/ to do.  Scenarios can be used to test
understanding, but they do not create understanding.
A model is tantamount to a formal language for specifying
what you want.  As always, language design is only secondarily
a language issue;  understanding has to come first.  Otherwise
the language becomes impossibly complex and inscrutable, to
the point where it is useless even to experts, not to mention
laypersons such as Alice.
Offering an encyclopedia of scenarios and asking Alice to
choose one is not feasible.
I once had a student pilot who didn't want to build explicit
mental models of the situation.  She just wanted to practice
until she had seen all the scenarios.  I explained that during
landing there are 12 different things you need to worry about.
If we oversimplify it to the point where each variable can
have only three different values (high, nominal, and low) that
still leaves us with half a million scenarios, and it would be
spectacularly infeasible to learn them all by rote.  Instead
you must use /understanding/ in order to factor one infeasible
problem into 12 feasible problems, and then master those.
Obviously we have "some" understanding of what security policies
are supposed to do, but I'm not convinced it is enough, except
perhaps in certain tightly circumscribed micro-domains.  For
example, consider the PGP "web of trust".  What does that even
mean?  For one thing, trust is not transitive, and secondly,
trust is hard to quantify.  It's not even one-dimensional.  I
have some neighbors whom I would trust to borrow the proverbial
cup of sugar but wouldn't trust to borrow my car or my credit
Also:  The /language/ issue runs both ways:
 -- We need a way for Alice to tell her IT guy what she wants.
 -- We also need a way for Hillary's IT guy to tell her about
  the threats, in a way that she understands, e.g. that maybe
  the inconvenience of using two-factor authentication is
  small compared to the inconvenience of having your campaign
  pillaged and burned by Fancy Bear.
Bottom line:  A floridly detailed scenario is, ironically, a
way of illustrating the limitations of scenarios.  We also
need models aka languages and (!) understanding.

@_date: 2019-08-13 19:51:21
@_author: John Denker 
@_subject: [Cryptography] generated passphrases 
Interesting question.
You might consider this:
  "Data-driven sentence generation with non-isomorphic trees" (2015)
  with code available here:
  If that's not exactly what you want, perhaps it will serve as a source of
buzzwords you can search on.  Or you could contact the authors.
If that's not in the right ballpark, please clarify the question.

@_date: 2019-12-29 01:44:16
@_author: John Denker 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
What is "best practice" for detecting breakage
  of a supposedly-secure communication system?
One of my favorite proverbs says that the first step toward preventing
mistakes is to recognize that mistakes are possible.
If somebody breaks in once and trashes your system, that's bad ...
but it is very much worse if they are subtle about it, breaking in
again and again, living rent-free in your system.
  Tangential remark: About 5 years ago there was a very interesting
  discussion under the heading of "Can Enigma/Tunney be Fixed?"
  See e.g. the excellent post from Ray Dillinger on 1/13/15 2:28 PM.
  It seems to me that one of the reasons enigma never got fixed is
  that the Germans never realized it was broken!  The Brits read a
  ton of enigma traffic and the secret didn't leak out until YEARS
  later.
Returning to present day:  Suppose my phone were a wholly-pwned subsidiary
of (say) Unit 61398.  How would I know?  Would the NSA know?  Would they
tell me?
There is a vast literature bad-mouthing intrusion-detection systems in
general and honeypots in particular.  I reckon that's partly true, insofar
as such things are not particularly useful to Joe User.  However, large-
scale outfits such as MS and Google and various TLAs use them aggressively.
They just don't talk about it much.  Unlike Joe User, they are in a position
to maybe even *do* something about whatever intrusions get detected.
Returning to the WWII example:  Everybody likes to talk about how dumb
the Germans were ... but the story is not entirely one-sided.  The US
diplomatic "Black" code was stolen, and for most of 1942 the Nazis could
read US dispatches from Bonner Fellers and others, with very very serious
consequences.  This is maybe not as "elegant" as a cryptanalytic break,
but the consequences are the same ... and the consequences are what we
ought to be measuring.
On the other side, various Germans "suspected" something was amiss, but
they didn't try very hard to test their hypotheses.  My point is, there
were plenty of things they could have done.
You have to do a bit of differential diagnosis to differentiate between
 -- plain old treasonous espionage i.e. HUMINT, or
 -- low-tech COMINT such as prosaic high-frequency direction-finding, or
 -- high-tech COMINT such as codebreaking.
For example, consider this:  Arrange with the submarine captains a system
for sending fake messages, perhaps by using special "cancel" words:
  -- "immediately" is genuine
  -- "asap"        is genuine
  -- "forthwith"   means this entire message is fake.
  -- "discontinue" is genuine
  -- "break off"   means this entire message is fake.
  -- et cetera.
(Analogous sets of synonyms exist in other languages including German.)
Then send orders to several submarines directing them to "break off" and
proceed "forthwith" to rendezvous with a milchcow at such-and-such location.
This offers a highly tempting target to the opposition.  Obviously you do
not want your subs to go to the indicated location; instead send a long-range
patrol plane to see if ASW forces show up.  If they do, it is a strong
indication that your cipher is broken.
Similarly, send patrol planes to locate several convoys.  Track their
progress for a while.  Then send a fake message arranging for a wolfpack
to assemble at a particular location along one of the tracks.  If that
convoy suddenly diverts and the others do not, it is a strong indication
that your cipher is broken.
It is not easy to come up with lures like this, but not impossible.  I
reckon the folks on this list can come up with schemes far cleverer than
the examples I have given.
There are present-day examples I can think of that certain eavesdroppers
would find irresistible.  I hesitate to mention them, because I don't want
to invite trouble.  Maybe somebody with more resources would be interested.
Who knows, maybe Chris Hansen at NBC.
White-hat probing of your own systems is OK as a starting point, but not
nearly sufficient IMHO.  Cryptanalysis is fine as far as it goes, but it
will never detect theft of a codebook.
Bottom line:  What's best practice?  It seems kinda unprofessional to put
a system out there and not check whether it's working.

@_date: 2019-12-31 17:40:54
@_author: John Denker 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
There are a lot of things the Germans could have done if
they had realized how thoroughly broken Enigma was.
The Enigma machine was /almost/ unbreakable.  It took
tremendous cleverness and enormous resources to break it.
A layer of modest superencryption would have pushed it
far, far out of reach of the codebreaking technology of
the day.
The Enigma has some strengths and some weaknesses.
The main weaknesses are:
    a) The session key is waaaay too short.     b) The state of the machine doesn?t change enough from one letter to the next.     c) No letter can encode to itself.     d) The blocksize is too small (letter by letter).     e) It is vulnerable to operator errors, including weak keys. You can fix all of these except (d) at very low cost.  In
particular, it wouldn't take much to increase the key-space
from 26^3 to 26^5 or even larger.
With help from Bear I worked out in some detail one particular
way of doing this:
  Rather than giving them mere guidance, my recommendation would
be to give them /dice/, sealed in a dice popper permanently
attached to the machine so they don't go astray.  Using the
dice is mandatory.
Assign a roomful of clerks at headquarters the task of checking
for non-random session keys.  Violators will be assigned to the
eastern front, to clear minefields by hand.

@_date: 2019-02-21 11:27:38
@_author: John Denker 
@_subject: [Cryptography] How widely are the PSK modes used? 
You mean besides more-or-less every WPA wireless setup?

@_date: 2019-07-04 02:43:05
@_author: John Denker 
@_subject: [Cryptography] graph theory and sybil attack 
On 7/3/19 6:02 PM, James A. Donald wrote in part:
Here's a closely related problem:
This arises in the context of communication networks.  The weak links
make the network vulnerable to partition, where some subgroup is
unable to communicate with the rest of the rest of the group, and
no amount of re-routing will fix the problem.  A milder version of
the problem manifests itself by congestion on the weak links.
This is in some ways the opposite problem, because there are no
"bad" nodes and you are trying to prevent a partition rather than
create a partition.  However, the mathematics of identifying the
relevant nodes and links is the same.
This has been verrrry heavily researched:

@_date: 2019-06-03 11:48:19
@_author: John Denker 
@_subject: [Cryptography] Crptographic ticket tape (fake news) 
That's not good enough.
Nothing in that general category will ever be good enough,
because the information required to verify the assertion
is exactly the same as the information required to construct
a forgery.  And the work-factor is nearly the same.
To say the same thing another way:  If today I send you
a signed copy of tomorrow's ticker tape, that is (for the
moment!) evidence that I have a time-travel machine.
HOWEVER the validity of the evidence expires tomorrow!
I can't wait until tomorrow to send the message.
Again: In such schemes, the prover and the verifier do not
enjoy any information-based advantage over the forger.  They
might enjoy some slight work-factor advantage, namely the
work required to construct a good-quality forgery ... but
given the state-of-the-art in image editing software, this
is insignificant.  Maybe 80 years ago a movie of somebody
holding a newspaper might have been nontrivial evidence,
but not anymore.

@_date: 2019-05-10 16:23:12
@_author: John Denker 
@_subject: [Cryptography] peering through NAT 
Somewhat off-topic, but less so than the rest of this thread....
As ||ugh Daniel was fond of pointing out, NAT is an abomination.
It is a sad commentary on the whole industry that we are even
discussing it.
There is a STUN RFC from 2003, but there is an IPv6 RFC from
1995.  IPv6 was already widely supported in the early 00s
e.g. by windows 2000.
It should have been obvious for many years now that every box
that does NAT should do native IPv6 if available, and should
terminate 6to4 (or 6rd) tunnels otherwise.  This his how
every network I've set up in the last 15 years has done it.
a) IPv6 makes a great many NAT-related questions moot.
 Every client on the subnet behind the NAT box can have
 its own globally-routable address.
b) It also simplifies IPsec deployment.
c) Note that (a) and (b) are not unrelated.
When will NAT-box manufacturers get with the program?
You'd think this would be a selling point.
What in the name of Godot are they waiting for?

@_date: 2019-11-01 12:23:45
@_author: John Denker 
@_subject: [Cryptography] =?utf-8?q?Anonymous_online=3F_A_third_of_popular_?= 
Question:  Has anybody vetted Ublock Origin?  Any experience,
opinions, or recommendations?

@_date: 2019-11-25 16:03:21
@_author: John Denker 
@_subject: [Cryptography] Mention of Joe Biden in "Puzzle Palace" 
According to the Clerk of the Senate, only one Biden has
ever served as a senator.  He was first elected in Nov. 1972
and sworn-in in Jan. 1973.
Hard reference:

@_date: 2019-09-16 10:54:30
@_author: John Denker 
@_subject: [Cryptography] Russia carried out a 'stunning' breach of FBI 
Note that code-making (not just code-breaking) is part of the
NSA mission statement ... but apparently is sorely neglected
in practice.
They seem to systematically forget Rowlett's dictum:
  "In the long run it was more important to secure one's own
   communications than to exploit those of the enemy."

@_date: 2020-04-02 21:53:06
@_author: John Denker 
@_subject: [Cryptography] "Zoom's end-to-end encryption isn't 
How does jitsi fit into this story?
  It claims to use ZRTP.
Phil Zimmerman says ZRTP provides end-to-end security.
It's all open source.  Has anybody vetted it?

@_date: 2020-04-08 14:34:02
@_author: John Denker 
@_subject: [Cryptography] "Zoom's end-to-end encryption isn't 
That's probably true as stated.
So far so good.  However.....
Security is never about the "majority" case.  The adversary gets
to choose when & where to attack.
Suppose I am teaching modern world history.  In the class there
are foreign students of many flavors and colors.  It would be a
Bad Idea? to have repressive foreign governments eavesdropping
on all the discussions.
Also, without being too specific, I know of political party
officials who have *already* had campaign strategy meetings
over Zoom.
One longs for the good old days when attackers had to actually
do some work to capture John Podesta's emails.

@_date: 2020-08-20 15:47:58
@_author: John Denker 
@_subject: [Cryptography] any reviews of flowcrypt PGP for gmail? 
Has anybody vetted flowcrypt?  It purports to provide PGP for gmail.
  It claims to provide "end to end" crypto, but as we've seen lately,
that doesn't always mean what we might want it to mean.
It also claims to be easy to set up and use.
I know nothing about it; I only learned of it recently.

@_date: 2020-01-08 10:10:35
@_author: John Denker 
@_subject: [Cryptography] retro crypto 
Using 1970s technology, you can build a cipher machine on
rotor-like principles.  It has the virtue of "not running
any kind of malware because it can't".
For example:  Use LFSRs (linear feedback shift registers)
to drive the address lines on a bunch of EPROMs.  XOR the
EEPROM outputs.
Discussion:  If you use a LFSR directly, as a stream cipher,
it is straightforward to ascertain the current state, whereupon
you can predict the future state for all time.  But if you
muddy it up using an EPROM (as a form of S-box) then that's
not so easy.
By EPROM I mean something like a 27c1001:
  It is organized as 128k of 8-bit bytes.  You can easily
program all the bits yourself, so the manufacturer does
not know.  It has 17 bits of address.  The cost is a
few bucks per chip.
You can easily build LFSRs out of SSI (small-scale ICs)
to drive the address lines.  In particular, you can use
counters with the following periods:
1: 2**19 - 1		    (could drive 4 EPROMS)
2: 2**17 - 1
3: 2**13 - 1,  2**4 - 1	    (two LFSRs drive one EPROM)
4: 2**11 - 1,  2**6 - 1	    (ditto)
5: 2**10 - 1,  2**7 - 1	    (ditto)
6: 2**17		    (binary counter, not LFSR)
Conceptually this is just a rotor machine with 6 rotors,
but the rotors are electronic.  Each rotor has a period
on the order of 2**17, which is rather long ... and
every rotor moves a lot at every step.
This is sorta like a SIGABA in the sense that a PRNG is
used to control the stepping of the rotors.
The 6-rotor version has a combined period of 1 ? 10??.
It's slightly smaller than the simple product, because
the factors are almost but not quite all relatively
Initialize the counters according to the session key.
That gives 96 bits of key.
EPROMs have the advantage that when they go out of
date (or if you think you're about to be captured)
you can erase them.  This provides forward secrecy.
Such a device could operate at a rate of many megabytes
per second.
The device is open to related-key attacks, but the
key space is big enough to make that difficult, and
there are tricks for making it even more difficult.
There are a few weak keys, involving long strings
of consecutive zero bits, but they are trivial to
detect and defend against.
This is not mathematically secure by modern standards,
but it is probably at the point of diminishing returns,
in the sense that it is more secure than many other
elements in the system (such as whatever device you
used to type up the plaintext).

@_date: 2020-01-11 23:13:28
@_author: John Denker 
@_subject: [Cryptography] improved identification of non-targets 
Hi Folks --
There have been outrageously many incidents of people shooting
down airliners without really meaning to.  This looks partly
analogous if not identical to a classical crypto problem,
namely identification and authentication.  Heretofore it has
been handled very badly.
1) The term IFF (identification friend of foe) is a gross
 *) A favorable result from the IFF system identifies a
  friendly military aircraft.
 *) The only other result is a non-result which could be:
  -- an out-and-out foe,
  -- a neutral,
  -- a friendly non-military aircraft,
  -- or even a friendly warplane with a broken or
   misconfigured transponder.
2) There exists such a thing as "non cooperative target
 identification" but that is very much the answer to the
 wrong question.  Airliners are not targets, and more
 importantly, they would cooperate if given half a chance.
 So the question is, why are they not given the chance?
3) To ask the same question in a slightly different way:
 Can we provide airliners with IFF functionality?  What
 would that involve?
 The equipment would have to be highly trusted.  If there
 were any appreciable risk that identifications could be
 stolen or forged, missile crews would ignore the IFF and
 shoot at anything that moves.
4) You can't just install military transponders in airliners,
 partly because the equipment is classified, and partly because
 the task is different.  Military IFF responds only if you
 ask nicely, using a coded query, but an airliner should
 respond to anybody who asks.  Instead, the airliner needs
 some kind of nonce (to prevent trivial replay attacks).
 So, if we can come up with some sort of design that makes
 sense, perhaps ICAO could standardize it.  Once it is
 deployed, there would be a lot of pressure for militaries
 to respect it.
5) The existing ADS-B Mode-S transponder is a step in the
 right direction.  No crypto is involved.  The reply carries
 a 24-bit claim of identity.  This enormously simplifies the
 missile battery's task, because rather than trying to figure
 out what this object is, ab_initio, based only on its primary
 radar signature, they need only verify that it is exactly
 what it claims to be.
6) Layering some crypto on that shouldn't be toooo hard.
 The aircraft can perform some sort of public-key signature
 or zero-knowledge proof of identity.  Append the nonce to
 your claimed ID, sign it, and send it back.
 This raises the usual questions about what certificates
 to trust.  My standard answer to all such questions is
 that I trust certificates that I have issued myself.
 Each country could issue its own certificates, good for
 one flight only, and send them to the airline via a
 secure channel.  Friendly and neutral airlines would
 have every incentive to not let the certificates leak
 to foes.
 Conversely, airliners belonging to my out-and-out foes
 are not allowed to operate in my airspace.  Too many
 opportunities for hanky-panky.
7) There have been rumors of military aircraft flying
 in close formation with unwitting airliners, using the
 airliner partly as a stalking horse and partly as
 human shields.  I'm not sure what to do about this.
 It seems unsporting, but that doesn't mean it can't
 happen.
 As mentioned in item (5), knowing exactly what the object
 is supposed to be makes it easier to detect a primary
 radar return that isn't quite right.  Buzzword = MASINT.
8) There are implementation issues.  In an integrated air
 defense network implementation shouldn't be too bad, but
 for things like Buk or Tor launchers, which were designed
 in the Soviet era, designed to operate more-or-less
 autonomously, I don't know what all would be involved.
9) We have to ask, what is the threat model? Obvious
 starting points include:
 -- From the airliner's point of view, the main threat is
  trigger-happy missile crews.  Also bad guys trying to
  steal your authentication certificates.
 -- From the air defense point of view, the threat includes
  foes masquerading as neutrals.  Also stalking horses.
 -- What else?  I don't know.
Bottom line:  There's obviously a problem here.  How do
we understand the problem?  Is it fixable?

@_date: 2020-01-11 23:32:56
@_author: John Denker 
@_subject: [Cryptography] retro crypto 
It's not a one-time pad.  It's meant to be a few-time pad.
A true OTP requires that no byte of the pad ever gets
reused.  In contrast, in a rotor-like machine, the
various bytes do get reused, but in a key-dependent
pseudo-random order that the adversary cannot easily
figure out.
This means the combined length of all messages sent
by all users of this system can greatly exceed the
amount of randomness stored in the EPROMs, while
still maintaining some decent security.
Not quite so trivial.  The S-boxes in DES are not
secret, yet triple-DES is not trivial to break,
because there is still some security that comes
from the key.  You need the key-space to be big
enough to foil brute-force searching.
I still strongly recommend that the EPROMs be kept
secret.  However, the key space is large enough to
make life unpleasant for the attacker, even if one
or two of the EPROMs were compromised.
Add a few more rounds it you think it would help.

@_date: 2020-01-12 23:35:32
@_author: John Denker 
@_subject: [Cryptography] improved identification of non-targets 
Agreed.  The captain of the Vincennes got a medal for his
Agreed!  The idea of a global root CA is just ridiculous.  As
I said previously:  I trust certificates that I issue myself.
A long international flight will need to acquire multiple
certificates, one for each jurisdiction along the way.
I disagree; see below.
Much depends on *TRUST*.
It does not matter whether *you* think a plain present-day
ATC transponder counts as "IFF";  it only matters whether
the guy with the missile launcher thinks it does.  Which
he doesn't.
Without upgrading the airline equipment, there is no feasible
upgrade to the missile systems (including humans) that will
make a dent in the problem.
Missile crews do not trust present-day ATC transponders, and
there is no reason why they should, because squawk codes are
trivially easy to falsify.  Just find an airliner that is 27
minutes behind schedule and clone its assigned 12-bit code.
That is 99.9% irrelevant.  Iran just spent two years fighting
ISIS, who committed war crimes 24?7.  Torture is a war crime,
but that didn't stop the US from adopting it as policy, or even
deter Darth Cheney from bragging about it on TV.  The colonials
in 1776 often didn't play by the established rules.  The attack
on Pearl Harbor was a war crime.  Any act of war outside of a
declared war is a war crime ... which means that any low-intensity
conflict is a war crime from start to finish.
Not helpful.  Approximately none of the combatants in Afghanistan
wear uniforms.  Guerillas don't wear uniforms.
Again it comes down to *TRUST*.  No missile crew is going to
assume that such-and-such can't happen just because it would
be illegal.  We need a solution that provides a rich, flexible
semantics:  Whom do I trust today?
Agreed, that's definitely an issue ... but I assume that the
existing military "mode five" EFF embodies a solution (Mode 5
with a five is not to be confused with Mode S with an ess.)
Here is an obvious possibility, which may serve as an existence
proof:  Daisy chain the nonces.  That is, send a interrogation
pulse  which contains a nonce labeled  and does not require
a reply.  Then come back a couple seconds later with pulse which repeats nonce  and expects a reply, which the aircraft
has already had plenty of time to compute.  Pulse  also
carries nonce  needed for later, and so on.  There are
ways to make this much fancier using clocks and/or PRNGs, as
embodied in widely-used 2FA dongles.  I haven't worked out all
the details, but I'm pretty confident that good solutions exist.
Iran is not going to do that, because it would cause economic
harm to themselves.
As the proverb says:  Between two stools one sits in the ashes.
There are rules that make sense in peacetime, and there are
rules that make sense in a hot war, but the world has not figured
out how to handle low-intensity conflicts.
The Buk and Tor missile systems were clearly designed to work in
a hot war situation.  They are inappropriate to a low-intensity
conflict.  So Iran at present has to choose between:
 -- missiles disabled, which provides no air defense, or
 -- missiles enabled, which provides inappropriate air defense.
The Ukrainian and Canadian civilians would be better off if Iran
had a third option, namely a *TRUSTED* way for air defense to
identify friendly airliners.
I absolutely did not forget that.  I discussed it near the end
of the original post.  Enemy airliners are not allowed into my
airspace.  ATC will tell them to go away.  If they come anyway,
they will be forced down.
There is always the possibility of trojans, e.g. when an erstwhile
friend decides to turn against you, and uses a regularly-scheduled
cargo flight to send a planeload of heavily-armed commandos into
your capital ... but that's irrelevant, because they can do that
already.  We are not trying to solve all the world's problems at
This is a WYTM issue.  The assigned problem is how to avoid shooting
down airliners that you didn't want to shoot down.  We should be
able to do that without making the trojan problem worse.
Here's another ingredient I'd like to add to the mix:
There should be a mechanism, and a procedural requirement, so that
an aircraft can verify that its IFF is working *before* it becomes
a critical issue.
 -- This includes a pre-takeoff check at the departure airport,
  pertaining to the local jurisdiction.
 -- This includes an airborne check for each new jurisdiction
  along the way, so that the aircraft knows they have been
  positively and favorably identified *before* entering the
  new airspace.
This should be easy to do, since ATC radar already has lots of
military radar functionality.  (The converse is not true; a big
part of the problem is that the Vincennes, the Buk launcher,
and the Tor launcher were not integrated with ATC.)

@_date: 2020-01-13 12:35:30
@_author: John Denker 
@_subject: [Cryptography] PK without PKI (was: improved identification) 
No, we won't.  That is not what I am proposing.
More generally:  PK is not synonymous with PKI.
In particular, you can have public and private keys without
having anything resembling a globally-trusted root CA.
For the *third time* I emphasize the simple principle:
  I trust keys that I have issued myself.

@_date: 2020-01-15 07:32:47
@_author: John Denker 
@_subject: [Cryptography] improved identification of non-targets 
Hi Folks --
Executive summary: It feels like this "should" be a
solvable problem, but AFAICT there is still much work
that needs doing.
As a general principle, in cryptology and more generally,
there is a huge difference between adversarial and non-
adversarial situations.  Consider the following contrasts:
 1a) A very simple RNG may be good enough for an atomic
  physics Monte Carlo integration.  The atoms are not
  adversarial.  To paraphrase Einstein, the atoms are
  subtle, but they're not out to get you.
 1b) A vastly stronger RNG is needed for adversarial
  situations, e.g. high-stakes gaming, serious crypto,
  et cetera.
 2a) Consider the classic crypto situation of an embassy
  or a military unit sending a message back to HQ.  The
  message is subject to attack enroute, but the classic
  assumption is that the endpoints are on the same team
  and want to keep the message secure.
 2b) Compare that to copyright protection.  This is much
  muuuuch more challenging, because one of the endpoints
  has little if any motivation to cooperate.
I mention this because:
 3a) Ordinary ATC is a non-adversarial situation.
  Airliners have no incentive to steal each others'
  squawk codes.
 3b) Military IFF is something else entirely.  It is a
  seriously adversarial situation.  By definition.  As
  it says on the tin:  there are friends and foes.
THEREFORE please do not make shallow na?ve analogies
between ATC and IFF.  The amount of crypto required
is not the same.  The level of secure authentication
required is not the same.  Not even close.
As a mostly-separate matter:  Do not assume that missile
launchers are part of an integrated air defense network.
They often aren't.  For example:
 -- IR655:  The Vincennes was not listening to ATC,
  and had no means of doing so, and might not have
  trusted what Iranian ATC was saying even if they
  had heard it.
 -- MH17:  The Buk launcher was not listening to ATC,
  and had no means of doing so, and might not have
  trusted what Ukrainian ATC was saying.
 -- PS752:  The Tor launcher was not listening to ATC,
  and had no means of doing so.
This is not surprising.  Keep in mind that Iraq under
Saddam Hussein built an elaborate integrated air defense
network, but the communication links were destroyed in
the first few seconds of the war.  They were verrrry
high on the list of targets for the first US bombs and
missiles.  For this reason among others, we should not
be surprised to learn that missile launchers are designed
to operate autonomously.
We must also keep in mind the various use-cases.  A
system that is suitable for one purpose may be wildly
unsuitable for another purpose.
 -- Peacetime ATC is a solved problem, if you think
  there is any such thing as "peacetime" any more.
 -- IFF in the context of low-intensity conflict is
  not a solved problem.  Saying "ATC does such-and-such
  all the time" is *not* a solution to the shoot-down
  problem.  If you don't believe me, there are some
  Canadians and some Ukrainians who can explain it
  to you.
In more detail:
Civil ATC		works fine	exists		no
airspace		open		open		closed
nonlethal remedies	yes		sometimes	rarely
for infractions				not
*trusted* IFF on	no		no		mostly
all friendlies
Integrated air		exists		has trust	probably
defense network				issues		destroyed
autonomous  		should be	active		active
missile launchers	forbidden
The middle column is highly problematic.  There are various steps
that can be taken, alone or in combination, that might alleviate
the problem.
*) One could add *trusted* IFF to airliners, which is where this
 thread started.
*) One could almost imagine closing the airspace in which autonomous
 missile launchers are active, but this doesn't entirely solve the
 problem.  The Vincennes did not have legal authority to close the
 nearby airspace (and wouldn't have had a mechanism for doing so
 even if they wanted).  Also this would be open to abuse, as a
 slimy way of imposing a de-facto air blockade.
*) Conversely, one could imagine requiring all missile launchers
 to be at least lurking on ATC, so they would at least know the
 nominal squawk code and radio contact frequencies for airliners.
 Require this *even if* they do not 1000% trust the information.
 The point is to enable non-lethal measures.  Hypothetically, the
 missile crew could contact the airliner on an *appropriate*
 frequency, e.g. "Iran Air 655 this is US Navy Warship Vincennes.
 Turn left immediately or you will be fired upon.  Turn left
 immediately heading 180."
 This would be a major departure from previous practice.  In the
 real world, non-hypothetically, the Vincennes was not physically
 capable of communicating with IR655.
 Implementation is not easy.  It is nontrivial for missile crews
 on the surface to lurk on both sides of the ATC conversation.
 Hint: line-of-sight propagation physics.  There are also major
 trust issues.  Integrating ATC with Soviet-era Buk and Tor
 launchers would be a very heavy lift.  Integrating it with
 Stinger-class MANPADs would be next to impossible.
*) ADS-B could have a role to play here.  ADS-B-out discloses the
 aircraft's unique ICAO ID.  ADS-B-in could give the missile crew
 a way to command the aircraft to turn away.
 Retrofitting this onto Soviet-era Buk and Tor launchers would be
 complicated but maybe not impossible.  Stingers are more iffy.
 Some details on ADS-B extended squitter message format can be
 found here:
    *) There is *still* a need for crypto.  There is a need for good
 authentication.  Otherwise there is the risk of protocol failure,
 where some bad guy uses the communication channel to trick an
 airliner into doing something it shouldn't.
*) More about non-lethal remedies:  At the procedural level, it
 would be nice if missile launchers had a buffer zone, so that an
 airliner could be enough to elicit a warning without being so
 close that they get shot down.
 For example, Washington DC is essentially a low-intensity war
 zone, and has been ever since 9/11.  NORAD has implemented
 a buffer zone with non-lethal warnings, namely flashing
 red-red-green laser signals:
    The signals don't reach aircraft in or above clouds, but
 they're better than nothing.

@_date: 2020-01-20 16:57:37
@_author: John Denker 
@_subject: [Cryptography] Proper Entropy Source 
Why do you think so?
Is there any reason to imagine such a scheme would be portable?
What happens if you guess wrong?
Why not use some physical process that has some *provable*
lower bound on the entropy density??????
There is dire, fundamental problem here.  To paraphrase
 -- testing can show the absence of randomness;
  but it can never show the presence of randomness.
If your tests do not find any patterns, it doesn't
prove there are no patterns;  it just means you haven't
found any.  Yet.
You would need more than 64 bits to have any hope of
detecting any nontrivial nonrandomness ... and (!)
you would still have the Dykstra problem.
Even on top of that fundamental problem with the stuff
you can and can't measure, there is a problem with
*other* stuff you can't measure:  You could buy new
hardware tomorrow with wildly different performance.
I have been singing this song for a very long time.
Almost the first paying job I ever had, the guy who
was paying me directed me to use such-and-such as a
source of randomness.  I told him it just because
*he* couldn't predict it didn't make it reliably
random.  He told me to stop arguing.  I measured it
and proved it was in fact a constant.  You couldn't
necessarily predict the constant, but it would remain
constant for hours, so it was compleeeetely unsuitable
for the intended purpose.
Seriously:  Just because you think it might be random
doesn't make it reliably random.

@_date: 2020-01-22 04:53:53
@_author: John Denker 
@_subject: [Cryptography] Proper Entropy Source 
Alas, Mr. Carboni is falling prey to several of the same fallacies that Mr. McNamara did, namely:
a) Just because you can throw a bunch of numbers together
and come up with a quantitative answer (e.g. "64") that
does not mean it is the right calculation, much less the
right answer.
b) OTOH just because some quantitative analyses are wrong
does not prove that a hand-wavy analysis is better.  The
latter is likely to be just as wrong, or worse.
c) Just because you want it to be true, and think it might
be true, that doesn't mean it is true.
d) It is easy to underestimate the complexity of a proper RNG.
In particular, in the present case, the calculation quoted
above tells us how random the proposed scheme (involving
memory timing) "needs" to be.  The question of how random
it actually *is* remains open. It remains unaddressed.  We
have not seen proof, or even evidence.
By way of analogy, NASA engineers calculated that the
Challenger needed reliable O-rings.  Needing them is not
the same as having them.
Here's a point that may be of some help in understanding
the complexity of the issue.  This was mentioned a few
years ago but bears repeating:
It helps to think in terms of a trichotomy:
 -- some processes are reliably deterministic
 -- some processes are reliably random
 -- some processes are what I call /squish/,
  meaning they are neither reliably random
  nor reliably deterministic.
Let's be clear:  Just because it is not reliably deterministic
does *not* mean it is reliably random.
The poster child is the time-of-day at which the program
was started.  I cannot reliably predict a_priori the time
at which some user will start the program I've written ...
but that does *not* make it random.  It's squish.  An
adversary may well be able to closely estimate the time
a_posteriori, and may even be able to manipulate it
to some extent.
Seeding your PRNG with the time of day is not good enough.
Really not.
Somewhat more generally, a squishy signal may be /correlated/
with or /conditional/ upon something else.  That means it
is random or nonrandom, depending on the conditions.  An
encrypted counter is a good example; much depends on whether
the encryption key is known.
Again:  Just because you can't easily predict it does
not mean it is random enough for any serious purpose
(cryptologic or otherwise).
Also keep in mind the fundamental equation:
In other words: Combining multiple squishy sources does *not* produce randomness.  It might make it harder for
you to predict, but again, that is not the criterion.
I trust thermodynamics.  The second law can be used to
establish a reliable lower bound on the entropy density
of certain sources.  For anything(*) else, I'm from
Missouri:  show me the proof.
Cheap, provably-good sources exist.  Given the choice
between that and a not-provably-good source, it is
hard to imagine any sane reason for choosing the latter.
(*) Quantum noise is not fundamentally different from
thermal noise.  The theoretical analysis is the same
either way.  Quantum fluctuations are not in any sense
"better" or more random than thermal fluctuations.  In
practice they're usually smaller and harder to deal with.
There is nothing simple about a proper RNG.
A PRNG depends on computational complexity, and also
depends on a seed.  We still need a good RNG to produce
the seed.
I have never seen a hardware process that produces 100%
entropy density.  I suspect no such thing exists.  There
is always "some" squish.
Not all squish is equally troublesome.  Therefore any
decent RNG depends on hardware *and* on computational
complexity.  The former is used to create a signal that
has almost 100% entropy density, and the latter is used
to hide the remaining squish so it cannot feasibly be
On Wed, Jan 22, 2020 at 1:20 AM Theodore Y. Ts'o
 wrote in part:
We agree that such tests are very, very widely misused.
Specifically:  A RNG that fails a statistical test is
surely broken, but one that passes such a test is not
guaranteed to be any good.  h/t Dykstra.

@_date: 2020-01-22 17:27:36
@_author: John Denker 
@_subject: [Cryptography] Proper Entropy Source 
OK, but beware that the word "randomness" means different
things to different people.  Some people /define/ randomness
to be synonymous with unguessability in this context.
I disagree.  That's not what I mean by the word.  Squish
exists even along the guessability axis:
 -- somethings are provably deterministic, and therefore
  100% trivially guessable.
 -- some things are provably unguessable, and therefore
  random enough for ordinary crypto purposes, even in
  quite demanding situations
 -- some things are neither provably guessable nor
  provably unguessable.  I call this squish.
No!  Harder for the good guys to predict is not the criterion.
It does us no good at all.  What matters is whether the
That's entirely correct, provided (!) we are talking about
real provable unguessability, *not* squish.
I'm skeptical of that.  It depends on how high the resolution
is.  Packet arrival times are something that the adversary
might well be able to observe -- or maybe even manipulate.
Yes, that is ideal.  And that is sufficiently cheap and easy
that I cannot imagine why people keep hatching grossly nonideal
I disagree for multiple reasons:
1) If the adversary has pwned the platform, all your data are
belong to them.  They won't mess with the RNG, because there
are a dozen easier lines of attack.
2) From an engineering point of view, squish is worthless.
The goal is to build a reliable system.  Squish is by
definition not reliable.
 -- If the source was random enough i.e. unguesable enough
  without the squish, adding squish doesn't help.
 -- If the source was *not* random enough i.e. guessable
  without the squish, adding squish doesn't help.
As the proverb says: It does not matter how much squish the source produces;  what
matters is how much randomness aka unguessability it produces.
If it produces randomness at a moderately low rate, that's
usually OK, because you can accumulate it over time.
3) Using multiple unreliable sources is almost never an
acceptable substitute for one reliable source.  It "might"
make sense if you could prove that the failures were anti
correlated, but I've never seen such a proof.  Instead what
happens is that the bad guys defeat each of the unreliable
sources, then eat your lunch.

@_date: 2020-01-23 03:42:21
@_author: John Denker 
@_subject: [Cryptography] Proper Entropy Source 
A couple of balloonists managed to get completely lost.
They called down to a guy on the ground.   --Balloonist:   Hey!  Excuse me!
  --Pedestrian:   You're in a balloon.
The answer was true but completely useless.  I mention this
because on 1/22/20 9:30 PM, Mansour Moufid wrote in part:
Why yes, we *are* in a balloon, TYVM.
Again, true but uninformative.
Some guy named von Neumann pointed that out in the 1940s.
Not true.  Not a consequence of the foregoing equations.
Software can increase randomness, as defined below, even
though it does not increase entropy.  This is often not
merely desirable but essential.
  The word means different things to different people,
  but in crypto context the following definition is
  often close enough:
        random means "not guessable by adversaries"
Software contributes a great deal, because every source
of entropy produces less than 100% entropy density.
There is some squish mixed in.  Software is needed to
increase the /density/ of entropy (without increasing
the total entropy) ... and/or to hide the squish so it
cannot feasibly be exploited.
There is no such thing as a RNG that is 100% physics
entropy with 0% algorithmic crypto.  Or vice versa.
1) Even if that were true, it would be silly.  How to you propose to increase a thing without increasing the measure
of the thing?
2) The word "random" means different things to different
people.  Nothing anybody says in this forum is going to
change that.
The definition implied by assertion [a] is not particularly
useful, because it means that PRNGs do not exist.
Let's be clear:  In a great many real-world situations, a
cryptologically strong PRNG is random enough for the purpose
(i.e. not guessable by adversaries) even though its entropy
density is nowhere near 100%, in fact very close to 0%.
A good HRNG produces an entropy density that is within epsilon
of 100%.  However (!) this epsilon is not the only (or even
the primary) measure of quality.  Reducing epsilon is often
not a good use of time and effort; it is better to use crypto
algorithms to make sure epsilon doesn't matter.

@_date: 2020-01-29 12:14:34
@_author: John Denker 
@_subject: [Cryptography] Proper Entropy Source 
Talking about "operating system sources" is too vague.
Furthermore, is misguided to focus on what can't be done.
Very often a sound card is available to the operating system,
and it can be used as the basis for a strong, reliable
randomness generator.
  There are some server-class boards out there that
  lack a sound card, but that is a fixable problem.
  Fixable at very low cost.

@_date: 2020-07-02 05:52:10
@_author: John Denker 
@_subject: [Cryptography] Stream Cipher over Unreliable Transport 
How is this different from encrypting a random-access disk?
I would use ChaCha20.
This is slightly heretical, but I have always considered cipher
"modes" such as CBC to be fig leaves used to cover up a bad
The Right Thing? is to use a different key for each block. The
chaining mode serves only to disguise the fact that you are
re-using a key.  The only reason you would be tempted to do
this is if the cipher is vulnerable to related-key attacks, or
is too expensive to re-key, or both.
Tangential remark:
The is rarely a good reason to use a stream cipher.  There are
situations where it's not particularly worse, but it is hard to
imagine a situation where it is better than the alternative.

@_date: 2020-09-30 13:57:43
@_author: John Denker 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
Let's talk about DES in particular.  At the time it came along, it was
common to implement crypto in SSI (small scale integration) hardware.
In that world, a rotate is the opposite of irregular, the opposite of
exotic.  It is easy to implement.  The hierarchy looks like this:
  ? bitwise and, or, xor, etc.
  ? shift and rotate
  ? arithmetical add and subtract
  ? multiply
  ? divide
Let's be clear, add and subtract are very much more complicated than
shift and rotate.  Multiplication is eeenormously more complicated still.
When you hear that the hardware can do a multiply (or a multiply-add) in
a single clock cycle, please do not underestimate the amount of on-chip
acreage, electrical energy, and cleverness go into making that possible.
Subtle bugs have been found in hardware floating point algorithms.  It
is hard to imagine a subtle bug in shift or rotate.  (There are ways of
misusing the instructions, but they're not subtle.)
With bitwise operations plus shift and rotate, it is particularly easy
to prove that all bits in the word are encrypted equally strongly.  With
arithmetical operations this is a lot trickier, because the lower-order
bits are inherently dissimilar to the high-order bits.  So there are good
reasons why bit-wise thinking has persisted for decades.
So from this viewpoint, especially when looking at the bare metal (as
opposed to high-level compiled code), shift and rotate are regular, and
arithmetic is exotic.
