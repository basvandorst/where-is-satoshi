
@_date: 2009-01-26 15:41:17
@_author: Zooko O'Whielacronx 
@_subject: Proof of Work -> atmospheric carbon 
I was one of those people, a decade and a half ago, on the cypherpunks
mailing list.  In fact, as I recall I once discussed with John Gilmore
after a Bay Area Cypherpunks Physical Meeting whether he would pay me to
implement some sort of solution to spam, but we didn't agree on a
Hey, the future is long.  (We hope.)
Coincidentally, I just blogged today about how we are much closer to
this now than we were then, even though none of the smart people that
you were probably thinking of are involved in the new deployments:
WoW-gold, for example, appears to have at least millions of transactions
a day.  Does anyone have more detail about the scale and scope of these
Thanks!  I'll read this.

@_date: 2010-04-22 11:08:54
@_author: Zooko O'Whielacronx 
@_subject: What's the state of the art in factorization? 
There is some interesting work in public key cryptosystems that reduce
to a *random* instance of a specific problem.
Here is a very cool one:
Public-Key Cryptographic Primitives Provably as Secure as Subset Sum
Vadim Lyubashevsky and Adriana Palacio and Gil Segev
Abstract: We propose a semantically-secure public-key encryption
scheme whose security is polynomial-time equivalent to the hardness of
solving random instances of the subset sum problem. The subset sum
assumption required for the security of our scheme is weaker than that
of existing subset-sum based encryption schemes, namely the
lattice-based schemes of Ajtai and Dwork (STOC '97), Regev (STOC '03,
STOC '05), and Peikert (STOC '09). Additionally, our proof of security
is simple and direct. We also present a natural variant of our scheme
that is secure against key-leakage attacks, as well as an oblivious
transfer protocol that is secure against semi-honest adversaries.
Unless I misunderstand, if you read someone's plaintext without having
the private key then you have proven that P=NP!
Nice. :-)

@_date: 2010-04-22 11:40:44
@_author: Zooko O'Whielacronx 
@_subject: What's the state of the art in factorization? 
On Wed, Apr 21, 2010 at 5:29 PM, Samuel Neves  wrote
(on the cryptography at metzdowd.com list):
I've been looking at that one, with an eye to using it in the One
Hundred Year Cryptography project that is being sponsored by Google as
part of the Google Summer of Code (see recent discussions on the
tahoe-dev archives for April 2010 [1]).
Later I discovered this paper [2] which appears to be an improvement
on that one in terms of performance (see Table 1 in [2]) while still
having a tight reduction to the Computational Diffie-Hellman (CDH)
problem. Strangely, this paper [2] doesn't appear to have been
published anywhere except as an eprint on eprint.iacr.org. I wonder
why not. Is there something wrong with it?
I still have some major questions about the funky "hash into a curve"
part of these schemes. I'm hoping that [3] will turn out to be wrong
and a nice simple dumb efficient hack will be secure for these
particular digital signature schemes.
Of course if the newfangled schemes which reduce to a random instance
of a classic hard problem work out, that would provide an even
stronger assurance of long-term safety than the ones that reduce to
CDH. See for example the paper [4] that I mentioned previously on the
cryptography at metzdowd.com mailing list. Unless I misunderstand, if you
can break that scheme by learning someone's plaintext without knowing
their private key, then you've also proven that P=NP!
Unfortunately that one in particular doesn't provide digital
signatures, only public key encryption, and what I most need for the
One Hundred Year Cryptography project is digital signatures.
[1] [2] [3] [4]

@_date: 2010-04-22 12:18:34
@_author: Zooko O'Whielacronx 
@_subject: What's the state of the art in digital signatures? Re: What's the  
By the way, the general idea of One Hundred Year Security as far as
digital signatures go would be to combine digital signature
algorithms. Take one algorithm which is bog standard, such as ECDSA
over NIST secp256r1 and another which has strong security properties
and which is very different from ECDSA. Signing is simply generating a
signature over the message using each algorithm in parallel.
Signatures consist of both of the signatures of the two algorithms.
Verifying consists of checking both signatures and rejecting if either
one is wrong.
Since the digital signature algorithms that we've been discussing such
as [1] are related to discrete log/Diffie-Hellman and since an
efficient implementation would probably be in elliptic curves, then
those are not great candidates to pair with ECDSA in this combiner
Unfortunately I haven't stumbled on a digital signature scheme which
has good properties (efficiency, simplicity, ease of implementation)
and which is based on substantially different ideas and which isn't
currently under patent protection (therefore excluding NTRUSign).
Any ideas?
[1]

@_date: 2010-04-28 23:51:23
@_author: Zooko O'Whielacronx 
@_subject: What's the state of the art in digital signatures? Re: What's the  
I see. I did misunderstand. So although cracking the Lyubashevsky,
Palacio, Segev encryption scheme [1] doesn't mean that you've proven
P=NP, because NP is about worst-case rather than average-case, it
*does* mean that you've solved the subset sum problem for a random
instance. If you can do that for all keys that people use in real life
then you can solve the subset sum problem for almost all random
instances, which seems like it would still be a breakthrough in
complexity theory. If you can do it for only a few keys then this
means that the Lyubashevsky, Palacio, Segev scheme is susceptible to
"weak keys".
Is that right?
Anyway, although this is not one, there do exist proposals for public
key crypto schemes where breaking the scheme implies solving a worst
case instance of a supposedly hard problem, right?
Here is a recent paper which surveys several of them (all
lattice-based) and estimates secure key sizes: [2].
None of the signature schemes mentioned therein appear to have the
sort of efficiency that we are used to. For example the "ecdonaldp"
(ECDSA) signature schemes measured on
 have key sizes on the order of
tens of bytes, where the most efficient digital signature algorithm
described in [2] has key sizes on the order of thousands of bytes.
(And that one is a one-time signature scheme!)
Okay, so I'm still searching for a signature algorithm which has the
following properties (or as many of them as I can get):
1. efficient (signing time, verification time, key generation time,
key size, signature size)
2. some kind of strong argument that it really is secure (the gold
standard would be reduction to a worst-case instance of an NP-complete
or, if we can't have (2) then at least we want (3) and (4):
3. rather different from ECDSA, so that a breakthrough is unlikely to
invalidate both ECDSA and this other scheme at once
4. not known to be vulnerable to quantum computers
and finally but importantly:
4. easy to understand and to implement
Suggestions welcome!
Zooko Wilcox-O'Hearn
[1] [2]

@_date: 2010-06-12 22:21:51
@_author: Zooko O'Whielacronx 
@_subject: Merkle Signature Scheme is the most secure signature scheme possible  
Regarding earlier discussion on these lists about "the difficulty of
factoring" and "post-quantum cryptography" and so on, you might be
interested in this note that I just posted to the tahoe-dev list:
"100-year digital signatures"
Here is an excerpt:
As David-Sarah [Hopwood] has pointed out, a Merkle Signature Scheme is at least
as secure as *any* other digital signature scheme, even in the
long-term?even if attackers have quantum computers and the knowledge
of how to solve math problems that we don't know how to solve today.
If you had some other digital signature scheme (even, for the sake of
argument, a post-quantum digital signature scheme with some sort of
beautiful reduction from some classic math problem), then you would
probably start wanting to digitally sign messages larger than the few
hundreds of bits that the digital signature algorithm natively
handles. Therefore, you would end up hashing your messages with a
secure hash function to generate "message representatives" short
enough to sign. Therefore, your system will actually depend on both
the security of the digital signature scheme *and* the security of a
hash function. With a Merkle Signature Scheme you rely on just the
security of a hash function, so there is one less thing that can go
wrong. That's why a Merkle Signature Scheme is at least as secure as
the best digital signature scheme that you can imagine. :-)
In that note I go on to talk about more Tahoe-LAFS-specific
engineering considerations and expose my ignorance about exactly what
properties are required of the underlying secure hash functions.

@_date: 2013-10-11 17:32:37
@_author: Zooko O'Whielacronx 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I like the ideas, John.
The idea, and the protocol you sketched out, are a little reminiscent
of ZRTP ? and of tcpcrypt ?. I think you can go one step further,
however, and make it *really* strong, which is to offer the "higher"
or "outer" layer a way to hook into the crypto from your inner layer.
This could be by the inner layer exporting a crypto value which the
outer layer enforces an authorization or authenticity requirement on,
as is done in ZRTP if the "a=zrtp-hash" is delivered through an
integrity-protected outer layer, or in tcpcrypt if the "Session ID" is
verified by the outer layer.
I think this is a case where a separation of concerns between layers
with a simple interface between them can have great payoff. The
"lower"/"inner" layer enforces confidentiality (encryption),
integrity, hopefully forward-secrecy, etc., and the outer layer
decides on policy: authorization, naming (which is often but not
necessarily used for authorization), etc. The interface between them
can be a simple cryptographic interface, for example the way it is
done in the two examples above.
I think the way that SSL combined transport layer security,
authorization, and identification was a terrible idea. I (and others)
have been saying all along that it was a bad idea, and I hope that the
related security disasters during the last two years have started
persuading more people to rethink it, too. I guess the designers of
SSL were simply following the lead of the original inventors of public
key cryptography, who delegated certain critical unsolved problems to
an underspecified "Trusted Third Party". What a colossal, historic
The "foolscap" project ? by Brian Warner demonstrates that it is
possible to retrofit a nice abstraction layer onto SSL. The way that
it does this is that each server automatically creates a self-signed
certificate, the secure hash of that certificate is embedded into the
identifier pointing at that server, and the client requires the
server's public key match the certificate matching that hash. The fact
that this is a useful thing to do, and inconvenient and rare thing to
do with SSL, should give security architects food for thought.
So I have a few suggestions for you:
1. Go, go, go! The path your thoughts are taking seems fruitful. Just
design a really good "inner layer" of crypto, without worrying (for
now) about the vexing and subtle problems of authorization,
authentication, naming, Man-In-The-Middle-Attack and so on. For now.
2. Okay, but leave yourself an out, by defining a nice simple
cryptographic hook by which someone else who *has* solved those vexing
problems could extend the protection that they've gained to users of
your protocol.
3. Maybe study ZRTP and tcpcrypt for comparison. Don't try to study
foolscap, even though it is a very interesting practical approach,
because there doesn't exist documentation of the protocol at the right
level for you to learn from.
 ? verifiably end-to-end-encrypted storage
P.S. Another example that you and I should probably study is cjdns ?.
Despite its name, it is *not* a DNS-like thing. It is a
transport-layer thing. I know less about cjdns so I didn't cite it as
a good example above.
? ? ? ?

@_date: 2014-01-09 22:28:55
@_author: Zooko O'Whielacronx 
@_subject: [Cryptography] Cuckoo Cycles: a new memory-hard proof-of-work 
Hello John Tromp!
That is neat! The paper could use a related work section, for example
Litecoin uses scrypt in the attempt to make it harder to implement in
The current Password Hashing Contest (disclosure: I am on the panel)
may be relevant to your interests:

@_date: 2014-03-09 04:23:06
@_author: Zooko O'Whielacronx 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
If you like RC4, you might like ChaCha20.
Like RC4, it is a stream cipher, but unlike RC4 it is widely liked by
modern cryptographers. That's a generalization, of course, but:
* ChaCha20 is a variant of Salsa20, which was one of the winners of
the eSTREAM competition: * There is work to implement it in TLS (to replace RC4):
* It's now included in OpenSSH:
* It is the core of my favorite secure hash function, BLAKE2!
 (Disclosure: I'm one of the authors of BLAKE2, but
not of the original "BLAKE" from which BLAKE2 is derived.)
Oh yes, and ChaCha is much more efficient than RC4.
 says that modified alleged RC4
("MARC4") takes about 14 cycles per byte and that Salsa20 takes about
4 cycles per byte.  says that
ChaCha20 is usually around 15% more efficient than Salsa20 on modern
Intel CPUs.

@_date: 2014-03-13 17:45:24
@_author: Zooko O'Whielacronx 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
The lesson I take is that when you design a protocol today, someone
might want to deploy it into new, more constrained environments
tomorrow (eyeglasses, wristwatches, smart dust, Javascript, a
cryptocurrency blockchain, SNARKs, ?) and the more efficient your
protocol is then the less likely they'll have a showstopper problem at
that point.
Fortunately there are well-studied cryptographic primitives available
today that are *both* secure and efficient, so we don't have to spend
a lot of energy trying to balance a difficult security-vs-efficiency
* cipher: ChaCha20
* Diffie-Hellman: Curve25519
* digsig: Ed25519
* MAC: Poly1305
* hash: BLAKE2
* KDF: HKDF
[Bias alert: I'm one of the authors of BLAKE2.]
Interesting to note that these are all designed by Daniel J.
Bernstein, except for HKDF and for BLAKE2, which re-uses the ChaCha
function as its core. However, I didn't choose these ones out of sheer
DJB-fandom, but rather these happen to be my current favorites.
There are also other good options for some of these.
