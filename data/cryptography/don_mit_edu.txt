
@_date: 2003-04-08 14:41:15
@_author: Don Davis 
@_subject: Via puts RNGs on new processors 
a one-time evaluation of the RNG's design and of
its output aren't really enough.  there are three
related issues, which arise because effective and
thorough TRNG testing are too expensive:
  * production-line QA:  with modern chip-fab
    technology, salable chip yields aren't 100%.
    each chip gets run through a validation test,
    to make sure that its various functions work
    correctly, and a lot of chips get scrapped
    because of validation failures.  unfortunately,
    thorough validation of each chip's TRNG would
    take too long (generate some bulk of random
    bits, do a few hours or days of CPU-intensive
    statistical computations...).
  * surely, vendors are going to be unwilling to
    discard a chip whose CPU and on-board memory
    work, but whose TRNG doesn't work.  the ven-
    dor might bother to disable the TRNG circuits,
    and then sell the faulty chips at a reduced
    price for non-crypto applications.  but i
    expect that most vendors won't bother, but
    will silently sell the TRNGs as-is.
  * detection of run-time TRNG failures:  how
    will the CPU or operating system detect that
    the TRNG has stopped working properly?  surely,
    neither the CPU nor the OS is going to spon-
    taneously sample and test the TRNG's output
    for randomness failures, because proper RNG
    testing is computationally expensive.

@_date: 2003-04-08 17:28:20
@_author: Don Davis 
@_subject: Via puts RNGs on new processors 
> This could be solved by selling the chip
   > "as is".  With no guaruntee of performance,
   > leaving each user to make their own tests.
   this isn't a salable answer for commercial
   deployments, though it's a good-enough answer
   for crypto hobbyists.
   > No guaruntee, then no problem.  This seems
   > to be a userland problem, solved by some
   > user program that tests the output, run on
   > an application basis.
   again, not a commercially salable solution.
   good RNG tests run _slow_.  i once found a
   correlation in an RNG's output that showed
   up only in a set of 30M samples.
   > run the TRNG for a couple of seconds,
   > compress the result, and use the result
   > to decide whether the chip goes into the
   > "good" bucket or the "no-TRNG" bucket.
   i'm sorry, but compressibility is not a
   sensitive randomness test at all, much
   less is it a thorough randomness test.
agreed.  but this undermines the on-chip TRNG's
"one stop shopping" claim to fame.

@_date: 2003-04-09 00:45:38
@_author: Don Davis 
@_subject: Via puts RNGs on new processors 
Ian griggs replied:
since your goal is to feed a mixing function like
yarrow, it's overkill for the hardware to produce
uniformly-distributed random integers.  if instead
the hardware produces a chaotic but structured
signal, then the signal's expected structure can
continually be checked as part of the validation,
before the chaotic signal is fed into the mixing
your argument is valid only if yarrow (or some
other mixing function) is getting bits from several
devices, so that a single device's failure doesn't
matter.  but, vendors avoid such redundancy (so as
to minimize cost), and tend to ship products with
single-source RNGs.
but commercial customers do prefer one-stop-
shopping, wherever possible, and especially in
security.  they like turnkey solutions, which
isn't a common characteristic of open-source
systems (for good reasons).  i happen to
disagree with your claim that customers
_shouldn't_ want one-stop-shopping, but my opinion
doesn't matter as much as the customers' well-
known preference for simplicity in purchasing,
integration, and support.

@_date: 2003-04-15 23:34:32
@_author: Don Davis 
@_subject: FW: DMCA used to shut down campus ID security talk 
here's an interesting paper from the cyberprof list [mailto:owner-cyberprof_list at uclink4.berkeley.edu],
forwarded to me by a friend.  the author, Joe Liu,
worked with EFF on the felten v. RIAA case.
-----Original Message-----
Pam Samuelson replied:  Joe Liu replied:
 .
The DMCA and the Regulation of
Scientific Research?
By Joseph P. Liu, BC Law School
Abstract:  This Essay is an analysis of the Digital
Millennium Copyright Act's impact on academic encryption
research. Recently, there has been some debate over the
extent to which academic encryption researchers should
reasonably fear liability under the DMCA for conducting
and publishing their research. In this Essay, I will
argue that, for both legal and practical reasons,
academic encryption researchers should be able to conduct
and publish their research without significant fear of
liability. However, the DMCA will have a non-trivial
impact on the conditions under which such research takes
place, and this impact can be expected to have several
undesirable effects. More broadly, this impact highlights
the problematic nature of the DMCA's effect of regulating
scientific research in furtherance of intellectual property
rights. The Essay ends with a number of suggested ways of
mitigating some of these negative effects.

@_date: 2003-02-10 21:16:00
@_author: Don Davis 
@_subject: Columbia crypto box 
Ilya Mironov (Stanford), (Not So) Random Shuffles of RC4
   Advances in Cryptology - CRYPTO 2002 Proceedings,
   ed. by Moti Yung.  Springer LNCS 2242, 2002. pp. 304-319.
   Abstract. Most guidelines for implementation of the RC4
stream cipher recommend discarding the first 256 bytes
of its output. This recommendation is based on the
empirical fact that known attacks can either cryptanalyze
RC4 starting at any point, or become harmless after these
initial bytes are dumped. The motivation for this paper
is to find a conservative estimate for the number of bytes
that should be discarded in order to be safe. To this end
we propose an idealized model of RC4 and analyze it apply-
ing the theory of random shuffes. Based on our analysis
of the model we recommend dumping at least 512 bytes.
7 Conclusion
We identified a weakness in RC4 stemming from an
imperfect shuffing algorithm used in the key scheduling
phase and the pseudo-random number generator. The
weakness is noticeable in the first byte but does not
disappear until at least the third or the fourth pass
(512 or 768 bytes away from the beginning of the
output). ... Our most conservative recommendation ...
means that discarding the initial 12 * 256 bytes most
likely eliminates the possibility of a strong attack.
Dumping several times more than 256 bytes from the
output stream (twice or three times this number)
appears to be just as reasonable a precaution. We
recommend doing so in most applications.

@_date: 2003-07-12 11:21:38
@_author: Don Davis 
@_subject: traffic analysis of phone calls? 
with similar import, here's cringely's article on
insecure CALEA workstations:
"Not only can the authorities listen to your phone calls,
 they can follow those phone calls back upstream and
 listen to the phones from which calls were made.  They
 can listen to what you say while you think you are on
 hold.  This is scary stuff.
"But not nearly as scary as the way CALEA's own internal
 security is handled. The typical CALEA installation on
 a Siemens ESWD or a Lucent 5E or a Nortel DMS 500 runs
 on a Sun workstation sitting in the machine room down
 at the phone company. The workstation is password
 protected, but it typically doesn't run Secure Solaris.
 It often does not lie behind a firewall. Heck, it
 usually doesn't even lie behind a door. It has a direct
 connection to the Internet because, believe it or not,
 that is how the wiretap data is collected and transmitted."

@_date: 2003-07-15 00:30:49
@_author: Don Davis 
@_subject: Information-Theoretic Analysis of Information Hiding 
"An electrical engineer at Washington University
 in St. Louis has devised a theory that sets the
 limits for the amount of data that can be hidden
 in a system and then provides guidelines for how
 to store data and decode it. Contrarily, the
 theory also provides guidelines for how an
 adversary would disrupt the hidden information.
"The theory is a fundamental and broad-reaching
 advance in information and communication systems
 that eventually will be implemented in commerce
 and numerous homeland security applications --
 from detecting forgery to intercepting and
 interpreting messages sent between terrorists.
"Using elements of game, communication and
 optimization theories, Jody O'Sullivan, Ph.D.,
 professor of electrical engineering at Washington
 University in St. Louis, and his former graduate
 student, Pierre Moulin, Ph.D., now at the
 University of Illinois, have determined the
 fundamental limits on the amount of information
 that can be reliably hidden in a broad class of
 data or information-hiding problems, whether they
 are in visual, audio or print media."
Information--Theoretic Analysis of Information Hiding
by Pierre Moulin and Joseph A. O'Sullivan
"An information--theoretic analysis of information
 hiding is presented in this paper, forming the
 theoretical basis for design of information--hiding
 systems.  Information hiding is an emerging research
 area which encompasses applications such as copyright
 protection for digital media, watermarking, finger-
 printing, steganography, and data embedding.  In these
 applications, information is hidden within a host data
 set and is to be reliably communicated to a receiver.
 The host data set is intentionally corrupted, but in
 a covert way, designed to be imperceptible to a casual
 analysis.  Next, an attacker may seek to destroy this
 hidden information, and for this purpose, introduce
 additional distortion to the data set.  Side information
 (in the form of cryptographic keys and/or information
 about the host signal) may be available to the information
 hider and to the decoder.
"We formalize these notions and evaluate the {\em hiding
 capacity}, which upper--bounds the rates of reliable
 transmission and quantifies the fundamental tradeoff
 between three quantities: the achievable information--
 hiding rates and the allowed distortion levels for the
 information hider and the attacker.  The hiding capacity
 is the value of a game between the information hider
 and the attacker.  The optimal attack strategy is the
 solution of a particular rate-distortion problem, and
 the optimal hiding strategy is the solution to a channel
 coding problem.  The hiding capacity is derived by
 extending the Gel'fand-Pinsker theory of communication
 with side information at the encoder.  The extensions
 include the presence of distortion constraints, side
 information at the decoder, and unknown communication
 channel.  Explicit formulas for capacity are given in
 several cases, including Bernoulli and Gaussian problems,
 as well as the important special case of small distortions.
 In some cases, including the last two above, the hiding
 capacity is the same whether or not the decoder knows
 the host data set.  It is shown that many existing
 information--hiding systems in the literature operate
 far below capacity." Sept. '02 version of the paper:

@_date: 2003-06-28 22:21:59
@_author: Don Davis 
@_subject: google crypto? 
does anyone know anything about AP's claim that Google
"encrypts" credit-card numbers?  specifically, which
cipher and what kind of key management do they use?
        Friday, June 27, 2003
  "The new software out Thursday for the [Google] toolbar
   includes ... a program that automatically fills out
   Internet forms seeking a customer's name and address.
  "The function that fills in forms offers an option to
   store credit card numbers too, but the information
   is encrypted on the hard drive of a user's computer
   instead of Google's computers, for security and
   privacy reasons."

@_date: 2003-05-12 10:10:04
@_author: Don Davis 
@_subject: New Hampshire's WiFi bill 
Being a New Hampshire resident, I have read this press coverage of this issue with a certain amount of amusement.  The actual text of the proposed modification to the statute is as follows:
"b) The owner of a wireless computer network shall be responsible for securing such computer network. It shall be an affirmative defense to a prosecution for unauthorized access to a wireless computer network if the unauthorized access complies with the conditions set forth in subparagraph I(a)(1)-(3)."
The only big change is the first sentence.  I read this as a statement of public policy -- it certainly doesn't entail any consequences for the network owner if they fail to secure their WLAN.  The second sentence basically re-affirms the existing statute provisions [subparagraph I(a)(1)-(3)] in terms of an affirmative defense.  These sections of the law do not condone intentional connection to networks for which you do not have a reasonable belief that you are authorized to attempt such access.  I don't see this as an attempt to legitimize hostile attacks or malicious "war driving".
I think that the intention of the legislation is to "decriminalize" one of the default behaviors of Windows XP, i.e. to automatically connect to any non-secured network it can find via probe requests.  The story behind this legislation is that a "good Samaritan" who inadvertently connected to a private, non-secured WLAN with happenstance RF coverage in a downtown Manchester, NH coffee shop was sued by the company operating the WLAN when they were informed by the "good Samaritan" that their WLAN was open to the public.
David B. Nelson
Wireless & AAA Architect, Office of the CTO
Enterasys Networks, Inc.
50 Minuteman Road
Andover, MA 01810-1008
Phone:  ...
E-mail: ...

@_date: 2003-05-19 12:16:31
@_author: Don Davis 
@_subject: copyright in crisis 
a college friend of mine is now a law professor,
specializing in the law & economics of intellectual
property and especially copyright (he has an econ
angle, because he used to teach economics).  because
IANAL, i've been asking his informed opinion about
the crypto libertarians' claim that digital copying
makes the recording industry's copyrights and licens-
ing fees all-but-obsolete.  this post is my summary
of his reply.
he points out that throughout its history, copy-
right has constantly been faced with technology-
driven crises, just like the current one.  in
each such crisis, a copyright-protected industry
supposedly faced death, but ultimately went on to
make more money than before.  some of the examples
my friend described were familiar to me, but others
   * player pianos:  apparently, the advent of
     automated performance was thought to undermine
     sheet music copyrights, until piano rolls came
     to be subject to the same royalties as sheet
     music;
   * musical accompaniment for silent movies:
     somehow, this stymied the sheet-music royalty-
     payments system for a while;
   * free live performances (as in nightclubs):      it took a while to figure out and establish
     the current royalty system, in which the
     venue owner pays an annual flat fee to each
     of the two biggest music copyright companies;
   * broadcast of recorded performances:  well-
     discussed elsewhere;
   * photocopiers: ditto;
   * home-use video recorders:  ditto;
   * video rentals:  apparently, the movie industry
     treated VHS as an early version of napster.
     the US and European courts came to opposite
     findings, yet both continents ended up with
     blockbuster franchising and a lot of money
     getting made.   who'd a thunk it?
my friend argues that apple's new $1/song model
is probably _the_ natural solution to the RIAA's
current fears.  even if it's not the right solution,
he completely dismisses, legally and economically,
the idea that the RIAA is a dinosaur whose extinc-
tion is nigh.  he says the current crisis is just
too cookie-cutter similar to past crises in the
industry's very profitable history.  i think the
main point is that there are so very many possible
ways for the entertainment industry to make money:
some are ancient, some are fairly new, but all
became well-established surprisingly quickly.
my friend said he has read pretty systematically
into the previous technical crises in copyright-
dependent industries, so i asked him for source
materials.  he said he read a lot of law review
articles analyzing these precedents,  but those
citations aren't gathered together in any easy-to-
post bibliography, because law review articles use
footnotes, not bibliographies.  he suggested three
books as accessible accounts of this history:
  * Digital Copyright, by Jessica Litman,
    (Prometheus Books 2001)
  * Copyright's Highway: From Gutenberg to the
    Celestial Jukebox (1996), by Paul Goldstein
  * The Digital Dilemma, by the Nat'l Research
    Council (2000).  Available online at:
BTW, i've decided to withhold my friend's name,
because he's got enough to do without having to
teach copyright law 101 a lot of non-specialists.
this post is my rendition from memory of what he
told me over coffee a couple of days ago.  if my
summary of his remarks misrepresents the facts,
that's completely my fault, and reflects my ig-
norance and carelessness, not his.

@_date: 2003-10-01 11:22:44
@_author: Don Davis 
@_subject: Monoculture 
there's another rationale my clients often give for
wanting a new security system, instead of the off-
the-shelf standbys:  IPSec, SSL, Kerberos, and the
XML security specs are seen as too heavyweight for
some applications.  the developer doesn't want to
shoehorn these systems' bulk and extra flexibility
into their applications, because most applications
don't need most of the flexibility offered by these
some shops experiment with the idea of using only
part of OpenSSL, but stripping unused stuff out of
each new release of OpenSSL is a maintenance hassle.
note that customers aren't usually dissatisfied with
the crypto protocols per se;  they just want the
protocol's implementation to meet their needs exactly,
without extra baggage of flexibility, configuration
complexity, and bulk.  they want their crypto clothing
to fit well, but what's available off-the-rack is
a choice between frumpy one-size-fits-all, and a
difficult sew-your-own kit, complete with pattern,
fabric, and sewing machine.  so, they often opt for
tailor-made crypto clothing.
my clients' concern (to keep their crypto code as
small and as simple as possible) doesn't justify
their inventing and deploying broken protocols, but
their concern does point out that neither the crypto
industry nor the crypto literature has fully met
these customers' crypto needs.

@_date: 2003-10-01 14:39:26
@_author: Don Davis 
@_subject: Monoculture 
i agree, except that simplifying the SSL protocol
will be a daunting task for a non-specialist.  when
a developer is faced with reading & understanding
the intricacy of the SSL spec, he'll naturally be
tempted to start over.  this doesn't exculpate the
developer for biting off more than he could chew,
but it's unfair to claim that his only motivation
was NIH or some other sheer stupidity.
btw, i also agree that when a developer decides to
design a new protocol, he should study the literature
about the design & analysis of such protocols.  but
at the same time, we should recognize that there's a
wake-up call for us in these recurrent requests for
our review of seemingly-superfluous, obviously-broken
new protocols.  such developers evidently want and
need a fifth option, something like:
   (5) use SSSL: a truly lightweight variant of
       SSL, well-analyzed and fully standardized,
       which trades away flexibility in favor of
       small code size & ease of configuration.
arguably, this is as much an opportunity as a wake-up

@_date: 2003-10-02 08:26:13
@_author: Don Davis 
@_subject: Monoculture 
jill ramonsky replied:
how it's done depends on the browser:
in Moz 1.0:  Edit > Preferences... > Privacy & Security >
             Certificates > Manage Certificates >
            {Authorities, Web Sites}
in MSIE 5:   Edit > Preferences.., > Web Browser >
             Security > Certificate Authorities
            (there seems to be no way to tell MSIE 5 to
             trust Alice's server cert for SSL connections,
             except to tell MSIE 5 to trust Alice's CA.)
in NS 4.75:  Communicator > Tools > Security Info >
             Certificates > {Signers, Web Sites}

@_date: 2003-10-03 09:05:30
@_author: Don Davis 
@_subject: Monoculture 
well, before dismissing MSIE's cert-mgt completely,
you should check whatever version of MSIE ships on
Win2k/XP .  MSIE 5 is all i have on my mac, because
i very rarely use MSIE.
but, if you want to complain about MSIE's security
features, you'll have to take a number and wait in
line...  B^(
