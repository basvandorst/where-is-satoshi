
@_date: 2010-04-20 18:03:24
@_author: Samuel Neves 
@_subject: What's the state of the art in factorization? 
The state of the art in factorization is the same as for, e.g., the
factorization of RSA-768 [1] --- there haven't been many advances in the
number field sieve algorithm itself. The current effort, as Bernstein
puts it, is in speeding up smoothness detection, as part of the relation
collection process.
Both the RSA-768 factorization paper and a previous one by the same
authors [2] try to predict the effort needed for a 1024-bit prediction,
which is estimated to be around 1000 times harder than a 768-bit
modulus. [1] estimates to number of operations in the RSA768
factorization to be in the ballpark of 2^67 instructions: a thousand
times harder puts this on about 2^77, which puts it in the realm of
doable, but very hard, even for a well funded organization.
We also have to take into account the logistics of doing such a
factorization. Unlike an elliptic curve discrete logarithm computation,
that takes (relatively) negligible storage and communication, the number
field sieve requires massive amounts of data, and the linear algebra
step could become (even more of) a problem.
Best regards,
Samuel Neves
[1] [2]

@_date: 2010-04-22 00:29:33
@_author: Samuel Neves 
@_subject: What's the state of the art in factorization? 
While EC (by that I assume you mean ECDSA) does not have a formal
security proof, i.e., it is as hard as the EC discrete log, it it much
closer to one than RSA is to factoring. In particular, Pointcheval and
Stern, and later Brown come close to a formal proof for ECDSA [1].
If one goes further into other schemes, there is Rabin-Williams for the
factoring problem. There are also the schemes by Goh et al. [2] that are
reducible to the CDH and DDH problems in generic abelian groups (like
EC.)  Would patents also apply to one of these schemes over an elliptic
Best regards,
Samuel Neves
[1] [2]

@_date: 2010-08-16 00:10:25
@_author: Samuel Neves 
@_subject: non 2048-bit keys 
If an attacker creating a special-purpose machine to break your keys is
a realistic scenario, why are you even considering keys of that size?
Best regards,
Samuel Neves

@_date: 2010-08-17 22:19:10
@_author: Samuel Neves 
@_subject: 2048-bit RSA keys 
A breakthrough is a rather strong term. But it's not unreasonable to
believe that the number field sieve's complexity could be lowered on the
near future by an *incremental* improvement --- it would only require
lowering the complexity from L[1/3, ~1.92] to L[1/3, ~1.2] to make 2048
bit factorization roughly as easy as 768 bits today.
Coppersmith's variant of the number field sieve proposed a tradeoff that
dramatically lowered the exponent, if one wanted to break many keys of
roughly the same size. The idea was to fix m, the 'base' of the number
field polynomial, and precompute many many pairs (a,b) such that a - bm
was smooth. With this precomputation, the NFS runs in L[1/3, ~1.639],
which is dramatically faster (and quite worth it for a large
organization --- they're bound to want to break multiple keys).
It is not unreasonable to think that a small(ish) improvement to the
number field sieve could significantly lower the strength of current
keys. It *looks* more likely to happen than a significant improvement on
the speed of ECDLP breaking (I'll make no bets on AES, though).
Best regards,
Samuel Neves

@_date: 2010-08-18 02:18:34
@_author: Samuel Neves 
@_subject: 2048-bit RSA keys 
Forwarded at Andrew's request.
-------- Original Message --------
It is not unreasonable to consider the possibility of
algorithmic improvements.  But that does not guarantee
My 1995 paper "The future of integer factorization,"
    published in CryptoBytes (The technical newsletter of RSA
Laboratories), vol. 1, no. 2, 1995, pp. 5-12, considered
the historical record of integer factorizations up to that
point, and took account both of the increasing computing
power and the progress in algorithms (which over the preceding
20 years contributed about as much as the growth in the
number of available cycles).  It then projected when
integers of various sizes might get factored, and even
the most conservative projection had 768-bit integers
getting factored by 2004 or so.
In retrospect, the latest 768-bit factorization shows that
over the preceding 15 years there has been practically no
progress in algorithms, and even the computing power that
is actually used for the experiments has fallen behind
majordomo at metzdowd.com

@_date: 2010-07-11 17:26:08
@_author: Samuel Neves 
@_subject: 1280-Bit RSA 
The exponent can be further lowered from that (somewhere between 1.6 and
1.7) --- RSA-768 took about 2^67 Opteron instructions to complete, and
RSA-512 can be done in about 2^54 similar operations (it is in the realm
of possibility for a single box over a few days/weeks).
Best regards,
Samuel Neves

@_date: 2010-10-01 19:27:52
@_author: Samuel Neves 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
It is slightly stronger than RSA-1024 against ECM, since ECM is then
performed modulo a 2048 bit value instead of a 1024 bit one. This slows
down arithmetic by a factor between 3 and 4 (Karatsuba vs Schoolbook
multiplication). Further, there are now 3 factors to find by ECM instead
of just 1.
Going by asymptotic complexities, factoring 4-prime RSA-2048 by NFS
should cost around 2^116 operations. Using ECM to find a 512-bit prime
costs around 2^93 elliptic curve group additions (add arithmetic cost
here). Factoring RSA-1024 by NFS costs around 2^80 operations.
Thus, I believe that 4-prime RSA-2048 is slightly easier than 2-prime
RSA-2048, but still significantly harder than RSA-1024.
Best regards,
Samuel Neves

@_date: 2010-10-01 01:52:49
@_author: Samuel Neves 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
One solution would be to use 2048-bit 4-prime RSA. It would maintain the
security of RSA-2048, enable the reusing of the modular arithmetic units
of 1024 bit VLSI chips and keep ECM factoring at bay. The added cost
would only be a factor of ~2, instead of ~8.
Best regards,
Samuel Neves

@_date: 2010-10-01 02:01:55
@_author: Samuel Neves 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
IIRC, multi-prime RSA is already supported in standards, but not in
practice (read: OpenSSL):
Best regards,
Samuel Neves

@_date: 2014-08-27 03:09:16
@_author: Samuel Neves 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
I can see 3 analyses of CAST5 (aka CAST-128) in the last 7 years:
 - A linear attack on 3 rounds (out of 16) by Nakahara and Rasmussen [1];
 - More linear cryptanalysis on up to 6 rounds by Wang, Wang, and Hu [2];
 - A differential attack on up to 9 rounds by Wang, Wang, Chow, and Hui [3].
You might recognize the name Xiaoyun Wang as the cryptanalyst who broke MD5, SHA-0, SHA-1, and many other primitives.
The best of all 3 (mostly theoretical) attacks goes up to 9 out of 16 rounds of CAST-128; this is still a better
security margin than AES had when it was selected in the 1990s (7 out of 10).
[1] [2] [3]

@_date: 2014-05-22 04:27:06
@_author: Samuel Neves 
@_subject: [Cryptography] New attacks on discrete logs? 
The article links to this paper,  which implies that the "next-generation cryptography"
here is pairing-based crypto in small characteristic.

@_date: 2014-10-16 01:50:42
@_author: Samuel Neves 
@_subject: [Cryptography] RFC: Generating RSA moduli / semiprimes with 
[1] points out that [2, Section 2.1] predates Vanstone and Zuccherato, invalidating their 1994 patent on the technique.
[1] [2] The article is freely available here, as far as I can tell:
Section 3.1 does mention that "Another method for leaking information is to choose p and q such that the least
significant bits of n have a special form not required by the specifications".
This one is also available here:  As far as I can tell,
there is no mention of generating special semiprimes; the subliminal channel are the signatures themselves, not the modulus.

@_date: 2015-08-17 20:29:02
@_author: Samuel Neves 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
It is arguable that the metric used, which only cares about operation counts, is not the right one. For typical
parameter choices the storage cost (aka machine size) of the NFS is roughly proportional to the square root of the
computational cost. To see this, note that the cost of the matrix step is matrix_size^(2 + o(1)), and the matrix step is
asymptotically as costly as sieving. So at 15k bits, we get ~2^256 time complexity and ~2^128 memory. Multiplying area
and time, the asymptotic cost here is L[1/3, 2.85], much larger than the L[1/3, 1.901] usually advertised.
Taking the machine size into account gets you to the circuit or batch NFS, whose complexity is worked out in the AT
(area-time) metric. For a single 15k-bit factorization this gets you time ~2^165 on a machine of size ~2^110 (as usual,
ignoring o(1) factors in the asymptotic complexities). The asymptotic AT cost here is L[1/3, 1.976]. 12288-bit keys
would suffice to thwart an attack of AT cost significantly below 2^256; 5120-bit keys would be enough for 256-bit AT
security against the conventional, non-circuit, NFS.
