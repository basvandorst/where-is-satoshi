
@_date: 2015-08-02 09:28:06
@_author: Ron Garret 
@_subject: [Cryptography] More efficient and just as secure to sign 
I think it’s important to note here that the collision resilience you are losing is resilience against collisions in the underlying hash H.  Ed25519 *is* a hash of M and the secret key, and it obviously cannot be resilient against collisions in *that* hash (i.e. collisions in ed25519 itself).  So if you hash first, you now have two collision risks whereas before you only had one.  But the output of Ed25519 is 256 bits, so if H is, say, SHA512 the incremental risks of collisions in H over the inherent risk of collisions in Ed25519 are (almost certainly) pretty darn low.  Almost certainly the least of your worries in any real-world application.
If you’re really worried about collisions, you can probably produce an overall more collision-resistent signature scheme by concatenating the signatures of two different hashes of M.  (But I am not an expert so don’t do this until someone who actually knows what they’re doing has analyzed it.)

@_date: 2015-08-02 23:04:56
@_author: Ron Garret 
@_subject: [Cryptography] More efficient and just as secure to sign 
This is probably obvious, but I thought it might be worth stating explicitly for the benefit of lurkers: it’s important that the hash you sign be at least 256 bits.  512 is probably better just to give yourself a little more margin.  If you sign a hash narrower than 256 bits then you really do lose.
(And, as long as I’m stating the obvious, these numbers are for Ed25519.  If you are using a generalized EdDSA signature scheme you should sign a hash that is at least as wide as the signature you are producing.  Making it wider is probably not a bad idea.)

@_date: 2015-08-03 10:01:23
@_author: Ron Garret 
@_subject: [Cryptography] More efficient and just as secure to sign 
I don’t see how Ed25519 is resistant against length extension attacks.  It is true that collisions in H do not produce collisions in Ed25519 because Ed25519 applies H twice to two different inputs.  But it seems to me that a collision in Ed25519 itself could be length-extended if that collision resulted from two collisions in H, because both applications of H put M at the end.
If you are really worried about future collisions in SHA-512 you can sign an HMAC instead of a simple hash.  (In fact, if I’m right and Ed25519 really is vulnerable to length-extension attacks on two collisions in H, then signing an HMAC might actually be (very slightly) more secure than signing the message directly.)

@_date: 2015-08-04 21:10:42
@_author: Ron Garret 
@_subject: [Cryptography] SRP for mutual authentication - as an 
FYI/FWIW I took a whack a re-inventing authentication a few years back and came up with this:
It’s essentially browser certs implemented in Javascript, which essentially delegates authentication to a trusted third party.  It was designed to be more secure than usernames and passwords (which is a pretty low bar) but super-easy for both users and relying-parties to use.
If there’s any interest in this I’d be happy to provide more details.

@_date: 2015-08-05 10:09:18
@_author: Ron Garret 
@_subject: [Cryptography] SRP for mutual authentication - as an 
And in particular, has there ever been an attempt that was integrated into the browser so that the user could actually have a hope of knowing whether or not they were dealing with the One True Password Box?  (No, browser certificates don’t count.  Certs got the underlying auth right but dropped the ball in a big way on the UX.)

@_date: 2015-08-06 15:09:35
@_author: Ron Garret 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
In keeping with this advice, I am pleased to announce that my super-simple (<1000 LOC + TweetNaCl) PGP replacement, SC4, now has a command-line version written in Python.  If crypto in the browser made you queasy, this is for you.
NOTE: This is an ALPHA release.  It has undergone only very cursory testing (I would really appreciate some help with that, actually).  The web version of SC4 has been audited, but the Python version has not (though it was mostly ported directly from the Javascript implementation, so it should not have any gaping holes).
Feedback of all sorts very much appreciated.

@_date: 2015-08-06 15:13:27
@_author: Ron Garret 
@_subject: [Cryptography] Announcing a command-line version of SC4 
SC4 is my attempt to produce a minimalist and super-easy-to-use replacement for PGP using TweetNaCl as the core crypto.  The original SC4 was a web application.  Since crypto in the browser makes a lot of people queasy, I have produced a command-line version written in Python.  It uses the C TweetNaCl library (the web version obviously had to use a Javascript port).  Python is only used to implement the UI.  You can find SC4-PY, along with the original web version of SC4, on github:
NOTE: This is an ALPHA release.  It has undergone only very cursory testing (I would really appreciate some help with that, actually).  The web version of SC4 has been audited, but the Python version has not (though it was mostly ported directly from the Javascript implementation, so it should not have any gaping holes).
Feedback of all sorts very much appreciated.

@_date: 2015-12-12 23:19:43
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Yes, of course.  The Von-Neuman trick can be used to eliminate arbitrary biases.
Probably not.  The reason is that there are much easier ways to avail yourself of (essentially) the same physics.  Thermal noise, for example, gives you just as much “true randomness” as quantum measurements (because thermal noise is, at root, a quantum effect) but it’s much (much!) easier to obtain.

@_date: 2015-12-13 22:04:05
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
This is exactly right.  The quantis device is pure marketing hype, designed for the PHB who needs to be able to say that s/he’s using something “certified.”  It might work as advertised, or it might not.  The only way to tell is 1) trust the certification or 2) audit the device yourself.  And option 1 requires an awful lot of trust: you have to trust not only that the people doing the certification knew what they were doing, but also that the particular device you’re using was constructed according to the certified design.
It’s really all marketing hype.  A properly configured op-amp will give you every bit as much true randomness as the quantis device for a tiny, tiny fraction of the cost, and will be much more difficult to attack.  I can think of a dozen way the quantis device could be compromised, but to attack a thermal noise source you would have to do something like dunk it in liquid nitrogen.

@_date: 2015-12-17 07:36:10
@_author: Ron Garret 
@_subject: [Cryptography] What should I put in notifications to NSA? 
Wait, what?  There’s actually a law that requires this???  That is news to me.  Can someone please post a pointer to the actual law?

@_date: 2015-12-20 07:47:12
@_author: Ron Garret 
@_subject: [Cryptography] Questions about crypto that lay people want to 
Because before the advent of modern electronic surveillance, people could rely to a certain extent on the laws of physics and social norms to protect their privacy.  Steaming open an envelope and making use of the information thus acquired is orders of magnitude more expensive than snooping on an internet connection.  Internet snooping is also viewed as less invasive than opening an envelope, i.e it's. seen as the equivalent of reading a postcard addressed to someone else as opposed to a letter.  Crypto is the digital equivalent of a physical envelope.

@_date: 2015-12-21 12:33:41
@_author: Ron Garret 
@_subject: [Cryptography] Questions about crypto that lay people 
Two more worthwhile (and slightly less politically charged) links:
It’s a really hard problem.

@_date: 2015-12-22 16:45:22
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I’m honestly not sure if this discussion is serious or not because rolling dice is so obviously silly (and quibbling over the number of sides is even sillier).  But on the off chance that I’m not just being punked, here are some better ways of generating entropy for yourself:
Take a picture with your cell phone and take the SHA512 hash of the resulting image file.  That will give you 4096 bits of high quality entropy.  If you want to be extra paranoid, find an old TV, tune it to a non-existant channel and take a picture of the screen.
Alternatively, make an audio recording of a radio tuned to an inactive FM channel (or of yourself saying “Shhhh…” for a couple of seconds) and take the SHA512 hash of the resulting audio file.
Much, much faster than rolling dice or flipping coins, and much more high quality entropy too.

@_date: 2015-12-24 10:50:18
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Bias in the dice is not the only consideration.  The requirements of casinos are very different from the requirements of someone who wishes to generate a secure cryptographic key.
Specifically, casinos have to *play to an audience*.  They need to convince the players that the game is fair, and they need to generate *drama*.  These are definitely *not* requirements for generating crypto keys.
Yes.  That’s a big clue that the requirements in the two cases are very different.  Biased dice are a show stopper for casinos, not for crypto keys.
Not really.  Doing a manual base conversion on a 256-bit number is a non-trivial task, and verifying a secure hash can be done by testing the implementation against published test vectors, or against another reference implementation.  I think the efforts involved are probably comparable.
Very true.
Yes, and those are exactly why it’s silly to use those techniques to generate crypto keys.  The requirements are different.  For casinos, theatre visibility is a requirement.  For crypto keys, it’s a disaster.
Another requirement for lottery results is the generation of drama.  So low bandwidth is a benefit for lottery results, but a liability for crypto keys.
Indeed.  But one of the requirements for generating crypto keys is bandwidth: the faster you can generate the key, the better — up to a point.  The difference between one second and 100ms probably doesn’t matter.  But the difference between 1 second and 10 seconds matters.  And the difference between 1 second and 100 seconds *definitely* matters.
If you want to generate a 128-bit key by flipping a coin, that will take you O(100 seconds) if you do O(1 flip/second).  If you use 16-sided dice you improve that by a factor of 4.  If you use six-sided dice you probably make the situation worse because you now have to do a base conversion.
By way of contrast, taking the SHA sum of a photo can be done in O(1 second), including the process of actually taking the photo.  Maybe O(10 seconds) if you need to transfer the photo from one device to another.  Faster, more convenient, and every bit as random as dice or coins.  Hence superior in every conceivable way for the generation of crypto keys.

@_date: 2015-12-25 14:16:12
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Of course.  But if you’re going to trust your computer to do a base conversion for you (and in Python no less!), why on earth would you not trust it to do a SHA sum?
In a casino, yes.  When generating crypto keys, no.  Generating crypto keys obviously must not be a publicly visible process.
I think you’re conflating public visibility of the *process* with public visibility of the *mechanism* that carries out that process.  In a casino, the former is public, but the latter is (typically) not (i.e. gamblers are not typically allowed to open up a slot machine to inspect the mechanism).  In crypto, the process of generating keys must not be public, but the mechanism by which the keys are generated might be.
Dual_EC_DRBG is a red herring in this thread.  What is under discussion here is the generation of random *seeds*.  (Go back and look at the subject line.)
So?  If you can’t keep an SD card physically secure, how are you going to keep the hardware performing your crypto secure?  (And, BTW, an SD card is actually very easy to erase simply by physically destroying it.  A hammer would probably do the trick.  Or a blow torch.  Or use a webcam plugged in to a USB port instead.  Or use an audio input of a white noise source instead of an image.  There are easy solutions here for any reasonable level of paranoia.)
The *dice* may be turtle-free, but the computational process downstream of the dice won’t be unless you’re going to start doing public key crypto and AES by hand.
That is not *a* big problem, that is *the* big problem.  And you are not going to mitigate that problem at all by using dice.
Indeed it is.
Making extraordinary efforts to remove the bias in dice or coins (or any other source of randomness) is silly (as is using photon beam splitters).  High bandwidth sources of randomness are readily available to anyone with access to the kind of hardware required to do crypto in the first place, and there are mathematical techniques for removing bias and extracting high quality entropy from low quality sources.  The combination of those two things is the Right Answer.  Yes, there are risks associated with that approach, but neither dice nor coins nor quantum mechanics will do anything to mitigate any of those risks.  If you’re going to do crypto, you have to do complicated math one way or another.  A little more math is not going to appreciably increase your risk exposure.

@_date: 2015-12-27 11:59:50
@_author: Ron Garret 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Concatenation would be more secure than XOR.  XOR can actually lose entropy if two of your sources are correlated.  Concatenation (followed by a hash) doesn’t have that problem.

@_date: 2015-12-30 08:14:06
@_author: Ron Garret 
@_subject: [Cryptography] Understanding state can be important. 
Hm, this is interesting.  How do I know I can trust kanguru?  I see some things here ( that look like yellow flags to me, e.g.:
• On-board AntiVirus by BitDefender*
• Remote Management Ready
A “secure” USB device that can be remotely managed?  And why would it need on-board anti-virus if it has a write-protect switch?
Seems a tad hinky to me.

@_date: 2015-07-24 16:02:55
@_author: Ron Garret 
@_subject: [Cryptography] Whitening Algorithm 
Wait, what???
RC4 is broken.  If Spritz âretains all the problems that RC4 hadâ doesnât that mean that Spritz is also broken?

@_date: 2015-11-01 08:26:27
@_author: Ron Garret 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
I think you meant âpathologiesâ, not âpathogens.â  A pathogen is a biological agent that causes disease.  If youâre going to use inflammatory rhetoric you should choose your words with care.

@_date: 2015-11-12 07:53:33
@_author: Ron Garret 
@_subject: [Cryptography] Post Quantum Crypto 
That is far from clear.  The D-Wave device is not a quantum computer, itâs a classical analog computer doing simulated annealing.  And itâs not even doing it all that well.
Whether the D-Wave technology can be some day extended to implement real QC is an open question, but Iâll give long odds against.

@_date: 2015-11-13 01:09:38
@_author: Ron Garret 
@_subject: [Cryptography] Post Quantum Crypto 
Disclaimer: I am not an expert.  All this is AFAIK/CT:
The number of measurements required  is not the limiting factor.  The limiting factor is the implementation of quantum error correction (QEC), and *combining* that with quantum gates.  Both quantum gates and QEC have been demonstrated in the lab, but combing them is really hard because the way QEC works is by splitting up a single q-bit into its own set of mutually entangled particles.  The more you split, the more robust your q-bit becomes, but the harder it becomes to use that q-bit to control a Hadamard gate.  Itâs far from clear whether the âsweet spotâ of q-bit robustness and Hadamard implementability can actually be achieved, and D-wave sheds no light on this because what they arenât implementing Hadamard gates.

@_date: 2015-11-16 00:40:54
@_author: Ron Garret 
@_subject: [Cryptography] Post Quantum Crypto 
Itâs the exact opposite.  The theory of QC is completely worked out, and the basic principles have all been demonstrated in the lab.  Itâs purely an engineering problem, but a really really hard engineering problem.  The reason itâs hard is that you have to control entanglement with great precision.  You need to build a system with a large number (hundreds or thousands) of mutually entangled degrees of freedom, none of which are entangled with anything else, and you need to maintain all of those entanglements as the system evolves through quantum gates.
A very rough analogy: take an old-style wooden pencil and balance it on its end.  Now try to balance another pencil on top of the first one.  If youâre very, very careful you can probably manage it.  Maybe even three.  But to do practical QC you need to make a tower of several hundred pencils, and it needs to stay balanced for a minute or two.

@_date: 2015-11-18 00:12:43
@_author: Ron Garret 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
You canât be serious.  Forcing people to update their software on pain of having their devices stop working basically puts the ultimate power in the hands of the device vendors.  How do you know if the update they can now force you to adopt is really more secure than the version you are running now?  Maybe between when you bought the device and when they force-updated it they made a secret deal with the NSA to install a back door.  I donât see how giving this power to the device vendor is any better than giving it to the government.  (And given the way society is progressing, these two things are becoming increasingly difficult to distinguish from one another.)

@_date: 2015-11-19 00:32:58
@_author: Ron Garret 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Because when I buy, I at least have the option of *trying* to ascertain exactly what it is that Iâm buying and choosing not to buy if I donât like what I find.  Forced update robs me of that option.
Auto update and forced update are not the same thing.  Iâm all for auto-update as an option, maybe even the default option.  But I think it is crucial to retain the ability to opt out without bricking your device. Thatâs the only leverage consumers have against vendor abuse after purchase because, as you yourself presciently observe:
Yes, of course.  But ceasing to operate altogether is a rather heavy-handed way of delivering a notification.
Amen to that.  (What *is* the matter with kids today?)

@_date: 2015-11-23 09:10:53
@_author: Ron Garret 
@_subject: [Cryptography] 
This is ironic:
[ron at mighty:~]â wget --09:08:54--             => `ISIS-OPSEC-Guide.pdf'
Resolving  23.235.39.239
Connecting to  connected.
ERROR: Certificate verification error for  unable to get local issuer certificate
ERROR: certificate common name `wired.com' doesn't match requested host name `
To connect to  insecurely, use `--no-check-certificate'.
Unable to establish SSL connection.

@_date: 2015-10-01 11:38:23
@_author: Ron Garret 
@_subject: [Cryptography] Insecure Chip 'n' PIN starts tomorrow 
The actual security model is: the vast majority of people are honest, and so the actual systemic cost of fraud is low relative to the cost of replacing the infrastructure.  Therefore it makes more economic sense to just use a risk-pool model to pay the cost of fraud rather than replace the infrastructure.
This model is valid in the short term, not in the long term.  Alas, resistance to long-term thinking is not limited to the banking industry nowadays.

@_date: 2015-10-12 08:38:28
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Usable Security Based On 
[Administrative note: Ralf sent his reply to the wrong list.  His original post was sent to both cryptography at metzdowd.com and crypto-practicum at lists.sonic.net.  My response was sent only to the latter, but Ralf’s response was sent only to the former.  I’m sending this response to both.]
OK, but then this doesn’t make sense:
The phrase “malware running on the user’s behalf” generally means malware that is running *on the same machine* where the secret is stored, and with the user’s (non-zero) UID.  So the code that took advantage of the heartbleed bug was not “malware running on the user’s behalf” because it was not running on the same machine.
In any event, I am still at a loss to understand the problem you are trying to solve and the solution you are proposing.  Heartbleed was just a bug in openssl.  It doesn’t matter what fancy games your secret keeping program plays with permissions or inode numbers; if it contains a bug that reveals the secret in response to a request that comes in over the network, then you lose.  It’s that simple.

@_date: 2015-10-20 15:04:52
@_author: Ron Garret 
@_subject: [Cryptography] Other obvious issues being ignored? 
I disagree that there is no good reason.  If you’re trying to go back through your records to find a particular piece of correspondence, having unencrypted Subject lines can come in awfully handy.
Search is an essential part of today’s workflows, but AFAICT searching encrypted contents securely is an unsolved problem.

@_date: 2015-10-21 17:00:34
@_author: Ron Garret 
@_subject: [Cryptography] multicollision resistance in protocol design? 
Intuitively it seems one should be able to construct a hash with strong collision resistance out of one with weak collision resistance by concatenating (or maybe even xoring) two HMACs with different keys.
This seems like the sort of thing that, if it were actually true, would be a known result.  Is it?  Or does it turn out that this doesn’t actually work?

@_date: 2015-10-21 17:34:02
@_author: Ron Garret 
@_subject: [Cryptography] Other obvious issues being ignored? 
You can affirmatively erase data in Lisp using destructive operations.
In Common Lisp in particular you can create arrays whose element type is unsigned-byte (with a specifiable width).  When you write into those, most modern compilers will produce store instructions that write data directly to memory in exactly the way that C does, at least when you remember to specify “volatile”.
The fact that there is GC is a feature because it adds fuzz which helps thwart timing-based side-channel attacks.
In fact, I’m going to go out on a limb here and claim that Common Lisp is actually a more suitable language for crypto than C.

@_date: 2015-10-25 14:27:47
@_author: Ron Garret 
@_subject: [Cryptography] composing EC & RSA encryption? 
Encrypting directly with RSA is a theoretical possibility that is taught in textbooks but AFAIK no one actually does it because it is fraught with all manner of peril.  Encrypting directly with EC is not even a theoretical possibility.  What is invariably done in practice is a Diffie-Helman key exchange (or the EC equivalent ECDH key exchange) followed by a symmetric encryption.
So yes, it’s possible to combine regular DH and ECDH into a composite key exchange algorithm that would stand up against an attack against either (but obviously not both).  The most straightforward construction would be to do the two key exchanges separately, then encrypt one key using the second key and use the result as the final key.  A simpler approach (like just XORing the two keys) might work as well because the keys are theoretically sampled from a PRF but I’d be leery of that without further analysis by someone who actually knew what they were doing.

@_date: 2015-10-25 14:33:59
@_author: Ron Garret 
@_subject: [Cryptography] letter versus spirit of the law ... UB delenda 
Common Lisp.  Seriously.  If you constrain yourself to a particular subset of Common Lisp then you can get constant-time bounds-checked code that doesn’t cons (and hence does not invoke GC) using the language as currently defined and compilers as they currently exist.  You can even layer a C-like syntax on top of it and make a crypto-specific C-like DSL without too much pain.

@_date: 2015-09-23 10:38:39
@_author: Ron Garret 
@_subject: [Cryptography] Non-Authenticated Key Agreement 
Another good rule of thumb when getting into crypto protocol design: if you find yourself saying âone-time padâ you have almost certainly made a mistake.  There is no security advantage to an OTP over a PRF.  The only difference between them is that an OTP has len(msg) bits of entropy where a PRF has a fixed amount of entropy.  But a fixed amount of entropy is the right thing.  Entropy is expensive and you donât want to use more of it than you actually need.

@_date: 2015-09-29 13:09:55
@_author: Ron Garret 
@_subject: [Cryptography] Future GPG/PGP 
Iâm working on this:
Thereâs also a version that uses a keyserver to make key management easier.  Itâs currently in private beta.  Contact me off-list if youâd like an invitation.

@_date: 2016-04-05 06:45:30
@_author: Ron Garret 
@_subject: [Cryptography] Secure universal message addressing 
SC4 ( is a simple open protocol based on TweetNaCl which uses Curve25519 keys as a userâs identity.  I have a prototype chat system working that uses a server to delivery messages, but extending this to a P2P model is pretty easy.  The encryption is end-to-end so it almost doesnât matter how messages are delivered.  You could publish them on github if you wanted to.  SC4 chat is currently in private beta, but if youâd like to try it drop me a line.

@_date: 2016-04-12 08:28:41
@_author: Ron Garret 
@_subject: [Cryptography] Show Crypto: prototype USB HSM 
One of the biggest challenges in crypto is protecting your keys against an attacker who pwns your machine.  The fundamental problem is that such an attacker can do anything you can do, including access hardware tokens that are connected to the machine.  Some hardware tokens have an input device built in (usually a push button, sometimes a fingerprint sensor) which needs to be activated before the token will operate, but these are still subject to phishing attacks.  In order to really be secure, a hardware token must have not just an input device, but a display as well so that information about the operation being authorized can be shown to the user in a way that is guaranteed to be out of the control of an attacker who pwns the host machine.
I did a market survey and could not find a device that met these requirements.  The closest thing I could find was the Trezor bitcoin wallet, but at $99 it seemed a bit pricey so I decided to roll my own.  The result is the SC4-HSM, a USB dongle with an STM32F405 processor (32-bit ARM cortex M4 with a built-in hardware RNG, 1MB flash, 192k RAM) and a 128-32 pixel monochrome Adafruit display.  It also has two user pushbuttons and two LEDs (though Iâm going to be changing that to a single tri-color LED).  It currently runs TweetNaCl, but thereâs a lot of headroom for more complex crypto.  Itâs also possible to swap the F405 for an F415, which has built-in crypto operations (AES, 3DES, various SHA hashes).  Both processors have hardware support for freezing a firmware load so that it cannot be overwritten, and so the contents of the flash cannot be read out even with physical access to the device.  The target market for these chips is medical devices and process controllers, and one of the requirements is to keep the firmware out of the hands of Chinese industrial espionage agents.
Photos of the prototype are attached.  Iâm about to do a small production run (O(10) units) which will cost about $50 each.  If anyone here is interested in obtaining one of these please contact me privately.
Iâm also actively recruiting a consultant to help with firmware development and auditing.
NOTE: The pins sticking up out of the case on the right end of the device are a hardware debug interface and will probably not be on the next round of prototypes.

@_date: 2016-04-12 10:41:08
@_author: Ron Garret 
@_subject: [Cryptography] Is storing a hash of a private key a security 
Why not just sign an empty string and verify that the signature verifies against the public key?

@_date: 2016-04-12 19:26:54
@_author: Ron Garret 
@_subject: [Cryptography] Show Crypto: prototype USB HSM 
This HSM is much more general-purpose than a U2F token.  It could be used as a standalone bitcoin wallet a la Trezor.  It can be used to decrypt messages and display them on the built-in display so that even an adversary with root accesss to your laptop couldnât read the cleartext.  The firmware doesnât support this yet, but itâs a mere matter of programming :-)
But even U2F tokens can be phished for some value of âphishedâ.  Itâs true that you canât extract the keys, but if an attacker owns your machine and you have a U2F token installed, the attacker can log into any site you can log into.  Even if the token has a button you need to push to activate it, itâs probably not hard to fool most users into pushing the button to authorize an authentication for an attacker.
With a display, the token can say, âYou are about to authorizeâ¦â and describe exactly what it is that it is being asked to do so that you know what youâre authorizing in a way that an attacker cannot control even with a completely compromised client.

@_date: 2016-04-13 09:40:47
@_author: Ron Garret 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
Everyone needs to choose their own risk posture, and different applications have different needs.  There are certainly people out there for whom Yubikeys are adequate, and for whom the SC4-HSM wonât make sense.  But I believe that there are applications and not-entirely-unreasonable risk postures for which a Yubikey might not be adequate.  If nothing else, having a programmable USB dongle with a display makes kind of a cool toy to noodle around with.
Tony: I really donât mind negative feedback when itâs constructive.  In fact, I very much appreciate it.  But Iâm really having a hard time discerning a constructive purpose in your critique.  What exactly do you think that I should be doing differently?  Change the design?  Give up and join you in being an evangelist for Yubikeys?  Something else?  I really donât get it.

@_date: 2016-04-13 10:14:28
@_author: Ron Garret 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
Ah.  OK, well, that is certainly doable, though how small you can make it is ultimately limited by the size of the display.  How small do you want it, and how much are you willing to pay?
Please keep in mind that the current SC4-HSM is just a proof-of-concept prototype developed on a shoestring budget.  Itâs more of an Apple-I than a Mac.  But you have to start somewhere.
Hereâs a photo of an earlier version of the HSM using a seven-segment display instead of the current 128x32 pixel OLED, next to the current version for size comparison:
Is that small enough for you?

@_date: 2016-04-13 10:20:29
@_author: Ron Garret 
@_subject: [Cryptography] Show Crypto: prototype USB HSM 
I didnât want to assume that *everyone* on this list was a professional cryptographer.  And it certainly does not appear to be common knowledge that a USB token needs to have built-in I/O in order to be secure against ownership of the client device.  In fact, it seems to be a rather controversial claim.  But if you thought my original post was inappropriate I apologize.
You can make the SC4-HSM secure against decapping by encrypting the keys with a pass-phrase.
In that case youâd need to obtain a new HSM.  But the SC4-HSM is ridiculously cheap, only about $20 in parts in single quantities, so replacing it is a viable solution.
That may well be, but I donât understand why you think itâs relevant.  Preventing firmware from being copied is a feature of sufficient value that hardware manufacturers are building it into their chips, and thatâs all that matters for this application.
Thank you for your feedback.

@_date: 2016-04-13 15:08:45
@_author: Ron Garret 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
The prototype SC4-HSM case is 57x21x9 mm.  The USB connector extends out 14mm beyond the edge of the case.  Itâs bigger than some portable USB devices, smaller than others.  Itâs almost exactly the same size as e.g. the MobileMate SD+ card reader:
The case is 3-D printed plastic so a production version could probably shave a couple of mm off of that.  There is also quite a bit of extra length on the board to support the debug connector that will be be taken of in the final design.
I abandoned the smaller design because the seven-segment display turned out to be inadequate for security.  4x7 segments gives you 28 bits of output at most (and realistically only 16 bits under non-geek UI constraints), and thatâs not enough.

@_date: 2016-04-13 17:08:46
@_author: Ron Garret 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
Hm, that is an interesting idea.  But I think itâs a little more than I want to bite off for version 1.
Only if you have an adversary that pwns your client machine *and* then obtains physical control of the device.  For either of these attacks in isolation, a non-secure input suffices.
Iâm not trying to protect against every conceivable attack, Iâm just trying to design an 80/20 solution (actually I think what I have is closer to a 99/1 solution, but itâs early yet).  One must always keep Munroeâs law in mind:

@_date: 2016-04-14 14:06:08
@_author: Ron Garret 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
You can re-flash it through the USB port.  In fact, the whole point of these prototypes is to let people experiment with the firmware.
I had not seriously considered building the USB directly onto the PCB like that.  I did consider a micro-USB socket instead of the A type connector, but decided that I like the aesthetics of a built-in connector better.  But these things are easy to change.

@_date: 2016-04-18 12:41:23
@_author: Ron Garret 
@_subject: [Cryptography] How to get certificates on email server? 
I presume that by âmy email programâ you mean an email *client*, not an MTA (like Postfix).  Which client are you using?  Because Iâve never seen a client that give you the option to accept a certificate without allowing you to look at it.  And what port are you connecting to?  The answer depends on whether you are using SMTP+STARTTLS (port 25) or SMTPS (port 465).  (See If youâre using SMTPS then you can get the cert using openssl.
If youâre using SMTP then I donât know the answer offhand.

@_date: 2016-04-18 12:45:31
@_author: Ron Garret 
@_subject: [Cryptography] 
=?windows-1252?q?s_Global_Decryption_Key_since_2010?=
In theory perhaps, in practice not so much.  There are a lot of ways around the exclusionary rule, including parallel construction ( and the good faith exception.

@_date: 2016-08-10 11:27:04
@_author: Ron Garret 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
A side-channel defense against an attack!  I love it!

@_date: 2016-08-21 22:43:12
@_author: Ron Garret 
@_subject: [Cryptography] Robust Linked Timestamps without Proof of Work. 
At least with cash you can be sure who you are giving the money to.  Bitcoin is more like leaving cash in a locker in a bus terminal and hoping you put it in the right locker.

@_date: 2016-08-23 23:05:33
@_author: Ron Garret 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
There are two fundamental problem with C.
The first is that the built-in arrays and string literals are unsafe.  You can build safe arrays and strings in C, but you can’t access them using operator[].  You have to access them (and modify them) using function calls.
The second is that C doesn’t have exceptions, which means that functions have to signal exceptional situations via their return values or via a global variable (like errno).  But C functions also can only return a single value of a single type, so you have to either use that return value to signal success or failure and stick the “real” returned value somewhere else (typically in a buffer, a pointer to which is passed in as an argument to the function), or you have to “overload” the return value so that certain values are real and others indicate exceptions.  And then, of course, you have to check that return value in the caller.
Because of these constraints, it is not possible to write safe C in a way that is “natural” to the language.  You have to put a safe layer on top of the native language.  That safe layer requires the programmer to adhere to some discipline in order not to undermine the safety.  But there is no standard on how to implement a safe layer, only different and mutually incompatible conventions.

@_date: 2016-08-26 00:42:23
@_author: Ron Garret 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
C has (at least) one serious flaw even when judged as an assembly language: the only way to do a transfer of control outside of the current function is to either call another function or return to the callee.  You can’t GOTO a label outside of the current function.  That makes it impossible to implement tail recursion in pure C unless you compile your entire program into a single C function.
This is a serious problem IMHO because entire generations of programmers have been raised to believe that function calls are fundamentally different from loops, and that for FOR(init; step; end) construct is somehow fundamental, that any language that doesn’t have this construct is weird and probably broken.  This single design flaw has probably done more to retard progress in programming language development than any other mistake in the history of computer science.

@_date: 2016-08-27 04:25:14
@_author: Ron Garret 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
No, it’s not.  Some languages, like Scheme and Haskell, require proper tail-call optimization as part of the language spec.  It’s not optional.  This is why very few compilers for non-C-like languages actually use C as an intermediate representation.  It’s too constraining.  There are exceptions, of course, but they all rely on various hacks.  There is in fact a whole branch of research devoted to the completely artificial problem of how to compile non-C-like languages to C.  If C were truly a good portable assembler, this would be a non-issue.
Yes, of course, though I think it’s a mistake to write it off as “just” culture.  C culture has become so prevalent that a majority of programmers aren’t even aware that there are viable alternatives.  Any language that doesn’t conform to the C culture is considered “weird” and not worthy of serious consideration.
Remember that my original comment was in response to the claim that C is a good portable assembler.  That’s only true if you’re using it as a target for a C-like language.  For a non-C-like language C is a terrible portable assembler.  The reason C is considered a good portable assembler is that many people think that C-like languages are all there is, or at least all that matter.
Yes, all this is true.  But debuggability train left the C station a long time ago.  The fact that there is so much undefined behavior in C, and that the standard allows a program with undefined behavior to do *anything at all* means that you pretty much can’t count on a C compiler to do anything reasonable under any but the most extraordinary of circumstances.  This is not to say that there aren’t C compilers that do reasonable things under a broad range of circumstances.  But you can’t count on it.
All of these are reasons why I dispute the claim that C is “a good portable assembler.”  C is a really cool hack that lets you do amazing things under very tight resource constraints.  But it’s not 1970 any more.  We don’t need to run our compilers on machines with 64k of RAM.  But for some reason we are still programming with a language designed primarily to meet that constraint.

@_date: 2016-08-28 00:10:00
@_author: Ron Garret 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
If you want to argue that C is “a good portable assembler” (which is the claim I was responding to) then these sorts of things matter.  You should not have to determine whether or not your code is going to blow the stack by disassembling the output of your assembler.

@_date: 2016-08-28 00:16:08
@_author: Ron Garret 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
You can’t be serious.  Remember, we’re not talking about the efficiency of the compiled code here, we’re talking about the efficiency of the *compiler*.  Arguing that we should use unsafe languages in order to conserve electricity is just ridiculous.

@_date: 2016-08-28 00:25:42
@_author: Ron Garret 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
“Clever” != “portable”.  Remember, the topic at hand is whether or not C is a “good portable assembler.”  It’s not.  It is true that C can often be pressed into service to do things it was not designed for given the right combination of cleverness and compiler-specific hacks, but that is a different question.

@_date: 2016-08-28 14:57:41
@_author: Ron Garret 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
The first half of my career I worked on autonomous mobile robots.  Back in those days (late 80’s, early 90’s) we didn’t have Raspberry Pi’s, we had 8-bit processors with typically 16k of flash and 1-2k of RAM.  If we were lucky we had a 68020 with 8MB of RAM.  We programmed those systems in Common Lisp.  On the 68020 systems we ran Lisp directly, and on the 8-bit processors we wrote code in Forth and in DSL’s whose compilers were written in CL because writing a compiler in CL was (and still is) really easy.
Yes, a portable assembler is a very handy thing.  But C is not the only option.

@_date: 2016-12-22 12:58:53
@_author: Ron Garret 
@_subject: [Cryptography] USB hardware token for $2?? 
What would make it “secure” then?  If all you want is a place for storing shares of secrets, why not just use a thumb drive?
To my mind, the defining feature of an HSM is that the keys are generated by an on-board HWRNG and never leave the device (except perhaps in passphrase-encrypted form).  All the crypto operations performed using the keys are also performed on-board.  You also need some on-board I/O.  If you don’t have that, then you need to secure whatever is on the other end of the communications channel that you use to communicate with the HSM, and if you can do that then you don’t need an HSM.
You can do crypto on a Teensy3 ($20 retail) or a Raspberry Pi Zero ($5 retail).  You can even turn them into an HSM by adding some on-board I/O.  But once you do that what you end up with will look an awful lot like this:

@_date: 2016-12-22 22:20:16
@_author: Ron Garret 
@_subject: [Cryptography] USB hardware token for $2?? 
That is a very good question, to which I have three answers.
Answer  at the moment the device doesn’t know it’s you, but it doesn’t matter because before it does any cryptographic operation it illuminates the LED to let you know it needs attention, displays what it is about to do on the display, and waits for you to push one of the built-in push buttons before it will proceed.
Answer  I am working on an ssh-like protocol for communicating with the device.
Answer  I’m working on a new version of the device that has a female USB plug on the other end into which you can plug a keyboard.

@_date: 2016-12-22 22:24:31
@_author: Ron Garret 
@_subject: [Cryptography] [FORGED]  USB hardware token for $2?? 
Unfortunately, that is not true.  To be secure, the device needs its own on-board I/O of some sort.  If it doesn’t then it is not possible to draw a security perimeter around the device.  If you can access the device from your laptop, then so can an attacker who pwns your laptop.  (And if you are convinced that your laptop is secure, then you don’t need an HSM.)
The reason the SC4-HSM exists is that I was originally going to use a Teensy3 (which is an awesome little piece of hardware, BTW) but then realized that I could not secure it against an attack launched from the machine it is plugged in to.
You are correct that the software is the hard part.  But unfortunately you do need at least a little bit of help from the hardware.  Blame Alan Turing.

@_date: 2016-12-23 12:51:43
@_author: Ron Garret 
@_subject: [Cryptography] USB hardware token for $2?? 
Much more than a keylogger.  It has a display, so you could use it as a completely standalone computing system with no host.  It has specs comparable to an early IBM PC.  You could probably run FreeDOS or CP/M on it if you really wanted to.  It already runs TinyScheme.

@_date: 2016-12-26 05:46:24
@_author: Ron Garret 
@_subject: [Cryptography] Are there random sources? 
In the context of cryptography the answer is very straightforward: these are the wrong questions.  The right question is: how do you generate N bits of data that your adversary cannot guess with odds better than chance.
That is the perceived wisdom, but it is far from clear.  See, e.g.:
"Unfortunately the corresponding convergent series converges very slowly. That is, getting the value to any useful precision requires so many terms, that his solution is of little practical use. Indeed, in 1930 David Beloriszky calculated that if Sundman’s series were going to be used for astronomical observations then the computations would involve at least 10^8000000 terms.”
(To put this number in perspective, the number of elementary particles in the universe is about 10^80.)
True, but again that doesn’t matter.  As long as your adversary can’t carry out the required calculations (and they can’t) then that’s good enough.
The free will theorem is a red herring.  It is a technical result in QM that has to do with certain assumptions about entanglement experiments and it has (almost) nothing do with quantum randomness.  That the outcomes of quantum measurements are truly random (i.e. they cannot be predicted even with infinite computing power and complete knowledge of the prior state of the universe) has been well established for decades.  It is completely un-controversial.  (It is also completely irrelevant to cryptography because cryptography does not need quantum randomness.)
I’m pretty sure Conway was being glib.  Not only is it possible to understand QM, it isn’t even particularly difficult if it’s explained properly.  The problem is that QM pedagogy is hopelessly entangled (pun intended) with the Copenhagen interpretation, which is demonstrably wrong.  Copenhagen is a reasonable approximation to the truth in most cases, but it is not the truth.  If you want to go down this rabbit hole, read this:
or watch the movie version:
(If you want to comment on the content of these links, please don’t do it on this list because this is definitely off topic.)
Absolutely.  But more to the point, remember that what really matters is not randomness per se, but generating data that your adversary cannot guess.  Randomness is one way to do that, probably the best way to do it, but it is absolutely *not* worth worrying about whether or not your “random” numbers are “really” random because that doesn’t matter.  All that matters is unguessability.

@_date: 2016-12-28 13:31:43
@_author: Ron Garret 
@_subject: [Cryptography] where shall we put the random-seed? 
What about putting it in the bootloader configuration and passing it to the kernel as a boot-time argument?

@_date: 2016-02-05 09:03:07
@_author: Ron Garret 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
I have long considered this one of the strongest arguments for switching to ECC, and curve25519 in particular because you donât even need to validate the curve point.

@_date: 2016-02-05 17:39:45
@_author: Ron Garret 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
So does Rabin-Miller.
The Socat âprimeâ is not remotely close to being a pseudo-prime even after you take out the small factors.  The only way it could have been more obviously non-prime was for it to have been an even number.

@_date: 2016-02-11 13:43:07
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
Or maybe:
Ciphertext = E(hash(counter+key) XOR plaintext, key)
Something about the (counter XOR Plaintext) makes me queasy, though I canât offhand put my finger on what it is.
But do you really need non-malleability at the level of an encryption block?  Why is it not enough to use an authenticated encryption mode with the authenticator applied to a disk block?  That would seem to me to defeat the malleability attack youâre suggesting while maintaining random access.

@_date: 2016-02-11 15:41:38
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
Aha, I just figured out what made me queasy about it.
If part of my cleartext happens to be a numerical sequence, then simply xoring with a counter will expose that because the result will be a sequence of identical blocks.  If I xor with a keyed hash of the counter then the probability that this will happen is negligible.

@_date: 2016-02-16 13:42:29
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
I donât think so.  You have an iterated encryption E(E(â¦)).  That looks like a serial dependency to me.

@_date: 2016-02-17 17:09:05
@_author: Ron Garret 
@_subject: [Cryptography] Apple ordered to decrypt cellphone 
Then it seems to me that this is not a question of âhackingâ, this is a question of data recovery.  I presume the county government, i.e. the owner of the phone, is consenting.  If so then (AFAICT, IANAL) this is no different than if someone walked into an Apple store with an iPhone and said, âI canât remember my pass code but I have some really important data on this phone that I need to recover, and Iâm willing to pay whatever it takes to get it back.  Can you help me?â  I donât see how it is reasonable to refuse that request on *privacy* grounds.

@_date: 2016-02-21 09:22:19
@_author: Ron Garret 
@_subject: [Cryptography] OCR'd DOJ Motion to Compel Apple 
George Orwell would be proud.
"Behind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.â

@_date: 2016-02-25 17:18:22
@_author: Ron Garret 
@_subject: [Cryptography] Hope Apple Fights This! 
I did an analysis of this about 20 years ago:
Plus ca change, plus câest la meme chose.

@_date: 2016-02-25 18:15:22
@_author: Ron Garret 
@_subject: [Cryptography] Response to "I don't have anything to hide" 
I usually say: maybe the government is trustworthy *now* but how do you know it will still be trustworthy tomorrow?  (Nowadays I sometimes add, âor after the next election?â)
Then I point them to this book:

@_date: 2016-02-26 10:56:02
@_author: Ron Garret 
@_subject: [Cryptography] USG v. Apple, 
Why?  Lying to the public (and even to Congress) seems to be standard operating procedure for the executive branch these days.

@_date: 2016-02-26 10:58:34
@_author: Ron Garret 
@_subject: [Cryptography] USG v. Apple, 
Youâre probably right about that, but this would not be an indefensible position.  The government itself classified crypto as a weapon during Crypto War I in the 90s.

@_date: 2016-02-29 14:21:04
@_author: Ron Garret 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Just posted this, targeted more towards the general public than the people on this list, but I would appreciate feedback (and maybe an upvote on Hacker News):

@_date: 2016-01-02 16:47:16
@_author: Ron Garret 
@_subject: [Cryptography] Any Electrical Engineers here who know about 
But it does matter.  That’s what makes it secure.
Yes.  In a linear system.  XOR is non-linear.

@_date: 2016-01-03 08:58:35
@_author: Ron Garret 
@_subject: [Cryptography] Any Electrical Engineers here who know about 
Then you and I mean different things by “linear” then, in this context.
“Linear” in the context of signal processing means that the spectrum of the sum of two signals is the sum of the spectrums of the individual signals.  That is not true for XOR.  Linearity in this sense is the foundational assumption of the theory of filters.
There are non-linear filters ( but "nonlinear filters are considerably harder to use and design than linear ones, because the most powerful mathematical tools of signal analysis (such as the impulse response and the frequency response) cannot be used on them”.  And in the case of XOR, nothing works.  It is the ultimate non-linear function with respect to filtering (That’s why it’s information-theoretically secure).  The spectrum of the output (the “ciphertext”) is identical to the spectrum of the key and bears no relationship to the spectrum of the plaintext.
An interesting consequence of this is that it is meaningless to distinguish between the ciphertext and the key.  They are completely interchangeable.  In fact, one potential “practical" use of a one-time pad is providing plausible deniability to people who want to distribute illegal content (e.g. copyrighted material).  If I send you an OTP key and you use that to encrypt a copyrighted work and send me the result, then in order to prove that you sent me the pirated content and not vice versa the plaintiff would have to prove that I sent you the key *before* you sent me the output.  Otherwise you could plausibly claim that what I sent you *was* the output and not the key (because the output and the key are indistinguishable after the fact).  This protocol can be extended so that all parties have plausible deniability even if the authorities have access to *all* of the communications between the parties.  All parties can plausibly claim to have done nothing but publish OTP keys, i.e. strings of random bits.

@_date: 2016-01-09 14:42:00
@_author: Ron Garret 
@_subject: [Cryptography] LED DICE -- random??? 
Looks like just the ticket for the most demanding cryptographic applications.
Or maybe these:

@_date: 2016-01-14 14:36:33
@_author: Ron Garret 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
IMHO the right approach is to not worry about the quality of your noise source and just use a whitener with a large margin of safety.  Feed 1000 raw 10-bit ADC readings into SHA512 and you’re almost certainly secure against any attack.  If you want to be super duper paranoid, run some basic sanity checks on the raw input, like make sure that the standard deviation of your samples is >>0.

@_date: 2016-01-15 09:05:56
@_author: Ron Garret 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Even in that case why not give yourself a comfortable safety margin?  Are there really applications that require you to squeeze every last bit of entropy out of a noise source?

@_date: 2016-01-15 17:23:34
@_author: Ron Garret 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Well, yeah, obviously.  But the point I’m trying to make is that things like this:
are overthinking the problem.  You shouldn’t even try to engineer something that will “randomly flip” between two values.  Just run a noise source directly into the ADC, sample it 1000 times or so over the course of a second, and shove the whole bucket o' bits through SHA3.  For extra safety, check that the mean and variance of the signal look reasonable. For extra extra safety, check the FFT and make sure it’s not too peaky.  For extra extra extra safety take two successive FFTs and make sure they’re different.  No need to do a lot of fancy shmancy electrical engineering.  In fact, you can probably get pretty reasonable results with no circuitry at all and just letting the ADC pin float (I’ve tried this on a Teensy3 and it actually works quite well).

@_date: 2016-01-16 11:10:26
@_author: Ron Garret 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Yes, of course.  I would not recommend running a floating ADC for anything really mission critical.  But even for mission-critical keys you don’t have to work very hard.  Generating noise is easy.  Generating high quality noise may be hard, but that’s my whole point: you don’t need high quality noise in order to generate perfectly secure keys in a reasonable amount of time.
I can’t speak to the Arduino.  Maybe the Arduino really does suck.  An 8-bit ADC might really be fairly immune to environmental noise.  But the Teensy3 (retail price $20) has a 16-bit ADC that can operate at at 20KHz.  So in 100ms I can generate 32,000 raw bits.  If I boil that down to a 256-bit key I need less than 1% entropy to be perfectly secure.  Also, with a 16-bit ADC you would have to work incredibly hard to keep noise out of the system.  Here’s an experiment someone did on a Teensy3.2, which has two ADCs:
They tied the two ADCs together, so they are seeing the same voltage.  There’s enough noise just in the ADC system itself that the mean error between the two readings is 0.02%.  That’s 2-3 bits of entropy per sample.  That’s enough to generate a 256-bit key in 10ms.  And that’s what you get when you’re not even trying to introduce noise (indeed, when you’re trying to keep the noise out!)
That’s possible, though by the time the AC goes through the wall transformer, the laptop battery, and the USB port, it’s probably pretty clean.  But what difference does it make where the noise is coming from as long as there’s actually some noise?
You know, if you poke yourself in the eye hard enough, it will hurt.  The solution to this problem is not, it seems to me, to wear safety googles all the time, but rather to simply avoid poking yourself in the eye.
Yes, if you go to extraordinary efforts to shield your system from all external sources of noise, you might be able to get to the point where your ADC doesn’t measure any noise (though if you have a 16-bit ADC even that is doubtful).  But why on earth would you want to do that?

@_date: 2016-01-16 17:29:53
@_author: Ron Garret 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Just to put this issue to rest, I did this experiment.  I plugged the Teensy3 into my laptop (a Macbook Pro Retina) and collected ADC samples with the laptop plugged in and unplugged, as well as with the teensy unshielded, and shielded in several layers of aluminum foil.  I ran the resulting data through some basic statistical tests.  The mean and standard deviation of the data were the same in all four cases (400 and 45 respectively).  A histogram looked normal to casual observation, and there were no obvious peaks in the FFT.  I ran these tests on multiple sequences of 1024 data points collected in a tight loop, which took about 500 ms, so this is far from the fastest that the ADC is capable of going according to the book.  My guess is that the limiting factor is the analogRead library routine.  I don’t have the source code for that, so I can’t tell what is actually going on in there.
So I think two bits of entropy per sample (i.e. 4 kilobits per second) is a very conservative estimate of what a floating analog input on a Teensy3 provides without even trying hard.  That should be more than adequate for all but the most demanding applications.
A plot of a typical sample is attached.

@_date: 2016-01-19 12:34:12
@_author: Ron Garret 
@_subject: [Cryptography] Design of a secure hardware dongle 
I’m working on a design for a minimalist secure hardware dongle.  The goal is to have it be usable as an HSM for the secure storage of secrets.  I have a prototype running on a Teensy3, but I’ve come to the conclusion that in order to really be secure there has to be some I/O on the dongle itself.  Hence, I am commissioning a new design that is essentially a Teensy3 with the addition of an OLED display and two push-buttons.  It will also have an on-board noise source for key generation.  The resulting device will be very similar to the Trezor, but not designed specifically for BitCoin.  I expect to be able to sell them for about $50.
If anyone here has an interest in such a device and would like to see a feature that I have not listed please let me know.

@_date: 2016-01-25 11:01:13
@_author: Ron Garret 
@_subject: [Cryptography] Design of a secure hardware dongle 
The chip we’re using was chosen in part because it has a hardware lock-down feature specifically designed to defeat such an attack:
It’s designed to keep the Chinese from stealing proprietary code, but it works for keys as well.  But if you’re still worried that an adversary is going to, say, do a direct hardware probe of the EEPROM, you can add a layer of security in software by encrypting the keys with a pass-phrase.  And if you really want to batten down the hatches you can have the device erase the keys after N unsuccessful attempts to decrypt them, or (for you Mission Impossible fans out there) if the correct pass phrase is not provided within a certain amount of time.

@_date: 2016-06-30 23:02:29
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
No.  Life is too short to argue with angle trisectors.

@_date: 2016-07-06 19:47:47
@_author: Ron Garret 
@_subject: [Cryptography] What to put in a new cryptography course 
The evidence may be thin, but the argument seems compelling to me: the more complex a system is, the more possible places there are for vulnerabilities to hide.

@_date: 2016-07-08 16:11:21
@_author: Ron Garret 
@_subject: [Cryptography] What to put in a new cryptography course 
It’s the elliptic curve discrete logarithm problem:
(I feel like I must be misunderstanding your question because surely you knew that already?)

@_date: 2016-07-11 11:23:58
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Unless someone manages to prove that P != NP we have no other choice but to make our peace with this.

@_date: 2016-07-12 09:05:50
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
It does have a bearing.  It is just not the straightforward one that you imply.  (That’s a very good explanation of P=NP, BTW, probably the best I’ve ever seen.)
The reason P?=NP matters is not because we want crypto algorithms that are in NP.  The reason that it matters is that what you asked for is not an *algorithm* but a *proof*:
(Emphasis added)
What you asked for is not a crypto algorithm whose complexity is X^20, what you asked for is a *proof* that there exists no algorithm that computes the same function in less than X^20.  Such a proof is a very different beastie than an algorithm.  (It’s actually even worse than that, because to prove security, proving that there is no equivalent algorithm is not enough.  You have to prove that there does not even exist a *probabilistic approximation* to the algorithm that runs in  Even many computer scientists do not seem to appreciate how different the world would be if we could solve NP-complete problems efficiently. I have heard it said, with a straight face, that a proof of P = NP would be important because it would let airlines schedule their flights better, or shipping companies pack more boxes in their trucks! One person who did understand was G ̈odel. In his celebrated 1956 letter to von Neumann (see [69]), in which he first raised the P versus NP question, G ̈odel says that a linear or quadratic-time procedure for what we now call NP-complete problems would have “consequences of the greatest magnitude.” For such an procedure “would clearly indicate that, despite the unsolvability of the Entscheidungsproblem, the mental effort of the mathematician in the case of yes-or-no questions could be completely replaced by machines.”

@_date: 2016-07-14 09:42:33
@_author: Ron Garret 
@_subject: [Cryptography] The Laws (was the principles) of secure 
I vehemently disagree with this.
Security is only meaningful with respect to a threat model.  Something can be completely secure against a casual hacker and totally insecure against a nation-state with a supercomputer and the ability to decap chips.  Moreover, the boundaries of the “something” that is secure (or not) with respect to a threat model can vary widely.  These boundaries can be physical or virtual.  I can draw a security perimeter around my account, around my computer, around the safe in my basement, around my house, around my company, around my cage in the datacenter, around the datacenter itself, around my nation-state…  Furthermore, the effort that you put into making an asset more or less secure has to be weighed against the value of the asset.  I care a lot more if someone breaks in to my on-line bank account than if they break in to my Reddit account.
Law 11 is not merely wrong, it is *dangerously* wrong, the exact opposite of what we should be telling people.

@_date: 2016-07-15 16:58:19
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Yes, all true.  I confess I was being a bit glib.  Let me try this again.
You want a proof of security.  Just trying to write down what the *claim* would look like formally is tricky, but I don’t want to get lost in those weeds.  Lets just presume that we can formulate some formal property of an algorithm A that we call SEC which conforms to what we want.  If SEC(A) is true, algorithm A is secure.
So there are four possibilities with respect to any particular algorithm A:
1.  SEC(A) is false (and hence, obviously, no proof exists)
2.  SEC(A) is true but nonetheless no proof exists.  (Godel sentences are not necessarily the only unprovable truths.)
3.  SEC(A) is true, and a proof exists, but we can’t find it because it’s too big to fit in the observable universe, or too long to enumerate before the heat death of the universe, or something else fundamental like that
4.  SEC(A) is true, a proof exists, and it is feasible for us to find it (by which I mean that writing the proof down would not violate the known laws of physics, not that it is within the mental capacity of humans to find it)
If what you care about is security then any of these situations is acceptable except So let’s go back to your original statement:
I took this to mean: we should work harder on finding proofs of security so that we don’t discover the hard way that we are in situation But even in situation  there are three possibilities
1a.  SEC(A) is false, and hence an attack exists, but it is not feasible for us to find it
1b.  SEC(A) is false, and an attack exists, and it is feasible for us to find it, but it isn’t a practical attack because the attack is still too expensive to actually carry out
1c.  SEC(A) is false, an attack exists, it is feasible to find, and it is practical
Again, if what you care about is security, then any of these situations is acceptable except 1c.
So what we are talking about here is not so much *being* secure as *knowing* we are secure (or at least knowing we are immune from a certain class of nasty surprises.  A proof won’t protect you from a monkey-wrench attack.)
There are three possible reasons we don’t have proofs:
1-3. We are not in situation 4
4a.  A proof exists, finding it would not violate the laws of physics, but finding it is infeasible given our current mental and technological limitations
4b.  A proof exists and we can find it, we just haven’t tried hard enough
Your admonition that "the cryptographic community is ... too comfortable” with the absence of proofs is only constructive in situation 4b.  We don’t know whether or not we are in situation 4b.  One way to find out is to try to find proofs.  If we succeed, then obviously we are in 4b, but if we fail (the situation we are in fact facing) then what?  Should we just keep trying, or should we perhaps re-focus our efforts to try to ascertain if perhaps we are in some situation other than 4b (like, maybe 1c)?
The quip about someone finding a proof that P!=NP was really intended to be a shorthand (or maybe a metaphor) for someone making a major breakthrough in this kind of meta-mathematical problem of ascertaining whether or not a proof of the kind you are calling for is even possible.  The point I was trying to make was simply that until and unless someone makes *that* kind of a breakthrough we may have no choice but to make our peace with having a lot of empirical evidence that SEC(A) is true, just as we have a lot of empirical evidence that P!=NP, but never knowing for sure.

@_date: 2016-07-20 12:14:14
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
It turns out that there is reason to believe that this is in fact the case.
This is also a real possibility:

@_date: 2016-07-22 08:00:59
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Yes, I agree with everything you say.  But you’re still missing my point: the crypto systems we are using are not Turing-equivalent (they are finite-state machines), but the systems we use to *prove* things about them *are* Turing-equivalent.  It might be the case that the only way to prove that a crypto system is secure is to exhaustively enumerate all possible algorithms and eliminate each one as a candidate attack by actually running them on all possible keys.  This of course doesn’t even begin to touch on any “interesting” problems in complexity theory like busy-beaver numbers or Kolmogorov complexity, but it would nonetheless be intractable.  The question of whether or not it is the case that this is the only way to prove this result *is* a question of Turing-equivalence because it is a question about proof systems, not a question about crypto algorithms.

@_date: 2016-07-23 08:25:10
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
We seem to be using different definitions of the word “intractable.”  When I say “intractable” don’t mean intractable in principle, I mean intractable in practice, in *this* universe, on *this* planet, in the face of actual technological and physical limitations.  Cryptography matters, not because the math is beautiful, but because it addresses contemporary human needs and desires.
I’ve tried to explain this twice now, I’ll try one more time.
There are two distinction questions on the table:
1.  Given a cryptographic algorithm, does an attack exist?
2.  Can we *prove* that the answer to question 1 is “no”?
Question 2 can be interpreted in two different ways:
2a.  Can we *in principle* prove that the answer to question 1 is “no”?
2b.  Can we actually generate such a proof under contemporary physical, technological and economic constraints?
The answer to question 2a is obviously “yes” because, as you say, the problem is finite and we can in principle enumerate all the possibilities.  But no one in their right mind cares about this.  What matters is whether someone anyone can *actually* attack your algorithm.  I don’t care if God knows how to attack my crypto, I care if there’s a *human* who knows how to attack my crypto.
All else being equal I would, of course, love to have a proof that even God cannot attack my crypto.  But all else is not equal.  We live in a world with constraints, and we have to decide how to allocate limited resources.  Should we spend our effort looking for proofs, or should we spend it doing something else?
It is *that* decision which can be informed by the theory of Turing machines because proofs are Turing-complete.  And, BTW, it is *that* decision that could be informed by an answer to the question of whether not P?=NP because that answer would almost certainly contain some deep conceptual breakthrough. Because of the deep connection between this question and proof theory, it would almost certainly give us some insight into how to make the quest for a proof more fruitful, or tell us that the effort is indeed hopeless.  At the moment we just don’t know, and so there is no principled basis for making these decisions.  All we have are some data points with no theory.  Under those circumstances, everyone needs to choose their own risk posture.

@_date: 2016-07-26 08:26:19
@_author: Ron Garret 
@_subject: [Cryptography] Code is Cruel -- The DAO 
You’re missing something very important: double-entry bookkeeping is not just an error correcting mechanism, it’s a more accurate conceptual model of what’s really going on in the underlying financial system.  When you do single-entry bookkeeping you are necessarily keeping track of only part of the real situation.  When a consumer balances their checkbook, they keep track of money coming in from their employer and going out to merchants, but the employer and the merchants are black-boxes to the consumer.  A complete model of the situation necessarily includes the employer and the merchant, and necessarily entails double-entry, not to correct errors, but because every time money changes hands it leaves one account and ends up in another.
Even at the consumer level, double-entry bookkeeping is a very valuable tool that is seldom actually used.  For example, suppose you sign up to take a cruise and pay a deposit on your credit card.  You have now accrued two debts.  One of those is a short-term debt that shows up on your credit card statement, and the other is a long-term debt that shows up on the invoice for your cruise.  You really ought to record those somewhere, but if all you have is a checkbook register there’s no way to do it because your checking account is a cash account and you can’t record debts in a cash account without screwing up the balance.  So you need a separate account to record debts, which credits against the cash in your checking account and debits against your net worth.
Even cryptocurrencies have an implicit double-entry system because because you need to keep track of the total money supply *somewhere* even if you don’t do so explicitly.  (This is one thing that most people don’t understand about sovereign debt: it really is different from consumer debt because sovereign debt is a measure of the money supply.  The underlying reason that people don’t understand this is that they don’t understand double-entry bookkeeping.)

@_date: 2016-07-26 11:09:01
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
And this is indeed the case.  It might not be formally undecidable in principle, but it might be actually undecidable under the physical or economic constraints of this universe.
Yes, obviously all finite problems are decidable in principle.  Why do you think that’s relevant?  Formal decidability is not the issue here.  The issue is whether or not it is worth spending scarce resources looking for proofs.  Just because a question is formally decidable doesn’t mean it’s worth pursuing the answer.  The question of whether or not the Goldabch conjecture is true for all even integers less than Graham’s number is formally decidable, but it would be the very definition of foolishness to pursue the answer in the straightforward way.
You’re attacking a straw man here.  I never said that it would.

@_date: 2016-07-27 10:13:57
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
You didn’t include any context so it’s hard to know exactly what you’re referring to here, but I think you’re attacking a straw man.  All of the references to brute-force in this discussion have been to brute-force searches for *algorithms*, not brute-force searches for keys.
The argument has gone like this:
Arnold: We should not be content with the lack of proofs of security.
Me: We may have no other choice unless (and here I insert my tongue slightly into my cheek) we first make progress on P?=NP
Arnold: P?=NP is irrelevant because crypto algorithms are not NP-hard
Me: P?=NP is relevant not because crypto algorithms are NP-hard (Arnold is right — they aren’t) but because finding a *proof* might be impossible without a conceptual breakthrough of a similar magnitude to answering P?=NP.
Arnold: Finding a proof cannot possibly be NP-hard either.  In fact, there is a *finite procedure* for finding a proof: simply enumerate all possible algorithms that halt in time <N where N is the cost of a brute-force attack and see if any of them break the cipher with probability greater than chance.  (N.B.: not only do you have to enumerate all algorithms, but you have to run them all against all possible keys.  But it’s all finite, so we’re in FSA-land, not Turing-land, and so everything is decidable.)
Me: That this procedure is finite is irrelevant.  This universe is also finite in both time and space (assuming our current understanding of physics and cosmology is correct).  You can do the math if you want to, but I guarantee you that finding a proof of security by brute force in this universe is completely hopeless.  Finding a proof by some other means may or may not be hopeless, we just don’t know.  The fact that we don’t know might be a result of our complacency, but it might be a consequence of the laws of physics.  We don’t know that either (that’s really the P?=NP question).  You can lather-rinse-repeat that line of reasoning to generate an infinite tower of ignorance and meta-ignorance and meta^N-ignorance.  Breaking that cycle probably requires, at the very least, a conceptual breakthrough unrivaled in the history of human intellectual endeavor.
This is not to say that people ought not to look for proofs of security.  If proofs are your thing, more power to you.  I wish you all success.  All I’m saying is that people ought not to be admonished for considering the above argument and then deciding to direct their efforts elsewhere.

@_date: 2016-07-27 11:00:50
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Sorry, I’m not following this.  A proof and an algorithm are not the same thing.  I don’t see how a cipher algorithm is a “proof that particular numbers have particular properties relative to the boolean equations that comprise the cipher algorithm.”  Can you please explain that?
No, you don’t have to wait forever.  You don’t need to run an algorithm forever to know that it’s not an attack.  You can stop once it has run as long as a brute-force attack.  It really is a finite problem.  The halting problem really does not come into play.  It really is just about the fact that the cost is larger (by a huge margin!) than the computational power of our universe.

@_date: 2016-07-28 10:48:53
@_author: Ron Garret 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Yes, but the point of that paper was not that the machine was undecidable.  The point was that this could be done with a machine that had a small number of states (<8000).
The real point of the paper was that you could get into deeply unknown mathematical territory with very simple machines.  In addition to the undecidable machine they also exhibited machines with even fewer states corresponding to the Goldbach conjecture and the Riemann hypothesis, two notorious unsolved problems.  The point being that once you get into this level of complexity you are solidly in the realm of the mathematically unknown, and therefore possibly in the realm of the mathematically unknowable.  We don’t know.  But now we know that once a machine has more than 7917 states it, in general, impossible for us to know.
So the question on the table now is: can a TM that does an exhaustive search over *algorithms* be constructed in fewer than 7918 states.  If not, then, in the absence of a major breakthrough, the only way for us to know the result of running such a machine is to actually run it.
Note that the fact that the exhaustive search is finite is irrelevant.  What we care about in this case is not whether or not it halts — we know that it will.  What we care about is the *answer*, i.e. whether or not an attack was found.  But it is easy to convert *this* question into an instance of the halting problem: let the TM that conducts the exhaustive search and outputs YES or NO be machine M1.  Construct a new machine M2 identical to M1 but replace the code that outputs “NO” with an infinite loop.  Now M2 halts IFF M1 outputs YES.  So the question of whether M2 halts is equivalent to the question of what answer M1 produces.  If M2 has more than 7918 then this question is generally undecidable.  We happen to be able to compute an upper bound on the running time of M2 (lets call it N), and so we can determine that M2 does not halt if it runs for longer than N steps, but it might be the case that there is NO OTHER WAY to answer the question.  Finding a way to answer the question more efficiently than running T2 for N steps would (almost certainly) be a major mathematical breakthrough.
There are other ways to potentially attack this problem, like finding a TM that did an exhaustive search over algorithms in <<7918 states.  But that seems to me unlikely to be fruitful.  We don’t actually know where the boundary between the mathematically knowable and unknowable actually is, we just know it’s less than 7918 states.
Sure.  The fate of nations also hinges on the availability of cheap energy, but that’s not a good reason to start a research program into perpetual motion machines.
Fair enough.  But the misunderstandings of the relevance of P?=NP are more widespread than that.  It seems to be an equally widely held belief that if “A because B” is false then A must be false.  (Here A = “P?=NP is relevant” and B = “If P=NP it would let us break crypto algorithms because crypto algorithms are NP-hard”.)

@_date: 2016-07-29 10:55:49
@_author: Ron Garret 
@_subject: [Cryptography] The cost of exhaustive search for crypto attacks 
============================== START ==============================
On a lark I decided to actually do this calculation.  This is not intended to be a rigorous analysis, just a rough back-of-the-envelope exercise, mainly for fun.
Deciding what constitutes an effective attack is something of a judgement call.  In the literature, attacks get attention if they reduce the key space at all, even if it’s just by a few bits.  But what really matters is not extent to which the search space is reduced, but rather the absolute size of the resulting search space.  An attack that reduces the search space of a 256-bit cipher by 100 bits is still less effective than an attack that reduces the search space of a 128-bit cipher by 1 bit.  So instead of focusing on the amount of reduction, let us simply pick a number k such that a search space of <2^k is insecure, and a search space of >2^k is secure.
Let us further assume that we can ascertain whether or not a particular algorithm reduces a cipher’s search space in 2^k steps by running it for 2^k steps.  This is a questionable assumption, but it errs on the conservative side.  It essentially means that we evaluate a candidate attack by running it on a single ciphertext and examining the result.  In reality evaluating a candidate attack is almost certainly going to be more expensive than that.
In order to prove that no attack exists by exhaustive search we therefore need to enumerate all algorithms and group them into equivalence classes according to their behavior in <2^k steps.  The only way to do that is to enumerate all Turing Machines with <=2^k states.  The reason for this is that TM equivalence is undecidable (because it reduces to the halting problem) so the only way to insure that we haven’t missed an attack is to enumerate all of the possibilities.  We can stop at 2^k states because no TM can possibly visit more than that number of states in <=2^k steps.
The number of machines with <=2^k states is 2^(2^k).  Then we need to run each of those machines for 2^k steps, but that factor is lost in the noise and can be safely ignored.
To calculate the value of k that we can handle in this universe we need to compute the computational capacity of the universe.  There are various ways to do this.  The most rigorous calculation I know of is this one:
But that’s a calculation on the upper bound of the capacity of a computer weighing one kilogram under all kinds of realistic physical constraints.  We want an upper bound on the computational capacity of the entire universe, so let’s do a conservative calculation of our own: the number of elementary particles in the universe is estimated to be about 10^80.  (Wikipedia gives the number as 10^86, but most of those are neutrinos.  It doesn’t matter.  You can use 10^100 if you like, it won’t significantly affect the result.)  Let’s suppose that each of those elementary particles can somehow be made into a computer, and that each of those computers can enumerate states at the Planck frequency, ~10^43 Hz.  The current age of the universe is 13 billion years, or about 10^18 seconds.  Estimates of the time until the heat death of the universe are a little harder to come by; it depends on what assumptions you make about what “heat death” actually means, but let’s just pick 10^100 seconds.  Multiply all those together and you get 10^260 or so, or 2^860 total states that can be enumerated in our universe.  Let’s round up to 2^1024 to make the math easier (what’s a few orders of magnitude among friends?).
So k < log-base-2(1024) = 10.
And there you have it.  If you start right now, then by the time the last black hole evaporates due to Hawking radiation you might be able to prove by exhaustive search that a particular cipher does not have an attack that reduces it search space to less than 10 bits.  Anything more than that is hopeless.
It’s interesting to compare this to what one might realistically achieve.  A typical contemporary computer runs at ~10^9 Hz.  So a single computer running for (say) a year (10^8 seconds) could enumerate 10^17 = 2^56 states.  So doing an exhaustive search for attacks with k<6 is potentially achievable.  It’s interesting that the gap between what is achievable and what is not is quite narrow.  This is due to the hyper-exponential cost of enumerating algorithms that run in 2^k steps.

@_date: 2016-06-11 20:34:30
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
This doesn’t work because:
Alice can refuse by (falsely) claiming that she sent (S(X), Z) instead of (S(X), Y).  If this were not the case (i.e. if Alice could not plausibly make this false claim), then Alice would already be committed after sending (S(X), Y), and the protocol would cease to be fair.
My hunch is that it’s a theorem that the sort of fair commitment protocol that you’re describing is not possible without a trusted third party, and that the proof would look something like the proof of the impossibility of establishing common knowledge with unreliable point-to-point communications.

@_date: 2016-06-12 12:43:06
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
In that case, Alice is not committed after Bob signs (but Bob is) and again, the protocol is unfair.
The details don’t matter.  Logically, there are only two possibilities:
1.  Alice is actually committed after Bob signs, in which case the protocol is unfair to Alice.
2.  Alice is not actually committed after Bob signs, in which case the protocol is unfair to Bob.
This is the reason that in real life, commitment protocols work by the first party extending an offer with a time limit for acceptance, and the second party either accepting the offer (in which case the first party is bound — i.e. option 1 above) or making a counter-offer, in which cast they become the offering party and the protocol iterates.  The protocol is fair not because the commitment happens simultaneously, but because the offering party controls the offer and thus cannot be coerced into agreeing to terms that they do not actually accept.
What you have is not a fair commitment protocol, but rather a protocol where an offering party (Alice) can reneg on their offer by not completing the protocol, but her defection can be publicly proven.  This may be useful, but it isn’t “fair” in the sense that it imposes time-symmetric commitments on the parties.
There are real-world examples of your kind of protocol too.  When you buy something from a merchant, there is a period of time where either you have paid but have not yet taken possession of the goods, in which case the merchant can reneg by refusing to hand the goods over, or there is a period of time where you have taken possession but not yet paid, in which case the merchant is extending you credit, and you can reneg by defaulting on the debt.  Defections are rare enough that there is usually no formal method for dealing with them for everyday retail transactions.  In cases where it really matters, like buying real estate, a trusted third party (an escrow agent) is employed to effect the transfer.  If someone truly came up with a way to produce a fair commitment protocol that did not require a trusted third party, it would destroy the escrow industry.  That is another reason that I strongly suspect it’s not possible.

@_date: 2016-06-13 15:38:50
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Not according to the fairness criterion that you originally stated:
You stated goal was to remove the asymmetry, i.e. the period of time where Bob has the option to commit or not while Alice no longer has that option.  Your protocol does not achieve that goal.  No protocol that does not use a trusted third party can accomplish that.  As I suspected and danimoth confirmed, it’s a theorem.

@_date: 2016-06-13 15:52:26
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
This is actually much more subtle than you might imagine.
Suppose Alice extends an offer to Bob.  Bob accepts the offer within the deadline and sends his acceptance to Alice.  Everything is properly hashed and signed.  Are they committed?
Not necessarily.  Suppose that Alice decides she wants to rescind her offer.  She’s not actually allowed to do that, but she can claim that she never received Bob’s acceptance, and Bob can’t prove otherwise.
So we add another step to the protocol: Alice has to acknowledge to Bob that she received his acceptance in order to complete the protocol, which she does.  Are they committed now?  No, because now Bob can decide to rescind his acceptance by claiming (falsely but plausibly) that he never received Alice’s acknowledgement.
This is a general problem (no pun intended — see below).  It is provably unsolvable in the face of unreliable point-to-point communications.  See:

@_date: 2016-06-14 00:03:52
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
You are assuming the communications channel between Alice and Bob is reliable.  If it’s not, then the participants cannot reach a reliable conclusion about whether or not they have actually reached an agreement.  In the real world this rarely matters because modern communications channels are reliable and for most negotiations the stakes are low.  But you and I actually experienced a real-world failure of exactly this sort recently where the result was that I showed up for a meeting and you didn’t (twice!).  And this happened because during a critical period we did not have a reliable communications channel.  So this sort of thing really does matter even in the real world on occasion.
Yes, I know that.  But this is not a discussion about the law, it’s a discussion about a proposed cryptographic protocol.
But this actually is a problem in the real world too.  Disputes over the content of signed documents are rare, but when they happen they can be hard to sort out because there is often no binding of signature to content.  This is why for important documents you often have to sign (or at least initial) every page separately.

@_date: 2016-06-14 10:12:57
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
It *is* the two generals problem.  That’s exactly the point I’m trying to make.  I even included that exact link in one of my earlier posts.

@_date: 2016-06-14 15:28:03
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Yes, I know that.  But again, this is not about what “people" care about, this is about a particular problem and proposed solution advanced by the OP.  (Re-read the subject line of this thread.)
It’s not quite that simple.  The OP’s protocol fails even in the face of perfectly reliable communications.  Understanding why might be interesting to some people on this list.  It certainly should be interesting to the OP.  It might even be interesting to you if you give it a chance.
Just for the record, the OP’s problem had to do with fair commitment while the well-known two-generals problem has to do with achieving consensus/common knowledge.  These problems are related, but they are not identical.  Understanding how they are related is not entirely trivial.  Indeed, the academic results on fair commitment cited by danimoth post-date the results on two-party consensus by 10-25 years.
Yes, I know that too.  There are plenty of ways to solve the problem that the OP posed as a practical matter using a TTP or some variation on that theme.  But again, that’s not really what this is about.  Just because a problem is well understood in academia doesn’t mean all discussion about it on this mailing list should be shut down.  Look at how much time is spent talking about generating secure random numbers here even though this too is a very well understood problem with a wide variety of easy-to-implement real-world solutions.

@_date: 2016-06-22 13:54:59
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Here is Mok-Kong’s problem statement:
Mok-Kong’s claim is that his protocol is *fair* in the sense that there is never a time when Alice is committed and Bob isn’t, or vice-versa.  But this cannot possibly be the case if Alice and Bob’s actions are interleaved in time and there is no trusted third party.
This is not quite the same as the two-generals problem.  The 2G problem is solvable if you have reliable communications.  The fair commitment problem is not solvable even with reliable communications.
Proof by reductio: assume that the problem is solvable, i.e. there is some sequence of interleaved actions taken by A and B that results in fair commitment, i.e. at some point there is some key action K where neither A or B are committed before the action but both are committed after.  Since actions are interleaved, K must be performed either by A or by B.  Let us assume WOLOG that K is performed by A.  A, by assumption, is uncommitted before performing K, and so can choose to perform K or not.  But B can no longer make this choice.  B’s commitment (or lack thereof) hinges entirely on a choice made by A.  Therefore the protocol cannot be fair.

@_date: 2016-06-22 18:42:54
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
You are playing fast and loose with the definition of the word “commit”.
If Alice “must” do something then she is committed to doing it.  That’s what being committed means.
No.  It doesn’t matter whether “C has come into being” or not.  What matters is whether there is simultaneous commitment, and there isn’t, because there can’t be.  Alice is effectively committed as soon as she signs the first time because she has promised (a.k.a. committed) to sign the second time.

@_date: 2016-06-22 23:50:37
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
No.  She also "promises to sign Y”.  Whether you call it a “promise” or a “commitment” is irrelevant.  Alice is still bound by Bob’s decision to sign.
In the original “unfair” protocol, the one that contains the problem you are trying to solve, Alice is also not committed until Bob signs.  You are trying to draw a distinction between “being committed” and “promising to be committed” but this is a distinction without a difference.
If you don’t understand this we will simply have to agree to disagree.  I don’t know how I can make it any clearer.

@_date: 2016-06-23 11:34:03
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Yes, that is the root of the problem, that Mok-Kong’s doesn’t have a precise definition of “commit” and “fair”.  But there is an implicit definition contained in his problem statement:
From this I think it is reasonable to infer that “unfair” means that there exists a period of time where one party is “committed” during which the other party "has no corresponding commitment”, i.e. for a protocol to be fair it has to produce simultaneous commitment.  And that not possible with interleaved actions by the participants and no TTP.
AFAICT, MK’s argument hinges on trying to draw a distinction between actually being committed and merely having made a promise to commit, or something like that.  But this seems to me like a distinction without a difference, mere wordplay, like being engaged to be engaged.  To me, being “committed to X” means that you are in a state where there will be some negative consequence if you do not do X.  Whether you call that “committed” or “promised” or “frabnobulated” is irrelevant.

@_date: 2016-06-29 09:03:10
@_author: Ron Garret 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
It is true that this problem is different than the 2G problem.  However, coming up with a proof of impossibility of fair commitment protocols without an RTP (even with reliable communications) is a pretty elementary exercise.  I even exhibited such a proof on this list.  Here it is again, with elaboration:
Theorem: fair commitment protocols using only interleaved actions by the participants (i.e. without a trusted third party) are not possible, even with reliable communications
Let’s start with some definitions:
An agent A is COMMITTED to some action C with respect to a contractual counter-party B if there exists an action or sequence of actions S that B can take following A’s failure to perform C by some deadline that results in negative consequences for A.  The canonical example would be B suing A for failing to perform C.  Note that it is not necessary that B actually perform S for a commitment to exist on the part of A.  B might elect not to sue A for failure to perform C, but that doe not vitiate A's commitment.
A FAIR CONTRACT is a contract that commits A to performing an action CA and B to performing an action CB with the property that there is no period of time where A is committed to CA while B is not committed to CB.  A FAIR PROTOCOL is a protocol that produces a fair contract.  INTERLEAVED ACTIONS are a set of actions with the property that 1) every action is performed by either A or B, and 2) no two actions occur simultaneously (and hence they have a total order with respect to time).
We now proceed with the proof by reductio: assume that a fair protocol exists.  Then there is some key action K with the property that performing K results in the simultaneous commitment of A to CA and B to CB.  Assume without loss of generality that K is performed by A.  Then B is committed to CB after K, i.e. after K, if B does not perform CB then there exists an action S that A can take that results in negative consequences to B.
Now consider the situation before K.  In this situation, A can produce negative consequences for B by performing K followed by S.  Therefore, by the definition of commitment, B is committed to CB before K.  This contradicts our assumption that B was not committed to CB before K.  QED.
You might as well open with “proposal for trisecting an angle with compass and straightedge."

@_date: 2016-03-01 13:12:27
@_author: Ron Garret 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Yes, I know this, and I specifically addressed it.  The attack is not a brute force attack on the AES key, it’s a brute force attack on the PIN.  It works like this:
1.  De-solder the flash chip and read its contents
2.  Replace the flash chip with a ZIF socket (probably connected to a short ribbon cable).
3.  Re-install the flash chip and make five guesses at the PIN.
4.  Power down, replace the flash chip with a fresh copy of the original, and go to Step 3.

@_date: 2016-03-02 18:17:01
@_author: Ron Garret 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
I disagree.  I think it is still possible to build a secure system that can be fully understood and vetted by its users.  I am in fact working on such a system.  The initial prototype is here:
I’m currently working on an iOS app and an HSM.  I’m actively recruiting people to help me in this effort.  If you’re interested please contact me off-list.

@_date: 2016-03-05 08:43:48
@_author: Ron Garret 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
You can’t be serious.  On this line of reasoning, it would be OK for law enforcement to force you to sign a confession as long as they didn’t force you to *write* the confession.  There’s no difference between a digital signature and a regular signature.  Both have the same semantics: endorsement of the content being signed.  If Apple is forced to sign one compromised version of iOS that undermines the trust they have painstakingly built which gives their customers confidence in their product.  If I know that the government has the power to force Apple to sign a compromised iOS, how do I know that the signed version of iOS that I’m about to install on my phone isn’t also compromised?

@_date: 2016-03-05 11:07:43
@_author: Ron Garret 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
Apple does.  Apple has put in significant (one might even call it extraordinary) engineering effort to create technological infrastructure in which their digital signature can be relied upon to mean: we, Apple, certify that this content is safe for you to run on your device.
No, that’s not true.  There is quite a bit of existing law — both common and statutory — around signatures, both digital and analog.  The courts are not free to just make shit up.  IANAL but based on what I know about the existing law of signatures it is quite clear that Apple’s digital signature meets all of the criteria for being a legal signature under both statutory and common law.  (The principal criterion for being a legal signature is that it is the intent of the signer that it be a legal signature.  This is why someone who can’t write can legally sign a document with an “X”.)

@_date: 2016-03-05 12:01:54
@_author: Ron Garret 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
That doesn’t matter.  If you sign a confession under duress, that is technically not a legal signature because it was not your intent.   But that doesn’t change the fact that it is 1) illegal to coerce someone to sign a confession and 2) that signing a confession under duress can have very similar deleterious effects on the signer as signing one willingly.  Specifically, Apple will have revealed itself as being willing to sign code under duress, and so now Apple’s signature can no longer be trusted by its users.  That trust has significant value, and destroying it constitutes actual harm.

@_date: 2016-03-12 08:47:02
@_author: Ron Garret 
@_subject: [Cryptography] Is Non-interactive Zero Knowledge Proof an 
No, because a NIZKP assumes that Bob and Alice share a common reference string drawn from a random distribution, which is the basis for the proof.  The proof doesn’t work between Bob and Charlie because Bob and Charlie don’t share that common reference string.
The NI part of NIZKP is slightly misleading because some interaction between Bob and Alice is required to establish the CRS between then.  But this can be done in advance, so it’s not considered part of the proof.

@_date: 2016-03-15 09:33:41
@_author: Ron Garret 
@_subject: [Cryptography] Is Non-interactive Zero Knowledge Proof an 
The same thing happens in the RO model.  Here’s the relevant passage from the paper [1] that you cited:
"We note that an important part of the intuition behind zero-knowledge is lost in these two models in a multi-party scenario, if the CRS string or the random oracle may be reused. An easy way of seeing this is simply by noting that non-interactive zero-knowledge proofs are possible in both these model. A player having received a non-interactive proof of an assertion, it could not have proved before the interaction, can definitely do something new: it can simply send the same proof to someone else. This fact may seem a bit counter-intuitive since the intuition tells us that the simulation paradigm should take care of this. We note, however, that the simulator is much “stronger” in these models than in the plain model. As it is, the simulator is allowed to choose the CRS string, or random oracle, and this fact jeopardizes the zero-knowledge intuition. In fact the zero-knowledge property in these model only guarantees that the verifier will not be able to do anything without referring to the CRS string or the random oracle, it could not have done before.”
So I’m pretty certain that this:
is not true.
[1] Rafael Pass, On Deniability in the Common Reference String and Random Oracle Model.

@_date: 2016-03-17 23:51:35
@_author: Ron Garret 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
Indeed not.
TL;DR: In the 1990s we formally verified the a portion of the flight software for an unmanned spacecraft.  Despite this, and despite extensive ground testing, the software nonetheless failed in flight.  (Happily, the failure was not catastrophic.)
There’s no getting around the GIGO problem.

@_date: 2016-03-19 12:56:29
@_author: Ron Garret 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
No, it was the application of formal methods that failed.  But it’s not just a random bit of stupidity, there was an interesting structure to this particular failure.  It was never practical to verify the entire software stack, there was just too much of it.  So we tried to structure the software with a secure, formally verified “core” that was supposed to maintain certain invariants (like no deadlocks).  The theory was that anything built on top of that core would maintain those invariants.  It was a riff on this theme from Perry’s original post:
It’s a very attractive theory.  I believed it myself.  But I now think that the RAX bug falsified it.
My intent here is not to discourage anyone from applying formal methods, only to caution against being overconfident in their results.

@_date: 2016-03-19 14:05:06
@_author: Ron Garret 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
The RAX executive was designed like an operating system (indeed, it arguably *was* an operating system, though it ran as an application on top of a “real” operating system) with a formally verified kernel and non-verified application code running on top of it.  The kernel exposed high-level operations with semantics like, “Do the following tasks in parallel until either one task succeeds (in which case terminate all the others and report success) or they all fail (in which case report failure).”  Application coders were supposed to use these high level operations exclusively, and if they had it probably would have worked.  But the only enforcement mechanism we had to ensure that only the high-level operations were being invoked was coder discipline and code audits.  Basically, we looked at the code to make sure that none of the low-level primitives (like mutexes) were being invoked.
Well, what happened (to the best of my recollection — this was 20 years ago) was that the application programmer had a task for which he needed a mutex, and though he knew that he wasn’t allowed to use them, he ended up (inadvertently) inventing one.

@_date: 2016-03-24 14:25:41
@_author: Ron Garret 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Saying "How on earth did it come to this?” strongly implies that you think that the trend towards DJB’s crypto suite a problem, but you don’t offer much in terms of proposals for how to solve it, or even what a solution would look like.  You seem to agree that a solution would *not* look like the status quo.  So what exactly are you advocating here?
I submit that the impending monoculture in crypto is not necessarily a problem, any more than the monoculture in physics (what?  No alternatives to GR and QM?) or climate science is necessarily a problem.  It’s possible that crypto has a Right Answer, and that Dan Bernstein has discovered/invented it.  If you believe that simplicity and minimalism ought to be part of the quality metric then there may be very few local maxima in the design space, and DJB may simply have found one of them.

@_date: 2016-03-26 09:27:37
@_author: Ron Garret 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Ironically, it is precisely perennial confusions like this that are driving people towards the DJB monoculture: if you use NaCl you don’t need to worry about ETM/MTE.  You just call crypto_box and the Right Thing happens automagically.
Actually, it turns out that even crypto_box has a small pitfall: an attacker can take a message sent by Alice to Bob and make it look like it was sent from Bob to Alice simply by swapping the keys.  There’s a fix for this (encode the order of the keys into the nonce) but crypto_box does not do this automatically (though SC4 does).  Thanks to Mario Heiderich at Cure53 for pointing this out to me.

@_date: 2016-03-27 12:35:02
@_author: Ron Garret 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
The right way to deal with this is by versioning the entire crypto suite.  So NaCl is Comprehensive Tool Chain version 1.  When some problem is identified with NaCl, the community can work on CTC version 2.  Maybe there could be two versions active at any one time, a standard suite, and a backup that is “in the bullpen” in case something goes wrong with the primary.  A backup for NaCl would probably look just like NaCl but with Curve25519 replaced with Curve448-Goldilocks.
It’s very rare that a weakness is discovered in a core crypto algorithm without a lot (like several years) of warning so a single backup should cover even the most catastrophic of circumstances.

@_date: 2016-03-27 22:57:42
@_author: Ron Garret 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I’m working on designing some minimalist protocols based on TweetNaCl.  I could use help.

@_date: 2016-03-29 14:06:25
@_author: Ron Garret 
@_subject: [Cryptography] USG Moves to Vacate Apple Decrypt Order 
Apple could respond by counter-suing for legal fees.  If they did, we’d know.  If they decided not to, or reached a back-room settlement with the FBI, then we wouldn’t.

@_date: 2016-04-30 23:23:35
@_author: Ron Garret 
@_subject: [Cryptography] Mathematics of variable substitutions? 
Can you please elaborate on that a bit?  How does the non-existence of a birational substitution that converts Edwards to circles follow from the fact that the genus is a birational invariant?

@_date: 2016-05-01 08:12:53
@_author: Ron Garret 
@_subject: [Cryptography] Mathematics of variable substitutions? 
Oh, never mind.  The genus of the two curves must be different.  Duh.

@_date: 2016-05-01 22:43:35
@_author: Ron Garret 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
The day is not far off when every hardware-store screw will have an NFC chip in it.

@_date: 2016-05-02 10:56:37
@_author: Ron Garret 
@_subject: [Cryptography] Craig Wright is Satoshi 
Lots of evidence that the press has been snookered:
TL;DR: the “proof” Craig provided is not publicly verifiable.  It was given credence only because Gavin Andresen vouched for Craig’s “proof” and Gavin has to date been considered trustworthy in the bitcoin community.  My guess is that he just squandered that trust.

@_date: 2016-05-06 12:06:51
@_author: Ron Garret 
@_subject: [Cryptography] Proof-of-Satoshi fails Proof-of-Proof. 
No, that’s not true.  Ed25519 signatures use a hash of the content being signed as the “random” value and so are deterministic.  This is one of the things that makes Ed25519 better than standard ECDSA.

@_date: 2016-05-07 14:30:15
@_author: Ron Garret 
@_subject: [Cryptography] Proof-of-Satoshi fails Proof-of-Proof. 
No, that’s not true either.  Ed25519 is not merely ECDSA with a specified nonce, it has structural changes from ECDSA specifically to prevent the kind of attack you are suggesting.  The message content is hashed twice, once to produce the nonce, and again with the secret key as a prefix to produce the signature.  Not only does this prevent malleability attacks, but it also protects against collisions in the underlying hash.  Two different messages can actually have hash collisions and still produce different signatures.  (The converse is also possible: two messages which do not collide in the underlying hash can collide in the signatures, but this is extremely unlikely because Ed25519 is, essentially, a keyed hash construction.)
(I hereby coin Ron's first law of cryptography: if you think you’ve found a flaw in a DJB design, you are almost certainly wrong.)

@_date: 2016-05-12 13:40:12
@_author: Ron Garret 
@_subject: [Cryptography] 2nd Amendment Case for the Right to Bear Crypto 
It’s done more than become popular, it is the law of the land:
Personally, I think the supreme court got this right.  The structure of the 2nd amendment is, “Because A, therefore B.”  The argument that because A is no longer applicable in today’s world then neither is B makes the tacit assumption that A is the *only* justification for B.  I see no basis for making that leap.  If the American people want to clarify the matter (or change it outright) they can amend the Constitution.  But polls show an overwhelming majority supporting the Heller decision.

@_date: 2016-11-09 16:28:32
@_author: Ron Garret 
@_subject: [Cryptography] protecting information ... was: we need to 
This really needs to be emphasized.  I meet people all the time who ask me if I can build them a magic USB dongle that they can plug in to their computer and somehow make it magically secure.  I have to explain to them that the very fact that they are asking this question -- that this possibility is part of their world view -- is a big part of the problem.  Most of them don’t like hearing that.

@_date: 2016-11-15 18:34:19
@_author: Ron Garret 
@_subject: [Cryptography] On the deployment of client-side certs 
I would like to take this opportunity to draw you attention to this, which is almost exactly what you just described (modulo some software):
It’s a user-flashable STM32F4 with a display and user inputs, which are crucial for preventing phishing attacks.  It currently functions as a FIDO-U2F token, but since it’s user programmable you can make it do pretty much whatever you want.
Unfortunately, I am currently out of stock but will be shipping again in January.  If you want one you can go ahead and place and order and your card will not be charged until your order ships.

@_date: 2016-11-15 18:39:58
@_author: Ron Garret 
@_subject: [Cryptography] On the deployment of client-side certs 
Why not?  They are not very expensive.  You can get one on Amazon for $10.

@_date: 2016-11-20 02:24:41
@_author: Ron Garret 
@_subject: [Cryptography] On the deployment of client-side certs 
It’s probably a theorem: if someone pwns your machine they can (by definition) do anything you can do.  Therefore, if you can access the “secure” chip then so can an attacker.
This is why secure hardware needs its own dedicated I/O.

@_date: 2016-11-22 12:03:23
@_author: Ron Garret 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
Talk about burying the lede.
If you XOR, then correlations among sources manifest themselves.  If you hash, then they don’t.  Hashing extracts any and all entropy in the system wherever it happens to be (assuming a cryptographically secure hash, of course).  XORing does not.
I am constantly surprised by how often discussions of randomness arise on this list, and how long they continue.  Everything that matters about randomness can be summarized in four bullet points:
1. You need two things: an entropy source, and a whitener. No entropy source is perfect, so you need a whitener no matter what. You don't have to do anything fancy in your whitener. Any cryptographically secure hash function (like SHA512) will do.
2. Since you need a whitener no matter what, it doesn't really matter how good your entropy source is, except insofar as it might take a long time to collect enough entropy from a very poor source. All that matters is that you have an accurate lower bound for how much entropy your source actually provides, and this is the case no matter how good (or bad) your source actually is. As long as you feed >N bits of entropy into your whitener, you can safely extract N bits of true randomness out of it.
3. You don't need more than a few hundred bits of randomness. 128 bits is enough, 256 is a comfortable margin, 512 is serious overkill. Seed a cryptographically secure PRNG with a few hundred bits of entropy and you can safely extract gigabytes of key material out of it.
4. Because of 1-4, putting a lot of effort into designing and tuning a HWRNG is pointless. Such a device provides no value over a poor entropy source fed through a whitener provided you have an accurate lower bound on the entropy content so you can be sure you have seeded your PRNG with enough initial entropy.
But if it makes you feel warm and fuzzy to pay a lot of money for a HWRNG, I am more than happy to sell you one!   (NOTE: I am currently out of stock but will be shipping again in January.)

@_date: 2016-11-23 15:22:08
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
Yes ;-)
I would also add:
5.  It is not possible to assess the quality of a random number generator by looking at post-whitener output.  Post-whitener output will *always* pass all statistical tests (otherwise you there is a flaw in the hash function).  This is why most of the performance data for e.g. OneRNG and RDRand is useless.

@_date: 2016-11-23 19:26:57
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
That’s true, but it’s a straw man.  I very deliberately did *not* say that you need an accurate estimate of how many bits of entropy you get in every M bits of source.  What you need is an accurate *lower bound* on this quantity.  You can make that lower bound as conservative as you want.
As to “how do you do it”, that is ultimately a judgement call that you have to make based on your risk posture and the totality of the circumstances.  But my baseline recommendation if you want to be exceptionally paranoid is to make an audio recording of some white-ish noise (e.g. record yourself saying “Shhh”) and then extract 1% or 0.1% of the result.  Of course, you have to do this in a secure environment.  An attacker is vastly more likely to compromise you by obtaining a copy of this recording than because it didn’t contain enough entropy.

@_date: 2016-11-23 19:54:13
@_author: Ron Garret 
@_subject: [Cryptography] RNG design principles 
A fair point.  The definition I use is: entropy is information (i.e. components of the state of a system) whose values your attacker cannot guess more efficiently than by conducting an exhaustive search.  I don’t want to quibble over terminology either so I’m happy to use a different word, but I thought this was common usage.
Yes.  That’s why I said “>N” and not “N”.
Sure, and I don’t doubt that my original draft could be improved.  But if you are going to critique my draft, you should critique what I actually wrote rather than attacking a straw man.
This is debatable, but probably not worth the effort to debate because entropy is really easy to obtain so there is no reason not to err on the side of caution.  Use 1024 bits of entropy if you want, it doesn’t matter.  But don’t pay $10,000 for specialized hardware to generate megabits of entropy per second, or to try to use quantum effects to produce entropy.  It’s simply not necessary.  Ever.
Why?  It is not hard to build a CSPRNG with a cycle time longer than the age of the universe.  The only reason to re-seed is if your system has been compromised.  Re-seeding is a policy issue, not a technical one.
Just because people have done this doesn’t mean it’s necessarily the right thing to do.
These are fair points, and they probably ought to be broken out into separate threads.  But I’ll open the bidding by asking:
1.  What is an example of a system that needs an RNG “right away” (and what does “right away” mean)?
2.  What is an example of system where you would have any doubt whatsoever that a very conservative lower bound was sufficient?

@_date: 2016-11-24 03:22:02
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
I agree 100%.  The “Shh” example is meant more as an illustration of how simple a solution to the problem can be than a serious suggestion for production use, though it would in fact work, you would just want to choose a high safety factor.  A typical audio input samples at >500 kbps, so one second of audio with a safety factor of 1000 will give you 500 bits of entropy.  That’s a pretty comfortable margin IMHO, but if you don’t agree, I’ll be happy to sell you an HSM with a HWRNG for $75.  Under no circumstances should anyone be paying more than that.

@_date: 2016-11-26 03:44:17
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
Reference please?  Because this would be news to me (and, I think, a lot of other people as well).

@_date: 2016-11-26 16:30:23
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
Build a very clean interface to an entropy source to make it easy for a user to plug in different ones.  Include two by default:
1.  /dev/urandom, which should be the default.  A user should have to take positive action to use anything other than /dev/urandom.  But that said, I would recommend including as an option:
2.  User input, like typing keys or moving a mouse around.  (Need to use a high safety margin here.  User input contains less entropy than most people think.)
3.  If OpenSSL wanted to include support for an external hardware source of randomness (like, say, the SC4-HSM) I wouldn’t mind ;-)

@_date: 2016-11-26 19:20:52
@_author: Ron Garret 
@_subject: [Cryptography] RNG design principles 
Well, yes, that’s true.  But unfortunately, one of the possible reasons for including things in government reports is politics.  To cite but one example, the report you cite includes a section on Dual_EC_DRBG, which is now known to contain a back door.
Also, a lot of things in crypto get complicated once you fill in all the details.  That does not mean that a short summary like mine cannot be substantially correct.  If you think that my summary has actual errors or omissions, by all means point them out.  But it is not a valid argument to cite a long paper written by experts and say that because this report is long that a short summary must be wrong simply because it’s short.

@_date: 2016-11-26 19:43:47
@_author: Ron Garret 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
I think you are both missing the forest for the trees.  What matters is not so much the audit per se.  What matters is *auditability*.
The reason you want an audit is because you don’t trust the vendor. But if you don’t trust the vendor, why should you trust the auditor?  An auditor is no less fallible and no less likely to be corrupted or otherwise turned to the dark side than a vendor.  To coin a phrase, who audits the auditors?
It should not matter so much that an audit has been done (though that matters too, of course) as that it *can* be done — by anyone, not just a privileged party approved by the vendor and who has signed appropriate NDAs.  State actors can influence or infiltrate or even outright control both vendors and auditors.  The only way to protect against this is to insist that the system be architected in such a way that anyone could audit it if they wanted to.

@_date: 2016-11-26 22:07:03
@_author: Ron Garret 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
No, that is neither what I said, nor what I meant.
Yes, that’s right.  But what matters is that I rather than the vendor have the ability to choose who does the audit.  That is the only way to protect against a compromised vendor.  And, as I pointed out before, if you don’t have vendor compromise as part of your threat model, there’s no reason for you to care about an audit in the first place.

@_date: 2016-11-26 22:27:26
@_author: Ron Garret 
@_subject: [Cryptography] Is Ron right on randomness 
And I would agree.
I’m not really serious.  Using the SC4-HSM as a HWRNG is like using a Macbook Pro to run Calculator.app.  But a lot of people make a lot of money selling snake oil in the security business, so if someone wants to overpay me to provide them with random numbers I’m not going to object.
The real use of the SC4-HSM is, well, as an HSM, i.e. to securely store keys and to use those keys to do crypto operations, mainly generating signatures.  But you can also, for example, do decryption on board and display the result on the built-in display.
There is no proprietary hardware or software in the SC4-HSM.  It is really a very simple piece of hardware: an STM32F405 chip, a display, two buttons, and a USB connector.  And all the software is open-source.  But of course you can run anything you want on it.
The SC4-HSM exists not because I think it’s a revolutionary piece of hardware that is going to take over the world (though that would be nice) but simply because I wanted something like it and no one else was building it.  I started this work using a Teensy3, but realized very quickly that to be secure you need dedicated I/O.  There was nothing on the market that met those requirements so I decided to build it myself.
At the moment, the SC4-HSM presents itself as a virtual com port.  I couldn’t make that proprietary even if I wanted to.

@_date: 2016-11-29 18:11:19
@_author: Ron Garret 
@_subject: [Cryptography] RNG design principles 
I gather that was directed at me.
I don’t necessarily disagree with the sentiment here, but I’d just like to state two things for the record:
1.  My “3 or 4 pithy axioms” were about how to *produce* randomness, not about how to *deploy* it, which is what this discussion is about.  However…
2.  I disagree that deploying is “very much more complicated.”  You yourself have just done a very good job of summarizing what needs to be done, at least in an environment where you are running a traditional operating system: the OS needs to provide a device from which you can reliably read cryptographhically secure random data without blocking for too long (for some value of “too long”).  Providing this device might involve employing some engineering techniques, like storing randomness in persistent storage so that it is available on reboot without delay.  All of these things are well understood.  The main obstacles towards getting it right are political, not technological.

@_date: 2016-10-01 07:43:20
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
It guarantees no such thing.  This is not a problem with Lisp, at worst it’s a problem with the Lisp *reader*.  Lisp != Read.
But it’s not even a problem with the reader, it’s a problem with Lisp's default array serialization syntax.  *No* language can read that syntax — or any open-ended variable-length serialization format, including JSON — efficiently because the information needed to do so simply isn’t there.
The Right Answer is to define a new syntax that includes information about the size of the array.  Fortunately, Common Lisp allows the reader to be extended so that doing this is an elementary exercise.
But if you really care about reading arrays efficiently you probably want to use a binary representation with a header that tells you the size and use read-sequence.  That can be as fast as C.
(Surely you knew all this?  Why are you of all people spreading FUD about Lisp???)

@_date: 2016-10-01 08:25:29
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
At least CL *tries* to get this right.  Yes, it is true that on a strict language-lawyerly-reading of the spec, passing invalid arguments to aref (or subseq) results in undefined behavior.  But it is universally understood among all CL compiler writers and consumers that when (optimize (safety 3)) is in effect that dereferencing an out-of-bounds array index will result in a run-time error.  And if you want to be a language lawyer, you can use array-in-bounds-p ( to 100% guarantee that your array references are safe.
BTW, the only reason that CL doesn’t get this completely right is that the committee ran out of money before they could address it.  It was on their radar.
One can do more than hope.  AFAIK there is not a single CL implementation where this is not the case.
You say that as if it’s a bad thing, but this flexibility is generally considered a feature, not a bug.  Here again, CL excels, allowing one to explicitly trade off various incommensurate dimensions in the software quality space (speed, safety and debuggability).  C compilers typically only give you control over a single optimization dimension.
What???  Where does it say that?
What actually matters is that Lisp does not have first-class pointers, so it is *possible* to do automatic bounds checking, which makes code significantly safer.  In C this is not possible.

@_date: 2016-10-01 08:32:17
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
I’m actually only using two jquery features: ajax and DOM selectors, so I could pare that down a lot, but until I find a more reliable revenue source I have to choose my battles.  If the jquery dependency is the only thing standing between you and using SC4 please let me know and I’ll bump it up in my priority list.  (Personally, I consider the UI to be separate from the SC4 standard and reference implementations, whose only external dependency is TweetNaCl.)

@_date: 2016-10-01 10:17:46
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
Yes.  Deserved.  Past-tense.  SBCL has been competitive with C for a long, long time now.  Again, surely you knew this?
Also, safety isn’t free.  You can pay for it with silicon or you can pay for it with time.  Or you can go without, which is a perfectly defensible position.  Everyone needs to choose their own risk posture.  But *if* you want safety, I think CL has a very good value proposition and it should be considered more seriously than it generally is.  Could CL be improved?  Certainly.  Would it be worth the effort?  That is far from clear.

@_date: 2016-10-01 13:18:10
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
Just to be clear, SC4 is not an HTTP server application, it’s a client-side application.  It just happens to be written in Javascript and uses a browser for a GUI, but different implementations are possible.  There’s already a command-line implementation written in Python, and implementations in other languages would not be hard to do.  It’s only about 3 kLOC (and a third of that is TweetNaCl) so it’s pretty easy to port.
There is a version of SC4 that uses a server to distribute keys, but that’s optional.

@_date: 2016-10-02 14:14:21
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
Because AI winter happened and there was no funding available to continue the work.
That is not an answer.  I asked *where* in the “language rules” it says what you claim it says.
I suspect that you are looking at section 3.5.1.1 in the spec, “Safe and unsafe function calls.”  But note that this is a sub-section of 3.5.1 “Argument mismatch detection”.  This has to do only with making sure that a call has the correct *number* of arguments.  AFAIK the spec says nothing about when array bounds checking must happen (except that it obviously has to happen in the computation of array-in-bounds-p).
When it comes to security, almost isn’t good enough.  Doing array bounds verification in C requires solving the halting problem:
T x[10];
T y[100];
T* f() { return goldbach_conjecture_is_true() ? &x : &y }

@_date: 2016-10-03 22:43:31
@_author: Ron Garret 
@_subject: [Cryptography] French credit card has time-varying PIN 
No, it just means you have a one-hour window between phishing the CVV and using it.

@_date: 2016-10-04 08:48:39
@_author: Ron Garret 
@_subject: [Cryptography] French credit card has time-varying PIN 
I am dumbfounded that you of all people have missed the very obvious flaw in this argument.  You *literally* wrote the book on this stuff! [1]  Here is straightforward adaptation of the anecdote with which you open the section on threats (p. 239):
A time-changing CVV code is useless for exactly the same reason that the CVV code itself is useless.  Why do we have CVV codes?  Because someone observed that hackers were stealing credit card numbers, which were supposed to be secret.  So they said to themselves, “What we need is a secret that can’t be stolen.  So we won’t put it on the magstripe and we won’t emboss it on the card.  Then there’s *no way* that hackers will be able to steal it!  Problem solved.”
Well, no, problem not solved.  Why?  Because criminals easily adapted to the new circumstances by adding a CVV code field to their phishing sites.  (How anyone could have not seen that one coming is still beyond my comprehension.)
So where we are today is a very similar situation: someone looked at the phishing ecosystem and observed that it currently takes several days from phishing to use and said to themselves, “If we could change the CVV code every hour then the phished CVV code will be invalid by the time criminals try to use it.  Then there’s *no way* hackers will be able to use phished credit card numbers.  Problem solved.”
Well, guess what: problem not solved.  Why?  Because criminals will trivially adapt to the new circumstances.  It’s just not that hard for phishers to set up a distribution channel with latency measured in seconds rather than days.  The only reason they haven’t done it so far is that it hasn’t been necessary.  If it becomes necessary, they will do it.  This is their livelihood after all.
[1]

@_date: 2016-10-04 16:56:27
@_author: Ron Garret 
@_subject: [Cryptography] French credit card has time-varying PIN 
That’s the price for dead fullz.  Live ones cost $100+
Why?  Switching to a JIT supply chain just isn’t that hard, particularly for digital goods.  I don’t see any reason why haxers won't do it if that’s what it takes to stay in business.

@_date: 2016-10-13 09:08:19
@_author: Ron Garret 
@_subject: [Cryptography] Defending against weak/trapdoored keys 
Or we could just use ECDH on Curve25519 and be done with it.

@_date: 2016-10-14 13:21:46
@_author: Ron Garret 
@_subject: [Cryptography] Announcement: the SC4-HSM is now a FIDO U2F token 
I’m pleased to announce the first real industrial-strength application for the SC4-HSM (  The device can now function as a FIDO U2F (universal second factor) token.  The code is currently available only through our github repository ( but I will be releasing a pre-built binary soon.  All units ordered from this point forward will ship with the new firmware.
Note that this code has been tested but not audited.  I believe it to be secure, but cannot yet guarantee it.  If anyone is interested in doing a code review (which could be a paid gig) please contact me off-list.

@_date: 2016-10-15 13:53:38
@_author: Ron Garret 
@_subject: [Cryptography] Announcement: the SC4-HSM is now a FIDO U2F token 
Yes, the SC4-HSM is easily capable of doing this, and it would not be hard for someone who knows what they’re doing.  Unfortunately, I’m not yet such a person :-)  The crypto part is straightforward, but I’m still climbing the learning curve on the USB drivers.  If you know how to write CCID drivers, or where I could get some reference code, that would be very helpful.

@_date: 2016-10-16 23:13:37
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Announcement: the SC4-HSM is 
Yes, that’s no my to-do list.  Hopefully this week.

@_date: 2016-10-17 11:00:03
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Announcement: the SC4-HSM is 
I don’t think this is what grarpamp was referring to, rather s/he was talking about accepting bitcoin to buy an SC4-HSM, and offering to pay bitcoin for code review and development.
I am familiar with the Trezor, and it is indeed very similar to the SC4-HSM, but with one very important difference: the Trezor does not give you access to DFU mode on the SoC, so you can’t actually re-flash the device, nor can you inspect the contents of the flash.  Yes, you can upload new firmware, but this is done through a bootloader that permanently installed on the device, and which is not user-inspectable.  The security of the Trezor is thus not verifiable by the user.  You have to trust Satoshi Labs.
The SC4-HSM is designed to be 100% open and auditable, at least in terms of the software.  You don’t have to trust me.  You can build *all* of the firmware and the tool chain from source, and you can access DFU mode so you have direct hardware-level access to the flash.  You still have to trust me that the chip on the board is indeed an STM32F405 (you can remove the display and see the markings on the chip) and you have to trust ST Micro to have not built a back-door into that chip, but that’s a much higher bar than trusting Satoshi Labs to not have a weakness, either intentional or otherwise, in their bootloader.

@_date: 2016-10-28 18:10:25
@_author: Ron Garret 
@_subject: [Cryptography] 
=?windows-1252?q?vent_their_own_crypto_and_avoid_eavesdropping=22?=
I suspect the best use of these AI-generated crypto algorithms will be for training human cryptanalysists, and for providing cautionary tales as to why you should never trust a crypto algorithm whose code can’t be audited.

@_date: 2016-09-07 16:23:56
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Secure erasure in C. 
No, but it seems like overkill to me.  AFAICT the problem is more straightforward than that:
The problem seems to me to be that the buffer contents are not volatile.  You want:
static void *(*const volatile deleter)(volatile char*, int,size_t) =
  (void *(*const volatile)(volatile char *, int, size_t)) memset;
static void erase(volatile char *buf, size_t len) { deleter(buf, 0, len); }
But why not simply:
void erase(volatile char *buf, size_t len) {
  for (int i=0; i<len; i++) buf[i]=0;
Or, if you really want to be paranoid:
int  erase(volatile char *buf, size_t len) {
  for (int i=0; i<len; i++) buf[i]=0;
  for (int i=0; i<len; i++) if (buf[i]) return -1;
  return 0;
or if you want to be really REALLY paranoid:
volatile int _dummy = 0;
int  erase(volatile char *buf, size_t len) {
  for (int i=0; i<len; i++) buf[i]=0;
  for (int i=0; i<len; i++) if (buf[i]) return -1;
  _dummy++;  // Or _dummy = get_random_number() or gettimeofday()
  return 0;

@_date: 2016-09-07 17:27:52
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Secure erasure in C. 
OK, so then why not:
void erase_non_volatile(char* buf, size_t len) {
  volatile char* buf1 = buf;
  erase(buf1, len);
void erase_non_volatile(char* buf, size_t len) {
  volatile char* volatile buf1 = buf;
  erase(buf1, len);

@_date: 2016-09-09 16:00:12
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum]  Secure erasure in C. 
How would that help?  (BTW, if you want array bounds checking you can have it in C, you just can’t use C’s [] operator to access arrays.)
This has nothing to do with the language the OS is written in, this has to do with whether or not the OS provides a secure memory model.  Any OS written in any language can copy anything anywhere any time.  Or not — it’s (obviously!) a design decision.  But if you really want to be secure you do need a secure OS.  (Of course, the most secure OS is no OS.)
Again, this has nothing to do with C.  Ultimately assembly language is all there is on a computer, everything else is an abstraction.  So if you can’t do it in assembly, you can’t do it in any language.
Of course.  If the hardware can't do X then your assembly language will likewise be unable to do X for any X.
What’s your threat model?

@_date: 2016-09-13 08:14:17
@_author: Ron Garret 
@_subject: [Cryptography] Secure erasure 
Who are you, and what have you done with Henry Baker?  Because Henry Baker would surely know that:
1.  Ada doesn’t have GC, not even in “some versions of Ada.”  Ada was designed for writing embedded controllers, and everyone (you included, apparently, whoever you are) thinks that you can’t do that in a GC’d language.  (Everyone is wrong, and Henry Baker would surely know that too.)
2.  Even if some versions of Ada had a GC you could easily avoid any problems that might cause by simply not using those versions.
3.  Crypto code very rarely has real-time requirements.  Constant-time performance is desirable not for its own sake, but in order to avoid side-channel attacks.  Random timing variations produced by circumstances unrelated to key material (like a GC) can actually be beneficial.
4.  Constant-time code can easily be written even in a GC'd language by writing code that doesn’t cons.
5.  Hard-real-time GC’s exist if for some reason you absolutely had to have real-time consing code.
6.  The only thing standing between you and the ability to "control the fine details of the implementation" of a language is your willingness to hack a compiler, or write a vendor a check.
 is the smoking gun that proves you aren’t Henry Baker, because the real Henry Baker could write three Ada compilers before breakfast.  ;-)
All that notwithstanding, I’ll go ahead and take the bait: What language would you advocate using for crypto if not Ada?  Surely not Scheme if your principal complaint about Ada is the GC?

@_date: 2016-09-16 15:48:40
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Not much.  It’s actually a lot easier to make a safer C than it is to make a faster C.
I would not start with gcc, I would start with tcc.  In fact, simply using tcc instead of gcc is probably already a huge win.
There’s a boatload of low-lying fruit.
1.  Simply warning about all detected undefined behavior (instead of silently emitting stupid code) would be huge win.
2.  Treating all lvalues as if they were volatile (i.e. not even trying to optimize anything, just a completely straightforward translation of C into assembly) would likewise be a huge win.
3.  If you really want to win big, have your compiler distinguish internally between T* v and T v[c], and likewise distinguish between *(v+offset) and v[offset], and add bounds checking in the latter case.  Yes, this violates the C standard, but so what?  The C standard is stupid.
4.  If you really REALLY want to win big, define a new language that is just like C but where v[offset] and *(v+offset) are NOT equivalent operations, and deprecate the latter.

@_date: 2016-09-17 09:42:09
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Yes, it is.  There is a significant difference between x[y] and *(x+y) despite the fact that the C standard specifies that these are equivalent constructs: in *(x+y) the offset calculation and dereference operation are *syntactically separable*, whereas in x[y] they are not.  So in the case of x[y] the compiler can know that this is an offset+dereference operation without having to do any analysis.  *(x+y) is just a special case of *(f(x,y)), and figuring out whether or not that is equivalent to x[y] is uncomputable in general (it’s equivalent to the halting problem).
This fact is reflected in real systems.  The following code:
int main(int argc, char* argv[]) {
  int x[100];
  int y = x[101];
  int z = *(x+101);
  return y+z;
generates one warning under clang, not two (and zero under gcc even with -Wall).
Of course, both clang and gcc will happily compile:
  int y = x[argc];
with no warnings or runtime checks.

@_date: 2016-09-17 16:47:30
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
What matters here is that you get an array bounds warning for x[101] but NOT for *(x+102)
The use-of-uninitialized-data warnings are a red herring.  They have nothing to do with the point I was trying to make about the effective difference between x[y] and *(x+y).

@_date: 2016-09-17 16:51:42
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
What version of gcc are you using?  With 4.8.4 I get no warnings:
[ron at stage:~]$ cat foo.c
 int foo(a) { return a+1; }
[ron at stage:~]$ gcc -Wstrict-overflow=5 -c foo.c
[ron at stage:~]$
You are right, however, that this is indeed undefined behavior, although I think you’d be hard pressed to find a compiler that actually took advantage of that fact to do anything other than do the math mod 2^n for some appropriate value of n.  But still, maybe if this was in people’s face enough that would motivate them to fix the broken standard.

@_date: 2016-09-17 17:00:42
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
This has nothing to do with secure erasure, this has to do with avoiding side-channel timing attacks introduced by the compiler eliding what it thinks is dead code.
Are you seriously disputing that C defines x[y] and (x+y) to be equivalent operations?
The legality of bounds checking and the flatness of the address space are two completely orthogonal issues.
That is certainly true, but that’s not the point.  The point is whether or not the compiler can tell whether or not what you are doing is “legitimate”.
I want *all* data access (reads and writes) to be checkable, either at compile time or run time, to make sure that they only access memory that has been properly allocated.

@_date: 2016-09-20 08:18:46
@_author: Ron Garret 
@_subject: [Cryptography] defending against common errors (was: secure 
The same day this thread started, I suggested:
with three associated versions of “erase” depending on exactly how paranoid you wanted to be.
Bear’s response was (in part):
But then he never followed up on that, nor did anyone else.  So AFAICT, this solution is still viable.
Note that if Bear’s objection is correct then you are necessarily hosed unless you control the allocation of the buffer you are trying to erase so you can make it unambiguously volatile.  Because of this I’m pretty sure that Bear’s objection is not correct because if it were, not being able to reliably erase a buffer would be the least of your worries.
Either way, this ought to have been a much shorter conversation than it has been.

@_date: 2016-09-21 10:07:36
@_author: Ron Garret 
@_subject: [Cryptography] Ada vs Rust vs safer C 
The salient difference being that in the software world it’s actually possible to just go build your own plane the way you like it.

@_date: 2016-09-24 16:08:28
@_author: Ron Garret 
@_subject: [Cryptography] Spooky quantum radar at a distance 
Anyone interested in more details may want to look at:

@_date: 2016-09-28 09:33:24
@_author: Ron Garret 
@_subject: [Cryptography] Use Linux for its security 
I’m working on reducing complexity every single day:

@_date: 2016-09-28 09:44:07
@_author: Ron Garret 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct DSA 
But whatever.
I have never understood this phenomenon of treating specs as if they were holy writs handed down by God.  Any software that either generates or accepts DSA keys with low multiplicative orders is clearly broken.  Asking whether that's a bug in the spec or a bug in the code is like asking whether the deck chairs on the Titanic should have been arranged differently.  It’s broken.  That’s all that matters.
In this case the fix is trivial: add a line of code that rejects any key whose multiplicative order is too small.

@_date: 2016-09-28 22:44:30
@_author: Ron Garret 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct 
Not everything requires bringing out the big number theoretic guns.  Here is one of the keys in question:
Private-Key: (1024 bit)
priv: 13 (0xd)
pub:  1 (0x1)
P:       00:90:df:c4:88:8f:91:41:57:b9:b0:9d:9f:8d:53:
    ce:3b:ac:8e:f9:59:7a:47:08:c7:3d:6f:ab:45:e2:
    0b:3e:6f:da:a8:d0:08:7a:9f:f0:bb:19:9b:c8:60:
    d1:af:91:81:03:bf:2c:f2:dd:0e:09:fc:db:4a:1d:
    ab:a6:99:17:f5:a2:f4:0c:b1:2c:5e:f4:9d:21:2d:
    9c:0b:4f:b6:f0:b0:0c:a0:87:36:b3:f0:ff:cc:a1:
    d8:a3:32:8b:cb:b6:e0:3a:a5:a0:8f:ad:43:9f:fc:
    f6:de:28:18:da:af:86:80:c2:6e:63:95:0a:4e:0f:
    9b:00:09:1a:b6:74:34:ce:a9
Q:       00:d7:14:b8:0b:1d:52:ff:da:64:7b:ba:c7:20:00:
    98:f9:fc:4c:b2:4b
G:    1 (0x1)
writing DSA key
-----BEGIN DSA PRIVATE KEY-----
-----END DSA PRIVATE KEY——
If it takes you more than three seconds to figure out what is wrong with it you need to turn in your tinfoil hat.

@_date: 2016-09-29 09:18:08
@_author: Ron Garret 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct DSA 
Like what?  (Let me be clear what I mean here: I don’t dispute that openssl is a mess.  What I’m asking is: what “other subtleties" are relevant to these weak DSA keys?)
I don’t understand this question.
Sure, this is a bug in openssl.  But there are two possible bugs here:
Possible bug  is that openssl does not detect weak DSA keys.  This is clearly the case.  But I would say that this is not a particularly serious problem, and it’s not particularly hard to fix (assuming anyone actually cares).
Possible bug  is that openssl actually *generates* weak DSA keys.  That would be a much more serious problem.  But AFAICT there is no evidence for this.  The provenance of these keys is not known.  The most likely explanation for the existence of these keys is that someone designed them.  The fact that it is possible to create weak DSA keys is not news.
The problem with those keys is not quite as obvious as it was in the first case, but it’s still pretty obvious.  It makes a nice little puzzle to figure it out.

@_date: 2017-04-02 21:00:54
@_author: Ron Garret 
@_subject: [Cryptography] a new attack on ECDLP 
The last sentence of the paper is all you need to know:
"The largest group of elliptic group of prime order that we tested is 129159847, in which a discrete logarithm problem was solved.”

@_date: 2017-04-30 09:17:01
@_author: Ron Garret 
@_subject: [Cryptography] 
=?windows-1252?q?tor_for_AES_Counter_Mode=85?=
Why not just add the counter to the key and then encrypt?

@_date: 2017-08-01 10:37:00
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Please critique this 
I want both speed and efficiency.  I want an electronic currency that is as fast, reliable, and cheap as cash.  If we are in physical proximity to each other then I can transfer a dollar to you by handing you a dollar bill.  That process takes a few seconds, and it’s very reliable and efficient.  There’s some fixed overhead in the form of taxes to support the TTP (the U.S. government), but the incremental cost per transaction is essentially zero.
Mining-based cryptocurrencies can never be fast or efficient.  Being slow and inefficient is the very foundation of their security model.
The reason I don’t want a consortium of users is that I want it to be at least potentially as universal as cash.  I want to be able to pay anyone using this system just as I can with cash.  The vast majority of users are not technically savvy enough to participate in a ctyptocurrency consortium.  Even if it could be made to work somehow, I don’t really see the benefit.  I’d rather pay someone who knows what they’re doing operating in a competitive market to provide this service.  I believe in specialization and division of labor.

@_date: 2017-08-14 10:20:21
@_author: Ron Garret 
@_subject: [Cryptography] National Navajo Code Talkers Day 
And how was it determined that this word means “shark”?

@_date: 2017-01-31 23:03:53
@_author: Ron Garret 
@_subject: [Cryptography] FOI of NSA's cryptanalysis of DES 
I have no FOIA experience, but if someone is willing to do the legwork I’m happy to be the front man.

@_date: 2017-01-31 23:05:37
@_author: Ron Garret 
@_subject: [Cryptography] FOI of NSA's cryptanalysis of DES 
Doh!  That’ll teach me to respond to a crypto posting before reading all the way through my feed.  Turns out you don’t need to be a US citizen to file a FOIA request.

@_date: 2017-02-01 10:32:07
@_author: Ron Garret 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
The HMAC spec says that the key should be hashed if it is longer than the block size but zero-padded to the right if it is shorter.  Why?  Why not hash a short key?  That would simplify the code without (AFAICT) compromising security in any way.

@_date: 2017-02-02 14:14:45
@_author: Ron Garret 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
If that’s your quality metric, why hash a long key then instead of just (say) truncating it?

@_date: 2017-02-16 11:27:23
@_author: Ron Garret 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
The “extra layer” (assuming you mean the LED and user input button) is actually crucial to security.  Without it, U2F adds zero security because an attacker who has compromised your client machine can access the dongle just as easily as you can.  Pushing the button is the only thing the attacker cannot do without your help, so that is the lynchpin of U2F’s security.  AFAICT the Yubikey Nano doesn’t have it, which makes it worse than useless (worse because it provides the illusion of security without any actual security).

@_date: 2017-02-16 11:32:20
@_author: Ron Garret 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
Or you could just buy one of these:
(Disclosure: this is my product.)
It runs U2F out of the box.  It’s user-flashable.  The code is 100% open source.

@_date: 2017-02-17 07:49:48
@_author: Ron Garret 
@_subject: [Cryptography] Verification of Identity 
And how are you going to verify that these email addresses are not all controlled by a single person trying to forge an identity?

@_date: 2017-02-20 09:46:55
@_author: Ron Garret 
@_subject: [Cryptography] Lower bound for the size of an RSA key's private 
The section on “lower bounds” is towards the bottom.

@_date: 2017-02-24 17:27:52
@_author: Ron Garret 
@_subject: [Cryptography] Just in case it isn't obvious... 
I am going to make a rather obvious observation about the recent SHA1 collision, but I thought it would be worthwhile making it just in case there were people lurking on this list for whom it is not obvious.
Notwithstanding that there is a site that lets you generate “arbitrary” SHA1 collisions ( there is really only one SHA1 collision currently known.  It consists of a pair of sequences of exactly 320 bytes.  They look like this:
➔ hexdump s1
0000000: 2550 4446 2d31 2e33 0a25 e2e3 cfd3 0a0a  %PDF-1.3.%......
0000010: 0a31 2030 206f 626a 0a3c 3c2f 5769 6474  .1 0 obj.<>.st
0000090: 7265 616d 0aff d8ff fe00 2453 4841 2d31  ream......$SHA-1
00000a0: 2069 7320 6465 6164 2121 2121 2185 2fec   is dead!!!!!./.
00000b0: 0923 3975 9c39 b1a1 c63c 4c97 e1ff fe01  .
0000090: 7265 616d 0aff d8ff fe00 2453 4841 2d31  ream......$SHA-1
00000a0: 2069 7320 6465 6164 2121 2121 2185 2fec   is dead!!!!!./.
00000b0: 0923 3975 9c39 b1a1 c63c 4c97 e1ff fe01  .

@_date: 2017-02-27 08:07:55
@_author: Ron Garret 
@_subject: [Cryptography] PAKE for embedded device (<64K RAM)? 
First, in your subject line you mention that your device has <64k RAM.  That is very unlikely to be your limiting factor.  64k RAM is plenty to implement just about any crypto algorithm and protocol (except, obviously, memory-hard password hashes, but those are trivially avoided simply by not using them).
Second, it’s impossible to answer your question without a more detailed description of the application and the threat model.  Who is your adversary and what are their powers and motives?  Do you need to protect against DOS attacks?  Impersonation attacks?  With or without physical proximity?  Do you really mean PAKE and not just AKE?  (Embedded devices generally do not have the kind of UI that would let a user enter a password.)

@_date: 2017-02-27 08:14:54
@_author: Ron Garret 
@_subject: [Cryptography] Just in case it isn't obvious... 
That would only work for new repos.  Your new patched git would break on existing repos.
The fundamental problem with fixing git is not that it commits to SHA1 as the One True Hash, it is that it assumes that there is a One True Hash to begin with.  That assumption is woven deeply into the structure of git, even down to its data representations.  Git repos have no place to store information about which has is being used, even on the repository level, let alone on the individual blob level.
(Just on general principle, it’s a pretty good bet that if there were a simple solution that actually worked, Linus would have adopted it long ago.  It’s not like the existence of a SHA1 hash is a surprise.)

@_date: 2017-02-27 09:37:31
@_author: Ron Garret 
@_subject: [Cryptography] Soliciting code review for a double-ratchet 
I have written an implementation of the Signal double-ratchet in Javascript using tweetnacl as the cryptographic primitives.  If anyone is willing to do a code review that would be much appreciated.
I’ll give a free SC4-HSM to anyone who finds an exploitable security flaw.  I know that’s not much of a bug bounty, but this is open-source code so it’s all charity work anyway.

@_date: 2017-02-27 17:21:12
@_author: Ron Garret 
@_subject: [Cryptography] jammers, nor not 
Wrapping your gadgets in aluminum foil should be pretty effective at (ahem) foiling their attempts to contact a wifi hub.

@_date: 2017-02-28 10:35:44
@_author: Ron Garret 
@_subject: [Cryptography] jammers, nor not 
I’m pretty sure you’ll be able to buy a non-connected toothbrush for some time yet.  If you’re not as optimistic as I am, you can stock up now.  Toothbrushes have extraordinarily long shelf lives.  Your toothbrush collection will almost certainly outlive you, if not all of modern civilization.
TV could be a real problem though.  You may have to do without.
As for laundry, may I suggest:

@_date: 2017-02-28 12:50:56
@_author: Ron Garret 
@_subject: [Cryptography] Attaching the signing public key to data being signed 
One of the things you have to decide when designing a signature protocol is exactly what is to be signed.  Simply signing a raw document is a bad idea because that leaves you vulnerable to chimera/Dali attacks (  At the very least you need to integrate the mime-type (or something equivalent) and maybe even the file name into the data being signed.
My question is: would it help to also integrate the public key being used to produce the signature into the data being signed?  Are there any attacks that this would help prevent?  Has this construction been studied?  Can anyone point me to a paper?

@_date: 2017-01-10 16:57:45
@_author: Ron Garret 
@_subject: [Cryptography] Elliptic curve primer 
I started working on an elliptic curve primer just before the holidays when I stumbled across this really excellent series of posts by Andrea Corbellini:
When I saw those, I kind of ran out of steam because I felt I was covering much of the same territory and not doing as good a job as Andrea did.  I found myself thrashing over whether or not it was worthwhile finishing my own effort, so I thought I would ask on these lists.  I’ve included my current incomplete draft as an attachment.  If you are interested in this sort of thing I would appreciate it if you could take a look and tell me if you think it adds enough value to be worth finishing.

@_date: 2017-01-14 10:08:29
@_author: Ron Garret 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
How can anyone exchange anything without a trusted third party?  There is nothing special about cryptocurrencies in this regard.  When people engage in a risky transaction they use an escrow service.  Escrow could be implemented with a blockchain to eliminate the trusted third party, but then you have the problem of how to compensate miners.  One way or another you have to pay for the service.
What I still don’t understand is why people don’t want to use trusted third parties.  A TTP is vastly more efficient than a block chain.  If you think about it, a blockchain is just a distributed TTP.  The theoretical advantage of this is that no one actor can defect and enrich themselves by acting alone.  In practice the actual constraint is that the computational cost of defecting is higher than one could reasonably hope to gain by doing so.  It is certainly possible to hijack the block chain, it’s just not lucrative enough for anyone to bother (yet).
It is worth noting, BTW, that the bitcoin block chain is *not* what provides security against theft.  The digital signatures do that, and those can be used with a TTP just as easily as with a blockchain.
BTW, an almost identical issue was discussed to death on this list last June under the subject “Proposal of a fair contract signing protocol.”  The bottom line is: it’s a theorem that it is impossible to design a protocol that has the desired properties if the actions of the actors are interleaved in time unless you use a TTP.

@_date: 2017-01-15 13:37:42
@_author: Ron Garret 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
OK, I agree with all of that.  However: it seems to me that there is another solution to the problem that a TTP can screw you over (which I acknowledge as a very real problem): make the TTP *auditable*, that is, design your TTP protocol in such a way that *if* the TTP defects that defection will be immediately discoverable to anyone who bothers to check.  Now the TTP can only defect successfully if no one bothers to check.  The odds of that are small enough that no rational TTP will take the risk *even if no one is actively auditing them*.  The net result (I claim) will be a system that is as reliable in practice as a blockchain, but a lot more efficient.
What’s wrong with that argument?

@_date: 2017-01-15 13:49:56
@_author: Ron Garret 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
Sure, but that is obviously not what I was referring to.
As an aside, the underlying situation with respect to inflation is actually exactly the same for both bitcoin and fiat currencies.  Bitcoin could inflate if a majority of the mining pool agreed to a change in the protocol.  Likewise, inflation in fiat currencies can be halted by government decree.  The presence or absence of inflation is simply a policy decision.  It is completely orthogonal to the technology used to implement and enforce that decision.

@_date: 2017-01-15 14:41:01
@_author: Ron Garret 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
You misunderstood my claim.  I did not mean that the rate of inflation itself is a policy decision.  I meant that the choice of *process* by which the rate of inflation is decided is a policy decision.  Bitcoin’s policy is an algorithm.  The USD’s policy is that the board of the Fed decides.  Either of these could be changed if enough people decide it should be changed.

@_date: 2017-01-23 14:25:47
@_author: Ron Garret 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
A defense against this is to add a step to the build process that computes a keyed hash of the source code and adds a generated file to the source tree that binds a global variable to this hash.  This will cause the hash of the binary to change in an unpredictable way upon any change to the source, whether the RCS detects it or not.

@_date: 2017-01-27 13:03:50
@_author: Ron Garret 
@_subject: [Cryptography] HSM's to be required for Code Signing 
Unless the HSM has its own I/O, like this one:
When asked to sign, the SC4-HSM displays the hash to be signed on the built-in display and waits for the user to confirm by pressing a button on the HSM.
(Disclosure: this is my product.)

@_date: 2017-01-29 19:11:47
@_author: Ron Garret 
@_subject: [Cryptography] HSM's to be required for Code Signing 
You could also build such an audit trail into the HSM itself.

@_date: 2017-07-04 10:31:27
@_author: Ron Garret 
@_subject: [Cryptography] actual journalism, was LRB article, 
[Removing cipherpunks from the cc list since I am not a subscriber]
The original bitcoin paper by Satoshi ( explicitly dealt with this issue in section 7, and specifically says:
"Once the latest transaction in a coin is buried under enough blocks, the spent transactions before it can be discarded to save disk space. To facilitate this without breaking the block's hash, transactions are hashed in a Merkle Tree [7][2][5], with only the root included in the block's hash.”
What happened?

@_date: 2017-07-09 15:29:11
@_author: Ron Garret 
@_subject: [Cryptography] Trump wants to collaborate with the Russians to 
In case you missed it…
"Putin & I discussed forming an impenetrable Cyber Security unit so that election hacking, & many other negative things, will be guarded..and safe."
Yes, it’s every bit as absurd as it sounds.  Happily, it is being widely recognized as absurd:
Notwithstanding that this already is getting a fair amount of press, as a U.S. citizen I think it’s important to draw as much attention as possible to this, so I would like to encourage anyone on this list who is inclined to comment publicly about this to do so.  The future of (whatever remains of) our democracy may be at stake.  If anyone here with credentials is willing to go on the record with a journalist on this matter please contact me.

@_date: 2017-07-09 19:30:06
@_author: Ron Garret 
@_subject: [Cryptography] Trump wants to collaborate with the Russians to 
It would be terrific we could all Just Get Along as Rodney King once put it.  And the U.S. is certainly not without sin.  But the sad fact of the matter is that at present the U.S. and Russia are still adversaries, and have been for over 70 years.  That kind of legacy is not so easily overcome.  And you certainly don’t transform a long-standing adversary into an ally simply by treating them as a trusted party in the design of your defenses.

@_date: 2017-07-10 08:42:58
@_author: Ron Garret 
@_subject: [Cryptography] Trump wants to collaborate with the Russians to 
[I’ve taken the liberty of re-formatting your reply so that it conform’s to Perry’s rule against top-posting.]
Do you really not see any other possible damage that this collaboration could do other than having compromised software actually used in elections?
Do you really believe that there are no more effective ways of transforming our diplomatic relations than collaborating on a matter of national security in which the Russians have already attacked us once?
I would have thought this would be obvious to the people on this list, but maybe it needs stating: the premise behind cryptography is that you have an adversary who is bound by technical limitations but not by any other rules.  They will lie, cheat, and steal in order to compromise you.  That includes, but is not limited to, pretending not to be your adversary, which is the basis of the most elementary attacks: phishing and social engineering.
Yes, it is true that sometimes one can turn the tables on an adversary by accepting their false proffer of friendship (e.g. 419eater.com) but this is fraught with peril and requires a fair amount of skill.  I’m trying very hard to keep my political views out of this discussion, but I’m sorry, I don’t see how a reasonable person can possibly believe that Donald Trump possesses the necessary skills.

@_date: 2017-07-15 09:11:20
@_author: Ron Garret 
@_subject: [Cryptography] Defeating timing attacks 
Like this for example:
(Disclaimer: this is my product.)
The SC4-HSM is not currently packaged in a Faraday cage because that would obscure the display.  But if you’re really worried about tempest attacks you could make a little hood for it out of aluminum foil (a literal tin-foil hat!)
The current firmware doesn’t do this, but modifying it to do this is trivial as there is a hardware real-time clock on-chip (along with a hardware TRNG, so no need to worry about whether or not /dev/urandom does the Right Thing).
(However, the current firmware *is* based on TweetNaCl so it should be constant-time regardless.)

@_date: 2017-07-15 08:59:38
@_author: Ron Garret 
@_subject: [Cryptography] Checkoin: physical crypto-cash 
I can’t help but picture the following scenario: I accept a coin of the sort shown in the animation on your web site, the one with the scratch-off scratch-to-cash doodad on the back side.  I decide to cash the coin, so I scratch off the obscuring layer to reveal the word, “SUCKER!” written underneath.

@_date: 2017-07-15 12:16:17
@_author: Ron Garret 
@_subject: [Cryptography] Checkoin: physical crypto-cash 
You can’t be serious.  Did you even bother to do a Google search for “counterfeit Golden Eagle”?

@_date: 2017-07-16 09:10:32
@_author: Ron Garret 
@_subject: [Cryptography] Checkoin: physical crypto-cash 
The attack model is not that someone counterfeits a coin ab initio.  The attack model is that someone *copies* a *legitimate* already-issued coin and circulates the copy *instead* of the original legitimate coin.

@_date: 2017-07-18 12:05:27
@_author: Ron Garret 
@_subject: [Cryptography] Raspberry Pi-like FPGA ?? 
You can noodle around with FPGAs on AWS now:
But I really would like to know why you think that a dedicated ARM chip with a crypto app running on the bare metal is not good enough.  You can get a Rapsberry Pi Zero and put it in a metal case for about $10.  Why is that not good enough?

@_date: 2017-07-18 19:01:46
@_author: Ron Garret 
@_subject: [Cryptography] [Crypto-practicum] Please critique this 
Yes, I thought of that.  That’s why the hash at the tip of the tree is periodically committed to a publication of record.
The strictly serialized design prevents records from being omitted or inserted.  Every entry commits to the signing key that will be used for the next entry, so you can always trace back a unique history that led to any entry.
The TTP could fork the chain by signing two entries with the same key, but that would be discovered no later than at the periodic commitment to the publication of record.
Thanks for these pointers!
BTW, I noticed that you cc’d cryptography at metzdowd.com, which was (intentionally) not included in the original distribution.  For the benefit of readers on that list, here is the original post:
Bitcoin achieves consistency through a combination of proof-of-work (mining) and a longest-chain Merkle tree.  This eliminates the need for a trusted third party to achieve a shared consensus. But it comes at a high cost: mining is inherently expensive (it is the cost of mining that is the principal defense against attacks), and hence transaction confirmations are slow (tens of minutes).
If one’s risk posture or ideology does not lead them to reject trusted third parties (TTPs) then more efficient solutions are possible.  If you really trust the TTP then you just take them at their word that the ledger they publish is authentic.  This is an efficient solution, but this gives the TTP too much power: the TTP can easily cheat without being caught, and so the incentive to cheat is high. But if the ledger is structured so that it is self-authenticating like the BitCoin blockchain, then any attempt by the TTP to cheat will be immediately detected.  This removes the incentive to cheat, and so you get the best of both worlds: reliable distributed consensus that is both fast and efficient.
Here’s the design: Ledger entries are arranged in a linear linked list.  Every entry is signed by the TTP and contains the hash of the previous entry.
The twist is the following:
1.  Every entry is signed with a different key
2.  In addition to the hash of the previous entry, each entry also contains the public key that will be used to sign the *next* entry.
Hashes are published electronically in real time and periodically committed to publications of record.
The reason for committing to the next key in the current entry is that it allows the chain to maintain continuity while still allowing the key generator to be re-seeded between any two transactions. The TTP *could* use a fresh key for every transaction, but in practice they would probably use keys generated by a CSPRNG which is periodically re-seeded.  The reason for this is that losing the next key before it is actually used to sign the next transaction is  catastrophic: the chain is irretrievably broken, and some other process must be invoked to start a new chain, link it to the old one, and establish trust in the authenticity of the new chain.  Because of this, any key material must be securely written to non-volatile storage (with multiple backups) before it is used.  Then it must be securely erased after it is no longer needed.  It might be possible to arrange for this to happen fast enough that a truly fresh key can be used for every transaction, but it is not necessary.  Re-seeding the key generator every few minutes should be plenty.  I can’t think of any attacks that this would enable that could not be carried out with equal ease against a key generator that is re-seeded more quickly.
The main advantage of this design over mining-based blockchains is that transactions can be processed almost instantaneously.  The limit on latency is the time it takes to generate a signature. Throughput can be increased virtually without limit by pipelining the process of signing entries.  To make this work, the previous-entry hash cannot includes the signature.  It can only be a hash over the content of the entry.  At this point, the limiting factor on throughput becomes hashing, which cannot be pipelined since each entry must contain the hash of the previous entry.  However, if a product based on this design ever encounters that as a limiting factor that would be a very nice problem to have.
Committing to the signing key for the next ledger entry in the current entry prevents the TTP from publishing two different versions of the ledger.  Any attempt by the TTP to cheat in this way would immediately be caught and proven by exhibiting two ledger entries signed by the same key.  Any entry signed by a key other than the next key would be immediately detected as a forgery.
The TTP can still unilaterally deny service by delaying the recording of an entry, and they could use this as leverage to extract rents.  This could be addressed by entering into a contractual obligation with users not to engage in such practices, and by establishing a competitive marketplace for TTP services.

@_date: 2017-07-26 09:39:25
@_author: Ron Garret 
@_subject: [Cryptography] Anyone interested in a cheap security module for 
Thank you!  :-)
Can you elaborate on what you mean by this?  The SC4-HSM can be made secure against an adversary with physical possession of the device.  If you’re willing to type in a pass-phrase, it can even be made secure against an adversary capable of decapping the SoC.  What more do you want?

@_date: 2017-07-26 13:24:54
@_author: Ron Garret 
@_subject: [Cryptography] Anyone interested in a cheap security module for 
I think we may have a fundamental disconnect here.  The SC4-HSM is not intended to be used in a data center.  It’s intended to be used by an individual who has it in their physical possession and plugs it in to a USB port of a host that is also by necessity in their physical possession.  So there is no “network” that an attacker could possibly control.
Also, the SC4-HSM has an on-board display and push-buttons which are integral to its security model.  You could emulate an SC4-HSM, but absent some really major breakthroughs in virtual reality technology you could not hide the fact that you were emulating it.  :-)
Yes.  There are two ways to disable the debug port, one temporary, the other permanent.  These are described in detail in the documentation.
I don’t understand this at all.  How can the owner of a device be an attacker?  That seems impossible by definition.
Yes, this is exactly how the reversible lock-down feature works.
The SC4-HSM does not have this, though it could be added.  However, this would undermine what I consider to be one of the SC4-HSM’s significant features: the ability to open the device and visually inspect the circuitry to insure that what is in the package is what is supposed to be there.  If you’re willing to trust your vendor then you might as well just use an iPod Touch.
The SC4-HSM gives you access to DFU mode, which gives you direct hardware-level access to the flash.  There is no bootloader.  (You could, of course, upload one if you wanted to, but why would you want to?)
In addition, all of the current firmware and the tool chain is open-source.  It doesn’t get any more verifiable than that.  (Note however that the current version of the firmware has NOT been audited.  Verifiable != verified.)
The SC4-HSM is designed to minimize the trust you need to put in third parties.  The only part of the design that is not independently verifiable is the SoC chip itself.  Everything else is either open to inspection or under the user’s direct control.  This is *more* secure than firmware signatures because those require that you trust both the bootloader and the signer of the firmware.
Yes.  See above.
The STMF405 does not have AES and SHA in hardware, but the STMF415 does (that is the only difference between these two chips).  The current production run has 405 chips but I would be more than happy to make a batch of HSMs with 415s if that were desirable.  However, the limiting factor on performance for any given application is much more likely to be the USB2 interface than the fact that you have to do your crypto in software.
The SC4-HSM computes an Ed25519 signature in about half a second.  I haven’t timed P256, but it seems about the same.
As you have noted, the SC4-HSM has more than enough RAM and Flash for your needs, but it was not designed for high-throughput.  It was designed to be e.g. a U2F token or a bitCoin wallet for personal use by a single individual who carries it around in their pocket.  A significant portion of the cost is in the display (not just the hardware but the assembly costs associated with it) which you don’t need in a data center.
However, if you want to explore a low-cost HSM for data center use I would be more than happy to discuss it with you.

@_date: 2017-06-18 14:24:46
@_author: Ron Garret 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
That won’t help unless you have designed and fabricated it entirely by yourself using 100% hand-operated tools, and if you do that then it will almost certainly be insecure because of something you overlooked.  It’s possible to hide a back door even in a mechanical device if it gets complicated enough.

@_date: 2017-06-27 17:44:02
@_author: Ron Garret 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Thomas may know what he’s talking about, but his advice is based on some tacit assumptions which may not always be true, and which a reasonable person might choose not to accept.
Thomas’s argument is: there are a zillion ways to screw up a CSPRNG, so it’s best to just have one very carefully vetted implementation that everyone uses.  That is /dev/urandom.
It’s a reasonable argument if you’re a security expert, less so if you’re a user.  If you’re a security expert you know how to vet your /dev/urandom, so if everyone uses /dev/urandom you have now eliminated multiple possible points of failure.  If you’re a user, you have either N vendors (one for each of your security-dependent applications) or N+1 vendors (one for each of your security-dependent applications plus one for your /dev/urandom driver).  If you have the choice of trusting N vendors vs trusting N+1 vendors it is not unreasonable to choose the former, particularly if N is small.

@_date: 2017-06-28 09:09:51
@_author: Ron Garret 
@_subject: [Cryptography] Possible SHA2 vulnerability 
Turns out to be a false alarm.
Oh well, learn something new every day.

@_date: 2017-06-28 17:03:00
@_author: Ron Garret 
@_subject: [Cryptography] OpenSSL CSPRNG work 
So?  What does that have anything to do with what I said?
Security is all about avoiding hypothetical problems that might never occur.  Having an attacker insert a back door into a /dev/urandom driver is not an unreasonable threat model for some people.

@_date: 2017-06-28 17:16:42
@_author: Ron Garret 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Actually, I am too for the most part.  But let’s not forget the context of this discussion.  This discussion originated because a member of the OpenSSL development team asked for some advice.  The OpenSSL development team is not a bunch of naive users.  Their expertise is probably comparable to the expertise of the people who wrote /dev/urandom for whatever OS you are using.  So the risk of the OpenSSL team screwing something up is probably comparable to the risk of the /dev/urandom team screwing something up, which is to say, the risk is low, but not zero.
Whatever the risk is, because of the way security risks compose, it is a defensible position for OpenSSL to use its own CSPRNG rather than /dev/urandom.

@_date: 2017-03-08 13:11:02
@_author: Ron Garret 
@_subject: [Cryptography] encrypting bcrypt hashes 
As with all questions of this type: what is your threat model?  Who is your adversary, and what are their capabilities?
Generally if you are hashing passwords/PINs it means that part of your threat model is that your adversary can break in to your servers (because if your adversary can’t do that then you can safely store passwords in the clear).  In which case my next question would be: where are you storing your encryption key?  If it’s on the same server as your database (or in another server that your adversary can break in to) then encrypting does you no good.
BTW, the problem you have here is not a small key space, but a small space of possible plain texts.  Not quite the same thing.  Designing a system that stores PINs against an adversary who can break in to your servers will be quite challenging.  Even a slow hash will not help much if the brute-force search space is only 20 bits (6 decimal digits).

@_date: 2017-03-08 13:26:36
@_author: Ron Garret 
@_subject: [Cryptography] encrypting bcrypt hashes 
Hoo boy.  Then it doesn’t much matter what you tell them because, as Ray Dillinger already pointed out, this is the limiting factor.
Not if your adversary steals the encryption key along with the hashed PINs.  That made it sound like latency matters.  10 seconds is a pretty long time to make a user wait.

@_date: 2017-03-13 11:05:08
@_author: Ron Garret 
@_subject: [Cryptography] USB firewall/condom HW/SW 
There is a defense against the TT attack.
A more difficult problem is how do you know you can trust the STM32F4 processor.

@_date: 2017-03-16 18:21:10
@_author: Ron Garret 
@_subject: [Cryptography] Crypto best practices 
You need to go back and rethink this.  If someone takes this advice as stated they will conclude that AES in ECB mode is preferable to using the NaCl library because XSalsa20 is a stream cipher and AES is not.  I think I know what the two of you are trying to say, but if there has ever been an area of human endeavor that demands precision in the formulation of recommendations, it’s this.

@_date: 2017-03-19 12:57:12
@_author: Ron Garret 
@_subject: [Cryptography] Crypto best practices 
Given that the weakness in IVs is not unpredictability but in re-use, I don’t see how encrypting an IV buys you anything.  An encrypted repeating IV is still going to be a repeating IV, and since IVs are not secret, the encryption by itself buys you nothing.  To the contrary, if your cleartext IV is predictable you have just given your adversary an opening for a known-plaintext attack on K.
Why not just set IV= HASH(K, data)?  That’s just the Ed25519 ECDSA nonce trick.  Can anyone here think of a reason why that would not work equally well for IVs?

@_date: 2017-03-24 18:18:41
@_author: Ron Garret 
@_subject: [Cryptography] Google distrusts Symantec for mis-issuing 30, 
Or you can use Chrome and manage your trust roots manually.

@_date: 2017-03-29 08:56:55
@_author: Ron Garret 
@_subject: [Cryptography] "Perpetual Encryption" 
How many points for “post quantum”?  (How many points for putting both “post quantum” and “agile” in the name if your company?)

@_date: 2017-11-13 18:11:45
@_author: Ron Garret 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I got fed up with the complexity of ASN.1 and DER and designed my own binary serialization format for my implementation of the Signal double-ratchet.  The only documentation is in the in-line comments of the original implementation, which was done in Common Lisp:
Docs start at line 82.  The Javascript version is at:
The format is not quite unambiguous.  Binary fields whose length is a power of 2 can be encoded in two different ways.  But that is the only ambiguity, and it could easily be fixed if needed.
Feedback would be very much appreciated.

@_date: 2017-11-19 23:29:10
@_author: Ron Garret 
@_subject: [Cryptography] Is ASN.1 still the thing? 
This is the reason I designed my serialization format so that LengthA is a count of the number of fields, not the total length of the structure.  (I am obsessive about DRY code and designs.)
In case you missed it, the description can be found in the comments of the reference implementation:
Feedback would be appreciated.  If you don’t like ASN1 you might like my design.  It was born of a frustration with ASN1’s complexity.

@_date: 2017-11-25 17:18:19
@_author: Ron Garret 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I haven’t.
Well, this format is not intended for “modern data interchange”, it’s intended for encoding data for crypto protocols, so it’s intentionally minimal.  The idea is to keep the potential attack surface as small as possible.  It’s intended for encoding message headers, not payload data.  I’m not aware of any crypto protocols that require floats or arbitrary dictionaries.  Some protocols require time stamps but there are too many different ways to encode those to bake one format into the standard.  If you need a datetime you can either encode it as an integer, or define a structure type for it.
BTW, it’s actually pretty straightforward to send arbitrary key-value dictionaries using my encoding: all you have to do is define a key-value structure, and then assemble a list of those.

@_date: 2017-10-05 12:02:01
@_author: Ron Garret 
@_subject: [Cryptography] Double ratchets, useful or PITA? 
If it’s helpful, I wrote a Javascript reference implementation of the Signal double-ratchet:
For session-based comms like real-time chat I think it’s probably worthwhile.  For asynchronous comms, probably not so much.  IMHO.

@_date: 2017-10-12 23:08:42
@_author: Ron Garret 
@_subject: [Cryptography] ? recommendations for secure communications 
I took a whack at it a while back and implemented a TweetNaCl-based system that runs in a browser.  It got very little positive feedback when I first announced it so I stopped working on it, but it would not take a lot of encouragement for me to pick it back up again.

@_date: 2017-09-13 11:45:36
@_author: Ron Garret 
@_subject: [Cryptography] letsencrypt.org 
I haven’t contributed, but I have been using it since it was released and it has always worked flawlessly for me.

@_date: 2017-09-13 14:53:24
@_author: Ron Garret 
@_subject: [Cryptography] letsencrypt.org 
Right.  An attacker who gets access to any machine that has a DNS record for your domain can get a cert for your domain using LE.  This is true whether or not you use it (because an attacker can just install it themselves) so this is not a good reason not to use it.

@_date: 2017-09-30 15:26:42
@_author: Ron Garret 
@_subject: [Cryptography] Crypto basic income 
The problem is that it is not enough to have a unique identifier.  What you really want is a unique identifier that is *bound* to an individual *person*.  Fingerprints and iris scans are attractive because these things originate from physical objects (fingers, eyes) that are (generally) physically attached to bodies that house brains, and it is this physical attachment that provides the binding between the fingerprint and the person it belongs to.
The connection between your brain and your DNA is very different.  Your brain was built by your DNA, but it is not physically bound to your DNA the way it is physically bound to your eyes and fingers.  Your DNA can escape your body in ways that your eyes and fingers generally can’t, at least not without physical violence.  But if I can get a discarded soda can or discarded hair out of your trash I can produce copies of your DNA that are as indistinguishable from the original, because DNA is just digital data rendered in a particular medium.
The only way you could make it work is if you could reliably extract a sample from your body and sequence it in such a way that the provenance of the sample all the way to the output of the sequence was not in doubt.  That is very hard to do.

@_date: 2018-08-18 15:47:30
@_author: Ron Garret 
@_subject: [Cryptography] God Mode backdoors 
4.  Use hardware targeted specifically at non-consumer markets where security actually matters.  This is no guarantee, of course, but it’s much less likely that a company would tolerate a back door in such a device because, were it to be discovered, it would probably bankrupt the company.
One of the reasons I chose the STM32F405 for the SC4-HSM is that it specifically offers secure delivery of embedded code as a documented feature.  Medical device companies rely on this to secure extremely valuable trade secrets.  A back door would be grounds for a very costly class action lawsuit.

@_date: 2018-08-19 09:30:40
@_author: Ron Garret 
@_subject: [Cryptography] God Mode backdoors 
Sure.  But the Chinese government is neither omniscient nor omnipotent.  Most chips go from the manufacturer to a distributor.  Many of those distributors are not in China.  I don’t see any way that the Chinese government could specifically target chips destined for a particular niche application.
No, when I say “where security actually matters” I mean where a security compromise would cost rich people (including but not limited to rich Chinese people) substantial amounts of money.  That is the force that moves the world nowadays.

@_date: 2018-02-17 09:05:43
@_author: Ron Garret 
@_subject: [Cryptography] Quantum computers will never overcome noise 
The limiting factor for going 3D is not fabrication, it’s heat dissipation.

@_date: 2018-05-02 11:45:52
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
I have a “sacrificial iPod” that I don’t use for anything mission-critical in order that I can update it and evaluate the latest Apple software without having to worry about bugs and backwards-incompatibility (which are serious issues in the Apple ecosystem nowadays).  I also have about a dozen other Apple devices.  All of them are logged into iCloud to prevent the denial-of-service attack described here:
None of these devices have iCloud Keychain enabled except the iPod.  Nonetheless, the latest iOS update (11.3) includes a new password manager feature, and that drew my attention to the fact that somehow ALL of the passwords on ALL of my machines were resident on the iPod, and accessible in plain text with nothing more than the iPod’s PIN code (which is only four digits because it’s supposed to be a non-mission-critical machine).
When I discovered this, I disabled iCloud Keychain on the iPod, whereupon it asked me if I wanted to delete all my passwords from the iPod.  Of course I said yes.  Nonetheless, the passwords are still there, and now I don’t know of any way to get rid of them except to manually delete them one by one.  And there are a LOT of passwords.  And not all of them are mine.  It seems to have grabbed every password that anyone who has ever had an account on any of my machines has ever had.
This leaves me wondering:
1.  How did these passwords get there?  It must have been through iCloud Keychain, but that feature is definitely disabled on all my other machines.
2.  Does the fact that I can access stored passwords in plain text without the password that secures my active keychain belie Apple’s claim that these passwords are encrypted and can’t be read by Apple?  I can’t think of any way that Apple could transfer my passwords to my iPod and make them readable without my knowledge if Apple cannot read them.
3.  Is this behavior known?  I can’t find anything written about it on the web.  But it feels to me like this should be a major scandal.  I had no idea that this iPod had such a huge vulnerability, so I hadn't taken any measures to secure it.  If it had fallen into the wrong hands it could have been a total catastrophe.

@_date: 2018-05-03 23:35:59
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
I actually tried that today and got some ambiguous results.
I turned off iCloud keychain on the iPod.  As noted earlier, it offered to delete all the stored passwords, and offer which I accepted, but it didn’t actually work.  So I manually deleted all the passwords.  This took a long time.  The settings app crashed several times, and I had to delete in small batches to avoid this.
Then I turned iCloud keychain back on.  Two passwords re-appeared.  Unfortunately, they were two extremely sensitive ones!
I tried two or three times to delete these rogue passwords, and on the last attempt they stayed gone.
Further investigation will have to wait until I can set up a new Mac with no sensitive information on it.  I don’t dare turn on iCloud keychain on a machine that has passwords that I actually care about.

@_date: 2018-05-04 12:12:45
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Yes, I know this.  I am “the poor guy”.  That is my blog.  And, BTW, there’s a sequel:
TL;DR: iCloud lock is actually totally useless unless your machine is stolen by someone who is completely clueless.
So: the reason I log all my devices into iCloud is because that is the only way I know of to insure that they aren’t bound to anyone else’s iCloud account.
Well, yeah, but remember this device was *specifically* purposed to be a sacrificial machine.  It was not supposed to contain any sensitive data at all.  So there was no reason to use a stronger PIN.
My wife and I have separate accounts on all of our machines, and we each have one machine that is designated our primary from which all sensitive work gets done.  We also have separate iCloud accounts.  The iPod was logged in to my iCloud account.  AFAIK it was never logged in to my wife’s account.
The only way I can think of for my wife’s passwords to have ended up on my iPod would be either:
1.  Her user account on her primary Mac somehow logged in to my iCloud account, *and* iCloud keychain got turned on and then turned back off, and then her machine got logged back into her own iCloud account.  This sequence of events is highly unlikely.
2.  My sacrificial iPod somehow got logged in to her iCloud account, snarfed her passwords, and then kept them when it was later logged in to my iCloud account.  IMHO, that would be a serious bug.
There is at least one other possibility: someone could have exfiltrated our passwords through some other means and entered them all into my iPod manually.
Now, of course this is *extremely* unlikely.  But it is possible.  I point this out not to be pedantic, but just to point out that your certainty that “there is no other way for them to get there” is unwarranted.  It is possible that there is yet another way for them to get there that involves neither iCloud nor a password gnome that neither of us has thought of.
FWIW, I do think it’s likely that iCloud plays a prominent role in whatever happened.  But I also think the story is probably more complicated than just “it’s iCloud working as intended."
Again, with all due respect, I don’t see how you could possibly know that.  How could you distinguish what you describe from a MITM attack?
Huh???  Did you leave out the word “not”?
The (erstwhile) scandal is that I, a reasonably tech-savvy and privacy-conscious user, was able to get myself into a situation where ALL of my passwords were:
1.  Resident on an easily stolen device
2.  Without my knowledge (so I took no measures to secure this device) and
3.  Protected only by a 4-digit PIN
Furthermore, I still don’t know the details of how this could have happened.  But however it happened, at no point during the process did I ever get the slightest hint from the system that I was doing anything even remotely dangerous.  So either:
1.  It is easy to get to the very edge of catastrophe without realizing it, and this is *by design*, or
2.  There’s a bug in the system
Notwithstanding the murkiness around some of the details of how I got myself into this mess, I don’t see any other possibilities.
Thanks!  I’ve elided a lot of links that you provided, but everything you said was very helpful.  I appreciate you taking the time.

@_date: 2018-05-08 11:03:23
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
OK, I’ll bite, because this seems like a no-brainer to me.
I want to replace passwords with a protocol that allows me to authenticate by signing a nonce with a secret key that I have previously registered.  And I want the protocol to be open so that I can choose the implementation of my signing software.  And I want to choose an implementation that stores my secret keys in a known location, encrypted by a secret key derived from a master password by a secure KDF.
This, to me, is the Right Answer because:
1.  I only have to remember one password, and I have completely free choice over what that password is.
2.  Other than  the UX is more or less identical to the status quo.
3.  The only way I can be compromised is if the machine on which I store my secret keys is compromised *and* my master password is compromised.  If I am super-super paranoid, I can store my keys on dedicated hardware with its own keyboard for entering my master password.  If I’m not super-duper paranoid, I can just store everything in my regular keychain.
That’s it.  We’re done here.

@_date: 2018-05-08 12:49:33
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Hopefully it was obvious that I was being a little intentionally glib.  I was going for a little (dark) ironic humor.  Nonetheless...
(I’m going to rearrange the order of your list a bit.)
This falls under the purview of my scheme.  Instead of comparing your input password to a stored one, the device would see if the password you entered successfully decrypts your local keychain.
These are the canonical examples of where my scheme would work.  In fact, SSH already uses my scheme if you use ssh keys instead of passwords — which you should.  Password-based ssh auth is insecure.
This use case is a little trickier.  It would require sudo to be re-implemented to authenticate a local user whose credentials are not stored locally.  But it’s doable, and IMHO would be worth the effort.  The hard part is agreeing on a protocol.  The implementation would be straightforward, exactly the same as any other remote authentication (send nonce, check signature).
Sorry, I don’t understand this one.

@_date: 2018-05-08 15:38:49
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Well, it’s optional.  You can protect you store of secret keys however you like, including memorizing them and entering them manually (good luck with that), generating them from separate passwords (less secure, but at least it’s possible), writing them all down on a piece of paper (inconvenient, but doable), or storing them unencrypted on digital media (not advisable, but meets your criteria as stated).
My proposal eliminates passwords *at the protocol level* between me and a third-party service that wants to authenticate me.  That’s what matters, because if you don’t do that then you can’t get rid of passwords.
You don’t have to.  You can, as I pointed out before, build a device that you trust to store your passwords.  (In fact, I have a design for such a device ready to go to fab, so the only obstacle to obtaining one is getting enough people to place an order to make it worth my while to do a production run.)
You can build a device that uses any second factor you like to protect your secret keys, limited only by your imagination and your ability to implement things or hire someone to do it for you.  But NONE of it will be possible unless we first change the PROTOCOL that third parties use to authenticate people.  That is more of a political problem than a technical one, but unfortunately that doesn’t make it any easier to solve.
The master password would be the input to a KDF ( in case you don’t know.  This sort of thing is generally taken for granted.)
No.  That’s what the KDF is for.

@_date: 2018-05-09 11:38:12
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
It turns out this is not true.  By sheer coincidence (at least I’m pretty sure it was a coincidence) shortly after starting this thread, my iPod developed a battery problem and needed to be replaced.  (Apple authorized service centers can’t replace the battery, so they give you a new iPod instead.)  I wiped the old iPod before turning it in (i.e. logged out of iCloud and invoked the Reset function from general settings).  I just now fired up the new one they gave me to replace it.  When I did this, a test password that I had entered manually on the old iPod appeared on the new one.  There is no place that password could have been stored other than in iCloud.
Even worse: at one point during the setup process for my new iPod it asked me for the passcode I had set for the old one.  So Apple must have stored that too.  I find that to be particularly disturbing.

@_date: 2018-05-09 12:39:52
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
And it turns out this is documented here:
It gets worse: at the bottom of the page there are instructions for how to remove your iCloud keychain from Apple’s servers.  Those instructions say:
Tap Settings > [your name] > iCloud > Keychain > Advanced.
When I do this (on iOS 11.3.1) there is no Advanced option.
I also just noticed that my weather app settings somehow managed to get transferred from my old iPod despite the fact that I wiped it and never made an iCloud backup.

@_date: 2018-05-09 15:14:18
@_author: Ron Garret 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Assuming you mean this document:
I have not read every word, but I’ve read parts of it.  In particular I’ve read the iCloud section.
But that turns out simply not to be true, as evidenced by both my experience and the document that you just asked if I’d read.  Storing your keychain in iCloud is an advertised feature!
"Keychain recovery provides a way for users to optionally escrow their Keychain with Apple, without allowing Apple to read the passwords and other data it contains. Even if the user has only a single device, Keychain recovery provides a safety net against data loss.”
This is not unreasonable.  What *is* unreasonable is that the instructions for turning this feature off once it has been enabled ( don’t actually work.

@_date: 2018-05-19 15:33:32
@_author: Ron Garret 
@_subject: [Cryptography] Durable HSM with fingerprint reader? 
The hardware you’ve described is an iPhone with touchID.  However...
Whoa, nellie.  How is this device going to distinguish “alive and well” from (how shall I put this delicately?) out of commission and digitally truncated?

@_date: 2018-11-09 16:01:39
@_author: Ron Garret 
@_subject: [Cryptography] Seeking recommendations for a dedicated server/VPS 
Subject line says it all.  My server provider, which I have been happily using for fifteen years (Zerolag), was recently acquired by a Big Faceless Corporation whose people seem to be only marginally competent, so I am on the prowl for an alternative.  My baseline plan is to just use AWS because, although Amazon is the paragon of the Big Faceless Corporation, they are competent and their products Just Work the vast majority of the time.  But I’m willing to pay a considerable premium to be able to call a tech support line and have a competent human pick up the phone.  Does anyone here have a provider they can recommend?

@_date: 2019-08-02 15:21:17
@_author: Ron Garret 
@_subject: [Cryptography] How to convince web site to use HTTPS ? 
Obviously: set up a wifi hotspot with a MITM and demonstrate to them how easy it is to intercept login credentials.  Start your patter with, ?Imagine you?re in an airport or a hotel?"

@_date: 2019-05-28 23:23:54
@_author: Ron Garret 
@_subject: [Cryptography] The race to Quantum machines. 
That’s true, but...
That’s not quite true.  (Or perhaps it’s more correct to say that it’s true, but misleading.)  It’s true in the same sense that the speed of light being a constant is the same in all reference frames is an “assumption.”  The evidence that this is correct is overwhelming, and the evidence that quantum mechanics applies to arbitrarily large systems of entangled states is likewise overwhelming.  All attempts to find an experimental regime where quantum mechanics fails have failed.  If you seek solace against the possibility of cryptographic keys falling to quantum computers you are better off looking for it in engineering constraints than in the prospect of discovering new physics.

@_date: 2019-05-29 08:45:57
@_author: Ron Garret 
@_subject: [Cryptography] The race to Quantum machines. 
Let me be more precise then: QM has been experimentally demonstrated with objects up to 10,000 AMU mass [  The smallest gravitational field that has ever been measured (AFAICT) was produced by a mass of ~10^22 AMU [ (~100 mg).  That’s a gap of 17 orders of magnitude.  It is known that QM and GR are mathematically incompatible with each other, so somewhere in that gap one or the other of the two theories has to give.  But all attempts to find an experimental regime where either theory can be demonstrated to fail have failed, and all attempt to determine whether it is QM or GR that has to give have also failed.  All I’m saying is that, given the above facts, betting the future of digital security on the hypothesis that QM fails before it gets to the point where you can implement Shor’s algorithm is unwise.
Personally, my money is on gravity being quantized, and also that to demonstrate this requires a field strength on the order of what is found near the event horizon of a black hole, so we’re unlikely to see this question definitively settled in a laboratory any time soon.
That has been well understood for decades now.  The seminal papers on decoherence were published in 1970.  For an accessible layman’s account I recommend David Z. Alberts excellent book, “Quantum Mechanics and Experience”, chapter 5.  The short version of the story is that a system decoheres if any of its degrees of freedom become entangled with anything outside of the system (and note that entanglement is not an all-or-nothing phenomenon.  Entanglement is a continuum.)  When that happens, the system considered in isolation is no longer in a pure state and can no longer self-interfere.  The more degrees of freedom a system has, the harder it becomes as a practical matter to keep all of them isolated from (i.e. prevent them from becoming entangled with) their environment.  It really is just as simple as that.
No, the assumption is that breaking RSA will be catastrophic, and so the prospect of developing QC is a cause for concern in the context of a discussion list dedicated to cryptography.  I certainly never meant to imply that that’s the *only* reason anyone should care about quantum computing, but it’s certainly *a* reason.
Yes, I’m not denying that.  All I’m saying is that *if* there turns out to be a insurmountable obstacle to breaking crypto with QC, that obstacle is more likely to be an engineering limitation than a scientific one.

@_date: 2019-09-11 17:09:07
@_author: Ron Garret 
@_subject: [Cryptography] Need some help regaining access to a server 
An acquaintance of mine is a close relative of one of the victims of the dive boat fire that killed 34 people in southern California on Sept 2.  This person left behind a server whose hatches are pretty tightly battened down.  I?ve volunteered to help regain access to the server.  I was able to recover the victim?s ssh key and log in to the server, but unfortunately sudo access is password protected so that has me stuck.  I also have their SHA512-hashed password from a backup.  Is there anyone here who can help either point me to a recent (Debian 3.2.102-1 x86_64) Linux local privilege escalation exploit or a good password cracker that I can use to try to get root on this machine?

@_date: 2019-09-12 08:29:13
@_author: Ron Garret 
@_subject: [Cryptography] Need some help regaining access to a server 
I forgot to mention: the machine is an AWS VM.  The owner took their AWS admin password to the grave with them as well.

@_date: 2019-09-13 10:21:09
@_author: Ron Garret 
@_subject: [Cryptography] Need some help regaining access to a server 
Yes, that?s the backup plan.  But it?s a time-consuming process that we were hoping to short-circuit.  Getting access to the emails is somewhat time-critical.

@_date: 2020-02-15 05:26:02
@_author: Ron Garret 
@_subject: [Cryptography] Extracting TOTP credentials 
I use a client-provided VPN that requires a TOTP token.  I had been using Authy to store and deploy these tokens but recently decided that Authy is no longer trustworthy so I decided to write my own implementation of TOTP.  The problem I?m having now is that I don?t actually know how to extract the TOTP token from the provisioning system.  The TOTP standard calls for the token to be provided in base32 format but that is not what Authy uses (which is one of the many reasons I want to ditch it).  Instead, the key is provided as a sequence of seven four-digit alphanumeric sequences, similar to a Microscof software license key.  Does anyone here know how to convert one of these to a standard TOTP key?

@_date: 2020-02-17 14:29:34
@_author: Ron Garret 
@_subject: [Cryptography] Extracting TOTP credentials 
Never mind, I figured it out.  The alphanumeric sequences ARE the token, you just have to fram them all together and then base32-decode them.  I didn?t think this was the case because I wasn?t expecting the token to be that long.  The token turns out to be 160 bits which seems like a ridiculous amount of overkill to me.  But what do I know?

@_date: 2020-02-17 14:31:20
@_author: Ron Garret 
@_subject: [Cryptography] Extracting TOTP credentials 
Because it auto-updates with no user confirmation and no way to disable.  A few weeks ago it auto-updated itself to a version that won?t run on my OS (I?m still running Mavericks).
