
@_date: 2002-04-16 20:44:06
@_author: Anonymous 
@_subject: Schneier on Bernstein factoring machine 
Bruce Schneier writes in the April 15, 2002, CRYPTO-GRAM,
Does anyone else notice the contradiction in these two paragraphs?
First Bruce says that businesses can reasonably be content with 1024 bit
keys, then he appears shocked that Lucky Green still has a 1024 bit key?
Why is it so awful for Lucky to "still" have a key of this size, if 1024
bit keys are good enough to be "reasonably content" about?

@_date: 2002-04-22 05:27:04
@_author: Anonymous 
@_subject: objectivity and factoring analysis 
But what does that mean, to specify (and stand by) the cost of
construction of a factoring machine, without saying anything about how
fast it runs?  Heck, we could factor 1024 bit numbers with a large abacus,
if we don't care about speed.  A cost figure is meaningless unless in the
context of a specific performance goal.
And yet here you say that it took you completely by surprise when someone
asked how fast the machine would run.  In all of your calculations on the
design of the machine, you had apparently never calculated how fast it
would be.
How could this be?  Surely in creating your hundreds of millions
of dollars estimate you must have based that on some kind of speed
consideration.  How else could you create the design?  This seems very
And, could you clarify just a few more details, like what was the size
you were assuming for the factor base upper bounds, and equivalently for
the size of the matrix?  This would give us a better understanding of the
requirements you were trying to meet.  And then, could you even go so far
as to discuss clock speeds and numbers of processing and memory elements?
Just at a back of the envelope level of detail?
No, Lucky made a few big mistakes.  First, he invoked Ian Goldberg's
name as a source of the estimate, which was wrong.  Second, he presented
Nicko's estimate as being more authoritative than it actually was,
as Nicko makes clear here.  And third, he fostered panic by precipitously
revoking his key and widely promulgating his "sky is falling" message.
We wouldn't be in this situation of duelling bias and authority if
people would provide some minimal facts and figures rather than making
unsubstantiated claims.

@_date: 2002-04-24 11:36:02
@_author: Anonymous 
@_subject: objectivity and factoring analysis 
Not so.  His actual comment, from one of the three messages Google
finds at the above URL, was:
The question is, is it practical?  At the time of that message, February
Yet since then he has had no more substantive comment, just a couple of
snide digs at RSA Labs.
Surely Silverman is indeed as qualified as anyone to judge whether
Bernstein's ideas have any practical value.  Yet almost two months later
he is apparently still unable to make a public judgement.
The fact is, the jury is still completely out on whether Bernstein's
ideas will reduce the cost of factoring 1024 bit keys.  Bernstein doesn't
say they will.  Silverman doesn't say they will.  In fact there almost
seems to be an inverse correlation between how much people know about
factoring and how much confidence they are willing to express that
Bernstein's machine will work for keys of this size.
The main people who have publically declared that Bernstein's machine is
a practical threat are Ray Dillinger, Nicko van Someren, Lucky Green,
and Joseph Ashwood, Since then Nicko van Someren has characterized his
comment as an estimate he came up with on the spot that he later found
was off by a factor of 100 billion.  Lucky Green relied on Nicko van
Someren's estimate.
So far no one who has claimed the machine to be practical has offered
the barest, sketchiest ghost of a design!  The most elementary, simple,
basic parameter which drives the design of such a machine is the size of
the factor base (or bases).  If they would just tell us how big the factor
base was they assumed, how many processing elements were are involved in
the matrix solution phase, and what clock speed they are assuming, that
would basically define that half of the design.  If they then indicated
what algorithm they were assuming for the "sieving" phase, how many
processors and what clock speed, that would define the other half.
Specifying these few parameters would allow a wide range of reviewers
to at least sanity-check the claims.  It should be a minimal requirement
for anyone who wants to claim that the Bernstein machine is a practical
threat to at least tell us the factor base size they are assuming.

@_date: 2002-04-25 20:21:06
@_author: Anonymous 
@_subject: objectivity and factoring analysis 
In his paper Bernstein uses a relatively larger factor base than in
typical current choices of parameters.  It's likely that the factor
bases which have been used in the past are "too small" in the sense that
the linear algebra step is being limited by machine size rather than
runtime, because of the difficulty of parallelizing it.  For example in
 we find that the sieving took
8000 mips years but the linear algebra took 224 CPU hours on a 2GB Cray.
If there were a larger machine to do the matrix solution, the whole
process could be accelerated, and that's what Bernstein's figures assume.
Specifically he uses a factor base size of L^.7904, where L for 1024 bit
keys is approximately 2^45.  This is a matrix size of about 50 billion,
50 times larger than your estimate.  So a closer order of magnitude
esimate would be 10^11 for the factor base size and 10^13 for the weight
of the matrix.
The assumption of a larger factor base necessary for the large asymptotic
speedups would increase the cost estimate by a factor of about 50.
Instead of several hundred million dollars, it would be perhaps 10-50
billion dollars.  Of course at this level of discussion it's just as
easy to assume that the adversary spends $50 billion as $500 million;
it's all completely hypothetical.
Actually the sort algorithm described takes 8*sqrt(10^11) or about 2.5 *
10^6 cycles, and there are three sorts per dot product, so 10^7 cycles
would be a better estimate.
Using the larger factor base with 10^13 entries would imply a sort
time of 10^8 cycles, by this reasoning.
Taking into consideration that the sort algorithm takes about 8 times
longer than you assumed, and that "a few" minimal polynomials have to
be calculated to get the actual one, this adds about a factor of 20
over your estimate.  Instead of 4 months it would be more like 7 years.
This is pretty clearly impractical.
Apparently Ian Goldberg expressed concerns about the interconnections
when the machine was going to run at 1 MHz.  Now it is projected to run
10,000 times faster?  That's an aggressive design.  Obviously if this
speed cannot be achieved the run time goes up still more.  If only 1
GHz can be achieved rack to rack then the machine takes 70 years for one
factorization.  Needless to say, any bit errors anywhere will destroy the
result which may have taken years to produce, requiring error correction
to be used, adding cost and possibly slowing the effective clock rate.
Using the larger factor base from the Bernstein paper would increase
the time to something like 10^11 seconds, thousands of years, which is
out of the question.
These estimates are very helpful.  Thanks for providing them.  It seems
that, based on the factor base size derived from Bernstein's asymptotic
estimates, the machine is not feasible and would take thousands of years
to solve a matrix.  If the 50 times smaller factor base can be used,
the machine is on the edge of feasibility but it appears that it would
still take years to factor a single value.

@_date: 2002-04-25 20:47:06
@_author: Anonymous 
@_subject: Lucky's 1024-bit post [was: RE: objectivity and factoring analysis 
What he wrote originally was:
: The panel, consisting of Ian Goldberg and Nicko van Someren, put forth
: the following rough first estimates:
: While the interconnections required by Bernstein's proposed architecture
: add a non-trivial level of complexity, as Bruce Schneier correctly
: pointed out in his latest CRYPTOGRAM newsletter, a 1024-bit RSA
: factoring device can likely be built using only commercially available
: technology for a price range of several hundred million dollars to about
: 1 billion dollars....
: Bernstein's machine, once built, ... will be able to break a 1024-bit
: RSA or DH key in seconds to minutes.
It's not a matter of assuming parallel engineering estimates, but rather
the implication here is that Ian endorsed the results.  In saying that
the panel put forth a result, and the panel is composed of named people,
it implies that the named people put forth the result.  The mere fact
that Ian found it necessary to immediately post a disclaimer makes it
clear how misleading this phrasing was.
Another problem with Lucky's comment is that somewhere between Nicko's
thinking and Lucky's posting, the fact was dropped that only the matrix
solver was being considered.  This is only 1/2 the machine; in fact in
most factoring efforts today it is the smaller part of the whole job.
Neither Nicko nor Ian nor anyone else passed judgement on the equally
crucial question of whether the other part of the machine was buildable.
It is obvious that in fact Nicko had not spent much time going over
his figures, else he would have immediately spotted the factor of 10
million error in his run time estimate.  Saying that his estimates had
not changed is meaningless if he has not reviewed them.
Lucky failed to make clear the cursory nature of these estimates, that the
machine build cost was based on a hurried hour's work before the panel,
and that the run time was based on about 5 seconds calculation during
the panel itself.  It's not relevant whether this was in part Nicko's
fault for perhaps not making clear to Lucky that the estimate stood in
the same shape a week later.  But it was Lucky who went public with the
claim, so he must take the blame for the inaccuracy.
In fact, if Lucky had passed his incendiary commentary to Nicko and
Ian for review before publishing it, it is clear that they would have
asked for corrections.  Ian would have wanted to remove his name from
the implied endorsement of the numeric results, and Nicko would have
undoubtedly wanted to see more caveats placed on figures which were
going to be attached to his name all over the net, as well as making
clear that he was just talking about the matrix solution.  Of course
this would have removed much of the drama from Lucky's story.
The moral is if you're going to quote people, you're obligated to check
the accuracy of the quotes.  Lucky is not a journalist but in this
instance he is playing one on the net, and he deserves to be criticized
for committing such an elementary blunder, just as he would deserve
credit for bringing a genuine breakthrough to wide attention.
They are not mutually exclusive, and the difference is clear.  In the
first paragraph, Bruce is saying that Bernstein's design is not practical.
To get his asymptotic results of 3x key length, Bernstein must forego the
use of sieving and replace it with a parallel ECM factoring algorithm
to determine smoothness.  Asymptotically, this is a much lower cost
approach for finding relations, and this asymptotic improvement plays
a major part in Bernstein's dramatic result.
However, this specific improvement is almost certainly impractical for key
sizes in current use.  There is no way that sieving is going to be slower
than taking each value and doing a brute force ECM factoring effort on it!
We came up with estimates on this list a few weeks ago suggesting that
even with unreasonably extreme parallelism and clock rates, that this
approach would take 100 million years to factor.  (These estimates were
posted 3 weeks before Lucky's alarmist pronouncement.)
What Bruce is also saying, though, is that with sufficient money and
effort using conventional technology for the sieving, it might indeed be
possible to build a machine that could factor 1024 bit keys.  This would
not use Bernstein's sieving improvements and hence would not be a matter
of using his machine.  It has been known for years that factoring 1024
bit keys should be about 10^7 times more expensive than factoring 512.
And 2048 bit keys are another 10^9 times harder.  Obviously every key
can be factored with sufficient resources.
The bottom line is that Lucky made a mistake.  He went public with a
dramatic announcement that turns out to be based on inaccurate and off the
cuff estimates which have since been disclaimed by the relevant parties.
He should have waited a few weeks for Nicko to post his estimates and for
others to respond before sounding the alarm.  It was wrong to broadcast
an urgent warning based on the limited and crude figures available at
the time, which now appear to greatly underestimate the true costs.
Fine, people make mistakes, but they should take responsibility
afterwards.  It would be nice to see Lucky post a message to Bugtraq and
wherever else his first one appeared saying that things don't look quite
so dire as they appeared a few weeks ago, that at this point we have to
adopt a wait and see stance.  But it's probably not going to happen.

@_date: 2002-04-26 23:32:08
@_author: Anonymous 
@_subject: objectivity and factoring analysis 
In his paper Bernstein uses a relatively larger factor base than in
typical current choices of parameters.  It's likely that the factor
bases which have been used in the past are "too small" in the sense that
the linear algebra step is being limited by machine size rather than
runtime, because of the difficulty of parallelizing it.  For example in
 we find that the sieving took
8000 mips years but the linear algebra took 224 CPU hours on a 2GB Cray.
If there were a larger machine to do the matrix solution, the whole
process could be accelerated, and that's what Bernstein's figures assume.
Specifically he uses a factor base size of L^.7904, where L for 1024 bit
keys is approximately 2^45.  This is a matrix size of about 50 billion,
50 times larger than your estimate.  So a closer order of magnitude
esimate would be 10^11 for the factor base size and 10^13 for the weight
of the matrix.
The assumption of a larger factor base necessary for the large asymptotic
speedups would increase the cost estimate by a factor of about 50.
Instead of several hundred million dollars, it would be perhaps 10-50
billion dollars.  Of course at this level of discussion it's just as
easy to assume that the adversary spends $50 billion as $500 million;
it's all completely hypothetical.
Actually the sort algorithm described takes 8*sqrt(10^11) or about 2.5 *
10^6 cycles, and there are three sorts per dot product, so 10^7 cycles
would be a better estimate.
Using the larger factor base with 10^13 entries would imply a sort
time of 10^8 cycles, by this reasoning.
Taking into consideration that the sort algorithm takes about 8 times
longer than you assumed, and that "a few" minimal polynomials have to
be calculated to get the actual one, this adds about a factor of 20
over your estimate.  Instead of 4 months it would be more like 7 years.
This is pretty clearly impractical.
Apparently Ian Goldberg expressed concerns about the interconnections
when the machine was going to run at 1 MHz.  Now it is projected to run
10,000 times faster?  That's an aggressive design.  Obviously if this
speed cannot be achieved the run time goes up still more.  If only 1
GHz can be achieved rack to rack then the machine takes 70 years for one
factorization.  Needless to say, any bit errors anywhere will destroy the
result which may have taken years to produce, requiring error correction
to be used, adding cost and possibly slowing the effective clock rate.
Using the larger factor base from the Bernstein paper would increase
the time to something like 10^11 seconds, thousands of years, which is
out of the question.
These estimates are very helpful.  Thanks for providing them.  It seems
that, based on the factor base size derived from Bernstein's asymptotic
estimates, the machine is not feasible and would take thousands of years
to solve a matrix.  If the 50 times smaller factor base can be used,
the machine is on the edge of feasibility but it appears that it would
still take years to factor a single value.

@_date: 2002-05-01 01:37:09
@_author: Anonymous 
@_subject: Lucky's 1024-bit post 
This is probably not the right way to approach the problem.  Bernstein's
relation-finding proposal to directly use ECM on each value, while
asymptotically superior to conventional sieving, is unlikely to be
cost-effective for 1024 bit keys.  Better to extrapolate from the recent
sieving results.
 is the paper
from Eurocrypt 2000 describing the first 512 bit RSA factorization.
The relation-finding phase took about 8000 MIPS years.  Based on the
conventional asymptotic formula, doing the work for a 1024 bit key
should take about 10^7 times as long or 80 billion MIPS years.
For about $200 you can buy a 1000 MIPS CPU, and the memory needed for
sieving is probably another couple of hundred dollars.  So call it $500
to get a computer that can sieve 1000 MIPS years in a year.
If we are willing to take one year to generate the relations then
($500 / 1000) x 8 x 10^10 is $40 billion dollars, used to buy
approximately 80 million cpu+memory combinations.  This will generate
the relations to break a 1024 bit key in a year.  If you need it in less
time you can spend proportionately more.  A $400 billion dollare machine
could generate the relations in about a month.  This would be about 20%
of the current annual U.S. federal government budget.
However if you were limited to a $1 billion budget as the matrix
solver estimate assumed, the machine would take 40 years to generate
the relations.
The $40 billion, 1-year sieving machine draws on the order of 10 watts
per CPU so would draw about 800 megawatts in total, adequately supplied
by a dedicated nuclear reactor.

@_date: 2002-08-15 19:06:06
@_author: Anonymous 
@_subject: Overcoming the potential downside of TCPA 
Actually, this is not true for the endoresement key, PUBEK/PRIVEK, which
is the "main" TPM key, the one which gets certified by the "TPM Entity".
That key is generated only once on a TPM, before ownership, and must
exist before anyone can take ownership.  For reference, see section 9.2,
"The first call to TPM_CreateEndorsementKeyPair generates the endorsement
key pair. After a successful completion of TPM_CreateEndorsementKeyPair
all subsequent calls return TCPA_FAIL."  Also section 9.2.1 shows that
no ownership proof is necessary for this step, which is because there is
no owner at that time.  Then look at section 5.11.1, on taking ownership:
"user must encrypt the values using the PUBEK."  So the PUBEK must exist
before anyone can take ownership.
I don't quite follow what you are proposing here, but by the time you
purchase a board with a TPM chip on it, it will have already generated
its PUBEK and had it certified.  So you should not be able to transfer
a credential of this type from one board to another one.
Actually I don't see a function that will let the owner wipe the PUBEK.
He can wipe the rest of the TPM but that field appears to be set once,
retained forever.
For example, section 8.10: "Clear is the process of returning the TPM to
factory defaults."  But a couple of paragraphs later: "All TPM volatile
and non-volatile data is set to default value except the endorsement
key pair."
So I don't think your fraud will work.  Users will not wipe their
endorsement keys, accidentally or otherwise.  If a chip is badly enough
damaged that the PUBEK is lost, you will need a hardware replacement,
as I read the spec.
Keep in mind that I only started learning this stuff a few weeks ago,
so I am not an expert, but this is how it looks to me.

@_date: 2002-01-07 21:40:09
@_author: Anonymous 
@_subject: CFP: PKI research workshop 
It's not clear what you mean by the limited threat model in encrypting web
forms, but one correction is necessary: known plaintext is not an issue.
See the sci.crypt thread "Known plaintext considered harmless" from June,
2001 (available by advanced search at groups.google.com).  Especially note
the perceptive comments by David Wagner and David Hopwood.  There is no
need to be concerned that encrypted web forms contain known plaintext:
no plausible threat model can exploit that information.

@_date: 2002-06-24 20:27:05
@_author: Anonymous 
@_subject: Ross's TCPA paper 
The amazing thing about this discussion is that there are two pieces
of conventional wisdom which people in the cypherpunk/EFF/"freedom"
communities adhere to, and they are completely contradictory.
The first is that protection of copyright is ultimately impossible.
See the analysis in Schneier and Kelsey's "Street Performer Protocol"
paper,   Or EFF
columnist Cory Doctorow's recent recitation of the conventional wisdom
at  "providing
an untrusted party with the key, the ciphertext and the cleartext but
asking that party not to make a copy of your message is just silly,
and can't possibly work in a world of Turing-complete computing."
The second is that evil companies are going to take over our computers
and turn us into helpless slaves who can only sit slack-jawed as they
force-feed us whatever content they desire, charging whatever they wish.
The recent outcry over TCPA falls into this category.
Cypherpunks alternate between smug assertions of the first claim and
panicked wailing about the second.  The important point about both of
them, from the average cypherpunk's perspective, is that neither leaves
any room for action.  Both views are completely fatalistic in tone.
In one, we are assured victory; in the other, defeat.  Neither allows
for human choice.
Let's apply a little common sense for a change, and analyze the situation
in the context of a competitive market economy.  Suppose there is no
law forcing people to use DRM-compliant systems, and everyone can decide
freely whether to use one or not.
This is plausible because, if we take the doom-sayers at their word,
the Hollings bill or equivalent is completely redundant and unnecessary.
Intel and Microsoft are already going forward.  The BIOS makers are
on board; TPM chips are being installed.  In a few years there will
be plenty of TCPA compliant systems in use and most new systems will
include this functionality.
Furthermore, inherent to the TCPA concept is that the chip can in
effect be turned off.  No one proposes to forbid you from booting a
non-compliant OS or including non-compliant drivers.  However the TPM
chip, in conjunction with a trusted OS, will be able to know that you
have done so.  And because the chip includes an embedded, certified key,
it will be impossible to falsely claim that your system is running in a
"trusted" mode - only the TPM chip can convincingly make that claim.
This means that whether the Hollings bill passes or not, the situation
will be exactly the same.  People running in "trusted" mode can prove
it; but anyone can run untrusted.  Even with the Hollings bill there
will still be people using untrusted mode.  The legislation would
not change that.  Therefore the Hollings bill would not increase the
effectiveness of the TCPA model.  And it follows, then, that Lucky and
Ross are wrong to claim that this bill is intended to legislate use of
the TCPA.  The TCPA does not require legislation.
Actually the Hollings bill is clearly targeted at the "analog hole", such
as the video cable that runs from your PC to the display, or the audio
cable to your speakers.  Obviously the TCPA does no good in protecting
content if you can easily hook an A/D converter into those connections and
digitize high quality signals.  The only way to remove this capability
is by legislation, and that is clearly what the Hollings bill targets.
So much for the claim that this bill is intended to enforce the TCPA.
That claim is ultimately a red herring.  It doesn't matter if the bill
exists, what matters is that TCPA technology exists.  Let us imagine a
world in which most new PCs have TCPA built-in, Microsoft OS's have been
adapted to support it, maybe some other OS's have been converted as well.
The ultimate goal, according to the doom-sayers, is that digital content
will only be made available to people who are running in "trusted"
mode as determined by the TPM chip built into their system.  This will
guarantee that only an approved OS is loaded, and only approved drivers
are running.  It will not be possible to patch the OS or insert a custom
driver to intercept the audio/video stream.  You won't be able to run
the OS in a virtual mode and provide an emulated environment where you
can tap the data.  Your system will display the data for you, and you
will have no way to capture it in digital form.
Now there are some obvious loopholes here.  Microsoft software has a
track record of bugs, and let's face it, Linux does, too.  Despite the
claims, the TCPA by itself does nothing to reduce the threat of viruses,
worms, and other bug-exploiting software.  At best it includes a set of
checksums of key system components, but you can get software that does
that already.  Bugs in the OS and drivers may be exploitable and allow
for grabbing DRM protected content.  And once acquired, the data can
be made widely available.  No doubt the OS will be built to allow for
frequent updates, similar to antivirus software, so that as an exploit
becomes publicized, it will be closed.  There will be an ongoing war
between the hackers and the software companies, just as we see today.
Presumably this will see-saw back and forth for quite a while.
Hardware hacking will be another line of attack.  The TPM chip isn't
exactly omniscient.  It's a pretty simple gadget; its only view of the
world is through a few tiny wires.  Of course it will be surface-mount
soldered to the motherboard, but for a price you will probably be able
to get yours unsoldered and mounted in a socket which gives the chip a
"sanitized" view of your hardware configuration before boot, and switches
over to your real, hacked, system once things get running.  This will
allow you to run your supposedly "secure" OS in virtual mode and still
grab the protected data.  But it's probably an expensive hack.
Clearly no system can be perfect, and the same is true of the TCPA.
There will be ongoing leakage of digitally protected data.  Perhaps
watermarking technologies will be brought into play for another layer of
protection, but by and large those have been defeated as well.  The goal
of these systems is to reduce the quantity of piracy and to raise the
price, so that we move away from the system today where do-it-yourself
piracy is the norm.
Let us suppose that this is the world ten years from now: you can run a
secure OS in "trusted" mode and be eligible to download movies and music
for a price; or you can run in untrusted mode and no one will let you
download other than bootleg copies.  This is the horror, the nightmare
vision which the doom-sayers frantically wave before us.
The important thing to note is this: you are no worse off than today!
You are already in the second state today: you run untrusted, and none
of the content companies will let you download their data.  But boolegs
are widely available.
All the TCPA "threatens" to do is to provide new options to the world.
You will still be able to use your system in exactly the same ways that
you use it today; you will be able to run all of the software that you
run today.  The TPM chip can be disabled or ignored if you don't run
in "trusted" mode, and you get the same effect you have today with no
TPM chip.  You have lost nothing.
Ironically, if we lived in a world of honest people, the TCPA would
not be necessary.  You would be able to buy DRM protected data already,
agreeing to the restrictions in exchange for the content, and you would
follow the rules.  We would have a thriving market in digital content.
But we don't live in that world.  People can make all the promises
they like and the vendors know there is no way to hold them to what
they have said.  There is not even social opprobrium; look at how eager
everyone was to look the other way on the question of whether the DeCSS
reverse engineering violated the click-through agreement.
The TCPA allows you to do something that you can't do today: run your
system in a way which convinces the other guy that you will honor your
promises, that you will guard his content as he requires in exchange for
his providing it to you.  It allows you to be honest.  It doesn't force
it; you can still do everything you can do today.  But it allows it.
It gives you the chance to present an honest face even across the
anonymizing medium of the net.
Lucky, Ross and others who view this as a catastrophe should look at
the larger picture and reconsider their perspective.  Realize that the
"trusted" mode of the TCPA will always be only an option, and there
is no technological, political or economic reason for that to change.
The TCPA gives people new capabilities without removing any old ones.
It makes possible a new kind of information processing that cannot be
accomplished in today's world.  It lets people make binding promises that
are impossible today.  It makes the world a more flexible place, with
more opportunities and options.  Somehow that doesn't sound all that bad.

@_date: 2002-05-13 22:04:05
@_author: Anonymous 
@_subject: Lucky's 1024-bit post 
Silverman's comment makes sense; the memory needed is probably
proportional to the size of the factor base, and going from 512 to 1024
bits would plausibly increase the factor base size by at least 11 bits,
corresponding to a memory increase of a factor of ~ 2500 as he says.
If the 512 bit factorization used 50 MB per node for the sieving then
that would require extreme amounts of per node memory for 1024 bits.
But how about using disk space instead of RAM for most of this?  Seems
like a sieve algorithm could have relatively linear and predictable memory
access patterns.  With a custom read-ahead DMA interface to the disk it
might be possible to run at high speed using only a fraction of the RAM,
acting as a disk buffer.  A 125 GB disk costs a few hundred dollars,
so that might bring the node cost back down to the $1000 range.
