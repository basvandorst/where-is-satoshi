
@_date: 2002-04-24 12:30:26
@_author: Wei Dai 
@_subject: Lucky's 1024-bit post [was: RE: objectivity and factoring analysis] 
I have one other question about the panel analysis. Why did it focus only on the linear algebra part of the NFS algorithm? I would like to know, given the same assumption on the factor base size (10^9), how much would it cost to build a machine that can perform the relationship finding phase of the algorithm in the same estimated time frame (i.e. months)?
Using a factor base size of 10^9, in the relationship finding phase you
would have to check the smoothness of 2^89 numbers, each around 46 bits
long. (See Frog3's analysis posted at
  Those numbers look correct to me.)  If you assume a chip that can check
one number per microsecond, you would need 10^13 chips to be able to
complete the relationship finding phase in 4 months. Even at one dollar
per chip this would cost ten trillion dollars (approximately the U.S. So it would seem that it's still not possible for even a major government
to factor 1024-bit keys within an operationally relevant time frame unless
it was willing to devote most of its national income to the effort.
BTW, if we assume one watt per chip, the machine would consume 87 trillion
kWh of eletricity per year. The U.S. electricity production was only 3.678 trillion kWh in 1999.

@_date: 2002-04-29 10:31:34
@_author: Wei Dai 
@_subject: Lucky's 1024-bit post [was: RE: objectivity and factoring analysis] 
Sorry, there's a mistake in my post, which makes the relationship finding
phase look easier than it actually is. BTW, why did it take 5 days for
that post to go through?
Obviously there are not 2^89 integers that are 46 bits long. Each of the numbers that need to be checked for smoothness is actually around 195 bits

@_date: 2002-04-30 17:36:29
@_author: Wei Dai 
@_subject: Lucky's 1024-bit post 
============================== START ==============================
Yes, good point.
You need a lot more than a couple of hundred dollars for the memory, because you'll need 125 GB per machine. See Robert Silverman's post at According to pricewatch.com, 128MB costs $14, so each of your sieving machines would cost about $14000 instead of $500.
($14000 / 1000) x 8 x 10^10 is $1.1 trillion. So my earlier estimate for a
$10 trillion 4-month machine was only off by a factor of 3, which is a
nice coincidence. :)
(Too bad about the memory, otherwise you can get your sieving machine
almost for free by writing a worm to take over half of the Internet, which
currently has about 190 million hosts.)

@_date: 2002-02-28 16:01:21
@_author: Wei Dai 
@_subject: Bernstein's NFS machine 
I haven't checked all of the numbers, but I think Frog3's analysis is
largely correct. However, the two O(1)'s below should be replaced with
Very informally, think of O(1) as some constant, and o(1) as converging to
0. One possibly confusing thing about Berstein's paper is that in the
sentence "for a given cost, the number of digits of n has grown by a
factor of 3.0090581972... + o(1)", the o(1) is most likely negative for
practical key lengths.
