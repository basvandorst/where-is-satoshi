
@_date: 2011-02-23 18:37:43
@_author: Michael Militzer 
@_subject: [p2p-hackers] P2P file storage systems 
Dear Zooko,
thanks for your detailed comments. I'd like to elaborate some more on
them in the following:
Quoting Zooko O'Whielacronx :
True. But the repair cost is minimal compared to replacing a lost erasure
coded fragment. So assuming long-lived data stored in a network of dynamic,
unreliable peers (P2P), repair may be frequently needed and cost much more
bandwidth over the whole life-time of a data object than the initial upload.
In such an environment replication can hence be more bandwidth efficient
than erasure coding - while of course requiring more storage in any case.
"Tahoe-LAFS grids would still be comprised exclusively of servers whose
owners gave you some reason to believe that they were reliable."
                    ^^
That is exactly what I initially referred to as "trusted" and "trusted
environment". Because the servers that take part in the grid are selected
(friends, an operator you trust, etc.), you can assume within Tahoe's
use-case that all (or almost all) servers are honest and have no
malicious intentions.
This assumption is not important for data integrity and confidentiality
because this is assured by cryptographic primitives. However, this trust
relationship ("friends network") is necessary to gain reliability with
regard to data availability / accessibility and censorship-resistance.
When you give up the assumption of a trusted environment and allow random
strangers to join the network and serve as storage nodes, the Tahoe design
can become vulnerable to DoS attacks. That's not a criticism of Tahoe at
all because it's not the use-case of Tahoe. I just wanted to point out why
Tahoe cannot be employed (at least not without modification) for the use
case I have in mind: A global P2P file store made of arbitrary, unreliable
Ok, we can use the term reliable or maybe it's clearer referring to game
theory's "selfish but honest". I realize now that my claim that Tahoe
would not be deployable to "untrusted nodes" might have sound as if I
wanted to challenge Tahoe's least authority principle. That's of course
not the case.
I just wanted to point out that a) Tahoe doesn't scale to the size I'd
like and b) is not designed to withstand the presence of malicious nodes.
Both would however be needed for the use-case I have in mind.
I know. That's why I also said that I don't think Tahoe has deficiencies
with regard to confidentiality and integrity even if used on a global
scale in an uncontrolled / partly malicious environment.
Correct. But you can design the system in such a way that the data will
still be available as long as only a certain percentage of the nodes are
"honest". Then the key to achieve a reliable storage system reduces to
making it practically impossible that any single malicious entity can
gain control over the necessary set of network nodes. And here, size
becomes your friend: The larger the network and the wider the stored data
is dispersed, the harder for a single attacker to reach the necessary
percentage to break availability. That's mathematical magic too ;)
Yep, I understand the concept. That's the trust relationship I initially
referred to. However, I don't quite like the concept ;)
It certainly has a use-case but it is limited imho. Your average Joe will
pretty certainly not be able to set up a dedicated storage network with
his friends. It's too complicated (or he doesn't have friends ;)). The
only easy option is to connect to the storage grid of a commercial provider
(like allmydata was). But then you suffer from some similar risks than
when using a central cloud storage system in the first place: provider
insolvency, management fault, etc.
So: One large, public network where everyone could freely join or leave
would be much easier from a usability point of view: Just download and
run the software and you're ready to start. No setup, no configuration,
no further management required.
Thanks for the cross-post. I'm not on the Tahoe dev list, so I missed the
earlier discussion. To clarify: I don't think your documentation on Tahoe
is incomprehensible. Seems it is rather me who is widely misunderstood ;)
I think I've made it clear above what I was referring to with "untrusted
nodes" and that I don't think Tahoe has any design problem within the
scope of its targeted use-case.
The question is rather whether "BitTorrent for storage" is really a bad
idea and whether there's really no value for Tahoe in dropping the "friends
network" assumption.
I named "ease of use" as an argument already. And I think this is not just
a "nice to have" feature. If ease of use is not provided built-in, people
will look for alternatives to easily join a storage grid. If you think
experiments like "volunteer-grid" a bit further you are not far from my
envisioned ad-hoc network of strangers. The participants in such a volunteer
grid are not your friends. Can you still disregard the possibility of
malicious nodes for this use-case?
For several considerations:
In a P2P network made of home computers, the availability of the single
nodes will be significantly lower than for dedicated storage nodes in a
grid (maybe just around 30-40% instead of 90%). To still achieve high
overall availability, n needs to be increased (or more precisely: n/k).
Because we however don't want to introduce too much redundancy and waste
storage, we'd like to increase system availability primarily by increasing
k. This has also the advantage that we could download from many sources
in parallel and better level the node heterogenity with regard to available
upstream bandwidth.
Finally, large k (and n) makes it much harder for a malicious entity to
attack the availability of a file because the attacker then needs to gain
control over not just a few but many specific storage nodes, which - with
proper admission control and network size - can be made largely infeasible.
Hm, what is plenty efficient for you? Again, I have to admit that I didn't
run any specific tests with zfec, so my opinion is mostly guess-work: I
read James Plank's paper on open-source erasure coding libraries that
included some numbers on zfec and did some actual tests with Alexandre
Soro's Fermat-prime based reed-solomon implementation (which should be more
efficient than zfec for k, n >= 256 - at least in theory): For k = 256 and
n = 1024, I get a decoding speed of about 7.5 Mbit/s on my two years  old notebook.
But this is not efficient enough for me. Erasure decoding speed should be
much higher than network bandwidth. And considering that the next-gen
broadband internet links offer 100 MBit/s for the home user, 7.5 Mbit/s
erasure decoding seems by far not sufficient.
Yes, the size of the hash tree should indeed be less of an issue.
Could be even worse. Actually, if an attacker sits somewhere along the DHT
lookup path he could pretend to be the root node for a searched key. In
that case (and assuming a traditional DHT) just one compromised node could
already be sufficient to make certain data unavailable.
Yes. But again: I didn't challenge that ;) Tahoe is good for its current
use case. I was rather responding to questions like "Why don't you simply
use Tahoe for what you have in mind?".
BTW: While Tahoe is pretty censorship-resistant (depending on the size and
ratio of k to n as well as the number of independent parties in the grid),
censorship is a problem you shouldn't need to worry about too much in a
network of "friends" anyway. And for the use-case where the whole storage
grid is operated by just one commercial provider you have no censorship-
resistance anyway - no matter what is the network topology ;)
Yes, that's the same consideration why I proposed to increase k and n too:
Larger k and n reduce the feasibility of DoS attacks.
I agree that bootstrapping can be rather easily distributed and thereby
made more robust against potential attacks.
Not really. I'm not convinced of the onion routing or mixnet concepts for
such a storage use-case. First, it's inefficient as lots of bandwidth is
"wasted". Second, is sender and receiver anonymity really what we need?
I think in a storage network there is no need to disguise who communicates
with whom or who participates in the network at all. For privacy, it's  important to obfuscate which nodes store which data and consequently who
accesses or forwards which data.
Anonymous routing is not sufficient to solve the former and does not fix
the "legal attack" problem I mentioned (rather the opposite seems to be
true). Let's consider the case of downloading a MP3 file in Freenet
(the infamous "anonymous filesharing" use-case):
The file is stored (or cached) encrypted on some node. It is transferred
to the requester by onion routing. Indeed, the downloader cannot determine
the identity of the storage node and the storage node doesn't know the
identity of the requester. So with regard to criminal liability this scheme
may provide some protection (plausible deniability, benefit of the doubt).
However, in civil law it doesn't matter who is the original sender and who
the actual receiver of the copyrighted work. Everybody who participates in
the copying and distribution of the file can become liable to recourse. This
means that the copyright holder can send a C&D letter to every node along
the onion route - and not just sue sender or receiver. And it should not be
difficult to log the identity of some of the hops along the route.
Now I don't want to promote any "anonymous filesharing" nonsense. I just
think that in order for people to accept the use of online storage their
data needs to be as "safe" from third-party delete requests as on their
local hard disk. Further, people will only be ready to participate in such
a network (and dedicate storage to it) when they don't risk to be held
liable for the actions of others.
p2p-hackers mailing list
p2p-hackers at lists.zooko.com
