
@_date: 1993-08-28 08:18:08
@_author: Doug Merritt 
@_subject: AT&T Home Security Plus 
"George A. Gleason"  said:
The term "handle", as well as the fact that this person could say anything
even slightly knowledgeable about any computer related subject, points to
him being a hobbyist who frequents BBS's. ("Handle" is pretty much not
used in newsgroups, nor have I run into it on muds, but it is very widely
used in BBS circles.)
Telephone soliciting is generally a minimum wage job, although in high
price sales "professional" salespeople are used who get a commission. Even
if you assume a conspiracy of some sort, there isn't any reason for the
company to make a particular effort to use someone more knowledgeable for
cold calling.
The question about pirating probably arose because it has always been a
very frequent topic of conversation on BBS's. The caller himself might
be a pirate; that would be my first guess.
As for knowing your address, phone companies sell phone books that are
reverse-indexed by address. Companies use these for cold-calling because
they can pick out more affluent neighborhoods and skip e.g. ghetto areas
where a call might be a waste of time or even worse. Generally "taking
great pains" to protect information about your address doesn't guarantee
that you are successful; I've heard that some of these sales-tool phone
books have more information in them than is released to the public in
the usual phone books. Perhaps they cross-reference with mailing lists.
I could believe that there's a backdoor built into home security systems,
if I could figure out how they would make use of it. Sell the info to
organized crime for the purpose of burglary, perhaps? A little paranoid
considering you're talking about AT&T. How would they be able to implement
such a thing and still keep it secret? Or in cahoots with the FBI/CIA/NSA?
Doesn't make sense; if spooks want to get into someone's house, they can
do so in any number of ways, they needn't risk a large scale conspiracy,
which would only pay off on the very small percentage of homes that used
AT&T's system in particular.
And regardless of that, again, even if there's a conspiracy, there's
just no reason to let the phone solicitors in on it. All they need to
do is build a backdoor into the security system and then do everything
else aboveboard. Calling people up and hoping to catch someone in the
admission of being a pirate during a sales pitch is just ludicrous.

@_date: 1993-08-29 07:33:27
@_author: Doug Merritt 
@_subject: AT&T Home Security Plus 
"George A. Gleason"  said:
Yes, unfortunately there are institutions that are obnoxious about this.
Here's an innocent anecdote that shows how bad things can be.
Last year I got to know a woman via her nome de plume on a mud. She was
highly privacy-conscious, to the point where she told no one there anything
about her real life (this is common on many muds, but not on that particular
one). We'd had a bunch of heart-to-heart's, and since I'm a smart ass, at
one point I called her up. Since she hadn't told me her real name nor what
city she was in, let alone given me her phone number, she naturally wanted
to know how I'd managed this.
(I underscore that this was innocent; we talked for 6 hours that time and
she recently visited me while passing through the area where I live, which
is to say that she wasn't bent out of shape by my doing the above.)
Anyway the scary part is that her university lists both *residence*
phone and address for all students in their phone directory services.
She said that she had tried to get unlisted but that university policy
*forbade* that, so she was stuck with being listed. (Yes, this is a U.S.
That's the sort of policy that ought to be prevented by law. Even non-deadly
stalkers can be a serious nuisance.

@_date: 1993-11-03 20:37:34
@_author: Doug Merritt 
@_subject: trusting software 
How does one know that one can trust the software that one is using
on one's own machine for encryption, mailing, etc, or worse yet, how
can one know whether to trust the software doing anonymous or other
remailing on other machines? Web-of-trust schemes are only statistically
reliable due to these concerns.
These are rhetorical questions; the point is, I just realized that I
didn't explain myself last month when I talked about an algorithm for
verifying *intentions*. A number of people emailed me to complain that
authentication should be a matter of establishing a person's *real*
identity -- a valid issue, but I was off on a tangent and neglected
to explain my actual point:
Imagine you have a single piece of software which runs a dcnet over
the internet by being instantiated on many nodes. Imagine that you're
concerned that the NSA or someone will spoof a whole bunch of nodes,
pretend to be the Real Software (which ordinarily helps guarantee
anonymity, defeat traffic analysis, etc), but actually works to defeat
the Real Software and the people who use it.
One would like to somehow guarantee that when one talks to remote software
as part of a web of trust scheme, that the software really is the One and
Only True Software, and not some deceitful counterfeit.
It is in *this* connection that one might wish to authenticate the
unique identity of multiply instantiated *software* by a hypothetical
process which ascertains the *intentions* of that software instantiation.
I previously phrased this as if it were a person that the hypothetical
algorithm was authenticating, leading to understandable objections. Apologies;
I had gotten into a digressive train of thought about using it with people
before I posted, and it's taken me this long to realize that I never
communicated clearly.
I still haven't described the algorithm ("this margin is too narrow" :-),
but I hope it's more clear that such an algorithm is potentially more
realizable for software than it would be for people.
        Doug

@_date: 1993-11-05 08:57:45
@_author: Doug Merritt 
@_subject: trusting software 
ogr at wyvern.wyvern.com (Jason Plank) said:
This helps, but is imperfect. How many people will read their particular
copy in sufficient detail to ascertain that there aren't any obvious
backdoors added by e.g. a sneaky archive site maintainer, or some sneaky
cracker who found a way to modify the archived copy?
Furthermore, even close reading won't absolutely *guarantee* the lack of
backdoors in all cases, even if the reader is an expert on relevant
We'll all continue to use software despite lack of absolute assurances,
but it's worth keeping in mind what the situation is.

@_date: 1993-11-05 09:17:45
@_author: Doug Merritt 
@_subject: trusting software 
greg at ideath.goldenbear.com (Greg Broiles) said:
The idea is to encode the important-to-be-trusted features of the software
and the inter-machine protocol handshake together into the equivalent
of a Goedel number which acts as a public key during the protocol handshake,
so that any change to that core encoding of the functionality would have
the side effect that it was no longer able to communicate.
Yeah...I'm having strong difficulties with doing it in a way that is
computationally feasible as well as theoretically sound. Several times
I thought I'd found the right approach but then found holes in it. So
I lied in implying that I really did have a final algorithm....I *thought*
I did, but I was wrong.

@_date: 1993-11-07 22:43:00
@_author: Doug Merritt 
@_subject: trusting software 
ogr at wyvern.wyvern.com (Jason Plank) said:
Surely. A certain percentage of people will. A certain percentage of
people lack the expertise to do so. That was my primary point.
My secondary point is that even those who *do* may not detect the presence
of a backdoor. The decade-and-a-half controversy over whether DES has
a backdoor, despite the fact that the alogorithm is public, is an example
of this. The eventual answer to the question is less important than the
period of debate...think about it.
Reading source code is never a guarantee; it is only a *statistically* safe
measure. Worse yet, the statistical issues tend to be hard to analyze,
and in no case does one attain a 100% confidence.
This is a limited response to a limited question; I'm aware that there
are a million other issues as well.

@_date: 1993-11-08 09:33:00
@_author: Doug Merritt 
@_subject: ID of anonymous posters via word analysis? 
"Perry E. Metzger"  said:
Good question. I just checked a terse history of machine translation,
and it didn't mention any version of this. I suspect that its origin
was as a hypothetical example of the kinds of problems that can arise,
and that it didn't actually happen in any real life situation. Examples
like that have always been common in linguistics papers on such subjects.

@_date: 1993-11-08 21:03:07
@_author: Doug Merritt 
@_subject: Mark Abene (Phiber Optik) sentenced 
gtoal at an-teallach.com (Graham Toal)
"Perfect" reproduction of an analog signal that may be noisy and smeared,
you mean. The original critique holds. Your argument applies if and only
if the entire transmission is digital from one end to the other.

@_date: 1993-11-08 21:23:32
@_author: Doug Merritt 
@_subject: Private and Public 
Arthur Chandler  said:
I'm a non-standard brand hybrid Libertarian plus other noncategorizable
views, and I agree with the notion of the social contract, but I have
critiques that fall outside anything you said, which cause me to view
with pleasure the possibility of short-circuiting the previous government
taxation schemes.
I think that government supported infrastructure can be a good thing;
I approve of having fire services which are not profit centers, for instance.
On the other hand, I'm in favor of minimizing such things, whereas
governments tend to maximize the number of "services" and therefore also
taxes to support them.
Governments and their bureaucracies and services and laws etc. appear to
inescapably grow ever-larger over time, regardless of the impact of that
I see online crypto-banking and related technologies/services as a trend that
will force governments to downsize back to their appropriate role of
providing only the most necessary of infrastructure.
The precise nature of "most necessary" is highly controversial. But if they
can only collect as much taxes as people are willing to pay in order to
maintain minimum infrastructure, then it becomes a system that continues
to stay in equilibrium rather than growing out of control.
In other words, avoid the tyranny of the majority and of self-serving
representative democracy, and create a world in which we get only that which
we are willing to pay for. People will pay a lot for that which is truly
valuable. At the moment we are a long way from getting what we pay for.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-11-08 21:38:32
@_author: Doug Merritt 
@_subject: Private and Public 
"Perry E. Metzger"  said:
I very strongly disagree. The social, political, and economic impact
of cryptographic techniques is at least as important as the technology
itself. Pure algorithms can be discussed in sci.crypt, after all.
Cypherpunks do not have a common agenda, but we do share an interest
in how the future world will be shaped by cryptographic technology.
Incorrect. You have complete freedom as to your citizenship. Any time that
you choose, you are free to renounce your citizenship, and thereby
reject the contract that citizenship gives you. Naturalized citizens
of a country/government *very* explicitly enter into the contract; those
of you born into citizenship tend to not to think about the subject very
deeply, but basically you are simply being granted the privilege of
skipping the formalisms, on the assumption that you either accept the
contract, or will explicitly opt out.
If you continue to accept the freely-granted citizenship you were born
into, then you are also accepting the entire contract, like it or not.
If you truly reject the contract that U.S. citizenship obligates you to,
with all its positive and negative points, then go ahead and give it up.
Put your money where your mouth is.
Otherwise, accept that citizenship is a two-way street, and work within
that system to change it to your tastes, rather than denying that the
contract even exists.
I completely agree. But this is quite a different subject.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-11-08 21:48:34
@_author: Doug Merritt 
@_subject: ID of anonymous posters via word analysis? 
Eli Brandt  said
I think we were already agreeing that the folklore is false. The
remaining question is 'where did it originate'? My hypothesis was
that it came out of a linguistic research paper giving an example.

@_date: 1993-11-08 21:53:12
@_author: Doug Merritt 
@_subject: Private and Public 
Arthur Chandler says:
"Perry E. Metzger"  said:
Gee, Perry, too bad for you, but it isn't *your* list, either. You
are assuming you speak for the list in your response. You're no spokesman.
This list is as unmoderated as any newsgroup. Your tastes have no
enforcement power.
Arthur can and will post about whatever he likes.
Tell you what...for the sake of all of us newcomers, why not quote the
initial charter? Perhaps it would help us all focus. That could be
a good thing.
If no one does so, then you can expect people to continue posting about
their own personal interests, whether or not it matches your view
of appropriateness.

@_date: 1993-11-08 22:03:11
@_author: Doug Merritt 
@_subject: Public and private. 
GRABOW_GEOFFREY at tandem.com said:
This can be avoided simply by exempting subsistence items, just as
current food stamps pretend to do.
This is *almost* a digression from the topics of the list, but not quite.
The connection is that consequences like this and strategies to handle
them are very much an issue when we talk about deploying new technologies.
Crypto-technology will change the world, will he nil he; we can either
shrug off the consequences, or we can plan for them.

@_date: 1993-11-08 22:18:33
@_author: Doug Merritt 
@_subject: PC random number hardware 
jon at balder.us.dell.com (Jon Boede) said:
Assuming a bit of leeway in interpretation, this is trivially true
mathematically. There's a great von Neumann quote that goes something
like "anyone who uses finite state machines to generate supposedly
random numbers is, of course, living in a state of sin."
Use of hardware random number generation does not automatically confer
a state of grace, however. Such processes sample through an aperture
and are subject to the Nyquist limit, the General Uncertainty Principle,
and frequently the Central Limit Theorem as well, which is to say that
you still have to mind your p's and q's quite carefully.

@_date: 1993-11-09 20:03:14
@_author: Doug Merritt 
@_subject: Private and Public 
Return-Path: Received: from relay2.UU.NET by mail.netcom.com (8.6.4/SMI-4.1/Netcom)
Received: from toad.com by relay2.UU.NET with SMTP Received: by toad.com id AA17446; Tue, 9 Nov 93 09:03:39 PST
Received: by toad.com id AA17443; Tue, 9 Nov 93 09:03:11 PST
Received: from lehman.com ([192.147.66.1]) by toad.com id AA17438; Tue, 9 Nov 93 09:03:06 PST
"Perry E. Metzger"  said:
Ok...I'll desist after a brief comment: I actually don't disagree with
any of the major points you make when you put it that way -- although
some of the other things I call minor points you might call major points :-)
I think I do have some quibbles which would probably turn *into* serious
disagreement, but even I would say that raising those things would take us
far afield, so I'll leave it at that.
P.S. This is ad hominem...tsk, tsk:

@_date: 1993-11-09 20:08:45
@_author: Doug Merritt 
@_subject: Info on Clipper chip and fabrication of it 
tcmay at netcom.com (Timothy C. May) quoted someone else saying:
What about triple layer metal interconnect? If they're serious about
making it reverse-engineering-resistant, they'd do that even if it
weren't functionally needed, simply to make it STM-opaque. I think (but
am not sure) that VLSI has that technology in their current fab lines.
I suppose that it's not quite "exotic" by now; maybe that's just assumed
as obvious?

@_date: 1993-11-09 20:18:44
@_author: Doug Merritt 
@_subject: Are we gatewayed to Usenet? 
tcmay at netcom.com (Timothy C. May) said:
Agreed. There are a fair number of topics that I don't care to
discuss in a newsgroup where J. Random Luser might briefly drop in, but that
I'm willing to talk about in a mail list, where there is a psychological
barrier plus time delay and general perceived effort etc to join.

@_date: 1993-11-09 20:33:15
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
Mike Godwin  said:
Ah...do you mean "should not build", or do you really mean that it contains
a discussion outlining the logic behind predicting that they *won't*
 I skipped that session because I was already bored to tears with
the prospect of "playing football from home while watching it on tv"
and by that of 500 pay-per-view-channels. Were they really boosting
HSN-type stuff? How completely evil!
I'm still dismayed by the recent FCC decision that HSN channels are included
in the category of (paraphrased) "for the public good". Was this due to
corruption, or merely a sharp drop in collective FCC IQ?

@_date: 1993-11-09 20:53:17
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII?v 
tcmay at netcom.com (Timothy C. May)
On that subject: my company receives substantial money from a government
agency (that prefers that we call it "DARPA" in public), to develop
technologies that are up our alley but not otherwise commercially viable.
That is, they're paying us to adapt our commercial technology to
applications that don't pay off in the market place, because they want
those non-viable applications for their own use.
As a result of this, it turns out that the government requires all
contractors and subcontractors to undergo not just financial audits,
but also Equal Opportunity Employment audits.
In preparing for an audit of that sort, we discovered that, not only
is it a Bad Thing to discriminate on the basis of race, sex, etc, etc,
which we all know by now, but it is also a Bad Thing to discriminate
on the basis of *personality*. To turn away an interviewee because they
would not fit into the existing group personality-wise apparently is
a Very Bad Thing Indeed.
I was flabbergasted. I had no idea that political correctness of this
extreme had been enshrined into federal policy.
On the flip side of the issue, I admit that I can see the point that
even...ah...personality-challenged people need to work so they can eat.
But still...yikes!
There go your "fundamental rights". Granted this (as far as I know) only
applies to government contractors at the moment. But what do you want to
bet but that this will soon apply to all businesses?

@_date: 1993-11-10 08:48:58
@_author: Doug Merritt 
@_subject: Personality BS (was: Should we oppose the Data Superhighway/NII?) 
"Alan (Gesture Man) Wexelblat"  said:
We're not in opposition, I very strongly agree. It's just that we're
talking about slightly but importantly different things. The thing you're
referring to is indeed outrageous. So are the presumption-of-guilt drug tests
that I understand that 80% of all Fortune 1000 companies now require. (The
latter is particular heinous, since all tests have a non-zero false-positive
rate, for one thing.)
But what I was referring to was the common interviewing strategy of trying
to figure out whether someone will get along with the existing group,
not by prying into their personal life or giving them personality tests,
but just by the age-old method of discussing everyone's impressions of
the candidate. This all by itself is what we were told is unacceptable
discrimination (against the personality-challenged, presumably. :-)
Sorry, but I am pretty sure that giving personality tests is quite ok,
so long as they are one of those bullshit Supposedly Scientific things based
on Meyers Briggs or the Minnesota Multiphasic Aptitude Test or some such,
and as long as they are uniformly given to all candidates.
So they're outlawing the reasonable and allowing the unreasonable, the
worst of both worlds.

@_date: 1993-11-10 23:09:02
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
Mike Godwin  said:
I hate to disagree, considering that I prefer to agree with the philosophy
here, but it *can't* work that way, regardless of what we wish.
The problem is that bandwidth is a highly limited resource, just like
real estate is a limited resource. Eventually we will complete saturate
network bandwidth no matter what technology is used. This has been discussed
in various forums for many years. Once optical fiber optic bandwidth
peaks, you have to move to ultraviolet for greater channel capacity.
Then that is exhausted, and we will continue pushing...gamma ray bandwidth
fiber optic (or line of sight transmission) will eventually be a target,
despite its extreme difficulties even in theory.
At the same time we will be laying fiber and raising dishes to beat the
band. But no matter how well all that goes, we will *very* quickly reach
a saturation point of facilities as each new technology is introduced.
These days it's easy to be optimistic, because bandwidth is growing
geometrically. The problem is that there is no way in hell that that
trend can continue indefinitely. One or two decades hence we will saturate
theoretical limits.
Bandwidth is and will always remain a scarce and precious resource.
On the other hand, if you mean "slow channels by comparison with state of
the art channels," then yeah, *that* may as well be free at any given
point. Right this instant one could make an argument for 110 baud
channels being free.
I agree, but this seems to be a subject change.

@_date: 1993-11-10 23:29:24
@_author: Doug Merritt 
@_subject: Personality BS (was: Should we oppose the Data Superhighway/NII?) 
arromdee at blaze.cs.jhu.edu said:
True. And even aside from extreme examples like that, it is notoriously
hard to judge such things, even given people who are reasonable and operating
on the basis of good will.
On the flip side, ideally one would not have hired bigots in the first
place, so there wouldn't be such people making such judgements. (I haven't
stopped to do a head count, but as a white male I may actually be in the
minority in my group. Our V.P. is female and Jewish, as one example of
that. We're probably atypical. ;-)
And lastly, every method of interviewing anyone has ever conceived of
has its bad side. Nothing's perfect. Therefore this particular interchange
of ours is really merely a digression.

@_date: 1993-11-11 22:43:37
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
Mike Godwin  said:
I understand. I think that we're looking at different sides of the same
coin. I'm taking the long view, where you're taking the short view. For
the next several years, I agree that bandwidth will continue to increase
even as cost-per-bit-per-second continues to fall.
My previous comments were directed at the long term view, which may
be inappropriate to discuss at the moment, since naturally the long term
has no immediate pragmatic import.
Just keep those comments in mind 5 or 10 years from now. ;-)

@_date: 1993-11-11 22:44:22
@_author: Doug Merritt 
@_subject: Privacy, Property, Cryptography (long) 
Mike Godwin  said:
It's always nice to get quotations down correctly, but surely the
original "let" translates in today's speech to "left"? If not, I'd
like to hear about the difference.

@_date: 1993-11-11 22:59:23
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
"Perry E. Metzger"  said:
This is a nice reduction to theory; the current optical modulation rates
fall vastly short of the theoretical limits, but yes, at their maximum
it would be something on that order.
Heh. Well. By today's standards, theoretical-capacity fiber optic will
be indeed be overkill; there would be plenty left over.
Keeping in mind that we're talking about the medium to long term future
rather than the immediate future, though: needs tend to grow easily
as fast as does capacity to meet needs. In the past one can point to
1950's quotes about how many computers would ever be needed worldwide,
or to 1970's arguments about why GUI interfaces would never be realistic,
or even to Bill Joy's late 1980's Nanotech Conf. talk when he coined the unit
of VAX-MIPS-Millenia, which he thought would be useless even if available.
Counterexamples to Joy's thesis are trivially found in cryptography,
and less obviously in things like computer generated holography. The latter
might easily become a GUI standard of the future, and will indeed require
VAX-MIPS-millenia of computation to compute in real time.
They would also require similarly astronomical amounts of bandwidth to
transmit. By today's standards, that's ridiculous to assume. But by the
standards of 10 years hence, two dimensional video may well appear as
primitive as 110 baud text transmission does to us today.
Judging the future by today's standards tends to leave one's predictions
high and dry.

@_date: 1993-11-11 23:14:52
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
Lyle_Seaman at transarc.com said:
I only ask that you remember my bizarre prediction over the next decade.
I believe that history will vindicate me. The problem is that history
has vindicated me a number of times in the last 15 years, but no one
remembers by the time that the future rolls around.
Clearly I'm not taking the right approach even now, given that.
I'll keep working on it.

@_date: 1993-11-13 12:13:51
@_author: Doug Merritt 
@_subject: Should we oppose the Data Superhighway/NII? 
"Perry E. Metzger"  said:
This is highly misleading. Consider this. The Nyquist limit puts a
fundamental limit on bandwidth; you cannot transmit more information
over a channel than (roughly) the cycles-per-second rate of the carrier.
If a fiber uses optical wavelengths, then that fiber cannot carry more
than a single fully quality analog optical *pixel*. We get a lot more
out of them than that by reducing the quality of the image being sent,
e.g. by sending only 60 frame-samples per second, where each frame-sample
is itself carved up into X * Y discrete pixels, and each pixel has a reduced
dynamic range etc.
My point in saying this is that you're speaking as if current day video
standards are some kind of ultimate load on information transmission,
whereas actually it's just something we've settled for. HDTV will vastly
improve the quality of what we transmit without increasing bandwidth
much, but it is still a far cry from what can be desirable.
60 frames per second makes it impossible to transmit adequate information
about objects moving quickly across the frame of view that are easy to
perceive in person. 1000 frames per second is desirable. 2D images
are less desirable than 3D images. A minimum of about 300 horizontal
views by perhaps 100 vertical views is desirable for 3D viewing. Assume
compression of that 300 * 100 down to a simple factor of 300. Now notice
that depth of field information is desirable for realism (without this
everything is always in focus, good for some things, bad for others).
Let's give that a simple factor of 10.
I'll leave out arguments for increasing e.g. the dynamic range of contrast
and color information, even though they are currently several orders of
magnitude worse than the human eye can perceive.
That all gives us roughly 10 * 300 * 10 = 30,000 times more information
in a single *really* high quality "video" signal than we are currently
accustomed to.
Ultra high quality image transmission like this won't begin to become
significantly widely used for quite a while. But it will happen eventually,
because we'll be able to, and will perceive differences, etc.
Truly high quality video is one answer, even without taking into account
the problems carriers would have in supporting the full bandwidth of
a single fiber in switched networks (combinatorial explosion means that
they can never support every possible connection simultaneously, therefore
switching and multiplexing is here to stay).
The more general answer is to just keep in mind that demand for uses of
technology *always* outstrips the capacity of technology, if it is
affordable. Demand is limited only because of economic issues.

@_date: 1993-11-13 12:44:53
@_author: Doug Merritt 
@_subject: Bandwidth limitations 
It occurs to me that Perry will refute my attempted refutation of his
refutation by pointing out that even a factor of 30,000 in video
won't saturate theoretical fiber limits, and that he may consider
your examples too fanciful.
But part of what we're talking about is just timing, even Perry said so.
We cannot yet modulate fiber at its theoretical limits. To do that we'll
need optical frequency sub-band frequency modulation, and that hasn't
even been achieved in the lab yet. (FM tuneable dye can't be modulated
at optical rates. Semiconductor "variable frequency" modulated lasers
can only switch between discrete frequencies.)
Some think it will take twenty years to achieve this. But I'm optimistic
and hoping for 5 to 10 years. (Unsure about commercial deployment, but
let's say it is fast and can use existing fibers, if not trunk equipment.)
So if you're "realistic" about when we'll be able to achieve fiber
saturation modulation, we're also far enough into the future that it
gets easier to see that we may have completely novel demands on information
transmission by then, and that existing demands will continue to cause
problems for existing fiber technology.
Conversely if one is optimistic about achieving theoretical limits on
fiber, then the fine points of the argument begin to be relevant. That
factor of 30,000 for video won't be enough to fill the fiber. Receiving
every global TV and Internet (ultra high quality) video transmission
simultaneously (to record and allow later channel switching) might do
it, but I have to admit that it seems chancy.
So it all comes down to the time frame in which the theoretical limits
are achieved.
Unless one gets speculative...for instance, nanotechonology scan-transmit-
and-rebuild could easily more than saturate even a large number of fibers.
Or slightly less blue sky: if your computer is an array of 10,000 optical
computers each operating at 100 gigahertz, and doing a distributed computation
with other systems over the net. (In this case networks are *always*
the bottleneck.)
Anyway the whole subject seems debatable and a matter of which numbers
one cares to predict for which future year. But we all agree that it's
merely a question of *when* fiber runs out of steam, not whether.

@_date: 1993-10-01 08:58:18
@_author: Doug Merritt 
@_subject: POISON PILL 
nate at VIS.ColoState.EDU (CVL staff member Nate Sammons) said:
Yeah, I guess thermite would be better than a shock wave bomb in some ways.
I could believe that, with sufficiently careful attention to detail, it might
be able to heat 100% of the magnetic material beyond the medium's Curie point
and therefore beyond all recovery. If on the other hand the thermite were
simply tucked next to the drive...well, it's amazing what forensic
investigators sometimes recover from things intended to be destroyed
by thermite. A lot can go wrong in real world situations.
Conceivably. I'm unclear about the effects of EMP on magnetic media.
Also, really good EMP requires fairly powerful shaped charges. Perhaps
a simple degaussing coil would do the trick. Not sure. Seems like it should.
Do any modern hard drives attempt magnetic shielding in their cases?
Now that prices on recordable CD ROM drives are down to $4K, personal
CDROM archiving will become more and more common; thermite would be
handy there, too, again assuming very careful design & placement.
This little discussion is all just for the fun of it from my point of view; I
agree with other comments here that leaving your data encrypted is more to
the point, not to mention far more reliable than just about any other

@_date: 1993-10-01 17:38:14
@_author: Doug Merritt 
@_subject: secretive API 
I don't know if this is old hat, but I was amused to see a moment ago
on another mailing list the following quote from the Windows NT API:
    ERROR_TOO_MANY_SECRETS -The maximum number of secrets has been exceeded.

@_date: 1993-10-01 18:08:13
@_author: Doug Merritt 
@_subject: TRAVELLER'S ADVISORY 
an36440 at anon.penet.fi said:
Now, now. Sandy's got a point, John could in fact get zapped by
overly zealous authorities. It may be mildly paranoid to think that they'd
plant evidence on him...the suits only do that if they're *REALLY*
pissed at someone, and even then only sometimes...they're risking their
jobs every time they do that, after all.
Actual conspiracies are comparatively rare, if only because they take a
lot of high level carefully planning and support to execute well. It's
not like John is a key figure in South American politics or something.
But nonetheless, there's still the chance that they'd get a fixation on
the notion that he was carrying something illicit out of the country
and interfere with his travel in a very unpleasant way.
It wouldn't be the first time such a thing happened purely out of
misplaced zealousness.
As for solutions, the age-old approach is to be a high profile
philanthropic and campaign contributor and elbow-rubber, so that one
has a lot of community support if one gets leaned on. This would be a
private matter for John, though, and not up to us to discuss. I mention
it simply because it *is* a well known partial solution. I actually
don't exactly recommend it, for complicated reasons.
Considering certain recent posts (and considering the Internet at
large, for that matter), I'll still give you points for showing signs
that you paid attention to everything Sandy said and responded to what
he actually said. I wish everyone would do that. ;-)

@_date: 1993-10-02 20:14:27
@_author: Doug Merritt 
@_subject: PGP in FIDO 
khijol!erc at uunet.UU.NET (Ed Carp) said:
Practical experience is a good thing. However, you are neglecting something:
the case where your spouse doesn't agree. This is real life, and such
things do happen.
Your ex-common-law wife can retroactively file for alimony, or even for
getting hold of your property during divorce proceedings, and use
testimony from mutual acquaintances to establish that a common law
marriage had been in effect.
The details vary from state to state, but I would be surprised if it
were as few as 9 states. There also tends to be a time interval involved...
for instance, if you live together and apparently share a budget,
pooling resources, and other such things, for N years, then if that
can be established by witnesses, then you've got a common law marriage.
The point that if no one knows, what difference does it make?... is
a lot like the proverbial tree falling in the forest. The difference
is that, in real life, people *will* know if you lived with an SO for
many years in a relationship that resembled marriage, and they can and often will testify to that fact.
Having a baby during that period certainly helps nail down the legal
status, but is not required.
Introductions as husband and wife helps, but is not required.
"Technically correct" versus pragmatics is often just a matter of whether
one's ex-SO is vindictive enough and knowledgeable enough to nail you.
I am not a lawyer; the above is merely my lay understanding of laws,
and hence may be completely incorrect.
P.S. Let me guess, despite the length of time that this issue has been
kicked around here, I'll get nailed for posting something that's not
apropos to cypherpunks. So let me point out how it is apropos: privacy.
If no one knew about your 10 year live-in lover, you wouldn't be able to get
nailed on such an issue. I'll leave it as an exercise to the reader
to figure out how to use cryptography to ensure the secret. :-)
Cryptographic sex? Naw....

@_date: 1993-10-02 21:44:27
@_author: Doug Merritt 
@_subject: Ultimate privacy/security 
Email conversations have made me realize that I didn't sufficiently
explain an important aspect of this hypothetical algorithm.
I understand that there are times that privacy, and authentication
schemes that aim at establishing unique identity for the purpose of
guaranteeing privacy, are an end in themselves.
The speculative algorithm I mentioned, which would authenticate intentions
and goals and such, was intended only to address situations where
authentication of identity for privacy was a means to an end, not
an end in itself.
In situations where only privacy and authentication of individual
identity of such will do, for arbitrary reasons as opposed to functional
reasons, I've nothing to say (for the moment. ;-)
But in situations where there is a *functional* reason to authenticate
identity, then and only then do I propose to consider a hypothetical
algorithm in which goals and/or motivations and/or philosophy and/or
ethics and/or etc is called in to play.
Thanks for all the responses and feedback to date.
        Doug

@_date: 1993-10-03 08:58:58
@_author: Doug Merritt 
@_subject: troglodyte MIND RAPIST flames, take III 
Mike Godwin  said:
An interesting form of immortality. :-)

@_date: 1993-10-03 10:04:32
@_author: Doug Merritt 
@_subject: PGP in FIDO 
"Perry E. Metzger"  said:
Sometimes it is and sometimes it isn't. I recall some years ago when
bartenders were getting convicted for their patron's drunk driving
One can't always count on laws being reasonable, and if they are, you
still can't always count on courts interpreting them reasonably.

@_date: 1993-10-03 10:09:32
@_author: Doug Merritt 
@_subject: a2 test 
a2 at ah.com (Arthur Abraham) said:
With a bit of wrestling I was able to decrypt this. It's a compressed
digitized image of Arthur; the background is out of focus, but it looks
like Wendy's holding up an A-Squared logo. Looks like one of his test
images from some years back.
Amazing compression ratio.

@_date: 1993-10-03 17:38:59
@_author: Doug Merritt 
@_subject: PGP in Fidonet 
Some time back (maybe quite a long time ago, I forget) I heard rumors
that a BBS that someone had uploaded child porn or some such to,
without the sysop's knowledge, resulted in the sysop's arrest. Was that
merely a rumor?
Or is that a different category than the mail traffic you're commenting on?

@_date: 1993-10-03 17:49:01
@_author: Doug Merritt 
@_subject: NPR? 
DON_HENSON at delphi.com said:
So phrase it in a slick way that gets past their bias. For instance,
lie and say that "other government agencies are concerned that..."
Anyone biased towards the govt will be willing to believe that some
part of it is a benevolent avuncular sort. It's part of that mind set.

@_date: 1993-10-07 13:45:34
@_author: Doug Merritt 
@_subject: Good editorial in the Merc 
The Thursday Oct 7 San Jose Mercury News lead editorial contains a note
acknowledging a sane point of view about cryptographic exports.
(It's titled "Unshackled Tech -- Looser export rules will boost Silicon
Valley Sales", and it discusses Clinton's decision last week to boost
the speed limit for exportable computers.)
The *good* part, though, is:
  "  * Congress should update the Export Administration Act.
     A 1990 version would have lifted export controls on
  telecommunications and software with encryption codes, which give
  the software users a better way to keep data secret. President Bush
  vetoed it. His administration wanted to deprive the Chinese and
  Russians of the latest, hard-to-bug telecommunications equipment, and
  the latest encrypted software.
     Well, the Chinese already know how to make the new
  telecommunications equipment. The encryption software is in the
  hands of anyone who can find a pirated copy."
Awareness is beginning to grow out there.

@_date: 1993-10-07 23:09:22
@_author: Doug Merritt 
@_subject: that internet security scanner 
Stanton McCandlish  said:
(A) The "evil" in question was posted to comp.sources.misc (or some other
similarly obvious group). (B) The advisory was about as neutral as such
a thing can be.
Further comment fails me.
        Doug

@_date: 1993-10-11 00:11:09
@_author: Doug Merritt 
@_subject: Virtual City (tm) and Virtual Capitalism (fwd) 
Er...there are a *lot* of different kinds of mu*'s, and at least as
many different definitions of "wizard". In some worlds they call people
wizards who would simply be players elsewhere; in other worlds "wizards"
have godlike status. Similarly with "programmer"; it can mean an advanced
builder, or it can mean somebody who actually hacks the C underpinnings.
You'd have to be a hardcore afficionado of the whole range to have basis
for sweeping statements.
(The kind that I'm least familiar with is apparently the most common,
where people earn wizard status by advancing in some kind of game, but
there's endless variation. When people create their own worlds, they
tend to create ranks to suit their own tastes, too. Gonna legislate
world creation? ;-)
        Doug -- a sometime but partially cured mu* wizard and programmer

@_date: 1993-10-11 13:51:08
@_author: Doug Merritt 
@_subject: RSA Security 
peb at PROCASE.COM (Paul Baclace) said:
That's also more attractive considering that the U.S. government is
quite capable of demanding the secret of the factorization method and
generally throw their weight around as "justified" by national security...
I'm not generally very paranoid, but I *would* be in a situation like that.
Just about any and all other laws go out the window once they invoke
national security.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
     (The above is a joke; the following are mailing lists:)
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-10-11 19:56:43
@_author: Doug Merritt 
@_subject: Breaking DES 
pmetzger at lehman.com said:
2^56 bytes equals 10^7 gigabytes. At roughly $1000 per gigabyte,
that equals 10^10 bucks...10 billion dollars. Or say there's a quantity
discount in orders totalling a million units, and you get the whole
capacity for 1 billion dollars.
Well, that's a bit steep for me, but there's no question but that the
NSA could afford it. Still, what do you say I wait a few years until it
comes down to 10 million dollars, which I happen to have available in
the year 2003 in my company budget? Ten years should do it, estimating
That ten years also means that rather than searching 10^7 units in parallel,
we will then be searching only 10^5 units in parallel. It'll still take
a few hours, but that's ok.
This all suggests that the NSA could do such a thing *now* if they *really*
cared to, and could do so fairly trivially in 10 years.
I did a double take on this at first too, since naively one would expect
the search to be (2^56)^2. However, this can be improved, for instance
by sorting each set in N lg N time (56 * 2^56 operations), and then doing
interleaved comparisons in N lg N time again, which can be mostly parallelized
over those 10^5 computers that are running those 10^5 disks, so that the
total time would be (since 10^16 = 2^56) 10^16 / 10^5 machines = 10^11 cycles,
and given 10^3 MIP machines, this gives 10^4 seconds (20 minutes) for each
phase...call it an hour total.
(In other words, as a first approximation, Karl is accurate to assume
linear rather than quadratic speed for this.)
This neglects coordination of the networked machines, which one might
expect to add a factor of 5 to 10 to those numbers.
This rough analysis demonstrates that Karl's scenario is merely expensive
now, and "cheap" (by NSA standards) ten years from now, rather than
completely inconceivable.
I guess the weakest point of the above back-of-the-envelope estimate
is that each e.g. plaintext & cyphertext is assumed to be representable
within one byte, but that's *not* fatal. You could use hashing to get
down to one byte, and when a hit is detected, try again using two
bytes. When hits are detected there, use four bytes...and so on. That
approach allows the real world scheme to be reasonably close to the
back of the envelope gross assumptions.

@_date: 1993-10-12 09:39:54
@_author: Doug Merritt 
@_subject: Breaking DES 
No, I'm not. I *am* assuming disk, obviously, since I quoted $1000 per
gigabyte, which is disk price range, not RAM.
Untrue. The disks will be used in a predictably serial fashion, and therefore
read-ahead can be arranged such that everything is in RAM by the time
the algorithm is ready to use it, so the whole thing runs at RAM speeds.
It's true I didn't factor in the cost of the systems, but that doesn't
give more than a factor of 2 to 5 in cost (depending on assumptions about
the precise kind of pc clone used), where you seem to have come up
with a factor of 100.
Skip the sarcasm and pick a different quantity discount. If you don't
like my 90% discount for quantity 1 million disk drives, pick another
one. There is always some discount for quantity, and this is just a back
of the envelope estimate, so I don't care much what you pick. 0% discount
leaves the estimate in the region of $10 billion...that's still not
inconceivable, merely expensive.
That was my only point, that this *could* be done, and I've proven that,
despite your misunderstandings.

@_date: 1993-10-12 19:29:57
@_author: Doug Merritt 
@_subject: Breaking DES 
I was originally assuming a one byte result per calculation and gave a
hashing justification. (I glossed over the overhead for this approach,
but we could analyze this, too, if anyone's interested.) You however are
multiplying by 8 for no clear reason. Feel free to explain, but the way I see
it, the calculation is:
(2^56 calculations) * (1 byte per) * ($1000/disk) / (10^9 bytes/disk)
...which comes to 72 billion dollars. This is significantly larger than
my first calculation, and the difference is purely due to roundoff error,
because I said that 2^56 = 10^16, where it's actually 10^16.8576.
10^16 * $1000 / 10^9 does indeed equal my original figure of $10 billion,
so it's just that I should have left another digit of precision in rounding
the exponent.
Now I agree that 72 billion dollars is a lot. Even 10 billion is a lot.
But all I was trying to establish was that these numbers are not *completely*
impossible, because you were giving Karl a hard time about this.
Ridiculously expensive is a very different thing than *impossible*.
There are schemes that result in figures like 10^9 billion dollars...
*that* I call plainly impossible.
I also projected forward 10 years to let prices come down by a factor
of 10...that's another way of underscoring *possibility*. Over the
last 10 years disk drive prices have fallen significantly more than
a factor of 10 per megabyte, so I am being quite cautious here. A
factor of 30 is justifiable, but I won't go that far, I'll continue
to be conservative.
Even your own figure of $576 billion becomes $57.6 billion in 10 years,
which is merely too expensive, not *impossible*...if WW III were underway
and such a project were of critical importance, it would happen...
$50 billion would not be too much under *those* circumstances. That's
the difference between "impossible" and "expensive".
If you want to critique this part of my back-of-the-envelope, its
weakest part is the sorting, in which it is very hard to effectively
serialize disk access.
For the benefit of the doubt, let's give that a factor of 100 slowdown...
so that 10 years from now we have an *average* disk transfer rate of 10^7
bytes per second for this algorithm rather than the 10^9 that I was assuming.
I think it could be done faster, but even so, this increases the time from 1
hour to 4 days...still not an impossibility, just not as *nice* as 1 hour.
Again, I need only establish possibility for the algorithm Karl related;
I'm not saying the NSA *will* do this.
Ok, so figures are doubled...my 72 billion becomes 144 billion. Pretty
expensive. Not *impossible*.
I never claimed this was likely...all I was after was to see whether it
worked out to e.g. 10^9 trillion dollars...*that* I would call impossible.
Actually, yes I have; I've been a hardware and software systems architect
in all kinds of different subspecialties. The fact that we may disagree
doesn't make me an idiot...hell, I may even make drastic mistakes and say
things that *are* idiotic. It still doesn't make me an idiot...it would
make me "someone who made a mistake".
Considering that I was doing a quickie back-of-the-envelope, I'm not even
embarassed about such mistakes.  No one else did an estimate. 2^56 *sounds*
ridiculously huge; I'm content to be within a factor of 100 in showing that
it is merely quite expensive. I daresay that you yourself have a better
feel for the expense now than you did when you first critiqued Karl's post.
Flaming me is a poor way to win an inherently technical argument. Stick to
the point.
It is true that you can't do better than the average transfer rate,
and here you have a valid point, I neglected this. It would be unrealistic
given the other assumptions to assume better than 10^7 bytes per second
transfer rate with technology 10 years hence. In fact even 100 megabytes
per second might seem high to you, so let's call it 50Mb/s (surely a
very conservative figure), for a total of 20 times slower than I estimated.
That increases the 4 days to 80 days. Not very nice...but *possible*.
You see the pattern here...you are raising valid technical objections, a whole series of good points that I glossed over with my back-of-the-
envelope calculations.
But even so, it doesn't change my basic point that the approach is *possible*.
You need to find a factor of perhaps 1000 in cost and a factor of perhaps
1000 in time in order to demonstrate that this approach is inherently
The fact that you spot flaws in my back-of-the-envelope also doesn't mean
that it's called for to flame me. Again, let's stick to technical discussion.
Tsk, another flame.
Even for a flame, I don't get this. I said, if you think that a 90%
discount for quantity-million is unrealistic, tell me what discount
you think *is* realistic. That's a valid question. The $1000 per
gigabyte drive is roughly accurate *today* in quantity *one*. The
higher the quantity you buy, the better a discount you get; that's the
way it works, and I'm sure you know that as well as I. So perhaps my
90% discount is overly optimistic...fine, I say...tell me a different
figure. If you say 10% I'll argue. Anything between 10% and 90% is
conceivable, so pick your figure.
It still doesn't affect the bottom line argument as to whether the algorithm
Karl mentioned will be possible in 10 years. It clearly would be very very
expensive. It would also clearly *not* be completely impossible.
Karl posted something which is theoretically reasonable but that is
nontrivally expensive even ten years from now. He deserves credit for
discussing a theoretical possibility which is even marginally conceivable.
He does not deserve a harsh response...and I think you *were* harsh to
Do me a favor and skip the flames in your future responses; they're not
very much fun.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
     (The above is a joke; the following are mailing lists:)
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-10-12 20:36:35
@_author: Doug Merritt 
@_subject: Spread-spectrum net (vulnerability of) 
Matthew J Ghio  said:
I used to work with the person who set up 80% of the West Coast ham
radio digital packet system repeaters. He had a bit of money to burn,
and he set up a *bunch* of these repeaters on various mountain tops up
and down the West Coast. This was circa '85 by the way.
Each repeater required:
Your opponents wouldn't do that...the "enemy" here is simply the FCC,
count on it. They have field agents who triangulate illicit transmitters,
and once they find one, they simply get all the warrants and court orders
needed to deal with it.
Sure, this is possible. But it doesn't help *that* much. Triangulation
of signal spots any antenna quickly, and they cut that off immediately.
You can get increasingly elaborate about hiding the signal source, and
restoring antennae quickly as they're pinched, but it's sort of a losing
battle unless you assume real time response by the underground lead by
a brilliant EE type.
I am not saying that an underground wireless net is impossible. I *am*
saying that the difficulties are much higher than they may seem at first
I think doing something like this is possible, and it would have definite
benefits. But anyone moving on this would do well to get in touch with
the existing ham radio crowd who have dealt with the pragmatic issues
involved for the better part of a decade (or more).
Right...in a sense, this is the easy part. Definitely one can hide
physical locations if everything is set up carefully.

@_date: 1993-10-12 20:46:35
@_author: Doug Merritt 
@_subject: Native American Encryption?! 
tcmay at netcom.com (Timothy C. May) said:
It's mentioned in a bunch of crypto sources, but coincidentally, PBS
just ran an entire show about this precise subject a couple weeks back.
Not just passing mention...the whole show was about this.
Therefore talking to PBS people would likely yield some good info...
and possibly videos.
Err...."of course"??? Codes are (all else being equal) quite a bit
more secure than ciphers.
(or do I have "code" and "cypher" reversed...whatever. :-)
There's a classic SF story, whose title I forget, about anthropologists
trying to figure out writings of a dead species on Mars. The table of
elements finally proved to be the Rosetta Stone equivalent.
The interesting thing about codes, which in a sense includes all natural
languages, versus ciphers, is that code systems represent semantics. If
the underlying semantics is radically different than what the code-breaker
knows...too bad.
It was more complicated than that. They used a hybrid system that native
Navajo speakers could not decrypt, because the system used not only Navajo,
but on top of that, arbitrary (and newly invented) metaphors for concepts,
and (newly invented) puns to represent ciphers, too.
It is certainly true that part of the security was "through obscurity",
but (A) that part was effective..."security through obscurity" can be
effective over short periods of time...and (B) they layered ciphers on
top of codes.
The obscure linguistic aspects of Navajo vs. other modern languages is also
said to have played a part, but I haven't researched this yet, so I won't
Arthur Chandler  said:
In the absence of a Rosetta-Stone-sort-of-thing, we're still lost. For
instance, the Easter Island hieroglyphs are still completely mystifying.
Some of Nyquist's mathematical results are still classified, so one
never knows, but...
Arbitrary semantic systems encoded in writings are not decipherable,
period, barring some breakthrough in mathematical semantics...don't hold
your breath. :-)

@_date: 1993-10-12 21:06:36
@_author: Doug Merritt 
@_subject: Spread-spectrum net (vulnerability of) 
jkreznar at ininx.com (John E. Kreznar) said:
This is true only for casual observers. If the FCC were after you, they
could most definitely triangulate on unusual noise sources as well as
they could a coherent signal. Power signals are a giveaway.
Good enough as far as it goes. But this implies a large drop in efficiency
of the transmitted signal. That's not a stopper...*if* you've got power
to spare. But that implies enough power for bad guys to triangulate your
noise source...ouch.
If they pin you down to within a building, you've lost.
There are other approaches...phase-sweeping...phase-conjugation...

@_date: 1993-10-12 21:09:58
@_author: Doug Merritt 
@_subject: Breaking DES 
Thank you, Perry, for some good comments that were flame-free. I
personally appreciate that, especially considering that your comments
are both apropos and good food for thought.

@_date: 1993-10-12 21:16:35
@_author: Doug Merritt 
@_subject: Breaking DES 
pmetzger at lehman.com said:
Yes, I did. If hashing doesn't work, you'll have to say why not. It's a
technique that works in most other situations.
As I said, we can tackle this if anyone cares to...it's unclear that
this is an invitation, but assuming it is: hashing gives a first-pass
screening good for every 1/256 calculations, given the assumptions I
stated. For each collision more work is needed...but you haven't invited
that analysis, nor addressed it yourself.
Impractical? Your response to Karl implied that it was *impossible*. If
you wish to apologize to Karl, and say that it is merely "impractical",
then I will agree with you and drop the subject. The expense required
definitely indicates that it is "impractical."
Clearly you are preparing to drop the argument because you sense that
your tactic of flaming didn't work. I welcome the lessening of flames,
so thank you for that. We could use less flames here.

@_date: 1993-10-13 09:16:42
@_author: Doug Merritt 
@_subject: Spread-spectrum net (vulnerability of) 
jkreznar at ininx.com (John E. Kreznar) said:
Very low power transmitters are actually legitimate, at least in some
bands, so you don't have to drop the signal to the point where it'd
merge seamlessly with ambient noise.
However I doubt that extremely low power transmitters will accomplish what
is desired.
I also wonder whether extremely low data rates are desired.
Different topic. Try "phase conjugate mirrors" in optical and physics
journals. I'm not positive that this would be good enough to help avoid

@_date: 1993-10-13 09:42:03
@_author: Doug Merritt 
@_subject: Breaking DES 
Instead of storing the underlying value, I am assuming that it is *thrown
away*, and recalculated whenever there is a collision. This cuts down
on the expense of this disk drives, at the cost of increasing runtime
by roughly a factor of 7 (log_base_256(2^56)). I didn't include that
cost in my previous estimates because I was doing a very rough back of
the envelope calculation, but I accept that it should be included.
It's a question of where you draw the line. A budget of one hundred billion
dollars and a runtime of say a year, I'm willing to call "impractical".
A budget of 10 trillion dollars and a runtime of 100 years, I'd be
willing to call "impossible".
The 2000 bit key is over everyone's threshold.

@_date: 1993-10-13 19:26:46
@_author: Doug Merritt 
@_subject: pornography & the ``cypherpunk cause'' 
Edward Elhauge  said:
Doesn't this need to be phrased more cautiously? As I understand it, you
need to demonstrate to the jury that you feared for your life, and someone
breaking down your door is not enough for that. Unlike in some other states,
in California it is essential that there be a clear threat to your life.
The phrasing of this makes me think that you and I have the same impression
of the law, but that you just were a bit terse in phrasing I tend to be concerned about arguments that don't take the infamous
"ignorance of the law is no excuse" into account. That is, one needs to
make a clear distinction between ignorance of the law and ignorance of the
action itself.
There have been times in this particular thread when I wasn't clear whether
people were making that distinction.

@_date: 1993-10-13 19:46:47
@_author: Doug Merritt 
@_subject: Spread-spectrum net (vulnerability of) 
Matthew J Ghio  said:
Think from the point of view of the spotters. They can look at a
broad-spectrum scan, gradually eliminate known sources, and end up
homing in on the remaining high power signals.

@_date: 1993-10-14 20:12:04
@_author: Doug Merritt 
@_subject: Spread-spectrum net (vulnerability of) 
[ several good points ]
On the other hand, this particular approach to spread spectrum is
mostly depending on avoiding getting the FCC annoyed in the first place,
which means that bandwidth consumed per unit metropolitan area must be
below some particular threshold. If that net became very popular, it
might be difficult to hold down to the required level.
If the FCC *did* go after the transmitters, it wouldn't cost them anything
like $10K per to find. Well, I dunno, I suppose you could assume that
the transmitters are awfully hard to distinguish from ambient sources,
but it would take more than handwaving to establish such a high cost to
the FCC. I'm willing to believe some such design is possible, but I
wouldn't want to leave it to chance and oversight.
Also, let's say you had a good design that cost the FCC a fair amount of
bucks to design a specific detector using a targeted active filter, for
instance. Then you'd be safe right up to the point where it's not worth
their while, and the network grows, and then suddenly they invest the
time and money to build something that can catch dozens of transmitters
per day.
In other words, you'd want something that still worked even given growth
and success.
        Doug

@_date: 1993-10-18 09:17:15
@_author: Doug Merritt 
@_subject: Cypherwaffle on spoofing 
Arthur Chandler  said:
I don't understand your point. If it really is impossible to prevent
people from creating multiple anonymous identities, what good does it
do for any or all of us to say that we abhor the results? I grant
you that there will be bad effects...but what precisely do you suggest
we do about it?
If you think that people *can* be somehow limited to a single anonymous
identity, you'd best explain how you think so.
If on the other hand you agree that such a limitation can't be enforced,
then I fail to see your point at all.

@_date: 1993-10-18 20:17:20
@_author: Doug Merritt 
@_subject: "True Names" and "Ender's Game" (was: Cypherwaffle...) 
tcmay at netcom.com (Timothy C. May) said:
I second that. Not only are they aropos, they are also some of the
better stories by those two excellent authors.

@_date: 1993-10-18 22:32:23
@_author: Doug Merritt 
@_subject: backing? 
tcmay at netcom.com (Timothy C. May) said:
You are a modern person in this thinking. Not all are. The uncoupling of
the U.S. dollar from a government-specified gold standard in the 1970's
is *still* a controversial issue with some people (not all of whom are
idiots, by the way, although I personally with disagree with 95% of them).
It is practically a truism that bull markets bring out modern thinking
about currency and that bear markets bring out gold-standard thinking
about currency.
Low-margin speculators regularly make money by predicting that kind of
psychological reaction alone. (The "low-margin" qualifier is a short-hand
to say that "no, *you* can't count on making money that way." :-) I assume
that some will disagree that *anyone* makes money that way, but that's not
really my point.
My point is, for digital currency, it makes sense to model non-digital
forms. There will be times that people feel insecure and believe (for
whatever reason) that gold-backed digital currency is the way to go.
Other people in other times won't be interested in gold-backed digital
currency, and that brings up different algorithms.
The psychology of the market (past, present and future) seems to me to
say that one shouldn't consider algorithms of only one form.
Anyone for digital currency mutual funds? :-)

@_date: 1993-10-19 07:57:29
@_author: Doug Merritt 
@_subject: backing? 
pmetzger at lehman.com said:
Just in case I was unclear: my point was that it makes sense to have
digital currency that is backed by gold, not just the other forms which
are not thereby backed.
I expect that lots of folks will support their own idiosyncratic forms
of digital currency in the future, somewhat similarly to the way that
banks used to issue their own paper currency, and that therefore there
will not be just a single kind of digital currency.

@_date: 1993-10-20 21:52:39
@_author: Doug Merritt 
@_subject: backing 
F_GRIFFITH at CCSVAX.SFASU.EDU said:
So in essence you're saying that poorly-backed digital currency will
win out over e.g. U.S. dollars?
That's an interesting concept.
(I mean the above literally, not sarcastically; I don't care to take
either one side or the other of this question right this instant,
although I will say that there must be some extra caveats to add.)

@_date: 1993-09-06 11:51:22
@_author: Doug Merritt 
@_subject: Law Review Articles 
Here's some more:
        authenticated signatures in regard to digital banking,
                direct democracy (both net and actual government),
Conceivably it's too early in the state of things to address some of these
topics, but they'll all arise at some point, will he nill he.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-09-14 22:44:13
@_author: Doug Merritt 
@_subject: more than spread spectrum 
cme at ellisun.sw.stratus.com (Carl Ellison) said:
You might want to contact the Garden project, which I believe is operational
in San Jose and in Santa Cruz. I'm not up on all the details, but it is
far too similar to what you're talking about to ignore.
        Doug

@_date: 1993-09-15 17:10:33
@_author: Doug Merritt 
@_subject: Joke of the Day 
an23412 at anon.penet.fi said:
Uh-oh...your anonymity is partially compromised; we can now narrow down
your identity 91.7 percent more closely than before!
A few more slips like that and we'll have it down to one of several
thousand choices...
Speaking of privacy violations, a coworker relates a story from his
father (ok, just a FOAF story, but it's still interesting): He needed
to find his wife, who'd headed off for some errand and thence to the
airport, leaving her spending cash at home. He called her on her cellular
car phone, but she'd turned it off. He called Cellular One and explained
the problem, and they told him over the phone where she was...he sped off
and caught up with her.
Although this was handy for him, it's an obvious privacy problem for
Cellular One to tell someone where one of their customers is.

@_date: 1993-09-17 09:09:28
@_author: Doug Merritt 
@_subject: Trust fund to support free software.  Backed, in part, by PGP. 
Although that's an interesting scheme, it would be even more trustworthy
to send the money directly to the trust bank account, which can be done
in simple ways. We need better algorithms than the one suggested.
        Doug

@_date: 1993-09-18 19:15:38
@_author: Doug Merritt 
@_subject: NIST proposes software key escrow development 
an12070 at anon.penet.fi said:
Clearly this is true for cypherpunk sw developers, but others see an
opportunity to make some bucks.
Not necessarily. Zero knowledge proof techniques, for instance, can be
applied to make source code as impenetrable as one wishes. This tends to
carry a heavy runtime overhead, of course.
And even hardware solutions can be reverse engineered. In fact, it's
guaranteed to happen eventually. Triple layer metal interconnect chips
can be selectively peeled via ion beam etching to reveal them to scanning
tunneling electron microscope probing. Camouflage in the form of unnecessary
functional units that mask actual operation can be uncovered by data flow
analysis. Such a project would be extremely expensive...but someone will
eventually do it. The Mafia or the KGB, for instance, if no one else.
I think everyone assumes that the NSA is technologically several steps
ahead of the game at all times, and clearly they have their own agenda.
Some people just don't see their hidden agendas as threatening. C'est
la vie. I think it makes for a very interesting chess game, myself. The
NSA is attempting checkmate, but they're not strongly enough positioned
to do so. In chess parlance, it's a bluff, but one with enough steel behind
it to force a response, which gives them a minor but real tactical advantage.
The obvious counter-response is to advance a pawn towards queening...which
is already in progress.
I'm reasonably happy with what the NSA appears to be doing in regard to
foreign intelligence gathering; it's their domestic agenda that threatens
the constitution. But that's in the nature of spook organizations.
"Eternal vigilance is the price of liberty."

@_date: 1993-09-19 08:59:46
@_author: Doug Merritt 
@_subject: gopher links 
henry strickland  said:
"cool" links are the opposite of "hot links". The latter are active;
nontrivial computation is done when the link is followed. The former
are passive and merely point to information.
Hypertext jargon.
Then again, the man page you quoted implies that the author just
thinks that such things are cool.

@_date: 1993-09-19 09:16:50
@_author: Doug Merritt 
@_subject: Definition of "Zero Knowledge" 
An arbitrary algorithm can be translated into a zero proof theory model
that is intractable to functionally analyze. Its operations on inputs and
outputs can take place within the realm of the intractable model, with
the inputs and outputs being transformed from the encoding of the outside
realm into an encoding useful to the realm of the model. The inputs and
outputs are the queries and answers of zero proof theory.
With such a thing, knowing every detail of the registers and instructions
being executed at all times still wouldn't tell you what you really
wanted to know.
I'm unsure whether this has been published, let alone implemented; I just
thought it was an obvious corollary back when ZPT itself was first published.
It might have been discussed in the literature at the time, but if so,
I've forgotten.
I'm also not claiming this is necessarily a useful approach in practice,
because the obvious ways of implementing such a thing would be more than
a little slow. I suspect that it could be done efficiently with a little
algorithmic cleverness, but I don't have evidence of that this instant.
I'm currently designing something with very much the same flavor, but
with somewhat different goals, and computational expense of the operators
in the model is precisely the difficulty with that, too, so far.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1993-09-19 09:29:46
@_author: Doug Merritt 
@_subject: NIST proposes software key escrow development 
Brad Huntting  said:
Because we weren't (just then) talking about foiling it, we were talking
about whether it could be figured out. Unless I misunderstood the discussion
from careless reading, the point seemed to be that they wanted to make
the workings of the chip secret.
I was furthmore assuming that there would be motivation for people to
figure out such secrets because it would help them do their own illicit
decryptions, but that's getting slightly off subject.
That's not what I said. The charter of the NSA isn't to abuse the rights
of your foreign friends. Now that you've raised the subject, I wouldn't
be happy with the NSA doing so, either. I have friends outside the U.S.
too. I just meant that countries do intelligence gathering on each other
all the time as a matter of course, and most of that isn't abusive in
nature, it's just the usual game of politics and defense etc.
But I suppose it's silly of me to say something like that on a list with
so many rabid paranoids; how could I possibly imagine that not *every*
act of the NSA is inherently ***EVIL***!?

@_date: 1993-09-19 12:19:47
@_author: Doug Merritt 
@_subject: more deranged lunatic ravings -- just delete 'em! 
erc at apple.com said:
True. But the question is, specifically what are you suggesting "all of us"
do right now? When I say specifically, I mean for instance not just
"find out information about XYZ" like you said before, I mean the
"and then what?" part as well. Most of us will probably do our best to
learn whatever we can rather than ignoring the situation. But *then* what?
What's your suggestion?
Besides that, I mean. :-)

@_date: 1993-09-19 17:40:55
@_author: Doug Merritt 
@_subject: Definition of "Zero Knowledge" 
bill at twwells.com (T. William Wells) said:
In the current context, the best reference that I know of is to
the methodology of Goedel's Theorem rather than to ZPT; it has each
of the properties that I mentioned except for the ZPT operations,
which can be added in a conceptually straightforward way. The most
readable in depth treatment of that that I know of is "Goedel's Proof"
by Ernest Nagel and James R. Newman, c. 1958 and still in print as
a cheap paperback.
If someone has good ZPT references that would be interesting too; I've
lost the stuff I used to have on that.

@_date: 1993-09-19 22:16:00
@_author: Doug Merritt 
@_subject: Definition of "Zero Knowledge" 
khijol!erc at apple.com (Ed Carp) said:
"The Gold at the Starbow's End", Frederik Pohl, Ballantine Books, 1972.
Serialized in Analog in '71 or 72. Great book!
You're showing your age. ;-)
( Like I'm not...if anyone on the list wasn't born at that point, I don't
wanna hear about it :-)
I think this was the first place I ever heard of Go"del numbering.

@_date: 1993-09-19 22:19:53
@_author: Doug Merritt 
@_subject: Does this seem illegal to you? 
mgream at acacia.itd.uts.edu.au (Matthew Gream) said:
In the U.S., you can buy CD-ROM white pages for the entire US for $99,
and the same database reverse-directoried for another $99. (Approximately;
this is from memory from a magazine ad I saw 4 days ago.)
Such things have been available on paper for a long time, but this media
and these prices will doubtless have social repercussions.
As before, people can make a point of having their address not listed
in any such directory, forward or reverse, if they're careful enough.
However, the need to do so probably increases as reverse indexing becomes
so vastly more available.

@_date: 1993-09-20 09:17:13
@_author: Doug Merritt 
@_subject: a quote 
Sorry to add to the S/N ratio, but I can't resist passing on this
(unattributed) quote I saw in someone's .sig:
Well, I thought it was amusing. :-)

@_date: 1993-09-21 00:17:55
@_author: Doug Merritt 
@_subject: Master Key: A Clipper Story 
Several thoughts come to mind:
- The difference between science fiction and fantasy can be very sharp.
- If wishes were horses then beggars would ride.
- Preaching to the choir is remarkably pointless.
I would also add that genetic algorithms are much too nice to
treat as mysterious magic that can solve any and all problems, but that's
doubtless obvious.

@_date: 1993-09-21 19:46:42
@_author: Doug Merritt 
@_subject: Bidzos on PGP and ITAR verbatim 
Since no one else has mentioned it...the NSA tried to head off the
original publication of public key cryptography, threatening star
chambers, national security directives, and jail terms. This was thwarted
by profs at a hundred colleges across the U.S. immediately assigning
the algorithm as homework to every software class they were teaching
at the same time, thus rather letting the cat out of the bag. The NSA
then backed off, and publications followed.
The material has been a natural part of certain courses ever since,
depending on tastes of the prof.
Part of the above story I experienced directly, since I was at Berkeley
at the time, and other parts I heard from Ralph Merkle, who was either
still there or had just gone to Stanford...I can't recall which.
So if someone is collecting stories about the commonality of teaching
such things, I imagine there must be some hundreds of thousands of
eye witness reports.
I implemented a 512 bit pubkey algorithm for kicks on a Z80 CPM system
around 1982 based on a paper in the open literature that aimed at
non-mathematicians and gave details such as efficient GCD algorithms.
There cannot be any question about how widely known these techniques are.

@_date: 1993-09-22 09:26:54
@_author: Doug Merritt 
@_subject: Why no publication? 
David Colston <0005542837 at mcimail.com> said:
That is not generally required for publication; what counts is whether
the submitted paper follows the norms.
In some instances referees may want to see that the paper has the support
of someone else in the field who is known or at a known institute to
save themselves wasted time, if the paper is novel and would require
nontrivial time to check, but that's easy enough to obtain.
Page fees can often be waived, too, when someone doesn't have a grant
or institution to pay them.

@_date: 1993-09-23 08:32:16
@_author: Doug Merritt 
@_subject: more than spread spectrum 
Phil Karn said:
A little while ago I mentioned an SF Bay Area grass roots internet
connection project. When I talked to Qarin about it almost 1.5 years
ago, I'd thought that she was proposing an actual independent network
beginning in Santa Cruz, which is why I brought it up. However apparently
it is simply a connection to existing sub-Inets. After chasing down some
leads, Tom Jennings' .vacation notice says to contact John Gilmore
regarding SF area Little Garden business while Tom is away.
This naturally leads me to suspect that everyone here except me is
already well acquainted with all this...I've no idea what's going
on with Little Garden myself.
I like Phil's ideas, just for the record. :-)
Anyway, here's what I found on the Santa Cruz project, as long as I've
got it lying around.
P.S. The following does not include some rumors I heard some months ago,
that this project ran into opposition from commercial Internet providers
around here. It also says nothing about Qarin's radio wave internet link
ideas, so maybe that fell through.
-------- Item 2 of 2 ---------------------------
well, the best source of information is to just ask us questions.
The most interesting piece of information that someone who wants
to start something similar should know is that there's a group
like us in the Bay Area (San Francisco, Palo Alto and Mountain View)
called The Little Garden, and they're in fact who we're working
with to get IP connectivity to Santa Cruz. The contact person
for that is Tom Jennings (tomj at wps.com)
If they want to do something substantially south of Mountain View,
it might be cheaper to connect to us, or perhaps we could work
out an arrangement to reduce the digital line charges somehow,
so definitely they should keep in touch with us as well.

@_date: 1993-09-27 18:46:19
@_author: Doug Merritt 
@_subject: Verilog encryption broken 
Bruce R Koball  said:
In looking at the newsgroup in question, it appears that someone
cancelled the controversial article, but it is clear from responses
that the poster was "an33929 at anon.penet.fi".

@_date: 1993-09-28 19:31:38
@_author: Doug Merritt 
@_subject: the public key minefield (fwd) 
[ In the following 100 line post about the origin and philosophy of relevent
  law, I gradually lead in to privacy issues; I discuss U.S. laws
  regarding patents and privacy and such because I am largely unfamiliar
  with such laws in other countries; mea culpa.
  If you don't really care about legal issues, skip this. ]
Svetlana Borisova  said:
I agree that the patent system in the U.S. (and elsewhere, 'though I
know much less about that) has severe faults in implementation. The entire
area of software patents is being grossly mishandled, for instance.
On the other hand, you seem to have a distaste for the entire *theory*
behind it, and I must differ on that point.
The legal philosophy of patents is to encourage invention that will be of
value to society in general. It is *not* directly intended for the benefit of
the person holding the patent, although it often seems to work out that way.
"smb" is referring to that philosophy. The alternative to the general
philosophy is to refuse to grant legal protection to invention.
This is incorrect; ask any patent attorney. In the U.S., anyway. Ask
a Canadian patent attorney...but I'm 99.9% sure that Canada follows precisely
the same legal philosophy.
In the U.S., the legal philosophy is derived from the fundamental meta-
philosophy of its law which evolved out of British common law dating back
to at least the Magna Carta, which is that (loosely) the purpose of law is
for the common good. Every year there are cases in the U.S. where judges make
a "surprising" decision that overturns the apparent letter of the law
in favor of an appeal to the common good of society.
Case law is filled with such things.
There is always a tradeoff against rights of the individual. But the Magna
Carta itself was necessary in order to begin to establish some rights for
individuals against that of society (as represented in that time by the
sovereign). Similarly in the U.S., the Bill of Rights acts to establish
those minimal rights. But whereever the Bill of Rights is not explicit,
on average you can expect courts to rule in favor of the rights of society.
(And sometimes even then...)
There are some cases where this is easy to view as a bad thing, others
where it seems clearly a bad thing. But there is nonetheless many centuries
of tradition behind this approach.
Patent law is merely one more example. As smb said, it offers a monopoly
for an individual, which would usually be considered to be contrary to
the good of society. But it does so in order to foster more invention,
which is considered to be a good for society. It simply appeals to individual
avarice for the sake of the common good, trading off the global long term
good against the short term loss.
In short, it has an unusual lack of short-sightedness to it...in *theory*.
Patent law most certainly and unquestionably is *not* in existence for the
benefit of the inventor, like it or not.
If you are implying that you think that there should never be legal
protection for intellectual property of any sort, as e.g. Stallman has told
me he believes, then we'll just have to agree to disagree.
Stallman believes that such a philosophy is in favor of individual rights,
but historically that philosophy has resulted in a *loss* of individual
rights. But then, Stallman isn't too hot on history of law...
If on the other hand you merely mean that you disagree with the way
that the patent system is working out in a lot of cases currently in
the U.S., then you should be more careful to distinguish the legal
theory from the legal implementation of the theory.
It's very much like the U.S. Bill of Rights in theory versus practice;
the two can be quite different. Cypherpunks is in large part about privacy.
In Roe versus Wade, privacy was (rather amazingly) held to be an implication
of the U.S. Constitution, and as a side effect abortion was judicially
held to be legal.
The right to privacy is not explicitly spelled out in the Constitution nor
Bill of Rights, though, and so most courts, including the U.S. Supreme
Court, have far more often held that there is no automatic right to
privacy. That's why Roe vs. Wade was both amazing at the time, and has
been in such jeopardy since.
California is an interesting case, because its state constitution *does*
guarantee a right to privacy, but that doesn't slow down the right to life
protests, naturally, just as an aside. :-)
Public key cryptography is a mechanism for privacy. There are vast
complications because:
The question is what to do given all of this.
If one works within the system, the answer is to find someone with bucks
for a defense and devise a test case...one intended to lose at every level
until it reaches the Supreme Court, where it is then intended to win and
thus establish ultimate precedent.
Chancy proposition. The other in-system approach is to lobby and to educate.
Also chancy. But worth doing.
P.S. I am not a lawyer, nor do I play one on t.v.

@_date: 1993-09-30 09:01:54
@_author: Doug Merritt 
@_subject: FIDOnet encryption (or lack thereof) 
J. Michael Diehl  said:
According to Mike Godwin:
I haven't kept up on this, so correct me if I'm wrong, but I thought that
BBS's had a choice as to whether to operate as Common Carriers or not,
as long as they were strictly consistent. If they want to be categorized
as Common Carriers then they have to have strict policies of hands-off
privacy, are not liable for the content of messages on their board,
and the ECPA applies. But if they do not guarantee privacy, do not perform
any kind of censorship or other control of message contents, then they
are not Common Carriers and the ECPA does not apply.
Prodigy would be an example of the former, Internet email & news would
be an example of the latter.
Yes? No? Is this stale, ancient, and incorrect info? Or if the concept
is correct, is the problem that they are merely forwarding email from
systems that *are* CC's, and so the ECPA applies to that particular
service, whether or not it applies to the rest of what they do?

@_date: 1993-09-30 18:52:29
@_author: Doug Merritt 
@_subject: Ultimate privacy/security 
The following starts out in a very philosophical way, but that's only to
explain the thinking behind an algorithm I've been developing. The algorithm
is nonetheless extremely unorthodox, so if you've no taste for that sort
of thing, skip this.
On the other hand, the following long term thinking is what lead me to
join cypherpunks, so it's not just a passing fancy.
* 1. Pitfalls of every previous scheme:
It would be nice to find ultimate measures of security and privacy,
but there always seems to be a hole somewhere. The point of passwords
and encryption keys is to verify identity, but of course they can be
stolen/intercepted/etc. If you have a highly secure 10K bit password,
preferably generated by an analog hardware random number generator,
you'll have to store it somewhere, e.g. in your wristwatch, and *that*
could be stolen or taken by search warrant.
Similar comments apply to favorite science fictional devices that are
now becoming possible or even commercially available, such as retinal
pattern checkers and prompted voice signatures...e.g. it's quite possible
to record someone's voice print (a staple of movies by now), or in
principle even synthesize their voice uttering a brand new sentence
in response to a challenge.  DNA authentication is *almost* possible in
real time (give it a few years), but pretty much the same problem there:
someone could easily steal a few of your skin cells.
So is there any ultimate method of identity authentication? I thought of
one, but it is philosophically unsettling, not to mention problematic
in implementation.
* 2. Getting to the roots of the issue:
Why do we care about authenticating identity? It comes down to a matter
of trust. Different people have different goals. We can't trust everyone
else to share our goals; they may be malevolent from our point of view.
We trust ourselves, in the sense of security issues, but to be on the
safe side, we trust no one else. Therefore our identity is the issue.
The problem is that passwords, voice prints, retinal prints, handwriting
signatures, DNA signatures...none of these things actually guarantee
identity. All of those can potentially be usurped by those who do not
share our personal goals.
And that points to a solution:
* 3. Goals as the definition of identity:
If we had a method of authentication that assured that the person in
question shared our personal goals, then we would have the ultimate
security/privacy scheme. If we cloned our mental selves, then that
second occurrence of our own minds would be as trustworthy to us
as *we* are to ourselves. So a hypothetical authentication scheme that
managed to somehow authenticate the (relevant) goals of the person being
tested as being identical to our own would assure us that the person
may or may not be us, but is nonetheless trustworthy.
In fact, such a scheme would have the unusual safety factor that it
would protect against we ourselves having a change of heart and "going
over to the enemy."
Sounds good. Also sounds impossible. Maybe. However, I do have an
algorithm in mind that *partially* satisfies the above criteria. In its
current form, it is susceptible to forgery...what's to prevent bad guys
from pretending to philosophies and goals that they don't truly believe
This is essentially the same weakness as all previous schemes, so there
would be no advancement in that sense (without some further strengthening
of the scheme, if possible). But at least now we're operating on the
absolute fundamentally direct level, where other schemes are indirect.
Is this a strength or a weakness? (And what if the forgery-hole were
plugged somehow?)
I don't have an answer for that yet. I'm working on it, but it may be
an insoluble problem...or maybe not, we shall see. Meanwhile, aside
from the details of the algorithm, I'm interested in hearing people's
thoughts about the strengths and weaknesses of this general approach
as opposed to other authentication philosophies.
Getting feedback about this is why I joined this list, but I've been
a bit shy about bringing up such an unorthodox approach...not to mention
learning what people here are like, and learning from the example of
Tim, who consistently teaches me by being simultaneously insightful
and supportive of people here. That is an approach that I long to emulate...
thank you for the example.
P.S. Some of you high powered people out there will shoot the above full
of holes, which is fine, that is helpful in itself; others *might* find
some material to use in their professional research. Also fine, if that
happens, but please mention my name if it leads to anything. I rarely
manage to take things to the point of publication, so an acknowledgement
here and there is gratifying. Thanks. :-)

@_date: 1993-09-30 18:57:08
@_author: Doug Merritt 
@_subject: POISON PILL 
Actually, you'd be surprised what is recoverable in the aftermath of
an explosion. Bombs truly are no guarantee of unrecoverability of data,
at least not simple things like dynamite and pipe bombs.
Furthermore, pipe bombs throw shrapnel, and as such are anti-personnel
devices. The goal was to destroy data, not FBI agents. Booby traps that
take lives are considered in court as 1st degree murder.
There are more elegant approaches.
        Doug

@_date: 1993-09-30 19:07:07
@_author: Doug Merritt 
@_subject: steganography & fidonet 
In regard to things like Fidonet sysops noting forbidden encryption
pasing through their systems:
Encrypted messages could be encoded in a final pass as English sentences.
A 2000 line C program with a 200,000 word dictionary could encode and
decode sentences that were roughly grammatical, even if they sounded
really weird on average.
Although I've never heard of this being done, it's pretty obvious in one
sense, so I'm sure it's no surprise to the spooks. It surely wouldn't
be obvious to snoopy Fidonet sysops, though, so it may have its uses.
BTW a more complex program + dictionary could confine the encoded utterances
to topical words and therefore sound even less weird. With enough
sophistication, such an encoding could generally pass muster as
"confusing and poorly worded jargon" to anyone but the most devoted analyst.
I've got enough (or almost enough) sw & dictionaries & word clusters on hand
to implement such a thing, but I've personally no purpose to use it for.

@_date: 1994-02-24 09:52:11
@_author: Doug Merritt 
@_subject: RATINGS: Subject tags 
hughes at ah.com (Eric Hughes) said:
This kind of thing was discussed quite exhaustively in news.future last
year, and one of the spinoffs was an actual software implementation
which is currently available via ftp. I haven't kept up on it so I
don't know whether it can be used for mailing lists at the moment,
or just for "virtual" newsgroups, but in any case I include three
items of info below.
The first is a finger message from last summer that gives an overview,
the second is an equally old alpha-test message from the primary author
that gives more info, and the third is a recent ftp site announcement from
him. Between the three you should get some notion of what strn
is all about and whether it's worthwhile following up on.
If it does not currently support email lists, perhaps the authors could
be persuaded to add such support. There's also a mention of public
key cryptography below which is somewhat interesting.
Since his address changed over the last 6 months, I took the liberty
of changing references from his old to his new address in the older
two items, to avoid confusion.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs
identify and implement methods of dealing with "human bandwidth"
problems of USENET.  Its goal is to allow users to be presented with
the articles they wish to read in the order they wish to read them,
without restricting the posters of articles.  There are currently three
stages planned:
    1. Filtering/Prioritizing
which the user is unlikely to read and present the remaining articles
in the order of their priority (score).  This builds on and enhances
the currently existing mechanisms such as separate hierarchical
newsgroups and KILLfiles.
    2. Suggestion/Addition
process more accurate (accuracy is measured by how well the presented
order matches the user's wishes).  Rating services may replace
moderated newsgroups, where individuals decide which rating services to
follow and what level of quality is acceptable.  Summarizing and
keywording services can also be made available to allow better
selection and prioritizing.  This stage will introduce problems in
distributing the additional information and security (especially since
the reputation of moderators may become much more important).
    3. Alteration/Change
original posting are only a suggestion of what the reader will
eventually view.  While the original text would be propagated
unaltered, various people would propagate their suggested editorial
changes (such as hypertext links, spelling or factual corrections,
moderator's notes, or summaries).  What a reader finally views would
be the combination of the original text with the changes of trusted
editors.  Another possibility at this stage would be the "virtual
newsgroup" which is simply a collection of articles chosen by

@_date: 1994-01-01 20:43:53
@_author: Doug Merritt 
@_subject: Radiation experiments & not trusting gov 
Mike Ingle  said:
I agree with Perry, and even more with what Mike there. Considering
that such *has* been well known for so many years, I was a little
bit startled at the current media reaction to the radiation experiments.
Did they only just wake up or something? Or more likely, it's just
been a few years since the media has had the opportunity to make
a fuss over such things, so now it's "fresh news" again, as if that
category of things had never happened before.
BTW on the subject of how much was known about the long term effects
of low level radiation exposure 4 or 5 decades ago: *LOTS*! Let
us not forget that the nuclear age was not ushered in during WWII;
decades prior to that it was well known that prolonged exposure
to low level radiation could cause hideous cases of cancer. Remember
the radium elixer cases? The luminous watchdial painters? (I think the
latter came up twice; once early on with radium-based pigments, and
again later with tritium.) What did Madame Curie die of? Even Roentgen
got cancer from x-raying his hand so much.
That's a bit of a digression, but people seem to forget, so there it
is for the record.
True enough, but that doesn't really explain why the participating
*physicians* did it. Probably 50% callousness and 50% willful ignorance,
I would guess.
I've been quiet here the last few months because that's what I mainly
do. :-)

@_date: 1994-01-11 09:15:18
@_author: Doug Merritt 
@_subject: amusing quote 
I found this quote amusing:
I've been keeping out of the Detweiler fray, and wouldn't ordinarily
quote him, but this is unusual...I'm taking him at face value here;
that probably *is* the source of his motivation for all his frothing
at the mouth. Paranoia is entertaining to him.
The quote comes from talk.politics.crypto, where I was grazing the
other day.
Doug Merritt				doug at netcom.com
Professional Wild-eyed Visionary	Member, Crusaders for a Better Tomorrow
Unicode Novis Cypherpunks Gutenberg Wavelets Conlang Logli Alife HC_III
Computational linguistics Fundamental physics Cogsci SF GA VR CASE TLAs

@_date: 1994-01-11 09:37:08
@_author: Doug Merritt 
@_subject: Weak Random Number Generators 
jerry at terminus.dell.com a while ago said he'd made a hardware random
number generator, and offered to send out data generated by it, inviting
people to look for weaknesses.
I followed up on that and found problems with one of the two sets of
data he sent (files a.bin and b.bin, one produced with his hardware and
one with a software RNG, but which is which was not identified). He
apparently is too busy to acknowledge my response, so I thought I'd
post the results I emailed him here, for those of you interested in
weaknesses of RNG's.
The weakness is shown via ascii graphics of the results of the analysis,
which makes it accessible and intuitive.

@_date: 1994-01-24 22:38:30
@_author: Doug Merritt 
@_subject: Randomness of a bit string 
Tim May said:
To expand a bit on Perry's arguments, the bottom line of all this research
is that a claim regarding randomness can only be made *relative* to
a particular system for specifying algorithms.
In that sense, Tim's statement can be regarded to be correct, iff one
assumes that a context (an algorithmic specification system) is not
given. That is a huge qualifier, though, and not one to be taken for
A trivial example of this: pick some constant bitstring of length 32 or less.
Call it K. Now look at the class of algorithms specifiable by the
C code fragment printf("%x", K) --- i.e. print K as a hexadecimal number.
Relative to that particular set of (one) algorithms, that value of K is
trivially nonrandom, in the sense that the probability of of finding
that bitstring produced by that class of algorithm is precisely 1.
Next consider a program that computes an output by multiplying some
input by two. The probability that the output will be K, given any
possible (but unknown) input, is exactly zero if K happens to be odd.
If K is not odd, then the probability depends on the distribution
(randomness) of the inputs.
Proceeding in this fashion, it becomes increasingly clear that the
randomness of the output of an algorithm can only be measured relative
to the properties of the class of algorithms being considered. Randomness
in isolation is meaningless.
The best sources of intuition regarding randomness usually derive from
systems which shift the burden into an existing intuition on a slightly
different subject. For instance, flipping a coin can be regarded as a random
process in an intuitive sense, but only because it appeals to existing
intuitions about equiprobablistic outcomes.
Therefore one sees confused appeals to intuition about randomness, probability,
entropy, or related ideas, in cryptography, quantum mechanics, information
theory, statistical mechanics, philosophy (in regard to free will versus
determinism versus randomness), etc, etc, but given Chaitin/Kolmogorov/et al,
no intuition from any such subject should be taken at face value.
There's more, but I'll pause to allow flames. :-)
