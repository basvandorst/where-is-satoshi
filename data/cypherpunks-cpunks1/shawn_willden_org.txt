
@_date: 2012-04-03 06:15:44
@_author: Shawn Willden 
@_subject: [volunteergrid2-l] Introduction 
Hi Aaron,
Yeah, sorry, we've had a half-dozen people join the list and express
interest in hosting their own nodes in the last few days, and we haven't
really figured out how to deal with rapid growth.  The process is that
someone already in the grid has to create an account for you on the wiki so
you can get access to the classified info.
Give us a little bit to figure out how to handle this sudden 25% growth.

@_date: 2012-03-28 09:00:56
@_author: Shawn Willden 
@_subject: [tahoe-dev] erasure coding makes files more fragile, not less 
Although your provocative statement tries so hard to be provocative it
fails to be true, there's clearly a kernel of truth there -- and there's no
argument at all that Tahoe needs better monitoring and more transparency.
The arguments make are the basis for the approach I (successfully) pushed
when we started the VG2 grid:  We demand high uptime from individual
servers because the math of erasure coding works against you when the
individual nodes are unreliable, and we ban co-located servers and prefer
to minimize the number of servers owned and administered by a single person
in order to ensure greater independence.
How has that worked out?  Well, it's definitely constrained the growth rate
of the grid.  We're two years in and still haven't reached 20 nodes.  And
although our nodes have relatively high reliability, I'm not sure we've
actually reached the 95% uptime target -- my node, for example, was down
for over a month while I moved, and we recently had a couple of outages
caused by security breaches.
However, we do now have 15 solid, high-capacity, relatively available (90%,
at least) nodes that are widely dispersed geographically (one in Russia,
six in four countries in Europe, seven in six states in the US; not sure
about the other).  So it's pretty good -- though we do need more nodes.
I can see two things that would make it an order of magnitude better:
 monitoring and dynamic adjustment of erasure-coding parameters.
Monitoring is needed both to identify cases where file repairs need to be
done before they become problematic and to provide the node reliability
data required to dynamically determine erasure coding parameters.
Dynamic calculation of erasure coding parameters is necessary both to
improve transparency and to provide more reliability.  The simple 3-of-7
(shares.total is meaningless; shares.happy is what matters) default
parameters do not automatically provide high reliability, even if server
failure is independent (and the direct relationship between individual
server reliability and K/N is meaningless; it's more complicated than that).
The only way erasure coding parameters can be appropriately selected is by
doing some calculations based on knowledge of the size of the available
storage nodes and their individual reliabilities.  Since these factors
change over time, therefore, the only way to know what the parameters
should be at the moment of upload is calculate them dynamically.
 Specifically, N/H should be set to the number of storage nodes currently
accepting shares and K should be computed to meet a user-specified per-file
reliability probability over a user-specified timeframe (the repair
Not only would this approach make it easier for users to specify their
reliability goals (at the expense of less-predictable expansion), it would
also make Tahoe inherently more robust, particularly if it actually
observed and measured individual node reliabilities over time, with
conservative initial assumptions.  It would likely reduce failure-to-upload
errors, because rather than just giving up when there aren't "enough"
storage nodes available, it would just increase redundancy.  At the same
time, it would be able to properly fail uploads when it is simply
impossible to meet the desired reliability goals.
It would also simplify repair and monitoring, at least from a conceptual
perspective.  The goal of a "reliability monitor" would be to check to see
if, under current estimates of reliability of the nodes holding a file's
shares, if that file's estimated reliability meets the stated user
requirement (assuming independence of node failures -- interdependence
actually could also be easily factored into the calculations, but
configuration would be a bear and it would require lots of ad-hoc estimates
of hard-to-measure probabilities).  It wouldn't even be difficult to
include path-based considerations in reliability estimates.
The biggest downside of this approach, I think, would be that it would
still be hard to understand how the specified reliability relates to the *
actual* file reliability, because it would be neither an upper bound nor a
lower bound but an estimate with unknown deviation.

@_date: 2012-03-30 17:33:58
@_author: Shawn Willden 
@_subject: [tahoe-dev] erasure coding makes files more fragile, not less 
(Sorry I'm slow to respond -- I'm vacationing with my family and often have
better things to do than read email :-) ).
Volunteer Grid 2 is a Tahoe grid composed of volunteers all over the world.
Learning from some problems that the first volunteer grid had, I suggested
to early members that VG2 establish some clear and somewhat restrictive
policies in order to ensure that the grid was useful for system backups.
Two specific backup-driven requirements we had were high
reliability/availability and relatively high capacity.  To that end, we
established a 95% nominal up time requirement and a 500 GB minimum node
capacity requirement.  We also avoid co-located nodes and disallow usage of
more than min(storage_provided, 1 TB).  The limit on usage is to avoid
having one user deploy, say, 10 TB and then try to consume that much from
the grid, swamping the rest of the servers.
 And
I'm surprised.   It was definitely announced here when it was created, and
discussed occasionally since.
Total storage capacity, as reported by the stats gatherer, is around 14 TB.
 That's disk used (~6 TB) plus disk available (~8 TB).  As near as I can
tell by eyeballing the graph and summing my estimates is that consumption
grows by about 40 GB per day.  We have a helper but it's lightly used.
 Only one introducer.  The node on the slowest network connection has about
1 Mbps of bandwidth, two or three nodes are on gigabit links, most are 6-50
Mbps, IIRC.  Hardware is similarly varied, with the low end being a small
NAS box, the high end being some fairly powerful servers in data centers,
and everything in between including some virtual servers.
Upload performance, as measured from my machine (which has a 50 Mbps up,
100 Mbps down connection), averages about about 300 KBps, before erasure
coding, so with my settings I get around 100 KBps net upload rate.  I
haven't done any download tests recently, but in the past they've been
approximately the same as upload speeds, but without the erasure coding
