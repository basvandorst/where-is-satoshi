
@_date: 2003-04-25 13:58:06
@_author: Hallam-Baker, Phillip 
@_subject: [Asrg] A New Plan for No Spam / Velocity Indicator 
legal and posted to the VeriSign Web Site. It is in PDF (sorry no HTML or
plaintext version).
I would draw folks attention to the Velocity Indicator described in the
Authentication section. This provides an overview of the trusted hardware
scheme I devised a few months back. While this is a proprietary technology
which we have applied for patent protection on and intend to enforce those
rights the scheme requires trusted hardware as a precondition and so the
license costs would fall on the harware vendor, not the software developer
so the patent would not be a bar to open source implementations (unless
there is a GNU CPU planned). This post does not waive the rights VeriSign
has to that technology, etc.
I will be publishing a fuller paper on the scheme in the large at a later
date. However here is a brief summary:
The trusted hardware base can be anything that is manufactured in controlled
conditions. It could be a palladium class PC, but it could equally be simply
a trusted bios, or a PDA feature, or it need not be a PC at all, it could be
a peripheral such as a smartcard or even a cable modem, nat box, anything
you like.
In the simplest version of the scheme a private key, a certificate and the
current time are loaded into the TCB during manufacture.
To create a message the client asks the TCB to provide an authenticator
token bound to the message in the usual fashion. This carries the 'velocity
indicator' as an authenticated attribute.
Each time a signature is created the velocity indicator is updated to
reflect the current rate of signing (you could also have a count of the
total signatures over the lifetime of the message). This could be the
signatures in the past hour and the past day (say).
When a recipient receives a message the velocity indicator and signature are
checked. The probability that a message is spam is low if BOTH the signature
binds to the specific delivery of the message to the user (i.e. has a valid
There are a couple of possible tweaks to protect anonymity. For example it
is not necessary that the signature be bound to a particular key. You could
use the key installed during manufacture to request new keys. You can even
partition the box so you have separate counts for different signers (this
could be appropriate for a bulk emailer box).
If someone takes a box apart and extracts a key we revoke it - not such a
big issue as you might think, the gaming studies we have done show that that
would be a very poor strategy for an attacker.
In essence what we have done is to reinvent the 'sender-pays' concept -
hence my previous arguments against handing over actual cash.
It is not necessary to have the expense of a transfer system with all the
excessive mechanism to prevent fraud that would entail. All that is
necessary is the scarcity property of money which we simulate in an entirely
scalable fashion. Unlike hashcash and the like this scheme is not vulnerable
to moore's law or assumptions of computational cost.
The one big drawback is that is does depend on replacing the hardware of the
Internet. This is not actually as big a deal as it sounds since it need not
be the PC, it could be a cable modem or a NAT box or even a dongle.
We could even sell/rent a box to an ISP that would be built on trusted
hardware that would sign all the emails going through their mail servers
with the velocity indicator being recorded on a per IP address basis. Bulk
mailers that send out mailing lists etc have to be dealt with differently
but they cannot conceal the fact they are generating large numbers of
There is also a layer to defend against certain obvious abuses and such but
I will describe those separately since they don't depend on the trusted
hardware. Here I have to untangle an issue that came up with the Borderware
people who have had similar ideas to some we came up with but I could not
talk about at the time.
Asrg mailing list
Asrg at ietf.org
--- end forwarded text
--- end forwarded text

@_date: 2003-11-21 16:20:34
@_author: Hallam-Baker, Phillip 
@_subject: [Asrg] Re: [Politech] Congress finally poised to vote on 
We need to consider the technical workings of the do-not-spam list and the
requirements that we would like the FTC to meet.
I propose as a minimum:
1) Allow individual subscribers to list their email addresses with the
2) Permit mail sender to quickly determine whether a given email is on the
3) Be distributable in a form that does not permit use as a mailing list.
4) Permit the storage of attributes in association with each listing,
minimally the date of subscription.
In addition we might add:
5) Allow domain name owners to list their domains.
6) Provide for authentication of listing requests
These requirements can be met using completely generic and to my knowledge
unencumbered technology. For the purposes of avoiding patent encumberabces I
disclose the following - I published note on the basic idea of using a one
way hash to conceal an email address on a do not spam list in 1995, I also
implemented the scheme at that time. The idea is not entirely novel, hash
databases have been used for at least twenty years, there may also be
similar ideas in the cryptography litterature.
My proposal would be to use a message authentication function such as
HMAC-SHA1 with a  key such as SHA1 ("FTC Do Not Spam List") to create a
unique digest function for the purpose. There is a security consideration
here, use of a digest such as SHA1(email) might lead to chosen protocol
To add an individual email address "alice at example.com" to the list we
calculate HMAC ("alice at example.com") to create the key. A domain may be
represented by the string "example.com".
To determine whether the address "bob at example.com" is on the list it is
necessary to test for both the specific email address and the domain.
[This can be made to meet arbitrarily complex requirements]
The list is distributed as a set of key/value pairs. Sorting the list
according to the key values allows rapid lookups by means of binary search,
or since the hash function is guaranteed homogenous using ranged search
using the hash value as an estimator for the index position. It is not
necessary to distribute the list sorted.
There are also a few tricks that can be used to reduce the usefulness of
such a list for address validation.
This same concept can be used to conceal the filter terms used in

@_date: 2003-11-21 20:25:24
@_author: Hallam-Baker, Phillip 
@_subject: [Asrg] Re: [Politech] Congress finally poised to vote on 
Yeah, Yeah dictionary attacks...
The key is that the search space is actually thinly populated enough to make
dictionary attack hard. Most usernames are 6 characters or more, many
include numbers, that is about 26^6 worth of search space per domain. Of
course this is not evenly populated, but the odd thing is that the usernames
turn out to be more random than the average password. This is because random
is not unguessable. Many usernames are surnames, many are compounds of
initial plus surname, only a relative handfull are commonly used names and
those tend to get grabbed fast. so you have a pretty big search space,
millions of possibilities and that for each one of fifty million domains. The same does not hold for do-not-call lists. The problem there is that
something like 80% of the numbers available at active exchanges are already
allocated. Most of the stock of unused numbers are on exchanges that have
not yet been allocated. Since something like 30% of subscribers sign up for
do not call the result is that dictonary attacks are easy.
Also we add out of service addresses that get spammed anyway to the list. So
the list is not an accurate way to find out if an address is in service or
not. Alan knows quite a few addresses that get spammed that are invalid.

@_date: 2003-11-25 06:02:11
@_author: Hallam-Baker, Phillip 
@_subject: [Asrg] Re: [Politech] Congress finally poised to vote on 
DNSSEC is not happening, blame Randy Bush and the IESG for refusing the working group consensus and imposing their own
idea that cannot be deployed. An experimental protocol that increases the volume of data in the .com zone by an order of magnitude (read Gbs of data) is simply unacceptable.
We do not need DNSSEC, we just need a notice in the DNS.
It would be a relatively easy task to walk the .com zone
and dump out a list of all the zones which contain a 'do not spam' TXT property record.
This has the secondary advantage that it is not necessary to actualy consult the list, the authoritative information is in DNS.
I do not expect that to be a problem, that would be a
problem for the contractor. Limit the number of direct
registrations from a particular IP address within a given
time interval.
It is likely to result in the cost of the system being considerably more than the cost of a couple of mid range
servers and some software. This is not a new phenomena.

@_date: 2003-11-27 07:15:56
@_author: Hallam-Baker, Phillip 
@_subject: [Asrg] Re: [Politech] Congress finally poised to vote on 
It is a sad story. Politics and the magic circle. If people are wondering
why the major industry players have abandoned the IETF read on. This is only
one example of the type, other companies have similar issues.
When VeriSign bought Network Solutions one of the main opportunities we saw
was to deploy DNSSEC. There is a limit to what you can achieve in the
context of DNS, anyone can get a domain name without providing
authentication so proving that someone is the legitimate holder of
example.com does not mean you want to give them your credit card number. On
the other hand it would be quite feasible to deploy a class 1 level
assurance system with low cost and ubiquitous coverage.
The problem with the DNSSEC specification is the NXT record that links from
one signed zone to the next. In the original specification you have to
create a link record for every single domain in the zone. This causes the
amount of data in the zone to increase enormously.
This is fine if you have a typical zone with a few hundred or thousand
entries. It is a completely different matter if you are running the dotCOM
zone and you have several Gb of zone data already, a contract that specifies
a very highl level of reliability and a constant series of DDoS and other
attacks going on (about 1000 penetration attempts per day).
There is no way that the people with responsibility for running the dotCOM
zone are going to deploy a system that has such an immediate effect on
operations. The amount of data expands by an order of magnitude.
So we proposed a fix. The original security review was performed by myself
and Warwick Ford. Instead of linking between every record you only link from
one secured zone to the next. This was called 'optin'. This has exactly the
same security as the original proposal but the impact on deployment is much
less. The cost of deployment scales with the number of people using DNSSEC.
The only change in the security is that with OPTIN there is a diferent way
that an attacker can perform an insertion attack, that is causing someone to
believe a zone is registered when it is not. The attack is not very
plausible and at the end of the day the only impact is that we are out the
six bucks for the registration. Anyone can insert domains into dotCOM, just
see a registrar.
The objection to the idea was that this is a VeriSign problem and the WG had
zero responsibility for creating a specification that was deployable by the
operators of large zones which should not exist anyway.
There was also a claim that there was a personality issue, that if
proponents of OPTIN had adopted the correct position as a supplicant that
their petition might have been considered more favorably. The evidence is
against this, every time the go with the flow strategy was attempted the DNS
people would call me up six months later and say 'we have been screwed
This was understood by virtually everyone in the DNSSEC working group. The
chair disagreed. It was at this point that I discovered that the IETF is not
open and not inclusive. Every time the working group agreed on OPTIN the
specification would be taken on a detour. The first time for consultation in
a closed committee called the DNS Directorate. To cut a long story short the
plan was filibustered for three years and then after finally comming to last
call. After passing last call without objection the chair scheduled two
further last calls before we finally came to a result where a clear majority
of the group were in favor, four fifths were either in favor or willing to
allow it to go forward and two individuals were opposed.
So the chair used his perogative to impose his 'consensus' on the group.
The result is that OPTIN is on the experimental track, not a proposed
standard as the clear consensus of the group was that it should be. This in
turn means that it is far more difficult to persuade ICANN to allow
deployment of the specification with its experimental status.
The IETF was designed the way it is to allow a small clique to hold power
while pretending to be open and inclusive. All that Nomcon gumpf is really
designed to make it impossible for the nominating committee to make more
than a few changes to the IESG each time arround.
The result of this type of behaviour is that the IETF has practically no
influence in the industry. DNSSEC and IPv6 have been 'about to deploy' for
over a decade now. There is still no clue as to how IPSEC works in any
application beyond VPN, which is not what it is designed for. SSL makes a
better remote access VPN protocol than IPSEC, works through NAT boxes
without kludges for a start.
The other industry players have similar stories.
The industry is taking notice of the ideas comming out of this WG. But they
are not very likely to accept a standards process unless it is based on
bi-weekly teleconference calls and all major decisions are subject to vote.
