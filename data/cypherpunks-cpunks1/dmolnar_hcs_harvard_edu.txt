
@_date: 2000-12-10 18:53:42
@_author: dmolnar 
@_subject: A piece of advice?? 
amazon.com is one place. see also for an online copy of the Handbook of Applied Cryptography.
I'm not sure what you mean by "a whole system." Do you mean something like
"how do I take a string of letters and represent it as a number so I can
encrypt it?" Then you want to look at ASCII or Unicode and random padding
like Optimal Asymmetric Encryption Padding (OAEP). I'm not sure what else you mean.

@_date: 2000-12-22 00:47:15
@_author: dmolnar 
@_subject: china-taiwan and limits of state action 
Recently a friend asked me what my opinion was as a "computer guy" about
the China-Taiwan "cyber warfare." At first it seemed that there wasn't
much to say, except maybe to point out that this seems to be a ways away
from Schwartau's info-war. One thing has started to bother me a bit, though. How does mainland China
distinguish an attack by the Taiwanese state from an attack launched by
private Taiwainese citizens? Do they even *care*, since they have such
poor relations with Taiwan anyway?
What happens if Taiwan's government says it wants to normalize relations
with China (and vice versa), but the attacks continue? Will they have to
find and punish their own citizens in order for the normalization to move forward? Where do treaty obligations compel a state to prosecute citizens
for behavior which it may have tacitly encouraged before?
Interestingly enough, an attack where the originator is identified seems
to be more of a problem. At least with an anonymous attack, a state can
plausibly deny that one of its citizens was involved. In fact, you could
see identified attacks on Chinese systems coming to be a form of civil
disobedience if Taiwan were to go this route. (I don't think Taiwan will - I'm just interested in this interplay between
private action and the state's responsibility.)
Suppose Taiwan proves unwilling or unable to stop private citizens from
attacking mainland Chinese systems. Now there seems to be a parallel with
situations where states are considered either supportive of terrorism or
too incompetent to prevent terrorist activity. Israel occupied southern
Lebanon because it didn't see any other way to prevent terrorist activity.
The alleged use of Libya and Sudan as "training grounds" could be viewed
as a kind of jurisdictional arbitrage, and a kind which has been reacted
against violently in the past. Fear of an analogous situation online seems
to be behind the "world cyber-crime treaty" mentioned here recently. Now bringing it closer to home, does that mean opposition to the world
cyber crime treaty could be cast as "support for cyber-terrorism"?

@_date: 2000-12-23 00:50:15
@_author: dmolnar 
@_subject: china-taiwan and limits of state action 
What intrigues me about this conflict is that it seems possible for
ordinary citizens to have the same kind of access to attack that the state
does. So speaking of "the mainlanders" or "Taiwan likes" may be misplaced.
Of course, most private citizens won't be able to do much with it, but
there may be some who will.
I agree with you with respect to the mainland and Taiwanese governments,
though.

@_date: 2000-12-23 05:12:16
@_author: dmolnar 
@_subject: china-taiwan and limits of state action 
I suspected as much. The problem with this is that I saw the "individual
action indistinguishable from state action" quickly and have been having a
hard time thinking past it. I'm sure that the picture is much more
nuanced than what I have...
 There are actually other "cyber-war" examples which come to mind where it
wasn't clear whether an "attack" was the result of a state action or just
some crackers. One such was when NATO's web site was defaced; there was a
quote to the effect of "Now the war is fought on all fronts" which made
the rounds.
The quote is interesting first because it places defacing a web site on
the same level as firing bullets at people. Next because I'm not sure if
it was clear who exactly defaced the site. Recently I've heard that Israel and neighboring Arab countries are going
back and forth. For instance
I suppose the closest the U.S. has had to this was the Cold War. We did
have some kind of MO with the USSR, but we didn't (don't) share the same kind of common heritage that China and Taiwan do. Yes - what seems interesting is that cracking makes offense as
"democratic" as defense. That is, anyone with a weapon can defend their
home and territory. That's what a militia is supposed to be, after all. (of course, given the massive inequality in weapons available to armies
and available to private citizens, the militia may not last long...)
But the local militia usually can't unilaterally launch an attack on some
foreign country. (Well, maybe those on the border; the film "Canadian
Bacon" comes to mind). A minor nitpick - it seems strange to say that we are "powerless" and
then note how we can launch an assault. Maybe it would be better to say
that this gives us a different kind of power or "redefines power."
If we think about it at all. Perhaps you're living in a country where
more people remember other countries exist. :-)
In any case, I find it interesting to see the resistance to the current
proposed cyber-crime treaty
which rests on notions of human rights and so on. Values I agree with. At the same time, this seems to place the signing organizations "against"
the Israelis, Chinese, or others who may find that current informal
arrangements aren't enough. That's why I'm posting here, after all.
Thanks, -David

@_date: 2000-12-23 05:33:17
@_author: dmolnar 
@_subject: have we had any "cyber-terrorist" attacks in the U.S.? 
I can think of two terrorist attacks on U.S. soil without too much
difficulty: the World Trade Center and Oklahoma City. In both cases, computers and "internet" loosely defined played a minor
organizational role. McVeigh used AOL, and the Trade Center bombers had some plans on a floppy disk. Have we had any instance of "cyber terrorism" -- you know, the kind of
awful scenario which most people reading this could cook up without too
much difficulty. or is the question not well formed since if it was a
successful attack, we wouldn't know?

@_date: 2000-12-25 21:14:01
@_author: dmolnar 
@_subject: More half-baked social planning ideas 
They've never read a story which mentions such a thing?

@_date: 2000-12-25 21:50:19
@_author: dmolnar 
@_subject: Dude!  It's wired! 
It's my impression that mature sciences don't have the same kind of
foundational or engineering problems cryptography does. We still see
surprises about what a "definition of security" should be, even in the
public-key setting where people have investigated such things for nearly
20 years. Plus even when we figure that out, we'll still have to deal with
the fact that the models used in theoretical crypto don't deal with some
of the attacks possible in real life -- timing and power analysis come to
mind. As does the van Someren and Shamir trick for finding keys because
they look "too random." To say nothing of the nasty fact that passphrases, and therefore keys
based on them, aren't random at all. Which does not play nice with models
which assume keys are picked randomly. It may be true that this year was a lull in "interesting" cryptographic
research (I don't know if that's quite true), but it doesn't seem to be
because too many problems are solved. Rather, there are lots of open
problems left which no one seems to know how to solve...

@_date: 2000-12-26 01:56:18
@_author: dmolnar 
@_subject: More half-baked social planning ideas 
Oh, right. Maybe the SAT is biased towards people who read. Since I read, that
doesn't seem so bad to me.
-David (exulting in the logic of ... oh, wait)

@_date: 2000-12-26 02:42:47
@_author: dmolnar 
@_subject: That 70's Crypto Show (Re: Dude!  It's wired!) 
I don't think I'd go that far. As far as I'm concerned, elliptic curves
are just another group to do Diffie-Hellman & friends in. What I'd call
the "core" of mathematical crypto is the work that Goldreich, Goldwasser,
Micali, et. al. have been doing over the past fifteen years -- trying to
rough out just what kind of assumptions are necessary and sufficient to
give us the kind of cryptography we want.
That being said, almost none of it works without those pesky one-way
functions. or trapdoor one-way functions. and we have too few examples of
either. That's true, and in some sense it's a good thing - we have some confidence
that these problems are hard because "Euler worked on them." (On the other
hand, Euler didn't have the ability to experiment today's mathematicians
do). In another sense, it's a bad thing, because the number of one-way
functions we have is so small. To say nothing of trapdoor one-way
So I have noticed. (and so I have to caution myself against every day).
Reminds me of the reaction I got when I asked some friends about
doing a term project on mix-nets.
"So, has there been any recent academic work on this?" There's some hope. There was a workshop on "Design Issues in Anonymity and
Unobservability" this past summer which brought people together to talk
about these issues. The Info Hiding Workshops are still going strong.
With luck, this year's IHW may have a paper on reputations in it...
This year's ACM CCS conference had two papers of special interest. The "Hordes" paper, _A protocol for anonymous communication over the
Internet_ by Clay Shields and Brian Neil Levine, gives a definition of
anonymity which seems convincing. Then the paper by Franklin and Durfee on "Distribution Chain Security"
discusses the problems of dealing with contracts in a distribution chain.
They have to balance the rights of buyers, sellers, and various middlemen
- and develop some cute cryptographic tricks to do it. Obfuscated
contracts, zero-knowledge proofs, and special "contract certifiers" make
an appearance. It wouldn't surprise me if this ended up having application
beyond the content distribution network scenario they propose.  > Depressingly enough, we keep finding that the focus *needs* to move back
to simple encryption. Birgit Pfitzmann published a paper in the 1980s on
"How To Break the Direct-RSA Implementation of MIXes." Today, nearly
fifteen years later, we still don't know "really" what we need from an encryption system for MIXes; David Hopwood has some good thoughts,
but we're not done yet. On the other hand, we can oppose this to the fact that we have a bunch of
remailers, and they seem to work. They may be unreliable, but no one seems
to have used padding flaws to break a remailer, as far as we know.  > (And, as I have been saying for close to 10 years, the
insurance This may have to wait until liability issues in general for software are
straightened out, won't it? More than that, if the "tragedy of the commons" really happens for
Gnutella and Napster and friends, then people will look for ways to avert
it. Maybe it won't happen ("The Cornucopia of the Commons"), but if
it does, reputation systems might see some sudden interest. Actually, to read this message, it sounds more like it should be part of
the economics department! There are people working on that. Joan
Feigenbaum came to speak at Harvard last spring on her recent work on fair
pricing for multicast trees; this was a case of finding the best algorithm in the face of an "adversary" model specified by economic considerations.
Noam Nisan has a list of some academic research groups also working on
this at I suppose the next step is to apply this to cypherpunkish

@_date: 2000-12-28 03:56:28
@_author: dmolnar 
@_subject: That 70's Crypto Show (Remailers, science and engineering) 
I'm in the midddle of composing a reply to Tim's message (which is getting
bigger every time I sit down to finish it, ominously enough). One of the
points that has popped into my mind so far is that while we've had
academic crypto research since the 80s, thanks to Rivest, Shamir, Aldeman,
Diffie, Hellman, and others willing to defy the NSA, we have _not_ had a
similar tradition of commercial cryptography - or at least, not a
tradition of companies obtaining money for cryptographic *protocols* as
opposed to ciphers.
It seems to me that it took a long while for people to even recognize that
there was more to cryptography than secrecy. Maybe it happened quickly in
academia, but it doesn't seem to have filtered out quickly (and then
there's still the chilling effect from export controls). This is one of
the reasons why the early Cypherpunk work is so damn important -- it
showed the amazing, powerful things you can do given cryptography and a
little cleverness, and it did so to a (comparatively) wide audience!
Even after "everyone" knows that you can do, say, cryptographic voting,
there's still the question of "who's going to pay for it?"
That question seems to have found a partial answer with the
Internet/Web/"e-commerce" frenzy. The thing is, that is *new*, only 4 or 5
years old. Before, you could go out and say "I want to go commercialize
neat protocol X," and good luck to you...today, you might get funding.
Until you get that funding, you can't start the engineering work that's
required to take a protocol from the "cool CRYPTO paper" stage to the
"real world product." Before Tim jumps on me, yes, I know there were early electronic markets,
and yes, electronic trading was around before the Web. Yes, these could
have been viable markets for digital cash, fair exchange protocols,
whatever. Even electronic voting could and did get started earlier
(though not using cryptographic techniques AFAIK) I do not dispute
this! It simply seems to me that the climate today has the possibility of
demand for such protocols (and more) on a wider scale than previously.
Maybe it will force smart people to move the mix from the hand-waving
level to something highly precise. Insh'allah. The 2nd and 3rd are, via Springer-Verlag LINK service. Tables of contents
are free; you should be able to recover the papers from their authors'
home pages (use Google!). If you can't find something, e-mail me. Page for past proceedings:
Page for IHW 2001:
Unfortunately, the TOC for the first IHW is not online, nor do the papers
seem to be available. You can extract the papers from Petitcolas'
bibliography at and may be able to get some of the papers that way. I note a previous
message from Hal Finney which has some links as well
(I haven't tried them)
I should state up front that the workshops are a little heavy on
watermarking papers, which may not be of too much interest to cypherpunks.
The papers on breaking watermarks, on the other hand, may be of more
interest. :-)
[snip a well-deserved beating]
Well, this is what I get for trying to moderate myself. Everything you say
is correct - of course. I actually agree with you! I mentioned this
because I wanted to avoid playing the part of a "theoretical Cassandra," which is something I do too often. (In fact, if I'm not mistaken, that's
part of what Tim's response about different adversary models attempts to
speak to - the fact that traditional cryptographic models assume a
maximally powerful adversary, while we might want a finer grained
hierarchy of adversaries and their effects...)

@_date: 2000-12-28 04:02:46
@_author: dmolnar 
@_subject: That 70's Crypto Show (Remailers, science and engineering) 
Uh, it just occurs to me that I may have misread you. The Design Issues in Anonymity and Unobservability is currently being
turned into Springer-Verlag LNCS 2009. So the proceedings aren't online as
a whole yet (indeed, we just submitted our final final draft two weeks
ago). You can find a list of papers at our paper is at and searching for authors' home pages or e-mail may reveal other papers.

@_date: 2000-12-28 20:13:13
@_author: dmolnar 
@_subject: That 70's Crypto Show (Remailers, science and engineering) 
While we're at it, check out this web page
Avi Rubin and Matt Franklin's course on crypto protocols.
First you need to identity a set of common building blocks! I thought
about this briefly a year or two ago. Then realized that many protocols
for, say, "digital cash" do not actually share many components with
each other. Sure, all of them may have a public-key cryptosystem, but
the exact requirements are different...and sometimes a protocol needs
specific properties of a cryptosystem in order to work.
My programming languages professor recently pointed me to a paper
describing a library for doing smart contracts and options in Haskell.
(I'll post the reference later; I'm having trouble finding it). They put
together a library of combinators, which together could be used to write
real contracts. Even have a semantics for this beast. It seems that the
reason they could do that is that the contracts they're looking at
decompose nicely into distinct parts. It's not clear to me how to do that
for crypto protocols. Maybe a place to start would be to go through a bunch of papers on crypto
protocols and analyse the all the "Alice sends to Bob" messages. See what
commonalities pop out.

@_date: 2000-12-29 02:18:10
@_author: dmolnar 
@_subject: That 70's Crypto Show (Remailers, science and engineering) 
So maybe it takes away the incentive for the original Mojo folks. So? That
may actually be a good thing, if it gets the technology spread far and
wide so that other people can produce an absolutely robust Mojo++ which
rides on top of Mojo. Plus it raises the profile of these kinds of
Today's teenager reading about Mojo on slashdot (or wherever) is going to
be tomorrow's data haven architect...
Yes, BUT
I think one of the reasons why a maximally powerful adversary model is so
appealing, however, is that it sidesteps the question of evaluating "value
of what is being sent through remailers." If you can prove security
against a maximally powerful adversary, then you don't have to answer that
question - no matter how much it's worth to the adversary, it won't win.
If you take this tack, then you seem to start worrying about what the
adversary wants -- and as Terry Ritter often points out on
sci.crypt, you don't know much about your adversary. Plus putting a
"value" on what is sent through remailers seems to require that you be
sensitive to the way the system is used after it's designed.
This is *not* to discourage an economic analysis, but to point out a
potential benefit to the "modern" approach. It wouldn't be much of a
benefit, EXCEPT that in encryption and digital signatures, we have
actually been able to achieve security against maximal adversaries (or at
least probabilistic polytime ones assuming some problems are hard).
Hmm. I know of some results on some two-player games which shows that
playing them "optimally" is PSPACE-complete. The two I can think of,
however - Hex and Go - are perfect information games. I'm not sure how
hiding information changes things.
Maybe one way to cast crypto as a game would be to consider protocol
verification. "Here's a state machine. Here's Alice's state. Here's Bob's
state. Can an eavesdropper learn their shared key if he has the following
 > (* A standard assumption--it probably has a name that I have Kerchoff's principle, I think.

@_date: 2000-12-29 02:23:53
@_author: dmolnar 
@_subject: maurers algorithm 
Are you sure you don't mean Maurer's algorithm for generating primes
together with certificates of their primality?
You can find that in the _Handbook of Applied Cryptography_ or on Maurer's publications page
see also Preda Mihailescu's page
I believe that Wei Dai implemented the algorithm; it may be part of
Crypto++. I don't know of a deterministic algorithm by Maurer for generating strong
primes. (Wouldn't you want such an algorithm to be randomized, so as to
obtain new primes?) Are you sure you don't mean a deterministic primality

@_date: 2000-12-29 04:20:44
@_author: dmolnar 
@_subject: your favorite protocols 
Hi, In view of the recent thread ("That 70's Crypto Show") turning into yet
another call for protocol building blocks -- and Wei Dai pointing out very
sensibly the problems involved in turning protocols into building blocks

@_date: 2000-12-29 06:17:07
@_author: dmolnar 
@_subject: your favorite protocols 
Probably because I didn't give the correct title of the paper. It's the
same one I referred to in a previous message
"Distribution Chain Security"
M. Franklin and G. Durfee
ACM CCS 2000
It's actually a not-bad example of how a "standard" crypto component is
taken and then tweaked for use in a particular protocol. The standard
component is a homomorphic commitment scheme designed by Cramer and
Damg*rd and published in 1998. This paper shows how to use it to prove a
series of contracts satisfies certain relations w/o revealing the
contracts - and then adds a method to make the particular relations they
care about more efficient. well, OK, "published in 1998" is not exactly "standard", but still.
Now, you could try to represent this in an object-oriented language by
something like "DurfeeFranklinCommitmentScheme inherits from
CramerDamgardCommitmentScheme inherits from CommitmentScheme" , but I'm
not sure if you could get real reuse this way. Especially since it seems
that a paper can't get published for a cool idea alone - it
needs to have some real crypto in it. So most new papers will have an
(Another example: the "Identity Escrow" paper in Crypto '98 by Kilian and
Petrank. The idea - extend 'key escrow' to identities - is pretty
straightforward. "Anyone on this list" could have come up with that. What
separates the authors from "anyone on this list" is the fact that they
came up with the idea *and* a reasonable and interesting crypto way to do
it, together with a notion of security and a proof that they meet that

@_date: 2000-12-29 16:48:44
@_author: dmolnar 
@_subject: recent paper on composition of protocols 
Hi, In case anyone wants to take a look, Ran Canetti has a 63-page paper on
secure composition of protocols up at eprint.iacr.org . I haven't read
through it yet, but it may be of interest.

@_date: 2000-12-30 04:28:24
@_author: dmolnar 
@_subject: your favorite protocols 
I agree with you that the term is stretched a bit thin. Still, there seem
to be at least two major things which fall under it in the work I've seen
Simon Peyton Jones, Jean-Marc Ebert, and Julian Seward in ICFP 2000 ``Composing Contracts: an Adventure in Financial Engineering.''
(thanks to Norman Ramsey for the reference)
It's unfortunate that the Franklin and Durfee paper focused on 1) as their
raison d'etre for their protocol -- for all the reasons you cited. It's
not clear that 1) will fly, and even if it does, the end user won't be
doing much meaningful negotiation...
Even so, it seems to me that their techniques will have wider application.
Suppose Alice wants to selectively reveal parts of her contact with Bob to
Carol -- maybe she wants to convince Carol that Bob didn't give Alice a
better price than Bob did to Carol in order to win Carol's goodwill. The techniques in the paper seem to  allow Alice to do that without having
to reveal exactly what her price from Bob was (or learn what Carol's price
was, for that matter) -- and have it backed up by a contact signed by Bob. Then again, I don't know how often these kinds of situations come up in
real life...or will come up in the future.  > Where would I go to learn about the difference between contracts and
licenses so as to understand what you mean here? (Besides law school;
that's still an option for me, but not now). Because I'd like to avoid
contributing to this problem if that's possible...
Thanks,

@_date: 2000-12-30 04:58:59
@_author: dmolnar 
@_subject: That 70's Crypto Show (Remailers, science and engineering) 
That's right - that's part of the fact that cryptographic engineering (as
opposed to "cryptographic science") is still in its infancy. This is the
downside of the current approach, which focuses on getting the protocol
right first, and only later considers the "real world." Bruce Schneier had another way of putting it - something along the lines
of "The math is perfect, the hardware is so-so, the software is a mess,
and the people are awful!" (not an exact quote, but I remember it from one
of his DEF CON speeches). That being said, there is some benefit to considering the protocols in an
ideal, polite model - because in the past we haven't even been able to get
security in *that* model. So in some sense this is a case of "publishing
what we can prove." It's only comparatively recently that we've had protocols which we can
prove secure, even in weak models -- the first real
definitions of security from Yao, Goldwasser and Micali, and probably
others weren't until the early to mid 1980s. Truly practical cryptosystems
which meet these definitions of security didn't arrive until the 1990s.
(Some would argue that they still aren't here - Bellare and Rogaway's
Optimal Asymmetric Encryption Padding (OAEP) satisfies a strong definition
of security, but only if you buy the "random oracle assumption.")
Now on the "science" side we can and should extend the model to deal with
more of the real world. You might find the recent paper I posted a link to
by Canetti interesting - he sets out to deal with an asynchronous network
with active adversaries. I didn't see torture included yet, but maybe next
version. Birgit Pfitzmann and Michael Waidner are considering something
called "reactive systems" which may also yield results.
On the engineering side -- well, there's a long way to go. Ross Anderson
has a new book coming out which may help a little bit. The fact remains that I don't think we have enough experience implementing
protocols beyond encryption and signatures. At least not on a wide scale.
Take digital cash and voting protocols as an example. Digital cash has
been implemented and re-implemented several times. It's even had a "live"
test or two. But how many people have managed to buy something tangible
with it? and how does that compare to the amount cleared by credit cards?
Electronic voting seems to be on the upswing - at least with votehere.com
and the recent election debacle hanging over our heads. Still, who has
implemented, tested, and deployed a truly large-scale voting system
based on cryptographic protocols? The one which comes to mind is the MIT
system built on the FOO protocol - and while that *works* (modulo operator
error), that's only a few thousand undergrads. It's at times like this that I wish I knew more about formal verification
of protocols...
 > If the passphrase is in the dictionary, nearly no time at all. Some take
this to mean that now we should write passphrases down, and use the
opportunity to pick long random ones unlikely to be in any dictionary...

@_date: 2000-12-31 02:52:15
@_author: dmolnar 
@_subject: Anarchy Eroded: Project Efnext 
Something I don't see much of on the efxnet page - "why?"
This is in the FAQ:
"EFNext is the name of a project geared towards making IRC a more stable,
    uniform, chat environment."
and they say "introductory document coming soon." I still don't know why
this is happening (I don't hang out on EFnet). What do the efxnet people
give as their reasons for a new IRC network?

@_date: 2000-12-31 02:58:07
@_author: dmolnar 
@_subject: Anarchy Eroded: Project Efnext 
Then it seems imperative to find ways to acheive both convenience and

@_date: 2000-12-31 04:22:30
@_author: dmolnar 
@_subject: That 70's Crypto Show (Re: Dude!  It's wired!) 
Some things come to mind:
1) Efficient realizations of provably secure cryptosystems.
2) 	Commercialization and availability of SSL/TLS
3) 	Differential Cryptanlysis, Linear Cryptanalysis, Block 4) 	Practical and Theoretical advances in Digital Cash
None of these are as fundamental as creating the first real definition of
security for a cryptosystem or as ground-breaking. But they follow up on
those first efforts and make it easier to do what we want with
There have been two major problems, as far as I see, which have held this
up. First, the theory has not always been cooperative. Second, despite the
myriad of amazing special protocols which have appeared over the years,
we have only a few core protocols which people seem to want badly enough
to justify moving from the CRYPTO paper to the engineering stage. You can
point the finger at patents, too, if you like - but they may have helped
as well as hurt (I'll explain why in a bit). When I say "the theory has not always been cooperative," I mean such
things as the fact that zero-knowledge proofs are not zero-knowledge under
parallel composition or in a concurrent setting. That is, you "can't" take a ZKP out of a textbook and stick it in a situation where thousands
of clients are interacting with a server all at once. At best you lose a
security guarantee, at worst you get killed. Composing two different kinds of protocols can have similar problems; one
example of this would be an "anonymous" digital cash scheme which uses
zero-knowledge proofs to prove coins are "well-formed"...and the
withdrawer's identity is included in the ZKP. (Pfitzmann and Waidner, "How
to Break Another 'Provably Secure' Payment Scheme" ..and for fairness I
should mention that there is a rebuttal)
Then there's the fact that while we have a general result which tells us
that "any function can be computed in secure multiparty computation," the
constructions used to establish this result are impractical - it takes
more work to find a particular protocol to do what you actually *want*,
and there's no guarantee that it will be acceptably efficient. This is
changing; people are finding more efficient ways to do "CryptoComputing" (Donald Beaver for one) and we have some cute new toys like
ring-homomorphic commitments (Cramer and Damgard 1998, used in the
"Distribution Chain Security" paper I cited previously), but we don't seem
to be at the point yet where it's straightforward. *Each* new efficient protocol still yields a new paper. (that may be overstating things a bit; I have no idea how many protocol
papers are rejected from conferences)
Ideally we'd have a situation akin to where complexity theory is with
NP-completeness. You can't get new NP-completeness results published any
more because the techniques have become standard things taught in
introductory undergrad courses. If and when we get to that point,
it'll be easier to just build the protocol yourself instead of trying to
look it up somewhere. I mentioned the problem of "only a few protocols that people actually
want" in a separate e-mail. Face it, as cool as the Dining Cryptographers
in the Disco is, who's going to implement it? and who besides cypherpunks
would want to use it?
This is why the commercial research labs are important - they seem to
actually implement and occasionally release their protocols. Bell Labs
released PAK and PAK-X clients; I know that some of their other work has
at least been prototyped. This wouldn't have happened without the
incentive of patents to fund that research and development.
Unfortunately, the flip side is that none of us are ever going to get to
use that material, or at least not without selling our souls in a
licensing deal. :-(
 > I agree -- but remember that the Web, and the "e-commerce" craze that came
with it, has only been in widespread play since 1995 or 1996. That means
we've only had 5 or 6 years in which nearly any idea with "crypto"
associated with it could get funded. There's also another point - it's supposed to be an "easier" model to deal
with than the one you outline below. The famous question of
Goldwasser and Micali is Their answer is twofold. First, "no." Second, *it doesn't matter*, since
we can The thing to notice, however, is that if you can show that the last bit of
the plaintext is _all_ the cryptosytem leaks, AND you can prove that your
application doesn't need the last bit of the plaintext, then you don't
need to go through these steps. The system is secure - for you. But figuring out what exactly you need is tough. The idea is that by
considering a maximally powerful adversary, you can avoid having to
analyse every protocol to see if it needs the last bit to be secure or not. The point Greg Broiles raises later is that as powerful as this adversary
is, it is not "powerful enough" - for instance, it doesn't torture people.
Part of that seems to be a split between the cryptographic "science" of
protocol design and the "engineering" of protocol implementation.
Without passing judgement on whether this split is a Good Thing or not,
I'll just note that the engineering is in its infancy compared to even
ordinary software engineering.
 > Yes - even today, most people seem to consider models with a threshold of
failure. If the adversary has less than n/2 of the nodes, you're safe. If
it has more, you're toast. I can't think of a model which allows gradual
failure or probabilistic failure off the top of my head.
Note that each of these speaks to different problems facing a remailer
network. The different countries is for jurisdictional arbitrage and so
against a geographically local adversary. The automatic pilot works
against an adversary which is observing users and trying to see which are
operators. Self-mailing nodes are tied up with traffic analysis. The
middleman only nodes seem to be tied up with the abuse problem...and so
on. All of which are separate attacks to be pulled out and inspected, and
then those techniques can be evaluated...
 > Crypto does encompass the idea of a "work factor," of course.
Usually By assuming the worst about all of them - you turn it into a homogenous
mix by assuming the worst. Pow - no more heterogenous problem, but with
the drawbacks we all know... Pulling back from that is difficult. First, it may be hard to model, especially for those of us without
economics background. Second, it runs counter to the way "things are
done"(but you knew that). An aside -- from what I've seen, mostly the symmetric cipher people deal
with "work factor" in MIPS-years. When you move to the public-key and
protocol side of things, it's back to the "polynomial-time adversary."
This didn't change until Bellare and Rogaway started asking about "exact
security" in which they wanted to know what the exact probability of an
adversary to recover information would be if it could invert RSA or some
such in a given amount of time.
Oh, OK. I was thinking about it in terms of the software vendor being
insured against lawsuit in case its software failed. You seem to be
referring to insurance issued to a business against its transaction
failing. I hadn't considered that - it is indeed plausible. Indeed, now that I look at the page with the "list of research groups in
crypto & economics", Varian is on it. There's also L. Jean Camp here at
the Kennedy School - although he's more interested in the intersection of
politics and cryptography, I think. I don't know about anyone else, but it takes me a while to learn,
and an undergraduate education is only so long. I do have a number
of friends in economics (sometimes it seems like all my friends from high
school are in econ...) but we don't talk about this. It's hard for me to
know how to frame these questions to them. Plus I still have the naive
hope that the right protocols will end up provable in the end...

@_date: 2000-11-04 14:20:01
@_author: dmolnar 
@_subject: CDR: Re: Minesweeper and defeating modern encryption technology 
I'm sorry - by the definitions I know, Declan has it closer.
I'm not sure what you're getting at with "any time that can't be
described..." or "something that executes in factorial or exponential
time." As far as I know, NP is a class of *problems*, not a
class of running times or even a class of algorithms.
It doesn't make sense to say "x^2 is in NP", strictly speaking. It doesn't make sense to say "Ford-Fulkerson is in NP", strictly speaking. It makes more sense to say "primality testing" is in NP, if that
refers to the problem of "Given a number n, is n prime?" NP can be defined as the class of problems for which there exist "succint
certificates". That is, given a problem instance, there is some string S
        will convince you beyond doubt that this really is a solution.
By the way, I don't recall if anyone's defined polynomial time yet in this
thread. "Polynomial time" means that a computer will take a number of
"steps" bounded by some polynomial which takes as parameter the length
of the problem instance. Here "steps" mean what you think they mean;
pinning them down precisely requires specifying your model of computation
precisely, which can be time-consuming. NP is the class of all problems for which these "succint certificates"
exist. Once you've found an answer, you can check it easily. But you
are *not* guaranteed anything about finding the answer. Finding the answer
might be "hard." This is one way of thinking about NP. Declan seems to have in mind
an alternative (but equivalent) definition, in which we consider NP as the class of problems solvable by machines which have the magic
ability to always know the "right" way to go at every branch point
of a computation. This is another way to think about it; I personally
prefer the "succint certificate" definition. Then P is the subset of NP -- problems for which finding the certificate
is "easy."  That is, there is a polynomial-time algorithm for finding a
solution to the problem in the form of one of these "succint
certificates." The P vs NP question is then whether P is a proper subset - i.e. is there some problem in NP but not in P?
Factoring in in NP. A succint certificate that you've factored a number
n are its factors, because you can just multiply them together to check.
Finding the factors is another thing entirely...
There are many more isssues here, of course. One such issue is the average
case hardness of a problem. In the case of RSA, we actually have that RSA
is "randomly self-reducible" -- the ability to solve even a small fraction
of instances (say 1%) of all RSA instances would imply the ability to
solve every RSA instance. This gives some evidence that RSA is "uniformly
hard." But this is not known for many other problems in NP, for which
the average case complexity is unclear. More fun on that subject may be found at Russell Impagliazzo's page
in the paper "A Personal View of Average Case Complexity."

@_date: 2000-11-04 17:51:26
@_author: dmolnar 
@_subject: CDR: Re: Minesweeper and defeating modern encryption technology 
[much material elided - apologies for not going through everything precisely at this time. this point particularly struck me.]
I'm sorry. I've been talking about the class "NP" as I've seen it defined
in class and as in books such as Papadimitriou's _Computational
Complexity_ or Garey & Johnson's _Computers and Intractability: A Guide
to NP-Completeness_. This is only partly an appeal to authority; it's
primarily to clarify what I mean when I'm writing and so try to avoid
confusion. For the class NP as defined there (and I can give the definition here in a
separate message if you want, but they do it with more skill and care than
I would), it is *not* the case that "if we can resolve a single NP to P
then that should resolve ALL NP to P."
We *can* identify particular problems for which we can prove
"if we can solve this problem P in polynomial time, then P = NP." Then P is called an "NP-hard" problem. If the problem P is also itself in
NP, then P is called an "NP-complete" problem. We can prove these theorems by giving explicit methods to convert a
solution algorithm for one such problem into a solution algorithm for any
problem in NP. The original problem for which this was shown was formula satisfiability:
"given a boolean formula, does there exist an assignment to all of its
variables which makes the formula evaluate to true?"
If you can solve formula satisfiability in polynomial time, you can solve
any NP problem in polynomial time.
The proof is due to Cook (or independently Levin in the USSR). You can
look it up, or I can try to explain it. If you don't believe it just on my
assertion here, fine. Please say what you need in order to be convinced.  But if you deny that the Cook theorem is proved or take it to mean
something different, please say so.
You seem to believe that an NP-hardness result is a "pretty big leap with
no analysis behind it. A canon of faith rather than proof or fact." Why do
you think this? How does it resolve with the Cook result? Where are
you getting your definition of NP?
Is every problem in NP NP-complete? Now, if P = NP, then every single problem in NP is "NP-hard" trivially.
Because we can solve them ALL in polynomial time. But if P != NP, then
the question "are all problems in NP NP-hard?" becomes interesting. Not every problem in NP is known to be NP-hard, however. For instance,
factoring is not known to be NP-hard.  If factoring is found to be
polynomial time, then P ?= NP is still open. We seem to have different definitions of "the class NP" at work here,
and so we're going to talk past each other until this is resolved. Is it
clear to you what I'm using now? if not, what more do you need?
-David

@_date: 2000-11-04 19:51:26
@_author: dmolnar 
@_subject: CDR: Re: Minesweeper and defeating modern encryption technology 
I was trying to make a distinction between two kinds of problems. I'm sorry if I wasn't clear.
1) The first kind are NP-hard problems. These are problems for which
knowing an algorithm to solve the problem *would* allow you to solve
every problem in NP. These are the kinds of problems I was referring
to in the paragraph I snipped from this message.
2) The second kind are problems which are not known to be in P, which ARE in NP, but which are not known to be NP-hard. Factoring is
an example of one such problem. So is discrete log. Knowing a solution algorithm for one of these types of problems does not
necessarily tell you anything about solutions for any other problem in NP.
These are the kinds of problems I was referring to in "the previous
e-mail." I agree with you that the popular press often ignores this distinction,
and it's annoying as all get out. It's not a single algorithm, per se. It's a combination of the "reduction"
from primality, salesman, etc. to satisfiability + the algorithm for
solving SAT. The reduction can be thought of as "reformatting" the problem
in terms of SAT; it is specific to each problem. Suppose I have an efficient algorithm FINDSAT(F) which takes a boolean
formula F and returns "Y" if it's satisfiable, "N" if it is not. Now my
algorithm for factoring looks rougly like this:
What I would say, according to my understanding of the definitions,
is that there is no succint certificate for unsatisfiability known.
So the problem of recognizing  irresolvable = unsatisfiable formulas is
not a problem in NP. As are you, but the hope is that in mathematical matters we can come to
some kind of agreement.

@_date: 2000-11-05 01:49:55
@_author: dmolnar 
@_subject: CDR: Re: Minesweeper and defeating modern encryption technology 
OK. This seems to be a point where the definitions are a crucial issue.
Let me define the set
You're quite right in noticing that this set is infinite. In particular,
it has all sentences of the form (P or NOT P) OR (Q or NOT Q) OR ...
i.e. all tautologies ORed with themselves. To show SAT is in NP, we only need to show how to construct a succint
certificate that B has a satisfying assignment. We only need to know how to answer "yes" to the question "Is the formula F in SAT?" If a formula has no satisfying assignment, then we don't care about it.
I assume that "has a satisfying assignment" is equivalent to "true" is
equivalent to "provable" in your lexicon. Please let me know if I'm off
base, because that will screw up the following discussion. Then "false" is
equivalent to "not provable" is equivalent to "has no satisfying
assignment." Please let me know if that's off base as well. By the way -- you mentioned Goedel's Theorem at one point. I'm not clear
how that is relevant. It seemed to me that you wanted to invoke the
theorem to guarantee the existence of sentences which are somehow neither
true nor false, or cannot be checked as being true or false.
But for ANY finite Boolean formula, and for ANY assignment, there is
ALWAYS a way to *check* whether the assignment satisfies the formula or
not. Replace the variables with the values from the assignment. Evaluate
the formula according to the usual laws of Boolean algebra. This always
works. So the formula assignment can always be checked and the question
"Is F in SAT?" is therefore in NP.
The problem is finding the satisfying assignment if one exists. Notice
that a Turing Machine which is allowed to take exponential time is capable
of trying EVERY assignment and then finding a correct assignment if one
exists, or rejecting the formula if one does not exist. So determining the
satisfiability of Boolean formulae is always computable in exponential
There does not seem to be a question of "unprovability" or
"uncomputability" involved. Give me a finite Boolean formula (and those
are the only kind we're considering, aren't we?), I give you a computable
algorithm for finding the satisfying assignment: try all the
possibilities. Sure it's exponential-time, but it's computable and
provably so.
Are you invoking Goedel's theorem to say that there exist sentences which
have no satisfying assignment? or have I completely misunderstood what you
are getting at with the term "sentences" ?
Now I'm confused. What are these sentences? Are you taking these sentences
as each representing some problem in the class NP? or are they just
members of all the possible boolean formulae? What's going on? What do you mean by "a logical statement in the P & NP class"? I'm having
trouble here because I tend to regard "NP" as a class of decision
problems only, and you seem to argue for an "NP" which allows for
equivocation between
and	c) the computational complexity of an algorithm
so I am not sure if this is a new extension of "NP" to include
I suspect it is, but I'm not sure. 	
I can't understand this until we clear up the previous point. I'm sorry.
Thanks, -David Molnar

@_date: 2000-11-05 15:44:14
@_author: dmolnar 
@_subject: CDR: Re: Minesweeper and defeating modern encryption technology 
Um, the definition of "nondeterministic Turing machine" implies such a
guarantee. You seem to be thinking of a probabilistic Turing machine - a
machine which can flip coins and use the results in an algorithm.
They are **not** the same thing.

@_date: 2000-11-10 21:34:56
@_author: dmolnar 
@_subject: CDR: Re: Late-postmarked ballots from ZOG-occupied Palestine 
Sometimes you're in school. I am currently in Massachusetts, but I'm registered in Nevada. My absentee
ballot contains numerous NV state questions which I'd like to vote on; a
registration here in MA would not be the same. In my case, I'm a
transplant to Nevada - I don't actually have strong opinions on most of
the state questions (with some specific exceptions). For someone who lived
in one state for most of his or her life, then left for school in another,
however, absentee balloting seems to have a stronger case.
-David Molnar

@_date: 2000-11-27 21:33:25
@_author: dmolnar 
@_subject: CDR: RE: Internet anonymity/pseudonymity meeting invitation 
Pseudonymity as well, isn't it?
Old habits are, unfortunately, hard to break.  "Anarchy is Order" - P.J. Proudhon

@_date: 2000-11-30 21:58:53
@_author: dmolnar 
@_subject: IBM Cries Crypto Wolf, Experts Say  
This sounds vaguely like Charanjit Jutla's preprint It's a chaining mode for block ciphers. Yes, this is annoying. I think it reflects more on IBM marketing than IBM
Research.

@_date: 2000-10-03 09:18:13
@_author: dmolnar 
@_subject: CDR: Re: Anonymous Remailers 
By the way, reading the newsgroup alt.privacy.anon-server offers a view as
to what one anon-remailing "scene" looks like these days. Lots of stories
there about experiences running remailers, dealing with ISPs, complaints
from recipients, and so on.

@_date: 2000-10-03 09:26:23
@_author: dmolnar 
@_subject: CDR: Re: Anonymous Remailers 
People will use your remailer to send spam and death threats. There may
even be people who will use your remailer to send spam and death threats
to themselves, simply because they hate remailers. The recipients will
contact you and your ISP. Repeatedly.
My impression from reading alt.privacy.anon-server is that for many ISPs,
it doesn't take too much of this before the ISP asks the remailer to
leave. It's not a question of legal liability so much as the spam and the
hassle. (An example of how life is lived mainly outside the law, though
maybe in view of it.)
You can implement spam-blocking filters on your remailer...but that's
another can of worms.

@_date: 2000-10-04 02:27:00
@_author: dmolnar 
@_subject: CDR: Re: Anonymous Remailers 
It's a nice first step...it's just that if an adversary knows you
are running a middleman and has control over one of the hosts relaying
mail for your ISP, it may be able to A contact to the ISP follows. You can try to convince your ISP that
"no, this shouldn't happen because I'm running as a middleman," but it's not clear how you could prove that you're under this kind of
attack. The threat here is an adversary who wants to see the remailer
go down, but is unwilling or unable to just mailbomb it. The
adversary succeeds after your ISP gets enough complaints about your
crappy remailer administration to pull the plug.
I'd have to go read the code to figure out whether a plaintext message
could be sent this way, or just a message actually encrypted to another
remailer. Might not be so bad if only encrypted messages go through, but
if an adversary can get plaintext messages through then you seem to have
the same possible exposure as if you were a public remailer. (though in real life, of course, it will be much less because who's
going to do this?)

@_date: 2000-10-04 23:49:29
@_author: dmolnar 
@_subject: CDR: Re: Anonymous Remailers 
Yes, running a middleman is a good idea. Unfortunately if an adversary
knows you're running a middleman, it seems that he can make it seem as
though you're sending spam and so on (or just claim it w/o proof,
depending on ISP). My impression is that there are some people out
there actively going after remailers, but it's a vague impression.

@_date: 2000-10-06 22:03:46
@_author: dmolnar 
@_subject: CDR: Re: Disposable remailers 
This reminds me of something I was looking at this spring. Markus
Jakobsson has two papers on "A Practical Mix" and "Flash Mixing" which
look at mix-nets in a different way than we see in remailers. There,
instead of a message being successively encrypted for a particular
path through a series of remailers, the remailers pass a prepared
encrypted message around and perform a distributed computation on it. At
the end of the computation, the decrypted name of the recipient automagically pops out. These kinds of remailers are not original to Jakobsson - but previous
efforts that I know about are ridiculously inefficient. The number I
remember for one of them is 1600 modexps per message per server.
Jakobsson's "Practical Mix" proposal is more like 160. The "Flash Mixing"
paper investigates ways to use precomputation to get this to 160
I should mention here that Yvo Desmedt and Karou Kurosawa showed in
Eurocrypt 2K that the original "Practical Mix" paper has a flaw -- an
evil node can cause one of the distributed computations to abort without
being caught. They noted that their results didn't extend to the "Flash
Mixing" paper; it's been a back-burner project of mine to look at this
for...well...too long. Anyway, both papers deal with a collection of mix servers fixed in
advance. It seems that disposable remailers would work well with
extensions of these protocols modified to deal with dynamic leave and
joins of servers. Add this to wireless and you have mobile disposable
remailers. Slightly related would be the idea of using commodity computation to do
remailing -- just tell people to "go to this page, download this
applet, become a remailer!" (or have your HD erased, but...)
There are massive issues with trusting new remailer nodes, unfortunately.
Imagine what happens when your adversary decides to show up with
polynomially many of her closest friends. So a further question would be whether we can design a mix protocol
which can Pretty much everything, if you believe some people. The Oxygen project at
MIT has a vision of computation in absolutely everything. Desmedt has an
intriguing article about just what might happen then..

@_date: 2000-10-17 19:51:00
@_author: dmolnar 
@_subject: CDR: Re: why should it be trusted?  
Um, "NP-hard" just means that it's polynomial time reducible to any
problem in NP (or perhaps the other way around, I always get the
directions mixed  up). It is fairly straightforward to show this - you
exhibit a reduction to another problem you already know to be NP-hard. The
"original" such problem is bounded halting : given a TM description M, an
input x, and a polynomial bound p(n), does M halt on input x in
p(length(x)) time?
The famous theorem of Cook consists exactly of a reduction relating
SATISFIABILITY and bounded halting. That's annoying. But once it's done
you can give reductions to SATISFIABILITY instead. See Garey & Johnson's
book for more examples. Put another way, showing a problem is NP-hard doesn't actually show that
it is "hard." It just shows that the problem is no easier than any problem
in the class NP. It could still be the case that P = NP, in which case
there is a rash of suicides in the crypto world...
At the same time, it is believed unlikely that factoring is NP-hard. This
is because "factoring" (the function problem 'find the factors of n'; not
sure exactly how to formalize as a decision problem) is in NP intersect
coNP. If factoring is NP-hard, then NP = coNP.
This is believed to not be the case (but of course not proven). In addition, it's not at all clear how you could solve arbitrary SAT
instances given an oracle for factoring. Try it and see. He has the outline right, if not all the details.

@_date: 2000-10-18 01:20:13
@_author: dmolnar 
@_subject: CDR: Re: why should it be trusted? 
Strictly speaking, this is a conjecture, which is what Jordan was pointing
out in the original post. We do not have any proof for the above
statement, just our long experience trying and failing to solve these
problems.  Different people give different weight to this experience. There really
are people out there who think P = NP. I am NOT one of these. In any case, factoring is not known to be NP-hard, in the technical sense which I'll mention below. In fact, the following "evidence" indicates
that factoring is in some sense easier than general NP-hard problems: the
running time for GNFS is O(e^1.92 (log N)  N^1/3)  for a number with N bits while the running time for the best generic SAT solving algorithm I
know of is in the neighborhood of O(1+something^N) -- there's a better
algorithm known for factoring than for generic SAT. But that's nothing more than suggestive, especially since it doesn't
involve algorithms for other NP-hard problems. Another example of a problem which is believed to be in NP - P and yet
not NP-hard is graph isomorphism. There is an O(n^ln n) algorithm known,
yet the problem is not known to be NP-hard. At the same time, that bound
is just about polynomial...
Yes, that's right, lower bounds *can* be proved. Unfortunately, they tend
to be very *hard* to prove. Especially for general computations. A
superpolynomial lower bound on the work required to solve a problem in NP
would separate P from NP, so I don't think one such is known right now. So AFAIK the question is open. Different people have different attitudes
towards its resolution. This is all very nice, but it runs a high risk of
becoming based solely on feeling and assertion very quickly.
Often you can get a very nice lower bound in a so-called "restricted
model" in which only a few operations are allowed -- the n log n lower
bound for sorting given in many CS algorithm classes is of this type. The
bound is correct, but it says literally nothing about radix sort, bucket
sort, etc. because those sorts use operations not in the model spoken
about by the bound.
The cryptographic analogue might be the so-called "generic group model" --
a model in which all you have is a group's generators, the ability to
compose elements, and the ability to test for identity. You *can* prove
that there is no algorithm in this model which solves discrete log in time
better than about O(2^n/2) (I may be off by a bit).  But for the
particular group Z_p^*, there is a much much better algorithm for finding
discrete logs which takes advantage of that group's special structure
(i.e. GNFS).
This should suggest some of the difficulty in acheiving lower bounds which
we as cryptographers might care about. For more suggestion of such
difficulty, a book on complexity theory like Papadimitriou might be
worth looking at. You want to say "Hence it is in NP - P".
The term "NP-hard" has a technical meaning, namely that the problem can
be "reduced" to all problems in NP. Here "reduced" is another technical
term which in turn needs to be defined carefully. I've screwed up that
definition before and it's too late to look it up, so I regret that I'll
leave it as is unless someone really wants it.  It is known that if NP != P, then there is a hierarchy of decision
problems which are neither in P nor NP-hard; the proof unfortunately takes
the form of a diagonalization style construction on Turing machines and so
doesn't tell us anything about what the natural problems might be. The
result *does* tell us, however, that it is *not* enough for a problem to
have a superpolynomial lower bound on decision for it to be NP-hard --
there's more work which must be done to show NP-hardness.
It's possible that factoring is neither NP-hard nor in P, but still hard
enough to be useful for cryptography. Similarly, it is possible that graph
isomorphism is not NP-hard, yet is so close to P as to be practically
efficient (and not at all useful for cryptography).
(BTW - as an aside, "NP-complete" means that a problem is both in NP and
NP-hard ... a problem may be actually harder than NP, in EXP or
something, and still be NP-hard. or we may not actually _know_ whether
the problem is in NP or not).  In any case, whether or not a problem is NP-hard is sort of irrelevant. As
a general notion, NP-completeness seems disappointing for cryptography.
There are a lot of known NP-complete problems, but few of these seem to be
hard enough on average to build cryptosystems with. Even fewer can be used
for public-key cryptography.  Some more discussion of these points may be
found in Russell Impagliazzo's paper on "A Personal View of Average Case
Complexity," which is on his UCSD page.
If you look at the major problems used for public key cryptosystems, you
see the Diffie Hellman assumption, factoring, and RSA. None of these AFAIK
is known to be NP-hard and no one expects them to be. There was even a
theorem due to Brassard in 1979 which suggested that the problem of
"breaking" public key cryptosystems cannot be NP-hard...but it only
applies to deterministic cryptosystems, so it doesn't say that much.
For more on that, see Oded Goldreich and Shafi Goldwasser "On the
Possibility of Basing Cryptography on the Assumption P \neq NP",
in the theory of cryptography library at UCSD, now eprint.iacr.org.
Note that when they say "cryptography" they mostly mean "public key
cryptography." On the other hand, this notion of _reduction_ , of showing that one
problem is "as hard as" another, has been VERY useful. This is how you can
prove that OAEP is a good padding scheme - you give a reduction between
breaking an OAEP-padded RSA message and breaking RSA directly. You can
prove that there aren't any stupid subtle padding mistakes...
Anyway, that was a lot of silly detail.
 From what I can tell, the point is just this: as Tim pointed out, there
are problems for which the best known algortihms cause growth which dwarfs
any sort of cracking farm we can imagine. If the adversary builds a 1000x
bigger machine, we add 200 bits to the key and he's back at spending 10
million years.  But as Jordan pointed out, no one knows that these are the
best algorithms or useful lower bounds on solving the problems.
Now it becomes a discussion on what you believe the NSA can do or can't
do, the relative smartness of mathematicians inside and outside the NSA,
and all that other stuff. Fine, but it's a discussion which runs the risk
of becoming quasi-theological very quickly...and frankly it's one which
just isn't that interesting the way it is usually run. I think it is more interesting to figure out what kinds of expertise the
NSA might have that the academic sector doesn't, and especially
interesting to find some way of confirming or denying. A way which doesn't
involve "a friend of a friend of a friend with a .mil address stationed in
Saudi Arabia for 4 months during Desert Storm."  Tamper-resistant hardware
expertise has been mentioned here; we had that patent notice a few months
ago about speech transcription for ECHELON; we have SKIPJACK, KEA, and now
SHA-2 to play with; TEMPEST research is now quasi-legendary; there's
likely more fun things to figure out about these guys.

@_date: 2000-10-19 21:10:38
@_author: dmolnar 
@_subject: CDR: Re: Reading list 
I've only had occasion to flip through this in a bookstore. I remember
thinking that it could have used another 2 or 3 re-edits. Also that its
treatment of semantic security and probabilistic encryption was pretty
bad. Must have missed the pro-GAK stuff.
I think one of the books which made me wonder "what the hell" was _The
Frozen Republic_ in the last part -- the author argues that separation of
powers is an outmoded and silly concept, our government can't act fast
enough, and wouldn't we be better off with a British style system for
doing things which didn't have all these checks and balances? With due respect to British readers, I think the RIP act shows one of the
reasons why we would not be better off. Not that our own equivalent
isn't far behind; haven't there been murmurs for a while about making
"the use of cryptography in committing a crime" a separate crime? :-\

@_date: 2000-10-25 02:00:23
@_author: dmolnar 
@_subject: CDR: Re: Re: Re: Gort in granny-shades (was Re: Al Gore goes cypherpunk?) 
This is why the Gnostics had such a good run of it in the first century,
right? At least until they were wiped out...

@_date: 2000-09-05 02:51:08
@_author: dmolnar 
@_subject: CDR: Re: "ChronoCryption" algorithm - $50 reward for spotting a flaw 
ah, and let me guess -- we can tell which of us have extra computing power
in our basement by seeing who can tell the difference and who can't?

@_date: 2000-09-05 15:23:26
@_author: dmolnar 
@_subject: CDR: Re: RC4 source as a literate program 
Does this really work? I can't imagine this working for murder (but on the
other hand, that's  a bad example since it's unreasonable to imagine
murder legal in the USA). Even for something like tax laws or other
complicated regulations this sounds dubious. Well, a lawyer who advised a client that something was legal when in fact
it wasn't might have a problem. Such a statement would help, but more because it would be from an expert
on the law than because of any legal shield. I am not a lawyer, and so I'd
like to have one's opinion before doing anything that could land me in
jail. That kind of thing.
Prohibiting what - publishing cryptography code?
In any case, even if the law is unconstitutional, you may have to go
through several layers of court cases to prove it. c.f. Bernstein. :(

@_date: 2000-09-07 01:21:51
@_author: dmolnar 
@_subject: CDR: Re: StoN, Diffie-Hellman, other junk.. 
The modulus should be rather large -- something like 512 or 1024 bits.
With 64 bits, someone can use Pollard's method to find discrete logs in
roughly 2^32 trials, which is Bad. Taking discrete logs for larger primes
requires a variant of the number field sieve; the largest announced
modulus for which I'm aware of this being done is 300-400 bits, but it
hasn't received as much attention as factoring. I think  has some key length recommendations. You might
also check the April RSA Data Security Bulletin for Bob Silverman's
dispute of their model. The storage problem he mentions is actually worse
for discrete logs; while the vectors involved in the final step of
factoring are 0-1, the vectors in the final stage of discrete log finding
have full-size group elements and are therefore harder to store and
The size of the generator is a different issue. I don't see any reason why
a small size generator would hurt...but I haven't thought about it very
much. Note that you need the factors of p-1 in order to test if
something's a generator, which means you may want to look into Maurer or
Mihailescu's methods for prime generation. (Mihailescu has a paper on
the subject aimed at implementors at  )
It was after my time, but the AP Computer Science curriculum now has a
BigInteger library as its "case study." :-)
A web search turned up which has, among other things, a Pascal header for the Gnu MP library.

@_date: 2000-09-07 02:07:37
@_author: dmolnar 
@_subject: CDR: Re: StoN, Diffie-Hellman, other junk.. 
Erastothenes, I think. I don't know what a sieve of eros is. I think I'd like to try one
sometime. :>
Right - I think you may find that this slows down a bit at the 500-bit
range. Still, there are supposed to be ways to use sieving in conjunction
with random search to speed up prime generation. Once  you have the primitives, Rabin-Miller is straightforward to
implement from the Handbook of Applied Cryptograpy. I was surprised at how
easy it was...
Another nice trick -- compute the product of the first 1000 primes or so.
Take the GCD of this product and a candidate number. Eliminates candidates
with small prime factors and often faster than trial division.  Backdoors are your responsibility with GMP, so no worries, right. :). It
is GPL'd, though, so be careful. Looking forward to it.

@_date: 2000-09-18 14:39:21
@_author: dmolnar 
@_subject: CDR: RE: was: And you thought Nazi agitprop was controversial? 
Dude, do you read _Communications of the ACM_ ? Professional licensing of
"software engineers" is a fact in Texas. Not an issue goes by without
someone piping up about how great it would be if every state followed that
lead. Some discussion and context from licensing of other kinds of engineers is
at An "American Programmer's Association" may be closer than you think.

@_date: 2000-09-18 14:42:55
@_author: dmolnar 
@_subject: CDR: RE: was: And you thought Nazi agitprop was controversial? 
Here's another link on licensing of software engineers, this time from the
it seems that cryptographic/security software, if we ever get the
liability structure whose lack is often pointed out by Schneier ("we don't
have good security because we don't have to"), may be a prime target for
such licensing.

@_date: 2000-09-19 00:11:36
@_author: dmolnar 
@_subject: CDR: RE: was: And you thought Nazi agitprop was controversial? 
Hm. That's true, but it's not in the spirit of what I meant. I was
thinking more along the lines of something tied to legal liability for
defects. That is, the registration/licensing comes about more
"organically" instead of being called into being straight by regulation. Yup, any such list is dangerous - although the pressure may not come from
key escrow per se, but from businesses who become fed up with security
being "not the vendor's problem." As in "show us that all your crypto
engineers and subcontractors are properly licensed." Maybe you can think
of this as touching on reputation management or credential management,
although I expect most Professional Engineer certs are issued to True
Schneier has made the point several times that vendors do not provide
strong security because they generally aren't liable for the consequences.
I tend to agree with him. My worry is what the world will look like after
more people agree with him and then try to "fix" things their way. By the way, I am glad to hear from Choate that licensing is not as
draconian as I thought down in Texas. My apologies for the scare; I
suspect I was reading too much into the ACM reports about "Licensing of
Professional Engineers." Thankfully, the ACM seems to be resisting such moves for now (see second link), but who knows about five years down
the line.
(Bureaucratic inertia is no reason for complacency; I remember reading in
WIRED of 1995 or thereabouts of a "Digital Copyright Working Group"
about to convene and study the Internet "problem." Then nothing. Five years later, the U.S. has the DMCA.)
In fairness, "vendors don't provide security because they don't have to"  seems to be a symptom of a larger issue with liability for software,
especially software sold to us mass market consumers.  I expect markets
exist in which software has to be held to an extremely high standard of
reliability (e.g. Space Shuttle, financial markets, health software,
embedded systems spring to mind). How are liability issues dealt with in
those fields, and how did they come to be that way? would the same thing
happen with crypto and security software? (how do I ask that question better, because it seems too vague now?)

@_date: 2000-09-19 18:36:18
@_author: dmolnar 
@_subject: CDR: Re: Where are the crypt source blackboards? 
do people have code which they want to post? personally, the prohibition wouldn't affect me much. but most of my crypto
source code issues are problems with using OpenSSL (I'm an OpenSSL newbie,
sad to say, and trying to change that). there are other places to look for
help with that...
what I'd expect to be posted here or to coderpunks would be discussions of
the right techniques and so on to use when developing crypto applications. sometimes with code as an illustration. and we do have that, c.f.
discussions on wagner blinding, recipient hiding signatures, and so on. just not so much of it because, well, coming up with this stuff takes a
fair amount of time. at least for me. so it is not as prevalent as the
latest phil/pol musings.

@_date: 2000-09-21 11:19:34
@_author: dmolnar 
@_subject: CDR: Re: email interception 
In the United States, this used to be covered under the 1986 Electronic
Communications Privacy Act. Intercepting e-mail was illegal, but there
was a distinction between "communications in transit" and "stored
communications." This distinction had bearing on the difficulty of
obtaining a search warrant; I don't remember if it had bearing on the
penalty as well. I wouldn't be surprised if this has changed due to more recent
legislation, however, given recent tendencies of the U.S. Congress.

@_date: 2001-04-04 00:55:03
@_author: dmolnar 
@_subject: Seth Finkelstein, reluctant cypherpunk? 
Has anyone ever done a study - a worthwhile, entertaining, informative,
cautionary study - of rhetoric employed by revolutionaries? The "simple
understanding and toleration" here makes me think of other solutions which
have been put forth as "simple"; other principles of "brotherly love"
or perhaps "universal peace" which I associate with radical and reform
texts from the late 1800s and early 1900s (though, alas, without examples
to draw on right now - I want to say Sinclair's _The Jungle_ and Emma
Goldman's essays)
Also the great rhetorical question "WHAT IS TO BE DONE?" comes to mind -
the punch line after a description of the sorry state of the world, often
accompanied implicitly or explicitly with a helpful list of answers. Great
projects set in motion, called to action by this question, hell-bent on
applying the "simple" principles set forth, presumably validated,
previously. We (who?) know how those projects turned out, don't we?
The point is -- I'm not sure that using this kind of rhetoric is helpful
to anyone any more. Independent of what the argument at hand is. thanks,

@_date: 2001-04-15 20:07:35
@_author: dmolnar 
@_subject: chaffing and winnowing 
There was a cypherpunks discussion on "chaffing and winnowing without
overhead" archived at You can also find a paper by Bellare and Boldyreva "On The Security of
Chaffing and Winnowing"
and the "Chaffinch" system which extends the idea to deal with
forced disclosure of keys (RIP act):
I seem to recall a paper with a title along the lines of "A Comment on
Chaffing and Winnowing" in either Financial Cryptography or maybe
Selected Areas in Cryptography, but I can't find it right now.

@_date: 2001-04-19 15:21:14
@_author: dmolnar 
@_subject: text translator babelfish! 
You may be thinking of babelfish. to portugese:
Voc pode pensar de Babelfish.
portugese to english:
You can think of Babelfish.
 seems to be the URL.
Oddly enough, while they have "Russian to English", they don't seem to
have "English to Russian." So can't try to old "Wine is fine but the flesh
is rotten" trick...

@_date: 2001-08-13 19:49:33
@_author: dmolnar 
@_subject: alt.anonymous.messages practice 
I just took a look at the first 10 messages I could pull down from
alt.anonymous.messages using pgp2.6.2 . Eight were encrypted with
symmetric encryption. One was encrypted with keyID 591B0E69. A last one
identified itself as encrypted with public-key crypto, but in a format not
otherwise intelligible to poor 2.6.2 .
Now, keyID 591B0E69 isn't in the keyservers, of course, but it will be
interesting to watch alt.anonymous.messages for the next few days and see
if any other messages encrypted to that key should show up. Well, if I
get around to writing the scripts to watch for it, which I probably won't.
I don't suppose anyone's been gathering data like this in public? In
particular, it'll be interesting to see if 591B0E69 is simply receiving an
initial message (to set up a shared password for conventional encryption)

@_date: 2001-08-21 19:30:38
@_author: dmolnar 
@_subject: Lawyers, Guns, and Money 
Perhaps in evaluating potential programs, it would be helpful to list
people in the policy + technology area who are worth looking into. Along
perhaps with which institutions they studied at? I'll start. I regret that
I'm not familiar with this area, and so I'm sure I'll overlook many
interesting people. I'm also not sure what to do about people with some
policy interests who are primarily cryptographers -- do we include Ron
Rivest because of his work on electronic voting?
By the way, what exactly do you *do* after you graduate from a technology
and policy program?
Every now and then I wonder if I will eventually end up in law school or a
policy + technology program. The thought is alternately exciting and
saddening. Then again, so is the prospect of being a "pure" researcher.
-David Molnar

@_date: 2001-08-22 18:07:32
@_author: dmolnar 
@_subject: Send Law Students, Idealists and Grant Proposals.  Was:  Re: 
The Berkman Center for Internet and Society.
The name always takes me aback, because I automatically think "Alexander
Berkman" whenever I read it. They run online courses, which sometimes
accept Internet participants. See
for one such course on defining "trust."
They also have a newsletter, "The Filter," which is sometimes interesting.
This year they started running a 5-day "Internet Law Program of
Instruction,"  if you happen to have a spare $2500.
(Tangentially, who attends MIT's 6.87s? and what do they do
with it afterwards? I've received solicitations to attend via mail for the
past couple of years; I suspect because I am an ACM SIGSAC member.
If *anyone* could get the material across in 4 days, it would be those two

@_date: 2001-08-25 15:19:49
@_author: dmolnar 
@_subject: Comped scribblers the bane of conferences 
Just off the top of my head:
O'Reilly P2P Conference, Standard Conference Fee: $1595
RSA 2002 Conference:  $995 to $1795, depending on when you register
Overheard at a conference business meeting:
"What about special rates for attendees who are neither business nor
"ARE there any such people?"
Hey, at least DEF CON is still $50 ! although Black Hat is $1195 + $700
per training course.
By the way, in case anyone knows Neal Stephenson, I know a science fiction
conference which would love to invite him.  (Yes, I've read his web page,
yes, it's a lost cause, but writing of luminaries...)
There was that workshop on "Privacy by Design" at the 99 or 00 CFP, wasn't
there? The one report I had from that was not favorable, but it's not a
*bad* idea.
This year, there's a workshop on Privacy Enhancing Technologies being held
immediately prior to CFP in the same building. Maybe this will lead to
more interaction.
(Disclaimer: one of the co-chairs is a co-author of mine. So yeah, this is
thinly disguised plugging for the workshop. No, I don't know how much
it will cost.)
Well, come a few days earlier and sit in comfy chairs for PET, while
you're at it...

@_date: 2001-08-25 21:52:20
@_author: dmolnar 
@_subject: Comped scribblers the bane of conferences 
I suspect I'm guilty of doing this. Not just at SF conferences. (Actually,
I've been to too few SF conferences.)
I'm just learning. I missed out on most of this "growing up,"
first being out of the country and then off in New Hampshire. Something to
fix soon. Especially since you can meet interesting people at cons.
(The con I mentioned, by the way, is  in case anyone's wondering. )
[on the PET 2002 workshop]
I think that all three refer to the same workshop.
I'm not sure I understand this comment, though. Do you think that the
committee members are doing it solely because it's a "corporate task"
which they have been ordered to do? or that they've lost interest in the
research now that it is a "corporate task"  to be on the program
committee? What exactly is the problem with "corporate folks"?
I can't claim to speak for the committee members. From what I know of the
co-chairs, however, they are not doing this simply because it is a
"corporate task." Both of them have been interested in this area for as
long as I've known them. As far as I can tell, their interest is genuine.
Now, it *is* being run as a straight-up academic workshop, with
Springer-Verlag proceedings, refereed papers, and that whole nine yards.
This has certain disadvantages. Long lead times between genesis of an idea
and publication (not to *mention* implementation), for one. Arguably too
much emphasis on theory and citations rather than just "cypherpunks write
code," for another. You can go after it on those grounds (and we can argue
about that for another four or five messages if you want), but that seems
to be distinct from talking about "corporate folks" on the program
committee -- have I missed something?
It's my hope that workshops like this will help attract smart people to
work on the problems in remailers, implementing digital cash, and other
fun Cypherpunkish topics. People who've never even heard of "Cypherpunks,"
and who would otherwise go off and do number theory or something else.
[script and rant skipped]
When my family lived in Saudi Arabia, we had our passports covered with a
sticker which identified which company we were from. No foreigners in the
country without a sponsor. The rant about "affiliations" reminds me very
much of that...

@_date: 2001-08-26 02:51:14
@_author: dmolnar 
@_subject: Comped scribblers the bane of conferences 
That's what targeted publicity is for -- making sure the right people see
the message and show up (and maybe publish something). While I didn't make
it to the 2001 Berkeley workshop, I know that some of the Freenet
developers were there. ZKS was well-represented. I think the Mojo people
were there, though I could be wrong. That's a start.
Then once everyone's there, the rest is a matter of (non)scheduling and
beer ordering. (well, and Kahlua maybe).
So what I should do now, I guess, is contact the Morpheus team and
convince them to come. maybe submit something if they feel like it.

@_date: 2001-08-26 09:55:24
@_author: dmolnar 
@_subject: Comped scribblers the bane of conferences 
Right, I should make this crystal clear.
I do *not* speak for this workshop. I am *not* one of the organizers, and
I'm not on the program committee. I happen to know (some of) the
organizers, but that's about it.

@_date: 2001-12-21 18:01:06
@_author: dmolnar 
@_subject: More on remailers. 
One way around this management issue might be to use a public-key
cryptosystem which supports "key blinding." (Note - A Google search
reveals that this term seems to be used in other places as well, and it
looks like the usage there is not quite consistent with the way it is used
in this message. Caveat lector.)
David Hopwood has done some amazing work on formalizing the concept, which
informally goes something like this:
A public-key cryptosystem supports "blinded keys" if there is an efficient
randomized algorithm which takes a public key PK and outputs a new public
key PK' such that
for this application, we also need something perhaps stronger
This is somewhat like Ross Anderson's "compatible weak keys," except that
it does not require that a new private key be created along with the new
public key.
With blinded keys, the management issue you mention becomes much simpler,
I think.  Instead of managing dozens of public-private key pairs, you have
only one pair (PK,SK). To create a one-use public key for the purpose of
status notification, you create a blinded key PK'. Anything encrypted with
PK' can be decrypted by your SK, but the key unlinkability properties
prevent the remailers from correlating messages based on PK'.
One way to create a key-blinded cryptosystem is to consider Elgamal public
keys as the pair (g, y), where y = g^x for private key x. Then to blind a
public key, pick a random blinding factor b and let the blinded key be
(g^b, y^b). With this, you do not need to keep track of the blinding
factor b; the change in generator to g^b will do that for you.  So you can
get away without keeping any state on your end beyond your private key x.
For more detail, currently the best thing to do is a groups.google.com
search for David Hopwood and "BRH-DHAES." We keep meaning to write this
up, but never get around to it.

@_date: 2001-02-05 01:53:24
@_author: dmolnar 
@_subject: plausible deniability 
You're right - now that I look at it in more depth, I'm not at all sure
that it's a PHD in philosophy. As opposed to just an ordinary doctorate in
some other field. Thanks for pointing this out...
This makes me even more interested to find material which discusses
zero-knowledge proofs from, say, a philosophy of math standpoint. or
anything "interesting" yet outside the usual technical standpoint.
As for your question, I don't know of any e-mail software which offers
plausible deniability. Maybe something to add to gpg?

@_date: 2001-02-05 18:37:53
@_author: dmolnar 
@_subject: anonymity 
Back in high school, we had a story that this had happened the year
before I started there. The details were fuzzy, but variously involved a
chat room comment or a direct threat to president at whitehouse.gov . I
wonder how many kids end up in trouble for this each year...

@_date: 2001-02-07 13:27:45
@_author: dmolnar 
@_subject: fast way to decode RSA encryption 
Yes, but...trying to factor using trial division is not the best currently
known method of factoring. In order to be worth a look, this method should
have a running time better than the general number field sieve.
Rivest also makes this point in his response, and in more detail than I
have here. Unless I've misunderstood what you mean by "trying to factor a
number using multiplication/division" ?

@_date: 2001-01-02 21:14:11
@_author: dmolnar 
@_subject: Anarchy Eroded: Project Efnext 
If memory serves, there is a project underway to do "usenet-on-freenet."

@_date: 2001-01-04 21:14:22
@_author: dmolnar 
@_subject: OceanStore - anonymous and distributed data storage? 
The list of Project People only has one person on "cryptography" - Steve
Weiss. His paper on security issues for a file system on top of OceanStore
can be found at

@_date: 2001-01-12 18:17:06
@_author: dmolnar 
@_subject: Consensus Actions in Cipherspace? 
There are distributed signing protocols which could go partway towards
meeting this goal. For some details, you can look at the IBM project on
"Proactive Security" They've implemented some of the protocols involved; there's a paper in ACM
CCS 5 I think which gives the API for their "Proactive Security Toolkit."
Java 1.1 implementation of distributed DSA signatures. Unfortunately, the
web page, which advertises a download for 8/99, seems to be out of date. Now, if you could get your hands on this, then you could do what you want
by having each voter hold a share of the distributed secret key. The
voters agree amongst themselves however on what orders to sign, and then
execute distributed key signing to sign an order. The robot doesn't do
anything unless it receives a valid signed order.

@_date: 2001-01-13 00:23:14
@_author: dmolnar 
@_subject: Consensus Actions in Cipherspace? 
One way to address this problem is to use secret sharing. Everyone gets
a share. Only a certain threshold need to cooperate to reconstruct.
Everyone's secret counts the same, so in order to deny service you need to
have fewer than threshold non-cooperatives. You never reconstruct the database in one place. Instead, you figure out a
clever way to do a distributed query on the database shares, such that at
the end of the protocol, out pops the result. There are plausibility
results due to Ben-Or, Goldwasser, Goldreich, Wigderson, and others about
this under the name "secure multiparty computation." Briefly, if you can
express a boolean F function with n inputs, then n parties can get
together and evaluate F(x1,x2,...,xn) such that
So in particular you can build an F() which reconstructs a database from
shares x1,x2,...xn and then runs a query on the database. Only the results
of the query are output; the theorems tell you that the shares stay
"So is it practical?" The answer is NO. These protocols tell you how to
secure multiparty compute a function gate by gate. With nontrivial
computational overhead and communication per gate. But, hey, at least it's

@_date: 2001-01-13 01:01:59
@_author: dmolnar 
@_subject: Consensus Actions in Cipherspace? 
Well, the totally trivial and stupid thing is for a list reader to
sign a message saying "I think message X is spam" and send it to the list
server. Actually, he doesn't even have to send the message; he can just
send the signature if the message is in some canonical format. The server can verify the signature, verify the user's ID, increment a
counter, and throw away the signature. When the counter passes a
threshold T, -chomp- the server eats the bond. The server can even keep the signatures around if it wants to prove to the
luser later that yes, lots of people really did think his message was
spam. This has at least two problems

@_date: 2001-01-13 01:38:04
@_author: dmolnar 
@_subject: Consensus Actions in Cipherspace? 
Sorry, I re-read your message and noted the requirement to ahve no central
server. How about this:
Now if enough group agents (1/2, 1/3, whatever) think the message is spam,
enough shares collect at step 4) to reconstruct the 2-dollar coin.
Otherwise not enough shares collect and the coin is never reconstructed.
Presumably S kept a copy and can spend it later. No central server now, just needs a verifiable secret sharing scheme.
Pedersen has one, and another is part of the Proactive Security work I
mentioned previously. -David

@_date: 2001-01-14 09:12:11
@_author: dmolnar 
@_subject: Consensus Actions in Cipherspace? 
Pedersen's verifiable secret sharing:
Non-interactive and information-theoretic secure verifiable secret
sharing. In J. Feigenbaum, editor, Advances in Cryptology -- CRYPTO '91,
volume 576 of Lecture Notes in Computer Science, pages 129-140, 11-15
August 1991. Springer-Verlag, 1992
Stadler's publically verifiable secret sharing:
Schoenmakers' publically verifiable secret sharing:
Wenbo Mao explains what "publically verifiable" or "universally
verifiable" means and why to use it:
Rosario Gennaro's thesis on VSS:
Stinson's bibliography on secret sharing schemes:
Actually, I meant that a verifiable secret sharing scheme is used in the
proactive security work.

@_date: 2001-01-16 19:10:01
@_author: dmolnar 
@_subject: automatic security protocol generation 
This looks interesting, given the discussion here recently about the need
for "protocol building blocks." It's only at a basic stage, but still
promising. A First Step towards the Automatic Generation of Security Protocols
Adrian Perrig, Dawn Song. In Proc. of Network and Distributed System
Security NDSS 2000, February 2000.

@_date: 2001-01-20 16:37:15
@_author: dmolnar 
@_subject: Recommendations for Cypherpunks Books 
Why? No, really - do we really need a book in which the majority of the
time is devoted to a minute analysis of how the Evil Terrorist screws up
his web of trust? Can you imagine the prose that would result?

@_date: 2001-01-21 11:02:32
@_author: dmolnar 
@_subject: Recommendations for Cypherpunks Books 
"If I name a future, then it won't happen" - ? This reminds me of the view that SF writers are trying to "predict the
future." I don't think picturing "the future" and then being "wrong" or
"right" about that is what fiction is usually about. More often about
commenting on the present. Just in passing, you may want to check out Stefan Brands' thesis -
depending on your background. If you're new to cryptography per se, then
_Applied Cryptography_ is a place to start. The definition would assert some standard by which to judge. Not
necessarily grounded in any "absolute standard." You can reject that
standard, of course, but there are still people trying to formulate and
defend these standards.
One effort in this direction which comes to mind is the "communtarian"
approach applied to privacy by Amitai Etizoni. What I've heard of it I
don't like, but I don't know much more than a few basic things -
"community" above all, corporate invasions of privacy pure evil, state
intrusions less evil because subject to scrutiny. To this you could oppose the sort of libertarian standard more often seen
on cypherunks, with its familiar consequences. Yes. _Secrets and Lies_ is a start towards this in an information security
I am reminded of the Salon article "Twilight of the crypto-geeks." I am also reminded of the phrase "technology is neutral" and how it seems
to polarize a debate. One side regards it as an argument that banning
technology is misguided. The other side as evidence of total naivete. I
don't suppose anyone has actually tried analysing where that phrase or
"technical vs. social means" pops up?
Why, write a killer version of Solitaire and barter it for medicine!
Probably not. Does this thing have connections to the outside world? To other PDAs
within range? How much range? Who can you contact with this? Can you sell
the results of your computation to others? Now if you were the only one with a PDA, maybe you could figure out a way
to sell computation to other people just by doing arithmetic. Perform
parlor tricks and pass yourself off as a calculator. But everyone has a
PDA. So now you need to have skills...
There's a bit more on computer security generally - _Terminal Compromise_
by Schwartau comes to mind. I think I owe him about $3.50...
Who said anything about the community being paranoid? You can use
cryptography to do new things beyond hiding data, you know. Probably the
most pressing would be authentication and controlling the data presented
to the world about you. Digital signatures and credentials.
If this PKI stuff works, then in 50 years it won't be the paranoids that
can exist only through ubiquitous crypto - it'll be *all* of us. Digital
driver's licenses and all. (Thanks to VeriSign for that awful phrase). Now you can go further and ask "what if a society had digital auction
protools?" or "what if selling your CPU cycles was normal and easy?" or "what if everyone knew about time-lock puzzles and time-release crypto
and could use it in everyday life?" or "what if the elections went
according to protocol X?" Even further, what if a society had the will and the capability to use all
of these crypto protocols just as soon as they were developed? (For
instance, posit that the provable security problem and the protocol
assembly problem are solved. You think of something you want to do, you
can put it together yourself and have it work.)
 > How would anyone have the time to make the horde
PGP is here now, though of course no one uses it. PKI seems to be driven
by something, e-commerce maybe. Academics and corporate research are
looking for new and fun things to do with math. Sometimes they hire
students to implement those fun things. Sometimes other people read the
papers and implement the fun things themselves.
Where did anonymous remailers come from?
thanks,

@_date: 2001-01-22 01:07:20
@_author: dmolnar 
@_subject: Recommendations for Cypherpunks Books 
It's also strange that there are relatively few science fiction books
which talk about math. There are some noted short story collections (_The
Mathematical Magpie_ and its sequel), short stories (Asimov's story about
rediscovering "graphitics," Heinlein's "And He Built A Crooked House"),
and authors (Rudy Rucker), but nowhere near the volume of SF based on
Perhaps it's that there are fewer people familiar with math than with
physics - which leads to fewer people writing such fiction and a smaller
market for it. The same is true for crypto, except more so. The obvious one would be Stephenson's _Cryptonomicon_. I wonder if Greg
Egan has written anything in this vein; he seems to have interests in
computer science, and he even had an alternate history/worldline
travelling story about Turing in Asimov's last year ("Oracle").

@_date: 2001-01-22 19:09:29
@_author: dmolnar 
@_subject: Recommendations for Cypherpunks Books 
Yes, that had been nagging at me. I haven't read it in years so didn't
want to speak up and find that I'd confused it with some other book...but
I remember it being really good.
Hmm. So this explains all those papers on "fair cryptosystems." Well, at
least one paper (and patent!) by Micali...

@_date: 2001-01-23 01:27:06
@_author: dmolnar 
@_subject: Recommendations for Cypherpunks Books 
Hi Jim, This suggests a tangent - If we look at works of fiction which were
politically or socially influential in their day, how many were
entertaining? how many were "good stories"? A lot of polemics end up
seeming transparent and thin today (I'm thinking in particular of
Bellamy's _Looking Backward_, but there are probably other examples). They had to capture their audience somehow, which seems to say something
about the audience of the time (or maybe just about the tendency people
have to overlook faults in a book which agrees with them). As for things unexpected - maybe it would be interesting to look at the
literature issued just after the possibility of the new invention becomes
known. Atomic power, for instance, was written about by H.G. Wells long
before the atomic bomb was built. Maybe atomic power is too extreme a
case, though. Bringing this back to "cypherpunk literature", such a look might provide
parallels with the emergence (or lack thereof) of crypto-oriented fiction. Yes, it seems we agree. Except it seems that instead of dismissing
definitions of "good" and "evil" as "an affect, no more" (if I'm reading
you correctly?) - it seems to me that this is where the real battles are
fought. So instead of being dismissive, it seems like a better idea to
*pay attention*. (This may be a sign of youth). Even so - in math class I am told "if two reasonable people start from
the same premises, they should arrive at the same conclusion." In
philosophy I find that Frege called a failure to apply the same laws of
logic a "new form of madness." In the ethics course, I am told "we always expect reasonable people to
arrive at *different* conclusions." Odd. Here I thought you were going to say COMMUNISM! :-) I just came across a biography of Robespierre. In it he's mentioned as
writing an essay for a prize competition in the 1770s, in which "under the
influence of Montesquieu" he condemns the republican ideal of <> as
requiring unnatural conformity of action and dishonourable actions. Before
turning around and reflecting that the monarchist alternative cannot be
justified on grounds of public utility...well, we know where he eventually ends up.  Anyway, it seems that "community" has taken the place of the "general
will" or "will of the people" as the utopian abstraction of the day. This
is annoying, because there *does* seem to be some merit to talking about a
"community" (or "society" for that matter) as a unit for purposes of
analysis. Even anarchists (especially anarchists?) talk about community.
(Godwin's "public opinion more powerful than whips and chains.")  As soon
as you do so, however, suddenly you've accidentally imported all this
"communitarian" baggage...
Onto the to-skim pile - thanks. Aren't they considered two sides of the die? I always thought that was the
point of the pervasive "Two Cultures" dichotomy - that you have a binary
opposition between "techs" on the one hand and "humanists" on the other.
Which, like many binary oppositions, fails to satisfy.
Sounds like an invitation to build a genetic algorithm. Heh. OK, but this does not strike me as *absurdly* paranoid. I understood your
point to be that any society paranoid enough to use massive amounts of
cryptography would be absurdly paranoid (maybe unstable). Maybe there's a
question of degree here?
Plus not every member of the society must be equally paranoid, once the
infrastructure is in place. How paranoid are my parents when they use SSL
to send credit card info to a web site - without even realizing that
they're using SSL or how it works? Paranoia on the part of a few can
change the lives of many.
So you think we'll end up with "one citizen, one identity?" Do you think
this will be an explicit norm - that people will react to the idea of
having two distinct identities online the way we would to having two
distinct identities in "real life" today? It wasn't that long ago that Sherry Turkle's _Life on the Screen_ was
supposed to be *the* account of how "we" were going to relate in
cyberspace. Except that who uses MUDs anymore? Where do we find the open
vistas of text, the vast plains of meaning, the mirror-stage-online which
so beautifully "informed" Turkle's account? Is anyone still talking about
the liberatory power of multiple identity - instead of footnotes in books
about e-commerce noting that web polls can be easily pseudospoofed?
(Plus I found the book much less convincing than _The Second Self_ - maybe
because it strayed from the focus on cognitive development and questions
like "what is alive?" which made _The Second Self_ gripping. Last I heard,
Turkle is back asking kids "what is alive?" with respect to Furbies.
Should be intriguing to see what comes out. )
 > "Humane" and "reasonable" ? I'm sure you're right, but those two words do
not inspire much confidence in me right now. (Coming out of a course on
French Social and Political Doctrines 1789-present will do that). Frankly, a "humane" and "reasonable" society issuing from Open Source
principles makes me think of a Committee on Public Safety run by Slashdot
readers. (Disclaimer: I am a Slashdot reader). This is no doubt unfair,
but the semi-political pronouncements I've seen from GNU have a nasty
could-be-called "communitarian" streak in them.
Oh, cool. Some friends of mine are working with Plan 9. I'll have to check
this out at some point...I've been too busy to pay much attention. Perhaps. I'm wary of making these kinds of pronouncements. It's a
curve-fitting problem. "Here are six events - build a trend around them." The rise of planned communities (including Summerlin, where I live in
Nevada) *could* point the way towards arcologies and master-planned
living. It could also engender a backlash which ends up with everyone
going back to live in the cities to create closer communities with their
fellow (wo)man. Can you imagine a latter-day Gandhi who exhorts people to move back to the
cities to live with each other again? No? Why? Yes? Why arcologies and not Gandhi? Then our entire deliberations are blown to bits by advances in
It would provide large family-owned corporations with even more
interesting politics than they might currently posess, that's for sure.
An alternative may be that the generation gap asserts itself with a
vengeance. Dad and Jr. can't get along - what about Dad and the 17th?
Instead of isolating vertically, societies isolate horizontally. Lots of
parallel institutions with mandatory minimum and mandatory retirement
ages. Kids born in years between large bumps end up caught on the edge -
perpetually too old for the ones behind, too young to ever be accepted in
the society born before them. So all this is fine, but I dislike saying that this is what "will happen." I tend to agree with you - but I also remember that in the 70's we had
predictions of world disaster by the 00's. Not quite there yet. Still
Malthus has to be right in the long run...
Anyway, I want space for less easily justifiable reasons. Such as "what's
out there?" and "we can do it." Also I want to go (was into space before
ever heard of cryptography) :-). Unfortunately I'm too tall and too heavy
to make the trip on anything NASA has right now. Besides my family
history of heart trouble. If the will is there, it will be developed. Look at what's going to happen
to electronic voting now. After Florida, people really seem to want it.
MIT and Cal Tech have announced an initiative to build a "real" system.
Build a market by building consensus and the technology will step up.
(I suppose I should dispute in passing the dichotomy between a society and
"a group of friends that numbers in a few hundred" - but the fundamental
point seems to be scaling. Which is a problem no matter what you call the
The reason why remailers are limited to a group of friends that numbers in
a few hundred is that no one has articulated a clear and compelling reason
to "everybody" why "everybody" needs to use them. People have argued why
"everybody" needs remailers *around* or why they're a good thing, or at
least why *not to ban them*, but this is different. No one is advertising for anonymous remailers on the radio. No one is on
the television talking about how we need a national network of anonymous
remailers. No one seems to be making any money off the things, *except*
maybe zks.net . Change this and we get the infrastructure for a society. -David

@_date: 2001-01-23 11:11:23
@_author: dmolnar 
@_subject: Some other math/crypto sci-fi 
I've had the idea of "collectible algorithm trading cards" kicking around
for a while...

@_date: 2001-01-24 01:44:01
@_author: dmolnar 
@_subject: using braid groups vs. number theory for pki 
Helger Lipmaa's truly amazing collection of links reveals
"Practical comparison of Fast Public-key Cryptosystems"
P. Karu, J. Loikkanen
which is an account of an implementation of braid group encryption and a
performance comparison to ECC and NTRU. The implementation was done in
C++, so I bet your assembly version would kill it - but of course you'd
have to do the same for NTRU to acheive proper comparison...
(I didn't see it available for download - maybe contact the authors)
The rest of the links are worth checking out as well.
and include some cryptanalysis and other links. Be careful when reading
these - the "word problem" is not what you may think it is, and some of
the cryptosystems seem to depend on more than just the word problem for
their security. If you have a good library nearby, you may want to look
for Rotman's _Introduction to Combinatorial Group Theory_ as a guide; it's
the best introduction I've found so far to the general issues. Short answer to "is it secure" is "maybe." Keep in mind that all you need
these days is a very weak trapdoor function and then you can "boost" it to
create a full public key cryptosystem. So if we can find any "hardness" at
all, that may be enough. Sorry, don't know more here than "look for white papers." Most of which
probably won't help you.

@_date: 2001-01-25 01:17:57
@_author: dmolnar 
@_subject: Some other math/crypto sci-fi 
Wait, there are non-Hasbro collectable card games, aren't there? Do they
all simply license from Hasbro? In any case, simple collectible cards would be all right to start with. So
what if they "happen" to fall into patterns. They're algorithms!
"I'll trade you a Floyd-Warshall for a Rabin-Miller, but only if you
throw in a Nisan-Wigderson Derandomizing Pseudorandom Generator...I'm low
on randomness." Instead of Mana, have "time," "space," "randomness," and other complexity
measures. (Death to the first person who suggests "ink.") Trade off
between the two as appropriate. Special cards ("Blum Speedup Theorem")
affect resource consumption. Maybe offer other cards which give benefits
at a cost ("Superstitious Mathematician" - halves time required to run
algorithms, but doesn't believe in randomness so you lose all randomness

@_date: 2001-01-25 02:59:26
@_author: dmolnar 
@_subject: Some other math/crypto sci-fi 
That's a relief. Yup! That's a bad thing? OK, maybe it would lead to too many in-jokes...

@_date: 2001-01-29 02:52:03
@_author: dmolnar 
@_subject: "Cryptography and Evidence," philosophy, and ZKPs 
I just came across this PhD thesis in philosophy:
"Cryptography and Evidence" Michael Roe
I have just started to read it. The thesis aims to give a careful account
of just what a protocol can and can *not* establish in the context of
repudiation. I'm pretty excited to see it, because I know of little
material on ZKPs from a "philosophical" point of view (besides Cypherpunks
What's more, the discussions I have seen with friends and such tend too
often to focus on the probabilistic nature of ZKPs, most often questioning
whether a probabilistic "proof" is a contradiction in terms. This isn't
interesting to me - not least because you can make perfectly rigorous
statements about the soundness of ZKPs. In the end, no matter what you
call it, it must convince you. Better, but still short of the discussion I'd like, is that found in "The Ongoing Value of Proof"
Gila Hanna
which shows an understanding of what ZKPs are, but regretfully limits
itself only to a brief comment that "..the most significant feature of the zero-knowledge method is that it is
entirely at odds with the traditional view of proof as a demonstration
open to inspection. This clearly thwarts the exchange of opinion among
mathematicians by which a proof has traditionally come to be accepted."
I wish the author had commented more, because as it stands I do *not*
think that it is so clear that a ZKP is "entirely at odds" with the
traditional view of proof. The interaction in a ZKP is certainly a
"demonstration," and every round is open to inspection. Perhaps you could
argue that the commitments used in ZKPs create a part of a ZKP "not open
to inspection," but the commitment values and decommits certainly are open
to inspection, so how far can you push this?
If you're familiar with Wittgenstein's "say vs. show" distinction, this is
how I might put it as a rough guide -- a ZKP's transcript is simulable,
therefore "says nothing," but the interaction "shows" the truth of a
proposition. (To some bounded probability of error.) Does anyone know of other works which comment on ZKPs from standpoints in
philosophy? (or otherwise outside the usual standpoint of trying to
develop new technical results about ZKPs?)
Thanks, -David

@_date: 2001-07-18 17:19:04
@_author: dmolnar 
@_subject: I'm looking for FSE2001 proceedings 
It also doesn't seem to be on Springer LINK yet.
For what it's worth, this is not my experience. While not every paper is
online, a large number of people make papers available from their web
Plus I'm very lucky in that my library subscribes to the online Springer
LINK service.
It's been a while since looking at the Springer-Verlag copyright notice. I
remember that it allows authors to publish papers on their web pages. Is
this correct?
What are the ACM and IEEE copyright terms like? Do they also allow
publication on the web?
Computer science might have it better than the life sciences in this
regard. de facto if not de jure.

@_date: 2001-07-24 23:00:55
@_author: dmolnar 
@_subject: THE INCHOATE LAWYER 
You're assuming he'll "shut up." I'm not so sure of that. I think that the
results, regardless of what they are, will generate plenty of conversation
by themselves. Worse, the results will multiply conversations, and not in
a good way; some argument will come up, and then the results will come up
as a point of credibility. Then it will be back to the same old back and
forth about the results and what they "mean." We might as well activate
some infinite automata and leave the room for drinks.
What's slightly pathetic about all of this is that it says that if only I,
too, could gain the disrespect of my peers, maybe I, too, could have
someone else pay for me to take the LSAT. Hey, at least that might make
my parents happy. They sometimes tell me that maybe I should look into
being an IP lawyer. Now there's a way to gain the disrespect of your

@_date: 2001-07-26 01:52:53
@_author: dmolnar 
@_subject: Open 802.11b wireless access points and remailers 
I heard recently that Starbucks is piloting 802.11b access in selected
Manhattan locations. The issue is support, of course - they need to see if
they'll have to hire a sysadmin for every Starbucks before rolling it out.
I haven't taken my laptop and tried to verify this yet.
Matthew Skala had some material on his web page concerning "community
wireless" networks, as well, in which people offer free wireless
connectivity as a public service. Presumably this too would offer
opportunities for anonymous net access.
I would be less willing to trust a static box connected to one of these
networks, though. Once identified as a remailer, it seems that it might be
too easy to track it to its physical location, at which point it can be
borged or destroyed. After all, if it's going to be an active remailer, it
will be sending and receiving several messages each day. You might try
to get around this by developing a protocol in which there are many, many
remailers, each of which only speaks once in a very long while. I don't
know how easy or hard it is exactly to do this kind of tracking, however,
which makes it difficult to say what such a protocol would look like.
Perhaps mobile remailers might be more useful or more difficult to track
to their physical implementation. The only problem with a mobile remailer
is the question of "who's moving it?" (or what). I can imagine a mobile
remailer the size of a Walkman without too much difficulty; I can also
imagine that if I were to wear such a remailer and walk around in the
wrong kind of environment, I'd be asking for a "mugging." or worse. Now
that I think about it, it's not clear that wireless actually buys us more
than obscurity of physical location. The real win, as you point out, is
ease of access and ease of setup. Maybe less dependence on upstream
connections, as well, so you can get around the problem of ISPs shutting
down remailers for spam.
Plus mobile remailers seem to require either a global address space or
developing the notion of remailer confederations which allow dynamic leave
and join of remailer nodes. I recall that the notion of dynamic
collections of remailers came up in at least one previous discussion of
disposable remailers. I don't remember that too many conclusions were
reached, but it was a while abck.
One problem is that an adversary can show up with polynomially many of its
closest friends and have them all try to join a remailer confederation at
once. While the MIX protocol is theoretically OK as long as even one MIX
is honest, this may have bad implications for traffic analysis. Perhaps
one thing we could do would be to borrow Levien's advogato metric.
Let anyone who wants to start a remailer confederation. They form the root
set of the trust metric for that confederation. Anyone can join the
confederation's address space, but will start out with no trust links
between them and the root set. Nodes can rate each other, establishing
trust links. This way you can develop a trust metric / reputation system
local to that particular remailer confederation.
Now the issues are how the ratings are set up and maybe more important,
how routing of messages is influenced by the trust metric. Ratings could
be manual. We know how well that works from the PGP web of trust
experiment - and here life is harder since remops usually will not know
each other personally nor want to.
Another issue is dealing with nodes which leave the confederation. What if
all the confederation founders leave? what happens to the root set then?
Also, building up trust may require time, which makes this unsuitable for
nodes which want to pop in for 20 ins and then leave (say their owner is
on the freeway).

@_date: 2001-07-26 03:38:50
@_author: dmolnar 
@_subject: Open 802.11b wireless access points and remailers 
You could probably hack this up now, if you were willing to lose the
cell phone functionality of your cell phone. Maybe you could even get by
with just replacing the web browser on your cell phone. (I still can't
make head or tails out of my phone's browser, but apparently people use
cell phones do handle text. In fact people are trying to make a
business out of cell phone mailing lists. see You'd have to add would be some kind of scripting language for
forwarding text messages on the phone.
 >
Heh. "One for me, one for my phone."
Batch transmissions every hour on the hour might help with this. No reason
to be up all the time for sending e-mail. You could also play games in
which every phone picks a different minute each hour, then wakes up during
that minute for transmission. Your chance of being in the same minute as
your destination isn't great, but you could transmit the packet to each of
your neighbors in that minute, each of whom tries to relay the packet in
different minutes during the next hour.
One issue with that, though, is how to stop packets from flying around
long after they've been first delivered. A gnutella/freenet-style limit on
the number of tries might help. So might announcements of packets
received; i.e. a phone says "I've received packet X, so you can drop it."
You'd have to be careful about an adversary trying to create packets which
live forever (i.e. the hop limit should not live in the packet, unless the
packet is signed and these announcements had better have some way of
proving they come from the 'intented' sender) (but at the same time, we
should avoid any protocol which requires a PKI for phones or even
public-key crypto on efficiency and speed grounds; it takes 20 seconds
for my phone to negotiate one RSA key exchange).
In order to prevent what Anderson calls "sleep deprivation" attacks, you'd
also want that the number of minutes the phone is up depends weakly or not
at all on parameters under the control of an adversary. like how many
packets received during the previous minute up. Random dropping of
incoming packets might be a way to get around this, since I'm thinking
that every phone broadcasts to every other phone in the same minute
(I keep thinking of _Dayworld_ through all of this, but I don't yet see a
good joke or a useful metaphor -- phones are not assigned set minutes
for life, unlike in _Dayworld_, so what's a "dayworld breaker"? a phone
that continues to relay during the entire hour? that would be a good
thing, since it'd speed up packet relay.)
I'd be pretty surprised if people haven't already looked at these sorts of
schemes and come up with much better ones. Although maybe they haven't
been considered with adversaries in mind. Anyone know of references?
 >
That's where spread-spectrum came from, isn't it? How much is publically
known about the toys they already have?

@_date: 2001-06-15 18:26:55
@_author: dmolnar 
@_subject: New patent: Auto-escrowable and auto-certifiable cryptosystems 
There was a paper on a similar topic in this year's ASIACRYPT from the
same authors. I have *not* reviewed the patent yet to see if the claimed
techniques are the same as that paper.
The paper seems to work; it's based on a cute technique involving what
they call "double-decker exponentiation." Instead of working with g^x, you
work with g1^(g2^x). They use this to perform what could be called "RSA in
the exponent" and leverage this to acheive the claimed signature-only
property. Double-decker exponentiation is interesting in its own right,
One of the sections in their paper note that after too many signatures,
the scheme could leak a "shadow" public key. The signatures were needed to
solve a system of simultaneous equations; it made me wonder how a lattice
reduction algorithm would fare in practice. I apologize for being so
imprecise here, but the paper is at

@_date: 2001-03-22 12:33:01
@_author: dmolnar 
@_subject: PGP flaw found by Czech firm allows dig sig to be forged 
Probably. Do you need only write access? What does that do for smart
cards - if anything?

@_date: 2001-03-25 00:04:16
@_author: dmolnar 
@_subject: kinds of computation? 
This reminds me of something I've had in the back of my mind for a while
(thanks to the Oxygen project over at MIT).  Is it useful to divide "available computation power" into these
Here "personalized" is a purposely vague notion which is supposed to
capture such things as, say, a Palm Pilot with your passphrases saved on
it. That would be a "trusted and personal" computation device. A "trusted
but impersonal" device might be something you have good reason to believe
works correctly, but doesn't have any special information about you. I'm not clear on whether it's useful to distinguish between 1) and 2).
Probably that depends on what is shoved into the word "personalized." The point of dividing computation into categories is that 4) is very rare,
often computationally weak, but a little goes a long way. At least, given
the right protocols. My brain is trusted and personal, but it can't do
much more than remember a password. My Palm Pilot and my brain together
can execute SRP or PAK or whatever - but my Palm Pilot doesn't need to
remember my password. (it can be "trusted but impersonal", contrary to the
example above). I'd be interested to hear where else this sort of categorisation has
popped up, or whether people think it's useful. thanks, -David

@_date: 2001-03-30 00:15:02
@_author: dmolnar 
@_subject: Face Recognition Technolgy Is Next - Big Brother Arrives 
Well, there's always the Arab version of Islam...if you happen to be a
woman, anyway. Not exactly "new."

@_date: 2001-03-30 15:53:08
@_author: dmolnar 
@_subject: Phillip Morris ID database? 
I walked into the local 7-11 today. There was a scanner there and a man
next to it offering a free pack of cigarettes if he could scan your
drivers' license or other form of photo ID *and* you are an adult smoker. When I asked what this was all about, he told me that Phillip Morris is
trying to "update their ID database." Various promotional material was
present as well, but I didn't read any of it. Does anyone know more about this? Does Phillip Morris have an ID database,
and how often do they update it?

@_date: 2001-03-31 11:32:20
@_author: dmolnar 
@_subject: Phillip Morris ID database? 
That's interesting - I asked the guy at the scanner if they wanted the IDs
to verify ages, and he denied it - said that PM was just "updating its
database." Probably he didn't know what the database is used for. In any
case, if they're refusing IDs from minors, then this database seems less
useful for establishing how many minors smoke. I have to wonder just what
the rationale in the settlement for such a database is.
Thanks for the information - for a little bit I wasn't sure if this was
for real or not.

@_date: 2001-11-18 12:40:51
@_author: dmolnar 
@_subject: The Crypto Winter 
I don't know about what's happening to Mojo Nation exactly, but it seemed
to me that they were focusing on "P2P." Since that isn't doing so well
these days, I'm not sure where they are. They were at the O'Reilly P2P
conference earlier this month, but I wasn't, so I don't know what was
On the other hand, PGP integration with mailers and OS is further along
than in 1996, at least on Windows. I use Outlook/PGP for work all the
time. (On Unix, there's premail, some pgp/pine scripts, built-in mutt
support, and I'm not sure what else).
Our chapter for that book included some of this. In retrospect it didn't
include enough; in particular, omitting Ian Grigg and systemics, inc was a
particularly bad oversight (and mostly mine). The second edition of the
book was recently cancelled, unfortunately.

@_date: 2001-11-18 12:58:44
@_author: dmolnar 
@_subject: NetCamo project 
"Within the NetCamo ( Network Camouflaging ) project, we study how to
prevent traffic analysis in mission-critical QoS-guaranteed networks."
Some of the people from this project gave a talk at the USENIX Security
works-in-progress session on "a quantitative analysis of anonymous
communications." Abstract is at:
Anyone else taken a look at these people?

@_date: 2001-11-18 13:44:28
@_author: dmolnar 
@_subject: Pricing spare resources and options? 
The recent comments on Mojo Nation prompted me to look at their site
again. I don't see much guidance on how to set prices for network
services. There's a mention someplace that business customers will build
pricing schemes on top of Mojo Nation, but not much indication of what
these schemes might be.
So what is the "right" way to price resources? (Preferably beyond the
obvious "supply and demand.")
Fleet announced at Hettinga's DCSB that they were developing some kind of
spare resource market. I couldn't make it. Does anyone know whether they
addressed this point?
A related question - I ran into a friend of mine who had just finished an
internship in options trading. He suggested it might be worth looking at
options on spare disk space or other resources, as a means of figuring out
how to make Mojo-type systems eventually profitable in the real world. Now
I have a copy of Natenberg's _Option Volatility and Pricing_ to look at...

@_date: 2001-11-20 18:00:26
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
Mojo Nation doesn't have to "sell" directly to consumers. In fact, as you
point out, it's not realistic to expect consumers to manage their own
pricing. On the other hand, this might be an opportunity for people to
build services on top of Mojo Nation. Such a "middle" service could
provide a consumer with a turnkey, flat fee, no hassle service, while
using Mojo Nation to obtain the most efficient prics possible for
resources. The difference between the flat fee and the "most efficient"
price provided by Mojo becomes the profit for the middle service.
I don't even know if this works in theory, however. Presumably I could set
up a model to investigate the relationship between "best possible" Mojo
prices and the characteristics of the network - things like how many
users, how much trading, liqudity, which pricing algorithms people use,
and so on. Then we could ask questions like "how many people need to be
offering which kinds of services before a service built on top of Mojo can
be profitable?"
The ideal situation would be one in which the middle service can undercut
"traditional" providers -- and still make money. Except that doesn't look
like it's going to happen in the real world. You've mentioned the fact
that not enough people are running Mojo clients ("thin
market"/"illiquid"). It's not clear what it would take to get more people
in the Mojo marketplace; even if the theory works out beyond my wildest
dreams and I could somehow "prove" that Mojo will make everyone money **if
only ten million people sign up tomorrow**, I suspect no one would pay
There are other problems as well. For one thing, who are the "traditional
providers" in the previous paragraph against whom a Mojo service would
compete? in markets for which resources? In the case of disk space, it
seems to be the people who make and sell hard drives; both entrenched and
selling physical goods which any middle service would be hard pressed to
emulate perfectly. Plus, as one of the anonymous posters noted, the disk
market is quite non-volatile (maybe "boring" is the right word) with a
pronounced trend downward; what is the incentive to use a Mojo-based
service there if the customer can just wait a week and then buy a bigger HD?
So are there markets for which a "middle service" could work?
I apologize for going on at length about this, but if you want Mojo to
correspond at all to real money instead of being a DoS protection
feature, I don't see many other ways than "middle services" to make it
work on a wide scale. Then again, I may just be lacking imagination.
Does anyone happen to know of real-world current examples like this, in
which some aggregator buys and sells a commodity on an exchange, then
turns around and offers it at a flat rate to end users?
Have you seen the SemioText(e) anthology? "TAZ" is in it, along with stuff
from J.G. Ballard, the Church of the Subgenius, stories vaguely inspired
by gnosticism, and other usual suspects. Reading it reminded me of reading
Douglas Rushkoff's _Cyberia_ for the first time -- another book which
gives new meaning to your "see where the degrees of freedom of the Web
will take us." Heady stuff, all of it, and now seems to be out of fashion,
but that's a tangent...
-David Molnar

@_date: 2001-11-20 23:14:56
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
On the off chance that this isn't rhetorical, because at least one way of
doing this seems straightforward...
Some guy with a true name good enough to sign up for PayPal gets an
account, publishes a public key, and acts as a human "cash remailer."
Well, several people do, actually, and then we run a "payment MIX." Some
anonymous poster brought up the idea about a year ago IIRC and attributed
it to Ron Rivest. It sparked a short discussion which went into questions
of whether such an operation might run afoul of money laundering laws and
then sort of petered out. Or maybe I just stopped reading.
To spell it out, Nomen offers to do the service and picks an incoming
payment MIX plus a chain of MIXes. (N.B.: will use MIX to refer to a
member of the chain).  Publishes a "payment reply block" which has the
incoming payment MIX PayPal address in clear, plus a block encrypted with
the payment MIX's public key. The encrypted portion has the PayPal address
of the next MIX in the chain, plus an encrypted portion for that next MIX.
Eventually it decrypts to reveal Nomen's real PayPal account info.
Assuming everyone plays along, the money flows down the chain and ends up
in Nomen's PayPal account. I expect that's not such a great assumption
when dealing with "real" money.
You still have the BlackNet problem, though - Nomen1 and Nomen2 can both
publish public payment blocks. Which one do you pay? In this particular
case, though, you can address that (and the fair exchange problem) by
doing what Stephen King did -- Nomen does a little bit of the work first
for free, then continues iff enough people ante up for more. You use the
payment block signed by the same key which signed the current work.
Now that I think about it, you could probably take this down the route
Eric Hughes suggested in his "Universal Piracy Network" presentation at
DEF CON IV. That is, people pay Nomen and in return Nomen sends them the
new work first; they get 0-day access to the warezzz. Hughes had something
about "completion bonds" in his presentation as well, but I don't remember
at all what the particulars were, just that there was a rough analogy to
the movie industry. Anyone remember more details?
I have half a mind to sign up for PayPal just to try this. (Assuming I
won't be thrown in jail for money laundering, anyway). Anyone with me?
Nomen? :)
Although I feel compelled to point out that with only one MIX, it's not
going to be particularly secure for Nomen. Not to mention with no
provision for detecting MIXes who shave off the payment or eat it
entirely, this might just be me trying to make a few quick $$$. Of course
if this were large-scale, you could use reputations -- a MIX which eats
the entire $$$ now loses out on the possibility of shaving small fractions
of $$$ later.
Also, with provision for detecting MIXes who give good payments, there's
nothing to stop Nomen from alleging that I'm screwing him and ruining my
reputation. As a potential MIX, that bothers me. So there are issues here.
right, like the barter a Nomen is trying to do with Marc right now...and
which doesn't seem to be working too well so far. Although in that case it
seems like the problem was just that this Nomen set up the terms of the
barter without bothering to ask if the other party actually wanted to
trade. That's like this guy at Coney Island I met a few months back.
"You wanna throw a dart?? Here! Here! Free!"
"Now you owe me five dollar!"
(I paid. Yes, I'm a wuss. I got a cute fan out of it. I'm never doing
business like that at Coney Island ever again.)

@_date: 2001-11-20 23:45:57
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
A friend of mine was considering a business plan for physical remailer+
"infomediary" for a class project a year or two ago. Precisely to get
around this problem. Sell learns the remailer's address. More than a few
remailers and you can chain them, etc. etc.
He was thinking about it in terms of the single proxy model, but if the
idea ever took off enough to have multiple competitors, you could try a
MIX-net. I don't think I ever pointed this out to him. I'll have to ask
him whatever came of the project.
Right - it's for optimizing load balancing and data distribution. Roughly
stated, it seems to me to be a DoS prevention mechanism. It's not at all
clear that Mojo will ever be meaningfully convertible to "real" money, at
least not to me. but then again, I'm often unclear.
and that in turn is a holdover from the old BBS days.
BBSes seem special in that the resources available are so *drastically*
limited. A BBS with one phone line could serve one user at a time. When
one person is on, nobody else has a shot. So a BBS without upload/download
ratios runs the risk of collapsing pretty quick under the weight of
leeches and m0es. When I ran a BBS, I ended up removing the files section
altogether; I thought messages were the most important part (and in any
case, didn't have a large enough HD to hold files...plus didn't want to
deal with the tension bewteen running a "free speech" BBS and screening
for pirate warez so as to not get arrested.) I think this accounted in
part for the obsession with "access level" which seemed common to many BBS
(On the other hand, I also gave everyone a 90 minute time quota; way more
than most people ever used. So perhaps this "quotas or die" doesn't hold
true universally. Anyone else have anecdotal evidence from BBSing? )
Anyway, the point is that in such a resource-limited environment, quotas
and ratios are basic rationing tools. Use them or die(mostly). When you
move to an environment which has more resources, things seem to change.
You can get away for longer with less in the way of resource control.
So the principle that "networks which reward providers of information
should flourish" may be tempered by less which selects for those networks
over others.
another question -- is Slashdot popular because of its moderation system?
OK, I should *know* this, but -- what about "flooz" and "beenz" ? weren't
they something along these lines before they folded?

@_date: 2001-11-20 23:46:42
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
Maybe. I regret I'm not familiar with Hawala. I'll go google it.

@_date: 2001-11-21 01:00:51
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
Gee, it's even in the cypherpunks archives. Sorry, everyone.
Yes, as described sure sounds similar. The point of doing it over PayPal
would just be to make it easy for people on this list to pay Nomen. Even
though hawala works in the real world, I'm not so sure we could just start
it and expect it to work here.
One thing that came to mind while reading about it -- does it buy us
anything in a MIX-net to separate control messages from payload messages?
This came to mind because one of the descriptions of the hawala network
seemed to imply that payment would come in from one source and then the
name of the recipient would come in from another.
The analogy in a MIX-net for e-mail would be having a message delivered to
a MIX, and then later forwarding instructions for that message delivered
by someone else. (said instructions identifying message by hash or
something). Another way to look at this is putting delay in the hands of
the client. Not clear to me that it helps; maybe make an adversary think a
certain node is the final destination? I can't think of a MIX design off
the top of my head which does this. Anyone else? something like this
discussed way back when?

@_date: 2001-11-21 23:51:04
@_author: dmolnar 
@_subject: why market to Joe Sixpack?  
Declan's comment on operating a physical remailer for suitably valuable
cargo, plus some of Tim's recent comments about integration, made me think
of the question in the subject line. So far I see at least three possible
1) Make lots of money.
2) Spread awareness (that "funny feeling in the stomach" recently
discussed) and save our fellow man. Make the world safe for privacy.
3) Ensure that cryptography and privacy-enhancing technologies have uses
besides "Four Horsemen of the Infocalypse," so that they aren't banned.
anything else?
I think the physical remailer for only FedEx will fail 3.

@_date: 2001-11-23 23:59:28
@_author: dmolnar 
@_subject: Pricing Mojo, Integrating PGP, TAZ, and D.C. Cypherpunks 
Do you know this because you've communicated with the Mojo team, are they
on record about it, is this speculation, your inference from events, or
something else?
Please don't take that the wrong way - I don't doubt your integrity. It's
an interesting suggestion and I'm just wondering where it came from.
Perhaps one initial market might be one of the web sites which offer
people a chance to connect with "experts" in various fields. One such web
site solicited me based in part on postings to sci.crypt (guess that was
good for something, after all). I tried it out, but found the interface
too cumbersome and had qualms about being billed as an "expert" (it wasn't
clear to me what guarantees were being made, and I didn't have time to do
a proper job anyway).
I regret I've forgotten the name. Anyone know if these sites are still

@_date: 2001-10-10 19:25:31
@_author: dmolnar 
@_subject: Documented criminal/terrorist uses of cryptography? 
Does anyone have pointers to documented instances of terrorists or
criminals using stego or crypto? As opposed to the speculation recently
seen regarding Osama bin Laden.
Thanks much,
-David Molnar

@_date: 2001-09-11 18:18:51
@_author: dmolnar 
@_subject: BLOOD 
I just signed up to donate blood at St. Vincent's, which is near where I'm
living. They have a waiting list to donate blood roughly 3 days long
(unless you're 0- or extra rare, neither of which I am). They expect to
use everyone on the list. So yes, please donate.
There's *nothing* where the WTC used to be except smoke. At least not that
I could see from a midtown office.

@_date: 2001-09-11 18:27:37
@_author: dmolnar 
@_subject: Manhattan Mid-Afternoon 
I'm living in west village, which is mid-to-lower manhattan. No panic yet,
streets are orderly and very sparsely populated, almost deserted. Everyone
seems to have either gone home or found someplace to wait it out. Roads
lower than about west 11th seem to be closed; St. Vincent's hospital seems
to be taking injured (although I don't see too many coming in right now. I
don't know if that's a bad sign).
My roommate tells me that the ferry line reaches 4 across from 53rd to
23rd...and back again.
I've heard speculation about curfew, martial law, etc. etc. Nothing like
that has been confirmed AFAIK. Then again I've been avoiding the radio and
TV for the last few hours.
As to whether cypherpunk technologies assisted in this tragedy -- please
explain how anonymous remailers affect airport security? I am personally
more worried about the lack of cryptography and system security.
I didn't learn about what happened until walking into work this morning.
On the way up people were crying and the headlines in the elevator talked
about FAA grounding all planes. I thought that the ATC computers had
finally given up. What happens if in the next attack the terrorists take
control of the ATC system?

@_date: 2001-09-12 09:06:03
@_author: dmolnar 
@_subject: Manhattan Mid-Afternoon 
I'd like to think I would have, but honestly I don't know. Also, if the
first rusher is killed, that acts as a negative example for the others.
The scene from "Sunshine" where Fiennes is berated for standing aside and
watching his father die keeps coming back.
Keep in mind that the hostages likely did not know the terrorists were
planning to down the plane. For all they knew, they were going to be
exchanged for prisoners in Israeli jails. By the time it became clear what
was going to happen (if it ever did), it might have been too late. Then
it's not "my life vs 20,000" but "my life vs squat if they kill me and
retain control."

@_date: 2001-09-22 10:53:45
@_author: dmolnar 
@_subject: Preparedness 
If I remember correctly, it's an accidental explosion, not a bomb. The
tunnel actually does collapse.
( mentions this)
The movie is about Stallone's attempt to rescue people from inside a
collapsing tunnel. Naturally he more or less succeeds.
A cabbie a few days ago mentioned something about a 2.5 hour wait to get
through tunnels; he claimed they were searching *all* the cars. I don't
know if this is still in effect. I know that my roommate and I made it out
of the city through a tunnel, without being stopped, last Friday.

@_date: 2001-09-26 11:02:24
@_author: dmolnar 
@_subject: Preparedness 
Oh! I'm sorry for not making that clear. I am posting from NYC. Despite
the Harvard e-mail address, I am in NYC and will be here for the year.

@_date: 2002-08-16 04:21:22
@_author: dmolnar 
@_subject: employment market for applied cryptographers? 
Don't forget schedule pressure, the overhead of bringing in a contractor
to do crypto protocol design, and the not-invented-here syndrome. I think
all of these contribute to keeping protocol design in-house, regardless of
the technical skill of the parties involved. It takes a serious investment
in time to qualify a consultant. If having the protocol right isn't a top
priority, that investment won't be made...and I'd guess that designing a
new protocol isn't common enough to merit a separate job/new hire in most

@_date: 2002-08-16 04:24:35
@_author: dmolnar 
@_subject: Bay area cypherpunks 
I am currently in the SF Bay Area and wondering whether any cypherpunks
are around and might want to say hi. Right now I'm in Berkeley, but I'm
willing to travel (public transportation) to see people.
-David Molnar

@_date: 2002-08-18 01:46:09
@_author: dmolnar 
@_subject: employment market for applied cryptographers? 
I agree with this as far as "crypto" protocols go. But one thing to keep
in mind is that almost all protocols impact security, whether their
dsigners realize it or not. Especially protocols for file transfer, print
spooling, or reservation of resources. most of these are designed without
people identifying them as "crypto protocols."
Another thing that makes it worse -- composition of protocols. You can do
an authentication protocol and prove you're "you." Then what? Does that
confer security properties upon following protocols, and if so what?

@_date: 2002-07-10 10:04:32
@_author: dmolnar 
@_subject: CP meet at H2K2? 
The buzz on the cypherpunks-NYC list is that the most convenient bar is
the "blarney rock", just near the hotel pennsylvania. How about say we'll
meet there at midnight on Saturday?

@_date: 2002-07-10 12:50:11
@_author: dmolnar 
@_subject: Fwd: Re: CP meet at H2K2?] 
According to the TimeOutNewYork eating and drinking guide:
Blarney Rock
137 W 33rd street between Broadway and Seventh Avenue
Let's say midnight for *sure* as a meeting time, and perhaps people can
dart out there earlier if they feel like it. I plan to arrive at the con
around 2pm on Saturday and will stick my head in Blarney Rock on the way.
In case anyone wants my number, e-mail me.

@_date: 2002-07-12 21:22:20
@_author: dmolnar 
@_subject: S-DART  
This seems to be related to the "Stego Watch" program sold by Wetstone
Technologies. Does anyone have more information about it? I've found
citations for a few papers on it, but none are online. I'll go to the
library later, but in the meantime has anyone read these papers or had
experience with the system in action?
How does it compare to Provos' stegdetect?
-David Molnar

@_date: 2002-06-21 07:49:00
@_author: dmolnar 
@_subject: CP meet at H2K2? 
I should be there, since I'm free and in the area.
In a similar vein, who's going to be at DEF CON?

@_date: 2002-05-28 12:35:38
@_author: dmolnar 
@_subject: Forward-secure public-key encryption eprint 
Forward-secure public-key encryption has been discussed here, on
sci.crypt, and elsewhere. To recap - the goal is that an adversary who
breaks into your computer today can't read messages sent/received
yesterday. In the interactive case, you use ephermal Diffie-Hellman. The
non-interactive case is more complicated and has had some ideas considered
by Ross Anderson, Adam Back, and David Hopwood (among others). Cypherpunks
relevance: forward security is nice for remailers.
Anyway, there's a new eprint up which shows how to construct such a scheme
starting from an ID-based encryption scheme by Boneh + Franklin.
"A Forward-Secure Public-Key Encryption Scheme"
Jonathan Katz
It's worth noting that the scheme this is based on has code available.

@_date: 2002-05-30 12:18:54
@_author: dmolnar 
@_subject: Forward-secure public-key encryption eprint 
Did I miss a separate message in which David Hopwood followed up to my
post? Cypherpunks is more reliable for me than it used to be, but it's not
always all there.
Thanks for the detailed summary! Even if the system may not be ready for
prime time, I think it may still be worth looking at it and following
future developments.
