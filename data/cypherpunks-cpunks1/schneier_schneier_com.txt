
@_date: 2007-12-15 01:13:51
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, December 15, 2007 
CRYPTO-GRAM
              December 15, 2007
              by Bruce Schneier
               Founder and CTO
                BT Counterpane
             schneier at schneier.com
                       A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     How to Secure Your Computer, Disks, and Portable Drives
     Defeating the Shoe Scanning Machine at Heathrow Airport
     News
     Gitmo Manual Leaked
     Schneier/BT Counterpane News
     Security in Ten Years
     Comments from Readers
** *** ***** ******* *********** *************
     How to Secure Your Computer, Disks, and Portable Drives
Computer security is hard. Software, computer and network security are all ongoing battles between attacker and defender. And in many cases the attacker has an inherent advantage: He only has to find one network flaw, while the defender has to find and fix every flaw.
Cryptography is an exception. As long as you don't write your own algorithm, secure encryption is easy. And the defender has an inherent mathematical advantage: Longer keys increase the amount of work the defender has to do linearly, while geometrically increasing the amount of work the attacker has to do.
Unfortunately, cryptography can't solve most computer-security problems. The one problem cryptography *can* solve is the security of data when it's not in use. Encrypting files, archives -- even entire disks -- is easy.
All of this makes it even more amazing that Her Majesty's Revenue & Customs in the United Kingdom lost two disks with personal data on 25 million British citizens, including dates of birth, addresses, bank-account information and national insurance numbers. On the one hand, this is no bigger a deal than any of the thousands of other exposures of personal data we've read about in recent years -- the U.S. Veteran's Administration loss of personal data of 26 million American veterans is an obvious similar event. But this has turned into Britain's privacy Chernobyl.
Perhaps encryption isn't so easy after all, and some people could use a little primer. This is how I protect my laptop.
There are several whole-disk encryption products on the market. I use PGP Disk's Whole Disk Encryption tool for two reasons. It's easy, and I trust both the company and the developers to write it securely. (Disclosure: I'm also on PGP Corp.'s Technical Advisory Board.)
Setup only takes a few minutes. After that, the program runs in the background. Everything works like before, and the performance degradation is negligible. Just make sure you choose a secure password

@_date: 2007-07-15 16:34:11
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, July 15, 2007 
CRYPTO-GRAM
                July 15, 2007
              by Bruce Schneier
               Founder and CTO
                BT Counterpane
             schneier at schneier.com
                       A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Correspondent Inference Theory and Terrorism
     TSA and the Sippy Cup Incident
     News
     Ubiquity of Communication
     4th Amendment Rights Extended to E-Mail
     Credit Card Gas Limits
     Schneier/BT Counterpane News
     Designing Voting Machines to Minimize Coercion
     Risks of Data Reuse
     Comments from Readers
** *** ***** ******* *********** *************
     Correspondent Inference Theory and Terrorism
Two people are sitting in a room together: an experimenter and a subject. The experimenter gets up and closes the door, and the room becomes quieter. The subject is likely to believe that the experimenter's purpose in closing the door was to make the room quieter.
This is an example of correspondent inference theory.  People tend to infer the motives -- and also the disposition -- of someone who performs an action based on the effects of his actions, and not on external or situational factors. If you see someone violently hitting someone else, you assume it's because he wanted to -- and is a violent person -- and not because he's play-acting. If you read about someone getting into a car accident, you assume it's because he's a bad driver and not because he was simply unlucky. And -- more importantly for this column -- if you read about a terrorist, you assume that terrorism is his ultimate goal.
It's not always this easy, of course.  If someone chooses to move to Seattle instead of New York, is it because of the climate, the culture or his career? Edward Jones and Keith Davis, who advanced this theory in the 1960s and 1970s, proposed a theory of "correspondence" to describe the extent to which this effect predominates. When an action has a high correspondence, people tend to infer the motives of the person directly from the action: e.g., hitting someone violently. When the action has a low correspondence, people tend to not to make the assumption: e.g., moving to Seattle.
Like most cognitive biases, correspondent inference theory makes evolutionary sense. In a world of simple actions and base motivations, it's a good rule of thumb that allows a creature to rapidly infer the motivations of another creature. (He's attacking me because he wants to kill me.) Even in sentient and social creatures like humans, it makes a lot of sense most of the time. If you see someone violently hitting someone else, it's reasonable to assume that he's a violent person. Cognitive biases aren't bad; they're sensible rules of thumb.
But like all cognitive biases, correspondent inference theory fails sometimes. And one place it fails pretty spectacularly is in our response to terrorism. Because terrorism often results in the horrific deaths of innocents, we mistakenly infer that the horrific deaths of innocents is the primary motivation of the terrorist, and not the means to a different end.
I found this interesting analysis in a paper by Max Abrahms in "International Security."  "Why Terrorism Does Not Work" analyzes the political motivations of 28 terrorist groups: the complete list of "foreign terrorist organizations" designated by the U.S. Department of State since 2001. He lists 42 policy objectives of those groups, and found that they only achieved them 7 percent of the time.
According to the data, terrorism is more likely to work if 1) the terrorists attack military targets more often than civilian ones, and 2) if they have minimalist goals like evicting a foreign power from their country or winning control of a piece of territory, rather than maximalist objectives like establishing a new political system in the country or annihilating another nation. But even so, terrorism is a pretty ineffective means of influencing policy.
There's a lot to quibble about in Abrahms' methodology, but he seems to be erring on the side of crediting terrorist groups with success. (Hezbollah's objectives of expelling both peacekeepers and Israel out of Lebanon counts as a success, but so does the "limited success" by the Tamil Tigers of establishing a Tamil state.) Still, he provides good data to support what was until recently common knowledge: Terrorism doesn't work.
This is all interesting stuff, and I recommend that you read the paper for yourself. But to me, the most insightful part is when Abrahms uses correspondent inference theory to explain why terrorist groups that primarily attack civilians do not achieve their policy goals, even if "The theory posited here is that terrorist groups that target civilians are unable to coerce policy change because terrorism has an extremely high correspondence. Countries believe that their civilian populations are attacked not because the terrorist group is protesting unfavorable external conditions such as territorial occupation or poverty. Rather, target countries infer the short-term consequences of terrorism -- the deaths of innocent civilians, mass fear, loss of confidence in the government to offer protection, economic contraction, and the inevitable erosion of civil liberties -- (are) the objects of the terrorist groups. In short, target countries view the negative consequences of terrorist attacks on their societies and political systems as evidence that the terrorists want them destroyed. Target countries are understandably skeptical that making concessions will placate terrorist groups believed to be motivated by these maximalist objectives."
In other words, terrorism doesn't work, because it makes people less likely to acquiesce to the terrorists' demands, no matter how limited they might be. The reaction to terrorism has an effect completely opposite to what the terrorists want; people simply don't believe those limited demands are the actual demands.
This theory explains, with a clarity I have never seen before, why so many people make the bizarre claim that al Qaeda terrorism -- or Islamic terrorism in general -- is "different": that while other terrorist groups might have policy objectives, al Qaeda's primary motivation is to kill us all. This is something we have heard from President Bush again and again -- Abrahms has a page of examples in the paper -- and is a rhetorical staple in the debate.
In fact, Bin Laden's policy objectives have been surprisingly consistent. Abrahms lists four; here are six from former CIA analyst Michael Scheuer's book "Imperial Hubris":
* End U.S. support of Israel
* Force American troops out of the Middle East, particularly Saudi Arabia
* End the U.S. occupation of Afghanistan and (subsequently) Iraq
* End U.S. support of other countries' anti-Muslim policies
* End U.S. pressure on Arab oil companies to keep prices low
* End U.S. support for "illegitimate" (i.e. moderate) Arab governments, like Pakistan
Although Bin Laden has complained that Americans have completely misunderstood the reason behind the 9/11 attacks, correspondent inference theory postulates that he's not going to convince people. Terrorism, and 9/11 in particular, has such a high correspondence that people use the effects of the attacks to infer the terrorists' motives. In other words, since Bin Laden caused the death of a couple of thousand people in the 9/11 attacks, people assume that must have been his actual goal, and he's just giving lip service to what he *claims* are his goals. Even Bin Laden's actual objectives are ignored as people focus on the deaths, the destruction and the economic impact.
Perversely, Bush's misinterpretation of terrorists' motives actually helps prevent them from achieving their goals.
None of this is meant to either excuse or justify terrorism. In fact, it does the exact opposite, by demonstrating why terrorism doesn't work as a tool of persuasion and policy change.  But we're more effective at fighting terrorism if we understand that it is a means to an end and not an end in itself; it requires us to understand the true motivations of the terrorists and not just their particular tactics.  And the more our own cognitive biases cloud that understanding, the more we mischaracterize the threat and make bad security trade-offs.
Cognitive biases:
 or This essay originally appeared on Wired.com:
 or ** *** ***** ******* *********** *************
     TSA and the Sippy Cup Incident
This story is pretty disgusting:  "I demanded to speak to a TSA [Transportation Security Administration] supervisor who asked me if the water in the sippy cup was 'nursery water or other bottled water.' I explained that the sippy cup water was filtered tap water. The sippy cup was seized as my son was pointing and crying for his cup. I asked if I could drink the water to get the cup back, and was advised that I would have to leave security and come back through with an empty cup in order to retain the cup. As I was escorted out of security by TSA and a police officer, I unscrewed the cup to drink the water, which accidentally spilled because I was so upset with the situation.
"At this point, I was detained against my will by the police officer and threatened to be arrested for endangering other passengers with the spilled 3 to 4 ounces of water. I was ordered to clean the water, so I got on my hands and knees while my son sat in his stroller with no shoes on since they were also screened and I had no time to put them back on his feet. I asked to call back my fianci, who I could still see from afar, waiting for us to clear security, to watch my son while I was being detained, and the officer threatened to arrest me if I moved. So I yelled past security to get the attention of my fianci.
"I was ordered to apologize for the spilled water, and again threatened arrest. I was threatened several times with arrest while detained, and while three other police officers were called to the scene of the mother with the 19 month old. A total of four police officers and three TSA officers reported to the scene where I was being held against my will. I was also told that I should not disrespect the officer and could be arrested for this too. I apologized to the officer and she continued to detain me despite me telling her that I would miss my flight. The officer advised me that I should have thought about this before I 'intentionally spilled the water!'"
This story portrays the TSA as jack-booted thugs.  The story hit the Internet in mid-June, and quickly made the rounds.  I saw it on BoingBoing. But, as it turns out, it's not entirely true.
The TSA has a webpage up, with both the incident report and video.
"TSO [REDACTED] took the female to the exit lane with the stroller and her bag.  When she got past the exit lane podium she opened the child's drink container and held her arm out and poured the contents (approx. 6 to 8 ounces) on the floor.  MWAA Officer [REDACTED] was manning the exit lane at the time and observed the entire scene and approached the female passenger after observing this and stopped her when she tried to re-enter the sterile area after trying to come back through after spilling the fluids on the floor.  The female passenger flashed her badge and credentials and told the MWAA officer 'Do you know who I am?'  An argument then ensued between the officer and the passenger of whether the spilling of the fluid was intentional or accidental. Officer [REDACTED] asked the passenger to clean up the spill and she did."
Watch the second video.  TSO [REDACTED] is partially blocking the scene, but at 2:01:00 PM it's pretty clear that Monica Emmerson -- that's the female passenger -- spills the liquid on the floor on purpose, as a deliberate act of defiance.  What happens next is more complicated; you can watch it for yourself, or you can read BoingBoing's somewhat sarcastic summary.
In this instance, the TSA is clearly in the right.
But there's a larger lesson here.  Remember the Princeton professor who was put on the watch list for criticizing Bush?  That was also untrue. Why is it that we all -- myself included -- believe these stories?  Why are we so quick to assume that the TSA is a bunch of jack-booted thugs, officious and arbitrary and drunk with power?
It's because everything seems so arbitrary, because there's no accountability or transparency in the DHS.  Rules and regulations change all the time, without any explanation or justification.  Of course this kind of thing induces paranoia.  It's the sort of thing you read about in history books about East Germany and other police states.  It's not what we expect out of 21st century America.
The problem is larger than the TSA, but the TSA is the part of "homeland security" that the public comes into contact with most often -- at least the part of the public that writes about these things most.  They're the public face of the problem, so of course they're going to get the lion's share of the finger pointing.
It was smart public relations on the TSA's part to get the video of the incident on the Internet quickly, but it would be even smarter for the government to restore basic constitutional liberties to our nation's counterterrorism policy.  Accountability and transparency are basic building blocks of any democracy; and the more we lose sight of them, the more we lose our way as a nation.
The story:
 or The TSA's rebuttal:
Princeton professor:
 or ** *** ***** ******* *********** *************
     News
Remote sensing of meth labs, another NSF grant:
Ridiculous "age verification" for online movie trailers: "It seems like 'We want to protect children' really means, We want to give the appearance that we've made an effort to protect children. If they really wanted to protect children, they wouldn't use the honor system as the sole safeguard standing between previews filled with sex and violence and Internet-savvy kids who can, in a matter of seconds, beat the impotent little system."
Direct marketing meets wholesale surveillance: a $100K National Science Foundation grant:
In 1748, the painter William Hogarth was arrested as a spy for sketching fortifications at Calais.
Sound familiar, doesn't it?
Fogshield: silly home security.
Someone claims to have hacked the Bloomsbury Publishing network, and has posted what he says is the ending to the last Harry Potter book.  I don't believe it, actually. Sure, it's possible -- probably even easy. But the posting just doesn't read right to me.  And I would expect someone who really got their hands on a copy of the manuscript to post the choice bits of text, not just a plot summary. It's easier, and it's more proof.
The French government wants to ban BlackBerry e-mail devices, because of worries of eavesdropping by U.S. intelligence.
 or Vulnerabilities in the DHS network:
TSA uses Monte Carlo simulations to weigh airplane risks
Good comments in the blog post:
The Onion on terrorist cell apathy:
"Cocktail condoms" are protective covers that go over your drink and "protect" against someone trying to slip a Mickey Finn (or whatever they're called these days).  I'm sure there are many ways to defeat this security device if you're so inclined: a syringe, affixing a new cover after you tamper with the drink, and so on.  And this is exactly the sort of rare risk we're likely to overreact to.  But to me, the most interesting aspect of this story is the agenda.  If these things become common, it won't be because of security.  It will be because of advertising
Does this cell phone stalking story seem real to anyone?
 or There's something going on here, but I just don't believe it's entirely cell phone hacking.  Something else is going on.
Really good "Washington Post" article on secrecy:
 or Back in 2002 I wrote about the relationship between secrecy and security.
Surveillance cameras that obscure faces, an interesting privacy-enhancing technology.
At the beach, sand is more deadly than sharks.  And this is important enough to become someone's crusade?
Essay: "The only thing we have to fear is the 'culture of fear' itself," by Frank Furedi.
Making invisible ink printer cartridges: a covert channel.
 or Bioterrorism detection systems and false alarms:
 or Robotic guns:
Airport security: Israel vs. the United States
 or Why an ATM PIN has four digits:
Security cartoon: it's always a trade-off:
Look at the last line of this article, about an Ohio town considering mandatory school uniforms in lower grades:  "For Edgewood, the primary motivation for adopting uniforms would be to enhance school security, York said."  What is he talking about?  Does he think that school uniforms enhance security because it would be easier to spot non-uniform-wearing non-students in the school building and on the grounds?  (Of course, non-students with uniforms would have an easier time sneaking in.)  Or something else?  Or is security just an excuse for any random thing these days?
 or  or Good commentaries on the UK terrorist plots:
 or  or In former East Germany, the Stazi kept samples of people's smells.
The Millwall brick: an improvised weapon made out of newspaper, favored by football (i.e., soccer) hooligans.
When coins are worth more as metal than as coins.
This guy has a bottle taken away from him, then he picks it out of the trash and takes it on the plane anyway.  I'm not sure whether this is more gutsy or stupid.  If he had been caught, the TSA would have made his day pretty damn miserable.  I'm not even sure bragging about it online is a good idea.  Too many idiots in the FBI.
 or I've written about this Greek wiretapping scandal before.  A system to allow the police to eavesdrop on conversations was abused (surprise, surprise).  There's a really good technical analysis in IEEE Spectrum this month.
Police don't overreact to strange object.  What's sad is that it feels like an exception.
 or I'm sure glad the Australian Federal Police have their priorities straight: "Technology such as cloned part-robot humans used by organised crime gangs pose the greatest future challenge to police, along with online scamming, Australian Federal Police (AFP) Commissioner Mick Keelty says."
 or Dan Solove comments on the recent ACLU vs. NSA decision regarding the NSA's illegal wiretapping activities.
Dan Solove on privacy and the "nothing to hide" argument:
Funny airport-security photo:
 at N02/755509753/
** *** ***** ******* *********** *************
     Ubiquity of Communication
In an essay by Randy Farmer, a pioneer of virtual online worlds, he describes communication in something called Disney's ToonTown. Designers of online worlds for children wanted to severely restrict the communication that users could have with each other, lest somebody say something that's inappropriate for children to hear.
Randy discusses various approaches to this problem that were tried over the years.  The ToonTown solution was to restrict users to something called "Speedchat," a menu of pre-constructed sentences, all innocuous.  They also gave users the ability to conduct unrestricted conversations with each other, provided they both knew a secret code string.  The designers presumed the code strings would be passed only to people a user knew in real life, perhaps on a school playground or among neighbors.
Users found ways to pass code strings to strangers anyway.  Users invented several protocols, using gestures, canned sentences, or movement of objects in the game.
  "By hook, or by crook, customers will always find a way to connect with each other."
 or ** *** ***** ******* *********** *************
     4th Amendment Rights Extended to E-Mail
This is a great piece of news in the U.S. For the first time, e-mail has been granted the same constitutional protections as telephone calls and personal papers: the police need a warrant to get at it.  Now it's only a circuit court decision -- the Sixth U.S. Circuit Court of Appeals in Ohio -- it's pretty narrowly defined based on the attributes of the e-mail system, and it has a good chance of being overturned by the Supreme Court...but it's still great news.
The way to think of the warrant system is as a security device.  The police still have the ability to get access to e-mail in order to investigate a crime.  But in order to prevent abuse, they have to convince a neutral third party -- a judge -- that accessing someone's e-mail is necessary to investigate that crime.  That judge, at least in theory, protects our interests.
Clearly e-mail deserves the same protection as our other personal papers, but -- like phone calls -- it might take the courts decades to figure that out.  But we'll get there eventually.
 or  or ** *** ***** ******* *********** *************
     Credit Card Gas Limits
Here's an interesting phenomenon: rising gas costs have pushed up a lot of legitimate transactions to the "anti-fraud" ceiling.
Security is a trade-off, and now the ceiling is annoying more and more legitimate gas purchasers.  But to me the real question is: does this ceiling have any actual security purpose?
In general, credit card fraudsters like making gas purchases because the system is automated: no signature is required, and there's no need to interact with any other person.  In fact, buying gas is the most common way a fraudster tests that a recently stolen card is valid.  The anti-fraud ceiling doesn't actually prevent any of this, but limits the amount of money at risk.
But so what?  How many perps are actually trying to get more gas than is permitted?  Are credit-card-stealing miscreants also swiping cars with enormous gas tanks, or merely filling up the passenger cars they regularly drive?  I'd love to know how many times, prior to the run-up in gas prices, a triggered cutoff actually coincided with a subsequent report of a stolen card.  And what's the effect of a ceiling, apart from a gas shut-off?  Surely the smart criminals know about smurfing, if they need more gas than the ceiling will allow.
The Visa spokesperson said, "We get more calls, questions, when gas prices increase."  He/she didn't say: "We *make* more calls to see if fraud is occurring."  So the only inquiries made may be in the cases where fraud isn't occurring.
 or ** *** ***** ******* *********** *************
     Schneier/BT Counterpane News
Slate wrote an article on my movie-plot threat contest.
** *** ***** ******* *********** *************
     Designing Voting Machines to Minimize Coercion
If someone wants to buy your vote, he'd like some proof that you've delivered the goods.  Camera phones are one way for you to prove to your buyer that you voted the way he wants.  Belgian voting machines have been designed to minimize that risk.
"Once you have confirmed your vote, the next screen doesn't display how you voted. So if one is coerced and has to deliver proof, one just has to take a picture of the vote one was coerced into, and then back out from the screen and change ones vote. The only workaround I see is for the coercer to demand a video of the complete voting process, instead of a picture of the ballot."
The author is wrong that this is an advantage electronic ballots have over paper ballots.  Paper voting systems can be designed with the same security features.
 or ** *** ***** ******* *********** *************
     Risks of Data Reuse
We learned the news in March: Contrary to decades of denials, the U.S. Census Bureau used individual records to round up Japanese-Americans during World War II.
The Census Bureau normally is prohibited by law from revealing data that could be linked to specific individuals; the law exists to encourage people to answer census questions accurately and without fear. And while the Second War Powers Act of 1942 temporarily suspended that protection in order to locate Japanese-Americans, the Census Bureau had maintained that it only provided general information about neighborhoods.
New research proves they were lying.
The whole incident serves as a poignant illustration of one of the thorniest problems of the information age: data collected for one purpose and then used for another, or "data reuse."
When we think about our personal data, what bothers us most is generally not the initial collection and use, but the secondary uses. I personally appreciate it when Amazon.com suggests books that might interest me, based on books I have already bought. I like it that my airline knows what type of seat and meal I prefer, and my hotel chain keeps records of my room preferences. I don't mind that my automatic road-toll collection tag is tied to my credit card, and that I get billed automatically. I even like the detailed summary of my purchases that my credit card company sends me at the end of every year. What I don't want, though, is any of these companies selling that data to brokers, or for law enforcement to be allowed to paw through those records without a warrant.
There are two bothersome issues about data reuse. First, we lose control of our data. In all of the examples above, there is an implied agreement between the data collector and me: It gets the data in order to provide me with some sort of service. Once the data collector sells it to a broker, though, it's out of my hands. It might show up on some telemarketer's screen, or in a detailed report to a potential employer, or as part of a data-mining system to evaluate my personal terrorism risk. It becomes part of my data shadow, which always follows me around but I can never see.
This, of course, affects our willingness to give up personal data in the first place. The reason U.S. census data was declared off-limits for other uses was to placate Americans' fears and assure them that they could answer questions truthfully. How accurate would you be in filling out your census forms if you knew the FBI would be mining the data, looking for terrorists? How would it affect your supermarket purchases if you knew people were examining them and making judgments about your lifestyle? I know many people who engage in data poisoning: deliberately lying on forms in order to propagate erroneous data. I'm sure many of them would stop that practice if they could be sure that the data was only used for the purpose for which it was collected.
The second issue about data reuse is error rates. All data has errors, and different uses can tolerate different amounts of error. The sorts of marketing databases you can buy on the web, for example, are notoriously error-filled. That's OK; if the database of ultra-affluent Americans of a particular ethnicity you just bought has a 10 percent error rate, you can factor that cost into your marketing campaign. But that same database, with that same error rate, might be useless for law enforcement purposes.
Understanding error rates and how they propagate is vital when evaluating any system that reuses data, especially for law enforcement purposes. A few years ago, the Transportation Security Administration's follow-on watch list system, Secure Flight, was going to use commercial data to give people a terrorism risk score and determine how much they were going to be questioned or searched at the airport. People rightly rebelled against the thought of being judged in secret, but there was much less discussion about whether the commercial data from credit bureaus was accurate enough for this application.
An even more egregious example of error-rate problems occurred in 2000, when the Florida Division of Elections contracted with Database Technologies (since merged with ChoicePoint) to remove convicted felons from the voting rolls. The databases used were filled with errors and the matching procedures were sloppy, which resulted in thousands of disenfranchised voters -- mostly black -- and almost certainly changed a presidential election result.
Of course, there are beneficial uses of secondary data. Take, for example, personal medical data. It's personal and intimate, yet valuable to society in aggregate. Think of what we could do with a database of everyone's health information: massive studies examining the long-term effects of different drugs and treatment options, different environmental factors, different lifestyle choices. There's an enormous amount of important research potential hidden in that data, and it's worth figuring out how to get at it without compromising individual privacy.
This is largely a matter of legislation. Technology alone can never protect our rights. There are just too many reasons not to trust it, and too many ways to subvert it. Data privacy ultimately stems from our laws, and strong legal protections are fundamental to protecting our information against abuse. But at the same time, technology is still vital.
Both the Japanese internment and the Florida voting-roll purge demonstrate that laws can change -- and sometimes change quickly. We need to build systems with privacy-enhancing technologies that limit data collection wherever possible. Data that is never collected cannot be reused. Data that is collected anonymously, or deleted immediately after it is used, is much harder to reuse. It's easy to build systems that collect data on everything -- it's what computers naturally do -- but it's far better to take the time to understand what data is needed and why, and only collect that.
History will record what we, here in the early decades of the information age, did to foster freedom, liberty and democracy. Did we build information technologies that protected people's freedoms even during times when society tried to subvert them? Or did we build technologies that could easily be modified to watch and control? It's bad civic hygiene to build an infrastructure that can be used to facilitate a police state.
Individual data and the Japanese internment:
 or  or  or Marketing databases:
Secure Flight:
Florida disenfranchisement in 2000:
This article originally appeared on Wired.com:
 or ** *** ***** ******* *********** *************
     Comments from Readers
There are hundreds of comments -- many of them interesting -- on these topics on my blog. Search for the story you want to comment on, and join ** *** ***** ******* *********** *************
CRYPTO-GRAM is a free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.  You can subscribe, unsubscribe, or change your address on the Web at .  Back issues are also available at that URL.
Please feel free to forward CRYPTO-GRAM, in whole or in part, to colleagues and friends who will find it valuable.  Permission is also granted to reprint CRYPTO-GRAM, as long as it is reprinted in its entirety.
CRYPTO-GRAM is written by Bruce Schneier.  Schneier is the author of the best sellers "Beyond Fear," "Secrets and Lies," and "Applied Cryptography," and an inventor of the Blowfish and Twofish algorithms. He is founder and CTO of BT Counterpane, and is a member of the Board of Directors of the Electronic Privacy Information Center (EPIC).  He is a frequent writer and lecturer on security topics.  See BT Counterpane is the world's leading protector of networked information - the inventor of outsourced security monitoring and the foremost authority on effective mitigation of emerging IT threats.  BT Counterpane protects networks for Fortune 1000 companies and governments world-wide.  See .
Crypto-Gram is a personal newsletter.  Opinions expressed are not necessarily those of BT or BT Counterpane.
Copyright (c) 2007 by Bruce Schneier.

@_date: 2007-06-15 03:11:02
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, June 15, 2007 
CRYPTO-GRAM
                June 15, 2007
              by Bruce Schneier
               Founder and CTO
                BT Counterpane
             schneier at schneier.com
                       A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Rare Risk and Overreactions
     Tactics, Targets, and Objectives
     News
     Portrait of the Modern Terrorist as an Idiot
     Teaching Viruses
     Bush's Watch Stolen?
     Schneier/BT Counterpane News
     Second Movie-Plot Threat Contest Winner
     Perpetual Doghouse: Meganet
     Non-Security Considerations in Security Decisions
     Comments from Readers
** *** ***** ******* *********** *************
     Rare Risk and Overreactions
Everyone had a reaction to the horrific events of the Virginia Tech shootings.  Some of those reactions were rational. Others were not.
A high school student was suspended for customizing a first-person shooter game with a map of his school.  A contractor was fired from his government job for talking about a gun, and then visited by the police when he created a comic about the incident.  A dean at Yale banned realistic stage weapons from the university theaters -- a policy that was reversed within a day.  And some teachers terrorized a sixth-grade class by staging a fake gunman attack, without telling them that it was a drill.
These things all happened, even though shootings like this are incredibly rare; even though -- for all the press -- less than one percent  of homicides and suicides of children ages 5 to 19 occur in schools. In fact, these overreactions occurred, not despite these facts, but *because* of them.
The Virginia Tech massacre is precisely the sort of event we humans tend to overreact to.  Our brains aren't very good at probability and risk analysis, especially when it comes to rare occurrences.  We tend to exaggerate spectacular, strange and rare events, and downplay ordinary, familiar and common ones.  There's a lot of research in the psychological community about how the brain responds to risk -- some of it I have already written about -- but the gist is this: Our brains are much better at processing the simple risks we've had to deal with throughout most of our species' existence, and much poorer at evaluating the complex risks society forces us to face today.
Novelty plus dread equals overreaction.
We can see the effects of this all the time.  We fear being murdered, kidnapped, raped and assaulted by strangers, when it's far more likely that the perpetrator of such offenses is a relative or a friend.  We worry about airplane crashes and rampaging shooters instead of automobile crashes and domestic violence -- both far more common.
In the United States, dogs, snakes, bees and pigs each kill more people per year than sharks.  In fact, dogs kill more humans than any animal except for other humans. Sharks are more dangerous than dogs, yes, but we're far more likely to encounter dogs than sharks.
Our greatest recent overreaction to a rare event was our response to the terrorist attacks of 9/11.  I remember then-Attorney General John Ashcroft giving a speech in Minnesota -- where I live -- in 2003, and claiming that the fact there were no new terrorist attacks since 9/11 was proof that his policies were working.  I thought: "There were no terrorist attacks in the two years preceding 9/11, and you didn't have any policies.  What does that prove?"
What it proves is that terrorist attacks are very rare, and maybe our reaction wasn't worth the enormous expense, loss of liberty, attacks on our Constitution and damage to our credibility on the world stage. Still, overreacting was the natural thing for us to do.  Yes, it's security theater, but it makes us feel safer.
People tend to base risk analysis more on personal story than on data, despite the old joke that "the plural of anecdote is not data."  If a friend gets mugged in a foreign country, that story is more likely to affect how safe you feel traveling to that country than abstract crime We give storytellers we have a relationship with more credibility than strangers, and stories that are close to us more weight than stories from foreign lands.  In other words, proximity of relationship affects our risk assessment.  And who is everyone's major storyteller these days?   Television.  (Nassim Nicholas Taleb's great book, "The Black Swan: The Impact of the Highly Improbable," discusses this.)
Consider the reaction to another event from last month: professional baseball player Josh Hancock got drunk and died in a car crash.  As a result, several baseball teams are banning alcohol in their clubhouses after games.  Aside from this being a ridiculous reaction to an incredibly rare event (2,430 baseball games per season, 35 people per clubhouse, two clubhouses per game.  And how often has this happened?), it makes no sense as a solution.  Hancock didn't get drunk in the clubhouse; he got drunk at a bar.  But Major League Baseball needs to be seen as doing *something*, even if that something doesn't make sense -- even if that something actually increases risk by forcing players to drink at bars instead of at the clubhouse, where there's more control over the practice.
I tell people that if it's in the news, don't worry about it.  The very definition of "news" is "something that hardly ever happens."  It's when something isn't in the news, when it's so common that it's no longer news -- car crashes, domestic violence -- that you should start worrying.
But that's not the way we think.  Psychologist Scott Plous said it well in "The Psychology of Judgment and Decision Making": "In very general terms: (1) The more *available* an event is, the more frequent or probable it will seem; (2) the more *vivid* a piece of information is, the more easily recalled and convincing it will be; and (3) the more *salient* something is, the more likely it will be to appear causal."
So, when faced with a very available and highly vivid event like 9/11 or the Virginia Tech shootings, we overreact.  And when faced with all the salient related events, we assume causality.  We pass the Patriot Act. We think if we give guns out to students, or maybe make it harder for students to get guns, we'll have solved the problem.  We don't let our children go to playgrounds unsupervised.  We stay out of the ocean because we read about a shark attack somewhere.
It's our brains again.  We need to "do something," even if that something doesn't make sense; even if it is ineffective.  And we need to do something directly related to the details of the actual event.  So instead of implementing effective, but more general, security measures to reduce the risk of terrorism, we ban box cutters on airplanes.  And we look back on the Virginia Tech massacre with 20-20 hindsight and recriminate ourselves about the things we *should have done.
Lastly, our brains need to find someone or something to blame.  (Jon Stewart has an excellent bit on the Virginia Tech scapegoat search, and media coverage in general.)  But sometimes there is no scapegoat to be found; sometimes we did everything right, but just got unlucky.  We simply can't prevent a lone nutcase from shooting people at random; there's no security measure that would work.
As circular as it sounds, rare events are rare primarily because they don't occur very often, and not because of any preventive security measures.  And implementing security measures to make these rare events even rarer is like the joke about the guy who stomps around his house to keep the elephants away.
"Elephants?  There are no elephants in this neighborhood," says a neighbor.
"See how well it works!"
If you want to do something that makes security sense, figure out what's common among a bunch of rare events, and concentrate your countermeasures there.  Focus on the general risk of terrorism, and not the specific threat of airplane bombings using liquid explosives.  Focus on the general risk of troubled young adults, and not the specific threat of a lone gunman wandering around a college campus.  Ignore the movie-plot threats, and concentrate on the real risks.
Irrational reactions:
 or Risks of school shootings (from 2000):
Crime statistics -- strangers vs. acquaintances:
 or Me on the psychology of risk and security:
Risk of shark attacks:
Ashcroft speech:
Me on security theater:
Baseball beer ban:
Nicholas Taub essay:
 or VA Tech and gun control:
 or VA Tech hindsight:
Jon Stewart video:
Me on movie-plot threats:
Another opinion:
This essay originally appeared on Wired.com, my 42nd essay on that site.
 or French translation:
** *** ***** ******* *********** *************
     Tactics, Targets, and Objectives
If you encounter an aggressive lion, stare him down. But not a leopard; avoid his gaze at all costs. In both cases, back away slowly; don't run. If you stumble on a pack of hyenas, run and climb a tree; hyenas can't climb trees. But don't do that if you're being chased by an elephant; he'll just knock the tree down. Stand still until he forgets about you.
I spent the last few days on safari in a South African game park, and this was just some of the security advice we were all given. What's interesting about this advice is how well-defined it is. The defenses might not be terribly effective -- you still might get eaten, gored or trampled -- but they're your best hope. Doing something else isn't advised, because animals do the same things over and over again. These are security countermeasures against specific tactics.
Lions and leopards learn tactics that work for them, and I was taught tactics to defend myself. Humans are intelligent, and that means we are more adaptable than animals. But we're also, generally speaking, lazy and stupid; and, like a lion or hyena, we will repeat tactics that work. Pickpockets use the same tricks over and over again. So do phishers, and school shooters. If improvised explosive devices didn't work often enough, Iraqi insurgents would do something else.
So security against people generally focuses on tactics as well.
A friend of mine recently asked me where she should hide her jewelry in her apartment, so that burglars wouldn't find it. Burglars tend to look in the same places all the time -- dresser tops, night tables, dresser drawers, bathroom counters -- so hiding valuables somewhere else is more likely to be effective, especially against a burglar who is pressed for time. Leave decoy cash and jewelry in an obvious place so a burglar will think he's found your stash and then leave. Again, there's no guarantee of success, but it's your best hope.
The key to these countermeasures is to find the pattern: the common attack tactic that is worth defending against. That takes data. A single instance of an attack that didn't work -- liquid bombs, shoe bombs -- or one instance that did -- 9/11 -- is not a pattern. Implementing defensive tactics against them is the same as my safari guide saying: "We've only ever heard of one tourist encountering a lion. He stared it down and survived. Another tourist tried the same thing with a leopard, and he got eaten. So when you see a lion...." The advice I was given was based on thousands of years of collective wisdom from people encountering African animals again and again.
Compare this with the Transportation Security Administration's approach. With every unique threat, TSA implements a countermeasure with no basis to say that it helps, or that the threat will ever recur.
Furthermore, human attackers can adapt more quickly than lions. A lion won't learn that he should ignore people who stare him down, and eat them anyway. But people will learn. Burglars now know the common "secret" places people hide their valuables -- the toilet, cereal boxes, the refrigerator and freezer, the medicine cabinet, under the bed -- and look there. I told my friend to find a different secret place, and to put decoy valuables in a more obvious place.
This is the arms race of security. Common attack tactics result in common countermeasures. Eventually, those countermeasures will be evaded and new attack tactics developed. These, in turn, require new countermeasures. You can easily see this in the constant arms race that is credit card fraud, ATM fraud or automobile theft.
The result of these tactic-specific security countermeasures is to make the attacker go elsewhere. For the most part, the attacker doesn't particularly care about the target. Lions don't care who or what they eat; to a lion, you're just a conveniently packaged bag of protein. Burglars don't care which house they rob, and terrorists don't care who they kill. If your countermeasure makes the lion attack an impala instead of you, or if your burglar alarm makes the burglar rob the house next door instead of yours, that's a win for you.
Tactics matter less if the attacker is after you personally. If, for example, you have a priceless painting hanging in your living room and the burglar knows it, he's not going to rob the house next door instead

@_date: 2008-07-15 02:21:12
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, July 15, 2008 
CRYPTO-GRAM
                July 15, 2008
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     CCTV Cameras
     News
     Kill Switches and Remote Control
     LifeLock and Identity Theft
     Schneier/BT News
     The First Interdisciplinary Workshop on Security and
       Human Behavior
     The Truth About Chinese Hackers
     Man-in-the-Middle Attacks
     Comments from Readers
** *** ***** ******* *********** *************
     CCTV Cameras
Pervasive security cameras don't substantially reduce crime.  There are exceptions, of course, and that's what gets the press.  Most famously, CCTV cameras helped catch James Bulger's murderers in 1993.  And earlier this year, they helped convict Steve Wright of murdering five women in the Ipswich area.  But these are the well-publicized exceptions. Overall, CCTV cameras aren't very effective.
This fact has been demonstrated again and again: by a comprehensive study for the Home Office in 2005, by several studies in the US, and again with new data announced last month by New Scotland Yard.  They actually solve very few crimes, and their deterrent effect is minimal.
Conventional wisdom predicts the opposite.  But if that were true, then camera-happy London, with something like 500,000, would be the safest city on the planet.  It isn't, of course, because of technological limitations of cameras, organizational limitations of police and the adaptive abilities of criminals.
To some, it's comforting to imagine vigilant police monitoring every camera, but the truth is very different. Most CCTV footage is never looked at until well after a crime is committed. When it is examined, it's very common for the viewers not to identify suspects. Lighting is bad and images are grainy, and criminals tend not to stare helpfully at the lens. Cameras break far too often. The best camera systems can still be thwarted by sunglasses or hats.  Even when they afford quick identification -- think of the 2005 London transport bombers and the 9/11 terrorists -- police are often able to identify suspects without the cameras. Cameras afford a false sense of security, encouraging laziness when we need police to be vigilant.
The solution isn't for police to watch the cameras. Unlike an officer walking the street, cameras only look in particular directions at particular locations.  Criminals know this, and can easily adapt by moving their crimes to someplace not watched by a camera -- and there will always be such places.  Additionally, while a police officer on the street can respond to a crime in progress, the same officer in front of a CCTV screen can only dispatch another officer to arrive much later. By their very nature, cameras result in underused and misallocated police Cameras aren't completely ineffective, of course. In certain circumstances, they're effective in reducing crime in enclosed areas with minimal foot traffic.  Combined with adequate lighting, they substantially reduce both personal attacks and auto-related crime in car parks. And from some perspectives, simply moving crime around is good enough. If a local Tesco installs cameras in its store, and a robber targets the store next door as a result, that's money well spent by Tesco. But it doesn't reduce the overall crime rate, so is a waste of money to the township.
But the question really isn't whether cameras reduce crime; the question is whether they're worth it. And given their cost (500 million pounds in the past 10 years), their limited effectiveness, the potential for abuse (spying on naked women in their own homes, sharing nude images, selling best-of videos, and even spying on national politicians) and their Orwellian effects on privacy and civil liberties, most of the time they're not. The funds spent on CCTV cameras would be far better spent on hiring experienced police officers.
We live in a unique time in our society: the cameras are everywhere, and we can still see them. Ten years ago, cameras were much rarer than they are today.  And in 10 years, they'll be so small you won't even notice them.  Already, companies like L-1 Security Solutions are developing police-state CCTV surveillance technologies like facial recognition for China, technology that will find their way into countries like the UK. The time to address appropriate limits on this technology is before the cameras fade from notice.
CCTV research:
 or London's cameras:
 or  or CCTV abuses:
Orwellian cameras:
 or Privacy concerns:
Surveillance in China:
 or A rebuttal:
 or  or More good survey articles:
 or This essay was previously published in The Guardian.
** *** ***** ******* *********** *************
     News
The Storm worm is being used to sell pharmaceuticals such as Viagra.
I've never figured out the fuss over ransomware.  Yes, it encrypts your data and charges you money for the key.  But how is this any worse than the old hacker viruses that put a funny message on your screen and erased your hard drive? The single most important thing any company or individual can do to improve security is have a good backup strategy. It's been true for decades, and it's still true today.
 or Magnetic ring attack on electronic locks: impressive.
A great "security through obscurity" story, about a collection of coins and currency worth hundreds of millions of dollars being moved without a whole lot of security:
It's possible to eavesdrop on encrypted compressed voice, at least a little bit, through traffic analysis:
 or A Jura F90 Coffee Machine can be hacked remotely over the Internet.
A runner-up in last year's Underhanded C Contest was a flawed implementation of RC4 that, after some use, just passed plaintext through unencrypted.  Plausibly deniable, and very clever.
Dilbert on workplace surveillance:
New technology to detect chemical, biological, and explosive agents.
 or Swimming pools around Shanghai are examining liquids by smelling them. This liquid ban has gotten weirder.
A new study claims that insiders aren't the main threat to network security.  The whole insiders vs. outsiders debate has always been one of semantics more than anything else.  If you count by attacks, there are a lot more outsider attacks, simply because there are orders of magnitude more outsider attackers.  If you count incidents, the numbers tend to get closer: 75% vs. 18% in this case.  And if you count damages, insiders generally come out on top -- mostly because they have a lot more detailed information and can target their attacks better.  Both insiders and outsiders are security risks, and you have to defend against them both.  Trying to rank them isn't all that useful.
 or Confused security reasoning by Toronto Mayor David Miller: "'In a day when you can't bring a large tube of toothpaste on a plane how can you allow guns to wander through Union Station, the biggest transit hub in Canada?' he asked his colleagues on city council."  By that logic, I think we can ban anything from anywhere.
 or UK teens are using Google Earth to find swimming pools they can crash. How long before someone finds a more serious crime that can be aided by Google Earth?
I've seen the IR screening guns at several airports, primarily in Asia.  The idea is to keep out people with bird flu, or whatever the current fever scare is.  This essay explains why it won't work:
 or Carrier pigeons bringing contraband into prisons in Brazil:
I think this is the first security vulnerability found in RFC 1149: "Standard for the transmission of IP datagrams on avian carriers." Deep packet inspection seems to be the only way to prevent this attack, although adequate fencing will prevent the protocol from running in the first place.
Top ten anti-terrorism patents -- not a joke.  My favorite is the airplane trap door.
 or The Pentagon is consulting social scientists on security.  The article talks a lot about potential conflicts of interest and such, and less on what sorts of insights the social scientists can offer.  I think there is a lot of potential value here.
One, possibly the only, writer of the Nugache worm was arrested in Wyoming.  The 19-year-old will plead guilty.
 or It's been a while since I've written about electronic voting machines, but Dan Wallach has an excellent blog post about the current line of argument from the voting machine companies and why it's wrong.
This paper measures insecurity in the global population of browsers, using Google's web server logs.  Why is this important?  Because browsers are an increasingly popular attack vector.  The results aren't Random stupidity in the name of terrorism, part one:  An air traveler in Canada is first told by an airline employee that it is "illegal" to say certain words, and then that if she raised a fuss she would be falsely  or Random stupidity in the name of terrorism, part two:  A British man is forced to give up his hobby of photographing buses because he's being harassed too often.
Random stupidity in the name of terrorism, part three:  Israelis label a random homicidal Palestinian nut a terrorist:
Random stupidity in the name of terrorism, part four:  New Jersey public school locked down after someone saw a ninja.  Turns out the ninja was actually a camp counselor dressed in black karate garb and carrying a plastic sword.
 or A fine newspaper headline: "Giraffe helps camels, zebras escape from  or The U.K. is learning that encrypting disks means that you don't have to worry if they're lost.
Time bomb neckties.  Not to be worn at airports.
Automatic profiling is useless:
 or The U.S. wants to do it anyway: "The Justice Department is considering letting the FBI investigate Americans without any evidence of wrongdoing, relying instead on a terrorist profile that could single out Muslims, Arabs or other racial or ethnic groups."
 or I've written about profiling before:
These are sunglasses that hide your face from cameras.  It's either real or a hoax, I can't tell which.
In a continued cheapening of the word "terrorism," the Premier of New South Wales called a potential rail-worker strike "industrial terror tactics."  Terrorism is a heinous crime, and a serious international problem.  It's not a catchall word to describe anything you don't like or don't agree with, or even anything that adversely affects a large number of people.  By using the word more broadly than its actual meaning, we muddy the already complicated popular conceptions of the issue.  The word "terrorism" has a specific meaning, and we shouldn't debase it.
George Carlin on airport security, filmed before 9/11.
Petty thieves are exploiting the "war on photography" to steal memory cards:
Great essay on TSA stupidity:
Security cartoon on password guessing:
 or Daniel Solove on the new FISA law:
Using a file erasure tool is considered suspicious:
Unbreakable fighting umbrellas.
Be sure to watch the video.
** *** ***** ******* *********** *************
     Kill Switches and Remote Control
It used to be that just the entertainment industries wanted to control your computers -- and televisions and iPods and everything else -- to ensure that you didn't violate any copyright rules. But now everyone else wants to get their hooks into your gear.
OnStar will soon include the ability for the police to shut off your engine remotely. Buses are getting the same capability, in case terrorists want to re-enact the movie Speed. The Pentagon wants a kill switch installed on airplanes, and is worried about potential enemies installing kill switches on their own equipment.
Microsoft is doing some of the most creative thinking along these lines, with something it's calling "Digital Manners Policies." According to its patent application, DMP-enabled devices would accept broadcast "orders" limiting their capabilities. Cell phones could be remotely set to vibrate mode in restaurants and concert halls, and be turned off on airplanes and in hospitals. Cameras could be prohibited from taking pictures in locker rooms and museums, and recording equipment could be disabled in theaters. Professors finally could prevent students from texting one another during class.
The possibilities are endless, and very dangerous. Making this work involves building a nearly flawless hierarchical system of authority. That's a difficult security problem even in its simplest form. Distributing that system among a variety of different devices -- computers, phones, PDAs, cameras, recorders -- with different firmware and manufacturers, is even more difficult. Not to mention delegating different levels of authority to various agencies, enterprises, industries and individuals, and then enforcing the necessary safeguards.
Once we go down this path -- giving one device authority over other devices -- the security problems start piling up. Who has the authority to limit functionality of my devices, and how do they get that authority? What prevents them from abusing that power? Do I get the ability to override their limitations? In what circumstances, and how? Can they override my override?
How do we prevent this from being abused? Can a burglar, for example, enforce a "no photography" rule and prevent security cameras from working? Can the police enforce the same rule to avoid another Rodney King incident? Do the police get "superuser" devices that cannot be limited, and do they get "supercontroller" devices that can limit anything? How do we ensure that only they get them, and what do we do when the devices inevitably fall into the wrong hands?
It's comparatively easy to make this work in closed specialized systems

@_date: 2008-10-15 04:30:25
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, October 15, 2008 
CRYPTO-GRAM
               October 15, 2008
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     The Seven Habits of Highly Ineffective Terrorists
     The Two Classes of Airport Contraband
     News
     The More Things Change, the More They Stay the Same
     NSA's Warrantless Eavesdropping Targets Innocent Americans
     Schneier/BT News
     Taleb on the Limitations of Risk Management
     "New Attack" Against Encrypted Images
     Nonviolent Activists Are Now Terrorists
     Does Risk Management Make Sense?
     Comments from Readers
** *** ***** ******* *********** *************
     The Seven Habits of Highly Ineffective Terrorists
Most counterterrorism policies fail, not because of tactical problems, but because of a fundamental misunderstanding of what motivates terrorists in the first place. If we're ever going to defeat terrorism, we need to understand what drives people to become terrorists in the first place.
Conventional wisdom holds that terrorism is inherently political, and that people become terrorists for political reasons. This is the "strategic" model of terrorism, and it's basically an economic model. It posits that people resort to terrorism when they believe -- rightly or wrongly -- that terrorism is worth it; that is, when they believe the political gains of terrorism minus the political costs are greater than if they engaged in some other, more peaceful form of protest. It's assumed, for example, that people join Hamas to achieve a Palestinian state; that people join the PKK to attain a Kurdish national homeland; and that people join al-Qaida to, among other things, get the United States out of the Persian Gulf.
If you believe this model, the way to fight terrorism is to change that equation, and that's what most experts advocate. Governments tend to minimize the political gains of terrorism through a no-concessions policy; the international community tends to recommend reducing the political grievances of terrorists via appeasement, in hopes of getting them to renounce violence. Both advocate policies to provide effective nonviolent alternatives, like free elections.
Historically, none of these solutions has worked with any regularity. Max Abrahms, a predoctoral fellow at Stanford University's Center for International Security and Cooperation, has studied dozens of terrorist groups from all over the world. He argues that the model is wrong. In a paper published this year in International Security that -- sadly -- doesn't have the title "Seven Habits of Highly Ineffective Terrorists," he discusses, well, seven habits of highly ineffective terrorists. These seven tendencies are seen in terrorist organizations all over the world, and they directly contradict the theory that terrorists are political Terrorists, he writes, (1) attack civilians, a policy that has a lousy track record of convincing those civilians to give the terrorists what they want; (2) treat terrorism as a first resort, not a last resort, failing to embrace nonviolent alternatives like elections; (3) don't compromise with their target country, even when those compromises are in their best interest politically; (4) have protean political platforms, which regularly, and sometimes radically, change; (5) often engage in anonymous attacks, which precludes the target countries making political concessions to them; (6) regularly attack other terrorist groups with the same political platform; and (7) resist disbanding, even when they consistently fail to achieve their political objectives or when their stated political objectives have been achieved.
Abrahms has an alternative model to explain all this: People turn to terrorism for social solidarity. He theorizes that people join terrorist organizations worldwide in order to be part of a community, much like the reason inner-city youths join gangs in the United States.
The evidence supports this. Individual terrorists often have no prior involvement with a group's political agenda, and often join multiple terrorist groups with incompatible platforms. Individuals who join terrorist groups are frequently not oppressed in any way, and often can't describe the political goals of their organizations. People who join terrorist groups most often have friends or relatives who are members of the group, and the great majority of terrorist are socially isolated: unmarried young men or widowed women who weren't working prior to joining. These things are true for members of terrorist groups as diverse as the IRA and al-Qaida.
For example, several of the 9/11 hijackers planned to fight in Chechnya, but they didn't have the right paperwork so they attacked America instead. The mujahedeen had no idea whom they would attack after the Soviets withdrew from Afghanistan, so they sat around until they came up with a new enemy: America. Pakistani terrorists regularly defect to another terrorist group with a totally different political platform. Many new al-Qaida members say, unconvincingly, that they decided to become a jihadist after reading an extreme, anti-American blog, or after converting to Islam, sometimes just a few weeks before. These people know little about politics or Islam, and they frankly don't even seem to care much about learning more. The blogs they turn to don't have a lot of substance in these areas, even though more informative blogs do exist.
All of this explains the seven habits. It's not that they're ineffective; it's that they have a different goal. They might not be effective politically, but they are effective socially: They all help preserve the group's existence and cohesion.
This kind of analysis isn't just theoretical; it has practical implications for counterterrorism. Not only can we now better understand who is likely to become a terrorist, we can engage in strategies specifically designed to weaken the social bonds within terrorist organizations. Driving a wedge between group members -- commuting prison sentences in exchange for actionable intelligence, planting more double agents within terrorist groups -- will go a long way to weakening the social bonds within those groups.
We also need to pay more attention to the socially marginalized than to the politically downtrodden, like unassimilated communities in Western countries. We need to support vibrant, benign communities and organizations as alternative ways for potential terrorists to get the social cohesion they need. And finally, we need to minimize collateral damage in our counterterrorism operations, as well as clamping down on bigotry and hate crimes, which just creates more dislocation and social isolation, and the inevitable calls for revenge.
This essay previously appeared on Wired.com.
 or Interesting rebuttal:
** *** ***** ******* *********** *************
     The Two Classes of Airport Contraband
Airport security found a jar of pasta sauce in my luggage last month. It was a 6-ounce jar, above the limit; the official confiscated it, because allowing it on the airplane with me would have been too dangerous. And to demonstrate how dangerous he really thought that jar was, he blithely tossed it in a nearby bin of similar liquid bottles and sent me on my way.
There are two classes of contraband at airport security checkpoints: the class that will get you in trouble if you try to bring it on an airplane, and the class that will cheerily be taken away from you if you try to bring it on an airplane. This difference is important: Making security screeners confiscate anything from that second class is a waste of time. All it does is harm innocents; it doesn't stop terrorists at all.
Let me explain. If you're caught at airport security with a bomb or a gun, the screeners aren't just going to take it away from you. They're going to call the police, and you're going to be stuck for a few hours answering a lot of awkward questions. You may be arrested, and you'll almost certainly miss your flight. At best, you're going to have a very unpleasant day.
This is why articles about how screeners don't catch every -- or even a majority -- of guns and bombs that go through the checkpoints don't bother me. The screeners don't have to be perfect; they just have to be good enough. No terrorist is going to base his plot on getting a gun through airport security if there's a decent chance of getting caught, because the consequences of getting caught are too great.
Contrast that with a terrorist plot that requires a 12-ounce bottle of liquid. There's no evidence that the London liquid bombers actually had a workable plot, but assume for the moment they did. If some copycat terrorists try to bring their liquid bomb through airport security and the screeners catch them -- like they caught me with my bottle of pasta sauce -- the terrorists can simply try again. They can try again and again. They can keep trying until they succeed. Because there are no consequences to trying and failing, the screeners have to be 100 percent effective. Even if they slip up one in a hundred times, the plot can The same is true for knitting needles, pocketknives, scissors, corkscrews, cigarette lighters and whatever else the airport screeners are confiscating this week. If there's no consequence to getting caught with it, then confiscating it only hurts innocent people. At best, it mildly annoys the terrorists.
To fix this, airport security has to make a choice. If something is dangerous, treat it as dangerous and treat anyone who tries to bring it on as potentially dangerous. If it's not dangerous, then stop trying to keep it off airplanes. Trying to have it both ways just distracts the screeners from actually making us safer.
 or  or  or This essay originally appeared on Wired.com.
 or ** *** ***** ******* *********** *************
     News
According to U.S. government documents, fear of terrorism could cause a psychosomatic epidemic:
GPS spoofing:
NSA -- and others -- snooping on cell phone calls with off-the-shelf The NSA teams up with the Chinese government to limit Internet anonymity:
The Pentagon's World of Warcraft Movie-Plot threat:
TSA employees are bypassing airport screening.
This isn't a big deal.  Screeners have to go in and out of security all the time as they work.  Yes, they can smuggle things in and out of the airport.  But you have to remember that the airport screeners are trusted insiders for the system: there are a zillion ways they could break airport security.  On the other hand, it's probably a smart idea to screen screeners when they walk through airport security when they aren't working at that checkpoint at that time.  The reason is the same reason you should screen everyone, including pilots who can crash their plane: you're not screening screeners (or pilots), you're screening people wearing screener (or pilot) uniforms and carrying screener (or pilot) IDs.  You can either train your screeners to recognize authentic uniforms and IDs, or you can just screen everybody.  The latter is just easier.  But this isn't a big deal.
I can think of specific instances where the ability to unlock your door over the Internet can be useful, but in most places it's not a good idea.
 or India using brain scans to prove guilt in court.
The pseudo-science here is even worse than for lie detectors.
People have been asking me to comment about Sarah Palin's Yahoo e-mail account being hacked.  I've already written about the security problems with "secret questions" back in 2005:
More commentary:
 or The $20M camera system at New York's Freedom Tower is pretty sophisticated.
 or We're developing a pre-crime detector that detects hostile thoughts.
 or Spykee is your own personal robot spy.  It takes pictures and movies that you can watch on the Internet in real time or save for later.  You can even talk with whoever you're spying on via Skype.  Only $300.
Security maxims from Roger Johnston.  Funny, and all too true.
Send your personalized message to TSA X-ray screeners using metal plates you can put in your carry-on luggage.
 or Another bomb scare.  Hot dogs this time.
 or The Hackers Choice has released a tool allowing people to clone and modify electronic passports.  The problem is self-signed certificates. A CA is not a great solution, and the link gives a good explanation as to why.  "So what's the solution? We know that humans are good at Border Control. In the end they protected us well for the last 120 years. We also know that humans are good at pattern matching and image recognition. Humans also do an excellent job 'assessing' the person and not just the passport. Take the human part away and passport security falls apart."
 or Hand grenades are now weapons of mass destruction:
MI6 camera -- including secrets -- sold on eBay.  The buyer turned the camera in to the police.
 or  or "Scareware" vendors sued -- it's about time.
 or This is clever: bank robber hires accomplices on Craigslist.
 or New cross-site request forgery attacks.
 or "Clickjacking" is a stunningly sexy name, but the vulnerability is really just a variant of cross-site scripting.  We don't know how bad it really is, because the details are still being withheld.  But the name alone is causing dread.  Here's a good Q&A on the vulnerability:
 or Turns out you can add anyone's number to -- or remove anyone's number from -- the Canadian do-not-call list. You can also add (but not remove) numbers to the U.S. do-not-call list, though only up to three at a time, and you have to provide a valid e-mail address to confirm the addition.  Here's my idea.  If you're a company, add every one of your customers to the list.  That way, none of your competitors will be able to cold call them.
Chinese monitoring Skype messages:
 or According to a massive report from the National Research Council, data mining for terrorists doesn't work.
 or  or Interesting paper by Adam Shostack on threat modeling at Microsoft:
Elcomsoft is claiming that the WPA protocol is dead, just because they can speed up brute-force cracking by 100 times using a hardware accelerator.  Why exactly is this news?  Yes, weak passwords are weak -- we already know that.  And strong WPA passwords are still strong.  This seems like yet another blatant attempt to grab some press attention with a half-baked cryptanalytic result.
Clever counterterrorism attack against the IRA: set up a laundromat, and watch who has bomb residue on their clothes:
There's a new chip-and-pin scam in the UK.  The card readers were hacked when they were built, "either during the manufacturing process at a factory in China, or shortly after they came off the production line." It's being called a "supply chain hack."  Sophisticated stuff, and yet another demonstration that these all-computer security systems are full of risks.
BTW, what's it worth to rig an election?
BART, the San Francisco subway authority, has been debating allowing passengers to bring drinks on trains.  There are all sorts of good reasons why or why not -- convenience, problems with spills, and so on

@_date: 2009-04-15 02:55:52
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, April 15, 2009 
CRYPTO-GRAM
                April 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Fourth Annual Movie-Plot Threat Contest
     Who Should be in Charge of U.S. Cybersecurity?
     News
     Privacy and the Fourth Amendment
     Schneier News
     The Definition of "Weapon of Mass Destruction"
     Stealing Commodities
     Comments from Readers
** *** ***** ******* *********** *************
     Fourth Annual Movie-Plot Threat Contest
Let's face it, the War on Terror is a tired brand.  There just isn't enough action out there to scare people.  If this keeps up, people will forget to be scared.  And then both the terrorists and the terror-industrial complex lose.  We can't have that.
We're going to help revive the fear.  There's plenty to be scared about, if only people would just think about it in the right way.  In this Fourth Movie-Plot Threat Contest, the object is to find an existing event somewhere in the industrialized world -- Third World events are just too easy -- and provide a conspiracy theory to explain how the terrorists were really responsible.
The goal here is to be outlandish but plausible, ridiculous but possible, and -- if it were only true -- terrifying.    Entries should be formatted as a news story, and are limited to 150 words (I'm going to check this time) because fear needs to be instilled in a population with short attention spans.  Submit your entry, by the end of the month, in comments to the blog post.
Submit your entry here:
An example from The Onion:
The First Movie-Plot Threat Contest:
The Second Movie-Plot Threat Contest:
The Third Movie-Plot Threat Contest:
** *** ***** ******* *********** *************
     Who Should be in Charge of U.S. Cybersecurity?
U.S. government cybersecurity is an insecure mess, and fixing it is going to take considerable attention and resources. Trying to make sense of this, President Barack Obama ordered a 60-day review of government cybersecurity initiatives. Meanwhile, the U.S. House Subcommittee on Emerging Threats, Cybersecurity, Science and Technology is holding hearings on the same topic.
One of the areas of contention is who should be in charge. The FBI, DHS and DoD -- specifically, the NSA -- all have interests here. Earlier this month, Rod Beckstrom resigned from his position as director of the DHS's National Cybersecurity Center, warning of a power grab by the NSA.
Putting national cybersecurity in the hands of the NSA is an incredibly bad idea. An entire parade of people, ranging from former FBI director Louis Freeh to Microsoft's Trusted Computing Group Vice President and former Justice Department computer crime chief Scott Charney, have told Congress the same thing at this month's hearings.
Cybersecurity isn't a military problem, or even a government problem -- it's a universal problem. All networks, military, government, civilian and commercial, use the same computers, the same networking hardware, the same Internet protocols and the same software packages. We all are the targets of the same attack tools and tactics. It's not even that government targets are somehow more important; these days, most of our nation's critical IT infrastructure is in commercial hands. Government-sponsored Chinese hackers go after both military and civilian Some have said that the NSA should be in charge because it has specialized knowledge. Earlier this month, Director of National Intelligence Admiral Dennis Blair made this point, saying "There are some wizards out there at Ft. Meade who can do stuff." That's probably not true, but if it is, we'd better get them out of Ft. Meade as soon as possible -- they're doing the nation little good where they are now.
Not that government cybersecurity failings require any specialized wizardry to fix. GAO reports indicate that government problems include insufficient access controls, a lack of encryption where necessary, poor network management, failure to install patches, inadequate audit procedures, and incomplete or ineffective information security programs. These aren't super-secret NSA-level security issues; these are the same managerial problems that every corporate CIO wrestles with.
We've all got the same problems, so solutions must be shared. If the government has any clever ideas to solve its cybersecurity problems, certainly a lot of us could benefit from those solutions. If it has an idea for improving network security, it should tell everyone. The best thing the government can do for cybersecurity world-wide is to use its buying power to improve the security of the IT products everyone uses. If it imposes significant security requirements on its IT vendors, those vendors will modify their products to meet those requirements. And those same products, now with improved security, will become available to all of us as the new standard.
Moreover, the NSA's dual mission of providing security and conducting surveillance means it has an inherent conflict of interest in cybersecurity. Inside the NSA, this is called the "equities issue." During the Cold War, it was easy; the NSA used its expertise to protect American military information and communications, and eavesdropped on Soviet information and communications. But what happens when both the good guys the NSA wants to protect, and the bad guys the NSA wants to eavesdrop on, use the same systems? They all use Microsoft Windows, Oracle databases, Internet email, and Skype. When the NSA finds a vulnerability in one of those systems, does it alert the manufacturer and fix it -- making both the good guys and the bad guys more secure? Or does it keep quiet about the vulnerability and not tell anyone -- making it easier to spy on the bad guys but also keeping the good guys insecure? Programs like the NSA's warrantless wiretapping program have created additional vulnerabilities in our domestic telephone networks.
Testifying before Congress earlier this month, former DHS National Cyber Security division head Amit Yoran said "the intelligence community has always and will always prioritize its own collection efforts over the defensive and protection mission of our government's and nation's digital systems."
Maybe the NSA could convince us that it's putting cybersecurity first, but its culture of secrecy will mean that any decisions it makes will be suspect. Under current law, extended by the Bush administration's extravagant invocation of the "state secrets" privilege when charged with statutory and constitutional violations, the NSA's activities are not subject to any meaningful public oversight. And the NSA's tradition of military secrecy makes it harder for it to coordinate with other government IT departments, most of which don't have clearances, let alone coordinate with local law enforcement or the commercial sector.
We need transparent and accountable government processes, using commercial security products. We need government cybersecurity programs that improve security for everyone. The NSA certainly has an advisory and a coordination role in national cybersecurity, and perhaps a more supervisory role in DoD cybersecurity -- both offensive and defensive -- but it should not be in charge.
A copy of this essay, with all embedded links, is here:
A version of this essay appeared on The Wall Street Journal website.
** *** ***** ******* *********** *************
     News
Privacy in Google Latitude: good news.
Leaving infants in the car.  It happens, and sometimes they die.
Interesting piece of cryptographic history: a cipher designed by Robert Patterson and sent to Thomas Jefferson in 1801.
The Bayer company is refusing to talk about a fatal accident at a West Virginia plant, citing a 2002 terrorism law.
The meeting has been rescheduled.  No word on how forthcoming Bayer will be.
 or Research on fingerprinting paper:
 or Blowfish on the television series 24, again:
Interesting analysis of why people steal rare books.
Last month, I linked to a catalog of NSA video courses from 1991. Here's an update, with new information (the FOIA redactions were appealed).
 or You just can't make this stuff up: a UK bomb squad is called in because someone saw a plastic replica of the Holy Hand Grenade of Antioch, from the movie Monty Python and the Holy Grail.
Interesting research in explosives detection.
A Psychology Today article on fear and the availability heuristic:
 or fraud in the U.S. using electronic voting machines (there have been lots of documented cases of errors and voting problems, but this one involves actual maliciousness).  Lots of details; well worth reading.
Sniffing keyboard keystrokes with a laser:
Where you stand matters in surviving a suicide bombing.
Presumably they also discovered where the attacker should stand to be as lethal as possible, but there's no indication they published those results.
An impressive solar plasma movie-plot threat.
 or Security fears drive Iran to Linux:
A gorilla detector, from Muppet Labs.
Bob Blakley makes an interesting point about what he calls "the zone of essential risk": "if you conduct medium-sized transactions rarely, you're in trouble. The transactions are big enough so that you care about losses, you don't have enough transaction volume to amortize those losses, and the cost of insurance or escrow is high enough compared to the value of your transactions that it doesn't make economic sense to protect yourself."
Massive Chinese espionage network discovered:
Thefts at the Museum of Bad Art:
Be sure to notice the camera:
Here's a story about a very expensive series of false positives.  The German police spent years and millions of dollars tracking a mysterious killer whose DNA had been found at the scenes of six murders.  Finally they realized they were tracking a worker at the factory that assembled the prepackaged swabs used for DNA testing.
 or This story could be used as justification for a massive DNA database. After all, if that factory worker had his or her DNA in the database, the police would have quickly realized what the problem was.
Identifying people using anonymous social networking data:
What to fear: a great rundown of the statistics.
Crypto puzzle and NSA problem:
Clever social networking identity theft scams:
Police powers and the UK government in the 1980s:
Research into preserving P2P privacy:
Fact-free article about foreign companies hacking the U.S. power grid suggests we panic.  My guess is that it was deliberately planted by someone looking for leverage in the upcoming budget battle.
Here's a tip: when walking around in public with secret government documents, put them in an envelope.  Don't carry them in the open where people can read (and photograph) them.
Details of the arrests made in haste after the above disclosure:
It is a measure of our restored sanity that no one has called the TSA about Tweenbots:
How to write a scary cyberterrorism story.  From Foreign Affairs, no less.
** *** ***** ******* *********** *************
     Privacy and the Fourth Amendment
In the United States, the concept of "expectation of privacy" matters because it's the constitutional test, based on the Fourth Amendment, that governs when and how the government can invade your privacy.
Based on the 1967 Katz v. United States Supreme Court decision, this test actually has two parts. First, the government's action can't contravene an individual's subjective expectation of privacy; and second, that expectation of privacy must be one that society in general recognizes as reasonable. That second part isn't based on anything like polling data; it is more of a normative idea of what level of privacy people should be allowed to expect, given the competing importance of personal privacy on one hand and the government's interest in public safety on the other.
The problem is, in today's information society, that definition test will rapidly leave us with no privacy at all.
In Katz, the Court ruled that the police could not eavesdrop on a phone call without a warrant: Katz expected his phone conversations to be private and this expectation resulted from a reasonable balance between personal privacy and societal security. Given NSA's large-scale warrantless eavesdropping, and the previous administration's continual insistence that it was necessary to keep America safe from terrorism, is it still reasonable to expect that our phone conversations are private?
Between the NSA's massive internet eavesdropping program and Gmail's content-dependent advertising, does anyone actually expect their e-mail to be private? Between calls for ISPs to retain user data and companies serving content-dependent web ads, does anyone expect their web browsing to be private? Between the various computer-infecting malware, and world governments increasingly demanding to see laptop data at borders, hard drives are barely private. I certainly don't believe that my SMSs, any of my telephone data, or anything I say on LiveJournal or Facebook -- regardless of the privacy settings -- is private.
Aerial surveillance, data mining, automatic face recognition, terahertz radar that can "see" through walls, wholesale surveillance, brain scans, RFID, "life recorders" that save everything: Even if society still has some small expectation of digital privacy, that will change as these and other technologies become ubiquitous. In short, the problem with a normative expectation of privacy is that it changes with perceived threats, technology and large-scale abuses.
Clearly, something has to change if we are to be left with any privacy at all. Three legal scholars have written law review articles that wrestle with the problems of applying the Fourth Amendment to cyberspace and to our computer-mediated world in general.
George Washington University's Daniel Solove, who blogs at Concurring Opinions, has tried to capture the Byzantine complexities of modern privacy. He points out, for example, that the following privacy violations -- all real -- are very different: A company markets a list of 5 million elderly incontinent women; reporters deceitfully gain entry to a person's home and secretly photograph and record the person; the government uses a thermal sensor device to detect heat patterns in a person's home; and a newspaper reports the name of a rape victim. Going beyond simple definitions such as the divulging of a secret, Solove has developed a taxonomy of privacy, and the harms that result from their His 16 categories are: surveillance, interrogation, aggregation, identification, insecurity, secondary use, exclusion, breach of confidentiality, disclosure, exposure, increased accessibility, blackmail, appropriation, distortion, intrusion and decisional interference. Solove's goal is to provide a coherent and comprehensive understanding of what is traditionally an elusive and hard-to-explain concept: privacy violations. (This taxonomy is also discussed in Solove's book, Understanding Privacy.)
Orin Kerr, also a law professor at George Washington University, and a blogger at Volokh Conspiracy, has attempted to lay out general principles for applying the Fourth Amendment to the internet. First, he points out that the traditional inside/outside distinction -- the police can watch you in a public place without a warrant, but not in your home

@_date: 2009-08-15 02:24:22
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, August 15, 2009 
CRYPTO-GRAM
               August 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Risk Intuition
     Privacy Salience and Social Networking Sites
     Building in Surveillance
     News
     Laptop Security while Crossing Borders
     Self-Enforcing Protocols
     Schneier News
     Another New AES Attack
     Lockpicking and the Internet
     Comments from Readers
** *** ***** ******* *********** *************
     Risk Intuition
People have a natural intuition about risk, and in many ways it's very good. It fails at times due to a variety of cognitive biases, but for normal risks that people regularly encounter, it works surprisingly well: often better than we give it credit for. This struck me as I listened to yet another conference presenter complaining about security awareness training. He was talking about the difficulty of getting employees at his company to actually follow his security policies: encrypting data on memory sticks, not sharing passwords, not logging in from untrusted wireless networks. "We have to make people understand the risks," he said.
It seems to me that his co-workers understand the risks better than he does. They know what the real risks are at work, and that they all revolve around not getting the job done. Those risks are real and tangible, and employees feel them all the time. The risks of not following security procedures are much less real. Maybe the employee will get caught, but probably not. And even if he does get caught, the penalties aren't serious.
Given this accurate risk analysis, any rational employee will regularly circumvent security to get his or her job done. That's what the company rewards, and that's what the company actually wants.
"Fire someone who breaks security procedure, quickly and publicly," I suggested to the presenter. "That'll increase security awareness faster than any of your posters or lectures or newsletters." If the risks are real, people will get it.
You see the same sort of risk intuition on motorways. People are less careful about posted speed limits than they are about the actual speeds police issue tickets for. It's also true on the streets: people respond to real crime rates, not public officials proclaiming that a neighborhood is safe.
The warning stickers on ladders might make you think the things are considerably riskier than they are, but people have a good intuition about ladders and ignore most of the warnings. (This isn't to say that some people don't do stupid things around ladders, but for the most part they're safe. The warnings are more about the risk of lawsuits to ladder manufacturers than risks to people who climb ladders.)
As a species, we are naturally tuned in to the risks inherent in our environment. Throughout our evolution, our survival depended on making reasonably accurate risk management decisions intuitively, and we're so good at it, we don't even realize we're doing it.
Parents know this. Children have surprisingly perceptive risk intuition. They know when parents are serious about a threat and when their threats are empty. And they respond to the real risks of parental punishment, not the inflated risks based on parental rhetoric. Again, awareness training lectures don't work; there have to be real consequences.
It gets even weirder. The University College London professor John Adams popularized the metaphor of a mental risk thermostat. We tend to seek some natural level of risk, and if something becomes less risky, we tend to make it more risky. Motorcycle riders who wear helmets drive faster than riders who don't.
Our risk thermostats aren't perfect (that newly helmeted motorcycle rider will still decrease his overall risk) and will tend to remain within the same domain (he might drive faster, but he won't increase his risk by taking up smoking), but in general, people demonstrate an innate and finely tuned ability to understand and respond to risks.
Of course, our risk intuition fails spectacularly and often, with regards to rare risks, unknown risks, voluntary risks, and so on. But when it comes to the common risks we face every day -- the kinds of risks our evolutionary survival depended on -- we're pretty good.
So whenever you see someone in a situation who you think doesn't understand the risks, stop first and make sure you understand the risks. You might be surprised.
This essay previously appeared in The Guardian.
 or Risk thermostat:
 or Failures in risk intuition
** *** ***** ******* *********** *************
     Privacy Salience and Social Networking Sites
Reassuring people about privacy makes them more, not less, concerned. It's called "privacy salience," and Leslie John, Alessandro Acquisti, and George Loewenstein -- all at Carnegie Mellon University -- demonstrated this in a series of clever experiments. In one, subjects completed an online survey consisting of a series of questions about their academic behavior -- "Have you ever cheated on an exam?" for example. Half of the subjects were first required to sign a consent warning -- designed to make privacy concerns more salient -- while the other half did not. Also, subjects were randomly assigned to receive either a privacy confidentiality assurance, or no such assurance. When the privacy concern was made salient (through the consent warning), people reacted negatively to the subsequent confidentiality assurance and were less likely to reveal personal information.
In another experiment, subjects completed an online survey where they were asked a series of personal questions, such as "Have you ever tried cocaine?" Half of the subjects completed a frivolous-looking survey -- How BAD are U??" -- with a picture of a cute devil. The other half completed the same survey with the title "Carnegie Mellon University Survey of Ethical Standards," complete with a university seal and official privacy assurances. The results showed that people who were reminded about privacy were less likely to reveal personal information than those who were not.
Privacy salience does a lot to explain social networking sites and their attitudes towards privacy. From a business perspective, social networking sites don't want their members to exercise their privacy rights very much. They want members to be comfortable disclosing a lot of data about themselves.
Joseph Bonneau and Soeren Preibusch of Cambridge University have been studying privacy on 45 popular social networking sites around the world. (You may not have realized that there *are* 45 popular social networking sites around the world.) They found that privacy settings were often confusing and hard to access; Facebook, with its 61 privacy settings, is the worst. To understand some of the settings, they had to create accounts with different settings so they could compare the results. Privacy tends to increase with the age and popularity of a site. General-use sites tend to have more privacy features than niche sites.
But their most interesting finding was that sites consistently hide any mentions of privacy. Their splash pages talk about connecting with friends, meeting new people, sharing pictures: the benefits of disclosing personal data.
These sites do talk about privacy, but only on hard-to-find privacy policy pages. There, the sites give strong reassurances about their privacy controls and the safety of data members choose to disclose on the site. There, the sites display third-party privacy seals and other icons designed to assuage any fears members have.
It's the Carnegie Mellon experimental result in the real world. Users care about privacy, but don't really think about it day to day. The social networking sites don't want to remind users about privacy, even if they talk about it positively, because any reminder will result in users remembering their privacy fears and becoming more cautious about sharing personal data. But the sites also need to reassure those "privacy fundamentalists" for whom privacy is always salient, so they have very strong pro-privacy rhetoric for those who take the time to search them out. The two different marketing messages are for two different audiences.
Social networking sites are improving their privacy controls as a result of public pressure. At the same time, there is a counterbalancing business pressure to decrease privacy; watch what's going on right now on Facebook, for example. Naively, we should expect companies to make their privacy policies clear to allow customers to make an informed choice. But the marketing need to reduce privacy salience will frustrate market solutions to improve privacy; sites would much rather obfuscate the issue than compete on it as a feature.
This essay originally appeared in the Guardian.
 or Privacy experiments:
Privacy and social networking sites:
 or ** *** ***** ******* *********** *************
     Building in Surveillance
China is the world's most successful Internet censor. While the Great Firewall of China isn't perfect, it effectively limits information flowing in and out of the country. But now the Chinese government is taking things one step further.
Under a requirement taking effect soon, every computer sold in China will have to contain the Green Dam Youth Escort software package. Ostensibly a pornography filter, it is government spyware that will watch every citizen on the Internet.
Green Dam has many uses. It can police a list of forbidden Web sites. It can monitor a user's reading habits. It can even enlist the computer in some massive botnet attack, as part of a hypothetical future cyberwar.
China's actions may be extreme, but they're not unique. Democratic governments around the world -- Sweden, Canada and the United Kingdom, for example -- are rushing to pass laws giving their police new powers of Internet surveillance, in many cases requiring communications system providers to redesign products and services they sell.
Many are passing data retention laws, forcing companies to keep information on their customers. Just recently, the German government proposed giving itself the power to censor the Internet.
The United States is no exception. The 1994 CALEA law required phone companies to facilitate FBI eavesdropping, and since 2001, the NSA has built substantial eavesdropping systems in the United States. The government has repeatedly proposed Internet data retention laws, allowing surveillance into past activities as well as present.
Systems like this invite criminal appropriation and government abuse. New police powers, enacted to fight terrorism, are already used in situations of normal crime. Internet surveillance and control will be no Official misuses are bad enough, but the unofficial uses worry me more. Any surveillance and control system must itself be secured. An infrastructure conducive to surveillance and control invites surveillance and control, both by the people you expect and by the people you don't.
China's government designed Green Dam for its own use, but it's been subverted. Why does anyone think that criminals won't be able to use it to steal bank account and credit card information, use it to launch other attacks, or turn it into a massive spam-sending botnet?
Why does anyone think that only authorized law enforcement will mine collected Internet data or eavesdrop on phone and IM conversations?
These risks are not theoretical. After 9/11, the National Security Agency built a surveillance infrastructure to eavesdrop on telephone calls and e-mails within the United States.
Although procedural rules stated that only non-Americans and international phone calls were to be listened to, actual practice didn't always match those rules. NSA analysts collected more data than they were authorized to, and used the system to spy on wives, girlfriends, and famous people such as President Clinton.
But that's not the most serious misuse of a telecommunications surveillance infrastructure.  In Greece, between June 2004 and March 2005, someone wiretapped more than 100 cell phones belonging to members of the Greek government -- the prime minister and the ministers of defense, foreign affairs and justice.
Ericsson built this wiretapping capability into Vodafone's products, and enabled it only for governments that requested it. Greece wasn't one of those governments, but someone still unknown -- a rival political party? organized crime? -- figured out how to surreptitiously turn the feature on.
Researchers have already found security flaws in Green Dam that would allow hackers to take over the computers. Of course there are additional flaws, and criminals are looking for them.
Surveillance infrastructure can be exported, which also aids totalitarianism around the world. Western companies like Siemens, Nokia, and Secure Computing built Iran's surveillance infrastructure. U.S. companies helped build China's electronic police state. Twitter's anonymity saved the lives of Iranian dissidents -- anonymity that many governments want to eliminate.
Every year brings more Internet censorship and control -- not just in countries like China and Iran, but in the United States, the United Kingdom, Canada and other free countries.
The control movement is egged on by both law enforcement, trying to catch terrorists, child pornographers and other criminals, and by media companies, trying to stop file sharers.
It's bad civic hygiene to build technologies that could someday be used to facilitate a police state. No matter what the eavesdroppers and censors say, these systems put us all at greater risk.  Communications systems that have no inherent eavesdropping capabilities are more secure than systems with those capabilities built in.
This essay previously appeared -- albeit with fewer links -- on the Minnesota Public Radio website.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     News
Data can leak through power lines; the NSA has known about this for decades:
These days, there's a lot of open research on side channels.
South Africa takes its security seriously.  Here's an ATM that automatically squirts pepper spray into the faces of "people tampering with the card slots." Sounds cool, but these kinds of things are all about false positives:
 or Cybercrime paper: "Distributed Security: A New Model of Law Enforcement," Susan W. Brenner and Leo L. Clarke.  It's from 2005, but I'd never seen it before.
Cryptography has zero-knowledge proofs, where Alice can prove to Bob that she knows something without revealing it to Bob.  Here's something similar from the real world.  It's a research project to allow weapons inspectors from one nation to verify the disarming of another nation's nuclear weapons without learning any weapons secrets in the process, such as the amount of nuclear material in the weapon.
I wrote about mapping drug use by testing sewer water in 2007, but there's new research:
Excellent article detailing the Twitter attack.
 or Social Security numbers are not random.  In some cases, you can predict them with date and place of birth.
 or I don't see any new insecurities here.  We already know that Social Security numbers are not secrets.  And anyone who wants to steal a million SSNs is much more likely to break into one of the gazillion databases out there that store them.
NIST has announced the 14 SHA-3 candidates that have advanced to the second round: BLAKE, Blue Midnight Wish, CubeHash, ECHO, Fugue, Grostl, Hamsi, JH, Keccak, Luffa, Shabal, SHAvite-3, SIMD, and Skein.  In February, I chose my favorites: Arirang, BLAKE, Blue Midnight Wish, ECHO, Grostl, Keccak, LANE, Shabal, and Skein.  Of the ones NIST eventually chose, I am most surprised to see CubeHash and most surprised not to see LANE.
Nice description of the base rate fallacy.
This is funny: "Tips for Staying Safe Online":
Seems like the Swiss may be running out of secure gold storage.  If this is true, it's a real security issue.  You can't just store the stuff behind normal locks.  Building secure gold storage takes time and money.
 or I am reminded of a related problem the EU had during the transition to the euro: where to store all the bills and coins before the switchover date.  There wasn't enough vault space in banks, because the vast majority of currency is in circulation.  It's a similar problem, although the EU banks could solve theirs with lots of guards, because it was only a temporary problem.
A large sign saying "United States" at a border crossing was deemed a security risk:
Clever new real estate scam:
Bypassing the iPhone's encryption.  I want more technical details.
Excellent essay by Jonathan Zittrain on the risks of cloud computing:
Here's me on cloud computing:
More fearmongering.  The headline is "Terrorists could use internet to launch nuclear attack: report."  The subhead: "The risk of cyber-terrorism escalating to a nuclear strike is growing daily, according to a study."
 or Note the weasel words in the article.  The study "suggests that under the right circumstances."  We're "leaving open the possibility."  The report "outlines a number of potential threats and situations" where the bad guys could "make a nuclear attack more likely."  Gadzooks.  I'm tired of this idiocy.  Stop overreacting to rare risks.  Refuse to be terrorized, people.
Interesting TED talk by Eve Ensler on security.  She doesn't use any of the terms, but in the beginning she's echoing a lot of the current thinking about evolutionary psychology and how it relates to security.
In cryptography, we've long used the term "snake oil" to refer to crypto systems with good marketing hype and little actual security.  It's the phrase I generalized into "security theater."  Well, it turns out that there really is a snake oil salesman.
 or Research that proves what we already knew:  too many security warnings results in complacency.
The New York Times has an editorial on regulating chemical plants.
The problem is a classic security externality, which I wrote about in 2007.
Good essay on security vs. usability: "When Security Gets in the Way."
A 1934 story from the International Herald Tribune shows how we reacted to the unexpected 75 years ago:
New airport security hole: funny.
Here's some complicated advice on securing passwords that -- I'll bet -- no one follows. Of the ten rules, I regularly break seven.  How about you?
 or Here's my advice on choosing secure passwords.
 or "An Ethical Code for Intelligence Officers"
Man-in-the-middle trucking attack:
"On Locational Privacy, and How to Avoid Losing it Forever"
** *** ***** ******* *********** *************
     Laptop Security while Crossing Borders
Last year, I wrote about the increasing propensity for governments, including the U.S. and Great Britain, to search the contents of people's laptops at customs. What we know is still based on anecdote, as no country has clarified the rules about what their customs officers are and are not allowed to do, and what rights people have.
Companies and individuals have dealt with this problem in several ways, from keeping sensitive data off laptops traveling internationally, to storing the data -- encrypted, of course -- on websites and then downloading it at the destination. I have never liked either solution. I do a lot of work on the road, and need to carry all sorts of data with me all the time. It's a lot of data, and downloading it can take a long time. Also, I like to work on long international flights.
There's another solution, one that works with whole-disk encryption products like PGP Disk (I'm on PGP's advisory board), TrueCrypt, and BitLocker: Encrypt the data to a key you don't know.
It sounds crazy, but stay with me. Caveat: Don't try this at home if you're not very familiar with whatever encryption product you're using. Failure results in a bricked computer. Don't blame me.
Step One: Before you board your plane, add another key to your whole-disk encryption (it'll probably mean adding another "user") -- and make it random. By "random," I mean really random: Pound the keyboard for a while, like a monkey trying to write Shakespeare. Don't make it memorable. Don't even try to memorize it.
Technically, this key doesn't directly encrypt your hard drive. Instead, it encrypts the key that is used to encrypt your hard drive -- that's how the software allows multiple users.
So now there are two different users named with two different keys: the one you normally use, and some random one you just invented.
Step Two: Send that new random key to someone you trust. Make sure the trusted recipient has it, and make sure it works. You won't be able to recover your hard drive without it.
Step Three: Burn, shred, delete or otherwise destroy all copies of that new random key. Forget it. If it was sufficiently random and non-memorable, this should be easy.
Step Four: Board your plane normally and use your computer for the whole Step Five: Before you land, delete the key you normally use.
At this point, you will not be able to boot your computer. The only key remaining is the one you forgot in Step Three. There's no need to lie to the customs official, which in itself is often a crime; you can even show him a copy of this article if he doesn't believe you.
Step Six: When you're safely through customs, get that random key back from your confidant, boot your computer and re-add the key you normally use to access your hard drive.
And that's it.
This is by no means a magic get-through-customs-easily card. Your computer might be impounded, and you might be taken to court and compelled to reveal who has the random key.
But the purpose of this protocol isn't to prevent all that; it's just to deny any possible access to your computer to customs. You might be delayed. You might have your computer seized. (This will cost you any work you did on the flight, but -- honestly -- at that point that's the least of your troubles.) You might be turned back or sent home. But when you're back home, you have access to your corporate management, your personal attorneys, your wits after a good night's sleep, and all the rights you normally have in whatever country you're now in.
This procedure not only protects you against the warrantless search of your data at the border, it also allows you to deny a customs official your data without having to lie or pretend -- which itself is often a crime.
Now the big question: Who should you send that random key to?
Certainly it should be someone you trust, but -- more importantly -- it should be someone with whom you have a privileged relationship. Depending on the laws in your country, this could be your spouse, your attorney, your business partner or your priest. In a larger company, the IT department could institutionalize this as a policy, with the help desk acting as the key holder.
You could also send it to yourself, but be careful. You don't want to e-mail it to your webmail account, because then you'd be lying when you tell the customs official that there is no possible way you can decrypt the drive.
You could put the key on a USB drive and send it to your destination, but there are potential failure modes. It could fail to get there in time to be waiting for your arrival, or it might not get there at all. You could airmail the drive with the key on it to yourself a couple of times, in a couple of different ways, and also fax the key to yourself ... but that's more work than I want to do when I'm traveling.
If you only care about the return trip, you can set it up before you return. Or you can set up an elaborate one-time pad system, with identical lists of keys with you and at home: Destroy each key on the list you have with you as you use it.
Remember that you'll need to have full-disk encryption, using a product such as PGP Disk, TrueCrypt or BitLocker, already installed and enabled to make this work.
I don't think we'll ever get to the point where our computer data is safe when crossing an international border. Even if countries like the U.S. and Britain clarify their rules and institute privacy protections, there will always be other countries that will exercise greater latitude with their authority. And sometimes protecting your data means protecting your data from yourself.
This essay originally appeared on Wired.com.
 or ** *** ***** ******* *********** *************
     Self-Enforcing Protocols
There are several ways two people can divide a piece of cake in half. One way is to find someone impartial to do it for them.  This works, but it requires another person.  Another way is for one person to divide the piece, and the other person to complain (to the police, a judge, or his parents) if he doesn't think it's fair.  This also works, but still requires another person -- at least to resolve disputes.  A third way is for one person to do the dividing, and for the other person to choose the half he wants.
That third way, known by kids, pot smokers, and everyone else who needs to divide something up quickly and fairly, is called cut-and-choose. People use it because it's a self-enforcing protocol: a protocol designed so that neither party can cheat.
Self-enforcing protocols are useful because they don't require trusted third parties.  Modern systems for transferring money -- checks, credit cards, PayPal -- require trusted intermediaries like banks and credit card companies to facilitate the transfer.  Even cash transfers require a trusted government to issue currency, and they take a cut in the form of seigniorage.  Modern contract protocols require a legal system to resolve disputes. Modern commerce wasn't possible until those systems were in place and generally trusted, and complex business contracts still aren't possible in areas where there is no fair judicial system. Barter is a self-enforcing protocol: nobody needs to facilitate the transaction or resolve disputes.  It just works.
Self-enforcing protocols are safer than other types because participants don't gain an advantage from cheating.  Modern voting systems are rife with the potential for cheating, but an open show of hands in a room -- one that everyone in the room can count for himself -- is self-enforcing.  On the other hand, there's no secret ballot, late voters are potentially subjected to coercion, and it doesn't scale well to large elections.  But there are mathematical election protocols that have self-enforcing properties, and some cryptographers have suggested their use in elections.
Here's a self-enforcing protocol for determining property tax: the homeowner decides the value of the property and calculates the resultant tax, and the government can either accept the tax or buy the home for that price.  Sounds unrealistic, but the Greek government implemented exactly that system for the taxation of antiquities.  It was the easiest way to motivate people to accurately report the value of antiquities. And shotgun clauses in contracts are essentially the same thing.
A VAT, or value-added tax, is a self-enforcing alternative to sales tax.  Sales tax is collected on the entire value of the thing at the point of retail sale; both the customer and the storeowner want to cheat the government.  But VAT is collected at every step between raw materials and that final customer; it's the difference between the price of the materials sold and the materials bought.  Buyers wants official receipts with as high a purchase price as possible, so each buyer along the chain keeps each seller honest. Yes, there's still an incentive to cheat on the final sale to the customer, but the amount of tax collected at that point is much lower.
Of course, self-enforcing protocols aren't perfect.  For example, someone in a cut-and-choose can punch the other guy and run away with the entire piece of cake.  But perfection isn't the goal here; the goal is to reduce cheating by taking away potential avenues of cheating. Self-enforcing protocols improve security not by implementing countermeasures that prevent cheating, but by leveraging economic incentives so that the parties don't want to cheat.
One more self-enforcing protocol.  Imagine a pirate ship that encounters a storm.  The pirates are all worried about their gold, so they put their personal bags of gold in the safe.  During the storm, the safe cracks open, and all the gold mixes up and spills out on the floor.  How do the pirates determine who owns what?  They each announce to the group how much gold they had.  If the total of all the announcements matches what's in the pile, it's divided as people announced.  If it's different, then the captain keeps it all.  I can think of all kinds of ways this can go wrong -- the captain and one pirate can collude to throw off the total, for example -- but it is self-enforcing against individual misreporting.
This essay originally appeared on ThreatPost.
** *** ***** ******* *********** *************
     Schneier News
I am speaking at the OWASP meeting in Minneapolis on August 24:
Audio from my Black Hat talk is here:
 or ** *** ***** ******* *********** *************
     Another New AES Attack
A new and very impressive attack against AES has just been announced.
Over the past couple of months, there have been two new cryptanalysis papers on AES.  The attacks presented in the papers are not practical -- they're far too complex, they're related-key attacks, and they're against larger-key versions and not the 128-bit version that most implementations use -- but they are impressive pieces of work all the same.
This new attack, by Alex Biryukov, Orr Dunkelman, Nathan Keller, Dmitry Khovratovich, and Adi Shamir, is much more devastating.  It is a completely practical attack against ten-round AES-256:
    Abstract.  AES is the best known and most widely used
    block cipher. Its three versions (AES-128, AES-192, and AES-256)
    differ in their key sizes (128 bits, 192 bits and 256 bits) and in
    their number of rounds (10, 12, and 14, respectively). In the case
    of AES-128, there is no known attack which is faster than the
    2^128 complexity of exhaustive search. However, AES-192
    and AES-256 were recently shown to be breakable by attacks which
    require 2^176 and 2^119 time, respectively. While these
    complexities are much faster than exhaustive search, they are
    completely non-practical, and do not seem to pose any real threat
    to the security of AES-based systems.
    In this paper we describe several attacks which can break with
    practical complexity variants of AES-256 whose number of rounds
    are comparable to that of AES-128. One of our attacks uses only
    two related keys and 2^39^ time to recover the complete
    256-bit key of a 9-round version of AES-256 (the best previous
    attack on this variant required 4 related keys and 2^120
    time). Another attack can break a 10 round version of AES-256 in
    2^45 time, but it uses a stronger type of related subkey
    attack (the best previous attack on this variant required 64
    related keys and 2^172 time).
They also describe an attack against 11-round AES-256 that requires 2^70 time -- almost practical.
These new results greatly improve on the Biryukov, Khovratovich, and Nikolic papers mentioned above, and a paper I wrote with six others in 2000, where we describe a related-key attack against 9-round AES-256 (then called Rijndael) in 2^224.  (This again proves the cryptographer's adage: attacks always get better, they never get worse.)
By any definition of the term, this is a huge result.
There are three reasons not to panic:
*  The attack exploits the fact that the key schedule for 256-bit version is pretty lousy -- something we pointed out in our 2000 paper -- but doesn't extend to AES with a 128-bit key.
*  It's a related-key attack, which requires the cryptanalyst to have access to plaintexts encrypted with multiple keys that are related in a specific way.
*  The attack only breaks 11 rounds of AES-256.  Full AES-256 has 14 rounds.
Not much comfort there, I agree.  But it's what we have.
Cryptography is all about safety margins.  If you can break n rounds of a cipher, you design it with 2n or 3n rounds.  What we're learning is that the safety margin of AES is much less than previously believed. And while there is no reason to scrap AES in favor of another algorithm, NST should increase the number of rounds of all three AES variants.  At this point, I suggest AES-128 at 16 rounds, AES-192 at 20 rounds, and AES-256 at 28 rounds.  Of maybe even more; we don't want to be revising the standard again and again.
And for new applications I suggest that people don't use AES-256. AES-128 provides more than enough security margin for the foreseeable future.  But if you're already using AES-256, there's no reason to change.
The paper:
Older AES cryptanalysis papers:
** *** ***** ******* *********** *************
     Lockpicking and the Internet
Physical locks aren't very good. They keep the honest out, but any burglar worth his salt can pick the common door lock pretty quickly.
It used to be that most people didn't know this. Sure, we all watched television criminals and private detectives pick locks with an ease only found on television and thought it realistic, but somehow we still held onto the belief that our own locks kept us safe from intruders.
The Internet changed that.
First was the MIT Guide to Lockpicking, written by the late Bob ("Ted the Tool") Baldwin. Then came Matt Blaze's 2003 paper on breaking master key systems. After that, came a flood of lockpicking information on the Net: opening a bicycle lock with a Bic pen, key bumping, and more. Many of these techniques were already known in both the criminal and locksmith communities. The locksmiths tried to suppress the knowledge, believing their guildlike secrecy was better than openness. But they've lost: never has there been more public information about lockpicking -- or safecracking, for that matter.
Lock companies have responded with more complicated locks, and more complicated disinformation campaigns.
There seems to be a limit to how secure you can make a wholly mechanical lock, as well as a limit to how large and unwieldy a key the public will accept. As a result, there is increasing interest in other lock As a security technologist, I worry that if we don't fully understand these technologies and the new sorts of vulnerabilities they bring, we may be trading a flawed technology for an even worse one. Electronic locks are vulnerable to attack, often in new and surprising ways.
Start with keypads, more and more common on house doors. These have the benefit that you don't have to carry a physical key around, but there's the problem that you can't give someone the key for a day and then take it away when that day is over. As such, the security decays over time -- the longer the keypad is in use, the more people know how to get in. More complicated electronic keypads have a variety of options for dealing with this, but electronic keypads work only when the power is on, and battery-powered locks have their own failure modes.  Plus, far too many people never bother to change the default entry code.
Keypads have other security failures, as well. I regularly see keypads where four of the 10 buttons are more worn than the other six. They're worn from use, of course, and instead of 10,000 possible entry codes, I now have to try only 24.
Fingerprint readers are another technology, but there are many known security problems with those. And there are operational problems, too: They're hard to use in the cold or with sweaty hands; and leaving a key with a neighbor to let the plumber in starts having a spy-versus-spy feel.
Some companies are going even further. Earlier this year, Schlage launched a series of locks that can be opened either by a key, a four-digit code, or the Internet. That's right: The lock is online. You can send the lock SMS messages or talk to it via a website, and the lock can send you messages when someone opens it -- or even when someone tries to open it and fails.
Sounds nifty, but putting a lock on the Internet opens up a whole new set of problems, none of which we fully understand. Even worse: Security is only as strong as the weakest link. Schlage's system combines the inherent "pickability" of a physical lock, the new vulnerabilities of electronic keypads, and the hacking risk of online. For most applications, that's simply too much risk.
This essay previously appeared on DarkReading.com.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     Comments from Readers
There are thousands of comments -- many of them interesting -- on these topics on my blog. Search for the story you want to comment on, and join in.
** *** ***** ******* *********** *************
Since 1998, CRYPTO-GRAM has been a free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.  You can subscribe, unsubscribe, or change your address on the Web at .  Back issues are also available at that URL.
Please feel free to forward CRYPTO-GRAM, in whole or in part, to colleagues and friends who will find it valuable.  Permission is also granted to reprint CRYPTO-GRAM, as long as it is reprinted in its entirety.
CRYPTO-GRAM is written by Bruce Schneier.  Schneier is the author of the best sellers "Schneier on Security," "Beyond Fear," "Secrets and Lies," and "Applied Cryptography," and an inventor of the Blowfish, Twofish, Phelix, and Skein algorithms.  He is the Chief Security Technology Officer of BT BCSG, and is on the Board of Directors of the Electronic Privacy Information Center (EPIC).  He is a frequent writer and lecturer on security topics.  See .
Crypto-Gram is a personal newsletter.  Opinions expressed are not necessarily those of BT.
Copyright (c) 2009 by Bruce Schneier.

@_date: 2009-07-15 00:12:09
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, July 15, 2009 
CRYPTO-GRAM
                July 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Imagining Threats
     Security, Group Size, and the Human Brain
     North Korean Cyberattacks
     Why People Don't Understand Risks
     Fraud on eBay
     News
     Authenticating Paperwork
     The Pros and Cons of Password Masking
     The "Hidden Cost" of Privacy
     Fixing Airport Security
     Schneier News
     Homomorphic Encryption Breakthrough
     New Attack on AES
     MD6 Withdrawn from SHA-3 Competition
     Ever Better Cryptanalytic Results Against SHA-1
     Comments from Readers
** *** ***** ******* *********** *************
     Imagining Threats
A couple of years ago, the Department of Homeland Security hired a bunch of science fiction writers to come in for a day and think of ways terrorists could attack America. If our inability to prevent 9/11 marked a failure of imagination, as some said at the time, then who better than science fiction writers to inject a little imagination into counterterrorism planning?
I discounted the exercise at the time, calling it "embarrassing." I never thought that 9/11 was a failure of imagination. I thought, and still think, that 9/11 was primarily a confluence of three things: the dual failure of centralized coordination and local control within the FBI, and some lucky breaks on the part of the attackers. More imagination leads to more movie-plot threats -- which contributes to overall fear and overestimation of the risks. And that doesn't help keep us safe at all.
Recently, I read a paper by Magne Jorgensen that provides some insight into why this is so. Titled More Risk Analysis Can Lead to Increased Over-Optimism and Over-Confidence, the paper isn't about terrorism at all. It's about software projects.
Most software development project plans are overly optimistic, and most planners are overconfident about their overoptimistic plans. Jorgensen studied how risk analysis affected this. He conducted four separate experiments on software engineers, and concluded (though there are lots of caveats in the paper, and more research needs to be done) that performing more risk analysis can make engineers more overoptimistic instead of more realistic.
Potential explanations all come from behavioral economics: cognitive biases that affect how we think and make decisions. (I've written about some of these biases and how they affect security decisions, and there's a great book on the topic as well.)
First, there's a control bias. We tend to underestimate risks in situations where we are in control, and overestimate risks in situations when we are not in control. Driving versus flying is a common example. This bias becomes stronger with familiarity, involvement and a desire to experience control, all of which increase with increased risk analysis. So the more risk analysis, the greater the control bias, and the greater the underestimation of risk.
The second explanation is the availability heuristic. Basically, we judge the importance or likelihood of something happening by the ease of bringing instances of that thing to mind. So we tend to overestimate the probability of a rare risk that is seen in a news headline, because it is so easy to imagine. Likewise, we underestimate the probability of things occurring that don't happen to be in the news.
A corollary of this phenomenon is that, if we're asked to think about a series of things, we overestimate the probability of the last thing thought about because it's more easily remembered.
According to Jorgensen's reasoning, people tend to do software risk analysis by thinking of the severe risks first, and then the more manageable risks. So the more risk analysis that's done, the less severe the last risk imagined, and thus the greater the underestimation of the total risk.
The third explanation is similar: the peak end rule. When thinking about a total experience, people tend to place too much weight on the last part of the experience. In one experiment, people had to hold their hands under cold water for one minute. Then, they had to hold their hands under cold water for one minute again, then keep their hands in the water for an additional 30 seconds while the temperature was gradually raised. When asked about it afterwards, most people preferred the second option to the first, even though the second had more total discomfort. (An intrusive medical device was redesigned along these lines, resulting in a longer period of discomfort but a relatively comfortable final few seconds. People liked it a lot better.) This means, like the second explanation, that the least severe last risk imagined gets greater weight than it deserves.
Fascinating stuff. But the biases produce the reverse effect when it comes to movie-plot threats. The more you think about far-fetched terrorism possibilities, the more outlandish and scary they become, and the less control you think you have. This causes us to overestimate the Think about this in the context of terrorism. If you're asked to come up with threats, you'll think of the significant ones first. If you're pushed to find more, if you hire science-fiction writers to dream them up, you'll quickly get into the low-probability movie plot threats. But since they're the last ones generated, they're more available. (They're also more vivid -- science fiction writers are good at that -- which also leads us to overestimate their probability.) They also suggest we're even less in control of the situation than we believed. Spending too much time imagining disaster scenarios leads people to overestimate the risks of disaster.
I'm sure there's also an anchoring effect in operation. This is another cognitive bias, where people's numerical estimates of things are affected by numbers they've most recently thought about, even random ones. People who are given a list of three risks will think the total number of risks are lower than people who are given a list of 12 risks. So if the science fiction writers come up with 137 risks, people will believe that the number of risks is higher than they otherwise would -- even if they recognize the 137 number is absurd.
Jorgensen does not believe risk analysis is useless in software projects, and I don't believe scenario brainstorming is useless in counterterrorism. Both can lead to new insights and, as a result, a more intelligent analysis of both specific risks and general risk. But an over-reliance on either can be detrimental.
Last month, at the 2009 Homeland Security Science & Technology Stakeholders Conference in Washington D.C., science fiction writers helped the attendees think differently about security. This seems like a far better use of their talents than imagining some of the zillions of ways terrorists can attack America.
This essay originally appeared on Wired.com.
 or A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     Security, Group Size, and the Human Brain
If the size of your company grows past 150 people, it's time to get name badges. It's not that larger groups are somehow less secure, it's just that 150 is the cognitive limit to the number of people a human brain can maintain a coherent social relationship with.
Primatologist Robin Dunbar derived this number by comparing neocortex -- the "thinking" part of the mammalian brain -- volume with the size of primate social groups. By analyzing data from 38 primate genera and extrapolating to the human neocortex size, he predicted a human "mean group size" of roughly 150.
This number appears regularly in human society; it's the estimated size of a Neolithic farming village, the size at which Hittite settlements split, and the basic unit in professional armies from Roman times to the present day. Larger group sizes aren't as stable because their members don't know each other well enough. Instead of thinking of the members as people, we think of them as groups of people. For such groups to function well, they need externally imposed structure, such as name badges.
Of course, badges aren't the only way to determine in-group/out-group status. Other markers include insignia, uniforms, and secret handshakes. They have different security properties and some make more sense than others at different levels of technology, but once a group reaches 150 people, it has to do something.
More generally, there are several layers of natural human group size that increase with a ratio of approximately three: 5, 15, 50, 150, 500, and 1500 -- although, really, the numbers aren't as precise as all that, and groups that are less focused on survival tend to be smaller. The layers relate to both the intensity and intimacy of relationship and the frequency of contact.
The smallest, three to five, is a "clique": the number of people from whom you would seek help in times of severe emotional distress. The twelve to 20 group is the "sympathy group": people with which you have special ties. After that, 30 to 50 is the typical size of hunter-gatherer overnight camps, generally drawn from the same pool of 150 people. No matter what size company you work for, there are only about 150 people you consider to be "co-workers." (In small companies, Alice and Bob handle accounting. In larger companies, it's the accounting department -- and maybe you know someone there personally.) The 500-person group is the "megaband," and the 1,500-person group is the "tribe." Fifteen hundred is roughly the number of faces we can put names to, and the typical size of a hunter-gatherer society.
These numbers are reflected in military organization throughout history: squads of 10 to 15 organized into platoons of three to four squads, organized into companies of three to four platoons, organized into battalions of three to four companies, organized into regiments of three to four battalions, organized into divisions of two to three regiments, and organized into corps of two to three divisions.
Coherence can become a real problem once organizations get above about 150 in size.  So as group sizes grow across these boundaries, they have more externally imposed infrastructure -- and more formalized security systems. In intimate groups, pretty much all security is ad hoc. Companies smaller than 150 don't bother with name badges; companies greater than 500 hire a guard to sit in the lobby and check badges.  The military have had centuries of experience with this under rather trying circumstances, but even there the real commitment and bonding invariably occurs at the company level. Above that you need to have rank imposed by The whole brain-size comparison might be bunk, and a lot of evolutionary psychologists disagree with it. But certainly security systems become more formalized as groups grow larger and their members less known to each other. When do more formal dispute resolution systems arise: town elders, magistrates, judges? At what size boundary are formal authentication schemes required? Small companies can get by without the internal forms, memos, and procedures that large companies require; when does what tend to appear? How does punishment formalize as group size increase? And how do all these things affect group coherence? People act differently on social networking sites like Facebook when their list of "friends" grows larger and less intimate. Local merchants sometimes let known regulars run up tabs. I lend books to friends with much less formality than a public library. What examples have you seen?
An edited version of this essay, without links, appeared in the July/August 2009 issue of IEEE Security & Privacy.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     North Korean Cyberattacks
To hear the media tell it, the United States suffered a major cyberattack last week.  Stories were everywhere. "Cyber Blitz hits U.S., Korea" was the headline in Thursday's Wall Street Journal. North Korea was blamed.
Where were you when North Korea attacked America?  Did you feel the fury of North Korea's armies?  Were you fearful for your country?  Or did your resolve strengthen, knowing that we would defend our homeland bravely and valiantly?
My guess is that you didn't even notice, that -- if you didn't open a newspaper or read a news website -- you had no idea anything was happening.  Sure, a few government websites were knocked out, but that's not alarming or even uncommon. Other government websites were attacked but defended themselves, the sort of thing that happens all the time. If this is what an international cyberattack looks like, it hardly seems worth worrying about at all.
Politically motivated cyber attacks are nothing new. We've seen UK vs. Ireland. Israel vs. the Arab states. Russia vs. several former Soviet Republics. India vs. Pakistan, especially after the nuclear bomb tests in 1998. China vs. the United States, especially in 2001 when a U.S. spy plane collided with a Chinese fighter jet. And so on and so on.
The big one happened in 2007, when the government of Estonia was attacked in cyberspace following a diplomatic incident with Russia about the relocation of a Soviet World War II memorial. The networks of many Estonian organizations, including the Estonian parliament, banks, ministries, newspapers and broadcasters, were attacked and -- in many cases -- shut down.  Estonia was quick to blame Russia, which was equally quick to deny any involvement.
It was hyped as the first cyberwar, but after two years there is still no evidence that the Russian government was involved. Though Russian hackers were indisputably the major instigators of the attack, the only individuals positively identified have been young ethnic Russians living inside Estonia, who were angry over the statue incident.
Poke at any of these international incidents, and what you find are kids playing politics. Last Wednesday, South Korea's National Intelligence Service admitted that it didn't actually know that North Korea was behind the attacks: "North Korea or North Korean sympathizers in the South" was what it said. Once again, it'll be kids playing politics.
This isn't to say that cyberattacks by governments aren't an issue, or that cyberwar is something to be ignored. The constant attacks by Chinese nationals against U.S. networks may not be government-sponsored, but it's pretty clear that they're tacitly government-approved. Criminals, from lone hackers to organized crime syndicates, attack networks all the time. And war expands to fill every possible theater: land, sea, air, space, and now cyberspace. But cyberterrorism is nothing more than a media invention designed to scare people. And for there to be a cyberwar, there first needs to be a war.
Israel is currently considering attacking Iran in cyberspace, for example.  If it tries, it'll discover that attacking computer networks is an inconvenience to the nuclear facilities it's targeting, but doesn't begin to substitute for bombing them.
In May, President Obama gave a major speech on cybersecurity.  He was right when he said that cybersecurity is a national security issue, and that the government needs to step up and do more to prevent cyberattacks. But he couldn't resist hyping the threat with scare stories: "In one of the most serious cyber incidents to date against our military networks, several thousand computers were infected last year by malicious software -- malware," he said. What he didn't add was that those infections occurred because the Air Force couldn't be bothered to keep its patches up to date.
This is the face of cyberwar: easily preventable attacks that, even when they succeed, only a few people notice.  Even this current incident is turning out to be a sloppily modified five-year-old worm that no modern network should still be vulnerable to.
Securing our networks doesn't require some secret advanced NSA technology.  It's the boring network security administration stuff we already know how to do: keep your patches up to date, install good anti-malware software, correctly configure your firewalls and intrusion-detection systems, monitor your networks. And while some government and corporate networks do a pretty good job at this, others fail again and again.
Enough of the hype and the bluster. The news isn't the attacks, but that some networks had security lousy enough to be vulnerable to them.
This essay originally appeared on the Minnesota Public Radio website.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     Why People Don't Understand Risks
Last week's Minneapolis Star Tribune had the front-page headline: "Co-sleeping kills about 20 infants each year."  The only problem is that there's no additional information with which to make sense of the How many infants don't die each year?  How many infants die each year in separate beds?  Is the death rate for co-sleepers greater or less than the death rate for separate-bed sleepers?  Without this information, it's impossible to know whether this statistic is good or bad.
But the media rarely provides context for the data.  The story is in the aftermath of an incident where a baby was accidentally smothered in his Oh, and that 20-infants-per-year number is for Minnesota only.  No word as to whether the situation is better or worse in other states.
The headline in the web article is different.
 or ** *** ***** ******* *********** *************
     Fraud on eBay
I expected selling my computer on eBay to be easy.
Attempt 1:  I listed it.  Within hours, someone bought it -- from a hacked account, as eBay notified me, canceling the sale.
Attempt 2:  I listed it again.  Within hours, someone bought it, and asked me to send it to her via FedEx overnight.  The buyer sent payment via PayPal immediately, and then -- near as I could tell -- immediately opened a dispute with PayPal so that the funds were put on hold.  And then she sent me an e-mail saying "I paid you, now send me the computer."  But PayPal was faster than she expected, I think.  At the same time, I received an e-mail from PayPal saying that I might have received a payment that the account holder did not authorize, and that I shouldn't ship the item until the investigation is complete.
I was willing to make Attempt 3, but someone on my blog bought it first.  It looks like eBay is completely broken for items like this.
It's not just me.
 or A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     News
Did a public Twitter post lead to a burglary?
Prairie dogs hack Baltimore Zoo; an amusing story that echoes a lot of our own security problems.
 or The U.S. Department of Homeland Security has a blog.  I don't know if it will be as interesting or entertaining as the TSA's blog.
Carrot-bomb art project bombs in Sweden:
Fascinating research on the psychology of con games.  "The psychology of scams: Provoking and committing errors of judgement" was prepared for the UK Office of Fair Trading by the University of Exeter School of New computer snooping tool:  or This week's movie-plot threat -- fungus:
Engineers are more likely to become Muslim terrorists.  At least, that's what the facts indicate.  Is it time to start profiling?
 or John Mueller on nuclear disarmament: "The notion that the world should rid itself of nuclear weapons has been around for over six decades -- during which time they have been just about the only instrument of destruction that hasn't killed anybody."
Eavesdropping on dot-matrix printers by listening to them.
Research on the security of online games:
Ross Anderson liveblogged the 8th Workshop on Economics of Information Security (WEIS) at University College London.
I wrote about WEIS 2006 back in 2006.
Clear, the company that sped people through airport security, has ceased operations.  It is unclear what will happen to all that personal data they have collected.
This no-stabbing knife seems not to be a joke.
I've already written about the risks of pointy knives.
The Communication Security Establishment (CSE, basically Canada's NSA) is growing so fast they're running out of room and building new office  or Cryptography spam:
More security countermeasures from the natural world:
1.  The plant caladium steudneriifolium pretends to be ill so mining moths won't eat it.
2.  Cabbage aphids arm themselves with chemical bombs.
 or 3.  The dark-footed ant spider mimics an ant so that it's not eaten by other spiders, and so it can eat spiders itself.
 or  or Information leakage from keypads.  (You need to click on the link to see the pictures.)
Good essay -- "The Staggering Cost of Playing it 'Safe'" -- about the political motivations for terrorist security policy.
 or My commentary on a article hyping the terrorist risk of cloud computing:
Pocketless trousers to protect against bribery in Nepal:
 or Anti-theft lunch bags:
U.S. court institutes limits on TSA searches.  This is good news.
Spanish police foil remote-controlled zeppelin jailbreak.  Sometimes movie plots actually happen.
 or Almost two years ago, I wrote about my strategy for encrypting my laptop.  One of the things I said was:  "There are still two scenarios you aren't secure against, though. You're not secure against someone snatching your laptop out of your hands as you're typing away at the local coffee shop. And you're not secure against the authorities telling you to decrypt your data for them."  Here's a free program that defends against that first threat: it locks the computer unless a key is pressed every n seconds.  Honestly, this would be too annoying for me to use, but you're welcome to try it.
You won't hear about this ATM vulnerability, because the presentation has been pulled from the BlackHat conference:
The NSA is building a massive data center in Utah.
 or I was quoted as calling Google's Chrome operating system "idiotic." Here's additional explanation and context.
How to cause chaos in an airport: leave a suitcase in a restroom.
Interesting paper from HotSec '07: "Do Strong Web Passwords Accomplish Anything?" by Dinei Florencio, Cormac Herley, and Baris Coskun.
 or Interesting use of gaze tracking software to protect privacy:
Poor man's steganography -- hiding documents in corrupt PDF documents:
 or ** *** ***** ******* *********** *************
     Authenticating Paperwork
It's a sad, horrific story. Homeowner returns to find his house demolished. The demolition company was hired legitimately but there was a mistake and it demolished the wrong house. The demolition company relied on GPS co-ordinates, but requiring street addresses isn't a solution. A typo in the address is just as likely, and it would have demolished the house just as quickly.
The problem is less how the demolishers knew which house to knock down, and more how they confirmed that knowledge. They trusted the paperwork, and the paperwork was wrong. Informality works when everybody knows everybody else. When merchants and customers know each other, government officials and citizens know each other, and people know their neighbors, people know what's going on. In that sort of milieu, if something goes wrong, people notice.
In our modern anonymous world, paperwork is how things get done. Traditionally, signatures, forms, and watermarks all made paperwork official. Forgeries were possible but difficult. Today, there's still paperwork, but for the most part it only exists until the information makes its way into a computer database. Meanwhile, modern technology -- computers, fax machines and desktop publishing software -- has made it easy to forge paperwork. Every case of identity theft has, at its core, a paperwork failure. Fake work orders, purchase orders, and other documents are used to steal computers, equipment, and stock. Occasionally, fake faxes result in people being sprung from prison. Fake boarding passes can get you through airport security. This month hackers officially changed the name of a Swedish man.
A reporter even changed the ownership of the Empire State Building. Sure, it was a stunt, but this is a growing form of crime. Someone pretends to be you -- preferably when you're away on holiday -- and sells your home to someone else, forging your name on the paperwork. You return to find someone else living in your house, someone who thinks he legitimately bought it. In some senses, this isn't new. Paperwork mistakes and fraud have happened ever since there was paperwork. And the problem hasn't been fixed yet for several reasons.
One, our sloppy systems generally work fine, and it's how we get things done with minimum hassle. Most people's houses don't get demolished and most people's names don't get maliciously changed. As common as identity theft is, it doesn't happen to most of us. These stories are news because they are so rare. And in many cases, it's cheaper to pay for the occasional blunder than ensure it never happens.
Two, sometimes the incentives aren't in place for paperwork to be properly authenticated. The people who demolished that family home were just trying to get a job done. The same is true for government officials processing title and name changes. Banks get paid when money is transferred from one account to another, not when they find a paperwork problem. We're all irritated by forms stamped 17 times, and other mysterious bureaucratic processes, but these are actually designed to detect problems.
And three, there's a psychological mismatch: it is easy to fake paperwork, yet for the most part we act as if it has magical properties of authenticity.
What's changed is scale. Fraud can be perpetrated against hundreds of thousands, automatically. Mistakes can affect that many people, too. What we need are laws that penalize people or companies -- criminally or civilly -- who make paperwork errors. This raises the cost of mistakes, making authenticating paperwork more attractive, which changes the incentives of those on the receiving end of the paperwork. And that will cause the market to devise technologies to verify the provenance, accuracy, and integrity of information: telephone verification, addresses and GPS co-ordinates, cryptographic authentication, systems that double- and triple-check, and so on.
We can't reduce society's reliance on paperwork, and we can't eliminate errors based on it. But we can put economic incentives in place for people and companies to authenticate paperwork more.
This essay originally appeared in The Guardian.
 or A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     The Pros and Cons of Password Masking
Usability guru Jakob Nielsen opened up a can of worms when he made the case against password masking -- the practice of hiding computer password characters behind asterisks -- in his blog. I chimed in that I agreed. Almost 165 comments on my blog (and several articles, essays, and many other blog posts) later, the consensus is that we were wrong.
I was certainly too glib. Like any security countermeasure, password masking has value. But like any countermeasure, password masking is not a panacea. And the costs of password masking need to be balanced with the benefits.
The cost is accuracy. When users don't get visual feedback from what they're typing, they're more prone to make mistakes. This is especially true with character strings that have non-standard characters and capitalization. This has several ancillary costs:
* Users get pissed off.
* Users are more likely to choose easy-to-type passwords, reducing both mistakes and security. Removing password masking will make people more comfortable with complicated passwords: they'll become easier to memorize and easier to use.
The benefits of password masking are more obvious:
*Security from shoulder surfing. If people can't look over your shoulder and see what you're typing, they're much less likely to be able to steal your password. Yes, they can look at your fingers instead, but that's much harder than looking at the screen. Surveillance cameras are also an issue: it's easier to watch someone's fingers on recorded video, but reading a cleartext password off a screen is trivial.
* In some situations, there is a trust dynamic involved. Do you type your password while your boss is standing over your shoulder watching? How about your spouse or partner? Your parent or child? Your teacher or students? At ATMs, there's a social convention of standing away from someone using the machine, but that convention doesn't apply to computers. You might not trust the person standing next to you enough to let him see your password, but don't feel comfortable telling him to look away. Password masking solves that social awkwardness.
* Security from screen scraping malware. This is less of an issue; keyboard loggers are more common and unaffected by password masking. And if you have that kind of malware on your computer, you've got all sorts of problems.
* A security "signal." Password masking alerts users, and I'm thinking users who aren't particularly security savvy, that passwords are a secret.
I believe that shoulder surfing isn't nearly the problem it's made out to be. One, lots of people use their computers in private, with no one looking over their shoulders. Two, personal handheld devices are used very close to the body, making shoulder surfing all that much harder. Three, it's hard to quickly and accurately memorize a random non-alphanumeric string that flashes on the screen for a second or so.
This is not to say that shoulder surfing isn't a threat. It is. And, as many readers pointed out, password masking is one of the reasons it isn't more of a threat. And the threat is greater for those who are not fluent computer users: slow typists and people who are likely to choose bad passwords. But I believe that the risks are overstated.
Password masking is definitely important on public terminals with short PINs. (I'm thinking of ATMs.) The value of the PIN is large, shoulder surfing is more common, and a four-digit PIN is easy to remember in any And lastly, this problem largely disappears on the Internet on your personal computer. Most browsers include the ability to save and then automatically populate password fields, making the usability problem go away at the expense of another security problem (the security of the password becomes the security of the computer). There's a Firefox plug-in that gets rid of password masking. And programs like my own Password Safe allow passwords to be cut and pasted into applications, also eliminating the usability problem.
One approach is to make it a configurable option. High-risk banking applications could turn password masking on by default; other applications could turn it off by default. Browsers in public locations could turn it on by default. I like this, but it complicates the user A reader mentioned BlackBerry's solution, which is to display each character briefly before masking it; that seems like an excellent I, for one, would like the option. I cannot type complicated WEP keys into Windows -- twice! what's the deal with that? -- without making mistakes. I cannot type my rarely used and very complicated PGP keys without making a mistake unless I turn off password masking. That's what I was reacting to when I said "I agree."
So was I wrong? Maybe. Okay, probably. Password masking definitely improves security; many readers pointed out that they regularly use their computer in crowded environments, and rely on password masking to protect their passwords. On the other hand, password masking reduces accuracy and makes it less likely that users will choose secure and hard-to-remember passwords, I will concede that the password masking trade-off is more beneficial than I thought in my snap reaction, but also that the answer is not nearly as obvious as we have historically A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     The "Hidden Cost" of Privacy
Forbes ran an article talking about the "hidden" cost of privacy. Basically, the point was that privacy regulations are expensive to comply with, and a lot of that expense gets eaten up by the mechanisms of compliance and doesn't go toward improving anyone's actual privacy. This is a valid point, and one that I make in talks about privacy all the time.  It's particularly bad in the United States, because we have a patchwork of different privacy laws covering different types of information and different situations and not a single comprehensive privacy law.
The meta-problem is simple to describe: those entrusted with our privacy often don't have much incentive to respect it.  Examples include: credit bureaus such as TransUnion and Experian, who don't have any business relationship at all with the people whose data they collect and sell; companies such as Google who give away services -- and collect personal data as a part of that -- as an incentive to view ads, and make money by selling those ads to other companies; medical insurance companies, who are chosen by a person's employer; and computer software vendors, who can have monopoly powers over the market.  Even worse, it can be impossible to connect an effect of a privacy violation with the violation itself -- if someone opens a bank account in your name, how do you know who was to blame for the privacy violation? -- so even when there is a business relationship, there's no clear cause-and-effect What this all means is that protecting individual privacy remains an externality for many companies, and that basic market dynamics won't work to solve the problem.  Because the efficient market solution won't work, we're left with inefficient regulatory solutions.  So now the question becomes: how do we make regulation as efficient as possible?  I have some suggestions:
*  Broad privacy regulations are better than narrow ones.
*  Simple and clear regulations are better than complex and confusing ones.
*  It's far better to regulate results than methodology.
*  Penalties for bad behavior need to be expensive enough to make good behavior the rational choice.
We'll never get rid of the inefficiencies of regulation -- that's the nature of the beast, and why regulation only makes sense when the market fails -- but we can reduce them.
Forbes article:
 or ** *** ***** ******* *********** *************
     Fixing Airport Security
It's been months since the Transportation Security Administration has had a permanent director. If, during the job interview (no, I didn't get one), President Obama asked me how I'd fix airport security in one sentence, I would reply: "Get rid of the photo ID check, and return passenger screening to pre-9/11 levels."
Okay, that's a joke. While showing ID, taking your shoes off and throwing away your water bottles isn't making us much safer, I don't expect the Obama administration to roll back those security measures anytime soon. Airport security is more about CYA than anything else: defending against what the terrorists did last time.
But the administration can't risk appearing as if it facilitated a terrorist attack, no matter how remote the possibility, so those annoyances are probably here to stay.
This would be my real answer: "Establish accountability and transparency for airport screening." And if I had another sentence: "Airports are one of the places where Americans, and visitors to America, are most likely to interact with a law enforcement officer - and yet no one knows what rights travelers have or how to exercise those rights."
Obama has repeatedly talked about increasing openness and transparency in government, and it's time to bring transparency to the Transportation Security Administration (TSA).
Let's start with the no-fly and watch lists. Right now, everything about them is secret: You can't find out if you're on one, or who put you there and why, and you can't clear your name if you're innocent. This Kafkaesque scenario is so un-American it's embarrassing. Obama should make the no-fly list subject to judicial review.
Then, move on to the checkpoints themselves. What are our rights? What powers do the TSA officers have? If we're asked "friendly" questions by behavioral detection officers, are we allowed not to answer? If we object to the rough handling of ourselves or our belongings, can the TSA official retaliate against us by putting us on a watch list? Obama should make the rules clear and explicit, and allow people to bring legal action against the TSA for violating those rules; otherwise, airport checkpoints will remain a Constitution-free zone in our country.
Next, Obama should refuse to use unfunded mandates to sneak expensive security measures past Congress. The Secure Flight program is the worst offender. Airlines are being forced to spend billions of dollars redesigning their reservations systems to accommodate the TSA's demands to preapprove every passenger before he or she is allowed to board an airplane. These costs are borne by us, in the form of higher ticket prices, even though we never see them explicitly listed.
Maybe Secure Flight is a good use of our money; maybe it isn't. But let's have debates like that in the open, as part of the budget process, where it belongs.
And finally, Obama should mandate that airport security be solely about terrorism, and not a general-purpose security checkpoint to catch everyone from pot smokers to deadbeat dads.
The Constitution provides us, both Americans and visitors to America, with strong protections against invasive police searches. Two exceptions come into play at airport security checkpoints. The first is "implied consent," which means that you cannot refuse to be searched; your consent is implied when you purchased your ticket. And the second is "plain view," which means that if the TSA officer happens to see something unrelated to airport security while screening you, he is allowed to act on that.
Both of these principles are well established and make sense, but it's their combination that turns airport security checkpoints into police-state-like checkpoints.
The TSA should limit its searches to bombs and weapons and leave general policing to the police - where we know courts and the Constitution still None of these changes will make airports any less safe, but they will go a long way to de-ratcheting the culture of fear, restoring the presumption of innocence and reassuring Americans, and the rest of the world, that - as Obama said in his inauguration speech - "we reject as false the choice between our safety and our ideals."
This essay originally appeared, without hyperlinks, in the New York Daily News.
 or ** *** ***** ******* *********** *************
     Schneier News
I am speaking at Black Hat and DefCon, in Las Vegas, on 30 and 31 July 2009.
** *** ***** ******* *********** *************
     Homomorphic Encryption Breakthrough
Last month, IBM made some pretty brash claims about homomorphic encryption and the future of security. I hate to be the one to throw cold water on the whole thing -- as cool as the new discovery is -- but it's important to separate the theoretical from the practical.
Homomorphic cryptosystems are ones where mathematical operations on the ciphertext have regular effects on the plaintext. A normal symmetric cipher -- DES, AES, or whatever -- is not homomorphic. Assume you have a plaintext P, and you encrypt it with AES to get a corresponding ciphertext C. If you multiply that ciphertext by 2, and then decrypt 2C, you get random gibberish instead of P. If you got something else, like 2P, that would imply some pretty strong nonrandomness properties of AES and no one would trust its security.
The RSA algorithm is different. Encrypt P to get C, multiply C by 2, and then decrypt 2C -- and you get 2P. That's a homomorphism: perform some mathematical operation to the ciphertext, and that operation is reflected in the plaintext. The RSA algorithm is homomorphic with respect to multiplication, something that has to be taken into account when evaluating the security of a security system that uses RSA.
This isn't anything new. RSA's homomorphism was known in the 1970s, and other algorithms that are homomorphic with respect to addition have been known since the 1980s. But what has eluded cryptographers is a fully homomorphic cryptosystem: one that is homomorphic under both addition and multiplication and yet still secure. And that's what IBM researcher Craig Gentry has discovered.
This is a bigger deal than might appear at first glance. Any computation can be expressed as a Boolean circuit: a series of additions and multiplications. Your computer consists of a zillion Boolean circuits, and you can run programs to do anything on your computer. This algorithm means you can perform arbitrary computations on homomorphically encrypted data. More concretely: if you encrypt data in a fully homomorphic cryptosystem, you can ship that encrypted data to an untrusted person and that person can perform arbitrary computations on that data without being able to decrypt the data itself. Imagine what that would mean for cloud computing, or any outsourcing infrastructure: you no longer have to trust the outsourcer with the data.
Unfortunately -- you knew that was coming, right? -- Gentry's scheme is completely impractical. It uses something called an ideal lattice as the basis for the encryption scheme, and both the size of the ciphertext and the complexity of the encryption and decryption operations grow enormously with the number of operations you need to perform on the ciphertext -- and that number needs to be fixed in advance. And converting a computer program, even a simple one, into a Boolean circuit requires an enormous number of operations. These aren't impracticalities that can be solved with some clever optimization techniques and a few turns of Moore's Law; this is an inherent limitation in the algorithm. In one article, Gentry estimates that performing a Google search with encrypted keywords -- a perfectly reasonable simple application of this algorithm -- would increase the amount of computing time by about a trillion. Moore's law calculates that it would be 40 years before that homomorphic search would be as efficient as a search today, and I think he's being optimistic with even this most simple of examples.
Despite this, IBM's PR machine has been in overdrive about the discovery. Its press release makes it sound like this new homomorphic scheme is going to rewrite the business of computing: not just cloud computing, but "enabling filters to identify spam, even in encrypted email, or protection information contained in electronic medical records." Maybe someday, but not in my lifetime.
This is not to take anything away anything from Gentry or his discovery. Visions of a fully homomorphic cryptosystem have been dancing in cryptographers' heads for thirty years. I never expected to see one. It will be years before a sufficient number of cryptographers examine the algorithm that we can have any confidence that the scheme is secure, but

@_date: 2009-06-15 07:34:09
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, June 15, 2009 
CRYPTO-GRAM
                June 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Obama's Cybersecurity Speech
     "Lost" Puzzle in Wired Magazine
     Last Month's Terrorism Arrests
     News
     Me on Full-Body Scanners in Airports
     Schneier News
     The Doghouse: Net1
     Cloud Computing
     The Second Interdisciplinary Workshop on Security and
        Human Behaviour
     Comments from Readers
** *** ***** ******* *********** *************
     Obama's Cybersecurity Speech
I am optimistic about President Obama's new cybersecurity policy and the appointment of a new "cybersecurity coordinator," though much depends on the details. What we do know is that the threats are real, from identity theft to Chinese hacking to cyberwar.
His principles were all welcome -- securing government networks, coordinating responses, working to secure the infrastructure in private hands (the power grid, the communications networks, and so on), although I think he's overly optimistic that legislation won't be required. I was especially heartened to hear his commitment to funding research. Much of the technology we currently use to secure cyberspace was developed from university research, and the more of it we finance today the more secure we'll be in a decade.
Education is also vital, although sometimes I think my parents need more cybersecurity education than my grandchildren do. I also appreciate the president's commitment to transparency and privacy, both of which are vital for security.
But the details matter. Centralizing security responsibilities has the downside of making security more brittle by instituting a single approach and a uniformity of thinking. Unless the new coordinator distributes responsibility, cybersecurity won't improve.
As the administration moves forward on the plan, two principles should apply. One, security decisions need to be made as close to the problem as possible. Protecting networks should be done by people who understand those networks, and threats needs to be assessed by people close to the threats. But distributed responsibility has more risk, so oversight is Two, security coordination needs to happen at the highest level possible, whether that's evaluating information about different threats, responding to an Internet worm or establishing guidelines for protecting personal information. The whole picture is larger than any single agency.
This essay originally appeared on The New York Times website, along with several others commenting on Obama's speech.
All the essays are worth reading, although I want to specifically quote James Bamford making an important point I've repeatedly made:  "The history of White House czars is not a glorious one as anyone who has followed the rise and fall of the drug czars can tell. There is a lot of hype, a White House speech, and then things go back to normal. Power, the ability to cause change, depends primarily on who controls the money and who is closest to the president's ear.  Because the new cyber czar will have neither a checkbook nor direct access to President Obama, the role will be more analogous to a traffic cop than a czar."
Gus Hosein wrote a good essay on the need for privacy:  "Of course raising barriers around computer systems is certainly a good start. But when these systems are breached, our personal information is left vulnerable. Yet governments and companies are collecting more and more of our information.  The presumption should be that all data collected is vulnerable to abuse or theft. We should therefore collect only what is absolutely required."
I wrote something similar to my essay above in 2002, about the creation of the Department of Homeland Security:  "The human body defends itself through overlapping security systems. It has a complex immune system specifically to fight disease, but disease fighting is also distributed throughout every organ and every cell. The body has all sorts of security systems, ranging from your skin to keep harmful things out of your body, to your liver filtering harmful things from your bloodstream, to the defenses in your digestive system. These systems all do their own thing in their own way. They overlap each other, and to a certain extent one can compensate when another fails. It might seem redundant and inefficient, but it's more robust, reliable, and secure. You're alive and reading this because of it."
More news links on Obama's speech:
 or  or  or  or Good commentary from Gene Spafford:
 or Good commentary from Bob Blakley:
Me in 2002:
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     "Lost" Puzzle in Wired Magazine
For the April 09 issue of Wired Magazine, I was asked to create a cryptographic puzzle based on the television show Lost.  Specifically, I was given a "clue" to encrypt.
Details are in the links.  Creating something like this is very hard. The puzzle needs to be hard enough that people don't figure it out immediately, and easy enough that people eventually do figure it out. To make matters even more complicated, people will share their ideas on the Internet.  So if the solution requires -- and I'm making this up -- expertise in Mayan history, carburetor design, algebraic topology, and Russian folk dancing, those people are likely to come together on the Internet.  The puzzle has to be challenging for the group mind, not just for individual minds.
 or  or ** *** ***** ******* *********** *************
     Last Month's Terrorism Arrests
I have four points to make on the arrest of the three men for plotting to blow up synagogues in New York.  One: There was little danger of an actual terrorist attack:  "Authorities said the four men have long been under investigation and there was little danger they could actually have carried out their plan, NBC News' Pete Williams reported."
And: "'They never got anywhere close to being able to do anything,' one official told NBC News.  'Still, it's good to have guys like this off the street.'"
Of course, politicians are using this incident to peddle more fear: "'This was a very serious threat that could have cost many, many lives if it had gone through,' Representative Peter T. King, Republican from Long Island, said in an interview with WPIX-TV. 'It would have been a horrible, damaging tragedy. There's a real threat from homegrown terrorists and also from jailhouse converts.'"
Two, they were caught by traditional investigation and intelligence. Not airport security.  Not warrantless eavesdropping.  But old-fashioned investigation and intelligence.  This is what works.  This is what keeps us safe.  I wrote an essay in 2004 that says exactly that.  "The only effective way to deal with terrorists is through old-fashioned police and intelligence work -- discovering plans before they're implemented and then going after the plotters themselves."
Three, they were idiots:  "The ringleader of the four-man homegrown terror cell accused of plotting to blow up synagogues in the Bronx and military planes in Newburgh admitted to a judge today that he had smoked pot before his bust last night.
"When U.S. Magistrate Judge Lisa M. Smith asked James Cromitie if his judgment was impaired during his appearance in federal court in White Plains, the 55-year-old confessed: 'No. I smoke it regularly.  I understand everything you are saying.'"
Four, an "informant" helped this group a lot:  "In April, Mr. Cromitie and the three other men selected the synagogues as their targets, the statement said. The informant soon helped them get the weapons, which were incapable of being fired or detonated, according to the authorities."
The warning I wrote in "Portrait of the Modern Terrorist as an Idiot" is timely again: "Despite the initial press frenzies, the actual details of the cases frequently turn out to be far less damning. Too often it's unclear whether the defendants are actually guilty, or if the police created a crime where none existed before."
Actually, that whole 2007 essay is timely again.  Some things never change.
My "Portrait of the Modern Terrorist as an Idiot"
** *** ***** ******* *********** *************
     News
Kylin is a secure operating system from China.  Seems to be a Linux variant.
 or A great movie-plot threat: pirates in Chesapeake Bay.
 or Remember: if you don't like something, claim that it will enable, embolden, or entice terrorists.  Works every time.
Invisible ink pen:
Microsoft bans memcopy() from its code base.  Interesting discussion in comments about whether this helps, or is mostly cosmetic.
Your home/work location pair can uniquely identify you.  This is very troubling, given the number of location-based services springing up and the number of databases that are collecting location data.
IEDs are now weapons of mass destruction.
Research into the insecurity of "secret questions."
Defending against movie-plot threats with movie characters:
Fantastic automatic dice thrower, a random number generator for computer Steganography using TCP retransmission.  I don't think these sorts of things have any large-scale applications, but they are clever.
What do you do if you have too many background checks to do for people's security clearances, and not enough time to do them?
It's all a matter of incentives.  The investigators were rewarded for completing investigations, not for doing them well.
Man held for hours by immigration officials because he had no fingerprints:
 or And in other biometric news, four states have banned smiling in driver's license photographs.
Research on movie-plot threats: "Emerging Threats and Security Planning: How Should We Decide What Hypothetical Threats to Worry About?"
Secret government communications cables buried around Washington, DC:
This month's movie-plot idea: arming the Boston police with semi-automatic rifles:
I don't know how I missed this great series from Slate in February. It's eight essays exploring why there have been no follow-on terrorist attacks in the U.S. since 9/11 (not counting the anthrax mailings, I guess).  Read the whole thing.
In May's Crypto-Gram, I blogged about the Boston police seizing a student's computer for, among other things, running Linux.  Earlier this month, the Massachusetts Supreme Court threw out the search warrant.
This combination door lock is very pretty.  Of course, four digits is too short an entry code, but I like the overall design and the automatic rescrambling feature.  It's just a prototype, and not even a physical one at that.
 or Earlier this year, I blogged about a self-defense pen that is likely to easily pass through airport security.  On the other hand, this normal pen in the shape of a bullet will probably get you in trouble.
 or Time for some more fear about terrorists using maps and images.  (I thought I wrote a good blog post, but Crypto-Gram is already too long this month.  So read it online, please.)
If you think that under-20-year-olds don't care about privacy, this is an eloquent op-ed by two students about why CCTV cameras have no place in their UK school.
 or Here's a site that sells corrupted MS Word files.  The idea is that you e-mail one of the files to your professor when your homework is due, buying you a few hours -- or maybe days -- of extra time before your professor notices that it's corrupted.  On the one hand, this is clever.  But on the other hand, it's services like these that will force professors to treat corrupted attachments as work not yet turned in, and harm innocent homework submitters.
Here's how to make a corrupted pdf file for free:
Teaching children to spot terrorists: you can't make this stuff up.
Industry differences in types of security breaches.
Malware steals ATM data
** *** ***** ******* *********** *************
     Me on Full-Body Scanners in Airports
I'm very happy with this quote in a CNN.com story on "whole-body imaging" at airports:
"Bruce Schneier, an internationally recognized security technologist, said whole-body imaging technology 'works pretty well,' privacy rights aside. But he thinks the financial investment was a mistake. In a post-9/11 world, he said, he knows his position isn't 'politically tenable,' but he believes money would be better spent on intelligence-gathering and investigations.
"'It's stupid to spend money so terrorists can change plans,' he said by phone from Poland, where he was speaking at a conference. If terrorists are swayed from going through airports, they'll just target other locations, such as a hotel in Mumbai, India, he said.
"'We'd be much better off going after bad guys ... and back to pre-9/11 levels of airport security,' he said. "There's a huge "cover your ass" factor in politics, but unfortunately, it doesn't make us safer.'"
I've written about "cover your ass" security in the past, but it's nice to see it in the press.
 or Me on CYA security:
** *** ***** ******* *********** *************
     Schneier News
Marcus Ranum and I did two video versions of our Face-Off column: one on cloud computing:
 or And the other on who should be in charge of cyber-security:
 or Another interview with me on cloud computing:
 or ** *** ***** ******* *********** *************
     The Doghouse: Net1
cryptographic authorities around the world as the most innovative and secure protocol ever invented to manage offline and online smart card related transactions. Please see the independent report by Bruce Schneider [sic] in his book entitled Applied Cryptography, 2nd Edition published in the late 1990s."
After I posted this on my blog, someone -- probably from the company -- said that it was referring to the UEPS protocol, discussed on page 589.  I still don't like the hyperbole and the implied endorsement in the quote.
** *** ***** ******* *********** *************
     Cloud Computing
This year's overhyped IT concept is cloud computing. Also called software as a service (Saas), cloud computing is when you run software over the internet and access it via a browser. The Salesforce.com customer management software is an example of this. So is Google Docs. If you believe the hype, cloud computing is the future.
But hype aside, cloud computing is nothing new. It's the modern version of the timesharing model from the 1960s, which was eventually killed by the rise of the personal computer. It's what Hotmail and Gmail have been doing all these years, and it's social networking sites, remote backup companies, and remote email filtering companies such as MessageLabs. Any IT outsourcing -- network infrastructure, security monitoring, remote hosting -- is a form of cloud computing.
The old timesharing model arose because computers were expensive and hard to maintain. Modern computers and networks are drastically cheaper, but they're still hard to maintain. As networks have become faster, it is again easier to have someone else do the hard work. Computing has become more of a utility; users are more concerned with results than technical details, so the tech fades into the background.
But what about security? Isn't it more dangerous to have your email on Hotmail's servers, your spreadsheets on Google's, your personal conversations on Facebook's, and your company's sales prospects on salesforce.com's? Well, yes and no.
IT security is about trust. You have to trust your CPU manufacturer, your hardware, operating system and software vendors -- and your ISP. Any one of these can undermine your security: crash your systems, corrupt data, allow an attacker to get access to systems. We've spent decades dealing with worms and rootkits that target software vulnerabilities. We've worried about infected chips. But in the end, we have no choice but to blindly trust the security of the IT providers we use.
Saas moves the trust boundary out one step further -- you now have to also trust your software service vendors -- but it doesn't fundamentally change anything. It's just another vendor we need to trust.
There is one critical difference. When a computer is within your network, you can protect it with other security systems such as firewalls and IDSs. You can build a resilient system that works even if those vendors you have to trust may not be as trustworthy as you like. With any outsourcing model, whether it be cloud computing or something else, you can't. You have to trust your outsourcer completely. You not only have to trust the outsourcer's security, but its reliability, its availability, and its business continuity.
You don't want your critical data to be on some cloud computer that abruptly disappears because its owner goes bankrupt. You don't want the company you're using to be sold to your direct competitor. You don't want the company to cut corners, without warning, because times are tight. Or raise its prices and then refuse to let you have your data back. These things can happen with software vendors, but the results aren't as drastic.
There are two different types of cloud computing customers. The first only pays a nominal fee for these services -- and uses them for free in exchange for ads: e.g., Gmail and Facebook. These customers have no leverage with their outsourcers. You can lose everything. Companies like Google and Amazon won't spend a lot of time caring. The second type of customer pays considerably for these services: to Salesforce.com, MessageLabs, managed network companies, and so on. These customers have more leverage, providing they write their service contracts correctly. Still, nothing is guaranteed.
Trust is a concept as old as humanity, and the solutions are the same as they have always been. Be careful who you trust, be careful what you trust them with, and be careful how much you trust them. Outsourcing is the future of computing. Eventually we'll get this right, but you don't want to be a casualty along the way.
This essay originally appeared in The Guardian.
 or Another opinion:
 or A rebuttal:
The reason I am talking so much about cloud computing is that reporters and inverviewers keep asking me about it.  I feel kind of dragged into this whole thing.
At the Computers, Freedom, and Privacy conference earlier this month, Bob Gellman said that the nine most important words in cloud computing are: "terms of service," "location, location, location," and "provider, provider, provider" -- basically making the same point I did.  You need to make sure the terms of service you sign up for are ones you can live with.  You need to make sure the location of the provider doesn't subject you to any laws you can't live with.  And you need to make sure your provider is someone you're willing to work with.  Basically, if you're going to give someone else your data, you need to trust them.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     The Second Interdisciplinary Workshop on Security and
        Human Behaviour
Last week, I was at SHB09, the Second Interdisciplinary Workshop on Security and Human Behaviour, at MIT.  This was a two-day gathering of computer security researchers, psychologists, behavioral economists, sociologists, philosophers, and others -- all of whom are studying the human side of security -- organized by Ross Anderson, Alessandro Acquisti, and myself.  I liveblogged the workshop; here are the talk summaries.  (People were invited to submit a link for themselves, and links to applicable things they wrote.  Those links will be included after each talk summary.)
The first session was about deception, moderated by David Clark.
Frank Stajano, Cambridge University, presented research with Paul Wilson, who films actual scams for "The Real Hustle." His point is that we build security systems based on our "logic," but users don't always follow our logic. It's fraudsters who really understand what people do, so we need to understand what the fraudsters understand. Things like distraction, greed, unknown accomplices, social compliance are important.
Usability of Security Management: Defining the Permissions of Guests
David Livingstone Smith, University of New England, is a philosopher by training, and goes back to basics: "What are we talking about?" A theoretical definition -- "that which something has to have to fall under a term" -- of deception is difficult to define. "Cause to have a false belief," from the Oxford English Dictionary, is inadequate. "To deceive is intentionally have someone to have a false belief" also doesn't work. "Intentionally causing someone to have a false belief that the speaker knows to be false" still isn't good enough. The fundamental problem is that these are anthropocentric definitions. Deception is not unique to humans; it gives organisms an evolutionary edge. For example, the mirror orchid fools a wasp into landing on it by looking like and giving off chemicals that mimic the female wasp. This example shows that we need a broader definition of "purpose." His formal definition: "For systems A and B, A deceives B iff A possesses some character C with proper function F, and B possesses a mechanism C* with the proper function F* of producing representations, such that the proper function of C is to cause C* to fail to perform F* by causing C* to form false representations, and C does so in virtue of performing F, and B's falsely representing enables some feature of A to perform its proper Less than human: self-deception in the imagining of others
Talk on Lying at La Ciudad de Las Ideas
a subsequent discussion
Why War?
I spoke next, about the psychology of Conficker, how the human brain buys security, and why science fiction writers shouldn't be hired to think about terrorism risks (to be published on Wired.com this week).
Dominic Johnson, University of Edinburgh, talked about his chapter in the book Natural Security: A Darwinian Approach to a Dangerous World. Life has 3.5 billion years of experience in security innovation; let's look at how biology approaches security. Biomimicry, ecology, paleontology, animal behavior, evolutionary psychology, immunology, epidemiology, selection, and adaption are all relevant. Redundancy is a very important survival tool for species. Here's an adaption example: The 9/11 threat was real and we knew about it, but we didn't do anything. His thesis: Adaptation to novel security threats tends to occur after major disasters. There are many historical examples of this; Pearl Harbor, for example. Causes include sensory biases, psychological biases, leadership biases, organizational biases, and political biases

@_date: 2009-05-15 02:13:07
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, May 15, 2009 
CRYPTO-GRAM
                 May 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Fourth Annual Movie-Plot Threat Contest Winner
     Book Review: The Science of Fear
     An Expectation of Online Privacy
     News
     Malicious Contamination of the Food Supply
     Unfair and Deceptive Data Trade Practices
     Schneier News
     Mathematical Illiteracy
     Conficker
     Comments from Readers
** *** ***** ******* *********** *************
     Fourth Annual Movie-Plot Threat Contest Winner
For this contest, the goal was "to find an existing event somewhere in the industrialized world -- Third World events are just too easy -- and provide a conspiracy theory to explain how the terrorists were really I thought it was straightforward enough, but, honestly, I wasn't very impressed with the submissions.  Nothing surprised me with its cleverness.  There were scary entries and there were plausible entries, but hardly any were both at the same time.  And I was amazed by how many people didn't bother to read the rules at all, and just submitted movie-plot threats.
But after reading through the entries, I have chosen a winner.  It's HJohn, for his kidnap-blackmail-terrorist connection:  "Though recent shooting sprees in churches, nursing homes, and at family outings appear unrelated, a terrifying link has been discovered. All perpetrators had small children who were abducted by terrorists, and perpetrators received a video of their children with hooded terrorists warning that their children would be beheaded if they do not engage in the suicidal rampage. The terror threat level has been raised to red as profiling, known associations, and criminal history are now useless in detecting who will be the next terrorist sniper or airline hijacker. Anyone who loves their children may be a potential terrorist."
Fairly plausible, and definitely scary.  Congratulations, HJohn.
A copy of this article, with all embedded links, is here:
** *** ***** ******* *********** *************
     Book Review: The Science of Fear
Daniel Gardner's The Science of Fear was published last July, but I've only just gotten around to reading it. That was a big mistake. It's a fantastic look at how humans deal with fear: exactly the kind of thing I have been reading and writing about for the past couple of years. It's the book I wanted to write, and it's a great read.
Gardner writes about how the brain processes fear and risk, how it assesses probability and likelihood, and how it makes decisions under uncertainty. The book talks about all the interesting psychological studies -- cognitive psychology, evolutionary psychology, behavioral economics, experimental philosophy -- that illuminate how we think and act regarding fear. The book also talks about how fear is used to influence people, by marketers, by politicians, by the media. And lastly, the book talks about different areas where fear plays a part: health, crime, terrorism.
There have been a lot of books published recently that apply these new paradigms of human psychology to different domains -- to randomness, to traffic, to rationality, to art, to religion, and etc. -- but after you read a few you start seeing the same dozen psychology experiments over and over again. Even I did it, when I wrote about the psychology of security. But Gardner's book is different: he goes further, explains more, demonstrates his point with the more obscure experiments that most authors don't bother seeking out. His writing style is both easy to read and informative, a nice mix of data an anecdote. The flow of the book makes sense. And his analysis is spot-on.
My only problem with the book is that Gardner doesn't use standard names for the various brain heuristics he talks about. Yes, his names are more intuitive and evocative, but they're wrong. If you have already read other books in the field, this is annoying because you have to constantly translate into standard terminology. And if you haven't read anything else in the field, this is a real problem because you'll be needlessly confused when you read about these things in other books and So here's a handy conversion chart. Print it out and tape it to the inside front cover. Print another copy out and use it as a bookmark.
That's it. That's the only thing I didn't like about the book. Otherwise, it's perfect. It's the book I wish I had written. Only I don't think I would have done as good a job as Gardner did. The Science of Fear should be required reading for...well, for everyone.
The paperback will be published in June.
A copy of this essay, with all embedded links, is here:
** *** ***** ******* *********** *************
     An Expectation of Online Privacy
If your data is online, it is not private. Oh, maybe it seems private. Certainly, only you have access to your e-mail. Well, you and your ISP. And the sender's ISP. And any backbone provider who happens to route that mail from the sender to you.  And, if you read your personal mail from work, your company. And, if they have taps correct points, the NSA and any other sufficiently well-funded government intelligence organization -- domestic and international.
You could encrypt your mail, of course, but few of us do that. Most of us now use webmail. The general problem is that, for the most part, your online data is not under your control. Cloud computing and software as a service exacerbate this problem even more.
Your webmail is less under your control than it would be if you downloaded your mail to your computer. If you use Salesforce.com, you're relying on that company to keep your data private. If you use Google Docs, you're relying on Google. This is why the Electronic Privacy Information Center recently filed a complaint with the Federal Trade Commission: many of us are relying on Google's security, but we don't know what it is.
This is new. Twenty years ago, if someone wanted to look through your correspondence, he had to break into your house. Now, he can just break into your ISP. Ten years ago, your voicemail was on an answering machine in your office; now it's on a computer owned by a telephone company. Your financial accounts are on remote websites protected only by passwords; your credit history is collected, stored, and sold by companies you don't even know exist.
And more data is being generated. Lists of books you buy, as well as the books you look at, are stored in the computers of online booksellers. Your affinity card tells your supermarket what foods you like. What were cash transactions are now credit card transactions. What used to be an anonymous coin tossed into a toll booth is now an EZ Pass record of which highway you were on, and when. What used to be a face-to-face chat is now an e-mail, IM, or SMS conversation -- or maybe a conversation inside Facebook.
Remember when Facebook recently changed its terms of service to take further control over your data? They can do that whenever they want, you We have no choice but to trust these companies with our security and privacy, even though they have little incentive to protect them. Neither ChoicePoint, Lexis Nexis, Bank of America, nor T-Mobile bears the costs of privacy violations or any resultant identity theft.
This loss of control over our data has other effects, too. Our protections against police abuse have been severely watered down. The courts have ruled that the police can search your data without a warrant, as long as others hold that data. If the police want to read the e-mail on your computer, they need a warrant; but they don't need one to read it from the backup tapes at your ISP.
This isn't a technological problem; it's a legal problem. The courts need to recognize that in the information age, virtual privacy and physical privacy don't have the same boundaries. We should be able to control our own data, regardless of where it is stored. We should be able to make decisions about the security and privacy of that data, and have legal recourse should companies fail to honor those decisions. And just as the Supreme Court eventually ruled that tapping a telephone was a Fourth Amendment search, requiring a warrant -- even though it occurred at the phone company switching office and not in the target's home or office -- the Supreme Court must recognize that reading personal e-mail at an ISP is no different.
This essay was originally published on the SearchSecurity.com website, as the second half of a point/counterpoint with Marcus Ranum.
 or ** *** ***** ******* *********** *************
     News
New frontiers in biometrics.  Ears:
 or Arm swinging:
I guess biometrics is now the "it" thing to study.
Hacking a Time Magazine poll.  Not particularly subtle, but clever Department of Homeland Security recruitment drive:
Funny "war on photography" anecdote:
I was going to write a commentary on NSA Director General Alexander's keynote speech at the RSA Conference, but he didn't actually *say* anything.
Low-tech impersonation trick at restaurants:
Encrypting your USB drive is smart.  Writing the encryption key down is smart.  Writing it on a piece of paper and attaching it to the USB drive is not.
Hacking U.S. military satellites is more widespread than you might think:
Fake facts on Twitter: the medium makes authentication hard.
Remember those terrorism arrests that the UK government conducted, after a secret document was accidentally photographed?  No one was charged:
Cell phones and hostage situations:
This apparently non-ironic video warns that people might impersonate census workers in an effort to rob you.  But while you shouldn't trust the ID of a stranger, you should trust that same stranger to give you a phone number where you can verify that ID.  This, of course, makes no sense.
Preventing impersonation is hard.
"No-fly" also means "no-flyover": plane from Paris to Mexico isn't allowed to fly over the United States.
Lessons from the Columbine school shooting: it's not the high-tech gear, but trained and alert staff that actually make a difference:
Ireland does away with electronic voting, returning to paper ballots again.  Smart country.
A sad tale of fingerprint biometrics gone wrong.  Amusing and interesting:
Interesting article from The New York Times on preparing for cyberwar:
And yet another New York Times cyberwar article, from two days later:
I was particularly disturbed by the last paragraph of the newspaper article:  "Introducing the possibility of a nuclear response to a catastrophic cyberattack would be expected to serve the same purpose." Nuclear war is not a suitable response to a cyberattack.
Law professor Googles Justice Scalia just to see what he can collect. Scalia isn't amused:
 or Security considerations in the evolution of the human penis: a fascinating bit of evolutionary biology
 or The U.S. Air Force is using a secure version of MS Windows:
Lie detector charlatans:
Virginia health data held for ransom:
MI6 and a lost memory stick:
Marc Rotenberg on security vs. privacy:
 or Researchers hijack a botnet:
The Zeus Trojan has a self-destruct option:
 or This is bad.  I see it as a sign that the botnet wars are heating up, and botnet designers would rather destroy their networks than have them fall into "enemy" hands.
Using surveillance cameras to detect cashier cheating.
Software problems with a breath alcohol detector.
A U.S. District Court has ruled that the police do not need a warrant to place a GPS tracking device on someone's car:
** *** ***** ******* *********** *************
     Malicious Contamination of the Food Supply
Terrorists attacking our food supply is a nightmare scenario that has been given new life during the recent swine flu outbreak. Although it seems easy to do, understanding why it hasn't happened is important. G.R. Dalziel, at the Nanyang Technological University in Singapore, has written a report chronicling every confirmed case of malicious food contamination in the world since 1950: 365 cases in all, plus 126 additional unconfirmed cases. What he found demonstrates the reality of terrorist food attacks.
It turns out 72% of the food poisonings occurred at the end of the food supply chain -- at home -- typically by a friend, relative, neighbor, or co-worker trying to kill or injure a specific person. A characteristic example is Heather Mook of York, who in 2007 tried to kill her husband by putting rat poison in his spaghetti.
Most of these cases resulted in fewer than five casualties -- Mook only injured her husband in this incident -- although 16% resulted in five or more. Of the 19 cases that claimed 10 or more lives, four involved serial killers operating over several years.
Another 23% of cases occurred at the retail or food service level. A 1998 incident in Japan, where someone put arsenic in a curry sold at a summer festival, killing four and hospitalizing 63, is a typical example. Only 11% of these incidents resulted in 100 or more casualties, while 44% resulted in none.
There are very few incidents of people contaminating the actual food supply. People deliberately contaminated a water supply seven times, resulting in three deaths. There is only one example of someone deliberately contaminating a crop before harvest -- in Australia in 2006

@_date: 2009-11-14 22:24:50
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, November 15, 2009 
CRYPTO-GRAM
              November 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Beyond Security Theater
     Fear and Overreaction
     News
     Zero-Tolerance Policies
     Security in a Reputation Economy
     Schneier News
     The Commercial Speech Arms Race
     The Doghouse: ADE 651
     "Evil Maid" Attacks on Encrypted Hard Drives
     Is Antivirus Dead?
** *** ***** ******* *********** *************
     Beyond Security Theater
[I was asked to write this essay for the "New Internationalist" (n. 427, November 2009, pp. 10--13).  It's nothing I haven't said before, but I'm pleased with how this essay came together.]
Terrorism is rare, far rarer than many people think. It's rare because very few people want to commit acts of terrorism, and executing a terrorist plot is much harder than television makes it appear. The best defenses against terrorism are largely invisible: investigation, intelligence, and emergency response. But even these are less effective at keeping us safe than our social and political policies, both at home and abroad. However, our elected leaders don't think this way: they are far more likely to implement security theater against movie-plot threats.
A movie-plot threat is an overly specific attack scenario. Whether it's terrorists with crop dusters, terrorists contaminating the milk supply, or terrorists attacking the Olympics, specific stories affect our emotions more intensely than mere data does. Stories are what we fear. It's not just hypothetical stories: terrorists flying planes into buildings, terrorists with bombs in their shoes or in their water bottles, and terrorists with guns and bombs waging a co-ordinated attack against a city are even scarier movie-plot threats because they actually Security theater refers to security measures that make people feel more secure without doing anything to actually improve their security. An example: the photo ID checks that have sprung up in office buildings. No-one has ever explained why verifying that someone has a photo ID provides any actual security, but it looks like security to have a uniformed guard-for-hire looking at ID cards. Airport-security examples include the National Guard troops stationed at US airports in the months after 9/11 -- their guns had no bullets. The US colour-coded system of threat levels, the pervasive harassment of photographers, and the metal detectors that are increasingly common in hotels and office buildings since the Mumbai terrorist attacks, are additional examples.
To be sure, reasonable arguments can be made that some terrorist targets are more attractive than others: airplanes because a small bomb can result in the death of everyone aboard, monuments because of their national significance, national events because of television coverage, and transportation because of the numbers of people who commute daily. But there are literally millions of potential targets in any large country (there are five million commercial buildings alone in the US), and hundreds of potential terrorist tactics; it's impossible to defend every place against everything, and it's impossible to predict which tactic and target terrorists will try next.
Feeling and Reality
Security is both a feeling and a reality. The propensity for security theater comes from the interplay between the public and its leaders. When people are scared, they need something done that will make them feel safe, even if it doesn't truly make them safer. Politicians naturally want to do something in response to crisis, even if that something doesn't make any sense.
Often, this "something" is directly related to the details of a recent event: we confiscate liquids, screen shoes, and ban box cutters on airplanes. But it's not the target and tactics of the last attack that are important, but the next attack. These measures are only effective if we happen to guess what the next terrorists are planning. If we spend billions defending our rail systems, and the terrorists bomb a shopping mall instead, we've wasted our money. If we concentrate airport security on screening shoes and confiscating liquids, and the terrorists hide explosives in their brassieres and use solids, we've wasted our money. Terrorists don't care what they blow up and it shouldn't be our goal merely to force the terrorists to make a minor change in their tactics or targets.
Our penchant for movie plots blinds us to the broader threats. And security theater consumes resources that could better be spent elsewhere.
Any terrorist attack is a series of events: something like planning, recruiting, funding, practicing, executing, aftermath. Our most effective defenses are at the beginning and end of that process -- intelligence, investigation, and emergency response -- and least effective when they require us to guess the plot correctly. By intelligence and investigation, I don't mean the broad data-mining or eavesdropping systems that have been proposed and in some cases implemented -- those are also movie-plot stories without much basis in actual effectiveness -- but instead the traditional "follow the evidence" type of investigation that has worked for decades.
Unfortunately for politicians, the security measures that work are largely invisible. Such measures include enhancing the intelligence-gathering abilities of the secret services, hiring cultural experts and Arabic translators, building bridges with Islamic communities both nationally and internationally, funding police capabilities -- both investigative arms to prevent terrorist attacks, and emergency communications systems for after attacks occur -- and arresting terrorist plotters without media fanfare. They do not include expansive new police or spying laws. Our police don't need any new laws to deal with terrorism; rather, they need apolitical funding. These security measures don't make good television, and they don't help, come re-election time. But they work, addressing the reality of security instead of the feeling.
The arrest of the "liquid bombers" in London is an example: they were caught through old-fashioned intelligence and police work. Their choice of target (airplanes) and tactic (liquid explosives) didn't matter; they would have been arrested regardless.
But even as we do all of this we cannot neglect the feeling of security, because it's how we collectively overcome the psychological damage that terrorism causes. It's not security theater we need, it's direct appeals to our feelings. The best way to help people feel secure is by acting secure around them. Instead of reacting to terrorism with fear, we -- and our leaders -- need to react with indomitability.
Refuse to Be Terrorized
By not overreacting, by not responding to movie-plot threats, and by not becoming defensive, we demonstrate the resilience of our society, in our laws, our culture, our freedoms. There is a difference between indomitability and arrogant "bring 'em on" rhetoric. There's a difference between accepting the inherent risk that comes with a free and open society, and hyping the threats.
We should treat terrorists like common criminals and give them all the benefits of true and open justice -- not merely because it demonstrates our indomitability, but because it makes us all safer. Once a society starts circumventing its own laws, the risks to its future stability are much greater than terrorism.
Supporting real security even though it's invisible, and demonstrating indomitability even though fear is more politically expedient, requires real courage. Demagoguery is easy. What we need is leaders willing both to do what's right and to speak the truth.
Despite fearful rhetoric to the contrary, terrorism is not a transcendent threat. A terrorist attack cannot possibly destroy a country's way of life; it's only our reaction to that attack that can do that kind of damage. The more we undermine our own laws, the more we convert our buildings into fortresses, the more we reduce the freedoms and liberties at the foundation of our societies, the more we're doing the terrorists' job for them.
We saw some of this in the Londoners' reaction to the 2005 transport bombings. Among the political and media hype and fearmongering, there was a thread of firm resolve. People didn't fall victim to fear. They rode the trains and buses the next day and continued their lives. Terrorism's goal isn't murder; terrorism attacks the mind, using victims as a prop. By refusing to be terrorized, we deny the terrorists their primary weapon: our own fear.
Today, we can project indomitability by rolling back all the fear-based post-9/11 security measures. Our leaders have lost credibility; getting it back requires a decrease in hyperbole. Ditch the invasive mass surveillance systems and new police state-like powers. Return airport security to pre-9/11 levels. Remove swagger from our foreign policies. Show the world that our legal system is up to the challenge of terrorism. Stop telling people to report all suspicious activity; it does little but make us suspicious of each other, increasing both fear and helplessness.
Terrorism has always been rare, and for all we've heard about 9/11 changing the world, it's still rare. Even 9/11 failed to kill as many people as automobiles do in the US every single month. But there's a pervasive myth that terrorism is easy. It's easy to imagine terrorist plots, both large-scale "poison the food supply" and small-scale "10 guys with guns and cars." Movies and television bolster this myth, so many people are surprised that there have been so few attacks in Western cities since 9/11. Certainly intelligence and investigation successes have made it harder, but mostly it's because terrorist attacks are actually hard. It's hard to find willing recruits, to co-ordinate plans, and to execute those plans -- and it's easy to make mistakes.
Counterterrorism is also hard, especially when we're psychologically prone to muck it up. Since 9/11, we've embarked on strategies of defending specific targets against specific tactics, overreacting to every terrorist video, stoking fear, demonizing ethnic groups, and treating the terrorists as if they were legitimate military opponents who could actually destroy a country or a way of life -- all of this plays into the hands of terrorists. We'd do much better by leveraging the inherent strengths of our modern democracies and the natural advantages we have over the terrorists: our adaptability and survivability, our international network of laws and law enforcement, and the freedoms and liberties that make our society so enviable. The way we live is open enough to make terrorists rare; we are observant enough to prevent most of the terrorist plots that exist, and indomitable enough to survive the even fewer terrorist plots that actually succeed. We don't need to pretend otherwise.
** *** ***** ******* *********** *************
     Fear and Overreaction
It's hard work being prey. Watch the birds at a feeder. They're constantly on alert, and will fly away from food -- from easy nutrition

@_date: 2009-10-15 02:08:57
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, October 15, 2009 
CRYPTO-GRAM
               October 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Ass Bomber
     News
     Unauthentication
     The Futility of Defending the Targets
     Schneier News
     Texas Instruments Signing Keys Broken
     The Doghouse
     UK Defense Security Manual Leaked
     Comments from Readers
** *** ***** ******* *********** *************
     Ass Bomber
Nobody tell the TSA, but last month someone tried to assassinate a Saudi prince by exploding a bomb stuffed in his rectum.  He pretended to be a repentant militant, when in fact he was a Trojan horse:  "The resulting explosion ripped al-Asiri to shreds but only lightly injured the shocked prince -- the target of al-Asiri's unsuccessful assassination attempt."
For years, I have made the joke about Richard Reid: "Just be glad that he wasn't the underwear bomber."  Now, sadly, we have an example of one.
Lewis Page, an "improvised-device disposal operator tasked in support of the UK mainland police from 2001-2004," pointed out that this isn't much of a threat for three reasons: 1) you can't stuff a lot of explosives into a body cavity, 2) detonation is, um, problematic, and 3) the human body can stifle an explosion pretty effectively (think of someone throwing himself on a grenade to save his friends).
But who ever accused the TSA of being rational?
 or  or  or  or Page on the feasibility of the tactic:
** *** ***** ******* *********** *************
     News
Printing police handcuff keys using a 3D printer:
 or  or The DHS is considering modifying the color-coded threat alert system -- the useless system that's widely mocked -- by removing two of the five levels.  I hope you all feel safer now.
Good essay on "terrorist havens" -- like Afghanistan -- and why they're not as big a worry as some maintain.
 or Inferring friendship from location data:
Back in 2005, I wrote about the failure of two-factor authentication to mitigate banking fraud.  We're now seeing attacks that bypass that security measure.
Quantum computer factors the number 15.  It's an important development, but don't give up on public-key cryptography just yet.
This is a good thing: "An Illinois district court has allowed a couple to sue their bank on the novel grounds that it may have failed to sufficiently secure their account, after an unidentified hacker obtained a $26,500 loan on the account using the customers' user name and password."  As I've previously written, this is the only way to mitigate this kind of fraud.  It's an important security principle: ensure that the person who has the ability to mitigate the risk is responsible for the risk.  In this case, the account holders had nothing to do with the security of their account.  They could not audit it.  They could not improve it.  The bank, on the other hand, has the ability to improve security and mitigate the risk, but because they pass the cost on to their customers, they have no incentive to do so.  Litigation like this has the potential to fix the externality and improve security.
More information on the Monopoly sets with hidden escape information given to WWII POWs:
 or Sears spies on its customers; it's not just hackers who steal financial and medical information.
 or The Sears story reminds me of the 2005 Sony rootkit, which -- oddly enough -- is in the news again, too:
 or "Authorities Called in to Examine Suspicious-Looking Ham," from the Onion:
A stick figure guide to AES.
Predicting characteristics of people by the company they keep:
The average American commits three felonies a day: the title of a new book by Harvey Silverglate.  More specifically, the problem is the intersection of vague laws and fast-moving technology.
Immediacy affects risk assessments:
During a daring bank robbery in Sweden that involved a helicopter, the criminals disabled a police helicopter by placing a package with the word "bomb" near the helicopter hangar, thus engaging the full caution/evacuation procedure while they escaped.  This attack worked, even though the police had been warned.
Reproducing keys from distant and angled photographs:
Those of you who carry your keys on a ring dangling from a belt loop, take note.
Proving a computer program's correctness:
Security theater in New York for the U.N. General Assembly:
 or If you were curious what the DHS knows about you, here's an actual DHS travel record.
Moving hippos in a post-9/11 world:
There's a Trojan horse out there that not only makes transactions in your name from your bank accounts, but alters your online bank statements so you won't notice the money transfers.  If there's a moral here, it's that banks can't rely on the customer to detect fraud.  But we already knew that.
You'd think this would be an obvious piece of advice: don't let hacker inmates reprogram the prison's computers.  But, then again, this is the same prison that gave a lockpicking inmate access to the prison's keys.  What's next: inmate sharpshooters in charge of prison's gun locker?
 or Witnesses are much more accurate at identifying criminals when computers assist in the identification process rather than police officers.
 or Behavioral detection: detecting people who want to do harm:
 or Interesting hotel safe scam:
Detecting forged signatures using pen pressure and angle:
Earlier this month, DHS Secretary Janet Napolitano said that the U.S. needed to hire 1,000 cybersecurity experts over the next three years. Bob Cringeley doubts that there even are 1,000 cybersecurity experts out there to hire.  I suppose it depends on what she means by "experts."
Pigs defeating RFID-enabled feeding systems:
Using wi-fi to "see" through walls:
Wi-fi blocking paint:
Good essay by David Dittrich: "Malware to crimeware: How far have they gone, and how do we catch up?"
The current state of P versus NP:
 or 1777 steganography.
** *** ***** ******* *********** *************
     Unauthentication
In computer security, a lot of effort is spent on the authentication problem.  Whether it's passwords, secure tokens, secret questions, image mnemonics, or something else, engineers are continually coming up with more complicated -- and hopefully more secure -- ways for you to prove you are who you say you are over the Internet.
This is important stuff, as anyone with an online bank account or remote corporate network knows. But a lot less thought and work have gone into the other end of the problem: how do you tell the system on the other end of the line that you're no longer there? How do you unauthenticate My home computer requires me to log out or turn my computer off when I want to unauthenticate. This works for me because I know enough to do it, but lots of people just leave their computers on and running when they walk away. As a result, many office computers are left logged in when people go to lunch, or when they go home for the night. This, obviously, is a security vulnerability.
The most common way to combat this is by having the system time out. I could have my computer log me out automatically after a certain period of inactivity -- five minutes, for example. Getting it right requires some fine tuning, though. Log the person out too quickly, and he gets annoyed; wait too long before logging him out, and the system could be vulnerable during that time. My corporate e-mail server logs me out after 10 minutes or so, and I regularly get annoyed at my corporate e-mail system.
Some systems have experimented with a token: a USB authentication token that has to be plugged in for the computer to operate, or an RFID token that logs people out automatically when the token moves more than a certain distance from the computer.  Of course, people will be prone to just leave the token plugged in to their computer all the time; but if you attach it to their car keys or the badge they have to wear at all times when walking around the office, the risk is minimized.
That's expensive, though. A research project used a Bluetooth device, like a cell phone, and measured its proximity to a computer. The system could be programmed to lock the computer if the Bluetooth device moved out of range.
Some systems log people out after every transaction.  This wouldn't work for computers, but it can work for ATMs.  The machine spits my card out before it gives me my cash, or just requires a card swipe, and makes sure I take it out of the machine.  If I want to perform another transaction, I have to reinsert my card and enter my PIN a second time.
There's a physical analogue that everyone can explain: door locks.  Does your door lock behind you when you close the door, or does it remain unlocked until you lock it?  The first instance is a system that automatically logs you out, and the second requires you to log out manually.  Both types of locks are sold and used, and which one you choose depends on both how you use the door and who you expect to try to break in.
Designing systems for usability is hard, especially when security is involved.  Almost by definition, making something secure makes it less usable. Choosing an unauthentication method depends a lot on how the system is used as well as the threat model.  You have to balance increasing security with pissing the users off, and getting that balance right takes time and testing, and is much more an art than a science.
Automatic logout:
Proximity logout:
This essay originally appeared on ThreatPost.
** *** ***** ******* *********** *************
     The Futility of Defending the Targets
This is just silly:
    Beaver Stadium is a terrorist target. It is most likely the No. 1
    target in the region. As such, it deserves security measures
    commensurate with such a designation, but is the stadium getting
    such security?
    [..]
    When the stadium is not in use it does not mean it is not a
    target. It must be watched constantly. An easy solution is to
    assign police officers there 24 hours a day, seven days a week.
    This is how a plot to destroy the Brooklyn Bridge was thwarted --
    police presence. Although there are significant costs to this, the
    costs pale in comparison if the stadium is destroyed or damaged.
    The idea is to create omnipresence, which is a belief in
    everyone's minds (terrorists and pranksters included) that the
    stadium is constantly being watched so that any attempt would be
    futile.
Actually, the Brooklyn Bridge plot failed because the plotters were idiots and the plot -- cutting through cables with blowtorches -- was dumb.  That, and the all-too-common police informant who egged the plotters on.
But never mind that.  Beaver Stadium is Pennsylvania State University's football stadium, and this article argues that it's a potential terrorist target that needs 24/7 police protection.
The problem with that kind of reasoning is that it makes no sense.  As I said in an article that will appear in "New Internationalist":
    To be sure, reasonable arguments can be made that some terrorist
    targets are more attractive than others: aeroplanes because a
    small bomb can result in the death of everyone aboard, monuments
    because of their national significance, national events because of
    television coverage, and transportation because of the numbers of
    people who commute daily. But there are literally millions of
    potential targets in any large country (there are five million
    commercial buildings alone in the US), and hundreds of potential
    terrorist tactics; it's impossible to defend every place against
    everything, and it's impossible to predict which tactic and target
    terrorists will try next.
Defending individual targets only makes sense if the number of potential targets is few.  If there are seven terrorist targets and you defend five of them, you seriously reduce the terrorists' ability to do damage.  But if there are a million terrorist targets and you defend five of them, the terrorists won't even notice.  I tend to dislike security measures that merely cause the bad guys to make a minor change in their And the expense would be enormous.  Add up these secondary terrorist targets -- stadiums, theaters, churches, schools, malls, office buildings, anyplace where a lot of people are packed together -- and the number is probably around 200,000, including Beaver Stadium.  Full-time police protection requires people, so that's 1,000,000 policemen.  At an encumbered cost of $100,000 per policeman per year, probably a low estimate, that's a total annual cost of $100B.  (That's about what we're spending each year in Iraq.)  On the other hand, hiring one out of every 300 Americans to guard our nation's infrastructure would solve our unemployment problem.  And since policemen get health care, our health care problem as well.  Just make sure you don't accidentally hire a terrorist to guard against terrorists -- that would be embarrassing.
The whole idea is nonsense.  As I've been saying for years, what works is investigation, intelligence, and emergency response:
    We need to defend against the broad threat of terrorism, not
    against specific movie plots. Security is most effective when it
    doesn't make arbitrary assumptions about the next terrorist act.
    We need to spend more money on intelligence and investigation:
    identifying the terrorists themselves, cutting off their funding,
    and stopping them regardless of what their plans are. We need to
    spend more money on emergency response: lessening the impact of a
    terrorist attack, regardless of what it is. And we need to face
    the geopolitical consequences of our foreign policy and how it
    helps or hinders terrorism.
Beaver Stadium piece:
Terrorists as idiots:
My essay on investigation, intelligence, and emergency response:
** *** ***** ******* *********** *************
     Schneier News
I'm speaking at Information Security Decisions in Chicago on October 21.
 or I'm speaking at the 4th International Workshop on Security in Toyama, Japan on October 28.
I'm speaking at the ISF Annual World Congress in Vancouver on November 2.
I'm speaking at the Gartner Identity and Access Management Conference in San Diego on November 9.
I'm speaking at the Internet Governance Forum in Sharm el-Sheikh, Egypt, on November 15.
** *** ***** ******* *********** *************
     Texas Instruments Signing Keys Broken
Texas Instruments' calculators use RSA digital signatures to authenticate any updates to their operating system.  Unfortunately, their signing keys are too short: 512 bits.  Earlier this month, a collaborative effort factored the moduli and published the private keys.  Texas Instruments responded by threatening websites that published the keys with the DMCA, but it's too late.
So far, we have the operating-system signing keys for the TI-92+, TI-73, TI-89, TI-83+/TI-83+ Silver Edition, Voyage 200, TI-89 Titanium, and the TI-84+/TI-84 Silver Edition, and the date-stamp signing key for the TI-73, Explorer, TI-83 Plus, TI-83 Silver Edition, TI-84 Plus, TI-84 Silver Edition, TI-89, TI-89 Titanium, TI-92 Plus, and the Voyage 200.
Moral: Don't assume that if your application is obscure, or if there's no obvious financial incentive for doing so, that your cryptography won't be broken if you use too-short keys.
 or ** *** ***** ******* *********** *************
     The Doghouse
Two entries this time.
Privacy Inside:
Both are entertaining to read.
** *** ***** ******* *********** *************
     UK Defense Security Manual Leaked
It's over 2,000 pages, so it'll take time to make any sense of. According to Ross Anderson, who's given it a quick look over, "it seems to be the bureaucratic equivalent of spaghetti code: a hodgepodge of things written by people from different backgrounds, and with different degrees of clue, in different decades."
The computer security stuff starts at page 1,531.
 or ** *** ***** ******* *********** *************
     Comments from Readers
There are thousands of comments -- many of them interesting -- on these topics on my blog. Search for the story you want to comment on, and join in.
** *** ***** ******* *********** *************
Since 1998, CRYPTO-GRAM has been a free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.  You can subscribe, unsubscribe, or change your address on the Web at .  Back issues are also available at that URL.
Please feel free to forward CRYPTO-GRAM, in whole or in part, to colleagues and friends who will find it valuable.  Permission is also granted to reprint CRYPTO-GRAM, as long as it is reprinted in its entirety.
CRYPTO-GRAM is written by Bruce Schneier.  Schneier is the author of the best sellers "Schneier on Security," "Beyond Fear," "Secrets and Lies," and "Applied Cryptography," and an inventor of the Blowfish, Twofish, Threefish, Helix, Phelix, and Skein algorithms.  He is the Chief Security Technology Officer of BT BCSG, and is on the Board of Directors of the Electronic Privacy Information Center (EPIC).  He is a frequent writer and lecturer on security topics.  See .
Crypto-Gram is a personal newsletter.  Opinions expressed are not necessarily those of BT.
Copyright (c) 2009 by Bruce Schneier.

@_date: 2009-09-14 23:04:42
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, September 15, 2009 
CRYPTO-GRAM
              September 15, 2009
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays appear in the "Schneier on Security" blog: .  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Eighth Anniversary of 9/11
     Skein News
     Real-World Access Control
     News
     File Deletion
     On London's Surveillance Cameras
     Robert Sawyer's Alibis
     Schneier News
     Stealing 130 Million Credit Card Numbers
     "The Cult of Schneier"
     Comments from Readers
** *** ***** ******* *********** *************
     Eighth Anniversary of 9/11
On September 30, 2001, I published a special issue of Crypto-Gram discussing the terrorist attacks.  I wrote about the novelty of the attacks, airplane security, diagnosing intelligence failures, the potential of regulating cryptography -- because it could be used by the terrorists -- and protecting privacy and liberty.   Much of what I wrote is still relevant today.
Me from 2006: "Refuse to be Terrorized."
** *** ***** ******* *********** *************
     Skein News
Skein is one of the 14 SHA-3 candidates chosen by NIST to advance to the second round.  As part of the process, NIST allowed the algorithm designers to implement small "tweaks" to their algorithms.  We've tweaked the rotation constants of Skein.
The revised Skein paper contains the new rotation constants, as well as information about how we chose them and why we changed them, the results of some new cryptanalysis, plus new IVs and test vectors.
Tweaks were due today, September 15.  Now the SHA-3 process moves into the second round.  According to NIST's timeline, they'll choose a set of final round candidate algorithms in 2010, and then a single hash algorithm in 2012.  Between now and then, it's up to all of us to evaluate the algorithms and let NIST know what we want.  Cryptanalysis is important, of course, but so is performance.
The second-round algorithms are: BLAKE, Blue Midnight Wish, CubeHash, ECHO, Fugue, Grxstl, Hamsi, JH, Keccak, Luffa, Shabal, SHAvite-3, SIMD, and Skein.  You can find details on all of them, as well as the current state of their cryptanalysis, at the SHA-3 Zoo.
In other news, we're making Skein shirts available to the public.  Those of you who attended the First Hash Function Candidate Conference in Leuven, Belgium, earlier this year might have noticed the stylish black Skein polo shirts worn by the Skein team.  Anyone who wants one is welcome to buy it, at cost.  All orders must be received before 1 October, and then we'll have all the shirts made in one batch.
Skein website:
Revised Skein paper:
Revised Skein source code:
My 2008 essay on SHA-3:
Details on Skein shirts:
NIST SHA-3 Website:
SHA-3 Zoo:
** *** ***** ******* *********** *************
     Real-World Access Control
Access control is difficult in an organizational setting. On one hand, every employee needs enough access to do his job. On the other hand, every time you give an employee more access, there's more risk: he could abuse that access, or lose information he has access to, or be socially engineered into giving that access to a malfeasant. So a smart, risk-conscious organization will give each employee the exact level of access he needs to do his job, and no more.
Over the years, there's been a lot of work put into role-based access control. But despite the large number of academic papers and high-profile security products, most organizations don't implement it -- at all -- with the predictable security problems as a result.
Regularly we read stories of employees abusing their database access-control privileges for personal reasons: medical records, tax records, passport records, police records. NSA eavesdroppers spy on their wives and girlfriends.  Departing employees take corporate secrets
A spectacular access control failure occurred in the UK in 2007. An employee of Her Majesty's Revenue & Customs had to send a couple of thousand sample records from a database on all children in the country to National Audit Office. But it was easier for him to copy the entire database of 25 million people onto a couple of disks and put it in the mail than it was to select out just the records needed. Unfortunately, the discs got lost in the mail and the story was a huge embarrassment for the government.
Eric Johnson at Dartmouth's Tuck School of Business has been studying the problem, and his results won't startle anyone who has thought about it at all. RBAC is very hard to implement correctly. Organizations generally don't even know who has what role. The employee doesn't know, the boss doesn't know -- and these days the employee might have more than one boss -- and senior management certainly doesn't know. There's a reason RBAC came out of the military; in that world, command structures are simple and well-defined.
Even worse, employees' roles change all the time -- Johnson chronicled one business group of 3,000 people that made 1,000 role changes in just three months -- and it's often not obvious what information an employee needs until he actually needs it. And information simply isn't that granular. Just as it's much easier to give someone access to an entire file cabinet than to only the particular files he needs, it's much easier to give someone access to an entire database than only the particular records he needs.
This means that organizations either over-entitle or under-entitle employees. But since getting the job done is more important than anything else, organizations tend to over-entitle. Johnson estimates that 50 percent to 90 percent of employees are over-entitled in large organizations. In the uncommon instance where an employee needs access to something he normally doesn't have, there's generally some process for him to get it. And access is almost never revoked once it's been granted. In large formal organizations, Johnson was able to predict how long an employee had worked there based on how much access he had.
Clearly, organizations can do better. Johnson's current work involves building access-control systems with easy self-escalation, audit to make sure that power isn't abused, violation penalties (Intel, for example, issues "speeding tickets" to violators), and compliance rewards. His goal is to implement incentives and controls that manage access without making people too risk-averse.
In the end, a perfect access control system just isn't possible; organizations are simply too chaotic for it to work. And any good system will allow a certain number of access control violations, if they're made in good faith by people just trying to do their jobs. The "speeding ticket" analogy is better than it looks: we post limits of 55 miles per hour, but generally don't start ticketing people unless they're going over 70.
This essay previously appeared in Information Security, as part of a point/counterpoint with Marcus Ranum.  You can read Marcus's response here -- after you answer some nosy questions to get a free account.
 or Role-based access control:
 or  or Abuse of database access:
 or  or  or  or  or UK access control failure:
 or Johnson's work:
 or  or ** *** ***** ******* *********** *************
     News
Flash has the equivalent of cookies, and they're hard to delete.
 or Movie-plot threat alert: robot suicide bombers.  Let's all be afraid.
 or I'm sure I've seen this stuff in movies.
It's possible to fabricate DNA evidence:
 or The legal term "terroristic threats" is older than 9/11, but these days it evokes too much of an emotional response:
Interesting developments in lie detection:
 or Marc Webar Tobias on hacking the Assa Solo lock:
Me on locks and lockpicking:
the History of Terrorism."  Yes, it's funny.  But remember that these are the terrorist masterminds that politicians invoke to keep us scared.
 or My 2007 essay, "Portrait of the Modern Terrorist as an Idiot," is also relevant.  But less funny.
Modeling zombie outbreaks: the math doesn't look good.  "When Zombies Attack!: Mathematical Modelling of an Outbreak of Zombie Infection."
It turns out that flipping a coin has all sorts of non-randomness:
 or As part of their training, federal agents engage in mock exercises in public places.  Sometimes, innocent civilians get involved.  It's actual security theater.
 or The sorts of crimes we've been seeing perpetrated against individuals are starting to be perpetrated against small businesses.  The problem will get much worse, and the security externalities means that the banks care much less.
Interesting video demonstrating how a policeman can manipulate the results of a Breathalyzer.
 or There is a movement in the U.K. to replace the pint glasses in pubs with plastic because too many of them are being used as weapons.  I don't think this will go anywhere, but the sheer idiocy is impressive. Reminds me of the call to ban pointy knives.  That recommendation also came out of the UK.  What's going on over there?
More security stories from the natural world: marine worms with glowing Weird:  "The U.S. Federal Bureau of Investigation is trying to figure out who is sending laptop computers to state governors across the U.S., including West Virginia Governor Joe Mahchin and Wyoming Governor Dave Freudenthal. Some state officials are worried that they may contain malicious software."
 or Fascinating story of a 16-year-old blind phone phreaker.
 or Hacking swine flu:  "So it takes about 25 kilobits -- 3.2 Kbytes -- of data to code for a virus that has a non-trivial chance of killing a human. This is more efficient than a computer virus, such as MyDoom, which rings in at around 22 Kbytes.  It's humbling that I could be killed by 3.2 Kbytes of genetic data. Then again, with 850 Mbytes of data in my genome, there's bound to be an exploit or two."
Good article on the exaggerated fears of cyberwar:
The real risk isn't cyberterrorism, it's cybercrime.
SIGABA and the history of one-time pads:
I wrote about one-time pads, and their practical insecurity, in 2002:
Interesting discussion of subpoenas as a security threat:
 or A very interesting hour-long interview with David Kilcullen on security and insurgency.
Nils Gilman's lecture on the global illicit economy  Malware is one of Nils Gilman's examples, at about the nine-minute mark.
The seven rules of the illicit global economy (he seems to use "illicit" and "deviant" interchangeably in the talk):
1.  Perfectly legitimate forms of demand can produce perfectly deviant forms of supply.
2.  Uneven global regulatory structures create arbitrage opportunities for deviant entrepreneurs.
3.  Pathways for legitimate globalization are always also pathways for deviant globalization.
4.  Once a deviant industry professionalizes, crackdowns merely promote 5.  States themselves undermine the distinction between legitimate and deviant economics.
6.  Unchecked, deviant entrepreneurs will overtake the legitimate economy.
7.  Deviant globalization presents an existential challenge to state Perfectly legal (obtained with a FISA warrant) NSA intercepts used to convict liquid bombers.
The BBC has a video demonstration of a 16-ounce bottle of liquid blowing a hole in the side of a plane.  I know no more details than what's in the video.
** *** ***** ******* *********** *************
     File Deletion
File deletion is all about control. This used to not be an issue. Your data was on your computer, and you decided when and how to delete a file. You could use the delete function if you didn't care about whether the file could be recovered or not, and a file erase program -- I use BCWipe for Windows -- if you wanted to ensure no one could ever recover the file.
As we move more of our data onto cloud computing platforms such as Gmail and Facebook, and closed proprietary platforms such as the Kindle and the iPhone, deleting data is much harder.
You have to trust that these companies will delete your data when you ask them to, but they're generally not interested in doing so. Sites like these are more likely to make your data inaccessible than they are to physically delete it. Facebook is a known culprit: actually deleting your data from its servers requires a complicated procedure that may or may not work. And even if you do manage to delete your data, copies are certain to remain in the companies' backup systems. Gmail explicitly says this in its privacy notice.
Online backups, SMS messages, photos on photo sharing sites, smartphone applications that store your data in the network: you have no idea what really happens when you delete pieces of data or your entire account, because you're not in control of the computers that are storing the data.
This notion of control also explains how Amazon was able to delete a book that people had previously purchased on their Kindle e-book readers. The legalities are debatable, but Amazon had the technical ability to delete the file because it controls all Kindles. It has designed the Kindle so that it determines when to update the software, whether people are allowed to buy Kindle books, and when to turn off people's Kindles entirely.
Vanish is a research project by Roxana Geambasu and colleagues at the University of Washington. They designed a prototype system that automatically deletes data after a set time interval. So you can send an e-mail, create a Google Doc, post an update to Facebook, or upload a photo to Flickr, all designed to disappear after a set period of time. And after it disappears, no one -- not anyone who downloaded the data, not the site that hosted the data, not anyone who intercepted the data in transit, not even you -- will be able to read it. If the police arrive at Facebook or Google or Flickr with a warrant, they won't be able to read it.
The details are complicated, but Vanish breaks the data's decryption key into a bunch of pieces and scatters them around the web using a peer-to-peer network. Then it uses the natural turnover in these networks -- machines constantly join and leave -- to make the data disappear. Unlike previous programs that supported file deletion, this one doesn't require you to trust any company, organization, or website. It just happens.
Of course, Vanish doesn't prevent the recipient of an e-mail or the reader of a Facebook page from copying the data and pasting it into another file, just as Kindle's deletion feature doesn't prevent people from copying a book's files and saving them on their computers. Vanish is just a prototype at this point, and it only works if all the people who read your Facebook entries or view your Flickr pictures have it installed on their computers as well; but it's a good demonstration of how control affects file deletion. And while it's a step in the right direction, it's also new and therefore deserves further security analysis before being adopted on a wide scale.
We've lost the control of data on some of the computers we own, and we've lost control of our data in the cloud. We're not going to stop using Facebook and Twitter just because they're not going to delete our data when we ask them to, and we're not going to stop using Kindles and iPhones because they may delete our data when we don't want them to. But we need to take back control of data in the cloud, and projects like Vanish show us how we can.
Now we need something that will protect our data when a large corporation decides to delete it.
This essay originally appeared in The Guardian.
 or Me on cloud computing:
Control and the iPhone:
Social networking companies don't want to delete data:
How to delete your Facebook account:
 or ** *** ***** ******* *********** *************
     On London's Surveillance Cameras
A recent report has concluded that the London's surveillance cameras have solved one crime per thousand cameras per year.
I haven't seen the report, but I know it's hard to figure out when a crime has been "solved" by a surveillance camera.  To me, the crime has to have been unsolvable without the cameras.  Repeatedly I see pro-camera lobbyists pointing to the surveillance-camera images that identified the 7/7 London Transport bombers, but it is obvious that they would have been identified even without the cameras.
And it would really help my understanding of the report's per-crime cost-to-detect of  (I assume it is calculated from  million for the cameras times 1 in 1000 cameras used to solve a crime per year divided by ten years) if I knew what sorts of crimes the cameras "solved."  If the  million solved 10,000 murders, it might very well be a good security trade-off.  But my guess is that most of the crimes were of a much lower level.
 or ** *** ***** ******* *********** *************
     Robert Sawyer's Alibis
Back in 2002, science fiction author Robert J. Sawyer wrote an essay about the trade-off between privacy and security.  I've never forgotten the first sentence:  "Whenever I visit a tourist attraction that has a guest register, I always sign it. After all, you never know when you'll need an alibi."
Since I read that, whenever I see a tourist attraction with a guest register, I do the same thing. I sign "Robert J. Sawyer, Toronto, ON" -- because you never know when he'll need an alibi.
Sawyer's essay:
Essays I've written about privacy:
** *** ***** ******* *********** *************
     Schneier News
I'm speaking at the University of Kentucky on September 17.
I'm also speaking at the II International Symposium on Network and Data Communications, in Lima, Peru on September 25.
And I'm speaking at GOVCERT.NL in Rotterdam on October 6.
Here's a video of a talk, "The Future of the Security Industry," I gave at an OWASP meeting in August in Minneapolis.
** *** ***** ******* *********** *************
     Stealing 130 Million Credit Card Numbers
Someone has been charged with stealing 130 million credit card numbers.
Yes, it's a lot, but that's the sort of quantities credit card numbers come in.  They come by the millions, in large database files.  Even if you only want ten, you have to steal millions.  I'm sure every one of us has a credit card in our wallet whose number has been stolen.  It'll probably never be used for fraudulent purposes, but it's in some stolen database somewhere.
Years ago, when giving advice on how to avoid identity theft, I would tell people to shred their trash.  Today, that advice is completely obsolete.  No one steals credit card numbers one by one out of the trash when they can be stolen by the millions from merchant databases.
** *** ***** ******* *********** *************
     "The Cult of Schneier"
If there's actually a cult out there, I want to hear about it.  In an essay by that name, John Viega writes about the dangers of relying on Applied Cryptography to design cryptosystems:
    But, after many years of evaluating the security of software
    systems, I'm incredibly down on using the book that made Bruce
    famous when designing the cryptographic aspects of a system. In
    fact, I can safely say I have never seen a secure system come out
    the other end, when that is the primary source for the crypto
    design. And I don't mean that people forget about the buffer
    overflows. I mean, the crypto is crappy.
    My rule for software development teams is simple: Don't use
    Applied Cryptography in your system design. It's fine and
    fun to read it, just don't build from it.
    [...]
    The book talks about the fundamental building blocks of
    cryptography, but there is no guidance on things like, putting
    together all the pieces to create a secure, authenticated
    connection between two parties.
    Plus, in the nearly 13 years since the book was last revised, our
    understanding of cryptography has changed greatly. There are
    things in it that were thought to be true at the time that turned
    out to be very false....
I agree.  And, to his credit, Viega points out that I agree:
    But in the introduction to Bruce Schneier's book, Practical
    Cryptography, he himself says that the world is filled with
    broken systems built from his earlier book. In fact, he wrote
    Practical Cryptography in hopes of rectifying the problem.
This is all true.
Designing a cryptosystem is hard.  Just as you wouldn't give a person -- even a doctor -- a brain-surgery instruction manual and then expect him to operate on live patients, you shouldn't give an engineer a cryptography book and then expect him to design and implement a cryptosystem.  The patient is unlikely to survive, and the cryptosystem is unlikely to be secure.
Even worse, security doesn't provide immediate feedback.  A dead patient on the operating table tells the doctor that maybe he doesn't understand brain surgery just because he read a book, but an insecure cryptosystem works just fine.  It's not until someone takes the time to break it that the engineer might realize that he didn't do as good a job as he thought.  Remember: Anyone can design a security system that he himself cannot break.  Even the experts regularly get it wrong.  The odds that an amateur will get it right are extremely low.
For those who are interested, a second edition of Practical Cryptography will be published in early 2010, renamed Cryptography Engineering and featuring a third author: Tadayoshi Kohno.
Applied Cryptography:
Practical Cryptography:
** *** ***** ******* *********** *************
     Comments from Readers
There are thousands of comments -- many of them interesting -- on these topics on my blog. Search for the story you want to comment on, and join in.
** *** ***** ******* *********** *************
Since 1998, CRYPTO-GRAM has been a free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.  You can subscribe, unsubscribe, or change your address on the Web at .  Back issues are also available at that URL.
Please feel free to forward CRYPTO-GRAM, in whole or in part, to colleagues and friends who will find it valuable.  Permission is also granted to reprint CRYPTO-GRAM, as long as it is reprinted in its entirety.
CRYPTO-GRAM is written by Bruce Schneier.  Schneier is the author of the best sellers "Schneier on Security," "Beyond Fear," "Secrets and Lies," and "Applied Cryptography," and an inventor of the Blowfish, Twofish, Threefish, Helix, Phelix, and Skein algorithms.  He is the Chief Security Technology Officer of BT BCSG, and is on the Board of Directors of the Electronic Privacy Information Center (EPIC).  He is a frequent writer and lecturer on security topics.  See .
Crypto-Gram is a personal newsletter.  Opinions expressed are not necessarily those of BT.
Copyright (c) 2009 by Bruce Schneier.

@_date: 2010-07-14 21:50:48
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, July 15, 2010 
CRYPTO-GRAM
                July 15, 2010
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays and news items appear in the "Schneier on Security" blog at , along with a lively comment section.  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     The Threat of Cyberwar Has Been Grossly Exaggerated
     Internet Kill Switch
     News
     Third SHB Workshop
     Schneier News
     Data at Rest vs. Data in Motion
     Reading Me
** *** ***** ******* *********** *************
     The Threat of Cyberwar Has Been Grossly Exaggerated
There's a power struggle going on in the U.S. government right now.
It's about who is in charge of cyber security, and how much control the government will exert over civilian networks. And by beating the drums of war, the military is coming out on top.
"The United States is fighting a cyberwar today, and we are losing," said former NSA director -- and current cyberwar contractor -- Mike McConnell. "Cyber 9/11 has happened over the last ten years, but it happened slowly so we don't see it," said former National Cyber Security Division director Amit Yoran. Richard Clarke, whom Yoran replaced, wrote an entire book hyping the threat of cyberwar.
General Keith Alexander, the current commander of the U.S. Cyber Command, hypes it every chance he gets. This isn't just rhetoric of a few over-eager government officials and headline writers; the entire national debate on cyberwar is plagued with exaggerations and hyperbole.
Googling those names and terms -- as well as "cyber Pearl Harbor," "cyber Katrina," and even "cyber Armageddon" -- gives some idea how pervasive these memes are. Prefix "cyber" to something scary, and you end up with something really scary.
Cyberspace has all sorts of threats, day in and day out. Cybercrime is by far the largest: fraud, through identity theft and other means, extortion, and so on. Cyber-espionage is another, both government- and corporate-sponsored. Traditional hacking, without a profit motive, is still a threat. So is cyber-activism: people, most often kids, playing politics by attacking government and corporate websites and networks.
These threats cover a wide variety of perpetrators, motivations, tactics, and goals. You can see this variety in what the media has mislabeled as "cyberwar." The attacks against Estonian websites in 2007 were simple hacking attacks by ethnic Russians angry at anti-Russian policies; these were denial-of-service attacks, a normal risk in cyberspace and hardly unprecedented.
A real-world comparison might be if an army invaded a country, then all got in line in front of people at the DMV so they couldn't renew their licenses. If that's what war looks like in the 21st century, we have little to fear.
Similar attacks against Georgia, which accompanied an actual Russian invasion, were also probably the responsibility of citizen activists or organized crime. A series of power blackouts in Brazil was caused by criminal extortionists -- or was it sooty insulators? China is engaging in espionage, not war, in cyberspace. And so on.
One problem is that there's no clear definition of "cyberwar." What does it look like? How does it start? When is it over? Even cybersecurity experts don't know the answers to these questions, and it's dangerous to broadly apply the term "war" unless we know a war is going on.
Yet recent news articles have claimed that China declared cyberwar on Google, that Germany attacked China, and that a group of young hackers declared cyberwar on Australia. (Yes, cyberwar is so easy that even kids can do it.) Clearly we're not talking about real war here, but a rhetorical war: like the war on terror.
We have a variety of institutions that can defend us when attacked: the police, the military, the Department of Homeland Security, various commercial products and services, and our own personal or corporate lawyers. The legal framework for any particular attack depends on two things: the attacker and the motive. Those are precisely the two things you don't know when you're being attacked on the Internet. We saw this on July 4 last year, when U.S. and South Korean websites were attacked by unknown perpetrators from North Korea -- or perhaps England. Or was it Florida?
We surely need to improve our cybersecurity. But words have meaning, and metaphors matter. There's a power struggle going on for control of our nation's cybersecurity strategy, and the NSA and DoD are winning. If we frame the debate in terms of war, if we accept the military's expansive cyberspace definition of "war," we feed our fears.
We reinforce the notion that we're helpless -- what person or organization can defend itself in a war? -- and others need to protect us. We invite the military to take over security, and to ignore the limits on power that often get jettisoned during wartime.
If, on the other hand, we use the more measured language of cybercrime, we change the debate. Crime fighting requires both resolve and resources, but it's done within the context of normal life. We willingly give our police extraordinary powers of investigation and arrest, but we temper these powers with a judicial system and legal protections for We need to be prepared for war, and a Cyber Command is just as vital as an Army or a Strategic Air Command. And because kid hackers and cyber-warriors use the same tactics, the defenses we build against crime and espionage will also protect us from more concerted attacks. But we're not fighting a cyberwar now, and the risks of a cyberwar are no greater than the risks of a ground invasion. We need peacetime cyber-security, administered within the myriad structure of public and private security institutions we already have.
This essay previously appeared on CNN.com.
 or  or  or  or  or  or  or  or  or  or  or  or  or  or Good article:
Earlier this month, I participated in a debate: "The Cyberwar Threat has been Grossly Exaggerated."  Marc Rotenberg of EPIC and I were for the motion; Mike McConnell and Jonathan Zittrain were against.  We lost.
We lost fair and square, for a bunch of reasons -- we didn't present our case very well, Jonathan Zittrain is a way better debater than we were

@_date: 2010-11-14 23:16:43
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, November 15, 2010 
CRYPTO-GRAM
              November 15, 2010
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and  commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit  You can read this issue on the web at  .  These same essays and  news items appear in the "Schneier on Security" blog at  , along with a lively comment section.  An  RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Crowdsourcing Surveillance
     Internet Quarantines
     News
     Cargo Security
     Changes in Airplane Security
     Young Man in "Old Man" Mask Boards Plane in Hong Kong
     Schneier News
     Kahn, Diffie, Clark, and Me at Bletchley Park
     Changing Passwords
** *** ***** ******* *********** *************
     Crowdsourcing Surveillance
Internet Eyes is a U.K. startup designed to crowdsource digital  surveillance. People pay a small fee to become a "Viewer." Once they do,  they can log onto the site and view live anonymous feeds from surveillance cameras at retail stores.  If they notice someone shoplifting, they can alert the store owner. Viewers get rated on their ability to differentiate real shoplifting from false alarms, can win 1000 pounds if they detect the most shoplifting in some time interval, and otherwise get paid a wage that most likely won't cover their initial fee.
Although the system has some nod towards privacy, groups like Privacy  International oppose the system for fostering a culture of citizen spies. More fundamentally, though, I don't think the system will work. Internet Eyes is primarily relying on voyeurism to compensate its Viewers. But most of what goes on in a retail store is incredibly boring. Some of it is actually voyeuristic, and very little of it is criminal. The incentives just aren't there for Viewers to do more than peek, and there's no obvious way to discouraging them from siding with the shoplifter and just watch the scenario unfold.
This isn't the first time groups have tried to crowdsource surveillance  camera monitoring.  Texas's Virtual Border Patrol tried the same thing:  deputizing the general public to monitor the Texas-Mexico border.  It ran out of money last year, and was widely criticized as a joke.
This system suffered the same problems as Internet Eyes -- not enough  incentive to do a good job, boredom because crime is the rare exception -- as well as the fact that false alarms were very expensive to deal with.
Both of these systems remind me of the one time this idea was  conceptualized correctly.  Invented in 2003 by my friend and colleague Jay Walker, US HomeGuard also tried to crowdsource surveillance camera  monitoring. But this system focused on one very specific security concern: people in no-mans areas. These are areas between fences at nuclear power plants or oil refineries, border zones, areas around dams and reservoirs, and so on: areas where there should never be anyone.
The idea is that people would register to become "spotters." They would  get paid a decent wage (that and patriotism was the incentive), receive a stream of still photos, and be asked a very simple question: "Is there a person or a vehicle in this picture?"  If a spotter clicked "yes," the  photo -- and the camera -- would be referred to whatever professional  response the camera owner had set up.
HomeGuard would monitor the monitors in two ways. One, by sending stored, known, photos to people regularly to verify that they were paying attention. And two, by sending live photos to multiple spotters and correlating the results, to many more monitors if a spotter claimed to have spotted a person or vehicle.
Just knowing that there's a person or a vehicle in a no-mans-area is only the first step in a useful response, and HomeGuard envisioned a bunch of enhancements to the rest of that system.  Flagged photos could be sent to the digital phones of patrolling guards, cameras could be controlled remotely by those guards, and speakers in the cameras could issue warnings. Remote citizen spotters were only useful for that first step, looking for a person or a vehicle in a photo that shouldn't contain any. Only real guards at the site itself could tell an intruder from the occasional maintenance Of course the system isn't perfect. A would-be infiltrator could sneak  past the spotters by holding a bush in front of him, or disguising himself as a vending machine.  But it does fill in a gap in what fully automated systems can do, at least until image processing and artificial  intelligence get significantly better.
HomeGuard never got off the ground. There was never any good data about  whether spotters were more effective than motion sensors as a first level of defense. But more importantly, Walker says that the politics  surrounding homeland security money post-9/11 was just too great to  penetrate, and that as an outsider he couldn't get his ideas heard. Today, probably, the patriotic fervor that gripped so many people post-9/11 has dampened, and he'd probably have to pay his spotters more than he envisioned seven years ago. Still, I thought it was a clever idea then and I still think it's a clever idea -- and it's an example of how to do surveillance crowdsourcing correctly.
Making the system more general runs into all sorts of problems. An amateur can spot a person or vehicle pretty easily, but is much harder pressed to notice a shoplifter. The privacy implications of showing random people pictures of no-man's-lands is minimal, while a busy store is another matter

@_date: 2010-10-15 03:17:12
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, OCTOBER 15, 2010 
CRYPTO-GRAM
              October 15, 2010
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and  commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit  You can read this issue on the web at  .  These same essays and  news items appear in the "Schneier on Security" blog at  , along with a lively comment section.  An  RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Wiretapping the Internet
     News
     Me on Cyberwar
     Putting Unique Codes on Objects to Detect Counterfeiting
     Schneier News
     Stuxnet
** *** ***** ******* *********** *************
     Wiretapping the Internet
In September, The New York Times reported that President Obama will seek  sweeping laws enabling law enforcement to more easily eavesdrop on the  internet. Technologies are changing, the administration argues, and modern digital systems aren't as easy to monitor as traditional telephones.
The government wants to force companies to redesign their communications  systems and information networks to facilitate surveillance, and to  provide law enforcement with back doors that enable them to bypass any  security measures.
The proposal may seem extreme, but -- unfortunately -- it's not unique.  Just a few months ago, the governments of the United Arab Emirates and  Saudi Arabia threatened to ban BlackBerry devices unless the company made eavesdropping easier. China has already built a massive internet  surveillance system to better control its citizens.
Formerly reserved for totalitarian countries, this wholesale surveillance of citizens has moved into the democratic world as well. Governments like Sweden, Canada and the United Kingdom are debating or passing laws giving their police new powers of internet surveillance, in many cases requiring communications system providers to redesign products and services they sell. More are passing data retention laws, forcing companies to retain customer data in case they might need to be investigated later.
Obama isn't the first U.S. president to seek expanded digital  eavesdropping. The 1994 CALEA law required phone companies to build ways  to better facilitate FBI eavesdropping into their digital phone switches. Since 2001, the National Security Agency has built substantial  eavesdropping systems within the United States.
These laws are dangerous, both for citizens of countries like China and  citizens of Western democracies. Forcing companies to redesign their  communications products and services to facilitate government  eavesdropping reduces privacy and liberty; that's obvious. But the laws  also make us less safe. Communications systems that have no inherent  eavesdropping capabilities are more secure than systems with those  capabilities built in.
Any surveillance system invites both criminal appropriation and government abuse. Function creep is the most obvious abuse: New police powers, enacted to fight terrorism, are already used in situations of conventional nonterrorist crime. Internet surveillance and control will be no different.
Official misuses are bad enough, but the unofficial uses are far more  worrisome. An infrastructure conducive to surveillance and control invites surveillance and control, both by the people you expect and the people you don't. Any surveillance and control system must itself be secured, and we're not very good at that. Why does anyone think that only authorized law enforcement will mine collected internet data or eavesdrop on Skype and IM These risks are not theoretical. After 9/11, the National Security Agency built a surveillance infrastructure to eavesdrop on telephone calls and e-mails within the United States. Although procedural rules stated that only non-Americans and international phone calls were to be listened to, actual practice didn't always match those rules. NSA analysts collected more data than they were authorized to and used the system to spy on wives, girlfriends and famous people like former President Bill Clinton.
The most serious known misuse of a telecommunications surveillance  infrastructure took place in Greece. Between June 2004 and March 2005,  someone wiretapped more than 100 cell phones belonging to members of the  Greek government -- the prime minister and the ministers of defense,  foreign affairs and justice -- and other prominent people. Ericsson built this wiretapping capability into Vodafone's products, but enabled it only for governments that requested it. Greece wasn't one of those governments, but some still unknown party -- a rival political group? organized crime?

@_date: 2010-09-14 23:58:08
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, September 15, 2010 
CRYPTO-GRAM
             September 15, 2010
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit You can read this issue on the web at .  These same essays and news items appear in the "Schneier on Security" blog at , along with a lively comment section.  An RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Consumerization and Corporate IT Security
     News
     Schneier News
     More Skein News
     Wanted: Skein Hardware Help
** *** ***** ******* *********** *************
     Consumerization and Corporate IT Security
If you're a typical wired American, you've got a bunch of tech tools you like and a bunch more you covet. You have a cell phone that can easily text. You've got a laptop configured just the way you want it. Maybe you have a Kindle for reading, or an iPad. And when the next new thing comes along, some of you will line up on the first day it's available.
So why can't work keep up? Why are you forced to use an unfamiliar, and sometimes outdated, operating system? Why do you need a second laptop, maybe an older and clunkier one? Why do you need a second cell phone with a new interface, or a BlackBerry, when your phone already does e-mail? Or a second BlackBerry tied to corporate e-mail? Why can't you use the cool stuff you already have?
More and more companies are letting you. They're giving you an allowance and allowing you to buy whatever laptop you want, and to connect into the corporate network with whatever device you choose. They're allowing you to use whatever cell phone you have, whatever portable e-mail device you have, whatever you personally need to get your job done. And the security office is freaking.
You can't blame them, really. Security is hard enough when you have control of the hardware, operating system and software. Lose control of any of those things, and the difficulty goes through the roof. How do you ensure that the employee devices are secure, and have up-to-date security patches? How do you control what goes on them? How do you deal with the tech support issues when they fail? How do you even begin to manage this logistical nightmare? Better to dig your heels in and say "no."
But security is on the losing end of this argument, and the sooner it realizes that, the better.
The meta-trend here is consumerization: cool technologies show up for the consumer market before they're available to the business market. Every corporation is under pressure from its employees to allow them to use these new technologies at work, and that pressure is only getting stronger. Younger employees simply aren't going to stand for using last year's stuff, and they're not going to carry around a second laptop. They're either going to figure out ways around the corporate security rules, or they're going to take another job with a more trendy company. Either way, senior management is going to tell security to get out of the way. It might even be the CEO, who wants to get to the company's databases from his brand new iPad, driving the change. Either way, it's going to be harder and harder to say no.
At the same time, cloud computing makes this easier. More and more, employee computing devices are nothing more than dumb terminals with a browser interface. When corporate e-mail is all webmail, corporate documents are all on GoogleDocs, and when all the specialized applications have a web interface, it's easier to allow employees to use any up-to-date browser. It's what companies are already doing with their partners, suppliers, and customers.
Also on the plus side, technology companies have woken up to this trend and -- from Microsoft and Cisco on down to the startups -- are trying to offer security solutions. Like everything else, it's a mixed bag: some of them will work and some of them won't, most of them will need careful configuration to work well, and few of them will get it right. The result is that we'll muddle through, as usual.
Security is always a tradeoff, and security decisions are often made for non-security reasons. In this case, the right decision is to sacrifice security for convenience and flexibility. Corporations want their employees to be able to work from anywhere, and they're going to have loosened control over the tools they allow in order to get it.
This essay first appeared as the second half of a point/counterpoint with Marcus Ranum in Information Security Magazine.  You can read Marcus's half there.
 or ** *** ***** ******* *********** *************
     News
Breaking into a garage in seconds.  Garage doors with automatic openers have always seemed like a lot of security theater to me: people regularly treat their garage door as if it had the same security as their front door.
Hacking cars through wireless tire-pressure sensors.  It's minor, but this kind of thing is only going to get worse.
 or  or Earlier paper on automobile computer security:
Good essay by Seth Godin on the "Fear Tax":
Intel buying McAfee is another example of a large non-security company buying a security company.  I've been talking about this sort of thing for two and a half years.
Malware might have been a contributory cause of an air crash.  I say "might" because it's hard to get reliable information.
Skeletal identification:
And you thought fingerprints were intrusive.
danah boyd on social steganography:
 or Detecting deception in conference calls:
Their detection system is only slightly better than random, but this kind of thing will only get better.
Full-body scanners in roving vans:
Since a fatal crash a few years ago, Boston T (their subway) operators have been forbidden from using -- or even having -- cell phones while on the job.  Passengers are encouraged to report violators.  But sometimes T operators need to use their official radios on the job, and passengers can't tell the difference.  The solution: mark their official radios with orange tape.  Of course, no T operator would ever think of putting bright orange tape on his cell phone.  Because if he did that, the passengers would immediately know not to report him.
 or Chilling interview about misidentification and the court system.
In Australia, a high school teacher assigned a movie-plot threat contest problem to his students, and everyone went crazy.  He sounds like me, Australian police are claiming the assignment was illegal, so Australians who enter my movie-plot threat contests should think twice. Also anyone writing a thriller novel about terrorism, perhaps.
Interesting research: eavesdropping on smart homes with distributed wireless sensors.
 or This, about the Pentagon and cyber-offense, is beyond stupid.
Very clever attack against a quantum cryptography system.
UAE man-in-the-middle attack against SSL.
Great article on terrorism entrapment:
 or Parental fears vs. realities:
 or The new German ID card is hackable.  No surprise there.
In Japan, paint-filled orange balls are an anti-robbery device.
Problems with Twitter's OAuth authentication system.
 or  or The Onion on national security:  "Smart, Qualified People Behind the Scenes Keeping America Safe: 'We Don't Exist.'"
 or Kenzero is a Japanese Trojan that collects and publishes users' porn surfing habits, and then blackmails them to remove the information.
Vulnerabilities in US-CERT network:
 or Not answering questions at U.S. customs.
 or Police set up a highway sign warning motorists that there are random stops for narcotics checks ahead, but actually search people who take the next exit.
Popular usernames and passwords, in graphical form.
** *** ***** ******* *********** *************
     Schneier News
Back in May, I attended the EastWest Institute's First Worldwide Cybersecurity Summit in Dallas.  I only had eight minutes to speak, and tried to turn the dialog to security, privacy, and the individual.
The conference:
Commentary on my short talk:
On September 16, I'll be a keynote speaker at IDC's IT Security Conference 2010 in London.
 or On September 18, I'll be a keynote speaker at Hacktivity in Budapest.
On October 1, I'll be a keynote speaker at CELAES 2010: XXV FELABAN Conference on Bank Security in Miami.
On October 8, I'll be giving a luncheon keynote speech at the Minnesota Library Association Conference in Rochester, MN.
On October 12, I'll be a keynote speaker at RSA Europe in London.
** *** ***** ******* *********** *************
     More Skein News
Skein is my new hash function.   Well, "my" is an overstatement; I'm one of the eight designers.  It was submitted to NIST for their SHA-3 competition, and one of the 14 algorithms selected to advance to the second round.
Last week was the Second SHA-3 Candidate Conference.  Lots of people presented papers on the candidates: cryptanalysis papers, implementation papers, performance comparisons, etc.  There were two cryptanalysis papers on Skein.  The first was by Kerry McKay and Poorvi L. Vora. They tried to extend linear cryptanalysis to groups of bits to attack Threefish (the block cipher inside Skein). It was a nice analysis, but it didn't get very far at all.
The second was a fantastic piece of cryptanalysis by Dmitry Khovratovich, Ivica Nikolie, and Christian Rechberger.  They used a rotational rebound attack to mount a "known-key distinguisher attack" on 57 out of 72 Threefish rounds faster than brute force.  It's a new type of attack -- some go so far as to call it an "observation" -- and the community is still trying to figure out what it means.  It only works if the attacker can manipulate both the plaintexts and the keys in a structured way.  Against 57-round Threefish, it requires 2**503 work -- barely better than brute force.  And it only distinguishes reduced-round Threefish from a random permutation; it doesn't actually recover any key Even with the attack, Threefish has a good security margin.  Also, the attack doesn't affect Skein.  But changing one constant in the algorithm's key schedule makes the attack impossible.  NIST has said they're allowing second-round tweaks, so we're going to make the change.  It won't affect any performance numbers or obviate any other cryptanalytic results -- but the best attack would be 33 out of 72 rounds.
The second-round algorithms are: BLAKE, Blue Midnight Wish, CubeHash, ECHO, Fugue, Grostl, Hamsi, JH, Keccak, Luffa, Shabal, SHAvite-3, SIMD, and Skein.  You can find details on all of them, as well as the current state of their cryptanalysis, at the SHA-2 Zoo site.  NIST will select approximately five algorithms to go on to the third round by the end of the year.
In other news, we're once again making Skein polo shirts available to the public.  Those of you who attended either of the two SHA-3 conferences might have noticed the stylish black Skein polo shirts worn by the Skein team.  Anyone who wants one is welcome to buy it, at cost.  All orders must be received before October 1, and we'll have all the shirts made in one batch.
The Second SHA-3 Candidate Conference:
Conference program:
 or Kerry McKay and Poorvi L. Vora's presentation and paper:
 or  or Dmitry Khovratovich, Ivica Nikolie, and Christian Rechberger's presentation and paper:
 or  or Known-key distinguisher:
 or Our Skein update from the SHA-3 conference:
 or Skein website:
Skein paper:
Skein source code:
My previous essays on Skein:
SHA-3 website:
SHA-3 Zoo:
** *** ***** ******* *********** *************
     Wanted: Skein Hardware Help
As part of NIST's SHA-3 selection process, people have been implementing the candidate hash functions on a variety of hardware and software platforms.  Our team has implemented Skein in Intel's 32 nm ASIC process, and got some impressive performance results. Several other groups have implemented Skein in FPGA and ASIC, and have seen significantly poorer performance.  We need help understanding why.
For example, a group led by Brian Baldwin at the Claude Shannon Institute for Discrete Mathematics, Coding and Cryptography implemented all the second-round candidates in FPGA.  Skein performance was terrible, but when they checked their code, they found an error.  Their corrected performance comparison has Skein performing much better and in the top ten.
We suspect that the adders in all the designs may not be properly optimized, although there may be other performance issues.  If we can at least identify (or possibly even fix) the slowdowns in the design, it would be very helpful, both for our understanding and for Skein's hardware profile. Even if we find that the designs are properly optimized, that would also be good to know.
A group at George Mason University led by Kris Gaj implemented all the second-round candidates in FPGA.  Skein had the worst performance of any of the implementations.  We're looking for someone who can help us understand the design, and determine if it can be improved.
Another group, led by Stefan Tillich at University of Bristol, implemented all the candidates in 180 nm custom ASIC. Here, Skein is one of the worst performers.  We're looking for someone who can help us understand what this group did.
Three other groups -- one led by Patrick Schaumont of Virginia Tech, another led by Shin'ichiro Matsuo at National Institute of Information and Communications Technology in Japan, and a third led by Luca Henzen at ETH Zurich -- implemented the SHA-3 candidates.  Again, we need help understanding how their Skein performance numbers are so different from We're looking for people with FPGA and ASIC skills to work with the Skein team.  We don't have money to pay anyone; co-authorship on a paper

@_date: 2011-04-15 02:13:34
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, April 15, 2011 
CRYPTO-GRAM
                April 15, 2011
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and  commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit  You can read this issue on the web at  .  These same essays and  news items appear in the "Schneier on Security" blog at  , along with a lively comment section.  An  RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Detecting Cheaters
     Ebook Fraud
     Unanticipated Security Risk of Keeping Your Money in a Home Safe
     News
     Changing Incentives Creates Security Risks
     Euro Coin Recycling Scam
     Security Fears of Wi-Fi in London Underground
     Schneier News
     Epsilon Hack
     Schneier's Law
     How did the CIA and FBI Know that Australian Government
       Computers Were Hacked?
** *** ***** ******* *********** *************
     Detecting Cheaters
Our brains are specially designed to deal with cheating in social  exchanges.  The evolutionary psychology explanation is that we evolved  brain heuristics for the social problems that our prehistoric ancestors  had to deal with.  Once humans became good at cheating, they then had to  become good at detecting cheating -- otherwise, the social group would  fall apart.
Perhaps the most vivid demonstration of this can be seen with variations  on what's known as the Wason selection task, named after the psychologist who first studied it.  Back in the 1960s, it was a test of logical reasoning; today, its used more as a demonstration of evolutionary psychology. But before we get to the experiment, let's get into the mathematical background.
Propositional calculus is a system for deducing conclusions from true  premises.  It uses variables for statements because the logic works  regardless of what the statements are. College courses on the subject are taught by either the mathematics or the philosophy department, and they're not generally considered to be easy classes.  Two particular rules of inference are relevant here: *modus ponens* and *modus tollens*.  Both allow you to reason from a statement of the form, "if P, then Q."  (If Socrates was a man, then Socrates was mortal. If you are to eat dessert, then you must first eat your vegetables.  If it is raining, then Gwendolyn had Crunchy Wunchies for breakfast. That sort of thing.)  *Modus ponens* goes like this:
    If P, then Q. P. Therefore, Q.
In other words, if you assume the conditional rule is true, and if you  assume the antecedent of that rule is true, then the consequent is true.  If Socrates was a man, then Socrates was mortal.  Socrates was a man.  Therefore, Socrates was mortal.
*Modus tollens* is more complicated:
    If P, then Q.  Not Q.  Therefore, not P.
    If Socrates was a man, then Socrates was mortal.  Socrates was not
    mortal. Therefore, Socrates was not a man.
This makes sense: if Socrates was not mortal, then he'd be a demigod or a stone statue or something.
Both are valid forms of logical reasoning.  If you know "if P, then Q" and "P," then you know "Q."  If you know "if P, then Q" and "not Q," then you know "not P."  (The other two similar forms don't work.  If you know "if P, then Q" and "Q," you don't know anything about "P."  And if you know "if P, then Q" and "not P," then you don't know anything about "Q.")
If I explained this in front of an audience full of normal people, not  mathematicians or philosophers, most of them would be lost.  Unsurprisingly, they would have trouble either explaining the rules or  using them properly. Just ask any grad student who has had to teach a  formal logic class; people have trouble with this.
Consider the Wason selection task.  Subjects are presented with four cards next to each other on a table.  Each card represents a person, with each side listing some statement about that person.  The subject is then given a general rule and asked which cards he would have to turn over to ensure that the four people satisfied that rule.  For example, the general rule might be, "If a person travels to Boston, then he or she takes a plane."  The four cards might correspond to travelers and have a destination on one side and a mode of transport on the other.  On the side facing the subject, they read: "went to Boston," "went to New York," "took a plane," and "took a car."  Formal logic states that the rule is violated if someone goes to Boston without taking a plane. Translating into propositional calculus, there's the general rule: "if P, then Q."  The four cards are "P," "not P," "Q," and "not Q."  To verify that "if P, then Q" is a valid rule, you have  to verify *modus ponens* by turning over the "P" card and making sure that the reverse says "Q."  To verify *modus tollens*, you turn over the "not Q" card and make sure that the reverse doesn't say "P."
Shifting back to the example, you need to turn over the "went to Boston"  card to make sure that person took a plane, and you need to turn over the "took a car" card to make sure that person didn't go to Boston.  You don't

@_date: 2011-03-15 01:23:43
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, March 15, 2011 
CRYPTO-GRAM
                March 15, 2011
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and  commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit  You can read this issue on the web at  .  These same essays and  news items appear in the "Schneier on Security" blog at  , along with a lively comment section.  An  RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Anonymous vs. HBGary
     News
     Schneier News
     NIST Defines New Versions of SHA-512
** *** ***** ******* *********** *************
     Anonymous vs. HBGary
One of the effects of writing a book is that I don't have the time to  devote to other writing.  So while I've been wanting to write about  Anonymous vs. HBGary, I don't think I will have time.  Here's an excellent series of posts on the topic from ArsTechnica.
In cyberspace, the balance of power is on the side of the attacker.  Attacking a network is *much* easier than defending a network.  That may  change eventually -- there might someday be the cyberspace equivalent of  trench warfare, where the defender has the natural advantage -- but not  anytime soon.
 or  or  or  or  or  or This is a really good piece by Paul Roberts on Anonymous vs. HBGary: not  the tactics or the politics, but what HBGary demonstrates about the IT  security industry.
 or Stephen Colbert on HBGary:
 or  or Another article:
 or ** *** ***** ******* *********** *************
     News
Interesting article from Wired: "How a Remote Town in Romania Has Become  Cybercrime Central."
Recently declassified: "Historical Study: The National Security Agency  Scientific Advisory Board 1952b1963."
A physical biometric wallet: $825.
 or  or I don't think I understand the threat model.  If your wallet is stolen,  you're going to replace all your ID cards and credit cards and you're not going to get your cash back -- whether it's a normal wallet or this  wallet.  I suppose this wallet makes it less likely that someone will use your stolen credit cards quickly, before you cancel them.  But you're not going to be liable for costs incurred during that delay in any case.
Interesting story about a con man who conned the U.S. government, and how the government is trying to hide its dealings with him.
Susan Landau's testimony before the House Judiciary Committee,  Subcommittee on Crime, Terrorism, and Homeland Security on government  The testimony of Valerie Caproni, General Counsel of the FBI, on the same  or Good article about the terrorist non-threat from Reason:
"Reliably Erasing Data From Flash-Based Solid State Drives," by Michael  Wei, Laura M. Grupp, Frederick E. Spada, and Steven Swanson.
News article:
Video of talk:
NIST has finally published its rationale for selecting the five SHA-3    or Pickpocketing as a trade is dying out in America, because there's no one  to train newer pickpockets in the craft.
Interesting research in using animals to detect substances.  Basically,  sniffer dogs respond to unconscious cues from their handlers, and generate false alarms because of them.  It makes sense, as dogs are so attuned to humans.  I'll bet bomb-sniffing bees don't make the same mistakes.
Full paper:
Bomb-sniffing bees:
"American Cryptography During the Cold War 1945-1989; Book IV: Cryptologic Rebirth 1981-1989."  Document was first declassified in 2009.   Here are some newly declassified pages.
Criminals are stealing cars by calling tow trucks.  It's a clever hack,  but an old problem: the authentication in these sorts of normal operations isn't good enough to prevent abuse.
A programmer installed malware into the Whack-a-Mole arcade game as a form of job security.  It didn't work.
Wired.com has a good three-part story on full-body scanners.
Another attempt to sort out scanner claims:
Using language patterns to identify anonymous email.  It only works when  there's a limited number of potential authors.
** *** ***** ******* *********** *************
     Schneier News
I'm speaking at Black Hat Europe in Barcelona on March 17.
I'm speaking at the Oracle Chief Security Officer Summit in New York City on March 30.
This three-part video interview with me was conducted at the RSA  Conference last month.
 or  or  or I was interviewed on chomp.fm.
** *** ***** ******* *********** *************
     NIST Defines New Versions of SHA-512
NIST has just defined two new versions of SHA-512.  They're SHA-512/224  and SHA-512/256: 224- and 256-bit truncations of SHA-512 with a new IV.  They've done this because SHA-512 is faster than SHA-256 on 64-bit CPUs, so these new SHA variants will be faster.
This is a good thing, and exactly what we did in the design of Skein. We defined different outputs for the same state size, because it makes sense to decouple the internal workings of the hash function from the output   or ** *** ***** ******* *********** *************
Since 1998, CRYPTO-GRAM has been a free monthly newsletter providing  summaries, analyses, insights, and commentaries on security: computer and otherwise.  You can subscribe, unsubscribe, or change your address on the Web at .  Back issues are also available at that URL.
Please feel free to forward CRYPTO-GRAM, in whole or in part, to  colleagues and friends who will find it valuable.  Permission is also  granted to reprint CRYPTO-GRAM, as long as it is reprinted in its entirety.
CRYPTO-GRAM is written by Bruce Schneier.  Schneier is the author of the  best sellers "Schneier on Security," "Beyond Fear," "Secrets and Lies,"  and "Applied Cryptography," and an inventor of the Blowfish, Twofish,  Threefish, Helix, Phelix, and Skein algorithms.  He is the Chief Security Technology Officer of BT BCSG, and is on the Board of Directors of the Electronic Privacy Information Center (EPIC).  He is a frequent writer and lecturer on security topics.  See .
Crypto-Gram is a personal newsletter.  Opinions expressed are not  necessarily those of BT.
Copyright (c) 2011 by Bruce Schneier.

@_date: 2011-05-14 20:04:28
@_author: Bruce Schneier 
@_subject: CRYPTO-GRAM, May 15, 2011 
CRYPTO-GRAM
                 May 15, 2011
              by Bruce Schneier
      Chief Security Technology Officer, BT
             schneier at schneier.com
            A free monthly newsletter providing summaries, analyses, insights, and  commentaries on security: computer and otherwise.
For back issues, or to subscribe, visit  You can read this issue on the web at  .  These same essays and  news items appear in the "Schneier on Security" blog at  , along with a lively comment section.  An  RSS feed is available.
** *** ***** ******* *********** *************
In this issue:
     Status Report: "The Dishonest Minority"
     RFID Tags Protecting Hotel Towels
     News
     Hijacking the Coreflood Botnet
     Schneier News
     Drugging People and Then Robbing Them
     Interviews with Me About the Sony Hack
** *** ***** ******* *********** *************
     Status Report: "The Dishonest Minority"
Three months ago, I announced that I was writing a book on why security  exists in human societies.  This is basically the book's thesis statement:
    All complex systems contain parasites.  In any system of
    cooperative behavior, an uncooperative strategy will be effective
    -- and the system will tolerate the uncooperatives -- as long as
    they're not too numerous or too effective.	Thus, as a species
    evolves cooperative behavior, it also evolves a dishonest minority
    that takes advantage of the honest majority.  If individuals
    within a species have the ability to switch strategies, the
    dishonest minority will never be reduced to zero.  As a result,
    the species simultaneously evolves two things: 1) security systems
    to protect itself from this dishonest minority, and 2) deception
    systems to successfully be parasitic.
    Humans evolved along this path.  The basic mechanism can be
    modeled simply.  It is in our collective group interest for
    everyone to cooperate. It is in any given individual's short-term
    self-interest not to cooperate: to defect, in game theory terms.
    But if everyone defects, society falls apart.  To ensure
    widespread cooperation and minimal defection, we collectively
    implement a variety of societal security systems.
    Two of these systems evolved in prehistory: morals and reputation.
    Two others evolved as our social groups became larger and more
    formal: laws and technical security systems.  What these security
    systems do, effectively, is give individuals incentives to act in
    the group interest.  But none of these systems, with the possible
    exception of some fanciful science-fiction technologies, can ever
    bring that dishonest minority down to zero.
    In complex modern societies, many complications intrude on this
    simple model of societal security.	Decisions to cooperate or
    defect are often made by groups of people -- governments,
    corporations, and so on -- and there are important differences
    because of dynamics inside and outside the groups.	Much of our
    societal security is delegated -- to the police, for example --
    and becomes institutionalized; the dynamics of this are also
    important.
    Power struggles over who controls the mechanisms of societal
    security are inherent: "group interest" rapidly devolves to "the
    king's interest."  Societal security can become a tool for those
    in power to remain in power, with the definition of "honest
    majority" being simply the people who follow the rules.
    The term "dishonest minority" is not a moral judgment; it simply
    describes the minority who does not follow societal norm.  Since
    many societal norms are in fact immoral, sometimes the dishonest
    minority serves as a catalyst for social change.  Societies
    without a reservoir of people who don't follow the rules lack an
    important mechanism for societal evolution.  Vibrant societies
    need a dishonest minority; if society makes its dishonest minority
    too small, it stifles dissent as well as common crime.
At this point, I have most of a first draft: 75,000 words.  The tentative title is still "The Dishonest Minority: Security and its Role in Modern Society."  I have signed a contract with Wiley to deliver a final manuscript in November for February 2012 publication.  Writing a book is a process of exploration for me, and the final book will certainly be a little different -- and maybe even very different -- from what I wrote above.  But that's where I am today.
And it's why my other writings -- and the issues of Crypto-Gram --  continue to be sparse.
Lots of comments -- over 200 -- to the blog post.  Please comment there; I want the feedback.
** *** ***** ******* *********** *************
     RFID Tags Protecting Hotel Towels
The stealing of hotel towels isn't a big problem in the scheme of world  problems, but it can be expensive for hotels.  Sure, we have moral  prohibitions against stealing -- that'll prevent most people from stealing the towels.  Many hotels put their name or logo on the towels.  That works as a reputational societal security system; most people don't want their friends to see obviously stolen hotel towels in their bathrooms.  Sometimes, though, this has the opposite effect: making towels and other items into souvenirs of the hotel and thus more desirable to steal.  It's against the law to steal hotel towels, of course, but with the exception of large-scale thefts, the crime will never be prosecuted.  (This might be different in third world countries.  In 2010, someone was sentenced to three months in jail for stealing two towels from a Nigerian hotel.)  The result is that more towels are stolen than hotels want.  And for expensive resort hotels, those towels are expensive to replace.
The only thing left for hotels to do is take security into their own  hands.  One system that has become increasingly common is to set prices  for towels and other items -- this is particularly common with bathrobes
