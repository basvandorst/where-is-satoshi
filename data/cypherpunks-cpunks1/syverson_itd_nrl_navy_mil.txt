
@_date: 2000-10-13 15:27:46
@_author: Paul Syverson 
@_subject: Revised Deadline for FC'01 submission 
Many have been asking about extensions. Here is a CFP with the
extension explicitly included as a revised deadline. Note that there
will be no further extensions beyond what is listed below.
-Paul Syverson
                           Final Call for Papers
                         Financial Cryptography '01
            ************DEADLINE EXTENDED: SEE BELOW************
                             February 19-22, 2001
                      Grand Cayman Marriott Beach Resort
                             Cayman Islands, BWI
Original papers are solicited on all aspects of financial data security and
digital commerce in general for submission to the Fifth Annual Conference on
Financial Cryptography (FC01). FC01 aims to bring together persons involved
in the financial, legal and data security fields to foster cooperation and
exchange of ideas. Relevant topics include
Anonymity Protection                   Infrastructure Design
Auditability                           Legal/ Regulatory Issues
Authentication/Identification          Loyalty Mechanisms
Certification/Authorization            Payments/ Micropayments
Commercial Transactions                Privacy Issues
Copyright/ I.P. Management             Risk Management
Digital Cash/ Digital Receipts         Secure Banking Systems
Economic Implications                  Smart Cards
Electronic Purses                      Trust Management
Implementations                        WaterMarking
INSTRUCTIONS FOR AUTHORS: Electronic submission strongly encouraged.
Electronic submissions due 6AM, Washington DC time, October 23, 2000.
(Instructions available at Alternatively, send a cover letter and 15 copies of an extended
abstract to be received no later than October 23, 2000 to the Program
Chair. The extended abstract should start with the title, names of
authors, abstract, and keywords followed by a succinct statement
appropriate for a non-specialist reader specifying the subject
addressed, background, main achievements, and significance to
financial data security. Submissions are limited to 15 single-spaced
pages of 11pt type and should constitute substantially original
material. Panel proposals are due no later than November 27, 2000 (or
postmarked and airmailed by November 20).  Panel proposals should
include a brief description of the panel and a list of prospective
panelists.  Notification of acceptance or rejection of papers and
panel proposals will be sent to authors no later than December 8,
2000.  Authors of accepted papers must guarantee that their papers
will be presented at the conference and must be willing to sign an
acceptable copyright agreement with Springer-Verlag.  Use the above
address for electronic submissions or mail hardcopy to:
Paul Syverson, FC01 Program Chair
Center for High Assurance Computer Systems  (Code 5540)
Naval Research Laboratory
Washington DC 20375  USA
email: syverson at itd.nrl.navy.mil
Web: phone: +1 202 404-7931
PROCEEDINGS: Final proceedings will be published by Springer Verlag in
their Lecture Notes in Computer Science (LNCS) series.  Preproceedings
will be available at the conference, but final versions will not be
due until afterwards, giving authors the opportunity to revise their
papers based on presentations and discussions at the meeting.
Program Committee
Matt Blaze, AT&T Labs - Research
Yair Frankel, Ecash
Matt Franklin, UC Davis
David Kravitz, Wave Systems Corp.
Arjen Lenstra, Citicorp
Philip MacKenzie, Lucent Bell Labs
Avi Rubin, AT&T Labs - Research
Jacques Stern, Ecole Normale Sup�rieure
Kazue Sako, NEC
Stuart Stubblebine, CertCo
Paul Syverson (Chair), Naval Research Laboratory
Win Treese, Open Market, Inc.
Doug Tygar, UC Berkeley
Michael Waidner, IBM Zurich Research Lab
Moti Yung, CertCo
Important Dates
Extended Abstract Submissions Due: Oct. 23, 2000, 6AM, Washington DC time
                                   (or hardcopy received by Oct. 23, 2000)
Panel Proposal Submissions Due: November 27, 2000
Notification: Dec 8, 2000
Electronic submission information:
See General Chair
Stuart Haber, InterTrust STAR Lab
Electronic Submission chair
George Davida, UWM
Further Information about conference registration and on travel, hotels, and
Grand Cayman itself will follow in a separate general announcement. FC01 is
organized by the International Financial Cryptography Association.
Additional information will be found at --- end forwarded text

@_date: 2000-09-11 17:08:37
@_author: Paul Syverson 
@_subject: FC'01 Final Call for Papers 
Final Call for Papers
                         Financial Cryptography '01
                             February 19-22, 2000
                      Grand Cayman Marriott Beach Resort
                             Cayman Islands, BWI
Original papers are solicited on all aspects of financial data security and
digital commerce in general for submission to the Fifth Annual Conference on
Financial Cryptography (FC01). FC01 aims to bring together persons involved
in the financial, legal and data security fields to foster cooperation and
exchange of ideas. Relevant topics include
Anonymity Protection                   Infrastructure Design
Auditability                           Legal/ Regulatory Issues
Authentication/Identification          Loyalty Mechanisms
Certification/Authorization            Payments/ Micropayments
Commercial Transactions                Privacy Issues
Copyright/ I.P. Management             Risk Management
Digital Cash/ Digital Receipts         Secure Banking Systems
Economic Implications                  Smart Cards
Electronic Purses                      Trust Management
Implementations                        WaterMarking
INSTRUCTIONS FOR AUTHORS: Electronic submission strongly encouraged.
(Instructions available at   Alternatively,
send a cover letter and 15 copies of an extended abstract to be
received no later than October 13, 2000 (or postmarked by October 6,
2000 and sent via airmail) to the Program Chair. The extended abstract
should start with the title, names of authors, abstract, and keywords
followed by a succinct statement appropriate for a non-specialist
reader specifying the subject addressed, background, main
achievements, and significance to financial data security. Submissions
are limited to 15 single-spaced pages of 11pt type and should
constitute substantially original material. Panel proposals are due no
later than November 27, 2000 (or postmarked and airmailed by November
20).  Panel proposals should include a brief description of the panel
and a list of prospective panelists.  Notification of acceptance or
rejection of papers and panel proposals will be sent to authors no
later than December 8, 2000.  Authors of accepted papers must
guarantee that their papers will be presented at the conference and must
be willing to sign an acceptable copyright agreement with Springer-Verlag.
Use the above address for electronic submissions or send hardcopy to:
Paul Syverson, FC01 Program Chair
Center for High Assurance Computer Systems  (Code 5540)
Naval Research Laboratory
Washington DC 20375  USA
email: syverson at itd.nrl.navy.mil
Web: phone: +1 202 404-7931
PROCEEDINGS: Final proceedings will be published by Springer Verlag in
their Lecture Notes in Computer Science (LNCS) series.  Preproceedings
will be available at the conference, but final versions will not be
due until afterwards, giving authors the opportunity to revise their
papers based on presentations and discussions at the meeting.
Program Committee
Matt Blaze, AT&T Labs - Research
Yair Frankel, Ecash
Matt Franklin, UC Davis
David Kravitz, Wave Systems Corp.
Arjen Lenstra, Citicorp
Philip MacKenzie, Lucent Bell Labs
Avi Rubin, AT&T Labs - Research
Jacques Stern, Ecole Normale Sup�rieure
Kazue Sako, NEC
Stuart Stubblebine, CertCo
Paul Syverson (Chair), Naval Research Laboratory
Win Treese, Open Market, Inc.
Doug Tygar, UC Berkeley
Michael Waidner, IBM Zurich Research Lab
Moti Yung, CertCo
Important Dates
Extended Abstract Submissions Due: Oct. 13, 2000
Panel Proposal Submissions Due: November 27, 2000
Notification: Dec 8, 2000
Electronic submission information:
See General Chair
Stuart Haber, InterTrust STAR Lab
Electronic Submission chair
George Davida, UWM
Further Information about conference registration and on travel, hotels, and
Grand Cayman itself will follow in a separate general announcement. FC01 is
organized by the International Financial Cryptography Association.
Additional information will be found at --- end forwarded text

@_date: 2001-02-12 14:02:42
@_author: Paul Syverson 
@_subject: FC01 E-Voting Panel Description 
Panel: The Business of Electronic Voting
Place: Financial Cryptography 2001, Grand Cayman, Feb 21st, 2001 10:40 AM.
 Panel Chair:  Moti Yung, CertCo
 Panelists: Ed Gerck, safevote.com
            Andy Neff, VoteHere.net
            Ron Rivest, MIT
            Avi Rubin, AT&T research
This panel will concentrate on the emerging business of e-voting.
The problems associated with traditional voting machines in a national
election---their unreliability, inaccuracy and other potential
hazards---were placed in an international limelight by the last US
presidential election.  At the same time, but less conspicuously, an
industry centered around e-voting has started to emerge, offering
various solutions for national, boardroom, company-wide, and other
sorts of elections.
Indeed, the cryptographic research community has dealt with issues
related to security and robustness of e-voting as a fundamental
protocol problem.  In contrast, this panel will discuss issues
regarding the real-life aspects of actual implementations of voting
We will discuss basic requirements and problems associated with the
reality of election technology and the business built around it,
covering issues of reliability, fairness, and scalability, and asking
such questions as: Does one solution fit all situations?  How much
security is actually required?  Is e-voting for real?  How far are we
from ``real'' voting?  Is the Internet the right arena for voting?
What is the interaction between the technology and its quality and the
business?  Is it a business at all?  (Is there money to be made, and
how?  Alternatively: does e-voting really belong in ``financial
cryptography''?)  What are the social and legal implications of
We hope to learn about new angles to examine voting problems, to learn
about related burning issues of all kinds (social, business,
technology), and to learn about new questions for further basic,
systems, market, legal or social research.
--- end forwarded text

@_date: 2001-02-12 14:02:42
@_author: Paul Syverson 
@_subject: FC01 E-Voting Panel Description 
Panel: The Business of Electronic Voting
Place: Financial Cryptography 2001, Grand Cayman, Feb 21st, 2001 10:40 AM.
 Panel Chair:  Moti Yung, CertCo
 Panelists: Ed Gerck, safevote.com
            Andy Neff, VoteHere.net
            Ron Rivest, MIT
            Avi Rubin, AT&T research
This panel will concentrate on the emerging business of e-voting.
The problems associated with traditional voting machines in a national
election---their unreliability, inaccuracy and other potential
hazards---were placed in an international limelight by the last US
presidential election.  At the same time, but less conspicuously, an
industry centered around e-voting has started to emerge, offering
various solutions for national, boardroom, company-wide, and other
sorts of elections.
Indeed, the cryptographic research community has dealt with issues
related to security and robustness of e-voting as a fundamental
protocol problem.  In contrast, this panel will discuss issues
regarding the real-life aspects of actual implementations of voting
We will discuss basic requirements and problems associated with the
reality of election technology and the business built around it,
covering issues of reliability, fairness, and scalability, and asking
such questions as: Does one solution fit all situations?  How much
security is actually required?  Is e-voting for real?  How far are we
from ``real'' voting?  Is the Internet the right arena for voting?
What is the interaction between the technology and its quality and the
business?  Is it a business at all?  (Is there money to be made, and
how?  Alternatively: does e-voting really belong in ``financial
cryptography''?)  What are the social and legal implications of
We hope to learn about new angles to examine voting problems, to learn
about related burning issues of all kinds (social, business,
technology), and to learn about new questions for further basic,
systems, market, legal or social research.
--- end forwarded text

@_date: 2005-08-30 10:22:22
@_author: Paul Syverson 
@_subject: Tor on USB 
You might also see the following commercial distribution that
bundles Tor, a tiny linux, and related software on a USB stick
Looks cool and got favorable reviews, but I haven't used or examined
it first hand. This is a pointer, not an endorsement.
Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-09-29 09:56:19
@_author: Paul Syverson 
@_subject: Hello directly from Jimbo at Wikipedia 
Some points of clarification that I hope will help:
(1) On anonymity and authentication/pseudonymity/etc.
All versions of Onion Routing, including Tor,  were designed to separate
identification from routing.  The slogan way that I have put this for
the last five or six years is:
 Onion Routing is about anonymizing the communication pipe, not what
 goes through it.
The devil's always in the details, but as one-line summaries go, I
think that sums it up pretty well.{1}
(2) On various pseudonym authentication or anonym
authentication{2}, etc. approaches to solving the problem at hand.
Some of this is ultimately necessary for various applications,
especially once the Internet looks as Geoff described it. (In fact I
think it's one precondition to realizing anything like that vision.)
But I'm dubious about any of those proposed to date here providing
enough friction to identifier acquisition to deter abusers but not
honest users in this context. They may be worth trying.  Roger's
suggestion about the temporary IP blocks and Steven's about the
separate puzzle servers come to mind, probably some others I'm
forgetting just now.  But as Roger says, somebody's gotta code them
up---and probably much more work---deploy them, maintain them, and
evaluate their effectiveness, all on the Tor-Wikipedia frontier.  I
suspect that the abuser who goes postal as Jimmy described is willing
to waste lots of time acquiring IDs, but perhaps stereotypes about
attention span are close enough to true for some of the proposals to
be effective.  I had my own proposal that doesn't rely on any of this,
and that could be implemented and deployed in a few days (OK after
spending at least a few months or so thinking about the design, the
engineering, and the implications.) In the spirit of mutt: All these
ideas suck; I just think that one sucks a little less.
Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2007-03-07 17:16:43
@_author: Paul Syverson 
@_subject: Boulder Tech report on low-resource routing attacks on Tor 
The following are some comments on the Univ. Colorado at Boulder tech
report "Low-Resource Routing Attacks Against Anonymous Systems" that
has been getting lots of press and other web attention lately and been
somewhat discussed on this list.  It is only today that I have managed
to find time to sit down and read the paper.
The nutshell for people that don't want to read the details below:
A good paper. It does _not_ show Tor to be broken. (Nor did it ever
claim to. I only state that because of some of what has appeared in
the blagnpress, which to their credit, the authors tried to curtail.)
It is a nice contribution, especially in showing the limitations of
the current approach to entry guard selection. Overstates its novelty
over prior work, which is really unnecessary because it makes valuable
contributions of its own (and which is more or less my fault not theirs,
cf. below).
More details:
This is a nice piece of work. Its greatest contribution is in
directing attacks on entry guards. In the theory and simulation work
in which such ideas were introduced by Wright et al. they were
introduced (as "helper nodes") to reduce vulnerability.  As a recent
addition to Tor, the nature of defense they provide but also the
possible risks from how they are used in actual implementation and
deployment needed to be explored. It was understood from the start
that there is something of a tradeoff in introducing them. It was
realized that profiling without entry guards was in practice trivial
enough that the additional risk of adding entry guards and thus
simplifying and enhancing profiling for anyone who unfortunately had
an adversary guard node was clearly worth it. I don't think this paper
changes that. However, by attacking the guard selection process
itself, the research forces us to examine it more closely.
What they did was apply techniques that Lasse and I developed in
"Locating Hidden Services" to ordinary client circuits. Though we had
said this would be straightforward to do, we didn't actually do it.
Because we were focused on the deployed Tor network we could not
pursue this sort of attack there. We were also focused primarily on
what could be accomplished with a single hostile node. This limits to
cases of either a hostile website (as in Murdoch and Danezis and as
mentioned on p. 10 of this tech report) or a hostile client and a
hidden service, which is what we reported on.
Deploying a Tor network on PlanetLab and using synthetically generated
data removes some of the "in the wild" reality from the results.  But,
by accepting this limitation, it allowed them to obtain data at all
about Tor circuits for ordinary use (not hidden services). Much in the
practicality spirit of onion routing.
The experimental networks were more than an order of magnituded
smaller than the current deployed Tor network. One cannot be sure
something will scale until actually trying it, but in this case there
is no reason to doubt it does scale. Still if we take the 9 percent
figure given by the authors as an arbitrary line at which attacks
become significant, that is still almost a hundred nodes in the
current network. At about twice the entire size of the experimental
networks that were set up this starts to be a bit more than
low-resource.  Still one could do quite a bit with less than 9
percent. Also, as a counter to my own point, see "On the
countermeasures" below.
On prior work:
Before I start noting all the things that the authors didn't properly
cite, I should observer that they first contacted me about their work
way back in last October and have cc'd me on correspondence with
Roger, who did read their work and respond to them. If these are
indeed omissions and oversights in their paper, then it is my fault
because they gave me plenty of opportunity to comment before it hit
the interblag.  I just didn't squeeze in the time before now.
I already mentioned that the basic techniques are similar to what was
in my paper with Lasse Xverlier, "Locating Hidden Servers".  The
authors say that they think theirs is "the first approach capable of
[compromising anonymity] before the client starts to transmit any
payload data". I believe that the code we ran would be entirely able
to do this. We mentioned it only briefly in passing in our paper, and
we didn't actually do it. The authors of this report did.
They also say they have introduced the idea of nodes falsely reporting
bandwidth and uptime. As they note this is central to the way their
basic attack works. As they do not note, this was explicitly used by
Lasse and me in our attacks. I quote from our paper, "Alice's server
node will report a false higher uptime and the maximum network
bandwidth to the directory server in order for other nodes to trust it
for their circuits. This is still possible as there is (yet) no method
for keeping reliable track of uptime at the different servers."
They also introduce the idea of selective path disruption to speed up
attacks by dropping circuits, because that will cause more circuits to
be built. This was also part of our attacks albeit since we controlled
the client connecting to the hidden service, it could be done there.
The statement that the algorithm for choosing entry guards was
"implemented to protect the first hop of a circuit by using what are
believed to be more reliable and trustworthy nodes" is false.  Using
more reliable nodes was seen as sensible because it should minimize
the number of entry guards a client uses, which is the whole
point. Nobody thought that this made them more trustworthy. In fact
the general threat of adversaries running reliable nodes (in both
onion routing and mixnets) to attract more traffic is well recognized.
On the threat model. While the design document does use the c^2/n^2
basic result from "Towards an Analysis of Onion Routing Security"
as a starting point. This was not thought to be accurate once we
had substantial deployment and was acknowledged as such.
Cf., "Challenges in deploying low-latency anonymity"
On the countermeasures:
Rather than discuss all of them in detail in an already overly long
post, I'll just note that I think they may help against an adversary
that has only several hundred to a few thousand dollars in resources.
The authors note that they are considering just that case, and it is
certainly worthwhile to see what it takes to mount an attack and what
works against a low resource adversary. That was also part of our
motivation to show what could be done with a single bad node.  But, an
adversary that has a few tens of thousands of dollars can simply run
many reliable high bandwidth nodes and thus mount the attack
invulnerable to any countermeasure against lying. Michael Gersten
noted the threat of this attack in a separate context in a post to
this list yesterday (March 6). And it has long been recognized as a
potential threat to Tor in general. I have begun to look at
countermeasures that should work unless the adversary owns major hunks
of the network, e.g., social network based, but will not get into that
further here.
More than you wanted to read. Hope it was useful anyway.
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2009-11-23 10:05:49
@_author: Paul Syverson 
@_subject: The Case for Banning Reduced Hop Count Implementations 
Thank you Lucky. I had been meaning to write something like your
Even if Lucky's basic points are eventually born out, you are right
that more analysis of latency and incentives would be valuable.  To
get an idea about incentive issues in anonymous communication in
general and Tor in particular you might want to look at "On the
Economics of Anonymity". Also "Anonymity Loves Company: Usability and
the Network Effect" both available from the Freehaven anonbib. Also,
"Deploying Low-Latency Anonymity: Design Challenges and Social
Factors" which is available from the onion-router.net publications
This has nothing to do with how long the connections are. Onion
routing going back even before Tor acknowledges that if the entry and
exit nodes are controlled/observed then an adversary will quickly and
trivially link them. The nutshell way we have said this is that onion
routing [including Tor] guards against traffic analysis not traffic
confirmation. This was acknowledged in the original Tor design paper
and was later born out by analysis ("Passive Attack Analysis for
Connection-Based Anonymity Systems") experiments on the live Tor
network ("Locating Hidden Services") and in simulation on PlanetLab
("Low-Resource Routing Attacks Against Tor"). These confirmed that
correlation was fast and easy. "Sampled Traffic Analysis by
Internet-Exchange-Level Adversaries" showed that it could also be done
sampling a tiny fraction of the traffic passing through an IX.  This
is also why onion routing's security is said to be roughly c^2/n^2,
where c is the number of compromised nodes in the network and n is the
total number of nodes. (Yes that is a little too quick, and you can
raise questions. See "Towards an Analysis of Onion Routing Security",
"A Model of Onion Routing with Provable Anonymity", and "Probabilistic
Analysis of Onion Routing in a Black-box Model" for details.)
The "Low-Resource" paper is especially telling wrt your point: the
attacks were done during connection setup _before a single data cell
was transmitted_ (and with vanishingly few false-positives). You just
don't need to have a long-lived connection to fall victim to this.  So
why bother with multiple hops? One part of the answer is already given
above: it reduces the threat quadratically. But why three hops instead
of just two? This comes back to Lucky's other point that you skipped
over. And this one is not subtle at all. Three hops is the minimum to guarantee that all an exit node knows is
that a circuit came from someone using Tor. The exit cannot say even where
in the Tor network the circuit started. Similarly, all an entry node
knows is that the circuit is headed somewhere. (Yes, this too is
actually more subtle; cf. "How Much Anonymity does Network Latency
Leak?" But, a priori, given ordinary log information, it is correct.
(Of course honest Tor nodes do not do any such logging.))
So, reducing the number of hops means that exit nodes have
significantly more information about connection origins. Reducing hops
to one means that they know everything about the origin of a
connection (up to the IP address from which the connection entered the
Tor network, which is all that Tor is designed to hide.)  That makes
their deniability of what they know about traffic exiting through them
no longer plausible (because, well now it will be false). That any of
the connections going through the network are single hop thus
increases incentives to attack any exit node, also any entrance
node---which basically means all the public nodes. Details would
depend on likelyhood that a given circuit is one hop and on the
incentives, legal considerations, resources, etc. of the
adversary. But absent such details, it would be unwise to allow such a
fundamental threat to the infrastructure itself.
As Lucky observed, this is a threat to the public Tor network itself
and should be treated as such. There are other drawbacks that could be
noted, but that is the central one.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-09-23 10:59:03
@_author: Paul Syverson 
@_subject: Random chaff [was: more work for Grobbages] 
Yes. But packet counting can also play a role. Cf, "Passive Attack Analysis for Connection-Based Anonymity Systems"
at It's not. Cf. my "Locating Hidden Servers"
wherein we had zero false positives on any timing attacks conducted
in finding hidden services, which generally was very quick.
(That such attacks existed were known for years. That they were not
just possible but so fast and effective using merely a single
node in the network was the reason that guard nodes were introduced
into the Tor network.)
And building on that see, "Low-Resource Routing Attacks Against Tor"
where timing attacks with epsilon false positives
were based simply on circuit setup and were shown on general
Tor circuits, not just for hidden services.
There's been a lot of research on this. I think Nick pointed at
some. Cf. the anonbib.
Research against timing attacks continues. (I'm doing some myself.)
But so far, any "chaff" strategy in the literature is both too
expensive and not at all effective against active attacks on
general low-latency systems for wide use, such as Tor.

@_date: 2009-09-24 12:56:35
@_author: Paul Syverson 
@_subject: Random chaff [was: more work for Grobbages] 
I did, but I don't get the sigh.
I was trying to succinctly say that this is a component of a different
system architecture with different assumptions. In the second
generation onion routing system we developed, i.e., the one before
Tor, we actually included mixing for experimental purposes.  The
lessons so far has been that it isn't worth it and we did not bother
to put that in Tor. That could change, but so far there are no
positive indications from the research.
Yes of course. You say that like it's trivial (to design, implement, etc.)
rather than huge.
Plus, keeping the existing network nodes synched even just to the point that
things don't actually break has not been 100 percent successful, and
this would imply much tighter synchronization not just across the
nodes but across all the clients as well. And the synchronization is
not just to keep things running but now becomes security-critical.  Wah!
More importantly, it is trivial to beat this with an active attack.
Just delay circuit setup packets slightly and watch for the pattern
at the other end. Or if the circuit is established, stomp some bits
at one end and see if the other end has junk come out shortly thereafter.
I'm not saying it's forever hopeless. The things I've mentioned and
more have been considered and people have design and evaluated
countermeasures to them and continue to do so. As Nick said, the
problem isn't that padding doesn't work. It's that it doesn't work
nearly well enough (at least so far).
Ermm. The stuff that Lasse and I did _was_ on the deployed Tor
network. Now that is not today's network. The network then was
much smaller, it didn't have guard nodes, etc. Testing in the wild in general is very tricky because Tor _is_ an
operational network, and you don't want to do anything that would
inadvertently create problems. This is also an ongoing research
challenge. We would like to understand and improve performance by
gathering data but without doing anything to increase risk to users or
operators. Karsten and others have been working on that.
Yes. There might be. But you would first have to justify the overhead
cost to the network by giving at least some reasonable argument that
it might work reasonably well, at least better than anything that's
been considered to date.  "Hey we don't know this won't work unless we
try," is not an adequate justification. Vetting ideas through the
research community seems like a reasonable first step. You would also
have to adequately analyze the impact on client and relay performance
and security before deploying. Again, nobody's discouraging research
into these questions. They just want answers before deploying. So far
none of the research has been giving encouraging answers.
Sorry. I thought that was standard. It means 'Cf.' means _confer_, i.e.,
see here.  Woops, "i.e." stands for  'id est' which means _that is_.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2012-04-05 08:41:23
@_author: Paul Syverson 
@_subject: [tor-talk] access sites 
We created Tor to protect military communications. Much like other
things invented at NRL (e.g., the joystick controller for remote
control---patented in 1923! or GPS) it also has widespread civilian
use. For most of those, the civilian, business, or other government
use is icing on the cake of the purpose that prompted research on
them.  For some, e.g., IFF (identification friend-or-foe) their
development into civilian use (ATCRBS, the air traffic control radar
beacon system) importantly facilitates military use of the shared
space. For onion routing, we argued publicly right from the start that
the diversity of users was an essential element in effective use of
the technology---even way back when we were just calling our systems
onion routing, rather than _the_ onion routing (Tor) to distinguish
from instances of onion routing developed elsewhere.
As you can see, I'm not averse to touting these creations. But I am a
researcher who does publicly published research in this area and whose
work largely benefits from visibility. As Roger and others have
pointed out earlier in this thread, people who rely on Tor to protect
sensitive communications are rarely going to be happy to have anything
revealed about their usage. You are simply not going to hear from
(most of) those people, and you are definitely not going to get a
representative sample of such use. At best you are going to be lucky
to have anecdotal examples or even just anecdotal claims of usage
from which to extrapolate.
You seem to be asking for a statistically accurate demographic study
of all users. But I am sure there are whole classes of users who don't
want even their class of activity on Tor, much less their specific
activity, known. I have no idea what classes, but that just makes
sense. And on a more individual level, for every stalking victim who
both managed to connect to Andrew and decided to trust him to help her
protect herself online there are ???  others who did not have that
opportunity or were unsure enough about trust to not reveal. (Of
course ideally anyone regardless of technical background should know
about the benefits of Tor and how to use it for their needs without
having to talk to Andrew or someone. Let's not get into any of those
A well-designed user study will tell us something interesting about
Tor users. What it will definitely not do is give us a representative
distribution of Tor users by purpose (and as already touched on usage
demographics, e.g., by country can be significantly
dynamic). And inferring user distributions from traffic distributions in
studies that are methodologically controversial is not helpful.  If
that's all we've got for now, then that's all we've got. But we should
be very careful what we infer from it.
tor-talk mailing list
tor-talk at lists.torproject.org

@_date: 2012-02-29 09:48:15
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
Maybe. The important thing is to understand what security is provided
and what is not. Then you can make an informed decision about whether
or not it's bad news.
The short but incomplete answer is yes. Generally, what you are
describing we experimentally verified on the live Tor network back in
2005.  See "Locating Hidden Servers" by Lasse Overlier and myself.
Available at  These sorts of attacks are what motivated us to introduce guard nodes,
also described in that paper.  We all knew, and had seen analyzed in
earlier work by Wright et al., that onion routing circuits were
subject to predecessor attacks, and that what Wright et al. had called
helper nodes would, well help.  What Lasse and I showed was that the
public Tor network as of 2005 was subject to such attacks working very
quickly (minutes) using very limited resources. You could generally
find a hidden server within minutes using just a single hostile Tor
relay (no cooperation from evil web server required). We wanted to
show what you could do with a single relay, which limited us to hidden
server circuits. If you owned at least two relays, you could attack
arbitrary Tor circuits. This was confirmed in simulation on PlanetLab
shortly after by Bauer et al.  ("Low-Resource Routing Attacks Against
Tor", also available at anonbib).
Entry guards don't change the asymptotic threat of such attacks, they
just move it around. Since you could be screwed so quickly and easily
by building random circuits, Tor was changed so that you pick just a
few relays as your entry guards. If one of them is evil, it will see
the entry side of (a large fraction of) your circuits and will be able
to associate you with your destination whenever you go to a hostile
destination or use a hostile exit. What Lasse and I showed was that
you weren't really much worse off from this than when choosing
circuits with random entry relays. And if none of your guards is evil,
an adversary can never de-anonymize you in this way. (Never say
"never". ;>) Cf. the experiments and discussion of layered guards in
"Locating Hidden Servers", and our subsequent research on building
trust into path selection.)
tor-talk mailing list
tor-talk at lists.torproject.org

@_date: 2012-01-26 10:18:00
@_author: Paul Syverson 
@_subject: [tor-talk] Fwd: ANONdroid 
JonDoNym and Tor have some significant aspects in common, but they are
fundamentally different in the basis for their security. JonDoNym is a
mix cascade design (or `MIX' as some would write it). This means that
all the messages (packets/cells) that enter the network together
(enter a node at roughly the same time) proceed through the network
together in a batch and leave the network together (exit at a
predictable but distinct node all at the same time). In principle,
this means that an adversary will have a difficult time separating
messages in a batch from one another.  (In practice, research
indicates that the situation for low-latency traffic is, let's just be
generous and say, more complicated.) This is the basis of the security
they provide.  Onion routing designs like Tor derive their security
primarily from the unpredictability of routes: just seeing a message
from a client enter the network does not tell you where it is going to
come out.
The pros and cons of these are a matter of long debate. (I miss you,
Andreas.) I come down strongly on the side of onion routing as an
approach. Many detailed research papers address lots of the issues,
but much of my position is captured in my "Why I'm not an Entropist."
(Let me know if you want to see it and can't find it.)  The basic
difference is roughly that mix cascades attempt to hide you well in a
predictable 'anonymity set'. Onion routing networks attempt to make it
hard to know where to look/attack if you want to de-anonymize, e.g.,
what website someone is browsing. And the Tor network is intended to
be big enough and in diverse enough jurisdictions that it is hard for
any one adversary to look everywhere (thousands of nodes
worldwide). That's the main idea, skipping a lot of aspects, such as
the possible benefits (or not) of combining the two approachers.
Many security researchers, including myself, are quite resistant to
the idea of revocable anonymity. Designing, building, and deploying
a system to be secure is incredibly hard even without the added burden
of building in a systematic means to selectively remove its security
properties. See, for example, Matt Blaze's work on key escrow. See
 for his recent retrospective.
But leaving that entire issue aside, from my above comment, I hope it
is apparent that there is another important difference from mix
cascades. For Tor such a surveillance order for purposes of tracking
particular communicants would be pointless, and thus presumably
unjustified. Unlike cascades, a message entering the network in one
location might come out at any exit node, anywhere in the world. And
if, e.g., one observes communication exiting a particular network node
and arriving at a destination of interest, there is no reason to think
that future communication, even from the same originating client is
likely to emerge from that same node. In fact just the opposite.
tor-talk mailing list
tor-talk at lists.torproject.org

@_date: 2012-03-06 23:14:39
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
I'll try to get to it soon.
Actually there are many papers over the last several years (e.g., at
ACM CCS and Info Hiding) showing that one can place undetectable
timing channels on flows (for some schemes provably undetectable for
others practically undetectable).  But passive correlation is adequate
anyway, even at very low sampling rates (cf. Murdoch and Zielinski,
PETS 2007). This is long known and well understood. It's why we have
always said that onion routing resists traffic analysis not traffic
tor-talk mailing list
tor-talk at lists.torproject.org

@_date: 2012-03-09 10:51:39
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
And there was a star in the east...
I actually updated my webpage, not as fully as I would like, but
I updated all the dead links and added about a dozen or so papers.
In particular there is now a link to the "Why I'm not an Entropist"
paper, the "Practical Vulnerabilities of the Tor Anonymity Network"
paper that Andrew asked me to post over a year ago, my recent
historical review "A Peel of Onion", etc. HTH.
tor-talk mailing list
tor-talk at lists.torproject.org
