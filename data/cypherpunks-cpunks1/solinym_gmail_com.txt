
@_date: 2005-12-01 02:08:00
@_author: Travis H. 
@_subject: security modifications to current PCs 
I've been reading through the TCPA documents and thinking a bit about
changes that might give higher assurance to an ordinary PC, or at
least a PC with only minor changes.
Specifically, one of the things I've always been mulling over is a
secure boot sequence.  Basically, like the TCPA, I want a sequence
where each stage decrypts and validates the next one so that a user
doesn't have to worry about modifications to the bootup state. Basically, I've been thinking about rewriting the BIOS (perhaps with
large portions in FORTH a la openfirmware*) such that instead of
prompting the user for a password which is compared to a stored copy
(that can be erased by removing the battery), it instead prompts the
user for a passphrase that is used to decrypt and authenticate the MBR
(boot block) and possibly the first-stage boot loader.  The boot
loader in turn decrypts and authenticates the kernel and any
associated crud it needs (perhaps supporting the multiboot spec), and
the kernel and crud are smart enough to decrypt and authenticate the
root partition, and away we go.
[*] Similarly, I wouldn't mind seeing a PCI card or something that is
designed for securely storing crypto keys (from DMA among other
things) and performing crypto operations.  These parts of the TCPA are
okay.  I don't see the need to curtain memory, as I'm comfortable with
the "ring 0 can do anything" property.
Additionally, it would be nice to have a "trusted path" to the OS,
whereby a certain key sequence triggers a direct input path to a
program, or the user is assured of what program he/she is talking to.
Is it possible to implement most block ciphers in FPGAs?  It'd be nice
to have a bus-mastering crypto co-processor device to do, say, disk
encryption without requiring CPU help, but I want to be able to update
it to new algorithms as new attacks against the cipher appear.  I use
some disk encryption stuff on a dual processor machine and it's still
slow.  The load climbs to 10 or 12 all too easily, then stuff becomes
unresponsive (perhaps because swap is one of the things I'm
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-12-11 02:05:53
@_author: Travis H. 
@_subject: [Clips] Engineer Outwits Fingerprint Recognition Devices with 
A recent magazine article suggested a spoofing technique involving
wrapping one's finger with a few layers of cellophane; the latent
print on the reader apparently is visible enough to be reused in this
manner, at least with some currently-available scanners.
  -><- Knight of the Lambda Calculus
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-04 19:09:07
@_author: Travis H. 
@_subject: On Digital Cash-like Payment Systems 
By my calculations, it looks like you could take a keypair n,e,d and
some integer x and let e'=e^x and d'=d^x, and RSA would still work,
albeit slowly.  Reminds me of blinding, to some extent, except we're
working with key material and not plaintext/ciphertext.
Since I'm on the topic, does doing exponentiation in a finite field
make taking discrete logarithms more difficult (I suspect so), and if
so, by how much?
Is there any similar property that could be used on e' and d' to make
computing e and d more difficult?  Of course whatever algorithm is
used, one would need to feed e' and d' to it en toto, but a really
clever attacker might be able to take the xth root prior to
exfiltrating them.
Also, application of a random pad using something like XOR would be
useful; could be done as a postprocessing stage independently of the
main algorithm used to encrypt the data, or done as a preprocessing
stage to the plaintext.  I prefer the latter as it makes breaking the
superencryption much more difficult, and fixed headers in the
ciphertext could give away some OTP material.  However, the
preliminary encryption in something like gpg would suffer, so it would
have the effect of making the ciphertext bigger.  Perhaps this is an
advantage in your world.
An alternate technique relies in specifying, say, 256 bits of key,
then using a cryptographically strong PRNG to expand it to an
arbitrary length, and storing that for use.  Pilfering it then takes
more bandwidth, but it could be reconstructed based on the 256-bit
seed alone, if one knew the details of the PRNG.  So the key could be
"compressed" for transfer, if you know the secret seed.  Search for
the seed would still be expensive, even if PRNG details are known. Alternately, in a message encrypted with gpg-like hybrid ciphering,
one could apply a secret, implicit PRNG to the message key seed before
using it as a symmetric key.  For example, you could take a 256-bit
message key, run it through the PRNG, create 3x256 bits, then use
triple-AES to encrypt the message.  In this case, the PRNG buys
forgery resistance without the use of PK techniques.  The PRNG
expander could not be attacked without breaking the PK encryption
(which supports arbitrarily large keys) of the seed or the triple-AES
symmetric encryption of the message.
You know, they specify maximum bandwidth of covert channels in bits
per second, I wonder if you could use techniques like this to prove
some interesting property vis-a-vis covert channel leakage.  It's
remarkably difficult to get rid of covert channels, but if you inflate
whatever you're trying to protect, and monitor flows over a certain
size, then perhaps you can claim some kind of resilience against them.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-04 19:23:59
@_author: Travis H. 
@_subject: gonzo cryptography; how would you improve existing 
Hi folks,
If one had the ability to create standards over, with reckless
disregard for performance, how would you improve their security?
Feel free to pick a protocol or system (e.g. gpg or isakmp) and let me
know how it is done, and how it should have been done.
For example, pgp doesn't hide the key IDs of the addressees.  Many
systems use hashes that are too small.  DSA keys are too small
compared to large ElG keys.  How would you make a signature with a
larger keyspace?  Does the protocol wrap encryption in authentication
instead of vice-versa?  Does ISAKMP do encryption where the input is
meant to be secret, instead of the key?  Does it use a rinky-dink
algorithm, now that much better ones are available?
I've got a hankering to re-write something, and I want to know what
can be improved the most.
PS:  There's a paper on cryptanalyzing CFS on my homepage below.  I
got to successfully use classical cryptanalysis on a relatively modern
system!  That is a rare joy.  CFS really needs a re-write, there's no
real good alternatives for cross-platform filesystem encryption to my
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-04 20:25:46
@_author: Travis H. 
@_subject: gonzo cryptography; how would you improve existing 
I meant MAC, not encryption, sorry.
Of course encryption inputs are secret.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-07 01:09:02
@_author: Travis H. 
@_subject: On the orthogonality of anonymity to current market demand 
I'd recommend DRM (I think what you really mean is Palladium, err,
excuse me, the Trusted Computing Platform Alliance, see the web site
and Ross Anderson's take on it) to my grandmother, because I don't
trust her to understand the implications of clicking on something in
an email (thank you active content!).  Many OSes don't allow ordinary
users the privileges of compromising their security so easily as
Microsoft.  I suppose we can rely on vendor-written code to do
approximately what it claims to do, most of the time, but have you
actually read the claims in EULAs and Privacy Policies lately?
It seems like you'd be trading one set of problems for another. Personally, I'm less suprised by my own software (and, presumably,
key-handling) than vendor software, most of the time.  I think TCPA is
about control, and call me paranoid, but ultimate control isn't
something I'm willing to concede to any vendor, or for that matter any
other person.  I like knowing what my computer is doing, to the bit
and byte level, or at least being able to find out.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-08 05:58:04
@_author: Travis H. 
@_subject: gonzo cryptography; how would you improve existing 
The only thing close that I've seen is Bestcrypt, which is commercial
and has a Linux and Windows port.  I don't recall if the Linux port
came with source or not.  I had problems with the init script hanging
the boot process, or at least delaying it significantly, so I
uninstalled it until I could devote the time to analyze what was going
on.  Right after installation I tried using it to read a container
copied from a corrupted Windows machine, but was not successful.  It
is unclear to me if this was due to the corruption which occured, or
some kind of incompatibility between the Windows and Linux ports.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-14 06:16:55
@_author: Travis H. 
@_subject: On Digital Cash-like Payment Systems 
Looks like it (common modulus attack involves same n, different (e,d) pairs).
However, you're likely to be picking a random symmetric key as the
"message", and Schneier even suggests picking a random r in Z_n and
encrypting hash(r) as the symmetric key.
More generally, I wonder about salting all operations to prevent using
the same value more than once.  It seems like it's generally a bad
idea to reuse values, as a heuristic, and applying some kind of
uniquification operation to everything, just as it's a good idea to
pad/frame values in such a way that the output of one stage cannot be
used in another stage of the same protocol.
What I really meant was, if it wasn't computed in a finite field, how
difficult would it be to compute the logarithm?  I'm just curious
about how much work factor is involved in reducing modulo n.
I also wonder about some of the implications of choosing a message or
exponent such that not enough reductions take place during
Well, it depends on how you define the attack, which wasn't defined. If the attack is to smuggle out a key using a covert channel, it may
apply.  If the attack is to download the key on a conventional
network, it wouldn't make much difference.
Unless, of course, you're auditing network flows over a certain size
or lasting a certain amount of time.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-30 05:35:12
@_author: Travis H. 
@_subject: test disk from cgsecurity versus data security 
The documentation doesn't say it can do that.  It just says it can
find where they start (and/or end) when that information (which is
usually in the MBR) has been lost.  This is easy as most filesystems
have signatures ("magic numbers") which can be identified, and/or they
tend to be at certain places on the disk (for example, cylinder
boundaries).  Overwriting an entry in your partition table is much
different than overwriting the partition itself.
So far, as Simson Garfinkel has pointed out, nobody has shown any
evidence that you can recover data after just one overwrite with
zeroes.  Then again absence of evidence is not evidence of absence. Lacking any evidence one way or the other, I assume it is possible in
my risk analyses, since that way I don't get any nasty surprises.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-10-23 22:52:30
@_author: Travis H. 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
That's a fairly interesting review, and Skype should be commended for
hiring someone to do it.  I hope to see more evaluations from vendors
in the future.
However, I have a couple of suggestions.
My understanding of the peer-to-peer key agreement protocol (hereafter
p2pka) is based on section 3.3 and 3.4.2 and is something like this:
A -> B: N_ab
B -> A: N_ba
B -> A: Sign{f(N_ab)}_a
A -> B: Sign{f(N_ba)}_b
A -> B: Sign{A, K_a}_SKYPE
B -> A: Sign{B, K_b}_SKYPE
A -> B: Sign{R_a}_a
B -> A: Sign{R_b}_b
Session key SK_AB = g(R_a, R_b)
0) The p2pka allows us to use a peer as a signing oracle for nonces by
performing steps 1 through 4.  Only the one-wayness of f (specified
only as "modified in a standard way") stands in the way of arbitrary
forgery, which would allow us to bypass the security on steps 3, 4, 7,
and 8.  It would not stop us from knowing the session key, since there
is no restriction on the form of R_a or R_b.
1) It's not clear that the identity certificates are bound to a
[externally visible] network [source] address at registration time. IMHO, this would be a good idea.
2) He implicitly ignores the fact that the skype key is a trusted CA,
so skype can impersonate anyone (or delegate that impersonation by
signing a bogus ID).  This is obvious to a cryptographer but should be
mentioned for the layperson.  An evaluation should explicitly specify
who must be trusted by whom, and everyone must trust the Skype
3) It looks like the peer-to-peer communication involves the same key,
SK_AB, in both directions, opening the door for keystream re-use, but
there's 64 bits of presumably random salt so it shouldn't be very
1) They use an unencrypted 2-byte CRC on each packet between peers. Undetected modification to a packet is possible, since the CRC is
computed over the encrypted data and stored en clair.  In this case,
arbitrary bits can be flipped, the CRC recomputed, and no future
packets depend on the current packet, so there's no tell-tale garbling
afterwards like there is in most other block modes.  He alludes to
this in section 3.4.4 but doesn't really specify the impact, merely
compares it to WEP.
2) The session established with the Skype server during registration
is protected with a 256-bit key, which is random, but he doesn't say
how the client and Skype agree on it.
3) It's not clear why they used rc4 instead of ICM to generate key
material, but at least it's not being used for confidentiality.
4) The details of the random number generation are vague ("makes a
number of win32 calls").
5) The details of the SK_AB key composition are vague ("combined in a
cryptographically-sound way"), shown by g in the p2pka above.
6) It doesn't say who sends the nonces first --- is it the recipient
of the connection, or the initiator?  Can we DoS people by repeated
connections triggering digital signatures?
7) It doesn't say whether it's a TCP or UDP protocol, what ports it
uses, etc.  I'm curious if it will work through NAT at both ends.
8) The skype server's timeout on login passwords can be used for a
denial-of-service against the registration protocol and doesn't affect
username guessing (fixed password variable username, a/k/a "reverse
9) It doesn't specify how the salts used in ICM mode are communicated.
10) It doesn't specify how streams are created and numbered.
It'd be nice to see the protocol clearly specified and analyzed via
automated means (finite state analysis via murphy, etc.).
Obsession with performance:
He makes no fewer than six comments about performance (of the AES
code, of the modular exponentiation, of the primality testing, of
modular inversion, of multi-precision arithmetic libraries, and SHA-1
implementation), which should normally be the least of anyone's
worries, especially cryptographers.  Is this is a security evaluation,
or a performance test?
However, since we're talking about real-time audio streams, perhaps
some discussion of the bandwidth and especially latency of the p2p
protocol would be in order.  Unfortunately, there's no quantification
("... performs favorably in terms of clock cycle per encryption").
Trust us:
Finally, the whole thing is closed source, so none of it is easily
verifiable.  We just have to take his word on it, and often he just
offers opinions (see the complaints of vagueness above).
All that having been said, I still have more confidence in Skype than
I did before reading the paper.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-10-25 23:40:24
@_author: Travis H. 
@_subject: [PracticalSecurity] Anonymity - great technology but hardly 
Part of the problem is using a packet-switched network; if we had
circuit-based, then thwarting traffic analysis is easy; you just fill
the link with random garbage when not transmitting packets.  I
considered doing this with SLIP back before broadband (back when my
friend was my ISP).  There are two problems with this; one, getting
enough random data, and two, distinguishing the padding from the real
data in a computationally efficient manner on the remote side without
giving away anything to someone analyzing your traffic.  I guess both
problems could be solved
by using synchronized PRNGs on both ends to generate the chaff.  The
two sides getting desynchronzied would be problematic.  Please CC me
with any ideas you might have on doing something like this, perhaps it
will become useful again one day.
On packet-switched networks, running full speed all the time is not
very efficient nor is it very friendly to your neighbors.  Again, if
you have any ideas on how to deal with this, email me.
Many of the anonymity protocols require multiple participants, and
thus are subject to what economists call "network externalities".  The
best example I can think of is Microsoft Office file formats.  I don't
buy MS Office because it's the best software at creating documents,
but I have to buy it because the person in HR insists on making our
timecards in Excel format.  In this case, the fact that the HR person
(a third party to the transaction) is using it forces me to buy it
from Microsoft.  Similarly, the more people use digital cash, the more
likely I am to decide to use it.  The more Tor nodes we have, the more
high speed and close nodes there will be, and the more enjoyable the
experience will be (assuming Tor is smart enough to use the close,
fast nodes).  For more information on network externalities, see the
book "Information Rules", available from Amazon for just over $4. Everyone working in IT or interested in computers should read that
Another issue involves the ease of use when switching between a
[slower] anonymous service and a fast non-anonymous service.  I have a
tool called metaprox on my website (see URL in sig) that allows you to
choose what proxies you use on a domain-by-domain basis.  Something
like this is essential if you want to be consistent about accessing
certain sites only through an anonymous proxy.  Short of that, perhaps
a Firefox plug-in that allows you to select proxies with a single
click would be useful.
It would be nice if the protocols allowed you to specify a chain of
proxies, but unfortunately HTTP only allows you to specify the next
hop, not a chain of hops. Perhaps someone could come up with an
encapsulation method and cooperative proxy server that is more like
the old cpunk remailers, using nested encrypted "envelopes" in the
body of the request.  Perhaps crowds or Tor already does this, I don't
Where anonymizing facilities fail are fairly obvious to anyone who has
used them, listed in descending order of importance:
ease of configuration (initial setup cost)
ease of use
locator services for peers or servers
network effects (not enough people using it)
efficient use of resources (see quote in sig about why this is the
least important)
There are some technical concerns limiting their security:
resistance to traffic analysis or trojaned software
ad-hoc systems for crypto key updates or revocation
I think one way to encourage adoption is to amortize the cost of setup
over a group of people.  For example, everyone who reads this could
set up a hardened co-loc box and install all the relevant software,
then charge their friends a small fee to use it.  An ISP could make
these services available to their customers.  An ASP could make them
available to customers over the web.  People could start creating
open-source Live! CD distributions* with all the software clients
installed and preconfigured (or configured easily through a
wizard-like set of menus invoked automatically at bootup).  With Live!
CDs in particular, you'd have a bit of a problem with generating
crypto keys since the RNG fires up in the same state for everyone, but
perhaps you could seed it by hashing the contents of a disk drive, or
the contents of memory-mapped hardware ROMs (e.g. ethernet MAC
address), network traffic, and/or with seed state persisted on a
removable USB drive.
[*] See I don't see a distro specifically for anonymity; if you have friends
who want to create Yet Another Linux Distro, perhaps they could fill
this niche.  Two alternatives suggest themselves; a client distro for
end-users and a server distro for people with a machine that's not
doing anything.  You'd just pop in the CD and it announces its
availability to various locator services to act as a Tor, mixmaster,
or whatever node.  Again, keep me informed if anyone starts work on
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-10-26 00:24:07
@_author: Travis H. 
@_subject: [fc-discuss] Financial Cryptography Update: On Digital 
Maybe the trusted computing platform (palladium) may have something to
offer after all, namely enabling naive users to use services that
require confidence in their own security.  One could argue it's like
going to a Vegas casino; software vendors (MS *cough* MS) probably
won't cheat you in such a system because they don't have to; the odds
are in their favor already.  The whole system is designed to assure
they get paid, and they have a lot to lose (confidence in the
platform) by cheating you (at least in ways that can be detected). And since you won't be able to do anything to compromise the security,
you can't screw it up.
While I wouldn't see an advantage in that, I might recommend it for my
More on topic, I recently heard about a scam involving differential
reversibility between two remote payment systems.  The fraudster sends
you an email asking you to make a Western Union payment to a third
party, and deposits the requested amount plus a bonus for you using
paypal.  The victim makes the irreversible payment using Western
Union, and later finds out the credit card used to make the paypal
payment was stolen when paypal reverses the transaction, leaving the
victim short.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-10-28 19:47:35
@_author: Travis H. 
@_subject: packet traffic analysis 
Good catch on the encryption.  I feel silly for not thinking of it.
I'm not so sure.  If we're talking about thwarting traffic on the link
level (real circuit) or on the virtual-circuit level, then you're
adding, on average, a half-packet latency whenever you want to send a
real packet.  And then there's the bandwidth tradeoff you mention,
which is probably of a larger concern (although bandwidth will
increase over time, whereas the speed of light will not).
I don't see any reason why it's necessary to pay these costs if you
abandon the idea of generating only equal-length packets and creating
all your chaff as packets.  Let's assume the link is encrypted as
before.  Then you merely introduce your legitimate packets with a
certain escape sequence, and pad between these packets with either
zeroes, or if you're more paranoid, some kind of PRNG.  In this way,
if the link is idle, you can stop generating chaff and start
generating packets at any time.  I assume that the length is
explicitly encoded in the legitimate packet.  Then the peer for the
link ignores everything until the next "escape sequence" introducing a
legitimate packet.
This is not a tiny hack, but avoids much of the overhead in your
technique.  It could easily be applied to something like openvpn,
which can operate over a TCP virtual circuit, or ppp.  It'd be a nice
optimization if you could avoid retransmits of segments that contained
only chaff, but that may or may not be possible to do without giving
up some TA resistance (esp. in the presence of an attacker who may
prevent transmission of segments).
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-10-28 20:05:09
@_author: Travis H. 
@_subject: packet traffic analysis 
I should point out that encrypting PRNG output may be pointless, and
perhaps one optimization is to stop encrypting when switching on the
chaff.  The peer can then encrypt the escape sequence as it would
appear in the encrypted stream, and do a simple string match on that. In this manner the peer does not have to do any decryption until the
[encrypted] escape sequence re-appears.  Another benefit of this is to
limit the amount of material encrypted under the key to legitimate
traffic and the escape sequences prefixing them.  Some minor details
involving resynchronizing when the PRNG happens to produce the same
output as the expected encrypted escape sequence is left as an
exercise for the reader.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-01 01:01:23
@_author: Travis H. 
@_subject: packet traffic analysis 
During lulls, you are constantly sending chaff packets.  On average,
you're halfway through transmitting a chaff packet when you want to
send a real one.  The system has to wait for it to finish before
sending another.  QED.
I'm talking about a stream, with packets embedded in it.  For
circuit-switched circuits, this is no problem.  For a packet-switched
network, you must packetize the stream, which is unrelated to the
packets embedded in the stream.
This is somewhat inefficent, which is why I suggested that it is more
applicable ot something like PPP, SSH, or OpenVPN links, which are
already virtual circuits.  This is a fair criticism, but just think of
the number of such circuit/packet conversions when someone uses a TCP
virtual circuit over packet-based IP over an analog POTS link, which
is itself a virtual circuit that is packetized and sent over a circuit
(long-haul wirepair or fiber) in the telco network.
If you explain to me how an eavesdropper can tell where plaintext
packet begins or ends, then I'll agree with you that it is indeed
vulnerable to length analysis.
That might or might not be a problem.  With ECB, it's vulnerable to
analysis (chaff is constant, so encryption of it is constant).  With
some modes, the amount you can transmit is limited (e.g. CTR mode). Modes that are based on a small window of previous plaintext, such as
OFB, would be vulnerable too.  It could very well be that it's a bad
idea to send a lot of constant plaintext under other modes, as well. For example, if most of the data is constant, then you have a close
approximation of known-plaintext.
It's not necessary to run a PRNG on the receiver.  You just have to be
able to tell when you're looking at random data, or an encrypted
version of an escape sequence and a valid packet, which can be
recognized, as per your point 4a.  If you find that it's not a
legitimate packet, you treat it as PRNG data, and start looking for
the encrypted escape sequence.  However, with a 32-bit escape
sequence, the chances of getting such a false positive are low.
I personally think sending encrypted versions of constant data under
the same key you use for real data is not crazy, but somewhat
imprudent.  Do you know what the unicity distance is?  Have you read
of attacks that require a large amount of ciphertext encrypted under
the same key?
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2005-11-01 01:15:02
@_author: Travis H. 
@_subject: packet traffic analysis 
============================== START ==============================
My mistake, OFB does not have this property.  I thought there was a
common mode with this property, but it appears that I am mistaken.
If it makes you feel any better, you can consider the PRNG the
encryption of constant text, perhaps using the real datastream as some
kind of IV.  The content of the chaff is not relevant; ideally you
would use a high-bandwidth HWRNG such as Quantis.
  -><-
"We already have enough fast, insecure systems." -- Schneier & Ferguson
GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B

@_date: 2006-01-18 00:28:08
@_author: Travis H. 
@_subject: Echelon papers leaked 
Two chapters are online here:
"If I could remember the names of these particles, I would have been a
  -- Enrico Fermi -><- GPG fingerprint: 50A1 15C5 A9DE 23B9 ED98 C93E 38E9 204A 94C2 641B
