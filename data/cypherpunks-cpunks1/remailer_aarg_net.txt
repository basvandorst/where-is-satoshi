
@_date: 2001-12-13 05:50:09
@_author: AARG! Anonymous 
@_subject: [linux-elitists] Phil Zimmermann on key exchange 
This has been repeated ad nauseam, but obviously not frequently enough.
No one has been using CAs for anything serious and no one ever will.
Outside of circles of fashionable crypto, commercial scams like verisign and greedy non-profits that want to help freedom fighters/armed thugs (definition changes with proximity), no one whose life and well-being depends on it has ever used CA.
The simple fact is that it is impossible to have shared secrets of utmost importance with someone that you do not have a secure physical channel with (which automagically obsoletes CA). If your life depends on it you will not risk it by sending such information to a person you have no means of directly authenticating. Strangers do not have secrets, by definition.
Why is this so hard to understand ?
The beauty of public key schemes created many seemingly plausible PHantasies pursued by quite a few technically savvy folks. But that does not change the basic problem.
USG operatives, including Osama bin Laden, do not use public computer-based web of trust to authenticate. These guys KNOW each other. Even in non-government business environments, PGP keys between People Who Matter are exchanged manually. MIS department goons never get to see those.
Automated CAs are fine for ad-hoc crypto that prevents casual data harvesting*. But lying to the public about limits of CA schemes will not do crypto any good.
* provided, of course, that one does not use popular OS, all of which will soon make all storage available to casual harvesting. Keep those DOS boxes around.

@_date: 2001-12-23 15:25:28
@_author: AARG! Anonymous 
@_subject: A poll for remailer operators 
This is simple.
There is a HUGE unknown about the actual number of remailer users. Many otherwise intelligent discussions hinge on this number being small or big.
Unreliable as it will be, could you please provide an estimate of monthly number of human-generated messages that enter your system (excluding those from other remailers.)

@_date: 2001-12-24 19:40:06
@_author: AARG! Anonymous 
@_subject: Illusional delusions 
The solution for money laundering is to remove the "money", as defined
by the state, from the equation.
Crypto removes the content from everyones's eyes except the two parties
that communicate. That is what crypto can do. The moment one wants to
convert some bits to state-money she is doomed. If you want to
rely/convert at any point to the state-money you will become a money
launderer/terrorist/whatever. There is simply no way around this. Paper
cash is not a solution, since it is observable. Anything observable is
not a solution.
This is a fundamental problem - not unlike futile attempts to stop
copying the content one can hear or see, or to invent foolproof
Instead, we need a new principle that will not include state-money in
any form or shape.
Thinking aloud ... this may be silly:
Let's start from something that works - secret key message exchange,
maybe enhanced with PK key exchange for the carefree. A person, by
defintion, trusts itself, so currency known only to the two parties
should be reasonably safe. Every pair of traders have their own
currency. A disbalance (A owes B but C owes A) is resolved by creating
new B-C currency. There is no anonymity, but the network is hardly
connected and therefore reasonably safe. The system is hardly new but it
was never done in software AFAIK. You never do business with someone
whose reputation you can't instrument. But someone can start a business
for reputation building. Again, hardly new.
This doesn't enable global trade, but frankly I don't care. I don't gain
much from it anyway. Going through a series of intermediaries to buy
some fine Afghan pot is a small price to pay for getting out of the
state's sight.
In other words, with small transistors and digsigs there is no need for
centralised money at all. No more mainframes, thank you. Big business
will suffer, but somehow I don't feel sad.
So, tell me, is this silly ?

@_date: 2001-12-27 08:45:54
@_author: AARG! Anonymous 
@_subject: P2P Stego Treasure Hunt 
We've put into Morpheus a song, which has a stego'd message in it.
The tool is mp3stego v 1.1.15 (source available; see  ) and the (3DES) passphrase is "writecode"
Another file "DrDidg_RaveOn.mp3" has
another message under the same passphrase.
We are curious how readily the Morpheus search
engine can be used for transport purposes.  In
this instance we give unique names to files not
otherwise found in the system.  Another experiment
in P2P percolation would be to add similar 'watermarks' (microdots) to files which are abundantly replicated.
(Stego Reminder/Caveat: use *original* content for true stealth
else presence of watermark is detected with diff on clean rip)

@_date: 2001-11-10 12:00:09
@_author: AARG! Anonymous 
@_subject: "Anti-Terrorist" Exception to Atty-Client Privilege? 
U.S. Defends Monitoring of Lawyer-Suspect Communication By James Vicini Reuters WASHINGTON (Nov. 9) - The U.S. Justice Department defended Friday its
rule to listen in on conversations between some inmates and their
lawyers to prevent violent and terrorist acts, but a civil liberties
group denounced the new policy as a ''police state'' tactic. The rule, signed by Attorney General John Ashcroft and published on Oct.
31, provided for the monitoring of mail and communications with
attorneys for as long as a year, as opposed to 120 days previously, and
took effect immediately. The eavesdropping would not be surreptitious. The defendant and the
attorney must get notice of the government's listening, Ashcroft said.
The rule expanded an existing regulation that allows for monitoring of
attorney-client communications. Rachel King of the American Civil Liberties Union said the rule set a
''terrifying precedent'' and was ''very scary.''  ''It's nothing short
of a police state,'' she added. King, the legislative counsel in the Washington office, said the group
would file comments opposing the rule. The National Association of Criminal Defense Lawyers said it will
challenge the new regulations through all available legal means. ''Rules and codes of professional responsibility are very clear: an
attorney cannot communicate with a client when confidentiality is not
assured,'' Irwin Schwartz, the group's president, said in a statement. NO EAVESDROPPING WITHOUT COURT ORDER ''The federal government has no business eavesdropping on these
conversations, absent a court order,'' he said. In the rule, Ashcroft said, ''Recent terrorist activities perpetrated on
U.S. soil demonstrate the need for continuing vigilance in addressing
the terrorism and security-related concerns identified by the law
enforcement and intelligence communities.'' The rule represented one in a series of steps by Ashcroft in what he has
described as an aggressive campaign to prevent future terrorist attacks
after the Sept. 11 hijackings of planes that hit the World Trade Center
and the Pentagon. Ashcroft has vowed to carry out tough new anti-terrorism legislation
that Congress approved giving law enforcement officers expanded powers
to wiretap phones, monitor Internet traffic and arrest suspects. On Thursday, he announced a ''wartime reorganization'' of the Justice
Department to carry out in its ''first and overriding priority'' of
defending against terrorist attacks. Justice Department spokeswoman Mindy Tucker defended the rule, saying
''important safeguards'' were in place. She said the team monitoring the communications will have no connection
to any ongoing prosecution that involves the inmate. Currently, less than 1/10 of one percent of the federal inmate
population is subject to the special administrative measure taken by the
U.S. Bureau of Prisons, Tucker said. Ashcroft said the rule would affect only a small portion of the federal
inmate population. He said it would affect those inmates for whom the attorney general or
the head of a federal law enforcement of intelligence agency has
determined there was a substantial risk the communications with others
could lead to violence or terrorism. Ashcroft maintained the constitutional rights of the inmates to a lawyer
would be protected. ''The attorney-client privilege protects confidential communications
regarding legal matters, but the law is clear that there is no
protection for communications that are in furtherance of the client's
ongoing or contemplated illegal acts,'' he said. REUTERS  Reut13:39 11-09-01

@_date: 2001-11-28 14:15:15
@_author: AARG! Anonymous 
@_subject: Leahy is now an Enemy of the State 
"The chairman of the Senate Judiciary Committee Wednesday sharply criticized the Bush administration for a series of practices it has adopted in the wake of the September 11 terrorist attacks, calling them a "marked departure" from long-held jurisprudence customs."

@_date: 2001-10-08 22:45:04
@_author: AARG! Anonymous 
@_subject: Tony Blair's body language 
Are his lips moving?
Please name one truthful politician.

@_date: 2001-10-13 16:40:02
@_author: AARG! Anonymous 
@_subject: F.B.I. Did Not Test Letter to NBC or Immediately Notify City 
F.B.I. Did Not Test Letter to NBC or Immediately Notify City HallBy JENNIFER
STEINHAUER and JIM DWYERAlthough the F.B.I. was notified on Sept. 25 about
suspicious letters sent to NBC, neither the letters, nor the powder residue
inside them were tested until nearly two weeks later, and then only because
a private doctor notified city public health officials about a troubling skin
condition in the news assistant who had handled the mail, officials acknowledged
yesterday.In fact, the F.B.I. laboratory neither performed nor sought any tests
on the powder or the skin samples taken from the employee, identified as Erin
M. O'Connor, a 38-year-old assistant to Tom Brokaw."That, unfortunately, did
not take place," said Barry W. Mawn, assistant director in charge of the F.B.I.'s
New York office.He also said the agents had intended to interview Ms. O'Connor
soon after they learned of the case, but did not, for reasons he did not explain.F.B.I.
officials said investigators picked up the envelopes on Sept. 26, the day after
NBC security officials called the agency. New York City officials were not
informed of the preliminary inquiry until days later.Indeed if the Health Department
had not been alerted to the case by a private doctor, it might well have hibernated
in the F.B.I. files.Mr. Mawn said yesterday that the agency had to investigate
dozens of threats, scares and false alarms, and that is how this case was initially
treated. Since Sept. 11, when the two planes slammed into the World Trade Center,
city and federal law enforcement officials have received hundreds of reports
of menace and foul play ranging from bomb threats to chemical attack scares.The
discovery of a case of apparently deliberate anthrax poisoning in the heart
of New York City is just the latest in a string of terrifying events that have
challenged the law enforcement and health care infrastructure in the last month.
It pushed health care officials to nail down a pathogen that most of them had
no experience with.And it once again tested the fragile relationship that has
always existed between the New York City Police Department and the F.B.I.,
agencies that are forced to work in tandem on unprecedented and constantly
evolving crimes."Information sharing between the F.B.I. and the N.Y.P.D. has
always been poor," said one person who has worked closely with both agencies.
"There is often a lack of willingness on the F.B.I.'s part to share information,
although it is getting better. As they move forward, clearly the F.B.I. is
going to have to be more forthcoming."In this case, Mr. Mawn said, the gravity
of the situation was not fully appreciated by the federal authorities until
recently.On Sept. 25., Ms. O'Connor handled a letter postmarked from St. Petersburg,
Fla., filled with white powder, according to law enforcement officials. She
also handled a second letter containing a sandy substance. Network officials,
immediately suspicious, called the F.B.I., which picked up the letters the
next day.Then, the agency began to prepare a cover letter for its own laboratory
indicating that the substances needed to be tested, but the letter was never
completed and the evidence was never sent from the F.B.I.'s office in New York
to its laboratory, said Joseph Valiquette, an F.B.I. spokesman.He added that
some delays happened because investigators were unable to interview Ms. O'Connor
to supplement the cover letter. "We wanted to send a complete package to the
laboratory," said Mr. Valiquette. So none of it was sent. Mr. Valiquette said
he did not know why the F.B.I. could not speak with Ms. O'Connor, who works
in Rockefeller Plaza and lives in the metropolitan area.On Sept. 28, Ms. O'Connor
developed a strange sore on her chest. Nervous, she went to see Dr. Richard
Fried, a Manhattan infectious disease specialist, said Dr. Annetta Kimball,
the doctor covering for Dr. Fried, who could not be reached last night.Armed
with the description of the rash  which she described as central scarring
surrounded by a lot of swelling  the doctor likely consulted his textbooks
to nail down what was going on. Dr. Kimball said that Dr. Fried suspected that
his patient had been exposed to anthrax, and immediately prescribed Cipro."There
is a good chance he had never seen anthrax before," said Dr. Kimball. "This
is New York City, not an agricultural area." He also took cultures from a wound
Ms. O'Connor developed, but those swabs were negative for anthrax, she said.At
some point, officials and Dr. Kimball said, the patient visited a dermatologist.One
of those doctors notified the city's Health Department on Oct. 6 of a possible
case of anthrax. The city has among the most sophisticated epidemiologists
and public health labs in the country.Mr. Valiquette said that the F.B.I. learned
from the city's Health Department that "this was an issue." The substance eventually
made its way to the Centers for Disease Control and Prevention in Atlanta,
which went to unusual lengths to identify it. There was little powder to work
with, and a power failure in the lab halted the work for nearly a day, said
Dr. James M. Hughes of the C.D.C."None of this ever did go to the F.B.I. lab,"
he said.One of the Ms. O'Connor's doctors ordered a skin biopsy, which was
sent to the C.D.C. But by the time the tissue was taken, the patient had begun
taking an antibiotic to counter possible anthrax. As intended, that drug degraded
the cellular structure of the bacteria. On Wednesday, the city was informed
of the case under investigation. The C.D.C. was able to identify the spores
of anthrax, and officials were informed of the results early yesterday morning.Mayor
Rudolph W. Giuliani announced the first confirmed case of anthrax yesterday
morning at NBC.He was described by a person who was with him early yesterday
as being "three feet off the ground" when he learned that the F.B.I. had not
brought word of the developments to city officials earlier.Mr. Mawn acknowledged
that investigators revisited the case after learning of the concerns of the
health officials. "A second notification came through to the Department of
Health, at which time the evidence response people and the F.B.I. also became
involved with it. It was initially assigned to two agents that just covered
the lead. And upon that, it was immediately submitted for tests. As you know
and as the mayor has talked about, those tests were initially negative."Yesterday,
The New York Times received a letter filled with white powder that was addressed
to a reporter, Judith Miller. The Times notified the mayor's office, and city
and F.B.I. officials responded immediately. Tests for radioactive and chemical
substances were negative, and results from a more definitive test for anthrax
DNA by state and federal labs were expected over the weekend.Coordinating the
efforts of the various law enforcement and public health officials is proving
tricky  there have been dozens of bomb and other threats around the city since
Sept. 11, and the city, which investigates each case, cannot inform the public
or other agencies about each one. Just yesterday, the city heard of about a
half dozen cases of suspicious powder or envelopes that it is investigating,
the mayor said."If there was a problem, it was in the way they first investigated
it," one Police Department official said of the F.B.I.'s performance.But there
was some concern last night among health care experts about the delay in the
testing. The inability of the agency to identify the substance was "not very
comforting," said one C.D.C. official, who spoke on the condition of anonymity.
"That is a little disappointing, to say the least."Anthrax spores, if kept
under appropriate conditions  sealed and unexposed to lots of light  can
be preserved for years, said Jerome M. Hauer, the former director of the Office
of Emergency Management and currently managing director of Kroll Inc., an investigation
firm. He added: "You don't want them sitting around. In this environment, you
hope there is good information sharing, especially when it involves biological

@_date: 2001-10-22 18:40:01
@_author: AARG! Anonymous 
@_subject: FBI considers torture as suspects stay silent 
FBI considers torture as suspects stay silent FROM DAMIAN WHITWORTH IN WASHINGTON AMERICAN investigators are considering resorting to harsher
interrogation techniques, including torture, after facing a wall of
silence from jailed suspected members of Osama bin Ladens al-Qaeda
network, according to a report yesterday.
More than 150 people who were picked up after September 11 remain in
custody, with four men the focus of particularly intense scrutiny. But
investigators have found the usual methods have failed to persuade any
of them to talk.
Options being weighed include truth drugs, pressure tactics and
extraditing the suspects to countries whose security services are more
used to employing a heavy-handed approach during interrogations.
Were into this thing for 35 days and nobody is talking. Frustration
has begun to appear, a senior FBI official told The Washington Post.
Under US law, evidence !
extracted using physical pressure or torture is
inadmissible in court and interrogators could also face criminal
charges for employing such methods. However, investigators suggested
that the time might soon come when a truth serum, such as sodium
pentothal, would be deemed an acceptable tool for interrogators.
The public pressure for results in the war on terrorism might also
persuade the FBI to encourage the countries of suspects to seek their
extradition, in the knowledge that they could be given a much rougher
reception in jails back home.
One of the four key suspects is Zacarias Moussaoui, a French Moroccan,
suspected of being a twentieth hijacker who failed to make it on board
the plane that crashed in Pennsylvania. Moussaoui was detained after
he acted suspiciously at a Minnesota flying school, requesting lessons
in how to steer a plane but not how to take off or land. Both Morocco
and France are regarded as having harsher interrogation methods than
the United States!
The investigators have been disappointed that the usual incentives to
break suspects, such as promises of shorter sentences, money, jobs and
new lives in the witness protection programme, have failed to break
the silence.
We are known for humanitarian treatment, so basically we are stuck.
Usually there is some incentive, some angle to play, what you can do
for them. But it could get to that spot where we could go to pressure
. . . where we dont have a choice, and we are probably getting
there, an FBI agent involved in the investigation told the paper.
The other key suspects being held in New York are Mohammed Jaweed
Azmath and Ayub Ali Khan, Indians who were caught the day after the
attacks travelling with false passports, craft knives such as those
used in the hijackings and hair dye. Nabil Almarabh, a Boston taxi
driver alleged to have links to al-Qaeda, is also being held. Some
legal experts believe that the US Supreme Court, which has a
conservative tilt, might !
be prepared to support curtailing the civil
liberties of prisoners in terrorism cases.
However, a warning that torture should be avoided came from Robert
Blitzer, a former head of the FBIs counter-terrorism section. He said
that the practice goes against every grain in my body. Chances are
you are going to get the wrong person and risk damage or killing
In all, about 800 people have been rounded up since the attacks, most
of whom are expected to be found to be innocent. Investigators believe
there could be hundreds of people linked to al-Qaeda living in the US,
and the Bush Administration has issued a warning that more attacks are
probably being planned.
Newsweek magazine reports today that Mohammed Atta, the suspected
ringleader who died in the first plane to hit the World Trade Centre,
had been looking into hitting an aircraft carrier. Investigators
retracing his movements found that he visited the huge US Navy base at
Norfolk, Virginia, in February and April t!
his year.

@_date: 2001-10-22 19:10:22
@_author: AARG! Anonymous 
@_subject: FBI considers torture as suspects stay silent 
[A whipping-boys-for-legible-content repost.]
    MONDAY OCTOBER 22 2001
    FBI considers torture as suspects stay silent
    FROM DAMIAN WHITWORTH IN WASHINGTON
    AMERICAN investigators are considering resorting to harsher
    interrogation techniques, including torture, after facing a wall of
    silence from jailed suspected members of Osama bin Ladens al-Qaeda
    network, according to a report yesterday.
    More than 150 people who were picked up after September 11 remain in
    custody, with four men the focus of particularly intense scrutiny.
    But investigators have found the usual methods have failed to
    persuade any of them to talk.
    Options being weighed include truth drugs, pressure tactics and
    extraditing the suspects to countries whose security services are
    more used to employing a heavy-handed approach during
    interrogations.
    Were into this thing for 35 days and nobody is talking. Frustration
    has begun to appear, a senior FBI official told The Washington Post.
    Under US law, evidence extracted using physical pressure or torture
    is inadmissible in court and interrogators could also face criminal
    charges for employing such methods. However, investigators suggested
    that the time might soon come when a truth serum, such as sodium
    pentothal, would be deemed an acceptable tool for interrogators.
    The public pressure for results in the war on terrorism might also
    persuade the FBI to encourage the countries of suspects to seek
    their extradition, in the knowledge that they could be given a much
    rougher reception in jails back home.
    One of the four key suspects is Zacarias Moussaoui, a French
    Moroccan, suspected of being a twentieth hijacker who failed to make
    it on board the plane that crashed in Pennsylvania. Moussaoui was
    detained after he acted suspiciously at a Minnesota flying school,
    requesting lessons in how to steer a plane but not how to take off
    or land. Both Morocco and France are regarded as having harsher
    interrogation methods than the United States.
    The investigators have been disappointed that the usual incentives
    to break suspects, such as promises of shorter sentences, money,
    jobs and new lives in the witness protection programme, have failed
    to break the silence.
    We are known for humanitarian treatment, so basically we are stuck.
    Usually there is some incentive, some angle to play, what you can do
    for them. But it could get to that spot where we could go to
    pressure . . . where we dont have a choice, and we are probably
    getting there, an FBI agent involved in the investigation told the
    paper.
    The other key suspects being held in New York are Mohammed Jaweed
    Azmath and Ayub Ali Khan, Indians who were caught the day after the
    attacks travelling with false passports, craft knives such as those
    used in the hijackings and hair dye. Nabil Almarabh, a Boston taxi
    driver alleged to have links to al-Qaeda, is also being held. Some
    legal experts believe that the US Supreme Court, which has a
    conservative tilt, might be prepared to support curtailing the civil
    liberties of prisoners in terrorism cases.
    However, a warning that torture should be avoided came from Robert
    Blitzer, a former head of the FBIs counter-terrorism section. He
    said that the practice goes against every grain in my body. Chances
    are you are going to get the wrong person and risk damage or killing
    them.
    In all, about 800 people have been rounded up since the attacks,
    most of whom are expected to be found to be innocent. Investigators
    believe there could be hundreds of people linked to al-Qaeda living
    in the US, and the Bush Administration has issued a warning that
    more attacks are probably being planned.
    Newsweek magazine reports today that Mohammed Atta, the suspected
    ringleader who died in the first plane to hit the World Trade
    Centre, had been looking into hitting an aircraft carrier.
    Investigators retracing his movements found that he visited the huge
    US Navy base at Norfolk, Virginia, in February and April this year.
    Copyright 2001 Times Newspapers Ltd.

@_date: 2001-10-23 21:45:08
@_author: AARG! Anonymous 
@_subject: FBI considers torture as suspects stay silent 
When the B-52's are performing even the Taliban develop rhythm.

@_date: 2001-10-27 00:36:45
@_author: AARG! Anonymous 
@_subject: FBI wants to have "Internet Off-switch" 
Per the following article:  it appears as if the now wants to route ALL Internet traffic through it's central servers!!!!
What gall!! What nerve!!!!
Now, for all of you who said, "Hey, I'm not doing anything wrong, let the FBI monitor what it wants to." can go shove hot spikes up your nose.
I don't think the FBI really wants to control the Internet, they want to destabilize it.  As tyranny approaches the only thing more dangerous than
an armed populace is an informed one.  If they can monitor all the
traffic, they can certainly control it.
The ISP's (whatever those are) need to collectively tell the FBI to go jump off a bridge.  Information campaigns need to be sent to the to alert them of the potential loss of civil liberties.
I'm gonna stop before I say something that will get me arrested.
Larry Diffey

@_date: 2001-10-27 14:50:01
@_author: AARG! Anonymous 
@_subject: FBI wants to have "Internet Off-switch" 
They'll probably lean on the big boys, the backbone providers
like MCI, Sprint, Cable & Wireless, etc. CALEA put taps in those
providers, so it's just a matter of expanding the data streams
they're "allowed" to scan.
Anyone know of a tunneling package that'll handle an OC3?...
Cheers -

@_date: 2002-08-01 16:15:18
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
You might be surprised to learn that under the TCPA, it is not necessary
for the TPM (the so-called "Fritz" chip) to trust *any* signing keys!
The TCPA basically provides two kinds of functionality: first, it can
attest to the software which was booted and loaded.  It does this by
taking hashes of the software before transferring control to it, and
storing those hashes in its internal secure registers.  At a later
time it can output those hashes, signed by its internal signature
key (generated on-chip, with the private key never leaving the chip).
The system also holds a cert issued on this internal key (which is called
the Endorsement key), and this cert is issued by the TPM manufacturer
(also called the TPME).  But this functionality does not require storing
the TPME key, just the cert it issued.
Second, the TCPA provides for secure storage via a "sealing" function.
The way this works, a key is generated and used to encrypt a data blob.
Buried in the blob can be a hash of the software which was running
at the time of the encryption (the same data which can be reported
via the attestation function).  Then, when the data is decrypted and
"unsealed", the hash is compared to that which is in the TPM registers
now.  This can make it so that data which is encrypted when software
system X boots can only be decrypted when that same software boots.
Again, this functionality does not require trusting anyone's keys.
Now, there is an optional function which does use the manufacturer's key,
but it is intended only to be used rarely.  That is for when you need to
transfer your sealed data from one machine to another (either because you
have bought a new machine, or because your old one crashed).  In this
case you go through a complicated procedure that includes encrypting
some data to the TPME key (the TPM manufacturer's key) and sending it
to the manufacturer, who massages the data such that it can be loaded
into the new machine's TPM chip.
So this function does require pre-loading a manufacturer key into the
TPM, but first, it is optional, and second, it frankly appears to be so
cumbersome that it is questionable whether manufacturers will want to
get involved with it.  OTOH it is apparently the only way to recover
if your system crashes.  This may indicate that TCPA is not feasible,
because there is too much risk of losing locked data on a machine crash,
and the recovery procedure is too cumbersome.  That would be a valid
basis on which to criticize TCPA, but it doesn't change the fact that
many of the other claims which have been made about it are not correct.
In answer to your question, then, for most purposes, there is no signing
key that your TPM chip trusts, so the issue is moot.  I suggest that you
go ask the people who misled you about TCPA what their ulterior motives
were, since you seem predisposed to ask such questions.
The point of being anonymous is that there is no persistent identity to
attribute motives to!  Of course I have departed somewhat from this rule
in the recent discussion, using a single exit remailer and maintaining
continuity of persona over a series of messages.  But feel free to make
whatever assumptions you like about my motives.  All I ask is that you
respond to my facts.
Of course, speculation is entirely appropriate - when labeled as such!
But David Wagner gave the impression that he was talking about facts
when he said,
   "The world is moving toward closed digital rights management systems
   where you may need approval to run programs," says David Wagner,
   an assistant professor of computer science at the University of
   California at Berkeley.  "Both Palladium and TCPA incorporate features
   that would restrict what applications you could run."
Do you think he was speculating?  Or do you agree that if he makes
such statements, he should base them on fact?  TCPA appears to have
no mechanism for the user to need approval in order to run programs.
That is how the facts look to me, and if anyone can find out otherwise,
I would appreciate knowing.  Maybe someone could ask David Wagner what
he based the above claim on?

@_date: 2002-08-01 16:45:15
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
We need to look at the text of this in more detail.  This is from
version 1.1b of the spec:
: This section introduces the architectural aspects of a Trusted Platform
: that enable the collection and reporting of integrity metrics.
: Among other things, a Trusted Platform enables an entity to determine
: the state of the software environment in that platform and to SEAL data
: to a particular software environment in that platform.
: The entity deduces whether the state of the computing environment in
: that platform is acceptable and performs some transaction with that
: platform. If that transaction involves sensitive data that must be
: stored on the platform, the entity can ensure that that data is held in
: a confidential format unless the state of the computing environment in
: that platform is acceptable to the entity.
: To enable this, a Trusted Platform provides information to enable the
: entity to deduce the software environment in a Trusted Platform. That
: information is reliably measured and reported to the entity. At the same
: time, a Trusted Platform provides a means to encrypt cryptographic keys
: and to state the software environment that must be in place before the
: keys can be decrypted.
What this means is that a remote system can query the local TPM and
find out what software has been loaded, in order to decide whether to
send it some data.  It's not that unapproved software "won't work",
it's that the remote guy can decide whether to trust it.
Also, as stated earlier, data can be sealed such that it can only be
unsealed when the same environment is booted.  This is the part above
about encrypting cryptographic keys and making sure the right software
environment is in place when they are decrypted.
But no, the TCPA does allow all software to run.  Just because a remote
system can decide whether to send it some data doesn't mean that software
can't run.  And just because some data may be inaccessible because it
was sealed when another OS was booted, also doesnt mean that software
can't run.
I think we agree on the facts, here.  All software can run, but the TCPA
allows software to prove its hash to remote parties, and to encrypt data
such that it can't be decrypted by other software.  Would you agree that
this is an accurate summary of the functionality, and not misleading?
If so, I don't see how you can get from this to saying that some software
won't run.  You might as well say that encryption means that software
can't run, because if I encrypt my files then some other programs may
not be able to read them.
Most people, as you may have seen, interpret this part about "software
can't run" much more literally.  They think it means that software needs
a signature in order to be loaded and run.  I have been going over and
over this on sci.crypt.  IMO the facts as stated two paragraphs up are
completely different from such a model.
That's true; in fact if you ran it earlier under TCPA and sealed some
data, you will have to run under TCPA to unseal it later.  The question
is whether the advantages of running under TCPA (potentially greater
security) outweigh the disadvantages (greater potential for loss of
data, less flexibility, etc.).
Right, the strongest case will probably be for DRM.  You might be able
to download all kinds of content if you are running an OS and application
that the server (content provider) trusts.  People will have a choice of
using TCPA and getting this data legally, or avoiding TCPA and trying to
find pirated copies as they do today.
I am inclined to agree; in fact I have made many postings (anonymously
of course) in recent weeks arguing that these systems will be entirely
voluntary.  If the functionality is useful, people will use it.
Software vendors who use TCPA will compete with those who don't.
The market will decide.  I am not as certain as you that TCPA will win,
but if it does, it will mean that TCPA is a good technology that solves
real problems for people.
The points I made earlier were that TCPA is unlikely to be mandated,
because it doesn't need to be; that TCPA should compete in the free
market with other solutions; and that this approach actually expands the
set of choices available to the participants.  More choice is always good.
Well, you would use TCPA.  But you have to look at how you got into that
situation.  The way it would have happened was by people voluntarily
adopting TCPA before it became a de facto standard, simply because it
was useful.
I have no credentials in this area other than a general knowledge of
crypto; I am just someone who was willing to devote some hours of his
free time to educating himself on this technology.  I agree that the
spec is written very poorly.
But let me put you on the spot: as someone who has a good
understanding of TCPA, what do you think of Ross Anderson's TCPA FAQ
at   For example,
how about his claim in answer 2, "Pirate software can be detected and
deleted remotely."  I must have missed that page of the TCPA spec.
And then he builds on this a couple of paragraphs later: "There will
be remote censorship: the mechanisms designed to delete pirated music
under remote control may be used to delete documents that a court (or
a software company) has decided are offensive...."  He further builds
on this later to claim (answer 11) that with TCPA, programs can be made
to ignore documents created by pirated versions of Word, etc.  All this
has so little to do with anything related to TCPA that it boggles my mind.
And then in answer 12 we're back to the claim that TCPA can stop computers
from booting.  Saddam better not buy a TCPA computer or the U.S. will
render it inoperative, using a "serial number revocation list", according
to the FAQ.  Are you aware of any such capability in TCPA?  I didn't see
any such data structure.
Ross Anderson means well, and so does David Wagner.  But in the long
run it hurts the credibility of critics when they make exaggerated and
unfounded claims.

@_date: 2002-08-01 21:15:09
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
TCPA apparently does not have "licensing terms" per se.  They say,
in their FAQ, "The TCPA spec is currently set up as a 'just publish' IP model."
So there are no licensing terms to enforce, and no guarantees that
people won't do bad things outside the scope of the spec.  Of course,
you realize that the same thing is true with PCs today, right?  There are
few guarantees in this life.
If you think about it, TCPA doesn't actually facilitate the kind of
crypto-signature-checking you are talking about.  You don't need all
this fancy hardware and secure hashes to do that.  Your worrisome
signature checking would be applied on the software which *hasn't
yet been loaded*, right?  All the TCPA hardware will give you is a
secure hash on the software which has already loaded before you ran.
That doesn't help you; in fact your code can pretty well predict the
value of this, given that it is running.  Think about this carefully,
it is a complicated point but you can get it if you take your time.
In short, to implement a system where only signed code can run, TCPA is
not necessary and not particularly helpful.
I don't follow what you are getting at with the open source.  Realize that
when you boot a different OS, the TCPA attestation features will allow
third parties to detect this.  So your open source OS cannot masquerade
as a different one and fool a third party server into downloading data
to your software.  And likewise, data which was sealed (encrypted)
under a secure OS cannot be unsealed once a different OS boots, because
the sealing/unsealing is all done on-chip, and the chip uses the secure
hash registers to check if the unsealing is allowed.
Not sure I follow this here... the sealed data cannot be reported by an
open source OS because the secret keys never leave the chip without being
themselves encrypted.  As for your second proposal, you are suggesting
that you could write an OS which would only run signed applications?
And run it on a TCPA platform?  Sure, I guess you could.  But you wouldn't
need TCPA features to do it.  See the comments above: any OS today could
be modified to only run apps that were signed with some special key.
You shouldn't blame TCPA for this.
TCPA is a hardware spec.  Peter was asking about TCPA, and I gave him the
answer.  You can hypothesize all the facist software you want, but you
shouldn't blame these fantasies on TCPA.
Doesn't Microsoft already sign their system DLLs in NT?
Again, you are being entirely hypothetical here.  Please describe exactly
how either attestation or secure storage would assist in creating a boot
loader that would refuse to run Linux, or whatever other horrible disaster
you envision.
Look, I have describe in detail how it works, and you're just giving
these meaningless slogans.  TCPA lets you prove to other people what
code you are running; it lets you seal data such that it can only be
unsealed by the same code which sealed it.  How does this relate to
your little saying?  The ability to encrypt data means... the ability
to encrypt code?  So what?
Nonsense, there is no need to stop people from compiling their own
code in order to protect data!  The steps are simple: trusted app runs,
connects to server; proves it is trusted via TCPA attestation; server
downloads data to trusted app based on attestation; trusted app seals
data.  User reboots into open source OS, can't access data because it
is sealed; can't fool server because of attestation.  He can write all
the code he wants and it won't change this logic.  TCPA does not depend
on stopping people from running their own code; it depends on verifying
what code is running, and tying it to the crypto.  That's all.

@_date: 2002-08-02 15:30:03
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Yes, my name is "AARG!".  That was the first thing my mother said after
I was born, and the name stuck.
Not really.  For Peter's information, the name associated with a
message through an anonymous remailer is simply the name of the
last remailer in the chain, whatever that remailer operator chose
to call it.  AARG is a relatively new remailer, but if you look at
 you will see that it is very
reliable and fast.  I have been using it as an exit remailer lately
because other ones that I have used often produce inconsistent results.
It has not been unusual to have to send a message two or three times
before it appears.  So far that has not been a problem with this one.
So don't read too much into the fact that a bunch of anonymous postings
have suddenly started appearing from one particular remailer.  For your
information, I have sent over 400 anonymous messages in the past year
to cypherpunks, coderpunks, sci.crypt and the cryptography list (35
of them on TCPA related topics).

@_date: 2002-08-02 16:56:42
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Peter Trei envisions data recovery in a TCPA world:
It's not quite as bad as all this, but it is still pretty bad.
You don't have to send your data to Intel, just a master storage key.
This key encrypts the other keys which encrypt your data.  Normally this
master key never leaves your TPM, but there is this optional feature
where it can be backed up, encrypted to the manufacturer's public key,
for recovery purposes.  I think it is also in blinded form.
Obviously you'd need to do this backup step before the TPM crashed;
afterwards is too late.  So maybe when you first get your system it
generates the on-chip storage key (called the SRK, storage root key),
and then exports the recovery blob.  You'd put that on a floppy or some
other removable medium and store it somewhere safe.  Then when your
system dies you pull out the disk and get the recovery blob.
You communicate with the manufacturer, give him this recovery blob, along
with the old TPM key and the key to your new TPM in the new machine.
The manufacturer decrypts the blob and re-encrypts it to the TPM in the
new machine.  It also issues and distributes a CRL revoking the cert on
the old TPM key so that the old machine can't be used to access remote
TCPA data any more.  (Note, the CRL is not used by the TPM itself, it is
just used by remote servers to decide whether to believe client requests.)
The manufacturer sends the data back to you and you load it into the TPM
in your new machine, which decrypts it and stores the master storage key.
Now it can read your old data.
Someone asked if you'd have to go through all this if you just upgraded
your OS.  I'm not sure.  There are several secure registers on the
TPM, called PCRs, which can hash different elements of the BIOS, OS,
and other software.  You can lock a blob to any one of these registers.
So in some circumstances it might be that upgrading the OS would keep the
secure data still available.  In other cases you might have to go through
some kind of recovery procedure.
I think this recovery business is a real Achilles heel of the TCPA
and Palladium proposals.  They are paranoid about leaking sealed data,
because the whole point is to protect it.  So they can't let you freely
copy it to new machines, or decrypt it from an insecure OS.  This anal
protectiveness is inconsistent with the flexibility needed in an imperfect
world where stuff breaks.
My conclusion is that the sealed storage of TCPA will be used sparingly.
Ross Anderson and others suggest that Microsoft Word will seal all of
its documents so that people can't switch to StarOffice.  I think that
approach would be far too costly and risky, given the realities I have
explained above.  Instead, I would expect that only highly secure data
would be sealed, and that there would often be some mechanism to recover
it from elsewhere.  For example, in a DRM environment, maybe the central
server has a record of all the songs you have downloaded.  Then if your
system crashes, rather than go through a complicated crypto protocol to
recover, you just buy a new machine, go to the server, and re-download
all the songs you were entitled to.
Or in a closed environment, like a business which seals sensitive
documents, the data could be backed up redundantly to multiple central
file servers, each of which seal it.  Then if one machine crashes,
the data is available from others and there is no need to go through
the recovery protocol.
So there are solutions, but they will add complexity and cost.  At the
same time they do add genuine security and value.  Each application and
market will have to find its own balance of the costs and benefits.

@_date: 2002-08-03 10:55:19
@_author: AARG! Anonymous 
@_subject: Privacy-enhancing uses for TCPA 
Here are some alternative applications for TCPA/Palladium technology which
could actually promote privacy and freedom.  A few caveats, though: they
do depend on a somewhat idealized view of the architecture.  It may be
that real hardware/software implementations are not sufficiently secure
for some of these purposes, but as systems become better integrated
and more technologically sound, this objection may go away.  And these
applications do assume that the architecture is implemented without secret
backdoors or other intentional flaws, which might be guaranteed through
an open design process and manufacturing inspections.  Despite these
limitations, hopefully these ideas will show that TCPA and Palladium
actually have many more uses than the heavy-handed and control-oriented
ones which have been discussed so far.
To recap, there are basically two technologies involved.  One is "secure
attestation".  This allows machines to securely receive a hash of the
software which is running remotely.  It is used in these examples to
know that a trusted client program is running on the remote machine.
The other is "secure storage".  This allows programs to encrypt data
in such a way that no other program can decrypt it.
In addition, we assume that programs are able to run "unmolested";
that is, that other software and even the user cannot peek into the
program's memory and manipulate it or learn its secrets.  Palladium has
a feature called "trusted space" which is supposed to be some special
memory that is immune from being compromised.  We also assume that
all data sent between computers is encrypted using something like SSL,
with the secret keys being held securely by the client software (hence
unavailable to anyone else, including the users).
The effect of these technologies is that a number of computers across
the net, all running the same client software, can form their own
closed virtual world.  They can exchange and store data of any form,
and no one can get access to it unless the client software permits it.
That means that the user, eavesdroppers, and authorities are unable to
learn the secrets protected by software which uses these TCPA features.
(Note, in the sequel I will just write TCPA when I mean TCPA/Palladium.)
Now for a simple example of what can be done: a distributed poker game.
Of course there are a number of crypto protocols for playing poker on the
net, but they are quite complicated.  Even though they've been around
for almost 20 years, I've never seen game software which uses them.
With TCPA we can do it trivially.
Each person runs the same client software, which fact can be tested
using secure attestation.  The dealer's software randomizes a deck and
passes out the cards to each player.  The cards are just strings like
"ace of spades", or perhaps simple numerical equivalents - nothing fancy.
Of course, the dealer's software learns in this way what cards every
player has.  But the dealer himself (i.e. the human player) doesn't
see any of that, he only sees his own hand.  The software keeps the
information secret from the user.  As each person makes his play, his
software sends simple messages telling what cards he is exposing or
discarding, etc.  At the end each person sends messages showing what
his hand is, according to the rules of poker.
This is a trivial program.  You could do it in one or two pages of code.
And yet, given the TCPA assumptions, it is just as secure as a complex
cryptographically protected version would be that takes ten times as
much code.
Of course, without TCPA such a program would never work.  Someone would
write a cheating client which would tell them what everyone else's cards
were when they were the dealer.  There would be no way that people could
trust each other not to do this.  But TCPA lets people prove to each
other that they are running the legitimate client.
So this is a simple example of how the secure attestation features of
TCPA/Palladium can allow a kind of software which would never work today,
software where people trust each other.  Let's look at another example,
a P2P system with anonymity.
Again, there are many cryptographic systems in the literature for
anonymous communication.  But they tend to be complicated and inefficient.
With TCPA we only need to set up a simple flooding broadcast network.
Let each peer connect to a few other peers.  To prevent traffic
analysis, keep each node-to-node link at a constant traffic level using
dummy padding.  (Recall that each link is encrypted using SSL.)
When someone sends data, it gets sent everywhere via a simple routing
strategy.  The software then makes the received message available to the
local user, if he is the recipient.  Possibly the source of the message
is carried along with it, to help with routing; but this information is
never leaked outside the secure communications part of the software,
and never shown to any users.
That's all there is to it.  Just send messages with flood broadcasts,
but keep the source locked inside the secure part.  Messages can be
sent and received, and neither participants nor outsiders can tell
what the source of any message is.
As with the earlier example, such a system would never work without TCPA.
Rogue software would easily determine which direction messages were coming
from, and the anonymity provided would be extremely limited
But by eliminating rogues using secure attestation, and keeping the
sensitive data safe from molestation, we are able to achieve using a
very simple system what otherwise takes tremendous complexity.
Here's one more example, which I think is quite amazing: untraceable
digital cash with full anonymity, without blinding or even any
cryptography at all! (Excepting of course the standard TCPA pieces like
SSL and secure storage and attestation.)
The idea is, again, trivial.  Making a withdrawal, the client sends the
user's password and account ID to the bank (this information is kept in
secure storage).  The bank approves, and the client increments the local
"wallet" by that amount (also kept in secure storage).  To make a payment,
use the anonymous network for transport, and just send a message telling
how much is being paid!  The recipient increments his wallet by that
amount and the sender decrements his.  Deposit works analogously to
Again, that's all there is to it.  Nothing could be simpler.  Yet it
provides for secure (assuming TCPA is secure), anonymous, untraceable
payments.  The secure attestation is crucial, of course, to make sure
that people are running legitimate clients, otherwise cheating would
be rampant.  And the secure storage is equally crucial, otherwise any
software could increment the sum stored in the wallet and everyone would
accept and believe those payments.
I understand, of course, that this specific example is not very practical
unless we have an extremely secure version of TCPA.  If anyone who
can break the security can give themselves unlimited money, it means
that the security has to be essentially perfect.  So this is more of a
proof of concept than a realistic proposal.  But eventually, with TCPA
technology integrated into a tamper-proof, nanotech CPU with molecular
sensors and built-in self-destructs, possibly this might be good enough.
Or you could augment this solution with some crypto, similar with the
"wallets with observers" proposals from Chaum and from Brands.  Note that
we can make the client open-source, allowing anyone to verify that it
has no back doors or cheating potentials, which allows all users to
trust that it is not going to hurt them (a problem that takes great
complexity to solve with the observer protocols).  But still the bare
simplicity of the system should make clear how powerful something like
TCPA can be for this kind of application.
I could go on and on, but the basic idea is always the same, and hopefully
once people see the pattern they will come up with their own ideas.
Being able to write software that trusts other computers allows for an
entirely new approach to security software design.  TCPA can enhance
freedom and privacy by closing off possibilities for surveillance and
interference.  The same technology that protects Sony's music content
in a DRM application can protect the data exchanged by a P2P system.
As Seth Schoen of the EFF paraphrases Microsoft, "So the protection of
privacy was the same technical problem as the protection of copyright,
because in each case bits owned by one party were being entrusted to
another party and there was an attempt to enforce a policy."
( 3rd bullet point)
In fact, TCPA and Palladium have tremendous potential for enhancing
and protecting privacy, if people will just look at them with an
open mind.

@_date: 2002-08-03 18:25:28
@_author: AARG! Anonymous 
@_subject: Other uses of TCPA 
I'm sorry, I'm just using the language and data structures from
TCPA to try to understand how your assertion could relate to it.
If you are making a claim about TCPA, perhaps you could express
it in terms of those specific features which are supported by
No, the TPM public key is not widely available to everyone.  In fact,
believe it or not, it is a relatively closely held secret.  This is
because the public key is in effect a unique identifier like the Intel
processor ID number, and we all know what a firestorm that caused.
Intel is paranoid about being burned again, so they have created a very
elaborate system in which the TPM's public key is exposed only as narrowly
as possible.
The TPM public key is called the Endorsement key - this is the key which
is signed by the manufacturer and which proves that the TPM is a valid
implementation of TCPA.  Here is what section 9.2 of the TCPA spec says
about it:
: A TPM only has one asymmetric endorsement key pair. Due to the nature of
: this key pair, both the public and private parts of the key have privacy
: and security concerns.
: Exporting the PRIVEK from the TPM must not occur. This is for security
: reasons. The PRIVEK is a decryption key and never performs any signature
: operations.
: Exporting the public PUBEK from the TPM under controlled circumstances
: is allowable. Access to the PUBEK must be restricted to entities that
: have a "need to know." This is for privacy reasons.
The PUBEK is the public part of the TPM key and is not supposed to
be widely available.  It is only for those who have a "need to know",
which definitely does not include everyone who would like to send some
software to the system.  In fact, it is only sent to Privacy CAs, which
use it to encrypt a cert on a transient key that will be widely exposed.
But I'm sorry, I'm going unintelligible again, aren't I?
Also, nothing in the TCPA standard refers to securely knowing the time.
Section 10.7 says "There is no requirement for a clock function in the
TPM", so the date/time info comes from the normal, insecure hardware
Well, without using any jargon, I will only say that TCPA doesn't work
like this, and if you don't believe me, you will have to read the spec
and verify it for yourself.

@_date: 2002-08-03 18:30:09
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
It's not quite that bad.  I mentioned the blinding.  What happens is
that before the master storage key is encrypted, it is XOR'd with a
random value, which is also output by the TPM along with the encrypted
recovery blob.  You save them both, but only the encrypted blob gets
sent to the manufacturer.  So when the manufacturer decrypts the data,
he doesn't learn your secrets.
The system is cumbersome, but not an obvious security leak.

@_date: 2002-08-03 23:50:24
@_author: AARG! Anonymous 
@_subject: Other uses of TCPA 
PRIVEK, the TPM's private key, is generated on-chip.  It never leaves
the chip.  No one ever learns its value.  Given this fact, who would
you say owns and controls it?
Not much information is provided about this feature in the Palladium
white paper.  From what I understand, no one is able to manipulate
the program when it is in this trusted space, not the machine owner,
nor any external party.  Only the program is in control.
No, for several reasons.  First, PRIVEK doesn't really have an owner
in the sense you mean.  It is more like an autonomous agent.  Second,
the PRIVEK stuff is part of the TCPA spec, while the trusted space is
from Palladium, and they don't seem to have much to do with each other.
And last, just because a program can run without interference, it is a
huge leap to infer that anyone can put a trojan onto your machine.
No one has said anything different despite the 40+ messages I have sent on
this topic.  Is this because TCPA is that bad, or is it because everyone
is stubborn?  Look, I just showed that all these bad things you thought
about TCPA were wrong.  The PRIVEK is not controlled by someone else,
it does not own the trusted space, and it allows no one to put a trojan
onto your machine.
But you won't now say that TCPA is OK, will you?  You just learned some
information which objectively should make you feel less bad about it, and
yet you either don't feel that way, or you won't admit it.  I am coming
to doubt that people's feelings and beliefs about TCPA are based on facts
at all.  No matter how much I correct negative misconceptions about these
systems, no one will admit to having any more positive feelings about it.

@_date: 2002-08-04 14:30:14
@_author: AARG! Anonymous 
@_subject: Other uses of TCPA 
I am judging the proposal on the basis of the spec.  I think that is the
correct way to do the analysis.  Then, you can extend your analysis on
the basis of ways you think the spec might change.  But surely the spec
ought to be a starting point for any judgement.  Otherwise there is no
factual basis for the analysis.
Yet no one here has said that now that they understand the spec better,
they don't think TCPA as specified would be as bad as they thought.
Some people, like James Donald and Ryan Lackey, have said that they
don't think TCPA would be all that bad if it weren't for government,
copyright laws, etc.  But no one has suggested that my many postings
have changed their opinion about TCPA in and of itself.
The Alliance consists of Compaq, Intel, IBM, HP, and Microsoft.
(Since then HP has bought Compaq.)  Even if you hate Microsoft, you
probably don't hate all of these companies, do you?
I think the spec directly contradicts this claim!  If they cared so little
about user privacy, why would they use an elaborate system with a Privacy
CA to make sure no user-identifiable information leaks onto the net?
Surely the simpler approach would be what James Donald suggested, to send
out the TPM's public key and let people use that.  But it is a per-user
identifier and so they went to great lengths to conceal it.
Furthermore, if their motivations were so bad, wouldn't it have been
better for them for TCPA to work the way most people assume, to only
load software which has been signed by some authority?  Instead they
are careful to let any software load, and to report its status to third
parties, so the third parties can make their own judgements about what
to trust.  Why do you think they did it like this, if they were so
determined to minimize the control of the end user?
Who cares what I am?  It's facts that count!  I could be Satan Incarnate
and it wouldn't matter.  I am giving you facts about TCPA based on my
personal investment of time to study the system.  Tell me this: if you
care about this standard, why not get it and learn it yourself?  Not one
person here has done this!  Everyone prefers to believe falsehoods than
to learn the truth for themself.  Do you think that is a good strategy
for survival in a potentially hostile and dangerous world?
All I am really asking for is someone to acknowledge that I have provided
information to them which makes them see TCPA as less dangerous and
damaging than they had thought based on the false information which has
been circulating.  I don't see how anyone can deny this.  The caricature
of TCPA that most people believe is very bad.  The truth is not so bad.
Logically, you *have* to believe that TCPA is not as bad as you thought,
when you are provided with the truth.
Let me ask you, Eugen: isn't a TCPA which is open, which will run all
software, which does not prevent any software from running, better than
a TCPA which will only run signed software?  I know you are a person who
is willing to think for himself and defy the conventional wisdom.  Please
respond to this message and explain to me how this logic strikes you.

@_date: 2002-08-04 22:30:14
@_author: AARG! Anonymous 
@_subject: Other uses of TCPA 
Maybe I wasn't too clear in my explanation.  I assume you know how public
key cryptography works.  The TPM chip generates an RSA key pair.  This key
pair is called the Endorsement Key.  The private key is called PRIVEK
and never leaves the chip.  The public key is called PUBEK and although
it is "sensitive", it does leave the chip under some circumstances.
One of those circumstances is when the chip is manufactured and comes off
the line.  It is powered up, generates the key pair, and exports PUBEK.
At that point the chip manufacturer creates an X.509 certificate that
signs PUBEK.
It is this cert which proves that the PUBEK is a legitimate Endorsement
Key.  While the cert is not widely shown (for privacy reasons), it is
used in a TCPA protocol, and this is ultimately what makes it impossible
for Joe Hacker to create a fake TCPA key.
Now, the part about recovering from a dead chip refers to a different
key.  It's called the "root of trust for storage" key, RTS.  This is used
for encrypting data on the disk.  The PUBEK was used for communicating
with third parties to prove that you had a legitimate TPM.  So there are
two different keys used for two different purposes.  Both of them are
generated on-chip, and no one ever learns either private key.
If your chip dies, you lose the PUBEK but that doesn't matter, nothing
is really locked to it.  You can just get a new motherboard and start
using the new PUBEK from that one's TPM chip.  It's the RTS key that
is a problem, because if you can't retrieve that, all the data on your
disk that was sealed (encrypted) using the TCPA mechanisms could be lost.
So they have a system to transfer the RTS key from one machine to another.
I've been thinking about writing a few pages summarizing TCPA and how the
crypto works, but then I think, why bother?  Everyone is already convinced
that the system is the spawn of Satan.  Nobody cares about the facts.
BTW I found a post by Ross Anderson,
in which he says that one of the worst claimed feaures from his TCPA FAQ
( censoring objectionable
programs/data off people's computers against their will, actually doesn't
rely on TCPA at all.  In fact he says they could do it with existing
Windows OS's just as well.
It's such an obviously nasty feature that I can't see them ever actually
trying this, but in any case it really doesn't have anything to do
with TCPA.  Maybe someone could ask Ross why his FAQ blames TCPA for a
feature that he admits isn't really related!  But no, that would be crazy.
Better to believe comfortable falsehoods than to seek the truth.

@_date: 2002-08-05 00:10:12
@_author: AARG! Anonymous 
@_subject: dangers of TCPA/palladium 
Actually there seem to be some hardware differences between TCPA and
Palladium.  TCPA relies on a TPM, while Palladium uses some kind of
new CPU mode.  Palladium also includes some secure memory, a concept
which does not exist in TCPA.
More generally, the hardware can attest to many aspects of the current
machine state, including the cumulative hash of the software which
has booted so far.
TCPA does not get into this part, only the Palladium white paper
mentions this.  However it does seem to be a logical component for
effective trusted computing.
Right.  This plus the attestation are what allow an application to create
a "closed world".  See my earlier message for examples of how this could
be used to enhance privacy and anonymity.  What better example of a
closed world than your own secrets?
I don't think his comments make that much sense.  I'd be curious to read
your take on them.  What is he talking about with the non-malleable
root of trusted storage?  Trusted storage seems like one of the least
objectionable aspects.  Is he confusing this with the endorsement key,
used to make the remote attestations?  Or is this related to the idea
that you won't be able to boot your OS of choice?
This is of course one of the biggest criticisms of TCPA - that it could be
changed so that you will only be able to boot certified OS's.  Don't you
think that would have to be done by law, rather than as a preemptive
act by the technologists (for antitrust reasons if nothing else)?
Why would such a law be passed?  IMO the social changes necessary to even
begin to imagine such a drastic step are so huge that the technological
implementation seems minor in comparison.  I don't think it is fair to
criticize this proposal for such a far-fetched possibility.
TCPA doesn't currently cover certifying operating systems.  They talk
about certifying TPMs, about certifying PC hardware designs and
implementations.  Possibly in the next version they will get into
issues like this.  In the mean time, supposedly HP is going forward
with an OS that can use TCPA features.
I think this analysis is largely correct, except that it won't be
as monolithic as you make it sound.  There won't be just one content
supplier who judges all software, that's obviously impossible.  Rather,
each different supplier will make its own determination of which software
you can trust.  And likewise for non-DRM applications.  Banks will decide
which banking software to trust.  Game networks will decide which game
clients to trust, etc.
I agree that it would be nice to see more flexibility there.  The Chaum
blinding patent expires in 2005, so maybe around then we can start seeing
privacy CA's that use blind signatures, which solves that problem.
The spec is obviously trying hard to protect privacy, it's just that
the mechanisms to do it right are extremely complex compared to the
straightforward way.
Nobody's putting a gun to your head and making you download content.
If you can't agree to the conditions, go do something else.  There are
much worse things that can happen in the world than that copyright
becomes enforceable.
Why not give the market a chance?  Company A provides the data with
Draconian DRM restrictions; company B gives you more flexibility in what
you do.  All else being equal, people will prefer company B.  So they
can charge more.  In this way a balance will be reached depending on how
much people really value this kind of flexibility and how much they are
willing to pay for it.  You and I don't get to decide, the people who
are making the decisions about what content to buy will decide.
And nobody's got the root key to my computer.  You make this claim in many
places in the document.  What exactly is this "root key" in TCPA terms?
The endorsement key?  It's private part is generated on-chip and never
leaves the chip!
I don't follow this.  What root key owners?  What APIs?  Could you say
more about how TCPA will help with software rental?
This is a good point, but again it depends on the specific content
realm.  There are not just one or two - there are thousands of kinds
of content, or even more.  Not everyone is going to require FIPS 140
levels of certification!
But possibly Disney and Sony will.  My guess is that if there ever is
a Linux program that will play their movies, it will be because those
companies contracted to get it written.  You may see this in many contexts
- software applications don't get certified, rather they are supplied
by the vendors, or the vendors arrange to get them done.
This part I don't understand too much; it's not a TCPA concept, and there
is little known about Palladium.  Supposedly the idea is that this is a
place that code can run without being touched by debuggers or viruses.
I don't know what happens if a virus gets itself loaded into this area,
if that is even possible.  Maybe all the different compartments are
isolated from each other.
Does this seem like a bad feature to you?
Now you're starting to go paranoid.  All the TCPA certification master
keys do is to certify that a system is TCPA compliant.  They don't have a
remote control over your machine!  They are more analogous to Verisign
in the X.509 world.  Last I checked they hadn't taken over my box.
As far as the field upgrade, it has to be authorized by the owner.
I'm disappointed to see this kind of fantasizing in what has been a
well grounded document until now.  If you're going to make this kind
of charge, that TCPA gives a universal remote control to government,
you need to back it up in detail.
I don't agree with your characterization that TCPA enforces policies
against the owner's interests.  He has to voluntarily agree to everything,
from turning on TCPA, to booting a TCPA compliant program, to running
an application which some third party will trust, to accepting data from
that third party under agreed-upon conditions.  If at any step he didn't
feel that what he was doing was in his interests, he can stop and do
something else.
When you walk into a store and pay money for food, is that store enforcing
policies against your interests?  Only from the most shallow perspective,
for if such policies were not widely enforced, you and I and everyone
else would starve.  We all participate voluntarily in these institutions.
Each payment we make is in our interests.  And the same thing is true
if you receive some data with conditions on how it is manipulated.
As far as the concern about changes, I think the smart thing to do is
to fight the bad and promote the good.  Definitely we should oppose any
proposal to make TCPA non-voluntary, to force people to boot a certain
OS, to limit what they can do on their computers.  But presently none
of those features are in TCPA.  Rather than saying TCPA is bad because
someone could make all these hypothetical changes, it makes more sense
to judge TCPA on its own, as a system that emphasizes user choice.
Involuntary TCPA is bad, voluntary is good.  So we should not fight TCPA,
we should fight proposals to make it involuntary.
I think you are looking at it far too narrowly.  Yes, this will provide
many opportunities for Microsoft to write new kinds of software.  But the
same is true for every other software company!  Financial software, web
services, security software, accounting - anything that involves trust
and security can benefit from TCPA.  Look at the example I gave earlier
for a TCPA based anonymous comm network.  Multiply that a thousand fold.
It's stupid to just look at what one company can do with this, without
considering what a whole world of creative people can accomplish.
Yes, it can make reverse engineering much more difficult.  But I'd rather
see people put their creative efforts into creating new products rather
than copying and piggybacking off someone else's success.
Again, you need to justify this remote root control notion.  I don't
see it at all.  Go back to your four functions of TCPA/Palladium -
they were pretty accurate.  Where was the remote root control in there?
I'd say that it is a powerful technology with an almost infinite number of
potential applications.  Being able to trust software running on a remote
system is something that has never been possible before on the Internet.
We can only begin to see what will be possible with this capability.

@_date: 2002-08-05 12:30:11
@_author: AARG!Anonymous 
@_subject: Suggested entry into the TCPA spec 
Here is a suggestion for how to appraoch the TCPA spec based on the parts
I have found to be relatively good explanations.  The spec is available
from First read the first few pages up to page 7.  This provides an overview
and a block diagram, although at this point not all the terms will be
familiar.  One hint: the "root of trust for measurement" is the set of
hardware which has to be working for the boot measurement process to work:
the CPU, the TPM, the part of the BIOS that deals with measurements, the
motherboard, the secure connections of the chips to the motherboard.
If all this stuff is OK then the measurements will be accurate.
(Measurements basically are hashes of code and of machine configuration
status.)  The "root of trust for reporting" is the endorsement key, or
more fundamentally the cert on the endorsement key.  The cert is issued by
the manufacturer, AKA the "TPM Entity" or TPME.  That's what makes other
people believe your attestations.  And the "root of trust for storage"
is the storage root key, described in the section on protected storage.
There are a few more pages of introduction which aren't too clear,
then a long section of data structures which should be skipped until
you need to reference them.
This brings you to page 97, authorization and ownership.  I haven't
really studied this part.  Probably just read this one page to get
an idea of what is involved.  I still need to learn more about this.
I'd skip on to pages 136-137, on the measurement process and the
PCRs which hold the results of the measurement.
Then I'd read pages 145-150 on protected storage.  This part is pretty
well written.  It is a reasonably self contained part of the TPM
functionality.  You just need to know a little bit about the PCRs from
the earlier section to understand how data is locked to the specific
program which is running.
Then I'd read page 261 on the endorsement key, and then 267-269 on
how it is used to create a pseudonymous identity.  This is the part
about communicating with the Privacy CA.  BTW an expert told me he has
concerns about possible security loopholes in this protocol, but he is
communicating with TCPA about them.
I think if you just read these selections, about 15 pages, you will have
a much better idea of how the spec works.  Then you can read some of
the specific API descriptions to see more details about the functionality.
There is also a glossary at the end which can be helpful for some (but
not all) of the terminology.
There is another spec,
that describes the specific register and trap binding for implementing
the TCPA API on Intel PCs.  It is much shorter but it is pretty
incomprehensible until you have at least read the basics of the main

@_date: 2002-08-05 16:25:26
@_author: AARG! Anonymous 
@_subject: dangers of TCPA/palladium 
Sure, but how many pages would it take in the spec to describe the
protocol?  Especially given their turgid technical-writer prose?
Brands took a whole book to describe his credentials thoroughly.
In any case, I agree that something like this would be an excellent
enhancement to the technology.  IMO it is very much in the spirit of TCPA.
I suspect they would be very open to this suggestion.
They don't say much about patents or intellectual property licensing in
the documents I have found on their site.  It's not clear to me that the
so-called Palladium patents actually cover TCPA.  You'd have to look at
them in detail.
What software updates, exactly?  Spec reference?
No, I don't recall seeing this in the spec.  Hopefully as you have a
chance to study it you can point out this part.  I may well have missed
a portion.  If so then I agree that this is a potentially serious problem.
I was talking about the optional TPM_FieldUpgrade function described on
page 251 of the spec.  It is apparently intended for bug fixes and such.
I doubt that there will be that many bug fixes, or that users will
install them that often.  And if an upgrade does obvious bad things
like the various despotic features you fear, keeping you from booting
Linux or whatever, people can avoid installing it.  I don't see this as
a mechanism for someone to take over the world.
It's true what you say about the user having to trust the manufacturer
about the upgrade - but he has to trust the manufacturer anyway that
the chip works right.  Whatever monitoring process may be in place to
further that trust can also protect the upgrade as well.
Well, he can choose who he buys the TPM chip from, I suppose.
But upgrades are basically new firmware for the TPM chip, so they will
probably always come from the manufacturer.
Why exactly is this so much more of a threat than, say, flash BIOS
upgrades?  The BIOS has a lot more power over your machine than the
TPM does.
Everything I have read indicates that he can boot other operating systems.
And the spec seems to bear that out.  I don't see anywhere in there that
it would stop people from booting whatever software they want.
The only way that TCPA will become as popular as you fear is if it really
solves problems for people.  Otherwise nobody will pay the extra $25 to
put it in their machine.
That's largely the case already.  That's why so many people choose
Windows, to be compatible with what everyone else is doing.
You seem to judge things by the outcome: Windows being more popular
and powerful is bad.  I judge by process: letting people make their own
decisions is good, even if it leads to an outcome I personally don't like.
Where would that fit in the spec?  The spec is intentionally not about
policy; there is no such thing as a "TCPA policy module".  How would
TCPA stop people from running their own strong cryptography?  You are
extrapolating way, way beyond anything that is in this spec.  This is
just imagination and paranoia.
Be concrete.  What changes would have to be made to TCPA to get the
effects of a mandatory Clipper chip.  Would they be made in secret or
would some government have to pass a law before it happened?  Would the
changes happen in one country or all countries?  Paint me a scenario
that has some kind of connection to reality.  Otherwise this sounds
like South Park logic:
1. Get TCPA widely used
2. ...
3. Take over the world
Again, what specific TCPA features will they exploit to accomplish this?
That's already the case.  Face it: if government decided to enforce
mandatory key escrow, most users would not object and would be unable
to help themselves if they did, whether TCPA existed or not.
That's possible, and if we lived in a dictatorship I would be more
concerned.  But if new technologies make laws more enforceable, and
people are uncomfortable with the loss of freedom, they will vote to
relax restrictions.
And as I have pointed out, it is possible that TCPA could allow for
other applications that would actually magnify freedom.  The thrust of
the proposal is to improve the ability for applications to keep their
secrets, both locally and on the net.
I'm just looking at the TCPA spec and trying to evaluate it.  I don't
see all the bad things that people have said are in there.  Instead I
see a lot of effort to provide security while still protecting user
control and privacy.  It's true that some developers may use the new
power of TCPA for bad purposes, while other people will use it for good.
I say, let us focus our criticism, don't waste time trashing a proposal
because of things that are not in it.
I just don't see that TCPA is of that much use to them, given that they
already have essentially unlimited power.  Ultimately, in the West,
governments are the responsibility of the populace.
If the Chinese government were to do a TCPA-like system, I doubt that
it would look much like this one.
Even if so, that's no excuse for trying to stop people from making their
own decisions about what to do with their resources.  You shouldn't stop
people from using a technology because you are afraid that someone else
may come along and make it mandatory.
Absolutely!  I fully agree with these sentiments.  I think as you study
the spec in detail you will see that it does a very good job by these
standards.  But ultimately you can't let users take control of their TPM
chip and force it to lie to other people, without losing the whole point
of the system.  Doing that would be like insisting on a PKI where every
user could make arbitrary modifications to certs issued by other people.
Sure, in some sense it may increase freedom, but it's at the cost of
making the whole infrastructure worthless.
The thing that makes your certified key useful is the raw fact that you
can't change it.  By the same token, the thing that makes TCPA useful
is the fact that you can't get at sealed data or get the system to lie.
By voluntarily giving up this ability locally, you gain tremendous power
in interacting with other people.

@_date: 2002-08-05 17:00:25
@_author: AARG! Anonymous 
@_subject: Magic Money 
Here is a link for Magic Money:
 also has a few different
versions.  I think you need the latest mgmnyxx.zip and the latest

@_date: 2002-08-06 15:15:17
@_author: AARG! Anonymous 
@_subject: USENIX Security TCPA/Palladium Panel Wednesday 
Amazing claims you are making there.  Claiming that the TPM will be
included on "all future motherboards"; claiming that an objective is
to meet the operational needs of law enforcement and intelligence;
claiming that TCPA members (all 170 of them?) have more access to his
computer than the owner; fantasizing about an "approved hardware list"
and "serial number revocation list" which don't exist in the spec(!);
further fantasies about a "list of undesirable applications" (where do
you get this stuff!).
On page 16, the OS is going to start the secure time counter (but TCPA
has no secure time feature!); synchronize time against authenticated
time servers (again, no such thing is in the spec); and download the
hardware and serial number revocation lists (nothing exists like this!).
I honestly don't understand how you can say this when there is nothing
like it in the TCPA specification.
Are you talking to insiders about a future revision?  Do you know for
a fact that TCPA will hae SNRL's and such in the future?
Or are you just being political, trying to increase pressure on TCPA
*not* to go with serial number revocation lists and the like, by falsely
claiming that this is in the design already?

@_date: 2002-08-06 15:20:02
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
This has to be true for the basic security goal of remote trust, right?
The purpose is so that the user can credibly convince a remote system that
he is running a certain program.  Explain to me how he could do this if
he were able to reload the TPM key with one of his own, or get access
to the private key?  Wouldn't that let him forge arbitrary messages?
You might as well complain that Verisign doesn't share their private key
with everyone.  Either way you lose the trust properties of the system.
We have had other systems which work like this for a long while.
Many consumer devices are sealed such that if you open them you void
the warranty.  This is to your advantage as a consumer; it means that you
can take the device in to get it fixed, and the intact seal proves that
you didn't mess with the insides and break it.  By your logic, consumers
ought to be able to bypass such seals since they own the device.  But if
this were true, don't you agree that it would make the seals useless?

@_date: 2002-08-06 15:30:13
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
I have in fact never claimed to be a TCPA insider; quite the opposite,
I have consistently explained that I am merely someone who has taken the
time to study the specification and other documents in order to educate
myself about the system.
My interpretation of the spirit of the proposal comes solely from
reading these documents.  They go to considerable lengths to protect user
privacy, even to the point that the main TPM key is an encrypt-only key,
not allowed to issue signatures!  I think this is to reduce the chance
of mistakenly using it to sign attestations.  Further, the protocol
with the Privacy CA is very complex and adds considerable complexity.
If they didn't care about privacy I don't think the design would devote
this much effort to it.
Maybe this is true, but I can certainly imagine reasons other than
a secret desire to compromise users' privacy.  Going with blinding
would make the spec more complex, and they might well have had their
hands full at the time just trying to get V1.0 out.  Then there are the
patent issues with either Chaum or Brands blinding.  Plus, Brands works
with very special-format keys, variants on discrete log keys, while the
spec generally assumes RSA keys (possibly going to ECC).  And finally,
they may simply not have been that familiar with blinding technology,
which isn't that widely known outside a small subset of the cryptographic
community.  TCPA is more of a security spec than a cryptographic one,
and it's likely that not one of the main developers had every read a
paper by Stefan Brands.
Besides, after reading Lucky's absurdly conspiratorial slide show I am
skeptical about how accurately he can be relied on to report information
about TCPA.  He obviously thinks they are the spawn of the devil
and is willing to say anything in public in order to discredit them.
Otherwise why would he have made so many charges at Defcon that are
utterly without foundation?

@_date: 2002-08-07 12:35:12
@_author: AARG! Anonymous 
@_subject: Palladium: technical limits and implications 
Obviously no application can reliably know anything if the OS is hostile.
Any application can be meddled with arbitrarily by the OS.  In fact
every bit of the app can be changed so that it does something entirely
different.  So in this sense it is meaningless to speak of an app that
can't be lied to by the OS.
What Palladium can do, though, is arrange that the app can't get at
previously sealed data if the OS has meddled with it.  The sealing
is done by hardware based on the app's hash.  So if the OS has changed
the app per the above, it won't be able to get at old sealed data.
And of course remote attestation will not work either, if the app has
been meddled with.
This means that an app can start running, attest to its "clean" status
to a remote server, download some data from that server, and seal it.
Then at a later time, IF the app is able to unseal that data, then it
is true that the app has not been meddled with and is not running
on virtualized hardware.
That is how I understand these sorts of claims.

@_date: 2002-08-07 12:50:29
@_author: AARG! Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
I'd like the Palladium/TCPA critics to offer an alternative proposal
for achieving the following technical goal:
  Allow computers separated on the internet to cooperate and share data
  and computations such that no one can get access to the data outside
  the limitations and rules imposed by the applications.
In other words, allow a distributed network application to create a
"closed world" where it has control over the data and no one can get
the application to "cheat".  IMO this is clearly the real goal of TCPA
and Palladium, in technical terms, when stripped of all the emotional
As I posted previously, this concept works especially well for open source
applications.  You could even have each participant compile the program
himself, but still each app can recognize the others on the network and
cooperate with them.  And this way all the participants can know that
the applications aren't doing anything different than what they claim.
This would be a very powerful capability with many uses that you might
find both good and bad.  I posted a long message earlier with three
examples of privacy-oriented applications: secure game playing, anonymous
P2P networking, and untraceable digital cash.  In addition it can be used
for DRM, restricting access to sensitive business or government data,
and similar applications.
For those of you who claim that such a technology is not necessarily
objectionable in itself, but that the implementations in TCPA and
Palladium are flawed, please explain how you could do it better.  How can
you maximize user control and privacy and minimize the potential for
government or corporate takeovers?
In other words, what *exactly* is wrong with the way that TCPA and
Palladium choose to do things?  Can you fix those problems and still
achieve the basic goal, above?

@_date: 2002-08-07 16:55:11
@_author: AARG! Anonymous 
@_subject: Palladium: hardware layering model 
I don't think this is right, as Peter said that the Palladium stuff could
load many days after boot.  So I don't think the "ring-0" mode underlies
normal supervisor mode as you have shown it.  Instead I think they are
relatively orthogonal.
I'm not sure how to draw it, but I would envision the TOR as a device
driver which controls two devices: the trusted execution space, which
is some special memory (on the cpu?), and the SCP, the crypto processor.
Let us suppose that there is a special instruction to load a block of
code into the trusted execution space and give it a new process ID.
At the same time this causes the SCP to hash it so that it can attest
to it later.  Let us also suppose that the ring-0 mode is used only when
running code out of the trusted execution space (TE space).
What is special about ring-0?  Two things: first, it can see the code
in the TE space so that it can execute it.  And second, it doesn't
trap into supervisor mode for things like debugger single-stepping.
I'm not familiar with the details of the Pentium family but on most CPUs
the debugger single-steps things by setting a flag and returning into
the code.  The code executes one instruction and then automatically traps
into supervisor mode, which hands off to the debugger.  This process must
be suppressed in ring-0 mode, and likewise for any other features which
can force a ring-0 process to trap involuntarily into supervisor mode,
which exposes the registers and such.
The TOR would then manage the various processes running in the TE space,
and their interactions with ordinary code, and possibly the interactions
of both with the SCP.  I'm not sure if the TOR runs in ring-0 mode and
in the TE space; probably it does, as the SCP can attest to it, and we
wouldn't want non-Palladium processes to debug it.
So really the whole TOR/SCP/TE-space/trusted-agents stuff is relatively
orthogonal to the rest of windows.  It's almost like you had a fully
functional 2nd CPU in there that you could load code into and have it run;
that CPU's memory is the TE space, its mode is the ring-0, it has access
to the SCP, and it runs the TOR and trusted agents.  But Palladium has
to use the regular CPU for this so they firewall it off with the ring-0
mode which locks it into this restrictive mode.
That's just a guess, probably wrong in many details, but consistent
with what I understand so far.  Mostly I am hoping to encourage Peter
to come forward and correct our misconceptions.
I have this as well; loading a user agent into TE space creates the hash
"fingerprint" which will be used for sealing and attestation; other ring-0
agents will have their own fingerprints and won't be able to unseal what
this agent does.  The SCP compares fingerprints at unseal time to what
it was at seal time and (optionally) won't unseal if they don't match.
(This is one of multiple sealing options.)
I don't think so; not necessary in my model, would require significant
re-architecting of Windows which won't happen, and inconsistent with
the claim that Palladium can load days after boot.
Must be true.  Some questions: how big is the TE space?  How many agents
can live there at once?  Do they swap in/out?  Does data go there, or
just code?
But ring-0 cannot make arbitrary restrictions on sup. mode.  Remember
they can't afford to re-architect either the entire CPU nor the entire
OS for this.  The simplest is that in ring-0 mode you disable certain
functions that could trap you into supervisor mode thereby losing control
of the CPU, and this ring-0 mode gains you access to the TE space.
I'm not much of artist but I would put the new stuff off to the side of
this in its own tower.  Ring-0 mode at the bottom, running the TOR which
is shown above it, which manages the user agents which would be on top.
The SCP is further off to the side, perhaps managed by the TOR.
I'm not sure what you mean by the OS observing system calls.
By definition, system calls go into the OS.  So I don't think that will
ever stop happening.  But it does mean that when a ring-0 trusted agent
makes a system call, we change to normal supervisor mode which makes
the trusted space invisible.
The point we are dancing around is this.  How does it protect the
data, along the whole path from the remote machine, through where it is
processed locally, until it is sealed on the local disk?  It seems that
it must be in the clear for a while on the local machine.  Where is that -
in regular memory, or TE space?
It's not that big a deal to be unable to read TE *code*.  From what
Peter says, that is typically not encrypted on the disk.  So the code
is no secret.  What we must be unable to read are the data being handled
by this code: the registers, the contents of memory that are sensitive.
And by the registers I include the PC, since that would leak information
about the data.  We can't single-step it, we can't put breakpoints into
it, we can't change it while it is running.
I am curious about this from the technical perspective.  I think this is
one of the most interesting developments in many years on the security
But frankly I don't think it will do them much good to tell you and most
other cypherpunks about it, because whatever they say, you and others will
twist it and lie if necessary a la Lucky to turn it into some horrible
perversion.  Even if the design were totally benign, that doesn't mean
Microsoft/Intel couldn't change it someday, right?  They could put a
machine gun into every PC aimed at the user, and a camera over his head.
That's the level of reasoning in Lucky's Defcon presentation, except
that he says that they've already done it.  I applaud Peter's patience
but pity him for his naive belief that he is engaging in a good faith
exchange where he will get a fair hearing.
I think this is pretty likely, but with the data encrypted by the time
the supervisor mode sees it.
If the data is in the clear, it would undermine the security guarantees!
Look at my online poker game - if the dealer can tap into the data going
out, he will learn what everyone else's hands are.  Look at the anonymous
network - an eavesdropper can learn where all the data is and how it
is flowing.  The data must not be made available in the clear anywhere
the user can get at it, to provide the proper security.
Well, he *does* know at least the address the data is going to.  There's
no way to hide that (short of anonymous message forwarding).
There are some applications which will still work if all the users can
*see* all of the data, but just not modify it.  Maybe my digital cash
example would fall into that category.  You can see how much you're
spending, but you can't manipulate your wallet.  But there are many
others, such as those above, where being able to hide even information
disclosure from network participants adds tremendous power.
You remember the Eternity network, how one concept had files being
shared across multiple nodes such that no one knew which files were on
his own computer.  That was crucially important for non-repudiation and
censorship-resistance.  This was done with cryptography, but the point
is the same: hiding information from network participants can greatly
increase security.  With TCPA/Palladium you can get some of the same
security properties with much simpler ode (with admittedly lower levels
of security until hardware improves).
Skipping down...
I am assuming that you are attesting to the remote system, and you only
can control your local one.  You want to get something from the remote,
and it will only give it if you are running "clean" on real hardware.
So you can't virtualize and still attest, since ultimately you don't
have a TPM endorsement key (or Palladium equivalent) with a nice TPME
endorsement certificate issued on it.
Of course if you have control of the server machine, you can ignore
attestation.  But that just says that the operator of the remote machine
can choose for himself which apps to run.  He can run an app that checks
remote client integrity or he can run one which doesn't care.
I'm not sure I follow this, but it sounds like you are talking about
manipulating the server machine doing the checks, while in most cases
you can only manipulate the client machine making the request.

@_date: 2002-08-07 17:35:14
@_author: AARG! Anonymous 
@_subject: Palladiated Huber: "Software's Cash Register", Forbes, 
I don't know why it should.  Wave, the company profiled here, is said
to be actively involved with both TCPA and Palladium.  They're just a
little behind schedule, is all.
It's not like the Cypherpunk Revolution is progressing according to plan,
either, is it?

@_date: 2002-08-08 16:55:45
@_author: AARG!Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
Matt Crawford replied:
It's likely that only a limited number of compiler configurations would
be in common use, and signatures on the executables produced by each of
those could be provided.  Then all the app writer has to do is to tell
people, get compiler version so-and-so and compile with that, and your
object will match the hash my app looks for.

@_date: 2002-08-09 10:05:15
@_author: AARG!Anonymous 
@_subject: Thanks, Lucky, for helping to kill gnutella 
An article on Salon this morning (also being discussed on slashdot),
discusses how the file-trading network Gnutella is being threatened by
misbehaving clients.  In response, the developers are looking at limiting
the network to only authorized clients:
They intend to do this using digital signatures, and there is precedent
for this in past situations where there have been problems:
Not discussed in the article is the technical question of how this can
possibly work.  If you issue a digital certificate on some Gnutella
client, what stops a different client, an unauthorized client, from
pretending to be the legitimate one?  This is especially acute if the
authorized client is open source, as then anyone can see the cert,
see exactly what the client does with it, and merely copy that behavior.
If only there were a technology in which clients could verify and yes,
even trust, each other remotely.  Some way in which a digital certificate
on a program could actually be verified, perhaps by some kind of remote,
trusted hardware device.  This way you could know that a remote system was
actually running a well-behaved client before admitting it to the net.
This would protect Gnutella from not only the kind of opportunistic
misbehavior seen today, but the future floods, attacks and DOSing which
will be launched in earnest once the content companies get serious about
taking this network down.
If only...  Luckily the cypherpunks are doing all they can to make sure
that no such technology ever exists.  They will protect us from being able
to extend trust across the network.  They will make sure that any open
network like Gnutella must forever face the challenge of rogue clients.
They will make sure that open source systems are especially vulnerable
to rogues, helping to drive these projects into closed source form.
Be sure and send a note to the Gnutella people reminding them of all
you're doing for them, okay, Lucky?

@_date: 2002-08-09 16:10:08
@_author: AARG! Anonymous 
@_subject: No subject 
Adam Back writes a very thorough analysis of possible consequences of the
amazing power of the TCPA/Palladium model.  He is clearly beginning to
"get it" as far as what this is capable of.  There is far more to this
technology than simple DRM applications.  In fact Adam has a great idea
for how this could finally enable selling idle CPU cycles while protecting
crucial and sensitive business data.  By itself this could be a "killer
app" for TCPA/Palladium.  And once more people start thinking about how to
exploit the potential, there will be no end to the possible applications.
Of course his analysis is spoiled by an underlying paranoia.  So let me
ask just one question.  How exactly is subversion of the TPM a greater
threat than subversion of your PC hardware today?  How do you know that
Intel or AMD don't already have back doors in their processors that
the NSA and other parties can exploit?  Or that Microsoft doesn't have
similar backdoors in its OS?  And similarly for all the other software
and hardware components that make up a PC today?
In other words, is this really a new threat?  Or are you unfairly blaming
TCPA for a problem which has always existed and always will exist?

@_date: 2002-08-09 17:15:19
@_author: AARG! Anonymous 
@_subject: TCPA/Palladium -- likely future implications 
I want to follow up on Adam's message because, to be honest, I missed
his point before.  I thought he was bringing up the old claim that these
systems would "give the TCPA root" on your computer.
Instead, Adam is making a new point, which is a good one, but to
understand it you need a true picture of TCPA rather than the false one
which so many cypherpunks have been promoting.  Earlier Adam offered a
proposed definition of TCPA/Palladium's function and purpose:
IMO this is total bullshit, political rhetoric that is content-free
compared to the one I offered:
: Allow computers separated on the internet to cooperate and share data
: and computations such that no one can get access to the data outside
: the limitations and rules imposed by the applications.
It seems to me that my definition is far more useful and appropriate in
really understanding what TCPA/Palladium are all about.  Adam, what do
you think?
If we stick to my definition, you will come to understand that the purpose
of TCPA is to allow application writers to create closed spheres of trust,
where the application sets the rules for how the data is handled.  It's
not just DRM, it's Napster and banking and a myriad other applications,
each of which can control its own sensitive data such that no one can
break the rules.
At least, that's the theory.  But Adam points out a weak spot.  Ultimately
applications trust each other because they know that the remote systems
can't be virtualized.  The apps are running on real hardware which has
real protections.  But applications know this because the hardware has
a built-in key which carries a certificate from the manufacturer, who
is called the TPME in TCPA.  As the applications all join hands across
the net, each one shows his cert (in effect) and all know that they are
running on legitimate hardware.
So the weak spot is that anyone who has the TPME key can run a virtualized
TCPA, and no one will be the wiser.  With the TPME key they can create
their own certificate that shows that they have legitimate hardware,
when they actually don't.  Ultimately this lets them run a rogue client
that totally cheats, disobeys all the restrictions, shows the user all
of the data which is supposed to be secret, and no one can tell.
Furthermore, if people did somehow become suspicious about one particular
machine, with access to the TPME key the eavesdroppers can just create
a new virtual TPM and start the fraud all over again.
It's analogous to how someone with Verisign's key could masquerade as
any secure web site they wanted.  But it's worse because TCPA is almost
infinitely more powerful than PKI, so there is going to be much more
temptation to use it and to rely on it.
Of course, this will be inherently somewhat self-limiting as people learn
more about it, and realize that the security provided by TCPA/Palladium,
no matter how good the hardware becomes, will always be limited to
the political factors that guard control of the TPME keys.  (I say
keys because likely more than one company will manufacture TPM's.
Also in TCPA there are two other certifiers: one who certifies the
motherboard and computer design, and the other who certifies that the
board was constructed according to the certified design.  The NSA would
probably have to get all 3 keys, but this wouldn't be that much harder
than getting just one.  And if there are multiple manufacturers then
only 1 key from each of the 3 categories is needed.)
To protect against this, Adam offers various solutions.  One is to do
crypto inside the TCPA boundary.  But that's pointless, because if the
crypto worked, you probably wouldn't need TCPA.  Realistically most of the
TCPA applications can't be cryptographically protected.  "Computing with
encrypted instances" is a fantasy.  That's why we don't have all those
secure applications already.
Another is to use a web of trust to replace or add to the TPME certs.
Here's a hint.  Webs of trust don't work.  Either they require strong
connections, in which case they are too sparse, or they allow weak
connections, in which case they are meaningless and anyone can get in.
I have a couple of suggestions.  One early application for TCPA is in
closed corporate networks.  In that case the company usually buys all
the computers and prepares them before giving them to the employees.
At that time, the company could read out the TPM public key and sign
it with the corporate key.  Then they could use that cert rather than
the TPME cert.  This would protect the company's sensitive data against
eavesdroppers who manage to virtualize their hardware.
For the larger public network, the first thing I would suggest is that
the TPME key ought to be in hardware, so it can't be given out freely.
Of course the NSA could still come in and get their virtual-TPM keys
signed one at a time.  So the next step is that the device holding the
TPME key must be managed in a high security environment.  This may be
difficult, given the need to sign potentially thousands of TPM keys a
day, but I think it has to be done.  I want to see watchdogs from the
EFF and a lot of other groups sitting there 24 hours a day watching over
the device.  Remember how Clipper was going to use a vault, split keys
and all this elaborate precautions?  We need at least that much security.
Think about it: this one innocuous little box holding the TPME key could
ultimately be the root of trust for the entire world.  IMO we should
spare no expense in guarding it and making sure it is used properly.
With enough different interest groups keeping watch, we should be able
to keep it from being used for anything other than its defined purpose.

@_date: 2002-08-09 19:30:09
@_author: AARG!Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
Re the debate over whether compilers reliably produce identical object
(executable) files:
The measurement and hashing in TCPA/Palladium will probably not be done
on the file itself, but on the executable content that is loaded into
memory.  For Palladium it is just the part of the program called the
"trusted agent".  So file headers with dates, compiler version numbers,
etc., will not be part of the data which is hashed.
The only thing that would really break the hash would be changes to the
compiler code generator that cause it to create different executable
output for the same input.  This might happen between versions, but
probably most widely used compilers are relatively stable in that
respect these days.  Specifying the compiler version and build flags
should provide good reliability for having the executable content hash
the same way for everyone.

@_date: 2002-08-09 20:25:40
@_author: AARG! Anonymous 
@_subject: Thanks, Lucky, for helping to kill gnutella 
Several people have objected to my point about the anti-TCPA efforts of
Lucky and others causing harm to P2P applications like Gnutella.
Bran Cohen agrees:
I will just point out that it was not my idea, but rather that Salon
said that the Gnutella developers were considering moving to authorized
clients.  According to Eric, those developers are "fundamentally stupid."
According to Bram, the Gnutella developers don't understand their
own protocol, and they are supporting an idea which will not help.
Apparently their belief that clients like Qtrax are hurting the system
is totally wrong, and keeping such clients off the system won't help.
I can't help believing the Gnutella developers know more about their
own system than Bram and Eric do.  If they disagree, their argument is
not with me, but with the Gnutella people.  Please take it there.
Ant chimes in:
Pete Chown echoes:
As far as Freenet and MojoNation, we all know that the latter shut down,
probably in part because the attempted traffic-control mechanisms made
the whole network so unwieldy that it never worked.  At least in part
this was also due to malicious clients, according to the analysis at
  And Freenet has been
rendered inoperative in recent months by floods.  No one knows whether
they are fundamental protocol failings, or the result of selfish client
strategies, or calculated attacks by the RIAA and company.  Both of these
are object lessons in the difficulties of successful P2P networking in
the face of arbitrary client attacks.
Some people took issue with the personal nature of my criticism:
Right, as if my normal style has been so effective.  Not one person has
given me the least support in my efforts to explain the truth about TCPA
and Palladium.
Anyway, maybe I was too personal in singling out Lucky.  He is far from
the only person who has opposed TCPA.
But Lucky, in his slides at  claims that TCPA's
designers had as one of their objectives "To meet the operational needs
of law enforcement and intelligence services" (slide 2); and to give
privileged access to user's computers to "TCPA members only" (slide 3);
that TCPA has an OS downloading a "serial number revocation list" (SNRL)
which he has provided no evidence for whatsoever (slide 14); that it
loads an "initial list of undesirable applications" which is apparently
another of his fabrications (slide 15); that TCPA applications on startup
load both a serial number revocation list but also a document revocation
list, again a completely unsubstantiated claim (slide 19); that apps then
further verify that spyware is running, another fabrication (slide 20).
He then implies that the DMCA applies to reverse engineering when
it has an explicit exemption for that (slide 23); that the maximum
possible sentence of 5 years is always applied (slide 24); that TCPA is
intended to: defeat the GPL, enable information invalidation, facilitate
intelligence collection, meet law enforcement needs, and more (slide 27);
that only signed code will boot in TCPA, contrary to the facts (slide 28).
He provides more made-up details about the mythical DRL (slide 31);
more imaginary details about document IDs, information monitoring and
invalidation to support law enforcement and intelligence needs, none of
which has anything to do with TCPA (slide 32-33).  As apparent support for
these he provides an out-of-context quote[1] from a Palladium manager,
who if you read the whole article was describing their determination to
keep the system open (slide 34).
He repeats the unfounded charge that the Hollings bill would mandate TCPA,
when there's nothing in the bill that says such a thing (slide 35);
and he exaggerates the penalties in that bill by quoting the maximum
limits as if they are the default (slide 36).
Lucky can provide all this misinformation, all under the pretence,
mind you, that this *is* TCPA.  He was educating the audience, mostly
people who were completely unfamiliar with the system other than some
vague rumors.  And this is what he presents, a tissue of lies and
fabrications and unfounded sensationalism.
Don't forget, TCPA and Palladium were designed by real people.  In making
these charges, Lucky is not just talking about a standard, he is talking
about its authors.  He is saying that those people were attempting to
serve intelligence needs, to make sure that people had to run spyware,
to close down the system so it could keep "undesirable" applications off.
He is accusing the designers of far worse than anything I have said
about him.  He is basically saying that they are striving to bring about
a technological police state.
And yet, no one (other than me, of course) dared to criticize Lucky for
these claims.  He can say whatever he wants, be as outrageous as he wants,
and no one says a thing.  I don't know whether everyone agrees with him,
or is simply unwilling to risk criticism by departing from the groupthink
which is so universal around here.
I asked Eric Murray, who knows something about TCPA, what he thought
of some of the more ridiculous claims in Ross Anderson's FAQ (like the
SNRL), and he didn't respond.  I believe it is because he is unwilling
to publicly take a position in opposition to such a famous and respected
But anyway, maybe I was too personal in criticizing Lucky.  Tell you what.
I'll apologize to Lucky as soon as he apologizes to the designers of
TCPA for the fabrications in his slide show.  Deal?

@_date: 2002-08-10 11:40:14
@_author: AARG! Anonymous 
@_subject: responding to claims about TCPA 
John Gilmore replied:
Maybe, but he could reply just based on public information.  Despite this
he was unable or unwilling to challenge Ross Anderson.
I don't agree with this distinction.  If I use a smart card chip that
has a private key on it that won't come off, is that protecting me from
third parties, or vice versa?  If I run a TCPA-enhanced Gnutella that
keeps the RIAA from participating and easily finding out who is running
supernodes (see  for
the latest crackdown), I benefit, even though the system technically is
protecting the data from me.
I wrote earlier that if people were honest, trusted computing would not
be necessary, because they would keep their promises.  Trusted computing
allows people to prove to remote users that they will behave honestly.
How does that fit into your dichotomy?  Society has evolved a myriad
mechanisms to allow people to give strong evidence that they will keep
their word; without them, trade and commerce would be impossible.  By your
logic, these protect third parties from you, and hence should be rejected.
You would discard the economic foundation for our entire world.
David Grawrock of Intel has an interesting slide presentation on
TCPA at His slide 3 makes a good point: "All 5 members had very different ideas
of what should and should not be added."  It's possible that some of
the differences in perspective and direction on TCPA are due to the
several participants wanting to move in different ways.  Some may have
been strictly focused on DRM; others may have had a more expansive
vision of how trust can benefit all kinds of distributed applications.
So it's not clear that you can speak of the "real goal" of TCPA, when
there are all these different groups with different ideas.
Nonsense.  The web is ubiquitous, but is not a monopoly.
That same language is in the Credible Interoperability document presently
on the web site at
So I don't think there is necessarily any kind of a cover-up here.
Yes, DRM can clearly benefit from TCPA/Palladium.  And you might be
right that they are downplaying that now.  But the reason could be
that people have focused too much on it as the only purpose for TCPA,
just as you have done here.  So they are trying to play up the other
possibilities so as to get some balance in the discussion.
You are reading an awful lot into this one word "transaction".  That
doesn't necessarily mean buying digital content.  In the abstract sense
"transaction" is sometimes used to refer to any exchange of information in
a protocol.  Even if we do stick to its commercial meaning, it can mean
a B2B exchange or any of a wide range of other e-commerce activities.
It's not specific to DRM by any means.
I agree that the documentation is a problem, but IMO it probably reflects
lack of resources rather than obfuscation.  I believe that TCPA has many
more applications than you and other critics are giving it credit for,
and that a good, clear explanation of what it could do would actually
gain it support.  Do a blog search at daypop.com to see what people are
really thinking about TCPA.  They read Ross Anderson's TCPA FAQ and take
it for gospel.  They believe TCPA has serial number revocations and all
these other features that are not described in any documents I have seen.
A good clear TCPA description could only improve its reputation, which
certainly can't go any lower than it is.
I agree in principle, but I am appalled that you believe that Lucky in
particular is heading in the right direction.  Adam on the other hand
has at least begun to study TCPA and was asking good questions about
Palladium before Peter Biddle flew the coop.  Will this document say
that TCPA is designed to support intelligence agency access to computers?
to kill free software?  and other such claims from Lucky's presentation?
If so, you will only hurt your cause.  On the other hand, if you do
come up with factual and unbiased information showing both good and bad
aspects of TCPA, as I think Adam has come close to doing a few times,
then it could be a helpful document.
Conclusions should be based on technology.  TCPA can be rightly
criticized for weak protections of privacy, for ultimately depending on
the security of a few central keys and of possibly-weak hardware, and on
other technical grounds.  But you should not criticize it for supporting
DRM, or for making reverse engineering more difficult, because people
are under no obligation to give their creative works away for free,
or to make it easy for other people to copy their software.  Leave your
values at home and just present the facts.
No one has made any such allegation, although presumably it happens to
be true.  The point in contention is whether TCPA has DRLs!  Lucky has
claimed this, and Ross claimed the related serial number revocation
list, SNRL.  Both of them have linked this technology to TCPA/Palladium.
Yet as Ross admitted in
SNRL's do not need TCPA!
In fact, you are perfectly correct that Microsoft architectures would
make it easy at any time to implement DRL's or SNRL's.  They could do
that tomorrow!  They don't need TCPA.  So why blame TCPA for this feature?
TCPA is a technology.  You can't take every bad thing Microsoft ever
will do and say that TCPA is at fault.
I don't even see that TCPA would particularly help with a SNRL, except
insofar as TCPA can generally strengthen security in all respects.
But remote attestation and sealing, the core TCPA technologies, don't
have anything to do with SNRLs.
The association of TCPA with SNRLs is a perfect example of the bias and
sensationalism which has surrounded the critical appraisals of TCPA.
I fully support John's call for a fair and accurate evaluation of this
technology by security professionals.  But IMO people like Ross Anderson
and Lucky Green have disqualified themselves by virtue of their wild and
inaccurate public claims.  Anyone who says that TCPA has SNRLs is making
a political statement, not a technical one.  For a credible evaluation,
you need people who have no track record of bias with regard to the

@_date: 2002-08-10 15:15:07
@_author: AARG!Anonymous 
@_subject: Seth on TCPA at Defcon/Usenix 
Seth Schoen of the EFF has a good blog entry about Palladium and TCPA
at   He attended Lucky's
presentation at DEF CON and also sat on the TCPA/Palladium panel at
the USENIX Security Symposium.
Seth has a very balanced perspective on these issues compared to most
people in the community.  It makes me proud to be an EFF supporter
(in fact I happen to be wearing my EFF T-shirt right now).
His description of how the Document Revocation List could work is
interesting as well.  Basically you would have to connect to a server
every time you wanted to read a document, in order to download a key
to unlock it.  Then if "someone" decided that the document needed
to un-exist, they would arrange for the server no longer to download
that key, and the document would effectively be deleted, everywhere.
I think this clearly would not be a feature that most people would accept
as an enforced property of their word processor.  You'd be unable to
read things unless you were online, for one thing.  And any document you
were relying on might be yanked away from you with no warning.  Such a
system would be so crippled that if Microsoft really did this for Word,
sales of "vi" would go through the roof.
It reminds me of an even better way for a word processor company to make
money: just scramble all your documents, then demand ONE MILLION DOLLARS
for the keys to decrypt them.  The money must be sent to a numbered
Swiss account, and the software checks with a server to find out when
the money has arrived.  Some of the proposals for what companies will
do with Palladium seem about as plausible as this one.
Seth draws an analogy with Acrobat, where the paying customers are
actually the publishers, the reader being given away for free.  So Adobe
does have incentives to put in a lot of DRM features that let authors
control publication and distribution.
But he doesn't follow his reasoning to its logical conclusion when dealing
with Microsoft Word.  That program is sold to end users - people who
create their own documents for the use of themselves and their associates.
The paying customers of Microsoft Word are exactly the ones who would
be screwed over royally by Seth's scheme.  So if we "follow the money"
as Seth in effect recommends, it becomes even more obvious that Microsoft
would never force Word users to be burdened with a DRL feature.
And furthermore, Seth's scheme doesn't rely on TCPA/Palladium.  At the
risk of aiding the fearmongers, I will explain that TCPA technology
actually allows for a much easier implementation, just as it does in so
many other areas.  There is no need for the server to download a key;
it only has to download an updated DRL, and the Word client software
could be trusted to delete anything that was revoked.  But the point
is, Seth's scheme would work just as well today, without TCPA existing.
As I quoted Ross Anderson saying earlier with regard to "serial number
revocation lists", these features don't need TCPA technology.
So while I have some quibbles with Seth's analysis, on the whole it is
the most balanced that I have seen from someone who has no connection
with the designers (other than my own writing, of course).  A personal
gripe is that he referred to Lucky's "critics", plural, when I feel
all alone out here.  I guess I'll have to start using the royal "we".
But he redeemed himself by taking mild exception to Lucky's slide show,
which is a lot farther than anyone else has been willing to go in public.

@_date: 2002-08-12 10:55:19
@_author: AARG! Anonymous 
@_subject: Palladium: technical limits and implications 
I don't think this works.  According to Peter Biddle, the TOR can be
launched even days after the OS boots.  It does not underly the ordinary
user mode apps and the supervisor mode system call handlers and device
        +---------------+------------+   trusted-agent | user mode  |      space      | app space  |      (code      +------------+   compartment)  | supervisor |                 | mode / OS  |  +---+   +---------------+------------+
+---+   +---------------+
This is more how I would see it.  The SCP is more like a peripheral
device, a crypto co-processor, that is managed by the TOR.  Earlier you
quoted Seth's blog:
as justification for putting the nub (TOR) under the OS.  But I think in
this context "more privilege" could just refer to the fact that it is in
the secure memory, which is only accessed by this ring--1 or ring-0 or
whatever you want to call it.  It doesn't follow that the nub has anything
to do with the OS proper.  If the OS can run fine without it, as I think
you agreed, then why would the entire architecture have to reorient itself
once the TOR is launched? In other words, isn't my version simpler, as it adjoins the column at
the left to the pre-existing column at the right, when the TOR launches,
days after boot?  Doesn't it require less instantaneous, on-the-fly,
reconfiguration of the entire structure of the Windows OS at the moment
of TOR launch?  And what, if anything, does my version fail to accomplish
that we know that Palladium can do?
I had thought the hardware might also produce the metrics for trusted
agents, but you could be right that it is the TOR which does so.
That would be consistent with the "incremental extension of trust"
philosophy which many of these systems seem to follow.
No, that doesn't make sense.  Why would the TOR need to compute a metric
of the OS?  Peter has said that Palladium does not give information about
other apps running on your machine:
: Note that in Pd no one but the user can find out the totality of what SW is
: running except for the nub (aka TOR, or trusted operating root) and any
: required trusted services. So a service could say "I will only communicate
: with this app" and it will know that the app is what it says it is and
: hasn't been perverted. The service cannot say "I won't communicate with this
: app if this other app is running" because it has no way of knowing for sure
: if the other app isn't running.
Nothing Peter or anyone else has said indicates that this is a property of
Palladium, as far as I can remember.
No, I think it is there to prevent debuggers and supervisor-mode drivers
from manipulating secure code.  TCPA is more of a whole-machine spec
dealing with booting an OS, so it doesn't have to deal with the question
of running secure code next to insecure code.

@_date: 2002-08-12 11:15:10
@_author: AARG! Anonymous 
@_subject: dangers of TCPA/palladium 
Ben Laurie replied:
David Wagner commented:
I don't believe that is an accurate paraphrase of what Mike Rosing said.
He said the purpose (not effect) was to remove (not reduce) his control,
and make the platform trusted to one entity (not "for the benefit of
others").  Unless you want to defend the notion that the purpose of TCPA
is to *remove* user control of his machine, and make it trusted to only
*one other entity* (rather than a general capability for remote trust),
then I think you should accept that what he said was wrong.
And Mike said more than this.  He said that if he could install his own
key into the TPM that would make it a very useful tool.  This is wrong;
it would completely undermine the trust guarantees of TCPA, make it
impossible for remote observers to draw any useful conclusions about the
state of the system, and render the whole thing useless.  He also talked
about how this could be used to make systems "phone home" at boot time.
But TCPA has nothing to do with any such functionality as this.
In contrast, Ben Laurie's characterization of TCPA is 100% factual and
accurate.  Do you at least agree with that much, even if you disagree
with my criticism of Mike Rosing's comments?

@_date: 2002-08-12 11:15:17
@_author: AARG!Anonymous 
@_subject: responding to claims about TCPA 
I believe you did, because if you look at what I actually wrote, I did not
say that "bringing up the topic of DRLs is an indication of bias":
My core claim is the last sentence.  It's one thing to say, as you
are, that TCPA could make applications implement SNRLs more securely.
I believe that is true, and if this statement is presented in the context
of "dangers of TCPA" or something similar, it would be appropriate.
But even then, for a fair analysis, it should make clear that SNRLs can
be done without TCPA, and it should go into some detail about just how
much more effective a SNRL system would be with TCPA.  (I will write more
about this in responding to Joseph Ashwood.)
And to be truly unbiased, it should also talk about good uses of TCPA.
If you look at Ross Anderson's TCPA FAQ at
 he writes (question 4):
: When you boot up your PC, Fritz takes charge. He checks that the boot
: ROM is as expected, executes it, measures the state of the machine;
: then checks the first part of the operating system, loads and executes
: it, checks the state of the machine; and so on. The trust boundary, of
: hardware and software considered to be known and verified, is steadily
: expanded. A table is maintained of the hardware (audio card, video card
: etc) and the software (O/S, drivers, etc); Fritz checks that the hardware
: components are on the TCPA approved list, that the software components
: have been signed, and that none of them has a serial number that has
: been revoked.
He is not saying that TCPA could make SNRLs more effective.  He says
that "Fritz checks... that none of [the software components] has a
serial number that has been revoked."  He is flatly stating that the
TPM chip checks a serial number revocation list.  That is both biased
and factually untrue.
Ross's whole FAQ is incredibly biased against TCPA.  I don't see how
anyone can fail to see that.  If it were titled "FAQ about Dangers of
TCPA" at least people would be warned that they were getting a one-sided
presentation.  But it is positively shameful for a respected security
researcher like Ross Anderson to pretend that this document is giving
an unbiased and fair description.
I would be grateful if someone who disagrees with me, who thinks that
Ross's FAQ is fair and even-handed, would speak up.  It amazes me that
people can see things so differently.
And Lucky's slide presentation,  is if anything
even worse.  I already wrote about this in detail so I won't belabor
the point.  Again, I would be very curious to hear from someone who
thinks that his presentation was unbiased.

@_date: 2002-08-12 15:50:48
@_author: AARG! Anonymous 
@_subject: Seth on TCPA at Defcon/Usenix 
In discussing how TCPA would help enforce a document revocation list
(DRL) Joseph Ashwood contrasted the situation with and without TCPA
style hardware, below.  I just want to point out that his analysis of
the hardware vs software situation says nothing about DRL's specifically;
in fact it doesn't even mention them.
His analysis actually applies to a wide range of security features,
such as the examples given earlier: secure games, improved P2P,
distributed computing as Adam Back suggested, DRM of course, etc..
TCPA is a potentially very powerful security enhancement, so it does
make sense that it can strengthen all of these things, and DRLs as well.
But I don't see that it is fair to therefore link TCPA specifically with
DRLs, when there are any number of other security capabilities that are
also strengthened by TCPA.
It's not always as easy as you make it sound here.  Adam Back wrote
Saturday about the interesting history of the giFT project, which
reverse-engineered the Kazaa file-sharing protocol.  That was a terrific
effort that required considerable cryptographic know-how as well as
supreme software reverse engineering skills.  But then Kazaa changed the
protocol, and giFT never managed to become compatible with the new one.
I'm not sure whether it was lack of interest or just too difficult,
but in any case the project failed (as far as creating an open Kazaa
compatible client).
It is clear that software hacking is far from "almost trivial" and you
can't assume that every software-security feature can and will be broken.
Furthermore, even when there is a break, it won't be available to
everyone.  Ordinary people aren't clued in to the hacker community
and don't download all the latest patches and hacks to disable
security features in their software.  Likewise for business customers.
In practice, if Microsoft wanted to implement a global, facist DRL,
while some people might be able to patch around it, probably 95%+ of
ordinary users would be stuck with it.
Therefore a DRL in software would be far from useless, and if there
truly was a strong commercial need for such a solution then chances are
it would be there today.
I might mention BTW that for email there is such a product,
disappearingink.com, which works along the lines Seth suggested, I
believe.  It encrypts email with a centralized key, and when that email
needs to be deleted, the key is destroyed.  This allows corporations to
implement a "document retention policy" (which is of course a euphemism
for a document destruction policy) to help reduce their vulnerability to
lawsuits and fishing expeditions.  I don't recall anyone getting up in
arms over the disappearingink.com technology or claiming that it was a
threat, in the same way that DRLs and SNRLs are being presented in the
context of Palladium.
First, as far as this last point, you acknowledge that if they can't
tell where it came from, your hacked hardware can be an ongoing source of
un-DRL'd documents.  But watermarking technology so far has been largely
a huge failure, so it is likely that someone clueful enough to hack his
TPM could also strip away any identifying markings.
Second, given that you do hack the hardware, you may not actually need
to do that much in terms of protocol hacking.  If you can watch the data
going to and from the TPM you can extract keys directly, and that may
be enough to let you decrypt the "sealed" data.  (The TPM does only
public key operations; the symmetric crypto is all done by the app.
I don't know if Palladium will work that way or not.)
Third, if a document is "liberated" via this kind of hack, it can
then be distributed everywhere, outside the "secure trust perimeter"
enforced by TCPA/Palladium.  We are still in a "break once read anywhere"
situation with documents, and any attempt to make one disappear is not
going to be very successful, even with TCPA in existence.
In short, while TCPA could increase the effectiveness of global DRLs,
they wouldn't be *that* much more effective.  Most users will neither
hack their software nor their hardware, so the hardware doesn't make
any difference for them.  Hackers will be able to liberate documents
completely from DRL controls, whether they use hardware or software
to do it.  The only difference is that there will be fewer hackers,
if hardware is used, because it is more difficult.  Depending on the
rate at which important documents go on DRLs, that may not make any
difference at all.
I agree that providing an option to store documents in restricted form
could be a desirable feature for businesses.  And having the ability
to delete documents on a company-wide basis, a la disappearingink.com,
could make sense as well.  I don't know that there is a huge market for
this capability, or I suspect we'd see it already.  But it does make
sense as an auxiliary part of a business document product.
But to me this points to a localized DRL, part of a document-management
library that is used solely for documents within the company.  The company
would want to control the administration of its documents.  I don't
see this leading to the kind of centralized, global system which I
think opponents of TCPA are attempting to invoke when they talk about it
allowing DRLs, and which is the kind of thing I was talking about above.

@_date: 2002-08-12 23:55:14
@_author: AARG! Anonymous 
@_subject: Another application for trusted computing 
I thought of another interesting application for trusted computing
systems: mobile agents.  These are pieces of software which get
transferred from computer to computer, running on each system,
communicating with the local system and other visiting agents,
before migrating elsewhere.
This was a hot technology from a couple of years ago, but it never
really went anywhere (so to speak).  Part of the reason was that there
wasn't that much functionality for agents which couldn't be done better
in other ways.  But a big part of it was problems with security.
One issue was protecting the host from malicious agents, and much work
was done in that direction.  This was one of the early selling points
of Java, and other sandbox systems were developed as well.  Likewise the
E language is designed to solve this problem.
But the much harder problem was protecting the agent from malicious hosts.
Once an agent transferred into a host machine, it was essentially at
the mercy of that system.  The host could lie to the agent, and even
manipulate its memory and program, to make it do anything it desired.
Without the ability to maintain its own integrity, the agent was
relatively useless in many ecommerce applications.
Various techniques were suggested to partially address this, such as
splitting the agent functionality among multiple agents which would run
on different machines, or using cryptographic methods for computing
with encrypted instances and the like.  But these were inherently
so inefficient that any advantages mobile agents might have had were
eliminated compared to such things as web services.
Ideally you'd like your agent to truly be autonomous, with its own data,
its own code, all protected from the host and other agents.  It could even
carry a store of electronic cash which it could use to fund its activities
on the host machine.  It could remember its interactions on earlier
machines in an uncorruptable way.  And you'd like it to run efficiently,
without the enormous overheads of the cryptographic techniques.
Superficially such a capability seems impossible.  Agents can't have that
kind of autonomy.  But trusted computing can change this.  It can give
agents good protection as they move through the net.
Imagine that host computers run a special program, an Agent Virtual
Machine or AVM.  This program runs the agents in their object language,
and it respects each agent's code and data.  It does not corrupt the
agents, it does not manipulate or copy their memory without authorization
from the agent itself.  It allows the agents to act in the autonomous fashion
we would desire.
Without trusted computing, the problem of course is that there is no
way to be sure that a potential host is running a legitimate version of
the AVM.  It could have a hacked AVM that would allow it to steal cash
from the agents, change their memory, and worse.
This is where trusted computing can solve the problem.  It allows
agents to verify that a remote system is running a legitimate AVM before
transferring over.  Hacked AVMs will have a different hash and this will
be detected via the trusted computing mechanisms.  Knowing that the remote
machine is running a correct implementation of the AVM allows the agent
to move about without being molested.
In this way, trusted computing can solve one of the biggest problems
with effective use of mobile agents.  Trusted computing finally allows
mobile agent technology to work right.
This is just one of what I expect to be thousands of applications which
can take advantage of the trusted computing concept.  Once you have a
whole world of people trying to think creatively about how to use this
technology, rather than just a handful, there will be an explosion of
new applications which today we would never dream are possible.

@_date: 2002-08-13 00:05:19
@_author: AARG! Anonymous 
@_subject: TCPA and Open Source 
One of the many charges which has been tossed at TCPA is that it will
harm free software.  Here is what Ross Anderson writes in the TCPA FAQ
at  (question 18):
First we have to deal with this certificate business.  Most readers
probably assume that you need this cert to use the TCPA system, and
even that you would not be able to boot into this Linux OS without such
a cert.  This is part of the longstanding claim that TCPA will only boot
signed code.
I have refuted this claim many times, and asked for those who disagree to
point to where in the spec it says this, without anyone doing so.  I can
only hope that interested readers may be beginning to believe my claim
since if it were false, somebody would have pointed to chapter and verse
in the TCPA spec just to shut me up about it if for no better reason.
However, Ross is actually right that TCPA does support a concept for
a certificate that signs code.  It's called a Validation Certificate.
The system can hold a number of these VC's, which represent the "presumed
correct" results of the measurement (hashing) process on various software
and hardware components.  In the case of OS code, then, there could be
VC's representing specific OS's which could boot.
The point is that while this is a form of signed code, it's not something
which gives the TPM control over what OS can boot.  Instead, the VCs
are used to report to third party challengers (on remote systems) what
the system configuration of this system is "supposed" to be, along with
what it actually is.  It's up to the remote challenger to decide if he
trusts the issuer of the VC, and if so, he will want to see that the
actual measurement (i.e. the hash of the OS) matches the value in the VC.
So what Ross says above could potentially be true, if and when TCPA
compliant operating systems begin to be developed.  Assuming that there
will be some consortium which will issue VC's for operating systems,
and assuming that third parties will typically trust that consortium and
only that one, then you will need to get a VC from that group in order
to effectively participate in the TCPA network.
This doesn't mean that your PC won't boot the OS without such a cert; it
just means that if most people choose to trust the cert issuer, then you
will need to get a cert from them to get other people to trust your OS.
It's much like the power Verisign has today with X.509; most people's
software trusts certs from Verisign, so in practice you pretty much need
to get a cert from them to participate in the X.509 PKI.
So does this mean that Ross is right, that free software is doomed under
TCPA?  No, for several reasons, not least being a big mistake he makes:
The big mistake is the belief that the cert is specific to the "Fritz"
chip (Ross's cute name for the TPM).
Actually the VC data structure is not specific to any one PC.  It is
intentionally designed not to have any identifying information in it
that will represent a particular system.  This is because the VC cert
has to be shown to remote third parties in order to get them to trust the
local system, and TCPA tries very hard to protect user privacy (believe
it or not!).  If the VC had computer-identifying information in it, then
it would be a linkable identifier for all TCPA interactions on the net,
which would defeat all of the work TCPA does with Privacy CAs and whatnot
to try to protect user privacy.  If you understand this, you will see
that the whole TCPA concept requires VC's not to be machine specific.
People always complain when I point to the spec, as if the use of facts
were somehow unfair in this dispute.  But if you are willing, you can look
at section 9.5.4 of which is the data structure for the validation certificate.  It is an
X.509 attribute certificate, which is a type of cert that would normally
be expected to point back at the machine-specific endorsement cert.
But in this case they have altered the Holder field, which normally
performs this function, to instead hold the measurement result that the
certificate asserts is correct (i.e. in this context, the hash of the OS).
As a result, no data in the VC is machine specific.
The bottom line is that VCs are not machine specific and are designed
not to be.  Therefore if HP or anyone else gets a cert on their version
of Linux, anyone will be able to use that same cert and prove to remote
users that they are running a TCPA compliant OS.  This contradicts one
of Ross's main claims above.
In fact, with Microsoft now apparently commited to Palladium, it appears
that if TCPA survives at all, it may be solely as a Linux phenomenon!
Reports are that both HP and IBM are working on Linux compatible
versions of TCPA.  Ironically, not only is Ross's claim wrong about TCPA
undermining open source, it could conceivably turn out that TCPA will
be completely reliant on open source operating systems.
At the same time I think it is fair to say that there is an inherent
conflict between the usual development techniques for open source code,
and the requirements for security certification.  Once an OS has been
certified by some respected body as being secure and trusted along TCPA
lines, then changes to the code will invalidate the certification.
This makes sense on two levels.  Logically, if the code has changed
from what was inspected and certified, we can no longer have the same
guarantees that the security properties have been preserved.  And
in terms of syntax, with the TCPA Validation Certificate holding a
hash of the certified code, making any change will change the hash
and so the certificate will be invalid.
The result is that any modification to the code, no matter how slight,
will change the hash and keep it from being usable with the old VC.
Of course, the same thing happens with commercial code.  But GPL tends
to rely more on frequent, small, incremental changes - "release early
and often".  This practice will not work well with a hash-based security
certification concept.
There may be some creative workarounds possible for "trusted Linux"
developers.  They could set up their own certification infrastructure
and allow core Linux kernel team members to issue certs.  Especially
if TCPA is not used on Windows systems, as seems likely to be the case
now, Linux kernel developers will have considerable leverage in working
out the best way to tackle this problem.
Keep in mind, too, that one can make a strong case that open source
code in general is a far better fit for the general concept of remote
trust that TCPA/Palladium attempt to build.  If your trusted system is
running code whose source you can inspect, code which you might even be
able to build yourself from source, you can have much greater confidence
and peace of mind when you allow it to run on your system.  I would be
much more likely to trust a piece of software that was going to lock up
its data, if I could be confident of exactly what it was doing.
Adam Back wrote earlier about ideas to let the user inspect the data
stream into or out of a Palladium application.  I responded that this
would invalidate the security guarantees for a large and interesting
class of apps, such as the secure P2P enhancements I have sketched.
But you can achieve much of what Adam wanted simply by choosing to run
open source Palladium apps.  Those are the ones where you know exactly
what you are trusting the system to do.  They are the ones that you know
have no backdoor, spyware, or other objectionable, secret features.
And note that in the context of application programs, there is no need
for the OS certification described above.  Trust in these programs would
be handled on a completely decentralized basis, with each distributed
application handling its own trust decisions.  There would be no
centralized certification system.  Rather, the software would decide
for itself which versions to trust.
Summing up, it is at best a vast oversimplification to say that TCPA is
a GPL-killer.  It may cause some problems specifically for Linux due to
the need to get certifications that are widely accepted.  But it sounds
like HP and possibly IBM are going to at least get the ball rolling
with these certifications, so we can expect a TCPA compliant Linux in
some form.  And Linux developers may be able to come up with mechanisms
that will allow them to continue making frequent Linux releases while
still being able to support TCPA features.
For general applications, TCPA/Palladium could be a tremendous marketing
opportunity for open source.  Transparency and trust go together like
hand in glove.  In the long run I think the open source community will
thrive and benefit from TCPA and Palladium.

@_date: 2002-08-13 10:10:08
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
This makes a lot of sense, especially for "closed" systems like business
LANs and WANs where there is a reasonable centralized authority who can
validate the security of the SCP keys.  I suggested some time back that
since most large businesses receive and configure their computers in
the IT department before making them available to employees, that would
be a time that they could issue private certs on the embedded SCP keys.
The employees' computers could then be configured to use these private
certs for their business computing.
However the larger vision of trusted computing leverages the global
internet and turns it into what is potentially a giant distributed
computer.  For this to work, for total strangers on the net to have
trust in the integrity of applications on each others' machines, will
require some kind of centralized trust infrastructure.  It may possibly
be multi-rooted but you will probably not be able to get away from
this requirement.
The main problem, it seems to me, is that validating the integrity of
the SCP keys cannot be done remotely.  You really need physical access
to the SCP to be able to know what key is inside it.  And even that
is not enough, if it is possible that the private key may also exist
outside, perhaps because the SCP was initialized by loading an externally
generated public/private key pair.  You not only need physical access,
you have to be there when the SCP is initialized.
In practice it seems that only the SCP manufacturer, or at best the OEM
who (re) initializes the SCP before installing it on the motherboard,
will be in a position to issue certificates.  No other central authorities
will have physical access to the chips on a near-universal scale at the
time of their creation and installation, which is necessary to allow
them to issue meaningful certs.  At least with the PGP "web of trust"
people could in principle validate their keys over the phone, and even
then most PGP users never got anyone to sign their keys.  An effective
web of trust seems much more difficult to achieve with Palladium, except
possibly in small groups that already trust each other anyway.
If we do end up with only a few trusted root keys, most internet-scale
trusted computing software is going to have those roots built in.
Those keys will be extremely valuable, potentially even more so than
Verisign's root keys, because trusted computing is actually a far more
powerful technology than the trivial things done today with PKI.  I hope
the Palladium designers give serious thought to the issue of how those
trusted root keys can be protected appropriately.  It's not going to be
enough to say "it's not our problem".  For trusted computing to reach
its potential, security has to be engineered into the system from the
beginning - and that security must start at the root!

@_date: 2002-08-14 20:45:25
@_author: AARG! Anonymous 
@_subject: Overcoming the potential downside of TCPA 
Actually, this is not true for the endoresement key, PUBEK/PRIVEK, which
is the "main" TPM key, the one which gets certified by the "TPM Entity".
That key is generated only once on a TPM, before ownership, and must
exist before anyone can take ownership.  For reference, see section 9.2,
"The first call to TPM_CreateEndorsementKeyPair generates the endorsement
key pair. After a successful completion of TPM_CreateEndorsementKeyPair
all subsequent calls return TCPA_FAIL."  Also section 9.2.1 shows that
no ownership proof is necessary for this step, which is because there is
no owner at that time.  Then look at section 5.11.1, on taking ownership:
"user must encrypt the values using the PUBEK."  So the PUBEK must exist
before anyone can take ownership.
I don't quite follow what you are proposing here, but by the time you
purchase a board with a TPM chip on it, it will have already generated
its PUBEK and had it certified.  So you should not be able to transfer
a credential of this type from one board to another one.
Actually I don't see a function that will let the owner wipe the PUBEK.
He can wipe the rest of the TPM but that field appears to be set once,
retained forever.
For example, section 8.10: "Clear is the process of returning the TPM to
factory defaults."  But a couple of paragraphs later: "All TPM volatile
and non-volatile data is set to default value except the endorsement
key pair."
So I don't think your fraud will work.  Users will not wipe their
endorsement keys, accidentally or otherwise.  If a chip is badly enough
damaged that the PUBEK is lost, you will need a hardware replacement,
as I read the spec.
Keep in mind that I only started learning this stuff a few weeks ago,
so I am not an expert, but this is how it looks to me.

@_date: 2002-08-15 11:55:15
@_author: AARG! Anonymous 
@_subject: TCPA hack delay appeal 
It seems that there is (a rather brilliant) way to bypass TCPA (as spec-ed.) I learned about it from two separate sources, looks like two independent slightly different hacks based on the same protocol flaw.
Undoubtedly, more people will figure this out.
It seems wise to suppress the urge and craving for fame and NOT to publish the findings at this time. Let them build the thing into zillion chips first. If you must, post the encrypted time-stamped solution identifying you as the author but do not release the key before TCPA is in many, many PCs.

@_date: 2002-08-15 15:26:20
@_author: AARG!Anonymous 
@_subject: TCPA not virtualizable during ownership change 
Basically I agree with Adam's analysis.  At this point I think he
understands the spec equally as well as I do.  He has a good point
about the Privacy CA key being another security weakness that could
break the whole system.  It would be good to consider how exactly that
problem could be eliminated using more sophisticated crypto.  Keep in
mind that there is a need to be able to revoke Endorsement Certificates
if it is somehow discovered that a TPM has been cracked or is bogus.
I'm not sure that would be possible with straight Chaum blinding or
Brands credentials.  I would perhaps look at Group Signature schemes;
there is one with efficient revocation being presented at Crypto 02.
These involve a TTP but he can't forge credentials, just link identity
keys to endorsement keys (in TCPA terms).  Any system which allows for
revocation must have such linkability, right?
As for Joe Ashwood's analysis, I think he is getting confused between the
endorsement key, endorsement certificate, and endorsement credentials.
The first is the key pair created on the TPM.  The terms PUBEK and PRIVEK
are used to refer to the public and private parts of the endorsement
key.  The endorsement certificate is an X.509 certificate issued on the
endorsement key by the manufacturer.  The manufacturer is also called
the TPM Entity or TPME.  The endorsement credential is the same as the
endorsement certificate, but considered as an abstract data structure
rather than as a specific embodiment.
The PRIVEK never leaves the chip.  The PUBEK does, but it is considered
sensitive because it is a de facto unique identifier for the system,
like the Intel processor serial number which caused such controversy
a few years ago.  The endorsement certificate holds the PUBEK value
(in the SubjectPublicKeyInfo field) and so is equally a de facto unique
identifier, hence it is also not too widely shown.

@_date: 2002-08-16 15:56:09
@_author: AARG! Anonymous 
@_subject: Cryptographic privacy protection in TCPA 
Here are some more thoughts on how cryptography could be used to
enhance user privacy in a system like TCPA.  Even if the TCPA group
is not receptive to these proposals, it would be useful to have an
understanding of the security issues.  And the same issues arise in
many other kinds of systems which use certificates with some degree
of anonymity, so the discussion is relevant even beyond TCPA.
The basic requirement is that users have a certificate on a long-term key
which proves they are part of the system, but they don't want to show that
cert or that key for most of their interactions, due to privacy concerns.
They want to have their identity protected, while still being able to
prove that they do have the appropriate cert.  In the case of TCPA the
key is locked into the TPM chip, the "endorsement key"; and the cert
is called the "endorsement certificate", expected to be issued by the
chip manufacturer.  Let us call the originating cert issuer the CA in
this document, and the long-term cert the "permanent certificate".
A secondary requirement is for some kind of revocation in the case
of misuse.  For TCPA this would mean cracking the TPM and extracting
its key.  I can see two situations where this might lead to revocation.
The first is a "global" crack, where the extracted TPM key is published
on the net, so that everyone can falsely claim to be part of the TCPA
system.  That's a pretty obvious case where the key must be revoked for
the system to have any integrity at all.  The second case is a "local"
crack, where a user has extracted his TPM key but keeps it secret, using
it to cheat the TCPA protocols.  This would be much harder to detect,
and perhaps equally significantly, much harder to prove.  Nevertheless,
some way of responding to this situation is a desirable security feature.
The TCPA solution is to use one or more Privacy CAs.  You supply your
permanent cert and a new short-term "identity" key; the Privacy CA
validates the cert and then signs your key, giving you a new cert on the
identity key.  For routine use on the net, you show your identity cert
and use your identity key; your permanent key and cert are never shown
except to the Privacy CA.
This means that the Privacy CA has the power to revoke your anonymity;
and worse, he (or more precisely, his key) has the power to create bogus
identities.  On the plus side, the Privacy CA can check a revocation list
and not issue a new identity cert of the permanent key has been revoked.
And if someone has done a local crack and the evidence is strong enough,
the Privacy CA can revoke his anonymity and allow his permanent key to
be revoked.
Let us now consider some cryptographic alternatives.  The first is to
use Chaum blinding for the Privacy CA interaction.  As before, the user
supplies his permanent cert to prove that he is a legitimate part of
the system, but instead of providing an identity key to be certified,
he supplies it in blinded form.  The Privacy CA signs this blinded key,
the user strips the blinding, and he is left with a cert from the Privacy
CA on his identity key.  He uses this as in the previous example, showing
his privacy cert and using his privacy key.
In this system, the Privacy CA no longer has the power to revoke your
anonymity, because he only saw a blinded version of your identity key.
However, the Privacy CA retains the power to create bogus identities,
so the security risk is still there.  If there has been a global crack,
and a permanent key has been revoked, the Privacy CA can check the
revocation list and prevent that user from acquiring new identities,
so revocation works for global cracks.  However, for local cracks,
where there is suspicious behavior, there is no way to track down the
permanent key associated with the cheater.  All his interactions are
done with an identity key which is unlinkable.  So there is no way to
respond to local cracks and revoke the keys.
Actually, in this system the Privacy CA is not really protecting
anyone's privacy, because it doesn't see any identities.  There is no
need for multiple Privacy CAs and it would make more sense to merge
the Privacy CA and the original CA that issues the permanent certs.
That way there would be only one agency with the power to forge keys,
which would improve accountability and auditability.
One problem with revocation in both of these systems, especially the one
with Chaum blinding, is that existing identity certs (from before the
fraud was detected) may still be usable.  It is probably necessary to
have identity certs be valid for only a limited time so that users with
revoked keys are not able to continue to use their old identity certs.
Brands credentials provide a more flexible and powerful approach than
Chaum blinding which can potentially provide improvements.  The basic
setup is the same: users would go to a Privacy CA and show their
permanent cert, getting a new cert on an identity key which they would
use on the net.  The difference is that Brands provides for "restrictive
blinding".  This allows the Privacy CA to issue a cert on a key which
would be unlinkable to the permanent key under normal circumstances,
but perhaps linkability could be established in some cases.
It's not entirely clear how this technology could best be exploited to
solve the problems.  One possibility, for example, would be to encode
information about the permanent key in the restrictive blinding.
This would allow users to use their identity keys freely; but upon
request they could prove things about their associated permanent keys.
They could, for example, reveal the permanent key value associated with
their identity key, and do so unforgeably.  Or they could prove that their
permanent key is not on a given list of revoked keys.  Similar logical
operations are possible including partial revelation of the permanent
key information.
However it does not appear possible to solve the case of a local crack
using this technology.  In that case it is unlikely that they would
respond favorably to a request to reveal the permanent key associated with
their identity, so that it could be revoked.  Brands' technology would
allow them to do so in a convincing manner, but they would not cooperate.
In the end it's not clear how much Brands certificates really add over
the basic Chaum blinding in this application.  With the specific usage
described above, they have the same basic security properties as in
the case of Chaum blinding, except potentially for being able to prove
that an identity cert is not associated with a revoked permanent key.
Perhaps some other approach using his technology would be more successful.
One other cryptographic method that might be relevant is the group
signature.  This allows someone to sign with a key where he does not
reveal his signing key, but he proves that it is part of some group.
In the relevant variants, the group is defined as the set of keys
which has been certified by a "group membership key".  This approach
can therefore dispense with the Privacy CA entirely, and with blinding.
Instead, the permanent key itself is used for signing on the net, but
via a group signature which does not reveal the key value.  Instead,
the group signature protocol proves that the key exists and that it has
been certified by the CA.
The main problem with the group signature approach is handling revocation.
In the case of a global crack, where someone has published his permanent
key, at a minimum it is necessary to create a revocation list for those
keys.  This means that the group signature protocol must be extended
to not only prove that a key exists and has been certified, but also
that the key is not on the list of revoked keys - and to do this without
revealing the key itself.  That's a pretty complicated requirement which
is pushing the state of the art.  There is a paper being presented at
Crypto 02 which claims to offer the first group signature scheme with
efficient revocation.
Group signatures also offer an optional mechanism which can deal
with local cracks.  The original group signature concept included the
concept of a "revocation manager" who could link signatures to keys -
that is, there is one trusted party who can tell which key issued a
given signature.  In most of the modern variants, this is accomplished
by creating, as part of the group signature, an encrypted blob which
holds the user's permanent key, where that blob can be encrypted to any
specified key.  The only one who can tell who made the signature is the
key holder that the blob is encrypted to.
If this mechanism is used, we can bring back the Privacy CA, who
now functions as the party who can link signatures to permanent keys.
When someone uses a group signature to participate in a TCPA network, he
would optionally specify a Privacy CA who could reveal his permanent key.
This would allow for a multiplicity of Privacy CAs with different policies
about when and how they would reveal idenities, similar to the original
(non-cryptographic) TCPA concept.  Then it would be up to the recipients
of the signature to judge whether they trusted that Privacy CA to unmask
rogues upon sufficient evidence.
The main advantage of this scheme over the non-cryptographic TCPA method
is, first, that the Privacy CA is optional - users don't have to reveal
their identity to anyone if they don't want; and second, that the Privacy
CA no longer has the power to forge identities and disrupt the system.
This strengthens the overall security of the system.
Summing up, none of the alternatives presented here is ideal.  The current
scheme is among the worst, as it provides the weakest privacy protection
and allows the Privacy CAs to break the security of the entire system.
The Chaum and Brands blinding methods strengthen privacy at the cost of
reducing the ability to respond to local cracks, where the user extracts
his TPM key but keeps it to himself.  Group signatures provide good
privacy protection and can optionally respond to local cracks, but they
are cutting edge cryptography and are generally less efficient than the
other methods.

@_date: 2002-08-17 11:45:28
@_author: AARG! Anonymous 
@_subject: Cryptographic privacy protection in TCPA 
Dr. Mike wrote, patiently, persistently and truthfully:
Fine, but let me put this into perspective.  First, although the
discussion is in terms of a centralized issuer, the same issues arise if
there are multiple issuers, even in a web-of-trust situation.  So don't
get fixated on the fact that my analysis assumed a single issuer -
that was just for simplicity in what was already a very long message.
The abstract problem to be solved is this: given that there is some
property which is being asserted via cryptographic certificates
(credentials), we want to be able to show possession of that property
in an anonymous way.  In TCPA the property is "being a valid TPM".
Another example would be a credit rating agency who can give out a "good
credit risk" credential.  You want to be able to show it anonymously in
some cases.  Yet another case would be a state drivers license agency
which gives out an "over age 21" credential, again where you want to be
able to show it anonymously.
This is actually one of the oldest problems which proponents of
cryptographic anonymity attempted to address, going back to David Chaum's
seminal work.  TCPA could represent the first wide-scale example of
cryptographic credentials being shown anonymously.  That in itself ought
to be of interest to cypherpunks.  Unfortunately TCPA is not going for
full cryptographic protection of anonymity, but relying on Trusted Third
Parties in the form of Privacy CAs.  My analysis suggests that although
there are a number of solutions in the cryptographic literature, none of
them are ideal in this case.  Unless we can come up with a really strong
solution that satisfies all the security properties, it is going to be
hard to make a case that the use of TTPs is a mistake.
A certificate is a standardized and unforgeable statement that some
person or key has a particular property, that's all.  The kind of system
you are talking about, of personal knowledge and trust, can't really be
generalized to an international economy.
Whoever makes a statement about a property should have the power to
revoke it.  I am astounded that you think this is a radical notion.
If one or a few entities become widely trusted to make and revoke
statements that people care about, it is because they have earned that
trust.  If the NY Times says something is true, people tend to believe it.
If Intel says that such-and-such a key is in a valid TPM, people may
choose to believe this based on Intel's reputation.  If Intel later
determines that the key has been published on the net and so can no
longer be presumed to be a TPM key, it revokes its statement.
This does not mean that Intel would destroy any person's ability to use
their computer on a whim.  First, having the TPM cert revoked would not
destroy your ability to use your computer; at worst you could no longer
persuade other people of your trustworthiness.  And second, Intel would
not make these kind of decision on a whim, any more than the NY Times
would publish libelous articles on a whim; doing so would risk destroying
the company's reputation, one of its most valuable assets.
I can't really respond to the remainder of the message.  It doesn't seem
to have anything to do with the real issues.  Hopefully my introduction
above will have put the problem into perspective.  I suggest you educate
yourself on cryptographic technologies for anonymity.  You might start
with David Chaum's early CACM article,

@_date: 2002-08-21 23:45:29
@_author: AARG! Anonymous 
@_subject: New Palladium FAQ available 
Microsoft has apparently just made available a new FAQ on its
controversial Palladium technology at
Hopefully Microsoft will continue to release information about Palladium.
That should help to bring some of the more outrageous rumors under

@_date: 2002-07-05 14:45:21
@_author: AARG! Anonymous 
@_subject: Ross's TCPA paper 
Wouldn't it be more accurate to say that a "trusted" OS will not peek
at system resources that it is not supposed to?  After all, since the
OS loads the application, it has full power to molest that application
in any way.  Any embedded keys or certs in the app could be changed by
the OS.  There is no way for an application to protect itself against
the OS.
And there is no need; a trusted OS by definition does not interfere with
the application's use of confidential data.  It does not allow other
applications to get access to that data.  And it provides no back doors
for "root" or the system owner or device drivers to get access to the
application data, either.
At  you provide more
information about your meeting with Microsoft.  It's an interesting
writeup, but the part about the system somehow protecting the app from the
OS can't be right.  Apps don't have that kind of structural integrity.
A chip in the system cannot protect them from an OS virtualizing that
chip.  What the chip does do is to let *remote* applications verify that
the OS is running in trusted mode.  But local apps can never achieve
that degree of certainty, they are at the mercy of the OS which can
twiddle their bits at will and make them "believe" anything it wants.
Of course a "trusted" OS would never behave in such an uncouth manner.
Absolutely.  The fantasies which have been floating here of filters
preventing people from typing virus-triggering command lines are utterly
absurd.  What are people trying to prove by raising such nonsensical
propositions?  Palladium needs no such capability.
Right, and you can boot untrusted OS's as well.  Recently there was
discussion here of HP making a trusted form of Linux that would work with
the TCPA hardware.  So you will have options in both the closed source and
open source worlds to boot trusted OS's, or you can boot untrusted ones,
like old versions of Windows.  The user will have more choice, not less.
Yes, your web page goes into somewhat more detail about how this would
work.  This way a program can run under a secure OS and store sensitive
data on the disk, such that booting into another OS will then make it
impossible to decrypt that data.
Some concerns have been raised here about upgrades.  Did Microsoft
discuss how that was planned to work, migrating from one version of a
secure OS to another?  Presumably they have different hashes, but it
is necessary for the new one to be able to unseal data sealed by the
old one.
One obvious solution would be for the new OS to present a cert to the chip
which basically said that its OS hash should be treated as an "alias"
of the older OS's hash.  So the chip would unseal using the old OS hash
even when the new OS was running, based on the fact that this cert was
signed by the TCPA trusted root key.
This seems to put more power than we would like into a single trusted
key, though.  It would be interesting to hear what Microsoft has in mind
along these lines.
If you've read the TCPA specs you're way ahead of most of the commentators
here.  You have undoubtedly noted how little connection there is between
the flights of fancy and speculation which have appeared recently and
the actual functionality of the TCPA system.

@_date: 2002-07-15 12:10:13
@_author: AARG! Anonymous 
@_subject: DRM will not be legislated 
The line you quoted was the summary from a message which described
the detailed reasoning that supported the claim.  To reiterate and lay
out the points explicitly:
 - Legislating DRM would be extremely expensive in the current environment
   as it would require phasing out all computers presently in use.
   This provides a huge practical burden and barrier for any legislation
   along these lines.
 - Some have opposed voluntary DRM because they believe that it would
   reduce the barrier above.  Once DRM systems are voluntarily installed
   in a substantial number of systems, it would be a relatively small
   step to mandate them in all systems.
 - But this is false reasoning; if DRM is so successful as to be present
   in a substantial number of systems, it is not necessary to legislate
   it.
 - Further, even if it is legislated, that will not stop piracy.  No
   practical DRM system will prevent people from running arbitrary
   3rd party software (despite absurd arguments by fanatics that the
   government seeks to remove Turing complete computing capabilities
   from the population).
 - Neither the content nor technology companies have incentive to support
   legislation, as they still must convince people that paying for
   content is superior to pirating it.  Legislating DRM will not help
   them in this battle, as piracy will still be an alternative.
 - What would help them legislatively is some kind of enforced
   watermarking technology, so that the initial "ripping" of content is
   impossible (this also requires closing the analog hole).  Only by
   intervening at this first step can they hope to break the piracy
   chain, and this is the real purpose of the Hollings bill.  See also
   the recent work by the BPDG.  But this is not DRM in the sense we
   are discussing it here.
Those were the points made earlier in support of the summary statement
quoted above.  As far as the Hollings bill in particular, the most notable
aspect of it was the tremendous opposition from virtually every sector of
the economy.  The Hollings bill was not just a failure, it was a massive,
DOA, stinking heap of failure which had not even the slightest chance
of success.  If anything, the failure of the Hollings bill fully supports
the thesis that legislation of DRM is not going to happen.
As for Macrovision, this is an example of "watermarking" technology and
as mentioned above, it does make sense to legislate along these lines
(although it is questionable whether it can work in the long run -
Macrovision defeaters are widely available).  It represents an attempt
to close the analog hole.
The point is that this is not a simple-minded or unreflective analysis.
We are looking specifically at the kind of DRM enabled by the TCPA.
This means the ability to run content viewing software that imposes DRM
rules which might limit the number of views, or require pay per view,
or require data to be deleted if it is copied elsewhere, etc.  The point
of TCPA and Palladium is for the remote content provider to be assured
that the software it is talking to across the net is a trusted piece of
software which will enforce the rules.
It is this kind of DRM to which the analysis above is directed.  This DRM
does not prevent piracy using any of the techniques available today, or
via exploiting bugs and flaws in future technology.  It does not and can
not prevent people from running file sharing programs and making pirated
content available on the Internet (at least without crippling computers
to the point where necessary business functionality is lost, which would
mean sending the country into a deep depression and making it an obsolete
competitor on world markets, i.e. it won't happen).  This kind of DRM
can nevertheless succeed on a voluntary basis by providing good quality
for good value, in conjunction with technological and legal attacks on
P2P systems such as are in their infancy now.
All of these arguments have been made in the past few weeks on this list.
Hopefully reiterating them in one place will be helpful to those who
have overlooked them in the past.

@_date: 2002-07-17 16:26:09
@_author: AARG! Anonymous 
@_subject: DRM will not be legislated 
The CBDTPA, available in text form at
does not explicitly call for legislating DRM.  In fact the bill is not
very clear about what exactly it does require.  Generally it calls for
standards that satisfy subsections (d) and (e) of section 3.  But (d) is
just a list of generic good features: "(A) reliable; (B) renewable; (C)
resistant to attack; (D) readily implemented; (E) modular; (F) applicable
in multiple technology platforms; (G) extensible; (H) upgradable; (I)
not cost prohibitive; and (2) any software portion of such standards is
based on open source code."
There's nothing in there about DRM or the analog hole specifically.
In fact the only phrase in this list which would not be applicable to any
generic software project is "resistant to attack".  And (e) (misprinted
as (c) in the document) is a consumer protection provision, calling
for support of fair use and home taping of over the air broadcasts.
Neither (d) nor (e) describes what exactly the CBDTPA is supposed to do.
To understand what the technical standards are supposed to protect we
have to look at section 2 of the bill, "Findings", which lays out the
piracy problem as Hollings sees it and calls for government regulation
and mandates for solutions.  But even here, the wording is ambiguous
and does not clearly call for mandating DRM.
The structure of this section consists of a list of statements, followed
by the phrase, "A solution to this problem is technologically feasible
but will require government action, including a mandate to ensure its
swift and ubiquitous adoption."  This phrase appears at points 12,
15 and 19.
The points leading up to  refer to the problems of over the air
broadcasts being unencrypted, in contrast with pay cable and satellite
systems.  The points leading up to  talk about closing the analog hole.
And the points leading up to  discuss file sharing and piracy.
DRM is mentioned in point 5, in terms of it not working well, then
the concept is discussed again in points 20-23, which are the last.
None of these comments are followed by the magic phrase about requiring
a government mandate.
So if you look closely at how these points are laid out, and which ones
get the call for government action, it appears that the main concerns
which the CBDTPA is intended to address are (1) over the air broadcasts
(via the BPDG standard); (2) closing the analog hole (via HDCP and
similar); and (3) piracy via file sharing and P2P systems, which the
media companies would undoubtedly like to see shut down but where they
are unlikely to succeed.  Although DRM is mentioned, there is no clear
call to mandate support for DRM technology, particularly anything similar
to Palladium or the TCPA, which is what we have been discussing.
As pointed out earlier, this is logical, as legislating the TCPA would
be both massively infeasible and also ultimately unhelpful to the goals
of the content companies.  They know they won't be able to use TCPA to
shut down file sharing.  The only way they could approach it using such
a tool would be to have a law requiring a government stamp of approval
on every piece of software that runs.  Surely it will be clear to all
reasonable men what a a non-starter that idea is.

@_date: 2002-07-18 14:00:03
@_author: AARG! Anonymous 
@_subject: DRM will not be legislated 
Read a great article on Slashdot about the recent DRM workshop,
 by "al3x":
   As the talks began, I was brimming with the enthusiasm and anger of an
   "activist," overjoyed at shaking hands with the legendary Richard
   Stallman, thrilled with the turnout of the New Yorkers for Fair
   Use. My enthusiasm and solidarity, however, was to be short lived....
   Comments from the RIAA's Mitch Glazier that there is "balance in the
   Digital Millennium Copyright Act" (DMCA), drew cries and disgusted
   laughter from the peanut gallery, who at that point had already been
   informed that any public comments could be submitted online. Even
   those in support of Fair Use and similar ideas began to be frustrated
   with the constant background commentary and ill-conceived outbursts
   of the New Yorkers for Fair Use and, to my dismay, Richard Stallman,
   who proved to be as socially awkward as his critics and fans alike
   report. Perhaps such behavior is entertaining in a Linux User Group
   meeting or academic debate, but fellow activists hissed at Stallman
   and the New Yorkers, suggesting that their constant interjections
   weren't helping.
   And indeed, as discussion progressed, I felt that my representatives
   were not Stallman and NY Fair Use crowd, nor Graham Spencer from
   DigitalConsumer.org, whose three comments were timid and without
   impact. No, I found my voice through Rob Reid, Founder and Chairman
   of Listen.com, whose realistic thinking and positive suggestions were
   echoed by Johnathan Potter, Executive Director of DiMA, and backed
   up on the technical front by Tom Patton of Phillips. Reid argued
   that piracy was simply a reality of the content industry landscape,
   and that it was the job of content producers and the tech industry
   to offer consumers something "better than free." "We charge $10
   a month for our service, and the competition is beating us by $10
   a month. We've got to give customers a better experience than the
   P2P file-sharing networks," Reid suggested. As the rare individual
   who gave up piracy when I gave up RIAA music and MPAA movies, opting
   instead for a legal and consumer-friendly Emusic.com account, I found
   myself clapping in approval.
Reading this and the other comments on the meeting, a few facts come
through: that the content companies are much more worried about closing
the "analog hole" than mandating traditional DRM software systems; that
the prospects for any legislation on these issues are uncertain given the
tremendous consumer opposition; and that extremist consumer activists are
hurting their cause by conjuring up farfetched scenarios that expose them
as kooks.  (That last point certainly applies to those here who continue
to predict that the government will take away general purpose computing
capabilities, allow only "approved" software to run, and ban the use of
Perl and Python without a license.  Try visiting the real world sometime!)
It is also good to see that the voices of sanity are being more and
more recognized, like the Listen.com executive above.  The cyber liberty
community must come out strongly against piracy of content and support
experiments which encourage people to pay for what they download.  It is
no longer tenable to claim that intellectual property is obsolete or evil,
or to point to the complaints of a few musicians as justification for
ignoring the creative rights of an entire industry.  There is still a
very good chance that we can have a future where people will happily pay
for legal content instead of making do with bootleg pirate recordings,
and that this can happen without legislation and without hurting consumer
Such an outcome would be the best for all concerned: for consumers, for
tech companies, for artists and for content licensees.  Anything else will
be a disaster for one or more of these groups, which will ultimately hurt
everyone.  Let's hope the EFF is listening to the kinds of clear-sighted
commentary quoted above.

@_date: 2002-07-29 12:25:25
@_author: AARG! Anonymous 
@_subject: Hollywood Hackers 
I like this scenario: Adam places his copyrighted content on his web site. His friend, Eve, violates his copyright and places Adam's copyrighted content on her site. Hollywood downloads the copyright-infringing content from Eve's site. Eve confesses that Hollywood did so, in a good faith effort to repent from her copyright infringement. Now Adam hacks Hollywood, as authorized by the proposed law. Lawsuits all around.

@_date: 2002-07-29 15:35:32
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Declan McCullagh writes at
   "The world is moving toward closed digital rights management systems
   where you may need approval to run programs," says David Wagner,
   an assistant professor of computer science at the University of
   California at Berkeley.  "Both Palladium and TCPA incorporate features
   that would restrict what applications you could run."
But both Palladium and TCPA deny that they are designed to restrict what
applications you run.  The TPM FAQ at
 reads, in
answer : The TPM can store measurements of components of the user's system, but
: the TPM is a passive device and doesn't decide what software can or
: can't run on a user's system.
An apparently legitimate but leaked Palladium White Paper at
says, on the page shown as number 2:
: A Palladium-enhanced computer must continue to run any existing
: applications and device drivers.
and goes on,
: In addition, Palladium does not change what can be programmed or run
: on the computing platform; it simply changes what can be believed about
: programs, and the durability of those beliefs.
Of course, white papers and FAQs are not technical documents and may not
be completely accurate.  To really answer the question, we need to look
at the spec.  Unfortunately there is no Palladium spec publicly available
yet, but we do have one for TCPA, at
Can you find anything in this spec that would do what David Wagner says
above, restrict what applications you could run?  Despite studying this
spec for many hours, no such feature has been found.
So here is the challenge to David Wagner, a well known and justifiably
respected computer security expert: find language in the TCPA spec to
back up your claim above, that TCPA will restrict what applications
you can run.  Either that, or withdraw the claim, and try to get Declan
McCullagh to issue a correction.  (Good luck with that!)
And if you want, you can get Ross Anderson to help you.  His reports are
full of claims about Palladium and TCPA which seem equally unsupported
by the facts.  When pressed, he claims secret knowledge.  Hopefully David
Wagner will have too much self-respect to fall back on such a convenient

@_date: 2002-07-30 22:05:15
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Maybe, but the point is whether the architectural spec includes that
capability.  After all, any OS could restrict what applications you
run; you don't need special hardware for that.  The question is whether
restrictions on software are part of the design spec.  You should be
able to point to something in the TCPA spec that would restrict or limit
software, if that is the case.
Or do you think that when David Wagner said, "Both Palladium and TCPA
incorporate features that would restrict what applications you could run,"
he meant "that *could* restrict what applications you run"?  They *could*
impose restrictions, just like any OS could impose restrictions.
But to say that they *would* impose restrictions is a stronger
statement, don't you think?  If you claim that an architecture would
impose restrictions, shouldn't you be able to point to somewhere in the
design document where it explains how this would occur?
There's enormous amount of information in the TCPA spec about how to
measure the code which is going to be run, and to report those measurement
results so third parties can know what code is running.  But there's not
one word about preventing software from running based on the measurements.

@_date: 2002-07-31 23:25:17
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
What would be an example of a privilege that you fear would be taken
away from you with TCPA?  It will boot any software that you want, and
can provide a signed attestation of a hash of what you booted.  Are you
upset because you can't force the chip to lie about what you booted?
Of course they could have designed the chip to allow you to do that, but
then the functionality would be useless to everyone; a chip which could
be made to lie about its measurements might as well not exist, right?

@_date: 2002-07-31 23:45:35
@_author: AARG! Anonymous 
@_subject: Challenge to David Wagner on TCPA 
So TCPA and Palladium "could" restrict which software you could run.
They aren't designed to do so, but the design could be changed and
restrictions added.
But you could make the same charge about any software!  The Mac OS could
be changed to restrict what software you can run.  Does that mean that
we should all stop using Macs, and attack them for something that they
are not doing and haven't said they would do?
The point is, we should look critically at proposals like TCPA and
Palladium, but our criticisms should be based in fact and not fantasy.
Saying that they could do something or they might do something is a much
weaker argument than saying that they will have certain bad effects.
The point of the current discussion is to improve the quality of the
criticism which has been directed at these proposals.  Raising a bunch
of red herrings is not only a shameful and dishonest way to conduct the
dispute, it could backfire if people come to realize that the system
does not actually behave as the critics have claimed.
Peter Fairbrother made a similar point:
Fine, but note that at least TCPA as currently designed does not have this
specific capability of keeping some software from booting and running.
Granted, the system could be changed to allow only certain kinds of
software to boot, just as similar changes could be made to any OS or
boot loader in existence.
Back to Peter Trei (and again, Peter Fairbrother echoed his concern):
Under TCPA, he can do everything with his computer that he can do today,
even if the system is not turned off.  What he can't do is to use the
new TCPA features, like attestation or sealed storage, in such a way as
to violate the security design of those systems (assuming of course that
the design is sound and well implemented).  This is no more a matter of
turning over control of his computer than is using an X.509 certificate
issued by a CA to prove his identity.  He can't violate the security of
the X.509 cert.  He isn't forced to use it, but if he does, he can't
forge a different identity.  This is analogous to how the attestation
features of TCPA works.  He doesn't have to use it, but if he wants to
prove what software he booted, he doesn't have the ability to forge the
data and lie about it.
This is why the original "Challenge" asked for specific features in the
TCPA spec which could provide this claimed functionality.  Even if TCPA
is somehow kept turned on, it will not stop any software from booting.
Now, you might say that they can then further change the TCPA so that
it *does* stop uncertified software from booting.  Sure, they could.
But you know what?  They could do that without the TCPA hardware.
They could put in a BIOS that had a cert in it and only signed OS's could
boot.  That's not what TCPA does, and it's nothing like how it works.
A system like this would be a very restricted machine and you might
justifiably complain if the manufacturer tried to make you buy one.
But why criticize TCPA for this very different functionality, which
doesn't use the TCPA hardware, the TCPA design, and the TCPA API?
How could this be true, when there are no features in the TCPA design
to allow this powerful third party to restrict your use of your computer
in any way?
(By the way, does anyone know why these messages are appearing on
cypherpunks but not on the cryptography at wasabisystems.com mailing list,
when the responses to them show up in both places?  Does the moderator of
the cryptography list object to anonymous messages?  Or does he think the
quality of them is so bad that they don't deserve to appear?  Or perhaps
it is a technical problem, that the anonymous email can't be delivered
to his address?  If someone replies to this message, please include this
final paragraph in the quoted portion of your reply, so that the moderator
will perhaps be prompted to explain what is going wrong.  Thanks.)

@_date: 2002-06-01 22:30:06
@_author: AARG! Anonymous 
@_subject: 2 Challenge Gun Cases, Citing Bush Policy 
Aim for the head, and use fragmenting/hydrashock ammo. Exploded heads seem to disturb others the most.

@_date: 2002-06-15 20:40:08
@_author: AARG! Anonymous 
@_subject: freedom 
Freedom's just another word,
For nothin' left to lose.

@_date: 2002-05-26 15:15:36
@_author: AARG! Anonymous 
@_subject: NYT: Techies Now Respect Government 
What really changed in the Valley is that the best are gone. There is always a very small number of real contributors, I'd say one in several hundreds, that shape the whole environment and dictate the overall mood.
This was best seen in Xerox PARC, where sleazy Gilman Louie was selling fatherland defense on May 16, with mannerism and vocabulary of a polished used car salesman. He was preaching to an auditorium packed with white middle managers and young aspiring nobodies, extracting applause and laughs at all the right places. No one threw up, and at the end he even didn't have to say "MEIN GOTT I CAN WALK !!" It was implied.
He said, after describing his enlightment that working for CIA is good after all, in the best tradition of government commercials from 50-ties, that VCs were always patriotic. He also said that they received 500 business plans in few weeks after demolition of WTC, and that government needs better tools to track arab student pilots.
This is the new silicon valley, future grounds of the Homeland Security Industries, where thousands of engineers will proudly churn out surveillance products, dissent-detecting chips and network tapping devices.

@_date: 2002-11-19 20:36:20
@_author: AARG! Anonymous 
@_subject: the wrong poem 
The saddest thing here is that this gets reported without any comment. Snuffing journalists seems far more cost effective than offing pigs.
Baker discounted claims by federal authorities that Maali
had financially supported terrorist groups when he made
donations to Palestinian charities, and that an essay and
poems he had written showed sympathy for suicide bombers
in Israel.

@_date: 2002-09-17 12:37:21
@_author: AARG! Anonymous 
@_subject: Palladium block diagram 
Here is a functional block diagram of the Palladium software, based on
a recent presentation by Microsoft.  My notes were a bit sketchy as I
rushed to copy down this slide, so there may be some slight errors.
But this is basically what was shown.  (Use a monospace font to see
it properly.)
         Normal Mode                          Trusted Mode
The idea is that initially only the left half exists.  To launch
Palladium the user runs the Nubsys.exe program.  This goes into
kernel mode and loads the NubMgr.sys module, which initiates trusted
mode and launches the secure executive or "nexus".  (This is what is
also sometimes called the Nub or the TOR.)
When a Palladium-aware app is launched in user mode, it is linked with a
PdLib and requests to the Nexus to load the corresponding Trusted Agent.
The Agent runs trusted in user mode, and has its own PdLib which lets it
make system calls into the Nexus.  The Trusted Agent and the application
then communicate back and forth across the trusted/normal mode boundary.

@_date: 2002-09-17 14:05:27
@_author: AARG! Anonymous 
@_subject: Cryptogram: Palladium Only for DRM 
Thanks for the explanation.  Essentially you can create a virtualized
Palladium, where you emulate the functionality of the secure hardware.
The kernel normally has access to all of memory, for example, but you can
virtualize the MMU as VmWare does, so that some memory is inaccessible
even to the kernel, while the kernel can still run pretty much the same.
Similarly your virtualizing software could comput a hash of code that
loads into this secure area and even mimic the Palladium functionality
to seal and unseal data based on that hash.  All this would be done at
a level which was inaccessible to ordinary Windows code, so it would be
basically as secure as Palladium is with hardware.
The one thing that you don't get with this method is secure attestation.
There is no way your software can prove to a remote system that it is
running a particular piece of code, as is possible with Pd hardware.
However I believe you see this as not a security problem, since in your
view the only use for such functionality is for DRM.
I do think there are some issues with this approach to creating a
secure system, even independent of the attestation issue.  One is
performance.  According to a presentation by the VMWare chief scientist [1],
VMWare sees slowdowns of from 8 to 30 percent on CPU-bound processes, with
graphics-intensive applications even worse, perhaps a factor of 2 slower.
Maybe Windows could do better than this, but users aren't going to be
happy about giving up 10 percent or more of their CPU performance.
Also, Palladium hardware provides protection against DMA devices:
"Even PCI DMA can't read or write memory which has been reserved to a
nub's or TA's use (including the nub's or TA's code). This memory is
completely inaccessible and can only be accessed indirectly through
API calls. The chipset on the motherboard is modified to enforces this
sort of restriction." [2]  It's conceivable that without this hardware
protection, a virus could exploit a security flaw in an external device
and get access to the secure memory provided by a virtualized Palladium.
But these are not necessarily major problems.  Generally I now agree
with your comments, and those of others, that the security benefits of
Palladium - except for secure remote attestation - can be provided using
existing and standard PC hardware, and that the software changes necessary
are much like what would be necessary for the current Palladium design,
plus the work to provide VMWare-type functionality.
However that still leaves the issue of remote attestation...
This is a simplistic view.  There are many situations in which it is in
the interests of the user to be able to prove to third parties that he
is unable to commit certain actions.  A simple example is possession
of a third-party cryptographic certificate.  The only reason that is
valuable is because the user can't create it himself.  Any time someone
shows a cert, they are giving up some control in order to get something.
They can't modify that certificate without rendering it useless.  They are
limited in what they can do with it.  But it is these very limitations
that make the cert valuable.
But let me cut to the chase and provide some examples where remote
attestation, allowing the user to prove that he is running a particular
program and that it is unmolested, is useful.  These will hopefully
encourage you to modify your belief that "The 'secure chip' in Pd is
only needed for DRM. All other claimed benefits of Pd can be achieved
using existing hardware. To me this is an objectively verifyable truth."
I don't think any of these examples could be solved with software plus
existing hardware alone.
The first example is a secure online game client.  Rampant cheating in the
online gaming industry is a major problem which has gotten much attention
in the past few months.  Players can load hacks to let them see through
walls, automate targetting and fire control, and employ other methods that
give them an overwhelming advantage.  The game industry is big business
and is growing rapidly.  Unless they can get a handle on this problem it
could hurt the success of the industry.  Remote attestation will allow
game servers to be confident that users are playing on a level playing
field, without the hacked and modified clients that allow cheating.
Another example is a P2P file sharing network client.  Faced with the
threat from these file sharing networks, content industries are attacking
them in various ways, including flooding them with bogus queries and
lying about file hashes and contents.  Also, some users are switching
to clients that give them unfair priority, at the expense of the rest
of the network.  Recently a Salon article [3] reported that Gnutella
developers are considering methods to check client integrity in order to
avoid these problems.  But this is not really possible today, especially
with open source clients.  What they need is secure attestation.  That way
they can be confident that client software is using the protocol fairly
and appropriately.  Of course, the Gnutella developers have no idea that
the evil bogeyman of Palladium could actually help them out - no one
in the security field is willing to say anything favorable about the
technology.  This conspiracy of silence is what I am trying to overcome.
Online distributed computations can also benefit.  Recently there was a
comment on cypherpunks that pointed out that the RC5 keysearch effort
has gone through 85% of the key space without success, raising the
possibility that it somehow skipped the key.  This particular search has
gone on for almost five years, and all that work may come to nothing!
Being able to check that users are running valid clients will eliminate
one possible source of problems in these efforts.  Remote attestation
will be an easy way to improve the chances that if someone says that
the key is not in a particular region, it really isn't.
Turning to banking and e-commerce, there are many cases where it is not
enough to be running a secure client; you must also be able to prove
that you are doing so.  According to Perry Metzger's earlier message,
if a user is defrauded in a banking transaction due to a virus, he
is supposed to be protected.  It's not clear who has to pay him back,
but let's suppose it is the bank.  Now the bank has a very real economic
interest in making sure that its clients are using the approved software.
If there is third-party software out there, the bank can't be sure of its
reliability and security.  Secure attestation can improve the security
of the entire system by making sure that third-party clients are not
allowed to participate in transactions.  The bank has control over its
liability and its risk.
The same reasoning applies to credit card purchases.  If you've got a
virtual-Palladium-based secure system, that's great; you're protected
against a virus somehow getting your card number and using it.
But actually it is the banks who eat the charges when cards are misused,
at least in the U.S.  It could make a great deal of sense for acceptors
of credit cards to be able to check what software the user is running,
make sure it is a secure system and that it is an approved client.
This will give users incentives to move to secure systems, which will
benefit everyone as the amount of fraud decreases.
Secure attestation could even provide a way for commercial companies to
prove to customers that they are running particular accounting software
to hold data on their customers.  Eventually, accounting software could
be developed which is oriented around protecting the privacy of consumer
data; for example, it could prevent wholesale exporting of the database
into some format that could be easily copied or transferred.  User agents
could refuse to upload their personal data unless the company can prove
that it is running one of these privacy-protecting software packages.
Adam Back (who opposes Palladium) described several other possible
applications all of which rely on secure attestation [4].  He listed:
:      - general Application Service Providers (ASPs) that you don't have to
:      trust to read your data
:      - less traceable peer-to-peer applications
:      - DRM applications that make a general purpose computer secure against
:      BORA (Break Once Run Anywhere), though of course not secure against
:      ROCA (Rip Once Copy Everywhere) -- which will surely continue to
:      happen with ripping shifting to hardware hackers.
:      - general purpose unreadable sandboxes to run general purpose
:      CPU-for-rent computing farms for hire, where the sender knows you
:      can't read his code, you can't read his input data, or his output
:      data, or tamper with the computation.
:      - file-sharing while robustly hiding knowledge and traceability of
:      content even to the node serving it -- previously research question,
:      now easy coding problem with efficient
:      - anonymous remailers where you have more assurance that a given node
:      is not logging and analysing the traffic being mixed by it
Of course, one of these is indeed DRM, and no one (least of all Microsoft)
denies that DRM will be facilitated by Palladium or that it was one
of the motivations for creating the technology.  But the rest are all
different, and hopefully they provide further evidence that Palladium,
in its full form, is good for more than DRM.  In particular, the idea of
CPU-for-rent farms has been stymied for years because of concerns about
sensitive corporate data.  Secure attestation could be a breakthrough that
will put the incredible power of idle machines to productive economic use,
for tremendous benefits to society.
The virtualized Palladium can't do any of the applications above, as
far as I can see.  These are all benefits of Palladium which rely on
the secure hardware chip, and they go far beyond DRM.  Many of these
applications actually serve to enhance the user's security and privacy.
I understand you aren't going to reply, but hopefully in your own mind
you will see that there are far more benefits to the full version of
Palladium, including its security chip, than just DRM.
[1] [2] [3] [4]  at wasabisystems.com/msg02526.html

@_date: 2002-09-19 11:13:44
@_author: AARG! Anonymous 
@_subject: Cryptogram: Palladium Only for DRM 
That's too bad.  Trusted computing is a very interesting technology
with many beneficial uses.  It is a shame that Microsoft has a patent on
this and will be enforcing it, which will reduce the number of competing
Of course, those like Lucky who believe that trusted computing technology
is evil incarnate are presumably rejoicing at this news.  Microsoft's
patent will limit the application of this technology.  And the really
crazy people are the ones who say that Palladium is evil, but Microsoft
is being unfair in not licensing their patent widely!
Well, I was actually referring to open source applications, not the OS.
Palladium-aware apps that are available in source form can be easily
verified to make sure that they aren't doing anything illicit.  Since the
behavior of the application is relatively opaque while it is protected by
Palladium technology, the availability of source serves as an appropriate
But it does appear that Microsoft plans to make the source to the TOR
available in some form for review, so apparently they too see the synergy
between open (or at least published) source and trusted computing.
UNLESS Microsoft means that the architecture is such that it does not
support encrypting applications!  The wording of the statement above seems
stronger than just "we don't plan on encrypting our apps at this time".
There are a couple of reasons to believe that this might be true.
First, it is understood that Palladium hashes the secure portions of
the applications that run.  This hash is used to decrypt data and for
reporting to remote servers what software is running.  It seems likely
that the hash is computed when the program is loaded.  So the probable
API is something like "load this file into secure memory, hash it and
begin executing it."
With that architecture, it would not work to do as some have proposed:
the program loads data into secure memory, decrypts it and jumps to it.
The hash would change depending on the data and the program would no
longer be running what it was supposed to.  This would actually undercut
the Palladium security guarantees; the program would no longer be running
code with a known hash.
Second, the Microsoft Palladium white paper at
describes the secure memory as "trusted execution space".  This suggests
that this memory is designed for execution, not for holding data.
The wording hints at an architectural separation between code and data,
when in the trusted mode.
Further reason to believe that Palladium's architecture may not support
the encryption of executable code.

@_date: 2002-09-21 13:15:18
@_author: AARG! Anonymous 
@_subject: Random Privacy 
Greg Broiles wrote about randomizing survey answers:
On the contrary, TCPA/Palladium can solve exactly this problem.  It allows
the marketers to *prove* that they are running a software package that
will randomize the data before storing it.  And because Palladium works
in opposition to their (narrowly defined) interests, they can't defraud
the user by claiming to randomize the data while actually storing it
for marketing purposes.
Ironically, those who like to say that Palladium "gives away root on your
computer" would have to say in this example that the marketers are giving
away root to private individuals.  In answering their survey questions,
you in effect have root privileges on the surveyor's computers, by this
simplistic analysis.  This further illustrates how misleading is this
characterization of Palladium technology in terms of root privileges.

@_date: 2003-01-08 10:05:59
@_author: AARG! Anonymous 
@_subject: Television 
I've never come across a Tim May post that I thought wasn't worth the time it took to read it. They are all either amusing, informative, or provocative, or some combination of those. I like that. I can't say that about many other posters.
