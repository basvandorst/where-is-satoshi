
@_date: 2000-12-04 00:59:45
@_author: Adam Back 
@_subject: ecash, cut & choose and private credentials (Re: Jim Bell) 
Offline means offline with fraud-tracing in event of double spending,
so the efficiency refers to the computation and communication cost of
the withdraw and deposit protocols which do the normal ecash thing,
plus encode identity in the coin in the withdraw protocol in a way
which will be revealed in a double spent show protocol.
The protocols you list are online.  Not that this is a bad thing -- I
kind of prefer the online idea -- rather than the "and then you go to
jail" implications of fraud tracing in the offline protocols.  Plus
you have a risk of accidentally double spending if your computer
crashes or something.

@_date: 2000-12-04 01:14:38
@_author: Adam Back 
@_subject: ecash, cut & choose and private credentials (Re: Jim Bell) 
Whatever the class of pornography it is that is popular on the
internet would indeed be a good starting market.  I'd consider this
class the "mainstream" pornography on the internet.  So I'm not sure
if you're referring to this or some more risky harder to find stuff
which people may arguably be willing to pay a higher premium for.  I'd
aim for the mainstream stuff due to volume.
I presume the crippling you're talking about is the payee traceability
with collusion of payer and bank.
However I'm not sure I agree that the payee anonymity identification
feature killed it for this application.  For this particular
application there appear to be plenty of sites willing to serve the
content sans anonymity -- they're VISA, etc merchants no less.
I think the thing that killed MT / digicash for this application was
MT at the time was reported to be closing accounts related to
pornography -- they apparently didn't want the reputation for
providing payment mechanisms for the porn industry or something.
Plus of course:
- the need to download a client
- the small userbase making it an unattractive proposition for
  users
- the need to setup an account (in US funds) -- no accountless operation
- accounts were difficult to setup too
- the greedy fee structure
- differentiating between merchants and users
I'd have thought the thing to do was put an ecash client in Mozilla
and work on getting it into netscape.  Plus download plugins for
Internet Explorer.
So whoever develops enough clue, capability and interest in making
money to do this someday needs to think about making it work with
existing credit card sites.
Give back a one time credit card number for legacy sites which is
cryptographically unlinkable with the ecash spender.
In general try to make it interoperate with other payment systems.
Try to make it as painless and instant as possible to buy ecash.
All of this you'd think would be obvious.

@_date: 2000-11-05 22:11:54
@_author: Adam Back 
@_subject: CDR: PipeNet protocol 
For those that don't know about PipeNet Wei has a description here
[1].  PipeNet is a synchronous mix-net where users stay connected and
consume bandwidth 24x365 to avoid revealing when they are using it.
PipeNet's synchronous behavior would be implemented on top of TCP with
it's best-effort service by using the following scheduling algorithm
(from [1]):
This is to defend against active attacks delaying packets to observe
the effect on the network and hence trace routes.
However I think this scheduling algorithm would have the side effect
of making this variant of PipeNet very vulnerable to DoS attacks.  Any
user can arbitrarily delay packet delivery for the entire network by
ceasing to send packets.
It would also seem that performance would degrade badly to effectively
the performance of the worst ping time link.
(PipeNet uses mixing at each node.)
Perhaps there are some engineering trade-offs you could make if you
were to try to practically implement PipeNet.
[1]

@_date: 2000-11-06 17:22:01
@_author: Adam Back 
@_subject: CDR: Re: PipeNet protocol 
[Doh!  Did it again from the r00t.besiex.org account.  Sorry for the
Adam Shostack made the comment about insufficiency of padding in
PipeNet you're referring to.
I discussed insufficiency of padding in the _freedom network_ as a
traffic analysis countermeasure in email with some folks who can speak
up if they wish.  This is because freedom does not use the synchronous
approach for performance reasons and hence some active attacks remain
even if you had end-to-end padding between client and exit node.
End-to-end padding in the PipeNet synchronous mix-net does appear to
be sufficient to provide good security against traffic analysis, but
it has the DoS vulnerability and performance problems we discussed in
the last pair of posts.
It seems to me that with the current internet TCP properties you would
have to distinguish network congestion from DoS attempts, which is not
generally possible.  Even if the Quality of Service (QoS) protocols
were implemented and widely deployed, people still put backhoes
through cables or have catastrophic equipment failures now and then.
I suppose you could have compensation or insurance from the QoS
enabled service provider and use that compensation to compensate you
for the loss of the anonymity network good behavior bond.
Well if we look at the problem there are three properties we desire
the system to have:
1. high security (idealised resistance to traffic analysis)
2. performance (reasonable performance which doesn't degrade as the
   network grows)
3. DoS resistance (reasonable resistance to DoS -- DoS or network
   outages should be local and not take out the entire network)
It seems to me that we can have at most 2 of those.
PipeNet provides 1, but not 2 or 3.  Freedom provides 2 and 3, but is
vulnerable to some active atacks even with end-to-end link padding.
The other thing we could do is move content inside the network -- much
of the traffic analysis material comes from the fact the exit traffic
is in the clear.  For example if many web servers supported
connections from the freedom cloud using freedom protocol, and nodes
in the network did per hop padding using a modified PipeNet scheduling
algorithm where you would try to use PipeNet scheduling, but instead
of delaying, if a packet didn't arrive in time, you would send some
cover instead on a hop by hop basis.  Then you package this thing as
an accompanying apache server and encourage lots of people to run it.
That coincidentally sounds quite similar to what I described above.
However it doesn't seem you gain much from the recipient knowing who
the sender is, beyond the change in definition allowing you to define
that the class of attacks that require compromise of the recipient
moot.  The main difference which appears to help is that the recipient
is part of the network, and can be relied upon to run software.

@_date: 2000-11-07 20:43:31
@_author: Adam Back 
@_subject: CDR: Re: PipeNet protocol 
I think I phrased that badly.  The hop that didn't receive enough
packets on a link would create padding, but the padding would be
end-to-end between that hop and the end hop (the recipient node, be
that client or freedom enabled web server).
Clearly if you compromise enough exit nodes, you can then selectively
DoS links and observe the padding as the exit node must see it.
With freedom and to some extent with the freedom on servers variant
(my earlier comment quoted above) I think 3 hops is roughly the
maximum useful number of hops.  This is because it's a real-time
protocol and it's not end-to-end padded.
In the case of freedom it's simply not end-to-end padded, with the
servers variant it could be end-to-end padded, but the attacker could
compromise other nodes, induce delay and therefore persuade the node
to pad and then correlate the padding with the compromised exit node.
Hence in either case two compromised nodes gives you a lot of info.
With PipeNet's synchronous mix-net you don't have this problem, but
you have the performance and DoS problems.
I agree with this conclusion despite the ambiguous explanation -- the
attack is a little more active, and requires more compromises, but
still holds I think.  So this type of network is not secure against
too many node compromises, where-as PipeNet's synchronous mix-net has
more mix-net like properties.  However it offers reasonable security,
it should have reasonable performance, be resistant to widespread DoS
and have similar bandwidth overhead to PipeNet.
You could use traffic shaping (in any of the networks we discussed) to
reduce the bandwidth consumption in quiet periods as Lucky described
in the past based on slowly moving average, typical period
fluctuations etc.
There is another parameter in the previous comparison I missed off:
4. Bandwidth Cost
With the server variant of freedom you could also mix different
classes of traffic -- some with end-to-end padding and some with no
padding.  The padding is fairly expensive in bandwidth.  Also one
could have different route create and destroy rules -- PipeNet create
and destroy mixing will make route setup quite slow.  In this way you
could build a network which supports different performance, cost, DoS
resistance and security trade-offs.  You could presumably allow
PipeNet in the same network also.
There is some advantage to having more people using the same protocol,
because there will be limits to the cover provided to each other of
different traffic classes.  Weaker things could be eliminated from the
picture for example with higher level threat models, if the threshold
of compromised nodes or capability of attacker is above the weaker
higher performance routing protocol, but below that required to
compromise the high security protocol.

@_date: 2000-11-22 01:00:47
@_author: Adam Back 
@_subject: CDR: ZKS -- the path to world domination 
For some reason I didn't see Greg's message earlier and only recently
saw Declan's forwarded snippets on politech (I'm not currently
subscribed to politech).  The closing remark at the bottom of Declan's
post (from Declan) was "Neither Austin nor anyone at Zero Knowledge
replied to the above message."  My personal reason for not responding
was I didn't see the message.  Austin travels an awful lot, so I
wouldn't take a lack of an immediate response as acquiescence or an
unwillingness to respond.
The following is as always my personal opinion.
I'm going to skip over the reporting and speculation about sales
figures discussion and the little skirmish over that.
Cypherpunk opinions matter as cypherpunks are privacy and
crypto-anarchy related crypto technology critics -- the analog of film
critics in this domain -- the punters listen to them, reporters listen
to them.  And in Declan's case some reporters are able technology
critics themselves.
Another reason would be that freedom is a popularisation and
development of cypherpunk developed technologies and ideas such as
cypherpunk type I and type II remailers, alpha nymservers, PipeNet,
traffic shaping etc.  So it is entirely expected that the opinons of
the people who developed and thought about these original
technologies, and had ideas about how one might progress with them are
important.  Indeed a number of cypherpunks who were involved in some
of these implementations and discussions are currently working at ZKS.
Cypherpunks also has a pretty high clue factor on privacy and
anonymity technology so you'd want to listen to what is said and worry
if they were saying things which couldn't be answered.
I understand that, and offer the additional comments above.
There hasn't been as much comment (apart from Wei's comments, and some
offlist comments from Lucky) as one might expect about technology
choices and protocol design despite the open white papers.  I'm hoping
the new clearer, more detailed white papers coming with 2.0 will help
stimulate such discussion.
Let me clarify a few things about this extrapolation.
- freedom 1.x mail system used reply blocks.  There were a number of
problems with this reliability, usability and performance wise.  Some
of these were inherent to reply blocks (bit rot, and server churn
causes reply blocks to die), some of it implementation related (retry
semantics for mail forwarding), some of it to do with relying on third
parties for long term operational reliability (which reply blocks do
for you).
- freedom 1.x allows you to post to news but not to read news
anonymously (you have to use dejanews or some other news browser).  So
(You could read news non-anonymously by just using your ISP NNTP
server, but clearly there are problems -- an attacker could mark
messages you read and correlate you to your nyms that way.)
These two things mean that there are more people using freedom 1.x
browsing than freedom 1.x mail.  So you aren't going to see an
accurate portrayal of user base from email alone.
- freedom 2.x has an all new mail system, the workings of which will
be described in fair detail in a white paper which will be released
RSN.  Those playing with the beta will have observed this mail system
in action.  This new mail system is much easier to use, much more
reliable, and much faster.  I'd also argue that the 2.x mail system is
more secure as it doesn't use reply blocks which are inherently
vulnerable to subpoena attack.  But then I designed it, so I'll let
others critique it.  (There is forward secrecy at all stages in the
movement of mail in the new system, with maximum of 1/2 hour key
- freedom 2.x is also much more configurable so you can route other
protocols over the cloud, or existing protocols over other ports.
Some negative experience with it's workings?  Could you elaborate?
I offer the above explanation for the large imbalance between web and
email users in 1.x.  It's really quite severe.
My gut feel is that email would be a popular app for pseudonymity.
Opinions solicited of course, but I personally was usually more
interested in pseudonymous or anonymous mail.  It does actually matter
if you use the web to look up things you're writing about and you're
trying to be strongly anonymous, but typically I haven't been that
Anyway we'll see if there is a big pick up in mail usage with freedom
2.0, which will be the proof of whether or not the freedom user base
likes mail.
Web is probably perceived still as "relatively anonymous" for many
uses despite the realities of profiling and a fair degree of logging
of IPs, logins, and caller-ID by ISPs which can relatively easily be
correlated with phone records.
The integration mechanism with the mail system (and web, IRC, telnet,
ssh etc) works as a transparent local proxy is pretty painless, and
works automatically with pretty much any mailer with no user
configuration of the mailer.  Much smoother integration than even
emacs mail-crypt's nym support.  (I haven't looked at windows stuff
that much, but I'm pretty sure it's nicer than private idaho etc as
you get to use your existing mailer).  The linux client is nicer than
premail for pseudonymity too.
This isn't a re-framing, it's phase II, and it's been planned since
day one.  Austin has been talking about being a privacy broker between
users and companies for years, it was part of the grand plan for
"total world domination" since the early days.  Probably some have
heard him speak about it at conferences over the last couple of years.
In this model you're trying to build a privacy architecture in which
users can conduct business privately.  So clearly involving businesses
is a good idea to enrich what you can do.  You're just starting to see
that with phase II.
The press release was kind of sloppy because it had lots of "all new"
claims about Managed Privacy Services (as well as the reference to
"split keys", which was actually trying to talk about reply blocks).
Reading it one would tend to come away with a very disjointed view.
But as I said actually MPS is only "new" in the sense that phase II of
the privacy architecture plan has been gearing up for a while now.
But it's all part of the big privacy architecture picture that ZKS is
trying to build.  So this means for example people using freedom to
conduct business pseudonymously and so on.
While the press release leaves one with a disjointed impression, it's
misleading.  Neither the "Zero Knowledge, after poor software sales,
tries new gambit" summary and title Declan came away with after
reading that press release, nor the extrapolation of users from the
observed mail usage are accurate pictures as I explain above.  They
are probably reasonable conclusions to draw from the available
information, but the available information was misleading and
incomplete respectively.
Austin quoted by Greg:
That's not the way I would express the effect of the changes in the
protocol, though it is an accurate description of understanding about
traffic analysis at the time the decision was made.
More recent understanding, as we examined how to strengthen the threat
model is that the existing attacks are not all prevented by the
original high bandwidth overhead link padding scheme.  In fact it
would appear that the padding does not even offer much in the way of
additional protection because a powerful attacker can with similar
resources to without the padding still engage in active attacks and
timing attacks to achieve similar result.
It's as strong as we could make it.  Private interactive
communications are a hard problem.  As Wei and I were discussing in
the "PipeNet protocol" thread in the last couple of weeks, there are 4
main properties you're trying to optimise over:
1. security (resistance to traffic analysis)
2. performance
3. bandwidth efficiency (cost)
4. DoS resistance
It appears pretty hard to get more than one of these properties with
theoretical optimality.  PipeNet gets the first one with good
theoretical security, but none of the others are good.  Freedom makes
an engineering tradeoff which does reasonably on all 4.
If anyone has anything to suggest about how freedom protocols could be
improved in any of these criteria, or how one could build a hybrid
based on PipeNet, freedom or dc-nets, or other new ideas, I'm always
interested to discuss.
Lucky had some comments in email about padding, however as I discussed
with him the padding costs bandwidth without defending against similar
cost attacks.  The other similar cost attacks do not appear to be
possible to defend against without using PipeNet or DC-net properties.
I'd invite Lucky to resume this discussion publicly as he is quoted by
Declan stating ZKS didn't make freedom as strong as we could have:
Continuing, Wei's PipeNet has some pretty nice security properties,
but it's hard to deal with the performance and DoS resistance issue.
PipeNet effectively deals with the traffic analysis problem by
shutting down the entire network immediately if any active traffic
analysis attempts are made.  It doesn't appear to be possible to
distinguish between active traffic analysis attempts and network
congestion or modem drops, so it also would suffer from poor
performance and unreliability.
DC-nets are nice too but bandwidth cost is probably prohibitively
high and DoS (disrupters) are a problem there too.
We're working on the traffic analysis problem trying to optimise this
I hope the above can start some discussion of strength against traffic
Note v2 has not shipped yet except in beta form.  The white papers are
being updated to ship before or with v2, including the new mail system
white paper.
I think we can more robustly defend the freedom protocol than that.
It's pretty close to the best you can do practically with current
state of the art and knowledge about defending against traffic
analysis.  That's a fairly aggressive statement with a practical
deployed system due to all the issues that come up with engineering
tradeoffs and complexities of actually developing such a complex
So as I say it's not because we've decided not to bother, it's because
when you actually look at the engineering issues, and the traffic
analysis attacks, it's harder than one might predict to start with.
Now I think this is a concern for everyone because with strong crypto,
mathematics is on our side, and we can effectively laugh at USG's
earlier attempts to put the genie back into the bottle.  They lost
that one.
But anonymity systems, particularly interactive ones, don't appear to
offer near as steep an advantage to the defender vs the attacker.
So I'd encourage people to think about the above described problems,
because in my view it is a problem that matters for crypto-anarchy. The section of the FAQ that covers the questions you're asking is:
The short answer is no, no, and very.  But with the caveat that this
is a relatively complex system, and despite our best efforts at
auditing code, and protocols, publishing protcols for peer review,
hiring third party auditors (counterpane) there may be bugs.  This is
to my mind the most important aspect of open source -- so people can
review what it does, and compare that to what the white papers say
it's intended to do.  I'd encourage people to help review the code in
the same way that PGP was scrutinised.  Also note the known issues
with the protocols and with the current implementation are in the
security issues white paper.  This is being updated for 2.0.
I'd just like to make these two comment commitments which I'll reveal
later when certain projects are announced to demonstrate that they
were planned for some time.
b26ecfce97bc6c090585a254a297ba5143280cce commit
a47d3b46da014002b34d02c3a0524a3209c3c6ae commit2
(They have big random nonces in them, so don't even think about

@_date: 2000-11-22 23:12:19
@_author: Adam Back 
@_subject: CDR: link padding & traffic analysis (Re: ZKS -- the path to world domination) 
So the attack you described without link padding involves the attacker
- eavesdropping on all links into and out of the network (untargetted
- or eavesdropping on a targetted or suspected user
  - and then eavesdropping on all exit nodes
  - or, on selected web sites you think the user visits
- or trying to work backwards from an exit node or web site to a user
  - by eavesdropping on entry nodes
If the attacker can observe those things, he can observe the effects
of network congestion because freedom is not synchronous.  (Making
freedom synchronous would adversely affect performance and make it
much more vulnerable to DoS.)  The attacker therefore already has the
necessary taps to match up congestion patterns between the client to
first hop link and the exit node or web site request.
If congestion is too noisy or uniform to discern anything, he can go
create some selective congestion by ping flooding (or inducing more
plausibly deniable types of congestion) on that link.  And observe the
effects.  He can very easily hide the source of his congestion
attempts by creating it using the freedom network.
The attacker can also substitute the need for extensive network taps
by measuring congestion inside the freedom network to narrow down
where a user is coming from.  Any unpriviledged user can measure
congestion in real time using the freedom protocol by creating routes
over all combinations of hops, pairs of hops and triples of hops.
This may allow a user to work backwards from an exit node to an entry
node with only one network tap.
So the main cost increase for the attacker of link padding is the
complexity of the analysis.  But it doesn't add many bits.  It would
probably be easiest to do what the Onion Routing analysis used: plot
some graphs of the timing data.  You would probably be able to
visually see the traffic analysis patterns.  Automating the analysis
would be a little harder due to the noise.
Next the cost of link padding.  Clearly it costs.  ISPs use "bandwidth
aggregation" figures to estimate their bandwidth requirements based on
online users.  This figure means what percentage of their modem
bandwidth a user uses while online (not 56 kbits on a 56kbaud modem
because the user pauses between downloads etc).  This figure varies
and changes over time.  With a bandwidth aggregation factor of 6:1 it
would cost 6 times as much bandwidth.  On top of that you don't want
to change bandwidth utilisation in response to demand or you leak
information about routes opening and closing.  So you need to have
some surplus padding available for users to use as they open
So how does one fix it if link padding alone doesn't work?  As I
suggested doing this efficiently and practically is an open problem.
PipeNet style synchronous works, or latency padding (which comes to
the same thing), but as discussed earlier these have pretty severe
performance implications.
So the challenge is, to find a more efficient hybrid or alternate
design which retains as much as possible PipeNet or DCNet security
properties, but improves performance and reduces bandwidth costs, if
such networks exists.

@_date: 2000-11-24 01:16:34
@_author: Adam Back 
@_subject: CDR: Re: ZKS -- the path to world domination 
Freedom will do that as it'll hide your IP address and ISP email
Potentially freedom can handle this, though it depends on the system.
Freedom works with IRC, but not yet with ICQ.  The issue is that you
need a anonymizing local proxy if the protocol violates layering and
includes IP addresses in it's higher level messages.  Some protocols
require this (eg. ftp due to the announcement of the port and IP to
connect back to in the request), others don't.  As the source code is
out (for linux) people can add handlers for their favourite
applications (Quake, ICQ etc).
Difficult to protect against this one.  One could think about applying
some of Nick Szabo's ideas to this as it's effectively a private
contract issue -- some of things I talked about in my recent post
about "smart privacy policies"
Your best defense is not to give the info.  Or perhaps to give it
pseudonymously if that is possible.
Freedom's approach to profiling is to let the cookies accumulate on a
cookie jar associated with each pseudonym.  You can also edit cookies.
In this way you still get advertisements more targetted than "random"
(which might be thought of as positive if the user is comfortable
being pseudonymously profiled).  Also you can use multiple persona's
to segregate your interests (financial interests, porn, etc.) which
reduces your chances of the profiler getting a unified profile -- he
gets separate profiles for each of your activities.
That's addressed by freedom as packets are all encrypted (modulo the
traffic analysis attacks discussed in the previous post).
Also protected for traffic.  Mail is protected in the new mail system
as the stored mail is encrypted by the sender, and transfers into and
out of the mail system are anonymous by being routed through the
freedom network, which uses forward secret keying.  The 1.x mail
system also protects against it as the mail is encrypted end to end,
and arrives in the ISP box encrypted.
That would be a nice thing to clean up.  The browsers keep a lot of
state, in history, pick-lists, cookies, bookmarks etc.  The whole lot
could do with storing in an encrypted per local user profile, or
having an option to wipe.
Even the machine keeps a lot of stuff -- the windows pick list, stuff
scattered all over the disk in deleted files etc.
I made a suggestion I think on cypherpunks a few months back that
freedom and anonymous systems meeting the type of requirements you
list could be thought of as legal insurance from bullshit legal
You could probably collect a list of freedom exit node IP addresses
and look for web hits from them?  The hit rate will depend on the site
topic and the user group, so it'll still be pretty hit and miss.
Freedom's trying to do some pretty ambitious things in interfacing
with the windows stack from within the tcp stack and transparently
re-writing and redirecting packets at that level.  That area of
windows isn't the best documented.  If you were using an early version
things may have improved a lot since then.  Also I think win2000 stuff
is more amenable to the things freedom is trying to do.
Well I would avoid using the web at all if I were doing something
sensitive.  That could be inconvenient.  I guess there are web2mail
gateways one could use with an alpha nymserver, but that's pretty
inconvenient.  So freedom is good for that (now that there's a linux
The managed privacy services are technology related and pushing the
"zero-knowledge" stance.  ZKS technologies include the freedom
network, and the freedom client.
Yes they would.  Sorry to be ambiguous.  You asked the question with
"you" which sounded like ZKS, and ZKS couldn't in general decrypt the
whole reply block (it would depend how many of the hops where ZKS
nodes).  Note there are multiple reply blocks -- default 3 -- to
combat reliability and bit rot, so given the mix of ZKS nodes someone
could end up with an all ZKS reply block.
Yes.  I mentioned this in the previous mail as a highlight of the new
mail system in my view.  That plus recording any traffic coming frmo
the users machine is all protected by forward secret encryption with
freedom 2.x.  No keys protecting the outside layer are kept for more
than 1/2 hour, and then only in RAM.
It wasn't written by legal types -- Adam Shostack & Ian wrote it.  I
take it there are some legal inaccuracies in the description of legal
process?  Perhaps one of the lawyers should review those parts.
Disclaimer: as always these are my personal comments.

@_date: 2000-11-27 23:47:47
@_author: Adam Back 
@_subject: CDR: Re: ZKS -- the path to world domination 
Greg wrote earlier about ZKS' Managed Privacy services:
Well ZKS should have an interest maintaining a good reputation for
acting in the interests of users privacy.  Companies who use such
services should also have an interest in using services of companies
with good privacy reputations -- as this would tend to give better
consumer confidence in the resulting systems.
I guess the only answers are maintaining professionalism, and
integrity and to maintain a strong stance on users privacy, with clear
long term objectives (avoiding short-sighted small incremental
improvements which may stay for a long time just because of the fact
that built working systems don't get replaced as long as they continue
to function).  Openness would be a guiding principle too I would think

@_date: 2000-11-28 01:03:20
@_author: Adam Back 
@_subject: CDR: Re: Internet anonymity/pseudonymity meeting invitation 
I don't think anyone will be demanding government ID.  Short of a ski
mask though attendees might get to see what you look like.
John did have some things in the organisation stuff about pseudonymous
participation.  If it's still in there you might find it here:
Yes it's still there:
Although I'm not sure the restrictions are possible except for people
who physically attend meetings, but I'd guess the intention is clear
enough: allow pseudonymity, encourage pseudonymous people to disclose
conflicts of interest.
Well you could always use freedom on the web archive :-)

@_date: 2000-11-28 22:41:07
@_author: Adam Back 
@_subject: CDR: ecash, cut & choose and private credentials (Re: Jim Bell) 
[Hey Hal, what happened to your Chaum's ecash description?  Can't find
it to link to].
Here's a short description of Chaum's [2] ecash and credentials vs
Brands' [1] more general and flexibile private credentials and ecash.
So as both anonymous and Ray know cut and choose refers to the method
of encoding identity into coin.  The basic problem is that Chaum's
blind signature doesn't the signer see what he is signing.
So cut and choose is just the user encodes identity in a number of
candidate coins, and the bank chooses one at random and the user must
reveal the rest.  This allows tunable probability of cheating (not
encoding identity in the chosen coin).  This is done in such a way
that a coin showing protocol will reveal the identity if the coin is
spent twice (due to random values chosen by the merchant resulting in
two simultaneous equations with two unknowns -- one of them the
Cut and choose is computation and communication inefficient.
Brands' private credentials stuff uses a technique he calls secret key
certificates to allow the issuer and the user to both see the
attributes being signed.  The user ends up with a certificate on the
attributes and the ability to prove the validity of the certificate
during a certificate showing protocol.
The user can also choose to selectively disclose attributes during
disclosure, for example if there are two attributes he can reveal one
(the coin denomination) and not the other (his identity).
With Chaum's ecash you have to use the cut and choose protocol which
is inefficient, with Brands' protocol the user and the issuer engage
in a protocol with 3 moves (Discrete Log based variant) or 2 moves
(RSA based variant) which is both computation and communication
With Chaum's ecash you have to encode the coin denomination by having
a separate public certification key for each denomination.  You don't
have to do this with Brands' private credentials, as you can put this
in an attribute.
Also the number of attributes is arbitrary, so if it makes sense to
have more attributes in the general private credential case, you can.
(For example sex, age, nationality, passport credential, etc. you can
reveal any combination you choose after certification.)
You can also show fairly arbitrary boolean expressions involving the
attributes and not reveal anything other than the truth of the
expression.  (Eg. Female US citizen over 60 or Male Canadian citizen
under 18, and not reveal sex, citizenship or age).
The certificates are linkable (ie the verifier or merchant can tell
that you are the same pseudonymous person who last showed the
certificate), because there is a public key chosen by the user and the
public key is revealed during the show protocol.
There is also a refresh protocol where the user can give a used
certificate and get a fresh certificate without having to show the
attribute vales of the certificate -- the certifier just knows he is
certifying the same as last time.  The refreshed cert would be
unlinkable at show time from the original cert.
For ecash purposes you can use private credentials to make both
offline and online cash.
So private credentials are nice and flexible for building generalised
private credentials, and also more flexible and so allow you to do
some things more efficiently, and do some things not directly possible
with Chaum's credentials.
Unfortunately both Brands' and Chaum's ecash and credential schemes
are patented.  David Wagner et al also had some ideas about an ecash
coin [3] composed roughly of a public key based MAC (ie the user can't
verify the validity of the coin directly -- only the bank can do
that), plus a zero-knowledge proof that the bank hasn't marked the
coin.  This may be unpatened in that it's not directly a certificate,
it's a MAC, plus a zero-knowledge proof so it seems like a fairly
different process.  I don't think you can do efficient offline ecash
with Wagner et al's mechanism -- I'd guess it's more comparable with
the functionality offered by Chaum's blind signature.
There is a high level white paper describing the applications of
Brands' private credentials and comparing to Chaum's credentials:
at the bottom Brands Private Credentials White Paper.
[1] [2] Hal Finney used to have a description of Chaum's protocol on rain.org
but he's at  now and I can't find the link.  (Cc'd
to see if he still has it.)  Actually I did once see a description of
Brands stuff by Hal also... still have that too?
[3] Ben Laurie has a paper describing Wagner et al's MAC + ZKP ecash /
credential protocol as theory2.pdf.
Disclaimer: As always my comments are my own.

@_date: 2000-11-28 23:50:43
@_author: Adam Back 
@_subject: CDR: Re: ecash, cut & choose and private credentials (Re: Jim Bell) 
Hal says:
 and
Wow look at the dates on those files -- Oct 93, and we still no
deployed ecash.  You'd think there would be a market there for porn
sites alone with merchant repudiation rates, and lack of privacy in
other payment systems.
 has some blurbs about "solutions", a few "demos" --
actually shockwave animations I can't view under linux -- and a few
press releases about deals -- but no indication of where would could
go to download a client or obtain a real account.
Another interesting and related use for ecash would be file
distribution systems which use economics to resist DoS, and give
people an incentive to run them, and profits to fuel the scaling of
the system to scale.  Examples are Mojonation's mojo and some of my
and Ryan Lackey's earlier musings about eternity / cypherspace /
distributed content sharing.
So in the mean time we have privacy less things like paypal apparently
getting reasonable adoption.
I was going to read about visa cash -- but more fscking shockwave --
the frontpage is shockwave no less so you get zip information out of

@_date: 2000-10-07 19:50:10
@_author: Adam Back 
@_subject: CDR: Re: stego for the censored 
So what you need is wide scale deployment of the stego decoder in some
otherwise popular software.  Eg. talk ID Games into putting a stego
subliminal channel in doom computer generated character movements so
network doom players can talk to each other steganographically by
playing a game and typing in some "cheat mode" to switch the mode on.
(The stego channel is the RNG).

@_date: 2000-10-31 23:08:49
@_author: Adam Back 
@_subject: CDR: ZKS "Smart Privacy Policies" 
[Sent this once from a dud address trying to work around a mail
problem -- apologies for duplicates]
This is referring to me right -- as I was involved in the big fight
about NAI's corporate message key escrow proposal?
Disclaimer: note I work for ZKS now, below are my personal opinions.
It's not key escrow, and it's not building tools for key escrow.
Yeah but the company's goal should be to satisfy users that there is a
reason to trust the solution, and ZKS has brand recognition for
building "trust no-one" (or at least distributed trust) solutions.
I agree with this argument.  I wrote up some GAK-resistant design
principles during the NAI/PGP key escrow argument
( in an attempt to get
people thinking about the social implications of protocols they write

@_date: 2000-09-03 11:49:16
@_author: Adam Back 
@_subject: CDR: auditable gaming PRNGs (Re: PRNG server) 
Seems to me you can do better with a gaming server.  If the gaming
server servers RNGs in a sequence such that each sample in the
sequence can be verified, they don't need to trust the server; or at
least there is an audit function.
Eg. say that the server publishes subsequent pre-images in a
h_0 h_{i+1} = h_i
and the server computes h_i values up to i = 10^8 and then publishes
them starting with h_{10^8}, h_{10^8-1}, ...
Then anyone can verify that the random number is the preimage of the
previous random number.
You do something similar with a more efficient (log(n)) auditing
function with merkle authentication trees.
If they aren't doing this someone should clue them in.

@_date: 2000-09-03 11:54:10
@_author: Adam Back 
@_subject: CDR: export reg timewarp? (Re: RC4 source as a literate program) 
The US export regulations no longer prevent export of crypto.  PGP
exported binary copies of PGP from US websites, as now do many other
companies.  Crypto source is exported also from numerous web sites.
I don't follow why all the discussion talking as if ITAR and EARs were
still in effect in unmodified form.

@_date: 2001-08-03 11:33:01
@_author: Adam Back 
@_subject: (file sharing) morpheus rules! 
If any were looking for a replacement for napster since
it buckled to pressure from RIAA, it's here: morpheus.
It's distributed like gnutella, but it scales, it has
fast searches, downloads work, and are not sensitive
to individual machines being switched off mid download.
It tries to download from network topologically close
machines.  (It downloads from multiple machines at
once to speed up download, and so that one machine
can go down without losing the download).
The architecture is a self organising network which
promotes some peers to being super-nodes based on
their bandwidth.  Super-nodes act as search hubs,
which avoids the gnutella melt-down which arises
due to their broadcast searches.
I've been informally plotting the growth of morpheus
for the last 3 weeks, and I figure it stands a fair chance of reaching 1 peta byte (1000 Terabytes)
in storage by the end of this month.  Last seen
with 600,000 simultaneous users sharing 50 million
files and 300 Tb of data.
windows only but worth rebooting into windows for.
on a side note napster deservd to die -- it's like
evolution for file sharing networks.  It's central
point of failure and central server involvement in searches made it too vulnerable to legal attack.
Gnutella wasn't but didn't scale, morpheus appears
to be scaling and if anything performance is improving
as the data density gets higher so the sharing surface
can give you the content you want from closer and closer
nodes.  There may be an inflection point where it
starts to really take off as the number of users
is still improving the usability, performance and
variety of content.

@_date: 2001-08-07 17:14:51
@_author: Adam Back 
@_subject: (file sharing) morpheus rules! 
It is closed source and closed protocol.  The closed source issue is a minor
problem, someone will reverse engineer and re-write it, or design a new
incrementally better file sharing protocol.
The reason for enthousiasm is the demonstration of feasibility of a number
of open questions in scalability and performance which arise after observing
gnutella.  The fact that morpheus has reached the level of scalability it
has, speed of searches, and download speed is interesting because it gives a
lower bound on what's possible.  That it is possible is good news for the
future of file sharing.
Empirical evidence gathered by trying to use it over a number of months.  It
starts slower, searches are slower, and downloads fail a lot (~90%) of the
time, resumes don't work as well, resumes can't switch source, it doesn't
download from multiple sites simultaneously, plus morphus has a number of
really good client GUI features which positively affect the richness of
meta-information available for searches.
While some of the excuses given here are undoubtedly valid, the primary
problem -- broadcast searches -- is not addressed except by a not widely
used and manually configured fast host acting as a proxy shielding a slow
host from search requests (if I interpret that right).
Morpheus on the otherhand automates it's solution to the general issue of
different host speeds and scalability of searches -- self organising
selection of super-nodes -- and does many many other things better than
gnutella, and discards it's speculated intra-version compatibility issue
effect on performance by virtue of there currently being only one version
(or more properly the versions all rely on the same comms layer library).

@_date: 2001-02-01 22:51:32
@_author: Adam Back 
@_subject: CypherPunks anti-License (CPL) 
Some may recall the "cypherpunks license" debate back in 1998 and
continuing for a few hundred posts with contributions from Richard
Stallman, Eric Raymond, Eric Young, and numerous others.
It starts around here (a little before, but the threading doesn't go
back past subject line change).
(The search tool on  doesn't
seem to work for pairs of words.  But google will find lots of
"cypherpunks license" on inet-one.)
So I was releasing some source code I wrote a couple of days ago, and
it struck me that perhaps it might be fun to try doing what was
discussed back then, and put a license on it which explained the
motivations and made the thing a little more free than putting it in
the public domain.  (I so far have put no license at all on anything
I've released).
So here's a cypherpunks license proposal.
Generally it's a statement of how things would be if there were no IP
laws governing copyright and related licensing, and an attempt to
allow people to act as close to as if such laws did not exist given
current laws.  In addition for people who publish anonymously, and
consume anonymously it avoids making silly claims based on force
monopoly backed legal constructs which couldn't be applied to an
anonymous actor.
It tries to say:
- you can do what you want with this, and I can't change my mind about
  these terms for this copy
- I won't attack you in court for IP related laws
- you can redistribute it under other licenses
- you can claim you wrote it
- you can advertise it anyway you like
- copyright isn't compatibile with privacy, and anyway needs to die.
- you don't have to send me patches
This is not to say a distributor would necessarily like all of these
things but it acknowledges that he would not able to enforce these
things in an IP law free world.  Instead such requests are recognised
for what they are -- requests only.
I'm not sure about the implications of the fact that the author could
change his mind, and/or distribute different copies of the same under
different licenses.  Presumably if someone has a copy you released
with the statement that it is in the public domain, and that you won't
revoke that, you can't then do anything to them in a court afterwards
relating to their use or further distribution.
The acronym is a little bit of a stretch, but I thought it would be
more easily fit into the GPL, MPL pattern and so be recognised as to
do with licensing.
It struck me after a while that it was not a license because a license
presumes you have some right to exert control.  So I used
anti-license.  It's a notice that the work is placed into the public
domain together with a pledge about behavior to expect from the
Also notice that the person making the pledge is not called the
licensor (as it isn't a license), and also not the author as the
distributor doesn't have to be the author.  The CPL presumes people
will distribute collections of things where they are not the author,
copyright holder, and are barred under existing laws from distributing
There's also this crappy law against illegal contracts, which may
easily affect this in some circumstances.
Also there is the requests clause.  It tries to say that the CPL
distributor won't enforce in court his requests.  This seems
reasonable enough otherwise that would violate the spirit of the CPL,
and would anyway not be a request, but more a threat.
And of course the CPL is itself distributed under the CPL, so anyone
can change it, call the changed versions whatever they want etc.
I suppose the only reasonable request to place on the CPL would be to
mark modified versions as modified to avoid confusion, so that people
know what they're referring to.
Well here we go... comments solicited.
Below in text, or here:
      (I may edit this as things occur to me, or people comment on things).
Cypherpunks anti-License
The intent of the Cypherpunks anti-License (CPL) is to inform users
that they are free to use and redistribute the indicated work or any
derived or modified work in any manner they choose. Works distributed
under the CPL are in the Public Domain.
The CPL is not a license, it does not require the user to do or not do
anything; the user does not agree to any terms, because there are no
terms, and the user does not need to do anything to indicate
acceptance or rejection of the CPL.
Non Litigation
The CPL serves to pledge to the user that the distributors will behave
in a manner consistent with the non-existance of Intellectual Property
(IP) laws as far as they are able. The distributors will not use or
participate as far as they are able to government legal systems to
attempt to enforce requests restricting the use, modifications, or
redistribution of the work for perpetuity. The distributor may prefer
to be anonymous to preclude attempts to coerce them into enforcing IP
laws relating to this work against their will.
The work may be distributed with some distributor requests in addition
to the CPL. The distributor pledges similarly to not attempt to use IP
laws to enforce these requests.
Users choosing to redistribute this work may change anything about the
work, including distributing it under a different license, and adding
or removing previous distributors requests.
The CPL is completely liberal. Here are some examples of implications
of this which are not true for many licenses. The user can
redistribute the work or a derived or modified work
   * under a different license of their choosing
   * with or without source code as they choose
   * without acknowledging the distributors or authors
   * with false or innaccurate claims about authorship of the work
   * advertise without acknowledging the authors
Requests can be arbitrary, but are requests only. Example of requests
that the distributor may choose to make:
   * that improvements to the work be drawn to the distributors attention
   * that improvements to the work be released back to the distributor      under the CPL
   * that the distributors name not be used to advertise derived works
     without the distributors approval
Legacy Considerations
The distributor may choose to inform the user of his opinion of the IP
status of the work, for example by identifying any IP law restricted
aspects such as the copyright holders of parts or the whole of the
work, trademark owners of trademarks used in the work, potentially
applicable patents on algorithms or ideas contained in the work, but
the distributor is not obliged to do so and takes no responsibility
for the accuracy of such information.
The CPL is written from a mindset which derides the very concept of
Intellectual Property restrictions as being incompatible with a free
Cryptographically assured anonymity and anonymous use of Internet
resources mean that denizens of cypherspace can ignore copyright,
licenses attempting to control use and distribution of works, and
patents on ideas. It is not possible to enforce IP laws by calls to
government legal systems when the flaunter is strongly anonymous.
The enforcement of IP law and anonymity are in direct conflict. To
fully enforce IP laws, anonymity would have to be
outlawed. Cypherpunks believe this would be a bad thing, because
control of information imparts power, and anonymity gives individuals
control over disclosure of information about themselves and their

@_date: 2001-02-05 22:45:20
@_author: Adam Back 
@_subject: IW: Tools Stunt DoS Attacks 
This sounds like just a short term work-around, easily countered by
the DoSers.
Rather than fix the problem, they propose to try to detect "unusual
activity" and block the IPs.  I'm not sure what "trace" means either

@_date: 2001-02-10 11:24:17
@_author: Adam Back 
@_subject: ZKS mail system questions (RE: anonymity) 
I think the 2.0 mail system has a number of advantages over the
reply-blocks based 1.0 pseudonymous mail system, though a couple of
disadvantages.  These trade offs are documented in the mail system
white paper.
In summary for people who don't want to wade through a wide paper, ZKS
version 2 mail system is a POP and SMTP server you connect to
pseudonymously via the freedom network and therefore tunnel your SMTP
and POP sessions through the freedom cloud.
The system is put together with Dan Bernstein's popular qmail high
performance mail system to build ZKS' mail hub and pop account system.
The differences are that ZKS has a number of qmail modifications to
deal with pseudonym to internet and internet to pseudonym mail.  (To
encrypt incoming mail for the pseudonym and a few other things to do
with authentication).  The details are in the white paper.
In the case of the connections between the user, the nodes in the
freedom network, and the ZKS mail system the traffic is all routed
over the internet.  The nodes in the freedom network are operated by a
mixture of owners.  Some ISPs, some individuals interested in privacy
and some ZKS operated.  The user can choose which nodes he trusts.
There is a diagram in the mail system white paper showing the internal
communications inside the mail system.  However these are just
messages passing information between the mail system cluster of
machines and are inside a firewall.  Conceptually you can consider
this cluster as a single machine owned by ZKS.  The point is if you
are connecting pseudonymously you have limited need to trust ZKS mail
system because it doesn't know who you are, and further, nym to nym
mail is end to end encrypted.  The mail server sees nym to internet user and internet user to nym
mail bodies, but so does any passive eavesdropper sniffing packets
entering and leaving the mail system as the internet user has no
compatible client software, unless the nym and internet user use end
to end encryption software such as openPGP or S/MIME.  The mail system
does encrypt internet to nym mail for the nym's key so it doesn't have
data it can read after the fact, although clearly it could record it,
at least it protects against after the fact requests to decrypt mail

@_date: 2001-02-11 13:07:17
@_author: Adam Back 
@_subject: watermarking sucks (Re: stego for the censored II) 
I think a watermark is more likely to be keyed.  Without the key
it's a bit harder to be sure to remove the watermark without affecting
the quality of the image.  With the key you can tell which parts of
the message to remove. In addition watermarking has to mean a mark tied to the identity
of the downloader, otherwise a simple presentation serves to
demonstrate it is the same work.  Ideally the watermark should
include a signature by the downloader, otherwise the watermarker
can forge watermarks and frame random people for unauthorised
redistribution.  I'd guess most of them don't bother with obtaining
signatures from downloaders due to the sparse independent PKI
(there is limited point the watermarker acting as a CA in it's
own PKI, as it can forge certificates and identities in it's
own controlled CA).  Though not equitable the systems relying
on the watermarker not to cheat is probably not too much of a
problem from the point of view of the watermarker because courts will just decide in favor of the trustworthiness of the
media cartel when it is case of one persons word against the
cartel. Of course the whole concept of watermarking is broken at all
levels, copying can not be prevented as the content can typically
be reencoded and lose the watermark, quality is in heavy contention
with the ease with which the watermark can be removed.  Even if it is keyed.  And ultimately content can plausibly deniably
be stolen and all it takes is one copy.

@_date: 2001-02-11 23:57:42
@_author: Adam Back 
@_subject: privacy regulations suck also (Re: Formal apology) 
I've been thinking about the current trend in privacy regulations
also.  I came to the same conclusion.  My bank sent me a shiny new
leaflet explaining their privacy position.  It wasn't even an
especially desirable or equitable position, but they presumably felt
the new regulations obliged them to write it.  I'm thinking: "so how
do the laws that caused this leaflet to be written help my privacy?"
These laws are almost exclusively about *handling* of data, rather
than questioning the fact that the data is collected in the first
place.  (Well there is a principle that they should have a reason for
collecting it, and/or that they get consent, but they do have some
reason to have pretty much all the data they collect by their
So here's the problem: these laws will if anything make it less
visible what information companies and governments have on you because
they will restrict uses.  How the data is handled and used isn't the
problem, the problem is that the information is collected, and
available to law enforcement, national intelligence and your average
dick (private detective).
Privacy to me means being able to keep my affairs private from
governments if I choose.  The UK princple allowing you to use any name
you want (so long as it is not for committing fraud or a crime) is
agood one.  (I'm hoping that using an alias does not affect the legal
systems evaluation of the severity of the crime -- and that there are
no "use of an alias in the commission of a crime" types things in
effect though I don't know the details).
So with this definition of privacy, the actual problem is the
existance of a whole raft of laws outlawing privacy.  New laws
governing use are window dressing.  I don't even care about the use
typically, junk mail is easy to throw away.  Think about for example
anti-money laundering laws vs the desire for financial privacy in a
free society.  The requirement to show and present government issued
ID for all sorts of things in society and so on.
So the solution appears to be technological countermeasures, and
repealing laws.  Neither of which appear even remotely likely within
the political system.  The political system has a systemic desire to
create more laws.  Every new law introduces more problems.  The people
writing the laws don't know the technology, they are control freaks,
and pander to media and take bribes and broker favors with special
interest groups.  So at this point I firmly believe in "write code not
laws", and think that "cypherpunks write code" is important. btw. The main reason I have not indulged overly in political
discussions for some time is that I resolved to not even spend the
time to read or keep up to date overly with government and legal
system intrusions into privacy.  The historical predictor that
whatever they are doing it's bad for privacy and the balance of power
is sufficient information for code writing.  Reading the crap is just
frustrating.  Time better spent writing code.
So the question of what code to write is the consuming question these
Actually you're pretty far above average first post in
insightfullness.  Welcome to cypherpunks.
(btw "Mr" etc is more formal than normal here, and you never know if
it's correct half the time -- I've come across at least two people who
I had presumed the wrong sex for.  Nyms, handles email aliases -- just
refer to people by whatever they refer to themselves by.  Some don't
even give a handle.)
Personal opinions of course.

@_date: 2001-02-12 21:37:33
@_author: Adam Back 
@_subject: Here you have, ;o) 
Heh, heh.  Guess who uses outlook :-)
Endless source of amusement as a linux user watching the VB
script worms play out.  I think you actually have to click on this one, though the double extension helps as many
users won't see the 2nd .vbs, just the .jpg.

@_date: 2001-02-15 22:39:17
@_author: Adam Back 
@_subject: Why Gnutella Can't Scale. No, Really 
Jordan Ritter (napster co-founder) wrote this, which RAH forwarded to
the DBS list.
So I'm presuming why people compare gnutella favorably to napster is
because napster is flawed at a different level: it is not robust
against legal attack.  The very recent developments highlight the
fragility of the napster model.
This is not a criticism of napster or gnutella, but comparing them
directly is difficult, gnutella is trying to do a much harder thing.
It is typically much easier to design, and build hierarchical systems.
Fully distributed algorithms and protocols with good characteristics
are hard to design.  But distributed systems _can_ be designed, and to
some extent their performance and scalability predicted.
So Gnutella has it's problems.  However I wouldn't say that
distributed sharing can't work, just that the first version of
Gnutella wasn't very scalable.  There are other systems: FreeNet,
MojoNation, which perhaps are better.
For one set of commentators the interesting factor is the ability of
the distributed document space to be censor resistant; it's one major
part of the application which generates the hype.  So Napster doomed
itself by being scalable enough to get enough users that it generated
enough press (mostly centered around distribution of arguably
copyrighted materials) to make the media industry notice it and the
predictable result follows.
It may be that Napster will continue in the form of the open napster
clones, even if Napster the company is prevented.  Anyone can start
napster servers at this point.  So this is the short term easier thing
to do: create lots of reasonably large individually hierarchical
napster clones.  We know the web is reasonably censor resistant, web
pages get shut down, but the content resurfaces at other sites, in
different jurisdictions, etc.  As long as there is an interest (and
there clearly is), new servers will pop up faster than the censors can
shut them down.
Personal opinions only, of course

@_date: 2001-02-18 11:09:19
@_author: Adam Back 
@_subject: Slashdot | Web Standards Project: Upgrade, Or Miss Out 
Why should we care about this?  It might be vaguely interesting, but it has nothing to do with cypherpunks?   Right?

@_date: 2001-02-25 00:17:26
@_author: Adam Back 
@_subject: other file sharing apps (Re: OpenNap Server) 
Also imesh ( seems to work ok.
No details of how it works, but it has a reasonable selection
of music (and porn videos by the look of it) and is faster
than gnutella.
I guess a big thing going for distributed file sharing is the
interest level.  These napster a-like people (imesh) claim to
have 4 million users.
If they get closed down next there are lots of others.  I
figure the thought police have lost already.

@_date: 2001-02-27 03:03:47
@_author: Adam Back 
@_subject: legetimised (or uncontrollable) piracy or info-market of the 
Mon, Feb 26, 2001 at 10:32:05PM -0800
If we get to the situation where ISPs want people to
use their bandwidth because they're getting paid for it,
it makes sense for the ISP to give a kick back to the
person who hosted the data or was involved in the chain
which caused the user to reach that content.
We already have many cable subscribers using
capped flat-rate services, where there are
charges over the capped limit.
However I think these caps are typically intended
to strongly discourage going over the cap (at least
the it would seem so from the charges they levy
over the 4GB download cap in my case).
I'm not sure of the trends, or the long term outcome
but if we do get to the case were people are more metered for service, something useful could be done
with strongly anonymous ecash micropayments:
the ISPs could pay their network connections to
other ISPs per Megabyte.  Then as an aspiring
content pimp the current day warez, video and music traders could migrate to making big bucks
by giving people what they want.  The smart content
authors / providers would compete.  The dumb ones
would try to outlaw such things and hopefully lose
rather than turning the world into the draconian place it would have to be for their full intent
in drafting the DMCA and WIPO offenses.
Whether or not artists get money for the works they
release depends on how badly people want to pay
the lowest price for their content.  It's a market
fought on price vs availability, scalability (to
handle flash crowds), and convenience.  Some vendors
may offer the claim or proof that they pay some
percentage to the author, some may not.  The market
will decide.  The author or original distributer
of content chooses his parameters (requested proportion
of content redistribution bandwidth costs) to optimise
his profits.  If he sets it too high, people will have
the incentive to save a few cents getting it from warez.com; if he sets it low enough it will be below
the threshold for which people are willing to search

@_date: 2001-01-02 22:39:23
@_author: Adam Back 
@_subject: monkey-wrenching efnext 
So it appears these efnext people are letting the administrator
control freak method to "fix" problems.  (The tendency to impose more
authentication, more logging, more central control -- instead of
fixing broken protocols -- the easy way out because it's simpler to
implement, though politically and technically broken).
Clearly they are making a mistake.
What could be done to persuade them or educate them that endeavour is
bad for net privacy?  Some possible technical / social engineering backlashes:
- someone will create some abuse that causes the central
  administrators and thought police to become legally liable.  Perhaps
  even designer abuse -- "abuse" anonymously created for the effect it
  will have on the operators.  (Where's Dimitri Vulis when we need
  him?)
- they are probably dumb and have done a bad job of making their
  changes -- their new centralised controls will get hacked, and their
  network will prove even more susceptible to catastrophic DoS than
  the original ircd.
- people aren't really trying hard to disrupt IRC -- there are
  doubtless many much more malicious and harder to stop ways to
  disrupt it.  People might demonstrate some of these attacks on their
  central failure points.

@_date: 2001-01-02 23:04:48
@_author: Adam Back 
@_subject: Anarchy Eroded: Project Efnext  
So what they should do is fix those problems robustly.  Instead
they're using central control as a "fix".  They get to decide what is
abuse.  They probably don't appreciate the kinds of problems that can
arise from that (see my other comments about designer abuse and the
implied risks of assuming editorial control that some ISPs have faced
It's typically easier to design hierarchical or even single central
control systems than distributed systems.  DoS resistance is hard too.
The real solution to Distributed DoS is Distributed Service and
they're headed in the wrong direction with that.
Indeed.  They're probably relatively clue free also.  (Just downloaded
the tar ball to reverse engineer what they are actually doing).
The problem is central control not monitoring -- monitoring affects
privacy, central control affects free speech.
(It's in clear text already, and they're not proposing to do anything
about this -- and for the application -- public chat -- it's unclear
how well you can protect privacy -- any narcs can just join in the
So the low number of servers is bad for protecting free speech also.
Also on the plus side it's not that big a network to fork with a fork
keeping the old protocols, with robust distributed DoS fixes.  A
corrolorary of Lucky's comment that there's more demand for crypto
than people competent to do it -- there aren't enough crypto clueful
people to keep up with internet protocols and steer them in sensible

@_date: 2001-01-11 23:09:55
@_author: adam@cypherspace.org 
@_subject: copy protecttion doesn't work (Re: MS Product Activation for Windows) 
Well I think the main reason no one bothered to write a license number
generator for win9x, win2k and the windows NT versions is because the
inconvenience factor is low right now -- most people have some license
numbers lying around.  People typically use the same code on all the
machines on their network just for convenience.
(Well probably someone has in fact written a crack for the different
windows versions, but my point is no one has an incentive to use such
work-arounds, because the copy protect code is not too much of a
The second they introduce this anti-piracy measure -- if they actually
go through with it -- the copy protect code check will be broken, with
a patch to disable it, or a program to generate licenses.  Everyone
will use it whether they paid for the windows CD, got it free with
their machine, or copied the OS.
So pretty much the only effect they'll have is to inconvenience their
users, and probably as Ray suggests make more people have negative
feelings about copy protection mechanisms because of the extra

@_date: 2001-01-17 07:32:19
@_author: Adam Back 
@_subject: copyright: moral right or outdated convention 
This is a response to Eric Flints comments in While I agree with your conclusion -- that in the short term by
putting your books online that will not lose you sales, I disagree
with the statements you make to the effect that copying is immoral.
And I am not optimistic for short sighted publishers and distributors
financial well being in the longer term.  This applies more
immediately to digital media, or easily digitised media such as music,
software and video.
Firstly the issue of morailty.  You don't seem to hold borrowing or
libraries to be immoral.  The distinction between photocopying and
borrowing seems minor to me.  In both cases two people enjoy a book
that only one person paid for in a way which reimburses the author.
So photocopying is time-consuming, the result is not convenient, and
it's cheaper and more convenient for most people when they consider
their time to be worth anything to buy the book instead of copying it.
Which is why photocopying entire books is not common.  (I've seen cash
starved students do it to save a couple of bucks, though).
Copyright, and copyright enforcement (which is very patchy) are
societal conventions -- fairly recent ones too in the big picture.
They are not moral rights.
Practically, anything that is published (has left your control) is now
freely copyable.  This is clearly a statement of reality.  People pay
for convenience -- they prefer a paper copy -- and that is fine and it
will likely take some time and technological change to move away from
that.  But the coercion that the premise that copying is immoral and
must be prevented at gun point is immoral itself -- it's ultimately
threatening the copier with physical harm for copying defacto public
information.  Whether or not it's the current law -- a very weakly and
patchily enforced law -- or not is not the issue; the law is
unenforceable, and people can and will copy.  Anonymity, and privacy
technology and the distibuted and decentralised control of the
internet mean that people can do these things with impunity.
Unless we turn the internet into the biggest tracking and police state
in the history of man, where if you copy the wrong bits you'll get
hauled off to jail, the physics and economics of the internet mean
that free and convenient copying of copyrighted works will increase.
Consider for a moment what would be implied to have near 100%
electronic copy enforcement.  Privacy of any kind would have to be
outlawed on the internet.  Peoples computers and hardware would have
to be turned into paid informers against their interests.  The
escalation of copyright enforcement you talk about if it were to
happen would lead to a very undesirable society.
Another technological reality which acts against the concept of
copyright is that all attempts at computerised software and content
copyright enforcement are fundamentally flawed at a conceptual level:
if your computer can display a digital bit stream to you, it can be
recaptured and re-encoded in a format of the re-distributors
convenience.  This leads to preposterous suggestions like a monitor or
TV screen that decodes the content, or CPUs that run software that the
owner and user of the computer can't read.
Electronic copying will increase.  I don't see how it can be any other
way.  This applies especially to works which are convenient to use on
a computer: music, video.  And to works which are convenient to
digitise or are already digital.  Recreational books are more
convenient to read on paper.  Reference books perhaps more convenient
on computer (search functionality, frequent updates).  We don't see
many instances of electronic copying of books -- because it takes a
lot of time to scan, correct errors formatting etc, and the original
copier already has the book so has little incentive.  Were it to
become possible for the copier to make money from the subsequent
electronic copies made perhaps we would see it increase.
So the physics and economics of the internet space is that people will
copy and redistribute whatever they wish.  Anyone in the publishing
industries who wishes to survive and thrive in this environment needs
to adapt to this reality.  It will be entirely feasible to make money.
But the author and his distributors will need to aggresively strive to
provide their customers with convenience.  The content distributors
just have to compete with the competition -- electronic copying,
rather than viewing it as illegal pirating which they somehow have a
moral right to prevent.  And you can compete -- there are real costs
of copying (which vary depending on the type of work) -- bandwidth
costs, and the competition doesn't have unlimited bandwidth.  The
price will stabalise.  It will be possible to charge more based on
convenience as people value their time.  It may be possible to charge
a small amount more based on goodwill -- consumers willing to pay a
few cents more given equivalent convenience if they know some
proportion of that goes back to the author, producer etc.
So the distributor will need to provide fast bandwidth, make his
copies easy to find in search engines, have prominent domain names,
brand name, and negotiate deals for share of bandwidth cost,
advertising revenue with the competition.  So the key is not to fight
electronic copying, but to work with it, and use it to make money.  I
don't think many people understand this yet.  mojonation.com seem to,
but I haven't seen much indication that any other businesses does.
Certainly the music industry largely has their collective heads in the
sand at this point.  The movie industry hasn't felt the pressure much
yet due to bandwidth, but give it a bit of time.
It's clear that publishers should be able to make money in this
economic landscape, and those able to adapt and not let near sighted
views on copyright and outdated distribution stand in their way, will
win in this space.
It's all about user convenience vs cost.

@_date: 2001-07-05 14:49:07
@_author: Adam Back 
@_subject: Declan misses the mark on ecash 
A little behind on mail, but I figure some of this is sufficiently
misleading to dig up again: The point is Chaums ecash never did deploy cut and choose.  So whether the
description of cut and choose was unclear or not it was irrelevant, and
confused to describe it at all.
According to who?  You?  Care to elaborate on your thinking?
So far your analysis of the patent discussion around Wagner's scheme
looks like just spurious uninformed dismisive comments.
The point of Wagner's scheme is that it is not a blind signature but a MAC
coupled with a zero-knowledge proof of non-coin marking.
I'd say it's more accurate to say it's not clear whether it is covered
by something in the patent minefield or not.  There was a comment on
dbs list that a lawyer at berkeley had offered the opinion that it was
not.  Chaum offers the opinion that it is covered by his blind signature
patents.  I find that unlikely as it is not a signature as it fails one
of the main aspects of a signature -- that there is something that is
verifiable by the holder and usually others relating to some message.
There are of course other opinions also, but I find your dismissive
comments misleading.
Also this comment :
I don't think anyone was claiming you would proceed in any patented
area without legal advise.  I don't think anonymous comments should
hold less weight.  Yet you were presuming to dismiss Wagner's approach
as clearly covered by patents which appears to be an uninformed and
probably incorrect opinion.

@_date: 2001-07-05 16:10:12
@_author: Adam Back 
@_subject: Declan misses the mark on ecash 
I am not sure why you suspect it is covered by Chaum's patents.
You said in response to anonymous and I quote:
I explained to you in the post you are replying to why I don't think this
claim is accurate.  Wagner's scheme was designed explicitly to avoid being
covered by Chaum's patents by people who have read Chaum's patents.
Wagner's scheme is clearly not a signature, blind or otherwise.
I have not personally asked a lawyer for an opinion.  Neither have you I
take it.  This was why I suggested "unclear" was a better description of the
current understanding than "appears to rely on Chaum's patents" which you
appear to have made up.
Perhaps the berkeley lawyers opinion could be tracked down?

@_date: 2001-07-09 12:03:55
@_author: Adam Back 
@_subject: Declan misses the mark on ecash 
So I asked Bob Hettinga as I thought I saw the comment on one of his lists,
and with Ben Laurie who apparently made one of the comments and it appears
there was confusion surrounding the comment by either Ben or Bob.
Which doesn't alter my opinion about Declan's comments about Wagner's
blinding method, but I thought I'd track it down and set the record straight
on the mythical berkeley law prof.

@_date: 2001-07-10 19:59:23
@_author: Adam Back 
@_subject: Dropping out of the USA 
I was thinking online obscurity (nyms, pseudonymous web pages etc) coupled
with a low tax jurisdiction like Anguilla wouldn't be one interesting
But there are plenty of disadvantages too -- limited amenities - shops,
computer parts, the advantages being within reasonable travelling distance
of a large western city affords.  The inconvenience and cost of travelling
from a remote locale such as Anguilla if you do much international
travelling to visit family, friends, conferences etc.
Apparently there are some tax advantages to residing in some Swiss cantons.
But as Tim says there aren't really any jurisdictions which offer
significant advantages in physical and financial privacy over general
western jurisdictions.

@_date: 2001-07-12 01:23:14
@_author: Adam Back 
@_subject: Digital Cash 
Probably people would be willing to accept other issuers currencies even if
they don't know the issuer so long as they had the reputation rating for the
currency / issuer.
But anonymous reptuations alone aren't any use as a rational issuer would
refuse to redeem if the action didn't adversely affect his reputation -- you
need to be assured that the rating of the anonymous issuer will be downrated
if they refuse to redeem.
So then perhaps you could proceed by having unlinkably anonymous credentials
for reputation with a trap-door for the rating party so that the rating
party can identify the pseudonym behind the unlinkable credential and
downrate it.  You also want the unlinkable rating credentials to need to be
refreshed by the rating credential issuer in order to re-show.  Brands'
credentials have this property if you reshow without collaboration with the
issuer, they are linkable (and hence would be linkable to the transaction
gone bad which triggered the downrating).
One might desire also that the rating credential issuer not be able to link
general transactions, even with collusion from all parties except the
issuer.  However I'm not sure if this is going to be possible; the rating
issuer must be able to link to the nym in event of foul play by the currency
issuer, and clearly ability to link from unlinkable payments to a nym links
the payments.  The only avenue I see is if the foul play were mathematically encapsulatable
and could be combined with the protocol so that the rating issuer is only
able to link payments to nyms in the event of foul play.
Do you think you can encapsulate foul play formally generally enough to be
useful in your application?

@_date: 2001-07-12 19:50:30
@_author: Adam Back 
@_subject: The Pulp Theorem (Re: Digital Cash) 
So I have two types of issuer.  The currency issuer (let's call them mints
to avoid confusion).  Ray has every one a mint (potentially, some users may
choose to use other peoples mints to avoid having to manage the reputation
of their own).  Floating exchange rates based on reptuation of the mint.
Then we have an issuer of one use (and hence unlinkable) credentials
representing the reputation of the mint.  So these are reputation credential
issuers.  My thought was that there would similarly be reputation credential
issuers -- (potentially) everyone a reputation credential issuer.
Also Stubblebine et al have a paper about abuse control with unlinkable
anonymous credentials.  They way they do this is to have one unlinkable
credential which you can show only once.  Then you can trade it for a fresh
unlinkable credential if there have been no complaints against the current
credential.  Because the fresh credential is freshly blinded it's not
linkable.  And yet you retain some scope for abuse control, if the proof of
misconduct arrives before you've handed over the new credential.  (The
Stubblebine paper doesn't say much more than that.  Do a web search if you
want the paper.  It was in the context of unlinkable subscriptions to
services, where you want to renew, but the service operator wants the
ability to cancel abusers of their AUP's subscriptions.)
Seems like this might be usable here.
This seems like a technology trust issue.  It seems just to do with
branding, advertising and common acceptance.  A mag-swipe card could fail, a
bank could empty your acount, their security could fail and someone else
empty your account via ATM.  People trust the systems because their friends
trust them and seem to use them without incident and they want to use the
system because of convenience or some other useful attribute.
Ray's scheme sounds interesting because it's a computer mediated Letts
scheme.  Letts schemes seem to exist with manual book-keeping.
Also trust levels needed to trust in something as a value store are much
higher than purely as a immediately cleared payment mechanism.  With the
reputation system you could even have insurance.

@_date: 2001-03-02 13:25:56
@_author: Adam Back 
@_subject: Decentralized Markets 
What is a .sdd file netscape on linux doens't know what to do
with it.  Do you have the info in a more portable format?

@_date: 2001-03-08 14:51:40
@_author: Adam Back 
@_subject: DeCSS in perl -- test vectors? 
Keith Winstein and Marc Horowitz implemented DeCSS in perl.
There is some description of using it here:
Does anyone have test vectors for DeCSS.  If one had a DVD player and a
content scrambled DVD one could use the example they give:
% /mnt/dvd/VOB_FILE_NAME | qrpff 153 2 8 105 225 | \
extract_mpeg2 | mpeg2dec -
Someone want to extract a few frames from a DVD and include the associated
plaintext for testing purposes.  I'd like to try reduce the size of the

@_date: 2001-03-18 12:34:28
@_author: Adam Back 
@_subject: What is the recommended variant of PGP. 
I use pgp6.5.8 - it has a single command line program (pgp5.x had a
separate program for key management), and it has more pgp2.x like
command line options.
I would use gnuPG as the code is cleaner, more readable and smaller
but they're still refusing to include IDEA by default, even though
the IDEA license allows free non-commercial use.  (They added in RSA
as default since the RSA patent expired, but that's almost useless
without IDEA, as you still can't talk to pgp2.x).  There is an IDEA
module I think, so I suppose the best situation would be to add in
the IDEA module to gnuPG and use it.

@_date: 2001-03-28 02:51:34
@_author: Adam Back 
@_subject: Zone On the Range 
This is cryptic -- how do I know what this link will be about from your
subject line.  Can't you quote as ascii the first paragraph, or write your
own summary, or something?
If you can't be bothered to do that, the reference is not useful; it's just
a random pointer.  Historically most of them are not even remotely crypto
I asked politely in private email if you would consider providing summaries. I received no reply.  If you aren't going to provide summaries and
meaningful subject lines, I politely ask that you stop sending the list
random URLs.

@_date: 2001-11-28 21:47:22
@_author: Adam Back 
@_subject: pipenet padding 
There is some discussion of pipenet and freedom attacks in:
"Traffic Analysis Attacks and Trade-Offs in Anonymity Providing Systems"
Adam Back, Ulf Moeller, and Anton Stiglic

@_date: 2001-11-28 23:23:18
@_author: Adam Back 
@_subject: zks freedom websecure trial 
I noticed some discussion of the SafeWeb cancellation of free
services here.
ZKS announced yesterday freedom websecure, which is an anonymous web
browsing system with more robust redirection and script blocking than
systems that rely on html re-writing.  There is a free trial offered
for a couple of months.
Unfortunately it only works as shipped with IE on windows in this
(The local browser plugin is the point where srcipting, java etc are
disabled, and traffic is encrypted (with SSL) and directed to a

@_date: 2001-12-01 01:24:58
@_author: Adam Back 
@_subject: Moving beyond "Reputation"--the Market View of Reality 
I think you'd want to have a regular public key change, where the user
creates a new public key, signs it with their old key.  This way key
change does not suggest possible ownership change.
Then to effect a nym transfer, the new public key is instead chosen by
the new owner, and the old owner signs it.
This still leaves the problem of the old nym revealing a previously
unpublished identity revocation, or simply some signed statements
which are damaging to the nyms reputation.  Some ideas on this:
- introduce a time-stamping service for certification signatures only
  (explicitly not for documents)
- re-define valid certificate signatures to be signatures made on
  public key and identity pairs within the validity period of a key
- publish signature private keys after expiry.
Now anyone can create nym revocation with old keys, and so no
reputational damage can be done by the old owner with nym revocations.
The time-stamper will not assist in signing arbitrary documents, and
so the old nym owner can not use it to prove a document was signed
during the validity period of the key.  (Were it signed after the
validity period it would anyway not be considered a valid signature).
Where general document signing time-stamping is used, to prevent
post-sale nym reputation suicide, the new owner could demand all past
signed messages and verify them against the merkle hash tree master
hash maintained by the time-stamper, and vet the messages as not
damaging to the nyms reputation.
For private encrypted messages the nym would not like to share, even
after nym sale, a non-transferable signature scheme could be used,
with the time-stamper having a separate hash-tree for such signatures.
The new owner would be somewhat assured that the old owner could not
have any signatures that he can both prove the time of authorship of
and transfer.  The availability of third party time-stamping services, and other
simple methods of dating documents (pre-published hash, 3rd party
vouching for time of authorship) limits the assurances provided by the
above approaches.
Another avenue might be designated verifier signatures where the
signature sheme necessarily requires collaboration of a verifier, and
the verifier will take instructions from the current owner.
Or better where only the current owner in collaboration with the
designated verifier can assist a user in verifying a signature.  (eg
using pro-active security to re-split the key on nym sale so the old
owner isn't able to collaborate in verification).
In this way the new owner gets to vet which signatures the public can

@_date: 2001-10-13 14:57:25
@_author: Adam Back 
@_subject: RIAA Safeweb Proxy ID 
A carnivore box at Safewb would work also for tying browsing habits to
users.  Think Safeweb with their CIA ties would balk at installing
one?  Do they already have one installed?

@_date: 2001-10-15 16:14:49
@_author: Adam Back 
@_subject: Schneier on Stego, Dead Drops, bin Laden 
Bruce writes about uses of steganography as digital dead drops.
But he also claims that there are no business uses for steganography.
I don't think this claim is valid.
There are business scenarios where traffic analysis can leak
information about potential mergers, investment analysis activity and
so on.
Steganography is just a valid mechanism to hide traffic as cover
traffic.  Stego in fact offers marginally higher security against
traffic analysis because it will not be evident that the two parties
exchanged information, nor even had the opportunity to.  The
opportunity to have communicated would be evident if they were using
just cover traffic.
Apart from business uses there are uses for civil rights workers, and
generally members of the public who choose to retain association
I don't think we should be giving the press and government ammunition
in their arguments to ban various forms of crypto, especially for
forms of communication which may help civil rights workers, and which
infringe on the tools available to the individual to partially regain
his privacy be that confidentiality or of association.

@_date: 2001-10-20 00:41:27
@_author: Adam Back 
@_subject: What info does Zero Knowledge collect on users of Freedom 
declan on Fri, Oct 19, 2001 at 02:36:00AM -0700
Let me try give some more details behind this.  The idea was to create
separate modules that can be separately shipped and sold.  Freedom 3.0
"privacy & security tools" is the first of those.  It has a subset of
the functions in freedom 2.2 (cookie management etc), but some of
those functions have new features or have been improved.
Declan's interivew with Austin and Ian's comments on slashdot and here
cover the aspect of the story that the current freedom network which
was the support infrastructure behind the anonymous browsing and mail
part of Freedom 2.2 was decomissioned.
Other consumer software related plans are not public.

@_date: 2001-09-13 19:04:41
@_author: Adam Back 
@_subject: escalation not the answer (Re: where do we go from here (and where should we have gone)) 
This has little to do with US middle east policies.
I understand people are emotional at this point, but to be living
under the impression that the US has no part in the escalation is
ignorant of history.  The fundamentalists behind these terrorist
attacks are justifying their actions on the basis of US sponsorship of
Israel which in their view is terrorising Palestinians, and various US
interentionist military acts -- missile attacks, assassination
attempts, blanket bombings etc.  They may or may not have other
agendas, but these historic grievances are what allow them to gain
It seems likely that we will see an escalation:
- Israel and Palestine conflict, Israel funded by US, both sides have
  greivances in their escalation of violence
- previous WTC bombing, and US reactions to that
if both sides continue to react with increasing violence where does it
I'm not saying there is an easy answer, but escalation seems unlikely
to help long term political stability.  Unfocused escalation "no
distinction between perpetrators and harborers" it would seem will be
likely to create more victims, who have had family members killed who
were bystanders and previously neutral or antagonistic to the
perpetrators who will then be fodder for future supporters of that
currently small minority of fundamentalist islam calling for jihad on
the US.
Pre-emptive strikes if any should be focused on military targets only
not for the purpose of revenge but for the purpose of reducing the
chance of further attacks.  If a strike would in fact increase chance
of further attacks and contribute to further escalation, I think it
would be a bad idea, and instead attempts should be made to suppress
the current cycle of escalations.

@_date: 2001-09-21 00:56:29
@_author: Adam Back 
@_subject: a libertarian approach to airport security: suggestions 
Well I guess a little rational thought might not hurt as the motivator
of policy rather than public perception, and hidden agendas to
increase state power and outlawing unrelated technologies they'd like
to outlaw anyway (eg privacy and encryption software to increase the
scope of open source signals intelligence.)
Identification doesn't appear to be that relevant to hardening
I saw some inanely stupid arguments on news over the last week about
policies and procedures along the lines of 'if our policy was
different this wouldn't have happend', the perpetrators knew the
policies in effect and planned their operation around them.  The world
can spend billions re-jigging policies and procedures, but few of them
even seem like speed bumps, and most I've seen proposed look like they
would have no effect at even complicating further attacks.  If one presumes the motives of the politicians are sincere they are
idiots -- as chess players go they aren't even able to think one move
ahead -- aren't able to ask the question of themselves: "what would
the opponent do if this candidate policy were in place?"
It's pretty impossible to harden a country with such low population
density and number of fat targets.  The attacker will simply attack
the target with the best trade-off of political value, destructive
value and ease of attack.  You can't pre-empt this stuff.
Also as well as the obvious fact that people of arabic descent are
rather a broad profile to single out for extra scrutiny, it seems on
reading more of the background of the region that there are
fundamentalist muslims in Afghanistan from different ethnic
backgrounds: Egyptians, Chinese, Africans, and perhaps even the odd
Russian.  It's pretty much a "doh" that scrutinizing arabic looking
people, will lead the would-be attacker to select suicide attackers
with different ethnic appearance.
A much better approach to aggresively and honestly persue would be to
attempt to improve relations with the minorities who are feeling
persecuted directly by previous US military and political actions and
by indirect actions in sponsoring, funding, training: Iraq, Iran,
Saudi Taleban, Israel etc, etc at various points in the past and
It seems that world stability is more likely to be achieved by
diplomacy than by engaging in tit-for-tat escalations of violence.
Guerilla tactics make it impossibly expensive to harden a country
against such attacks.
Whereas one might normally attribute actions with apparently the
opposite effect of the claimed intent to stupidity ("never attribute
to malice that which can be explained by stupidity"), I suspect that
the real reason for the disparity is that the stated intent is not the
real intent.  The real intents are probably economic, and the actions
planned inline with economic analysis and forcasts, though not the
interests of world stability.  This can't be explained openly to the
public as they would reject the strategies.

@_date: 2002-08-02 02:28:17
@_author: Adam Back 
@_subject: document popularity estimation / amortizable hashcash (Re: 
adam on Wed, Jul 31, 2002 at 09:34:35PM +0100
This paper is quite interesting and proposes another method of
metering content [1]:
It's proposed in the context of web site traffic metering to determine
site traffic rates (for advertising payment or other applications).
It relies on a trusted auditor, which could become a central failure
point so is perhaps less attractive for file sharing, but perhaps that
could be fixed.
Another problem is that it presumes a communication pattern where the
auditor sends a secret to each user, the user makes a cheap
computation involving the secret to send with each request, and then
the respective server collects all of the requests it gets and does
some computation to arrive at a compact proof that it received some
number k of requests.  (The server also receives a secret, but this is
not problematic, it it anyway wants to participate).
On the plus side their approach is not probabilistic -- it gives an
accurate measurement of traffic, it is also not vulnerable to server
traffic inflation, and is somewhat resistant to multiple client and
server collusion.  (Though of course any scheme is generically
vulnerable to server traffic inflation -- servers can _act_ as
multiple clients and simply generate the claimed traffic themselves,
or contract other parties to do this for them.)
    author = "Moni Naor and Benny Pinkas",
    title = "Secure and Efficient Metering",
    journal = "Lecture Notes in Computer Science",
    volume = "1403",
    pages = "576--??",
    year = "1998",
    note = "Also available as \url{

@_date: 2002-08-03 05:26:12
@_author: Adam Back 
@_subject: info-theoretic model of anonymity 
Just read this paper published in PET02 "Towards an Information
Theoretic Metric for Anonymity" [1]:
or	
it uses a Shannon like entropy model for the anonymity provided by a
system uses this model to analyse the effect of different parameters
one can tune with mixmaster (POOLSIZE, RATE, in mixmaster.conf).
The "anonymity entropy" measurement can be interpreted as how many
bits of information the attacker needs to identify a user and is
computed from probabilities.
Would be interesting to try estimate the entropy provided by the
current mixmaster network.  A number of nodes publish their parameter
choices, and traffic volume over time (in hourly increments).
  author = "Andrei Serjantov and George Danezis",
  title = "Towards an Information Theoretic Metric for Anonymity",
  booktitle = "Proceedings of the Workshop on Privacy Enhancing Technologies",
  year = "2002",
  note = "Also available as \url{

@_date: 2002-08-05 06:00:31
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Like anonymous, I've been reading some of the palladium and TCPA docs.
I think some of the current disagreements and not very strongly
technology grounded responses to anonymous are due to the lack of any
concise and informative papers describing TCPA and palladium.
Not everyone has the energy to reverse engineer a detailed 300-odd
pages of TCPA spec [1] back into high-level design considerations; the
more manageably short business level TCPA FAQs [2], [3] are too
heavily PR spun and biased to extract much useful information from.
So so far I've read Ross Anderson's initial expose of the problem [4];
plus Ross's FAQ [5].  (And more, reading list continues below...).
The relationship between TCPA, and Palladium is:
- TCPA is the hardware and firmware (Compaq, Intel, IBM, HP, and
Microsoft, plus 135+ other companies)
- Palladium is a proposed OS feature-set based on the TCPA hardware
The main 4 features proposed in the TCPA/palladium scheme are:
1. secure bootstrap -- checksums of BIOS, firmware, privileged OS code
are used to ensure the machine knows whether it is running certified
software or not.  This is rooted in hardware, so you can't by pass it
by using virtualization, only by hardware hacking (*).
2. software attestation -- the hardware supports attesting to a third
party whether a call comes from a certified software component as
assured by the hardware described in feature 1. 3. hardware assisted compartmentalization -- CPU can run privileged
software, and RAM can contain information that you can not examine,
and can not modify.  (Optionally the software source can be published,
but that is not necessary, and if it's not you won't be able to
reverse-engineer it as it can be encrypted for the CPU).
4. sealing -- applications can store data that can only be read by
that application.  This works based on more hardware -- the software
state checksums developed in feature 1 are used by hardware to
generate encryption keys.  The hardware will refuse to generate the
key unless the same software state is running.
One good paper to understand the secure bootstrap is an academic paper
"A Secure and Reliable Bootstrap architecture" [6].
It's interesting to see that one of the author's of [6] has said that
TCPA as curently formed is a bad thing and is trying to influence TCPA
to make it more open, to exhibit stronger privacy properties read his
comments at [7].
There are a lot of potential negative implications of this technology,
it represents a major shift in the balance of power comparable in
magnitude to the clipper chip:
1. Potentially cedes control of the platform -- while the palladium
docs talk about being able to boot the hardware with TCPA turned off,
there exists possibility that with minor configuration change the
hardware / firmware ensemble that forms palladium/TCPA could be
configured to allow only certified OSes to boot, period.  It's
intereseting to note, if I read correctly, that the X-box (based on
celeron processor and TCPA / TCPA-like features) does employ this
feature.  See for example: [8].  The documents talk about there being no barrier to certifying TCPA
aware extensions to open-source OSes.  However I'm having trouble
figuring out how this would work.  Perhaps IBM with it's linux support
would build a TCPA extension for linux.  Think about it -- the
extension runs in privileged mode, and presumably won't be certified
unless it passes some audit enforcing TCPA policies.  (Such as keeping
the owner of the machine from reading sealed documents, or reading the
contents of DRM policy controlled documents without meeting the
requirements for the DRM policy.)
2. DSS over-again -- a big aspect of the DSS reverse-engineering was
to allow DVDs to be played in software on linux.  The TCPA platform
seems to have the primary goal of making a framework within which it
is possible to build extensions to implement hardware tamper resistant
DRM.  (The DRM implementation would run in a hardware assisted code
compartment as described in feature 3 above).  So now where does that
put open source platforms?  Will they be able to read such DRM
protected content?  It seems likely that in the longer term the DRM
platform will include video cards without access to video memory,
perhaps encryption of the video signal out to the monitor, and of
audio out to the speakers.  (There are other existing schemes to do
these things which dovetail into the likely TCPA DRM framework.)  With the secure boot strap described in feature 1, the video card and
so on are also part of the boot strap process, so the DRM system would
have ready support from the platform for robustly refusing to play
except on certain types of hardware.  Similarly the application
software which plays these DRM policy protected files and talks to the
DRM policy module in the hardware assisted code compartment will
itself be an application which uses the security boot-strapping
features.  So it won't be possible to write an application on for
example linux to play these files without an audit and license etc
from various content, DRM and OS cartels.  This will lead to exactly
the kind of thing Richard Stallman talked about in his prescient paper
on the coming platform and right to develop competing software control
wars [9].
3. Privacy support is broken -- the "privacy" features while clearly
attempts to defuse a re-run at the pentium serial number debacle, have
not really fixed it's problems.  You have to trust the "Trusted Third
Party" privacy CA not to track you and not to collude with other CAs
and software vendors.  There are known solutions to this particular
sub-problem, for example Stefan Brands digital credentials [10], which
can be used to build a cryptographically assured privacy preserving
PKI avoiding the linking problems arising from identity based and
attribute certificates.
4. Strong enforcement for DMCA DRM excesses -- the types of DRM system
which the platform enables stand a fair chance of providing high
levels of enforcement for things which though strictly legally
mandated (copyright licensing restrictions, limited number of plays of
CDs / DVDs other disadvantageous schemes; inflexible and usurious
software licensing), if enforced strictly would have deleterious
effects on society and freedom.  Copyright violation is widely
practiced to a greater or less extent by just about all individuals.
It is widely viewed as acceptable behavior.  These social realities
and personal freedoms are not taken into account or represented in the
lobbying schemes which lead to the media cartels obtaining legal
support for the erosion of users rights and expansionist power grabs
in DMCA, WIPO etc.
Some of these issues might be not so bad except for the track records,
and obvious monopolistic tendencies and economic pressures on the
entities who will have the root keys to the worlds computers.  There
will be no effect choice or competition due to existing near
monopolies, or cartelisation in the hardware, operating system, and
content distribution conglomerates.
5. Strong enforcement for the software renting model -- the types of
software licensing policy enforcement that can be built with the
platform will also start to strongly enable the software and object
rental ideas.  Again potentially these models have some merit except
that they will be sabotaged by API lock out, where the root key owners
will be able to charge monopoly rents for access to APIs.
6. Audits and certification become vastly more prevalent.  Having had
some involvement with software certification (FIPS 140-1 / CC) I can
attest that this can be expensive exercises.  It is unlikely that the
open source community will be able to get software certified due to
cost (the software is free, there is no business entity to claim
ownership of the certification rights, and so no way to recuperate the
costs).  While certification where competition is able to function is
a good thing, providing users with a transparency and needed
assurance, the danger with tying audits to TCPA is that it will be
another barrier to entry for small businesses, and for open source
7. Untrusted, unauditable software will be able to run without
scrutiny inside the hardware assisted code compartments.  Some of the
documentation talks about open sourcing some aspects.  While this may
come to pass, but that sounded like the TOR (Trusted Operating Root);
other extension modules also running in unauditable compartments will
not be so published.
8. Gives away root control of your machine -- providing potentially
universal remote control of users machines to any government agencies
with access to the TCPA certification master keys, or policies
allowing them to demand certifications on hostile code on demand.
Central authorities are likely to be the only, or the default
controllers of the firmware/software upgrade mechanism which comes as
part of the secure bootstrap feature.
9. Provides a dangerously tempting target for government power-grabs

@_date: 2002-08-05 06:48:01
@_author: Adam Back 
@_subject: "trust me" pseudonyms in TCPA (Re: Other uses of TCPA) 
I haven't read the TCPA detailed spec yet (next on TCPA/Palladium list
of reading material), but this bit I can infer I think:
The corresponding public key is certified by the secure hardware
manufacturer, I think.
Then they have this privacy CA which accepts requests signed by the
platform's signature key, and gives in return a certified pseudonym of
the users choice.  They claim this gives privacy, which it only does
if you trusted the "privacy CA" -- the privacy CA can link all of your
anonymous and pseudonymous credentials.  (Anonymous may want to
straighten out the different keys names -- I think there are some
encryption, some signature, some sealing keys derived from other
secret keys and the checksum of the application and OS / firmware
Brands digital credentials could be used to fix this sub-problem I
They put in the privacy CA thing as a defense against the PR problems
Intel had with the pentium serial number.  The FAQs at
 talk about this arguing how this is better than
pentium serial number at avoiding linkability.
The documentation problem I find is there isn't much documentation
available which is technical except for the 330 page spec which drops
right down to implementation details in RFC standards style.

@_date: 2002-08-05 21:46:43
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Some comments on selected parts of anonymous post:
1) about claimed "complexity" of cryptographically assured privacy,
rather than the current "trust me" privacy via the privacy CA "TTP":
To address privacy with for example Brands digital credentials, the
underlying cryptography may be harder to understand, or at least less
familiar, but I don't think using a toolkit based on Brands digital
credentials would be significantly harder than using an identity or
attribute based PKI toolkit.  Similar for Chaum's credentials or other
approach.  Also I notice you imply patents are a problem.  However, the TCPA
itself has patents and will of course charge for the hardware.
Patents it doesn't seem would present a problem for this application,
where there is non-zero reproduction cost hardware involved.
2) about the "root key" / potential for malicious remote control claim
that I make (and Ross Anderson I think also makes):
The "root key" to your computing environment is the private key of the
CA that signs the software updates.  You'll recall that in the secure
boot-strapping process if you choose to boot in the TCPA mode, if
there are deviations or updates these are fetched and are only
accepted if certified by the layer owner.  (I presume different layers
would have updates and certification managed by different vendors
Eg. hardware vendor / TPM vendor for firmware, OS manufactturer for
OS, application manufacturer for application software etc, and that
the secure bootstrap process would accordingly transfer control to the
respective next layers certification keys in case of need for software
The closer to the hardware a software update is the more pervasive the
control a malicious update could exert.  For example there are
apparently plans for TPM mediated direct path to input devices
(esp. keyboard), a malicious update close enough to the hardware could
subvert this protection.
and more on the "root key" problem:
The root key is not the endorsement master keys -- that one just
allows the TPM vendor to extract rent from the hardware manufacturers

@_date: 2002-08-05 22:26:28
@_author: Adam Back 
@_subject: more TCPA stuff (Re: "trust me" pseudonyms in TCPA) 
Note there is one key that is endorsed, so per machine there is one
key, singular.
On the other interpretation of your question: do we trust that the
manufacturer didn't take a copy of the key while certifying it?
Good quesion.  The scenario is analogous to the pre-generated private key on a smart
card.  Do you trust what the hardware vendor did with it?  Did they
generate the private key it off chip and keep a copy?  Did they
generate the private key on chip but export it at the time of
certifying the public key?
Except in this case the smart card is attached to your motherboard,
mediates control of the platform and is called the "TPM" Trusted
Platform Module.
While there are approaches to having third party audits of the
process, publishing the source code, etc; it's still typically not a
very transparent affair as it's in tamper resistant hardware, plus
vulnerable to plausibly deniable snafus, and undetectable backdooring
even if it is generated on TPM.
Effectively I think the best succinct description of the platforms
motivation and function is that:
"TCPA/Palladium is an extensible, general purpose programmable dongle
soldered to your mother board with centralised points belonging to
It seems to me there is both strong possibility for it becoming a
focus for future government attempts at policy malware and legislated
technology implementation, and a focus RIAA/MPAA/WIPO polices imposing
futher expansionist and monopoly propping legislation and legislated
technology implementation to enforce the worst excesses of DMCA.
The technology components are very interesting.  The implications of
what can be done with sealing, secure boot-strapping and remote
attestation are a departure from what people were thinking was
possible with general purpose computing.  As anonymous points out it
makes possible all kinds of applications and changes the nature of
what can be cryptographically assured.
With current non-TCPA platforms the limit of what can be
cryptographically assured is for example what can be encrypted with
password, or other cryptographic mechanism.
Cryptographic assurance is also known as "data separation" -- the
concept that the crytography is able to completely cover the
applications policy restrictions without leaving "trusted" software
components necessary to enforce policies too complex to implement with
encryption / data separation.
With TCPA you can build general purpose policy code which does not
exhibit cryptographic assurance, and yet due to the TCPA platform
assures similar levels of security assurance.  That's a huge change in
world view in the domain of security applications.
In slightly more detail, you can either build applications rooted in
the remote attestation, sealing and secure boot-strapping functions I
described in an earlier post.  Or you can add your own custom policy
and even applications inside a hardware assured code compartment which
the user can not access or tamper with.
One aspect of the implications is the implementation and security
possibilities it lends to DRM applications.  Personally I don't find
this aspect a good thing because I think current copyright law has
reached a state of being a net negative for society and freedom, and
that it's time to rescind them and start-over.
I think we should try analyse as William Arbaugh suggested in [7] what
is desirable, what is safe to implement, and ways to change the
platform to remove the negative aspects.
control of this platform.  If it were completely open, and possible to
fix it's potential dangers, it would bring about a new framework of
secured computing and could be a net positive.  In it's current form
with centralised control and other problems it could be a big net
[7] "The TCPA; What's wrong; What's right and what to do about",
William Arbaugh, 20 Jul 2002

@_date: 2002-08-06 04:46:37
@_author: Adam Back 
@_subject: (fwd) Re: dangers of TCPA/palladium 
Response from Peter Biddle on cryptography list.  (I think he is a
microsoft tech manager involved with palladium from a quick google).
----- Forwarded message from "Peter N. Biddle"  -----

@_date: 2002-08-07 00:57:43
@_author: Adam Back 
@_subject: USENIX Security TCPA/Palladium Panel Wednesday 
Anonymous: clearly Lucky and Ross have been talking about two aspects
of the TCPA and Palladium platforms:
1) the implications of platform APIs planned for first phase
implementation based on the new platform hardware support;
2) the implications of the fact that the owner of the machine is
locked out from the new ring-0;
For 2) one obviously has to go beyond discussing the implications of
the APIs discussed in the documents, so the discussion has included
other APIs that could be built securely with their security rooted in
the new third-party controlled ring-0.
In my initial two messages looking at implications I did try to
clearly distinguish between documented planned APIs and new APIs that
become possible to build with third-party controlled ring-0s.  Other areas where analysis is naturally deviating from the aspects
covered by the available documentation (such as it is) are:
- discussion of likelihood that a given potential API will be built
- looking at history of involved parties:
  - Intel: pentium serial number
  - Microsoft: litany of anti-competetive and unethical business
    practices,   - governments: history of trying to push key-escrow, censorship,
    thought-crime and technologies and laws attempting to enforce
    these infringements of personal freedom
  - RIAA/MPAA: history of lobbying for legislation such as DMCA,
    eroding consumer rights
  - industry/government collaboration: Key Recovery Alliance
    ( which shows an interesting intersection of
    big-companies who are currently and historically were signed on to
    assist the government in deploying key-escrow
- suspicion that the TCPA/Microsoft are putting their own spin and
practicing standard PR techniques: like selective disclosure,
misleading statements, disclaiming planned applications and hence not
taking everything at face value.  TCPA/Microsoft have economic
pressures to spin TCPA/Palladium positively. - analysis is greatly hampered by the lack of definitive, concise,
clearly organized technical documentation.  Some of the main
informative documents even microsoft is pointing at are like personal
blog entries and copies of personal email exchanges.
a number of your responses have been of the form "hey that's not a
fair argument, what section number in the TCPA/Palladium documents
gives the specification for that API".
I suspect some arguing about the dangers of TCPA/palladium feel no
particular obligation to point out this distinction the fact that an
API is not planned in phase 1, or not publicly announced yet offers
absolutely no safe-guard against it's later deployment.

@_date: 2002-08-07 07:20:16
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Thanks for the clarifications of the differences between TCPA and
Palladium.  The lack of Palladium docs and fact that TCPA docs
describe OS level features led to the inference that Palladium unless
otherwise stated did what TCPA proposed.
* Documentation
I'm feeling frustrated in being unable to properly analyse Palladium
due to lack of documentation.  Surely microsoft must have _some_
internal documentation that could be released.  Two second hand blog
articles by third parties doesn't quite cut it!
The documents claim privacy advocacy group consultation?  Could the
information shared with these groups be published?
It's quite difficult to reason about implications and limits of a
novel new architecture with incomplete information -- you need
grounded facts down to the technical details to work out what the
implications are from first principles and compare and evaluate the
proponents claims.
* privacy CA
Insufficient data to comment on the degree of openness provided by
palladium for 1 (allow owner to load trusted root cert), and 2 (allow
TPM to be completely disabled).
For 3, the TCPA version of the "privacy CA" is broken (implemented
using "trust me" by a server the user does not and should not have to
trust).  Does Palladium do something different?
later in same message you said:
It sounds as if Palladium suffers from the same broken privacy problem
as TCPA then.  Saying you have a choice about whether to use a service
doesn't alter the fact that you are linkable and identifiable to some
extent -- the extent depending on the exact permutation of attribute,
identity and endoresment certificates you have used.
This vulnerability is unnecessary.  You can certifying things without
having to trust anyone.
* architecture functionality
main features of Palladium together with a hardware collection called
the SCP (= ?) can do the following things:
1. no secure-bootstrapping -- unlike TCPA this is not implemented 2. software-attestation -- Palladium uses SCP able to hash perhaps
   the TOR, Trusted Agents, and application software, and then uses
   TCPA-like endorsed hardware keys in the SCP to remotely attest to
   these hashes.  3. hardware assisted compartmentalization -- CPU can run another layer
   of privileged software with ability to prevent supervisor mode
   (and user mode) reading chosen user mode process memory areas.  The
   ubermode code is called the TOR.  The supervisor mode can install
   any TOR into the ubermode, but the TOR can be remotely attested(?)
4. sealing -- TCPA-style sealing
on 2, softeware-attestation from information so far, it's unclear what
is hashed and what is attested.
on 3, hardware assisted compartmentalization
- Presumably Palladium enabled applications would refuse to run
  unless a Palladium/Microsoft certified TOR is running?
  - Limits the meaningfulness of claims of openness in loading your
    own TOR.  More "you can turn x off but nothing will work if you
    do".
   - or perhaps it is up to the individual Palladium application
     which TOR it trusts?
     - but wouldn't this lead to a break:
       - If a microsoft written Palladium enabled application would
         talk to any TOR, user can load a TOR which is under user
         control to by-pass the compartmentalized memory
         restrictions, regaining root.  But if he can do this, he
         can break Palladium enforced DRM.
also about hardware assisted compartmentalization, earlier I said:
you responded:
If I understand the architecture makes it possible to write a TOR
which supports encrypted applications encrypted for the machines key
which is stored in the SCP.
I thought this scheme would be in the current design because as far as
I can see this would in fact be necessary for strong copy protection
for software (software licensed only for a given machine), which I
presumed microsoft has an interest in.
It would be a bit like a "sealed" application.
* on claim "palladium doesn't prevent anything":
Well as you can still run the same software that is a tautology.
The correct question is: can Palladium enabled software prevent you
from doing things you could do with non-Palladium enabled software.
Palladium is in fact designed explicitly to prevent the user doing
The are whole classes of things Palladium enabled software using
Trusted Agents, a default TOR and SCP features can prevent the user
from doing:
- it prevents the owner modifying application code running on his
  machine which uses the remote-attestation functions to talk to
  remote servers
- it could be used to robustly prevent the user auditing what
  information flowing into and out of his machine (the user can't
  obtain the keys negotiated by the SCP and a remote site, and can't
  grab the keys from the application because it's a trusted agent
  running in a TOR mediated code compartment.)
- it could be used to make file formats which are impossible for third
  parties to be compatible with (Ross Anderson came up with this
  example in [4])
- it could be used to securely hide undocumented APIs
- it could be used to securely implement software copy-protection
  using encrypt for endorsed SCP stored machine keys
- it could be used to prevent reverse-engineering applications
- it could be used for DRM to enforce play-once, to revoke fair use
  rights or any other arbitrary policies (on formats only available to
  Palladium enabled applications)
- it could be used to implement key-escrow of SCP stored keys to put
  government or corporate backdoors in sealed data, and comms with
  remote servers encrypted using SCP negotiated keys; wasn't there a
  statement somewhere that CAPI uses would more immediately get
  benefit from Palladium SCP functions?  Not sure how the mix of
  non-palladium using CAPI applications and palladium enabled SCP
  and/or CAP using applications work out for the key-escrow
  implementing TOR scenario.
now as I mentioned in an earlier post you could claim, oh that's ok
because the user has choice, he can still boot with his own custom
TOR.  In the short term that argument would work.  In the long term we
run the risk that:
a) many users will be so baffled by technology they won't know when
they are at risk and when they are not.
b) new non-backwards compatible file formats and feature creep
together with potential "palladium only format" enforcement could
start to make it very inconvenient to use non-standard TORs or non
palladium applications.
On top of that there are next gen issues, and on-going legal issues.
For example what happens when this scenario plays out:
1. recent release digital content is published early (same time as
cinema for the right price say).
2. hardware hackers do a break-once run anywhere by ripping all the
content on their hacked machine and start distributing it on kazaa
(2.5 Peta-bytes of ripped content and growing weekly)
3. RIAA/MPAA goes back and lobbys for further controls, DMCA
4. new legal, DMCA and RIAA/MPAA, and competition from Sony pressures
are placed on microsoft
difficult to see that far out what is going to happen, but blase
presumptions that it's unrealistic to expect key-escrow, or certified
TOR only and to assume that Lucky Green's Document Revocation Lists
don't get rolled out with DMCA style laws against interfering with --
that ignores a lot of history.
Also mentioned in previous post: just because it's law doesn't mean it
should be enforced.  People are currently afforded the ability to
ignore the masses of extreme and ridiculous IP law as individuals.
When a large chunk of it gets implemented into DRM, and narcware the
once free-wheeling internet information exchanges will become
marginalized, or underground only affairs -- the world will become
stifled by their own computers acting as the policeman inside -- your
own computer deputized by the US government, DMCA et al.
I think you must have misinterpreted what I said: I wasn't talking
about Palladium APIs.
I was talking about the fact that microsoft has historically used
undocumented APIs as a business tactic, and I presume this is still an
ongoing strategy, and the Palladium hardware architecture could be
used to build a TOR which forced competitors attempting to discover
undocumented APIs in order to fairly compete to resort to hardware
hacking.  Unless and until we get the software copyright analog of
DMCA reverse-engineering restrictions which makes that illegal.
I'm not sure what you mean by "de-centralized trust".  Perhaps that
the TOR is publicly audited?  Perhaps that there are multiple vendors
endorsing SCP implementations?
I think Palladium clearly has possibilities to magnify centralized
It is in microsoft's economic interests, and historically their
modus-operandi to aggresively try to create and exploit control points
to extract monopolistic rents, and/or suppress competition.  And of
course other companies also have used the same strategies, but the
current limits placed on such practices by the ability to reverse
engineer for compatibility may come to be eroded by TCPA/Palladium.
It's nothing personal, it's just that intent is open to evolutionary
change, legislative attack, pressures outside of your personal, or
microsoft's control -- economic and legal incentives will arise which
make the platform deviate from current stated intent.  2002 stated
intent is zip guarantee.
Someone sent me this hugely appropriate quote in email relating to
this point:
except here we are not talking about legislation directly but a
hardware platform with the tools to create different control points
and the legal and business pressures that act upon the use and misuse
of that platform and those created control points.
I can only think that your definitions of user control probably are in
terms of abstract "security" rather than a distrusting viewpoint that
wants to be able to audit and modify application behavior.  I presume you mean "freedom" in the sense of freedom from the ills
that it is claimed Palladium enabled applications could improve.
I think of freedom as the ability for self-determination, to
completely control and audit all aspects of software running on my
system.  Without these freedoms applications and vendors can conspire
against the machine owner.  I suspect the majority of people feel
With current information it seems that the Palladium platform is
damaging to freedom and indivdual liberty, and a future risk to free
society.  I am of course completely open to being proven wrong, and
look forward to seeing more detailed Palladium specs so that this can
be tested, and to see the community given the opportunity to point out
potential more open and less control point prone modifications.
[4] "Security in Open versus Closed Systems (The Dance of Boltzmann,
Coase and Moore)", Ross Anderson,
(Sections 4 and 5 only, rest is unrelated)

@_date: 2002-08-07 17:28:04
@_author: Adam Back 
@_subject: (fwd) Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA)] 
Another Peter Biddle reply to the TCPA/Palladium thread on
----- Forwarded message from "Peter N. Biddle"  -----

@_date: 2002-08-07 18:54:31
@_author: Adam Back 
@_subject: Palladium: technical limits and implications (Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA)) 
I have one gap in the picture: In a previous message in this Peter Biddle said:
But what feature of Palladium stops someone taking a Palladium enabled
application and running it in a virtualized environment.  ie They
write software to emulate the SCP, sealing and attestation APIs.
Any API calls in the code to verify non-virtualization can be broken
by putting a different endoresment root CA public key in the
virtualized SCP.
The Palladium application (without the remote attestation feature) has
no way to determine that it is not virtualized.  If the Palladium
application contains the endoresement root CA key that can be changed.
If the application relies on the SCP to contain the endoresmenet key
but doesn't verify it that can be virtualized with a replacement fake
endoresment root CA public key using the existing SCP APIs.
Then Palladiumized application could be debugged and it's features
used without the Palladium non-virtualization guarantee.
Am I free to do this as the owner of palladium compatible hardware
running a version of windows with the proposed palladium feature set?
If so how does this reconcile with your earlier statement that:
Then we also have the remote attestation feature which can't be so
fooled, but for local attestation does the above break work, or is
there some other function preventing that?
Does that imply that the BORA protections only apply to:
- applications which call home to use remote attestation before
  - and even here the remote attestation has to be enforced to data
    separation levels -- ie it has to be that the server provides a
    required and central part of the application -- like providing the
    content, or keys to the content -- otherwise the remote
    attestation call can also be nopped out with no ill-effect (much
    as calls to dongles with no other effect than to check for
    existance of a dongle could be nopped)
- applications which are encrypted to a machine key which is buried in
  the SCP and endorsed by the hardware manufacturer
  - however you said in your previous mail that this is not planned
* now about my attempts to provide a concise, representative and
  readily understandable summary of what the essence of palladium is:
my previous attempt which you point out some flaws in:
OK that is true.  I presume you focussed on SCP because you took the
dongle to mean literally the SCP component alone.
Let's me try to construct an improved succint Palladium motivation and
function description.  We need to make clear the distinction that the
programmability comes from the hardware/firmware/software ensble
provided by:
hardware: the SCP, new ring0 and code compartmentatlization
(btw what are the Palladium terms for the new ring0 that the TOR runs
in, and what is the palladium term for the code compartment that
Trusted Agents run in -- I'd like to use consistent terminology)
super-kernel: TOR operating in new ring0
software: palladium enabled applications using the features such as
software attestation, and sealing with control rooted in hardware and
the TOR
So would it be fair to characterize the negative aspects of Palladium
as follows:
"Palladium provides an extensible, general purpose programmable
dongle-like functionality implemented by an ensemble of hardware and
software which provides functionality which can, and likely will be
used to expand centralised control points by OS vendors, Content
Distrbuters and Governments."
So I think that statement though obviously less possitively spun than
Microsoft would like perhaps addresses your technical objections with
the previous characterization.
btw I readily concede of course that Palladium platform offers the
security compartmentalization and software assurance aspects anonymous
and Peter Biddle have described; I am just mostly examining the
flip-side of the new boundary of applications buildable to data
separation standards of security with this platform.  One could
perhaps construct a more balanced characterization covering both
positive features and negative risks, but I'll let Palladium
proponents work on the former.
So to discuss your objections to the previous version of my attempted
Palladium characterization:
- as you say the hardware platform does not itself provide control
  points (I agree)
- as you say, the TOR being publicly auditable does not itself provide
  control points
however the platform can, and surely you can admit the risk, and even
the likelihood that it will in fact be used to continue and further
the existing business strategies of software companies, the content
industry and governments.
The dongle soldered to your motherboard can conspire with the CPU and
memory management unit to lock the user out of selected processes
running on their own machine.  If you focus on the subset of buildable applications which operate in
the users interests you can call this a good thing.  If you look at
the risks of buildable applications which can be used to act against
the users interests it can equally be a very dangerous thing.  Also if
you look at historic business practices, legal trends, and the wishes
of the RIAA/MPAA there are risks that these bad practices and trends
will be futher accelerated, exarcerbated and more strongly enforced.
I'd be interested to see you face and comment on these negative
aspects rather than keep your comments solely on beneficial user
functions, claim neutrality of low level features, and disclaim
negative aspects by claiming at a low level user choice.  The low
level user choice may in the long run prove impractical or almost
impossible for novice users, or even advanced users without hardware
hacking tools, to technically exercise.  (I elaborated on the
possiblities for robust format compatibility prevention, and related
possibilities a previous mail.)
I mean it is programmable in the sense that software attestation,
certification and the endoresed new privileged ring0 code, together
with the hardware enforced code compartments allow the protections of
the firmware and hardware running in the SCP to be extended up to
complete applications -- the Trusted Agents running in code
That makes it general purpse because it is programmable.  It could be
used to build many classes of application across a spectrum of good,
debatable and evil:
- more flexibile and secure anonymity systems (clear user
  self-interest as anonymous suggested)
- DRM (mixed positive and negative, debatable depends on your point of
  view)
- and for example key-escrow of SCP support crypto functions
  implemented in the TOR accessed with say CAPI (very negative)
You're focussing on the low level platform specifics, not on the
bigger implications of the overall hardware, TOR, OS and software
ensemble which I was talking about.  Yes you can run different TORS.
But using a specific (and audited published) TOR which the user may
find himself choosing to run in order to run Palladium-enabled
applications, all the applications, file formats, services and content
which are tied to doing that -- control points could and likely will
be built.
It is my opinion that the directions and business pressures are such
that meaningful distributed control is unlikely to be the long term
result of the things that will be built using the Palladium
hardware/software ensemble.
This seems a fairly clearly consistent and predictable outcome, unless
the software, IP law, RIAA/DMCA and legal systems all make an
_astounding_ complete U-turn in policy and tacitcs coincident with the
deployment of Palladium.  Can you defend the arguement that Palladium and TCPA don't change the
balance of control against the user and against personal control in
our current balance between technical feasibility of building software
systems enforcing third party control and law?
(btw I'll stop nagging about documentation now, previous mail crossed
with your reply on the topic).
"You do not examine legislation in the light of the benefits it will
convey if properly administered, but in the light of the wrongs it
would do and the harms it would cause if improperly administered."

@_date: 2002-08-07 21:38:55
@_author: Adam Back 
@_subject: Palladium: hardware layering model (Re: Palladium: technical 
remailer on Wed, Aug 07, 2002 at 12:35:12PM -0700
some definitions:
hw layer -- SCP which I think provides crypto key store, crypto
 	    co-processor for sealing, remote attesation
ring 0 -- new layer which controls memory management unit and secured
code compartments
supervisor mode -- normal supervisor mode, which can now only read
user space, but not trusted agents running in code compartments
user mode -- legacy user level apps under complete control of
supervisor mode
and some ASCII art:
each layer below can decide policy and information disclosure through
APIs to the layer above.  The implications of which are:
- the SCP can implement sealing with data separation against ring-0
(ring-0 can't bypass sealing data separation)
- ring-0 can read all superviser, user, and trusted agent space, but
- ring-0 and MMU can compartmentalize trusted agents so they can't
tamper with each other, and
- ring-0 and MMU can exclude supervisor mode from trusted agent space
and ring-0 space; supervisor mode is itself just another
compartmentalized trusted-agent level space.  Therefore ring-0 can
restrict what supervisor mode (where the normal OS is located) can do.
whereas the normal protected CPU architecture is just:
- from these assumptions it appears an OS could be implemented so that
all OS calls pass through ring-0 APIs and mediation to get to
supervisor mode OS.  In this case the OS could observe system calls
the trusted agent makes, but not in general read, debug, modify
virtualize or modify trusted-agent code.  The non-virtualization
presumes encrypted trusted-agent code, which Peter said is not done,
so this can't be how it works.
I would be interested to hear what model takes for Palladium mapping
the interactions and restrictions between Trusted Agents, user space,
OS kernel, TOR to the hardware.  We need this kind of detail to reason
about limits of the Palladium and make distinctions between what is
possible with Palladium implementation choices vs what other types of
OSes could be built from the hardware features.
One idea I think would be interest is as follows:
- the TOR (which lives in ring-0) _could_ be used together with the OS
to force all trusted-agent in-flows and out-flows (network traffic) to
go through code under supervisor mode control.  I don't think this is likely in the current design; but this change
would be an improvement: - it would at least allow user audit and control of in-flows and
- the user could block suspicious phone-home information out-flows,
- the user could read out-flows and demand un-encrypted documented
formats, or if encrypted, encrypted with keys the supervisor mode gets
copies of.
- similarly in-flow control is interesting, because with no in-flows a
trusted agent could be more liberally allowed to make out-flows (if it
has no input knowledge, and is in a code compartment, and the user
gave it no sensitive it doesn't know anything to leak.)
(Even with encrypted code, or public code which could not otherwise be
audited actively in the sense of debugging it's actual operation to
see what it does in practice in your machine given your data and
circumstances rather than looking at static code and third party
certifications to try to deduce that.  Not all apps may be unencrypted
(a TOR and SCP could clearly be built to support this feature).
So on anonymous comments about OS control:
I'm not sure if anonymous is just generalising when he says the app
can't in any circumstances know anything if the OS is hostile, but I
think it could potentially know things if the OS is hostile.  As I
described with the control and layer I think the palladium hardware
uses.  It seems possible to build some of separations and exclude the
OS from certain types of application.  It depends what you include in
the OS; if the OS includes the TOR, then no.  But it was stated that
the TOR is somewhat independent from the OS.  You could mix and match
and use an MS Palladium TOR with linux potentially (though perhaps not
in practice, it would have to be designed to allow it).  It also
depends on how the OS, trusted agents and supervisor mode is mapped to
the hardware.
Peter seemed to claim these kinds of assurance.  Sealing doesn't
prevent application virtualization, it just prevents the sealed data
being shared between non-virtualized instances of the apps and
virtualized instances.
So I was wondering how Peter could simultaneously claim that
encryption was not used and that "SW can known that it is running on a
given platform."
Remote attestation, which is not itself general -- just a remote
dongle thing -- if not tied to remote dongle controlled sealing which
is necessary for the main application function could be nopped out.
So in the general case it seems that remote attestation is also
effectively virtualizable, modifiable and debuggable by first nopping
out remote attestation checks.  (This is not strictly virtualizable as
the remote dongle call nopping modification makes it no longer the
same application, but as I said unless this is necessary for the
application it doesn't otherwise change it's behavior, so it's
effectively virtualizable).

@_date: 2002-08-07 22:21:42
@_author: Adam Back 
@_subject: Challenge to TCPA/Palladium detractors 
The TCPA/Palladium folks have been working on this for apparently
around 5 years.  We don't yet have a complete definition of what
Palladium is, but anyway...
It may be that some interesting hardware, TOR and OS design changes
could be added which could change the balance.  Other aspects are as
John Denker said more to do with who will control keys and policies
and how much effective user control and choice remains over these
My initial thoughts were around hardware and TOR enforced in-flow and
out-flow control to trusted agents.
This idea was seeded by the smart-card setting of Stefan Brands
digital credentials.  (Read [1] if you are interested, it's a very
clever idea, related to observers in cryptogaphic protocols in
hardware settings).
Briefly the observer in Brands protocol (and observers have been
proposed in other cryptogaphic literature also) tackles an analogous
problem with cryptographic assurance in the special purpose case of
privacy preserving credentials, e-cash and other applications that can
be built from those techniques.
You have a tamper-resistant smart card.  However the user can't
reasonably audit the behavior of the smart-card processor because it
intentionally hides it's keys from the user.  Even if the source is
published, audited, and claims and endoresments about the hardware
made, the user still can't easily audit or reasonablly trust what is
actually in his smart-card.
The tamper-resistant smart card is somewhat related to the crypto
functions of the SCP in Palladium or the TPM in TCPA, but the observer
approach may offer lessons for TCPA/Palladium in general at higher
The tamper-resistant smart card is considered untrusted and hostile to
user privacy.  The tamper-resistant smart card processor and software
is acting in the interests of the credential issuer / ecash issuer to
prevent the user double-spending coins (*) / using credentials more
times than allowed.
The user has a general purpose computer running software he can
completely audit, control observe running and modify.  The smart-card
has to make all communications with ecash acccepting merchants,
certificate verifiers etc via the general purpose computer the
smart-card is connected to.  The general purpose computer implements
the observer protocols.
The smart-card setting variant of Brands protocol cryptographically
assures 2 things:
- the ecash issuer / credential CA can be assured that the user can
not double spend (or in general violate other properties mediated by
the tamper-resistant smart card)
- the user is cryptographically assured that the smart-card can not
invade his privacy.  This works because the in-flows and out-flows to the smart card are
hardware assured to pass via the general purpose computer, auditable,
use published formats and are cryptographically blinded, to the extent
of optimally frustrating even subliminal channels, via steganography
and the like.
In the same way that TCPA/Palladium are a generalisation of the dongle
concept, this would be a generalisation of the cryptographic concept
of observers.
So for your convenience here's a cut and paste of that initial thought
on applying the observer principle to general purpose TCPA/Palladium
platform from the previous message with subject "Palladium: hardware
layering model":
I wrote in that message:
this is not a fully fleshed out idea as I only thought of it
yesterday, and can't fully analyse it's implications because we don't
yet know proper details of what Palladium hardware is, nor how
microsofts proposed Palladium enhanced windows would be implemented on
that hardware.
(*) Actually he will still be caught and identified with Brands ecash
protocols when the coins are deposited if he does double-spend coins
after breaking hardware tamper-resistance, but that is a level of
detailed not central to this discussion.
[1] "A Technical Overview of Digital Credentials", Stefan Brands, Feb
2002, to appear in International Journal on Information Security.
See Section 8.

@_date: 2002-08-07 22:40:56
@_author: Adam Back 
@_subject: wow - palladiated! (Re: Palladium: technical limits and implications) 
that's pretty funny, rhymes with irradiated -- nice connotations of
radioactive material with radioactive half-lives spewing
life-hazardous neutron radiation ;-)
Helps that palladium is in fact a heavy metal.  Man, perhaps Pd even
_has_ a half-life on the decay path from plutonium down to lead or
something.  That would be very funny.
--- end forwarded text

@_date: 2002-08-08 02:48:19
@_author: Adam Back 
@_subject: Palladiated? (was re: wow - palladiated! (Re: Palladium: 
rah on Wed, Aug 07, 2002 at 06:37:51PM -0400
[Trimmed Cc a bit, I'll let Bob decide where it goes beyond this].
Now that Bob coined the neologism "palladiated" (blame Bob -- my
"palladiumized" was not in jest, just used in the middle of a tech
discussion) it has to be done, so I asked the universal oracle
(google.com) about palladium and half-life, and lo the Pd-103 isotope
has a 17-day half-life, and Pd-109 of 13.5 hours and are classified as
having moderate radiotoxicity.  Unfortunately for Bob's neologism not
quite up there with fission grade isotopes like like Plutonium which
rate as very high toxicity, but still you wouldn't want to ingest to
much of the stuff...
Pd isotopes are obtained by bombarding Gold with neutrons apparently.
Anyway, now back to the intersting tech discussion on the balance of
of owner vs third party control in the MS Palladium and TCPA

@_date: 2002-08-08 06:34:15
@_author: Adam Back 
@_subject: Palladium: hardware layering model 
No I think the above diagram is closer than what you propose.  Peter
also pointed us at Seth Schoen's blog [1] which is a write up of a
briefing Microsoft gave to EFF.  It contains the statement:
Looks consistent with my picture to me.
Your other objection:
I think would just be covered by the details of how the machine
switches from this picture:
to the one above.
For example imagine a default stub nub/TOR that leaves the new MMU
features wide open.  (Supervisor mode can access everything, no
Trusted Agent code compartments running).  The would be some API to
allow the supervisor mode code to load a TOR and switch the TOR code
to ring-0 while leaving the OS running in supervisor mode.
Or alternatively and with equivalent effect: with the boot state, the
OS runs in full ring-0 mode, but just isn't written to make use of any
of the extra ring-0 features.  When it switches to loading a nub/TOR
the OS is relagated to supervisor mode, some MMU permission bits are
juggled around and the TOR occupies ring-0, and the TOR is just an OS
micro-kernel which happens to be written to use the new hardware
features (code compartmentalization, new MMU features, sealing etc)
Clarification on this:
Not what I meant.  Say that you have some code that looks like this:
if ( ! remote_attest( /* ... */ ) ) { exit 0; }
then the remote attest is not doing anything apart from acting as a
remote dongle, so all I have to do to virtualize this code, or break
the licensing scheme based on the remote dongle is nop out the remote
attest verification, then the code can be run as a user application
rather than a trusted agent application and so can be run in a
debugger, have it's state examined etc.
If on the other hand the code says:
download_sealed_content( /* ... */ );
key = remote_attest_and_key_negotiate( /* ... */ );
decrypt_sealed_content( key, /* ... */ );
then nopping out the remote_attest will have a deleterious effect on
the applications function, and so virtualizing it with the remote
attests nopped out will not be useful in bypassing it's policies.

@_date: 2002-08-09 20:11:15
@_author: Adam Back 
@_subject: Signing as one member of a set of keys 
+0000
Very nice.  Nice plausible set of candidate authors also:
pub  1022/5AC7B865 1992/12/01  loki at obscura.com
pub  1024/2B48F6F5 1996/04/10  Ian Goldberg pub  1024/97558A1D 1994/01/10  Pr0duct Cypher pub  1024/2719AF35 1995/05/13  Ben Laurie pub  1024/58214C37 1992/09/08  Hal Finney <74076.1041 at compuserve.com>
pub  1024/C8002BD1 1997/03/04  Eric Young pub  1024/FBBB8AB1 1994/05/07  Colin Plumb Wonder if we can figure out who is most likely author based on coding
style from such a small set.
It has (8 char) TABs but other wise BSD indentation style (BSD
normally 4 spaces).  Also someone who likes triply indirected pointers
***blah in there.  Has local variables inside even *if code blocks*
eg, inside main() (most people avoid that, preferring to declare
variables at the top of a function, and historically I think some
older gcc / gdb couldn't debug those variables if I recall).  Very
funky use of goto in getpgppkt, hmmm.  Somewhat concise coding and
variable names.
Off the cuff guess based on coding without looking at samples of code
to remind, probably Colin or Ian.
Of course (Lance Cottrell/Ian Goldberg/Pr0duct Cypher/Ben Laurie/Hal
Finney/Eric Young/Colin Plumb) possibly deviated or mimicked one of
their coding styles.  Kind of interesting to see a true nym in there
Also the Cc -- Coderpunks lives?  I think the Cc coderpunks might be a
clue also, I think some of these people would know it died.  I think
that points more at Colin.
Other potential avenue might be implementation mistake leading to
failure of the scheme to robustly make undecidable which of the set is
the true author, given alpha code.

@_date: 2002-08-09 22:13:56
@_author: Adam Back 
@_subject: TCPA/Palladium -- likely future implications (Re: dangers of TCPA/palladium) 
The picture is related but has some extra wrinkles with the
TCPA/Palladium attestable donglization of CPUs.
- It is always the case that targetted people can have hardware
attacks perpetrated against them.  (Keyboard sniffers placed during
court authorised break-in as FBI has used in mob case of PGP using
Mafiosa [1]).
- In the clipper case people didn't need to worry much if the clipper
chip had malicious deviations from spec, because Clipper had an openly
stated explicit purpose to implement a government backdoor -- there's
no need for NSA to backdoor the explicit backdoor.
But in the TCPA/Palladium case however the hardware tampering risk you
identify is as you say relevant:
- It's difficult for the user to verify hardware.  - Also: it wouldn't be that hard to manufacture plausibly deniable
implementation "mistakes" that could equate to a backdoor -- eg the
random number generators used to generate the TPM/SCP private device
However, beyond that there is an even softer target for would-be
- the TCPA/Palladium's hardware manufacturers endoresment CA keys.
these are the keys to the virtual kingdom formed -- the virtual
kingdom by the closed space within which attested applications and
software agents run.
So specifically let's look at the questions arising:
1. What could a hostile entity(*) do with a copy of a selection of
hardware manufacturer endorsement CA private keys?
( (*) where the hostile entity candidates would be for example be
secret service agencies, law enforcement or "homeland security"
agencies in western countries, RIAA/MPAA in pursuit of their quest to
exercise their desire to jam and DoS peer-to-peer file sharing
networks, the Chinese government, Taiwanese government (they may lots
of equipment right) and so on).
a. Who needs to worry -- who will be targetted?
Who needs to worry about this depends on how overt third-party
ownership of these keys is, and hence the pool of people who would
likely be targetted.  If it's very covert, it would only be used plausibly deniably and only
for Nat Sec / Homeland Security purposes.  It if becomse overt over
time -- a publicly acknowledged, but supposedly court controlled
affair like Clipper, or even more widely desired by a wide-range of
entities for example: keys made available to RIAA / MPAA so they can
do the hacking they have been pushing for -- well then we all need to
To analyse the answer to question 1, we first need to think about
question 2:
2. What kinds of TCPA/Palladium integrity depending "trusted"
applications are likely to be built?
Given the powerful (though balance of control changing) new remotely
attestable security features provided by TCPA/Palladium, all kinds of
remote services become possible, for example (though all to the extent
of hardware tamper-resistance and belief that your attacker doesn't
have access to a hardware endorsement CA private key):
- general Application Service Providers (ASPs) that you don't have to
trust to read your data
- less traceable peer-to-peer applications
- DRM applications that make a general purpose computer secure against
BORA (Break Once Run Anywhere), though of course not secure against
ROCA (Rip Once Copy Everywhere) -- which will surely continue to
happen with ripping shifting to hardware hackers.
- general purpose unreadable sandboxes to run general purpose
CPU-for-rent computing farms for hire, where the sender knows you
can't read his code, you can't read his input data, or his output
data, or tamper with the computation.
- file-sharing while robustly hiding knowledge and traceability of
content even to the node serving it -- previously research question,
now easy coding problem with efficient
- anonymous remailers where you have more assurance that a given node
is not logging and analysing the traffic being mixed by it
But of course all of these distributed applications, positive and
negative (depending on your view point), are limited in their
assurance of their non-cryptographically assured aspects:
- to the tamper resistance of the device
- to the extent of the users confidence that an entity hostile to them
doesn't have the endorsement CA's private key for the respective
remote servers implementing the network application they are relying
and a follow-on question to question 2:
3. Will any software companies still aim for cryptographic assurance?
(cryptographic assurance means you don't need to trust someone not to
reverse engineer the application -- ie you can't read the data because
it is encrypted with a key derived from a password that is only stored
in the users head).
The extended platform allows you to build new classes of applications
which aren't currently buildable to cryptographic levels of assurance.
eg. It allows general purpose policies to be built just by writing
policy code that sits in a Trusted Agent code compartment, without
having to figure out how to do split-trust (a la mixmaster chaining),
or forward-secrecy or secret-sharing or any of the other funky stuff;
you can just implement some policy code and it becomes so.
The danger is people will use it to build applications with squishy
interiors, with no cryptographic assurance.  Forward-secrecy
implemented only by a policy in a Trusted Agent that sets a time-limit
on access.  Anonymity but only in the sense that you trust the
hardware isn't tampered with, etc etc.
It will be really tempting because:
- it's much easier: network distributed crypto protocols are
relatively complex
- you can build things you can't otherwise build, the are currently
unsolved problems with distributed crypto protocols
- even where good crypto protocols exist, people will defend not using
them by claims to paranoia: "What you think the NSA has tampered with
your CPU?", or just laziness, cost of implementation etc
So in short probably mostly the answer will be "No", people won't
still aim for cryptographic assurance.
And so a big networked world of distributed applications with a very
squishy and insecure interior inside the closed world will be built.
The new application spaces squishy interior -- like a corporate
firewall with poor to no internal security -- could be ok if you could
be sure the firewall is 100% guaranteed reliable.  TCPA/Palladium
proponents are effectively claiming it be an air-gap grade firewall
guarding the distributed closed world application spaces squishy
interior.  But there is a problem: there are master keys by-passing
all that -- the endorsement CA's private keys.
4. What coming political battles will result?
a. If TCPA/Palladium systems get built -- and it may be politically
unstoppable given the power of the distributed security paradigm it
opens up -- then the battle of the coming decades will center around
control of access to that squishy interior.  The keys that control
access to the closed world are the endoresment CA private keys.
b. You will see many clipper like attempts by governments attempting
to make policies surrounding conditional access to that closed world:
   - law enforcement access to the endorsement private CA keys
     controlling access, so they can setup sting operations,
     demand that ISPs and ASPs collaborate with virtualized versions
     of network services so they can trace things
   - NSA designed protocols to allow such things, black box mediated,
     court order approved, split database access to hardware
     manufacturers private keys
c. As b. progresses RIAA/MPAA will chime in protesting that:
   - Kazaa2 is distributing 10 exabytes a day of ripped recent release
     content not based on BORA (which is now somewhat harder), but on
     ROCA (Rip Once Copy Anywhere) as the content rippers move into
     hardware hacking territory
   - the RIAA/MPAA can't hack, spoof or jam kazaa2 with bogus content
     because cypherpunks have fixed the protocols using WoT, certified
     content, and other crypto-fu so they can't even observe who's
     downloading what or who's serving what
   - and therefore they also demand access to the closed world so they
     can exert their recent legal rights to hack and DDoS the file
     sharing networks
d. Unauthorised access to the closed environment (by hacking your own
hardware) will become illegal with DMCA like restrictions (if it
wouldn't already be where some relation to copyright could be drawn).
e. Software companies, and OS vendors will follow Microsoft's current
lead into an unholy battle with highlights such as:
   - undocumented APIs to gain advantage over competitors, not only
     hardware hacking required to discover APIs, but attestation to
     ensure only those companies who have licensed the right to use
     the API can use it
   - incompatible file formats to lock out competition with
     hardware tamper resistance levels of assurance, even file formats
     that must have certified documents for applications to open them,
     so even if you had the spec you couldn't be compatible
   - copyright protection with software encrypted for the CPU, so you
     can't even audit the static code
   - software renting models again enforced by hardware
   - whole collection of 2nd generation IP "innovations" which will be
     built on top of such things
     - charge per person you share file in a given document format;
     - charge per format conversion
f. Lucky's Documentat Revocation lists to allow governments, companies
etc to to some extent after the fact control distribution of data
g. Increasingly minute enforcement of repressive levels of IP
tracking, and arbitrarily user hostile, fair-use eroding document
viewing and use policies
5. What could be done to protect the user?
a. implement cryptographic assurance inside the closed space where
possible -- that way if you are targetted by someone able to get
inside it you still have the same protection as now.
b. use web-of-trust techniques to provide an overlay of trust on the
endorsement trust.  ie users endorse their own machines to say "this
is my machine" this implies that either:
- it's not tampered with (presuming the user himself was not a target
of some attack or investigation) - or the only tamperer is the certifying user
web-of-trust overlaid on the hardware endorsement helps as:
  - This makes the endorsement keys less useful in a
    covertly obtained endorsement CA private key scenario
  - Even if there are court authorized law enforcement access to
    endorsement CA private keys, or RIAA/MPAA access to endorsement CA
    private keys you can to the extent of your connectivity in the web
    of trust, better avoid using the services of rogue
    agents inside the closed space.
c. Demand ability to audit information in-flows into trusted agents
where there are unauditable out-flows; demand that this is implemented
in a way which allows code under user control to audit
d. Demand the ability to audit information out-flows, where there are
unauditable in-flows or sensitive user data processed by the
application; similarly demand that this is implemented in a way which
allows code under user control to audit
e. Demand cryptographically assured anonymity protection so that there
are no "trusted third parties" who can link your network usage and
identify you.
e. Other ideas?  (Other than to lobby to prevent the building or use
this model).
[1] "FBI Bugs Keyboard of PGP-Using Alleged Mafioso", 6 Dec 2000,

@_date: 2002-08-10 05:37:30
@_author: Adam Back 
@_subject: p2p DoS resistance and network stability (Re: Thanks, Lucky, for helping to kill gnutella) 
The point that a number of people made is that what is said in the
article is not workable: clearly you can't ultimately exclude chosen
clients on open computers due to reverse-engineering.
(With TCPA/Palladium remote attestation you probably could so exclude
competing clients, but this wasn't what was being talked about).
The client exclusion plan is also particularly unworkable for gnutella
because some of the clients are open-source, and the protocol is (now
since original reverse engineering from nullsoft client) also open.
With closed-source implementations there is some obfuscation barrier
that can be made: Kazaa/Morpheus did succeed in frustrating competing
clients due to it's closed protocols and unpublished encryption
algorithm.  At one point an open source group reverse-engineered the
encryption algorithm, and from there the contained kazaa protocols,
and built an interoperable open-source client giFT
 but then FastTrack promptly changed the
unpublished encryption algorithm to another one and then used remote
code upgrade ability to "upgrade" all of the clients.
Now the open-source group could counter-strike if they had
particularly felt motivated to.  For example they could (1)
reverse-engineer the new unpublished encryption algorithm, and (2) the
remote code upgrade, and then (3) do their own forced upgrade to an
open encryption algorithm and (4) disable further forced upgrades.
(giFT instead after the "ugrade" attack from FastTrack decided to
implement their own open protocol "openFT" instead and compete.  It
also includes a general bridge between different file-sharing
networks, in a somewhat gaim like way, if you are familiar with
I grant you that making simultaneously DoS resistant, scalable and
anonymous peer-to-peer networks is a Hard Problem.  Even removing the
anonymous part it's still a Hard Problem.
Note both Freenet and Mojo try to tackle the harder of those two
problems and have aspects of publisher and reader anonymity, so that
they are doing less well than Kazaa, gnutella and others is partly
because they are more ambitious and tackling a harder problem.  Also
the anonymity aspect possibly makes abuse more likely -- ie the
attacker is provided as part of the system tools to obscure his own
identity in attacking the system.  DoSers of Kazaa or gnutella would
likely be more easily identified which is some deterrence.
I also agree that the TCPA/Palladium attested closed world computing
model could likely more simply address some of these problems.
(Lucky slide critique in another post).

@_date: 2002-08-12 17:14:42
@_author: Adam Back 
@_subject: Palladium: technical limits and implications 
Here's a slightly updated version of the diagram I posted earlier:
+---------------+------------+  +----------------------------+  +----------------------------+  Integrity Metrics in a given level are computed by the level below.
The TOR starts Trusted Agents, the Trusted Agents are outside the OS
control.  Therefore a remote application based on remote attestation
can know about the integrity of the trusted-agent, and TOR.
ring -1/TOR is computed by SCP/hardware; Trusted Agent is computed by
The parallel stack to the right: OS is computed by TOR; Application is
computed OS.
So for general applications you still have to trust the OS, but the OS
could itself have it's integrity measured by the TOR.  Of course given
the rate of OS exploits especially in Microsoft products, it seems
likley that the aspect of the OS that checks integrity of loaded
applications could itself be tampered with using a remote exploit.
Probably the latter problem is the reason Microsoft introduced ring -1
in palladium (it seems to be missing in TCPA).

@_date: 2002-08-12 19:30:00
@_author: Adam Back 
@_subject: Palladium: technical limits and implications 
Peter Biddle, Brian LaMacchia or other Microsoft employees could
short-cut this guessing game at any point by coughing up some details.
Feel free guys... enciphering minds want to know how it works.
(Tim Dierks: read the earlier posts about ring -1 to find the answer
to your question about feasibility in the case of Palladium; in the
case of TCPA your conclusions are right I think).
I thought we went over this before?  My hypothesis is: I presumed
there would be a stub TOR loaded bvy the hardware.  The hardware would
allow you to load a new TOR (presumably somewhat like loading a new
BIOS -- the TOR and hardware has local trusted path to some IO
I don't know what leads you to this conclusion.
How would the OS or user mode apps communicate with trusted agents
with this model?  The TOR I think would be the mediator of these
communications (and of potential communications between trusted
agents).  Before loading a real TOR, the stub TOR would not implement
talking to trusted agents.
I think this is also more symmstric and therefore more likely.  The
trusted agent space is the same as supervisor mode that the OS runs
in.  It's like virtualization in OS360: there are now multiple "OSes"
operating under a micro-kernel (the TOR in ring -1): the real OS and
the multiple trusted agents.  The TOR is supposed to be special
purpose, simple and small enough to be audited as secure and stand a
chance of being so.
The trusted agents are the secure parts of applications (dealing with
sealing, remote attestation, DRM, authenticated path to DRM
implementing graphics cards, monitors, sound cards etc; that kind of
thing).  Trusted agents should also be small, simple special purpose
to avoid them also suffering from remote compromise.  There's limited
point putting a trusted agent in a code compartment if it becomes a
full blown complex application like MS word, because then the trusted
agent would be nearly as likely to be remotely exploited as normal
trusted-agents will also need to use OS services, the way you have it
they can't.
I don't think it's a big problem to replace a stub TOR with a given
TOR sometime after OS boot.  It's analogous to modifying kernel code
with a kernel module, only a special purpose micro-kernel in ring -1
instead of ring 0.  No big deal.
In TCPA which does not have a ring -1, this is all the TPM does
(compute metrics on the OS, and then have the OS compute metrics on
While Trusted Agent space is separate and better protected as there
are fewer lines of code that a remote exploit has to be found in to
compromise one of them, I hardly think Palladium would discard the
existing windows driver signing, code signing scheme.  It also seems
likely therefore that even though it offers lower assurance the code
signing would be extended to include metrics and attestation for the
OS, drivers and even applications.
I take this to mean that as stated somewhere in the available docs the
OS can not observe or even know how many trusted agents are running.
So he's stating that they've made OS design decisions such that the OS
could not refuse to run some code on the basis that a given Trusted
Agent is running.
This functionality however could be implemented if so desired in the

@_date: 2002-08-12 21:07:59
@_author: Adam Back 
@_subject: trade-offs of secure programming with Palladium (Re: Palladium: technical limits and implications) 
I think you are making incorrect presumptions about how you would use
Palladium hardware to implement a secure DRM system.  If used as you
suggest it would indeed suffer the vulnerabilities you describe.
The difference between an insecure DRM application such as you
describe and a secure DRM application correctly using the hardware
security features is somewhat analogous to the current difference
between an application that relies on not being reverse engineered for
it's security vs one that encrypts data with a key derived from a user
In a Palladium DRM application done right everything which sees keys
and plaintext content would reside inside Trusted Agent space, inside
DRM enabled graphics cards which retrict access to video RAM, and
later DRM enabled monitors with encrypted digital signal to the
monitor, and DRM enabled soundcards, encrypted content to speakers.
(The encrypted contentt to media related output peripherals is like
HDCP, only done right with non-broken crypto).
Now all that will be in application space that you can reverse
engineer and hack on will be UI elements and application logic that
drives the trusted agent, remote attesation, content delivery and
hardware.  At no time will keys or content reside in space that you
can virtualize or debug.
In the short term it may be that some of these will be not fully
implemented so that content does pass through OS or application space,
or into non DRM video cards and non DRM monitors, but the above is the
end-goal as I understand it.
As you can see there is still the limit of the non-remote
exploitability of the trusted agent code, but this is within the
control of the DRM vendor.  If he does a good job of making a simple
software architecture and avoiding potential for buffer overflows he
stands a much better chance of having a secure DRM platofrm than if as
you describe exploited OS code or rogue driver code can subvert his
There is also I suppose possibility to push content decryption on to
the DRM video card so the TOR does little apart from channel key
exchange messages from the SCP to the video card, and channel remote
attestation and key exchanges between the DRM license server and the
SCP.  The rest would be streaming encrypted video formats such as CSS
VOB blocks (only with good crypto) from the network or disk to the
video card.
Similar kinds of arguments about the correct break down between
application logic and placement of security policy enforcing code in
Trusted Agent space apply to general applications.  For example you
could imagine a file sharing application which hid the data the users
machine was serving from the user.  If you did it correctly, this
would be secure to the extent of the hardware tamper resistance (and
the implementers ability to keep the security policy enforcing code
line-count down and audit it well).
At some level there has to be a trade-off between what you put in
trusted agent space and what becomes application code.  If you put the
whole application in trusted agent space, while then all it's
application logic is fully protected, the danger will be that you have
added too much code to reasonably audit, so people will be able to
gain access to that trusted agent via buffer overflow.
So therein lies the crux of secure software design in the Palladium
style secure application space: choosing a good break-down between
security policy enforcement, and application code.  There must be a
balance, and what makes sense and is appropriate depends on the
application and the limits of the ingenuity of the protocol designer
in coming up with clever designs that cover to hardware tamper
resistant levels the the applications desired policy enforcement while
providing a workably small and pracitcally auditable associated
trusted agent module.
So there are practical limits stemming from realities to do with code
complexity being inversely proportional to auditability and security,
but the extra ring -1, remote attestation, sealing and integrity
metrics really do offer some security advantages over the current

@_date: 2002-08-12 22:13:58
@_author: Adam Back 
@_subject: trade-offs of secure programming with Palladium (Re: Palladium: technical limits and implications) 
At this point we largely agree, security is improved, but the limit
remains assuring security of over-complex software.  To sum up:
The limit of what is securely buildable now becomes what is securely
auditable.  Before, without the Palladium the limit was the security
of the OS, so this makes a big difference.
Yes some people may design over complex trusted agents, with sloppy
APIs and so forth, but the nice thing about trusted agents are they
are compartmentalized:
If the MPAA and Microsoft shoot themselves in the foot with a badly
designed over complex DRM trusted agent component for MS Media Player,
it has no bearing on my ability to implement a secure file-sharing or
secure e-cash system in a compartment with rigorously analysed APIs,
and well audited code.  The leaky compromised DRM app can't compromise
the security policies of my app.
Also it's unclear from the limited information available but it may be
that trusted agents, like other ring-0 code (eg like the OS itself)
can delegate tasks to user mode code running in trusted agent space,
which can't examine other user level space, nor the space of the
trusted agent which stated them, and also can't be examined by the OS.
In this way for example remote exploits could be better contained in
the sub-division of trusted agent code.  eg. The crypto could be done
by the trusted-agent proper, the mpeg decoding by a user-mode
component; compromise the mpeg-decoder, and you just get plaintext not
keys.  Various divisions could be envisaged.
Given that most current applications don't even get the simplest of
applications of encryption right (store key and password in the
encrypted file, check if the password is right by string comparison is
suprisingly common), the prospects are not good for general
applications.  However it becomes more feasible to build secure
applications in the environment where it matters, or the consumer
cares sufficiently to pay for the difference in development cost.
Of course all this assumes microsoft manages to securely implement a
TOR and SCP interface.  And whether they manage to succesfully use
trusted IO paths to prevent the OS and applications from tricking the
user into bypassing intended trusted agent functionality (another
interesting sub-problem).  CC EAL3 on the SCP is a good start, but
they have pressures to make the TOR and Trusted Agent APIs flexible,
so we'll see how that works out.

@_date: 2002-08-14 16:09:24
@_author: Adam Back 
@_subject: MS on Palladium, DRM and copy-protection (via job ad) 
It seems from this article that perhaps MS already had worked out how
to do copy protection with Palladium, or at least thinks it possible
contrary to what was said at USENIX security:
"control what others can do with [...] software.  [...] develop DRM
[...] and Software Licensing products".
Also again shows that Palladium is quite centrally a DRM platform,
which is kind of obvious from the design, and anyway from the naming
of the associated patent "DRM-OS".

@_date: 2002-08-14 20:02:21
@_author: Adam Back 
@_subject: TCPA/Palladium user interst vs third party interest (Re: responding to claims about TCPA) 
The remote attesation is the feature which is in the interests of
third parties.
I think if this feature were removed the worst of the issues the
complaints are around would go away because the remaining features
would be under the control of the user, and there would be no way for
third parties to discriminate against users who did not use them, or
configured them in given ways.
The remaining features of note being sealing, and integrity metric
based security boot-strapping.
However the remote attestation is clearly the feature that TCPA, and
Microsoft place most value on (it being the main feature allowing DRM,
and allowing remote influence and control to be exerted on users
configuration and software choices).
So the remote attesation feature is useful for _servers_ that want to
convince clients of their trust-worthiness (that they won't look at
content, tamper with the algorithm, or anonymity or privacy properties
etc).  So you could imagine that feature being a part of server
machines, but not part of client machines -- there already exists some
distinctions between client and server platforms -- for example high
end Intel chips with larger cache etc intended for server market by
their pricing.  You could imagine the TCPA/Palladium support being
available at extra cost for this market.
But the remaining problem is that the remote attesation is kind of
dual-use (of utility to both user desktop machines and servers).  This
is because with peer-to-peer applications, user desktop machines are
also servers.
So the issue has become entangled.
It would be useful for individual liberties for remote-attestation
features to be widely deployed on desktop class machines to build
peer-to-peer systems and anonymity and privacy enhancing systems.
However the remote-attestation feature is also against the users
interests because it's wide-spread deployment is the main DRM enabling
feature and general tool for remote control descrimination against
user software and configuration choices.
I don't see any way to have the benefits without the negatives, unless
anyone has any bright ideas.  The remaining questions are:
- do the negatives out-weigh the positives (lose ability to
reverse-engineer and virtualize applications, and trade
software-hacking based BORA for hardware-hacking based ROCA);
- are there ways to make remote-attestation not useful for DRM,
eg. limited deployment, other;
- would the user-positive aspects of remote-attestation still be
largely available with only limited-deployment -- eg could interesting
peer-to-peer and privacy systems be built with a mixture of
remote-attestation able and non-remote-attestation able nodes.

@_date: 2002-08-15 07:06:04
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-15 17:56:25
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
[resend via different node: cypherpunks at lne.com seems to be dead --
primary MX refusing connections]
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-16 01:44:58
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
I think a number of the apparent conflicts go away if you carefully
track endorsement key pair vs endorsement certificate (signature on
endorsement key by hw manufacturer).  For example where it is said
that the endorsement _certificate_ could be inserted after ownership
has been established (not the endorsement key), so that apparent
conflict goes away.  (I originally thought this particular one was a
conflict also, until I noticed that.)  I see anonymous found the same
But anyway this extract from the CC PP makes clear the intention and
an ST based on this PP is what a given TPM will be evaluated based on:
p 20:
(if only they could have managed to say that in the spec).

@_date: 2002-08-16 02:23:05
@_author: Adam Back 
@_subject: employment market for applied cryptographers? 
On the employment situation... it seems that a lot of applied
cryptographers are currently unemployed (Tim Dierks, Joseph, a few
ex-colleagues, and friends who asked if I had any leads, the spate of
recent "security consultant" .sigs, plus I heard that a straw poll of
attenders at the codecon conference earlier this year showed close to
50% out of work).
Are there any more definitive security industry stats?  Are applied
crypto people suffering higher rates of unemployment than general
application programmers?  (From my statistically too small sample of
acquaintances it might appear so.)
If this is so, why is it?
- you might think the physical security push following the world
political instability worries following Sep 11th would be accompanied
by a corresponding information security push -- jittery companies
improving their disaster recovery and to a lesser extent info sec
- governments are still harping on the info-war hype, national
information infrastructure protection, and the US Information Security
Czar Clarke making grandiose pronouncements about how industry ought
to do various things (that the USG spent the last 10 years doing it's
best to frustrate industry from doing with it's dumb export laws)
- even Microsoft has decided to make a play of cleaning up it's
security act (you'd wonder if this was in fact a cover for Palladium
which I think is likely a big play for them in terms of future control
points and (anti-)competitive strategy -- as well as obviously a play
for the home entertainment system space with DRM)
However these reasons are perhaps more than cancelled by:
- dot-com bubble (though I saw some news reports earlier that though
there is lots of churn in programmers in general, that long term
unemployment rates were not that elevated in general)
- perhaps security infrastructure and software upgrades are the first
things to be canned when cash runs short?  - software security related contract employees laid off ahead of
full-timers?  Certainly contracting seems to be flat in general, and
especially in crypto software contracts look few and far between.  At
least in the UK some security people are employed in that way (not
familiar with north america).
- PKI seems to have fizzled compared to earlier exaggerated
expectations, presumably lots of applied crypto jobs went at PKI
companies downsizing.  (If you ask me over use of ASN.1 and adoption
of broken over complex and ill-defined ITU standards X.500, X.509
delayed deployment schedules by order of magnitude over what was
strictly necessary and contributed to interoperability problems and I
think significantly to the flop of PKI -- if it's that hard because of
the broken tech, people will just do something else.)
- custom crypto and security related software development is perhaps
weighted towards dot-coms that just crashed.
- big one probably: lack of measurability of security -- developers
with no to limited crypto know-how are probably doing (and bodging)
most of the crypto development that gets done in general, certainly
contributing to the crappy state of crypto in software.  So probably
failure to realise this issue or perhaps just not caring, or lack of
financial incentives to care on the part of software developers.
Microsoft is really good at this one.  The number of times they
re-used RC4 keys in different protocols is amazing!
Other explanations?  Statistics?  Sample-of-one stories?
yes, still employed in sofware security industry; and in addition have
been doing crypto consulting since 97 ( if
you have any interesting applied crypto projects; reference
commissions paid.

@_date: 2002-08-18 16:58:56
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
With Brands digital credentials (or Chaums credentials) another
approach is to make the endorsement key pair and certificate the
anonymous credential.  That way you can use the endorsement key and
certificate directly rather than having to obtain (blinded) identity
certificates from a privacy CA and trust the privacy CA not to issue
identity certificates without seeing a corresponding endorsement
However the idea with the identity certificates is that you can use
them once only and keep fetching new ones to get unlinkable anonymity,
or you can re-use them a bit to get pseudonymity where you might use a
different psuedonym for a different service where you are anyway
otherwise linkable to a given service.
With Brands credentials the smart card setting allows you to have more
compact and computationally cheap control of the credential from
within a smart card which you could apply to the TPM/SCP.  So you can
fit more (unnamed) pseudonym credentials on the TPM to start with.
You could perhaps more simply rely on Brands credential lending
discouraging feature (ability to encode arbitrary values in the
credential private key) to prevent break once virtualize anywhere.
For discarding pseudonyms and when you want to use lots of pseudonyms
(one-use unlinkable) you need to refresh the certificates you could
use the refresh protocol which allows you to exchange a credential for
a new one without trusting the privacy CA for your privacy.
Unfortunately I think you again are forced to trust the privacy CA not
to create fresh virtualized credentials.  Perhaps there would be
someway to have the privacy CA be a different CA to the endorsement CA
and for the privacy CA to only be able to refresh existing credentials
issued by the endorsement CA, but not to create fresh ones.
Or perhaps some restriction could be placed on what the privacy CA
could do of the form if the privacy CA issued new certificates it
would reveal it's private key.
"Also relevant is An Efficient System for Non-transferable Anonymous
Credentials with Optional Anonymity Revocation", Jan Camenisch and
Anna Lysyanskaya, Eurocrypt 01
These credentials allow the user to do unlinkable multi-show without
involving a CA.  They are somewhat less efficient than Chaum or Brands
credentials though.  But for this application does this removes the
need to trusting a CA, or even have a CA: the endorsement key and
credential can be inserted by the manufacturer, can be used
indefinitely many times, and are not linkable.
As you point out unlinkable anonymity tends to complicate revocation.
I think Camenisch's optional anonymity revocation has similar
properties in allowing a designated entity to link credentials.
Another less "TTP-based" approach to unlinkable but revocable
credentials is Stubblebine's, Syverson and Goldschlag, "Unlinkable
Serial Transactions", ACM Trans on Info Systems, 1999:
(It's quite simple you just have to present and relinquish a previous
pseudonym credential to get a new credential; if the credential is due
to be revoked you will not get a fresh credential.)
I think I would define away the problem of local breaks.  I mean the
end-user does own their own hardware, and if they do break it you
can't detect it anyway.  If it's anything like playstation mod-chips
some proportion of the population would in fact would do this.  May be
1-5% or whatever.  I think it makes sense to just live with this, and
of course not make it illegal.  Credentials which are shared are
easier to revoke -- knowledge of the private keys typically will
render most schemes linkable and revocable.  This leaves only online
lending which is anyway harder to prevent.

@_date: 2002-08-21 03:24:21
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
There was some off-list discussion about possibility for sharing these
credentials once a given credential is extracted from it's TPM by a
user who broke the tamper resistance of his TPM.
I also said:
Because Camenisch credentials are unlinkable multi-show it makes it
harder to recognize sharing, so the user could undetectably share
credentials with a small group that he trusts.  (By comparison with linkable pseudonymous credentials and a privacy CA
the issuer and/or verifier would see unusually high activity from a
given pseudonym or TPM endorsement key if the corresponding credential
were shared too widely.)
However if the Camenisch (unlinkable multi-show) credential were
shared too widely the issuer may also learn the secret key and hence
be able to link and so revoke the overly-shared credentials.  This
combats sharing though to a limited extent.
Another idea to improve upon this inherent risk of sharing too widely
may be to use a protocol which it is not safe to do parallel shows
with.  (Some ZKPs are not secure when you engage in multiple show
protocols in parallel.  Usually this is considered a bad thing, and
steps are taken to allow safe parallel show.)  For this application a show protocol which it is not safe to engage in
parallel shows may frustrate sharing: someone who shared the
credential too widely would have difficulty coordinating amongst the
sharees not to show the same credential in parallel.  I notice
Camenisch et al mention steps to avoid parallel showing problem, so
perhaps that feature could be reintroduced.
In contrast, the TPM can easily ensure that the credential is not used
in parallel shows.

@_date: 2002-08-21 14:43:41
@_author: Adam Back 
@_subject: alternate dos pgp client? 
I put together a list of openpgp related software at:
this includes library only code, and add on software.
Not sure about your questions about key versions, but I forwarded it
to Ulf Moeller and Len Sassaman (current maintainer of mix3).
command line.  There was also Tom Zerucha's reference openPGP code,
which is command line but it's alpha level code I think and no longer

@_date: 2002-08-22 16:54:51
@_author: Adam Back 
@_subject: the underground software vulnerability marketplace and its hazards (fwd) 
Right.  And I fail to see how any of this is dangerous.
Clearly people are free to sell information they create to anyone they
choose under any terms they choose.  (For example the iDEFENSE promise
of the author to not otherwise reveal for 2 weeks to give iDEFENSE
some value.)
This commercialisation seems like a _good thing_ as it may lead to
more breaks being discovered, and hence more secure software.
(It won't remain secret for very long -- given the existance of
anonymous remailers etc., but the time-delay in release allows the
information intermediary -- such as iDEFENSE -- to sell the
information to parties who would like it early, businesses for example
people with affected systems.
Criminal crackers who can exploit the information just assist in
setting a fair price and forcing vendors and businesses to recognise
the true value of the information.  Bear in mind the seller can not
know or distinguish between a subscriber who wants the information for
their own defense (eg a bank or e-commerce site, managed security
service provider), and a cracker who intends to exploit the
information (criminal organisation, crackers for amusement or
discovery of further inforamtion, private investigators, government
agencies doing offensive information warfare domesticaly or
I don't see any particular moral obligation for people who put their
own effort into finding a flaw to release it to everyone at the same
time.  Surely they can release it earlier to people who pay them to
conduct their research, and by extension to people who act as
intermediaries for the purpose of negotiating better terms or being
able to package the stream of ongoing breaks into more comprehensive
subscription service.  I think HP were wrong, and find their actions in trying to use legal
scare tactics reprehensible: they should either negotiate a price, or
wait for the information to become generally available.

@_date: 2002-08-24 17:11:42
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
Since writing this I realised that there is a problem revoking
unlinkable multi-show credentials:
- I was presuming that revealing the credential and it's secret key is
sufficient to allow someone to link shows of that credential.
- but to link you'd have to try each revoked credential in turn.
Therefore the verifier would have to perform work linear in the number
of revoked credentials at each show, for the duration of the epoch.
Anonymous suggests one way out is to just define that the issuing CA
and the refreshing CA to be the same entity.  Then you already have to
trust the hardware manufacturer not to issue certs whose secrets are
outside of a TPM.  In this case Brands or Chaum credentials work.
The remaining desiderata are:
- it is not ideal from a risk management perspective to have to have
the hardware manufacturers endorsement private key online to refresh
certificates (or in general for there to be any private key online
that allows issuing of credentials whose private keys lie outside a
- not ideal to have to have an online protocol with an otherwise
non-existant third party (credential refresh CA) in order to avoid
Other ideas I gave in an earlier post towards fixing these remaining
issues now that it seems unlinkable multi-show credentials won't work:

@_date: 2002-07-02 20:53:30
@_author: Adam Back 
@_subject: [OT] why was private gold ownership made illegal in the US? 
mdpopescu on Tue, Jul 02, 2002 at 08:46:46PM +0300
Just curious, but what was the rationale under which private posession
of gold was made illegal in the US?  It boggles the mind...

@_date: 2002-07-03 23:33:01
@_author: Adam Back 
@_subject: personal freedom vs copyright (Re: Hayek was right. Twice.) 
There's been some recent discussion of ethics and markets relating to
copyright prompted by the Orwellian sounding overtones of the latest
Microsoft powergrab.
Seems about time to replay my periodic reminder that copyright is not
a black-and-white moral issue, it is merely a societal convention
which given public appetites for file sharing, and extreme difficulty
of preventing the public continuing apace (kazaa has some millions of
users online, with 2 peta-bytes of shared files and growing), it seems
to me that the natural evolution of laws etc would be for the laws
surrounding copyright be revoked as out-dated and no longer
applicable in an era of digital copying.  Without this adjustment
reality and content distribution laws are getting increasingly
out-of-synch, which is going to lead to some probable very undesirable
side effects in more laws further tilting the playing field in the
favor of the big media cartels, and starting to lead to very draconian
and Orwellian systems enforced under force of law.
Copyright is effectively a massive corporate welfare program to the
benefit of the media cartels at this point.  It's a business model
protection racket with the government providing the thugs at no
expense to the business.  No wonder the businesses that benfit from
this want to lobby to maintain this free enforcement corporate welfare
handout.  They get the financial benefits, and don't care about the
negative societal implications, such as described in Stallmann's
prescient essay on the long term implications of the coming brawl.
I don't see that the media cartels -- the main short-term benefactors
and lobbyists of the current and rapidly expanding copyright laws have
any moral right to have these conventions and corporate welfare
continue.  If society just said no, which it would appear of the
internet population they largely are, I think it likely we'd still
have movies, music etc., and that artists would continue to make money
and businesses associated with managing artists works would also make
money; the landscape might look a little different but so what.  Also,
even if one type of business model or content was no longer
economically supported, I can't see how that's a loss, or a bad thing

@_date: 2002-07-05 04:54:52
@_author: Adam Back 
@_subject: copyright restrictions are coercive and immoral (Re: Piracy is wrong) 
I agree with the Anonymous posters analysis.
I would further elaborate with regard to current copyright related
- parties are free to enter into NDA or complex distribution and use
contracts surrounding exchange of content or information generally as
anonymous describes, and this is good and non-coercive
- but that private contract places no burden on other parties if that
agreement is broken and the content distributed anyway.  This is
exactly analogous to the trade secret scenario where once the trade
secret is out, it's tough luck for the previous trade secret owner --
clearly it's no longer a secret.
- where I find current copyright laws at odds with a coercion free
society is in placing restrictions on people who did not agree to any
NDA contract.  ie. There are laws which forbid copying or use of
information by people who never entered into any agreement with the
copyright holder, but obtained their copy from a third party.
- in a free society (one without a force monopoly central government)
I don't think copyright would exist -- voluntary agreements -- NDAs of
the form anonymous describes -- would be the only type of contract.
- the only form of generally sanctioned force would be in response to
violence initiated upon oneself.
- if the media cartels chose to hire their own thugs to threaten
violence to people who did not follow the cartels ideas about binding
people to default contracts they did not voluntarily enter into, that
would be quite analogous to the current situation where the media
cartels are lobbying government to increase the level of the threats
of violence, and make more onerous the terms of the non-voluntary
contracts.  (Also in a free society individuals would be able to employ the
services of security firms protection services to defend themselves
from the media cartels thugs, as the media cartels would not have the
benefit of a force monopoly they have the lobbying power to bribe to
obtain enforcement subsidies).

@_date: 2002-07-08 22:20:38
@_author: Adam Back 
@_subject: movie distribution post copyright (Re: Artists) 
But right now copies of recent release movies (post screen release,
but pre DVD/VHS relase) are not generally available in high quality
format, suitable for projecting.
So one way that the movie distribution industry could plausibly
continue to make money would be rather than the movie theatre being
subject to copyright laws forbidding them from copying and further
distributing, they would be under a private contract not to do that.
Actually I'm not sure what they're doing now -- it would seem likely
that both private contract and copyright are used -- the movie
distributors may easily want to impose more restrictions than those
directly imposed by default copyright.
Post copyright, with private contract only, the movie theatre would
have an interest to comply with the contract due to the penalties
agreed to in the contract, which might include fines, escrowed monies,
or no access to further releases.
The movie industry has so far been succesful from what I've seen in
preventing DVD quality copies being distributed prior to DVD release.
Publicly distributed copies of pre-DVD release movies are "Screeners"
obtained with a CAM corder in the theatre.  Early releases
(unauthorised distribution shortly before general public release) come
from journalists or their guests making screeners from the pre-release
screenings offered to journalists.
The advent of digital projection which doesn't have much deployment at
theatres yet may alter this equation as perhaps it would then become
easier for an insider (a theatre projectionist for example) to convert
the content into MPEG4/DIVX format and retain good quality.

@_date: 2002-07-12 20:13:33
@_author: Adam Back 
@_subject: Rant: The U.S. facing the largest financial collapse ever 
Tim describes how US national debt may be as high as US$200k /
household.  Now some interesting question related questions are:
- who is that debt owed to?
- what proportion of current year US tax revenues go to service that
some of the debt may not be being serviced (no interest paid and just
left to increase -- eg pensions etc, but this just makes the problem
worse as the future debt will grow faster with no interest paid).
Some completely back of the envelope calculation: if the average US
household has an annual income of US$50k, and the interest rate on the
US national debt is 5%, that interest payments represent 20% of the
average US households gross income.  But isn't 20% fairly close to
what the average household's direct tax rate?
How close is the US to reaching a standstill where 100% of collectible
tax revenue goes to fund debt service, and all current spending comes
from increased future debt?

@_date: 2002-07-23 20:24:26
@_author: Adam Back 
@_subject: Tunneling through hostile proxy 
This isn't just the default behavior; it's the only defined behavior
While it's _possible_ to do this, I've never heard of a server hosted
application that advertises that it's doing this.  I would think it
would be quite hard to get a CA to issue you a certificate if this is
what you intended to do with it (act as a general MITM on SSL
connections you proxy).
There have been applications which do this locally eg. a no longer
shipped product called SafePassage by c2.net, and achilles a SSL
debugger both of which are local proxies and both of which ask the
user to install a certificate allowing this when they are installed.
The installed certificate is self-signed however, and not issued by a
CA, as it is only valid for that user machine anyway, the user won't
want to buy a cert to authenticate information to their own machine,
it would be less secure to do so, and the user won't want to pay for
this certificate.
Is there any software actually doing this?  (I know wild card certs
are available, but would think a wild card cert on .com would be a
very dangerous thing for a CA to issue, and you'd hope browsers would
be smart enough to reject such certs).
This is what SafePassage et al do.

@_date: 2002-07-31 21:34:35
@_author: Adam Back 
@_subject: document popularity estimation / amortizable hashcash (Re: 
g>; from eugen on Wed, Jul 31, 2002 at 04:25:30PM +0200
I proposed a construct which could be used for this application:
called "amortizable hashcash".
The application I had in mind was also file sharing.  (This was
sometime in Mar 2000).  I described this problem as the "disitrbuted
document popularity estimation" problem.  The other aspect of the
problem is you have to distribute the popularity estimate and make it
accessible, so I think you want it to be workably compact (you don't
want to ship around 1 million hash collisions on the document hash).
Amortizable hashcash addresses this problem.
There is also some discussion of it here:

@_date: 2002-06-06 17:12:42
@_author: Adam Back 
@_subject: overcoming ecash deployment problems (Re: all about transferable off-line ecash) 
I think you are assuming things about rational economic behavior when
a money system is subject to high deflation.
Consider during periods of high inflation people don't like holding
money, as it devalues too fast.  They will hold interest bearing
deposits instead.
During periods of high deflation, they will hold cash if it is the
most attractive "investment".  The result will be shortage of cash,
for people who actually want to use it to make purchases because
investors will buy all of it.
Perhaps there are some government monetary systems in history which
had this problem.  For example gold with sudden shortage of gold
supply, or similar.

@_date: 2002-06-12 15:39:16
@_author: Adam Back 
@_subject: What's with all the spam?... 
is quite close to that.
The moderation policy is:
- obvious spam gets dropped.
- one-line pointers to news articles will tend to get dropped.
- news articles posted in full without comment will tend to get dropped.
- content will tend to get passed, even if it's off-the-wall
  (eg. Xenix Chainsaw Massacre). - submissions from high signal posters will tend to get passed.
- the list is read-only. submissions should go to cypherpunks at minder.net
  or another CDR node.
- cypherpunks at minder.net will remain unfiltered and unmoderated.
To subscribe, send the text "subscribe cypherpunks-moderated" to
majordomo at minder.net.
it's archived here:

@_date: 2002-06-26 20:37:12
@_author: Adam Back 
@_subject: Ross's TCPA paper 
Hear, hear!  First post on this long thread that got it right.
Not sure what the rest of the usually clueful posters were thinking!
DRM systems are the enemy of privacy.  Think about it... strong DRM
requires enforcement as DRM is not strongly possible (all bit streams
can be re-encoded from one digital form (CD->MP3, DVD->DIVX),
encrypted content streams out to the monitor / speakers subjected to
scrutiny by hardware hackers to get digital content, or A->D
reconverted back to digital in high fidelity.
So I agree with Bear, and re-iterate the prediction I make
periodically that the ultimate conclusion of the direction DRM laws
being persued by the media cartels will be to attempt to get
legislation directly attacking privacy.
This is because strong privacy (cryptographically protected privacy)
allows people to exchange bit-strings with limited chance of being
identified.  As the arms race between the media cartels and DRM
cohorts continues, file sharing will start to offer privacy as a form
of protection for end-users (eg. freenet has some privacy related
features, serveral others involve encryption already).
There is lots of technical difference.  When was the last time you saw
your doctor use cryptlopes, watermarks etc to remind himself of his
obligations of privacy.
The point is that with privacy there is an explicit or implied
agreement between the parties about the handling of information.  The
agreement can not be technically *enforced* to any stringent degree.
However privacy policy aware applications can help the company avoid
unintentionally breaching it's own agreed policy.  Clearly if the
company is hostile they can write the information down off the screen
at absolute minimum.  Information fidelity is hardly a criteria with
private information such as health care records, so watermarks, copy
protect marks and the rest of the DRM schtick are hardly likely to
Privacy applications can be successful to the in helping companies
avoid accidental privacy policy breaches.  But DRM can not succeed
because they are inherently insecure.  You give the data and the keys
to millions of people some large proportion of whom are hostile to the
controls the keys are supposedly restricting.  Given the volume of
people, and lack of social stigma attached to wide-spread flouting of
copy protection restrictions, there are ample supply of people to
break any scheme hardware or software that has been developed so far,
and is likely to be developed or is constructible.
I think content providors can still make lots of money where the
convenience, and /or enhanced fidelity of obtaining bought copies
means that people would rather do that than obtain content on the net.
But I don't think DRM is significantly helping them and that they ware
wasting their money on it.  All current DRM systems aren't even a
speed bump on the way to unauthorised Net re-distribution of content.
Where the media cartels are being somewhat effective, and where we're
already starting to see evidence of the prediction I mentioned above
about DRM leading to a clash with privacy is in the area of
criminalization of reverse engineering, with Skylarov case, Ed
Felten's case etc.  Already a number of interesting breaks of DRM
systems are starting to be released anonymously.  As things heat up we
may start to see incentives for the users of file-sharing for
unauthorised re-distribution to also _use_ the software anonymsouly.
Really I think copyright protections as being exploited by media
cartels need to be substantially modified to reduce or remove the
existing protections rather than further restrictions and powers
awareded to the media cartels.

@_date: 2002-06-26 23:03:08
@_author: Adam Back 
@_subject: DRMs vs internet privacy (Re: Ross's TCPA paper) 
I don't mean that you would necessarily have to correlate your viewing
habits with your TrueName for DRM systems.  Though that is mostly
(exclusively?) the case for current deployed (or at least implemented
with a view of attempting commercial deployment) copy-mark
(fingerprint) systems, there are a number of approaches which have
been suggested, or could be used to have viewing privacy.
Brands credentials are one example of a technology that allows
trap-door privacy (privacy until you reveal more copies than you are
allowed to -- eg more than once for ecash).  Conceivably this could be
used with a somewhat online, or in combination with a tamper-resistant
observer chip in lieu of online copy-protection system to limit
someone for example to a limited number of viewings.
Another is the "public key fingerprinting" (public key copy-marking)
schemes by Birgit Pfitzmann and others.  This addresses the issue of
proof, such that the user of the marked-object and the verifier (eg a
court) of a claim of unauthorised copying can be assured that the
copy-marker did not frame the user.
Perhaps schemes which combine both aspects (viewer privacy and
avoidance of need to trust at face value claims of the copy-marker)
can be built and deployed.
(With the caveat that though they can be built, they are largely
irrelevant as they will no doubt also be easily removable, and anyway
do not prevent the copying of the marked object under the real or
feigned claim of theft from the user whose identity is marked in the
But anyway, my predictions about the impending collision between
privacy and the DRM and copy protection legislation power-grabs stems
from the relationship of privacy to the later redistrubtion
observation that:
1) clearly copy protection doesn't and can't a-priori prevent copying
and conversion into non-DRM formats (eg into MP3, DIVX)
2) once 1) happens, the media cartels have an interest to track
general file trading on the internet;
3) _but_ strong encryption and cryptographically enforced privacy mean
that the media cartels will ultimately be unsuccessful in this
4) _therefore_ they will try to outlaw privacy and impose escrow
identity and internet passports etc. and try to get cryptographically
assured privacy outlawed.  (Similar to the previous escrow on
encryption for media cartel interests instead of signals intelligence
special interests; but the media cartels are also a powerful
Also I note an slip in my earlier post [of Bear's post]:
Ross Anderson's comments were also right on the money (as always).

@_date: 2002-05-23 21:58:48
@_author: Adam Back 
@_subject: why OpenPGP is preferable to S/MIME (Re: NAI pulls out the 
from adam on Thu, May 23, 2002 at 03:05:49PM -0400
This won't achieve the desired effect because it will just destroy the
S/MIME trust mechanism.  S/MIME is based on the assumption that all
CAs are trustworthy.  Anyone can forge any identity for clients with
that key installed.  S/MIME isn't really compatible with the web of
trust because because of the two tier trust system -- all CAs are
assumed trustworthy and all users are not able to sign anything.  By
issuing a key and revealing it's private key, you elevate a rogue user
to being a CA and then the system would be broken.
I think you'd have to do it in reverse to stand a chance if you
literally published the private key -- they're never going to add the
public key for a known compromised private key.  Also it costs lots of
money, and takes some time to take effect.

@_date: 2002-05-25 05:13:36
@_author: Adam Back 
@_subject: S/MIME and web of trust (was Re: NAI pulls out the DMCA 
on Fri, May 24, 2002 at 04:40:36PM -0700
The S/MIME aware MUAs do not ignore the trust delegation bit.
Therefore you can not usefully sign other certs with a user grade
certificate from verisign et al.  If you make your own CA key (with
the trust delegation bit set) and self-sign it, S/MIME aware MUAs will
also flag signatures made with it as invalid signatures because your
self-signed "CA" key is not signed by a CA in the default trusted CA
key database.
While it is true that you can extend X.509v3 I don't see how useful it
would be to add a WoT extension until it got widely deployed.
Recipient MUAs will at best ignore your extensions, and worse will
fail on them until support for such an extension is deployed.  I view
the chances of such an extension getting deployed as close to nil.
The S/MIME MUA / PKI library / CA cartel has a financial incentive to
not deploy it -- as they view it as competition to the CAs business.

@_date: 2002-10-17 19:15:38
@_author: Adam Back 
@_subject: palladium presentation - anyone going? 
Would someone at MIT / in Boston area like to go to this and send a
report to the list?  Might help clear up some of the currently
unexplained aspects about Palladium, such as:
- why they think it couldn't be used to protect software copyright (as
the subject of Lucky's patent)
- are there plans to move SCP functions into processor?  any relation
to Intel Lagrange
- isn't it quite weak as someone could send different information to
the SCP and processor, thereby being able to forge remote attestation
without having to tamper with the SCP; and hence being able to run
different TOR, observe trusted agents etc.
I notice at the bottom of the talk invite it says but in this case how does it meet the BORA prevention.  Is it BORA
prevention _presuming_ the local user is not interested to reconfigure
his own hardware?
Will it really make any significant difference to DRM enforcement
rates?  Wouldn't the subset of the file sharing community who produce
DVD rips still produce Pd DRM rips if the only protection is the
assumption that the user won't make simple hardware modifications.
-------- Original Message --------
Open to the Public
Time:     10:30 a.m.- 12:00 noon Place:    NOTE: NE43-518, 200 Tech Square Title:    Palladium
Speaker:  Brian LaMacchia, Microsoft Corp.
Hosts:    Ron Rivest and Hal Abelson
Abstract: This talk will present a technical overview of the Microsoft
"Palladium" Initiative.  The "Palladium" code name refers to a set of
hardware and software security features currently under development
for a future version of the Windows operating system.  "Palladium"
adds four categories of security services to today's PCs:
  a. Curtained memory. The ability to wall off and hide pages of main
memory so that each "Palladium" application can be assured that it is
not modified or observed by any other application or even the
operating system.
  b. Attestation. The ability for a piece of code to digitally sign
or otherwise attest to a piece of data and further assure the
signature recipient that the data was constructed by an unforgeable,
cryptographically identified software stack.
  c. Sealed storage. The ability to securely store information so
that a "Palladium" application or module can mandate that the
information be accessible only to itself or to a set of other trusted
components that can be identified in a cryptographically secure
  d. Secure input and output. A secure path from the keyboard and
mouse to "Palladium" applications, and a secure path from "Palladium"
applications to an identifiable region of the screen.
Together, these features provide a parallel execution environment to
the "traditional" kernel- and user-mode stacks.  The goal of
"Palladium" is to help protect software from software; that is, to
provide a set of features and services that a software application can
use to defend against malicious software also running on the machine
(viruses running in the main operating system, keyboard sniffers,
frame grabbers, etc).  "Palladium" is not designed to provide defenses
against hardware-based attacks that originate from someone in control
of the local machine.

@_date: 2002-10-21 22:52:20
@_author: Adam Back 
@_subject: palladium presentation - anyone going? 
It doesn't sound breakable in pure software for the user, so this
forces the user to use some hardware hacking.
They disclaimed explicitly in the talk announce that:
However I was interested to know exactly how easy it would be to
defeat with simple hardware modifications or reconfiguration.
You might ask why if there is no intent for Palladium to be secure
against the local user, then why would the design it so that the local
user has to use (simple) hardware attacks.  Could they not, instead of
just make these functions available with a user present test in the
same way that the TOR and SCP functions can be configured by the user
(but not by hostile software).
For example why not a local user present function to lie about TOR
hash to allow debugging (for example).
A "trusted bit" in the segment register doesn't make it particularly
hard to break if you have access to the hardware.
For example you could:
- replace your RAM with dual-ported video RAM (which can be read using
alternate equipment on the 2nd port).
- just keep RAM powered-up through a reboot so that you load a new TOR
which lets you read the RAM.
But how will the SCP know that the hash it reads comes from the
processor (as opposed to being forged by the user)?  Is there any
authenticated communication between the processor and the SCP?

@_date: 2002-10-22 16:52:16
@_author: Adam Back 
@_subject: Palladium -- trivially weak in hw but "secure in software"?? (Re: palladium presentation - anyone going?) 
Remote attestation does indeed require Palladium to be secure against
the local user.  However my point is while they seem to have done a good job of
providing software security for the remote attestation function, it
seems at this point that hardware security is laughable.
So they disclaim in the talk announce that Palladium is not intended
to be secure against hardware attacks:
so one can't criticise the implementation of their threat model -- it
indeed isn't secure against hardware based attacks.
But I'm questioning the validity of the threat model as a realistic
and sensible balance of practical security defenses.
Providing almost no hardware defenses while going to extra-ordinary
efforts to provide top notch software defenses doesn't make sense if
the machine owner is a threat.
The remote attestation function clearly is defined from the view that
the owner is a threat.
Without specifics and some knowledge of hardware hacking we can't
quantify, but I suspect that hacking it would be pretty easy.  Perhaps
no soldering, $50 equipment and simple instructions anyone could
more inline below...
I think standard memory could be used.  I can think of simple
processor modifications that could fix this problem with hardware
tamper resistance assurance to the level of having to tamper with .13
micron processor.  The processor is something that could be epoxyied
inside a cartridge for example (with the cartridge design processor +
L2 cache housings as used by some Intel pentium class processors),
though probably having to tamper with a modern processor is plenty
hard enough to match software security given software complexity

@_date: 2002-10-25 02:37:32
@_author: Adam Back 
@_subject: internet radio - broadcast without incurring royalty fees 
Re. the recent rapacious "broadcast" royalties imposed on internet
radio in the US, it occurs to me it wouldn't be that hard to do the
following and it would probably avoid the royalties even under the
current imbalanced IP laws:
- have the station broadcast it's own content (commentary)
- have the station broadcast song titles, song authors, CDDB serial numbers
- the user would use third-party software capable of playing the
recommended track, such as:
- coincidentally owning the CD and having the CD in a CD jukebox
- owning (or not) the CD and having a mp3 rip of the track on hard
  disk
- queueing the track for download via kazaa
examples of the last are the morpheus plugin for winamp (I think it
was morpheus that had such a plugin -- though it is probably no longer
supported with the morpheus protocol switch).
For performance reasons the station could even pre-queue the tracks
during their commentary and then trigger the start of play after the
track has had some time to be selected by the jukebox / streaming
buffer fill from kazaa.
Seems to me this would pass current IP laws because it is like a radio
station which broadcast the name of a song and the user is expected to
insert the CD in his player and play along to keep up with the
commentary, only automated and with open APIs for the "load and play
this CD track" instructions so people can hook it up to whatever is
convenient to them.

@_date: 2002-10-31 00:28:56
@_author: Adam Back 
@_subject: patent free(?) anonymous credential system pre-print 
Some comments on this paper comparing efficiency, and functionality
with Camenisch, Chaum, Brands.
- efficiency
The non-interactive cut and choose protocol results in quite big
messages in the issuing and showing protcols to attain good security.
The user who wishes to cheat must create n/2 false attributes, and n/2
true attributes.  (True attributes being the ones he will try to
convince the CA are encoded in all the attributes).  The user can in
an offline fashion keep trying different combinations of false and
true attributes until he finds one where the attributes selected for
disclosure during issuing are the n/2 true attributes.  Then in the
showing protocol he can show the n/2 false attributes.
But C(n,n/2) grows sub-exponentially and so the user has to for
example encode 132 blinded hashed attributes to provide assurance of
work factor of 2^128 to the CA.  (C(132,66) ~ 2^128).  Without looking
in detail at what must be sent I presume each the issuing message for
a single credential would be order of 10KB.  Similar for the showing
Computational efficiency is probably still better than Camenisch
credentials despite the number of attribute copies which must be
blinded and unblinded, but of course less efficient than Brands.
- functionality
The credentials have a relatively inefficient cut-and-choose based
issuing and showing protocol.  Brands has efficient issuing protocols
which support offline showing.  Chaum's basic offline credentials are
based on interactive cut-and-choose, but there is an efficient
variant [1].
As with Brands and Chaum's certificates if they are shown multiple
times they are linkable.  (Camenisch offers unlinkable multi-show but
they are quite inefficient).
The credentials can be replayed (as there is no credential private
key, a trace of a credential show offers no defense against replay).
Brands credentials have a private key so they can defend against this.
(Chaum's credentials have the same problem).
The credentials unavoidably leave the verifier with a transferable
signed trace of the transaction.  Brands credentials offer a
zero-knowledge option where the verifier can not transfer any
information about what he was shown.
The credentials support selective disclosure of attributes, but only
in a restricted sense.  Attributes can be disclosed with AND
connectives.  However other connectives (OR, +, -, negation, and
formulae) are not directly possible.  Brands supports all of these.
The credentials do not support lending deterence (there is no option
to have a secret associated with a credential that must necessarily be
revealed to lend the credential as with Brands).
The credentials are not suitable for offline use because they offer no
possibility for a secret (such as user identity, account number etc)
to be revealed if the user spends more times than allowed.
Most of these short-falls stem from the analogous short-falls in the
Wagner blinding method they are based on.  Of course (and the point of
the paper) the credentials do offer over the base Wagner credentials
(a restrictive) form of selective disclosure which the base
credentials do not.
On citations:
Brands discusses the salted hash form of selective disclosure in his
book [2], you might want to cite that.  He includes some related
earlier reference also.  I reinvented the same technique before being
aware of the Brands reference also -- it seems like an obvious
construction for a limited hashing based form of selective disclosure.
[1] Niels Ferguson, "Single Term Off-Line Coins", eurocrypt 93.
[2] Stefan Brands, "Rethinking Public Key Infrastructures and Digital
Certificates; Building in Privacy", MIT Press, Aug 2000
viz p27: "Another attempt to protect privacy is for the CA to
digitally sign (salted) oneway hashes of attributes, instead of (the
concatenation of) the attributes themselves. When transacting or
communicating with a verifier, the certificate holder can selectively
disclose only those attributes needed.  Lamport [244] proposed this
hashing construct in the context of one-time signatures."

@_date: 2002-09-17 22:05:36
@_author: Adam Back 
@_subject: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The OS can stop user processes inspecting each others address space.
Therefor a remote exploit in one piece of application software should
not result in a compromise of another piece of software.  (So an IE
bug should not allow the banking application to be broken.)  (Note
also that in practice with must current OSes converting gaining root
once given access to local processes is not that well guaranteed).
However the OS itself is a complex piece of software, and frequently
remote exploits are found in it and/or the device drivers it runs.  OS
exploits can freely ignore the protection between user applications,
reading your banking keys.
Even if a relatively secure OS is run (like some of the BSD variants),
the protection is not _that_ secure.  Vulnerabilities are found
periodically (albeit mostly by the OS developers rather than
externally -- as far as we know).  Plus also the user may be tricked
into running trojaned device drivers.
So one approach to improve this situation (protect the user from the
risks of trojaned device drivers and too large and complex to
realistically assure security of OSes) one could run the OS itself in
ring0 and a key store and TOR in ring-1 (the palladium approach). Some seem to be arguing that you don't need a ring-1.  But if you read
the paper Peter provided a reference for, they conclude that the
pentium architecture is not (efficiently) securely virtualizable.  The
problem area is the existance of sensitive but unprivileged
The fact that VMWare works just means they used some tricks to make it
practically virtualize some common OSes, not that it is no longer
possible to write malicious software to run as user or privileged
level inside the guest OS and have it escape the virtualization.
(It is potentially inefficently securely virtualizable using complete
software emulation, but this is highly inefficient).
(Anonymous can continue on cypherpunks if Perry chooses to censor his
further comments.)

@_date: 2003-04-24 04:42:33
@_author: Adam Back 
@_subject: Makeup as low-tech measure against automated face 
shaddack on Thu, Apr 24, 2003 at 01:42:58AM +0200
There was a paper at Privacy Enhancing Technologies 03 on this topic:
"Engineering Privacy in Public: Confounding Face Recognition", James
Alexander and Jonathan Smith.
It's full of pictures of one of the authors with various forms of
facial makeup, glasses, hats, stockings (over head bank-robber style),
dazzled camera with pen-light laser, etc, plus an empirical analysis
of the disguise efficacy in hiding identity against I think a face
recognition system called FERET.
A copy seems to be online here:
Adam

@_date: 2003-04-24 22:27:36
@_author: Adam Back 
@_subject: double-spending prevention w. spent coins (Re: [Lucrative-L] 
patrick on Thu, Apr 24, 2003 at 03:52:28PM -0400
If the coins offer privacy, then unspent coins are unlinkable when the
same coin is deposited, so keeping just unspent coins doesn't work.
Spent coins on the other hand have their blinding removed, so you
notice double spending by looking at spent coins.
(There are zero-knowledge proofs of non-set membership as proposed for
use in ecash by Sander and Ta-Shma [1], but I think these are quite
computationally expensive.)
[1] "Auditable, Anonymous Electronic Cash", Tomas Sander, Amnon
Ta-Shma, Crypto 99

@_date: 2003-04-24 23:47:21
@_author: Adam Back 
@_subject: double-spending prevention w. spent coins (Re: 
patrick on Thu, Apr 24, 2003 at 06:12:34PM -0400
Your system as described is not in the slightest bit anonymous or
private.  Or at least the user has no cryptographic assurances that
the server is not logging everything, or that some adversary isn't
logging everything that goes over the connection even though the
server is not.
Right that is the linkability problem.  Plus of course as mentioned
above the user has no reason to trust the server.  Or at least he
would prefer a protocol where he did not need to trust the server.
That's where blinding comes into the picture.
Probably the simplest one to understand is Chaum's original scheme,
though there are others such as Brands, and Wagner's online system.
serial-no               = (b^e).[R||h(R)] mod n
proto-coin              = serial-no^d mod n
                        = b.[R||h(R)]^d mod n
coin                    = proto-coin . b^-1 mod n
                        = [R||h(R)]^d mod n
check-valid-coin(c)     = c^e mod n is of form [x||h(x)]
check-double-spent(c)   = bank records spent coins
trace-payee(c)          = payer gives bank b, bank records proto-coins as well
so a blind signature in this scheme is that the bank has an RSA
modules n, private key d, and public exponent e.
The user sends b^e.M mod n to the bank (where b is a random blinding
factor), the bank computes (b^e.M)^d mod n (a standard RSA siganture)
and sends back to the user.  The user then unblinds by dividing by b,
which works because:
(b^e.M)^d = b^{e.d}.M^d = b.M^d mod n
and b.M^d/b = M^d mod n
plus some other detais to avoid existential forgeries.
and so the bank can recognize it's signature later on a coin (because
it's a valid RSA signature made with it's private key d), but it won't
know which unspent coin it corresponds to because it doesn't know the
blinding factors b.

@_date: 2003-04-25 04:15:01
@_author: Adam Back 
@_subject: double-spending prevention w. spent coins 
So actually using Brands credentials which have an off-line fraud
tracing option you could if you wished exchange coins peer-to-peer
amongst users, who eventually after some number of peer-to-peer spends
deposit their coin back at the bank.  The bank checks deposited coins
and can tell which users double spent coins if any after the fact.
What you do about double spending when you detect a given user has
done it is a policy question for the bank -- eg fine user, prosecute
user for fraud to recuperate costs etc.
(You can also use the same protocol for online checking, so the
recipient has the choice of covenience of using peer-to-peer without
going via the bank, or the choice to deposit now and get a fresh coin
and be sure there won't be any dispute resolution later.)

@_date: 2003-04-26 02:50:46
@_author: Adam Back 
@_subject: double-spending prevention w. spent coins 
Most people I've noticed prefer to avoid the "and then he goes to
jail" step because it invites regulation and government involvement,
is expensive and unappealing.  It also involves a identifying
registration step to participate which is a barrier to entry.
I think the controversy surrounding political friendliness was
centered on properties which are not intrinsic but apparently selected
by implementors or proponents:
- there are five schemes we can look at:
- chaum online (CON), chaum/ferguson offline (CFOFF), brands online
(BON), brands offline (BOFF), brands p2p offline (BP2P), and wagner
online (WON)
  - offline means payees can receive funds without connecting to the
    bank immediately to check validity; their remaining assurance of
    not accepting double-spent coins is that if a coin they receive is
    double spent the bank will learn who is responsible; all offline
    schemes also have an online deposit protocol for when the money is
    paid into the bank.
  - in fact offline coins generally can not be respent without
    exchanging for a fresh coin at the bank, so the offline function is
    perhaps better described as "delayed deposit".
    - for why this is the case consider bank -> U1 -> U2 -> U3 -> bank
      with 3 payer/payees U1, U2, U3; bank->U1 is withdrawal, U3->
      bank is deposit, U1->U2 is pay, but U2->U3 isn't safe and here's why:
      - U2 can't convince U3 that he knows the private key for the
        coin because U2 does not have it to give him (U3 needs that
        proof to know that U2s identity is in the coin and will be
        revealed to the bank in case of double spending)
      - if U1 did give U2 his private key, so that U2 could convince
        U3 to accept his coin, then U2 could double spend
        and U1 would get blamed, so it is not in U1's interests to
        give U2 the coin private key
  - but in the special case of Brands offline, there is a peer-to-peer
    offline (which I called BP2P) which is a respendable offline
    option which allows safe offline peer-to-peer transfers.  (The
    trick is in fact to cryptographically bind peer2peer coins (which
    grow at each exchange) to 0-value coins with the p2p recipient's
    identity in them.  This trick only works with Brands offline I
    think, because CFOFF doesn't have a private key to bind with).
- all of the systems provide unconditional payer anonymity (CON, COFF,
BON, BOFF, BP2P, WON)
And collusion proof robust payee and payer anonymity is inherently
possible with all the systems by using accountless operation - this
works generically on all systems.  Basically the bank provides an
interface to allow deposit of coins and getting back fresh blind
coins.  In fact for this Brands has an extra protocol option to allow this to
be done in a single operation (so-called re-freshed coin -- same
attributes, new blinding factors).  This is not just an efficiency
win, it has important privacy value: with this protocol the bank does
not learn the coin attributes.  In particular this means the bank
would not learn the amount of the transaction, as one of the
attributes will be the transaction value (ie it can not distinguish 1c
from $1000).  This I'd argue makes the Brands protocol much more
pragmatically secure against flow analysis.  (With Chaum the bank has
a separate public key per coin denomination, and could to some extent
statistically trace groups of coin denominations).
Chosing not to offer accountless operation is a policy decision by
implementors and proponents (the usual argument is to avoid the
"blackmail attack" -- ie so an unwilling payer extorted can later
collude with the bank to identify the extorter).  However the
side-effect (which is bad) is to make sting operations possible
against anonymous sellers who are politicaly unpopular.  As Tim has
articulated before there are lots of good reasons a seller should be
able to be robustly anonymous.
Then are two approaches to extracting payee anonymity even if the bank
makes the political decision to not support accountless operation
which due to the math work as follows:
1. money changers - this works generically on all schemes -- basically
an entity launders the money handing out fresh coins for used coins,
optionally depositing the coins at the bank before handing out fresh
coins.  Typically it is supposed that the money changer would charge a
commission.  You do not have to trust the money changer with your
privacy because you chose your own blinding factors.
2. payer cooperation -- this also works (to varying extents) with all
schemes.    - one approach to getting payee privacy is if the payer cooperates
    with the payee in an online fashion so that only the payee knows
    the blinding factors (essentially the payee acts as the withdrawer
    also, and the payer acts as a bit pipe).  This protects the payee
    as the payer no longer has information allowing him to collude
    with the bank
    - the other side of adding payee privacy with this approach
      is presumably the payer would also like to retain his privacy     - with Chaum's online protocol double blinding works because of
      the math, so the payer and payee can both be private without
      needing to trust the other party not to collude with the bank
    - with the other schemes the double blinding trick does not work
      which creates a privacy risk for the payer -- the payee can
      collude with the bank and identify the payer -- this essentially
      means that only one of the payer or payee can be robustly
      private at a time (if the bank refuses to offer accounless
      operation)
So in summary the best and simplest way to generically get robust
payer and payee privacy is accountless operation.
If bank chooses to not offer this option, then Chaum online protocol
has the best workaround (retaining payer privacy); however even it is
quite inconvenient requiring both parties to be simultaneously online.
This requires non-standard software, and interferes with usage pattern

@_date: 2003-04-26 23:21:27
@_author: Adam Back 
@_subject: Making Money in Digital Money 
I think the first mail system at ZKS was relatively unreliable, and
complex for users to understand and use (setting up a nym as with
nymserver/type I involved reply blocks, waiting for confirmation etc).
It was reply block based, but reimplemented from scratch, not based on
cyphperpunk type I code.
Some of the issues were implications of the design (as with type I
based reply blocks, some mail does not arrive; also reply blocks
always seemed fragile to me), others were probably implementation
The 2nd gen mail system we built at ZKS (my design) had a different
set of tradeoffs.  I found a copy of the "Freedom 2.0 Mail System" white paper here:
It was definately more reliable (the main business reason for doing
it).  Also there was no reply block pointing back at your real
identity which is the main weakness of the reply-block design: it is a
subpeona risk.  Instead it was based on a pop server which you
optionally connect to via the anonymous freedom network to achieve
sender anonymity (or deliver via mixmaster if you prefer, it's accepts
regular mail to interface with non-anonymous users), and via the
freedom network again to achieve recipient pseudonymity.
So these interactive connections are immediately forward-secret, and
therefore you have much better protection against subpeona attack.
However they are more vulnerable to all-powerful observer attacks who
could probably figure out which pseudonym was which by sending lots of
unique sized email and then watching traffic patterns flow through the
So as you might expect different systems can be built which optimize
against different types of threat.  I'd argue the 2nd gen mail system
would be much better against subpoena attack, but weaker against
all-powerful passive adversaries.  The typical thing an end user with
strong desire for privacy would be concerned about (frivolous lawsuits
related to online discussion groups, defamatio, privacy against law
enforcement sting operations, etc) would be better protected; where as
national security issues where you might imagine NSA or such could
coordinate and implement the all-powerful passive adversary are less
well protected against.
I think there were more active users of the web browsing side of
things than the pseudonymous mail for the reasons above.  The version
might have been better given time, but I think freedom network (and
mail) was discontinued relatively soon after it's deployment.
Yes.  This is why I quit to do other stuff -- limited crypto stuff
left, and no distributed trust anonymity or privacy left.
ZKS still does have one anonymous networking type sytem called
websecure which they are actively selling and have subscribers of.
It's somewhat similar to anonymizer.com in that it is one hop only
anonymous traffic for web browsing only.  The differences (which make
it probably more secure I'd argue) are that it doesn't rely on html
re-writing which is a risky strategy to provide good assurance
(periodically somone finds some html extension which anonymizer.com
style html rewriting misses, until they fix it; or fuzzy parsing rules
in browsers which allow you to slip URLs past the re-writer in a way
that some browsers will fix up, but the re-writer doesn't recognize as
a URL).  Unfortunately the websecure approach is also Internet
Explorer specific, relying on a browser helper object to hook in an
SSL tunnel to the proxy (run by ZKS).
It's described here:
the fact that it's a browser helper object doesn't hurt the appearance

@_date: 2003-04-26 23:44:02
@_author: Adam Back 
@_subject: more about anonymous mail (Re: Making Money in Digital Money) 
I wrote about freedom 2.0 mail system:
So a couple of other comments:
- Ulf Moeller, Anton Stiglic and I published our thoughts about how
someone could go about doing the passive adversary traffic analysis
attacks on interactive systems such as the freedom anonymous network:
- and in fact the version 1 freedom mail system had other issues: the
mail was not split up into fixed sized chunks (as it is with
mixmaster), so it suffered the same vulnerabilities that type I based
nymservers do: it was in addition equally vunlerable to traffic
analysis.  I'd take this version 1 freedom mail vulnerability to
indiate that in essentially all respects version 2 was more secure
than version 1; though some of the version 1 design-issues could have
been fixed in similar ways that are proposed in the mixminion project.
The mixminion project project (aka Type III remailer) design and
implementation attempts to avoid these issues by merging reply block
functionality into a mixmaster like fixed sized message mix net.
Mixminion actually uses Single Use Reply Blocks (SURBs) to in addition
reduce vulnerability to flooding attacks (where someone just sends
lots of messages to see where they arrive as they flow down the reply
block).  The recipient I think is expected to send a few SURBs to nyms
he communicates with, and to send SURBs to the nymserver to pick up
mail from regular internet mail senders (who are not using the
mixminion client).
If I understand it is also planned that the mixminion / Type III
protocol will be implemented within mixmaster as mixmaster version 4.
(The current alpha mixminion code is a separate code base, written in
python scripting language).
The other good thing about mixminion / type III protocol is that
finally type I remailers with their traffic analysis issues could be
phased out.  (Their remaining reason for existance was to support
reply-block functionality for nymservers).

@_date: 2003-04-29 23:36:21
@_author: Adam Back 
@_subject: [Lucrative-L] double spends, identity agnosticism, and Lucrative 
There are also existantial forgeries.
Ie choose random x, compute y = x^e mod n, now x looks like a
signature on y because y^d = x mod n; and when he verifies the
verifier will just do x^e and see that it is equal to y.
These may also look like valid coins to this code!
It's missing a step: the coin should have some structure.  So it can't
be a hash of a message chosen by the user but hashed by the signer
(the normal practical RSA signature) because the server can't see that
it or it would be linkable.
What digicash did I think is something like c = [x||h(x)].  Then you
can reject existential forgeries and unblinded coins because they
won't have the right form.
(If you look back to the post where I gave a summary of the math,
you'll see I included that step.)

@_date: 2003-04-30 07:17:50
@_author: Adam Back 
@_subject: patriotism considered evil 
Some observations on the nouns "Patriot", and "American" etc as they
relate to current events.
I'm not American.  I'm dual-national British/Swiss, I've lived in
Britain, Scotland, Canada and now the US.  But I have not noticed
anyone in Britain, or British press discussing "un-British" behavior,
or putting down anyone attempting to ask questions as "un-patriotic".
(Ditto for the other countries).  Press coverage of Iraq is varyingly
biased in those countries (ridiculously so in the US, somewhat in the
UK due to their involvement).  Opinion in the UK is split, but I don't
see those on the pro-side of the fence arguing that those arguing
against are unpatriotic or anything.  They're just arguing too and fro
about issues.  Politicians were arguing on both sides and getting some
air time.
But in the US the issues are buried, it's difficult for detractors of
the government line to be heard without getting shouted down as
unpatriotic or unamerican.
So I guess the American-way used to stand for something -- beliefs in
freedoms etc., and that one symbol used historically to express
support of those freedoms was the US flag.  So I'm supposing this is
the historic reason people fly flags, on their cars, houses,
businesses etc.  (A practice virtually non-existant in any other
country I've lived in, or travelled to).
But today it seems that the words Patriotic and American (in their
negative forms un-American and un-patriotic as) have become sullied
and perverted and essentially synonymous with:
- unquestioning acceptance of the party line, of the military
- put down and outright aggression against anyone who dares to think
for themselves, to ask critical questions, to express interest in the
truth, or express any interest in hearing both sides of an argument
In Britain the Union Jack flag to some extent got co-opted by racist
political groups such as the National Front, British National Party.
At least to the extent that wearing a t-shirt with a union jack on it
might not convey the message you hoped -- particularly if you have a
skin-head haircut.
In a similar way to me at least the US flag is heading the same way
with (different but negative) connotations of blind adherence to the
party line.
To me as a non-American all these flags fluttering as a symbol of the
governmental and military groups who are currently eroding rights and
freedoms in the US feels bizarre.  The same rights and freedoms that
apparently the same flag used to stand for.
I guess there are some similarities with the negative cooption of the
Union Jack symbol, but I'm wondering if in the US most of the flag
flying population even noticed the switcheroo in connotation.  Flag
flying seems to be more popular than ever.
Personally I'm somewhat on the fence about whether the US/British
attack on Iraq will end up being a net positive or negative thing for
world stability and safety.  It was an illegal first strike action
against a sovreign country, and it was a highly interventionist
activity, but the outcome is less clearly bad though of course we
don't know yet what the long term side-effects will be.
Anyway I never liked patriotism.  What's important in my book is
thinking for yourself, thinking critically and forming your own
opinions.  The governments and prominent political parties in most
western democracies are sleaze pits deserving only of contempt.  And
they are the entities most closely associated with and in control of
the actions of a country on any large scale in the international
arena.  So to me patriotism was always synonymous with support for
this system.  A corrupt political system which needs to be replaced
with anarcho-capitalism for things to get better.

@_date: 2003-04-30 21:54:21
@_author: Adam Back 
@_subject: patriotism considered evil 
Yes but the tabloids are not examples of serious news sources; they
are titilation and shit-stirring.  They're continuously being sued for
slander, fabrication of titilating though coincidentally untrue
stories etc.
I agree news is biased also.  It just pisses me off to see major
network news who you might (or at least the average person
uninformedly does) consider to retain some level of integrity
dismissing and supressing most balanced discussion on with put-downs
involving "unpatriotic" and "unamerican".  WTF is that?  Can't they
engage in discourse where evidence and logical argument are used?
Similar vein is the apparent overnight animosity towards the French
who happened to take a different view.  It all comes down to this same
blind following of leaders, and Bush's inane statements such as "if
you're not for us you're against us".  So now France should be
boycotted because they expressed opinions not precisely aligned with
US views.  People are entitled to their opinion.  In fact if it were
not for Blair and whoever else was behind it in the UK government
over-riding public sentiment, Britain would not have been involved
either as public opinion in the UK was reportedly 80% against
involvement prior to the invasion.  In that case I suppose British
exports would now also be targets for calls for boycott.
I suppose I am just suprised and dismayed at the level of childish
behavior but on an international policy scale.

@_date: 2003-08-03 05:41:21
@_author: Adam Back 
@_subject: Secure IDE? 
I believe that is what some of them are doing.  I think it's a little
better to use some fast PRNG seeded from the sector (or eg HMAC of
sector number or encryption of sector number if you have hardware).
The sector number is changing in counter order and cancels with the
plaintext difference.  I did some tests on a 10GB disk full of windows
app and program data (accessed the raw windows partition from linux
number) you get a fair few collisions.
one of the products on show at RSA earlier this year would boot from
the IDE sector onto a virtual drive (it would pretend to be a boot
sector over the IDE connector), then that boot sector has code to ask
for your password, derive the key and load it, and then reboot onto
the real drive.  If you pulled power from the drive it would forget
the key.

@_date: 2003-08-06 19:05:27
@_author: Adam Back 
@_subject: What happened to the Cryptography list...? 
The problems with closed lists relying on a single human for
forwarding and filtering...
Couldn't he just let people post in his absence?  It kind of detracts
from a list if it disappears for weeks time on a regular basis.
Also there are delays, and then there's Perry decisions that a
discussion is no longer worth persuing when contributors are still
interested to discuss.
Adam

@_date: 2003-12-26 21:37:18
@_author: Adam Back 
@_subject: Microsoft publicly announces Penny Black PoW postage project 
I did work at Microsoft for about a year after leaving ZKS, but I quit
a month or so ago (working for another startup again).
But for accuracy while I was at Microsoft I was not part of the
microsoft research/academic team that worked on penny black, though I
did exchange a few emails related to that project and hashcash etc
with the researchers.
I thought the memory-bound approaches discussed on CAMRAM before were
along the lines of hash functions which chewed off artificially large
code foot-print as a way to impose the need for memory.  Arnold Reinhold's HEKS [1] (Hash Extended Key Stretcher) key stretching
algorithm is related also.  HEKS aims to make hardware attacks on key
stretching more costly: both by increasing the memory footprint
required to efficiently compute it, and by requiring operations that
are more expensive in silicon (32 bit multiplies, floating point is
another suggestion he makes).
The relationship to hashcash is you could simply use HEKS in place of
SHA1 to get the desired complexity and hence silicon cost increase.
"The main design goal of this algorithm is to make massively parallel
key search machines it as expensive as possible by requiring many
32-bit multiplies and large amounts of memory."
I think I also recall discussing with Peter Gutmann the idea of using
more complex hash functions (composed of existing hash functions for
security) to increase the cost of hardware attacks.
The innovation in the papers referred to by the Penny Black project
was the notion of building a cost function that was limited by memory
bandwidth rather CPU speed.  In otherwords unlike hashcash (which is
CPU bound and has minimal working memory or code footprint) or a
notional hashcash built on HEKS or other similar system (which is
supposed to take memory and generaly expensive operations to build in
silicon), the two candidate memory-bound functions are designed to be
computationally cheap but require a lot of random access memroy
utilization in a way which frustrates time-space trade-offs (to reduce
space consumption by using a faster CPU).  They then argue that this
is desirable because there is less discrepency in memory latency
between high end systems and low end systems than there is discrepency
in CPU power.
The 2nd memory [3] bound paper (by Dwork, Goldber and Naor) finds a
flaw in in the first memory-bound function paper (by Adabi, Burrows,
Manasse, and Wobber) which admits a time-space trade-off, proposes an
improved memory-bound function and also in the conclusion suggests
that memory bound functions may be more vulnerable to hardware attack
than computationally bound functions.  Their argument on that latter
point is that the hardware attack is an economic attack and it may be
that memory-bound functions are more vulnerable to hardware attack
because you could in their view build cheaper hardware more
effectively as the most basic 8-bit CPU with slow clock rate could
marshall enough fast memory to under-cut the cost of general purpose
CPUs by a larger margin than a custom hardware optimized
hashcash/computationally bound function.
I'm not sure if their conclusion is right, but I'm not really
qualified -- it's a complex silicon optimization / hardware
acceleration type question.
[1] [2] Abadi, Burrows, Manasse and Wobber "Moderately Hard, Memory-bound
Functions", Proceedings of the 10th Annual Network and Distributed
System Security Symposium, February 2003
[3] Dwork, Goldberg, and Naor, "On Memory-Bound Functions for Fighting
Spam", Proceedings of the 23rd Annual International Cryptology
Conference (CRYPTO 2003), August 2003.

@_date: 2003-12-28 13:29:27
@_author: Adam Back 
@_subject: Microsoft publicly announces Penny Black PoW postage project 
Oh yes forgot one comment:
One down-side of memory bound is that it is memory bound.  That is to
say it will be allocated some amount of memory, and this would be
chosen to be enough memory to that a high end machine should not have
that much cache so think multiple MB, maybe 8MB, 16MB or whatever.
(Not sure what is the max L2 cache on high end servers).
And what the algorithm will do is make random accesses to that memory
as fast as it can.
So effectively it will play badly with other applications -- tend to
increase likelihood of swapping, decrease memory available for other
applications etc.  You could think of the performance implications as
a bit like pulling 8MB of ram or whatever the chosen value is.
hashcash / computationally bound functions on the other hand have a
tiny footprint and CPU consumption by hashcash can be throttled to
avoid noticeable impact on other applications.

@_date: 2003-02-07 01:07:16
@_author: Adam Back 
@_subject: password based key-wrap (Re: The Crypto Gardening Guide and Planting Tips) 
Peter lists applied crypto problem in his "Crypto Gardening Guide" at:
One of the problems from the "Problems that Need Solving" section is:
I may not be fully understanding the problem spec: you want to encrypt
(wrap) a randomly generated key (a per message session key for
example) with a key derived from a password.
What would be wrong with using PBKDF2 (from PKCS  / RFC2898) as the
key derivation function to give you defense against dictionary attack.
(Allows choice of number of iterations to "stretch" the password,
allows a salt to frustrate precomputation.)
Why do you care about non-malleability of the key-wrap function?
If you do want non-malleability of th ekey-wrap function, isn't
encrypt and MAC a standard way to do this?
Then you would need two keys, and I presume it would make sense to
derive them (using KDF2 from IEEE P1363a) a start key:
sk = KDF2( password, salt, iterations )
ek = KDF( sk, specialization1 )
mk = KDF( sk, specialization2 )
and then AES in CBC mode with random IV encrypting with ek, with
appended HMAC with key mk.
That leaves the comment:
but in this case the attacker could take his pick with no significant
advantage of either method:
- brute force passwords to get sk, derive ek from sk, decrypt the
wrapped key and use some knowledge about the plaintext encrypted with
the wrapped key to tell if the write password was chosen; or
- brute force passwords to get sk, derive mk from sk, and see if the
MAC is valid MAC of the ciphertext (presuming encrypt and then MAC)
Or is the problem that the above ensemble is ad-hoc (though using
standardised constructs).  Or just that the ensemble is ad-hoc and so
everyone will be forced to re-invent minor variations of it, with
varying degrees of security.

@_date: 2003-05-04 23:17:37
@_author: Adam Back 
@_subject: Capitalism and monopolism 
-0700
Well I guess Microsoft and Intel aren't quite monopolies.  At least
with Intel there are viable competitors selling compatible products:
AMD, transmeta, VIA, etc.  And AMD processor are some of the time at
the top of the heap performance-wise, and most of the time offer the
best value for money processors (best performance/$).
Anyway my view is that what props up software virtual monopolies is
the current IP laws.  If they were revised to remove copyright, and
patents I think it would help level the playing field.
As to virtual monopolies being worse than government: I disagree
businesses aim to maximise profit margin and this places a limit on
the depths of unethical and bad for the individual behavior they can
do.  They won't do it becaues it's not profitable: unhappy customers
are not good business.
Current governments on the other hand are almost universally bad for
the economy, liberty and freedoms.  They have no competition and are
so corrupt that it's difficult for them to act anywhere near as
efficiently or sanely as a company.

@_date: 2003-05-04 23:53:53
@_author: Adam Back 
@_subject: Underestimating long-term consequences of cryptoanarchy 
Interesting discussion.
I'm thinking another inflection point which could tip the balance
would be some travel technology breakthrough -- 100x faster, 100x
cheaper (relative to individual wealth -- which itself increasing in
real terms over time as productivity improves due to automation,
efficiency, process improvement etc).  If you could shrink the world
so that people can basically commute from anywhere to anywhere for a
cost significantly less than the difference in tax rates between tax
havens such as Bahamas (0% income tax) compared with western direct
and investment tax rates of 40-60% and beyond marginal rates.
- This is mostly why I was disappointed to see the plans to scrap
concorde -- it was expensive in real terms due to current fuel and
current salaries as set by current economic climate; however with a
cruise speed of mach 2.0 it was 2.4x as fast as typical passenger
jets.  (Originally planned for mach 2.5 - mach 3.0, but material
science wasn't up to the task when concorde's were built in the 70s).
But scrapping them seems like a step backwards.  So there were
merchant bankers and celebrities jetting backwards and forwards from
new york on it.  But what wealthy are doing today can be what everyone
is doing some years on when things have become cheaper relatively
- The other aspect of travel speed -- the crappy depature and arrival
procedures -- have gotten significantly worse since WTC terror attack.
The current political climate is as a result a poorer one for business
as it has basically increased the cost of travel (in convenience).
- So what about other travel: magnetic levitation trains, mag lev
trains in vacuum tubes, nuclear powered transport (with design margin
to amply cope with safety issues); and further out maybe
- The other issue is how governments would react to transportation
advances -- maybe just change tax laws so you get charged the max of
countries you work or reside in.
- Another potential and probably more likely to happen medium term
technology could be improvements in display technology making
telepresence more functional.  3d projective displays able to project
into free-space for example allowing basically free-form tele-presence.
It would be harder for governments to attempt to tax remote workers,
but they might try it anyway by passing the tax burden on to the
employers -- forcing them to collect local taxes against remote
Crypto-anarchy has interface problems also, it just allows you to be a
virtual remote worker because your location is no longer discernable.
Still governments may try to force local companies to pass the tax
burden on.
India is an interesting example of remote workers -- many US companies
are apparently moving jobs wholesale to India to try to reduce costs
in the face of poor economy.
Another corporate trend to avoid US taxation is where companies move
their notional headquarters off-shore so that they are not taxed on
international sales.
Either way the fact that companies are doing this suggests that
currently companies themselves are ahead of individuals in mobility to
avoid taxation.  This same principle should allow for example remote
workers, or virtual remote workers to work for the notional off-short
company.  Virtual identities with documentation demonstrating domicile
in Bahamas or other tax-havens should even allow a virtual worker to
work for a company under government imposed obligations to employ
virtual remote workers in the US.

@_date: 2003-05-05 06:11:07
@_author: Adam Back 
@_subject: Capitalism and monopolism 
-0700
I think if anything the processor market would be made more
competitive yet by removal of patents as competitors with the
fabrication technology could just outright copy other companies
processors, after reverse-engineering them.  (If it was cheaper to
reverse-engineer than design one).
But that's a symptom of a corrupt government and laws against the
public interest.  Companies in many areas have more political power
than individuals.  For example the DMCA.  This to me doesn't argue for
more government, but for less government and system reform.
I agree: I think anonymous file sharing should be the next generation
in p2p evolution.
I don't think Palladium and other DRM hardware such as recently
proposed by Paul Kocher et al can realistically make any difference to
file sharing.  It seems unrealistic to think that hardware in the
hands of it's attackers (p2p file rippers) can withstand long term
attack.  And anyway the content will always be amenable to re-encoding
from the analog output, or digital signal.  Kocher's scheme is
essentially hardware-tamper resistant watermarking -- it encodes the
hardware identifier in the analog output to trace who ripped content.
However it is vunlerable to collusion over some parameter of colluders
(5 with the example system parameters).  Even then it can't be too
hard to obtain hardware anonymously removing the tracing risk even if
you don't bother colluding to avoid the tracing risk; the remaining
risk is that the player may be unable to decode new content if the
player keys are revoked after it is detected as a source of ripped
I agree.
But I think IP is something created, subsidized and only made possible
by governments.  Correct.  But at least they're trying to be profitable, which most of
the time means keeping their customers happy.  Governments on the
other hand have no such objective, and most governments burn off 25%+
of GDP -- that's a lot of money to do evil with.
While I agree political reform is badly needed in many aspects of
government and law, it's difficult to see how one can get there from
here using only the political process.  Your only chance is individual
mobility -- individuals voting with their feet to create competition
in government.  I guess it's a similar situation in some respects --
government is a virtual monopoly, in the sense that you don't have to
accept your current government, you can move.  However people have
ties to their country, they have friends and relatives who live there;
plus the alternatives have disadvantages too.
While IP laws do vary, it's a pity there are no major governments that
compete on IP -- by providing a copyright and patent free environment.
So while governments and corporations, and corporations buying
government favor are frequently against the interests of the
individual lobbying for reform doesn't seem likely to improve things

@_date: 2003-05-06 19:25:15
@_author: Adam Back 
@_subject: lucre double-blinding? (Re: Crypto-making vs Crypto-breaking) 
It's been a while since I looked at the Lucre white paper but
extrapolating from the Chaum context doesn't double blinding mean the
payer and payee have to be simultaneously online with the bank?

@_date: 2003-05-06 21:54:02
@_author: Adam Back 
@_subject: RIAA legal attacks on students 
Here's some more detail about the RIAA legal attacks on four students
and some more on one of the students web pages:
In fact it turns out that he was just running a search engine that
indexed any files available on windows shares on the local network.
You could search for any type of file; as you might expect some of the
files users searched for and downloaded were mp3 files.
But I don't see how they can claim he is guilty of contributory
infringement anymore than google!
So in the previous slashdot thread on the same topic, there were a
number of people calling for a fund to be setup that people could
donate to to defray the legal expenses and settlements.  (The
settlements were between $12k and $17kk).
Do any of our legally qualified list members know if it is legal to
collect money to reimburse a defendant against a settlement in a
politically unpopular case.  The students apparently admitted no guilt
if it helps.
I'm thinking all it takes is a paypal account, and a post to slashdot
and their expenses would probably be covered.  The remainder if any
should got to the anti-RIAA lobby; ie whichever of the online rights
lobbying groups that is mostly robustly defending against DMCA

@_date: 2003-05-07 21:09:04
@_author: Adam Back 
@_subject: lucre double-blinding? (Re: Crypto-making vs 
nobody on Wed, May 07, 2003 at 10:00:02AM +0200
Yes I remember the introduction of a 2nd blinding factor, your other
post in the thread where you reposted the remaining issues with
taggability jogged my memory; just the terminology threw me.
(Probably more proper to call it the introduction of another blinding
factor -- the result is just more effectively blinded -- Brands
constructs use 3 blinding factors in some scenarios for example and
that is still considered blinded not "triple-blinded") Brands has an optimization of his scheme where (as the user receiving
a coin) you have the option of not bothering to perform one of the
verifications, the weaker assurance being you are still assured that
the bank can't distinguish between tagged coins, though it can
distinguish an untagged coin from a tagged coin.
However as with Lucre I don't find this very convincing because the
bank can still tag one person at a time.  If you add in the general
lack of connection anonymity, it could certainly be used to confirm
suspicions and probably to effectively tag multiple users at once.
So I would consider the lucre two blinding factor approach still

@_date: 2003-05-07 23:53:30
@_author: Adam Back 
@_subject: Capitalism and monopolism 
The RIAA execesses are bad, but my point is that government is worse:
imagine what the government could do to the music industry if it
converted it into a state-run monopoly.  What is worse about
governments is that they are not even optimized for profitability (of
the economy as a whole, viz 50%+ marginal tax rates with most of the
proceeds burnt off with no retained value for anyone).  Also virtual
monopolies on the business side are not monopolies in the same way
that government is: you can buy independent music, you can not buy
music distributed by the most abusive distributors etc.

@_date: 2003-05-09 03:40:24
@_author: Adam Back 
@_subject: A Trial Balloon to Ban Email? 
Yes, there is some discussion of it on slashdot, including several
other people who have commented similarly to anonymous that it is a
pretty big privacy invasion and centralised control point problem.
The claim that you can optionally be anonymous and not use a cert, or
get an anonymous cert is plainly practically bogus.  You'd stand about
as much chance of having your mail read as if you shared mail hub with
spamford wallace -- ie 90+% of internet mail infrastructure would drop
your mail on the floor on the presumption it was spam.
Plus a point I made in that thread is that it is often not in the
internet user's interests to non-repudiably sign every message they
send just to be able to send mail because that lends amunition to
hostile recipients who from time-to-time target internet users for
bullshit libel and unauthorised investment advice etc. Companies also are I would expect somewhat sensitive to not signing
everything for similar reasons as those behind their retention
policies where they have policies of deleteing emails, files and
shredding paper files after some period.
In addition PKIs because of the infrastructure requirements have
probem complex to setup and administer.  So now we've taken one hard
problem (stopping spam) and added another hard problem (hierarchical
PKI deployment) and somehow this is supposed to be effective at
stopping spam.
In addition unless there is significant financial cost for
certificates and/or signifcant and enforceable financial penalty and
good identification and registration procedures enforced by the CAs it
wouldn't even slow spammers who would just get a cert, spam, get
revoked, get another cert and repeat.
Certificate revocation is already a weak point of PKI technology, and
to reasonably stop spam before the spammer manages to send too many
millions of spams with a cert, you have to revoke the cert PDQ!
And finally it all ends up being no more than an expensive
implementation of blacklists (or I suppose more properly whitelists),
because the CAs are maintaining lists of people who have not yet been
revoked as spammers.  Some click through agreement isn't going to stop
spammers.  Legislation or legal or financial threat is going to stop
spammers either because any level of registration time identity
verification that is plausibly going to be accepted by users, and this
is also limited by the cost -- higher assurance is more cost which
users also won't be willing to accept -- will be too easy for the
spammers to fake.  And email is international and laws are not.
It is pretty much an "internet drivers license" for email.
I also think that fully distributed systems such as hashcash are more
suitable for a global internet service.  My preferred method for
deploying hashcash is as a token exempting it's sender from bayesian
filtering, and any other content based or sender based filtering.
That way as an email user you have an incentive to install a hashcash
plugin  because it will ensure
your mail does not get deleted by ever-more aggressive filtering and
scattergun blackhole systems.  The camram system
 is a variant of this.
It also more directly addresses the problem: it makes it more
expensive for spammers to send the volumes of mail they need to to
break even.

@_date: 2003-05-10 06:02:45
@_author: Adam Back 
@_subject: blackhole spam => mail unreliability (Re: A Trial Balloon to 
lynn on Fri, May 09, 2003 at 10:11:52AM -0600
Any internet user needs to be able to send mail to any other internet
user.  Which means the default has to be open (blacklists rather than
whitelists).  Then you have the blackhole lists like ORBs etc, which
block domains used predominantly by spammers.  But the problem is
spammers don't stay in one place, they buy service from ISPs and spam
flat-out until the ISP notices and cancels the account.  Some ISPs are
more grey -- they want to make money from spammers by providing them
service, and some ISPs just don't notice or respond that quickly.  The
ISP can't distinguish spammers from non-spammers when they receive
customer orders.  The blackhole people are arbitrary vigilantes by and
large, so the overall effect you might argue does reduce spam, but it
also results in lost mail.
My experience was I couldn't get mail from my brother who was using
btinternet, one of the largest ISPs in the UK because some idiot
blackholer blackholed their dynamic IPs.  Not doubt there were at some
time some spammers using BTinternet as with just about any other ISP.
Recently I couldn't receive mail from John Gilmore, and so it goes.
So I don't see how this is a "solution", rather it is just a broken
countermeasure with scatter gun fall-out of false positives for all
the other people who find themselves sharing the same ISP as spammers
long enough for the blackhole people to add them.

@_date: 2003-05-10 06:55:20
@_author: Adam Back 
@_subject: blackhole spam => mail unreliability (Re: A Trial Balloon 
lynn on Fri, May 09, 2003 at 11:35:36PM -0600
So this would be the block port 25 except to ISP run mail hub
approach?  Firstly that only works for end-users; larger customers
want their own mail delivery and no abitrary restrictions on what they
can do with their pipe.  Then what is the ISP going to notice?  He
shouldn't be actively monitoring his customers traffic.  There are
lots of tunneling protocols, authentication is weak, spam can identify
other people as the sender (to some extent), host security is weak,
hosts are vulnerable to viruses.  Recently there was a virus with a
payload of an open proxy, which it was suspected was distributed by
spammers, or at least the spammers had discovered it and were using
So I understand what you're describing, but it sounds lik a big messy
nightmare, which is pretty much where we are now and rapidly getting
Let's try something concrete: say some spammer starts using AOL to
send a batch to Eathlink.  So Earthlink notices and blocks AOL.  If
you seriously think this is the outcome, then email reliability
planet-wide has probably just dropped by 1% (or whatever fraction of
internet email travels from AOL->earthlink).  Repeat for all major
ISPs who are being abused by spammers with disposable free AOL CDs,
accounts bought with stolen credit cards, or just regular paid
service.  Messy right?
So I think it is not realistic to assume ISPs can do this without
massive reliaibility loss.  Typically I'm presuming blackhole lists
don't block large ISPs (modulo the BTinternet example I gave) because
of the fall out.  Basically any ISP of any size has an ongoing
turn-around of some proportion of their users who are repeat hit and
run-spammers.  So a blackhole approach can stop a static IP leased to
a spammer, used by the spammer only, but the same approach applied to
the hit and run cheaper ISP account using type customers (dynamic IP)
causes no end of reliability issues.
Analogies about the wild west don't really help in thinking about
solutions I think.  I like the decentralised nature of the internet.
I don't want to have to show government ID to obtain an internet
drivers license to send email.  When I buy a pipe onto the internet I
don't want "no server" AUPs, nor a mish-bash of blocked ports.
I understand the problem is hard to address, but let's not damage the
useful decentralised open architecture of the internet trying!

@_date: 2003-05-12 21:45:57
@_author: Adam Back 
@_subject: economics of spam (Re: A Trial Balloon to Ban Email?) 
Bear discussed using hashcash-alike tokens as a challenge response
from the filtering MTA back to the sender giving the sender a chance
to compute a hashcash token.
This approach has the problem you identify -- namely that email is
store and forward; email can and often does go through multiple MTAs
on it's path to delivery, and the MTA doing the filtering may be
multiple hops from the sender.  Indeed sometimes the filterer is the
end-user who is also intermittently connected.
It's more convenient and fits better in the store-and-forward setting
if all email already includes the token at time of sending.  If it
turns out to be needed, then there is no interactive challenge-response needed.
Then the question is whether computing the token at sending time would
be incovenient for the normal sender.  This depends on what parameters
you choose.  A few seconds probably wouldn't be noticed, especially as
with deep MUA integration the token can be computed on each recipient
address as soon as it is selected for receipt.  Depending on MUA usage
therefore the token could be computed while the sender is composing
the message.
In addition it is expected that there would be a mechanism whereby
regular correspondents would white list each other.  (Probably
automatically via their mail clients).
Whether you think a few seconds is sufficient depends on your views of
the economics of spamming.  Ie how close to losing break-even the
spammers are, and whether a few seconds of CPU per message is enough
to significantly increase the cost.  This article for example
discusses the economics of spam:
they give an example of a spam campaign with a 0.0023% response rate,
and a yeild of $19 per response.  They estimate the cost of sending
the spam was less than 0.01c per message.  I've seen significantly
lower estimates for the sending costs.  To deter a given spam campaign
we just have to increase the cost to the point of making it
unprofitable given the response rate and profit per responder.  The
other side of this equation is what a second of CPU costs in monetary
terms to a spammer.  (To an end user it is essentially free because
his CPU is mostly idle anyway; the limiting factor for the user is his
preference for fast mail delivery (and in the dialup case an
unwillingness to sit waiting for tokens to be calcluated before his
mail can be sent).

@_date: 2003-05-12 21:51:35
@_author: Adam Back 
@_subject: changes at lne.com 
Do you filter on the same header that you build the white-list from?
Ie I forge all of my mail, it's really coming from ABack at ex.ac.uk; but
I want replies to adam at cypherspace.org.
I've encountered other "subscribers only" situations where my mail
gets bounced because they look too deeply at the headers and see the
received line or Sender so something is ABack at ex.ac.uk and block it
even though the From line is adam at cypherspace.org, and I subscribed as
adam at cypherspace.org

@_date: 2003-05-13 20:50:17
@_author: Adam Back 
@_subject: economics of spam (Re: A Trial Balloon to Ban Email?) 
To respond on the comments on costs of spamming and costs of CPU, the
figures one can draw from various papers and articles are highly
variable, one suspects they are variously including operator time,
electricity, spam software purchase, and email address list purchase.
To bring it back to just the raw computational costs (equipment
amortized plus electricity) lets do some rough estimates for this.
To take Tim's estimate $500 machine amortized over 2 years seems
entirely reasonable, say this machine has a 1Ghz CPU.  I'll add ADSL
line $500/year for a 1Mbit uplink, and say $200/year in electricity
for a total of $950/year.  For spamming without hashcash let's say
that it can send customized mail messages of size 1KB each, and by
pipelining it manages to max the link and send 64 messages/second.
(Divide by 2 to account for unreachable addresses, etc).
I make that 0.00005c / message.  Presuming the same machine is mostly
unloaded, and the spammer wants to send the same number of mails he
needs a bank of 63 additional CPUs each at a cost of 450/year
(amortized cost+electricity) for a total of $29300/year, so now his
spamming costs 0.0015 / message, and the purely computational costs
have increased by a factor of 30.  On could imagine this would reduce
the amount of untargetted spam a lot.  Clearly you will still receive
spam, just less of it, or more targetted to be likely to interest you
Other issues include that perhaps the spammer can get bandwidth
cheaper per Mbit if he needs more than 1Mbit, which would tend to
reduce purely computational cost of spamming (without tokens).
A 1 second CPU cost on a 1Ghz machine should be negligible and
acceptable to an email user even if the computation happens while he
waits after he clicks the send button.  If he is on a DSL or similar
it could be backgrounded.  On dialup delivery is slow anyway and a
second probably wouldn't be noticed.  Dialup users also often batch
their mail sending (deliver later from a local MUA maintained queue).
An additional cost for spammers is acquiring the email lists.  However
this cost can be amortized across multiple spamming campaigns on
behalf of different spam clients, and mostly seems to consist of
emails gathered from a web spider if one takes the claims of the CDT
spam report, so is itself just a bandwidth cost.
We could probably as was previously noted get away with a marginally
larger delay if tokens are only required to recipients who have never
replied to us in the past.
If one accepts these figures, at 1 second CPU per sent mail for new
recipients, perhaps it may even be economical for ISPs to do the
computation as part of mail service.
If we could think of a distributed way to precompute the token and yet
still have distributed verification without infrastructure, we could
increase the cost to 5 mins without normal users noticing.  It is not
obvious how one would do this however as unless the entire computation
is tailored to the recipient, parts of the computation could be
re-used across multiple recipients.  As Tim notes' Moore's law requires that we increase the collision cost
over time.  (But this is not so hard to do -- I can think of a simple
fully scalable mechanisms to achieve this slowly increasing
distribution of a minimum bit collision).
The possibility for accelerator hardware is definitely a limiting
factor.  Counter-measures to this which have been suggested include
(a) changing the algorithm over time with an authenticated code update
mechanism; (b) defining a cost function which makes use of features of
general purpose computers -- eg. IEEE floating point hardware, memory,
cache, larger code footprint algorithm etc.  This could in theory mean
that absent sufficient market general purpose CPUs remain the most
cost effective approach; (c) memory bound functions such as [1] which
are limited by memory latency rather than CPU speed.  Memory bound
functions have their own economic arguments (see conclusions section
of the paper): perhaps accelerator hardware is also a problem because
all you need is a memory chip, plus a really cheap CPU; they mean the
most cost effective hardware to buy is the cheapest CPU and so perhaps
2 or 3 times cheaper than best Mhz/$; plus they intentionally consume
memory data footprint which can interfere with applications.
Another possibility with accelerator hardware; if ISPs were the
primary deployers, then they are better positioned to buy accelerator
hardware to compete head on with spammers.
[1] C. Dwork, A. Goldberg, and M. Naor, "On Memory-Bound Functions for
Fighting Spam", Proceedings of CRYPTO 2003, to appear.

@_date: 2003-05-14 06:27:43
@_author: Adam Back 
@_subject: what fields to hash with hashcash (Re: A Trial Balloon to Ban 
roy on Mon, May 12, 2003 at 11:52:57PM -0500
Well there are different things you could hash.  This simplest is just
to hash the recipient address and the current time (to a day
The recipient looks at the token and knows it is addressed to him
because it's his address.  He stores it in his double spend database
and won't accept the same token twice.
After the validity period of a token has expired he can remove it from
his double-psend database to avoid the database growing indefinately.
(He can reject out-of-date mail based purely on it's date).
Hashing the message body is generally a bad idea because of minor
transformations that happen as mail traverses MTAs and gateways.
In fact I don't see a need to hash anything else if you're happy
keeping a double-spend database.

@_date: 2003-05-14 15:56:18
@_author: Adam Back 
@_subject: what fields to hash with hashcash (Re: A Trial Balloon to 
om>; from sunder on Wed, May 14, 2003 at 10:02:42AM -0400
I was suggesting 30 days.  You could up that if you want -- the
database won't be that big at say 32 bytes per recieved mail.  The day is matched against the day in the token, as Bill said the
tokens contain the date and the email address, in fact they look like
0:030514:foo at bar.com:482d3c37d5b5c112
where the first field is a version number, 2nd field is date
(year,month,day), 3rd field is resource name (for email the
recipient's email address) and last field is random junk to make it
hash to trailing zeros.
if you hash that with sha1:
% echo -n 0:030514:foo at bar.com:482d3c37d5b5c112 | sha1
You can see that this one has 20 leadings 0s (in binary -- 5x4bit hex
If the token is random (ie the spammer put no computational work into
it), it won't have the required number of bits of collision and the
recipient will reject it.
eg I'll just type this one in myself:
0:030514:sunder at sunder.net:0123456789abcdef
Then my MTA or my mail-client (or any MTA in the path that does
checking) will check:
% echo -n 0:030514:sunder at sunder.net:0123456789abcdef
and see that there are insufficient leading 0 bits (none in this
The tokens are only valid to a given recipient, if a spammer sends you
a token address to me, you'll reject it because it doesn't have an
address you receive mail for in it.
If a spammers sends you the same valid token twice, you'll reject it
because you keep a little database of received tokens.
There is software here (windows GUI, windows cmd line, unix cmd line):

@_date: 2003-05-14 16:18:07
@_author: Adam Back 
@_subject: A Trial Balloon to Ban Email? 
-0400
In the short term (when hardly anyone is sending hashcash tokens)
accepting a payment means that you exempt it from your other filtering
rules, which means that your filters are less likely to accidentally
delete mail you wanted to read.
Your reason to send hashcash tokens is to make it less likely that the
recipient's filters will accidentally delete your mail.
Very analogous.
I think I agree that a real cashable payment for 0.1c would be
preferable; however the infrastructure to support it is many orders of
magnitude harder to setup.  It will also likely have to be run as a
business because of the setup and ongoing hierarchically organized
infrastructure costs.  And it's not clear it will profitably scale
down to payments that low.  And if there is real money on your machine
you'll start to see viruses attempting to steal that money.  Also the
payment system better support privacy or email privacy would have just

@_date: 2003-05-14 16:34:23
@_author: Adam Back 
@_subject: what fields to hash with hashcash (Re: A Trial Balloon to 
justin on Wed, May 14, 2003 at 02:49:34PM +0000
Well the address the token was minted for is contined in the hashcash
header, and the recipient knows what email addresses he accepts mail
for.  To take your example:
The sender as he Bcc'd james at nowhere.net thinks this the recipient's
address, so it delivers to envelope address james at nowhere.net and for
that delivery adds header:
X-Hashcash: 0:030514:james at nowhere.net:b384c3cc66319383
Then the .forward file forwards to james.t.doe at treas.gov, who reads
his mail; his MUA sees that the message is to james at nowhere.net an
address he reads mail for, checks the collision:
% echo -n 0:030514:james at nowhere.net:b384c3cc66319383 | sha1
sees there are enough bits of collision, and if he hasn't seen this
token before he accepts the mail.
The message will also contain one hashcash header per to or cc
recipient.  (Bcc recipients must be delivered separately because
otherwise bcc semantics are lost -- other recipients should not learn
from the hashcash headers that the bcc recipient received the mail).
I take it this comment is about mailing lists?  Mailing lists have to
be treated separately.  The sender probably can't afford to create a
token for each recipient.  (Also he doesn't know the recipient's
addresses).  Mailing lists deal with spam with filtered versions of
You have to cope with multiple hashcash headers when a mail has
multiple recipients, Message-ID only suports one header.
For USENET postings putting the hashcash token in the Message-ID can
work because USENET uses the Message-ID to supress duplicates in it's
flooding algorithm, and you could argue that there is just one
recipient: USENET (or the cross-posted group list).

@_date: 2003-05-14 22:09:15
@_author: Adam Back 
@_subject: what fields to hash with hashcash (Re: A Trial Balloon to 
frantz on Wed, May 14, 2003 at 11:14:56AM -0700
I'm not sure what the comment alludes to as it includes a ;-), but you
can find multiple collisions against the same email address on the
same day, viz:
0:030514:frantz at pwpconsult.com:49916794a98728f2
0:030514:frantz at pwpconsult.com:ffbead9be92472a3
etc.  In fact they are also this way because you want there to be a
relatively low probability of there being an accidental collision in
the tokens created by different users sending you mail.  The example implementation chooses the random string from a 2^64
space, however on average only 2^44 of those will be valid tokens (if
you use 20 bit collisions), and so if you imagine someone receiving
256 mails in a day, they have a birthday probability of 2^-29 of
having a mail falsely deleted because of an accidental collision.
I guess that is a fairly low probability compared to email
reliability, but anyway the safety margin can be increased simply by
increasing the random string search space.

@_date: 2003-05-15 06:41:32
@_author: Adam Back 
@_subject: deterring coin re-use with offline coins (Re: A Trial Balloon to Ban Email?) 
[...spammer sends 170k mails all with same micropayment coin...]
So I'm not sure if you'd want to do it, and it has other issues
discussed on cpunks recently, but there are some other options here
with ecash that can avoid the bank having to say "already spent"
169,999 times for each valid but already spent coin.  (I concur with
Sunder that if the bank had to fit such usage patterns into their
business model, it would increase ther costs significantly which would
make running the bank even harder to do and still turn a profit,
especially as we are talking very high volume, and exceedingly low
value tokens.)
One assumption I'm making is presumably the micropayment system
provides the option for payer and payee anonymity, or email privacy
just got removed once and for all.  (Trace the payments at the bank
and you know who emailed who in a convenient central database - a
definite privacy no-no).
So with the offline brands protocol of which there was some discussion
recently, the MTA could verify the coin locally.  It would be assured
that if the coin was locally verified as valid, he either gets the
money later when he deposits, or the bank gets the spammers identity
and prosecutes them for payment fraud.
So (and this is why I said I don't know if you would want to do
this...) this payment choice where identity is revealed iff you
double-spend has the recently discussed issue:
A) you have to provide your identity in the first place, and if having
it revealed is any deterrent, you'd better be identified robustly
(doing this identification for every email user on the planet seems a
somewhat daunting task)
B) the spammer will have an incentive to find a way to provide fake
identity to the bank, or of buying someone else's identification
(eg. someone with no credit rating, or of stealing someone else's
tokens, or stealing someone else's mail services which automatically
add a payment (identifying them) on event of double spend
But aside from those issues (plus the showstopper issue of building a
payment infrastructure to support this volume in the first place which
was discussed earlier in this thread) this now gives the MTA the
ability to reject double-spends locally -- modulo the amount of
deterrent to double-spending anyway and being identified ends up
providing after the spammers have finished attacking issue B).
A remaining technical issue would be the MTA could have it's CPU
overloaded as verifying such tokens is while relatively cheap (I think
around DSA signature verification cost) still much more expensive than
it is for the DoS spammer to send you plausibly formatted random
numbers to burn off your CPU.  But we have a separate solution to
that: you make the spammer provide a hashcash token of comparable cost
to that verification and this can be verified an one order of
magnitude or more efficiently and increases the would-be DoSers costs
to be comparable to the signature verification.  (Or more if you wish

@_date: 2003-05-15 09:56:17
@_author: Adam Back 
@_subject: using PoW + filters to avoid false positives (Re: Re: A Trial Balloon to Ban Email?) 
The short term usefulness of a hashcash / PoW filter when used with
bayesian filters (which I think is what Joseph is saying below) is
that you are less likely to accidentally lose mail due to Bayesian
filters.  Ideally blackhole lists should also be exempted if there is
hashcash (they are another big source of loss of email, I've been hit
by that a number of times).
I suspect increasngly more email will be lost to filters and blackhole
lists because the anti-spam people are becoming increasingly gung-ho
and sweeping in their blackholing and filtering because the problem is
accelerating out of control, so the short term function of hashcash to
improve email reliability could be a useful extra function.
(Estimates vary but at ASRG kick off at IETF there were some very high
per month growth figures (10% and higher per month) for spam which
were far in excess of (non-spam) email growth).
Similarly your incentive to send hashcash in the short term is to
avoid your own mail similarly being swallowed by blackholes and
Bayesian filtering false positives.
The limitation with blackholes is it depends on the blackhole
implementation, some are simply refusing the TCP connection at
firewall level; others are accepting but giving you a 500 (or whatever
it is) response code explaining why -- but that is already too early
for them to have read the X-Hashcash headder.  One way around that is
to include hashcash as an ESMTP address parameter which I understand
allows you to say things after the RCPT TO, but even that may be too
late (if they already said go away after the HELO).
Another approach but only longer term and it is debatably too
aggressive/draconian, and in the short term has same problem as TCP
rejection of blackholed IPs would be integration of hashcash into TCP
like syncookie (see section 4.2 hashcash cookies of [1]) so that the
mailer can reject port 25 connections which don't have hashcash
tokens.  Or perhaps (less aggressively) to use a getsocketops or ioctl
to read from the socket whether the sender is using hashcash or not.
One problem with this approach is the PoW received by the MTA may not
be convincing to the recipient, so there remains risk that the
recipient could be spammed by a colluding or host compromised MTA at
their ISP.  (You could add envelope recipient emails to the puzzle,
but that's sufficiently SMTP related you'd just as well send it in
SMTP).  Another integration point could be IPSEC.
On the interactive connection DoS hardening side, there was a paper
about using Juel's and Brainard's Client Puzzles [2] (which is a known
solution puzzle where the server has to issue the challenge
interactively) for SSL DoS hardening [3].
More recently, though I haven't obtained a copy yet, Xiaofeng Wang and
Michael Reiter have a paper about an implementation hardening the
linux kernel TCP stack against DoS using puzzles [4], I'm presuming
this is similar to the hashcash-cookie approach from the abstract,
though I'm not sure which puzzle they used.  (Not sure what the puzzle
auction mechanism is).
[1] Aug 02 - "Hashcash - A Denial of Service Counter Measure" (5 years
on), Tech Report, Adam Back
[2] Ari Juels and John Brainard. Client puzzles: A cryptographic
countermeasure against connection depletion attacks. In Network and
Distributed System Security Symposium, 1999. Also available as
[3] Drew Dean and Adam Stubblefield. Using cleint puzzles to protect
tls. In Proceedings of the 10th USENIX Security Symposium, Aug
2001. Also available as [4] XiaoFeng Wang and Michael Reiter, "Defending Against
Denial-of-Service Attacks with Puzzle Auctions", IEEE Symposium on
Security and Privacy 2003

@_date: 2003-05-18 17:59:48
@_author: Adam Back 
@_subject: what fields to hash with hashcash (Re: A Trial Balloon to 
bill.stewart on Fri, May 16, 2003 at 05:20:44PM -0700
That particular approach is vulnerable to precomputation and
amortization fo computation against different target strings.
ie Attacker can pre compute and store 2**N inputs and have fair chance
of being able to solve by lookup.  Similarly he can for the same cost
find collisions on SHA1(T) and SHA1(T') simultaneously.
What the original hashcash function did was look for Bit(i,SHA1(T||X))
== B(i,SHA1(T)) for i = 1..n that way the candidate solutions are
useless against other targets.
A more recent simplifcation is to just use the all 0 bit string as the
target.  So you're looking for Bit(i,SHA1(T||X)) = 0 for i = 1..n.

@_date: 2003-05-24 07:30:44
@_author: Adam Back 
@_subject: 8-bit modular exponentiation code? 
Colin Plumb's crypto library bnlib supports multiple word size I

@_date: 2003-11-02 09:26:56
@_author: Adam Back 
@_subject: ECC and blinding. 
Fair enough.  But this is not Chaum's scheme, it is Wagners and it is
DH based (or ECDH based in your writeup).
You said earlier:
and the above scheme is not Chaumian blinding.  Chaum never invented
DH blinding, if you read Brands thesis even you'll see that Chaum (who
was Brands PhD supervisor for some of the time) told Brands to forget
about trying to do DH based blinding because it's not possible.
Brands credits Chaum for setting the challenge :-) which led him to
find ways to do DH based blinding.  (And the private key certificate
which is a generalisation of DH blinding to multiple attributes and
selective disclosure of those attributes).

@_date: 2003-10-28 13:49:49
@_author: Adam Back 
@_subject: ECC and blinding. 
There are two variants of Brands schemes: over RSA or DH.  The DH
variant can be used with the EC.  People don't do RSA over EC because
the security argument doesn't work (ie I believe you can do it
technically, but the performance / key size / security arguments no
longer work).
So for that reason I think Chaum's scheme practically would not be
viable over EC.  (Or you could do it but you'd be better off
performance, security and key/messag size doing Chaum over normal
There are other blinding schemes also such as David Wagner's blind MAC
approach, and that should work over EC as it is DH based.

@_date: 2003-10-31 15:26:05
@_author: Adam Back 
@_subject: ECC and blinding. 
So Chaumian blinding with public exponent e, private exponent d, and
modulus n is this and blinding factor b chosen by the client:
b^e.m mod n	 ->
 			= b.m^d mod n  (simplifying)
and divide by b to unblind:
m^d mod n
how are you going to do this over EC?  You need an RSA like e and d to
Brands DH based blinding scheme works in EC.  ECDH is directly
analogous, the usual conversion from discrete log (g^x mod p) to the
EC analog (x.G over curve E) works.

@_date: 2003-09-26 12:47:51
@_author: Adam Back 
@_subject: free hosting for cpunkly projects... 
remops and cpunks:
 are offering:
free for 3 years as an advertising ploy to get into small business /
personal web posting.
They use a computerized phone call back to prevent people being silly
and registering thousands of accounts, so you have to give them your
phone  (And that works for US and Canadian numbers only).
But they don't require a credit card (unless you start buying extras).
btw They also have the cheapest domain registration I've seen so far
at $6/year.
You also don't have to transfer your domain to them in order to host
there, you can just point your existing domain's DNS to them, and they
also will optionally keep your existing MX in their DNS so your mail
is handled however it was before.
Well just thought remops and cpunks might find some interesting uses
or it would be a bonus to people working on crypto / remailer coding,
or internet service type stuff to have some serious hosting to work
with or do dev work in.  (They have gcc, pgp, ssh etc installed).
They seem quite serious hosting-wise and I got a 12 hr turn around on
a fix for a system /etc/csh.cshrc bug that was preventing me changing
my shell to tcsh which is pretty impressive.  (Most hosting shops
wouldn't bother answering or would fob you off, or your query would
not get anywhere near someone clue-full enough to understand never
mind tweak and fix the minor problem).  Their bandwidth is supposed to be pretty good too as they have 12Gbits
connectivity.  Claim to have been in business for 11 years.
3 years free seems pretty generous (most free offers are a few months
or 1 year tops).  The offer is available only until 31st Dec this
year.  on serial number on account it looks like they might be signing up
around 1000 accounts per day.  So I could imagine they might pull the
offer also if they got too much take up!
Have phun.

@_date: 2004-08-02 05:36:26
@_author: Adam Back 
@_subject: you can't argue with economics (Re: On how the NSA can be 
But most cryptanalysis types of things are economic defenses.  (ie you
can spend $lots you can break; or you don't have enough $ to build
because the $ at current tech is an astronomical multiple of the US
national debt).
So if the NSA are being stupid, and uneconomical with the black budget
(and it's not that hard for large organizations even with competition
to be stupid), then they will be even less likely to break things that
they could break than if they outsourced the whole thing.
Probably to their advantage, I presume they do in fact outsource many
things and of course buy large expensive bits of machinery and
components, as anyone must do.
So anyway, doing uneconomical things with the black budge they would
lessen their chance of breaking various things, not increase it.
Now the sheer scale of the black budget allows some things, but no
doubt their best strategy will be to do economical things wrt their
objectives and priorities and put as much as they can out for
commercial tender, and/or try to create internal competition or

@_date: 2004-08-04 18:16:14
@_author: Adam Back 
@_subject: planet sized processors (Re: On what the NSA does with its 
The planet sized processor stuff reminds me of Charlie Stross' sci-fi
short story "Scratch Monkey" which features nanotech, planet sized
processors which colonize space and build more planet-sized
processors.  The application is upload, real-time memory backup, and
afterlife in DreamTime (distributed simulation environment), and an
option of reincarnation.

@_date: 2004-08-12 14:41:40
@_author: Adam Back 
@_subject: maybe he would cash himself in? (Re: A Billion for Bin Laden) 
Maybe Bin Laden would turn himself in in return for a billion $ for
his cause (through a middle-man of course).
Seem to remember that Bin Laden was relatively wealthy himself (>100
M$?), but you'd have to balance these rewards to not be too
excessively much more than net worth of the individual.  As a rational
adversary would include in his game plan swapping himself for the
money for the cause.
Especially if it could be arranged in a way which tends to cast Bin
Laden in the martyr role him and encourage the hydra effect where it
galvanizes leutenants to step in.
Bin Laden would have to balance also with how valueable he thought his
leader ship was.
Of course the lieutenants themselves might do the calculation and
figure they would be closer to their goals after cashing in Bin Laden.

@_date: 2004-08-18 16:33:12
@_author: Adam Back 
@_subject: hash attacks and hashcash (SHA1 partial preimage of 0^160) 
(This discussion from hashcash list is Cc'd to cryptography and
Hashcash uses SHA1 and computes a partial pre-image of the all 0bit
string (0^160).
Following is a discussion of what the recent results from Joux, Wang
et al, and Biham et al on SHA0, MD5, SHA1 etc might imply for hashcash
SHA1 (and for hypothetical hashcash SHA0, MD5 etc by way of seeing
what it will mean if SHA1 eventually suffers similar fate to SHA0).
(All as far as I understand so far).
Hashcash stresses the SHA1 function in a different direction than
sigantures and MACs -- in assuming partial pre-images are hard (ie an
k-bit partial pre-image should take about 2^k operations).  (Partial
2nd pre-images are also "interesting" against hashcash -- see below).
(As a security argument if partial pre-images say up to m To be clear:

@_date: 2004-12-16 05:50:22
@_author: Adam Back 
@_subject: pgp "global directory" bugged instructions 
So PGP are now running a pgp key server which attempts to consilidate
the inforamtion from the existing key servers, but screen it by
ability to receive email at the address.
So they send you an email with a link in it and you go there and it
displays your key userid, keyid, fingerprint and email address.
Then it says:
So here's the problem: it does not mention anything about checking
that this is your fingerprint.  If it's not your fingerprint but it is
your email address you could end up DoSing yourself, or at least
perpetuating a imposter key into the new supposedly email validated
keyserver db.
(For example on some key servers there are keys with my name and email
that are nothing to do with me -- they are pure forgeries).
Suggest they add something to say in red letters check the fingerprint
AND keyid matches your key.

@_date: 2004-07-07 16:09:31
@_author: Adam Back 
@_subject: Email tapping by ISPs, forwarder addresses, and crypto 
This is somewhat related to what ZKS did in their version 1 [1,2] mail
They made a transparent local pop proxy (transparent in that it
happened at firewall level, did not have to change your mail client
config).  In this case they would talk to your real pop server,
decrypt the parts (they were reply-block like onions), remove
duplicates (as with mixmaster etc you can send duplicates via separate
remailers to improve reliability).  So the transparent proxy would
leave alone your normal mail that you received in the pop box and
remove duplicates only from the reply-block delivered pseudonymous
Actually they implemented the reply-block from scratch, it always
seemed to me it would have been less development work to use mixmaster
(it was implemented before I started).  The ZKS reply block did not
even use chunking (ala mixmaster) so traffic analysis would have been
trivial as the message size would show through.
At least that's what I recall, no chunking.  However I am finding the
security issues paper [1] says otherwise.  The 1.0 architecture
document [2] is ambiguous, there is no mention of chunking.
(I've sent mail to one of the original developers to check I have it
It was also unreliable because it did not use SMTP, it used its own
transport AMTP and its own retry-semantics on nodes called
MAIPs. (Mail AIPs, an AIP is an "Anonymous Internet Proxy").
Then we implemented a replacement version 2 mail system that I
designed.  The design is much simpler.  With freedom anonymous
networking you had anyway a anonymous interactive TCP feature.  So we
just ran a standard pop box for your nym.  Mail would be delivered to
it directly (no reply block) for internet senders.  Freedom senders
would send via anonymous IP again to get sender anonymity.  Used qmail
as the mail system.
Unfortunately they closed down the freedom network pretty soon after
psuedonymous mail 2.0 [3] was implemented.
There is an interesting trade-off here.  The interactive
communications are perhaps more vulnerable to real-time powerful
adversary traffic analysis than mixmaster style mixed chunked
delivery.  However they are less vunerable to subpoena because they
are forward-secret on a relativey short time-frame.  (1/2 hr if I
recall; however more recent designs such as chainsaw internal
prototype, and cebolla [4] by ex-ZKSer Zach Brown change keys down to
second level by using a mix of backward-security based on symmetric
key hashing (and deleting previous key) and forward security using DH.)
It would be nice to get both types of anonymity, but I suspect for
most typical users the discovery / subpeona route is the major danger,
and if that is thwarted it is unlikely that their activities would
warrant the effort of real time analysis.  Well we have carnivore now,
so they could potentially do real-time traffic analysis more routinely
if they were to distribute enough collaborating analysis carnivore
[1] [2] [3] [4]

@_date: 2004-07-13 17:32:18
@_author: Adam Back 
@_subject: zks source (Re: Email tapping by ISPs, forwarder addresses, and 
You could try sending an email to Austin Hill  to see
if he could organize releasing source for remaining freedom related
source that they are not currently using.

@_date: 2004-07-21 11:46:16
@_author: Adam 
@_subject: unknown_subject 
>fotoinfo
[demime 1.01d removed an attachment of type application/octet-stream which had a name of Cat.com]

@_date: 2004-06-17 16:36:34
@_author: adam@cypherspace.org 
@_subject: Hello 
Please, have a look at the attached file.
   Password - [cid:otykglabjs.bmp]
[demime 1.01d removed an attachment of type image/bmp which had a name of otykglabjs.bmp]
[demime 1.01d removed an attachment of type application/octet-stream which had a name of Info.zip]

@_date: 2004-05-09 06:04:31
@_author: Adam Back 
@_subject: Brands' private credentials 
[copied to cpunks as cryptography seems to have a multi-week lag these
OK, now having read:
and seeing that it is a completely different proposal essentially
being an application of IBE, and extension of the idea that one has
multiple "identities" encoding attributes.  (The usual attribute this
approach is used for is time-period of receipt .. eg month of receipt
so the sender knows which key to encrypt with).
so here is one major problem with using IBE: everyone in the system
has to trust the IBE server!
One claim is that the system should hide sensitive attributes from
disclosure during a showing protocol.  So the example given an AIDs
patient could authenticate to an AIDS db server without revealing to
an outside observer whether he is an AIDs patient or an authorised
However can't one achieve the same thing with encryption: eg an SSL
connection and conventional authentication?  Outside of this, the usual approach to this is to authenticate the
server first, then authenticate the client so the client's privacy is
Further more there seems to be no blinding at issue time.  So to
obtain a credential you would have to identify yourself to the CA /
IBE identity server, show paper credentials, typically involving True
Name credentials, and come away with a private key.  So it is proposed
in the paper the credential would be issued with a pseudonym.  However
the CA can maintain a mapping between True Name and pseudonym.
However whenever you show the credential the event is traceable back
to you by collision with the CA.
I would not say your Hidden Credential system _is_ an anonymous
credential system.  There is no blinding in the system period.  All is
gated via a "trust-me" CA that in this case happens to be an IBE
server, so providing the communication pattern advantages of an IBE
What it enables is essentially an offline server assisted oblivious
encryption where you can send someone a message they can only decrypt
if they happen to have an attribute.  You could call this a credential
system kind of where the showing protcool is the verifier sends you a
challenge, and the shower decrypts the challenge and sends the result
In particular I don't see any way to implement an anonymous epayment
system using Hidden Credentials.  As I understand it is simply not
possible as the system has no inherent cryptographic anonymity?

@_date: 2004-05-09 07:07:47
@_author: Adam Back 
@_subject: anonymous IRC project needs new home... 
The anonymous IRC project (IIP -- provides encrypted anonymous IRC chat.
Haven't looked in the protocol in detail to see how they get their
anonymity, but the guy seemed aware of Chaum etc and they have crypto
protocols document up there.
They have resource problems in continuing to run it, and so have
announced end-of-life for the project, but source etc is available,
and they are calling for interest in taking over the project.
Anyone with a bit of bandwidth and interest in preserving anonymity of
IRC want to help them out?
(The way I first heard about the project is that they use hashcash to
throttle nym registration abuse -- before that people were creating
1000s of handles through it.)

@_date: 2004-05-10 05:35:28
@_author: Adam Back 
@_subject: Brands' private credentials 
Well SSL was just to convince you that you were talking to the right
server ("you have reached the AIDs db server").
After that I was presuming you use a signature to convince the server
that you are authorised.  Your comment however was that this would
necessarily leak to the server whether you were a doctor or an AIDs
However from what I understood from your paper so does your scheme,
from section 5.1:
P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)} With Hidden Credentials, the messages are in the other direction: the
server would send something encrypted for your pseudonym with P1 =
AIDs patient, and P2 = Doctor attributes.  However the server could
mark the encrypted values by encoding different challenge response
values in each of them, right?
(Think you would need something like Bert Jaap-Koops Binding
cryptography where you can verify externally to encryption that the
contained encrypted value is the same to prevent that; or some other
proof that they are the same.)
Another approach to hiding membership is one of the techniques
proposed for non-transferable signatures, where you use construct:
RSA-sig_A(x),RSA-sig_B(y) and verification is x xor y = hash(message).
Where the sender is proving he is one of A and B without revealing
which one.  (One of the values is an existential forgery, where you
choose a z value first, raise it to the power e, and claim z is a
signature on x= z^e mod n; then you use private key for B (or A) to
compute the real signature on the xor of that and the hash of the
message).  You can extend it to moer than two potential signers if
OK so the fact that the server is the AIDs db server is itself secret.
Probably better example is dissident's server or something where there
is some incentive to keep the identity of the server secret.  So you
want bi-directional anonymity.  It's true that the usual protocols can
not provide both at once; SSL provides neither, the anonymous IP v2
protocol I designed at ZKS had client anonymity (don't reveal
pseudonym until authenticate server, and yet want to authenticate
channel with pseudonym).  This type of bi-directional anonymity pretty
much is going to need something like the attribute based encryption
model you're using.
However it would be nice/interesting if one could do that end-2-end
secure without needing to trust a CA server.
this one is a feature auth based systems aren't likely to be able to
fullfil, you can say this because the server doesn't know if you're
able to decrypt or not
I think it would be fair to call it anonymity system, just that the
trust model includes a trusted server.  There are lots of things
possible with a trusted server, even with symmetric crypto (KDCs).

@_date: 2004-05-10 06:02:51
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: chaum's 
I think you mean so that the CA/IBE server even though he learns
pseudonyms private key, does not learn the linkage between true name
and pseudonym.  (At any time during a show protocol whether the
private key issuing protocol is blinded or not the IBE server can
compute the pseudonyms private key).
Seems like an incremental improvement yes.
Note PFS does not make end-2-end secure against an adversary who can
compute the correspondents private keys, as vulnerable to MITM.  Could
say invulnerable to passive eavesdropper.  However you might have an
opening here for a new security model combining features of Hidden
Credentials with a kind of MITM resistance via anonymity.  What I mean
is HC allows 2 parties to communicate, and they know who they are
communicating with.  The CA colluding MITM however we'll say does not
apriori, so he has to brute force try all psuedonym, attribute
combinations until he gets the right one.  Well still not desirable
security margin, but some extra difficulty for the MITM.

@_date: 2004-05-10 17:54:59
@_author: Adam Back 
@_subject: more hiddencredentials comments (Re: Brands' private 
OK that sounds like it should work.  Another approach that occurs is
you could just take the plaintext, and encrypt it for the other
attributes (which you don't have)?  It's usually not too challenging
to make stuff deterministic and retain security.  Eg. any nonces,
randomizing values can be taken from PRMG seeded with seed also sent
in the msg.  Particularly that is much less constraining on the crypto
system than what Bert-Jaap Koops had to do to get binding crypto to
work with elgamal variant.
The above approach should fix that also right?
dissident computing I think Ross Anderson calls it.  People trying to
operate pseudonymously and perhaps hiding the function of their
servers in a cover service.
Unless it's signifcantly less efficient, I'd say use it all the time.
Yes.  But you could explore public key based without IBE.  You may
have to use IBE as a sub-protocol, but I think ideally want to avoid
the IBE server being able to decrypt stuff.  Sacrificing the IBE
communication pattern wouldn't seem like a big deal.
Hmm well IBE is has a useful side-effect in pseudonymity systems
because it also has the side-effect of saving the privacy problems in
first obtaining the other parties key.  Other way to counteract that
is to always include the psuedonym public key with the pseudonym name
(which works for mailto: style URLs or whatever that are
electronically distributed, but not for offline distributed).
Btw one other positive side-effect of IBE is the server can't
impersonate by issuing another certificate in a pseudonyms name
because there is definitionally only one certificate.
I was thinking particularly if you super-encrypt with the psuedonym's
(standard CA) public key as well as the IBE public key you get the
best of both feature sets.
btw You could probably come up with a way to prevent a standard (non
IBE) CA from issuing multiple certs.  eg. if he does that and someone
puts two certs together they learn CA private key, ala Brands
credential kind of offline double spending protection.
Kind of a cryptographically enforced version of the policy enforced
uniqueness of serial numbers in X.509 certs.  And we change the policy
to one cert per pseudonym (kind of sudden death if you lose the
private key, but hey just don't do that; we'd have no other way to
authenticate you to get a new cert in the same psuedonyms name anyway,
so you may just as well backup your pseudonym private key).

@_date: 2004-05-10 18:24:31
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: 
But if I understand that is only half of the picture.  The recipient's
IBE CA will still be able to decrypt, tho the sender's IBE CA may not
as he does not have ability to compute pseudonym private keys for the
other IBE CA.
If you make it PFS, then that changes to the recipient's IBE CA can
get away with active MITM rather than passive eavesdropping.
An aside is that PKI for Psuedonym's is an interesting question.  The
pseudonym can't exactly go and be certified by someone else as an
introducer without revealing generally identifying things about the
network of trust.  But ignoring this presuming that the identities
were not subject to MITM from day one, and could build up a kind of
WoT despite lack of out-of-band way to check info to base WoT
signatures on.  It would still be interesting to defend the pseudonym
against MITM colluding with IBE CA that at some point after the
pseudonym has transferred keys without insertion of a MITM from.
This problem of addressing the who goes first problem for pseudonymous
communicants appears somewhat related to Public Key Steganography
where there is a similar scenario and threat model.  (Anderson and
Petitcolas"On The Limits of Steganography"
They also cite a "Prisoners' problem" which might be something you
could extend involving a warden who is eavesdropping and prisoners who
will be penalized if he can detect and identify communicants.
My earlier comment:
may not be that useful a distinction as the IBE CA server also gets
your private key, so he doesn't _need_ to generate a certificate
impersonating you as a conventional rogue CA would.
But if we could make the binding from pseudonym to the pseudonym's
non-IBE public key strictly first come first served, so that the IBE
CA's attemt to claim his later released non-IBE public key is the
correct one would be detectable.  Either secure time-stamping,
extending the psuedonym name to include fingerprint as
self-authenticator would allow this.

@_date: 2004-05-10 18:59:40
@_author: Adam Back 
@_subject: more hiddencredentials comments (Re: Brands' private 
Gap may be I'm misunderstanding something about the HC approach.  We have:
 P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)}
so one problem is marking, the server sends you different R values:
so you described one way to fix that by using symmetric crypto (where
it is difficult to get a message to decrypt 2 different ways with
different keys and get other than line noise out of the 2nd key).
But next problem you mentioned, server could simply lie and send you
for random value R2 now if you reply he knows you have property P1.
So I was suggesting that after you decrypt HC_E(R,P1) you encrypt it
again to check if R2 == HC_E(R,P2) which you should be able to do if
you know P2, you have R (because you just decrypted it), and if you
tweak the crypto system so that there is no non-deterministic aspect
such as OAEP, randomization factors etc.
If one were not explicitly interested in the IBE communication
pattern, and to avoid the patents in IETF protocol problems, I would
think one could do something without IBE.
eg. you mentioned earlier the problem of issuing one cert per
attribute permutation.  Instead how about you issue one cert per
attribute to psuedonym plus attribute.  In the case where you are not
due the attribute, you just don't learn the corresponding private key.
One problem with this is you have to avoid the server learning the
private key for the one you don't.
Now it might be possible eg. with Elgamal / DH to make an efficient
non-interactive ZKP that convinces you that the server chose the
private key fairly (and so does not know any corresponding private
key).  But another way to side-step the issue is to have the CA issue
you two certs per attribute.  You choose the private key for one it
chooses the private key for the other.  Data is encrypted with both
keys.  In the case of you not being due the attribute the CA does not
give you the private key it generated.
You could probably use some of the key gen stuff from multi-party
signatures (where multiple parties are involved and each holds a
private key fragment), however they tend to be inefficient I think so
above is probably simple and efficient enough.

@_date: 2004-05-12 07:20:33
@_author: Adam Back 
@_subject: who goes 1st problem 
I think this is ok.  Would suggest you remove the nym field, have
one-use credentials (to avoid linkability across provers), and only
reveal separate nym cert after have satisfied policy.
Again ok.  You send either fake cert, or real cert for as many
attributes as the CA issues.  You may not even know what some of the
attributes that the CA issues are, all you know is the number of them.
You use and / or connectives between them (using k xor r, k; or r, r
respectively) but using OBSE algorithm (xor refers to improved HC
scheme by HC authors in Yes, that works, but is defined required part of protocol; that way
optimal cover (within limits of partial policy concealment) is given
for sensitive attributes, policies etc.
Sounds same as above.
That's true.  Think there is a trade-off between degree of
concealment, and amount of permutations prover has to try.  You could perhaps define an ordering of attributes safely, followed by
dealing with unordered undeclared attributes.  Other thought perhaps a FPGA like layout where all possible
connectives patterns are represented, might allow to specify arbitrary
boolean formulae with and / or connectives with full policy
concealment but less space and time efficient.
(Calling it prover is kind of odd I find when the prover convinces only
himselfhe satisfies policy by default and optionally chooses whether
to disclose that to verifier.  And "the prover" is the passive entity
receiving encrypted comms, which is back-to-front to usual
prover-verifier comms pattern.  Maybe sender and recipient is better.)

@_date: 2004-05-18 23:49:11
@_author: Adam Back 
@_subject: 3. Proof-of-work analysis 
Here's a forward of parts of an email I sent to Richard with comments on
his and Ben's paper (sent me a pre-print off-list a couple of weeks ago):
One obvious comment is that the calculations do not take account of
the CAMRAM approach of charging for introductions only.  You mention
this in the final para of conclusions as another possible.
My presumption tho don't have hard stats to measure the effect is that
much of email is to-and-fro between existing correspondents.  So if I
were only to incur the cost of creating a stamp at time of sending to
a new recipient, I could bear a higher cost without running into
However the types of levels of cost envisaged are aesthetically
unpleasing; I'd say 15 seconds is not very noticeable 15 mins is
noticeable and 1.5 hrs is definately noticeable.
Of course your other point that we don't know how spammers will adapt
is valid.  My presumption is that spam would continue apace, the best
you could hope for would be that it is more targetted, that there are
financial incentives in place to make it worth while buying
demographics data.  (After all when you consider the cost of sending
junk paper mail is way higher, printing plus postage, and yet we still
receive plenty of that).
Also as you observe if the cost of spamming goes up, perhaps they'll
just charge more.  We don't know how elastic the demand curve is.
Profitability, success rates etc are one part of it.  There is an
interplay also: if quantity goes down, perhaps the success rate on the
remaining goes up.  Another theory is that a sizeable chunk of spam is
just a ponzi scheme: the person paying does not make money, but a lot
of dummy's keep paying for it anyway.
Another potential problem with proof-of-work on introductions only, is
that if the introduction is fully automated without recipient opt-in,
spammers could also benefit from this amortized cost.  So I would say
something like the sender sent a proof-of-work, and the recipient took
some positive action, like replying, filing otherwise than junk or
such should be the minimum to get white-listed.
On the ebiz web site problem, I think these guys present a problem for
the whole approach.  An ebiz site will want to send lots of mail to
apparent new recipients (no introductions only saving), a popular ebiz
site may need to send lots of mail.
Well it is ebiz so perhaps they just pass the cost on to the consumer
and buy some more servers.
Another possibility is the user has to opt-in by pre-white-listing
them, however the integration to achieve this is currently missing and
would seem a difficult piece of automation to retrofit.
One of the distinguishing characteristics of a spammer is the
imbalance between mail sent and mail received.  Unfortunately I do not
see a convenient way to penalize people who fall into this category.
Also because of network effect concerns my current hashcash deployment
is to use it as a way to reduce false positives, rather than directly
requiring hashcash.  Well over time this could come to the same thing,
but it gives it a gentle start, so we'll see how long it is before the
1st genuine spam with hashcash attached.
CAMRAM's approach is distinct and is literally going straight for the
objective of bouncing mail without some kind of proof (hashcash or
reverse-turing, or short term ability to reply to email

@_date: 2004-10-03 15:41:25
@_author: Adam Back 
@_subject: Foreign Travelers Face Fingerprints and Jet Lag 
I don't know if my info is still current (and I did not read the
article), but the last time I went to the US (early this year) my H1B
was no longer in effect (I quit microsoft last year, and H1B visas are
tied to employer), and I did not get fingerprinted.
However they had a camera and fingerprinting equipment, and notice
saying that if you _did_ have H1B and other such temporary US visa
documents you would be photographed and fingerprinted.

@_date: 2004-10-05 17:13:47
@_author: Adam Back 
@_subject: Brands credential book online (pdf) 
For people interested in ecash / credential tech: Stefan Brands book
on his credential / ecash technology is now downloadable in pdf format
from credentica's web site:
(previously it was only available in hardcopy, and only parts of the
content was described in academic papers).
also the credentica web site has gone live, lots of content. (credentica is Stefan's company around digital credentials).

@_date: 2004-09-03 11:37:03
@_author: Adam Back 
@_subject: gmail as a gigabyte of an external filesystem 
Don't know anything about EncFS, but you could also use loopback
encryption on top of gmailfs.  Just make a large file in gmail fs, and
make a filesystem in it via loopback virtual block device-in-a-file.

@_date: 2004-09-08 17:03:03
@_author: Adam Back 
@_subject: Seth Schoen's Hard to Verify Signatures 
I proposed a related algorithm based on time-lock puzzles as a step
towards non-parallelizable, fixed-minting-cost stamps in section 6.1
of [1], also Dingledine et al observe the same in [2].
The non-parallelizable minting function is in fact the reverse: sender
encrypts (expensively) and the verifier encrypts again (but more
cheaply) and compares, but I think the relationship is quite analogous
to the symmetry between RSA encryption and RSA signatures.
I think maybe you have observed an additional simplification.  In my
case I use sender chooses x randomly (actually hash output of random
value and resource string), and computes y = x^{x^w} mod n as the work
function (expensive operation); and z = x^w mod phi(n), y =?  x^z mod
n as the cheap operation (verification).
I think your approach could be applied on the encryption side too
resulting in simpler, faster verification.  Instead it would be:
x is random, compute y = x^{2^t+1} mod n; verify x =? y^d mod n
I'll add a note about that when I get around to updating it next.
[1] Hashcash - Amortizable Publicly Auditable Cost-Functions
[2] Andy Oram, editor.  Peer-to-Peer: Harnessing the Power of
Disruptive Technologies. O'Reilly and Associates, 2001.  Chapter 16
also available as

@_date: 2004-09-11 13:49:23
@_author: Adam Back 
@_subject: anonymous IP terminology (Re: [anonsec] Re: potential new IETF WG 
on anonymous IPSec (fwd from hal at finney.org))
User-Agent: Mutt/1.4.1i
Sender: owner-cryptography at metzdowd.com
I respectfully request that you call this something other than
"anonymous".  It is quite confusing and misleading.
Some people have spent quite a bit of time and effort in fact working
on anonymous IP and anonymous/pseudonymous transports.
For example at ZKS we worked on an anonymous/pseudonymous IP product
(which means cryptographically hiding the souce IP address from the
There are some new open source anonymous IP projects.
Your proposal, which may indeed have some merit in simplifying key
management, has _nothing_ to do with anonymous IP.  Your overloading
of the established term will dilute the correct meaning.
Zooko provided the correct term and provided references:
"opportunistic encryption".  It sounds to have similar objectives to
what John had called opportunistic encryption and tried to do with
freeSWAN.  Lowever level terms may be unauthenticated as Hal
suggested.  Or non-certified key management (as the SSH cacheing of
previously before seen IP <-> key bindings and warnings when they
The access is _not_ anonymous.  The originator's IP, ISP call traces,
phone access records will be all over it and associated audit logs.
The distinguishing feature of anonymous is that not only is your name
not associated with the connection but there is no PII (personally
identifiable information) associated with it or obtainable from logs
And to be clear also anonymous means unlinkable anonymous across
multiple connections (which SSH type of authentication would not be)
and linkable anonymous means some observable linkage exists between
sessions which come from the same source (though no PII), and
pseudonymous means same as linkable anonymous plus association to a
persistent pseudonym.
Again there are actually cryptographic protcols for_ having anonymous
authentication: ZKPs, multi-show unlinkable credentials, and
refreshable (and so unlinkable) single-show credentials.

@_date: 2004-09-11 15:09:54
@_author: Adam Back 
@_subject: anonymous IP terminology (Re: [anonsec] Re: potential new IETF 
WG on anonymous IPSec (fwd from hal at finney.org))
User-Agent: Mutt/1.4.1i
Sender: owner-cryptography at metzdowd.com
I think you are confusing a weak potential for a technical ambiguity
of identity under attack conditions with anonymity.  (The technical
ambiguity would likely disappear in most practical settings).
Anonymity implies positives steps to avoid linking with PII.  With
anonymity you want not just technical ambiguity, but genuinely
pluasible deniability from an anonymity set -- preferably a large set
of users who could equally plausibly have established a given
connection, participated in an authentication protocol etc.
We don't after all call TCP anonymous, and your system is cleary
_less_ "anonymous" than TCP as there are security mechanisms involved
with various keys and authentication protocols which will only reduce
Practically, knowing the IP address conveys a lot.  Many ISPs have
logs, some associated with DSL subscriber and phone records, for
billing, bandwidth caps, abuse complaints, spam cleanup etc etc.
The IP may be used for many different logged activities and some of
those activites may involve directly identified authentication.
People go to lengths to hide their IP precisely because it does
typically convey all too much.
If one wants this to be true in practice it has to propogate up the
stack.  (Not the problem of ANONSEC, a problem for the higher level
But even at the authentication protocol level one has to be quite
careful.  There are many gotchas if you really do want it to be
unlinkable.  (eg. pseudo random sequences occur in many settings at
different protocol levels which are in fact quite linkable).  I'll
give you one high level example.  At ZKS we had software to remail
MIME mail to provide a pseudonymous email.  But one gotcha is that
mail clients include MIME boundary lines which are pseudo-random
(purely to avoid string collision).  If these random lines are
generated with a non-cryptographic RNG it is quite likely that so
called unlinkable mail would in fact be linkable because of this
higher level protocol.  (We cared about unlinkability even tho' I said
pseudonymous because the user had multiple pseudonyms which were
supposed to be unlinkable across).
I would say if your interest in fixing such pseudo random sequeneces
is not present you should not be calling this anonymous.
But if it is part of your threat model, then you may in fact be using
anonymous authentication and that would be interesting to me at least
to participate.

@_date: 2004-09-13 16:43:57
@_author: Adam Back 
@_subject: will spammers early adopt hashcash? (Re: Spam Spotlight on 
User-Agent: Mutt/1.4.1i
Ben and Richard CLayton's paper makes several assumptions and we'll
see how those pan out in the field as time goes on.
We don't really know what the true cost of maintaining ownership of
many machines.  No doubt much lower than it should be because of poor
security on microsoft OSes.  But even so there must be some turn over
as the user instals AV, firewalls, gets cut off by ISP, gets IP
blacklisted etc.
The general argument is in the FAQ quoted below.
Essentially whatever resources spammers do have, hashcash is going to
slow them down because the balance of CPU power vs bandwidth is such
that 20-bit hashcahs with current hardware is likely to slow down the
output of a typical consumer destkop+DSL line down by afact or 10-100x
less spam.  (Depnds on CPU power, DSL uplink, and number of Bcc
recipients per message).  Hashcash costs equal cpu per Bcc recipient.
Without hashcash Bcc recipients to the same domain or to a hub cost a
tiny bit of bandwidth -- the size of the email address (+"RCPT TO
Will it be enough -- we don't know yet, but if widely deployed it
would make spammers adapt.  We just don't yet know how they will
The other question Ben & Richards paper doesn't explore is the CAMRAM
way of using hashcash.  In this model you only pay hashcash for
_introductions_.  After parties have replied to a mail, the mail is
whitelisted (short term by address only (risky no auth, joe-job
hazard) medium term with CAMRAM email header signatures).  If simple
hashcash per mail turns out not to be enough, CAMRAM can increase the
work factor, as people do not reply to spammers; and many emails are
to-and-fro vs first introduction emails.  (So the sender can afford to
pay more on average).  Eric sent a spreadsheet with some of this type
of calculation.
There may also be some mileage in Hal Finney's RPOW
 where the legitimate user can re-use stamps he
receives.  (The scaling issues of the RPOW servers would need to be
engineered carefully, there are servers, they can be per eg domain ,
but still compared to hashash this is more infrastructure as hashcash
is pure end-to-end).
 2c and 2d
--- end forwarded text

@_date: 2005-12-02 08:35:16
@_author: Adam Back 
@_subject: idealized content network properties (Re: [p2p-hackers] darknet) 
I think an ideal www2 network should:
1. have any content searchable by anyone (the contents are public)
2. make it hard to determine who the author of content is
3. make it hard for people other than the author to remove content
4. make it hard for people to observe what other people are downloading
5. make it hard for anyone to change content (new version and
navigating by version should be the way to "change")
It seems to me that this network can provide any of these subset
classifications trivially.
removing 1 makes a eg "friend-to-friend" network -- that just means
you encrypt the searchable tags and content with a shared key.
removing 2 you just sign the content.
and so forth.
(Making it hard for people other than the author to remove content
technically probably involves things like redundancy, transience of
service, opaque content to its current server location, indirection
(The author also should be able to arrange that he himself can't
remove the content, by intentionally discarding whatever keys give him
the technical means to remove or change the content).
Yeah.  I think my feature set at the top should be the default/base
set of properties exhibited by the www2 (next gen web).  Any voluntary
restrictions on these should be entered into by policy.  Say content X
is illegal in jurisdiction Y, then Y should publish a blacklist
identifying content X and the legal system in jurisdiction Y should if
it chooses make it illegal to not consult the blacklist.  I mean
illegality is not even consistent, there are things which are legally
required in Y that are illegal in Z.  There is and can be no globally
acceptable policy, so we must robustly technologically prevent global
p2p-hackers mailing list
p2p-hackers at zgp.org
Here is a web page listing P2P Conferences:
Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-01-07 15:34:32
@_author: Adam Back 
@_subject: Hamachi "mediated" peer-to-peer sounds interesting (fwd 
Well if they really relayed traffic between peers on their back end
server their pipe would be saturated.  (Think kazaa or bit-torrent
over hamachi).
I hope they actually use the server just for mediation, and send the
traffic direct between peers.
Unfortunately the documentation is rather light so it's difficult to
tell what it does in this regard.
I've cc'd Alex Pankratov who is the author (I presume).
However maybe this beta version is not complete in that regard.  Some
other things such as the server mediated key exchange are obviously
not shipable grade (server knows all symmetric keys!)

@_date: 2005-07-15 10:59:25
@_author: Adam Back 
@_subject: Reverse Palladium? 
Anonymous writes in favor of palladium arguing that it is optional, so
all is ok.
See I think it is entirely possible to get the benefits of secure
compartments, which are secured from hostile software, without locking
out the owner of the machine.
All that is needed is to turn over control of the machine to the
owner.  Give the owner of the machine keys for ring-1; he can have a
secured login to ring-1 where he gets to choose which ring-1 processes
he can attach a debugger to, binary patch etc and which loadable
things which are hashed for verification by remote attestation to lie
about the hash of.
In this way the owner can be sure he won't get valuable data hacked by
viruses, trojans etc; (well as secure as he can be under the palladium
model) but the evil remote non-optional control of your own hardware
is removed from the picture.
So the optionality anonymous is arguing about is your "option" to be
refused service outright, or cede ring-1 level (compartmented) access
to your machine.  ie to allow 3rd party software to run that you are
NOT able to debug, inspect, look at source or executable for, patch or
fix to your liking.
And how far this kind of optionality extends depends on the
architecture choices of microsoft eg al in how deeply they embed this
into the OS, their applications and programming frameworks, and how
much other companies choose to use this stuff.
So microsoft has already talked about software rental, etc etc; and
has a history of increasingly intrusive and annoying license
enforcement, so if you ask me you can bet your money that this will be
used throughout the whole system to the point where you can have the
option to switch off your machine, or give up control.  The OS will
become a container for rented, DRMed, uninspectable, unsniffable,
unpatchable corporate-warez.
It would if it was.  But its not.  If its voluntary, give me the keys
to my own computer.  If you're not going to do that then shutup about
"voluntary".  You have about as much control over your own machine
under palladium as you do over a user account on a remote system you
do not have root on.  Except it's your machine! and you still don't
get to control it.
You need to go read Richard Stallman's essay on the right to read.
You and others proposing this software are trying to fast-track us to
the scary but plausible future under Palladium that Richard
presciently paints.

@_date: 2005-06-01 18:09:29
@_author: Adam Back 
@_subject: /. [Intel Adds DRM to New Chips] 
[could you use CPU emulator to bypass these motherboard and CPU based
DRM systems].
Answer: no.  They have but private keys inside the DRM hardware, and
signed the corresponding public key with a CA that they control.  That
plus some hashing/bootstrapping etc of the startup and some other code
allows them to ensure that an emulated version of the same software
could not provide a valid signature + cert that a DRM content provider
would accept.
They also have models where the video card and/or monitor are in the
trust model -- and there are secured high bandwidth channels between
DRM provider and CPU, and CPU and graphics card/decoder.
There is also a model for software called "Trusted Agents" that
actually run on your CPU but are in a ring -1 (below ring 0) that you
can not debug.
Another possibility is read the stuff out of RAM or video RAM.
Midterm they can fix that also with on the fly RAM encrypt/decrypt.
But I still say it's futile and stupid, because people will hack the
digital display, tap into the graphics card, hack video card drivers
etc and re-encode.  (Rip-once copy anywhere).  Worst case people will
A2D from the display "telesync" style.

@_date: 2005-06-02 14:20:33
@_author: Adam Back 
@_subject: /. [Intel Adds DRM to New Chips] 
No the private key would be generated on the chip at manufacture, and
a signed certificate of it inserted by the manufacturer.
It is feasible in the following way to make a soft drm.  Step1. Get yourself a software controlled key signed by the hw
manufacturers.  Either:
1a. extract an already signed one out of the DRM hardware on your
machine by hardware hacking.
1b. find an insider at the manufacturing plant to sign a key actually
in the control of software;
1c. obtains the CA key used to do the signing (probably rather hard,
obviously they'll be trying to keep that one secure in tamper
resistant hardware with no key export function).
Step2. share the key, or setup a service to falsely authenticate
pure software DRM as hardware DRM with your key.
Now to stop you sharing this key directly or making a p2p DRM auth
server, they have to revoke the key.
I believe their revocation model is a bit weak from what I read of the
specs a while back.  They have a kind of challenge:
- to avoid criticism of privacy invasion, they have to make the thing
anonymous (or at least pseudonymous with lots of pseudonyms)
- however you can't blacklist a truly anonymous challenge-response.
(There was a protocol from Ernie Brickell with this kind of problem.)
Depending on what the final details are therefore their revocation
model might be weak.
Yes.  It is outrageous for the RIAA/MPAA and hardware companies to be
trying to foist this stuff on people.
The other way is to find a buffer overflow or such in one of these
privileged signed drivers and then you can inject code/or bypass DRM
restrictions in pure software.  They might at some point giving you
signed AND encrypted drivers so you can't even reverse-engineer them,
but I would say you have a right to know and control what is running
on your machine.
Another even more powerful buffer overflow would be one in the
supervisor / mini-OS that is hosting the Trusted Agents in ring -1.

@_date: 2005-05-09 12:28:25
@_author: Adam Back 
@_subject: Zero knowledge( a>b ) 
There is a simple protocol for this described in Schneier's Applied
Crypto if you have one handy...
(If I recall the application he illustrates with is: it allows two
people to securely compare salary (which is larger) without either
party divulging their specific salary to each other or to a trusted

@_date: 2007-02-20 19:40:29
@_author: Adam Back 
@_subject: private credential/ecash thread on slashdot (Re: announce: 
Credentica (Stefan Brands ecash/credentials) U-prove library and
open source credlib library implementing the same are on slashdot:
Maybe some list readers would like to inject some crypto knowledge
into the discussion.  There is quite some underinformed speculation as critique on the
thread...  Its interesting to see people who probably understand SSL,
SMIME and stuff at least at a power user if not programmer level, try
to make logical leaps about what must be wrong or limited about
unlinkable credential schemes.  Shows the challenges faced in
deploying this stuff.  Cant deploy what people dont understand!

@_date: 2011-12-02 09:33:44
@_author: Adam Back 
@_subject: [cryptography] really sub-CAs for MitM deep packet inspectors? 
Well I was aware of RA things where you do your own RA and on the CA side
they limit you to issuing certs belonging to you, if I recall thawte was
selling those.  (They pre-vet your ownership of some domains foocorp.com,
foocorpinc.com etc, and then you can issue  *.foocorp.com .. however you dont get a CA private key, you get web integration with the so you dont have to do the email verification dance for each cert you
Handing over a signed sub-CA is a much higher level of risk, unless perhaps
it has a constraint on the domain names of certs it can sign baked into it,
which is possible.
To hand over a blank cheque sub-CA cert that could sign gmail.com is
somewhat dangerous.  But you notice that geotrust require it to be in a
hardware token, and some audits blah blah, AND more importantly that you
agree not to create certs for domains you dont own.
Start of the thread was that Greg and maybe others claim they've seen a cert
in the wild doing MitM on domains the definitionally do NOT own.
(The windows 2k box sounds scary for sure, and you better hope that's not a
general unrestricted sub-CA cert, but even then you could admit its similar
to the security practices of diginotar.) Secure as the weakest link, and the weakest link just keeps getting lower.  It would be interesting to know if there really are CAs lax enough to issue
a sub-CA cert to a windows box with no hardware container for the private
key.  (Not that it makes that much difference... hack the RA and the private
key doesnt matter so much).
The real question again is can we catch a boingo or corp lan or government
using a MitM sub-CA cert, and then we'll know which CA is complicit in
issuing it, and delist them.
cryptography mailing list
cryptography at randombit.net

@_date: 2011-12-02 12:18:24
@_author: Adam Back 
@_subject: [cryptography] if MitM via sub-CA is going on, 
Now we're getting somewhere.  If this is going on even the policy
enforcement aspect of CAs is broken...  CAs are subverting their own
certification practice statement.  The actions taken by the user of the
sub-CA cert are probably illegal also in the US & europe where there are
expectations of privacy in work places (and obviously public places).
More below:
a public MitM proxy?  Or a corporate LAN.
That intermediate CA needs publishing, and the CA that issued it.  SSL
Observatory ought to take an interest in finding catalogging and publishing
all of these both public, corporate and government/law-enforcement.  It
breaks a clear expectation of security and privacy the user, even very
sophisitcated user, has about privacy of their communications.
There is an important difference between:
1. private label sub-CA (where the holder has signed an agreement not to
issue certs for domains they do not own - I know its policy only, there is
no crypto enforced mechanism, but thats the same bar as the main CAs
2. corporate LAN SSL MitM (at least the corporation has probably a contract
with all users of the LAN waiving their privacy).  Probably even then its
illegal re expectation of privacy in workplace in most contexts in US &
3. public provider SSL MitM - if your ISP, wifi hotspot, 3g data prov, is
doing this to you, paid or free, thats illegal IMO.  Heads should roll up
the CA tree.
4. government SSL MitM - we need to know which CAs have issued MitM sub-CAs
for places like Iran, Syria, pre-revolution Egypt etc.  If the CA isnt owned
by their local government or local company that they leant on, heads need to
roll.  Similar if US and European governments and Law Enforcement have been
up to this, we need to know.
Obviously the most interesting ones are 3 & 4.  But Peter says he has
evidence 2 (LAN mitm) is going on in the name of deep packet inspection I
guess in corporate LANs and that itself employees should be aware of that.
cryptography mailing list
cryptography at randombit.net

@_date: 2011-12-02 14:00:03
@_author: Adam Back 
@_subject: [cryptography] if MitM via sub-CA is going on, 
Of course, I would do the same if so asked.  But there are lots of people on
the list who have not obtained information indirectly, with confidentiality
assurances offered, and for them remailers exist.
personally I'd like to know who is doing this and at what scale.
I do not think its what you'd expect.  A CA should issue certificates only
to the holders of certificates.  It should NOT issue sub-CA certifactes to
third parties who will then issue certs to domains they dont own.  Not even
on the fly inside a "packet inspection" box.
If someone wants to inspect packets on a corporate lan they can issue their
own self-signed cert, and install that in their users browsers in their OS
install image.
Then if I go on their LAN with my own equipment, I'll get a warning.
I think its unacceptable to have CAs issuing such certs.
No.  Also IANAL but there were several cases where employees did have an
expectation of privacy upheld even in the US.  Certainly you cant do that in
the EU legally either.
I dont view this as business as usual.  If I am on a public hotspot, 3g or
my own DSL/cable I do ABSOLUTELY NOT expect the ISP to be getting inside my
SSL connection to my bank, my gmail account etc.  Whether I paid or not.  For any reason at all (not to do advert analysis, not to do anti-virus, not
to re-write pages etc).  I use airport/hotel wifi a lot and I've never seen
it and I am suspicious enough to use cert patrol etc.
Well yes I know the hardware exists.  I even helped design and implement a
software-only MitM at ZKS.  Before you get alarmed the cert it created was
known only to the user, generated on the machine, and its purpose was to
protect the user via a cookie manager protecting their privacy.
If you read the sonic wall stuff its fairly clear the model that they talk
about at least is that you do not have a sub-CA key.  You generate a
self-signed CA key, and install it in your corporate LAN browsers trusted CA
dbs.  Clearly it would also work with such a cert.
Similarly their doco about server SSL shows they dont expect you to have a
proper sub-CA key.  (scenario where the web server for public access is
behind the sonicwall) you are expected to import your server SSL certs
mapped per IP address into the box.  (Otherwise the public would see
self-signed MitM notices when they browse the site.  Or the sub-CA cert.
Well people were complaining about the "WAP gap" a long time ago.  I thought
this was more about the phones at the time being not CPU powerful enough to
terminate SSL.  The idea that a gap exists somewhere where your traffic is
decrypted was viewed as a significant security limitation people were
pleased to see go a way with phones fast enough to terminate their own SSL.
That is bad.  Are you saying there is anyone doing SSL mitm for stream
compression reasons?  Who?
cryptography mailing list
cryptography at randombit.net

@_date: 2011-12-22 09:40:37
@_author: Adam Back 
@_subject: [cryptography] How are expired code-signing certs revoked? 
Stefan Brands credentials [1] have an anti-lending feature where you have to
know all of the private components in order to make a signature with it.
My proposal related to what you said was to put a high value ecash coin as
one of the private components.  Now they have a direct financial incentive -
if they get hacked and their private keys stolen they lose $1m untraceably.
Now thats quite reassuring - and encapsulates a smart contract where they
get an automatic fine, or good behavior bond.  I think you could put a
bitcoin in there instead of a high value Brands based ecash coin.  Then you
could even tell that it wasnt collected by looking in the spend list.
[1]  a library implementing Brands
credentials - it has pointers to the uprove spec, Brands thesis in pdf form
cryptography mailing list
cryptography at randombit.net

@_date: 2011-06-07 16:14:51
@_author: Adam Back 
@_subject: [cryptography] RSA admits securID tokens have been compromised 
"RSA has finally admitted publicly that the March breach into its systems
has resulted in the compromise of their SecurID two-factor authentication
I guess everyone was suspecting as much reading between the lines of what
was said so far, but there it is.
cryptography mailing list
cryptography at randombit.net

@_date: 2011-06-12 14:15:50
@_author: Adam Back 
@_subject: [cryptography] attacks against bitcoin 
I was thinking a DoS might be a problem.  If you could prevent the p2p
network broadcasting or receiving broadcasts, maybe you could be the only
person able to proceed with minting.  If you could keep that up for a while
you could reduce the difficulty and create bitcoins with lower cost.  A full
enough DoS maybe difficult to do as the network is p2p.
Also maybe if you could temporarily come in with significantly more compute
power than the rest of the network - eg rent ec2's entire gpcpu farm for a
little while, or use of a huge bot farm, you could undo transactions.  If
those transactions were you selling bitcoins, you could then sell them
again.  eg buy, then sell $100k coins (minus the spread/fluctuation), rent
$50k worth of compute for a while; sell the $100k of coins again ...  profit
cryptography mailing list
cryptography at randombit.net

@_date: 2011-06-13 16:54:04
@_author: Adam Back 
@_subject: [cryptography] Digital cash in the news... 
Bitcoin does not have to end with the pyramid scheme outcome - where it
stalls and all those still holding any lose - so long as there remain people
willing to exchange goods for bitcoin after the dust has settled.
Anyway my point is even if the deployment phase is a wild ride, with some
winners and some late losers who bought in above the final stable value, so
long as a stable value results at the end, I dont see that as a big problem.
Its not like we havent had bubbles and instability in various phases of any
other forms of money or assets.
If you take out the speculation, currently with people minting coins until
they get to 21 million coins that would be inflation (limited inflation due
to the mining cost); but also that more people are joining is deflationary
(less coins per person).  Then there is supply and demand - supply from
minting (so long as the sell price is above minting cost), supply from
people cashing out, and demand from people buying in.  Cashing out and
buying in maybe for trading or speculation.
Once the 21 million coins are created bitcoin would remain deflationary
during the next phase as until the user base grows to saturation.  Once
bitcoin grows to saturation, the remaining deflation would be limited by the
underlying population and economic growth.  That might be workable rate of
cryptography mailing list
cryptography at randombit.net

@_date: 2011-06-15 10:22:21
@_author: Adam Back 
@_subject: [cryptography] crypto & security/privacy balance (Re: Digital cash 
Well said StealthMonger, I suspect Nico is in the minority on this list with
that type of view.
I read Nico's later reply also.  Short of banning crypto privacy and
security rights stand a better chance of being balanced by more deployment
of crypto.  (In terms of warrantless wiretaps etc which seem to just keeping
going and getting worse in many supposedly civilized western democracies.)
There are still plenty of things government security people can usefully do
towards security - spend the money on inflitration of groups who are real
security threats.
I would say privacy tech & crypto is essential to maintaining a good point
on the security/privacy balance in a world where security policy
encroachment has gone into overdrive.  To retain electronic liberty, crypto
is the answer.  I dont think crypto can be realistically banned in western
countries at this stage, the electronic part of security encroachment is
mostly opportunistic hoovering up things that are not protected.
There are multiple privacy properties - confidentiality of communication
contents, privacy of association (cryptographic freedom of association) like
pseudonymous email (protection against traffic analysis), cryptographic
enforced member only discussion groups/chats.
Then there are countries where crypto is officially or effectively already
banned - there being caught with privacy tech on your laptop, cell phone etc
would be dangerous.
Crypto and other privacy techniques can counteract somewhat - with
steganography, that though obviously its a tough threat model.  See Its also a kind of interesting conflict that western governments think of
themselves, or try to portray themselves as moral forces of good and yet
there are a few cases where this technology the US is helping fund really
needs to be used in western democracies, including the US.  The UK governments right to force key disclosure is an abomination, no
civilized country should be going in that direction.
cryptography mailing list
cryptography at randombit.net

@_date: 2011-11-27 13:38:10
@_author: Adam Back 
@_subject: [cryptography] fyi: Sovereign Keys: an EFF proposal for more 
Yes, its the way I would've done it.  Actually coincidentally I already did
propose doing it exactly that way in around 1999:
(That was about censor resistant DNS->ip mapping with public auditability.  And that might still be something to think about with the current us
governments habit of seizing domains without legal due process.  Anyway the
same technique should work for public keys ... map a name to its IP address
and the hash of its public key, or its public key.  My view is that these mappings are global and even with legal due process,
you dont want a local legal authority to be able to override a global view. Global views are conflicting.  Even somethings that are forbidden in one
area as may not be forbidden in another, or may even be mandatory!  Local
legal conclusions should be advisory for the local area.
cryptography mailing list
cryptography at randombit.net

@_date: 2012-02-12 10:04:13
@_author: Adam Back 
@_subject: [cryptography] trustwave admits issuing corporate mitm certs 
So it happened, per recent discussion on this list, it seems that at least
one CA *has* been issuing sub-CA certs for corporate use in mitm boxes.
mozilla is threatening to remove the CA from their browser.  Trustwave says
they have/will revoke all these sub-CAs and will not issue any more.
They also claim in their defense that other CAs are doing this.
cryptography mailing list
cryptography at randombit.net

@_date: 2012-05-12 01:22:44
@_author: Adam Back 
@_subject: [cryptography] Bitcoin-mining Botnets observed in the wild? 
Strikes me 12TH/sec is not actually very much computation?   also gives network hashrate at 12.4 TH/sec.
But a single normally clocked (925Mhz) AMD 7970 based graphics card which
has 2048 cores is claimed to provide 555MH/sec.   (ignore the
aggresively overclocked 7970s...  the one that counts has 925 in the clock
speed column!)
But that means the entire bitcoin network compute is only equivalent to 23
AMD 7970 GPUs?  That seems wrong by orders of magnitude, there are youtube
videos of private individuals with mining rigs in that range in their
(Somewhat amusing scale of raw compute being thrown via SHA256 hashcash at
the bitcoin core at this thing- still its arguably not wasted CPU because as
human endeavours go other forms of currencies have their costs too.  So a
p2p currency if it played out might save a lot.  Personally I lost a bundle
on major currency devaluation GBP, EUR, USD look at them against the CHF to
see how much you'all lost due to whatever human / political inefficiencies
you attribute currency devaluation on that scale to.  A stable mechanism for
value storage would be a rather useful instrument.)
cryptography mailing list
cryptography at randombit.net

@_date: 2012-05-12 01:39:48
@_author: Adam Back 
@_subject: [cryptography] Bitcoin-mining Botnets observed in the wild? 
An never mind... the 555MH and 12.4TH its 23,000 AMD 7970 GPU equivalents,
that back in the land of plausible.
Or 158 petflops if you accept bitcoinwatch.com's sha256 to "flop" conversion
ratio.  A human brain by comparison is generally considered to be about
10-100x less than the current bitcoin network (10^15 - 10^16 ops/sec).
So maybe the bitcoin network has just enough power to simulate a single
human brain.  Except a human brain consumes 25watts and even with 23,000
7970s the bitcoin network must be using more than 5 MW.
Moore's law still has a lot of work to do to catchup with biologial
computers in efficiency and horsepower.
cryptography mailing list
cryptography at randombit.net

@_date: 2013-04-17 23:49:00
@_author: Adam Back 
@_subject: [cryptography] summary of zerocoin (Re: an untraceability extension 
It appears to use cut-and-choose technique to create a non-interactive ZKP
on a one-way accumulator (from Camenisch & Lysanka).  That results in
relatively big ZKPs which impact bitcoin scalability, it doesnt say how big
they actually are but for good security margin I'm guessing something like
128 individual proofs, which can get kind of heavy.  They say its like to
exceed the bitcoins 10kb per tx limit.
(People may recall cut-and-choose as Chaum's original blind RSA signature
based proposal to support offline spendable ecash with identity exposure as
the penalty - the owners identity would be embedded in the coin via cut and
choose, and then if they double spend revealed as ther is some recipient
choice within the spend that would reveal two shares).  Stefan Brands ecash
by contrast with the more flexible discrete log representation problem
provided a direct (non-cut-and-choose) offline double spend deterrence
(Cut and choose is a simple idea that if you have to commit first and reveal
one of two options you have a 1 in 2 (0.5 or 2^-1) chances of cheating (and
putting someone elses identity in the coin eg for revealing during an
offline double spen), but if you do it 128 times you have a 2^-128 chance of
cheating.  The same approach plus demonstrably fair way to generate
witnesses is a general method for constructing NIZKPs from ZKPs.)
Also in principle you have to construct a NIZKP all the way back to the
first zerocoin, and while the accumulator is constructed incrementally and
stored in each block bitcoin requires public auditability so the verifier
has to go check the corresponding "spent" list that grows.  (The ZKP is that
the zerocoin is a member of the set of zerocoins previously exchanged for
bitcoin, and is not a member of the spent list.  The expensive part is the
former, the latter is just a bit list of spent zerocoin serial numbers.  Thy
do suggest just trusting the last block accumulator as a starting point, but
I think you can only do that if the zerocoin was issued after that, and the
effectiveness of that optimization is in some conflict with anonymity set
requiremnts to hold the zerocoin for a while to at least have a decent
number of coins your mixin with.
The accumulators allow faster NIZP of set membership than proving with a bit
list of OR operations, but the cut-and-choose itself isnt a direct NiZKP it
involves 128 rounds or whatever the security parameter is.
The accumulator uses a trusted party to generate an RSA key and discard the
private keys.  (Though happily there are RSA UFOs (way to generate an RSA
key without ever knowing the private key in a trustworthy way)).
So thats all pretty cool, and I think an efficiency improvement over
previous NIZKP of set membership (or non-set-membership) but a bit heavy. btw about payment privacy (or lack thereof in bitcoin) an amusing link
 to get an idea of the bitcoin transaction
volume and transaction sizes - highlighting visually the extremely open
nature of the realtime transaction history.  I saw a massive $640k
transaction float past when I looked.  Soothing sounds it makes too.  (Bitcoin is private only in the self chosen psuedonym sense, and is
otherwise an open book by design).
cryptography mailing list
cryptography at randombit.net

@_date: 2013-04-18 10:19:43
@_author: Adam Back 
@_subject: [cryptography] bitcoin stats (Re: OT: Skype-Based Malware Forces 
vs the other things malware does it also seems like a much more benign
"payload" - uses a bit more electricity!  I imagine they throttle it down
dynamically when the user actually does things to hide the computer slow
down.  (Though typically you wont notice with GPU mining unless you are
playing video games).  But you may notice the increased GPU fan noise and
heat on mid/high range cards.
Seems bitcoin mining is using 40MW of power (estimate) thats 0.025% of the
amount used by US households (30,000 US households - obviously bitcoin
mining is global - its just a comparative stat).
Thats a lot of 55.1 bit hashcash mining those miners are doing!  Bits is
more human readable by my thinking from hashcash difficulty measurement and
visually easy to confirm from the hash output.
you can see the SHA256 output has 13 hex 0 nibbles = 52 bits plus the next digit is a one 0001 in binary
so thats three more - so its a 55 bit.
And the difficulty parameter is measured in the number of 2^32 hashes
(MAX_UINT == ~4 billion) so you can get back to it from 55.1-32=23.1 via
2^23.1 (in bc e(l(2)*23.1) ~ 9 million.  Much better :)  And converting GPU
card mining stats into bits is also easy 400Mh (amd 7870) = 28.6.  So that
means I have 55.1-28.6 = 26.5 bits short (I need 2^26.5 bits to match the
network hash rate).  And also you can recreate the network hashrate as
2^55.1/600 (60 seconds x 10 mins per block) or subtract 9.2 as log2(600) =
9.22 so 55.1-9.2=45.9 = 2^45.9 so subtract 40 for terahash/sec = 2^5.9 =
59.7.  (Except some web stats are misreporting network hash in 1000^4 rather
than 1024^4).
Anyway I think humans work better with massive numbers in the log scale.
cryptography mailing list
cryptography at randombit.net

@_date: 2013-08-07 01:09:07
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
I dont get it.  Paillier is additively homomorphic only.  (And obviously by
implication multiplyable by non-encrypted constants.)
RSA is multiplicatively homomorphic.  And Elgamal additive.
Why is paillier proposed as "might scale homomorphic" the interesting
property is dual homomorphic crypto which Gentry and variants provide (but
at impractical computational and large space overhead huge).  Dual or fully
homomorphic is the interesting property because then you can do arbitrary
computations (using multiplication as single-bit AND and addition as
single-bit OR and building a CPU from gates - still expensive even if the
base algorithm was as efficient as Paillier/RSA/Elgamal but interesting).
Also why would they send the "encrypted numbers" to two peers and have them
do the encrypted computation?  The whole point is its zero-trust secure from
the point of view of the client - client encrypts, server does computations
on encrypted values, sends encrypted result back to client, client decrypts
- and you dont need to trust the server.  No need for threshold crypto,
having multiple peers do some kind of multi-party computation etc.

@_date: 2013-08-07 01:16:37
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
Cypherpunks and privacy tech had enough on their plate post 9-11 without
inexplicably using an Al-Qaeda related domain name presumably chosen by
someone's amusement at being controversial.  Its not related to the list,
and it just invites spurious trouble.  Why not the ownder of the domain use
it as his personal address.  Heck he can use the user name osama@ the domain
if he wants.  I have to say I see no upside whatsoever to using that domain
name for a mailing list on any topic.
You only have to look at various court cases to see how everything gets
heavily misinterpreted and nothing spun into something to pause and see why
using such a domain name is a "bad idea" tm.
I appreciate the "fearless crypto coder" mentality, but focus on the crypto,
not inviting stupid fights with authoritarian systems over non-topics eh.

@_date: 2013-08-07 11:05:42
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
No recall that the simplified paillier is c=g^m*r^n mod n^2 so
multiplication gives addition:
g^a*r1^n * g^b*r2^n = g^{a+b}*(r1*r2)^n
ie multiplication of ciphertexts gives you homomorphic addition of
but g^a*r1^n ^ (g^b*r2^n) != g^{a*b}*r3^n
(the core part is g^a ^ (g^b) = g^{a*g^b} != g^{a*b}).
what does work is as I said raising to the power of a constant eg k:
(g^m*r^n)^k = g^{k*m}*(r^k)^n so you can still decrypt and the operation is multiply by constant k).

@_date: 2013-08-08 02:20:32
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
OK let me put it this way, given each person only has so many hours in the
day, or so much energy and resources for politically-fighting or
write-code-fighting things which would you rather fight: defense of spurious
attention arising from a stupid domain name, or I dunno operating a
remailer, a tor exit node, a hidden tor server.
Apparently operating a hidden tor server as a service is pretty high risk as
the guy in Ireland is finding.  You can see in that they are trying to pin
the content on him, as if he authored it, whereas I am presuming he is no
more responsible for the content than a hosting company or youtube.  If he
was prominently using al-qaeda.net you can be sure they'd have spun that
into the story.
There is some history also - recall Jim Bell, he got in some fight over
taxes or something stupid, that took him out of the picture for a while.  I
wasnt really sold on his assassination politics idea anyway (gotta be a way
to vote someone out of office without assasinating them!), but at least it
was a political discussion which he thought had some merit vs a losers game
of tax protestation ending in jail time, anyone can see thats never going to
work out.
I wouldnt be so sure that using stupid domain names is entirely safe in the
US, europe etc.  IMO the US is past its peak in terms of a place of freedom
and others have overtaken it.  It doesnt seem likely the US will recover its
ranking, seems to be falling year on year.  Probably China itself will
overtake US economically, politically and for freedoms within 50-100 years. Not sure how you recover freedoms from a panopticon state with a one dollar
one vote and a 100 billion dollar+ military-spy-industrial complex and a
significantly biased politicial- judicial system.  If you watch RT which
airs a lot of the snowden thing, the stuff the USG is saying about snowden
is just ludicrous.  Pressuring european countries to deny overflight to a
presidents plane is an alarming breach of international law and shows how
far the US rogue state influence goes in seemingly other countries willing
to go along with its actions.
Also why would you even want to do it?  You care about crypto deployment, so
I dont see the logic in picking the most stupid, unrelated and controversial
domain name you can think of hitting as many peoples distaste as you can and
use that?  wtf back at you :)  cypherpunks  what next.
I guess we should go write some code!

@_date: 2013-08-10 02:56:15
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
Reading what Jon Callas wrote he said silent circle interoperated with
unencrypted SMTP email (unencrypted other than SSL over the transport), and
they used some bump in the wire PGP thingy that encrypts incoming email with
the silent circle users public key, and presumably sends out cleartext
possibly SSL SMTP where available, for non silent-circle recipients. Clearly therefore anyone tampering with the SSL (and often those mail
transport systems are not that smart about SSL as there is no security UI)
or just getting the NSA camel's nose inside the silent circle SSL
termination point prior to encrytion.
As they didnt think that would end well they decided to close it down. Alternatively they might have considered disabling the mail-in and mail-out
Its less clear what lavabit were talking about.  Perhaps something similar
in terms of an SMTP interoperability encryption gap, or alternatively about
being pressured to modify code (which people seem to assume, but I didnt see
explicitly stated).
There were some hushmail rumors about code modification some years back -
does anyone know what actually at hushmail?

@_date: 2013-08-10 03:19:59
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
(Thanks for the link.)  It says hushmail had a simplified web-only version (no
java applet) and that the disclosure of client emails did not involve
pressured code changes (at least code shipped to clients), rather that as a
natural consequence of the way passwords would be processed on the server
side and decryption happened on the server side so hushmail had the
passwords, private keys, and decrypted plaintexts at leas in memory to hand
over on request.

@_date: 2013-08-11 12:13:28
@_author: Adam Back 
@_subject: NSLs, gag-orders, code-changes, coerced backdoors - any tech 
About physical access - there is one non-physical solution to this - hide
the location of the server behind tor, proxies etc.  Seems to work
remarkably well for pirate bay.  I cant imagine its that big a secret as to
where the packets are routed from the current proxy to the current physical
host, but seemingly NSA type resources have not been brought to bear against
it.  Step one for the attacker is to find it.  Maybe physical tamper
detection can wipe the RAM, cold reboot as the cage unlocked, or box is
opened, and immediately switch to the back up server in a different tor
hidden physical location.
One thing that occurs to me is that aside from the laundering of NSA tip
offs to FBI etc with faked plausible trails, that have been reported on
lately; there was an aspect that they would be hesitant to reveal what they
could tap, correlate etc, or under what circumstances they would abuse
national security (military) resources for various levels of criminal
activity (major, organized to minor, petty, or political misuse).  But the very fact that Snowden did the world a favour in disclosing the
illegal activities of the NSA and global partners, now people know what they
are doing or can better imagine, and not discount as paranoia, consequently
maybe once the dust has settled they will feel freer to feed ever more petty
or political or corporate espionage related information.  After all they'd
no longer be risking knowledge of information capability, or political
willingness.  Everyone pretty much figures they're in it up to their elbows
with corporate espionage (boeing vs airbus wiretaps), minor crimes with
fabricated evidence trails (maybe they wont bother fabricating them even in
future) and perhaps the political stuff though that is really evil and
anti-democractic (eg tea-party member IRS audits, blackmail etc).
It seems to me companies need to delegate code review and signing to a civil
society charitable organization with smart use of jurisdictions.  eg Germany
(chaos computer club code signing silent circle code?), Switzerland,
Iceland, or psuedonymous but high reputation individuals or groups.  Or
privacy groups which may have a more clear disinterest and immunity from
financial blackmail (like USG will cancel contracts if ISP, internet
service, or softwre company doesnt fold to NSL or other extra-legal
threats).  Or maybe EFF, privacy international etc.  Via their lawyers they
could retain a highly competent and pseudonymous team of technical reviews
and code signing that companies that care to demonstrate their alignment to
providing end to end secure services to their users would if it became
popular given an explanation of why they were not protected by independent
review based code signatures.

@_date: 2013-08-21 02:46:27
@_author: Adam Back 
@_subject: Google to encrypt cloud storage 
Well I think its fair to denigrate it as obfuscation not encryption if the
key lives on the same machine as the ciphertext.  At best it makes it less
risky to dispose of dodgy disks - now and then such things turn up on ebay
with client data.  At least if you encrypt it properly, and do NOT put the
key on the disk, then you can safely toss them in a dumpster, not physically
destroy them etc.

@_date: 2013-08-21 02:51:02
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
I was thinking something like that about the silent circle shutdown.  It
seems to me their problem case was the mail in (they would be encrypting
that to the user PGP key or equivalent, after sender optional use of SSL to
deliver it to them).  So would not a more sensible change be to disable
mail in?  So then only silent circle users could encrypt messages to each
other.  Even that would add pressure to other users to also get a silent
circle account and so be a business advantage.
Puzzlingly spun "to protect our users privacy we removed their encryption
feature" - so they'll probably send it plaintext instead, great.

@_date: 2013-08-21 07:52:25
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
Yes but my point was they didnt have to throw out the baby with the
bathwater; silent circles email I think was basically two products combined:
1. end2end secure, store-and-forward encryption between silent circle users;
2. server-side encryption of opportunistically SSL encrypted (potentially
unencrypted) incoming emails + presumably unencrypted outgoing emails.
Why not keep 1?  They obviously have the technology for it because they have
retained encrypted SMS-like functionality which is the same key management
and information flow.
Not forgetting there is a 3rd "product" which is the defacto which is normal
3. opportunistically encrypted (SSL) email
(as well as SMIME (dont trust due to CA malfeasance) or self-managed PGP/GPG
which for some reason people find difficult).
and users who lose 1 & 2 due to the no-notice product end-of-life will
probably just switch to 3 as an alternative to stopping communicating.  Even catching a flight with a USB drive apparently is risky via UK re the
curiously named David Miranda (Miranda rights eh) seems they demanded
decryption keys.  Seems like people who are couriering data ought to encrypt
it with the recipients public key before travel.

@_date: 2013-08-23 02:25:43
@_author: Adam Back 
@_subject: [Doctrinezero] HTTPS 
Are you sure about that?  (*.com and *. being valid).  I thought the MITM
boxes were loaded with a sub-CA cert - a cert with a bit set authorizing it
to generate certs for sites, some of the smaller CAs are not directly in the
trusted browser databases, and have bought sub-CA certs from CAs that are.
Then what actually happens in the MITM box is to load a fake cert for a
domain (issued by its sub-CA cert), or to generate fake certs on the fly for
any targetted domains (or all domains) again issued by its sub-CA cert.  So
I thought the CA that got warned by mozilla had issued a sub-CA cert for
MITM purposes.
(I really dont think a browser vendor would accept *.com nor especially *.
as a valid site cert wildcard.  It does get fiddly because you also want
*.co.uk etc to be invalid but they have some built in tables of such things
to differentiate a TLD from a domain).

@_date: 2013-08-31 10:05:07
@_author: Adam Back 
@_subject: why not disable external mail, keep intenal mail (Re: Who bought off 
More precisely its the exposed meta-data in the SMTP.  But why would you use
meta-data rich transport for silent circle internal-mail?  (Internal-mail I
mean silent circle user to silent circle user vs external mail being smtp
mail to silent circle user or silent circle user to smtp mail user).
I said it before, but again: why not cancel external mail, and leave the
internal mail working - silent circle obviously have the tech for that
because they have SMS equivalent in-mail.  Good for you: users who want to
continue to communicate will encourage the people they are communicating
with to also pay for subscriptions.  Maybe you could allow people to give
each other gifts of 1month membership, which you hope they extend
themselves; or some referal system with a bonus free month to the existing
user etc.
Now there might be some software legacy, but that seems straight forward
enough.  The crypto gap is purely the in and out mail.  (Other than forced
software changes, but others have discussed how to combat that issue, and
some claim legal advice is that its harder for the mil-int community to
legally force companies to change their software.  (Hushmail saga not

@_date: 2013-08-07 01:09:07
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
I dont get it.  Paillier is additively homomorphic only.  (And obviously by
implication multiplyable by non-encrypted constants.)
RSA is multiplicatively homomorphic.  And Elgamal additive.
Why is paillier proposed as "might scale homomorphic" the interesting
property is dual homomorphic crypto which Gentry and variants provide (but
at impractical computational and large space overhead huge).  Dual or fully
homomorphic is the interesting property because then you can do arbitrary
computations (using multiplication as single-bit AND and addition as
single-bit OR and building a CPU from gates - still expensive even if the
base algorithm was as efficient as Paillier/RSA/Elgamal but interesting).
Also why would they send the "encrypted numbers" to two peers and have them
do the encrypted computation?  The whole point is its zero-trust secure from
the point of view of the client - client encrypts, server does computations
on encrypted values, sends encrypted result back to client, client decrypts
- and you dont need to trust the server.  No need for threshold crypto,
having multiple peers do some kind of multi-party computation etc.

@_date: 2013-08-07 01:16:37
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
Cypherpunks and privacy tech had enough on their plate post 9-11 without
inexplicably using an Al-Qaeda related domain name presumably chosen by
someone's amusement at being controversial.  Its not related to the list,
and it just invites spurious trouble.  Why not the ownder of the domain use
it as his personal address.  Heck he can use the user name osama@ the domain
if he wants.  I have to say I see no upside whatsoever to using that domain
name for a mailing list on any topic.
You only have to look at various court cases to see how everything gets
heavily misinterpreted and nothing spun into something to pause and see why
using such a domain name is a "bad idea" tm.
I appreciate the "fearless crypto coder" mentality, but focus on the crypto,
not inviting stupid fights with authoritarian systems over non-topics eh.

@_date: 2013-08-07 11:05:42
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
No recall that the simplified paillier is c=g^m*r^n mod n^2 so
multiplication gives addition:
g^a*r1^n * g^b*r2^n = g^{a+b}*(r1*r2)^n
ie multiplication of ciphertexts gives you homomorphic addition of
but g^a*r1^n ^ (g^b*r2^n) != g^{a*b}*r3^n
(the core part is g^a ^ (g^b) = g^{a*g^b} != g^{a*b}).
what does work is as I said raising to the power of a constant eg k:
(g^m*r^n)^k = g^{k*m}*(r^k)^n so you can still decrypt and the operation is multiply by constant k).

@_date: 2013-08-08 02:20:32
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
OK let me put it this way, given each person only has so many hours in the
day, or so much energy and resources for politically-fighting or
write-code-fighting things which would you rather fight: defense of spurious
attention arising from a stupid domain name, or I dunno operating a
remailer, a tor exit node, a hidden tor server.
Apparently operating a hidden tor server as a service is pretty high risk as
the guy in Ireland is finding.  You can see in that they are trying to pin
the content on him, as if he authored it, whereas I am presuming he is no
more responsible for the content than a hosting company or youtube.  If he
was prominently using al-qaeda.net you can be sure they'd have spun that
into the story.
There is some history also - recall Jim Bell, he got in some fight over
taxes or something stupid, that took him out of the picture for a while.  I
wasnt really sold on his assassination politics idea anyway (gotta be a way
to vote someone out of office without assasinating them!), but at least it
was a political discussion which he thought had some merit vs a losers game
of tax protestation ending in jail time, anyone can see thats never going to
work out.
I wouldnt be so sure that using stupid domain names is entirely safe in the
US, europe etc.  IMO the US is past its peak in terms of a place of freedom
and others have overtaken it.  It doesnt seem likely the US will recover its
ranking, seems to be falling year on year.  Probably China itself will
overtake US economically, politically and for freedoms within 50-100 years. Not sure how you recover freedoms from a panopticon state with a one dollar
one vote and a 100 billion dollar+ military-spy-industrial complex and a
significantly biased politicial- judicial system.  If you watch RT which
airs a lot of the snowden thing, the stuff the USG is saying about snowden
is just ludicrous.  Pressuring european countries to deny overflight to a
presidents plane is an alarming breach of international law and shows how
far the US rogue state influence goes in seemingly other countries willing
to go along with its actions.
Also why would you even want to do it?  You care about crypto deployment, so
I dont see the logic in picking the most stupid, unrelated and controversial
domain name you can think of hitting as many peoples distaste as you can and
use that?  wtf back at you :)  cypherpunks at child-porn-r-us.com?  what next.
I guess we should go write some code!

@_date: 2013-08-10 02:56:15
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
Reading what Jon Callas wrote he said silent circle interoperated with
unencrypted SMTP email (unencrypted other than SSL over the transport), and
they used some bump in the wire PGP thingy that encrypts incoming email with
the silent circle users public key, and presumably sends out cleartext
possibly SSL SMTP where available, for non silent-circle recipients. Clearly therefore anyone tampering with the SSL (and often those mail
transport systems are not that smart about SSL as there is no security UI)
or just getting the NSA camel's nose inside the silent circle SSL
termination point prior to encrytion.
As they didnt think that would end well they decided to close it down. Alternatively they might have considered disabling the mail-in and mail-out
Its less clear what lavabit were talking about.  Perhaps something similar
in terms of an SMTP interoperability encryption gap, or alternatively about
being pressured to modify code (which people seem to assume, but I didnt see
explicitly stated).
There were some hushmail rumors about code modification some years back -
does anyone know what actually at hushmail?

@_date: 2013-08-10 03:19:59
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
(Thanks for the link.)  It says hushmail had a simplified web-only version (no
java applet) and that the disclosure of client emails did not involve
pressured code changes (at least code shipped to clients), rather that as a
natural consequence of the way passwords would be processed on the server
side and decryption happened on the server side so hushmail had the
passwords, private keys, and decrypted plaintexts at leas in memory to hand
over on request.

@_date: 2013-08-11 12:13:28
@_author: Adam Back 
@_subject: NSLs, gag-orders, code-changes, coerced backdoors - any tech 
About physical access - there is one non-physical solution to this - hide
the location of the server behind tor, proxies etc.  Seems to work
remarkably well for pirate bay.  I cant imagine its that big a secret as to
where the packets are routed from the current proxy to the current physical
host, but seemingly NSA type resources have not been brought to bear against
it.  Step one for the attacker is to find it.  Maybe physical tamper
detection can wipe the RAM, cold reboot as the cage unlocked, or box is
opened, and immediately switch to the back up server in a different tor
hidden physical location.
One thing that occurs to me is that aside from the laundering of NSA tip
offs to FBI etc with faked plausible trails, that have been reported on
lately; there was an aspect that they would be hesitant to reveal what they
could tap, correlate etc, or under what circumstances they would abuse
national security (military) resources for various levels of criminal
activity (major, organized to minor, petty, or political misuse).  But the very fact that Snowden did the world a favour in disclosing the
illegal activities of the NSA and global partners, now people know what they
are doing or can better imagine, and not discount as paranoia, consequently
maybe once the dust has settled they will feel freer to feed ever more petty
or political or corporate espionage related information.  After all they'd
no longer be risking knowledge of information capability, or political
willingness.  Everyone pretty much figures they're in it up to their elbows
with corporate espionage (boeing vs airbus wiretaps), minor crimes with
fabricated evidence trails (maybe they wont bother fabricating them even in
future) and perhaps the political stuff though that is really evil and
anti-democractic (eg tea-party member IRS audits, blackmail etc).
It seems to me companies need to delegate code review and signing to a civil
society charitable organization with smart use of jurisdictions.  eg Germany
(chaos computer club code signing silent circle code?), Switzerland,
Iceland, or psuedonymous but high reputation individuals or groups.  Or
privacy groups which may have a more clear disinterest and immunity from
financial blackmail (like USG will cancel contracts if ISP, internet
service, or softwre company doesnt fold to NSL or other extra-legal
threats).  Or maybe EFF, privacy international etc.  Via their lawyers they
could retain a highly competent and pseudonymous team of technical reviews
and code signing that companies that care to demonstrate their alignment to
providing end to end secure services to their users would if it became
popular given an explanation of why they were not protected by independent
review based code signatures.

@_date: 2013-08-21 02:46:27
@_author: Adam Back 
@_subject: Google to encrypt cloud storage 
Well I think its fair to denigrate it as obfuscation not encryption if the
key lives on the same machine as the ciphertext.  At best it makes it less
risky to dispose of dodgy disks - now and then such things turn up on ebay
with client data.  At least if you encrypt it properly, and do NOT put the
key on the disk, then you can safely toss them in a dumpster, not physically
destroy them etc.

@_date: 2013-08-21 02:51:02
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
I was thinking something like that about the silent circle shutdown.  It
seems to me their problem case was the mail in (they would be encrypting
that to the user PGP key or equivalent, after sender optional use of SSL to
deliver it to them).  So would not a more sensible change be to disable
mail in?  So then only silent circle users could encrypt messages to each
other.  Even that would add pressure to other users to also get a silent
circle account and so be a business advantage.
Puzzlingly spun "to protect our users privacy we removed their encryption
feature" - so they'll probably send it plaintext instead, great.

@_date: 2013-08-21 07:52:25
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
Yes but my point was they didnt have to throw out the baby with the
bathwater; silent circles email I think was basically two products combined:
1. end2end secure, store-and-forward encryption between silent circle users;
2. server-side encryption of opportunistically SSL encrypted (potentially
unencrypted) incoming emails + presumably unencrypted outgoing emails.
Why not keep 1?  They obviously have the technology for it because they have
retained encrypted SMS-like functionality which is the same key management
and information flow.
Not forgetting there is a 3rd "product" which is the defacto which is normal
3. opportunistically encrypted (SSL) email
(as well as SMIME (dont trust due to CA malfeasance) or self-managed PGP/GPG
which for some reason people find difficult).
and users who lose 1 & 2 due to the no-notice product end-of-life will
probably just switch to 3 as an alternative to stopping communicating.  Even catching a flight with a USB drive apparently is risky via UK re the
curiously named David Miranda (Miranda rights eh) seems they demanded
decryption keys.  Seems like people who are couriering data ought to encrypt
it with the recipients public key before travel.

@_date: 2013-08-23 02:25:43
@_author: Adam Back 
@_subject: [Doctrinezero] HTTPS 
Are you sure about that?  (*.com and *. being valid).  I thought the MITM
boxes were loaded with a sub-CA cert - a cert with a bit set authorizing it
to generate certs for sites, some of the smaller CAs are not directly in the
trusted browser databases, and have bought sub-CA certs from CAs that are.
Then what actually happens in the MITM box is to load a fake cert for a
domain (issued by its sub-CA cert), or to generate fake certs on the fly for
any targetted domains (or all domains) again issued by its sub-CA cert.  So
I thought the CA that got warned by mozilla had issued a sub-CA cert for
MITM purposes.
(I really dont think a browser vendor would accept *.com nor especially *.
as a valid site cert wildcard.  It does get fiddly because you also want
*.co.uk etc to be invalid but they have some built in tables of such things
to differentiate a TLD from a domain).

@_date: 2013-08-31 10:05:07
@_author: Adam Back 
@_subject: why not disable external mail, keep intenal mail (Re: Who bought off 
More precisely its the exposed meta-data in the SMTP.  But why would you use
meta-data rich transport for silent circle internal-mail?  (Internal-mail I
mean silent circle user to silent circle user vs external mail being smtp
mail to silent circle user or silent circle user to smtp mail user).
I said it before, but again: why not cancel external mail, and leave the
internal mail working - silent circle obviously have the tech for that
because they have SMS equivalent in-mail.  Good for you: users who want to
continue to communicate will encourage the people they are communicating
with to also pay for subscriptions.  Maybe you could allow people to give
each other gifts of 1month membership, which you hope they extend
themselves; or some referal system with a bonus free month to the existing
user etc.
Now there might be some software legacy, but that seems straight forward
enough.  The crypto gap is purely the in and out mail.  (Other than forced
software changes, but others have discussed how to combat that issue, and
some claim legal advice is that its harder for the mil-int community to
legally force companies to change their software.  (Hushmail saga not

@_date: 2013-08-07 01:09:07
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
I dont get it.  Paillier is additively homomorphic only.  (And obviously by
implication multiplyable by non-encrypted constants.)
RSA is multiplicatively homomorphic.  And Elgamal additive.
Why is paillier proposed as "might scale homomorphic" the interesting
property is dual homomorphic crypto which Gentry and variants provide (but
at impractical computational and large space overhead huge).  Dual or fully
homomorphic is the interesting property because then you can do arbitrary
computations (using multiplication as single-bit AND and addition as
single-bit OR and building a CPU from gates - still expensive even if the
base algorithm was as efficient as Paillier/RSA/Elgamal but interesting).
Also why would they send the "encrypted numbers" to two peers and have them
do the encrypted computation?  The whole point is its zero-trust secure from
the point of view of the client - client encrypts, server does computations
on encrypted values, sends encrypted result back to client, client decrypts
- and you dont need to trust the server.  No need for threshold crypto,
having multiple peers do some kind of multi-party computation etc.

@_date: 2013-08-07 01:16:37
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
Cypherpunks and privacy tech had enough on their plate post 9-11 without
inexplicably using an Al-Qaeda related domain name presumably chosen by
someone's amusement at being controversial.  Its not related to the list,
and it just invites spurious trouble.  Why not the ownder of the domain use
it as his personal address.  Heck he can use the user name osama@ the domain
if he wants.  I have to say I see no upside whatsoever to using that domain
name for a mailing list on any topic.
You only have to look at various court cases to see how everything gets
heavily misinterpreted and nothing spun into something to pause and see why
using such a domain name is a "bad idea" tm.
I appreciate the "fearless crypto coder" mentality, but focus on the crypto,
not inviting stupid fights with authoritarian systems over non-topics eh.

@_date: 2013-08-07 11:05:42
@_author: Adam Back 
@_subject: [cryptography] Paillier Crypto for homomorphic computation 
No recall that the simplified paillier is c=g^m*r^n mod n^2 so
multiplication gives addition:
g^a*r1^n * g^b*r2^n = g^{a+b}*(r1*r2)^n
ie multiplication of ciphertexts gives you homomorphic addition of
but g^a*r1^n ^ (g^b*r2^n) != g^{a*b}*r3^n
(the core part is g^a ^ (g^b) = g^{a*g^b} != g^{a*b}).
what does work is as I said raising to the power of a constant eg k:
(g^m*r^n)^k = g^{k*m}*(r^k)^n so you can still decrypt and the operation is multiply by constant k).

@_date: 2013-08-08 02:20:32
@_author: Adam Back 
@_subject: [cryptography] a Cypherpunks comeback 
OK let me put it this way, given each person only has so many hours in the
day, or so much energy and resources for politically-fighting or
write-code-fighting things which would you rather fight: defense of spurious
attention arising from a stupid domain name, or I dunno operating a
remailer, a tor exit node, a hidden tor server.
Apparently operating a hidden tor server as a service is pretty high risk as
the guy in Ireland is finding.  You can see in that they are trying to pin
the content on him, as if he authored it, whereas I am presuming he is no
more responsible for the content than a hosting company or youtube.  If he
was prominently using al-qaeda.net you can be sure they'd have spun that
into the story.
There is some history also - recall Jim Bell, he got in some fight over
taxes or something stupid, that took him out of the picture for a while.  I
wasnt really sold on his assassination politics idea anyway (gotta be a way
to vote someone out of office without assasinating them!), but at least it
was a political discussion which he thought had some merit vs a losers game
of tax protestation ending in jail time, anyone can see thats never going to
work out.
I wouldnt be so sure that using stupid domain names is entirely safe in the
US, europe etc.  IMO the US is past its peak in terms of a place of freedom
and others have overtaken it.  It doesnt seem likely the US will recover its
ranking, seems to be falling year on year.  Probably China itself will
overtake US economically, politically and for freedoms within 50-100 years. Not sure how you recover freedoms from a panopticon state with a one dollar
one vote and a 100 billion dollar+ military-spy-industrial complex and a
significantly biased politicial- judicial system.  If you watch RT which
airs a lot of the snowden thing, the stuff the USG is saying about snowden
is just ludicrous.  Pressuring european countries to deny overflight to a
presidents plane is an alarming breach of international law and shows how
far the US rogue state influence goes in seemingly other countries willing
to go along with its actions.
Also why would you even want to do it?  You care about crypto deployment, so
I dont see the logic in picking the most stupid, unrelated and controversial
domain name you can think of hitting as many peoples distaste as you can and
use that?  wtf back at you :)  cypherpunks at child-porn-r-us.com?  what next.
I guess we should go write some code!

@_date: 2013-08-10 02:56:15
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
Reading what Jon Callas wrote he said silent circle interoperated with
unencrypted SMTP email (unencrypted other than SSL over the transport), and
they used some bump in the wire PGP thingy that encrypts incoming email with
the silent circle users public key, and presumably sends out cleartext
possibly SSL SMTP where available, for non silent-circle recipients. Clearly therefore anyone tampering with the SSL (and often those mail
transport systems are not that smart about SSL as there is no security UI)
or just getting the NSA camel's nose inside the silent circle SSL
termination point prior to encrytion.
As they didnt think that would end well they decided to close it down. Alternatively they might have considered disabling the mail-in and mail-out
Its less clear what lavabit were talking about.  Perhaps something similar
in terms of an SMTP interoperability encryption gap, or alternatively about
being pressured to modify code (which people seem to assume, but I didnt see
explicitly stated).
There were some hushmail rumors about code modification some years back -
does anyone know what actually at hushmail?

@_date: 2013-08-10 03:19:59
@_author: Adam Back 
@_subject: [cryptopolitics] Silent Circle and Secure Email 
(Thanks for the link.)  It says hushmail had a simplified web-only version (no
java applet) and that the disclosure of client emails did not involve
pressured code changes (at least code shipped to clients), rather that as a
natural consequence of the way passwords would be processed on the server
side and decryption happened on the server side so hushmail had the
passwords, private keys, and decrypted plaintexts at leas in memory to hand
over on request.

@_date: 2013-08-11 12:13:28
@_author: Adam Back 
@_subject: NSLs, gag-orders, code-changes, coerced backdoors - any tech 
About physical access - there is one non-physical solution to this - hide
the location of the server behind tor, proxies etc.  Seems to work
remarkably well for pirate bay.  I cant imagine its that big a secret as to
where the packets are routed from the current proxy to the current physical
host, but seemingly NSA type resources have not been brought to bear against
it.  Step one for the attacker is to find it.  Maybe physical tamper
detection can wipe the RAM, cold reboot as the cage unlocked, or box is
opened, and immediately switch to the back up server in a different tor
hidden physical location.
One thing that occurs to me is that aside from the laundering of NSA tip
offs to FBI etc with faked plausible trails, that have been reported on
lately; there was an aspect that they would be hesitant to reveal what they
could tap, correlate etc, or under what circumstances they would abuse
national security (military) resources for various levels of criminal
activity (major, organized to minor, petty, or political misuse).  But the very fact that Snowden did the world a favour in disclosing the
illegal activities of the NSA and global partners, now people know what they
are doing or can better imagine, and not discount as paranoia, consequently
maybe once the dust has settled they will feel freer to feed ever more petty
or political or corporate espionage related information.  After all they'd
no longer be risking knowledge of information capability, or political
willingness.  Everyone pretty much figures they're in it up to their elbows
with corporate espionage (boeing vs airbus wiretaps), minor crimes with
fabricated evidence trails (maybe they wont bother fabricating them even in
future) and perhaps the political stuff though that is really evil and
anti-democractic (eg tea-party member IRS audits, blackmail etc).
It seems to me companies need to delegate code review and signing to a civil
society charitable organization with smart use of jurisdictions.  eg Germany
(chaos computer club code signing silent circle code?), Switzerland,
Iceland, or psuedonymous but high reputation individuals or groups.  Or
privacy groups which may have a more clear disinterest and immunity from
financial blackmail (like USG will cancel contracts if ISP, internet
service, or softwre company doesnt fold to NSL or other extra-legal
threats).  Or maybe EFF, privacy international etc.  Via their lawyers they
could retain a highly competent and pseudonymous team of technical reviews
and code signing that companies that care to demonstrate their alignment to
providing end to end secure services to their users would if it became
popular given an explanation of why they were not protected by independent
review based code signatures.

@_date: 2013-08-21 02:46:27
@_author: Adam Back 
@_subject: Google to encrypt cloud storage 
Well I think its fair to denigrate it as obfuscation not encryption if the
key lives on the same machine as the ciphertext.  At best it makes it less
risky to dispose of dodgy disks - now and then such things turn up on ebay
with client data.  At least if you encrypt it properly, and do NOT put the
key on the disk, then you can safely toss them in a dumpster, not physically
destroy them etc.

@_date: 2013-08-21 02:51:02
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
I was thinking something like that about the silent circle shutdown.  It
seems to me their problem case was the mail in (they would be encrypting
that to the user PGP key or equivalent, after sender optional use of SSL to
deliver it to them).  So would not a more sensible change be to disable
mail in?  So then only silent circle users could encrypt messages to each
other.  Even that would add pressure to other users to also get a silent
circle account and so be a business advantage.
Puzzlingly spun "to protect our users privacy we removed their encryption
feature" - so they'll probably send it plaintext instead, great.

@_date: 2013-08-21 07:52:25
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
Yes but my point was they didnt have to throw out the baby with the
bathwater; silent circles email I think was basically two products combined:
1. end2end secure, store-and-forward encryption between silent circle users;
2. server-side encryption of opportunistically SSL encrypted (potentially
unencrypted) incoming emails + presumably unencrypted outgoing emails.
Why not keep 1?  They obviously have the technology for it because they have
retained encrypted SMS-like functionality which is the same key management
and information flow.
Not forgetting there is a 3rd "product" which is the defacto which is normal
3. opportunistically encrypted (SSL) email
(as well as SMIME (dont trust due to CA malfeasance) or self-managed PGP/GPG
which for some reason people find difficult).
and users who lose 1 & 2 due to the no-notice product end-of-life will
probably just switch to 3 as an alternative to stopping communicating.  Even catching a flight with a USB drive apparently is risky via UK re the
curiously named David Miranda (Miranda rights eh) seems they demanded
decryption keys.  Seems like people who are couriering data ought to encrypt
it with the recipients public key before travel.

@_date: 2013-08-23 02:25:43
@_author: Adam Back 
@_subject: [Doctrinezero] HTTPS 
Are you sure about that?  (*.com and *. being valid).  I thought the MITM
boxes were loaded with a sub-CA cert - a cert with a bit set authorizing it
to generate certs for sites, some of the smaller CAs are not directly in the
trusted browser databases, and have bought sub-CA certs from CAs that are.
Then what actually happens in the MITM box is to load a fake cert for a
domain (issued by its sub-CA cert), or to generate fake certs on the fly for
any targetted domains (or all domains) again issued by its sub-CA cert.  So
I thought the CA that got warned by mozilla had issued a sub-CA cert for
MITM purposes.
(I really dont think a browser vendor would accept *.com nor especially *.
as a valid site cert wildcard.  It does get fiddly because you also want
*.co.uk etc to be invalid but they have some built in tables of such things
to differentiate a TLD from a domain).

@_date: 2013-08-31 10:05:07
@_author: Adam Back 
@_subject: why not disable external mail, keep intenal mail (Re: Who bought off 
More precisely its the exposed meta-data in the SMTP.  But why would you use
meta-data rich transport for silent circle internal-mail?  (Internal-mail I
mean silent circle user to silent circle user vs external mail being smtp
mail to silent circle user or silent circle user to smtp mail user).
I said it before, but again: why not cancel external mail, and leave the
internal mail working - silent circle obviously have the tech for that
because they have SMS equivalent in-mail.  Good for you: users who want to
continue to communicate will encourage the people they are communicating
with to also pay for subscriptions.  Maybe you could allow people to give
each other gifts of 1month membership, which you hope they extend
themselves; or some referal system with a bonus free month to the existing
user etc.
Now there might be some software legacy, but that seems straight forward
enough.  The crypto gap is purely the in and out mail.  (Other than forced
software changes, but others have discussed how to combat that issue, and
some claim legal advice is that its harder for the mil-int community to
legally force companies to change their software.  (Hushmail saga not

@_date: 2013-12-21 12:10:42
@_author: Adam Back 
@_subject: soft backdoors: ECDSA vs RSA vs EdDSA (aka EC Schnorr) (Re: BlueHat 
Dan Bernstein Vaudenay's report writes up an attack developed by Daniel Bleichenbacher
which he presented to some standards groups but did not publish.  As a
result of that the DSA standard was modified.  As I recall with about
1million signatures he could recover the private key due to the small bias
from the formual Peter mentioned: k = G(t,KKEY) mod q ie if |n| = 256-bits
where n is the order of the group, then G(t,KKEY) is distributed with a
rectangular distribution in {0,2^256-1} and q is < 2^256-1.  As I read it
Bleichenbacker did not try that hard to optimize his attack, it was enough
to show the NIST/NSA designed DSA RNG was biased enough to break DSA in a
server / automated environment.
Maybe further optimizations would have been possible...  Maybe this DSA flaw
spotted by Bleichenbacker was another NSA soft-sabotage attempt (making
standards security brittle in the knowledge that it some people will fail to
harden it, and also it gives a plausibly deniable backdoor design for
colluding business entities, or double-agents on the payroll (former NSA
people say)).  In fact DSA was even designed by a former NSA cryptographer.
 (Dr David Kravitz,
a former NSA employee).
The approach I prefer is the deterministic DSA approach where k = MAC(d,M)
where d is the private DSA/ECDSA key and M is the message, plus bias
removal.  Bernsteins EdDSA (which despite the name is actually a Schnorr
signature over an Edwards curve) also uses the same technique.  This is
standardized in an RFC.  If people are going to use DSA/ECDSA they should
use this deterministic DSA.  Personally I prefer EC Schnorr because Schnorr
is just a better, simpler, more secure and more flexible signature (supports
simplel blinding, compact multi-sig, clearer security proofs, better
security margin, less dependence on hash properties etc).  To my mind DSA's
only reason for existence is historic due to patents.  It is inferior by all
metrics to Schnorr, just that Scnorr's patent didnt expire until
 feb 2008.
Anyway as Bernstein has put forward EdDSA with parameters and multiple
security, speed, simple constant time, non-key related, nor message
execution time, and provably non-cooked curve parameters (and there perhaps
remains some needless ambiguity about the magic constants used to seed the
ECDSA parameters) there is no reason in my opinion not to use EdDSA aka EC
Schnorr in any new systems.
Of course RSA is good also, and simpler parameter definition, the main
downside being the large keys for same security margin (3072-bit).

@_date: 2013-12-21 12:13:58
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim 
Its hard to prove unfortunately, unless more leaks come out.  Probably there
exists no documentation to prove or disprove it within RSA, as to whether
RSA knew about the backdoor at the time it signed the deal.  Maybe there
would be documents within NSA.
However what you could say is no one at RSA, or in general, reacted much
following Ferguson et al's pointing out the design issue of there being an
undetectable backdoor in the RNG.
ps I think its Bidzos.

@_date: 2013-12-22 19:43:30
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: 
Ask Gwen he wrote the OP.  My response was about the potential complicity not the personnel.  The bit you quoted that I wrote was me putting a ps to point out that Gwen
mispelt his name (and I saw you wrote Bidzous also below - again I believe
its Bidzos).

@_date: 2013-12-21 12:10:42
@_author: Adam Back 
@_subject: soft backdoors: ECDSA vs RSA vs EdDSA (aka EC Schnorr) (Re: BlueHat 
Dan Bernstein Vaudenay's report writes up an attack developed by Daniel Bleichenbacher
which he presented to some standards groups but did not publish.  As a
result of that the DSA standard was modified.  As I recall with about
1million signatures he could recover the private key due to the small bias
from the formual Peter mentioned: k = G(t,KKEY) mod q ie if |n| = 256-bits
where n is the order of the group, then G(t,KKEY) is distributed with a
rectangular distribution in {0,2^256-1} and q is < 2^256-1.  As I read it
Bleichenbacker did not try that hard to optimize his attack, it was enough
to show the NIST/NSA designed DSA RNG was biased enough to break DSA in a
server / automated environment.
Maybe further optimizations would have been possible...  Maybe this DSA flaw
spotted by Bleichenbacker was another NSA soft-sabotage attempt (making
standards security brittle in the knowledge that it some people will fail to
harden it, and also it gives a plausibly deniable backdoor design for
colluding business entities, or double-agents on the payroll (former NSA
people say)).  In fact DSA was even designed by a former NSA cryptographer.
 (Dr David Kravitz,
a former NSA employee).
The approach I prefer is the deterministic DSA approach where k = MAC(d,M)
where d is the private DSA/ECDSA key and M is the message, plus bias
removal.  Bernsteins EdDSA (which despite the name is actually a Schnorr
signature over an Edwards curve) also uses the same technique.  This is
standardized in an RFC.  If people are going to use DSA/ECDSA they should
use this deterministic DSA.  Personally I prefer EC Schnorr because Schnorr
is just a better, simpler, more secure and more flexible signature (supports
simplel blinding, compact multi-sig, clearer security proofs, better
security margin, less dependence on hash properties etc).  To my mind DSA's
only reason for existence is historic due to patents.  It is inferior by all
metrics to Schnorr, just that Scnorr's patent didnt expire until
 feb 2008.
Anyway as Bernstein has put forward EdDSA with parameters and multiple
security, speed, simple constant time, non-key related, nor message
execution time, and provably non-cooked curve parameters (and there perhaps
remains some needless ambiguity about the magic constants used to seed the
ECDSA parameters) there is no reason in my opinion not to use EdDSA aka EC
Schnorr in any new systems.
Of course RSA is good also, and simpler parameter definition, the main
downside being the large keys for same security margin (3072-bit).

@_date: 2013-12-21 12:13:58
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim 
Its hard to prove unfortunately, unless more leaks come out.  Probably there
exists no documentation to prove or disprove it within RSA, as to whether
RSA knew about the backdoor at the time it signed the deal.  Maybe there
would be documents within NSA.
However what you could say is no one at RSA, or in general, reacted much
following Ferguson et al's pointing out the design issue of there being an
undetectable backdoor in the RNG.
ps I think its Bidzos.

@_date: 2013-12-22 19:43:30
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: 
Ask Gwen he wrote the OP.  My response was about the potential complicity not the personnel.  The bit you quoted that I wrote was me putting a ps to point out that Gwen
mispelt his name (and I saw you wrote Bidzous also below - again I believe
its Bidzos).

@_date: 2013-12-21 12:10:42
@_author: Adam Back 
@_subject: soft backdoors: ECDSA vs RSA vs EdDSA (aka EC Schnorr) (Re: BlueHat 
Vaudenay's report writes up an attack developed by Daniel Bleichenbacher
which he presented to some standards groups but did not publish.  As a
result of that the DSA standard was modified.  As I recall with about
1million signatures he could recover the private key due to the small bias
from the formual Peter mentioned: k = G(t,KKEY) mod q ie if |n| = 256-bits
where n is the order of the group, then G(t,KKEY) is distributed with a
rectangular distribution in {0,2^256-1} and q is < 2^256-1.  As I read it
Bleichenbacker did not try that hard to optimize his attack, it was enough
to show the NIST/NSA designed DSA RNG was biased enough to break DSA in a
server / automated environment.
Maybe further optimizations would have been possible...  Maybe this DSA flaw
spotted by Bleichenbacker was another NSA soft-sabotage attempt (making
standards security brittle in the knowledge that it some people will fail to
harden it, and also it gives a plausibly deniable backdoor design for
colluding business entities, or double-agents on the payroll (former NSA
people say)).  In fact DSA was even designed by a former NSA cryptographer.
 (Dr David Kravitz,
a former NSA employee).
The approach I prefer is the deterministic DSA approach where k = MAC(d,M)
where d is the private DSA/ECDSA key and M is the message, plus bias
removal.  Bernsteins EdDSA (which despite the name is actually a Schnorr
signature over an Edwards curve) also uses the same technique.  This is
standardized in an RFC.  If people are going to use DSA/ECDSA they should
use this deterministic DSA.  Personally I prefer EC Schnorr because Schnorr
is just a better, simpler, more secure and more flexible signature (supports
simplel blinding, compact multi-sig, clearer security proofs, better
security margin, less dependence on hash properties etc).  To my mind DSA's
only reason for existence is historic due to patents.  It is inferior by all
metrics to Schnorr, just that Scnorr's patent didnt expire until
 feb 2008.
Anyway as Bernstein has put forward EdDSA with parameters and multiple
security, speed, simple constant time, non-key related, nor message
execution time, and provably non-cooked curve parameters (and there perhaps
remains some needless ambiguity about the magic constants used to seed the
ECDSA parameters) there is no reason in my opinion not to use EdDSA aka EC
Schnorr in any new systems.
Of course RSA is good also, and simpler parameter definition, the main
downside being the large keys for same security margin (3072-bit).

@_date: 2013-12-21 12:13:58
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim 
Its hard to prove unfortunately, unless more leaks come out.  Probably there
exists no documentation to prove or disprove it within RSA, as to whether
RSA knew about the backdoor at the time it signed the deal.  Maybe there
would be documents within NSA.
However what you could say is no one at RSA, or in general, reacted much
following Ferguson et al's pointing out the design issue of there being an
undetectable backdoor in the RNG.
ps I think its Bidzos.

@_date: 2013-12-22 19:43:30
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: 
Ask Gwen he wrote the OP.  My response was about the potential complicity not the personnel.  The bit you quoted that I wrote was me putting a ps to point out that Gwen
mispelt his name (and I saw you wrote Bidzous also below - again I believe
its Bidzos).
