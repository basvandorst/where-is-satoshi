
@_date: 2002-09-06 17:21:50
@_author: David E. Weekly 
@_subject: Wolfram on randomness and RNGs 
It would seem that while the bitstream generated by the center column of
rule 30 might be a good random number source, its repeatability is the very
thing that detracts from its usefulness in cryptographic application. An
obviously poor application would be to have a "one time pad" where two
parties would xor their plaintext with the bitstream produced by rule 30,
starting at the top. While the resulting bitstream would appear random, an
attacker with knowledge of the algorithm could just run rule 30 themselves
and decode the result. To have cryptographically strong random numbers, one
needs to have an *unreproducable* source of randomness -- the very thing
that Wolfram seems to sneer at as being purely academic but that the above
methodology makes clear. While a slightly modified approach of having both
sides start at a secret row of rule 30 could be used, the key is now merely
the row number; defeating the purpose.
One interesting possibility might be to "seed" a wide row of rule 30 with
bits gleamed from the environment; this would make it difficult to reproduce
the bitstream without the bits representing the initial conditions, but
without continuing to add bits to rows, the "bit strength" of the randomness
is only the width of the seeded row (namely, if you're using 8 bits of
randomness to seed rule 30, an attacker could brute force the 256
possibilities to find your random bitstream).
The problem is, IMHO, exactly analogous to deriving randomness from
irrational numbers, such as the digits of pi, e, or the square root of two;
this just might be a slightly more efficient way to generate the bitstream.
The point is, they're all very good sources of randomness, but the fact that
their sequences are so well-defined keeps them from being a good source of
secrecy; picking out which portions of the sequence to use end up becoming
your secret and your sequence is truly only as unpredictable as this secret.
In another sense, the sequence you're using is only as strong as its inputs.
Just my $0.02; please bitchslap me if I got this wrong.
 David E. Weekly
 Founder & Executive Director
 California Community Colocation Project (an OPG project)
  - the world's first non-profit colo!
----- Original Message -----
Sent: Friday, September 06, 2002 1:57 PM

@_date: 2002-09-13 18:29:36
@_author: David E. Weekly 
@_subject: The Case Against Steganography In Perceptually Encoded Media 
I had an interesting revelation last night. It's a bad idea to use
perceptually-coded media to embed steganographic data. By definition, it
means making the coder make decisions that it otherwise would not have made.
If the coder is good, then the coder's decisions are not arbitrary but
rather each bit is focused on producing the minimal representation necessary
for adequate presentation to humans. This means that encoding extra "random"
data on top of this will always produce compressed output that is of lower
quality than the original. From an information theory standpoint, if you're
tacking on a data stream to compressed output, the stream that is the sum of
the two contains more information and must be represented with more bits.
For example, to attack steganographically-encoded pictures, the pictures
could be analysed and those with lower-quality encoding than expected would
be flagged for analysis as suspect.
The conclusion is remarkable (to my little mind, at any rate): since most
media transmitted over the Internet is perceptually compressed (JPG, MOV,
AVI, MP3, etc.) the efforts to steganographically encode data within most
Internet media are fundamentally doomed.
Where, then, can one hide information streams? The answer is wherever
*random* information is communicated. (Even just partial randomness is okay;
I've got a paper on this I hope to be presenting soon!)
 David E. Weekly
 Founder & Executive Director
 California Community Colocation Project (an OPG project)
  - the world's first non-profit colo!

@_date: 2002-09-13 19:57:00
@_author: David E. Weekly 
@_subject: The Case Against Steganography In Perceptually Encoded  
The key is to steganographically encode "random" (encrypted) data and then
"shape" the result to match the mean probabilities seen in observation of a
system in usual operation. This defeats statistical analysis, since your
data is shaped just like everyone else's. If the bits are encrypted with a
recipient's public key, only with posession of the private key can the data
be perceived to be non-random, which is a nicely strong property.
 -David Weekly
