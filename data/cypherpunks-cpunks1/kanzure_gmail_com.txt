
@_date: 2013-04-05 16:45:43
@_author: Bryan Bishop 
@_subject: Science Defense Force 
Hey all:
We are starting to document IP addresses associated with anti-science
activity, especially evil IP addresses that are scanning the web for
science articles, watermarks, anti-regime content, etc. There are
other IP block lists out there, but none of them seem to have a
mission of anti-anti-infringement and most of them look dead.
For now, this information is compiled here:
However, maybe we will make up some automatic update mechanism for
nginx, or something that can detect HTTP referers/user agents/ip
addresses with nightly updates. For instance, certain referers are
obviously worrisome (like web dashboard products for publishers, which
have distinct urls) and their users should be blocked immediately.
I welcome any contributions you can offer... including organization
names, system design criticism, IP ranges, or contributions to
pdfparanoia (  for watermark
SDF is a play on Superdimensional Fortress.
- Bryan
1 512 203 0507

@_date: 2013-04-23 14:29:01
@_author: Bryan Bishop 
@_subject: [DIYbio] legality of distributing pdfs or hard copy papers? 
Manuscripts published in the United States prior to 1923 are in the
public domain, so those can be shared without wondering. Also, in the
past 10 years there has been the growing trend of publications
licensed permissively with a choice license from Creative Commons.
These tend to be very explicitly okay with all sorts of sharing and
Most publishers will send you a notification (or even a DMCA takedown
request) if they think you are infringing on their rights. That's a
good time to talk with them to work out how they believe you are
infringing, and how the situation could be resolved to mutual
Honestly, if you are worried about a publisher tracking you down for
partaking in science, then I would (with bias) recommend pdfparanoia
to strip out watermarks:
IIRC, you sometimes have people pay to attend BOSSlab ? I would be
cautious about distributing papers in that context. You have to be
especially vigilant in situations where money is changing hands. I
think publishers use sites like copyright.com to calculate how much
you owe them per distribution in a commercial/business/non-profit
I am not entirely sure about what goes down if you are an unaffiliated
individual. It would be a huge violation of the trust that science has
placed in publishers if they were to go around suing readers for
reading science.
IANAL. No implied warranty or implied fitness for a particular purpose.
- Bryan
1 512 203 0507

@_date: 2013-04-24 12:12:51
@_author: Bryan Bishop 
@_subject: [DIYbio] legality of distributing pdfs or hard copy papers? 
Hm, it says:
"Mat only removes metadata from your files, it does not anonymise
their content, nor handle watermarking, steganography  [...]"
As far as I can tell (from HTTP server logs), businesses looking for
"science violations" are searching by looking for the watermark
strings, not the metadata in the pdf's headers.
Here's some ip addresses you should block:
But if we need to start stripping journal names, paper titles, author
names, etc., from pdfs, I am not sure how we would re-assemble that
information later, because anyone would be able to re-assemble that
information and would to find the "science violators".
- Bryan
1 512 203 0507

@_date: 2013-02-05 04:58:50
@_author: Bryan Bishop 
@_subject: Removing watermarks from pdfs 
Working proof of concept:
To install:
git clone git://github.com/kanzure/pdfparanoia.git
sudo pip install pdfparanoia
sudo easy_install pdfparanoia
Right now there's IEEE and AIP support. I need more samples to work with.
- Bryan
1 512 203 0507

@_date: 2013-02-05 14:20:22
@_author: Bryan Bishop 
@_subject: [DIYbio] Removing watermarks from pdfs (pdfparanoia) 
How about removing those pesky watermarks from pdfs? Sometimes they
completely obfuscate the contents of a paper we're trying to read, or
sometimes they have more sinister purposes.
Working proof of concept:
Discussion history:
People who could theoretically benefit from this:
To get source code:
git clone git://github.com/kanzure/pdfparanoia.git
To install:
sudo pip install pdfparanoia
sudo easy_install pdfparanoia
Right now there's IEEE and AIP support. I need more samples to work with.
- Bryan
1 512 203 0507

@_date: 2013-02-07 02:26:09
@_author: Bryan Bishop 
@_subject: [DIYbio] Removing watermarks from pdfs (pdfparanoia) 
I don't recommend this method, because converting most pdfs into
images will cause loss of text. You can delete entire pages in the pdf
format by deleting the "stream" objects and modifying the xref table.
Most text in a pdf document is "semantic", surrounded by pdf markup
that can be directly deleted. I can imagine there might be one or two
cases where publishers are adding an image to a pdf with your ip
address, in which case you can delete that single image. However, if
the page content is an image itself (no selectable text), then they
might have chosen to add the image into the page, in which case the
only way to remove the watermark would be to use imagemagick as you
say, and draw over the offending image. So far I haven't seen this yet
in any of the documents I have read over the years.
How would you whitelist content you've never seen before?
- Bryan
1 512 203 0507

@_date: 2013-02-07 02:30:18
@_author: Bryan Bishop 
@_subject: [DIYbio] Removing watermarks from pdfs (pdfparanoia) 
One of the advantages of using pdfparanoia is that you can directly
remove watermarks based on what we know about what publishers are
doing, instead of blindly guessing. If there is metadata about ip
addresses, write a plugin for pdfparanoia to detect it and remove it.
(Also write a unit test, so that future contributors can make sure
your code doesn't break). So far, I haven't seen evidence of metadata
being used like this. Really, they are all extremely pdf servers like
itext that are serving up http requests for unsuspecting scholars. My
guess is that the most "advanced" watermarking infrastructure is just
some LaTeX template that is being applied for each incoming http
- Bryan
1 512 203 0507

@_date: 2013-02-08 18:44:44
@_author: Bryan Bishop 
@_subject: Cost of Knowledge update, Elsevier boycott one year on 
The Elsevier boycott one year on
A few days ago was the anniversary of the beginning of the Cost of
Knowledge boycott of Elsevier. It seems a good moment to take stock of what
the boycott has achieved and to think about what progress has or hasnbt
been made since it started. This post is a short joint statement by many of
the people who signed the original Cost of Knowledge statement last year.
At some point in the not too distant future I plan to write a longer post
giving a more personal view.
The Elsevier boycott: where do we now stand?
In the first few months after the boycott started, the number of
signatories grew very rapidly. The growth is now much slower, but this was
to be expected: given that, for understandable reasons, no editorial boards
of Elsevier journals were ready to take the drastic step of leaving
Elsevier, it was inevitable that further progress would depend on the
creation of new publication models, which takes time and work, much of it
not in the public eye. We are very pleasantly surprised by how much
progress of this kind there has already been, with the setting up of Forum
of Mathematics, a major new open-access journal, and the recent
announcement of the Episciences Project, a new platform for overlay
journals. We are also pleased by the rapid progress made by the wider Open
Access movement over the last year.
In one respect the boycott has been an unqualified success: it has helped
to raise awareness of the concerns we have about academic publishing. This,
we believe, will make it easier for new publishing initiatives to succeed,
and we strongly encourage further experimentation. We believe that
commercial publishers could in principle play a valuable role in the future
of mathematical publishing, but we would prefer to see publishers as
bservice providersb: that is, mathematicians would control journals,
publishers would provide services that mathematicians deemed necessary, and
prices would be kept competitive since mathematicians would have the option
of obtaining these services elsewhere.
We welcome the moves that Elsevier made last year in the months that
followed the start of the boycott: the dropping of support for the Research
Works Act, the fact that back issues for many journals have now been made
available, a clear statement that authors can post preprints on the arXiv
that take into account comments by referees, and some small price
reductions. However, the fundamental problems remain. Elsevier still has a
stranglehold over many of our libraries as a result of Big Deals (a.k.a.
bundling) and this continues to do real damage, such as forcing them to
cancel subscriptions to more independent journals and to reduce their
spending on books. There has also been no improvement in transparency: it
as hard as ever to know what libraries are paying for Big Deals. We
therefore plan to continue boycotting Elsevier and encourage others to do
the same.
The problem of expensive subscriptions will not be solved until more
libraries are prepared to cancel subscriptions and Big Deals. To be an
effective negotiating tactic this requires support from the community: we
must indicate that we would be willing to put up with cancelling overly
expensive subscriptions. The more papers are made freely available online
(e.g., through the arXiv), the easier that will be. Many already are, and
we regard it as a moral duty for mathematicians to make their papers
available when publishers allow it. Unfortunately, since mathematics papers
are bundled together with papers in other subjects, real progress on costs
will depend on coordinated action by mathematicians and scientists, many of
whom have very different publication practices. However, a statement by
mathematicians that they would not be unduly inconvenienced by the
cancelling of expensive subscriptions would be a powerful one.
We are well aware that the problems mentioned above are not confined to
Elsevier. We believe that the boycott has been more successful as a result
of focusing attention on Elsevier, but the problem is a wider one, and many
of us privately try to avoid the other big commercial publishers. We
realize that this is not easy for all researchers. When there are more
alternatives available, it will become easier: we encourage people to
support new ventures if they are in a position do so without undue risk to
their careers.
We acknowledge that there are differing opinions about what an ideal
publishing system would be like. In particular, the issue of article
processing charges is a divisive one: some mathematicians are strongly
opposed to them, while others think that there is no realistic alternative.
We do not take a collective position on this, but we would point out that
the debate is by no means confined to mathematicians: it has been going on
in the Open Access community for many years. We note also that the
advantages and disadvantages of article processing charges depend very much
on the policies that journals have towards fee waivers: we strongly believe
that editorial decisions should be independent of an authorbs access to
appropriate funds, and that fee-waiver policies should be designed to
ensure this.
To summarize, we believe that the boycott has been a success and should be
continued. Further success will take time and effort, but there are simple
steps that we can all take: making our papers freely available, and
supporting new and better publication models when they are set up.
Doug Arnold, John Baez, Folkmar Bornemann, Danny Calegari, Henry Cohn,
Ingrid Daubechies, Jordan Ellenberg, Marie Farge, David Gabai, Timothy
Gowers, Michael Harris, FrC)dC)ric HC) lein, Rolf Jeltsch, Rob Kirby, Vincent
Lafforgue, Randall J. LeVeque, Peter Olver, Olof Sisask, Terence Tao,
Richard Taylor, Nick Trefethen, Marie-France Vigneras, Wendelin Werner,
GCleitl ICBM: 48.07100, 11.36820  8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE

@_date: 2013-01-12 16:51:40
@_author: Bryan Bishop 
@_subject: [DIYbio] Re: Aaron Swartz has committed suicide 
Guerilla Open Access Manifesto
- Bryan
1 512 203 0507

@_date: 2013-01-14 11:01:00
@_author: Bryan Bishop 
@_subject: [DIYbio] Re: Aaron Swartz has committed suicide 
Also this:
Instead of working privately on academic scrapers, what if we avoided
mistakes by talking about best practices for collecting metadata and
- Bryan
1 512 203 0507

@_date: 2013-01-14 11:59:38
@_author: Bryan Bishop 
@_subject: [hackerspaces] Academic scraping 
One of my favorite scraping methods at the moment is phantomjs, a
headless wrapper around webkit.
But for academic projects, I highly recommend zotero's translators.
Here's why. There's already a huge userbase of zotero users actively
updating these scrapers. When they break, they fix them immediately.
They are all written in javascript and they extract not only the link
to the pdf but also the maximum amount of metadata. With the help of
the zotero/translation-server project, they can be used headlessly.
I have a demo of this working in irc.freenode.net (paperbot), he just grabs links from our conversation and posts the
pdfs so that we don't have to ask each other for copies.
- Bryan
1 512 203 0507

@_date: 2013-01-15 12:37:19
@_author: Bryan Bishop 
@_subject: Concept: Mobile proxies for downloading pdfs 
Here's a basic concept for using mobile proxies.
The idea is that college students would install an android or iphone
app that would phone home and tell a server that a phone is now
available to take requests. The app would travel over HTTP (to prevent
any sort of port blocks) and ask the server for any new papers to
download. The app would perform the download, then POST or PUT the
file back to the server along with the metadata. The metadata could be
served back to the server by POSTing the HTML, actually.
1) android app
2) iphone app
3) a common protocol, probably JSON over HTTP
4) a central server to send requests
5) some way to access this server to manage a fleet or get back pdfs.
Any thoughts on how to make this work out? I can write the android and
iphone versions for this, and the server, but it would be better if
there were others involved. Additionally, I have been having trouble
finding an appropriate proxy to run on android that isn't limited to
HTTP requests.. surely there's some sort of botnet-provided proxy on
android already ? Transproxy isn't it, and proxydroid is really just
for redirecting outgoing requests, not handling incoming requests from
other machines.
- Bryan
1 512 203 0507

@_date: 2013-01-15 18:34:33
@_author: Bryan Bishop 
@_subject: Removing watermarks from pdfs 
How about getting rid of those pesky watermarks in pdfs?
As far as I can tell, there are only visible watermarks. Invisible
watermarks can be detected by comparing the same pdf retrieved through
two different gateways (like from two different libraries). I have
checked Nature Publishing Group and Elsevier (specifically
ScienceDirect) and found no checksum differences.
But there are some culprits out there that do some nasty things to documents:
* lines of text added to the document containing an ip address,
timestamp, university name, etc. (IEEE Xplore)
* entire pages added to documents with tracking information (Wiley? I
can't remember exactly.)
* possibly some might be using CVE-2010-0188 to phone home to
publishers. PDF supports javascript and flash and other terrible
things, so it would be interesting to check if any publishers have
attempted to use these vulnerabilities to their advantage.
* there might be "hidden" information inside a pdf that changes when
you download a document, but so far no evidence of this has been found
(so I don't believe it's likely, but it's worth keeping in mind).
I think it would be useful to work on some ways to remove watermarks
from pdfs. I am aware of largely two types of pdfs that publishers
distribute. One is the feared "collection of images", which may or may
not have extra images slapped on with ip address information. The
second is a pdf with actual selectable text. The first type, with just
images everywhere, can be de-watermarked by just drawing images over
the offensive text. The second type requires some other creative
thinking, maybe just a collection of regular expressions.
For instance, here's a line that IEEE Xplore once added to a paper
that I was reading:
"Authorized licensed use limited to: University of Getting Schooled.
Downloaded on July 39, 2009 at 15:10 from IEEE Xplore. Restrictions
In fact, you can see this line appearing in other (4,000) papers that
other people have been reading:
Here's another example. AAAS/Science is of particular interest. They
attach an entire front page and add text in the margins everywhere:
"Downloaded from  on November 30, 1912"
So I think a good first step would be to collect examples of text
added to documents that need be detected by any eraser we write. In
fact, maybe all identifying information for an article should be
removed, and just replace it with an easy-to-copy-down text code (like
"blue-apple-oranges" to refer to a specific document in an index).
Does anyone else have some samples to share of nasty watermarks worth
removing? Also, any favorite ways to manipulate pdfs?
- Bryan
1 512 203 0507

@_date: 2013-01-16 19:43:42
@_author: Bryan Bishop 
@_subject: Papester Collective - nearly-automatic paper request fulfillment over twitter 
Yesterday my friend Hauke and I theorized about a kind of dream
scenario- a totally distributed, easy to use, publication liberation
system. This is perhaps not feasible at this point [1]. Today webre
going to present something that will be useful right now. The
essential goal here is to make it so that anyone, anywhere, can access
the papers they need in a timely manner. The idea is to take advantage
of existing strategies and tools to streamline paper sharing as much
as possible. Folks already do this- every day on twitter or in
private, requests for papers are made and fulfilled. Our goal is to
completely streamline this process down to a few clicks of your mouse.
That way a small but dedicated group of folks b the Papester
Collective b can ensure that  requests are fulfilled almost
instantly. This is a work in progress. Leave comments on how to
improve and further streamline this system and join the collective!
SHORT VERSION: HOW TO GET A PAPER BEHIND A PAYWALL QUICKLY
Tweet (for example): b

Show your support for the papester collective by tweeting: bThanks to
 my  request was fufilled in seconds! Join:
 
Click: Here you can find more detailed instructions.
HOW TO JOIN THE COLLECTIVE AND START SERVING REQUESTS
SHORT INSTRUCTIONS AND REQUIRED SOFTWARE:
Twitter: Monitor  Zotero and zotero browser plugin: after clicking on DOI link or
abstract page just click on bSave to Zoterob button to auto-grabs PDFs
Zotfile: automatically copies new Zotero pdfs files saved to public
Dropbox folder
Dropbox: Cloud storage system to seamlessly share files with anyone
without login.
Dropbox linker: automatically adds links from public folder to your clipboard
Reply to request tweets: paste URL from clipboard and if you want Thatbs it! Now you can just click request links, click the Zotero get
PDF button, and CTRL+V a dropbox direct download link in response!
Click: Here you can find more detailed instructions.
1.The fundamental problem: uploading huge repositories of scientific
papers is not sensible for now. Itbs too much data (50 million papers
* 0.5-1.5 megabytes together make up ~ 25-75 Terrabytes) and the
likelihood for every paper to be downloaded is more uniformly
distributed than with files traditionally shared like music. For
instance, there are 100 million songs x 3.5 mb songs, and it is
difficult to find exotic songs online b some songs have decent
availability now because there are only a few favourites b not so with
favourite papers. Also, fewer people will share papers than songs, so
this makes it more even more difficult to sustain a complete
repository. Thus, we need a system that fufills requests individually.
Disclaimer: Please make sure you only share papers with friends who
also have the copyrights to the papers you share.
Instead of forcing the user to manually monitor twitter, you could
just have a simple client that reads a twitter feed, passes the urls
through zotero (after resolving them from the t.co shortlink),
download the pdf and upload to dropbox. Zotero already has a Google
Scholar search feature and auto-save-pdf feature, so I'm not sure
about those steps as written.
- Bryan
1 512 203 0507

@_date: 2013-01-23 20:29:28
@_author: Bryan Bishop 
@_subject: Geurilla open access cookbook 
Open access guerilla cookbook
git clone git://github.com/c0nt3nt/oagcookbook.git
The Open Access Guerilla Cookbook
Dedicated to Aaron Swartz (1986-2013)
The remainder of this cookbook should be composed of recipes. These may
include instructions for the use of or the code for scrapers and other
tools that are appropriate for wider distribution. They may include
tutorials and descriptions of good practices for the various roles of the
movement. They may describe appropriate security measures. They may recount
past victories and failures of the movementbbut in a way that does not
compromise anyone's identity. They should ideally not include any direct
links to online resources, but may provide suggestions on how to use search
techniques to find them, as they may often move. Under each recipe, include
the date it was written, an optional author pseudonym, and if you
distribute an edited version of this document, update the timestamp and
version information at the bottom for the cookbook as a whole.
Securing Communication
Below are some simple code snippets to give you ideas about how to design
scrapers that will behave more like humans, thus concealing to some degree
your raid on a database. Of course, if you conduct the entire raid from a
single IP address, or your are always logging in with the same user
credentials, a careful analysis of server logs and traffic can lead to
discovery. However, the following may help fool the lazy server
administrator who merely glances over access logs from time to time.
*In Ruby:*
# => This method can be used to create some variety in pauses between
grabbing files from a server
# => and thus simulate human behavior. You set a default break time (here
twoseconds, in reality a
# => random time of 1-3 seconds) and several more rare longer breaks. You
then indicate a probability
# => for these other longer breaks to occur. For example, currently there
is a 0.2% chance that the
# => scraper will take a roughly one hour break, but a 0.5% chance it will
take a 30 minute break,
# => and so on.
def randomBreak()
   the various times for a break:
  twoseconds=rand(1..3)
  fifteensec=rand(12..18)
  onemin=rand(50..70)
  fivemin=rand(250..350)
  tenmin=rand(500..700)
  thirtymin=rand(1600..2000)
  onehour=rand(3300..3900)
   break time:
  sleeptime=twoseconds
   the dice:
  x=rand(1..1000)
   percentage chance that there is a longer break:
  if (1..2).member?(x) then sleeptime=onehour end
  if (3..7).member?(x) then sleeptime=thirtymin end
  if (8..18).member?(x) then sleeptime=tenmin end
  if (19..32).member?(x) then sleeptime=fivemin end
  if (33..50).member?(x) then sleeptime=onemin end
  if (51..120).member?(x) then sleeptime=fifteensec end
  puts "Sleeping "+sleeptime.to_s+" seconds."
  sleep sleeptime
# => This method can be used to create some variety in pauses between
grabbing files from a server
# => This method can be used to force your script to only operate within
certain "working hours"
# => and thus simulate human behavior. If you have a scraper pulling files
24 hours a day, anyone
# => inspecting server logs closely will immediately know automated
downloading is happening.
# => Use this script to make it at least somewhat more plausible that a
very diligent human being
# => was downloading files from their favorite database.
# =>
# => Call this method once each time a file has been downloaded completely
before proceeding to
# => the next round of the loop. Only proceed, if the method returns true.
If it is "outside
# => working hours" it will return false. You can use a while loop to wait
until working hours.
# fromgmt - how many hours earlier or later than GMT timezone you wish to
operate in
def workinghours(fromgmt=0,starthour=9,endhour=17)
   current hour at GMT timezone:
  currentgmt=Time.new.gmtime.hour
  currentmin=Time.new.min
   current hour at timezone of desired operation:
  currenthour=currentgmt+fromgmt
  if currenthour==24 then currenthour=0 end
   assume here workingg hours do not cross midnight, anyone want to redo
the following to
   for that possibility?
  if currenthour>=starthour && currenthour<=endhour
    return true
  end
  return false
 we might use this:
while !workinghours(fromgmt=1,starthour=1,endhour=11)
  # wait one minute and check the time again
  sleep 60
# => This very simple snippet can be used to set a maximum limit on
# => the time the script will run. Here again we simulate likely human
# => behavior. You can run the script comfortable that it will run
# => for a limited time.
# maxminutes - The number of minutes you want this script to run
maxminutes=120 # for example, 2 hours or 120 minutes
# OPEN YOUR MAIN LOOP HERE
if Time.new-starttime>maxminutes*60 then
  break # out of your loop and wrap up the script...
# => If you use Mechanize in Ruby to do your scraping you can set the "user
agent," that is,
# => the browser that you are pretending to be when you grab files from the
server. You can
# => add some noise to your activity by choosing a random user agent each
time you use the
# => script.
def randUserAgent()
   a random user agent but weighs towards popular browsers
   a random user agent but weighs towards popular browsers
   it doesn't include Chrome or Opera. Sample is for Ruby 1.9,
need change for
   versions of Ruby.
  return ["Windows IE 7","Windows IE 7","Windows IE 7","Windows IE
7","Windows IE 6","Windows Mozilla","Windows Mozilla","Windows
Mozilla","Windows Mozilla","Mac Safari","Mac FireFox","Mac FireFox","Linux
# => ...
Recipe: Overcome Built-In Limit on JSTOR Liberator
