
@_date: 1998-07-19 14:20:55
@_author: David Honig 
@_subject: IDEA chip evaluation? 
Anyone actually received the IDEA chip eval board from ASCOM?
How long does this take?

@_date: 1998-07-27 11:05:09
@_author: David Honig 
@_subject: encrypted FM radio hiss 
On the subject of RNGs.  Thinking about conditioning.
Suppose you have a "poor" random number stream, e.g., FM hiss digitized at
say 8 Ksamples/sec.  Can you get a crypto-secure random-number stream by
"whitening" the stream with a good block cipher?
This scheme uses the RNG to "kick" the cipher out of the deterministic
cycle its in, which is determined by the cipher key and initialialization
Poor RNG ----> XOR ----> BlockCipher ----> improved RNG?
                 ^                    |
The output of a good block cipher in feedback mode will pass Diehard tests,
though it is not crypto-secure.
slowly adding entropy to the output stream, at a rate determined by the
actual number of bits/iteration and the bits/symbol of your poor random
If you fed 64 bits of pure random values into a 64 bit cipher you would have
a true RNG, filtered by the xor/ciphering, but still crypto-secure.  With
fewer true bits, you have a 'smooth' way to introduce variable amounts of
true entropy.
If your RNG is 'stuck at' a constant value you are back to a deterministic
How do you cryptanalze the mix of a keyed PRNG and a true entropy source here?
Is there any mathematical literature on this?
   "Speech is not protected simply because it is written in a language"

@_date: 1998-07-28 09:37:57
@_author: David Honig 
@_subject: Re: encrypted FM radio hiss 
My rough understanding: the 'P' value is a measure on the hypothesis that
the test sample is a truly random sample, where truly random is defined by
the expected statistical properties being measured.  Eg in 100 bits you expect to find
50 1's; if you count 48, is your 100-bit sample consistant with it being
If you get values near 1.0 your sample is not likely taken from a random pool.
Try this: generate 10Meg from a block cipher feeding back on itself.  Diehard
will pass these.  (Diehard needs 10M samples)
Now run FM hiss into your soundcard.  Sample this at 8Khz (to avoid
temporal correlation)
and save to a file til you have 10Meg.  Diehard will reject this.  Make a
file, and then gzip it down to 10Meg.  (That it shrinks indicates its symbols
don't carry a full bit.)  Run Diehard on this.  It will pass more tests but
not all.
Take the FM hiss, feed it into a stream cipher, and start burning those OTPs.
Do this with a detuned *video* tuner for more bandwidth.    "Speech is not protected simply because it is written in a language"

@_date: 1998-07-29 10:45:32
@_author: David Honig 
@_subject: Re: encrypted FM radio hiss 
I don't believe the depth of info you want is in Marsaglia's distribution
You want to talk to a real statistician...
   "Speech is not protected simply because it is written in a language"

@_date: 1998-07-30 11:23:46
@_author: David Honig 
@_subject: Re: encrypted FM radio hiss 
Hmm.  What packages or tests do any experimentalists out there use for entropy measurements?  I've used Diehard.  Also I look at compressability (gzip).  I've found
spectrogramming utilities.  I think Walker has a utility, not yet looked at.
   "Speech is not protected simply because it is written in a language"

@_date: 1998-07-31 13:54:05
@_author: David Honig 
@_subject: Refining entropy 
This essay addresses the question, How can we improve the output of a bad
true RNG?  By "true RNG" I mean a source of unpredictability, which
necessarily derives from an analog measurement.  By "bad" RNG
I mean that its output does not carry one bit of information per binary
symbol.  This means that not all output symbols are equiprobable.  Methods
to improve various properties of RNGs are called conditioning algorithms.
I had begun by thinking that, since the output of a block cipher in
feedback mode (ie, a stream cipher) is uniformly distributed ("white") that
it would be sufficient conditioning to feed a bad RNG into a block cipher
with feedback.  Here, the bad RNG would kick the cipher out of the fixed,
but key-dependant, sequence it would otherwise generate.
But it bothered me that I was producing more apparently-random bits than I
actually had negentropy.
On cypherpunks, Bill Stewart mentioned using hashes instead of ciphers.  A
hash is like a cipher in that
changing any input or key bit changes the  output unpredictably, but a hash
is irreversible because there are fewer output bits than input bits.    The
simplest hash is a one-bit parity; MD5
is a more expensive keyed hashing function used for message authentication.
Thinking about a hash instead of a cipher was interesting because now we
reduce the number of output bits, which if done right, could improve the
information carried by each binary digit.  We take the MAC of an imperfect
random number and toss the original random, retaining the MAC as our better
random value.
Can we get an analytic handle on this?
Let's take Mr. Shannon's measure of information over a discrete alphabet.
The average amount of information per symbol is the (negation of the) sum
of the probabilities of each symbol times the log base 2 of that probability.
If we have two symbols, A and B, and each occurs half the time, we have If symbol A occurs, say, 3 times more often than B, we get:
2/4 = 3/4 log 3 - 2 = the optimum 1 bit/symbol, achieved when the symbols are equiprobable as above.
So now we have demonstrated how to use Shannon to measure the actual
entropy carried by each imperfectly random symbol. We now use this to
measure the result of XORing two such imperfect values.
We continue to use the imperfect RNG source where A occurs 3/4 time, B
occurs 1/4 time.  We take two of these badbits and XOR them: A.A=A, A.B=B, B.A=B, We now compute the liklihood of each output symbol: A.A	3/4 * 3/4 = 9/16	(A)
A.B	3/4 * 1/4 = 3/16	(B)
B.A	1/4 * 3/4 = 3/16	(B)
B.B	1/4 * 1/4 =1/16	(A)
So the output has symbol A 10/16 of the time, and B 6/16 of the time.  The
difference in frequency has decreased; the distribution becomes more
uniform, which means that the information content of the output has
increased.  The information content of the whole system can only decrease;
but if you destroy the inputs
the output carries their entropy.
The exact amount of information per symbol in the 10:6 distribution is -4) = started with.
Intuitively, we have gone from this distribution:
crossed with itself, yields this distribution of states: which when collapsed yields this: So: we can use Shannon's analytic measure to look at entropy collection
a combination of imperfect input bits.  It shows us that we'll never get
perfectly uniformly distributed symbols by combining arbitrary quantities of nonuniformly
distributed symbols, but we can get arbitrarily close.  All we need is more
Quantity vs. quality.
A video ADC acquires say 8 bits resolution at 4 msamples/sec.  This might
condense down to 4 Mbits/sec true negentropy, which is about half the rate of regular Ethernet.
   "Speech is not protected simply because it is written in a language"

@_date: 1998-08-01 16:39:39
@_author: David Honig 
@_subject: MAKE ENTROPY CHEAP!!! 
Producing True Random Numbers at 44Kbits/sec on a common PC
I found that FM radio hiss can be digitized and processed to yield true,
quality random numbers which pass the Diehard randomness tests.
A cheap FM radio was tuned to hiss at the high end of the FM band.  The radio's earphone-out signal was fed into ordinary computer soundcard and
digitized at 16bits/sample at 22Ksamples/sec.  Various spectral-analysis programs showed that the FM hiss was fairly white
analog noise, however the raw data did not pass Diehard tests.
I then processed the raw data: The parity of each raw byte
was shifted into a register until a byte accumulated, which was then output.
The resulting data, 1/8 the size of the raw data, passes Diehard randomness
tests with
no additional processing.
Diehard output:
 BIRTHDAY SPACINGS TEST, M= 512 N=2**24 LAMBDA=  2.0000
           bigsimple.bin   using bits  1 to 24 p-value=  .215128
           bigsimple.bin   using bits  2 to 25 p-value=  .749499
           bigsimple.bin   using bits  3 to 26 p-value=  .096196
           bigsimple.bin   using bits  4 to 27 p-value=  .658912
           bigsimple.bin   using bits  5 to 28 p-value=  .017458
           bigsimple.bin   using bits  6 to 29 p-value=  .574795
           bigsimple.bin   using bits  7 to 30 p-value=  .057843
           bigsimple.bin   using bits  8 to 31 p-value=  .755589
           bigsimple.bin   using bits  9 to 32 p-value=  .088504
   The 9 p-values were
        .215128   .749499   .096196   .658912   .017458
        .574795   .057843   .755589   .088504
  A KSTEST for the 9 p-values yields  .896831

@_date: 1998-08-02 14:02:44
@_author: David Honig 
@_subject: commercial cryptanalysis software 
Seems to be commercial TLA tools.
   "Speech is not protected simply because it is written in a language"
