
@_date: 1993-05-24 11:59:53
@_author: 1 
@_subject: Steganography and Steganalysis 
Summary: Steganography is essential for private communication since
 	well-encrypted messages stand out too easily and no "solidarity"
 Note: This is my "newbie" post to cypherpunks.  It asks many
Failed PGP Social Program
In his introduction to PGP, Phil Zimmerman compares plaintext messages
to mail sent on postcards and encrypted messages to mail in sealed
envelopes.  Currently, using envelopes does not arouse suspicion
because almost everyone uses envelopes, but using encryption does
arouse suspicion because almost nobody uses encryption.  Zimmerman's
proposed solution is for almost everyone to use encryption routinely,
so that encrypted messages will be the norm.
I do not believe that this will succeed, at least not in the way
Zimmerman hopes.  Even though PGP is highly regarded, free, and
fairly readily accessible, no "solidarity" of PGP users will arise
unless email with PGP encryption becomes transparently convenient to
use and also does not invite civil lawsuits or criminal charges.  (An
RSAREF version of PGP would help, though.)  The kinds of encryption
that *will* become readily available, easy-to-use, and legally
hassle-free will be the crippled kinds of encryption.  Encryption
that is not crippled always will be suspect, perhaps illegal.
By using sufficiently intelligent steganographic techniques, however,
we will not need any "solidarity" from other people at all.  If our
"envelopes" look like "postcards," they will not arouse the
Steganography and Steganalysis
A few people have experimented with inserting messages into image
files.  But most of our email traffic is text, so I am most interested
in steganographic techniques for normal English prose.  Furthermore, we
need to have a reasonably high efficiency for inserting the hidden
message while not contorting the text too far from normal.  Peter
Wayner's Mimic functions for producing a baseball game commentary are
notable.  (No, I still haven't done the C conversion of the Think
Pascal version I received almost two years ago.  But I haven't
forgotten!)  I am not certain how efficiently his program encodes the
hidden message, but I do want the resulting text to be less
conspicuous.  Imagine thousands of messages per day consisting of
similar sounding commentary on the Whappers and the Blogs!  That's too
obvious.  Gus Simmons [CRYPTO83] has described subliminal messages,
which certainly are suitably innocuous, but unfortunately far too low
A good steganographic system should insert encrypted messages into
English text so unobtrusively that nobody but the intended receiver
can show that a hidden message exists, even if the algorithm for the
steganographic system is made public.  (Perhaps I should call this
"stealthography"?)  The examples of steganography described in [KAHN]
all fail this test.  Similarly, so do silly kinds of "steganography"
such as the following "SECRET":
      So how have you been doing?
      Everything is fine here.
      Can we visit soon?
      Remember when we went white-water rafting?
      Everyone got soaked!
      That would be fun to do again!
This is silly not only because the hidden message is not encrypted but
also because anyone who knows the insertion algorithm can readily
discover that a hidden message does indeed exist.
To create a good cryptographic system, one must first do cryptanalysis.
Similarly, I suggest that to create a good steganographic system, one
should first do steganalysis.  For that reason, the next section of
this message focuses on potential tools for steganalysis.  Perhaps
people more knowledgeable about steganalysis will tell how best to make
use of these, and other, tools for steganalysis.
Disclaimer: I admit that my knowledge of steganalysis is limited.
Perhaps at this point I should just ask what I should read to learn
more about this, but I suspect that the public literature is
sparse and scattered.  For example, we have the words "encryption" and
"decryption", but what do we call the corresponding words for
steganography: steganization and desteganization?  If we don't even
have good terminology for the process, I suspect that we do not have
much well-organized literature on it, either.  What follows is my
best guess concerning steganographic issues.
The first goal of steganalysis is to determine that a hidden message is
likely.  The second goal is extracting that hidden message and the
third goal is decrypting that hidden message.  To be able to infer that
a hidden message is likely, we need measures that distinguish normal
from unusual English text.
Measures of Normal English Text
What is normal English text?  In general, this is unsolvable, and not
even well-defined.  It depends on the context, author, subject, etc.
Nevertheless, I can think of several kinds measures that are likely
to be useful and I hope that other people can suggest more.
(1) letter frequency
    Letter frequency is just the first order Markov model for English.
    Shannon showed how 2nd order, 3rd order, etc. Markov models enable
    increasingly English-like output from a memoryless source.
    How much deviation from these standard frequencies is normal?
    What other kinds of letter frequency-related statistics might
    be useful?  For example, if you measure the number of characters
    between each occurrence of a particular character, what type of
    distribution of intervals should you get?  (An exponential
    distribution?  A Poisson distribution?  An Erlang distribution?)
(2) word frequency
    Shannon also constructed 1st order, 2nd order, etc. Markov
    approximations to English using words rather than characters as
    the elements.  How much variation should we expect from these
    approximations in ordinary English?
    Zipf's Law [WELSH, p. 97] states that the word frequency for a
    language obeys the formula:
       p(n) = A / n
    where A is a constant chosen so that:
       SUM p(n) = 1
        n
    For example, in English, the most frequently used words are,
    in order, "the", "of", "and", and "to".  According to Zipf's Law,
    the word "the" should be used about twice about as often as the
    word "of" and about four times as often as the word "to".
    Mandelbrot suggested a more complex formula:
        p(n) = A / (n + V)^(1/D)
    where V and D are independent parameters.  I suppose that the
    intelligence agencies have even more sophisticated models.
(3) compressibility
    According to [WELSH, p. 96], Shannon's experiments measured the
    entropy of English (over a 26 letter alphabet plus a space) as
    only 0.6 to 1.3 bits per character.  Since normal English text
    has both upper and lower case, digits, and other characters,
    perhaps a better value for normal English is about 2.5 bits per
    character. (If so, then shouldn't compression programs be able
    to achieve about a factor of 8 / 2.5 > 3 compression?)
    Is "dense" writing less compressible than "fluff"?  Apparently
    so, since measurements of the redundancy of various English
    texts [WELSH, p. 100] show significant differences.
    Since well-encrypted messages are incompressible, will a message
    that hides an encrypted message be less compressible than normal
    English text?
(4) grammar, style, and readability
    Grammar checkers can distinguish normal sentences from text such as:
        "Distinguish normal can grammar checkers text sentences from."
    that may satisfy other statistics for normal English text.  But what
    is an ordinary distribution of legal grammars of English sentences?
    Also, how does one allow for the different conventions in formal,
    written English vs. conversational English vs. slang vs. email/USENET
    netspeak vs. special sublanguages such as computer languages or
    mathematics?  Bear in mind that netspeak has several distinguishing
    features.  For example, email addresses of the form xxx
    quoted text with a ">" in column 1, and smilies are typical net
    conventions. Mail headers and signatures (especially PGP signatures)
    have a special structure, too.
    Can a grammar checker help to distinguish normal text from text
    that may have a hidden message?  What useful clues may style and
    (Kincaid, Coleman-Liau, Flesch, etc.) readability scores give?
    An interesting experiment would be to compare automated readability
    scores with the compressibility of the text.
(5) semantic continuity and logic
    Do the sentences in a paragraph relate somehow to each other,
    or are they separate, independent constructions?  How can that
    be measured automatically?
(6) message context
    Does the content of the message look normal in its context?
    (For example, a baseball play-by-play would look out-of-place
    in sci.med.)  How can that be measured automatically?
(7) obvious
    Some people are known suspects, no matter how innocuous-looking
      their messages are.  All their messages are suspect.
(8) other measures
    What other measures might be useful for detecting the likely
    presence of a hidden message?  The distribution of number
    of words in a sentence?  The distribution of number of sentences
    (or words) in a paragraph?
What programs and/or databases are readily available for making
these measures?
Steganographic Capacity of English Text
If the public English text is N characters long, how long can a
perfectly hidden message within that public text be?  I think that it
can be about N/10 characters long, for a steganographic capacity of 10%.
I will show two ways to hide information in the public text: (1) the
grammatical structure of the sentence and (2) the word choice in the
sentence.  (These are not the only methods, but they may be the two
best methods.)
Do you recall back in school when you "diagrammed" sentences in your
English class?  That was actually imposing a parenthesization on
the sentence.  For example, the sentence:
    The tall boy ate the big pie.
    (The (tall boy)) (ate (the (big pie)))
The number of possible parenthesizations of a sentence of N words is
related to the number of ways to match N pairs of parentheses.  The
number of matchings is the Nth Catalan number:
    X(N) = --------  >=  2           [AHU, p. 73]
where C(2N, N) is the number of combinations of 2N objects, taken
N at a time, which is (2N)!/(N!^2).  The number of parenthesizations
is the N-1st Catalan number.  If all parenthesizations were
equally likely, then the parenthesization of a sentence of N words
would give greater than (N-1)-2 = N-3 bits of information for
1 - 3/N bits per word.  (Of course, not all parenthesizations are
equally likely.  But X(N) is also much larger than 2^(N-2), so
for now I'll assume that those two roughly cancel out.)
Since the average word length in English is about 4 characters
[WELSH, p. 101], or 5 characters counting a separating space,
and each ASCII character has 8 bits, we get a steganographic
efficiency of (1 - 3/N) / 40.  (Notice that I am ignoring punctuation
in my count of characters in English text.  Since this count is just
a rough approximation anyway, the effect of punctuation should get
lost in the noise.)
Another way to hide information in the public text is with the choice
of words.  Since English has a large vocabulary, I think that almost
always we can get one bit of information per word, just from the word
choice alone.  (Unusual words should not be used often, though, since
normal English text does not use them often.)  For example, we might
XOR all the bits of all the characters of the word and use its parity.
Can we get two bits per word?  Probably most of the time.  Suppose
that we try to get two bits per word from our word choice but succeed
only with probability p.  The channel capacity of a BSC is:
    1 + p log p + (1-p) log (1-p)
which is:
    1 - H(p)
By Shannon's noiseless coding theorem, we should be able to achieve
an error correcting coding that approaches this capacity.  (Use of
that encoding unfortunately may alter the statistics of the hidden
message sufficiently to expose the use of steganography, however.)
For what values of p will it be worthwhile to insert an uncertain
two bits per word rather than a (nearly) certain one bit?  Since
H(0.11) = 0.5 (approximately), p had better be .89 or higher.  If p
is .95, then H(p) = .29 (approximately), giving 1.4 bits / word rather
than just 1 bit / word.  I doubt that we can get better than 1.4 bits
per word with this method and still have normal looking English, though,
because of Zipf's Law.  The normal frequencies of the four words
"the", "of", "and", and "to" are high, totalling at least 10%, so
the public text has to include many of them, whether we want their
particular parity bit patterns or not.  We can improve the efficiency
by attempting two bits of information only for the long words and
attempting only one bit for the short words.  Maybe we should attempt
to achieve |K/5| bits for words of K characters, where "|x|" means "x
rounded down to the next integer".  Or maybe we should not try to hide
any bits at all in the extremely short words.  I don't have enough
information about typical English to analyze that.
What is the total steganographic efficiency we achieve by exploiting
both the grammatical structure and the word choice?  My estimates total:
    ( (1 - 3/N) + 1.4 ) / 40  =  0.06 - 3/(40N)
Just to get a number, let's assume that N = 10 words per sentence.
That gives us 0.0525, which I'll round down to 0.05.  That actually
gives us much better than 5%, though, because the hidden message is
first compressed and then encrypted.  If compression halves the length
of the hidden message, we get effectively a 10% efficiency for the
Steganographic capacity of English.  This estimate will decrease by
whatever amount typical English parenthesization departs from uniform
over all possibilities but it will increase by improved exploitation
of word choice and, especially, by improved compression.  Of course,
the effectiveness of this camouflage depends on the sophistication
of one's model of English text.  Perhaps normal English has enough
variation that a good, but not perfect, model of English will yield
public text that is indistinguishable from normal text, even to the
more resourceful eavesdroppers.
                              INTERNET    kqb
                                 or       kevin_q_brown
AHU - The Design and Analysis of Computer Algorithms,
      Aho, Hopcroft, and Ullman, Addison-Wesley, 1974.
KAHN - The Codebreakers, David Kahn, Macmillan, 1967.
WELSH - Codes and Cryptography, Dominic Welsh, Claredon Press, 1988.

@_date: 1993-05-25 14:49:12
@_author: N-1 
@_subject: Re: Steganography and Steganalysis 
I have received some useful feedback to yesterday's message on
steganography and steganalysis.  Here are some clarifications
to my cryptic presentation and a correction.
I was most interested in finding if the steganographic capacity of
English is high enough to make steganography practical for everyday
use, so I didn't even address the meaningfulness of the output.
For example, if I could only produce a capacity of a tenth of one percent,
the meaningfulness would not even be an issue because nobody would
want to send large messages via steganography anyway.  A capacity of
10%, requiring the public text to be only 10 times as long as the
hidden text, may be good enough for everyday use.  If that can be
achieved, then the next step is to see if meaningful output can
have a high steganographic content.  If so, then I expect that
several cypherpunks would want to pursue that.  (FYI: I plan to do
more analysis on my own, even if nobody else does.)
My guesstimate for the steganographic capacity of English did not
provide a steganographic algorithm.  For example, I haven't even
looked into how to map a bit string to a parenthesis grouping;
I was just noting that if you have X(N-1) possibilities, there must
be log (X(N-1)) bits available, assuming all possibilities are equally
likely.  Is there a simple-to-compute mapping of the numbers 1 through
X(N-1) to the X(N-1) parenthesizations of an N word sentence?
Fortunately, N rarely gets large for ordinary English sentences,
so a general solution may be unnecessary.
My presentation mistakenly implied that a good steganographic algorithm
may have the form:
    E(K, M) = E2( E1(K,M) )
where E1 is a cryptographically secure encryption function with public
key K and hidden message M, E2 somehow converts the encrypted message to
ordinary English text, and E1, K, and E2 are publicly known.
Unfortunately, if the inverse of E2 (let's call it D2), is easily found,
then the presence of a hidden message can be detected easily, even though
that message cannot be decrypted easily.  This is because the output of E1,
which is incompressible, is easily distinguishable from
D2(ordinary English text).  Here is a better formulation for the
steganographic schema:
    E(K1, K2, M) = E3( E2(K2, E1(K1,M) ))
  E1(K1,M) converts the hidden message M to a cryptographically secure
      cyphertext by using the key K1.  E1 and K1 are public, but the
      decryption function D1 is difficult to compute without the
      private key PK1.
  E3(C) converts a bit string to ordinary-looking English text.
      Assume that both E3 and its inverse D3 are public.
  E2(K2, C) converts the cyphertext C into another bit string such
      that E2(K2, C) has the same statistical characteristics as
      D3(ordinary English text).  Assume that E2 and K2 are public,
      but D2 is difficult to compute without the private key PK2.
Function E1 is normal public key cryptography, which produces an
incompressible cyphertext.  I hope that function E3 has a high enough
steganographic capacity to make steganalysis worthwhile.  Function E2
cannot be a normal encryption function because its output needs to be
as compressible as D3(ordinary English text).  Both functions E2 and E3
are new types of functions that require more research to work well.
I still haven't seen any references to this type of steganography
being done before, but thanks to the various people who gave pointers
to tools that may help in building it.
                              Kevin Q. Brown
                              INTERNET    kqb
                                 or       kevin_q_brown

@_date: 1993-06-17 15:16:40
@_author: slightly larger 
@_subject: Re: Weak steganography 
Hal Finney said:
I am building a "steganosaurus" and eventually will need to solve a
similar problem.  (A "steganosaurus" applies a primitive steganographic
technique to English text by using a thesaurus to generate enough word
variation to encode a hidden message.)  One of the weaknesses of this
"steganosaurus" is that the resulting output has statistical differences
from normal English text.  For example, word frequency will be skewed.
Worse, I have to assume that the eavesdropper knows my steganization
algorithm and can "desteganize" any innocuous-looking text I produce.
That "desteganized" text will show clearly the existence of a hidden,
encrypted message because, as Hal pointed out, it has a uniform histogram.
What I want is a program that will transform an encrypted file to a
(slightly larger) file that mimics the distribution achieved by
applying the "desteganization" algorithm to normal English text
that does *not* contain any hidden message.  The steganization
algorithm then gets applied to this stealthy, mimic file, not
directly to the encrypted hidden message.  By the way, since we
must assume that the eavesdropper knows all our algorithms but
not our secret keys, this algorithm will require a *second* secret
key in addition to the secret key used in the original encryption.
I'm not ready to tackle that yet.  Unless I hear otherwise, I'll
assume that if anyone knows how to achieve this, they're not telling...
                              Kevin Q. Brown
                              INTERNET    kqb
                                 or       kevin_q_brown
PS: I found that a simple, semi-automatic algorithm can generate a
    public message only 5 to 10 times as long as the hidden message.
    Unfortunately, the public message from my simple algorithm is
    almost always a bizarre, disconnected sequence of rants, which,
    for most people, is not normal.  That is why I am building my
    "steganosaurus".  After that I will see if combining a natural
    language parser with transformational grammars can produce a
    less primitive, more efficient "trans-steganosaurus".

@_date: 1993-11-30 18:07:51
@_author: Otherwise you are just relying on security through obscurity. 
@_subject: Re: Statistics of Low-Order Bits in Images 
Several people are attempting to create an algorithm to mask the
presence of a steganized encrypted message in the least significant
bits of an image.  Don't forget that no matter how fancy your algorithm
or how closely you mask your steganography with a model of what the
statistics of an ordinary image look like, you have to assume that your
opponent also knows your steganization algorithm, including your masking
technique.  (Otherwise you are just relying on security through obscurity.)
This leaves you with three problems:
  (1) your opponent may have a much better model of an ordinary
      image than you do, and still be able to discern the existence
      of masked steganography,
  (2) since your opponent knows your steganization algorithm,
      he/she can look for any "signature" that your steganography
      masking model leaves, and
  (3) your opponent can "desteganize" all your images and check their
      statistics for deviations from the statistics for "desteganized"
      ordinary images.
Resolving problems (1) and (2) requires a lot of work constructing
good models.  Resolving problem (3) requires, I think, a modeling
function for steganography that is invertible only with a secret key.
(Otherwise, your opponent could desteganize your image and find a
uniform random distribution, which indicates an encrypted message.)
Since this type of function is, to my knowledge, not well-developed,
don't expect it to be secure.  Thus, if breaking it could compromise your
secret key for desteganization, then don't use the same public/private
key pair for both encryption and steganography.
                              Kevin Q. Brown
                              INTERNET    kqb
                                 or       kevin_q_brown

@_date: 1993-12-14 16:20:29
@_author: Miburi-san 
@_subject: Re: Signing pictures -- how hard, how long? 
Alan (Miburi-san) Wexelblat  said:
If your signature does not include the low order bits of your image,
then someone could embed a secret message in those low order bits
(via Romana Machado's "Stego," for example) and your signature still
would be valid.  I wouldn't want my signature over someone else's
steganized message.
I'm sure there's a simple fix for that, such as ensuring enough bit rot
to blow away any but the most error-tolerant steganography or including
a disclaimer of responsibility for the low-order bits, but I couldn't
resist pointing it out.
                              Kevin Q. Brown
                              INTERNET    kqb
                                 or       kevin_q_brown

@_date: 1995-09-08 15:47:10
@_author: Kevin Q Brown +1 201 386 7344 
@_subject: GAK Hacks and Position Surveillance 
That's a start.  Superencryption can protect the _content_ of the
conversation, but it will not prevent _traffic analysis_.  That is
an important issue because, as I explain below, in our increasingly
wired world, effective traffic analysis may become a _position_
escrow system, except that there won't even be any escrow.
A GAK Hack that combines superencryption with a method to defeat traffic
analysis would raise a lot more eyebrows than superencryption alone.
Unfortunately, since we don't yet know what kind of LEAFs will be in
the next-generation GAK proposal, I can only refer to some comments
made awhile ago about Clipper-based traffic analysis:
  Date: Mon, 14 Mar 94 10:36:05 EST
  From: smb
  > The LEAF can be decrypted with just the family key; from what's been
  > disclosed so far, local law enforcement agents will be able to do that
  > without contacting the escrow sites.  The LEAF contains the unit id of
  > the chip, independent of what phone number it's being used from, ...
Imagine someone using a GAK/LEAF communication device while travelling
throughout the day.  Especially if the communications are wireless,
no court order will be needed to track position during his/her journeys
because a packet sniffer armed with the family key could detect any
of his/her communications automatically.  You may wonder "what packet
sniffer could track communications like that"?  Maybe I'm wrong, but
isn't that what the recent Digital Telephony legislation was for?
Now let's return to a recent message from tcmay
Or perhaps, once a GAK system with some kind of LEAFs is in place,
no justification at all will be needed to accomplish efficient and
fully automated massive position surveillance.  Maybe key escrow is
just a red herring to distract us from position surveillance?
Of course, we can assume that these LEAFs will not be as vulnerable
to forging as Matt Blaze demonstrated for Clipper (Tessera?).
First we had:
  GAK = Government Access to Keys.
Perhaps now we have:
  GULPS = Government Unlimited License for Position Surveillance?
Frankly, I wouldn't be surprised if I have overstated the threat
and more technically knowledgeable minds on this list will expose
the flaws in my reasoning.  Please do.
                              Kevin Q. Brown
                              kevin.q.brown
                              kqb
