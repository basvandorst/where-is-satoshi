
@_date: 2014-07-18 17:51:06
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [Bitcoin-development] Decentralizing ming 
I thought I'd chime in and point out some research results that might help.
Even if they don't, there is a cool underlying technique that some of you
might find interesting.
The problem being tackled here is very similar to "set reconciliation,"
peer A thinks that the set of transactions that should be in the block is
and peer B has actually included set S_B, and S_A and S_B are expected
to not differ much. Ideally, one would like the communication complexity
between A and B to be O(delta), not O(S_B) as it is right now. And ideally,
one would like B to send a single message to A, and for A to figure out the
difference between the two sets, without any lengthy back and forth
communication. In essence, I would like to give you some magical packet
that is pretty small and communicates just the delta between what you and
I know.
This paper from Cornell describes a scheme for achieving this:
   Yaron Minsky, Ari Trachtenberg, Richard Zippel: Set reconciliation with
nearly optimal communication complexity. IEEE Transactions on Information
Theory 49(9): 2213-2218 (2003)
   Those of you looking for a TL;DR should read the intro and then skip to
page 8 for the example. The underlying trick is very cool, comes from the
peer-to-peer/gossip literature, and it is underused. It'd be really cool if
could be applied to this problem to reduce the size of the packets.
This approach has three benefits over the Bloom filter approach (if I
understand the Bloom filter idea correctly):
(1) Bloom filters require packets that are still O(S_A),
(2) Bloom filters are probabilistic, so require extra complications
when there is a hash collision. In the worst case, A might get confused
about which transaction B actually included, which would lead to a
fork. (I am not sure if I followed the Bloom filter idea fully -- this may
not happen with the proposal, but it's a possibility with a naive Bloom
filter implementation)
(3) Bloom filters are interactive, so when A detects that B has included
some transactions that A does not know about, it has to send a message
to figure out what those transactions are.
Set reconciliation is O(delta), non-probabilistic, and non-interactive. The
naive version requires that one have some idea of the size of the delta,
but I think the paper has some discussion of how to handle the delta
I have not gone through the full exercise of actually applying this trick to
the Bitcoin p2p protocol yet, but wanted to draw your attention to it.
If someone is interested in applying this stuff to Bitcoin, I'd be happy
to communicate further off list.
- egs

@_date: 2014-07-18 17:54:14
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
I thought I'd chime in and point out some research results that might help.
Even if they don't, there is a cool underlying technique that some of you
might find interesting.
The problem being tackled here is very similar to "set reconciliation,"
peer A thinks that the set of transactions that should be in the block is
and peer B has actually included set S_B, and S_A and S_B are expected
to not differ much. Ideally, one would like the communication complexity
between A and B to be O(delta), not O(S_B) as it is right now. And ideally,
one would like B to send a single message to A, and for A to figure out the
difference between the two sets, without any lengthy back and forth
communication. In essence, I would like to give you some magical packet
that is pretty small and communicates just the delta between what you and
I know.
This paper from Cornell describes a scheme for achieving this:
   Yaron Minsky, Ari Trachtenberg, Richard Zippel: Set reconciliation with
nearly optimal communication complexity. IEEE Transactions on Information
Theory 49(9): 2213-2218 (2003)
   Those of you looking for a TL;DR should read the intro and then skip to
page 8 for the example. The underlying trick is very cool, comes from the
peer-to-peer/gossip literature, and it is underused. It'd be really cool if
could be applied to this problem to reduce the size of the packets.
This approach has three benefits over the Bloom filter approach (if I
understand the Bloom filter idea correctly):
(1) Bloom filters require packets that are still O(S_A),
(2) Bloom filters are probabilistic, so require extra complications
when there is a hash collision. In the worst case, A might get confused
about which transaction B actually included, which would lead to a
fork. (I am not sure if I followed the Bloom filter idea fully -- this may
not happen with the proposal, but it's a possibility with a naive Bloom
filter implementation)
(3) Bloom filters are interactive, so when A detects that B has included
some transactions that A does not know about, it has to send a message
to figure out what those transactions are.
Set reconciliation is O(delta), non-probabilistic, and non-interactive. The
naive version requires that one have some idea of the size of the delta,
but I think the paper has some discussion of how to handle the delta
I have not gone through the full exercise of actually applying this trick to
the Bitcoin p2p protocol yet, but wanted to draw your attention to it.
If someone is interested in applying this stuff to Bitcoin, I'd be happy
to communicate further off list.
- egs

@_date: 2014-07-18 17:54:36
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [Bitcoin-development] Decentralizing ming 
My apologies for posting to the wrong thread.
On Fri, Jul 18, 2014 at 5:51 PM, Emin G?n Sirer

@_date: 2014-07-18 23:06:21
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
Agreed. Yaron's scheme is magical because it is non-interactive. I send you
a packet of O(expected-delta) and you immediately figure out the delta
without further back and forth communication, each requiring an RTT.
FEC schemes are both fairly complex, because the set is constantly
changing, and (if i understand your suggestion correctly) they add
additional metadata overhead (albeit mostly during tx propagation). Set
reconciliation is near optimal.
In any case, I have no horse here (I think changing the client so it's
multithreaded is the best way to go), but Yaron's work is pretty cool and
may be applicable.
- egs

@_date: 2015-12-02 13:57:46
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and 
Thanks Peter for the careful, quantitative work.
I want to bring one additional issue to everyone's consideration, related
to the choice of the Lempel-Ziv family of compressors.
While I'm not familiar with every single compression engine tested, the
Lempel-Ziv family of compressors are generally based on "compression
tables." Essentially, they assign a short unique number to every new
subsequence they encounter, and when they re-encounter a sequence like "ab"
in "abcdfdcdabcdfabcdf" they replace it with that short integer (say, in
this case, 9-bit constant 256). So this example sequence may turn into
"abcdfd<258 for cd><256 for ab><258 for cd>f<261 for abc><259 for df>"
which is slightly shorter than the original (I'm doing this off the top of
my head so the counts may be off, but it's meant to be illustrative). Note
that the sequence "abc" got added into the table only after it was
encountered twice in the input.
This is nice and generic and works well for English text where certain
letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc) are
repeated often, but it is nowhere as compact as it could possibly be for
mostly binary data -- there are opportunities for much better compression,
made possible by the structured reuse of certain byte sequences in the
Bitcoin wire protocol.
On a Bitcoin wire connection, we might see several related transactions
reorganizing cash in a set of addresses, and therefore, several reuses of a
20-byte address. Or we might see a 200-byte transaction get transmitted,
followed by the same transaction, repeated in a block. Ideally, we'd learn
the sequence that may be repeated later on, all at once (e.g. a Bitcoin
address or a transaction), and replace it with a short number, referring
back to the long sequence. In the example above, if we knew that "abcdf"
was a UNIT that would likely be repeated, we would put it into the
compression table as a whole, instead of relying on repetition to get it
into the table one extra byte at a time. That may let us compress the
original sequence down to "abcdfd<257 for cd><256 for abcdf><256 for
abcdf>" from the get go.
Yet the LZ variants I know of will need to see a 200-byte sequence repeated
**199 times** in order to develop a single, reusable, 200-byte long
subsequence in the compression table.
So, a Bitcoin-specific compressor can perhaps do significantly better, but
is it a good idea? Let's argue both sides.
On the one hand, Bitcoin-specific compressors will be closely tied to the
contents of messages, which might make it difficult to change the wire
format later on -- changes to the wire format may need corresponding
changes to the compressor.  If the compressor cannot be implemented
cleanly, then the protocol-agnostic, off-the-shelf compressors have a
maintainability edge, which comes at the expense of the compression ratio.
Another argument is that compression algorithms of any kind should be
tested thoroughly before inclusion, and brand new code may lack the
maturity required. While this argument has some merit, all outputs are
verified separately later on during processing, so
compression/decompression errors can potentially be detected. If the
compressor/decompressor can be structured in a way that isolates bitcoind
from failure (e.g. as a separate process for starters), this concern can be
The nature of LZ compressors leads me to believe that much higher
compression ratios are possible by building a custom, Bitcoin-aware
compressor. If I had to guess, I would venture that compression ratios of
2X or more are possible in some cases. In some sense, the "O(1) block
propagation" idea that Gavin proposed a while ago can be seen as extreme
example of a Bitcoin-specific compressor, albeit one that constrains the
order of transactions in a block.
Compression can buy us some additional throughput at zero cost, modulo code
Given the amount of acrimonious debate over the block size we have all had
to endure, it seems
criminal to leave potentially free improvements on the table. Even if the
resulting code is
deemed too complex to include in the production client right now, it would
be good to understand
the potential for improvement.
How to Do It
If we want to compress Bitcoin, a programming challenge/contest would be
one of the best ways to find the best possible, Bitcoin-specific
compressor. This is the kind of self-contained exercise that bright young
hackers love to tackle. It'd bring in new programmers into the ecosystem,
and many of us would love to discover the limits of compressibility for
Bitcoin bits on a wire. And the results would be interesting even if the
final compression engine is not enabled by default, or not even merged.

@_date: 2015-12-12 21:01:45
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness 
, /,
   / ,
,,,  . // .,      .
_. ...  ..   ._.
,    _
  , , ...     _  _.
.  ,.,    _.
.,    ,  ..
.  .
,     ,    ,   /..,,
.     .
.,. _.. ,
.. _
   ..
,.,, _
, _
. ,
   / . ,.
  ,
. ,
, .,   ,. ._ ,  ,,,//
,        ,
  . . ,
, //  .
,  ,
      _,.
, . ,, .
  /,/ .
  .   .,,_//
.,  .
.  /_. ,
  /
,, / .
   . _ ,
,  ,
, ,,, ..  ,
  ,
  /.,.
  /. /
. ,/  ,
. .   /,
   ,/.
, .,,, , ,    , ,
,.   ,.,.  .
,  .    ,.  .,   ,
  ,.,. ,
, _ _ ,
. ,,   ,  _
  ,
__ /
!;"$'''. b
    __
On Sat, Dec 12, 2015, 3:01 PM Jorge Tim?n <

@_date: 2015-12-20 00:12:49
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
There's quite a bit of confusion in this thread.
Peter is referring to block withholding attacks. Ittay Eyal (as sole
author -- I was not involved in this paper [1]) was the first
to analyze these attacks and to discover a fascinating, paradoxical
result. An attacker pool (A) can take a certain portion of its hashpower,
use it to mine on behalf of victim pool (B), furnish partial proofs of work
to B, but discard any full blocks it discovers. If A picks the amount of
attacking hashpower judiciously, it can make more money using this
attack, than it would if it were to use 100% of its hashpower for its own
mining. This last sentence should sound non-sensical to most of you,
at least, it did to me. Ittay did the math, and his paper can tell you
exactly how much of your hashpower you need to peel off and use
to attack another open pool, and you will come out ahead.
Chris Priest is confusing these attacks with selfish mining, and further,
his characterization of selfish mining is incorrect. Selfish Mining is
guaranteed to yield profits for any pool over 33% (as a result, Nick
Szabo has dubbed this the "34% attack") and it may pay off even
below that point if the attacker is well-positioned in the network;
or it may not, depending on the makeup of the rest of the pools
as well as the network characteristics (the more centralized
and bigger the other pools are, the less likely it is to pay off). There
was a lot of noise in the community when the SM paper came out,
so there are tons of incorrect response narrative out there. By now,
everyone who seems to be Bitcoin competent sees SM as a
concern, and Ethereum has already adopted our fix. I'd have hoped
that a poster to this list would be better informed than to repeat the
claim that "majority will protect Bitcoin" to refute a paper whose title
is "majority is not enough."
Back to Ittay's paradoxical discovery:
We have seen pool-block withholding attacks before; I believe Eligius
caught one case. I don't believe that any miners will deploy strong KYC
measures, and even if they did, I don't believe that these measures
will be effective, at least, as long as the attacker is somewhat savvy.
The problem with these attacks are that statistics favor the attackers.
Is someone really discarding the blocks they find, or are they just
unlucky? This is really hard to tell for small miners. Even with KYC,
one could break up one's servers, register them under different
people's names, and tunnel them through VPNs.
Keep in mind that when an open pool gets big, like GHash did and
two other pools did before them, the only thing at our disposal used
to be to yell at people about centralization until they left the big
pools and reformed into smaller groups. Not only was such yelling
kind of desperate looking, it wasn't incredibly effective, either.
We had no protocol mechanisms that put pressure on big pools to
stop signing up people. Ittay's discovery changed that: pools that
get to be very big by indiscriminately signing up miners are likely to
be infiltrated and their profitability will drop. And Peter's post is
evidence that this is, indeed, happening as predicted. This is a
good outcome, it puts pressure on the big pools to not grow.
Peter, you allude to a specific suggestion from Luke-Jr. Can you
please describe what it is?
Hope this is useful,
- egs
[1]

@_date: 2015-12-20 02:56:03
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Initial reactions aren't always accurate, people's views change, and
science has its insurmountable way of convincing people. Gavin [1]
and others [2] now cite selfish mining as a concern in the block size
debate, and more importantly, the paper has been peer-reviewed,
cited, and even built-upon [3].
Let's elevate the discussion, shall we?
[1] Here's Gavin concerned about selfish mining:
[2] Here's Adam:
[3] This is a very nice extension of our work:
Ayelet Sapirshtein, Yonatan Sompolinsky, Aviv Zohar:

@_date: 2015-12-20 12:00:37
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Indeed, there are lots of weak measures that one could employ against
an uninformed attacker. As I mentioned before, these are unlikely to be
effective against a savvy attacker, and this is a good thing.
KYC has a particular financial-regulation connotation in Bitcoin circles,
of which I'm sure you're aware, and which you're using as a spectre.
You don't mean government-regulated-KYC a la FINCEN and Bitcoin
exchanges like Coinbase, you are just referring to a pool operator
demanding to know that its customer is not coming from its competitors'
data centers.
And your prediction doesn't seem well-motivated or properly justified.
There are tons of conditionals in your prediction, starting with the premise
that every single open pool would implement some notion of identity
checking. I don't believe that will happen. Instead, we will have the bigger
pools become more suspicious of signing up new hash power, which is a
good thing. And we will have small groups of people who have some reason
for trusting each other (e.g. they know each other from IRC, conferences,
etc) band together into small pools. These are fantastic outcomes for
Secondly, DRM tech can also easily be used to prevent block withholding
DRM is a terrible application. Once again, I see that you're trying to use
three letters as a spectre as well, knowing that most people hate DRM, but
keep in mind that DRM is just an application -- it's like pointing to Adobe
to taint all browser plugins.
The tech behind DRM is called "attestation," and it provides a technical
capability not possible by any other means. In essence, attestation can
ensure that
a remote node is indeed running the code that it purports to be running.
most problems in computer security and distributed systems stem from not
knowing what protocol the attacker is going to follow, attestation is the
technology we have that lets us step around this limitation.
It can ensure, for instance,
  - that a node purporting to be Bitcoin Core (vLatest) is indeed running an
unadulterated, latest version of Bitcoin Core
  - that a node claiming that it does not harvest IP addresses from SPV
clients indeed does not harvest IP addresses.
  - that a cloud hashing outfit that rented out X terahashes to a user did
indeed rent out X terahashes to that particular user,
  - that a miner operating on behalf of some pool P will not misbehave and
discard perfectly good blocks
and so forth. All of these would be great for the ecosystem. Just getting
of the cloudhashing scams would put an end to a lot of heartache.
Right, it's not clear at all that yelling at people has much effect. As much
fun as I had going to that meeting with GHash in London to ask them to
back down off of the 51% boundary, I am pretty sure that yelling at large
open pools will not scale. We needed better mechanisms for keeping pools
in check.
And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
time when we should count our blessings, not work actively to render
them inoperable.
Currently a significant % of the hashing power - possibly a majority -
There are indeed solo miners out there who can attack the big open
pools. The loss of the biggest open pools would not be a bad outcome.
Pools >25% pose a danger, and the home miner doesn't need a pool
Thanks, this requires a change to the Bitcoin PoW. Good luck with that!
Once again, this suggestion would make the GHash-at-51% situation
possible again. Working extra hard to re-enable those painful days
sounds like a terrible idea.
- egs

@_date: 2015-12-26 23:33:25
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
miner who finds the block a bonus.
of the game theory math?
Yes, Section 8.D. in Ittay's paper discusses this countermeasure, as well
as a few others:
    - egs

@_date: 2015-12-28 14:30:14
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
This is not quite right: we know that selfish mining is a guaranteed win
at 34%. We do not know when exactly it begins to pay off. The more
consolidated and centralized the other mining pools, the less of a threat
it is below 34%; the more decentralized, the more likely it is to pay off
at lower thresholds.
Far more concerning is network propagation effects between large and
On a related note, the Bitcoin-NG paper took a big step towards moving
these kinds of concerns out of the realm of gut-feelings and wavy hands
into science. In particular, it introduced metrics for fairness (i.e.
rate in orphans experienced by small and large miners), hash power
efficiency, as well as consensus delay.
Indeed, there is a slight, quantifiable benefit to larger pools. Which is
we need to be diligent about not letting pools get too big.
with block withholding, though AFAIK Eligius is the only one who has
I can see why they don't want to go public with this: it means that they
are less profitable than other pools.
It still looks to me like Ittay's discovery is doing exactly the right
this pool will need to be more careful when signing up new people,
curbing its otherwise steady march towards the 51% boundary.
- egs
- egs

@_date: 2015-12-30 15:08:36
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] How to preserve the value of coins after a fork. 
Ittay Eyal and I just put together a writeup that we're informally calling
Bitcoin-United for preserving the value of coins following a permanent fork:
Half of the core idea is to eliminate double-spends (where someone spends a
UTXO on chain A and the same UTXO on chain B, at separate merchants) by
placing transactions from A on chain B, and by taking the intersection of
transactions on chain A and chain B when considering whether a payment has
been received.
The other half of the core idea is to enable minting of new coins and
collection of mining fees on both chains, while preserving the 21M maximum.
This is achieved by creating a one-to-one correspondence between coins on
one chain with coins on the other.
Given the level of the audience here, I'm keeping the description quite
terse. Much more detail and discussion is at the link above, as well as the
assumptions that need to hold for Bitcoin-United.
The high bit is that, with a few modest assumptions, it is possible to
create a cohesive coin in the aftermath of a fork, even if the core devs
are split, and even if one of the forks is (in the worst case) completely
non-cooperative. Bitcoin-United is a trick to create a cohesive coin even
when there is no consensus at the lowest level.
Bitcoin-United opens up a lot of new, mostly game-theoretic questions: what
happens to native clients who prefer A or B? What will happen to the value
of native-A or native-B coins? And so on.
We're actively working on these questions and more, but we wanted to share
the Bitcoin-United idea, mainly to receive feedback, and partly to provide
some hope about future consensus to the community. It turns out that it is
possible to craft consensus at the network level even when there isn't one
at the developer level.
Happy New Year, and may 2016 be united,
- egs & ittay

@_date: 2015-12-30 15:22:43
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] How to preserve the value of coins after a fork. 
Bitcoin-United relies on a notion of transaction equivalence that doesn't
involve the transaction hash at all, so it should be immune to malleability
issues and compatible with segwit.
and result in the same outputs, not counting the miner fee. Simple
pay-to-pubkey-hash and pay-to-script-hash transactions are straightforward.
Multikey transactions are evaluated for equivalency by their inputs and
outputs, so it is allowable for a 2-out-of-3 payment to be signed by one
set of two keys on Dum and another set of two keys on Dee, as long as the
transaction consumes the same coins and produces the same outputs. Not that
we'll ever encounter such a case, but making this point helps pedagogically
with getting across the notion of transaction equivalence. What counts are
the consumed inputs and the destination and amounts of the outputs."
But you're right, if a naive implementation were to just use the
transaction hash, the result would be a mess.
- egs

@_date: 2015-05-20 06:25:01
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [Bitcoin-development] Virtual Notary. 
Hi everyone,
Given the recent discussions on projects that use the Bitcoin blockchain to
record factoids, people on this list might be interested in the Virtual
Notary project. Virtual Notary is essentially an online witness (aka
attestor) to online factoids. It can provide:
  * proof of Bitcoin funds (without revealing public addresses or fund
location on the blockchain)
  * proof of Bitcoin address ownership
  * proof of Tweet
  * proof of real estate value
  * proof of DNS ownership
  * proof of existence
  * proof of web page contents
  * proof of weather conditions
The factoids can be recorded on the blockchain (if you pay for the
transaction with Bitcoin or PayPal), or they can be part of a free
attestation chain that we maintain. The website provides a permanent URL to
the factoids it generates; it also provides an X.509 certificate that you
can download and keep safe in perpetuity, independent of the website.
The link to the website is here:
  The link to the writeup describing the various factoids and their use cases
is here:
  We are actively looking for people who are interested in developing the
service further. Specifically, if you have suggestions for how to extend
the service, for new proof/factoid types, or for how to build a business
case around the core idea, please let us know.
- egs

@_date: 2015-11-09 13:33:27
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
Hi everyone,
Thanks to everyone for a very friendly and scientifically-oriented
discussion. We have collated all the issues that have been raised related
to NG, and placed them in context, here:
    Overall, NG has a unique insight: turning the block creation process upside
down can provide many benefits. Most notably, throughput can go as high as
the network will allow, providing scalability benefits that increase as the
network improves. There are many other side benefits, including fast
confirmations that are stronger than 0-conf in Core, and come much more
quickly than Core's 1-confirmations. And there are ancillary benefits as
well, such as resilience to fluctuations in mining power, and healthier
incentives for participants to ferry transactions. We believe that a fresh
new permission-less blockchain protocol, designed today, would end up
looking more like NG than Core. Of course, if NG could possibly be layered
on top of Bitcoin, that would be the ultimate combination.
Many thanks for an interesting discussion, and as always, we're happy to
hear constructive suggestions and feedback,
- egs
On Wed, Oct 14, 2015 at 2:02 PM, Emin G?n Sirer

@_date: 2015-11-13 11:37:51
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] How to evaluate block size increase suggestions. 
By now, we have seen quite a few proposals for the block size increase.
It's hard not to notice that there are potentially infinitely many
functions for future block size increases. One could, for instance, double
every N years for any rational number N, one could increase linearly, one
could double initially then increase linearly, one could ask the miners to
vote on the size, one could couple the block size increase to halvings,
etc. Without judging any of these proposals on the table, one can see that
there are countless alternative functions one could imagine creating.
I'd like to ask a question that is one notch higher: Can we enunciate what
grand goals a truly perfect function would achieve? That is, if we could
look into the future and know all the improvements to come in network
access technologies, see the expansion of the Bitcoin network across the
globe, and precisely know the placement and provisioning of all future
nodes, what metrics would we care about as we craft a function to fit what
is to come?
To be clear, I'd like to avoid discussing any specific block size increase
function. That's very much the tangible (non-meta) block size debate, and
everyone has their opinion and best good-faith attempt at what that
function should look like. I've purposefully stayed out of that issue,
because there are too many options and no metrics for evaluating proposals.
Instead, I'm asking to see if there is some agreement on how to evaluate a
good proposal. So, the meta-question: if we were looking at the best
possible function, how would we know? If we have N BIPs to choose from,
what criteria do we look for?
To illustrate, a possible meta goal might be: "increase the block size,
while ensuring that large miners never have an advantage over small miners
that [they did not have in the preceding 6 months, in 2012, pick your time
frame, or else specify the advantage in an absolute fashion]." Or "increase
block size as much as possible, subject to the constraint that 90% of the
nodes on the network are no more than 1 minute behind one of the tails of
the blockchain 99% of the time." Or "do not increase the blocksize until at
least date X." Or "the increase function should be monotonic." And it's
quite OK (and probably likely) to have a combination of these kinds of
metrics and constraints.
For disclosure, I personally do not have a horse in the block size debate,
besides wanting to see Bitcoin evolve and get more widely adopted. I ask
because as an academic, I'd like to understand if we can use various
simulation and analytic techniques to examine the proposals.  A second
reason is that it is very easy to have a proliferation of block size
increase proposals, and good engineering would ask that we define the
meta-criteria first and then pick. To do that, we need some criteria for
judging proposals other than gut feeling.
Of course, even with meta-criteria in hand, there will be room for lots of
disagreement because we do not actually know the future and reasonable
people can disagree on how things will evolve. I think this is good because
it makes it easier to agree on meta-criteria than on an actual, specific
function for increasing the block size.
It looks like some specific meta-level criteria would help more at this
point than new proposals all exploring a different variants of block size
increase schedules.
- egs
P.S. This message is an off-shoot of this blog post:

@_date: 2015-10-14 14:02:15
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
Hi everyone,
We just released the whitepaper describing Bitcoin-NG, a new technique for
addressing some of the scalability challenges faced by Bitcoin.
Surprisingly, Bitcoin-NG can simultaneously increase throughput while
reducing latency, and do so without impacting Bitcoin's open architecture
or changing its trust model. This post illustrates the core technique:
     while the whitepaper has all the nitty gritty details:
     Fitting NG on top of the current Bitcoin blockchain is future work that we
think is quite possible. NG is compatible with both Bitcoin as is, as well
as Blockstream-like sidechains, and we currently are not planning to
compete commercially with either technology -- we see NG as being
complementary to both efforts. This is pure science, published and shared
with the community to advance the state of blockchains and to help them
reach throughputs and latencies required of cutting edge fintech
applications. Perhaps it can be adopted, or perhaps it can provide the
spark of inspiration for someone else to come up with even better solutions.
We would be delighted to hear your feedback.
- Ittay Eyal and E. G?n Sirer.

@_date: 2015-10-14 14:39:08
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
leader is,
Good point. If NG is layered on top of Bitcoin, we'd retain all of Bitcoin
as is. This would confer all the benefits of Bitcoin's retrospective
blocks, as well as add the ability to mint microblocks with low latency in
between. And despite the phrase "the leader," the actual leader in NG is a
key, not a specific node. That makes it possible to deter DDoS attacks by
dynamically migrating where in the network the leader is operating in
response to an attack. Finally, DDoS attacks against miners are already
possible, but they seem rare, and I suspect it's at least partly because of
the success of Matt Corallo's high speed bitcoin relay network. Similar
defenses can apply here.
- egs

@_date: 2016-02-26 11:05:20
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin Vaults. 
At the 3rd Bitcoin Workshop being held in conjunction with the Financial
Cryptography Conference in Barbados, my group will be presenting a new idea
for improving Bitcoin wallet security and deterring thefts today.
The write-up is here:
The paper with the nitty gritty details is here:
    The core idea:
Our paper describes a way to create vaults, special accounts whose keys can
be neutralized if they fall into the hands of attackers. Vaults are
Bitcoin?s decentralized version of you calling your bank to report a stolen
credit card -- it renders the attacker?s transactions null and void. And
here?s the interesting part: in so doing, vaults demotivate key theft in
the first place. An attacker who knows that he will not be able to get away
with theft is less likely to attack in the first place, compared to current
Bitcoin attackers who are guaranteed that their hacking efforts will be
handsomely rewarded.
Operationally, the idea is simple. You send your money to a vault address
that you yourself create. Every vault address has a vault key and a
recovery key. When spending money from the vault address with the
corresponding vault key, you must wait for a predefined amount of time
(called the unvaulting period) that you established at the time you created
the vault -- say, 24 hours. When all goes well, your vault funds are
unlocked after the unvaulting period and you can move them to a standard
address and subsequently spend them in the usual way. Now, in case Harry
the Hacker gets a hold of your vault key, you have 24 hours to revert any
transaction issued by Harry, using the recovery key. His theft,
essentially, gets undone, and the funds are diverted unilaterally to their
rightful owner. It?s like an ?undo? facility that the modern banking world
relies on, but for Bitcoin.
The technical trick relies on a single new opcode, CheckOutputVerify, that
checks the shape of a redeem transaction. Note that fungibility is not
affected, as the restrictions are at the discretion of the coin owner alone
and can only be placed by the coin owner ahead of time.
We suspect that this modest change could actually be a game-changer for
bitcoin security: clients and keys are notoriously hard to secure, and a
facility that allows you to possibly recover, and if not, permanently keep
the hacker from acquiring your funds, could greatly deter Bitcoin thefts.
As always, comments and suggestions are welcome.
- egs, Ittay Eyal and Malte Moeser.

@_date: 2016-03-01 19:57:43
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin Guarantees Strong, not Eventual, Consistency. 
There seems to be a perception out there that Bitcoin is eventually
consistent. I wrote this post to describe why this perception is completely
Bitcoin Guarantees Strong, not Eventual, Consistency
I hope we can lay this bad meme to rest. Bitcoin provides a strong
- egs

@_date: 2016-03-02 11:56:28
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Bitcoin Guarantees Strong, not Eventual, 
This is not the definition of eventual consistency. From
Eventual consistency is a consistency model used in distributed computing
to achieve high availability that informally guarantees that, if no new
updates are made to a given data item, eventually all accesses to that item
will return the last updated value.
The actual definition makes it quite clear that a system need not have a
final state to be evaluated for its consistency properties. Almost all
practical database systems execute continuously without a final state.
One could split hairs here by pedantically defining "Bitcoin by default" --
you could refer to just the reference client code and ignore the shim code
in the app that interfaces with the client -- but that'd drag us into a
fruitless email-list-style discussion from which no one would emerge any
wiser. I'll avoid that, and will instead dryly note that the reference
client's listreceivedbyaddress will return the number of confirmations by
default, and every application will then check the confirmations value to
confirm that it exceeds that application's own omega, while
getbalance,getreceivedbyaddress will take a number of confirmations as an
argument, shielding the app from reorgs of the suffix. That is precisely
the point made in the post.
The post covers this case. Technically, there is a difference between 0
probability and epsilon probability -- this is the reason why Nakamoto
Consensus was an exciting breakthrough result; the same reason why
Lamport's results regarding a 3f+1 bound on the Byzantine Generals Problem
do not apply to Nakamoto Consensus; and the same reason it took our paper
(Majority is Not Enough) to show that Nakamoto consensus has a similar 33%
bound as Lamport-style consensus when it comes to tolerating Byzantine
Practically, however, there is little difference between 0 and a value that
exponentially approximates 0, given that we operate on hardware subject to
random errors. The post makes the case that one can pick an omega such that
the probability of your processor mis-executing your code is larger than
the probability of observing a reorganization.
Bitcoin provides a probabilistic, accumulative probability. Not a perfect
Sometimes, non-technical people get confused about the difference between
very very very small probabilities that approximate 0 and 0. For instance,
some people get very worried about hash collisions, on which Bitcoin relies
for its correctness, whose probability also drops exponentially but is not
exactly 0. Your overall point seems to be an analogous concern that
Bitcoin's exponentially dropping probability of reorganization isn't quite
a "perfect" 0. If so, I agree and the original post made this quite clear.
Though I hope we can avoid that kind of discussion on this particular list.
- egs

@_date: 2017-03-24 09:27:47
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
Because there's no consensus on the contents of the mempool, this approach
is unsafe and will lead to forks. It also opens a new attack vector where
people can time the flood of new transactions with the discovery of a block
by a competitor, to orphan the block and to fork the chain.
The technical defense against an attacking majority of miners is to change
the PoW, effectively moving the community off into a new altcoin where the
attackers, hopefully, don't have majority hash power.
- egs
Sent from my phone, please compensate for autocorrect errors.
On Mar 24, 2017 9:06 AM, "CANNON via bitcoin-dev" <

@_date: 2017-03-28 23:24:05
@_author: =?UTF-8?Q?Emin_G=C3=BCn_Sirer?= 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
respect and admiration, I do not agree with some of their conclusions
I'm one of the co-authors of that study. I'd be the first to agree with
your conclusion
and argue that the 4MB size suggested in that paper should not be used
compensation for two important changes to the network.
Our recent measurements of the Bitcoin P2P network show that network speeds
have improved tremendously. From February 2016 to February 2017, the average
provisioned bandwidth of a reachable Bitcoin node went up by approximately
And that's just in the last year.
Further, the emergence of high-speed block relay networks, like Falcon (
and FIBRE, as well as block compression, e.g. BIP152 and xthin, change the
picture dramatically.
So, the 4MB limit mentioned in our paper should not be used as a protocol
limit today.
- egs
On Tue, Mar 28, 2017 at 3:36 PM, Juan Garavaglia via bitcoin-dev <
