
@_date: 2015-08-04 11:41:53
@_author: Dave Hudson 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
The paper is nicely done, but I'm concerned that there's a real problem with equation 4. The orphan rate is not just a function of time; it's also a function of the block maker's proportion of the network hash rate. Fundamentally a block maker (pool or aggregation of pools) does not orphan its own blocks. In a degenerate case a 100% pool has no orphaned blocks. Consider that a 1% miner must assume a greater risk from orphaning than, say, a pool with 25%, or worse 40% of the hash rate.
I suspect this may well change some of the conclusions as larger block makers will definitely be able to create larger blocks than their smaller counterparts.

@_date: 2015-08-04 16:37:57
@_author: Dave Hudson 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
Yes, but the block maker won't publish the second block it finds for the same set of transactions. It won't orphan its own block. In fact even if it does it still doesn't matter because the block maker still gets the block reward irrespective of which of the two solutions are published.
It's not about which hash wins, the issue is who gets paid as a result.

@_date: 2015-08-05 15:44:18
@_author: Dave Hudson 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
I think this would be really interesting. It's an area that seems to be lacking research.
While I've not had time to model it I did have a quick discussion with the author of the Organ-of-Corti blog a few months ago and he seemed to think that the Poisson process model isn't quite accurate here. Intuitively this makes sense as until a block has fully propagated we're only seeing some fraction of the actual hashing network operating on the same problem, so we actually see slightly fewer very quick blocks than we might expect.
I do suspect that if we were to model this more accurately we might be able to infer the "typical" propagation characteristics by measuring the deviation from the expected distribution.
Agreed - I think this would be really interesting!
I really look forward to seeing the revised version. Seeing the differences will also help assess how much impact there is from simplified models.

@_date: 2015-08-07 16:33:16
@_author: Dave Hudson 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I'm curious as I've not seen any data on this subject. How fast can a RP2 do the necessary cryptographic calculations to validate blocks of various sizes?
While everyone tends to talk in terms of 10 minutes per block that is, of course, only a typical time and doesn't account for situations in which 2 or more blocks are found in quick succession (which, of course, happens on a daily basis). At what point does, say, an RP2 node fail to be able to validate a second or third block because it's still not finished processing the first?
If someone were to be playing games with the system and mining transactions without first broadcasting them to the network then how long would that take? This would in essence define the ability to DoS lower-performance nodes (ignoring all of the other usual considerations such as bandwidth, etc).

@_date: 2015-07-11 16:04:47
@_author: Dave Hudson 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
This would probably be worse. The 1 MB would include the highest fee transactions, leaving the lowest fees in the remaining 0.5 MB.
If hashing isn't constantly applied all the time then the inter-block times will expand and the difficulty will reduce. This means that a pool that decides to use all of its available hashing 100% of the time now has a distinct advantage over those who are playing nicely. This is the same problem that the "proof of idle" idea had; it only works if no-one chooses to try to exploit it (which seems very unlikely).
One might ask why a pool might wish to try to exploit this, but let's assume we have a fees-only reward, so here's a quick thought experiment - the numbers are only approximate and would need a thorough simulation but serve for discussion:
Say, for the sake of argument that over a nominal 10 minute period we see 10 BTC worth of transaction fees. If the mempool is empty of interesting fees at the start of a block then we might like to imagine that rational miners will power down their hashing to save energy costs until the fees are worthwhile. Let's assume, for the sake of argument, that this nominally takes 5 minutes.
After 5 minutes we go from 0% to 100% as all hashing engines switch on. The difficulty will have corrected to mean that 100% of the work will nominally happen in the next 5 minutes, but that means that a malicious miner has a 2x amplification of their nominal hashing rate to do mischief in the preceding 5 minutes.
Such a malicious miner would choose to spend their 5 minutes re-mining the previous block, but dropping some amount of the transactions from it. Let's say that they try to re-mine only 9.5 BTC out the previous 10 BTC. If they succeed then they're offering everyone else an extra 0.5 BTC (5%) if they mine on top of their re-mined block and as an incentive to orphan the original block. Rational miners would definitely choose to build on the re-mined block because they get more reward from doing so.
Of course the extra hashing that our malicious miner is doing will actually push the difficulty back up somewhat, but they're still running at an advantage over the long-term. I've also ignored some of the other security implications of the hashing amplification effects (e.g. 25% of the hash rate can end up controlling 50% of the blocks in the scenario above).
An obvious objection to this scenario is that everyone would notice the malicious mining. Statistically, yes, the orphan rate would be much higher, but there's no guarantee that anyone could ever work out who was behind this. It's all too easy to create an "unknown" pool, or a series of "unknown" pools. Of course our malicious miner might choose to only target blocks that had particularly high fees associated with it, in which case the orphan rate might barely change.
The only way I can see that this wouldn't be the case would be if there were always useful fees available to mine immediately after a block is found. If block space is kept moderately scarce then immediately a block is found then everyone will keep mining and the incentives to try to deliberately orphan the last block are dramatically reduced.

@_date: 2015-07-30 07:25:43
@_author: Dave Hudson 
@_subject: [bitcoin-dev] 
=?utf-8?q?ee_market_from_a_worried_local_trader?=
I've not published any new figures for about 8 months (will try to do that this weekend), but the thing that that chart doesn't show is what's actually happening to fees per transaction. Here's a chart that does:  The data is also taken from blockchain.info so it's apples-for-apples. It shows that far from a fees going up they spent 3 years dropping. I just ran a new chart and the decline in fees continued until about 8 weeks when the "stress tests" first occurred. Even so, they're still below the level from the end of 2013. By comparison the total transaction volume is up about 2.4x to 2.5x (don't have the exact number).
I think it's equally easy to argue (from the same data) that wider adoption has actually caused wallet users to become much more effective at fee selection. Miners (as expected, assuming that they hadn't formed a cartel) have continued to accept whatever fees are available, no matter how small. Only where there has been an element of scarcity have we actually seen miners do anything but take whatever is offered.
Clearly history is not an accurate indicator of what might happen in the future, but it seems difficult to argue that there has been any sort of fee market emerge to date (other than as a result of scarcity during the stress tests).

@_date: 2015-07-31 07:29:14
@_author: Dave 
@_subject: [bitcoin-dev] 
=?utf-8?q?ee_market_from_a_worried_local_trader=E2=80=8F?=
How would email have looked if it required 300 MW of power to support it for "free" for 10 years?
In practice email was never free- it was paid for by the payments users made to ISPs. ISPs paid for email and network infrastructure from that.
The equivalent analogy here would be to drop fees completely and pay a specific miner to mine all of your transactions as a monthly subscription (which of course doesn't work in a non-permission-based network).
Miners have real (huge) costs - they will be in a lot of pain with reward halving if a few model does not replace that. That in turn poses a huge risk of smaller miners shutting down, which in turn centralises things even more. I would argue that the lack of pool diversity and thus lack of block makers is already the single biggest risk for a decentralised system; avoiding the issue of fees just accelerates this.

@_date: 2015-05-07 12:55:49
@_author: Dave Hudson 
@_subject: [Bitcoin-development] Block Size Increase 
I've been looking at this problem for quite a while (Gavin cited some of my work a few days ago) so thought I'd chime in with a few thoughts (some of which I've not published). I believe the major problem here is that this isn't just an engineering decision; the reaction of the miners will actually determine the success or failure of any course of action. In fact any decision forced upon them may backfire if they collectively take exception to it. It's worth bearing in mind that most of the hash rate is now under the control of relatively large companies, many of whom have investors who are expecting to see returns; it probably isn't sufficient to just expect them to "do the right thing".
We're seeing plenty of full 1M byte blocks already and have been for months. Typically whenever we have one of the large inter-block gaps then these are often followed by one (and sometimes several) completely full blocks (full by the definition of whatever the miner wanted to use as a size limit).
The problem with this particular discussion is that there are quite a few "knowns" but an equally large number of "unknowns". Let's look at them:
Known: There has been a steady trend towards the mean block size getting larger. See  Known: Now the trend was definitely increasing quite quickly last year but for the last few months has been slowing down, however we did see pretty much a 2x increase in mean block sizes in 2014.
Known: For most of 2015 we've actually been seeing that rate slow quite dramatically, but the total numbers of transactions are still rising so we're seeing mean transaction sizes have been reducing, and that tallies with seeing more transactions per block:  Unknown: Why are seeing more smaller transactions? Are we simply seeing more efficient use of blockchain resources or have some large consumers of block space going away? How much more block space compression might be possible in, say, the next 12 months?
Known: If we reach the point where all blocks are 1M bytes then there's a major problem in terms of transaction confirmation. I published an analysis of the impact of different mean block sizes against confirmation times:  . The current 35% to 45% mean block size doesn't have a huge impact on transaction confirmations (assuming equal fees for all) but once we're up at 80% then things start to get unpleasant. Instead of 50% of first confirmations taking about 7 minutes they instead take nearer to 19 minutes.
Known: There are currently a reasonably large number of zero-fee transactions getting relayed and mined. If things start to slow down then there will be a huge incentive to delay them (or drop them altogether).
Unknown: If block space starts to get more scarce then how will this affect the use of the blockchain? Do the zero-fee TXs move to some batched transfer solution via third party? Do people start to get smarter about how TXs are encoded? Do some TXs go away completely (there are a lot of long-chain transactions that may simply be "noise" creating an artificially inflated view of transaction volumes)?
Known: There's a major problem looming for miners at the next block reward halving. Many are already in a bad place and without meaningful fees then sans a 2x increase in the USD:BTC ratio then many will simply have to leave the network, increasing centralisation risks. There seems to be a fairly pervasive assumption that the 300-ish MW of power that they currently use is going to pay for itself (ignoring capital and other operating costs).
Unknown: If the block size is increased and yet more negligible fee transactions are dumped onto the network then that might well motivate some large fraction of miners to start to clamp block sizes or reject transactions below a certain fee threshold; they can easily create their own artificial scarcity if enough of them feel it is in their interest (it's not the most tricky setting to change). One can well imagine VC investors in mining groups asking why they're essentially subsidising all of the other VC-funded Bitcoin startups.
Known: the orphan rate is still pretty-high even with everyone's fast connections. If we assume that 20M byte blocks become possible then that's likely to increase.
Unknown: What are the security implications for larger blocks (this one (at least) can be simulated though)? For example, could large blocks with huge numbers of trivial transactions be used to put other validators at a disadvantage in a variant of a selfish mining attack? I've seen objections that such bad actors could be blacklisted in the future but it's not clear to me how. A private mining pool can trivially be made to appear like 100 pools of 1% of the size without significantly affecting the economics of running that private mine.

@_date: 2015-05-11 09:16:17
@_author: Dave Hudson 
@_subject: [Bitcoin-development] Reducing the block rate instead of 
I proposed the same thing last year (there's a video of the presentation I was giving somewhere around). My intuition was that this would require slowly reducing the inter-block time, probably by step reductions at particular block heights.
Having had almost a year to think about it some more there are a few subtleties:
1) I think it could discourage decentralisation if the nominal 2 week period per difficulty retarget is retained. If we reached 4032 blocks and a 5 minute block time then there would be 2x as many blocks at any given difficulty which increases the odds of a smaller pool finding a block and thus getting a reward. Block rewards would have to drop in proportion to the reduced interval to keep the total schedule of 21M coins on track though, but the reduction in variance is a win for smaller miners.
2) There are limits to the block time. The speed of light is an ultimately limiting factor here, but we would want to avoid excessive orphan rates.
3) There would be some amount of confusion about numbers of confirmations. I actually think that confirmation numbers are a really misleading idea anyway and it would be safer to think in terms of "minutes of security". A zero conf transaction has "zero minutes", while right now 1, 2, 3 and 6 would be "ten minutes", "twenty minutes", "thirty minutes" and "sixty minutes" respectively. If our block time were 5 minutes then 8 confirmations would be "forty minutes" of security; if the block time was 2.5 minutes then 8 confirmations would be "twenty minutes" of security. The "minutes of security" measure indicates the mean number of minutes of the entire network's hash rate would be required to undo a transaction.
4) Reducing the inter-block time reduces the variance in reaching that "sixty minutes" of security level. The variance around finding 6 blocks with a ten minute interval is much wider than the variance for finding 12 blocks with a 5 minute interval.

@_date: 2015-05-11 12:49:03
@_author: Dave Hudson 
@_subject: [Bitcoin-development] Reducing the block rate instead of 
This doesn't work because a large-scale miner can trivially make themselves look like a very large number of much smaller scale miners. Their ability to minimize variance comes from the cumulative totals they control so 10 pools of 1% of the network cumulatively have the same variance as 1 pool with 10% of the network. It's also very easy for miners to relay blocks via different addresses and the cost is minimal. The biggest cost would be in DDoS prevention and a miner that actually split their pool into lots of small fragments would actually give themselves the ability to do quite a lot of DDoS mitigation anyway. If no-one is doing this right now it's simply because they've not had the right incentives to make it worthwhile; if the incentives make it worthwhile then this is pretty trivial to do.
This is one area where anonymity on behalf of transaction validators and block makers essentially makes it pretty-much impossible to maintain any sort of sanctions against antisocial behaviour.

@_date: 2015-05-12 17:21:40
@_author: Dave Hudson 
@_subject: [Bitcoin-development] Long-term mining incentives 
I think proof-of-idle had a potentially serious problem when I last looked at it. The risk is that a largish miner can use everyone else's idle time to construct a very long chain; it's also easy enough for them to make it appear to be the work of a large number of distinct miners. Given that this would allow them to arbitrarily re-mine any block rewards and potentially censor any transactions then that just seems like a huge security hole?

@_date: 2015-05-31 15:17:10
@_author: Dave Hudson 
@_subject: [Bitcoin-development] Fwd: Block Size Increase Requirements 
There's an interesting incentives question if the mining fees ever become large enough to be interesting. Given two potential blocks on which to build then for the best interests of the system we'd want miners to select the block that confirmed the largest number of transactions since that puts less pressure on the network later. This is at odds with the incentives for our would-be block maker though because the incentive for mining would be to use whichever block left the largest potential fees available; that's generally going to be the smaller of the two.
This, of course, only gets worse as the block reward reduces and fees become the dominant way for miners to be paid (and my hypothesis that eventually this could lead to miners trying to deliberately orphan earlier blocks to "steal" fees because the fixed block reward is no longer the dominant part of their income).
When coupled with the block propagation delay problem increasing the risk of orphan races I'm pretty sure that this actually leads to miners having an incentive to continually mine smaller blocks, and that's aside from the question of whether smaller blocks will push up fees (which also benefits miners).

@_date: 2016-03-02 15:48:21
@_author: Dave Hudson 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
I think the biggest question here would be how would the difficulty retargeting be changed?  Without seeing the algorithm proposal it's difficult to assess the impact that it would have, but my intuition is that this is likely to be problematic.
Probabilistically the network sees surprisingly frequent swings of +/-20% in terms of the block finding rate on any given day, while the statistical noise over a 2016 block period can be more than +/-5%.  Any change would still have to require a fairly significant period of time before there would be a reasonable level of confidence that the hash rate really had fallen as opposed to just seeing statistical noise ( and How long would be required to deem that the hash rate had dramatically fallen?  Would such a change be a one-time event or would it be ever-present?
If we were to say that if the hash rate dropped 50% in one day (which could, of course be a 30% real drop and 20% variance) and the difficulty was retargeted to 50% lower then that would have to be matched with a similar rapid retarget if it were to increase by a similar amount.  Failing to do this both ways this would introduce an economic incentive for large miners to suppress the difficulty and gain dramatically larger numbers of block rewards.  The current fixed block count per difficulty change prevents this because the daily losses while suppressing hashing outweigh the potential gains when it's re-added.

@_date: 2016-03-09 18:30:19
@_author: Dave Hudson 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
A damping-based design would seem like the obvious choice (I can think of a few variations on a theme here, but most are found in the realms of control theory somewhere).  The problem, though, is working working out a timeframe over which to run the derivative calculations.
The problem is the measurement of the hashrate, which is pretty inaccurate at best because even 2016 events isn't really enough (with a completely constant hash rate running indefinitely we'd see difficulty swings of up to +/- 5% even with the current algorithm).  In order to meaningfully react to a major loss of hashing we'd still need to be considering a window of probably 2 weeks.
My other concern is that if we allow quick retargets to lower difficulties then that seems likely to expose the chain to being gamed.  I'd need to think about this some more, but a few scenarios I was thinking about earlier this week appeared to risk making some types of selfish mining strategies quite a lot more profitable.
With all this said though I'll be very surprised if there's a huge drop in the hash rate come July.  The hash rate has jumped up by almost 70% in the last 6 to 7 months and that implies some pretty serious investments by miners who are quite aware of the halving.  My guess is that quite a lot of the baseline 30% has also been replaced in the same cycle.  These same miners were mining with a coin price around $250 last year so in terms of profitability I'm pretty sure that one around $400 won't be a huge concern.
I'm sure that there will be some very public "I'm done with mining" announcements from a few smaller miners come July, but I suspect the bulk of the network will have a relatively small blip and continue on its way.

@_date: 2016-03-09 23:24:15
@_author: Dave Hudson 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
Sure, but I think there are 2 problems:
1) My guess is that errors over anything but a long period are probably too large to be very useful.
2) We don't have a strong notion of time that is part of the consensus.  Sure, blocks have timestamps but they're very loosely controlled (can't be more than 2 hours ahead of what any validating node thinks the time might be).  Difficulty can't be calculated based on anything that's not part of the consensus data.
Strictly it's a non-homogeneous Poisson Process, but I'm pretty familiar with the concept (Google threw one of my own blog posts back at me:  but I actually prefer this one:  because most people seem to find it easier to visualize).
Agreed, it's a thought experiment I ran in May 2014 (  I found that many people's intuition is that there would be little or no difficulty changes in such a scenario, but the intuition isn't reliable.  Given a static hash rate the NHPP behaviour introduces a surprisingly large amount of noise (often much larger than any signal over a period of even weeks).  Any measurements in the order of even a few days has so much noise that it's practically unusable.  I just realized that unlike some of my other sims this one didn't make it to github; I'll fix that later this week.
