
@_date: 2012-04-26 11:49:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Trusted identities 
It recently occured to me that we can use the public nature of the block
chain to create trusted identities, for a specific form of trust.
Lets suppose Alice has some bitcoins held at bitcoin address A. She
wants to establish trust in the "identity" associated with the ECC
keypair associated with A, for instance for the purpose of having other
users trust her not to attempt to double spend. Since the trust she
seeks is financial in nature, she can do this by valuing the identity
associated with A, by delibrately throwing away resources. A simple way
to do this would of course be to transfer coins to a null address,
provably incurring a cost to her.
A more socially responsible way would be for her to create a series of
transactions that happen to have large, and equal, transaction fees.
Bitcoin makes the assumption that no one entity controls more than 50%
of the network, so if she makes n of these transactions consecutively,
each spending m BTC to transaction fees, there is a high probability
that she has given up at least n/2 * m BTC of value. This of course is
all public knowledge, recorded in the block chain. It also increases the
transaction fees for miners, which will be very important for the
network in the future.
Now Bob can easily examine the block chain, and upon verifying Alice's
trust purchase, can decide to accept a zero-confirmation transaction at
face value. If Alice breaks that promise, he simply publishes her signed
transaction proving that Alice is a fraudster, and future Bob's will
distrust Alice's trusted identity, thus destroying the value needed to
create it.
In effect, we now have a distributed green address system.
Now Alice could try to mount a double-spend attack on a whole bunch of
people at once, hoping to have them all accept the transaction. However
as it is the "just trust them" model works pretty well already.
A good usecase for this idea, beyond the obvious fast payments
application, is a distributed anonymizer. Alice can now publish her
request to anonymize coins, and other trusted identities can make their
bids. If Alice accepts a bid from Bob, she will want Bob to send her the
anonymized coins *prior* to her transaction going through, thus breaking
the temporal connection between the transactions. Now Alice can give Bob
the signed payment transaction, and Bob can submit his payment
transaction to the network first, knowing that Alice isn't going to try
to rip him off. Bob can also have a trusted identity which signed the
contract for the anonymizer transaction, and similarly if he rips Alice
off, she can publish it for the world to see.
A more subtle effect, is this makes sybil attacks more difficult. To
pretend to be a thousand identities is going to now require 1,000 * n
coins, and attempting to pull this attack off inherently strengthens the
bitcoin network. Obviously we can apply this principle to other things
like tor nodes as well.

@_date: 2012-04-26 13:30:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Trusted identities 
There's gotta be a lot of subtlies there. For instance, if I just want
to double-spend, the easiest approach would be to first buy a whole
bunch of VPS's, each with different /16's for their IP address to defeat
that anti-sybil measure. Then figure out what is the set of nodes
closest to my target - easier for an active target that makes a lot of
Then it's just a matter of giving them my transaction, and immediately
flooding the network faster with my nodes than their single node. It's
not block-replacement, but it would be effective against people who
accept 0-confirmations. (although as Gavin has pointed out elsewhere, in
the future miners may be very happy to replace transactions for more
fees in that kind of circumstance)
Of course, this whole trusted identities business could be equally used
for the bitcoin flood network as a whole to prevent sybil's, and perhaps
even get guarantees of behavior like "My node respects nLockTime and
won't ignore it for a higher-fee transaction replacement"
Yup, especially when a human is in the loop.
My understanding is it's completely disabled.

@_date: 2012-06-17 15:05:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] Ultimate Blockchain Compression w/ 
How are you going to prevent people from delibrately unbalancing the
tree with addresses with chosen hashes?
One idea that comes to mind, which unfortunately would make for a
pseudo-network rule, is to simply say that any *new* address whose hash
happens to be deeper in the tree than, say, 10*log(n), indicating it was
probably chosen to be unbalanced, gets discarded. The "new address" part
of the rule would be required, or else you could use the rule to get
other people's addresses discarded.
Having said that, such a rule just means that anyone playing games will
find they can't spend *their* money, and only with pruning clients.
Unrelated people will not be effected. The coins can also always be
spent with a non-pruning client to an acceptable address, which can
later re-spend on a pruning client.
It also comes to mind is that with the popularity of firstbits it may be
a good idea to use a comparison function that works last bit first...
It's merkles all the way down...

@_date: 2012-06-18 06:14:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Ultimate Blockchain Compression w/ 
I strongly disagree on that point. What you're proposing needs miner
support to work, and miners generally run either the satoshi client as a
daemon, or some other custom code. Implementing the idea in armory
doesn't give those miners a nice upgrade path.
That said, *using* the hash tree is something that can be implemented in
any client, but a lot of the code will be shared between calculating it
and using it anyway, so again implementing in the satoshi client makes
Lets suppose we're trying to make a tree consisting of real numbers:
    /\
   /  \
   *   \
  / \   \
 /   \   \
 *   *   *
1 2 3 4 5 6
If the numbers are evenly distributed, as will happen with hashes of
arbitrary data, any number will be at most log(n) steps away from the
head of the tree.
Suppose though some malicious actor adds the following numbers to that
tree: 3.001 3.002 3.003
    /\
   /  \
   *   \
  / \   \
 /   \   \
 *   *   *
1 2 * 4 5 6
   / \
  \
   *   *
  / \ / \
  0 1 2 3 <- (3.000 to 3.003)
Ooops, the tree suddenly became a lot higher, with an associated
decrease in retrieval performance and an increase in memory usage.
Of course the exact details depend on what rules there are for
constructing the tree, but essentially the attacker can either force the
a big increase in the depth of the tree, or a large number of vertexes
to be re-organizationed to create the tree, or both.
Now, to be exact, since the key of each vertex is a transaction hash,
this malicious actor will need to brute chosen prefix hash collisions,
but this is bitcoin: the whole system is about efficiently brute forcing
chosen prefix hash collisions. Besides, you would only need something
like k*n collisions to product an n increase in tree depth, with some
small k.
My solution was to simply state that vertexes that happened to cause the
tree to be unbalanced would be discarded, and set the depth of inbalance
such that this would be extremely unlikely to happen by accident. I'd
rather see someone come up with something better though.
Another naive option would be to hash each vertex key (the transaction
hash) with a nonce known only to the creator of that particular merkle
tree, but then the whole tree has to be recreatred from scratch each
time, which is worse than the problem... Interestingly in a
*non-distributed* system this idea is actually quite feasible feasible,
as the nonce could be kept secret.

@_date: 2012-05-12 03:00:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stackoverflow with latest git head 
0x00007ffff5d021a6 in QApplicationPrivate::notify_helper(QObject*,
QEvent*) () from /usr/lib/libQtGui.so.4
 0x00007ffff5d086fb in QApplication::notify(QObject*, QEvent*) ()
from /usr/lib/libQtGui.so.4
 0x00007ffff582f06c in QCoreApplication::notifyInternal(QObject*,
QEvent*) () from /usr/lib/libQtCore.so.4
 0x00007ffff5d4da05 in QWidget::setToolTip(QString const&) () from
 0x0000000000589e43 in
GUIUtil::ToolTipToRichTextFilter::eventFilter(QObject*, QEvent*) ()
 0x00007ffff582e54b in
QEvent*) () from /usr/lib/libQtCore.so.4
 0x00007ffff5d021a6 in QApplicationPrivate::notify_helper(QObject*,
QEvent*) () from /usr/lib/libQtGui.so.4
The above is the loop that bitcoin seems to be stuck in. I ran
git-bisect and it looks like the first bad commit is
My QT version is 4.6.2
Give me a shout if there are any other tests you want me to run.

@_date: 2012-05-27 13:02:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] IPv6 bitcoin support now live 
I can confirm this seems to work quite well now. I tried running an
IPv6-only node a few days ago, and with the default seeds it couldn't
start up and sync with the network unless I manually created a dual-net
node and manually added it. Now things work just fine out of the box and
last I checked I had 5 peers.

@_date: 2012-11-28 03:33:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
I'm not sure this is actually as much of an advantage as you'd expect. I
looked into Google Protocol buffers a while back for a timestamping
project and unfortunately there are many ways in which the actual binary
encoding of a message can differ even if the meaning of the message is
the same, just like JSON.
First of all while the order in which fields are encoded *should* be
written sequentially, parsers are also required to accept the fields in
any order. There is also a repeated fields feature where the
fields can either be serialized as one packed key-list pair, or multiple
key-value(s) pairs; in the latter case the payloads are concatenated.
The general case of how to handle a duplicated field that isn't supposed
to be repeated seems to be undefined in the standard. Yet at the same
time the standard mentions creating messages by concatenating two
messages together. Presumably parsers treat that case as an error, but I
wouldn't be surprised if that isn't always true.
Implementations differ as well. The current Java and C++ implementations
write unknown fields in arbitrary order after the sequentially-ordered
known fields, while on the other hand the Python implementation simply
drops unknown fields entirely. As far as I know no implementation
preserves order for unknown fields.
Finally, while not a Protocol Buffers specific problem, UTF8 encoded
text isn't guaranteed to survive a UTF8-UTFx-UTF8 round trip.  Multiple
code point sequences can be semanticly identical so you can expect some
software to convert one to the other. Similarly lots of languages
internally store unicode strings by converting to something like UTF16.
One solution is to use one of the normalization forms such as NFKD - an
idempotent transformation - although I wouldn't be surprised if
normalization itself is complex enough that implementation bugs exist,
not to mention the fact that the normalization forms have undergone
different versions.
I think the best way(1) to handle (most) the above by simply treating the
binary message as immutable and never re-serializing a deserialized
message, but if you're willing to do that just using JSON isn't
unreasonable either.
1) Of course I went off an created Yet Another Binary Serialization for
my project, but I'm young and foolish...

@_date: 2012-11-28 07:57:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
FWIW I re-read the specs again and turns out my memory was wrong. (I
last looked at this about four months ago) Duplicated fields are handled
in a defined manner, with the last field seen in the serialization being
the one whose value is used. Again, repeated fields are treated as
elements of a list, preserving order.
It does raise the interesting question do the implementations that don't
preserve order of unknown fields, preserve the order of multiple unknown
fields, either repeated or not?
I gotta admit, I suspect they won't be that open. Protocol buffers was
designed because Google needed a fast serialization method suitable for
many different internal projects. Needing round-trip idempotence seems
like a rare requirement to me, especially for internal use.
Well, actually you can take advantage of the message concatination
ability of protocol buffers to extend a message by simply appending the
new fields to the existing thus either defining new fields, or
overriding old values as required. If you want to de-duplicate though
you run into the problem all over again.
On the other hand JSON handles this case fine too provided that your
JSON implementation supports dictionary objects with arbitrary fields.
Just use the object as is and the unknown fields will be re-serialized
properly at the other end. Some implementations will have to be careful
to handle collisions with existing keys in the namespace. (consider in
Python what would happen if you mapped your object to a class instance,
and the serialization included the key "__init__")
That said, JSON is quite problematic with numbers. For instance, you
have to be careful to keep integers represented as pure integers below
what Javascript can handle, the maximum integer exactly representable in
a double float, or the JSON won't be parsable in Javascript even if many
other languages handle it fine. Protocol buffers is at least pretty
explicit about what size integers are.
Note that I think the SignedInvoice message itself is broken, because
protobuf implementations have no reason to guarantee that they can give
you the serialized bytes of the Invoice sub-message. It's a quite
specific use-case that isn't needed for pretty much anything but crypto.
FWIW I took a quick look at the official API's, C++, Java and Python,
and as far as I can tell none of them support accessing the binary
serialization of a message field other than by re-serializing the
Really the invoice field should be declared as bytes serialized_invoice,
as inconvenient as that is to work with.
Since the Payment message includes an *untrusted* Invoice that the
vendor needs to authenticate the whole invoice no matter what on Payment
reception. In many cases that implies they have to keep some sort of
database of "quotes" or similar anyway as the client can change anything
they want otherwise. Again that leads back to the argument of why not
just stick with the merchant_dat as you suggest, which will usually be
some short invoice number attached to a database? A vendor that wants to
operation a stateless invoicing system can just stuff a HMAC-protected
serialized invoice into the merchant_data
I guess you could use a mutable invoice field as a way of achieving some
sort of negotiation protocol, but I think it's better to stick to the
original concept of just ensuring that the user is really paying the
right amount to the right address.

@_date: 2013-04-05 08:12:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] A mining pool at 46% 
Vandalize the chain how? By delibrately triggering bugs? (like the old
OP_CHECKSIG abuse problem) Regardless of whether or not the
vulnerability requires multiple blocks in a row, the underlying problem
should be fixed.
By putting illegal data into it? Fundementally we have no way to prevent
people from doing that other than by making it expensive. An attacker
having a lot of hashing power just means they can do so faster and a bit

@_date: 2013-04-09 05:28:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] On-going data spam 
It must be "shit on the blockchain" week:
Timestamping the stupid way, but the user experience is really nice:
Like it or not, people will do what's easiest regardless of how much it
harms everyone. I'd send this guy an email about opentimestamps yadda
yada, but really, why bother.

@_date: 2013-04-09 07:09:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] On-going data spam 
Makes bringing up a new node dependent on other nodes having consistent
uptimes, particularly if you are on a low-bandwidth connection.
No blacklists

@_date: 2013-04-09 13:58:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] On-going data spam 
As I pointed out in my initial post on the issue, SatoshiDice is pretty
much unaffected by the patch. They just have to deduct enough from
incoming bets to make the "you lost" output economical and they're good
to go. IIRC they already deduct fees on low-value bets anyway.
On the other hand, the patch makes a clear statement that Bitcoin is not
for microtransactions. If succesful it will gradually force a number of
users, ad-based faucet sites and the like, to off-chain transactions or
off Bitcoin entirely. The payment protocol has nothing to do with that.

@_date: 2013-04-09 23:03:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] 
=?utf-8?q?_in_txouts_=E2=80=94_The_Ultimate_Solution?=
Note how we can already do this: P2SH uses Hash160, which is
RIPE160(SHA256(d)) We still need a new P2SH *address* type, that
provides the full 256 bits, but no-one uses P2SH addresses yet anyway.
This will restrict data stuffing to brute forcing hash collisions. It'd
be interesting working out the math for how effective that is, but it'll
certainely be expensive in terms of time hashing power that could solve
shares instead.

@_date: 2013-04-09 23:08:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] 
=?utf-8?q?_in_txouts_=E2=80=94_The_Ultimate_Solution?=
Oh, and while we're at it, long-term (hard-fork) it'd be good to change
the tx hash algorithm to extend the merkle tree into the txouts/txins
itself, which means that to prove a given txout exists you only need to
provide it, rather than the full tx.
Currently pruning can't prune a whole tx until every output is spent.
Make that change and we can prune tx's bit by bit, and still be able to
serve nodes requesting proof of their UTXO without making life difficult
for anyone trying to spent old UTXO's. The idea is also part of UTXO
proof stuff anyway.

@_date: 2013-04-10 02:53:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] 
=?utf-8?q?_in_txouts_=E2=80=94_The_Ultimate_Solution?=
We can keep the length 160bits:
scriptPubKey: OP_HASH160 OP_HASH160  OP_EQUALVERIFY
You don't need to change the address type at all if new software is
written to check for both forms of txout in the actual
blockchain/transaction code at the deeper level. Basically now a P2SH
address could actually mean one of two scriptPubKey forms, must like a
normal address can mean either the hashed or bare OP_CHECKSIG form.
Of course, either way you have the odd side-effect that it's now
difficult to pay further funds to a random txout seen on the
blockchain... strange, although possibly not a bad thing.

@_date: 2013-04-10 03:29:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] 
=?utf-8?q?_in_txouts_=E2=80=94_The_Ultimate_Solution?=
Don't get your hopes up - I fully expect blockchain.info to keep a
publicly accessible database of inner-digest -> P2SH addresses...

@_date: 2013-04-11 07:27:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] 
=?utf-8?q?_in_txouts_=E2=80=94_The_Ultimate_Solution?=
You mean  ?
I would oppose it, and I wrote the above proposal. The code required to
implement UTXO fraud proofs is more complex than the entire Bitcoin code
base; obviously that much new fork-critical code opens up huge technical
risks. As an example, can you think of how UTXO fraud proofs can cause
an arbitrarily deep re-org?

@_date: 2013-04-14 01:09:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] RFC: extend signmessage/verifymessage to P2SH 
Currently signmessage/verifymessage only supports messages signed by a
single key. We should extend that to messages signed by n-of-m keys, or
from the users point of view, P2SH multisig addresses.
rpc.cpp:signmessage() returns the output of SignCompact(). That in turn
starts with a header byte marking the signs of the various keys to allow
for key recovery. The header byte can be one of 0x1B, 0x1C, 0x1D or 0x1E
For multisig signmessage signatures this is extended:
    <01>    {, ...}
Each signature or key can be one of the following forms:
    sig: <1B/1C/1D/1E> <32 byte r> <32 byte s>
    compress key: <02/03> <32 byte x>
    uncompressed key: <04> <32 byte x> <32 byte y>
Note that we have to provide all pubkeys, even if they aren't used for a
given signature, to allow the P2SH address to be reconstructed.
Decoding/encoding is a bit code-intensive due to the all the cases, but
on the other hand this format keeps the size down to an absolute
minimum. Alternatively I could use length bytes.
The format is backwards compatible in the sense that older versions will
fail safely on new signatures, even ones that have been truncated.
Similarly new signatures are easily distinguished from old, and going
forward if we for some reason need yet another signature format the
leading byte can be incremented.
Signing incomplete signatures on messages can be handled by converting
pubkeys to signatures. Similarly the RPC signmessage command can be
extended with an optional "existing signature" option.

@_date: 2013-04-14 02:25:35
@_author: Peter Todd 
@_subject: [Bitcoin-development] RFC: extend signmessage/verifymessage to 
I already looked into ASCII-armoring PGP style for a different project.
(timestamping) Basically you'd want to follow RFC2440, section 7
"Cleartext signature framework":
-----BEGIN BTC SIGNED MESSAGE-----
Hello world!
-----BEGIN BTC SIGNATURE-----
-----END BTC SIGNATURE-----
Pretty simple really and doesn't need to be done prior to other
signmessage changes. There may be an issue passing \r's through the RPC
interface; the RFC specifies CRLF line endings.

@_date: 2013-04-14 02:29:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] RFC: extend signmessage/verifymessage to 
Sure, which is why I have the header byte so that when we do come up
with a chain of keys thing, that in turn can get it's own magic number
FWIW I have an application now where a multisig signmessage would be

@_date: 2013-04-16 07:28:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] [bitcoin] Enable tx replacement on 
1) IsNewerThan() returns false if vin.size() != old.vin.size(), or if any of
   the prevouts (txhash/vout  don't match. In other words the inputs to a
   transaction can't be changed by the current transaction replacement
   mechanism.
2) If not for transaction replacement, nSequence could have been a boolean flag
   called fIgnoreLockTime.
3) The nSequence part of IsNewerThan() is essentially just another type of
   zero-conf transaction where you trust miners and relaying nodes to do what
   you tell them too. We shouldn't encourage zero-conf transactions.
4) Testnet may be for testing, but we know tx-replacement is trivially DoS-able
   right now. Those who want to experiment with attacks and counter measures
   should do so in a way that doesn't disrupt testnet for everyone - setting up
   their own tx-replacement enabled nodes is easy to do.
5) Zero-conf transactions, replacement or otherwise, have issues with consensus
   failure when network bandwidth is limited. In particular even if we require
   each re-broadcast of a transation to be accompanied by an increase in the
   fees it pays, fees += new tx size * MIN_RELAY_FEE has been proposed by a few
   people for instance, the cost to different nodes for that bandwidth is different.
   While less of an issue with 1MB blocks, with large blocks, especially blocksizes
   only limited by what miners think is reasonable considering available
   network bandwidth, lots of nodes and miners will be bandwidth starved
   and transaction fees will fall to the marginal cost of the network
   bandwidth and processing power to handle them. That cost is different
   for different parties - you definitely won't get consensus and thus
   zero-conf security in that scenario.
6) Finally zero-conf transactions have issues with consensus failures
   caused by limited mempool sizes.
In any case, the more pressing issue re: replacement is changing fees
attached to transactions after they have been broadcast. Lots of users
are getting their transactions stuck with few options to fix them.
The more I think about the issue the more I think we should nip this
zero-conf madness in the bud: change the relay rules so that
transactions are replaced based on fees regardless of how that changes
transaction outputs. Of course, this does make double-spending an
unconfirmed transaction trivial. On the other hand, this makes changing
fees after the fact trivial, and it lets us implement a limited 'undo'
button for when people screw up. It also allows for many of the
applications transaction replacement was meant for in the first place
anyway, and all the applications where it's actually secure.
We keep saying over and over again to stop accepting zero-conf
transactions, but people do it anyway because it seems secure. It's a
very dangerous situation because the security of zero-conf transactions
can change overnight simply by some fraction of the hashing power
implementing that exact change.
Some thought is required as to exactly what "replace by fees" looks
like, economically optimal is a bit complex due to it's dependency on
overall mempool backlog, but a rough first version should be easy to
hammer out.
Funny enough though: I'm working on a trusted-third-party ledger design,
intentially made to be close to 100% compatible with standard Bitcoin,
and in that system tx-replacement by nSequence will be a perfectly
reasonable thing to do.

@_date: 2013-04-16 14:43:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Post tge malicious miners and other bits so we can evaluate the system as a whole.

@_date: 2013-04-18 04:14:07
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
FWIW Gavin has spent quite a bit of time and effort ensuring that
Bitcoin is resistent to DoS attacks, as well as spearheading a move
towards better testing. The latter in particular is helpful against
chain-forking bugs, so better testing is very much a security issue. He
also spearheaded P2SH, and the current efforts to get a payment protocol
implemented. I'm less convinced about his stance against attackers that
pose a threat to the system as a whole, but it's not fair to accuse him
of not taking security seriously.
You should clarify if you want this patch to compute fees recursively or
not, IE, should the patch include fees paid by child transactions in how
it computes the total fee the transaction pays. Doing this is
non-trivial, although Luke-Jr has written a patch to do this without
replacement: Also, clarify if you want unit-tests and similar things included in the

@_date: 2013-04-18 05:04:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Yeah, an attack is a bit more subtle than perhaps John Dillon realizes.
Assuming that nodes prioritize the transactions with the fewest total
replacements first it becomes a multiplier on the standard attack of
just broadcasting transactions. So for non-replacement users it's
probably not that bad.
An attack still shuts down useful tx replacement though. For instance in
the adjusting payments example an attacker sets up a legit adjusting
payment channel, does a bunch of adjustments, and then launches their
attack. They broadcast enough adjustments that their adjustment session
looks like part of an attack, and then don't have to pay for the full
adjusted amount.
What's worse, the attack itself can be just a large number of these
micropayment sessions, IE, no bogus sessions at all.
It's *easily* DoSable, not trivially. Testnet has all the same
fee/priority rules that Bitcoin has, so any attack still costs some
amount of coins. For instance I did the math once on what it would cost
to flood testnet with 1MB blocks, and it came out to IIRC $100/month or
so in terms of lost mining revenue. Small, but still not trivial.
non-final nLockTime floods and timewarp assisted can be easily done mind
you, but the former is easy to fix and the latter is relatively tricky
to pull off and still requires some mining expenditure.
John's proposing something that would work in conjunction with fees, so
it's no different than the MIN_RELAY_FEE that has quite successfully
prevented flooding on mainnet. For that matter, what he proposed can be
used even with non-final == non-standard, which means the replacement
transactions can't be broadcast onto the network at all until they can
be mined.
Actually, that's an interesting point: one way to do replacement
anti-DoS would be to only allow each input a given number of chances to
do a replacement. Provided your design is asymmetric in who the attacker
would be, and the inputs controlled by the defender outnumber the
attacker, the defender can always count on doing the last replacement.
You would need some way of determining which input was responsible for a
replacement though - I can't think of an obvious way to within the
current transaction format, but I haven't thought hard about it yet.
How exactly do you envision replacement working with non-final ==
non-standard anyway?
If he's reasonable about the scope, IE just a initial implementation for
further evaluation, I figure it's about two days work. $250/day is
enough that I'd give it a go - I already looked into how to implement it

@_date: 2013-04-18 05:28:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
...and actually, that's not a problem if the defender is online, because
they can just broadcast the highest sequence numbered tx, which blocks
further broadcasts by the attacker. You still need some way of
distinguishing the two acts, by time is probably fine, but it'd make a
real attack difficult.
Of course, regardless you are still asking nodes to set aside however
many KB/second to tx replacement transactions, and they're all going to
use different settings, which makes overall network convergence
impossible to guarantee as legit replacement transactions outnumber
non-legit ones. Any protocol requiring the broadcast of more than one or
two replacements, either normally or against an attacker, just isn't
going to be reliable. But many don't, so they're probably doable.
But lets see some working code first...

@_date: 2013-04-18 06:08:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Mike, for the love of god, it's not acceptable to require Bitcoin
validating and mining nodes to buy unlimited bandwidth. The attacker
just has to spend more outgoing bandwidth than some fraction of the
network hashing power has incoming bandwidth (individually speaking) for
convergence to fail.
But anyway, as I said in my follow up email, with correct prioritization
it's probably a tractable problem.
Testnet has 40 nodes online right now that can be seen by my seeder. To
shut down all those nodes with a standard DoS attack would take at least
40 times more bandwidth than required by a tx-replacement DoS attack.
You can always add dummy inputs.
Well, all that making them relayable enables is letting all parties to a
transaction be offline when the nLockTime expires, so I'm not really
sure it's worth doing, at least immediately. Weakening the non-final ==
non-standard test to give a window of, say, 3 blocks, would be fine I
The whole point of tx-replacement by fee is that the algorithm doesn't
need to be fixed. It makes it very clear that unconfirmed transactions
aren't trustworthy, and levels the playing field between you and people
posessing lots of hashing power. Nodes can independently choose their
replacement policy and the system still works. It also makes adjusting
fees after the fact easy, again, functionality that we really need right
John's adjusting micropayments proposal would work best in conjuction
with some reputation stuff, like buying a fidelity bond promising you
will play nice with tx replacement. Implementing such bonds is a bit
subtle, because random chance can cause an earlier tx to be mined rather
than a latter, but you can, for instance, rebut accusations of fraud in
that case by simply creating an additional tx that pays the full amount
if a partial tx accidentally gets mined. Come to think of it, such a
system could be useful for inter-bank settlement, providing a security
guarantee somewhere between a standard transaction and a fully off-chain
transaction, at the cost of a single transaction fee.
More importantly, it's not subject to sudden "oh shit, slush's pool got
hacked and is doing double spends!" disasters. People should not be
trusting multiple mining pools run by individuals as hobbies on insecure
VPS services with the security of their payments, and zero-conf
transactions do exactly that. Not to mention the ~25% of hashing power
controlled by parties of unknown origin.

@_date: 2013-04-28 23:55:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
I think that pretty much sums it up.
With the block-range served in the anounce message you just need to find
an annoucement with the right range, and at worst connect to a few more
node to get what you need. It will be a long time before the bandwidth
used for finding a node with the part of the chain that you need is a
significant fraction of the load required for downloading the data
Remember that BitTorrent's DHT is a system giving you access to tens of
petabytes worth of data. The Bitcoin blockchain on the other hand simply
can't grow more than 57GiB per year. It's a cute idea though.
Also, while we're talking about the initial download:
Lots of options out there.

@_date: 2013-04-29 12:28:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Hardware BitCoin wallet as part of Google 
A word of caution: hardware Bitcoin wallets really do need some type of
display so the wallet itself can tell you where the coins it is signing
are being sent, and that in turn implies support for the upcoming
payment protocol so the wallet can also verify that the address is
actually the address of the recipient the user is intending to send
funds too. The current Crypto Stick hardware doesn't even have a button
for user interaction. (press n times to approve an n-BTC spend)
Having said that PGP smart cards and USB keys already have that problem,
but the consequences of signing the wrong document are usually less than
the consequences of sending some or even all of the users funds to a
thief. You can usually revoke a bad signature after the fact with a
follow-up message.
Not to say hardware security for private keys isn't a bad thing, but the
protections are a lot more limited than users typically realize.
I will say though I am excited that this implies that the Crypto Stick
could have ECC key support in the future.

@_date: 2013-08-05 13:49:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Safe auto-updating 
Hash: SHA256
Gregory Maxwell had some good ideas along these lines at the san jose conference. Extending gitian with these kinds of features would be a good approach.
But I think its worth thinking about attack models. A huge danger with auto-updating is that it is easy to target individuals; if I leave auto-updates on I am essentially trusting the developers capable of signing an update not to specifically try to attack me in the future, a much more risky thing to do than simply  trusting them not to release a malicious release.
Sure you can try to implement anonymous downloads and similar mechanisms, but they all tend to be fragile with regard to deanonymization attacks.
A better way is to ensure that the act of making a release available for download must be public, even if you can control what binaries are made available to a particular target. You can do this by putting a commitment in the blockchain itself. Each person on the signing list creates a transaction with a special form from a specific pubkey that commits to the digest of the binaries, and the auto-update code refuses to update unless it sees that special transaction with a sufficient number of confirmations. The developers now can't make a special release for a specific target without letting the world know they did so, even under coercion.
They developers could of course still make a release with code inside targeting a specific individual, but in theory at least the public can check if their builds are reproducible, and start asking questions why not?

@_date: 2013-08-08 14:20:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] Two factor wallet with one-time-passwords 
Quick note for patent prior-art/my own memory - did a talk yesterday
about multifactor wallets, one time passwords and hash digest based
oracles. Someone getting involved in the business of selling bitcoins
pointed out that legally it can actually be desirable to give the
bitcoins to the customer by giving them a physical private key, perhaps
on a sheet of paper in a mailed envelope. Obviously the customer would
be wise to sweep the funds. Of course, the advantage of doing it with
paper is the legal system has a long history of dealing with the concept
of a secret on a piece of paper. (your customers won't have handy PKI to
use after all)
With multi-factor wallets you can have the customer provide one or more
keys, and you give them one final key on a sheet of paper, with
instructions to scan it on their phone via QR-code or something. Now the
transfer is absolute on your end - you can't get the funds back. If it's
a large amount you may want to split it up among multiple addresses, and
deliver the keys to the customer in a way that makes it obvious when
they are revealed. (scratch off for instance)
Finally, one-time-passwords do much the same thing, but they don't
require the second device, and the sheet of paper the customer is
dealing with can be much shorter. Similarly the final approval could
just be done over the phone by telling the customer the ~6-8 magic words
that unlock their funds - legally it could be useful to record that
phone call. Similarly for a large transfer, make it clear how much each
scratched off text field is unlocking to defend yourself in court.
Of course, in both there is still the risk of the funds ending up locked
due to a mistake, but at least there isn't financial incentive to make
that event happen. (usually)
I'll admit I hadn't thought of any of this stuff until I talked to an
actual business with real problems, worth doing.
Finally it's too bad we didn't get OP_EVAL; the customer could have
provided a P2SH script with, well, anything in it, and the unlock could
could have easily been a "bolt-on":
HASH160  EQUALVERIFY
DUP HASH160  EQUALVERIFY EVAL
Oh well, MAST support can do the same thing one day.

@_date: 2013-08-16 05:52:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] LevelDB in master 
I ran into this problem while auditing Litecoin actually: the tools to
audit that a set of git patches/merges actually match upstream (or
downstream for litecoin) don't really exist yet. In this case manually
checking that individual files matched would have probably worked, but
it'd be good to automate the process.
I can't say I've looked into any of this in detail, but you're right to
bring up the issue.

@_date: 2013-08-16 09:29:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP 32.5 
Myself I would trust neither the hardware wallet nor my computer...
So looks like the first version of the TREZOR won't support multisig -
how far away are we from support? What about other manufacturers?
P2SH support is probably going to be a major sticking point. The payment
protocol is all well and good, but it doesn't (yet) help getting money
to the individual. bitcoinj P2SH support for sending would be a major
help here - lots of person-to-person trades happen via Android wallets.

@_date: 2013-08-16 10:01:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
Part of anti-DoS should include making it easier for people to
contribute back to the network. Lowest hanging fruit:
1) SPV nodes with spare bandwidth should relay whole blocks to reduce
the load on full-nodes. The SPV security model is based on hashing power
anyway, so there is no major harm in doing so - if you have the
resources to fake blocks, you probably have the resources to sybil the
network anyway.
2) It's probably reasonable for SPV peers with bandwidth to be willing
to do bloom filtering on the behalf of peers that don't have spare
bandwidth. Hence they would set NODE_BLOOM, but not NODE_NETWORK. (or
more likely some more nuanced flags) Again this has to apply to full
blocks only unless we come up with some PoW scheme or similar to
discourage DoS attacks via invalid transactions. (maintaining partial
UTXO sets is another possibility, although a complex one)
Both examples work best with payment protocols where the recipient is
responsible for getting the transaction broadcast. Similarly you can by
default not connect to full-node peers, and then connect to them on
demand when you have a transaction to send.
Doing this also makes it more difficult to sybil the network - for
instance right now you can create "SPV honeypots" that allow incoming
connections only from SPV nodes, thus attracting a disproportionate % of
the total SPV population given a relatively small number of nodes. You
can then use that to harm SPV nodes by, for instance, making a % of
transactions be dropped deterministicly, either by the bloom matching
code, or when sent. Users unlucky enough to be surrounded by sybil nodes
will have their transactions mysteriously fail to arrive in their
wallets, or have their transactions mysteriously never confirm. Given
how few full nodes there are, it probably won't take very many honeypots
to pull off this attack, especially if you combine it with a
simultaneous max connections or bloom io attack to degrade the capacity
of honest nodes.
Deanonymization is another attack that can be done at the same time of
In any case, the more peers on the network that can relay data the
3) Make it easier to run a full node. IMO partial mode is the way to go
here. Note that from a legal perspective we really want to make sure
that running full nodes (and mining p2pool or solo) continue to be
something ordinary users do. We do not want to give the impression to
regulators that running a full node is in any way unusual or rare, and
thus something that might be practical or desirable to regulate. Users
should be given clear options about how much disk space and bandwidth to
donate to the health of the network, and within those limits nodes
should always try to make progress towards being full nodes, and in the
meantime being increasingly productive partial nodes.
Even with pure SPV nodes if they are relaying block data and ideally
transactions they make it much more difficult for regulations to be made
that, say, require full node operators to apply blacklists to
transactions in the mempool or in blocks.
4) This is really a side effect, and not directly DoS-related, but
unfortunately we're going to have to create some kind of
proof-of-UTXO-possession that miners include in the blocks they mine.
With partial mode it's just too easy and tempting for people to mine
blocks with fee paying transactions in them without actually having the
full UTXO set; such nodes can't tell if a block is invalid due to a fake
txin, and thus will happily extend a invalid chain. This possession
proof can probably be make part of a UTXO commitment.
Lots of off-chain tx solutions are popping up which will help push back
the issue's relevance as well. Also your micropayment channels possibly.

@_date: 2013-08-16 10:06:35
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
Have you looked into what it would take to just apply the IP diversity
tests for outgoing connections to incoming connections? The code's
already there...

@_date: 2013-08-16 10:15:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
Oh, here's an even better way to do the "tx drop" attack: when you drop
a transaction, make a fake one that pays the same scriptPubKeys with the
same amount, and send it to the SPV peer instead. They'll see the
transaction go through and show up in their wallet, but it'll look like
it got stuck and never confirmed. They'll soon wind up with a wallet
full of useless transactions, effectively locking them out of their
Here's another question for you Mike: So does bitcoinj have any
protections against peers flooding you with useless garbage? It'd be
easy to rack up a user's data bill for instance by just creating junk
unconfirmed transactions matching the bloom filter.

@_date: 2013-08-16 10:59:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
UPNP seems to work well for the reference client. What's the situation
there on Android?
I leave my phone plugged in and connected via wifi for most of the day;
lots of people do that.
The user interface for this stuff is very simple: "How much bandwidth
will you contribute back? If you contribute more bandwidth back, other
peers will prioritize you and your wallet will be more reliable."

@_date: 2013-08-16 11:59:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
Do find out.
Also worth finding out.
Appeal to authority.
Stop bringing up Satoshi's "vision" - our understanding of
crypto-currencies has improved in the 4.5 years since Bitcoin was
released. Satoshi didn't even forsee pool mining, which says a lot about
his economic judgement.
Right, so you're giving priority to peers that have been around for
awhile. You've succeeded in forcing attackers to wait a bit.
A) What's the definition of a peer? What stops me from pretending to be
100 peers?
B) Given an attacker willing to wait, what's your plan?

@_date: 2013-08-17 22:59:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM BIP 
My draft is as follows.
Gregory Maxwell: Can you assign a BIP # for this? The next number, 38,
is on the wiki as "Passphrase-protected private key" by Mike Caldwell,
although it isn't in the list so I don't know if that is official or
BIP: ?
Title: NODE_BLOOM service bit
Author: Peter Todd Type: Standards Track (draft)
Created: 17-08-2013
This BIP extends BIP 37, Connection Bloom filtering, by defining a service bit
to allow peers to advertise that they support bloom filters explicitly.
BIP 37 did not specify a service bit for the bloom filter service, thus
implicitly assuming that all nodes that serve peers data support it. There are
however cases where a node may want to provide data, such as mempool
transactions and blocks, but do not want to or have not implemented bloom
filtering. Additionally it is good practice for nodes to be given options as to
the granularity of the services they are providing the public - a full-node
operator may be able to donate only a small amount of bandwidth and may want
those efforts to be used by other full-node operators.
The following protocol bit is added:
    NODE_BLOOM = (1 << 1)
In addition the protocol version is increased from 70001 to 70002 in the
reference implementation. Nodes that support bloom filters should set that
protocol bit. Otherwise it should remain unset.
NODE_BLOOM is distinct from NODE_NETWORK, and it is legal to advertise
NODE_BLOOM but not NODE_NETWORK.
If a node does not support bloom filters but receives a "filterload",
"filteradd", or "filterclear" message from a peer the node should disconnect
that peer immediately.
While outside the scope of this BIP it is suggested that DNS seeds and other
peer discovery mechanisms support the ability to specify the services required;
current implementations simply check only that NODE_NETWORK is set.
Design rational
A service bit was chosen as applying a bloom filter is a service.
The increase in protocol version is for backwards compatibility. Nodes that
require the bloom filter service can set NODE_BLOOM for peers advertising a
protocol version < 70002, allowing the rest of the implementation to be
unchanged. Nodes with implementations that do not know of the NODE_BLOOM bit
will be disconnected immediately as though the connection failed for some
reason, and thus will not have incoming bandwidth wasted by that peer and can
easily connect to another peer.
Supporting NODE_BLOOM but not NODE_NETWORK allows for situations where a node
may have data that its peers may be interested in, but is not a full node and
thus does not have block data in general. For instance an SPV node that
receives a full, unfiltered, block from a peer may want to let its SPV peers
know about the existence of that block and provide them that data if requested.
Those peers in turn may only be interested in knowing about the block if it
matches a specific bloom filter. Note how in this example DoS attacks are made
prohibitively expensive by the work required to create a valid block header.
Reference Implementation
This document is placed in the public domain.

@_date: 2013-08-18 18:59:30
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM BIP 
Bloom filtering isn't lossless so to speak.
A better analogy would be making re-sharing an optinal part of the
BitTorrent specification, and then expecting that the majority of users
would never upload data to peers that needed it.

@_date: 2013-08-18 19:11:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM BIP 
Mike's and others have been talking about persistent node-specific
identifiers, and after all at this level there are IP addresses;
fingerprinting nodes is trivial.
We need options so peopl can contribute to relaying and the health of
the network - these edge cases are going to be tested anyway by people
like me deciding to disable bloom filtering.

@_date: 2013-08-18 20:13:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom io attack effectiveness 
Did some tests with a varient of attack... In short it's fairly easy to
saturate a node's disk IO bandwidth and when that happens the node
quickly falls behind in consensus, not to mention becomes useless to
it's peers. Note that the particular varient I tried is different, and
less efficient in bandwidth, than others discussed privately.
Bandwidth required to, for example, take out a Amazon EC2 m1.small is
about 1KiB/second, and results in it getting multiple blocks behind in
consensus, or a delay on the order of minutes to tens of minutes. I had
similar results attacking a p2pool node I own that has a harddrive and
4GiB of ram - of course my orphan rate went to 100%
It'd be interesting to repeat the attack by distributing it from
multiple peers rather than from a single source. At that point the
attack could be made indistinguishable from a bunch of SPV wallets
rescanning the chain for old transactions.
In any case given that SPV peers don't contribute back to the network
they should obviously be heavily deprioritized and served only with
whatever resources a node has spare. The more interesting question is
how do you make it possible for SPV nodes to gain priority over an
attacker? It has to be some kind of limited resource - schemes that rely
on things like prioritizing long-lived identities fail against patient
attackers - time doesn't make an identity expensive if the identity is
free in the first place. Similarly summing up the fees paid by
transactions relayed from that peer also fail, because an attacker can
easily broadcast the same transaction to multiple peers at once - it's
not a limited resource. Bandwidth is limited, but orders of magnitude
cheaper for the attacker than a Android wallet on a dataplan.

@_date: 2013-08-18 21:34:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom io attack effectiveness 
Don't read too much into what I said; under normal circumstances when a
Bitcoin node isn't being attacked, there will be plenty of spare
capacity for SPV nodes. All I'm suggesting is that we make sure serving
those nodes doesn't come at the expense of maintaining consensus -
mainly distributing new blocks around the network so the blockchain
keeps moving forward. (something many SPV peers can help with anyway)
I'd much rather my Android wallet take a long time to sync up, than
blocks get re-organized because miners found themselves separated from
one another, let along someone clever using that to do double-spend
attacks during those re-orgs. After all, I can always find a private
node to connect to that won't be overloaded because it doesn't accept
connections from anonymous users. It's also easy to just use really
basic measures to limit abuse, like "sign up with your bitcointalk"
account, or "pay 10 cents for an account to discourage spammers"
Anyway all things less likely to be needed if attackers know all they
are going to manage to do is inconvenience people temporarily because
the network itself will keep running.

@_date: 2013-08-18 23:17:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
Would you please delay these bounties for a few weeks while things
settle down. The service bit selection is fine, but for the other two a
month would be appreciated.

@_date: 2013-12-01 13:12:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Bitcoin is and always will be limited in capacity - transactions may not
confirm in a reasonable about of time because of high-demand and/or DoS
attacks. Giving senders and/or receivers the ability to increase fees
after the fact is the only way you'll ever be able to deal with these
situations. Of course, in those situations revoke isn't going to be 100%
reliable until the txins get spent elsewhere, but that just indicates
the UI problem is around that kind of functionality is subtle.
re: merchants paying tx fees, child-pays-for-parent is inefficient, and
micropayments direct to miners isn't implemented. (though I did write up
a rough sketch of how to do that in a decentralized fashion on
 Propose something concrete.

@_date: 2013-12-01 13:37:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Maybe, maybe not. We have no idea what fees will be because the system's
entire capacity is, and always will be, limited. That's just how
fundementally unscalable systems with huge global state work. What
demand will be for that limited capacity is unknown.
No, Luke's existing code uses good algorithms with O(n) scaling for n
transactions. The inefficiency is needing a second transaction, bloating
the blockchain and driving up fees.

@_date: 2013-12-03 06:03:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Fees are per byte of tx data; call it allowfeeperkb, and given that fees
are required - the merchant would really rather not waste up to about
twice as much on fees for a child-pays-for-parent - it should be called
Back to your point, the merchant wants to limit total fees that have
been deducted - 'allowfee' is still a good idea - but only in
conjunction with specifying fee-per-kb requirements.
UI once both are implemented is to not show anything in the default
case, and explain to the user why they have to pay extra in the unusual
case where they are spending a whole bunch of dust.

@_date: 2013-12-03 06:31:17
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
FTFY s/hard-fork/soft-fork/
The proposals on the forums with regard to a soft-fork are for a new
transaction format - to be done in parallel with the old one - that
commits to transaction inputs and outputs via a merkle tree extending
into the transaction. This data is then committed to via the merge-mine
In addition, all this discussion about trying to figure out transaction
fees based on transaction is a bit irrelevant if you suppose a
soft-fork. We already know that we need a merkle-sum-tree of fees and
transaction size to be able to audit miners and create compact fraud
proofs, and using that merkle-sum-tree you can easily calculate sum
fees/sum size for the whole block to determine fees.
We know that we can make a good assumption that transactions in
blocks will have been broadcast prior by the assumption you and Gavin
are making that miners would by far prefer to mine transactions that
have already been broadcast so that block propagation via txid lists
works. That advantage is on the order of 10x to 100x (or even higher)
lower cost per KB to the miner. (if this is not true, let us all know,
because then your scalability arguments are bunk with regard to
In addition faking fees via non-disclosed high-fee transactions has a
cost of about 1% due to the risk your block gets orphaned and the tx fee
gets mined by another miner.
Between those two the merkle-sum-trees for fees and size can be used for
various levels of average fee for a whole block, plus statistical
Next it is also desirable for another, related, soft-fork to include a
sorted list of txids and/or H(scriptPubKey)'s in a block. (the latter is
explicitly part of my TXO commitments proposal) With the txids version
especially it's easy and low-bandwidth for SPV nodes to get proof that a
given transaction has been mined recently, by asking for proof of
inclusion or exclusion to accompany a bloom filter match.
Finally with various anti-sybil techniques that create long-term
identities it's easy to show that a peer lied about the data required
for fee estimates anyway.
Propagation failures are not a major problem if transactions can be
rebroadcast with new, higher, fees; propagation failures can be detected
easily and quickly with my proof-of-tx-mining proposal.

@_date: 2013-12-03 06:33:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Right, which is solved by requiring a fee-per-kb, and only allowing up
to a certain amount to be deducted from the amount paid to pay that
total fee.
But really, we're better off leaving fees visible to the user in the
first place: they're how Bitcoin works and it's not going to change.
(was just talking to Taylor from Hive Wallet about that in person

@_date: 2013-12-03 06:37:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Person-to-person payments are an *excellent* argument for keeping fees
visible to end-users; people will pay other people commonly in Bitcoin
and they will be very confused if those transactions act weirdly
differently than payments to merchants.
NAK on unconfirmed overrides - if something goes wrong even by accident
it just makes fixing the problem much harder and less intuitive.

@_date: 2013-12-03 07:07:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Transparency on fees is going to be good from a marketing point of view
as well: fact is, Bitcoin transations have fees involved, and if we're
up-front and honest about those fees and what they are and why, we
demystify the system and give people the confidence to tell others about
the cost-advantages of Bitcoin, and at the same time, combat fud about
fees with accurate and honest information.

@_date: 2013-12-04 11:57:48
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Here at the dark wallet conf there seems yo be rough consensus that replacement for fee bumping is a good thing and should be supported; I was talking to Taylor from hive specifically yesterday. The code is trivial on the node side of things and doesn't need consent of anymore than a small minority, and coinjoin forces wallets to handle double spends well anyway. I haven't heard anyone caring about zeroconf safety.
I'll be proposing it for "formal" inclusion in our wallet best practices guidelines.
Also fwiw apparently libbitcoin already implements a memory limited mempool and Amir is open to the idea of it using the satoshi consensus critical code for block validity. (therefor fairly safe mining) I wouldn't be surprised if libbitcoin based nodes start getting usage, and with a limited mempool it is very DoS attack safe for them to relay replacements regardless of miner support.

@_date: 2013-12-04 08:06:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
replace-by-fee is no less speculative than your original proposals;
you're also trying to convince people that things should work
differently re: fees

@_date: 2013-12-04 16:51:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Floating fees and SPV clients 
Other than you, replacement for fee changing isn't controversial; I know
this because no-one other than you comments on it... just like the
fundemental changes involving your proposed hardfork presumably. (which
I did comment on)
Besides, "Happily, there does not have to be One Correct Answer here.
Let wallets compete, and may the best user experience win..."

@_date: 2013-12-13 21:43:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Merge avoidance and P2P 
Hash: SHA256
So, vendors hat on, what would it take for, say, BitPay to implement merge avoidance and coinjoin together?
At the dark wallet hackathon when we were talking usability we decided that the main way to get coinjoin working well is to take advantage of non-time-critical payments to act as counterparties to time-critical payments. For instance BitPay could schedule a vendor payment to happen in full by some time in the future, say 1 day, and send the funds in one or more joins. The actual amounts sent in each tx are then picked to match the amounts desired by the counterparty who needs funds sent right now.
We expect this to be first implemented just as a "anonymize my coins" button for wallet software on always on machines; getting vendors on board would be gravy.
We may even allow joins to happen when one party pays less fees than the other, although this is tricky: the main Sybil resistance of coinjoin is fees so you don't want to overdo it. OTOH the idea of the NSA and Chinese equivalent wasting money completing each others joins is hilarious...

@_date: 2013-12-19 08:17:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] DarkWallet Best Practices 
Here's my draft. I don't claim this to be "official", but I think this
should represent the consensus we've come to at the DarkWallet
Hackathon; I'm writing it up as an email in part to preserve a record of
that consensus.
* General Principles
We believe in decentralization, user-defined privacy, education as
opposed to "magic", and security based on openness rather than just
trust. We consider users who are individuals as well as the needs of
businesses such as merchants and exchanges. We recognize that often the
more people who use privacy protection technologies such as CoinJoin,
the better protected we all are.
* Privacy
Bitcoin inherently makes the flow of money visible, however it does not
require that flow to have real-world identities attached, or even for
that matter, pseudonyms. We see this as an unfortunate flaw in the
Bitcoin protocol that is to be corrected; the Satoshi whitepaper itself
included one such correction by outlining how avoiding address re-use
helped preserve privacy.
** Threat model
We assume a worst-case sophisticated state-level attacker with the goal
of deanonymizing and otherwise subverting Bitcoin users. Such an
attacker can be assumed to control up to 100% of the Bitcoin relay
network as well as have the ability to wiretap up to 100% of the
node-to-node traffic. (for nodes that they do not control) Such
attackers are however constrained by politics and budget. We assume they
use their ability to conduct MITM attacks sparingly - for instance by
subverting certificate authorities - due to the risk of detection. (note
how such attackers may be more willing to use detectable attacks in the
future now that their activities are public knowledge) We also assume
that while their budgets for individual targets may be very large, the
equally large number of targets required for en-mass survailance leads
to relatively low budgets per target. In particular note how the 51%
assumption assumes that the overall "economic value" of Bitcoin to its
participants is greater than the attacker's budget by some margin.
** Address re-use
Wallet software SHALL avoid address re-use. New addresses will be used
for all change and users will be encouraged through the user-interface
and other measures to use new addresses for every payment to the wallet.
** CoinJoin
With CoinJoin the more users that make use of it, the larger the
anonymity set and the better protected user privacy is. Thus we
encourage wallet software to agressively make trade-offs between
absolute privacy and usability; compromise is not a dirty word.
Wallet software SHALL implement basic two-party mix functionality and
MAY implement more sophisticated CoinJoin functionality such as n-party
mixes. CoinJoin SHALL be the default way that all transactions are sent.
Wallet authors are cautioned that more sophisticated functionality may
be more secure in theory, but if users do not use it the functionality
is wasted; focus on the general case first and only then try to improve.
*** Two-Party Mixes
The most basic form of CoinJoin is for two parties to author a
transaction. A key distinction between a 2 party mix and an n-party mix
is that in the two party case both parties automatically learn the
other's inputs and outputs by simple elimination; sophisticated
cryptographic blinding protocols are useless. To an external attacker
each transaction doubles the size of the anonymity set: the coins may
have come from one party or the other and the attacker has no way of
knowing which. (modulo value analysis, which will be discussed later)
*** n-party Mixes and Blinding
If two parties can jointly author a transaction, n parties can too.
Without special effort this has the disadvantage of revealing the input
to output mapping to all parties. Various cryptographic blinding schemes
have been proposed to fix this problem, either with multi-party
computational techniques, or by making use of multiple communication
channels along with a robust anti-DoS scheme. In either case, for now we
reject such schemes as complex and inconvenient and prioritize robust
two-party mixing. However we do take the existance of such schemes into
account; note how a n-party mix can act as a single party in a two-party
mix scheme.
*** Four-stage two-party mix protocol
*** Defeating value analysis
Attackers can make good guesses at the mapping of inputs to outputs
based on value. For instance with two inputs of 3 and 5, and fours
outputs of 1.4, 1.6, 2.4 and 2.6 the attacker can easily map inputs to
outputs based on what values match up, in this case 3 split into 1.6 and
1.4, and 5 split into 2.4 and 2.6
**** Value Matching
Not all CoinJoin users need their transactions to have specific output
amounts; some users simply need to move money from one place to another
but do not need a specific amount moved or at a specific time. These
users can assist users with more stringent requirements by matching the
input or output values they request. As a general principle wallets
SHOULD make these anonymity optimizations possible by allowing users to
schedule transactions to complete by a specific time and/or allow users
to specify that they do not wish the transaction to happen unless
CoinJoin is used.
With four-stage two-party mixes the Alice, who advertised a desire to do
a transaction first, can easily do ths by picking the transaction output
amounts only after Bob replies with his desired inputs and outputs, and
picking those amounts so they match Bob's. (or some combination of Bob's
**** Merge Avoidance
Merge avoidance is the practice of avoiding the merging of multiple
transaction inputs into a single new transaction output, thus implying a
common relationship between those inputs. The most primitive form of
merge avoidance is to create multiple individual transactions, each
moving single transaction input to an output. (or perhaps some small
number) This is of course inefficient, and appears to have only been
proposed as a means to still allow for coin blacklists to function while
preserving some financial privacy.
Combined with CoinJoin however merge avoidance becomes much more
powerful. For instance even in its most simple form multiple parties can
combine their merge-avoiding transaction sets, giving even transactions
without actual counterparties a useful measure of plausible deniability.
In addition the underlyng features that make merge-avoidance possible -
the ability of a recipient to designate they are willing to receive
payments made to multiple addresses - synergisticly make very
sophisticated value matching strategies possible.
***** Cut-thru payments
Related to merge avoidance the idea of a cut-thru payment is that if an
intermediary is both a debitor and a creditor, with sophisticated
payment protocols they can request incoming payments to directly pay
outgoing liabilities, skipping them as an intermediary. While premature
to implement this feature now, it is worth thinking about for the future.
** Tor
While Tor isn't perfect there is real-world evidence - specifically the
Snowden leaks - that it works well enough to be considered a worthy
adversary by state-level attackers. Wallets MUST suppoort the basic
proxy options that allow the Tor proxy - or some other similar
technology - to be used for privacy enhancement and SHOULD make use of
Tor-specific features such as hidden services.
* Decentralization
** Fees
In a decentralized system distinguishing DoS attackers from legitimate
users is at best difficult, at worst impossible. Wallets that do not
provide users with the ability to set fees, both when a transaction is
created initially and after initial broadcast, do their users a
disservice by taking away a vital method of responding to an attack:
outspending the attacker.
Wallets MUST give users the ability to set the fee per KB they are
willing to pay for their transactions. Wallets SHOULD allow users to
change that fee after the fact via transction replacement. Wallets MAY
additionally implement fee estimation techniques, such as watching what
transactions in the mempool are finally mined, or using estimates
provided by miners. However it must be recognized that such data is
inherently unreliable, and this may become a problem in practice in the
future; giving users robust ways to alter fees after the fact will make
lying about fee data - perhaps to push fees upwards - less advantageous.
Note that the current direction of the Bitcoin Foundation maintained
reference implementation is weakly towards a pure estimation scheme;
deployment of full nodes supporting replacement and support from miners
is a precondition to doing things correctly.
*** Fees and privacy
Where there is a trade-off between fees and privacy - such as with merge
avoidance strategies - users should be given options to specify how much
extra they are willing to pay for extra privacy. Wallets SHOULD default
to being willing to pay some extra, perhaps 25% more over the basic fee.
** SPV, full nodes and partial nodes
Wallet software SHOULD whenever possible blur the distinctions between
full UTXO set nodes, SPV nodes, and partial UTXO set nodes. In addition
to those three basic categories there is also the question of whether or
not a node stores archival blockchain data, something that all three
categories of nodes can participate in.
Instead how a node contributes back to the health of the network should
be a function of what resources it has available to it. Of course in
some cases, like a phone wallet, that won't be very much, but for
desktop or business usage the resources available can be significant in
both bandwidth and storage capacity.
*** Relaying data
**** Blocks and blockheaders
Any node can safely relay blocks and block headers, where "safely" is
defined as SPV-level security. Our threat model implies that we don't
trust random peers on the network, thus we are not relying on them for
block validity; as a SPV node we are relying on miners to do validity
checking for us. In short feel free to relay data that you yourself
would trust.
**** Transactions
Remember that relaying transactions has a DoS-attack risk; the Bitcoin
model relies entirely on mining fees and/or priority as the limited
resource to prevent DoS attacks. Thus at present nodes SHOULD NOT relay
transactions if they do not have an up-to-date copy of the relevant
parts of the UTXO set spent by the transaction. (relaying transactions
spending only inputs in a partial UTXO set is acceptable):
**** Block-header diversity
Wallet software MUST make it possible to get block-header information
from a diverse set of sources. These sources SHOULD comprise more than
just peers on a single P2P network. Remember that it is acceptable to
use even centralized sources in addition to decentralized ones for
blockheader data - knowing that a miner did the work required to create
a block header is always valuable information. (for reasonable amounts
of work) For instance the author's block headers over twitter project -
while an April Fools joke - is equally a perfectly reasonable backup
source of blockheader data.
** Updating wallets from blockchain data
In an ideal world wallets wouldn't need to sync their state with
blockchain data at all: pervasive use of payment protocols would have
senders send txout proofs directly to recipients. But that's not the
case. Instead wallet implementations sync themselves from the
blockchain, and when bandwidth limited this becomes a tradeoff between
bandwidth and privacy: your transactions hide in the anonymity set of
the false positives matched by the filter.
*** Bloom filters
The current implementation for SPV nodes is to simply give peers a bloom
filter; the false-positives make the anonymity set. For n peers this has
O(n) cost when a new block comes in; Bloom filters are cheap to test
against and this system works reasonably well.
However, for archival blockchain data bloom filters are seriously
flawed: every block has to be read from disk in full, the bloom filter
matched, and some (potentially very small!) subset sent to the peer. n
peers. The result is high IO load on the node relative to the client,
enabling easy DoS attacks.
Wallet software SHOULD NOT implement only Bloom filters, however using
them when availalbe is acceptable. Note how the Bloom filter design has
at best O(n^2) scaling ruling it out for large-blocksize future
*** Prefix filters
TXO or UTXO data can be easily indexed by in radix trees with log2(k)
lookup cost per query. We can take advantage of the fact that the query
keys need not be provided in full by only providing partial keys.
Because scriptPubKeys are randomly distributed a prefix n bits long has
an anonymity set of roughly 1/2^n * # of transactions in total.
Wallet software SHOULD implement prefix filters and SHOULD use them in
preference to bloom filters whenever available. Wallet software that
currently uses full-key filtering - e.g. Electrum - MUST be upgraded to
support prefix filters in the future.
Wallet software MUST NOT assume that matching anyting other than
H(scriptPubkey) is possible. This applies to bloom filter matches as
In the future miners may commit to either the TXO set in conjunction
with per-block lookup trees, or possibly the full UTXO set. In either
case many of the leading designs may be implemented with only
H(scriptPubKey) lookup capability for reasons of scalability.
* Security
Bitcoin wallet software is unprecedented in how they provide attackers
targets that are highly profitable to attack and highly liquid. (note
the irony here!) A succesfull attack that injects malicious theft
routines into either sourcecode or binaries can steal thousands of
Bitcoins in one go, and the attacks target is you and your team.
Following basic good practices for robust code is a start, but it's far
from enough.
** Source-code integrity
Sourcecode MUST be maintained using a revision control system that
provides strong integrity guarantees; git is recommended.
Sourcecode MUST be PGP signed on a regular basis. Releases MUST be
signed - in git this is accomplished by signing the release tag.
Individual commits SHOULD be signed, particularly if source-code used in
day-to-day development is kept on an untrusted server, e.g. github.
Recovering from a server compromise is made significantly easier if
every commit is securely signed.
** Binary integrity
All things being equal it is better to use an interpreted language
rather than a compiled one; auditing the former is significantly easier
than the latter. Similarly, all things being equal, do not distribute
binaries of your software - have end-users compile binaries themselves.
Of course all things are not equal, and frequently compiled languages
and distributing binaries is the correct choice. If that is the case
deterministic build systems MUST be used when possible; if using them is
not possible take great care with the process by which binaries are
created and try to create long-term plans to move to a deterministic
build system in the future.
** PGP
Developers of wallet software MUST make use of PGP and participate in
the web-of-trust. Developers MUST advertise their PGP fingerprint
widely, for instance on personal websites, forum profiles, business
cards etc. simultaneously. Multiple paths by which someone can find a
fingerprint claimed to be of some developer make subterfuge easier to
detect and more costly to carry out. When possible it is highly
recommended to attach these advertisements to real-world, physical,
actions. For instance the author has included his PGP fingerprint in
highly public, videotaped, talks he has given at conferences. He has
also created a videotaped statement of his PGP key that was timestamped
in the Bitcoin blockchain. While it certainly is possible for such
artifacts to be faked, doing so convincingly is expensive, labour
intensive, and error prone.
Developers SHOULD sign as much communication as practical. Sourcecode is
one form; your emails to development lists and between each other are
another. Signing both leaves a large body of widely distributed work,
all tied to your identity. (it's highly unfortunate that nearly all
publicly available mail archives do not make mail accessible to the
public in such a way as to allow verification of PGP signatures; often
even inline signatures are broken for various reasons)
*** Increasing adoption of PGP
Keep in mind that end-users very rarely verify PGP fingerprints at all,
let alone correctly - the above advice with regard to PGP is currently
mostly useful in allowing *other developers* the tools they need to
verify the integrity of your code. For instance, in the event of a
suspected compromise consistantly signed code allows anyone competent in
the use of PGP to quickly evaluate the situation, and if warrented,
inform less sophisticated users through traditional measures such as the
While this is somewhat out of scope for this document the "DarkWallet
effort" should include work to make PGP more user-friendly and a better
experience. But that does *not* have to mean "making PGP easier for
grama", right now "making PGP easier for Joe Wallet Developer" is a
laudable goal. For instance understanding and using the web-of-trust
sucks right now. How can we make that experience better for a user who
understands the basics of cryptography?
** SSL/Certificate authorties
While certificate authorities are notoriously bad at the job they are
supposed to be doing the CA system is still better than nothing - use it
where appropriate. For instance if you have a website advertising your
software, use https rather than http.
** Multi-factor spend authorization, AKA multisig wallets
Assuming any individual device is uncompromised is risky; wallet
software SHOULD support some form of multi-factor protection of some or
all wallet funds. Note that this is a weak "should"; mainly we want to
ensure that users have options to keep large quantities of funds secure;
accepting higher risks for smaller quantities is an acceptable
FIXME: go into more detail.
*** P2SH
Wallet software MUST support paying funds to P2SH addresses.
** Payment Integrity
Multi-factor wallets protect your funds from being spent without your
knowledge, but they provide no assurance about where those funds went; a
Bitcoin address is not an identity. A payment protocol, such as BIP70,
is needed.
Wallet software SHOULD support BIP70. Yes, there are (justified)
concerns about its current dependence on the certificate authority
system, but those concerns should be addressed by a combination of
fixing that system, and extending BIP70 with different and better
identity verification options.
However, remember that in the absense of multi-factor wallets the "know
who you are paying" protections of BIP70 are actually pretty much
useless; malware that defeats the payment protocol locally is not much
different than malware that silently rewrites Bitcoin addresses. There
are other motivations for the BIP70 version of the payment protocol, but
whether or not they are actually interesting for users is an open
question; it was not designed by user-experience experts. Thus wallet
authors should consider supporting a low priority for now.

@_date: 2013-12-19 12:44:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] DarkWallet Best Practices 
Looks like for this to actually go to the email lists they need to be in
the To: field.
His point is valid, but it's valid in the context of how Linux
development is done, not Bitcoin. The key difference being that Linus
and other kernel developers have a model where code is passed around on
mailing lists and between developers rather than stored on untrustworthy
third-parties like github.
For instance typically someone will submit a patch to the kernel
development mailing list, example:
 at vger.kernel.org/msg558841.html
That patch isn't signed, and the email itself doesn't have to be PGP
signed either. However a trusted maintainer of the relevant subsystem
will (in theory) look over the patch carefully and commit it to their
personal tree on a secure computer. (in theory)
At some point the maintainer will create a *signed* tag on a commit with
one or more patches, often many patches, another another maintainer
higher in the hierarchy (maybe even Linus) will *merge* that tag into
their tree, hopefully checking the signature first! Modern versions of
git actually include the tag signature in the merge commit, so the
result is signed by the original maintainer; note how this contradicts
Linus's email with regard to the idea of separable signatures.
Eventually multiple such groups of patches build up and the result is
tagged as a release, and that release tag is signed.
Accountability in this model rests with maintainers, and source-code
stays on a multitude of personal, secure, locations. (in theory)
However since we like to use github and tend to get code directly from
it our main risk is github (or similar) being compromised. Given that I
think we're much better off using per-commit signatures, and in effect
continually making the statement "Yes, this commit/merge was really
produced by me on my machine, although I may have made a mistake and
might not have looked at the code as thoroughly as I maybe should have."
The statement *is* weaker than Linus's model of "This signature is
Really Official and Stuff and I've carefully checked everything." but I
think we're much more interested in getting a strong guarantee on who
made the commit than some strong statement about its actual contents -
humans are fallible anyway.
I think you're conflating identities with the messaging layer; focus on
the latter and use off-the-shelf identity systems like OpenPGP and SSL
certificate authorities. Remember that every new identity system that
gets involved is another way for an attacker to MITM attack you; you're
better off using whatever the user is using already.

@_date: 2013-12-20 05:48:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Could you expand more on how prefix trees could be used for
time-stamping and merged mining?
I'd be inclined to leave the unicode out of the code examples as many
editors and shells still don't copy-and-paste it nicely. Using it in BIP
documents themselves is fine and often has advantages re: typesetting,
but using it in crypto examples like this just makes it harder to
reproduce the results by hand unnecessarily.

@_date: 2013-12-20 07:47:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Right, last txout in an OP_RETURN like we discussed.
Don't you mean the value is the hash of the document and the key is
What's the advantage over the direction-based system I proposed before?
Seems to me the code required to validate the proof is significantly
more complex in your scheme.
 at lists.sourceforge.net/msg03149.html
By 43 bytes you mean the whole op_return txout right?
That example is python, so I'd suggest just using escape sequences
myself. You probably also should include the "b" prefix to make the
strings explicitly binary for py3 compatibility, ie dict[b'\xbe\xef']

@_date: 2013-12-20 08:17:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
I've thought about this for awhile and come to the conclusion that UTXO
commitments are a really bad idea. I myself wanted to see them
implemented about a year ago for fidelity bonded banks, but I've changed
my mind and I hope you do too.
They force miners and every full node with SPV clients to store the
entire UTXO set in perpetuity. This is bad by itself, but then they make
it even worse by making Bitcoin really useful and convenient to use as a
decentralized database; UTXO commitments make it easy and convenient to
implement systems like Namecoin on top of Bitcoin, yet we don't have the
UTXO expiration that might make such uses reasonable. Right now the UTXO
set is reasonable small - ~300MB - but that can and will change if we
make it an attractive way to store data. UTXO commitments do exactly
You're also *not* giving users what they actually want: the transactions
associated with their wallets. Even though Electrum could easily work
via a pure UTXO database they implemented transaction lookup instead;
Electrum servers cough up every transaction associated with a user's
wallet. If you're going to do that, it's just as easy to do per-block
lookup trees which don't force the UTXO set to be stored.
There's also a more subtle issue: the security model of UTXO commitments
sucks. It encourages wallets to essentially trust single confirmations
because it's unlikely that nodes will want to store the multiple copies
of the UTXO set required to provide proof of multiple confirmations.
Basically the issue is when you start your wallet you get a proof of
UTXO set for the most recent block; that's just one confirmation. To get
more confirmations you have to wait for subsequent blocks, checking the
set on each block. Per block indexes on the other hand naturally lead
wallets to count confirmations properly.
IMO you should take this technology to Namecoin instead. For them the
fast lookups are probably worth the trade-offs, and they expire domains
so the total set size doesn't grow unbounded.

@_date: 2013-12-20 11:54:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Censorship-resistance via timelock crypto for 
Embedded consensus systems such as Mastercoin face the risk that the
data they need to embed in the host consensus system will be subject to
censorship. This censorship can take two forms: whitelists and
blacklists. The former we can't do anything about, however the latter
requires someone to identify the data-carrying transactions that are to
be blacklisted.
Embedding data steganographically in transactions is known to be
possible in ways that can-not be detected. Even if P2SH^2 (1) is
implemented data can be hidden in pubkeys in P2SH scriptSigs, either by
using unused pubkeys in CHECKMULTISIG transactions with a simple
transform(2) to turn arbitrary data into valid-looking pubkeys, or with
some ECC math even usable pubkeys can have data hidden in them.(3)
However these methods are unsuitable if the data needs to be provably
made public; without the encryption key the data is securely hidden.
Almost all consensus systems rely on proof-of-publication(4) and even if
the encryption keys are later made public - perhaps by broadcasting them
on a P2P network - we've only shifted the problem to proving that the
keys were released. Of course, if we then publish them via our host
consensus system, *that* act of publishing can itself be censored!
Timelock cryptography offers a solution to this problem. Let S(n, k) be
a sequential-hard strengthening function that takes key k and number of
rounds n, outputting k'. A suitable S() might be the scrypt function.
Let E(d, k) be a symmetric encryption algorithm. Finally let H(m) be a
cryptographic hash function.
To hide data D in a transaction we set k to some random and publicly
known value in the transaction and compute k'=S(n, k) and D'=E(D, k')
Then D' is hidden in the transaction, perhaps in an unused pubkey of a
CHECKMULTISIG scriptPubKey.
Our intended audience can also calculate k' from the public data, and
thus recover D in time ~t, thus we know that after time ~t has elapsed
all participants in the system can reliably come to consensus.
However miners and other parties who may wish to censor D face a
dilemma: If they repeat the calculation for every transaction that may
be hiding data they delay all transactions for all users. In addition
miners have a financial incentive to defect and mine transactions
without checking for hidden data.
Practical Considerations
Efforts should be made to limit the scope of possible transactions as
much as possible to reduce the computation required, e.g. by restricting
the search space to only transactions with scriptPubKeys starting with
some short prefix. This is a balance between computation and censorship
Consideration needs to be made as to how the data will be validated as
part of the embedded consensus system, for instance via a checksum or
cryptographic signature.
Participates in the embedded consensus system should share k' keys among
each other to reduce overall effort. This ties back to validation: it
must not be possible to distribute a fake k' undetectably.
Picking n, and thus the time taken, is a balance. Also there should be
some mechanism to update n as technological improvements warrant. Along
those lines this method works best when t can be large and immediate
consensus is not required. A suitable use-case could be a key-value
consensus system for name information where mappings are infrequently
The source of k should be such that k' can be computed in advance,
however only by the sender. For instance simply using the first txin
hash allows the attacker to compute k' in advance themselves. A better
choice would be the first (real) pubkey in a scriptPubKey, a value we
can both compute in advance, yet is not known publicly.
Censorship resistant voting
With due care the scheme can be used to allow for censorship-resistant
voting. While previously it was believed that miners would inevitably be
able to censor any voting scheme - with the exception of certain special
cases(5) - provided that the financial incentive to collect fees
outweighs the incentive to not count votes we have strong censorship
resistance with strong consensus in a fixed amount of time.
1) Gregory Maxwell, [Bitcoin-development] To prevent arbitrary data
   storage in txouts ? The Ultimate Solution,
    at lists.sourceforge.net/msg01987.html
2) Peter Todd, Re: MasterCoin: New Protocol Layer Starting From ?The
   Exodus Address?,
   3) ByteCoin, Untraceable transactions which can contain a secure message
   are inevitable, 4) Peter Todd, [Bitcoin-development] Disentangling Crypto-Coin Mining:
   Timestamping, Proof-of-Publication, and Validation,
    at lists.sourceforge.net/msg03307.html
5) John Dillon, Proposal: We should vote on the blocksize limit with proof-of-stake
   voting,

@_date: 2013-12-24 09:02:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Peer Discovery and Overlay 
The logic is that by simply connecting to peers at random you keep the
network structure as a whole randomized. You don't need to make any
specific attempt at connecting to "far-apart" peers.
Keep in mind it's easy for better knowledge of the network to be a
vulnerability; the number of full nodes is small enough that DoS
attacking all of them is quite feasible.
The other big vulnerability is that getaddr data is best effort; we
currently have no mechanism to ensure that nodes are in fact operated by
separate individuals. It'd be quite easy for someone to set up a
relatively small number of nodes that only advertise themselves in the
getaddr information. Over time they would get proportionally more
incoming connections than is "fair"
As for node addresses being a service, that's what the DNS seeds are!
bitcoinj clients, for instance, depend very heavily on those seeds and
can be easily compromised in a variety of ways by them.

@_date: 2013-12-30 18:22:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] Looking for GREAT C++ developer for 
I would strongly suggest that if you have not done so already you hire
someone competent to do an analysis of whether or not your idea makes
sense at all; that you are using merge-mining is a red-flag because
without majority, or at least near-majority, hashing power an attacker
can 51% attack your altcoin at negligible cost by re-using existing
hashing power. If you are starting a timestamping service that may be an
exception, but how to turn a profit doing so is non-obvious.
I would offer that consulting myself, but it would likely be a conflict
of interest with my employers. I'd be happy to speak informally in
private, but am explicitly unwilling to agree to any
non-compete/non-disclosure terms.

@_date: 2013-02-08 06:01:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Blockchain as root CA for payment protocol 
Why don't you use namecoin or another alt-chain for this?
The UTXO set is the most expensive part of the blockchain because it
must be stored in memory with fast access times. It's good that you have
designed the system so that the addresses can be revoked, removing them
from the UTXO set, but it still will encourage the exact same type of
ugly squatting behavior we've already seen with first-bits, and again
it'll have a significant cost to the network going forward for purposes
that do not need to be done on the block chain.
In  you say that you have
a minimum amount required for an outpoint to be valid, set at 0.05BTC.
That's a nice touch, and sort of works because this is a consensus
protocol, but if the exchange rate climbs significantly there will be a
lot of pressure to reduce that value. (setting minimum value by chain
height) What will happen then is there will be a mad rush to squat on
previously unaffordable domains, further disrupting Bitcoin's purpose as
a financial network.
In addition you'll also have a second problem: squatting of subsequent
transactions, particularly for valuable bcvalues. Basically if someone
already has "microsoft" insert bcvalues after their tx in case they
accidentally spend it. Of course, this will be done by people buying
bcvalues as well. Again, all this further clogs up the UTXO set.
I also can't figure out why you say signature lookup and verification
takes 10s - this should be an O(1) operation if you maintain a mapping
of candidate pubkeys to linked-lists of sorted applicable transactions.
Finally, why is this implemented within the reference client? Use the
raw transaction API and make up your own database. If you want, create a
RPC command that allows you to query the UTXO set directly; this would
be a useful feature to have. This patch will never be accepted to the
reference client, so you'll just wind up having to maintain a fork. Even
for a prototype this approach is ill-advised - prototypes have a bad way
of turning into production code.
In short, please don't do this.

@_date: 2013-02-11 06:21:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Blockchain as root CA for payment protocol 
In what way are you not solving the same problem as DNS? I don't mean
the Luke-Jr's (quite correct) technical point about key-value maps, I
mean the human problem that I have these unique numbers that I can't
memorize, and I have some non-unique names that I can.
By creating Yet Another Totally Different System you are just creating
another way that users can be confused into thinking some name snatched
up by some scammers in some little-used PKI system is who they are
supposed to be communicating with. Fortunately your PKI system isn't
actually used and probably never will be, so it's not a big deal yet,
but ultimately you are adding to the problem.
Go work on namecoin and make it more usable. Then add some PKI to it
using the *same* domain names so when I see a PKI certificate for "foo"
I know it must be the same "foo" website I just visited and the same
"foo at foo" I just emailed.
Alt-chains don't have to be based on mining you know. Your proof-of-work
can be replaced by proof-of-sacrifice, specifically Bitcoins. A
particularly nice scheme is to use transaction fees and Bitcoin block
height. Specifically every block in your alt-chain will have a merkle
path to a transaction in a Bitcoin block. Of course there can be more
than one such block, so you introduce a tie-breaker rule: the
transaction with the highest mining fee wins.
The reason why this is nice is because it becomes really easy to be sure
that a better chain won't turn up after the fact - make sure the
transaction linking the alt-chain to the Bitcoin block has the highest
fee in the block. Thus if you want to, say, register a domain name, do
so in the alt-chain, then "mine" the block by creating a suitable
transaction. Make sure it's the biggest fee, wait a few confirmations,
and you're good to go with the same level of security as Bitcoin proper.
Because the rule is that a merkle *path* exists, multiple alt-chains can
use this mechanism at the same time, with the exact same security
guarantee re: max fees. (note that you're chain needs to store copies of
the txin's for the tx sacrificing the fee, transactions by themselves do
not prove fees) Multiple parties can also colaborate to create the
transaction, each providing, and signing for, an input for their portion
of the total fee.
There is the problem that miners get to keep the fee, thus they can
create these special proof-of-sacrifice transactions at low cost, and
potentially make it difficult to get a block mined, or to be sure a
block won't be undone later. This problem can be solved with my
"two-step sacrifice" protocol.(1) Essentially you create a transaction
that is invalid until some time in the future and sacrifices Bitcoins to
mining fees, then create a second transaction that includes the first
one as data. You publish the second in the block chain, proving the
whole world had an opportunity to mine it. Eventually the first is in
fact mined, thus sacrificing Bitcoins to a miner you have no control
over. For a alt-chain you would consider the sacrifice to be a "balance"
and then spend that balance as required in later blocks in a way that is
guaranteed to be public so you can still check the security guarantee of
knowing your tx had the max fee. For instance with the contract protocol
I describe in (1), shave off what ever percentage of the original
sacrifice, linking the merkle-root of the merkel tree of alt-chains at
the same time. Anyone can still monitor the set of all two-step
sacrifices and associated contract movements and check that their one in
a block was the largest possible. Finally if you want to be nice, modify
the contract value rules so only the successful max contract value tx
has it's balance decreased.
1) Actually, you know gmaxwell, the above would be a great way to run the
alt-chain I'm probably going to need for the fraud proofs in Trustbits.
Although it does have the minor problem of being ludicrously complex...
The blockchain grows at a maximum rate of 55GiB/year. Do you think your
users will all want to have that available just to validate some PKI

@_date: 2013-02-12 10:11:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] RFC: empty scriptPubKeys and OP_RETURN for 
In my fidelity bond protocol (1) I'm proposing the use of two possible
new features:
The first is the use of OP_RETURN at the end of a scriptPubKey to
designate that the txout can be immediately pruned as it is obviously
unspendable. My use-case is the publish part of the two-step
publish-sacrifice protocol. I specifically want to use OP_RETURN rather
than a spendable scriptPubKey like   OP_CHECKSIG
so that implementors can not get lazy and fail to actually write the
code to spend the non-standard outputs created, thus polluting the UTXO
set. Simply using  by itself as the scriptPubKey -
spendable with an empty scriptSig - is another possiblity, but I suspect
no-one will want to spend the tx fees to clean up those txouts; note how
long it took for someone to bother doing that with p2pools share chain
hash txout, and the effort(2) seems to have been a one-time experiment.
Of course, P2Pool itself could use this mechanism too.
OP_RETURN marks the script as invalid upon execution, and since a script
is invalid if an OP_IF or OP_ELSE is not terminated with OP_ENDIF it is
guaranteed to execute. (there is no op-code that marks a script as valid
and returns immediately) OP_FALSE is another possibility too; I don't
see clear advantages for one or the other modulo OP_FALSE's more
intuitive name.
Finally OP_VERIF and OP_VERNOTIF say that "Transaction is invalid even
when occuring in an unexecuted OP_IF branch" on the wiki, although a
look at EvalScript() leaves me less than convinced this is true.  More
to the point, the mechanism should be something that is as unlikely as
possible to have different behavior in alternate implementations.
(remember that often only valid transactions are put in unittests by
lazy implementors)
OP_RETURN doesn't need any special support in the reference client yet
nor am I suggesting to make it a standard transaction type, but I would
like some feedback on if the idea itself is reasonable.
The second idea is the use of an empty scriptPubKey to create trivially
spendable outputs; provide the the scriptKey OP_TRUE or similar. For
fidelity bonds the advantage is to create a mechanism where even
non-miners have a chance at taking the funds sacrificed, and thus
increase the incentive to scan the blockchain for sacrifices and makes
it more likely for the sacrifice to be a true sacrifice of value. An
additional advantage is you avoid having to provide the txin to prove
the value of the mining fee. The advantage over just using a pubkey with
a known secret key is that the transaction size is shorter; remember
that the sacrifice transaction has to be published as serialized data in
a prior transaction.
In the future another use would be as a way of multiple parties to
collectively sign an assurance contract(3) donating to miners. This is
effectively a mining fee because miners who chose to include the
transaction can always chose to include an additional transfer from the
txout to a scriptPubKey only they can spend.
For the purpose of fidelity bonds ideally an empty scriptPubKey spent by
the scriptSig OP_TRUE would be made a standard transaction type to make
collecting the funds as easy as possible until miners start doing so
themselves. Having it a standard transaction type would also make it
easier for miners to implement the code to do this themselves; in
particular this discourages them from just allowing all non-standard
transactions. The main disadvantage I see is that it makes it easier for
people with buggy custom transaction code to accidentally lose their
Again, thoughts?
1) 2) See the transactions associated with 1HfA1KHC7bT1hnPPCjpj9CB4koLM4Hz8Va
3)

@_date: 2013-02-12 23:12:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] RFC: empty scriptPubKeys and OP_RETURN 
So what exactly was the OP_RETURN bug anyway? I know it has something to
do with not executing the scriptSig and scriptPubKey separately
( but
commit 7f7f07 that you reference isn't in the tree, nor is 0.3.5 tagged.
You know, come to think of it, OP_FALSE doesn't get used by standard
transactions either, and it's behavior is a little odd in how it does
push to the stack. So lets make the standard OP_INVALIDOPCODE,
specifically 0xFF, and put it at the start of the scriptPubKey.
That's a good point. It would encourage efforts to identify as many
Bitcoin nodes as possible, particularly miners, and I don't think we
really want to incentivise that. It's not a problem unique to this
proposal - compromised private keys and SIGHASH_NONE (1) - but
fidelity bonds will give people incentive to develop the infrastructure
to exploit it.
    1) Speaking of, maybe I'm missing something, but if I have a
    transaction with one or more txin's and sign every signature with
    SIGHASH_SINGLE, what stops an attacker from adding their own txout
    at the end and diverting the mining fee to themselves?
Having said that, if we both make empty scriptPubKeys standard, and add
code so that miners will always try to spend the outputs for themselves
at the same time, we can get rid of this problem by removing the
incentive. It would also still make non-fidelity-bond uses viable as
Of course, if you want to go down that path, we might as well add code
to detect and spend fidelity bonds too, and make the publish
transactions IsStandard(). Basically for every script in a confirmed
block check if any pushdata op happens to be a script that we would be
willing to add to the mempool at nBlockHeight + N. (assuming current
utxo set) If so, add it to the mempool now. N should be at least 100
blocks I think for the same reason that coinbase tx's take 100 blocks to
spend. The limit also means the size of the mempool can't get larger
than MAX_BLOCK_SIZE * N. Meanwhile IsStandard() would allow the
scriptPubKey OP_INVALIDOPCODE P2SH already treats data as scripts, so treating data as entire tx's
isn't that big of a leap. Also since the txout is unspendable, the
Satoshi criteria that block-reorgs should not make child tx's vanish is
still met. (though tx mutability sort of breaks this anyway)
We would however open quite a few cans of worms:
1) We just made non-final !IsStandard() for a reason.
2) What if there are transactions already in the mempool that spend the
txin's of the sacrifice? If we ignore them, we've just created another
way to double-spend unconfirmed transactions. On the other hand, if we
don't ignore them, we've just created a way to give us a chance to mine
the sacrifice for ourselves.
Personally I'm with you Gavin and think assuming miners are greedy is
best, but lets just say I probably shouldn't write an implementation
with a function named IsTxOutStealable()?
2a) What if multiple sacrifice publications exist spending the same
txin's? We should have deterministic behavior and mine the most valuable
one. If you don't do that, the attackers effective hashpower is
increased. (and thus the true sacrifice value of the bond decreases)
2b) ...but this is actually an annoying optmization problem. You could
create a whole bunch of sacrifices, each spending two or more inputs,
one small, one large. Then create one large sacrifice, that spends all
the *small* inputs. If the value of the large sacrifice is less than the
combined totals of the smaller sacrifices, you should be mining the
small ones rather than the large for maximum return.
3) With the 10KB limit on scripts, a naive IsStandard() could wind up
recursing about 200 times; we probably should say recursive publish
transactions are non-standard.
3b) ...on the other hand, if they are non-standard, implementations that
use fidelity bonds better make sure they don't accept such monsters.
We probably should just define one standard sacrifice tx form with one
txin, one txout, and a standard P2SH scriptPubKey, but miners can still
do their own thing and cause problems in determining the true value
sacrificed if they don't get the optimization problem right.
Fidelity bonds needs a lot more thought, and a testnet implementation...

@_date: 2013-02-14 01:07:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] Incorporating block validation rule 
The idea that miners have a strong incentive to distribute blocks as
widely and as quickly as possible is a serious misconception. The
optimal situation for a miner is if they can guarantee their blocks
would reach just over 50% of the overall hashing power, but no more. The
reason is orphans.
Here's an example that makes this clear: suppose Alice, Bob, Charlie and
David are the only Bitcoin miners, and each of them has exactly the same
amount of hashing power. We will also assume that every block they mine
is exactly the same size, 1MiB. However, Alice and Bob both have pretty
fast internet connections, 2MiB/s and 1MiB/s respectively. Charlie isn't
so lucky, he's on an average internet connection for the US,
0.25MiB/second. Finally David lives in country with a failing currency,
and his local government is trying to ban Bitcoin, so he has to mine
behind Tor and can only reliably transfer 50KiB/second.
Now the transactions themselves aren't a problem, 1MiB/10minutes is just
1.8KiB/second average. However, what happens when someone finds a block?
So Alice finds one, and with her 1MiB/second connection she
simultaneously transfers her new found block to her three peers. She has
enough bandwidth that she can do all three at once, so Bob has it in 1
second, Charlie 4 seconds, and finally David in 20 seconds. The thing
is, David has effectively spent that 20 seconds doing nothing. Even if
he found a new block in that time he wouldn't be able to upload it to
his other peers fast enough to beat Alices block. In addition, there was
also a probabalistic time window *before* Alice found her block, where
even if David found a block, he couldn't get it to the majority of
hashing power fast enough to matter. Basically we can say David spent
about 30 seconds doing nothing, and thus his effective hash power is now
down by 5%
However, it gets worse. Lets say a rolling average mechanism to
determine maximum block sizes has been implemented, and since demand is
high enough that every block is at the maximum, the rolling average lets
the blocks get bigger. Lets say we're now at 10MiB blocks. Average
transaction volume is now 18KiB/second, so David just has 32KiB/second
left, and a 1MiB block takes 5.3 minutes to download. Including the time
window when David finds a new block but can't upload it he's down to
doing useful mining a bit over 3 minutes/block on average.
Alice on the other hand now has 15% less competition, so she's actually
clearly benefited from the fact that her blocks can't propegate quickly
to 100% of the installed hashing power.
Now I know you are going to complain that this is BS because obviously
we don't need to actually transmit the full block; everyone already has
the transactions so you just need to transfer the tx hashes, roughly a
10x reduction in bandwidth. But it doesn't change the fundemental
principle: instead of David being pushed off-line at 10MiB blocks, he'll
be pushed off-line at 100MiB blocks. Either way, the incentives are to
create blocks so large that they only reliably propegate to a bit over
50% of the hashing power, *not* 100%
Of course, who's to say Alice and Bob are mining blocks full of
transactions known to the network anyway? Right now the block reward is
still high, and tx fees low. If there isn't actually 10MiB/second of
transactions on the network it still makes sense for them to pad their
blocks to that size anyway to force David out of the mining business.
They would gain from the reduced hashing power, and get the tx fees he
would have collected. Finally since there are now just three miners, for
Alice and Bob whether or not their blocks ever get to Charlie is now
totally irrelevant; they have every reason to make their blocks even
Would this happen in the real world? With pools chances are people would
quit mining solo or via P2Pool and switch to central pools. Then as the
block sizes get large enough they would quit pools with higher stale
rates in preference for pools with lower ones, and eventually the pools
with lower stale rates would probably wind up clustering geographically
so that the cost of the high-bandwidth internet connections between them
would be cheaper. Already miners are very sensitive to orphan rates, and
will switch pools because of small differences in that rate.
Ultimately the reality is miners have very, very perverse incentives
when it comes to block size. If you assume malice, these perverse
incentives lead to nasty outcomes, and even if you don't assume malice,
for pool operators the natural effects of the cycle of slightly reduced
profitability leading to less ability invest in and maintain fast
network connections, leading to more orphans, less miners, and finally
further reduced profitability due to higher overhead will inevitably
lead to centralization of mining capacity.

@_date: 2013-02-14 01:39:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Incorporating block validation rule 
Speaking of fidelity bonded banks I think it needs to be made clear that
really trustworthy bonded banks require the maximum block size to be
kept limited. The problem is that even if you don't create any
transactions on the chain yourself, you still need to be able to keep
watch the chain to keep track of what the bank is doing. For instance if
you are trying to decide if you can trust the bank with a 1BTC deposit,
and they've purchased a 1000BTC fidelity bond, you still need to be able
to determine if all the unspent transaction outputs in the blockchain
that the bank could spend, in addition to all the unspen transactions in
the mempool, are less than the value of their fidelity bond. With 1MiB
blocks that will be practical on smartphones with wireless internet
connectivity without having to trust anyone else. With 1GiB blocks that
just won't be true and you'll be forced to trust the relatively few
nodes out there with the hardware to deal with the blockchain. You'll
pay for it too.
Potentially the various UTXO proposals will help, but they will need to
be quite sophisticated; we'll need sums of all txout values by
scriptPubKey and a fraud notice system for instance. All of this stuff
is at best many months away from even beginning to be deployed on the
network, and probably years away from getting to the point where it is
truely trustworthy. Maybe it'll never become trustworthy, either because
miners just don't bother, the code doesn't get written, or a flaw in the
whole idea is found. We're just not going to know until these
technologies are implemented and tested, and without them, large blocks
force us into trusting miners blindly and make many valuable
applications impossible.

@_date: 2013-02-18 11:22:35
@_author: Peter Todd 
@_subject: [Bitcoin-development] Incorporating block validation rule 
Then don't be so agressive; target 90% as I suggested and the miner
still comes out ahead by having 10% less hashing power to compete with.
50% is only a maximum because when more than 50% of the network does not
see your blocks the majority will inevitably create a longer chain than
you, but less than 50% and your part of the network will inevitably
create a longer chain than them.
What you are describing is either *voluntary* centralization, or won't
happen. Nothing in your scenario will stop people from transacting on
the Bitcoin network directly, it will just make it more expensive. For
instance suppose fees rose to the point where the value of the fees was
10x the value of the block reward today; miners would be taking in
$972,000/day, or $6750/block. At 1MiB/block that implies transaction
fees of $6.75/KiB, or about $2 per transaction. Even if the fees were
$20 per transaction that'd be pretty cheap for direct access to the
worlds bank-to-bank financial network; I can still transfer an unlimited
amount of money across the planet, and no-one can stop me. Importantly
there will be plenty of demand to have transactions mined from people
other than banks and large corporations.
Because there will continue to be demand, and because 1MiB blocks means
running a relay node is trivial enough that people can do it just for
fun, banks won't be able to force people to use their "high-speed
backbone". Not to say they won't create one, but it won't have any real
advantage over something that can be run in your basement.
On the mining side with 1MiB blocks the fixed costs for setting up a
mining operation are just a moderately powered computer with a bunch of
harddrive space and a slow internet connection. The marginal costs are
still there of course, but the cost of power and cooling are lower at
small scale than at larger industrial scales; power is often available
for free in small amounts, and cooling isn't a problem in small setups.
Because small-scale miners will still exist, there will still be a
market for "consumer" mining gear, and trying to regulate mining
equipment will just turn it into a black-market good. Small blocks let
you setup a mining operation anywhere in the world - good luck
controlling that. Mining also will remain a way to import Bitcoins into
Banks can try setting up exclusive mining contracts, but unless they
control more than 50% of the network they'll still have to accept blocks
found by these highly decentralized, small-scale miners. They'd be
better off broadcasting their transactions to those miners as well so
they don't get double-spent. Thus decentralized miners still can profit
from transaction fees, and still have an incentive to mine. Doesn't
sound like centralization to me at all.
On the other land, with large blocks, not only is mining solo
unprofitable due to the huge fixed costs required to process the blocks,
miners on pools can't effectively secure the network because they can't
independently verify that the blocks they are mining are valid. It would
be easy then to co-opt the relatively small number of pools, a number
that is not particularly large even now. Transaction relay nodes would
also be very expensive to run, and again the small number of them makes
them targets for control. Sure transactions will be cheap, but that
doesn't do you any good if the small number of miners out there are all
regulated and ignore your transactions.
Sounds like centralization to me.

@_date: 2013-02-23 20:06:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] How small blocks make delibrate orphan mining 
In the low-subsidy future fees will be the main source of income for
miners. Thus in some circumstances large miners may even have a reason
to delibrately try to mine a block that would orphan the current best
block. A simple example would be what would happen if a 1000BTC fee tx
was created, but more realistic examples would be just due to a large
number of tx's with decent fees.
However, with limited block-sizes such a strategy runs into a problem at
a point: you can't fit more tx's into your block so you can't increase
the fees collected by it even if you wanted too. Best strategy will soon
be to accept it and move on.
The second thing that could help defeat that strategy is if clients use
nLockTime by default. Clients should always create their transactions
with nLockTime set such that only the next block can include the
transaction, or if the transaction isn't time sensitive, possibly even
farther in the future.
Remember that to get ahead, you need to mine two blocks, and with
nLockTime the first block could only gain the transactions in the block
it orphans, so any further transactions could only go in the second.
With limited blocksizes that creates even more pressure in that the
block becomes full.
I don't see any reason why nLockTime in this fashion would harm clients,
so I think it's a perfectly reasonable thing to do and provides some
nice benefits down the road.

@_date: 2013-02-25 21:44:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fidelity-bonded ledgers 
Lets suppose we take away everything but the transaction/scripting
system from Bitcoin. What is left is basically a way for to prove that a
set of pubkeys authorized the movement of coins from one place to
another. Of course, such a system is flawed as a currency because of the
double spend problem - specifically the need to know that there exists
global consensus on what particular set of transactions is the accepted
However, in the event that a party commits double-spend fraud, it is
trivial to prove to others that they did so. Thus if a way to punish
that party can be found, we can give them an incentive to behave
Consider the following new opcode and scriptPubKey:
     n  m CHECK_DOUBLE_SPEND_PROOF
spent with the following scriptSig:
    {transaction 1} {transaction 2}
where where both transactions are part of the block chain starting at
the given genesis hash, and whose blocks are signed by n of m ledger
pubkeys; the ledger is the entity entrusted to keep the ledger accurate
and not allow double-spends to occur.
Anyone with proof of a double-spend attempt made in the blockchain
starting at a given genesis hash can collect a reward. In essence, the
txout is the fidelity bond holding the ledger accountable.
Of course, the devil's in the details...
Transactions and blocks
From the point of view of the Bitcoin validation machinery, double-spend
detection is undefined. Thus multiple systems are possible without
changing the validation rules.
One simple method would be for the ledger to maintain a publicly
accessible website, in particular publically accessible over Tor.
Transactions would be incorporated into a single log, and clients would
expect to be able to get copies of that log at any time anonymously. If
their transaction did not appear in the log, they could immediately
prove the double spend. (going as far back as the genesis block) The
optional block hash signed by every transaction would be incorporated
into the audit logs.
Equally trusted computing technologies can also be used instead of, or
in conjunction with, the ledger audit logs.
Ensuring the bond can-not be collected by the ledger
Auditing chuam token creating and redemption is made difficult by the
fact that the two acts are separated from each other. However, while it
isn't possible to audit any particular token issue/redemption, it is
possible to audit the sum of all tokens issued.
Specifically now every transaction involving tokens contains a number V,
which represents the total value of all tokens of that denomination. A
token creation transaction would look like the following:
      V+1  with the rule that if two transactions exist with the same prev hash,
both creating a token, fraud has occured. Equally if the total token
value is not incremented, fraud has also occured. Similarly token
redemption can look like the following:
       V-1  Note that here tokens themselves are pubkeys, which authorize their own
Token to token transactions do not change V, but they still require
signatures, and thus still can be used as fraud proofs.
Of course the ledger can still run with all the funds deposited, but if
clients never deposit more funds than the fidelity bond is worth, the
ledger is still unable to profit from fraud as any client can show that
either their redemption request has not been honored, or that the value
of outstanding tokens does not match up.
Forcing redemptions
I'm not sure the following has been proposed before; my apologies if it
Invalid opcodes do not make a transaction invalid if they are part of an
unexecuted IF/ELSE/ENDIF block. Previously it has been proposed to
carefully use the ten NOP opcodes as a way to extend the scripting
language - remember that an extension is a soft fork only if existing
clients consider the transaction valid - however we can instead redefine
just one NOP opcode to get access to all of the invalid opcodes.
Specifically consider the following scriptPubKey:
     CHECK_SCRIPT_VERSION NOTIF  ENDIF
The CHECK_SCRIPT_VERSION, NOP10, would essentially put the supported
script version on the stack, followed by the LESSTHANOREQUAL opcode.
Thus if the script version is supported, zero is placed on the stack,
and the NOTIF/ENDIF block is executed. Otherwise non-zero is left on the
stack, and the block is not executed, resulting in the script succeeding
unconditionally. Equally for non-supporting miners, the NOP10 does
nothing, non-zero is left on the stack, and the script succeeds.
Alternatively the comparison could be done as an XOR, and the "version"
actually being a set of capabilities. This has the advantage that
different versions could use the same invalid opcodes for different
purposes. (the context would remain until the end of the ENDIF block)
However I'm not sure that allowing a full-on "flag" system is worth the
complexity, and in any case if versions are assigned sequentially
essentially the same idea can be done later anyway.

@_date: 2013-02-25 21:49:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fidelity-bonded ledgers 
============================== START ==============================
One last thing: credit goes to Gregory Maxwell for his ideas about
adding unspent-chaum-token redemption op-codes to Bitcoin proper that
lead me down the path to the more general Fidelity-Bonded Ledger idea.

@_date: 2013-01-14 05:44:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: make the private key for testnet 
As the title says.
Basically on testnet we should give people the chance to test how their
code handles attempts to spend the genesis block coinbase, as well as
other tx's made to the genesis block address. After all, potentially
Satoshi is still out there, still has that key, and still may cause
exactly that situation himself.
So Gavin, if you have the private key for testnet3's coinbase, I'm
asking you to release it, and otherwise fix this for the eventual
Of course, obviously you can test this case yourself with a private
testnet, but it'd be good if people we forced to test this case against
their will...

@_date: 2013-01-24 02:01:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Testnet DNS seed 
I setup a testnet DNS seed using Pieter Wuille's bitcoin-seeder, with
some simple modifications for testnet. It's at
testnet-seed.bitcoin.petertodd.org I also created
static-testnet-seed.bitcoin.petertodd.org which currently has a single A
record pointing to a testnet node I'm running to bootstrap the seeder
Everything is running on a dedicated Amazon EC2 micro instance. Just
IPv4 is supported right now as EC2 doesn't support IPv6; even tunnels
are broken. I also haven't setup tor yet. I can do both if there is
I guess the next step is to create a new strTestNetDNSSeed in the
satoshi client, although it'd be better if at least one more person had
a testnet seed to include in the list. Probably best to leave IRC
enabled too.
Also, FWIW, it looks like the pnSeed list is way out of date...
Pieter: Have you written any start/stop/monitoring scripts for the
seeder? My mods are at git://github.com/petertodd/bitcoin-seeder.git in
the "testnet" branch. I'll send you a pull request once it's had some

@_date: 2013-01-27 05:27:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Testnet DNS seed 
Indefinitely. It's a pretty cheap thing to run, about $7.5/month. If
anyone else wants I can give them a machine image copy easily too.
seed.txt? You mean the dumpfile produced by bitcoin-seeder? That has
uptime info, although only a months worth if I understand it correctly.
pnSeed probably should be filtered with SORB's dynamic ip list or
similar too, and additionally add an expiry time. (1 year?)

@_date: 2013-07-12 09:18:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
Yeah, there's been a lot of doom and gloom about zerocoin that is
frankly unwarrented. For instance people seem to think it's impossible
to make a blockchain with zerocoin due to the long time it takes to
verify transactions, about 1.5 seconds, and never realize that
verification can be parallelized.
Anyway the way to do it is to get out of the model of large blocks and
think about individual transactions. Make each transaction into its own
block, and have each transaction refer to the previous one in history.
(zerocoin is inherently linear due to the anonymity)
Verification does *not* need to be done by every node on every
transaction. Make the act of creating a transaction cost something and
include the previous state of the accumulator as part of a transaction.
Participants verify some subset of all transactions, and should they
find fraud they broadcast a proof. Optionally, but highly recomended,
make it profitable to find fraud, being careful to ensure that it's
never profitable to create fraud then find it yourself.
Anyway Bitcoin is limited to 7tx/s average so even without probabalistic
verification it'd be perfectly acceptable to just limit transactions to
one every few seconds provided you keep your "blocksize" down to one
transaction so the rate isn't bursty. You're going to want to be
cautious about bandwidth requirements anyway to make sure participants
can stay anonymous.
As you suggest creating zerocoins from provably sacrificing bitcoins is
the correct approach. The consensus algorithm should be that you
sacrifice zerocoins (specifically fractions there-of - note how I'm
assuming support for non-single-zerocoin amounts) and whatever chain has
the highest total sacrifice wins. One way to think about
proof-of-sacrifice is it's really proof-of-work, transferred. It also
has the *big* advantage that to double-spend, or for that matter 51% the
chain, you have to outspend everyone with a stake in the viability of
the blockchain: they can sacrifice their zerocoins to combat you. In the
case of a double-spend to rip off an online merchant the total amount
you could profit is the same as the total amount they would rationally
spend to stop you, and soon there will be collateral damage too
increasing the amount third-parties are willing to sacrifice to stop
you. You can't win.
Of course, this does mean that even unsuccesful sacrifices need to be
costly. You can make this acceptable to users by allowing a sacrifice to
be reused, but only for the exact same transaction it was originally
committed to.
Sacrifices in this manner are *not* proof of stake. You really are
giving up something by publishing the information that proves you made
the sacrifice as that information can always be included in the
consensus thereby taking away a limited resource. (your zerocoins) It's
more heavily dependent on jam-free networks, and doesn't play nice with
SPV, but zero-knowledge proofs will may help the latter. (you've got
Bitcoin itself to act as a random beacon remember)
Speaking of, another similar approach is to take advantage of how a
Bitcoin sacrifice can be made publicly visible. Create a txout of some
value like the following:
    OP_RETURN   Now even if you fail to publish your blocks, at least the whole world
knows how much they need to outspend to be sure you can't 51% attack the
network. This approach and not-btc sacrifices can go hand in hand too,
especially if nodes follow rules where they consider btc txout
sacrifices as "fixed" and only subject to change by the bitcoin
blockchain re-organizing. Advantages and disadvantages to both
approaches. (remember that visible tx's can be censored by miners)
Sacrifice to mining fees may be acceptable in the future too, but only
if OP_DEPTH is implemented so as to not give Bitcoin miners bad
incentives. (the sacrificed coins should go to fees *months* or even
*years* after they have been sacrificed)
Turning zerocoins back into Bitcoins is just supply and demand: sell
them. You'll always lose a bit given by definition the maximum exchange
rate is 1:1, but anonymity may be worth it. Others have written about
cross-chain trading protocols, and I'll point out they are easier to
implement if one chain has full visibility into what's happening on the
other; zerocoin is most likely to be implemented as an extension to the
bitcoin client itself.
Finally if the transaction rate is too slow there's nothing wrong with
running multiple parallel zerocoin blockchains, although given the
usecase of moving your funds through zerocoin for anonymity, and using
the clean coins that come out the other side, there's no reason to think
the zerocoin chain transaction rate needs to be especially high anyway.

@_date: 2013-07-14 20:12:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
Your ideas about making an alt-coin have anything to do with hashing
power might be a lot more convincing if you hadn't 51% attacks alt-coins
in the past.

@_date: 2013-07-14 20:29:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
Basically you have one way of creating a Zerocoin: prove you sacrificed
a Bitcoin in a specific way. (spend to unspendable, or spend to mining
fees far into the future)
Now when you sell a Zerocoin what you do is create a Zerocoin
transaction with a txout that can only be spent if you can prove that a
Bitcoin transaction exists with specific conditions with sufficient
confirmations. The specific condition would most likely be it has a
txout of a specific value and scriptPubKey. Basically you'd have a
two-part scriptPubKey:
if   else  Note how if the buyer screws up there is a fallback so the seller can
retrieve their funds after some reasonable amount of time.
Of course if the Bitcoin chain is re-orged Bad Things Happen(TM), but
just set the required number of confirms to something reasonable and
you're good to go. It does mean Zerocoin needs to have consensus on the
Bitcoin blockchain, but that's required to verify sacrifice proofs
Economically the idea works because Zerocoins are gradually consumed by
the proof-of-sacrifice required to make Zerocoin transactions. If the
process by which Bitcoins are sacrificed is to fees, rather than
permanently, the overall affect is just a minor decrease in the Bitcoin
money supply. If they are sacrificed permanently, it'll result in
long-term Bitcoin deflation - potentially an issue as the blockreward

@_date: 2013-07-14 21:59:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
I wasn't aware you denied that accusation, so my apologies; I retract
that statement.

@_date: 2013-07-15 03:32:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Protecting Bitcoin against network-wide 
My mempool rewrite defined a CMemPoolTx subclass for CTransaction - it
shouldn't be too hard to add the required per-node accounting once nodes
get unique identifiers. (can be assigned randomly in the beginning,
later can be used for permanent node identifiers w/ ssl and message

@_date: 2013-07-15 05:51:07
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
Being able to have automated Bitcoin<->Zerocoin P2P trading without an
exchange is also significantly more desirable from a privacy standpoint.
Basically it reduces the privacy risks of doing the exchange to spending
the Zerocoins in the first place.

@_date: 2013-07-15 16:29:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] libzerocoin released, 
Which is why I'm not proposing that.
You are assuming value is the same for everyone - it's not.
If I mine in a jurisdiction where zerocoin is banned, and the blocks I
mine are public, the value of zerocoin blocks to me are at best zero.
Equally it would be easy for the local authorities to ask that I merge
mine zerocoin blocks to attack the chain - it doesn't cost me anything
so what's the harm? I may even choose to do so to preserve the value of
the coins I can mine legally - alt-coins are competition.
Incedentally keep in mind it is likely that in the future pools will not
allow miners to modify the work units they receive in any way as a means
of combating block-withholding fraud; there may not be very many people
willing or able to honestly merge-mine any given chain.
Proof-of-sacrifice can be done in a way that is opaque to the master
blockchain by creating txouts that look no different from any other
txout. Hopefully not required, but it would be a good strategy against
censorship of sacrifice-based chains.
SCIP is for now a dream. Give it a few more years and see how the
technology shakes out.

@_date: 2013-07-17 06:58:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
Keep in mind that SPV mode is newer than many realize: bloom filters are
a 0.8 feature, itself released only last Febuary. As John Dillon posted
earlier this week in "Protecting Bitcoin against network-wide DoS
attack" the Bitcoin codebase will have to implement much better anti-DoS
attack defences soon, and in a decentralized system there aren't any
options other than requiring peers to either do work (useful or not) or
sacrifice something of value. SPV peers can't do useful work, leaving
only sacrifice - to what extent and how much is unknown. In addition SPV
nodes have serious privacy issues because their peers know that any
transaction sent to them by the SPV node is guaranteed to be from the
node rather than relayed; bloom filters are only really helpful with
payment protocols that don't exist yet and don't apply to merchants.
Then you have MITM problems, vulnerability to fake blocks etc.
It'll be awhile before we know how serious these issues are in practice,
and we're likely to find new issues we didn't think of too. In any case
Bitcoin is far better off if we make it easy to run a full node,
donating whatever resources you can. Fortunately there's a whole
continuum between SPV and full nodes.
The way you do this is by maintaining partial UTXO sets. The trick is
that if you have verified every block in some range i to j, every time
you see a txout created by a transaction, and not subsequently spent,
you can be sure that at height j the txout existed. If height j is the
current block, you can be sure the txout exists provided that the chain
itself is valid. Any transaction that only spends txouts in this partial
set is a transaction you can fully verify and safely relay; for other
transactions you just don't know and have to wait until you see them in
a block.
So what's useful about that? Basically it means your node starts with
the same security level, and usefulness to the network, as a SPV node.
But over time you keep downloading blocks as they are created, and with
whatever bandwidth you have left (out of some user-configurable
allocation) you download additional blocks going further and further
back in time. Gradually your UTXO set becomes more complete, and over
time you can verify a higher and higher % of all valid transactions.
Eventually your node becomes a full node, but in the meantime it was
still useful for the user, and still contributed to the network by
relaying blocks and an increasingly large subset of all transactions.
(optionally you can store a subset of the chain history too for other
nodes to bootstrap from) You've also got better security because you
*are* validating blocks, starting off incompletely, and increasingly
completely until your finally validating fully. Privacy is improved, for
both you and others, by mixing your transactions with others and adding
to the overall anonymity set.
In the future we'll have miners commit a hash of the UTXO set, and that
gives us even more options to, for instance, have relayed transactions
include proof that their inputs were valid, allowing all nodes to relay
them safely.
As for specifics, you need to maintain a UTXO set, and in addition a set
of spent txouts (the STXO set) for which you haven't seen the
transaction that created the txout. As download newer blocks you update
the UTXO set; as you download older blocks you update the UTXO set and
STXO set.
Nodes now advertise this new variable to their peers:
nOldestBlock - The oldest block that we've validated. (and all
subsequent blocks)
We'll also want the ability to advertise what sub-ranges of the
blockchain data we have on hand:
listArchivedBlockRanges - lists of (begin, end pairs)
Nodes should drop all but the largest n pairs, say 5 or something. The
index -1 is reserved to indicate the last block to make it easy to
advertise that you have every block starting at some height to the most
recent. (reserving -n with n as the last block might be a better choice
to show intent, but still allow for specific proofs when we get node
We probably want to define a NODE_PARTIAL service bit or something; I'll
have to re-read Pieter Wuille's proposal and think about it. Nodes
should NOT advertize NODE_NETWORK unless they have the full chain and
have verified it.
Nodes with partial peers should only relay transactions to those peers
if the transactions spend inputs the peers know about - remember how
even an SPV node has that information if it's not spending unconfirmed
inputs it didn't create. Nodes will have to update their peers
periodically as nOldestBlock changes. That said it may also be
worthwhile to simply relay all transactions in some cases too - a
reasonable way to approach this might be to set a bloom filter for tx's
that you *definitely* want, and if you are interested in everything,
just set the filter to all 1's. If someone comes up with a reasonable
micropayment or proof-of-work system even relaying txs that you haven't
validated is fine - the proof-of-work and prioritization will prevent
DoS attacks just fine.
Remember that if you're running a partial node, it can get new blocks
from any partial node, and it can retrieve historic blockchain data from
any partial node that has archived the sequence of blocks you need next.
On a large scale this is similar to how in BitTorrent you can serve data
to your peers the moment you get it - a significant scalability
improvement for the network as a whole. Even if a large % of the network
was partial nodes running for just a few hours a day the whole system
would work fine due to how partial nodes can serve each other the data
they need.
On startup you can act as a SPV node temporarily, grabbing asking for
filtered blocks matching your wallet, and then go back and get the full
blocks, or just download the full blocks right away. That's a tradeoff
on how long the node has been off.
Anyway, it's a bit more code compared to pure-SPV, but it results in a
much more scalable Bitcoin, and if you can spare the modest bandwidth
requirements to keep up with the blockchain it'll result in much better
robustness against DoS attacks for you and Bitcoin in general.

@_date: 2013-07-17 08:50:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
Widespread dependence on SPV mode is very dangerous for Bitcoin in
general due to that reason. Fraud proofs may help, but they're also
another layer of never-before-tested crypto on top of an already poorly
understood technology, bitcoin itself.

@_date: 2013-07-18 07:13:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Note that with OP_DEPTH we can remove the small chance of the payee
vanishing and putting the funds in limbo:
     OP_DEPTH OP_LESSTHAN
    IF 2 PK1 PK2 CHECKMULTISIG
    ELSE PK1 CHECKSIG
    ENDIF
Though that shows how to implement OP_DEPTH as a true soft-fork we're
probably best off doing it as part of a script v2 using the soft-fork
mechanism I outlined before when talking about fidelity-bonded ledgers.
(best to do MAST (merklized abstract syntax tree) support at the same

@_date: 2013-07-18 08:13:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
Good idea.
SPV clients behaving normally are highly abusive: they use up maximum
node resources with minimum cost to themselves. (nodes doing an initial
block download are similar now, although with partial mode they can
contribute back to the network sooner)
We can't win if the attacker has more upstream bandwidth than we have
downstream, but fortunately botnets are generally comprised of computers
on asymetric residential connections. Thus our goal is to prevent the
attacker from using lots of downstream bandwidth, and more importantly,
from consuming more memory and similar resources than we posess.
Annoyingly the raw # of TCP connections is very much a limited resource
due to constraints on the # of ports a process can handle, and
constraints imposed by stateful firewalls, and memory used by kernel
Anything that allows for more incoming connections with less memory
usage is a good thing - bloom filters are limited to 32KiB and the
per-peer test if a INV item needs to be relayed to a peer is fairly
cheap, but we also have other buffers like pending INV messages and so
on. EC2 micro instances, as an example, often need -maxconnections
limited or they run out of memory - we've probably got room for
improvement; removing mapRelay and just grabbing relayed txs from the
mempool comes to mind.
More generally a good thing to do would be to force incoming peers to
use up RAM to make a connection. We can do that with a proof-of-data
posession engineered such that unless you store the data in high-speed
memory you will have your connection dropped. Per peer a node can pick a
nonce k and define j_i=H(k+i), sending the peer a set J=(j_0...j_n) to
store in RAM. With f(k, n, i) as a pseudo-random sequence generator we
create nonce x and ask our peer to compute J'(x, m) = j_f(x, n, 0) ^ ...
^ j_f(x, n, m)) and give us the result. (^ as the XOR operator) Because
we know the nonce k we can do that cheaply, calculating it on the fly,
but our peers have no choice but to store J and retrieve it on demand.
If they store J in RAM they can do so quickly; if they store J on disk
they can't. We then prioritize peers by how fast they respond to these
requests, both measuring ping times, and forcing attackers trying to
connect to large numbers of peers to posess large amounts of relatively
expensive RAM. This is particularly nice because we've can make it
significantly more expensive for anyone to peer to every node in the
Bitcoin network simultaneously to do things like watch transaction
propagation in real-time.
A more sophisticated approach would be possible if there existed a
version of H() with a computational trap-door - that is if there existed
H'(s, i)=H(i) where H' had significantly faster running time than H(),
but required knowledge of a secret. Our peers would then be able to
answer our challenges quickly only if they stored the intermediate
results in a lookup table, while we could check those challenges cheaply
without that table.
Adam: you're our local crypto-expert, what can we use for H'? Seems that
maybe some kind of asymmetric crypto system would work by requiring the
peer to crack weak secret keys that we generate deterministicly.

@_date: 2013-07-18 09:18:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
Actually, come to think of it a really easy way to create H' is for the
node to create some expensive to compute set of data associated with
their identity. The data set is then stored once by the node, cheap, but
the clients have to store one set for every unique node they connect
too, expensive. A set of the function scrypt(k | i) for i in 0..n is an
obvious way to do it.
This can equally be used as a proof-of-work to make creating lots of
nodes expensive given a cheap way to verify the POW; easily done with a
non-interactive zero-knowledge proofs. It'd be nice if that POW could
incorporate blockchain data, showing that the identity had access to
that data and thus could have computed the UTXO set honestly. (the POW
should be incrementally extendable as new data becomes available)
However that is back to using a bunch of bandwidth at startup if our
peer doesn't have access to blockchain data, so both mechanisms would
probably have to be done independently. Note how we also make MITM
attacks on encrypted P2P connections expensive this way too even without
any form of authentication. (works best when the proof-of-work is
dependent on your IP addresses)

@_date: 2013-07-18 09:43:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Satoshi was worried that in the event of a re-org long chains of
transactions could become invalid and thus impossible to include in the
blockchain again, however that's equally possibly through tx mutability
or double-spends;(1) I don't think it's a valid concern in general. When
accepting any payment you need to take the chance of a re-org into
account, and if the payment is large enough it'll call for more confirms
on that basis. It does increase that (small) risk however and a client
may want to trace the transaction chain back a few steps when accepting
a very large payment in leu of just waiting for more confirms.
1) Also via non-standard transactions as SetBestChain() calls
mempool.accept() which still applies IsStandard(). We also recently
broke re-acceptance of transactions with dependencies as they are
currently added in reverse order, broken when Matt removed the
fIgnoreMissingInputs flag.
Not a problem limited to OP_DEPTH either: consider the following
probabalistic payment:
    PREVBLOCKHASH HASH n LESSTHAN VERIFY  CHECKSIG
Obviously in a re-org the chance of it being succesfully included is
slim. (this example is simplistic and is vulnerable to double-spends in
a number of ways)
Mempool and relay code will have to take into account that a transaction
that can be included in the next block may not be possible to include in
the block after that for the purposes of protecting against tx-flood DoS
attacks - not an important issue unless we loosen IsStandard()

@_date: 2013-07-18 12:09:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
jl2012 pointed out we already have an OP_DEPTH instruction that returns
the number of items on the stack. In the future we should use the terms
OP_BLOCKHEIGHT, OP_TXOUTHEIGHT, OP_TXOUTDEPTH to talk about hypothetical
instructions that put the block height, confirmed txout height, and
confirmed txout depth on the stack. Thus the above example would now be:
      BLOCKDEPTH LESSTHAN
     IF 2   CHECKMULTISIG
     ELSE  CHECKSIG
     ENDIF

@_date: 2013-07-18 12:22:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
I've got one or two orders of magnitude more good ideas than I have time
to implement, but I will say this one would have a pretty big impact -
I'm considering it.
Of course, I would accept bribes. :) But in all seriousness I also
accepted funds from John Dillon to implement replace-by-fee, although
he's been good in understanding that the scope of the project was quite
a bit bigger than originally thought. (it turned out replace-by-fee can
enable very safe zero-conf transactions, but only with mempool and
relaying changes) I'd suggest looking at my git commit track record
before you offer anything FWIW; I've been much more of an academic than
a programmer.

@_date: 2013-07-18 19:03:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
I have plenty of confidence in my programming skills, I just don't have
very much evidence in the Bitcoin git history to convince you my
confidence is well placed. :)
I do have a day job I love, so it will certainly get done faster if you
can get someone else to do the actual coding; I'd be willing to write
the specifications and supervise/audit/advise for a few hours a week.

@_date: 2013-07-23 05:47:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
The REST API has nothing to do with SPV clients; it's similar to the RPC
interface and won't be exposed to the network as a whole.
Increasing the resource usage by SPV clients on full nodes is undesirable; we
have a lot of work to do regarding DoS attacks. John Dillon's comments here on
using micro-transactions to compensate full-nodes for maintaining expensive
blockchain indexes are worth reading:
In any case UTXO data currently requires you to have full trust in
whomever is providing you with it, and that situation will continue
until UTXO commitments are implemented - if they are implemented.

@_date: 2013-07-23 06:17:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
Read my proposal for "Partial UTXO" mode:
 at lists.sourceforge.net/msg02511.html
Actually the really scary thing about partial UTXO mode is miners can
get away without keeping the entire chain provided they don't (often)
try to mine transactions spending UTXO's that they haven't verified
They can get away with accepting blocks without checking that the UTXO's
exist, at least until enough miners do so that someone creates an
invalid block and the majority of hashing power never notices. Remember
that only with a complete UTXO set can you know that a UTXO *doesn't*
We're going to have to force miners to prove they possess the full UTXO
set in the future or the security of Bitcoin will be seriously
How do you know they actually are someone else?
Do you think you have SPV or full security in that situation?
Do you know the difference?

@_date: 2013-07-24 05:42:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] Distributing low POW headers 
Please provide equations and data justifying the 'magic constants' in
this proposal.
Currently we do not relay blocks to peers if they conflict with blocks
in the best known chain. What changes exactly are you proposing to that

@_date: 2013-07-27 19:49:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Two factor wallet with one-time-passwords 
Gavin Andresen recently suggested a design for a wallet protected by
two-factor authentication via one-time-passwords with the aid of a
third-party service to counter-sign 2-of-2 protected wallets.(1) The
design is useful when the user can't sign transactions on a second
device, such as a phone, but can provide one-time-passwords. (possibly
generated on a smart phone or stored on paper) However involving a
third-party has privacy and availability risks. Here is an alternate
design, also using one-time-passwords, that removes the requirement for
a third-party, along with other advantages and disadvantages.
User experience
The user has a wallet with a separate balances for savings and a smaller
day-to-day spending amount. Transactions spending the day-to-day balance
do not need two-factor authorization, while spending the savings balance
does. As the day-to-day balance becomes low the user is able to top it
up by authorizing the movement of discrete multiples of some amount from
savings to spending. That authorization requires one one-time-password
per multiple being moved.
Savings use P2SH outputs matching the following scriptPubKey form:
HASH160  EQUALVERIFY  CHECKSIG
spent with:
The way the pubkey/seckey is generated is unimportant, although some
kind of deterministic scheme is preferable. Nonces on the other hand are
generated deterministically using a counter-based one-time-password
scheme that takes some secret seed and an integer i.  A large number of
H(nonce_n) are generated in advance and moved to the computer holding
the wallet. (generating them on that computer is also possible, but
obviously risks the secret seed being compromised)
A brute-force attack to spend a signed txout requires the attacker to
find a preimage, thus the security level is the number of bits for the
nonce; 64 bits is sufficient. (remember the birthday attack doesn't
apply here) Unfortunately the most popular one-time-password scheme, the
RFC6238 used in Google Authenticator, only outputs six digits numbers,
well below the security level required. (Google Auth is generally used
in a time-mode, but also has a counter mode)
The older RFC2289 however turns the passwords into six words from a 2048
entry wordlist, giving a 64-bit nonce with 2-bits of checksum. RFC2289
implementations are also well suited to paper print-outs and generally
make it easy to do so. RFC2289 as written uses SHA1, however the
suspected vulnerabilities in SHA1 are partial-preimage collisions, not
relevant in this application.
In a sense the user is now acting as an oracle answering the question of
whether or not funds should be allowed to move from savings to spending,
without being responsible for where those funds are allowed to go. As
described in (2) it is easy to create a whole range of conditions by
using multiple nonces if the use-case demanded. For instance a corporate
environment may want multiple parties to be required to authorize the
funds to move, possible with multiple nonces.
It's interesting to note how in some cases it may be preferable that the
authorization is simply authorization to spend without any other
involvement. Here the party acting as an oracle not only doesn't need to
know where funds are going but can even authorize the spend in advance
without two-way communication - possibly even prior to the funds being
received in the first place. This authorization can be easily given
manually, for instance over the phone, and the accounting to keep track
of the total amount authorized can be easily done with pen and paper -
something not possible with CHECKMULTISIG wallets.
Funding the wallet
As with any multi-party wallet receiving funds must also be handled
carefully to ensure an attacker can't fool the user into giving the
sender the wrong address. This requires the involvement of all parties
required to authorize an outgoing payment. In addition here the
protection only works if funds sent to the wallet are split up into the
discrete authorization amounts the user wishes. (possibly with more than
one amount level)
There hasn't been as much thought put into these systems as there has
been on payment protocols between a customer and a merchant, but the
basic idea is to have more than one device participate in the generation
of payment request signed somehow. For fund splitting the request can be
that the funds are paid to multiple txouts in one go.  For recurring
payments the request could have some mechanism for multiple addresses to
be specified for future use. Fall-back to a standard multi-signature
wallet is possible as well.
More research is needed.
1) 2)

@_date: 2013-07-27 21:20:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Two factor wallet with one-time-passwords 
FWIW with some minor scripting language additions such as access to txin
and txout contents, along with merklized abstract syntax tree (MAST)
support, we can even implement a version where scriptPubKey's can be
     CHECKSIGVERIFY
    // Verify we aren't spending more than the maximum spend amount
    0 GET-TXIN-VALUE      // relative indexing
    0 GET-TXOUT-VALUE
    SUB
    LESSTHAN
    VERIFY
    // If the txout is greater than the maximum spend amount force it to
    // also follow these same rules.
    0 GET-TXOUT-VALUE
    LESSTHAN
    IFNOT
        GET-THIS-SCRIPT
        MAST-HASH
        CAT
        GET-TXOUT-SCRIPT
        EQUALVERIFY
    ENDIF
    // Hash the provided oracle nonce, saving original for later.
    DUP
    HASH160
    // Use the txid:vout nonce as an index to a table, embedded with MAST
    // script compression.
    0 GET-TXIN-TXID
    0 GET-TXIN-VOUT
    CAT
    HASH160
    // The table, n=64 levels deep, not all levels shown for brevity.
    DUP
    1
    AND
    IF
        1
        RSHIFT
        DUP
        1
        AND
        IF
            1
            RSHIFT
            DUP
            1
            AND
            IF
            ELSE
                1
                RSHIFT
                DUP
                1
                AND
                IF
                    // Lowest level contains the following pushdata,
                    // with 0 <= i < 2^64
                ELSE
                ENDIF
        ELSE
        ENDIF
    ELSE
    ENDIF
    // Drop the txid:vout nonce
    SWAP
    DROP
    // Verify that the hash of the nonce and the pre-committed value in
    // the H(nonce) table match.
    EQUALVERIFY
    // Stack now only contains the nonce preceeded by a merkle path linking
    // that nonce to the tip of a merkle tree over all nonces.
    //
    // Verify that path.
    SWAP // Move direction flag to the top
    IF
        SWAP
    ENDIF
    HASH160
    (repeat above five lines 63 more times)
    EQUAL
The scriptPubKey is spent by the following scriptSig:
    ...
(note that I've left off a number of possible optimizations for clarity)
Now when the user wishes to spend a txout greather than their spending
limit their wallet software will first give them a short 6 word string
calculated from the last 64-bits of H(txid:vout). They simply enter this
string into their phone, ideally via convenient qr-code or voice/thought
recognition, and their phone provides a second short 6 word string to
enter into the wallet software on their computer, authorizing the
payment. If they opt for a paper-based one-time-password table they
simply use the 6 word string as an index to their pre-printed OTP
encyclopedia set.
Like the previously described version the security level is still a
healthy 2^64 - again the attacker needs to find a 64-bit pre-image,
considered to be a highly difficult task for any attacker unable to
count from 0 to 2^64 or store a table containing 2^64 values.
There is the disadvantage of the large storage requirements for both
wallets, however because of the double hashed construction,
H(H(nonce-secret+i)), neither table needs to be kept secret. Thus
without loss of security both tables can be easily stored in a
distributed hash table in the cloud and queried as needed.

@_date: 2013-07-29 03:41:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Opcode whitelist for P2SH? 
We're talking about two use-cases here: wallets protected by
authorization tokens for multi-factor security, and allowing funds to be
controlled by oracles that attest that events have happened allowing the
funds to move.
The latter application especially demands a specialized wallet, yet can
only possibly work with non-standard script formats.
IMO bringing the issue of wallet standardization into this discussion is
kinda silly and premature; if you don't want to use those features, then
you're wallet can ignore them. As for the people that are, they can come
up with appropriate standards for their needs.
After all John's suggesting only allowing the loosened IsStandard()
rules within P2SH, so until the txout is spent all *any* wallet sees is
a P2SH address with no information as to what scriptPubKey is needed to
spend it.

@_date: 2013-07-29 04:13:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] Opcode whitelist for P2SH? 
One subtlety of what you are proposing is that we should still retain
the IsStandard() check, or to be exact the AreInputsStandard() check, if
a P2SH serialized script follows a standard form.
The reason is transaction mutability. Right now other than BIP11
CHECKMULTISIG only miners can mutate transactions because any change to
the scriptSig will render the transaction non-standard. As you know this
is a good thing because it means unconfirmed transaction chains don't
get broken in flight.
BIP11 is an interesting case because CHECKMULTISIG consumes one extra
stack item, so when you spend a BIP11 n ... m
CHECKMULTISIG scriptPubKey you have to provide an additional item prior
to the signatures; usually OP_0 is used.
But we don't actually check that! You can put anything there provided it
doesn't make the scriptSig go over the standard allowed scriptSig size
of 500 bytes; for instance I (ab)used that feature just now to timestamp
my Litecoin v0.8.3.6 audit report SHA256 hash:
in transaction:
It's been suggested that we consider transactions non-standard, or just
now allowed at all in a future soft-fork, if at the end of execution
there is more than one stack item left; a opcode whitelist should
probably do this. On the other hand I've come up with some soft-fork
upgrade mechanisms that would leave extra items on the stack for
non-upgraded nodes, suggesting a soft-fork imposing this is a bad idea.
(though note how it suggests considering such tx's non-standard is
reasonable in a few ways)
CHECKMULTISIG isn't helped here because the value really is ignored - a
soft-fork to force it to always be zero might not be a bad idea, though
it's far from the only example of mutability.
I'd be interested if you can come up with an example where imposing a
one stack item at the end of execution rule causes problems.
More generally, and getting a bit off topic, I think Bitcoin should have
been designed so that CHECKSIG signed hashes of scriptPubKeys, rather
than txid:vout outputs, so that malleability wouldn't affect the
validity of a signature. Of course, this would mean that signatures
could be reused if scriptPubKeys were reused, but address re-use is a
bad thing anyway! Not that I'll fault Satoshi here, type 2 deterministic
wallets were unknown back then. (though we should be careful that a
future CHECKSIG design can go back to txid:vout references - ECC is
unique in allowing for type 2 wallets)

@_date: 2013-07-30 14:30:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tor and Bitcoin 
There was a good reply to those concerns last time the issue came up:
    Tor does not act as a particularly effective man in the middle for nodes
    that support connections to hidden services because while your
    connections to standard Bitcoin nodes go through your exit node, the
    routing path for each hidden service peer is independent. Having said
    that we should offer modes that send your self-generated transactions
    out via Tor, while still maintaining non-Tor connections.
    Anyway Sybil attacks aren't all that interesting if you are the one
    sending the funds, and receivers are reasonably well protected simply
    because generating false confirmations is extremely expensive and very
    difficult to do quickly. After all, you always make the assumption that
    nearly all hashing power in existence is honest when you talk about
    replace-by-fee among other things, and that assumption naturally leads
    to the conclusion that generating false confirmations with a sybil
    attack would take more than long enough that the user would be
    suspicious that something was wrong long before being defrauded.
    I'd be surprised if anyone has ever bothered with a false confirmation
    sybil attack. I wouldn't be the slightest bit surprised if the NSA is
    recording all the Bitcoin traffic they can for future analysis to find
    true transaction origins. Which reminds me, again, we need node-to-node
    connections to be encrypted to at least protect against network-wide
    passive sniffiing.
    Regarding usage I would be interested to hear from those running Bitcoin
    nodes advertising themselves as hidden services.
    - at lists.sourceforge.net/msg02438.html
tl;dr: Users should be using Tor to preserve their privacy and the MITM
risks are minimal to anyone using Bitcoin correctly. (don't trust
zero-conf transactions, they are not secure!)
Yeah, he had the idea of adding .onion addresses of seed nodes
along-side the DNS seeds table; that would give an end-to-end MITM-proof
channel to a trusted seed who can in turn give an honest view of the
Ideally those .onion addresses would be of nodes run by the same people
as running the existing seeds so that it was clear who was being trusted
- I'll write a patch to do this soon with a .onion testnet seed first.
(I run one of the testnet DNSSEED seeds and have a small grant from the
foundation to do so)
Bitcoin relays .onion addresses over the P2P network, so once you are
connected you can gain additional peers with addresses that are MITM
resistant. Currently there isn't any equivalent to the (weak) anti-sybil
properties of IP address range diversity for .onion's, but in the future
we'll eventually add node identities and some way to make creating lots
of fake identities for a sybil attack expensive.

@_date: 2013-07-30 16:11:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tor and Bitcoin 
Yes, although remember that in general SPV nodes are significantly less
safe because they depend soley on confirmations for security; it's often
not appreciated that an attacker can target multiple SPV-using entities
at once by creating a invalid block header with any number of completely
fake payments linked to it; if you can attack n targets at once, the
cost to perform the attack is n times less per target. Unrelated to Tor, but an interesting possibility to improve SPV security
is to ask for the history of a given txout - that is the previous
transactions that funded it. You could even do this with a
zero-knowledge proof, sampling some subset of the prior transactions to
detect fraud. Unfortunately none of the infrastructure is setup to do
this, and txid's aren't constructed in ways that make these kinds of
proofs cheap. (you really want a merkle tree over the txin and txout
Work thinking about for the future in any case - the above can be
implemented as a soft-fork.

@_date: 2013-07-31 18:11:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Litecoin v0.8.3.7 audit report 
I thought this may be of interest to Bitcoin as well as an example.

@_date: 2013-07-31 18:37:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] Litecoin v0.8.3.7 audit report 
By request,
Zip archive:
The individual files:
report.txt.asc SHA256 hash:

@_date: 2013-06-01 15:30:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: soft-fork to make anyone-can-spend 
Currently the most compact way (proof-size) to sacrifice Bitcoins that
does not involve making them unspendable is to create a anyone-can-spend
output as the last txout in the coinbase of a block:
scriptPubKey:  OP_TRUE
The proof is then the SHA256 midstate, the txout, and the merkle path to
the block header. However this mechanism needs miner support, and it is
not possible to pay for such a sacrifice securely, or create an
assurance contract to create one.
A anyone-can-spend in a regular txout is another option, but there is no
way to prevent a miner from including a transaction spending that txout
in the same block. Once that happens, there is no way to prove the miner
didn't create both, thus invalidating the sacrifice. The announce-commit
protocol solves that problem, but at the cost of a much larger proof,
especially if multiple parties want to get together to pay the cost of
the sacrifice. (the proof must include the entire tx used to make the
However if we add a rule where txouts ending in OP_TRUE are unspendable
for 100 blocks, similar to coinbases, we fix these problems. The rule
can be done as a soft-fork with 95% support in the same way the
blockheight rule was implemented. Along with that change
anyone-can-spend outputs should be make IsStandard() so they will be
The alternative is sacrifices to unspendable outputs, which is very
undesirable compared to sending the money to miners to further
strengthen the security of the network.
We should always make it easy for people to write code that does what is
best for Bitcoin.

@_date: 2013-06-01 16:58:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
We have no way of preventing this, so ensure it's done in a way that
minimizes harm. For instance, my zookeyv key-value consensus system can
be implemented using transactions with txout pairs of the following
Let H(d) = RIPEMD160(SHA256(d))
txout_k*2  : OP_DUP H(key) OP_EQUALVERIFY
txout_k*2+1: OP_DUP H(value) OP_EQUALVERIFY
With an additional rule to allow for references to previous sacrifices
with txouts of the form:
txout_n: OP_DUP H(txid:vout) OP_EQUALVERIFY
This is perfectly compatible with Gregory Maxwell's address pre-image
fix to data-in-chain storage, and at the same time is completely
unblockable by making such transactions more expensive - the whole point
is to prove you've sacrificed funds.
Yet another reason why increasing the blocksize is madness.

@_date: 2013-06-02 02:13:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
Good idea.
Either way, looks like complex announce-commit logic isn't needed and a
simple txout with one of a few possible forms will work.
I'd say we tell people to sacrifice to (provably) unspendable for now
and do a soft-fork later if there is real demand for this stuff in the

@_date: 2013-06-02 14:41:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
Yeah, and Bitcoin sacrifices are kind of an odd middle ground there.
It's been suggested to make provably unspendable OP_RETURN IsStandard()
only if the txout value is zero, but considering the sacrifice use-case
I'm thinking we should allow people to throw away coins in a
non-UTXO-bloating way if they choose too.
Indeed, just recognize that those disincentives must be implemented in a
way that makes doing the less-harmful thing is to your advantage. For
instance people keep arguing for OP_RETURN to only be allowed as one
txout in a tx, which puts it at a disadvantage relative to just using
unspendable outputs. Similarly because people can play OP_CHECKMULTISIG
games, allow as much data as can be included in that form, 195 bytes.
Of course, you can't block everything:
Seems legit - traffic on my timestamper is significantly reduced from
what it was before. Incidentally, I've left the opentimestamps client
deliberately broken for months now to see if anyone used it, and other
than this guy I've had zero bug reports.

@_date: 2013-06-04 16:25:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
"High" is relative.
I could make a 100BTC apparently sacrifice via fees by just waiting a
month or two for my mining hardware to find a block that had a
pre-prepared fake sacrifice. It'd cost me roughly 1BTC when you take
orphans into account. Similarly I could hack into a pool and have them
do it on my behalf, or a pool could just offer the service for a fee.
I already worry enough that announce-commit sacrifices to mining fees
aren't secure enough given the potential of a few large pools teaming
up to create them cheaply, let alone what you're talking about...
Hey Luke: so what's the going rate to get Eligius to mine a fake mining
fee sacrifice? Can I get a discount on repeat orders? :)

@_date: 2013-06-06 04:31:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Revocability with known trusted escrow 
A few issues:
Revocable payments are almost always invoked in cases where the decision
that a payment needs to be revoked is done by humans. To worry about the
difficulty of finding a "trusted escrow service" is irrelevant at the
protocol level - this isn't a problem that can be solved by math.
Legally speaking revocation can generally happen any time in the future,
even years in the future. Note the controversies involved around a
variety of land transactions that occured hundreds of years in the past
in North America and other parts of the world, where distant relatives
of those who made the transactions are attempting to have them reversed
partially or fully. Technical solutions with a limited revocation window
are likely to be found unacceptable in the eyes of the law.
Focusing on the need to "revoke" a transaction is taking a banking idea,
and applying it very incorrectly to the Bitcoin world; in banking
revoking a transaction can result in your balance being negative.
What you need to focus on is the spirit of what revoking a transaction
is about, which is to take money from someone who thought they had it,
and give it to someone else. We can easily replicate this effect in
Bitcoin by simply giving the private keys for our wallets to the
relevant revocation authority, or, if more auditing is desired, storing
our coins in 1-of-2 multisig addresses spendable by either us or that
In the event that a transaction needs to be revoked, simply have the
escrow service make a transaction that takes the correct amount of coins
from your wallet, and gives it to the person who sent you the money.
Problem solved.

@_date: 2013-06-10 01:30:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
My general comments on the idea are that while it's hard to say if a
vote by proof-of-stake is really representative, it's likely the closest
thing we'll ever get to a fair vote. Proof-of-stake is certainely better
than just letting miners choose; as you point out miners can always
choose to decrease the blocksize anyway so we only need a vote on
allowable increases. Proof-of-stake also clearly favors those who
actually have invested in Bitcoin over those who only talk about
I'll also say that while I know people will complain about putting
politics into a technical problem, as I keep saying, is *is* a political
issue. The limitations may be technical, but the ultimate issue is a
very political decision about what we want Bitcoin to be. Yes, there
will be people campaigning left and right to get users to vote for
various limits with their coins, deal with it. Democracy is messy.
Voting would take a lot of the nastier politics out of the situation,
perhaps somewhat ironically. It would quite clearly take control away
from the core development team, and the Bitcoin Foundation, and put it
back in the hands of the community; you can't argue conspiracy theories
that the Foundation is trying to control Bitcoin when there is a
completely transparent voting system in place. People will complain that
big Bitcoin players are throwing their weight around, but the blockchain
itself is a voting mechanism that is anything but 1 person = 1 vote.
Of course I wouldn't be the slightest bit surprised if users happily
vote themselves into something looking like a centralized PayPal
replacement in the long run, but at least if that happens the process by
which they get there will be transparent and relatively democratic.
I just wanted to point out how general this mechanism is. Regardless of
what the scriptPubKey form is, standard, P2SH, multisig, whatever to
vote is to simply prove you could have spent the txout.
Ah, you're assuming a direct Patricia tree. Keep in mind that
scriptPubKey's can be up to 10,000 bytes long, and an attacker can use
that (with 10,000 other txouts) to create some extremely deep trees. I
said on IRC a few days ago about how skeptical I am of implementing
consensus critical systems with such huge differences in average and
worst case, but I'll admit this is a decent use-case.
Having said that, proof to SPV clients leaves open the interesting
possibility that a third-party holding Bitcoins on your behalf can prove
that they voted according to your wishes, or even prove they voted
according to all their users wishes. Basically we'd add a rule for the
UTXO tree where a specific OP_RETURN form is included in the UTXO tree,
even though it is unspendable, and is removed from the tree if the
master txout is spent. Note that in this case by "prove they voted" we
mean the service actually taking the step of ensuring their vote was
recorded in the blockchain.
I think the definition of the median requires knowledge of all the points so
it'll have to be a separate sorted tree - kinda complex unfortunately if
you really do want to be able to do full proof to SPV clients. Maybe
just putting the hash of the overall results in the coinbase is enough
for now.
The term to google is "moving median" - looks complex.
Good idea on keeping the code general.
Good points, although keep in mind you've created a lot of consensus
critical code that is easiest to implement with floating point... not a
good thing.
One way to mitigate that risk might be to take advantage of the fact
that unless the rolling median code itself is buggy, a consensus failure
in the calculation is likely to result in different implementations
still having a close agreement on the limit. So maybe we write some code
where we won't build on top of a block that is larger than, say, 95% of
the hard-limit unless another miner does so too?
Step  would be to think about OP_RETURN actually. FWIW Jeff Garzik has
a pull-req ( to enable it,
although only one txout per tx, and only with a 80-byte payload.
Even just some ad-hoc voting by the "raise-the-limit" crowd would be a
good first step to gaging interest.
Is it really? There might be someone clever with a cryptographic voting
protocol, although in the case of Bitcoin we have to let people vote
with arbitrary scriptPubKeys, so almost anything less general than full
on SCIP just means miners force people to use the protocol where
vote-buying is possible.
Good idea. So it'd decrease to the mean of the old and new limits
basically, and if Bitcoin becomes "too centralized" users can simply do
nothing and the process gradually reverses.
Same here.

@_date: 2013-06-10 13:43:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
Solving that problem is pretty easy actually: just add a voting only
public key to your outputs. Specifically you would have an opcode called
something like "OP_VOTE" and put a code-path in your script that only
executes for that specific key.
It'd work best if we implement merklized abstract syntax trees to allow
you to reveal only the part of a script that is actually executed rather
than the whole script, a feature useful for a lot of other things.
Incidentally remember that we can implement as many new opcodes as we
want with a soft-fork by redefining one of the OP_NOP's to be a
OP_VERSION opcode that returns false for a given version:
    version OP_VERSION OP_IFNOT {new opcodes} OP_ENDIF
Nodes with the existing codebase will think the script always succeeds,
because the IFNOT branch isn't taken, leaving the non-false version on
the stack, while new nodes will take that branch.

@_date: 2013-06-10 17:09:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralizing mining 
So here's the parts that need to be done for step # Protocol Work
Basic idea is the miner makes two connections, their pool, and a local
They always (usually?) work on the subset of transactions common to both
the pool's getblocktemplate and their local one. When they find a share
if it doesn't meet difficulty they just hand it to the pool. Currently
that is done by handing the whole block over, correct? I know the BIP
says otherwise, but we should optimize this to just hand over tx hashes
where possible.
If the share does meet difficulty, hand it to both the pool and the
local bitcoind. Should hand it to the pool first though, because the
pool likely has the fastest block propagation, then hand it to local
bitcoind. An optimized version may want to have some record of measured
bandwidth - this applies Bitcoin in general too, although also has other
 Reducing bandwidth
How about for normal shares we just pass the block header, and have the
pool randomly pick a subset of transactions to audit? Any fraud cancels
the users shares. This will work best in conjunction with a UTXO proof
tree to prove fees, or by just picking whole shares randomly to audit.
We'll need persistent share storage so if your connection disconnects
you can provide the pool with the full share later though.
Incedentally, note how the miner can do the reverse: pick random block
headers and challenge the pool to prove that they correspond to a valid
block. With some modifications Stratum can support this approach.
 Delibrate orphaning of slow to propagate blocks
Block headers can be flooded-filled much faster than blocks themselves.
They are also small enough to fit into a UDP packet. Nodes should pass
headers around separately via UDP, optinally with some tiny number of
transactions. When we see a valid block header whose contents we do not
know about a miner should switch to mining empty or near empty blocks in
solo mode that would orphan the still propagating block. Doing this is
safe, we'll never build on an invalid block, economically rational while
the inflation subsidy is still high, and helps reduce (although not
eliminate!) the advantage a large miner with high-bandwidth connections
has over those who don't.
Of course, the other option is to build a block extending the one you
don't know about, which is even more rational, but doing poses major
risks to Bitcoin...
This functionality can be implemented later - it's not strictly part of
pooled-solo mode.
# Pool work
So does eliopool already accept arbitrary shares like this and do the
correct accounting already? (IE adjust share amount based on fees?) What
happens when the pool doesn't get the share directly, but does see the
new block?
+ possible protocol extensions
# Miner work
Basically we need code to merge the two block templates together to find
commonality. I guess you probably want to implement this in bfgminer
first, so add the code to libblkmaker first, then maybe python-blkmaker.
We also want an automatic fallback to local solo mining if the pool
can't be contacted.
+ possible protocol extensions

@_date: 2013-06-14 15:20:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] is there a way to do bitcoin-staging? 
One way to look at what you are describing is to say you want to prove
your sacrifice of potential BTC earnings. That goes back to the PoW
hashcash stuff I mentioned earlier, and is accomplished by simply mining
shares with an unspendable coinbase to prove you did work that could
have resulted in Bitcoins, but didn't.

@_date: 2013-06-14 16:06:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralizing mining 
It strikes me that this would work best if the pool has a mempool with
child-pays-for-parent support where conflicts *are* allowed.
IE you record whatever transactions you know about, conflicting or not,
calculate which ones gives you the most fees/kb, and then figure out
which set of non-conflicting ones are optimal. Of course, "optimal" is
the knapsack problem...
Now you can easilly tell the miners working on shares for you which tx's
would be optimal if they wish to know, and at the same time whatever
shares they send you are most likely to include transactions in your
mempool inventory, and thus they can send just tx hashes to reduce
Part of the broader issue that when we send peers INV advertisements we
should be telling them what the fee/kb is so our peers can prioritize
properly. That'd also help for the case where you want to broadcast two
transactions in a row, but the pair is only profitable because the
second is paying the fee for the first.
Speaking of, the way we tell peers about new blocks is really
suboptimal: we tell every peer, in no particular order, about a new
block via a block INV message, and then we give them the new block in
parallel. I was looking through comp-sci papers on optimal
flood-fill/gossip algorithms for random graph networks and it appears
that optimal is to spend all your bandwidth to send the message to your
fastest peer first, followed by your next fastest and so on. This works
best because you get the exponential growth scaling faster by
propagating the message as "deep" as possible in the network, and it
then can flood outwards from there. Just sorting the peer list by
 when doing INV pushes and when attending to incoming
messages would probably be a big improvement.
Right, I guess the pool wants to be sure you were actually the one who
found the share, rather than just someone who was lucky enough to see it
on the network and submitted it as your own.
That's a good point - the current practice most pools seem to follow of
about a share per second seems very excessive to me. On the other hand,
it does have good user optics. The right solution might be something
akin to P2Pool where the UI level is telling the user shares are being
found so it's clear "stuff is happening", but under the hood only a
small subset are ever sent to the pool.
What part don't you follow?
Sounds good.

@_date: 2013-06-17 13:39:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralizing mining 
Actually the two are orthogonal: a low-priority no-fee tx might result
because it was from a customer paying a merchant via the payment
protocol. The merchant can then respend that tx with a fee to cover
both, but with the current mempool arrangement if the no-fee tx load is
high actually getting that first tx to propagate so the second can will
be difficult.
A nice way to do this would be to accept tx's into your mempool
indiscriminately but delay broadcasting INV messages until you find
child tx's that make the low-profit ones worth mining. When you do find
a child with a sufficiently high fee, send an INVGROUP message to notify
your peers of the new opportunity. Different nodes will have different
ideas of what priority TX deserves to be broadcast, but here provided
the group meets the threshold a peer will always find out.
Whether or not that is a improvement is a really complex question, even
without taking failure into account. If you agressively prioritize peers
that are the most connected and keep your # of peers reasonably low you
can afford the memory to keep track of what tx's your peers already know
about so to save on round trips for TX hash's they don't have. On the
other hand if you have a large number of peers and can't do that, or
need to cut down on bandwidth used up by the INV floods and have a
probabalistic scheme, you are risking more round-trip latency.
Not to mention the nasty problem of how *relying* on TX hashes to keep
your bandwidth down means that anything disrupting that system suddenly
has a big impact on the network. I don't think we really understand all
the nuances of that - look at how few people realize that you need
multiples of average bandwidth to have sufficient emergency bandwidth
available to catch up in the event of a chain fork.

@_date: 2013-06-30 06:12:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
Speaking of, I may have missed it but as far as I can tell Bitmessage
doesn't encrypt node-to-node communications, a serious oversight. Any
attacker that can sniff a large fraction of the network, like the NSA,
can easily use this to track down the originating node of any message,
just like they can do with Bitcoin.
Ah! I had a feeling that might be you. Were you the person who was
creating the 1BTC fee transactions as well?
I just got an email from someone saying they had a few Avalons with that
patch installed actually; that was probably them.
Keep in mind it's not just the mempool that needs changing - the network
protocol semantics need to change too. For the "scorched-earth" strategy
to work you need nodes tell their peers about the total fees a
transaction has attached in addition tot he tx hash. Essentially you are
advertising to your peers what would right now be an orphan, and your
peers need to recursively get dependencies; I'm sure there's a bunch of
edge cases there that would be need to thought out carefully. It's
useful for a lot of things though, for instance when a zero-fee,
zero-priority tx is given to a merchant who now wants to tell miners to
mine it anyway due to a child tx.
What I'd recommend actually for the nearer term is just adding recursive
fee evaluation with a depth*breadth anti-DoS limit, adding the rpc and
GUI adjfee and canceltx commands, adding better wallet support for
conflicts, (someone is already workng on this) and adding a service bit
with preferential peering.
By preferential peering I mean you set aside a portion of your outgoing
peer slots for peers with certain bits set and only fill those slots
with those peers. In addition you can have DNS seeds return peers with
specified service bits set: x0000000000000003.v1.seed.petertodd.org
could be nodes with the first and second bits set. (we might want to
define the upper 8 service bits as a service bit version field so we can
redefine the other 56 in the far off future if required)

@_date: 2013-03-07 06:00:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Large-blocks and censorship 
So with UTXO merkle-sum-fee-trees and fraud notices(1) we can
effectively audit the blocks produced by miners and provide a way for
SPV nodes to reject invalid blocks without requiring the whole
blockchain data.
Next step: How do we prevent censorship? Can we at all?
Basically while UTXO-style proofs allow anyone to determine if a block
is valid, it's fundementally still miners that choose what transactions
to accept into blocks in the first place. Unfortunately the very nature
of a blockchain is that it is meant to prove that transactions are
public and that a consensus exists about what transactions are
spendable, thus any attempt to hide the bare technical details, txins
and txouts, is futile.  Even using encryption doesn't work, because
assuming you convinced a miner to accept your encrypted transaction,
that just shifts the part that makes the transaction public to the act
of revealing the key, which again must be done publicly in the
blockchain to prove consensus.
As transaction volume makes running a validating node more expensive, we
can expect the number of independent pools to decrease, or at the very
least make monitoring those pools easier as volumes grow beyond what
technologies such as Tor can effectively accomodate. This provides the
opportunity to pressure the remaining, identifiable, independent miners
into accepting restrictions on what transactions can be mined.
It's also notably that auditable off-chain transaction systems are
vulnerable. All of the trustworthy ones that don't rely on trusted
hardware require at least some of their on-chain transactions to be
publicly known, specifically so that the total amount of reserves held
by off-chain transaction providers can be audited. At best you can use
Gregory Maxwell's suggestion of maintaining a "reserve" account backed
by funds that rarely move, where new deposits go to non-public addresses
and result in the depositor receiving funds from the reserve account,
but again, if the spendability of those funds is in question, the value
of the reserve itself is also in question. Additionally miners can block
fidelity bond sacrifice transactions easily; again a critical
technologies required to implement some types of off-chain transaction
systems, as well as for many other purposes.
Of course we can just assume that the current pseudo-anonymity of
transactions is "good enough", but consider the case of stolen coins:
even if the bulk of transactions are effectively anonymous, transactions
can always be made public delibrately and miners pressured into
preventing the movement of coins declared tainted.
Finally it's possible that some kind of chaum token system could be
implemented directly in the blockchain, but this has the problem that A:
no efficient ones are yet known, let alone demonstrated, and B: unless
non-chaum token systems are prohibited by a hard-fork with wide
adoption, the censorship risk is miners deciding to not mine any chaum
token transactions. It's easy to imagine a government deciding that
while they will accept transactions that occure on the public block
chain, and are thus at best pseudo-anonymous, are acceptable any attempt
to conduct truely anonymous transactions will be forbidden.
On the other hand, with small blocks the barriers to entry to becoming a
miner remain low, and mining anonymously behind low-bandwidth
anti-censorship technologies such as Tor remains feasible. Any attempt
by a major pool to censor, IE choose not to mine, a transaction will
naturally lead to an opportunity for an anonymous miner to get a profit
mining that transaction, thus we can expect transactions to be treated
fairly equally on a fee per KB basis. In addition, the ever present
possibility of this happening, further discourages large miners from
doing so in the first place, and in turn gives those miners additional
incentive to resist attempts to restrict what transactions they are
allowed to mine.
Of course off-chain transaction systems can still practice censorship of
transactions on their own, but because the decentralized blockchain
still exists communities subject to such censorship can always create
their own auditable and secure off-chain transaction systems for their
own use. Again, the existence of such systems creates economic
incentives to find ways to move value between all off-chain transaction
systems regardless of imposed restrictions, and again the overall
ability to transfer value freely is maintained.
1)

@_date: 2013-03-07 06:34:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] Large-blocks and censorship 
Oh, and it occured to me: merge-mining is also vulnerable to the exact
same censorship forces. Again, with small blocks running P2Pool is
feasible, and P2Pool does merge-mining just fine. With large blocks it's
easy for the pool to ignore shares that try to merge mine, so your
alt-chains competition is also censored.

@_date: 2013-03-07 13:30:35
@_author: Peter Todd 
@_subject: [Bitcoin-development] Large-blocks and censorship 
Now, can we solve this problem robustly with clever technology, as is
done with UTXO fraud proofs? I can't see a way - can you?
Gavin asked me to do a projection for what block sizes could be based on
technology improving, and I think that analysis should consider
carefully to what degree the current system's quite strong censorship
resistance will be impacted.
It's interesting to be talking about censorship of transactions, right
as the support for implementing technical means to block SatoshiDice
transactions is highest. If anything, I think Gregory Maxwell's findings
he has posted on IRC showing roughly three quarters of transactions in
blocks are SatoshiDice related shows how the current large number of
validating nodes makes any effort at even discouraging unwanted traffic
quite difficult. In other words, it's a strong sign the censorship
resistance of Bitcoin works as intended.

@_date: 2013-03-09 23:31:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] Blocking uneconomical UTXO creation 
As discussed endlessly data in the UTXO set is more costly, especially
in the long run, than transaction data itself. The fee system is per KB
in a block, and thus doesn't properly capture the long-term costs of
UTXO creation.
It's also been suggested multiple times to make transaction outputs with
a value less than the transaction fee non-standard, either with a fixed
constant or by some sort of measurement.
The above patch that implements the latter approach, and thus will not
accept into the mempool any txout whose value is <= the fee per KB paid
by the transaction. That fee is then bounded between MIN_TX_FEE and
COIN_DUST, 0.0005BTC and 0.01BTC respectively. The former due to the
fact that the fee can be zero, and the latter so that delibrate high-fee
creation is still allowed. (provably unspendable txouts can of course be
handled specially later)
By basing the calculation on the fee per KB the transaction itself pays
the limit automatically adjustes as the market for blockchain space
changes, and the value of Bitcoins change.
Since scriptSigs greater than 500 bytes are non-standard the marginal
bytes required to spend a txout is always less than 540 bytes. For
standard transactions the marginal cost is usually just a 80+1 byte
signature, and a 33+1 byte pubkey, or 155 bytes. Thus the choice of the
fee for 1000bytes allows a margin to ensure a net positive return, even
if tx fees become more expensive. In particular I think a reasonable
margin is important to deter users from simply deleting wallets filled
with dust-spam, something which gets reported as happening frequently.
It also protects users who do not understand how Bitcoin works from
thinking that repeated small amounts of coins collected from sites
giving away small amounts will add up to an amount that they can
usefully use, and equally protects the long-term health of the network
from those services.
By basing the threshold for what is considered a too-low output value on
ensuring that spending outputs has a net positive return, rather than
trying to come up with some sort of model of UTXO cost, we make the
logic significantly easier to reason about. In particular, it means that
Bitcoin clients can use an unchanging rule based on fees paid, rather
than some constant subject to change as the economics of UTXO costs
change. Note how the total cost of maintaining the UTXO set is
determined by the number of validating nodes, and what that number will
be in the future heavily debated with a possible range spanning many
orders of magnitude.
SatoshiDice will have to change their betting system to have their
"failed bet" messages return enough coins to be economically spendable.
It's notable that Satoshidice seems to have already changed their system
to return what appear to be randomly chosen amounts, likely to get
around the users who have applied custom patches to consider 1 satoshi
output values non-standard. Because this patch does not block
SatoshiDice, nor do I expect it to result in less SatoshiDice traffic, I
expect pool operators to be open to applying it.
Other services such as CoinAd will also have to make changes to either
collect multiple payments together, or use off-chain transactions. I've
spoken with a person who runs one of these sites, CoinAd IIRC, and he
was open to opening an instawallet or easywallet account and using it to
do direct off-chain transactions for users who wanted to be paid
Part of the patch includes code that sends change to fees if creating a
change output would produce an uneconomic txout. This will likely
occasionally generate confusion from users, especially as it will only
happen if they try to send almost all of their wallet.
For a non-upgraded client, accepting zero-confirmation transactions
becomes more risky as the change represents yet another way of creating
a transaction that won't be mined. Fortnately the nLockTime problem has
served to warn people yet again about those dangers.
If fees required on transactions go up in numerical value, the patch
adapts the minimum output size as required.
If fees go down numerically, the minimum output size is also adjusted as
required. If they go down sufficiently that MIN_TX_FEE requires
changing, only one constant needs to change. In particular, the
MIN_RELAY_TX_FEE blocks relaying small output values with small fees
anyway, and it's set to one fifth of MIN_TX_FEE. Additionally most
miners follow the MIN_TX_FEE default, so using that value ensures that
the logic holds true for the more likely case that fees numerically stay
stable or rise. In any case, Bitcoin will never be a good
microtransaction system.
Ouputs representing other assets; "colored coins" and "smartcoins"
To be written after more consensus. Essentially a UI testing script, and
unittests in wallet_tests.cpp need to be written.

@_date: 2013-03-10 04:18:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] Large-blocks and censorship 
Yes, but keep in mind the meta risk, which is that as Bitcoin becomes
centralized one of the types of transactions that will be censored are
ones that preserve your privacy. For instance, as it costs thousands of
dollars to setup a mining pool, and hence mining pools also become quite
visible, it would be very easy for local governments to start doing
things like specifying that transactions must be accompanied with a
proof of identification. With that proof of course Bitcoin can remain
totally legal, and the pool in business.
Why do you expect that? It's always harder to hide a large amount of
bandwidth than a small one, and stenography is limited by the bandwidth
of the data it's hiding it. HD video streams aren't going to require
more bandwidth in the future.
Right now the thing that keeps pools honest is that setting up another
pool is pretty easy; note how most pools are run as hobbyist projects.
Similarly you can always use P2Pool, which is totally decentralized.
But if running the validating node required to run a pool costs
thousands of dollars that competition just isn't there anymore and
starting a new pool isn't an option. Remember there will be a chicken
and egg problem in that the new pool has thousands of dollars in costs,
yet no hashing power yet.
As for constantly moving countries, The Pirate Bay is in the same
position, and as well as they've done, they still wind up getting shut
down periodically. Do you really want access to your funds contingent on
some highly visible mining pools, constantly wondering if their local
government will change their mind?
Anyway, seems that my question was answered: There aren't any clever
technical ways to avoid censorship if validating nodes and mining pools
are centralized.

@_date: 2013-03-12 03:49:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] Blocking uneconomical UTXO creation 
There's been a lot of discussion about this issue, and many people have
asked that Bitcoin not arbitrarily block interesting potential uses of
provably unspendable txouts for data applications, and similarly
spendable txouts representing assets. I've changed my hardline position
and now think we should support all that stuff. However, there is one
remaining class of txout not yet talked about, unspendable but not
provably so txouts. For instance we could make the following a standard
transaction type:
scriptPubKey: OP_HASH160 <20 byte digest> OP_EQUALVERIFY scriptSig: Of course, usually the 20 byte digest would be picked randomly, but it
might not be, and thus all validating nodes will always have a copy of
the data. With the 10KB limit on script sizes you can fit 9974 bytes of
data per transaction output with very little waste.
A good application is timestamping, with the advantage over
coinbase/merkle tree systems in that you don't have to wait until your
timestamp confirms, or even store the timestamp at all. Another
application, quite possible with large block sizes and hence cheap or
free transactions, is secure data backups. In particular such a service,
perhaps called Google Chain Storage, can offer the unique guarantee that
you can know you're data is secure by simply performing a successful
Bitcoin transaction.

@_date: 2013-03-12 05:47:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Changing the fee on already sent transactions 
We can allow for transaction replacement for the purpose of adding fees
to existing transactions safely, and while not increasing the risk of
double-spends by taking advantage of the stubbed out replacement code.
Specifically the replacement code allows for the replacement of a
transaction if a transaction spending the tx that is being replaced is
not in the mempool. Specifically:
664     // Check for conflicts with in-memory transactions
665     CTransaction* ptxOld = NULL;
666     for (unsigned int i = 0; i < tx.vin.size(); i++)
667     {
668         COutPoint outpoint = tx.vin[i].prevout;
669         if (mapNextTx.count(outpoint)){
Followed by the actual replacement logic. We could change this logic to
instead evaluate if the candidate replacement does not remove or
decrease the value of any existing outputs. Adding outputs is ok.
Changing the set of inputs is also ok, provided that there are no
conflicts with other spent transactions. DoS attacks would be prevented
by only forwarding/accepting into the mempool replacements that increase
the fees paid by at least MIN_RELAY_TX_FEE * size - essentially the same
decision to allow the broadcast of the transaction in the first place.
Because a transaction can not be replaced if another transaction already
depends on it the change would not increse double-spend risks for
unconfirmed transactions.
Along with this change code can be added to clients to examine the
mempool and recent blocks to determine what fee would be required to get
a transaction confirmed in what time.
Of course, considering our recent "fun" last night, I'll be the first to
admit that this change needs a lot of testing and careful thought.
However the ability to increase fees on already broadcast transactions
would be very valuable to users as competition for blockchain space

@_date: 2013-03-12 05:57:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
Note that nodes dying en-mass due to OOM failures is a relatively benign
failure mode as the point as which any particular node would die is
uncorrelated with other nodes - it won't cause a network fork.
Implementing a simple and stupid "while [ true ] do ; ./bitcoind ; done"
loop combined with ulimit to keep total memory usage to something sane
is a perfectly acceptable hack until proper mempool code with expiration
can be written. Gavin can talk more about his ideas in that regard.

@_date: 2013-03-12 06:17:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
I'm sure if "mass node death" becomes an issue miners will have plenty
of incentive to temporarily, or permanently, setup some high-memory and
high-bandwidth nodes to accept transactions. The DNS seeds sort by
reliability so it won't be long before nodes are connecting to them.
My home machine has 16GB of ram, bigger than the whole blockchain.

@_date: 2013-03-12 06:26:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
We just saw a hard-fork happen because we ran into previously unknown
scaling issues with the current codebase. Why follow that up immediately
with yet another jump into unknown scaling territory?
I suspect the PR fallout from another chain split, let alone multiple
splits, will be far damaging to Bitcoin than stories along the lines of
"Gee, actually it'd kinda expensive to do a Bitcoin transaction these
days due to all the competition. I dunno, I guess it must be really
popular and valuable or something?"
Lets let the issue rest for a while, and we can all have some time to
work on our various approaches to solving the problem. The worst that
will happen is growth temporarily slows - hardly a disaster I think.

@_date: 2013-03-13 11:05:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] 0.8.1 ideas 
If we're going to consider doing this, at minimum we need to also
include a separate limit for how much the UTXO set can be grown by each
block, calculated as the size of the scriptPubKey + constant metadata.
(tx hash, index  nValue, nVersion, nHeight should cover it)
A P2SH transaction txout would measure 71bytes under that model. Given
that we haven't even shown we can limit the creation of txouts that can
not be spent economically caution would dictate setting the UTXO growth
limit fairly low, say 1/4th of the block limit.

@_date: 2013-03-13 12:04:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] 0.8.1 ideas 
The very statement that we're willing to increase the blocksize as our
solution to increased transaction volume rather go down the path of
off-chain transactions is incredibly controversial.
Fuck it, I'll make this public: I've had at least one person who went to
the trouble of finding my personal phone number just so they could leave
a few text messages saying I was going to do serious harm to Bitcoin. At
the same time I've also had a few people asking questions along the line
of had started and/or was considering starting a formal group opposing
the blocksize increase. I even got a significant anonymous donation a
few weeks ago. (rather fittingly this was done by emailing me an
easywallet URL from a throwaway account)
It's not just forum trolls who care about the issue, even if they make
the most noise about it.

@_date: 2013-03-13 13:48:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Blocksize and off-chain transactions 
You said it best yourself:
10:48 < gavinandresen> Luke-Jr: argument for another day, but I can
almost guarantee that the blocksize limit will be raised in less than 2
years, just based on pressure from the big businesses using the chain
(and no, NOT satoshidice)
Decentralization offers big businesses nothing; they're a regulation
target already by virtue of size alone.

@_date: 2013-03-16 18:17:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Ok to use 0.8.x? 
Hardware mining rigs do not need updating - they all are designed to connect directly to a pool and it is the pool that makes all block related decisions. All the miner, or as I prefer to call them hasher, sees is an 80 byte block header and possibly with stratum and getblocktemplate enough other information like a partial merkle tree to roll the extranonce.

@_date: 2013-05-03 10:18:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
If you're going to take a step like that, the should be rounded off, perhaps to some number of bits, or you'll allow
DNS caching to be defeated.
Make clients check for the largest "rounded off" value first, and then
drill down if required. Some complexity involved...
Maybe I should make my blockheaders-over-dns thing production worthy
first so we can see how many ISP's come at us with pitchforks? :P

@_date: 2013-05-03 11:11:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
Hmm, on second thought you're probably right for the standard case where
it's really P2P. On the other hand it kinda limits us in the future if
seeds have high-bandwidth nodes they can just point clients too, but
maybe just assuming the DNS seed might need high bandwidth as well is
I dunno, given how badly behaved a lot of ISP dns servers are re:
caching, maybe we're better off keeping it simple.

@_date: 2013-05-06 12:12:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
More generally, I think this shows clearly how SPV nodes have weaker
security than constantly operating full nodes, which we knew already, so
why not build a better SPV-specific system instead?
I've noticed on my Android phone how it often takes quite awhile to find
a peer that will actually accept an incoming connection, which isn't
surprising really: why should a regular node care about responding to
SPV nodes quickly?
For fast startup you would be better served with dedicated nodes that
are backed by fast hardware and high bandwidth internet connections.
You can discourage non-SPV use by refusing to relay full blocks.
You can have trusted individuals vouch for these special servers with
SSL certificates so you run less of a risk of connecting to a malicious
one trying to limit what information you see. For the initial
implementation, maybe just make a quick SSL accessible service with HTTP
GET so you don't have to integrate SSL into the network protocol and
have a couple of these HTTP GETable servers running. (IE, the trust is
actually that the SPV seed is honest)
Security will be no worse than before - if any one server/seed is honest
you're ok - and hopefully better due to the accountability. Obviously
you can use the existing bootstrap method in parallel at the same time.
What's good about partitioning between SPV and full node bootstrapping,
is the regular DNS seeds can optimize the other way: accept that some
nodes may turn out to be evil, and limit the damage by returning peers
from the widest pool possible even if some of those peers may be a bit
slow and unreliable. An attacker can't dominate the results by running a
small number of fast reliable nodes because the results returned comes
from a huge pool, so they are stuck with getting access to lots of IP
addresses, and maybe in the future we'll have even better methods of
resisting sybil attacks, and we will be able to implement those methods
even if they mean initial bootstrapping is slower.

@_date: 2013-05-06 12:37:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
Yup, and lets be really clear here: what I'm saying about existing DNS
seeds selecting peers from a wider pool isn't to fundementally reduce
the trust in those seeds, it's to reduce the amount of effort the people
*running* the seeds need to expend to return safe results.
Anyway, DNS returns unsigned data usually - DNSSEC is not widely
implemented - so at least an alternative seed system with SSL certs
could provide a way of getting results from the seed to you in the first
place with a different set of vulnerabilities.  (I'm not going to say
it's really more secure - your ISP can MITM your connections to those
remote nodes anyway - but the types of attacks are at least different)
Speaking of, off-topic for this discussion, but in the future
node-to-node communicate should be encrypted and signed, and seeds
should have a mechanism to return the pubkey the node will use for
communication. This would protect against your ISP MITM attacking your
communications with every node. Of course, Tor hidden service nodes do
this already essentially.

@_date: 2013-05-06 13:19:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
You mean scam you with a zero-conf transaction that hasn't actually been
You know how I feel about zero-conf.
We already depend on OpenSSL, why not just use standard SSL?
Define a per-node compressed pubkey to pass around, and then do whatever
is easiest to get the actual SSL up and running. If we have to use that
pubkey to in-turn sign for a secondary RSA key or whatever due to
compatibility, no big deal.
Define a new service bit SSL and if you connect to a SSL supporting node
switch to SSL within the same TCP connection.
Obfusication probably isn't the hard part, it's SPV bloom filter privacy
that is the tough one, but probably a problem better handled by Tor.
For phone stuff you should work with The Guardian Project - they've
implemented Tor on Android among other things and want to find easier
ways for apps to use it.

@_date: 2013-05-06 13:53:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
We don't have non-repudiation now, why make that a requirement for the
first version? Adding non-repudiation is something that has to happen at
the Bitcoin protocol level,(1) so it's orthogonal to using SSL to make sure
you're connection isn't being tampered with and is encrypted.
1) Non-repudiation is only useful with fraud proofs, and they will have
to be thought out for everything the node might claim.
Exactly. Implement an SSL-protected transport, and leave non-repudiation
and broader issues of node identity as a later, long-term project. Many
client won't even want to support all that complexity, but they'll still
want to cheaply get the advantages SSL has with regard to MITM
resistance and privacy with little effort.
Anyway, the concept of a per-node identity keypair is the first step
towards non-repudiation, and implementing SSL transport.
I run a fast node on EC2 that only accepts inbound connections over Tor
and I regularly have about ~50 inbound peers.

@_date: 2013-05-06 14:19:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
stunnel only works if both sides support it.
re: discovery, the whole reason I brought up SSL was the idea that a
seed whome you have a secure connection to, like HTTPS or SSL, can
include the peer pubkey along with the peer's IP address, allowing you
to be sure you've connected to the peer the seed is giving you rather
than some other imposter.
Equally it'll let you be sure you've connected to the correct peer the
second time.
For applications where you *don't* need non-repudiation SSL is already
implemented and solves the secure peer communication issue, including
encryption, in an efficient way without requiring a lot of code
complexity to implement.
SSL could be implemented as a Google Summar of Code project by an
average developer, and importantly re-implemented by all the alt-clients
out there with relatively little work.
It may even be the case that some usage scenarios do find the CA system
useful. I might want to do -addnode ssl://petertodd.org on my Android
wallet to be sure I've connected to my Bitcoin node rather than some
MITM ISP imposter. I already have a SSL cert from a CA for petertodd.org
that I can use and my Android phone already has a list of CA's I can put
a reasonable amount of trust in.
Sure, but how will non-repudiation be implemented? By having the node
sign the messages they send with their pubkey, and as Mike suggests
likely doing so in some sort of chained hash or preferably merkle
mountain range to allow for constructing proofs over multiple messages.
That has nothing to do with encrypting the transport, and will always be
a lot slower than SSL's symmetric cipher for when you don't need
non-repudiation but do want to be sure you've connected to the right
Per-node keys really need to be per listening address by default. In
fact, I'd argue for creating new keys on startup by default.

@_date: 2013-05-06 15:08:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
re: double-spends - punishing relay nodes and miners for them is a very
bad idea. Ultimately it is the blockchain by which Bitcoin comes to
consensus about what transactions belong in the blockchain - to punish
double-spends implies a second consensus mechanism. Anyway it's
unnecessary: you can hold the actual spender accountable for
double-spends and punish them directly rather than adding a lot of
complexity and dangerous assumptions about propagation to the Bitcoin
core network.
Some useful things you can hold relay nodes accountable for without a
lot of complexity:
1) Having a reasonably correct view of the best block. Make the node
sign a statement including a block hash sequence (the last 3-6 blocks)
and what it believes the current time is.
2) Accurate knowledge of the blockchain. Sign a statement claiming that
what block hash is for a given chain height. Note that due to reg-orgs
this is actually a different statement than  and nodes should be
careful what they are claiming.
3) Accurate knowledge of the UTXO set. Sign a statement claiming that
a given txid:vout for the current best block hash is in or not in the
UTXO set.
4) Accurate bloom filtering; same idea as 5) Make the node identity expensive to obtain. For instance, construct
PoW's including the node pubkey somehow, or purchase fidelity bonds for
the node's identity. Makes sybil attacks more difficult, among other
5) Provide useful propagation/mining services. Sign a txid and
timestamp/blockhash-sequence, and hold the node accountable for how long
it takes the txid to make it into the blockchain. Useful especially for
miners offering the service of mining your transaction.
Be careful not to mix up the concept of a relay node with someone
posessing Bitcoins. Node's don't spend coins, people/wallets do.
That stuff is cool, but we should focus first on simple efforts, like
SSL transport, that do not require complex cryptography to obtain an
improvement in security.
Of course, not to say long-term research is bad, but that's just not
going into the Bitcoin reference client in the near future.

@_date: 2013-05-06 16:43:07
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
Sounds like a lot of research potential with many far off possiblities. :)
Bitcoin is interesting because it provides a nice way to determine the
value of a proof-of-work. Lets suppose you have a digest D and want to
create a proof of work for that digest.
1) Select a block B1 that is reasonably deep in the blockchain. (You
don't want it getting re-orged out of existence) Six blocks deep is
probably plenty.
2) Construct an invalid block header, BP, with SHA256(B1 | D) as the
previous block hash. All other fields can be set to whatever is required
by your hashing unit. (the merkle root would be an option too, but many
hashing setups can't put arbitrary data into it)
3) Hash until you have found the PoW with the difficulty you want.
4) Timestamp BP in the blockchain, resulting in a merkle path M leading to
a subsequent block B2. (1)
Now determining the value of D has a nice compact proof: B1, BP and M
and B2. Taking the minimum of the difficulties of B1 and B2 (in case
they cross a retarget boundry; don't want to create strange incentives)
determine the expected return in Bitcoins from the block reward had the
hasher solved valid blocks instead and you can determine exactly how
much the proof-of-work was worth, kinda...
Things get a bit complex from here on. First of all there isn't a
compact proof that will tell you how much the fees of solving that block
would have been worth, and there can't be because miners can easily
manipulate the apparent fees of a block in both directions.
Also as with fidelity bonds (
the question of which value to use, historic or current, is important
too. If you use the Bitcoin face value increases or decreases of the
value of a Bitcoin are arguably distorting. On the other hand, if you
use historical exchange rates, which currency do you use and where do
you get trustworthy historical exchange rate data? (2)
1) See 2) Which reminds me, I do need to get around to bugging Mt. Gox to PGP
sign their exchange rate data and timestamp it properly, or do one or
both myself. It should be archived at archive.org or something too,
heck, the blockchain should be too, although timestamping that will
require a bit more work...

@_date: 2013-05-06 17:29:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] Cold Signing Payment Requests 
The widespread disclosure we do is a good thing for sure.
Keep in mind that Bitcoin is brand new technology, and brand new fields
tend to get lots of people coming in and trying to patent them. Public
disclosure, and bitcointalk, the email list, and github all count, is a
valuable tool to ward off potential threats in the future if it ever
comes to that.
FWIW it might not be a bad idea to see if archive.org would accept some
of the key documentation like the development section of the forum, the
email list archives, and the irc logs. Some issues, especially on the
forum, with people's ability to edit posts after the fact, but we're
breaking new ground here and the history should be archived.

@_date: 2013-05-06 19:44:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
One last thought... suppose you want to make these proof-of-works
transferable on the blockchain, as is easily possible with
announce/commit fidelity bond sacrifices. The problem is of course
re-use - you don't want it to be possible to use the same proof-of-work
for a different asset.
So for D use the txid:vout pair of a txout that you can spend, then
spend it to some output to create the start of the smartcoin/contract
asset chain. The txout can only be spent once, so the PoW is inherently
The final proof is a more compact than a fidelity bond proof, just the
PoW block and a single transaction and existence proof rather than two
or three. (announce, commit, and commit txin if sacrifice is via fees)
Unfortunately PoW schemes do mean you are actually taking away from the
overall security of the network, and if there was a lot of demand for
these things it will lead to the undesirable effect of making it easy to
rent hashing power. Botnet owners will be happy to have a task that
requires even less communication than Bitcoin itself. Finally the
varience inherent in them is annoying too. But it's an interesting idea.

@_date: 2013-05-08 19:44:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
Who knows?
Satoshi used 32-bits and those fields can't be changed now without every
single Bitcoin user changing all at once. (a "hard-fork" change)
We'll probably need to do one of those eventually for other reasons, so
we might as well leave fixing the timestamps until then.

@_date: 2013-05-08 21:57:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
Remember that interpreting the timestamp on a block for the purposes of
timestamping is a lot more subtle than it appears at first.
Any node will accept a block with a timestamp no more than two hours
ahead of what it thinks the current time is. That time is adjusted by
the median of the timestamps reported by your peers. For instance the
RPC call getinfo returns, among other things:
    "timeoffset" : -1,
That is saying my node's wall clock time is 1 second behind the median
reported by it's peers - pretty good!
Naively you might think this means block timestamps are accurate to
within 2 hours right? Well, it's not so simple. Nodes will accept any
block with any timestamp *after* the median of the last 11 blocks. From
    // Check timestamp against prev
    if (GetBlockTime() <= pindexPrev->GetMedianTimePast())
        return state.Invalid(error("AcceptBlock() : block's timestamp is too early"));
So in theory a miner could prevent that block from moving forward,
although if they do they drive up the difficulty, so all miners have an
incentive to set the timestamp accurately.
There are two types of timestamps possible: proofs that data existed
before a time, and proofs that data existed after. With the former type
the *later* the proof says the data existed, the more conservative the
assumptions behind the proof. So simply adding two hours to the block's
timestamp is relatively reasonable. (this assumes the attack managed to
mine a single block, and all nodes have accurate clocks)
The latter type, where you prove data existed after a given time, is a
much more tricky thing to apply. The genesis block is a great example
with that famous newspaper headline:
    The Times 03/Jan/2009 Chancellor on brink of second bailout for
    banks
As I mentioned in my other (private) email to you a few minutes ago, the
sig of my emails has the latest block hash in each one. The basic idea
is called a random beacon; NIST has a good description and a project to
create one:
Now technically speaking a random beacon is actually a more
sophisticated concept than just timestamping, the random beacon's value
is public and distributed widely, but for timestamping the idea is
basically to have an unpredictable number known to have been produced at
a certain time.
So you know this email was written after block  timestamp
2013-05-09 01:21:52 right? Not so fast. All you actually know is the PGP
*signature* was created after that time, because the actual text of the
email is independent of the beacon nonce. (dunno if I have the correct
terminology here FWIW)
For a blockchain it's easy enough, the blocks naturally depend on a
genesis block, but applying the concept more generally is tricky and
application dependent; consider for example proving you created a
keypair after some data, which might be a useful thing to prove if the
secret key was created in some tamperproof hardware that you know has
left the factory and is in your possesion. It's easy to see how to do
this with ECC: just use the same techniques as in HD wallets to derive
To use the blockchain as a secure random beacon you need to make two
assumptions, 50% of the hashing power is controlled by honest miners,
and those honest miners have accurate clocks. With those assumptions you
can work out what is the minimum possible time the block could have been
accepted by the GetMedianTimePast() function and you are good to go.
What do people do in practice? Well look at
 they just give the timestamp and
nothing else. Same for OpenTimestamps. (although I'm adding this email
to my notes, half the reason it's so detailed...)
Back to the block header time... Frankly, the easiest thing to do is
just have a flag day where blocks after a certain height are considered
to have a different epoch from the standard 1970 one when computing
their time. Boring, but it works fine and only needs to be updated every
few decades.
You're midstate idea is very clever though and could come in handy in
the future for different purposes. Eventually we should discuss this
with the ASIC manufacturers - if it can be implemented as a firmware or
FPGA upgrade in the field all the better.

@_date: 2013-05-08 22:42:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
Ah, shoot, I just realized we both got missed Pieter's point entirely:
he means to change the meaning of the header timestamp to be relative
time passed since the last block...
Well, it was a nice writeup! Thanks for the correction re:
probabalistic; you are absolutely correct.

@_date: 2013-05-09 07:46:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] An initial replace-by-fee implementation 
The patch makes the concept of a 0-confirm double-spend obsolete
basically. The model is rather than having some vague, insecure, easily
broken, de-facto no-replacement rule it replaces it with something very
easy to reason about: you are bidding for blockchain space, and you can
adjust your bid after the fact.
The reality is zero-conf double-spends aren't that big of a problem
because the vast majority of payments have other mechanisms they can use
instead of relying on the defacto behavior of dozens of major miners and
Long story short, we're better off if we don't give people a false sense
of security.
A node has no idea which transaction output is change and which one
isn't; if nodes could distinguish change from payment your privacy would
be badly violated.
By allowing simple replacement without further rules the fee adjustment
process can go on as long as required, without you running out of
additional transaction inputs and without causing the transaction to get
bigger and bigger each time.
It also allows more interesting use cases, like adding additional
outputs to a transaction after the fact as more payees become known, or
if two unrelated parties decide to combine their transactions to save on
blockchain space and preserve their privacy.
Eventually the P2P protocol can have delta compression support, so the
network bandwidth required to merge two transactions into one will be

@_date: 2013-05-09 08:20:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] An initial replace-by-fee implementation 
Indeed. That's the point of the blockchain, to take all those potential
inconsistencies and vote on a true transaction history to achieve
Right now we don't have double-spend proof propagation, so the
"net-split" attack is actually totally trivial: just broadcast two
different, mutually incompatible, transactions at the same time. About
half the time the recipient will get the payment, the other half of the
time the payment they thought they were going to get is invalidated.
It's very, very rare for sites to have protection against that;
blockchain.info's shared-send mixer is one of the few exceptions. But
the have access to a whole network monitoring service with connections
to nodes all over the planet.
It's not ideal, but it still protects against after-the-fact blockchain
Statistics is hard - you can't get it right all the time. Besides, what
happens when everyone adds a safety margin? Some people can afford to
wait, so for them starting at a low bid and raises it makes a lot of
Think about that problem a bit harder. :)

@_date: 2013-05-11 00:53:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase TxOut Hashcash 
It has been previously(1) proposed that hashcash using the same PoW
function as the Bitcoin block hashing algorithm be used to create
hashcash whose value is denominated in Bitcoins. This poses two problems
however: widespread use of such hashcash would harm overall network
security and determining the value of the hashcash requires knowing the
revenue miners can gain from transaction fees at a given block height -
a non-computable function. However, with some modifications we can
extend the idea to directly denominate the hashcash in Bitcoins at the
cost of a small increase in proof size.
Recall that the fundemental problem is the need to do some work to make
digest D have value V, resulting in a proof that can be given to a third
party. We want V to be denominated in Bitcoins, and we want the actual
economic cost to create P to be as close as possible to the face-value
V. Finally should computing P result in a valid Bitcoin block header,
the creator of the proof should have a strong incentive to publish their
header to the P2P network and extend the current best chain.
# Proof structure
Lets look at the elements of the proof from the block header to the
 PoW Block Header
This must be a valid block header. It is particularly important to
ensure that the header can be linked to the actual blockchain, although
the header itself does not need to be a part of the chain, and hence the
block hash does not need to meet the difficulty requirements.
 Previous Block Headers
The proof may optionally include one or more previous block headers in
the event that the PoW block header's previous block is an orphan.
Unlike the PoW block header, these block headers MUST meet the
difficulty requirements although an implementation MAY skip actually
checking the difficulty if a difficulty retarget has not happened or the
PoW is timestamped. (see below)
 Partial Transaction and Merkle Path
The partial transaction consists of a SHA256 midstate followed by
exactly one transaction output. The merkle path to the PoW block header
MUST prove the transaction was the coinbase transaction and not any
other transaction.
 Transaction Output
The last transaction output must have a scriptPubKey consisting of
exactly one PUSHDATA op which pushes H(D | N) to the stack. Its value,
V', is the basis for determining the value of the proof of work. V' must
satisfy V' < k*Vi(h) where Vi is the inflation reward for the PoW block
height and k < 1 For a number of reasons, including making sure there
are strong incentives for broadcasting succesful PoW solutions, the
value of k should be chosen fairly conservatively; the author suggests k
= 1/10 as a ballpark figure. Finally N is some fixed value specific to
hashcash of this form to ensure the txout proof can-not be reused.
Vi can also be calculated as the median of the last n "anyone-can-spend"
outputs seen in coinbases when the value of the inflation reward falls
low enough that using the inflation reward is impractical.
 Timestamp
If the proof-of-work is used after a difficulty retarget the PoW needs
to be timestamped in the block chain with a merkle path leading to a
valid block header. The difficulty used for calculating the value of the
PoW then becomes the minimum of the difficulties of the PoW previous
block and the timestamp.
# Determining the actual value of the PoW
The proof proves that work was done to find a valid block header. That
block header, had it met the difficulty threshhold, could have created a
valid block worth at least the inflationary reward Vi(h) to the miner.
The coinbase transaction output and merkle path shows that were such a
block found, the miner would have then given away V' to whomever managed
to create a transaction spending it when the coinbase matured. The
coinbase takes 100 block to mature, so the chance of any one miner
collecting it is proportional to the hashing power they control.(*)
*) As with fidelity bonds we make the assumption that no party controls
more than 50% of the hashing power - the assumption underlying Bitcoin's
security anyway. If this assumption is proven incorrect or
insufficiently strong, possibly due to a cartel of miners banding
together to create low-cost PoW's, the output can use the provably
unspendable/prunable OP_RETURN  scriptPubKey instead with a
non-zero value.
With P(block hash, target), the expected probability of a valid PoW
being found given the work required to create the block hash with the
given difficulty target, we can finally calculate the value of the PoW
in terms of expected cost: V = P(hash, target) * V'
# Pool implementation and 51% attack security
Because doing the work required to create coinbase txout hashcash is
sufficient to also create a valid block a pool can safely rent out
hashing power to create hashcash of this form on demand without making
it possible to rent large amounts of hashing power directly on short
notice. (though some extensions to GetBlockTemplate for hashers
verifying it may be required)
Because the anyone-can-spend txout is the basis for the value of the
hashcash the value remains computable even if transaction fees become a
larger proportion of the block reward in the future.
Unlike announce-commit sacrificies(2) proofs with very small values can
be easily created; the pool operator can make a trade-off between the
profit varience - remember that a block header with a valid PoW
represents a loss - and latency by adjusting the proof of work
difficulty and V'.
As an aside, note how the mechanism of a anyone-can-spend txout in a
coinbase can replace the announce portion of an announce-commit
sacrifice; a coinbase transaction is the only case where a single merkle
path proves that the transaction output was possible to spend in a
subsequent block, but was not yet spent; also an argument for allowing
coinbase transaction inputs.
# Application: Paying for additional flood-fill bandwidth
Additional messaging applications built on top of the Bitcoin P2P
network would be useful, yet there needs to be some general mechanism to
make DoS attacks expensive enough that they are impractical. For
instance a useful P2P network feature would be a mechanism to propose
trust-free coin mixes transaction outputs, propose specific txout sets,
and finally a mechanism to broadcast valid ANYONECANPAY signatures so
the inputs and outputs can become a valid transaction. By separating the
txout and signature broadcasts, who is paying for what output is made
very difficult to determine.
Of course such a mechanism will likely come under attack by those trying
to combat anonymity. However with the coinbase txout hashcash mechanism
those attackers are forced to either contribute to the security of the
Bitcoin network or incur much higher opporuntity costs for conducting
their attack than honest nodes pay. (remember how the choice of k = 10
makes for a large ratio of maximum V' value to Vi(h) inflation reward)
To reduce amortized proof size one proof can be used for multiple
payments with Rivest PayWords and similar techniques.
# PowPay - Off-chain, anonymous, probabalistic payments
By setting the special txout to a scriptPubKey spendable by the
recipient we can prove to a third party that work was done that with
probability P(hash,target) could have resulted in a txout spendable by
them of value V' Thus the expected value of the payment is V = P(h,t)*V'
The recipient needs to make the proof non-reusable, either by recording
all proofs submitted, or by requiring a nonce in the scriptPubKey: (*)
     DROP {additional ops}
*) Note the implications for the IsStandardInput() test.
Because the recipient has no way of knowing how the sender paid to have
the hashing done on their behalf the source of the funds is unknown to
them. Additionally the payment can be of any amount less than a full
block reward, and the time varient between actual payments can be
reduced to, in theory, as little as the block interval itself with 100%
miner participation.
 Maximum Payment amount
Unlike coinbase txout hashcash the maximum value of a PowPay transaction
is strictly limited by the inflation reward; the trick of calculating
actual cost by prior sacrifices doesn't work because no honest sacrifice
is involved. In any case it is desirable for the mechanism to account
for a large percentage of total transaction value.
The issue is that should a valid block be found either the miner must
still have a strong incentive to broadcast that block that can be proven
to the recipient, or the miner must not be the one who controls that
The latter option is possible by inverting the relationship: now the
recipient constructs the block, and the sender simply arranges for a
valid PoW to be created - essentially the recipient acts as a mining
pool with an extremely high minimum work, and the sender provides
hashing power. With the 1MB blocksize the cost to operate the full
validating node required is low and attacks on block propagation are
difficult to successfully pull off.
 Supporting PowPay volume in excess of inflation reward + tx fees
To support overall PowPay volumes that are in excess of the inflation
reward and transaction fees the sender can provide the recipient with
signed transaction inputs subject to the constraint that only blocks
with PoW's generated by the sender can be used to spend them. For
instance a nonce in a well-known place can be provided by the sender and
included in a modified block header. By modifying the block hashing
algorithm so that PoW-withholding is not possible - a significantly more
serious problem in this application - the sender still is forced to send
all potential solutions to the recipient, including possible winning
ones. Provided that attacking block propagation is difficult the sender
can't prevent the reciver from spending their transaction inputs.
 Scalability
PowPay can provide much greater scalability than Bitcoin itself, in
terms of payments per second, however it is still limited in terms of
actual fund transfers to recipients per second. A naive implementation
would give a actual transfer every ten minutes maximum, and a highly
sophisticated solution 7/second. (albeit probably requiring a hardfork
to solve PoW withholding and/or use of third parties)
At the same time the proofs required become large with an increased
blocksize, and in the case of the inverted "recipient builds blocks"
mode the recipients either incur large costs running full nodes, or
greatly disrupt transaction flow for on-chain users by mining blocks
with no transactions in them at all. (remember that a recipient who
trusts someone else to construct the blocks for them is trusting that
third-party to do so correctly)
The latter is especially problematic because as the blocksize is
increased a higher percentage of the cost of mining goes to the overhead
required to run a validating node, rather than hashing, which has the
perverse effect of decreasing the cost of mining blocks with no
transactions in them at all. (or transactions that the miner knows have
not been revealed to other miners)
The analysis of this strange mixed bag of incentives is highly complex.
# Paying for mining
TxOut HashCash and PayPow both require the sender to somehow get someone
to mine on their behalf. The exact nature of these relationships will
vary and are beyond the scope of this paper.
# Eliminating PoW withholding
While the above examples have used economic incentives possible within
the existing Bitcoin system a structural incentive is possible as well.
A nonce N is chosen by the party paying for the PoW, such as a pool or
PowPay recipient, and H(n) is included in the block header.(*) The PoW
function is then modified to consider the PoW valid if the sum of the
expected hashes required to find H(B) and H(B | n) exceeds the current
difficulty target.
*) Note how the block header can be extended, while remaining fairly compatible
with existing ASIC mining hardware, by taking advantage of the fact that
ASIC's use the SHA256 midstate at a starting point for their PoW
1) "Re: [Bitcoin-development] Discovery/addr packets (was: Service bits
for pruned nodes)" - 2013-06-06 - Peter Todd  -
bitcoin-development email list
2) "Purchasing fidelity bonds by provably throwing away bitcoins" -
 - Peter Todd
3) "Re: 32 vs 64-bit timestamp fields" - 2013-06-09 - John Dillon
 - bitcoin-development email list

@_date: 2013-05-14 14:41:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin2013 Speakers: Include your PGP 
report: Every talk will be widely witnessed and videotaped so we can get some
reasonably good security by simply putting out PGP fingerprints in our
slides. Yeah, some fancy attacker could change the videos after the
fact, but the talks themselves will have wide audiences and a lot of
opportunities for fraud to be discovered. That means it'd also be
reasonable for people to sign those keys too if you are present and are
convinced you aren't looking at some impostor. (of course, presenters,
check that your PGP fingerprints are correct...)
Remember that PGP depends on the web-of-trust. No single measure in a
web-of-trust is needs to be absolutely perfect; it's the sum of the
verifications that matter. I don't think it matters much if you have,
say, seen Jeff Garzik's drivers license as much as it matters that you
have seen him in a public place with dozens of witnesses that would
recognize him and call out any attempt at fraud.
Secondly remember that many of us are working on software where an
attacker can steal from huge numbers of users at once if they manage to
sneak some wallet stealing code in. We need better code signing
practices, but they don't help without some way of being sure the keys
signing the code are valid. SSL and certificate authorities have
advantages, and so does the PGP WoT, so use both.
FWIW I take this stuff pretty seriously myself. I generated my key
securely in the first place, I use a hardware smartcard to store my PGP
key, and I keep the master signing key - the key with the ability to
sign other keys - separate from my day-to-day signing subkeys. I also
PGP sign emails regularly, which means anyone can get a decent idea of
if they have the right key by looking at bitcoin-development mailing
list archives and checking the signatures. A truly dedicated attacker
could probably sign something without my knowledge, but I've certainly
raised the bar.

@_date: 2013-05-14 15:31:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin2013 Speakers: Include your PGP 
What guarantees do you think a keyserver provides about the keys it

@_date: 2013-05-15 07:19:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
Protocols aren't set in stone - any attacker that controls enough
hashing power to pose a 51% attack can simply demand that you use a
Bitcoin client modified to provide the attack with the full transactions
from the beginning. Any blocks containing transactions with unknown
contents will be attacked into oblivion.
On the other hand if the "attacker" has less than 50% of the hashing
power, they have no choice but to let other blocks through, and provided
miners are free from regulation imposed on them you can bid to get your
transactions mined with fees. Anyone using a blockchain-based
crypto-currency simply has to accept that mining is a random process and
getting a transaction confirmed is inherently unreliable.

@_date: 2013-05-15 07:38:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] 2BTC reward for making probabalistic 
Now that I have the replace-by-fee reward, I might as well spread the
wealth a bit.
So for all this discussion about replace-by-fee and the supposed
security of zero-conf transactions, no-one seems to think much about how
in practice very few vendors have a setup to detect if conflicting
transactions were broadcast on the network simultaneously - after all if
that is the case which transaction gets mined is up to chance, so much
of the time you'll get away with a double spend. We don't yet have a
mechanism to propagate double-spend warnings, and funny enough, in the
case of a single txin transaction the double-spend warning is also
enough information to allow miners to implement replace-by-fee.
So I'm offering 2BTC for anyone who comes up with a nice and easy to use
command line tool that lets you automagically create one version of the
transaction sending the coins to the desired recipient, and another
version sending all the coins back to you, both with the same
transaction inputs. In addition to creating the two versions, you need
to find a way to broadcast them both simultaneously to different nodes
on the network. One clever approach might be to use blockchain.info's
raw transaction POST API, and your local Bitcoin node.
If you happen to be at the conference, a cool demo would be to
demonstrate the attack against my Android wallet. I'll buy Bitcoins off
of you at Mt. Gox rates + %10, and you can see if you can rip me off.
Yes, you can keep the loot. :) This should be videotaped so we can put
an educational video on youtube after.

@_date: 2013-05-15 08:19:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] 2BTC reward for making probabalistic 
Oh, and while we're at it, a good starting point for your work would be
Gavin's spendfrom utility in the contrib/spendfrom directory in the
Bitcoin-QT respository.
Also please do keep in mind that it's much better for the community if
an attack is demonstrated first, followed by releasing the code some
time later.

@_date: 2013-05-21 06:05:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] Double Spend Notification 
You can do better than that actually: you can arrange the transaction
such that the double-spender is hurt by asking them to pay an excess on
top of the initial payment, and having that excess get returned to them
in a subsequent transaction. Of course, that's trusting the merchant,
but you're trusting the merchant to ship to a product anyway so...
A really interesting example for this though would be applications where
you are making a deposit. You credit the customer account immediately
with half of the deposit amount, allowing them to immediately spend that
portion for something transferable. (perhaps an alt-coin) If the
customer tries to double-spend you burn half to fees, still leaving the
other half to pay for what they did spend. If they don't double-spend,
the rest of the balance becomes available after n confirmations. A
BTC->alt-coin exchange could use this mechanism for instance, although
it only works with widespread replace-by-fee adoption; blockchain.info's
shared-send service is another application, as is SatoshiDice. (the
failed bet tx can be the refund)
What's nice here is even if the customer tries to pay a miner to do the
dirty work, a short-term rational miner still has an incentive to screw
over the customer by accepting the merchant's double-spend. Now the
customer can promise the miner future business, but they've shown
themselves to be dishonest... how much honor is there among thieves?

@_date: 2013-05-21 06:06:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Double Spend Notification 
piuk wrote a double-spender that I think meets the criteria for the
reward: I'll get a chance to test it properly when I'm back from vacation, but
looks like he's getting the 2BTC. If it does work as intended I'm also
planning on doing a demo/video at the next Toronto Bitcoin Meetup to
demonstrate the attack in a real-life exchange.

@_date: 2013-05-31 12:57:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralizing mining 
I just posted the following to bitcointalk.
Right now between two to four running the largest pools control Bitcoin
in the short term. That's a lot of hashing power in the hands of very,
very few people. In addition pools have little incentive to run secure
operations, and many pools have been hacked with their funds stolen.
Those hacks could just have easily been used to attack the network
This needs to change.
Pooled-solo mining is a concept Gregory Maxwell, Luke Dashjr and I were
discussing at the conference two weeks ago. (credit goes to Greg and
Luke; I was mostly just listening) Basically the idea is that miners
with mining equipment run a local Bitcoin node and use that node to
construct the blocks they mine - the same as if they were solo mining.
The pools job is then to only track shares and organize payouts.
If the pool gets hacked the worst that can happen is miners are ripped
off, rather than Bitcoin itself being attacked. With pooled-solo mining
even a pool with a majority of hashing power wouldn't be able to do much
harm to Bitcoin. (depending on the implementation they may be able to
blacklist specific transactions - the pool needs to know what
transactions are in the share to credit fees properly)
Tech-wise Luke created getblocktemplate last year as a means to audit
mining pools. I'm sure Greg and Luke can explain the nitty gritty
details better than I can, but essentially the plan is to take
getblocktemplate and extend it as required for pooled-solo mining. This
will include pool and miner software initially, and later improvements
to GUIs and what not to make the whole process easier.
With the success of my recent video project I also want to make this
Keep Bitcoin Free's next project, specifically funding a developer
(likely Luke) to make this happen. Additionally once software is written
and easily usable a good follow-up would be a video and other media to
promote the idea to miners. No guarantees we'll be able to come up with
commercially competitive remuneration, but we can at least come up with
a "Thank you" tip. But first lets discuss the technical requirements to
get an idea of what the scope is.
Finally, for the record, a big part of the reason why myself and other
Keep Bitcoin Free supporters are interested in doing this is very much
to take power over the direction of the network from big pools and put
it into the hands of thousands of individual miners. It's much easier to
convince people that changes to Bitcoin, like increasing the blocksize,
are directly impacting decentralization when individual miners are
seeing that happen to themselves.

@_date: 2013-11-04 05:52:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Zeroconf-safe tx replacement (replace-for-fee) 
Here's the easy part done:
The rules are pretty simple: a replacement can only happen if every
output in the old transaction has a corresponding output in the new with
the same scriptPubKey, and of equal or greater value. All old tx outputs
must also be unspent. For implementation reasons, the order of the
outputs must also be the same, and the code will never replace two
transactions with one.
If someone wanted to mine with the above code, I'd say go right ahead.
(modulo general testing concerns)
Client-side though it shows a flaw with the Bitcoin wallet code that I
should have realized months ago: essentially a transaction in your
wallet with double-spent inputs forever blocks those inputs from being
spent. This doesn't happen too often because you're wallet will
currently never create double-spends, and will never respend unconfirmed
coins from someone else, but any CoinJoin implementation violates that
assumption and an attacker could easily cause a lot of havok.
I'll have to think about the issue further, but essentially the wallet
needs to recognize when a transaction's inputs no longer exist, and mark
the remaining inputs as unspent. Actually deleting those transactions
from your wallet is secondary to that more important concern.

@_date: 2013-11-04 06:53:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
I proposed this as a means of giving a mechanism for wallets to get
non-sybilled peers as well.
Doing so encourages pools to only bother connecting to other pools,
which is a strong centralizing force. But given the nasty incentives
present anyway - it's in your advantage to distribute your blocks to no
more than a majority of hashing power if you can do so consistently -
I'm unconvinced that this won't happen anyway.
The maximal benefit would be if two sets of addresses were published:
public and private. The issue with publishing addresses is DoS attacks,
but publishing Tor addresses doesn't stop attacks. What would discourage
attacks however would be to encrypt that data such that only the
creators of specific prior blocks could decrypt it. This limits the
audience to those with incentives not to commit a DoS attack. (DoS
attack the IP, and you'll no longer get preferential peering)
Say what you want about centralization, but for the pools involved it's
a good idea.
On a technical level, the coinbase is limited in size, and people use it
for other purposes, so lets define a standard where this data is stored
in an OP_RETURN txout of the form:
OP_RETURN     ...
Multiple values with the same key should be allowed. This data should be
placed in the last txout so that SPV nodes can eventually be given it
with a SHA256 midstate.

@_date: 2013-11-04 06:59:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] Zeroconf-safe tx replacement 
There's been a number of uses found for tx-replacement beyond simply
modifying fees. In additition, allowing for the value of a specificly
designated change address to be changed after the fact is not compatible
with current zero-conf-using implementations; they don't know to treat a
txout as special so allowing its value to be reduced would allow for a
zeroconf attack.
Anyway, if you look at the code that actually implements the
replacement, it's extremely simple already. I see no reason to make it
less general; transaction relaying rules are not part of consensus.

@_date: 2013-11-04 07:20:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
It's worth pointing out that my previous post on this list for
"near-block broadcasts" - where blocks that almost but not quite met the
proof-of-work threshold are also broadcast so that propagation of
transactions can be proven - also naturally leads to their proposed
solution. Any miner who sees a near-block-broadcast extending a chain
fork that they aren't mining on would naturally see that as evidence
that the other side has more hashing power, and thus it's in their
interest to mine it rather than the side they are mining.
You know, the whole paper follows the same logic as the point I made
months ago in how if there is no explicit blocksize limit miners have
incentives to make their blocks large enough that they only propagate to
just over 50% of the hashing power, thus causing their competitors to
waste effort.  They analyze the situation in terms of a sybil attack,
where I proposed a more fundemental mechanism to achieve the same goal
based on simple physics.

@_date: 2013-11-04 09:26:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Actually on further reflection this idea will make the attack described
in the paper easier to carry out, rather than harder.
I think where you're misunderstanding originates is the description of
this attack as requiring a sybil attack on the network - you see this
underlying sybil as one of numerical advantage, when it's actually one
of *informational* advantage.
Remember that the selfish miner strategy outlined in the paper is
essentially a way to use knowledge of what blocks miners will be mining
on, from the "first seen" rule, and the ability to broadcast blocks you
have mined more widely than other miners. That knowledge and ability is
then used in conjunction with a small lead (obtainable by chance) to
outpace the rest of the network.
By making all miners easily identifiable you make gaining that
informational and broadcast capability easier to obtain rather than
harder. The attacker now only needs to connect to every identified miner
with especially fast nodes. With judicious use of DoS attacks and low
latency they can still gain the informational and broadcast "upper hand"
over other miners and carry out the attack.
Where the paper goes wrong is they don't recognize the fundemental
nature of the strategy being based on an informational advantage. Their
"pick a random side of the fork" strategy may work to some extent, but
it's incomplete and isn't necessarily rational for the miners
The correct, and rational, approach for a miner is to always mine to
extend the block that the majority of hashing power is trying to extend.
The current relay rules don't give you that information at all, but they
can if we do two things:
1) Relay all blocks that meet the PoW target. (as suggested in the
   paper)
2) Relay block headers that nearly meet the PoW target.
Mining strategy is now to mine to extend the first block you see, on the
assumption that the earlier one probably propagated to a large portion
of the total hashing power. But as you receive "near-blocks" that are
under the PoW target, use them to estimate the hashing power on each
fork, and if it looks like you are not on the majority side, switch.
This very effectively defeats the paper's selfish-miner strategy, as all
miners will very quickly be mining on the block that truly has the
majority of hashing power trying to extend it. This is also a better
overall outcome, because it puts the 51% attack threshhold back at 51%

@_date: 2013-11-04 09:46:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Remember that the attack described in the paper *doesn't* depend on the
ability to selectively block or even just slow down anything - it works
even on a unlimited bandwidth jam-free network so long as latency is
As for other possible attacks, if you can selectively block or slow down
certain near-target headers you haven't achieved anything novel. Why not
use that ability to block or slow down blocks themselves? Even if you
did block some PoW headers for whatever reason the original purpose of
broadcasting them - getting all hashing power to work to extend the same
block - is still achieved.

@_date: 2013-11-04 10:04:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
The problem is picking which side of the fork you mine on randomly isn't
rational for an individual miner. The time that you heard about a block
is important information: the block you heard about first is more likely
to have propagated to the majority of the hashing power than the one you
learn about second. You're rational incentive is to always mine on the
majority side as that side has the highest probability of no competing
blocks being found when the next block is found. (with the one exception
of the previous block being yours) In addition the next block found will
propagate to the majority of hashing power faster, as that majority
already has the previous block. By suggesting that miners pick randomly
half the time they will be going against their best interests. (if not
the interests of the network as a whole)
On the other hand my near-target broadcast solution gives miners honest
proof of what the majority actually is. Making use of that information
is the economically rational choice even at an individual level. Yet it
still defeats the attack, and it does better in returning the threshold
to the originally assumed 51% level.

@_date: 2013-11-04 10:46:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Feedback basically. So suppose the hashing power is split exactly 50:50,
with half the hashing power hearing about one block first, and half the
other. Also suppose the near-target threshold is 1/64th, that is a block
header that means a target with difficulty 1/64th of the actual
difficulty will be broadcast around the network by nodes. With a 10
minute block interval, near-target block headers will be found on
average every 9.4 seconds.
Eventually one of the two halves will find a near-target PoW solution,
and the corresponding block-header will be broadcast on the network. Now
if you are a miner, and you receive such a PoW solution, that's evidence
that whatever block that block header built on has more hashing power
than other competing blocks. Thus you would be rational to switch, and
start mining to extend that block if you aren't already. Once miners
start doing that, very soon another near-block solution will be
generated, giving even more certainty about what block the majority are
mining on.
Of course, it may be the case that competing near-block headers are
found, but no matter: as long as miners switch to the block with the
most hashing power, this forms a feedback effect that quickly brings
everyone to consensus. With everyone mining to extend the same block,
there's nothing the selfish miner can do; there's no disagreement to

@_date: 2013-11-04 11:07:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
(not sure if you meant this to go to the list, my apologies if not)
Right, but the thing is, if all miners quickly come to consensus and are
all mining on the same block, there's nothing the attacker can exploit
in the first place.
Suppose Alice the attacker is 100 blocks ahead of the main network
somehow. We'll say the other miners are working to extend block n, and
she's in posession of 100 blocks extending that. She also has just under
50% of the hashing power.
Now when the main network finds a block n+1, Alice can do one of two
things: she can publish her own n+1 block, or she can do nothing. If she
does nothing, the main network will find block n+2 faster than she finds
n+101, so eventually she loses. Thus she has to publish.
In your attack she publishes to a subset of nodes strategicly, splitting
the hashing power between nodes working to extend her n+1, and the other
n+1 found. However, with near-target headers, very quickly all hashing
power will come to consensus and all work to extend the same block,
either theirs or Alice's. Given that they have the majority, they will
find another block faster on average than Alice can extend her lead, and
thus eventually Alice will lose.
Now there is still a slight advantage for Alice in that it takes some
time for the whole network to come to consensus, but this is a much
slimmer margin, maybe a few percentage points, so at best Alice might
need, say, 45% of the total hashing power.

@_date: 2013-11-04 11:51:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Yes, they lose their work, but that's irrelevant: what's important is
eventually Alice runs out of secret blocks and then has no advantage
over the other miners.
In your paper Alice created her lead by exploiting the fact that not all
of the hashing power was working to extend the same block due to the
"first-wins" rule. With my solution that situation doesn't happen in the
first place: forks are resolved quickly because both sides have both
forks, and consensus on which one is the winner is achieved very quickly
by proving which side has the majority of hashing power through
near-target PoW solutions. With the majority of hashing power in
consensus and working to extend the same block there's nothing Alice can
do to get ahead, defeating the attack.

@_date: 2013-11-04 12:36:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Re-read my proposal - the whole point of it is to give a way to quickly
come to consensus about which side of the fork has the majority of
hashing power. It doesn't, and doesn't need to, reliable determine what
the hashing power actually is on either side. Rather it's a feedback
mechanism that creates a clear majority consensus in a short amount of
time with the use of only a small amount of bandwidth. (~5KB/10minutes)

@_date: 2013-11-04 13:16:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] Committing to extra block data/a better 
No sense in compromising - you need a whole merkle path to prove the
extra data is valid so you might as well make this a full 256 bits;
another 22 bytes is insignificant compared to the size of the path.
Again, the right way to do this is define the standard to use the last
txout so that midstate compression can be applied in the future. We can
re-use this for merge-mining and other commitments easily by defining a
simple standard based on defined path directions. Essentially for each
thing you might want to commit, perhaps a merge-mined coin, a p2pool
share, a UTXO commitment, whatever, generate a random 128-bit UUID.
Now interpret the bits of that UUID as an allowed path: 0 = left, 1 =
right, from the top of the tree. When you build the tree, make sure
everything that is going to be committed to uses it's allowed path; the
tree will look a bit jagged. If everyone picks their per-purpose UUIDs
randomly the paths won't collide for very many levels on average, and
path lengths will remain short. Validating that some given data was
committed properly is simple and easy: just check the path, and check
that the directions from the top of the tree followed the spec.
For timestamping, just pick any empty spot in the tree.
You'll want to put some "reasonable" limit on actual path lengths, just
pick something like 32 levels; if applications pick their UUIDs honestly
a collision will be very unlikely. You can also make the allowed paths
block specific by defining them as H(uuid | nonce), with nonce as an
optional PUSHDATA just prior to the commitment pushdata, allowing overly
long paths to be eliminated entirely by simply incrmenting the nonce.
Unlike the original, broken, merge-mining standard alt-coins have used
this actually works, extends indefinitely, and is simple and easy to
validate given a single merkle-path for each purpose. Generating the
trees of commitments is a bit convoluted, but at least that code only
needs to be written once.

@_date: 2013-11-04 13:32:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Committing to extra block data/a better 
This could be H(uuid | nLockTime) Coinbase transactions still have a
nLockTime, and while it's generally left at zero it can be any value
valid for a transaction in the block.

@_date: 2013-11-04 16:04:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Speaking of, I'm going to take back my solution as well; I misunderstood
your paper.
So here's your argument in a ELI5 nutshell:
Alice is a miner with some amount of hashing power. She has the ability
to detect new blocks on the network extremely effectively for whatever
reason; in short she has unusually good knowledge of the state of the
network. She is also very good at publishing her blocks and getting them
to the majority of hashing power in very little time; she has unusually
good connectivity to all miners. (low-latency and high bandwidth)
She's so good at this that when she finds a new block, she keeps it a
secret! She can get away with this because she knows that the moment Bob
finds a block, she can immediately broadcast it to the rest of the
network before the other block propagates. Instead of building on Bob's
blocks, almost everyone builds on Alice's block, depriving Bob of the
revenue. Gradually Alice gets more and more miners because Bob, and
other pools, don't pay out as much.
You propose a rule where essentially miners extend Bob's block 50% of
the time, and show in your paper how that leads to a scenario where
Alice needs to have at leastr 1/4 of the total hashing power to
succesfully pull this attack off anyway.
What I did succesfully show is that for a short-term rational miner
they're still better off mining to extend the block they hear about
first rather than using your pick-one-at-random rule, because when you
hear about a block is important information about whether or not the
majority is mining on it. This is true even if others are using the
pick-one-at-random rule. (they're better defecting than doing what's
right for the whole network) Even worse is that miners have a rational
incentive to broadcast such near-target headers to try to encourage
other miners to work on the same fork that they are working on. The
near-target idea came about for a totally different reason, so it's
something that might wind up being implemented anyway.
Mike Hearn's idea of making it easy to identify nodes associated with
hashing power is still wrong. Although again, it's something that miners
themselves have rational incentives to do. (you always want to encourage
others to send you their blocks, and you also want to be able to send
your blocks to the majority of hashing power as quickly as possible)
Where the idea goes wrong is it makes it easier for Alice to identify
hashing power, specifically where she needs to send her blocks to
distribute them to the majority as quickly as possible. The second
problem occurs if those nodes also distribute blocks to connecting
peers: this makes it easy for Alice to be sure she'll hear about a new
block as soon as possible by connecting to every one of those peers with
a high-speed, low-latency connection. Bizzarely the idea does work if
the advertised nodes only accept blocks, and never send blocks - instead
miners would *only* send their blocks to other miners who have proven
their hashing power, and do so essentially largest miner to smallest.
Now unless Alice already is a large miner, her strategy can't work.  Of
course this will strongly encourage further centralization of pools. But
it is in the interests of rational miners sadly.
That blocks take a finite amount of time to propagate makes the problem
worse: for Alice to learn that another block has been mined only
requires her to receive the small 80 byte header from a peer; she
doesn't need the whole block. She thus can know the block exists well
before it has a chance to propagate fully. Even if every miner were
directly peered to every other as some suggest, Alice could simply make
smaller blocks, faster propagating than everyone else and use especially
low-latency connections to win the race.
On the other hand, the Bitcoin protocol is currently designed such that
a miner can mine a block without knowing the previous block in full.
Given the large block reward and/or a supply of transactions they knew
no other miner had a rational miner would start trying to extend the
longest chain they know about prior to actually receiving and validating
the full block. Again, when miners start doing this - perhaps out of
desperation due to low revenue - as long as Alice has the lowest latency
network she'll win. (she doesn't even need to have the highest bandwidth
in this case) We can change the protocol to force miners to fully
validate blocks prior to mining extensions, but that only forces Alice
to get more bandwidth - she still wins.
Speaking of low-latency, latency not only centralizes control in a
single pool, it centralizes pools and even mining hardware itself in a
single physical location. Anyone at the edges of the propagation network
will get comparatively less revenue than those in the center, gradually
tightening the network, even without selfish mining. Alice's strategy of
course should be to position her nodes in the geographical center. It's
worth noting how if Alice is the one with the lowest average latency,
she will win against any other miner trying to persue the same selfish
miner strategy that she is using.
Finally nLockTime makes the selfish miner strategy even more profitable.
You may not be aware, but it's possible to make a transaction that can't
be mined until some time in the future, measured by either block height
or block timestamp. I've proposed to use this mechanism in
announce/commit sacrifices: you create a transaction that can't be mined
until some point in the future that sacrifices a large amount to mining
fees, and then prior to that point you include it in the blockchain as
data, proving the whole world knew about your transaction. The idea was
that which miner managed to include the transaction, and collect the
reward, would be random. However whenever Alice is able to maintain a
lead over other miners she's able to reliably mine significantly more of
those valuable transactions, further increasing her revenue over other
I must say, you've really opened a can of worms...

@_date: 2013-11-04 17:03:17
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Right, but as I said, I think this is likely to become a contest of who
can create the lowest latency mining operation, or to be more precise,
who can get the best ratio of latency per dollar.
Unfortunately even with totally "honest" mining winning orphan rates is
a function of latency; what this paper has done is mainly show a
remarkably effective way of leveraging low-latency and very good
visibility to the network.
Regardless, globe-spanning low-latency networks cost a lot of money, so
if they are something that makes mining more profitable, for whatever
reason, that's an effect that will incentivise pools to grow larger and
more centralized.
Yeah, there's a lot of possible solutions, but what I'm seeing looking
at them is they all tend to be not economically rational, in the short
term, or even worse, they actually incentivize mining pools to get
larger. For instance anything that tries to prevent Alice from sybiling
the network by forcing nodes to prove they have mining capacity just
means that larger miners will have an advantage over smaller ones in
getting their blocks propagated as fast as possible. Once Alice does
have a reasonable amount of mining capacity, she can still use the
selfish miner attack to grow larger and more profitable.

@_date: 2013-11-04 23:39:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Go away.
Mike specifically mentioned the idea of using Tor addresses, which are
authenticated. In addition this mechanism to create a backbone
*automatically* is exactly as decentralized as Bitcoin mining itself is.
It has nothing to do with Google.
I suggested the mechanism myself for slightly different reasons, and if
you know me, you'd know I'm the first to jump on anyone pushing

@_date: 2013-11-05 12:05:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal - patch to raise selfish 
Credit goes to Gregory Maxwell for pointing this out, but the random
choice solution does in fact introduce a vulnerability in that it
creates incentives for pools over a certain size to withhold blocks
rather than immediately broadcasting all blocks found.
The problem is that when the pool eventually choses to reveal the block
they mined, 50% of the hashing power switches, thus splitting the
network. Like the original attack this can be to their benefit. For
pools over a certain size this strategy is profitable even without
investing in a low-latency network; Maxwell or someone else can chime in
with the details for deriving that threshold.
I won't get a chance to for a few hours, but someone should do the
analysis on a deterministic switching scheme.

@_date: 2013-11-05 12:14:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal - patch to raise selfish 
Oh, and I don't want to give the wrong impression: there's no need to
rush to get this problem fixed. Even if someone wanted to launch an
attack right now, with a fair amount of resources, there's a lot of
counter-measures based on human intervention that can definitely stop
the attack in the short-term; what's needed is at worst moderate-term,
and much more likely a long-term approach. In addition, keep in mind
that this attack is very easy to detect, so if one is actually launched
we will know immediately and can start taking direct counter-measures at
that time.
That Gregory Maxwell so quickly identified a flaw in this proposed
solution suggests we should proceed carefully.
It'd be good to do a test of this attack, as well as possible solutions,
on testnet to better explore it and possible counter-measures.

@_date: 2013-11-05 14:56:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal - patch to raise selfish 
Not this exploit.
Here's a perfectly plausible worst-case scenario, that could be
happening right now: RAND High Frequency Trading Corp (a subsidiary of
General Evil) has a globe-spanning low-latency network of fiber,
line-of-sight microwave, and some experimental line-of-site neutrino
links criss-crossing the globe. They can get data to and from any point
on this planet faster than anyone else. Of course, in addition to their
spectacular network they have an immense amount of computing power, as
well as exotic overclocked liquid nitrogen bathed CPU's that run at
clockspeeds double what commercial hardware can do; in short, they have
access to scalar performance no-one else has. Of course, they like to
keep a healthy reserve so, 99% of all this fancy gear is constantly
idle. Whatever, they can afford it.
RAND just hired a bunch of fresh MIT graduates, the best of the best.
Problem is the best of the best tends to make not so best mistakes, so
RAND figures a Training Exercise is in order. Fortunately for them the NSA (a
subsidiary of General Evil) slipped a rootkit into my keyboard a week or
so ago - probably that time when I woke up in that farmers field with a
*splitting* headache - and are reading what I'm typing right now.
I go on to explain how an excellent training exercise for these fresh
MIT graduates would be to implement this nifty attack some Cornell
researchers came up with. It's really simple, elegant even, but to do it
best what you really want is the kind of low-latency network a
high-frequency-trading corporation would have. I then point out how a
good training exercise ideally is done in a scenario where there is
genuine risk and reward, but where the worst-case consequences are
manageable - new hires to tend to screw up. (I then go on to explain my
analog electronics background, and squeeze in some forced anecdote about
how I blew up something worth a lot of money owned by my employers at
some point in the distant past)
Unfortunately for the operators of BTC Guild, one of these new MIT grads
happens to have a: passed General Evil's psych screening with flying
colors, and b: have spent too much time around the MIT Kidnappng Club.
He decides it'd be easier to just kidnap the guy running BTC Guild than
fill out the paperwork to borrow RAND's FPGA cluster, so he does.
As expected the attack runs smoothly: with 30% of the hashing power,
neutrino burst generator/encoders's rigged around the globe to fire the
moment another pool gets a block, and the odd DoS attack for fun, they
quickly make a mockery of the Bitcoin network, reducing every other
miners profitability to zero in minutes. The other miners don't have a
hope: they're blocks have to travel the long way, along the surface of
the earth, while RAND's blocks shave off important milliseconds by
taking the direct route.
Of course, this doesn't go unnoticed, er, eventualy: 12 hours later the
operators of GHash.IO, Eligius, slush, Bitminter, Eclipse and ASICMiner
open their groggy eyes and mutter something about how that simulcast
Tuesday party really shouldn't have had an open bar... or so much coke.
They don't even notice that the team from BTC Guild has vanished, but
they do notice a YouTube video of Gavin right on bitcoin.org doing his
best Spock impression, er, I mean appealing for calm and promising that
Top Men are working on the issue of empty blocks as we speak. Meanwhile
CNN's top headline reads "IS THIS THE END OF BITCOIN?!?!"
It takes another hour for the Aspirin's to finally kick in, but
eventually get all get on IRC and start trying to resolve the issue -
seems that whenever any of them produce a block, somehow by incredible
coincidence someone else finds another block first. After a few rounds
of this they're getting suspicious. (if they weren't all so hung-over
they might have also found suspicious the fact that whenever they found
a block they saw a sudden blue flash - Cherenkov radiation emitted when
those neutrino's interacted with the vitreous humour in their eyeballs)
It's quickly realized that "somehow" BTC Guild isn't affected...
GHash.IO and Eligius, 22% and 13% of the hashing power respectively,
decide to try a little experiment: they peer to each other and only each
other through an encrypted tunnel and... hey, no more lucky blocks!
slush, 7% of the hashing power is invited to the peering group next,
followed by Bitminter, 6%, and Eclipse, 2%, and finally ASICMiner, 1%,
for a grand total of... 51% of the hashing power!
Of course, just creating blocks isn't useful for users, they need to be
distributed too, so someone quickly writes up a "one-way firewall" patch
that allows the group's blocks to propagate to the rest of the network.
Blocks created by anyone else are ignored.
It takes a few more hours, but eventually the attacker seems to run out
of blocks, and transaction processing returns to normal, albeit a little
slow. (20 min block average) Of course, soon there's a 3,000 post thread
on bitcointalk complaining about the "centralized pool cartel", but
somehow life goes on.
The next day Gavin goes on CNN, and gives a lovely interview about how
the past two days events show how the strength of the Bitcoin network is
in the community. For balance they interview this annoying "Peter Todd"
guy from "Keep Bitcoin Free!" who blathers on about how relying on
altruism or something will doom the Bitcoin network in the long run.
After the interview Gavin respectfully points out that maybe next time
they find a so called "developer" with a ratio of bitcointalk posts to
actual lines of code in the Bitcoin git repository better than one
hundred to one. The producer just wishes that "Mike Hearn" guy was
available; at least he's got a sense of fashion, sheesh!
Anyway, I'm out of space for my little story, but yeah, the ending
involves a group of now-rich pool operators who decide to start a large
financial services and data networking company, oh, and time-travel...
Quite seriously, your attack is a serious long-term risk, but in the
short term the social dynamics of Bitcoin are such that it's just not a
show-stopping risk. At worst some miners will lose a bunch of money -
that's something that's happened before with the March chain fork, and
sure enough Bitcoin survived just fine.
You can hide *who* is the attacker - you can't hide the fact that an
attack is happening if done on a meaningful scale.
That's not what we're concerned about - what we're concerned about is
that your BIP doesn't discuss the issue, and you didn't seem to be aware
of it. That suggests that the analysis is incomplete. There's no
pressing need to rush changes, as explained above by example, so we're
best off understanding the issue thoroughly first.
There's a whole spectrum of potential solutions that haven't been
discussed - I myself have two approaches I'm working on that may solve
this problem in ways you haven't (publicly) considered. I'm sure there
are many others out there.

@_date: 2013-11-06 22:44:04
@_author: Peter Todd 
@_subject: [Bitcoin-development] we can all relax now 
Speaking of, I hadn't gotten around to doing up the math behind that
strategy properly; turns out 51% I was overly optimistic and the actual
threshold is 29.3%
Suppose I find a block. I have Q hashing power, and the rest of the
network 1-Q. Should I tell the rest of the network, or withhold that
block and hope I find a second one?
Now in a purely inflation subsidy environment, where I don't care about
the other miners success, of course I should publish. However, if my
goals are to find *more* blocks than the other miners for whatever
reason, maybe because transaction fees matter or I'm trying to get
nLockTime'd announce/commit fee sacrifices, it gets more complicated.
There are three possible outcomes:
1) I find the next block, probability Q
2) They find the next block, probability 1-Q
2.1) I find the next block, probability Q, or (1-Q)*Q in total.
2.2) They find the next block, probability (1-Q)^2 in total.
Note how only in the last option do I lose. So how much hashing power do
I need before it is just as likely that the other miners will find two
blocks before I find either one block, or two blocks? Easy enough:
Q + (1-Q)*Q = (1-Q)^2 -> Q^2 - Q + 1/2 -> Q = (1 - \sqrt(2))/2
Q ~= 29.2%
So basically, if I'm trying to beat other miners, once I have >29.3% of
the hashing power I have no incentive to publish the blocks I mine!
But hang on, does it matter if I'm the one who actually has that hashing
power? What if I just make sure that only >29.3% of the hashing power
has that block? If my goal is to make sure that someone does useless
work, and/or they are working on a lower height block than me, then no,
I don't care, which means my original "send blocks to >51% of the
hashing power" analysis was actually wrong, and the strategy is even
more crazy: "send blocks to >29.3% of the hashing power" (!)
Lets suppose I know that I'm two blocks ahead:
1) I find the next block: Q                    (3:0)
2) They find the next block: (1-Q)             (2:1)
2.1) I find the next block: (1-Q)*Q            (3:1)
2.2) They find the next block: (1-Q)^2         (2:2)
2.2.1) I find the next block: (1-Q)^2 * Q      (3:2)
2.2.2) They find the next block: (1-Q)^3       (2:3)
At what hashing power should I release my blocks? So remember, I win
this round on outcomes 1, 2.1, 2.2.1 and they only win on 2.2.2:
Q + (1-Q)*Q + (1-Q)^2*Q = (1-Q)^3 -> Q = 1 - 2^-3
Q ~= 20.6%
Interesting... so as I get further ahead, or to be exact the group of
miners who have a given block gets further ahead, I need less hashing
power for my incentives to be to *not* publish the block I just found.
Conversely this means I should try to make my blocks propagate to less
of the hashing power, by whatever means necessary.
Now remember, none of the above strategy requires me to have a special
low-latency network or anything fancy. I don't even have to have a lot
of hashing power - the strategy still works if I'm, say, a 5% pool. It
just means I don't have the incentives people thought I did to propagate
my blocks widely.
The other nasty thing about this, is suppose I'm a miner and recently
got a block from another miner: should I forward that block, or not
bother? Well, it depends: if I have no idea how much of the hashing
power has that block, I should forward the block. But again, if my goal
is to be most likely to get the next block, I should only forward in
such a way that >30% of the hashing power has the block.
This means that if I have some information about what % already has that
block, I have less incentive to forward! For instance, suppose that
every major miner has been publishing their node addresses in their
blocks - I'll have a pretty good idea of who probably has that most
recent block, so I can easily make a well-optimized decision not to
forward. Similarly because the 30% hashing power figure is the
*integral* of time * hashes/second, if miners are forwarding
near-target-headers, I might as well wait a few seconds and see if I see
any near-target-headers; if I do for this block then I have evidence
that hashing power does have it, and I shouldn't forward.
So yeah, we're fucked and have got to fix this awful incentive structure
somehow before the inflation subsidy gets any smaller. Also, raising the
blocksize, especially by just removing the limit, is utter madness given
it can be used to slow down block propagation selectively, so the
hashing power that gets a given block is limited repeatably to the same
P.S: If any large pools want to try this stuff out, give me a shout. You
have my PGP key - confidentiality assured.
P.P.S: If you're mining on a pool with more than, like, 1% hashing
power, do the math on varience... Seriously, stop it and go mine on a
smaller pool, or better yet, p2pool.

@_date: 2013-11-06 23:33:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] we can all relax now 
Gamblers ruin has nothing to do with it.
At every point you want to evaluate the chance the other side will get
ahead, vs. cashing in by just publishing the blocks you have. (or some
of them) I didn't mention it in the analysis, but obviously you want to
keep track of how much the blocks you haven't published are worth to
you, and consider publishing some or all of your lead to the rest of the
network if you stand to lose more than you gain.
Right now it's a mostly theoretical attack because the inflation subsidy
is enormous and fees don't matter, but once fees do start to matter
things get a lot more complex. An extreme example is announce/commit
sacrifices to mining fees: if I'm at block n+1, the rest of the network
is at block n, and there's a 100BTC sacrifice at block n+2, I could
easily be in a situation where I have zero incentive to publish my block
to keep everyone else behind me, and just hope I find block n+2. If I
do, great! I'll immediately publish to lock-in my winnings and start
working on block n+3
Anyway, my covert suggestion that pools contact me was more to hopefully
strike fear into the people mining at a large pool and get them to
switch to a small one. :) If everyone mined solo or on p2pool none of
this stuff would matter much... but we can't force them too yet.

@_date: 2013-11-07 08:09:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] we can all relax now 
You don't understand how to read papers.
A good author will state his assumptions. For instance my third
paragraph read:
    Now in a purely inflation subsidy environment, where I don't care about
    the other miners success, of course I should publish. However, if my
    goals are to find *more* blocks than the other miners for whatever
    reason, maybe because transaction fees matter or I'm trying to get
    nLockTime'd announce/commit fee sacrifices, it gets more complicated.
Now that you understand the assumptions made, you can attack the paper
in one of two ways:
1) Show it's wrong.
2) Show its assumptions make it irrelevant.
You've done neither.

@_date: 2013-11-07 08:24:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] we can all relax now 
I hope they listen.
A few months ago ASICMiner could have made use of that attack if my
memories of their peak hashing power were correct. They certainely could
have used the selfish miner version, (we need better name for that)
although development costs would eat into profits.
GHash.IO, 22%, says they're a "private Bitfury ASIC mining pool" - dunno
what they mean by that, but they're involved with CEX.IO who has
physical control of a bunch of hashing power so I guess that means their
model is like ASICMiners. They're a bit short of 30%, but maybe some
behind-the-scenes deals would fix that, and/or lowering the barrier with
reactive block publishing. (a better name)
...and remember, if you only do the attack a little bit, you still can
earn more profit, and only drive up the orphan rate a little bit. So who
knows, maybe the orphans are real, or maybe they're an attack? ASICMiner
was involved with a bunch of orphans a while back...
You know what this calls for? A witchhunt!
BURN THE LARGE POOLS!
Glad to hear.

@_date: 2013-11-07 15:31:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] On the optimal block size and why 
Your definition of P_fork is inaccurate for a miner with non-negligable
hashing power - a miner will never fork themselves. Taking that into
account we have three outcomes:
1) The block propagates without any other miner finding a block.
2) During propagation another miner finds a block. (tie)
2.1) You win the tie by finding another block.
2.2) You lose the tie because someone else finds a block.
We will define t_prop as the time it takes for a block to propagate from
you to 100% of the hashing power, and as a simplifying assumption we
will assume that until t_prop has elapsed, 0% of the hashing power has
the block, and immedately after, 100% has the block. We will also define
t_int, the average interval between blocks. (600 seconds for Bitcoin)
Finally, we will define Q as the probability that you will find the next
The probabilities of the various outcomes:
1) 1 - (t_prop/t_int * (1-Q))
2) t_prop/t_int * (1-Q)
2.1) Q
2.2) 1-Q
Note that to simplify the equations we have not taking into account
propagation in our calculations for outcomes 2.1 or 2.2
Thus we can define P_fork taking into account Q:
P_fork(Q) = (t_prop/t_int * (1-Q))(1-Q) = t_pop/t_int * (1-Q)^2
Over the range 0 < Q < 0.5 the probability of a fork decreases
approximately linearly as your hashing power increases:
d/dq P_fork(Q) = 2(Q-1)
Q=0   -> d/dq P_fork(Q) = -2
Q=1/2 -> d/dq P_fork(Q) = -1
With our new, more accurate, P_fork(Q) function lets re-calculate the
break-even fee/KB using your original approach:
t_prop = t_0 + \alpha*S
E_fee = f*S
E(Q) = Q*(1 - P_fork(Q))*(E_bounty + E_fee)
E(Q) = Q*[1 - (t_0 + k*S)/t_int * (1-Q)^2]*(E_B + f*S)
d/dS E(Q) = Q*[ -2fSk/t_int*(1-Q)^2 - f*t_0/t_int*(1-Q)^2 + f - E_b*k/t_int*(1-Q)^2 ]
Again, we want to choose the fee so that the more transactions we
include the more we earn, dE/dS > 0 We find the minimum fee to include a
transaction at all by setting S=0, thus we get:
d/dS E(Q, S=0) = Q*[ f - f*t_0/t_int*(1-Q)^2 - E_b*k/t_int*(1-Q)^2 ] > 0
f(1 - t_0/t_int*(1-Q)^2) > E_b*k/t_int*(1-Q)^2
f > [E_b*k/t_int(1-Q)^2] / [1 - t_0/t_int*(1-Q)^2]
f > [E_b*k*(1-Q)^2] / [t_int - t_0*(1-Q)^2]
With Q=0:
f > E_b*k / (t_int - t_0) ~ E_b*k/t_int
This is the same result you derived. However lets look at Q != 0:
df/dQ = 2*E_b*k * [t_int*(q-1)] / [t_int - t_0(q-1)^2]^2
With negligible latency we get:
df/dQ, t_0=0 = 2*E_b*k*(q-1)/t_int
So what does that mean? Well in the region 0 < q < 1/2, df/dQ is always
negative. In other words, as you get more hashing power, the fee/KB you
can charge and still break even decreases linearly because you will
never orphan yourself. Lets trythe same assumptions as your first
analysis, based on the work by Decker et al
Based on the work by Decker et al, lets try to calculate break-even
fee/KB for negligible, 10%, 25% and 40% hashing power:
t_0 = 10s
t_int = 600s
k = 80ms/kB
E_b = 25BTC
Q=0    -> f = 0.0033 BTC/kB
Q=0.1  -> f = 0.0027 BTC/kB
Q=0.25 -> f = 0.0018 BTC/kB
Q=0.40 -> f = 0.0012 BTC/kB
Let's assume every miner is directly peered with every other miner, each
of those connections is 1MB/s, and somehow there's no latency at all:
k = 1mS/kB
Q=0    -> f = 0.000042 BTC/kB
Q=0.1  -> f = 0.000034 BTC/kB
Q=0.25 -> f = 0.000023 BTC/kB
Q=0.40 -> f = 0.000015 BTC/kB
Regardless of how you play around with the parameters, being a larger
miner has a significant advantage because you can charge lower fees for
your transactions and therefor earn more money. But it gets even more
ugly when you take into account that maybe a guy with 0.1% hashing power
can't afford the high bandwidth, low-latency, internet connection that
the larger pool has:
k = 10mS/kB, t_0=5s, Q=0.01 -> 0.000411 BTC/KB
k =  1mS/kB, t_0=1s, Q=0.15 -> 0.000030 BTC/KB
So the 1% pool has an internet connection capable of 100kB/s to each
peer, taking 5s to reach all the hashing power. The 15% pool can do
1MB/s to each peer, taking 1s to reach all the hashing power. This small
different means that the 1% pool needs to charge 13.7x more per KB for
their transactions to break even! It's a disaster for decentralization.
Businesses live and die on percentage points, let alone orders of
magnitude differences in cost, and I haven't even taken into account
second-order effects like the perverse incentives to publish your blocks
to only a minority of hashing power.(1)
This problem is inherent to the fundemental design of Bitcoin:
regardless of what the blocksize is, or how fast the network is, the
current Bitcoin consensus protocol rewards larger mining pools with
lower costs per KB to include transactions. It's a fundemental issue. An
unlimited blocksize will make the problem even worse by increasing fixed
costs, but keeping the blocksize at 1MB forever doesn't solve the
underlying problem either as the inflation subsidy becomes less
important and fees more important.
1)  at lists.sourceforge.net/msg03200.html

@_date: 2013-11-15 04:54:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Here's what I've got to date. The first two sections is just a
relatively simple proof that mining is more profitable as centralization
increases under any circumstance, even before any real-world factors are
taken into account. (other than non-zero latency and bandwidth) Nice
homework problem, and neat that you can easily get a solid proof, but
academic because it doesn't say anything about the magnitude of the
The latter part is the actual derivation with proper model of
supply-and-demand for fees. Or will be: while you can of course solve
the equations with mathematica or similar - getting a horrid mess - I'm
still trying to see if I can simplify them sanely in a way that's
step-by-step understandable. Taking me longer than I'd like; sobering to
realize how rusty I am. That said if any you do just throw it at
Mathematica, looks like you get a result where the slope of your
expected block return is at least quadratic with increasing hashing
power. (though I spent all of five minutes eyeballing that result)
\title{Expected Return}
\author{Peter Todd}
\section{Expected return of a block}
Let $f(L)$, a continuous function,\footnote{Transactions do of course give a
discontinuous $f$. For a large $L$ the approximation error is negligible.} be
the fee-per-byte available to a rational miner for the last transaction
included in a block of size $L$. $f(L)$ is a continuous function defined for $L
\ge 0$. Supply and demand dictates that:
    f(L) \ge f(L+\epsilon) \label{eq:f-increases}
A reasonable example for $f$ might be $f(L) = kL$, representing the demand side
of a linear supply and demand plot. For a block of size $L$ that is optimally
filled with transactions the value of those fees is just the integral:
    E_f(L) = \int_0^L f(l)\,dl
Let $P(Q,L)$, a continuous function, be the probability that a block of size
$L$ produced by a miner with relative hashing power $Q$ will be orphaned.
Because a miner will never orphan their own blocks the following holds true:
    P(Q,L) \le P(Q + \epsilon,L) \label{eq:p-increases}
Similarly because larger blocks take longer to propagate and thus risk getting
orphaned by another miner finding a block at the same time:
    P(Q,L) \ge P(Q,L + \epsilon)
By combining $P(Q, L)$, $E_f(L)$ and the inflation subsidy $B$, gives us the
expected return of a block for a given size and hashing power:\footnote{Note
how real world marginal costs can be accommodated easily in the definitions of
$f$ and $B$.}
    E(Q,L) = P(Q,L)[E_f(L) + B]
The optimal size is simply the size $L$ at which $E(Q, L)$ no longer increases:
    \frac{d}{dL}\big[E(Q, L(Q))\big] = 0
We will define the function $L(Q)$ as the optimal value for a given $Q$. A
miner creating optimal blocks will thus have an expected return per block found
of $E'(Q)=E(Q,L(Q))$. Note how this definition is per unit hashing power by
virtue of being per block found.
\section{Optimal return $E'$ vs. hashing power $Q$}
We want to know if a large miner has a larger return for a given amount of
hashing power. We do this by taking the derivative with respect to $Q$ of the
expected return given optimal strategy:
    \frac{d}{dQ}\big[E'(Q)\big] &= \frac{d}{dQ}\big[P(Q,L(Q))\big]\big[E_f(L(Q)) + B\big] + P(Q,L(Q))\frac{d}{dQ}\big[E_f(L(Q))\big] \\
                                &= \frac{dL(Q)}{dQ}\Big[\frac{dP(Q,L(Q))}{dQ}\big[E_f(L(Q)) + B\big] + P(Q,L)\frac{dE_f(L(Q))}{dQ}\Big]
We know that $L(Q)$, $E_f$, $P$, and $B$ are all $\ge 0$. Thus for $dE'/dQ$ to
be negative requires either $dL/dQ$ to be negative, or for $dL/dQ$ to be
positive and one of $dP/dQ$ or $dE_f/dQ$ negative.
Suppose $dP/dQ$ negative and $dL/dQ$ positive:
    \frac{dL(Q)}{dQ} > 0    &\implies L(Q + \epsilon) > L(Q) \notag \\
    \frac{dP(L(Q))}{dQ} < 0 &\implies P(Q + \epsilon, L(Q + \epsilon)) < P(Q, L(Q)) \label{eq:dl-pos-dp-neg}
But that contradicts our definition \eqref{eq:p-increases} of $P$ as continuous
and increasing. Suppose instead that $dE_f/dQ$ is negative and $dL/dQ$
    \frac{dL(Q)}{dQ} > 0      &\implies L(Q) < L(Q + \epsilon) \notag \\
    \frac{dE_f(L(Q))}{dL} < 0 &\implies E_f(L(Q)) > E_f(L(Q + \epsilon)) \notag \\
                              &\implies \int_0^{L(Q)} f(l)\,dl > \int_0^{L(Q+\epsilon)} f(l)\,dl \notag \\
                              &\implies f(l) < 0 \label{eq:dl-pos-de-neg}
Again we have a contradiction with our definition \eqref{eq:f-increases} of
$f$. Finally suppose $dL/dQ$ is negative:
    \frac{dL(Q)}{dQ} < 0 &\implies L(Q) > L(Q + \epsilon) \notag \\
                         &\implies P(Q + \epsilon, L(Q + \epsilon)) < P(Q, L(Q)) \notag \\
                         &\implies \frac{dP(Q, L(Q))}{dQ} < 0 \notag \\
                         &\implies \frac{dL(Q)}{dQ}\frac{dP(Q, L(Q))}{dQ} > 0 \label{eq:dl-neg-dp-neg} \\
                         &\implies E_f(L(Q + \epsilon)) < E_f(L(Q)) \implies \frac{dE_f(L(Q))}{dQ} < 0 \notag \\
                         &\implies \frac{dL(Q)}{dQ}\frac{dE_f(L(Q))}{dQ} > 0 \label{eq:dl-neg-de-neg}
Even if $dL/dQ$ is negative \eqref{eq:dl-neg-dp-neg} and
\eqref{eq:dl-neg-de-neg} show that $dE'/dQ > 0$. In conjunction with
\eqref{eq:dl-pos-dp-neg} and \eqref{eq:dl-pos-de-neg} we prove that increased
hashing power always leads to increased return on investment per unit hashing
\subsection{Real-world implications to centralization}
While the author has shown that they still remember first-year, is this result
The proof holds regardless of what any of the functions actually are, provided
that they meet the requirements set out in section
\ref{sec:exp-return-of-a-block}. The requirements are met by any reasonable
real-world scenario\footnote{Negative fees are not reasonable!}, and show an
incentive for mining to centralize even in an ideal situation where all miners
are on a level playing field and have no fixed costs.
However the proof is abstract, and doesn't tell us anything about how strong
that pressure is; it may be insignificant enough to be outweighed by effects
such as social pressure.
We need to investigate $dE'/dQ$ in detail.
\section{Detailed derivation of of $P(Q,L)$}
The difficulty is assumed to be in a steady state condition and the
percentage of hashing power for any given miner is fixed. Unconfirmed
transactions are assumed to be known to all miners, giving everyone an
equal opportunity of mining any given transaction.
We assume that the graph of all Bitcoin miners is fully connected and
that the bandwidth, $1/k$, and latency, $t_0$, is identical for all
connections and unchanging. We assume that miners always attempt to
build upon the first block they see on the longest chain known to them,
and when they find a block, they always broadcast it to all other miners
simultaneously. From that we see that the time taken for a block of size
$L$ to propagate to $100\%$ of the hashing power is simply:
    t(L) = t_0 + kL
When miner $Q$ finds a block during the condition of full consensus the
outcomes can be described by the following state tree.  The numbers in brackets
are the "scorecard" of blocks found by $Q$ and all other miners should a given
state be reached:
    \item[1)] No other block is found prior to full propagation. (1:0)
    \item[2)] $Q$ finds another block prior to full propagation. (2:0)
    \begin{description}
        \item[2.1)] $Q$'s second block is not orphaned. (2:0)
        \item[2.2)] $Q$'s second block is orphaned. (2:3)
    \end{description}
    \item[3)] $(1-Q)$ finds another block prior to full propagation. (1:1)
    \begin{description}
        \item[3.1)] $(1-Q)$'s block is orphaned. (2:1)
        \item[3.2)] $(1-Q)$'s block is not orphaned. (1:2)
    \end{description}
Miner $Q$ wins if states $1$, $2.1$, or $3.1$ are reached. Though it is
possible to derive an equation for $P$ that accurately models possible states -
the author did exactly that in a fit of madness - the resulting equation is
unwieldly and offers no additional insight.
We want to end up with a $dE'/dQ$ that captures second order effects. Since
$L(Q)$ and thus $E'(Q)$ will depend on $Q$ our approximation of $P$ should be
such that $dP/dQ$ is at least linear.
With $\lambda$ as the block interval the probabilities of reaching states $1$,
$2$, and $3$ are as follows:
    p_1 &= 1 - \frac{t}{\lambda} \\
    p_2 &= \frac{t}{\lambda} Q \\
    p_3 &= \frac{t}{\lambda} (1-Q)
We could assume that states $2$ and $3$ both lead to the block being orphaned,
thus giving us:
    P(Q, L) = 1 - \frac{t}{\lambda} = 1 - \frac{t_o + kL}{\lambda}
However this gives us a linear $E(Q, L)$, linear $L(Q)$, and thus only a
quadratic $E'(Q)$. We need at least one more state in our model; state $2.1$ is
a good choice. Reaching state $2.2$ is exceptionally improbable - the miners
$(1-Q)$ have to find three blocks in time $t$ - so ignoring state $2.2$ and
thus using the probability for state $2$ instead has negligible impact on the
model. Meanwhile state $3$ requires that state $3.1$ be used directly and would
result in a third-order terms in $P$ when treating state $3$ as an always loss
is a conservative lower-bound.
This gives us:
    P(Q, L) &= p_1 + p_2 = 1 - \frac{t}{\lambda} + \frac{t}{\lambda} Q = 1 - (1-Q)\frac{t}{\lambda} \notag \\
            &= 1 - (1-Q)\frac{t_o + kL}{\lambda}
\subsection{Detailed derivation of E'(Q)}
Some preliminaries:
    \frac{dP(Q,L)}{dL} &= -(1-Q)\frac{k}{\lambda} \\
    \notag\\
    \frac{dE(Q,L)}{dL} &= \frac{dP(Q,L)}{dL}\big[E_f(L) + B\big] + P(Q,L)\frac{dE_f(L)}{dL} \notag\\
                       &= \frac{dP(Q,L)}{dL}\big[E_f(L) + B\big] + P(Q,L)\,f(L)
We're not going to get very far without a definition for $f$ so we'll use a
simple linear demand model:
    f(L) &= a - bL \\
    E_f(L) &= aL - \frac{1}{2}bL^2
Now we set $dE/dL=0$ and solve for $L$. To simplify the problem we will consider the no-subsidy, $B=0$ case:
    0 &= \frac{dP(Q,L)}{dL}E_f(L) + P(Q,L)\,f(L) \\
      &= -(1-Q)\frac{k}{\lambda}\big[aL - \frac{1}{2}bL^2] + \big[1 - (1-Q)\frac{t_o + kL}{\lambda}\big](a - bL) \\

@_date: 2013-11-15 05:32:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Assuming t_0 is negligible is wrong in this case. Or, it should be...
So alpha has units of seconds/byte, which lets us indirectly figure out
the bandwidth the blocks are propagating at assuming t_0=0 and all links
are equal. When you realize that P_fork is basically a multiplier on the
bandwidth required to get a block out fast enough, the derivation makes
sense. In any case we get:
alpha = (1/113)*600s/134kBytes = 39.62uS/byte = 24kB/second
Which is atrocious... but when you remember that Bitcoin nodes send
blocks to all peers simultaneously,(1) thus dividing up the bandwidth and
ruining latency you see why. t_0 shouldn't be at all negligible due to
speed of light, but with this low bandwidth it is anyway.
1) To be precise, nodes answer queries for blocks from all peers
This also indicates that pools haven't taken the simple step of peering
with each other using high-bandwidth nodes with restricted numbers of
peers, which shows you how little attention they are paying to
optimizing profits.  Right now mining pulls in $1.8 million/day, so
that's up to $16k wasted.
However, because miners don't orphan themselves, that $16k loss is born
disproportionately by smaller miners... which also means the 24kB/sec
bandwidth estimate is wrong, and the real number is even worse. In
theory anyway, could just as easily be the case that larger pools have
screwed up relaying still such that p2pool's forwarding wins.

@_date: 2013-11-15 05:46:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
How did you get those numbers exactly?
Also fee per txn is *not* useful and we really shouldn't quote it so
that newbies reading this stuff get the right understanding.

@_date: 2013-11-15 05:52:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] On the optimal block size and why 
Well if large pools wanted it would be trivial for all of them to just
connect to each other... but my 25kB/s average data rate sure indicates
that they either aren't bothering, or aren't bothering to do that
Which is an awful solution, although probably a correct one.... After
all, if you don't include transactions, you can start mining blocks
earlier too based on just the header.
That's a fundemental misunderstanding; there's no such thing as a min
As for economies of scale, the "product" we're paying miners for is
decentralization and resistance to 51% attack. If instead only get 51%
attack resistance, we're getting a bum deal. If that's all we're
getting, we don't actually have 51% resistance...

@_date: 2013-11-15 05:58:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] we can all relax now 
Remember how I started off by asking what was the correct strategy if a
miner wanted to get more blocks than their *competition*, not more
blocks in total. In some scenarios that strategy is the one that
maximizes returns, such as the case when you make your returns from
transaction fees, especially without a blocksize limit restricting how
many fee paying transactions you can stuff in your blocks. It's not
correct to say the cabal is trying to maximize immediate revenue.
As for the length of those secret chains, at every step you of course
want to weigh the value of the blocks you have found against the risk
that someone else catches up, and when it makes sense, publish some or

@_date: 2013-11-15 06:12:04
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Unfortunately the math doesn't work that way. For any Q, a bigger Q
gives you a higher return. Remember that the way I setup those equations
in section 3.2 is such that I'm actually modeling two pools, one with Q
hashing power and one with (1-Q) hashing power. Or maybe more
accurately, it's irrelevant if the (1-Q) hashing power is or isn't a
unified pool.
The other thing is the fraction of the block fee the pool reserves
indicates you're talking about real-world costs... and the moment you do
that you find that pools themselves have economies of scale simply by
virtue of using a small overhead infrastructure, their nodes etc., for a
large number of miners. On that basis alone a small miner joining a
larger pool would always be financially advantageous modulo situations
where the large pool had legal restrictions that artificially increased
their overheads.
Bitcoin rate?
The equations give an incentive to centralize all the way up to 1 miner
with 100% hashing power.
Of course, if that one pool were p2pool, that might be ok!
By defining f(L) you can model supply and demand, which can be relevant
in that a steep demand curve with a small number of high-fee
transactions can reduce centralization pressure in my model.
Of course, by defining f(L) = a-bL you also wind up with mathematica
spitting out some truly hideous polynomials. :P Setting f(L) = c as you
suggest is something I looked at, and results in equations that are more
reasonable, so I think I'll likely wind up doing that. You can make a
good argument anyway that the centralization would cause a flattening of
any demand curve anyway, as in the no-blocksize-limit case the larger
pools cost per transaction tends towards zero as their hashing power
increases - why pay high fees when the large pool will mine them almost
as fast?

@_date: 2013-11-15 14:09:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
The underlying issue is what is the pools expenses compared to yours.
There is an overhead to mining, you need to spend money and time (and
hence money) running and administering full nodes at the very minimum.
The pool can amortise that cost over many hashers; the solo miner can't.
Pools will of course have some profit margin, but why would you expect
that margin to not be sufficiently low to make it in a solo-miner's
interest to join the pool? Both the pool and the former solo-miner earn
more return after all if they centralize.
The fundemental issue is that in the design of Bitcoin there is an
incentive for miners to join into pools, and that incentive exists at
any amount of hashing power. Sure second order effects like regulation
and social pressure can counteract that incentive in some circumstances,
but that's not very strong protection.
However p2pool doesn't necessarily need a linear blockchain to function,
so there is a potential for stales to be much less relevant.

@_date: 2013-11-15 14:19:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Huh? Where did 454000 come from?
That's only true for a PPS pool though, not the more usual pools that
pay relative to blocks actually found. Heh, actually, that might be part
of the problem... also doesn't help how varience is going to make
noticing 1% hard.
P2Pool has 1% hashing power right now; I mine on it myself with what
little hashing power I have.
The more interesting thing is how do you grow P2Pool - requiring a full
node is going to make that tricky. Also the once we start adding more
efficient block propagation by transmitting headers + txids p2pool's
current advantage goes away.

@_date: 2013-11-15 17:06:48
@_author: Peter Todd 
@_subject: [Bitcoin-development] Committing to extra block data/a better 
You don't need level compression if you adopt my per-block randomization
idea. I think we'd be better off keeping the proofs as simple as
possible, just dumb merkle paths.
I mentioned UUID more in spirit than in terms of the official UUID
standard; any large randomly picked integer is fine.
Wouldn't hurt to run the idea past forrestv, given p2pool will be
affected as it'd need to adopt the standard. He's run into some oddness
with mining hardware and nonces that would be good to understand. (note
how p2pool blocks don't commit to a fully random hash - there's some
extra bytes in there due to stratum or something IIRC)

@_date: 2013-11-19 06:00:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] Disentangling Crypto-Coin Mining: 
In the design of Bitcoin mining serves two fundemental purposes:
proof-of-publication and order consensus.  Bitcoin's design entangles
these fundemental purposes with other goals, such as validation and
initial coin distribution. This leads to a design that is fundementally
unscalable, albeit effective on a small scale. Here we show how these
purposes do not need to be entangled together, and how by disentangling
them we can achieve better scalability and validation of the system as a
Let's first look at what role each of those purposes plays:
* Proof-of-publication
The fundemental problem Bitcoin solves is the double-spend problem.
Alice has some Bitcoins, and she wants to give them to Bob. She does
this by signing a digital message, a transaction, authorizing her coins
to be assigned to Bob. However, Bob has no way of knowing if Alice has
signed a conflicting digital message assigning her coins to Charlie
Bitcoin solves this problem by providing a way for Alice and Bob to
agree on a common place where *all* transactions will be published, the
blockchain. Because the definition of a valid transaction is that it has
been published in the blockchain, Bob can examine the contents of it,
and be confident that no conflicting transaction exists.
* Order consensus
Due to the constraints of physics no decentralized system can provide
instantaneous and reliable proof of publication; for a non-ideal
proof-of-publication system to be useful to solve the double-spend
problem we need to come to a consensus about the order in which data was
published. Once an order has been established, subsequent
double-spending transactions can be declared invalid.
Note that time itself isn't directly required, only the order of
transactions needs to be agreed upon.
* Why validation is an optional optimization
Given only proof-of-publication, and a consensus on the order of
transactions, can we make a succesful crypto-coin system? Surprisingly,
the answere is yes!
Suppose the rules of Bitcoin allowed blocks to contain invalid
transactions, in fact, suppose miners did no verification what-so-ever
of the contents of the blocks they mined. Could Bob still be confident
in the coins he received? Absolutely. There is consensus that the
transaction sending coins to Bob's came first and all prior transactions
can be verified as valid by checking the entire blockchain. In Bitcoin
all full nodes do this and Bitcoin could succesfully operate on that
What can't be supported in this model is SPV clients: the existance of a
transaction in a block tells you nothing about its validity, so no
compact proof can be made.
Real-world examples of this issue can be found in the parasitic
consensus system Mastercoin, and to a lesser extent Colored Coins: the
former uses Bitcoin as a proof-of-publication, applying it's own
independent set of rules to that published data. The latter tracks the
transfer of assets in a way that takes advantage of the Bitcoin
validation rules, but any given txout can only be proven to represent a
particular asset with a full chain of transfers back to the asset
genesis. It's notable that proponents of colored coins have proposed
that rules to validate colored coins be added to Bitcoin to make such
lengthy proofs not required.(1)
* What is the minimum domain for anti-double-spend proof-of-publication?
Answer: a single txout.
So what do we mean by "domain" here? In the existing Bitcoin system,
modulo validation, what Alice has proven to Bob is that an entire
transaction has been published. But that's not actually what Bob wants
to know: he only wants to be sure that no transaction inputs, that is
the CTxIn data structure containing a valid scriptSig and reference to a
previous output, have been published that spend outputs of the
transaction he is accepting from Alice. Put more simply, he doesn't care
where a double-spending transaction sends the money, he only cares that
it exists at all.
Suppose the blockchain consisted of blocks that only contained
information on the transaction outputs spent by that block; essentially
a block is a list of CTxIn's. We also, add a third field to the existing
CTxIn structure, hashTx, which commits to the rest of the transaction
spending that txout.
If we sort the CTxIn's in each block by the hash of the *transaction
output being spent* and commit to them with a merkle tree, Bob can now
determine if Alice's transaction is valid by checking the blockchain for
blocks that contain a conflicting spend of any of the inputs to that
transaction. For each block the proof that the block does not contain a
given spend is log2(n) in size.
Put another way, Bob needs proof that some data, a valid CTxIn spending
some CTxOut, has never been published before. He only cares about that
particular CTxOut, so the "publication domain" he is interested in is
that single CTxOut. (note that we are considering a CTxIn as valid if
its scriptSig satisfies the prevout's scriptPubKey; the rest of the
transaction may be invalid for other reasons)
Conversely a transaction is only considered to be valid if all CTxIn's
in that transaction have been succesfully committed to the blockchain
proper; there must be proof that every CTxIn has been published.
Note the parallels to the authors TXO commitments proposal: where TXO
commitments commit to the outputs of every transaction in each block,
here we are committing to the inputs of all transactions.
* Transaction validation
Miners still are doing almost no validation in this scheme, other than
the fact that a block is only valid if the data in it follows some
order. Bob still needs to examine the chain of of all transactions to
determine if Alice's payment was valid. However, the information he
needs to do this is greatly diminished: log(n) * m per txout in that
history, with n as the average number of spends in a block, and m the
number of blocks each txout was in existance for.
Of course, a practical implementation of this concept will have to rely
heavily on direct transfer of proof data from payor to payee.
** Privacy
The increased validation effort required on the part of Bob has an
important privacy advantage: whole transactions need never appear in the
blockchain at all. By incorporating a simple nonce into every
transaction blinding the miners have no way of linking CTxIn's to
CTxOut's. This achieves the end goal of Adam Back's blind symmetric
commitments(3) but by leaving data out of the blockchain entirely rather
than blinding it.
* The incentive to share blockchain data
What is the incentive for miners have in the Bitcoin system to share
their blocks? Why not just share the block header? Of course, the
incentive is that unless they share their block data, all other miners
in the system won't build upon their blocks because they have no idea if
they are valid or not.
But here there is no such thing as an invalid block! Blocks are just
arbitrary data with no specific meaning; whether or not the data is
valid in some sense is of no importance to the miner.
We can re-introduce this incentive by using a proof-of-work scheme that
has the requirement of posession of blockchain data. For instance we
could make the underlying computation be simply H(header + all previous
blocks) - without the entire blockchain you would be unable to mine, or
even validate the work done.
Of course this is impractical for a number of reasons. But it's
important to recognize that this simple scheme doesn't make any
compromises about the continual availability of blockchain data, and
thus the ability for users to validate history. Any lesser scheme will
be a trade-off between that guarantee and other objectives.
** Full TxIn set commitments
Since we have to require miners to posess blockchain data, we might as
well make a simple optimization: rather than commit to the CTxIn's in a
single block, commit to multiple blocks.
First, let's require that every CTxIn present in a block be have a valid
scriptSig for the corresponding scriptPubKey. To do this we need for
CTxIn's to commit to the H(txout) they are spending, and include the
CTxOut itself alongside the CTxIn in the block. Our hash commitments are
now chained as follows:
    CTxIn -> CTxOut ->  -> CTransaction ->  -> CTxIn
Now that we have valid and invalid CTxIn's, we might as well state that
only one valid CTxIn is allowed for a given CTxOut per block; proof that
a transaction is valid now doesn't have to take into account the problem
of an *invalid* CTxIn that you need to prove is invalid and thus can be
ignored. This validation is stateless, requiring only local data, and
still provides for strong privacy.(a) A fraud proof in this scheme is
simply the CTxIn and CTxOut and merkle path, and the code required to
evaluate it is the same code required to evaluate the data in a block.
a) Remember the mention of a per transaction nonce? It can be used
   between the CTxOut and the rest of the CTransaction so that even if
   every CTxIn and CTxOut is known, the actual transactions can't be
   derived.
Now that we have a definition of a valid CTxIn, we can naturally extend
this to define the set of all valid *oldest* CTxIn's. That is for any
given CTxOut, we include the first valid CTxIn found in any block in
this set. This is analogous to the concept of the UTXO set, except that
items can only ever be added to the TxIn set.
As with UTXO commitments we can commit to the state of the TxIn set
using a merkelized radix tree whose tip is committed to by the block
Of course because a block can manipulate the contents of this set in an
invalid way, we've strongly reintroduced the notion of an invalid block,
we've re-introduced the incentive to share blockchain data, and we've
re-introduced the requirement to have the full set of blockchain data to
*** Mining with incomplete blockchain data
Or have we? This requirement isn't particularly strong as all: if other
miners are usually honest we'll get away with just trusting them to mine
only valid blocks. Meanwhile the TxIn set in merkelized radix tree form
can have items added to it with only the subset of internal nodes
modified by your additions. A miner can easily produce blocks only
containing CTxIn's spending CTxOuts from a subset of the possible
values. Multiple such miners can even co-operate to produce blocks, with
each handling a specific subset, as multiple radix trees are easily
Note that Bitcoin is even worse in this regard: you don't need any
previous blockchain data at all to create a new block. For instance the
authors proof-of-tx-propagation concept(5) has the serious flaw that
unscrupulous miners can use the proof that other miners are mining
certain transactions as a way to avoid doing any validation themselves.
*** The deletion problem
What happens if a copy of some of the txin set can't be found? With
Bitcoin this isn't an issue in theory - the miners are supposed to never
extend blocks they haven't verified in full and they are supposed to
distribute blocks freely. Not necessarily a perfect assumption(6) but it
mostly holds true.
With any type of sharded blockchain, it is easy to see that assumption
may not hold true. Now rather than a 51% attack in terms of total
hashing power, you could have a "local" attack on some portion of the
commitment set. On the other hand, with the right set of incentives, the
existance of such an attack can be made to imply actual consent by those
owning the coins involved, e.g. through proof-of-stake combined with the
proof-of-work. (perhaps better described as proof-of-consent with
1) OP_CHECKCOLORVERIFY: soft-fork for native color coin support,
      jl2012
2) Merkle tree of open transactions for lite mode?
      Gregory Maxwell
3) Ultimate blockchain compression w/ trust-free lite nodes
      Alan C. Reiner
4) blind symmetric commitment for stronger byzantine voting resilience,
    at lists.sourceforge.net/msg02184.html,
   Adam Back
5) Near-block broadcasts for proof of tx propagation,
    at lists.sourceforge.net/msg02868.html,
   Peter Todd
6) Perverse incentives to withhold blocks
    at lists.sourceforge.net/msg03200.html
   Peter Todd

@_date: 2013-11-19 12:06:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
I already did that:
GitHub can render MediaWiki just fine, so I think leaving the BIPs as
MediaWiki is the way to go. New BIPs may want to use either markdown or
MediaWiki - the latter has advantages in terms of formatting
capabilities over the former, particularly when math needs to be

@_date: 2013-11-20 05:01:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Oh right, you're using the actual block interval, not the steady state

@_date: 2013-11-24 12:13:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Network propagation speeds 
Do you have the resources to save the raw log data? You'll also need to
save transaction timestamp data - whether or not a given node has a
transaction already matters re: propagation.
Of course given pool centralization the moment pools start peering
directly with each other all these stats might not mean all that much.
Note that the number that's important isn't seconds, rather rather
seconds/actual block interval as long as hashing power is growing.
Unfortunately actually determining that is tricky - block interval is
inherently noisy so you'll want to use a fairly agressively smoothed
So here's a rough calculation: right now blocks are happening roughly
%15 faster than they would at equilibrium, and blockchain.info reports
about 2 orphans a day. 2/166=1.2% orphan rate.
Now with a simplistic model where it takes exactly t seconds for a block
to propagate to 100% of the hashing power, and until then 0% has it,
you'd get:
    orphan rate = t / actual block interval -> t = rate * interval
Or 6.2 seconds with our orphan rate data. Now whether or not
blockchain.info succesfully captures all orphans I don't know, but given
you're reporting 4.5 to 9.4 seconds for 50th and 75th percentile
respectively that number 6.2s seems "ballpark" reasonable - remember
that hashing power is definitely not distributed evenly among the nodes
you are sampling from.
Which is another point... it may be the case that your propagation data
doesn't actually give any insight into real-world orphan rates because
the distribution of hashing power is concentrated into pools.

@_date: 2013-11-27 10:24:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proof-of-storage txouts 
So Sarchar and I were talking about his Bitstorage scheme(1) and we came
to the conclusion that it wouldn't work. However he came up with a less
abitious idea that I thought would work: force people to prove they were
still holding your data D by publishing transactions with scriptPubKeys
of the form:
    HASH160 H(D[i:i+n]) EQUALVERIFY { OP_CHECKSIG}
Where pubkey optionally lets you pick a specific person to hold your
data. (so the scheme isn't restricted to miners - hash-only
scriptPubKeys aren't secure) Basically you'd publish the data and store
a much smaller random set of D[] samples. If you ever needed the data in
full, you know it's out there, so it's just a matter of haggling on the
price to get it back. (you may want to do some dry-runs for negotiation
However, I realized you can improve upon this greatly by deriving the
ECC privkeys from the random samples of data instead using H(E_k(D)),
that is, use a block cipher with key k, and then hash that to form the
privkey. Then create a perfectly normal txout paying to the appropriate
pubkey. Now only people who actually have the data can claim the txout,
and everyone doesn't even know the scheme exists at all.
Furthermore you can create key k using k_i=HMAC(i, K), where i in [0,
n], so rewards for the proof can be released incrementally while only
storing a single secret key. Again, actual retrivial isn't necessarily
guaranteed, but the odd dry-run is simple enough.
One last issue is how to distribute k_i, although this is made easier by
the fact that they can be tiny 128-bit numbers - they should however be
signed to avoid DoS attacks as only by processing all the data can the
storage node know if k_i works for the given txout.
1)

@_date: 2013-10-04 07:35:17
@_author: Peter Todd 
@_subject: [Bitcoin-development] Code review 
Git is a revision *communication* system that happens to also make for a
good revision *control* system.
Remember that every individual commit is two things: what source code
has changed, and a message explaining why you thought that change should
be made. Commits aren't valuable in of themselves, they're valuable
because they serve to explain to the other people you are working with
why you thought a change should be made. Sometimes it makes sense to
explain your changes in 10 commits, sometimes it makes sense to squash
them all up into one commit, but there's no hard and fast rule other
than "Put yourself in your fellow coders' shoes - what's the best way to
explain to them what you are trying to accomplish and why?" You may have
generated a lot of little commits in the process of creating your patch
that tell a story that no-one else cares about, or equally by squashing
everything into one big commit you wind up with a tonne of changes with
little explanation as to why they were made.
Two caveats apply however: git-bisect works best if every commit in the
tree you are trying to debug works well enough that you can run tests
without errors - that is you don't "break the build". Don't make commits
that don't compile at the very least, and preferably everything you do
should be refactored to the point where the commit as a whole "works".
The second caveat is more specific to Bitcoin: people tend to rebase
their pull-requests over and over again until they are accepted, but
that also means that code review done earlier doesn't apply to the later
code pushed. Bitcoin is a particularly high profile, and high profit,
target for people trying to get malicious code into the codebase. It may
be the case that we would be better off asking reviewers making small
changes to their pull-requests to add additional commits for those
changes rather than rebasing, to make it clear what changes were
actually made so that code reviewers don't have to review the whole
patch from scratch. After all, the place where the most eyes will
actually look at the commits is during the pull-req process; after the
code has been pulled the audience for those commits is in most cases
almost no-one.
Having said that, there's currently a lot of other holes in the review
and source code integrity process, so fixing this problem is probably
not the low-hanging fruit right now.
FWIW personally I tend to review patches by both looking at the
individual commits to try to understand why someone wanted to make a
change, as well as all commits merged into one diff for a "what actually
changed here?" review.

@_date: 2013-10-04 07:53:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Code review 
When I'm reviewing multiple commit pull-requests and want to see every
change made, I always either click on the "Files Changed" tab on github,
which collapses every commit into a single diff, or do the equivalent
with git log.
Why doesn't that work for you?
One advantage of using github is that they're an independent third
party; we should think carefully about the risks of furthering the
impression that Bitcoin development is a closed process by moving the
code review it to a server that we control with explicit review groups.
Given that Review Board appears to remain cryptographically unverifiable
there may also be disadvantages in operating it ourselves in that if the
review server does get compromised we *don't* have a third-party to
blame. In addition GitHub is a third-party with a very valuable
reputation to uphold and full-time staff - they're doing a better job of
keeping their servers secure and running then we ever could.

@_date: 2013-10-04 08:14:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] Code review 
Ideally code review discussions would be PGP signed and have a mechanism
for someone to sign a commit saying they had in fact reviewed it.
Combined with git's per-commit signature mechanism it'd make it possible
to write a git-pull hook that checked that whatever was being pulled had
some sufficient number of signatures from people whose reviews you
trusted. With such a system you could host code review anywhere safely,
or for that matter, use a completely distributed system.
But that's going to be a long way off. In the meantime github is
probably more trustworthy and competent than anything we ran ourselves,
and we should focus on making sure reviewers eyeballs actually look at
the code that ends up in master.

@_date: 2013-10-19 19:57:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] A critique of bitcoin open 
Having it on the BIP page *does* make it more official, at least the way
we've been using the BIP page, which is to filter out the proposals that
haven't gotten much support at all. (or maybe are just controversial)
FWIW I myself haven't pushed hard for getting an "official" BIP number
for my draft NODE_BLOOM BIP, even though I've got support from most of
the dev team on the pull-request:
 I'm probably at the point
where I could get one assigned - Litecoin for instance has made that
change - but really I just see that as a formality; that it's still a
controversial idea is much more relevant.
In any case I don't see any working code in your email, I'd suggest
writing some. You're BIP would be much more likely to be accepted if you
were more involved in wallet development.

@_date: 2013-10-20 18:43:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] A critique of bitcoin open 
No, that just means the authors of BIP 38 know community acceptance is
the most important thing; BIP numbers are secondary.
FWIW I think that BIP's should have been done as a github repository,
allowing for dealing with this stuff transparently as a pull-request.
It'd also be useful to handle BIP's that way to make it easy to archive
them, update them, and keep a log of what and why they were updated.
Just put them in markdown format, which is pretty much feature
equivalent to the wiki now that markdown supports images.
No, just put the client up on github. If you think actually using it is
dangerous, just delibrately make it hard to use for people who shouldn't
be using it. Leave out compilation documentation for instance, or make
it check that it's on testnet first and refuse to run if it isn't.
Pond for instance doesn't make binaries available:
 IIRC only recently have they provided a

@_date: 2013-10-20 19:11:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] A critique of bitcoin open 
Figures, I'm told that's exactly how they were first done -
 - only people found it inconvenient and
used the wiki instead.
Pathetic IMO for standards, but it wouldn't exactly be the first time
I've seen strong resistance to using revision control. (I quite
literally work with rocket scientists/satellite engineers who can't be
convinced to use it)
I dunno, maybe something using git submodules or subtrees - letting the
individual BIP "owners" make changes frequently until they're happy -
might have more social acceptance.

@_date: 2013-10-21 02:25:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] A critique of bitcoin open source 
Done: GitHub supports MediaWiki these days, so just directly copying from
'View Source' in the bitcoin.it wiki worked pretty well; I archived the
exact text of BIP. Tables, images and math is all supported by github
and look fine, although github doesn't seem to support coloration in
tables. Users wishing to edit their pull-req's or create new ones can do
so easily by forking the repository - they can see their changes as they
go in GitHub.
I've probably missed some stuff re: formatting, and I haven't changed
any of the submission guideline text in bip 1 yet, but that's probably
90% of the work done.

@_date: 2013-10-21 02:43:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] A critique of bitcoin open source 
Sure, I think Jeff mentioned the idea of a specific drafts/ directory
within the repository. (could also do a rejected/)
Less of an issue in some ways when it's all in git - just point people
to your bips fork.

@_date: 2013-10-22 03:49:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
Writing such RFCs is dangerous due to the consensus nature of Bitcoin -
it makes people think the standard is the RFC, rather than the code.
I hear one of the better intros to Bitcoin is the Khan academy videos,
but I've never watched them myself. Once you understand how it works,
start reading source code - the Bitcoin codebase is actually really
simple and readable. However remember that the implications of that
codebase are anything but simple; there's lots of reasons to think
Satoshi himself didn't understand Bitcoin all that well, even by the
time he left the project.

@_date: 2013-10-23 15:29:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] "Bitcoin codebase is actually really 
The nature of Bitcoin is that in any language one change could
accidentally bring the whole house of cards down.
Also the time and effort it takes to review changes for maliciously or
accidentally added exploits.
I'm making the statement that "the Bitcoin codebase is actually really
simple and readable." based on personal experience: for what the
reference client does - solve a previously thought unsolvable problem in
cryptography - the code is simple and readable. (try reading the OpenSSL
source-code sometime as a comparison) My experience has consistently
been that understanding what the code does is by far the easiest part of
understanding Bitcoin; understanding what the effect of what the code
does in terms of the system as a whole is at least another one or two
orders of magnitude more difficult.

@_date: 2013-10-23 15:40:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
The reference implementation is the specification - the "specification"
on the wiki is best thought of as a set of Coles Notes on the real
specification. If you don't already understand that and the nuance of
that statement you should assume the protocol is fixed in stone and
doesn't evolve at all; that statement is not quite true, but it's very
close to the truth.
I gotta get around to writing a "Developers" section for the FAQ
explaining this stuff....

@_date: 2013-10-23 16:27:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
What's on the wiki is mostly the work of people who aren't working on
the reference implementation, so no, you can't say that.

@_date: 2013-10-24 10:30:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
Quick thought on how to make blockchain-based fee estimates work better
in the context of out-of-band mining contracts: have miners advertise in
their coinbase's what fees were actually paid, as opposed to appear to
have been paid.
The logic is very simple: we assume miners aren't an effective cartel
and are willing to undercut each other. Therefore it's in their
interests for people to broadcast a transaction with a fee that is
competitive the first time around so they can get onto mining it
Granted, those incentives may not be as strong as one would like, and
there could be some perverse ones as well, but it's a line of thought
worth thinking about more.
A related idea: let miners advertise a address to submit transactions
too. (like a node IP) The inherent proof-of-work is nice and could help
people more securely find someone to connect too with the inherent proof
that a lot of work went into mining the block with the address in it.
Obviously, lots of downsides too, but it's a different security model
than other forms of bootstrapping, and that's probably useful in of

@_date: 2013-10-24 10:34:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fix available for startup issues for git HEAD 
"DisconnectBlock() : outputs still spent? database corrupted"
If you can't get your node up and running and see the above in your
debug.log there's a potential fix available:
Be warned: highly rushed and poorly tested, so you're best to ask a dev
in IRC for more details.

@_date: 2013-10-24 10:43:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
For sure, although *usually* all kinds of odd-ball forms of compensation
can be turned into a dollar figure. :)
The thing is if a miner is mining a transaction, even in exchange for a
out-of-band fee if they succeed, they probably still have an incentive
to a: ask the sender to include enough of a fee that it propagates, and
b: broadcast it themselves to make sure it's in other nodes signature
caches so their blocks propagate fast. (esp. with by-txid-only relaying)
Anyway, in what circumstance would a customer want an exclusive contract
with a miner?

@_date: 2013-10-24 10:54:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
Eligius has contracts to do transaction mining, and it's currently 10%
of the hashing power.
As I said elsewhere, a good use-case for OOB fee payment is for
merchants who use the payment protocol, and want to get their customers
transactions mined as efficiently and cheaply as possible.
(child-pays-for-parent has more blockchain bloat and thus extra expense)
Sure, but even then there's no harm in letting more than one miner know
about it.
There's even an existing form of this: P2Pool lets shares be accompanied
by up to 50KB worth of transactions of any form.

@_date: 2013-10-25 03:07:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
I've responded to nearly all those arguments elsewhere, but anyway...
Indeed. Quoting myself here: "What we should have is both: fee
estimation with replacement so you can replace transactions in the event
that the estimate was too low."
So on IRC you were talking about very agressive mempool expiration - as
little as a block or two before tx's are expired. Now if a tx does fail
to get mined in that short window, am I correct in saying you want a way
to modify the fee it pays and rebroadcast? In which case wallet software
and other players in the ecosystem will have to adjust to the fact that
they can expect to see relatively frequent double-spends of unconfirmed
As you know I've already written relaying/mempool code for
tx-replacement and replace-by-fee; it's the wallet code that's the hard
part that I haven't done. If you're already planning on changing the
wallet side of things to handle replacement-through-expiration that'd
save me a lot of hard work. You're probably better qualified to write
that code too; I'm not very familiar with the wallet.
Worth thinking about the whole ecosystem of wallets involved; they all
have to handle double-spends gracefully to make tx replacement of any
kind user friendly. We should try to give people a heads up that this is
coming soon if that's your thinking.
Also, regarding tx replacement user experience:
Password-using wallets sign multiple versions of the transaction in
advance of course and release the higher fee versions only later if
required. (could be applied to coinjoin too)

@_date: 2013-10-25 12:13:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
Anyway, as I've said repeatedly my problem with fee estimation is that
it needs to be combined with some form of transaction replacement to
give users a way to recover from bad estimates, not that I think the
idea shouldn't be implemented at all. After all, we alrady have fee
estimation: wallet authors and users manully estimate fees!
This particular case is a nasty one re: recovering from a bad estimate,
and it's exactly why the payment protocol is designed for the sender to
give the receiver a copy of every transaction they make so the receiver
can be held responsible for getting them mined, eg. with
child-pays-for-parent, out-of-band fee payment, or maybe even by adding
inputs to the transaction. (SIGHASH_ANYONECANPAY)

@_date: 2013-10-25 18:13:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
If Bitcoin continues to grow we probably will be at some as-yet-unknown
point in the future.
Yes, but equally all estimates are imperfect, and you can trade-off risk
that your transaction will not go through initially for lower fees.
Estimates can be made sufficiently conservative that they are rarely
wrong - this is basically the strategy of the current system. Given that
demand for blockchain space isn't "saturated" it works reasonably well
for now. But without a good mechanism to recover from an initial bad
estimate you have to be more conservative than is efficient.
To a first approximation there's not much reason for miners to take
anything other than fee-per-KB into account when determining what
transactions to mine; you want to stuff your 1MB block full of high
paying transactions. That a child tx may make a parent more profitable
to mine complicates things - Gavin's current fee estimator also makes
too-low-estimates in that case - and not all algorithms to do so will
come to the same conclusion. (doing it perfectly is something like
O(n^2), and imperfectly is O(1) but doesn't handle multiple children
There are some second-order effects, a block is less likely to be
orphaned if all transactions in it have propagated sufficiently, thus a
miner should penalize very recently broadcast transactions. In addition
because miners never orphan themselves large miners have a significant
advantage regarding orphan-inducing effects. However those effects all
tend to be miner specific, and/or only temporary.
FWIW the logic behind orphans is currently rather frightening: a
rational miner will, the moment they learn that a block exists via the
quickly propagating block header, start working to extend that block
with one that either doesn't contain any transactions, or only contains
transactions they can be reasonably sure another miner didn't mine.
(e.g. via exclusive tx mining contracts) This boosts their profit
because they aren't wasting their effort while the rest of the block
propagates, removes much of the incentive have any limit on block size,
and incentivizes miners to extend chains they haven't actually validated
yet. (relying on the other miners incentive not to produce an invalid
With a size-limited blocks inclusion is more a matter of supply and
demand than policy.

@_date: 2013-10-25 18:49:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
State-of-the-art thinking has changed a lot; that document is over a
year old and needs significant changes to update it.
Network security is currently funded by inflation rather than
transaction fees. This is likely to remain true for at least a few more
years. FWIW the cost of that security on a per transaction basis is
about $18, see

@_date: 2013-10-26 00:15:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
Strong ACK on the basis of responding for forum trolls alone.
It's easy enough to make it a genuinely useful tool for multisig wallets
too: keep a copy of your Tor URL bookmarks on your second signing
computer. So long as either computer has the correct URL you're safe.

@_date: 2013-10-26 03:28:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Making fee estimation better 
Um... yeah. Note how I said on your original pull-req that I'd be happy
to see it merged once the bugs were fixed (95% of the transactions it
produced had zero fees even with zero priority txins for some reason)
and you added a lower bound on fees in the wallet code as a "do no harm"
In fact, I think I wasn't being conservative enough given that it
affects relaying of transactions. Instead add both lower and upper
bounds to what the wallet and relaying code uses for 0.9.0 and it'd
probably be safe to merge.  We can get relax those "training wheels" in
0.9.1 or 0.9.2 once we've had some real-world experience with how the
estimation system works in practice, particularly for how it affects

@_date: 2013-10-29 05:12:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
However note that for the rejection messages defined these are actually
covered by the "too-low-fees" rejection codes. What would would want a
rate limiting rejection code is things like getblock and other requests.

@_date: 2013-10-29 06:14:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
That would prevent us from using nVersion as a soft-forking mechanism.

@_date: 2013-10-29 07:38:33
@_author: Peter Todd 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Actually, that statement didn't go far enough: rejecting blocks with nVersions that you don't expect is a hard fork.

@_date: 2013-10-29 12:35:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] On soft-forks and hard-forks 
That's a nice sentiment, but there's a lot more nuance to it than
"soft-forks are bad"
We're talking about rejection here: you don't want to end up on an
isolated chain fork wondering if maybe miners have been unlucky. You
want to know that a longer chain exists so as to have solid evidence
that you're local configuration isn't what miners are mining.  Thus not
only should you "accept" blocks with versions you don't know about, you
should relay those blocks as well so that other out-of-date nodes have
the highest possible chance of finding out about them. Creating a block
is expensive, so with some minor safeguards - a high minimum difficulty,
and maximum size - relaying blocks you consider invalid is perfectly
safe and doesn't enable DoS attacks. Relaying block headers has similar
logic, and even less DoS attack worry. (don't apply bloom filters to
invalid blocks though!)
I had this discussion with Warren the other day actually: Litecoin is
considering banning old node versions and rejecting their attempts to
connect. I pointed out how you wanted to be damn sure there was no-one
mining with them, least you wind up creating a slowly growing fork mined
by nodes unaware of the main chain.
Soft-forks and SPV nodes is another topic. SPV nodes don't do any
meaningful validation - they usually don't even have the transaction
inputs spent by a transaction to determine if a scriptSig is valid.
Their security is already dependent on miners, so allowing those miners
to upgrade does no harm. In addition there are even cases where what
would be a hard-fork for a full node, is a soft-fork for a SPV node. On
the other hand if your "SPV" node is more sophisticated, then by all
means use a nVersion change to trigger an alert to the user. If you're
implementation relays blockchain data, continue doing so to ensure other
nodes find out about the new version as soon as possible. (all SPV nodes
should relay block headers if possible)
Note how the nVersion field is useful for voting: the "chain height in
coinbase" soft-fork was accomplished this way, changing nVersion from 1
to 2 with full enforcement of the rule triggered by a 95% supermajority.
Bitcoin is a decentralized system, so any changes need to be done by
voting to show that a clear consensus of hashing power will enforce and
validate the new rules. (time and height deadlines can be disasters if
the upgrade is ever ignored or delayed)
Interestingly this suggests that what we actually want is two nVersions
per upgrade: the first to signal that nodes wish to upgrade, and are
showing their intent to use the new rules. The second to signal that the
upgrade has actually happened and the old rules are now ignored. Client
software can use this two stage approach to know when rules may have
changed, and the user probably should consider upgrading. As applied to
the chain height upgrade we would have gone from version 2 during the
voting, to version 3 for any block produced while the rules were in
effect. Put another way, the last in nVersion is simply to signify that
the new blockchain rules are now active, as opposed to being proposed.

@_date: 2013-10-30 20:44:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
It's a bit more risky from a cryptography perspective, but provided your
wallet implementation is done correctly the extra risk is pretty much
theoretical. However this has caused real-world coin loss in the past in
the case of the Android PRNG flaw - re-using nonces in ECC signing
causes the private key to be revealed.
I think the real issue here is that John doesn't appear to have asked
any of the people whose signatures can release the funds if they were
willing to take part. If he had done that, he could have, and should
have, gotten separate pubkeys for the purpose of the bounty like was
done for Gregory Maxwell's CoinJoin bounty. Instead by not asking he is
in reality if not in theory placing demands on people who haven't
consented, particularly for the 1BTC bounty where he doesn't control any
of the private keys required to release the funds. IMO this is rude and
I encourage people not to do this.
Well, the issue with not disambiguating bounties is that if further
funds are sent to the bounty address it's unclear how do you handle
those funds. Note how he specified a specific txout for the 1BTC bounty,
but specified an address for the 4BTC bounty.
We're not that far off: I could cook up a Python script to do the
signature accumulation and signing in a few hours. There's just not all
that much demand yet to fully polish the UI's, and in any case, it'll
differ for every specific application.
FWIW blockchain.info added multisig escrow support ages ago, then
removed it not long after because usage was near zero.

@_date: 2013-10-31 07:07:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal to replace BIP0039 
I just wanted to say the fact that you're making key generation
auditable, and using deterministic signatures, is a clear sign that you
guys know what you're doing. Hearing this makes me a lot more confident
that the Trezor will prove to be a secure way to store my Bitcoins and
my pre-order will prove to be money well spent.

@_date: 2013-09-13 02:07:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] REWARD offered for hash collisions for SHA1, 
Rewards at the following P2SH addresses are available for anyone able to
demonstrate collision attacks against a variety of cryptographic
algorithms. You collect your bounty by demonstrating two messages that
are not equal in value, yet result in the same digest when hashed. These
messages are used in a scriptSig, which satisfies the scriptPubKey
storing the bountied funds, allowing you to move them to a scriptPubKey
(Bitcoin address) of your choice.
Further donations to the bounties are welcome, particularly for SHA1 -
address 37k7toV1Nv4DfmQbmZ8KuZDQCYK9x5KpzP - for which an attack on a
single hash value is believed to be possible at an estimated cost of
$2.77M (4)
Details below; note that the "decodescript" RPC command is not yet
released; compile bitcoind from the git repository at
$ btc decodescript 6e879169a77ca787
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_SHA1 OP_SWAP OP_SHA1 OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "37k7toV1Nv4DfmQbmZ8KuZDQCYK9x5KpzP"
$ btc decodescript 6e879169a87ca887
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_SHA256 OP_SWAP OP_SHA256 OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "35Snmmy3uhaer2gTboc81ayCip4m9DT4ko"
$ btc decodescript 6e879169a67ca687
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_RIPEMD160 OP_SWAP OP_RIPEMD160 OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "3KyiQEGqqdb4nqfhUzGKN6KPhXmQsLNpay"
$ btc decodescript 6e879169a97ca987
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_HASH160 OP_SWAP OP_HASH160 OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "39VXyuoc6SXYKp9TcAhoiN1mb4ns6z3Yu6"
$ btc decodescript 6e879169aa7caa87
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_HASH256 OP_SWAP OP_HASH256 OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "3DUQQvz4t57Jy7jxE86kyFcNpKtURNf1VW"
and last but not least, the absolute value function:
$ btc decodescript 6e879169907c9087
    "asm" : "OP_2DUP OP_EQUAL OP_NOT OP_VERIFY OP_ABS OP_SWAP OP_ABS OP_EQUAL",
    "type" : "nonstandard",
    "p2sh" : "3QsT6Sast6ghfsjZ9VJj9u8jkM2qTfDgHV"
For example, this pair of transactions created, and then collected, an
absolute value function bounty:
Specifically with the scriptSig: 1 -1 6e879169907c9087
1) We advise mining the block in which you collect your bounty yourself;
   scriptSigs satisfying the above scriptPubKeys do not cryptographically sign
   the transaction's outputs. If the bounty value is sufficiently large
   other miners may find it profitable to reorganize the chain to kill
   your block and collect the reward themselves. This is particularly
   profitable for larger, centralized, mining pools.
2) Note that the value of your SHA256, RIPEMD160, RIPEMD160(SHA256()) or
   SHA256^2 bounty may be diminished by the act of collecting it.
3) Due to limitations of the Bitcoin scripting language bounties can
   only be collected with solutions using messages less than 521 bytes
   in size.
4) "When Will We See Collisions for SHA-1?" - Bruce Schneier

@_date: 2013-09-23 09:34:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] Near-block broadcasts for proof of tx 
Currently there is no way for a node, SPV or otherwise, to know if a
transaction has been broadcast succesfully to any amount of hashing
power. This makes it difficult to determine if a transaction failed to
either propagate across the network, or failed to pay sufficient fees to
be worthy of inclusion in a block.
Broadcasting blocks that almost, but not quite, met the difficulty
target provides clients with fast and honest proof about the hashing
power mining their transaction. This proof is inherently honest because
making a "near-block" is an expensive operation; additionally given at
least one honest peer a node can detect near-block censorship by any
other peer statistically.
Limitations of fee estimation
Mempool-based fee estimation is limited by the ability of peers to lie,
particularly to SPV peers. Miners wishing to increase fees can conduct
sybil attacks where they lie to peers about the average fees required to
get transactions into blocks. This problem is particularly dangerous
given the lack of incentives to run full-nodes in the first place; the
number of full nodes has continued to drop over time as users switch to
SPV clients and web-wallets; it would be unfortunate if users started
switching to web-wallets because they could offer better fee estimation.
In any case creating monetary incentives to sybil the network is very
Out-of-band fee payment has the opposite effect of making fees in blocks
appear lower than actually required to get them mined; transactions will
get stuck unless an initial bad estimate can be replaced with a higher
paying one. The payment protocol makes out-of-band fee payment
attractive in the case where you want to accept a payment from a
customer and pay the fee for them; child-pays-for-parent wastes money
paying for additional blockchain space.
Replacement-based schemes allow for recovery from stuck too-low
transactions, but they are still succeptable to sybil attacks. (don't
relay the transaction to other pools)
Miner incentives to create near blocks
Why would a miner want to go to the trouble of broadcasting a near block
anyway? Wouldn't it be better if users didn't get feedback about fees
and over-paid instead?
If you are a large miner as a % of total profit efforts such as sybiling
the network have a greater rate of return; if you are a small miner the
greater income you receive from deception is outweighed by the cost.
Thus you have an incentive to provide mechanisms to force larger miners
to behave honestly.
Secondly near-blocks could help "pre-propagate" transactions that will
be mined in the near future, thus reducing block propagation times and
hence orphan rates. (the pre-propagation can use the proof-of-work to
rate-limit transactions that nodes would otherwise not forward, also
allowing non-full nodes to safely participate in relaying) Again, this
is something that most interests smaller miners with less peak bandwidth
rather than large pools.
In the event of a fork near blocks can be used to more quickly determine
which side of the fork has the majority of hashing power, allowing the
minority side to switch sooner. Again the reduction in orphan rates
benefits smaller miners more than larger ones. (though note how only
near-block headers are required for this application)
Contents of a near block
From the miner incentives we see that near blocks should contain two
types of information:
1) Transactions known to the miner, but not included in the current
block. This information allows nodes to determine if a transaction they
have broadcast was succesfully propagated to the majority of hashing
power regardless of whether or not it is being mined, allowing nodes to
avoid sybil attacks attempting to censor the transactions they make.
This information needs to be committed separately in the coinbase tx.
The incentive for miners here is to ensure that no-one can gain an
unfair advantage with a sybil attack.
2) Transactions the miner is attempting to mine, proved by the merkle
root. The incentives here are allowing non-full nodes to safely
propagate transactions, improving block propagation, as well as further
preventing deception by larger miners.
Transaction mutability complicates both  and  In the case of nodes can exploit mutability while relaying transactions, although at
least relaying mutability is increasingly difficult; the incentives are
such that the miners themselves have no reason to lie about the txids
they know of. For  the incentives are all harmed by mutating
transactions, so again we can expect miners to either leave transactions
as they are, or simply not publish near blocks at all.
Bandwidth usage is reasonable: the average transaction from the last
10,000 blocks is 450 bytes. Both data sets can be delta compressed
against previously sent txids. Even a naive implementation that sends
full txids would result in near blocks that are about 1/10th of the size
of full blocks. (32-byte txids, and 1/4 of that amount in the "seen but
not mined" list) The machinery for near blocks can also be easily
re-used to implement improved full-block relaying with transactions in
blocks being referenced to by txid.
Replacement with near blocks
An node making a transaction can do the following:
1. Broadcast the transaction with an initial estimated fee. (the txid is
added to the bloom filter here) The estimate can safely be be on the low
2. Wait
3. If transaction still hasn't appeared in either a block or near block,
rebroadcast with a higher fee, either by replace-by-fee method, or
zero-conf safe method of adding an additional txin+txout.
Peers practicing censorship of either transactions or near blocks can be
detected statisticly by preferring to connect to peers that provide more
near blocks. Note how a short 80-byte near block header is sufficient
information to detect a peer withholding near blocks, and that header
can be relayed by SPV peers safely. If the transaction fails to get into
the "seen-but-not-mined" list, a node can use that failure as an
indication to find other peers to relay too.
Currently SPV clients are vulnerable to their peers withholding valid
bloom filter matches; future UTXO commitments can be designed to make
this impossible, and spot-check auditing can detect it now.
Out-of-band fee payment with near blocks
A purchaser of out-of-band fee payment services can use near-blocks to
check that their fee offer has been accepted and miners are mining their
transaction. This would be particularly useful for a decentralized
system where offers backed by fidelity bonds are made; it would be good
to encourage such systems over arrangements between purchasers and large
There is a serious problem however with proof-of-propagation and
proof-of-mining: they let miners cheat. The proof that a given
transaction is being mined can be used to mine the transaction yourself,
without having to maintain a copy of the UTXO set or indeed do any
validation at all. Having said that this risk already exists due to
P2Pool, which forwards transactions along with shares already.
In any case, it's yet another argument that we need miners to prove they
possess the UTXO set.

@_date: 2013-09-26 02:37:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
...until the Bitcoin payment protocol showed up and let anyone with the
ability to MITM https turn that ability into untraceable cash.
I won't be at all surprised if one of the most valuable things to come
out of the payment protocol using the SSL PKI infrastructure is to give
us a good understanding of exactly how it's broken, and to give everyone
involved good reasons to fix it.
Even if the flaws of SSL PKI were exploited as a way to harm bitcoin by
governments and other large players - and SSL PKI remained unfixed - I'd
much rather have that solid evidence that it was broken than not.

@_date: 2014-04-01 22:00:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
What's interesting about this bug is we could also fix the problem - the
economic shock - by first implementing the OP_CHECKLOCKTIMEVERIFY opcode
in a soft-fork, followed by a second soft-fork requiring miners to
"pay-forward" a percentage of their coinbase outputs to the future.
(remember that whomever mines a block controls what
recently-made-available anyone-can-spend txouts are included in their
block) We could then pick the distribution rate fairly arbitrarily; I
propose the following linear distribution:
Each gold mine produces 21,000,000 coins over 210,000*64 blocks, or
1.5625 BTC/block evenly distributed. Measured as an absolute against the
monetary the inflation rate will converge towards zero; measured against
the actual economic monetary supply the value will converge towards some
low value of inflation. In the short run we get an immediate reduction
in inflation, which can help our currently sluggish price. Either
outcome should be acceptable to any reasonable goldbug - fortunately our
community is almost entirely made up of such calm and reasonable people.
Meanwhile maintaining a miner reward has significant advantages in terms
of the long-term sustainability of the system - everyone needs PoW
security regardless of whether or not you do transactions, thus we
should all pay into it.
As for your example of Python, I'm sure they'll accept a pull-req
changing the behavior in the language.

@_date: 2014-04-06 12:37:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] Standardizing OP_RETURN message format 
Why do you want to make it easier for third-parties to determine what
your OP_RETURN messages are for? You want the messages to be
indistinguishable from each other to avoid censorship of them, and give
node operators plausible deniability, just like you want your Bitcoin
transactions to be indistinguishable from each other. Efficient discover
should be done by controlled disambiguation, for instance with the
prefix filtering method. (easily applied to bloom filters as well)
Secondly do not restrict yourself to OP_RETURN - it is far from certain
that it will survive in its present form with the high centralization of
mining we currently have. Note how it was rather arbitrarily reduced
from 80 bytes to 40 bytes, screwing over a number of projects who had
naively written code assuming it would be deployed as promised in the
promised form.
In any case I have better encoding methods for proof-of-publication and
commitments on my TODO list and will be pubishing code and best
practices specifications in the coming weeks.

@_date: 2014-04-09 12:03:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Hash: SHA512
Two talking points for said developers yo their user re: "Why use a full node?":
1) It's more private. Bloom filters gives away quite accurate statistical information about what coins you own to whom ever you happen to be connected too. An attacker can easily use this to deanonymize you even if you don't reuse addresses; Tor does not help much against this attack.
2) It's more secure. SPV means you are trusting miners to do validation for you. With the extremely high degree of mining centralisation we currently have it would only take one or two pools getting hacked for an attacker to be able to get enough hashing power to easily fool your SPV wallet into accepting a fake transaction.
As for what we can offer those developers, partial UTXO set mode would be a great long term goal.

@_date: 2014-04-09 13:38:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
Hash: SHA512
Yup, that's part of the idea behind partial UTXO set mode. You could have a model where your node starts with no data at all, and hence SPV security. You tell your node what the oldest key birthday is that your interested in and it downloads the full block chain starting at that date, giving you your txs w/ SPV security and full node privacy.
What partial UTXO would add on top of that is then your node would gradually scan backwards until block zero, at which point it has a complete UTXO set and is a full node.

@_date: 2014-04-09 13:46:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Any collective group that has a majority of hashing power will have no major issues running enough nodes that follow their rules to make SPV insecure anyway.
There's no good reason not to have SPV security nodes distribute block chain data, particularly block headers. It helps provide redundancy in the network topology and helps provide more resources for full nodes to sync up faster. For instance in a network with a large number of partial UTXO set nodes if those nodes are forwarding block data to each other they can get enough data to become fully fledged full nodes without putting all the load on the existing full nodes.  This is a good thing.

@_date: 2014-04-09 14:04:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Big security advantages too. For instance if an attacker hacks, say, 10? of hashing power the next step for them to attack SPV clients is to try to Sybil attack them so they won't find out about the longer chain. The fewer providers of block chain data there are out there the easier that attack is - just simultaneously DoS a bunch of nodes, perhaps by a low-bandwidth exploit like the bloom io or division by zero DoS attacks. This is much harder to pull off if every SPV client is passing around block headers.
Similarly by passing around full blocks the attacker has a harder time knocking other miners off the network. Regardless of whether or not a miner's peers are fully validating chain data they still have the data they need to mine the next block and thus extend the longest correct chain.

@_date: 2014-04-10 05:33:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
 But we
Your social group is weird.
Nearly every coworker at my previous job had a tower computer at work and at home. Equally in my nontechnical social group lots of people, a significant minority if not majority, have Apple and PC desktops hooked up to large monitors at home for media production and games. Those who don't more often than not have laptops used as desktops, sitting in one place 95% of the time and left on.
People have found it most efficient to work at a static desk for centuries - that's not going away. Of course we're seeing desktop usage and sales falling, but that's only because previously the mobile usage was forced into suboptimal options by technical realities. The trend will bottom out a long way from zero.
Besides, even if just 1% of bitcoin users had a machine they left on that could usefully contribute to the network it would still vastly outweigh the much smaller percentage who would run nodes on expensive hosted capacity out of the goodness of their hearts. If we educated users about the privacy advantages of full nodes and gave them software that automatically contributed back within defined limits we'd have tens of thousands more useful nodes in the exact same way that user friendly filesharing software has lead to millions of users contributing bandwidth to filesharing networks. Similarly take advantage of the fault tolerance inherent in what we're doing and ensure that our software can shrug off nodes with a few % of downtime - certainly possible.
Of course, this doesn't fit in the business plans of those who might want to run full nodes to data mine and deanonymize users for marketing, tax collection, and law enforcement - one of the few profitable things to do with a full node - but screw those people.

@_date: 2014-04-10 05:47:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
You're both missing a more important issue: a core security assumption of bitcoin is that information is so easy to spread that censorship of it becomes impractical. If we're at the point where nodes are charging for their data we've failed that assumption.
More concretely, if my business is charging for block chain data and I can make a profit doing so via micro payments I have perverse incentives to drive away my competitors. If I give a peer a whole block they can sell access to that information in turn. Why would I make it easy for them if I don't have too?
Anyway, much of this discussion seems to stem from the misconception that contributing back to the network is a binary all or nothing thing - it's not. Over a year ago I myself was lamenting how I and the other "bitcoin-wizards" working on scalability had quickly solved every scaling problem *but* how to make it possible to scale up and keep mining decentralised.

@_date: 2014-04-10 07:36:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
No problem!
I'm sure we'll see payment channels implemented sooner or later
the form of "hub and spoke" payment networks. The idea there is you have one or more centralised hubs who in turn have payment channels setup to and from payors and payees. So long as the person you want to pay is connected to the same hub as you are, or in more advanced versions connected via a ripple style chain, you can push payment to the hub and get proof they did the same for the recipient. Your loss is always limited to the incremental payment amount and payment is essentially instant.
Of course, it's got some disadvantages compared to standard bitcoin transactions - its less decentralised - but when compared to other forms of off-chain payment in most situations its a strict improvement, and having the capability available is always a strict improvement. Like fidelity bonded banks the trust required in the hubs is low enough that with some minor effort applied to anti-DoS you could probably get away with using even hubs run by anonymous actors, making the centralisation less important. (hubs are essentially interchangeable) Unlike pure fidelity bonded banks the effort required to build this is relatively minor!
You can even combine it with chaum tokens for anonymity. You'll want to hold the tokens for some amount of time to thwart timing analysis, leaving you somewhat vulnerable to theft, but in that case fidelity bonded banking principles can be applied. Other than that case the idea is probably made obsolete by micropayment hubs.
Regulatory issues will be interesting... If you wind up with a few central payment hubs, without chaum tokens, those hubs learn all details about every transaction made. Obviously if a big actor like BitPay implemented this there would be a lot of pressure on them to make those records available to law enforcement and tax authorities, not to mention marketing and other data mining. Equally I suspect that if an alternative more decentralised version was implemented there would be strong government pressure for those approved hubs to not interoperate with the decentralised hubs, and equally for merchants to not accept payment from the decentralised hubs.
But all the same, if widely implemented this reduces pressure to raise the block size enormously, keeping the underlying system decentralised. So the net effect is probably positive regardless.
Oh yeah, credit goes to Mike Hearn for the payment channels, and if I'm correct, for the hub concept as well.
Amir: You should think about adding the above to dark wallet. It'd be good if the protocols are implemented in an open and decentralised fashion first, prior to vendor lock in.

@_date: 2014-04-10 07:43:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Why not just put an expiration date on that information and delay deletion until the expiration is reached?
Also, its worth noting how the node bit solution you proposed can be done as a gradual upgrade path for SPV client. From the perspective of nodes that don't know about it they just see the pruned nodes as SPV nodes without any chain data at all. The only issue would be if large numbers of uses turned off their full nodes, but that's a possibility regardless. Done with partial UTXO set mode this may even result in an eventual increase in the number of full nodes.

@_date: 2014-04-10 07:52:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Ah right, of course. Along those lines we should credit Jeremy Spilman (?) for figuring out how to get rid of the dependency on nSequence, makimg the protocol trust-free.
I do recall it having an issue with malleability, semi-fixed with the P2SH trick. Be good to clear that up for good for Pieter's proposed malleability patch.
...and only took another five hundred years for math to catch up and make it trust free, modulo miner centralisation!

@_date: 2014-04-10 07:54:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
I forgot to ask last night: if you do that, can you add new blocks to the chain with the encoding incrementally?

@_date: 2014-04-21 00:06:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Economics of information propagation 
Hash: SHA256
That is mistaken: you can't mine on top of just a block header, leaving small miners disadvantaged as they are earning no profit while they wait for the information to validate the block and update their UTXO sets. This results in the same problem as before, as the large pools who mine most blocks can validate either instantly - the self-mine case - or more quickly than the smaller miners.
Of course, in reality smaller miners can just mine on top of block headers and include no transactions and do no validation, but that is extremely harmful to the security of Bitcoin.

@_date: 2014-04-22 17:31:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Double-spending unconfirmed transactions is a 
You may have seen my reddit post of the same title a few days ago:
I've done some more experiments since, with good results. For instance
here's a real-world double-spend of the gambling service Lucky Bit:
Original: 7801c3b996716025dbac946ca7a123b7c1c5429341738e8a6286a389de51bd20
Double-spend: f4e8e930bdfa3666b4a46c67544e356876a72ec70060130b2c7078c4ce88582a
The double-spend was mined by Eligius and made use of the fact that
Eligius blacklists transactions to a number of addresses considered to
be "spam" by the pool operators; affected transactions are not added to
the Eligus mempool at all. Lucky Bit has a real-time display of bets as
they are accepted; I simply watched that display to determine whether or
not I had lost. With Eligius at 8% and the house edge at 1.75% the
attack is profitable when automated. My replace-by-fee patch(1) was
used, although as there are only a handful of such nodes running - none
connected directly to Eligius from what I can determine - I submitted
the double-spend transactions to Eligius directly via their pushtxn
Of course, this is an especially difficult case, as you must send the
double-spend after the original transaction - normally just sending a
non-standard tx to Eligius first would suffice. Note how this defeats
Andresen's double-spend-relay patch(3) as proposed since the
double-spend is a non-standard transaction.
In discussion with Lucky Bit they have added case-specific code to
reject transactions with known blacklisted outputs; the above
double-spend I preformed is no longer possible. Of course, if the
(reused) Lucky Bit addresses are added to that blacklist, that approach
isn't viable - I suggest they switch to a scheme where addresses are not
reused. (per-customer? rotated?) They also have added code to keep track
of double-spend occurances and trigger human intervention prior to
unacceptable losses. Longer term as with most services (e.g. Just-Dice)
they intend to move to off-chain transactions. They are also considering
implementing replace-by-fee scorched earth(4) - in their case a single
pool, such as Eligius, implementing it would be enough to make the
attack unprofitable. It may also be enough security to allow users to
use their deposits prior to the first confirmation in a Just-Dice style
off-chain implementation.
1) 2) 3)  and
   4)

@_date: 2014-04-23 11:05:55
@_author: Peter Todd 
@_subject: [Bitcoin-development] Economics of information propagation 
CC'ing bitcoin-research - may be more appropriate to move the discussion
there as this discussion is delving into future scenarios.
They're probabilistic; mining is progress free so per unit hashing power
every miner has an equal chance of finding a block. As for resolution,
well, currently nodes (and miners) mine on the block they saw first; if
they learn about another block at the same height they stick to the
block they are already mining on. First seen at same height is probably
generally economically rational as the first block you see probably
propagated to the most nodes, although tweaks to that are probably
Not at all, in fact mining on top of the block is the best thing to do
because it *prevents* your block from ending up as an orphan. Basically
imagine I find block b1a, and you find conflicting block b1b. What I
need to do is find block b2a, which is on top of b1a, before you find
block b1b to avoid my block being orphaned. The best way to do that is
mining on top of my block - that's what's most rational for me.
Sure thing.
So looking at the replies your post got in the past few days it looks
like there's some misinformation going around. First of all is the
question of how harmful it is if miners mine on top of blocks that they
haven't actually validated, and yes, that's extremely harmful. For
instance if I were an attacker I could mine an invalid block that makes
coins out of thin air and use it to defraud SPV-wallet-using clients;
everyone who is mining without validating is helping me succesfully pull
off that attack by increasing the chance I'll get enough confirms to
trick my target into thinking the coins are real. (remember I could have
stolen the hashing power by hacking into a pool)
Mark Friedenbach suggested headers first where the block header and
block contents are propagated around the network separately. What that
results in has a few different scenarios:
1) Fee's don't matter and miners aren't forced to validate
This is the scenario we're in right now. The block reward comprises the
supermajority of mining income, and it is possible to mine a block
without first validating the contents of the previous block. When a
miner receives a block header that extends the longest known blockchain
they can immediately switch to it and start mining.
Whether or not doing so is rational is just a matter of what's the
probability that the previous block was invalid? If it was, the miner
mining it just wasted 25BTC, $12.5k, so you can be almost sure it is
valid and you don't need to wait. Of course, if you then find a block,
you can pull the same trick all over again and the next guy might be
mining on top of two blocks they haven't validated, and so on.
Obviously this presents a very nasty failure mode if the majority of
miners follow this behavior and a block is invalid, or even just gets
lost. Similarly in the majority scenario there's no direct incentive to
actually propagate your blocks - they'll still get accepted to the main
chain anyway.
That said, small and large miners make roughly the same amount as the
block reward dominates and blocks of any size will get confirmations
fairly quickly.
2) Fee's do matter and miners aren't forced to validate
Now transaction fees represent a non-trivial portion of a miner's
income. Centralization incentives would depend on to what extent fees do
matter. Again, there's some nasty failure modes possible. That large,
slow-to-propagate blocks still get confirmed, yet small miners can't
mine transaction fees is likely a major centralization incentive.
3) Miners are forced to validate
Or at least, we can force miners to actually have the previous block.
Andrew Miller's Permacoin is one approach; some varients of UTXO
commitments have this as a side-effect too. On the one hand this solves
the really nasty failure modes that headers-first has; on the other hand
you're back to the centralization incentives we have right now.
What's important however to remember is that any attempt at saying
things like "[A]s soon as [Miners] receive and process the contents of
block A, they switch to that." - as Mark suggested(2) - doesn't belong
in an economic analysis as such rules are unenforcable. For instance
that'd suggest that in the forced-validation headers-first scenario a
large miner who received a block header, then found block themselves,
would switch to mining the block they *didn't* find simply because they
"got the header first". Obviously this is not economically rational for
them to do so they won't, leading to the same centralization incentives
as always.
As for why that's economically irrational: so the large miner finds that
second block and broadcasts it around the network. Do you the small
miner keep mining on the shorter chain just because the large miner
broke the gentlemans agreement to respect header times? Of course not,
time is relative and you have no idea whether or not anyone else is
doing so. If you mine on the shorter chain you're side is going to need
to find two blocks to catch up - not likely. Secondly you risk forking
the network due to a consensus failure, say by a divergent times the
headers were received.
1)

@_date: 2014-04-23 11:28:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Also replace-by-fee scorched earth.
To put it mildly. :) Beyond the obvious issues with adding mechanisms
for miners to vote on what blacklists they wish to apply, it's
interesting to consider how trying to make zeroconf transactions secure
directly is quite close to changing the block interval. Like the
blocksize that's a fundemental economic parameter of the system - how
low-latency and well connected you must be to be allowed to mine. Even
in a scheme where the punishment for allowing a double-spend was somehow
applied perfectly fairly, you'd still be favoring large well-connected
miners in a very similar way to reducing the block interval.
It's worth noting that the academic efforts studying Bitcoin are
spending quite a bit of effort focused on the incentive compatibility of
various mechanisms in the protocol: There's solid consensus in the academic community that Bitcoin can't
just depend on notions of "honesty" to work.
What exactly those rules are is up for debate too. Right now if, say,
just 5% of Bitcoin miners were willing to accept Colored Coin
transactions you could still use Colored Coins. The other 95% may want
to block said transactions, but there's huge practical difficulties in
organizing a reorg and ensuring that everyone co-operates; miners have
strong incentives to defect if the consensus isn't assured as any miners
attempting to reorg are wasting their hashing power if it doesn't
OTOH with a voting scheme the cost to propose that a specific block or a
transaction be blacklisted is much lower. In Mike's proposed scheme to
not just blacklist, but actually take coinbases it's downright
profitable. Rather than being a last resort option, it'll be easy for
miners to propose various things be blacklisted, if the vote goes
through, great, if it doesn't, no harm done. Obviously that makes
blacklists into a much more useful tool and greatly changes the
political landscape around them.
Remember, if you're operating a publicly known pool, and there's a
voting mechanism available to you to blacklist specific blocks, how are
you going to resist pressure from your local authorities to do just when
there's no cost to you to do so?

@_date: 2014-04-23 11:55:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Worse, it's a mechanism where miners can vote to penalize other miners
for any reason at all. Nothing in the mechanism requires any proof that
a double-spend happened, nor can it.  Even if you require the simple
"two signatures for same output" mechanism, that just proves the
existance of a second signature, and says nothing at all about whether
or not that signature was ever broadcast on any network.

@_date: 2014-04-23 14:15:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Agreed, although I think waxwing put it better: Bitcoin's most
fundamental property is its neutrality. If it loses this, it is not
But I also agree with Gavin that the bitcoin-development email list is a
perfectly good place to have these types of discussions. I myself have
used it repeatedly to publish ideas specifically due to wide readership
and multiple independent archives.
Actually we do: Eric Springer
 - joined Feb 11 2010
Anyway that's just twitter bootstrap or something; I hear the wizards
can pump out a site like that in a few hours.

@_date: 2014-04-24 08:59:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] 0 confirmation txs using replace-by-fee 
FWIW I'm running an experiment right now to detect how easy it is to
doublespend 0-conf transactions I need to collect more data, but initial
results indicate that transaction propagation is sufficiently unreliable
that double-spending frequently works without miners using
replace-by-fee even when both transactions pay high fees, there is a 60
second delay between first and second, and there's only about four
replace-by-fee nodes on the network.
With replace-by-fee scorched-earth the success rate of such
double-spends would be significantly reduced as the attacker would need
to get lucky with bad propagation not just once, but twice in a row.
Just to be clear, while that post is mine, original credit for the idea
actually goes to John Dillon as far as I know; I first heard about it
from him in private discussion.
A few things:
1) Replace-by-fee doesn't protect against sybil attacks; only
confirmations are solid evidence that a transaction has actually reached
the mining power and your communication channel to that mining power
isn't being blocked. Keep in mind that Bitcoin depends on the existence
of a jam-free network, and very importantly, lets you detect when that
network has failed and you are being jammed. No unconfirmed transaction
scheme can solve this problem in a decentralized network.
2) Replace-by-fee scorched earth does require you to keep private keys
online to sign the replacements. Not a big deal, but it's yet another
reason why you wouldn't want to use it for high-value stuff.
3) It doesn't directly solve finney attacks(1) where the miner mines the
transaction in private. However finney attacks are only relevant if
there is high centralization of hashing power, and all other proposed
mechanisms, e.g. coinbase reallocation, themselves do a lot of harm to
decentralization. (just look at how coinbase reallocation lets large
pools bully smaller miners out of business by blacklisting their blocks)
One interesting thing with regard to finney attacks and replace-by-fee
though is that enforcing hasher visibility of the blocks they are mining
- what getblocktemplate was meant to do voluntarily - lets any hasher
detect a finney attack double-spend and broadcast it. They have a weak
incentive to do so - the scorched earth reply is a high-fee transaction
of course and pre-broadcasting transactions makes blocks propagate
faster - at which point you're back to a public double-spend.  Enforcing
visibility of block contents to hashers is definitely a good thing for
1)

@_date: 2014-04-24 09:44:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Like I said before, that leads to the obvious next step of
deleting/stealing their coinbases if they don't identify themselves.
Another likely outcome would be for coinbase blacklisting to be used as
a way to force a minority of miners to adopt a transaction blacklist
that the majority of miners had adopted. Any block containing
transactions spending coins on the txout blacklist would itself be
punished by having the block reward either blacklisted or taken.
It's not possible to produce a cryptographic proof that a given block
engaged in a Finney attack. You're proposed coinbase blacklisting/reallocation
mechanism is simply a way of voting on what coinbases to either
blacklist or reallocation, nothing more.

@_date: 2014-04-24 11:03:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
I don't disagree with you at an end stage, but the thing with coinbase
blacklists/confiscation is because it's a voting mechanism the initial
stages of enforcing widespread censorship rules with it are much easier.
For instance, if a 10% pool that has been forced/wants to blacklist
certain transactions can do so, and then vote to blacklist blocks that
do not abide by that blacklist. Casting that vote does them no harm.
Every time another pool joins the blacklist, there's no harm to them to
doing so.  At some point they will reach a majority, which causes the
blacklist to actually apply. The whole process happens smoothly, letting
the blacklist be applied safely and easily.  With orphaning/reorging on
the other hand you just can't be sure that the other miners will
actually adopt it, making adoption risky.
Of course, that's above and beyond the fact that you can't prove a
Finney attack happened to a third-party, making it easy to attack
smaller miners with Sybil attacks, get them creating blocks with
double-spends in them, and using that as an excuse to punish them.
Decentralized markets are a great example: the bids and orders they
depend on are time-senstive and become much less valuable if they get
delayed greatly.

@_date: 2014-04-25 15:58:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP - Selector Script 
Keep in mind that P2SH redeemScripts are limited to just 520 bytes;
there's going to be many cases where more complex transactions just
can't be encoded in P2SH at all.

@_date: 2014-04-25 16:14:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
Actually I did some work looking at this problem a few months ago and
other than somewhat larger transactions it looks like implementing
oracles by having the oracle reveal ECC secret keys works better in
every case. Notably the oracle can prove they really do have the key by
signing a challenge message, and with some ECC math the transaction can
include keys that have been derived from the oracle keys, blinding what
purposes the oracle is being used for from the oracle itself.

@_date: 2014-04-25 17:14:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
Yup. Revealing EC points is *not* a replacement for the hash-locked
Same again, and on top of that the EC public point method still works
better in many circumstances with what are currently non-standard
transactions rather than trying to shoe-horn everything into one big
Along those lines, rather than doing up yet another format specific type
as Tier Nolan is doing with his BIP, why not write a BIP looking at how
the IsStandard() rules could be removed? Last year John Dillon proposed
it be replaced by a P2SH opcode whitelist(1) and I proposed some
extensions(2) to the idea to make sure the whitelist didn't pose
transaction mutability issues; very similar to Pieter Wuille's proposed
soft-fork to stamp-out mutability.(3)
The key reasons to have IsStandard() right now are the following:
1) Mitigate transaction mutability.
Pieter's soft-fork mitigates mutability well, and can be applied even
more easily as an IsStandard() rule.
2) Reduce the potential for scripting bugs to impact the ecosystem.
The scripting system has had a lot more testing since IsStandard() was
implemented. Additionally we have a large pool mining non-standard
transactions anyway, mostly negating the value of IsStandard() for this
3) Ensure that after a soft-fork upgrade transactions considered
   IsStandard() by the the remaining non-upgraded hashing power continue
   to be valid.
We don't want that hashing power to be able to be tricked into mining
invalid blocks. Future soft-forks for transactions will most likely be
done by either incrementing the transaction version number, or by
redefining one of the OP_NOPx opcodes with new functionality. We just
need to ignore transactions with version numbers that we are not
familiar with and additionally not include any of the OP_NOPx opcodes in
the whitelist.
One last detail is that sigops should be taken into account when
calculating fees; Luke-Jr's accept non-standard patch has code to do
this already.
1) 2) 3)

@_date: 2014-04-26 07:23:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Eliminating double-spends with two-party 
In the majority of high-value transactions the fact that funds will be
sent is known prior to when they actually are. For instance, if Alice is
to meet Bob in person to buy a car or sell some Bitcoins, both parties
know the transaction will probably happen in the near future some time
before it actually does. Existing escrow solutions already take
advantage of this fact; for instance Localbitcoins provides sellers the
ability to escrow their funds with Localbitcoins prior to when the funds
are released to the buyer.
However with nLockTime a third-party escrow agent is *not* required.
Instead prior to Alice sending the funds to the escrow address, she has
Bob sign a refund transaction that unlocks at some time in the future.
Generally the transaction does go through, and Alice and Bob sign a
second transaction sending the funds to Bob. Sometimes it doesn't, and
Alice either gets Bob to sign a transaction sending the funds back to
her, or in the worst-case, just waits for the timeout to elapse.
Note that this technique can be used in addition to a third-party escrow
agent - the third-party then only plays a role in exceptional
Implementation sketch: Mycelium Local Trader
While the above is fairly secure if transactions aren't being mutated
en-mass, better protections would be desirable. First of all adding a
third-party escrow to the two-party escrow is a prudent last ditch
measure to ensure that if malleability is an issue the third-party can
release locked funds manually; note how SIGHASH_SINGLE is used as
opposed to SIGHASH_NONE to prevent that third-party from having access
to the funds. Secondly a future soft-fork such as Pieter Wuille's
BIP62(2) can eliminate malleability. In particular, a soft-fork that
enabled the creation of signatures that did *not* include the txin txid
would be particularly valuable; in step 4 above Bob's refund signature
signed over the scriptPubKey and nLockTime only would cover all possible
cases whre a refund would be needed, such as accidental multiple
payments and previously unknown sources of malleability.
1) 2)

@_date: 2014-04-26 14:31:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] Eliminating double-spends with two-party 
The seller won't hand over the goods of course until they have a valid
transaction signed by the buyer sending them the escrowed funds. (and
the nLockTime deadline is sufficiently far away that the probability of
not being able to get the transaction mined in time is low)
Note how the mechanism I'm proposing is basically just a Jeremy
Spilman-style micropayment channel(1) used for a single payment; I
should have made that clear in my original post.
1)

@_date: 2014-04-26 15:37:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] Eliminating double-spends with two-party 
I swear, I'm getting alzheimers or something. This and stealth addresses
is now the second time I've totally forgotten I had just read an idea a
week prior:
Reddit user RubenSomsen, ten days ago:
"I would really like it if Mycelium allowed me to temporarily lock my
bitcoins in a 2-of-2 transaction with a potential buyer (of course with
nlocktime back to myself) so the network can start confirming the
transaction before we even meet."
Better explanation than mine too for someone wanting a quick intro.

@_date: 2014-04-28 08:02:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Replace-by-fee scorched-earth without 
Someone who wanted to remain anonymous sent me in this idea, which I'll
admit I'm kicking myself for not having thought of earlier. They sent
me this hash so they can claim credit for it later should they choose to
reveal their identity:
When Alice wants to pay Bob x bitcoins, rather than creating a single
transaction, tx1, that does that, she creates a pair of transactions,
with the second, tx2, spending the same inputs and an input provided by
Bob, but paying x*k bitcoins to fees. Should Bob detect a double-spend
he simply signs the extra input, making it clear that he intended for
the countermeasure to be deployed, and broadcasts tx2.
This mechanism has two advantages: 1) child-pays-for-parent isn't
required at avoiding changes to the relaying code and letting the
counter-transaction propagate quickly. 2) k can be adjusted such that
Alice is guaranteed to be worse off for attempting a double-spend even
taking into account the probability of getting away with it. For
instance, right now if just, say, Eligius adopted replace-by-fee a k
value of 20 would still make double-spends unprofitable.
However it does require payment protocol support. This lead me to
realize that if Alice signs all her inputs with the ANYONECANPAY sighash
bit set Bob can get the same effect by adding his own inputs to bump the
effective fee. While of course the funds to do so come out of his own
pocket, they are balanced out by the payment to him, with the net effect
being the same as the child-pays-for-parent version. Additionally in the
common case of "Bob would like Alice's transaction to go through sooner"
this also gives Bob the flexibility to add small sized inputs at will to
bump fees. (or for that matter Alice, giving a small privacy boost)
Using ANYONECANPAY does have one disadvantage in that transactions using
it are always malleable. However an "attacker" doing so is forced to
spend funds to do that. Secondly after the recent malleability attacks
wallet handling of malleability-related problems has greatly improved.
Finally it's worth noting how the k-overpaying version of scorched-earth
gives Finney attacking(1) miners - such as BitUndo - incentives to
defect knowing that they can earn significantly more fees by publishing
their supposedly secret transactions to the p2p network. Equally even in
the ANYONECANPAY version merchants may decide that discouraging fraud is
worth an overpayment.
1)

@_date: 2014-08-01 01:06:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] deterministic transaction expiration 
Anything that changes the semantics of nLockTime will do harm to
existing and future applications that make use of nLockTime for things
like refund transactions.
In any case, why do transactions need finite lifespans in mempools? If
you want to double-spend them with higher fees, then implement
replace-by-fee. In any case, lifetimes will never be deterministic as
not everyone runs the same software.
...in which case someone will circumvent this IsStandard() rule by
submitting "expired" transactions directly to miners with suitable

@_date: 2014-08-06 08:42:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] deterministic transaction expiration 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
To add a new field the best way to do it is create a new, parallel, tx format where fields are committed by merkle radix tree in an extensible and provable way. You'd then commit to that tree with a mandatory OP_RETURN output in the last txout, or with a new merkle root.
Changing the tx format itself in a hard-fork is needlessly disruptive, and in this case, wastes opportunities for improvement.

@_date: 2014-08-06 10:20:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] deterministic transaction expiration 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
The general case doesn't require transmission of any merkle data; it is derived from the tx data. Equally changing a data format is certainly: note how Freimarkets has no third-party library support because you've made it incompatible with the standard Bitcoin data structures. Merkle radix tree formatting OTOH is just a cryptographically committed extension of the tag-value concept seen in protobuf, among others.
re: efficiency, we need fundamental improvements in efficiency, not little micro-optimisations everywhere done at high cost to maintainability.
re: validation, note how the merkle radix tree meets that need by allowing the absence of data to be proven.
It's also rather useless without consensus. Expiry is only useful if it is a guarantee, if not you might as well just implement tx replacement directly.

@_date: 2014-08-06 10:38:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] deterministic transaction expiration 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
The general case is all committed information is included in the transaction; the merkle tree is a compatibility path, as well as an optimisation for lite clients and applications.
You should read more about soft-forks; see the BIP. Remember that Bitcoin protocol development and deployment is not a centrally controlled activity.

@_date: 2014-08-06 10:34:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] deterministic transaction expiration 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Bitcoin is already "broken" in that regard due to malleability, and more fundamentally, the existence of anyone-can-spend outputs, known private keys, SIGHASH_ANYONECANPAY, etc.
In any case, reorg-doublespend risk is no different than reorg-expiry risk.

@_date: 2014-08-06 23:33:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Payment ID #'s for Stealth Addresses 
Real-world experience with stealth address implementations used by
Cryptonote/Monero/etc. have shown that being able to attach a number of
some kind to each stealth-sent txout is valuable. For instance an
exchange with many customers can use such  to disambiguate payments
and credit the correct customer's account. Similarly an informal
person-to-person transaction can attach a number short enough to be
communicated verbally or on paper. Finally multiple payments with the
same ID # can be merged together in wallet UI's, allowing
merge-avoidance to be conveniently used with stealth addresses.
To avoid accidental collision such payment  should be at least
64-bits; to avoid privacy loss the encoded size should be the same for
all users. Thus we pick 64-bits or 8-bytes. In addition for the purposes
of CoinJoin and multiple outputs it would be desirable for all
stealth-using outputs the option of sharing a single 33-byte ephemeral
pubkey. Thus our OP_RETURN output becomes:
    OP_RETURN   { ... }
Of course, this can't be accomodated within the existing 40-byte, one
OP_RETURN per tx, IsStandard() rules, something which is already causing
issues w/ Dark Wallet when users try to send to multiple stealth
addresses at once, and when multiple stealth sends are CoinJoin'd
1) "Merge avoidance", Dec 11th 2013, Mike Hearn,

@_date: 2014-08-07 01:03:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] SIGHASH_ANYONECANPAY extra inputs DoS attack 
tl;dr: Transactions with SIGHASH_ANYONECANPAY-using inputs can be DoS
attacked by attackers adding extra inputs to them that make the fee/byte
paid unfavorable to miners, while still being high enough to be relayed.
While just a nuisance DoS attack, this is a serious obstacle towards
using ANYONECANPAY.
Background: What uses ANYONECANPAY?
Each input that does not use SIGHASH_ALL can be evaluated in terms of
whether or not it increases the fees/byte paid by the transaction. Thus
we can optimize a transaction to pay the highest fees/byte by doing the
    def optimize_tx(tx):
        tx2 = CTransaction(vin=[], vout=tx.vout, nLockTime=tx.nLockTime)
        for txin in :
            if :
                continue
            if :
                prev_fee_per_byte = tx2.fees / len(tx2.serialized())
                tx2.vin.append(txin)
                if tx2.fees / len(tx2.serialized()) < prev_fee_per_byte:
                    # adding txin decreased fees/byte
                    tx2.vin.pop()
                    return tx2
            else:
                tx2.vin.append(txin)
        return tx
Essentially txin's that reduce the profitability of the transaction are
dropped, including the attacker's added txins. Meanwhile txins that
increase the profitability can be added by anyone.
1) "[Bitcoin-development] Replace-by-fee scorched-earth without child-pays-for-parent",
   Apr 28th 2014, Peter Todd,
    at lists.sourceforge.net/msg05211.html

@_date: 2014-08-13 11:10:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] Another weird transaction question... 
Hash: SHA256
Have you looked at the Bitcoin Core script test cases yet? You might be surprised at what is allowed. Equally, read the source code! In particular follow the block acceptance logic line by line from start to finish.
In any case, the Bitcoin protocol doesn't care whether or not a public key is valid.

@_date: 2014-08-18 22:30:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] Cloud mining should be using merkle sum trees 
A number of people - most recently Gavin Andresen - have speculated that
cloud hashing operations may in fact be ponzi schemes that don't
actually own the hashing power they claim to own. The claim is that the
customers upfront purchase of hashing power is simply kept and used to
pay off existing customer profits rather than actually being used to
purchase mining equipment.
We can use merkle sum trees to detect this fraud cryptographically:
1) Put the MH/s paid for by each account into a merkle sum tree, each
with a customer supplied unique identifier. (like their email address)
This allows the customer to verify that the hashing power they paid for
has been included in the total hashing power claimed.
2) Mark blocks found by the operation publicly so they can be associated
with the specific cloud mining operation; putting the merkle sum tree
root hash into the coinbase or an OP_RETURN output would be ideal. This
allows anyone to verify that the hashing power claimed corresponds to
the # of blocks actually found.

@_date: 2014-08-19 20:16:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
That is simply incorrect. The resources required to do that kind of monitoring are very high; even the NSA can't pull it off consistently for non-targetted operations due to limitations on upstream bandwidth and other resources. (remember that many of their taps are non-cooperative ones, obtained by breaking into routers at ISP's) This I've confirmed with direct conversation with Jacob Applebaum and other Tor devs. Every additional bit of encrypted information flowing over the internet increases the work they need to so to deanonymize you. This is not unlike how CoinJoin, while not providing guaranteed anonymity, makes the job of attackers significantly more difficult by creating large amounts of statistical noise. In addition the Bitcoin P2P protocol has natural anti-traffic analysis properties due to its asynchronous nature.
Re: MITM attacks, again, the resources required to conduct them on a large scale instead of passive attacks just don't exist. For instance the NSA has to be relatively selective in using them for fear of being detected; being able to detect attacks is a huge improvement over the status quo anyway.
Having said that using Tor by default in Bitcoin Core is an even easier way of enabling encryption and authentication, and would help protect all Tor users from surveillance. The easiest way to do this would be to make the Debian/Ubuntu packages depend on Tor, and include a install-time script to setup the hidden service. I've verified with the Tor devs that they would welcome the additional load on the Tor network that Bitcoin would add.

@_date: 2014-08-19 20:41:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Emphasis on "likely", at best. Forcing you adversary to rely on uncertain statistics is a huge improvement over the status quo. Secondly your example is of a new block; the more general concern is determining where a given transaction originated. In the best of circumstances determining the origin of a few hundred bytes of days interspersed in dozens of kB/s of buffered data streams is very difficult and expensive even without padding and/or random delay features.
Again, I've spoken to people like Jacob Applebaum about this who have a solid understanding of what the NSA is actually capable of, and they've confirmed the above. Don't let perfect be the enemy of good.
Of course, that's not to say we shouldn't cost-benefit analysis the implementation; not using straight OpenSSL for this is a wise decision. Hence the suggestion of using the existing and tested Tor support to encrypt by default.

@_date: 2014-08-19 20:57:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
That library doesn't exist yet to my knowledge, and more importantly, would increase the attack surface of Bitcoin Core. (much like using OpenSSL for straight SSL support would)
Also, my proposal of adding Tor support to the Debian packages can be implemented in a relatively short install time script; no code changes required.

@_date: 2014-08-19 21:14:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Don't let perfect be the enemy of good.
You realize that by your own definition even the NSA is mostly a "weak passive attacker" They do *not* have the ability to attack more than a small, targeted, subset of connection for both technical and political reasons. For starters, MITM attacks are easily detected - "Bitcoin network attacked by unknown agents! Has your ISP been compromised?" would make for great headlines and would soon see the problem fixed both technically and politically.
In any case, my suggestion of enabling hidden service support by default adds both encryption and reasonably good authentication.

@_date: 2014-08-19 21:27:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Hence my suggestion of separating that surface by using the standalone Tor binary, which runs under a different user to the Bitcoin Core binary.
First of all, without encryption we're leaking significant amounts of information to any passive attacker trying to trace the origin of Bitcoin transactions, a significant privacy risk.
Secondly the upcoming v0.10's fee estimation implementation is quite vulnerable to Sybil attacks. Authentication and encryption are needed to make it secure from ISP-level targeting to ensure that your view of the network is representative. Tor support used in parallel with native connection is ideal here, as neither the Tor network nor your ISP alone can Sybil attack you. It's notable that Bitcoinj has already implemented Tor support for these same reasons.

@_date: 2014-08-23 10:32:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reconsidering github 
Git commits serve two purposes: recording public history and
communication.  While for the purpose of recording history immutable
commits make sense, for the purpose of communicating to other developers
what changes should be added to that history you *do* want the mutable
commits that git's rebase functionality supports. Much like how
university math classes essentially never teach calculus in the order it
was developed, it is rare indeed for the way you happened to develop
some functionality to be the best sequence of changes for other
developers to understand why and what is being changed.
Anyway, just because mercurial is designed around the assumption that
commit history is immutable doesn't mean it actually is; an attacker can
fake a series of mercurial commits just as easily as they can git
commits. The only thing that protects against history rewriting is
signed commits and timestamps.
The easiest and most useful way to achieve that would be to have a
formal program of code review, perhaps on a per-release basis, that
reviewed the diffs between the previous release and the new one. Master
repos in this scenario are simply copies of the "master master" repo
that someone has manually verified and signed-off on, with of course a
PGP signature.
If you feel like volunteering to maintain one of these repos, you may
find my Litecoin v0.8.3.7 audit report to be a useful template:

@_date: 2014-08-23 18:45:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reconsidering github 
What I would really like is a frontend and/or integration to Git/Mercurial that
A "bitcoin transaction" can't by itself serve as a signature, as there
isn't any way to link the transaction to what you actually care about -
a human being - without additional infrastructure. You may find it
helpful to reflect back upon your 2nd and 3rd year courses on
post-modernism and semiotics: Is a keypair in a public key cryptography
system what is being signified, or is it merely a (posssibly false)
If you just want to timestamp a git commit you can timestamp it in the
Bitcoin blockchain. I have the code to do so in my python-bitcoinlib:
    examples/timestamp.py To check timestamps the following should work, although I haven't tried:
    bitcoind searchrawtransactions You do need the searchrawtransactions patch. I've personally timestamped
most of the git tags for releases this way.
PGP of course has vast amounts of identity infrastructure already
developed for it, infrastructure that simply doesn't exist for "Bitcoin
In any case you'll be happy to know that secp256k1 has been added to the
GPG development branch, which means you can sign your code with a ECDSA
key corresponding to a Bitcoin address if you wish too.

@_date: 2014-08-23 18:51:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
Mike is correct here: It *might* be public information, and probably
won't be. We already can give weak assurance that it probably won't be
against many weaker attackers, simply because getting lots of IP
addresses is moderately expensive, and in the future additional methods
will be developed and deployed.

@_date: 2014-12-04 20:43:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] Serialised P2SH HD chains 
Hash: SHA256
It's quite common to run into situations where the payee is *not* online. Similarly requiring them to be online is a security risk and defeats many ways of authenticating payment addresses. This stuff isn't evident in trivial consumer<->merchant use-cases, but is very common in anything else. For instance, consider the case of moving funds from a hot wallet or cold, and vice-versa.
Luke-Jr: sounds like some of the ideas I've been playing around with for generalised stealth addresses, using a declarative template scheme to avoid specifying scriptPubKey formats too explicitly. (though obcs k-anon set issues)

@_date: 2014-12-12 09:05:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Setting the record straight on 
While not a new concept proof-of-publication is receiving a significant
amount of attention right now both as an idea, with regard to the
embedded consensus systems that make use of it, and in regard to the
sidechains model proposed by Blockstream that rejects it. Here we give a
clear definition of proof-of-publication and its weaker predecessor
timestamping, describe some usecases for it, and finally dispel some of
the common myths about it.
What is timestamping?
A cryptographic timestamp proves that message m existed prior to some
time t.
This is the cryptographic equivalent of mailing yourself a patentable
idea in a sealed envelope to establish the date at which the idea
existed on paper.
Traditionally this has been done with one or more trusted third parties
who attest to the fact that they saw m prior to the time t. More
recently blockchains have been used for this purpose, particularly the
Bitcoin blockchain, as block headers include a block time which is
verified by the consensus algorithm.
What is proof-of-publication?
Proof-of-publication is what solves the double-spend problem.
Cryptographic proof-of-publication actually refers to a few closely
related proofs, and practical uses of it will generally make use of more
than one proof.
Prove that message m has *not* been published. Extending the above real
world analogy the court can easily determine that a legal notice was not
published when it should have been by examining newspaper archives. (or
equally, *because* the notice had not been published, some action a
litigant had taken was permissable)
No, with some precautions. This myth is closely related to the above
idea that the data must be globally meaningful to be useful. The colored
coin and Zerocash examples above are cases where censoring the
publication is obviously impossible as it can be made prior to giving
anyone at all sufficient info to determine if the publicaiton has been
made; the data itself is just nonces.
In the case of encrypted data the encryption key can also often be
revealed well after the publication has been made. For instance in a
Certificate Transparency scheme the certificate authority (CA) may use
proof-of-publication to prove that a certificate was in a set of
certificates. If that set of certificates is hashed into a merkelized
binary prefix tree indexed by domain name the correct certificate for a
given domain name - or lack thereof - is easily proven. Changes to that
set can be published on the blockchain by publishing successive prefix
tree commitments.
If these commitments are encrypted, each commitment C_i can also commit
to the encryption key to be used for C_{i+1}. That key need not be
revealed until the commitment is published; validitity is assured as
every client knows that only one C_{i+1} is possible, so any malfeasance
is guaranteed to be revealed when C_{i+2} is published.
Secondly the published data can be timelock encrypted with timelocks
that take more than the average block interval to decrypt. This puts
would-be censoring miners into a position of either delaying all
transactions, or accepting that they will end up mining publication
proofs. The only way to circumvent this is highly restrictive
Proof-of-publication is easier to censor than (merge)-mined sidechains

@_date: 2014-12-13 02:34:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Near-zero fee transactions with hub-and-spoke 
From the So-Obvious-No-one-Has-Bothered-to-Write-It-Down-Department:
tl;dr: Micropayment channels can be extended to arbitrary numbers of
parties using a nearly completley untrusted hub, greatly decreasing
transaction fees and greatly increasing the maximum number of financial
transactions per second that Bitcoin can support.
So a micropayment channel enables a payor to incrementally pay a payee
by first locking a deposit of Bitcoins in a scriptPubKey of the
following form:
    IF
         CHECKLOCKTIMEVERIFY OP_DROP
    ELSE
         CHECKSIGVERIFY
    ENDIF
     CHECKSIGVERIFY
(obviously many other forms are possible, e.g. multisig)
Once the funds are confirmed, creating txout1, the payor creates
transactions spending txout1 sending some fraction of the txout value to
the payee and gives that half-signed transaction to the payee. Each time
the payor wants to send more money to the payee they sign a new
half-signed transaction double-spending the previous one.
When the payee is satisfied they can close the channel by signing the
most recent, highest value, tx with their key, thus making it valid. If
the payee vanishes the payor can get all the funds back once the timeout
is reached using just their key.
Since confirmation is controlled by the payee once the initial deposit
confirms subsequent increases in funds sent happen instantly in that the
payor can not double-spend the input until the timeout is reached.
(there's another formulation from Jeremy Spilman that can be almost
implemented right now using a signed refund transaction, however it is
vulnerable to transaction mutability)
Hub-and-Spoke Payments
Using a nearly completely untrusted hub we can allow any number of
parties to mutually send and receive Bitcoins instantly with near-zero
transaction fees. Each participant creates one or two micropayment
channels with the hub; for Alice to send Bob some funds Alice first
sends the funds to the hub in some small increment, the hub sends the
funds to Bob, and finally the hub gives proof of that send to Alice. The
incremental amount of Bitcoins sent can be set arbitrarily low, limited
only by bandwidth and CPU time, and Bob does not necessarily need to
actually be online. The worst that the hub can do is leave user's funds
locked until the timeout expires.
Multiple Hubs
Of course, hubs can in turn send to each other, again in a trustless
manner; multiple hops could act as a onion-style privacy scheme. The
micropayments could also use an additional chaum token layer for
privacy, although note that the k-anonymity set involves a trade-off
between privacy and total # of Bitcoins that could be stolen by the hub.
Of course, in general the micropayment hub breaks the linkage between
payor and payee, with respect to the data available from the blockchain.
Capital Requirements
A business disadvantage with a hub-and-spoke system is that it ties up
capital, creating a tradeoff between fees saved and Bitcoins tied up.
How exactly to handle this is a business decision - for instance opening
the micropayment channel could involve a small initial payment to
account fo rthe time-value-of-money.
Embedded consensus/Colored coins
Note how many embedded consensus schemes like colored coins are
compatible with micropayment channels. (though have fun figuring out who
deserves the dividends!)

@_date: 2014-12-14 23:17:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] Setting the record straight on 
You're quite mistaken actually. One of the first things to come out of
my research as Mastercoin's Chief Scientist - indeed after a week on the
job - was how to safely upgrade embedded consensus systems in a
decentralized fashion:
 at lists.sourceforge.net/msg03890.html
To recap, where valuable scarce tokens are concerned we want to ensure
that an attacker can't use a fork caused by an upgrade to double-spend
tokens. We solve this problem by ensuring that when a token visible to
version V_i is spent in a V_{i+1} client, the token appears spent to
version V_i clients as well. This is easy to accomplish by a "split
transaction" scheme that separates all operations into separate
"increment" and "decrement" operations.
The simplest example of this principle in action is colored coins, which
are certainly an example of an embedded consensus system. Colored coin
implementations naturally ensure that all versions of the system see a
token getting spent the same way - the corresponding txout is spent! So
long as changes to the coloring rules are handled such that only one set
of rules - one version - can apply to a given txout spend we get
anti-doublespend protection.
The second aspect of the problem is the social/political implications of
upgrades - because embedded consensus systems don't outsource trust they
very clearly require the co-operation of the economic majority in an
upgrade. For instance if the community has two competing proposals for
how to upgrade version V1 of Counterparty, V2a and V2b, choosing to move
your tokens to either version becomes a vote with serious economic
consequences. In fact, it's quite possible that a community would choose
to simply fork into two different systems each offering a different set
of features. Equally in the event of such a fork someone can create a
third version, V3, that recombines the two incompatible forks. Again,
anyone who agrees with version V3 can choose to move their tokens to it,
healing the fork.
Arguably this process of handling forks by direct economic voting is
significantly *more* decentralized than Bitcoin's soft-fork mechanism as
adoption of a new version is something all participants in the system
play a part in equally. (as measured by economic stake) Of course, it
will lead to sometimes ugly political battles, but that's simply part of
the cost of having democratic systems.

@_date: 2014-12-14 23:52:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Setting the record straight on 
I covered this in my original post: 1-way-pegs allow the creation of new
valuable tokens without those tokens being useful for speculation.
To recap, a 1-way-peg allows the conversion of Bitcoins to another token
by provably destroying the Bitcoins, thus capping the maximum possible
value of that token and ensuring the token can-not become an investment.
For owners of these tokens they can convert them back to Bitcoin by
selling them at a discount to buyers who would otherwise be able to
purchase them via provable destruction. A pragmatic implementation may
wish to make obtaining the token via destruction option unattractive
compared to obtaining them through trade by incorporating a time delay
into the destruction process to encourage liquidity. (interestingly a
natural outcome of an announce-commit sacrifice-to-fees scheme)
Of course even without 1-way-pegs there's a much more important issue
with your objection: worrying about creating new artificial scarcity
races while innovating is fundementally a *moralistic* and *regulatory*
concern that has no little if any bearing on whether or not the systems
created are useful and secure. It's also an objection that raises
serious questions about conflicts of interest between giving accurate
and honest technical advice and promoting ways of using Bitcoin that
will drive the price up.
A number of mechanisms for detecting divergence are possible in embedded
consensus systems, some of them even natural outcomes. For instance
transactions can contain a hash of the previous consensus state, thereby
creating an indicator of consensus measured in terms of economic stake.
Extending that idea many anti-censorship proposals are to use such state
hashes as encryption keys - if you are out of consensus you won't even
see the transaction. (and you can't be double-spent either if
implemented correctly; see my other reply to this thread today)
FWIW usually Satoshi's solution is described as a hack, sometimes as an
elegant hack.
Indeed I did, which is why I worked out a better way to do upgrades
almost a year ago:
 at lists.sourceforge.net/msg06578.html
The quality of Counterparty's software engineering has no bearing on
whether or not the underlying idea is a good one; you wouldn't say ring
signatures are an inherently bad idea just because the CryptoNote
implementation of them is atrocious.

@_date: 2014-12-14 23:59:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Setting the record straight on 
I prefer to make robust arguments; if I can start with accepting that
95% of what my opponents say is true, yet still end up being correct,
all the better!
My personal opinion is that what Mastercoin has started will turn the
world on its ear, but I'd be surprised if the succesful implementations
of the underlying ideas come from that team. But there's nothing
surprising about that - when was the last time you used Netscape
Navigator, let alone NCSA Mosaic?

@_date: 2014-12-15 07:39:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] Merged mining a side chain with proof of 
It's not a new idea actually - I outlined a system I eventually called
"zookeyv"? based on the notion of sacrificing Bitcoins to achieve
consensus a year and a half ago on  The discussion
started here and continued for a few days:
I later wrote up the idea in the context of adding Zerocoin to Bitcoin:
 at lists.sourceforge.net/msg02472.html
For key-value mapping I eventually decided that the system didn't
necessarily need to be a strict linear blockchain - a directed acyclic
graph of commitments had advantages as there needed to be less
syncronization between transactions. This also means that the graph
doesn't necessarily need to be revealed directly in the blockchain,
exposing it to miner censorship. OTOH revealing it makes it easy to
determine if an attacker larger than you exists. These days I'd suggest
using timelock crypto to defeat miner censorship, while ensuring that in
principle consensus over all possible parts of the chain can eventually
be reached.?
Proof-of-sacrifice for consensus has a few weird properties. For
instance you can defeat attackers after the fact by simply sacrificing
more than the attacker. Not unlike having a infinite amount of mining
equipment available with the only constraint being funds to pay for the
electricity. (which *is* semi-true with altcoins!)
As for your specific example, you can improve it's censorship resistance
by having the transactions commit to a nonce in advance in some way
indistinguishable from normal transactions, and then making the
selection criteria be some function of H(nonce | blockhash) - for
instance highest wins. So long as the chain selection is based on
cumulative difficulty based on a fixed target - as is the case in
Bitcoin proper - you should get a proper incentive to publish blocks, as
well as the "total work information rachet" effect Bitcoin has against
1) In honor of Zooko's triangle.
2) This doesn't necessarily take as much work as you might expect: you
   can work backwards from the most recent block(s) if the scheme
   requires block B_i to include the decryption key for block B_{i-1}.

@_date: 2014-12-15 07:47:30
@_author: Peter Todd 
@_subject: [Bitcoin-development] Recent EvalScript() changes mean 
BtcDrak was working on rebasing my CHECKLOCKTIMEVERIFY? patch to master a few
days ago and found a fairly large design change that makes merging it currently
impossible. Pull-req  specifically commit c7829ea7, changed the
EvalScript() function to take an abstract SignatureChecker object, removing the
txTo and nIn arguments that used to contain the transaction the script was in
and the txin # respectively. CHECKLOCKTIMEVERIFY needs txTo to obtain the
nLockTime field of the transaction, and it needs nIn to obtain the nSequence of
the txin.
We need to fix this if CHECKLOCKTIMEVERIFY is to be merged.
Secondly, that this change was made, and the manner in which is was made, is I
think indicative of a development process that has been taking significant
risks with regard to refactoring the consensus critical codebase. I know I
personally have had a hard time keeping up with the very large volume of code
being moved and changed for the v0.10 release, and I know BtcDrak - who is
keeping Viacoin up to date with v0.10 - has also had a hard time giving the
changes reasonable review. The  pull-req in question had no ACKs at all,
and only two untested utACKS, which I find worrying for something that made
significant consensus critical code changes.
While it would be nice to have a library encapsulating the consensus code, this
shouldn't come at the cost of safety, especially when the actual users of that
library or their needs is still uncertain. This is after all a multi-billion
project where a simple fork will cost miners alone tens of thousands of dollars
an hour; easily much more if it results in users being defrauded. That's also
not taking into account the significant negative PR impact and loss of trust. I
personally would recommend *not* upgrading to v0.10 due to these issues.
A much safer approach would be to keep the code changes required for a
consensus library to only simple movements of code for this release, accept
that the interface to that library won't be ideal, and wait until we have
feedback from multiple opensource projects with publicly evaluatable code on
where to go next with the API.
1) 2)

@_date: 2014-12-16 07:36:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] Merged mining a side chain with proof of 
It's notable that blockstream hasn't published much if anything concrete
on what exactly you'd use merge-mined sidechains for; they're even worse
than Ethereum in that regard.
It's even better than that: if an attack does happen, the participants
in the consensus system have an incentive to defend against it to
maintain the value of their tokens. Proof-of-burn allows that defense to
be in response to a threat, and essentially unlimited in size.
So now any attacker knows that if they launch an attack in theory the
response could be as strong as the value of the system itself.
This can be improved upon with systems that allow the tokens to be
burned, "internal" proof-of-burn. This suffers from "nothing-at-stake"
vulnerabilities to an extent, OTOH within the context of the system it
is a true sacrifice of value; probably not a big deal in a zookeyv-style
block-DAG where multiple lines of history can be combined. Here the
incentives of the defenders are even more strongly tipped towards
burning their value to preserve the system, not unlike
replace-by-fee-scorched-earth thinking.

@_date: 2014-12-20 09:48:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between Proof-of-Publication 
Gregory Maxwell recently pointed out to me in private conservation that
there potentially existed a fundemental disagreement between him and I
on our philosophical approaches to blockchains, in that he prioritised
the notion of the blockchain as an anti-replay oracle, and I prioritised
it as a publication layer. Here I'll talk about the differences and
simularities between those two approaches.
What's Anti-Replay?
We have some action - perhaps spending a txout - and we only want it to
be possible for that action to happen once; we don't want it to be
possible for that action to be replayed again. This is inherently not
possible with cryptography alone as cryptography is just math and any
mathematical calculation can be repeated with different parameters.
What's an Anti-Replay Oracle?
We need the following primitives operating on message m, pubkey p, and a
valid signature sig1 for m, p:
    AntiReplaySign(m, p, sig1) -> sig2
    VerifyAntiReplaySig(m, p, sig2) -> True or False
Additionally once AntiReplaySign() has been used once for a given pubkey
it is impossible to re-run the primitive on a different message m'. This
is of course impossible to implement with math alone, but we can
implement it with a trusted third party. For instance Carol can perform
the AntiReplaySign operation and make the promise that she will only
ever perform it once for any given (m,p) tuple.
Maxwell points out in CoinWitness? that the anti-replay oracle is
sufficient to implement a digital currency. So long as the trusted
oracle, or majority of a federation of trusted oracles, is honest coins
cannot be double-spent and can be securely passed from owner-to-owner
with an ever-growing transcript? proving each valid spend.
i) The transcript is needed in this model only because the oracles do
   nothing more than promise to never sign a message twice; it can be
   removed if the oracles additionally validate transactions in some
   way.
The Blockchain as an Anti-Replay Oracle
In Bitcoin miners act as a trusted anti-replay oracle. If they follow
the Bitcoin protocol faithfully for any given txout one and only one
valid scriptSig will ever be accepted into the blockchain. Thus the
spend of a txout is a valid anti-replay-protected signature, and the
validity of that signature can be verified by SPV clients with a merkle
path to the block headers.
Using proof-of-publication to prove non-replay
Given a secure proof-of-publication? system we can prove non-replay. We
define a valid signature as both being published on that system, as well
as there existing no other valid signature. (proof-of-non-publication)
An attempt to fraudulently create a second signature will either fail
the first test - not being published at all - or will fail the second
test - not being able to prove no other valid signature exists.
Thus we see that proof-of-publication can be used to securely audit the
honesty of an anti-replay oracle, resulting in secure anti-replay
protection without the requirement of trust.
However the converse is not possible: anti-replay cannot be used to
implement proof-of-publication. Knowing that no conflicting message
exists says nothing about who be in posession of that message, or
indeed, any message at all. Thus anti-replay is not sufficient to
implement other uses of proof-of-publication such as decentralized
Anti-replay in place of proof-of-publication to secure audit logs
The author's proof-of-concept Uniquebits? allows Alice to prove to Bob
that some set of records R_i that she has given to Bob is the same set
of records she has given to everyone else - that is no R_i' exists.
Secondly Alice can update the records producing R_{i+1}, and Bob can
determine if such an update exists.
Currently Uniquebits uses proof-of-publication via the Bitcoin
blockchain directly for both guarantees. It could however make use of
the anti-replay function provided by Bitcoin to satisfy the first
requirement with the following protocol:
0) Alice publishes record set R_i such that H(T_i + n_i) is committed in
   R_i, where T_0 is a txout she controls, and n_i is a nonce.
1) Alice creates T_{i+1}, another txout that she controls, and nonce
   n_{i+1}
2) Alice creates R_{i+1} which commits to H(T_{i+1} + n_i)
3) Finally to publish R_{i+1} she spends T_i in a transaction X_{i+1}
   that commits to R_{i+1} (e.g. in an OP_RETURN output, or with
   pay-to-contract?/sign-to-contract)
This process can be repeated again indefinitely, starting at step When Alice wants to prove to Bob - who has R_i - she simply gives him a
SPV proof that transaction X_{i+1} exists in the blockchain along with
n_i. This proves that T_i was spent, which can only happen once, and
that it committed to R_{i+1}. As the output can only be spent once it is
not possible to create a valid-looking R_{i+1}'
However to prove to Bob that R_{i+1} is the most recent set of records
Alice still needs to use proof-of-publication, by showing him txout
T_{i+1} is unspent.
Case study: Fidelity-bonded Ledgers/Federated Sidechains
The author's Fidelity-Bonded Ledgers? and the more general idea of
Federated Sidechains? both describe the notion of a trusted third party,
possibly implemented as a federated majority set, who guarantees the
correct maintenance of some financial ledger according to some set of
rules. Coins can be moved to/from the ledger by payment to a specific
set of scriptPubKey's. Federated sidechains proposes that the
scriptPubKey simply be a n-of-m multisig; fidelity-bonded ledgers
proposes new opcodes that allow redemption via proof-of-fraud.
In any case someone relying on a transaction within the ledger itself
still needs to be able to audit that their view of the ledger is the
same view everyone else sees; in short that there do not exist
double-spends on the ledger. The author's fidelity-bonded ledgers paper
proposed that the ledger be made available over a Tor-accessible website
to prevent selective censorship. The federated sidechains proposal is
mute on this issue.
As the state of the ledger is a specific instance of the more general
set of records problem that Uniquebits solves as can use the same
principles for fidelity-bonded ledgers/federated sidechains. The third
party periodically publishes the ledger state to the Bitcoin blockchain
allowing anyone to detect if their copy of the ledger is incomplete; if
not there may be double-spends in it. Finally proof of such
double-spends can trigger the destruction of a fidelity-bond? and/or
return funds to their rightful owners. (with appropriate opcodes?)
Censorship of the ledger state publications is an issue, however in the
case of financial ledgers with pegged funds we can use the pegged funds
themselves in the publication. Censoring those publications by
preventing the designated txouts from being spent then becomes
equivalent to blacklisting funds. This requires a majority of hashing
power supporting the blacklist, and is a highly politically charged
issue? in the Bitcoin community.
1) Really Really ultimate blockchain compression: CoinWitness,
   Gregory Maxwell, Aug 19th 2013, Accessed 2014-12-20,
   2) Setting the record straight on Proof-of-Publication,
   Peter Todd, Dec 12th 2014,
    at lists.sourceforge.net/msg06570.html
3) Decentralized digital asset exchange with honest pricing and market depth,
   Peter Todd, Feb 9th 2014,
   4) Uniquebits, Peter Todd, Accessed 2014-12-20,
   5) Homomorphic Payment Addresses and the Pay-to-Contract Protocol,
   Ilja Gerhardt and Timo Hanke, Dec 13th 2012,
   6) Fidelity-bonded ledgers, Peter Todd, Feb 25th 2013,
   7) Enabling Blockchain Innovations with Pegged Sidechains,
   Blockstream, Oct 22nd 2014,
   SHA256: 680c71aef9ed578720e25c58fd50de5cdbee755c3800e7601dad9a745ca65cf3,
   8) Fidelity-bonded banks: decentralized, auditable, private, off-chain payments,
   Peter Todd, Feb 23rd 2014,
   9) WARNING: Bitcoin Address Blacklists have been forced into the Gentoo Linux bitcoind distribution by Luke-jr against the will of other core devs. Gentoo maintainers are clueless and not reversing the change.  Boycott Gentoo now.,
   historian1111, Oct 10th 2014, Accessed 2014-12-20,

@_date: 2014-12-21 00:52:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
Do you realise that all those Freimarket's uses are either based on
proof-of-publication, or insecure due to sybil attacks?

@_date: 2014-12-21 01:12:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] Setting the record straight on 
1-way-pegs don't require the Bitcoin protocol to change; 2-way-pegs do.
No, they're in favor of systems that are client-side validatable vs.
systems that either allow anyone with sufficient hashing power to steal
coins *or* require "moon-math" that isn't yet available to production
But again, all these discussions about scarcity are fundementally
*moral* arguments that have no bearing on what's actually the most
appropriate solution for an *individual* problem.
In a decentralized system filled with anonymous actors telling people
"stop doing that! it's bad!" on reddit has pretty severe limitations in
trying to convince people to act against their own best interests.
I think you think consensus in Bitcoin is more "magical" than it really
is; Bitcoin is just code running on computers; consensus isn't really
incentivised at the protocol level beyond "screw it up and you'll lose
Embedded consensus systems are no different: screw up consensus and
you'll lose money in a variety of ways.
No it can't - the transactions are in the blockchain so the sybil attack
has to attack the host system as well.
In any case, keep in mind all of this is in the context of tradeoffs:
for a different and sometimes more fragile consensus mechanism embedded
consensus gets immunity to attack by miners. You're trading off one type
of fragility for another - I'd much rather take the "one-time" fragility
inherent in having to write really solid software than the ongoing
fragility of always being vulnerable to miners.
Notably this is the exact same tradeoff taken elsewhere by the majority
of the crypto world.

@_date: 2014-12-21 02:01:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
Right, so Freimarkets is delibrately insecure.
Best of luck on that.
Read my paper? - proof-of-publication is what allows you to detect
front-running robustly within certain parameters. Protecting against
that is widely considered to be a very important goal by people actually
in finance, to the point where I've had discussions with people where
anti-front-running protection might be the *only* thing they use a
decentralized system for.
1) Decentralized digital asset exchange with honest pricing and market depth,
   Peter Todd, Feb 9th 2014,

@_date: 2014-12-21 10:22:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
With the blockchain you prove the message in in the blockchain; anyone
in posession of the blockchain will be in posession of the message.
Secondly determining if you are in posession of the blockchain is
possible subject to the usual considerations about attacker size,
whether or not your local clock is up-to-date, etc.
I'm not sure you understand what an anti-replay system is; data isn't
written to them; they're an abstract mathematical model that may be
actually implemented in a variety of ways.
Now it is true that any conceivable implementation must involve some
type of storage, but that storage can easily 100% local to the
anti-replay oracle and need not store the data itself. For instance a
trusted computer in a vault that maintains an extremely large bloom
filter of previously used keys would be a perfectly reasonable
Wait, where did I say "ownership of the message"? What you quoted above
says *posession*, which != ownership.
You're confused about what an anti-replay system actually is - you're
really talking about a specific implementation of one based on
proof-of-publication, not the concept itself.

@_date: 2014-12-21 10:29:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
There's no need to get into the specifics of crypto math so early; you
can just as easily and only slightly less efficiently obtain the same
result with a few extensions to the Bitcoin scripting system to verify
ECDSA signatures directly.
The interesting question is how "risky" this actually is? Sybil attacks
are reasonably easy to pull off, and users have little incentive to
validate if 99% of the time everything works, so you don't want to
create a system where an actual attack will likely go undetected.
Talking about the low level details of how double-spend punishment is
actually detailed is just premature optimization.
As usual in Bitcoin, the hard part is *not* the math.

@_date: 2014-12-21 10:32:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
Sybil attacks leading to front-running.
You may not be aware of this, but not being able to get the best price
due to a sybil attack *is* considered to be a security issue by the
users of these systems.
It's superfluous until you have real businesses actually using these
Among other things, ever noticed how this incentivises people to sybil
attack the entire system? Not good.

@_date: 2014-12-21 11:07:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
Incidentally, SIGHASH_SINGLE is just as usable in embedded consensus;
it's not specific to native assets.
Like I said the last time this issue was discused on the mailing list,
it's silly to think the seller of an asset starts off with a specific
price they want to sell it at and is happy no matter what happens or how
it gets fufilled. In the real world sellers and buyers want to know
they're connected to actual sellers and buyers - not sybil attackers
trying to shave off a margin for themselves - and are willing to pay a
premium for that. Note all the hatred and vitrol directed towards
high-frequency traders...
How *much* of a premium is an interesting question, and depends a lot on
the specific scenario. For instance I fully expect to see a whole
variety of mediums become used for the proof-of-publication needed,
ranging from directly on a major blockchain to minor/less secure
blockchains like Bitmessage over treechains to centralized-but-audited
proof-of-publication schemes - AKA centralized exchanges - to standard
exchanges. Point is, the concept of proof-of-publication makes these
tradeoffs and options available and lets end-users pick the right one
for their needs.
Accurate unbiased price information is worth money. In systems that
allow third-parties to republish asset bids and offers we'll even see
third-parties republishing bids and offers from less secure systems to
more secure systems to get better price discovery.
I gotta ask, have you actually run the design and tradeoffs of
Friemarket's past actual finance types? I have, and it's remarkable how
excited many of them are about cryptographically provable fair price

@_date: 2014-12-21 13:51:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] one-show signatures (Re: The relationship 
Introducing a bunch of clever ECDSA math doesn't change the fact that
the clever math isn't what is preventing double-spending, clever
economics is. Just like Bitcoin itself.
No sense getting people potentially confused by a bunch of complex
equations that aren't relevant to the more fundemental and much more
important principle that math alone can't prevent double-spending.

@_date: 2014-12-21 19:11:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] The relationship between 
Andrew Miller asked me to publish the following to the mailing list on his
behalf: (
One of the main points in this note is that you can use a
"proof-of-publication" system to implement an "anti-replay" system.
However this isn't true - at least not given the description of
proof-of-(non)-publication in 2) and the definition of "anti-replay"
given here.
In 2), proof-of-*non*-publication allows you to prove that *some
specific message* has never been published. You can imagine having a
function ProveNotPublished(m), which proves "message m was not
However, the anti-replay mechanism is about proving that *no* message
satisfying some property has been published. Hence
VerifyAntiReplaySig(m, p, s) checks that "for all possible messages m'
(distinct from m), AntiReplaySign(m', p) has not been called."
This isn't *just* splitting hairs, this distinction is actually
relevant for analyzing several cryptocurrency designs. You can imagine
extending the definition of proof-of-(non)-publication to take in some
predicate P, so that you can prove "no message m such that P(m) holds
has ever been published." However, to do this efficiently requires
anticipating some classes of P and building some appropriate indices.
- As a baseline, as long as you have the whole blockchain available,
you can scan through the entire blockchain and evaluate P for every
transaction, but this is pretty inefficient.
- Other tradeoffs are available if you are willing to trust some
(quora of) servers to maintain indices for you
- Bitcoin's UTXO set effectively supports a predicate for each txout,
where P(x) = "x is a valid tranasction that spends "
- Ethereum contracts, in a sense, allow for general purpose contracts
to 'build-your-own" index. On the other hand its key-value store
doesn't support range queries, so it's not necessarily "universal" or
as expressive as SQL, for example.
But the point isn't to argue about design choices and tradeoffs in
this thread. The main point I want to make is:
*Indexes and Validation Matter!*
The classic "proof-of-publication" system is to embed opaque data (as
far as bitcoin miners are concerned) in transactions using OP_RETURN.
A significance of establishing "proof-of-publication" as a universal
underlying primitive is that this OP_RETURN trick is then sufficient
for anything you might want. But part of what Bitcoin provides is
indexing and validation/exclusion, and this is important for
supporting efficient anti-replay proofs. Proof-of-(non)-publication
alone isn't sufficient for this.

@_date: 2014-12-29 11:39:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Cartographer 
Hash: SHA256
A big one is the privacy is way too good: every DNS request goes through multiple levels of caching and indirection, so there's no way to figure out who made the request to subject them additional targeting.
A connection-oriented protocol gets rid of all those protections, giving us seed operators monetisation opportunities like selling usage statistics, per-client targeted results, etc. We recently got rid of all the "call-home" functionality that previously gave this type of insight; a connecyion-oriented seed protocol gives us this right back.
There's also this pesky problem of ISP's censoring DNS results with dumb automated systems to block malware - easily fixed with Gregory Maxwell's suggestion of permuting the results with XOR - but that kind of end-user driven solution really misses out in the needs of other Bitcoin stakeholders like law enforcement and marketing companies.

@_date: 2014-12-29 12:59:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Cartographer 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Yes I agree, Mike shouldn't be making ad-hominim attacks by calling people "a parody"
You'll note my response however carefully avoiding talking about the person who originated the idea, and merely stuck to criticising - via parody - the idea itself.

@_date: 2014-02-01 21:36:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] (space) efficient reusable addr via weil 
One of the main reasons I post to the bitcoin-development mailing list
rather than the forum is because the mailing list is archived by many
different, independent, parties. The forum is not - as an example
archive.org didn't have that URL until I manually told it to archive it.
So I'm taking the liberty of reposting your two posts there below:
[quote author=adam3us link=topic=431756.msg4729682
So have been talking with Greg Maxwell, Peter Todd, Jeremy Spillman,
Mike Hearn, Bytecoin and others about reusable addresses.
There is a summary of the situation here
 at lists.sourceforge.net/msg03792.html
and I had posed th question of whether it was possible to do better at
all with Peter Todd:
[quote author=adam3us on bitcoin-dev]
Now while it would be clearly a very nice win if reusable addresses
could be  made SPV-like in network characteristics and privacy, but we
dont have a plausible mechanism yet IMO.  Close as we got was Greg's
enhancement of my/your "bloom bait"/"prefix" concept to make multiple
candidate baits to provide some ambiguity (still allows elimination,
just slightly less of it).
If we can find some efficient crypto to solve that last one, we could
even adopt them generally if it was efficient enough without needing
interactive one-use address release
and Peter proposed also the related problem of proving something about
that existence or not of a solution to that problem.
I think I have a proof-of-concept solution that proves by example we can
do better in space efficiency, linkability defense and non-interactivity
than my bloom bait, Peter Todds related prefix; and Greg Maxwell's
extended bloom bait described
 at lists.sourceforge.net/msg03705.html.
So the idea is to use an IBE scheme as a building block in analogous way
to my 1996 problem statement for NIFS and 1998 observation that a novel
use for an IBE scheme can provide a generic solution to NIFS, and the
arrival in 2001 of the first efficient / sensible trapdoor steepness (*)
IBE with the introduction of the Weil Pairing problem by Dan Boneh and
Matt Franklin described here
Greg summarized IBE as follows on IRC:
(for those who may) not be familar with IBE stuff: The idea is that the
user has a master private key, which results in a master public key.
Anyone can take a prior block hash and combine it with the master public
key to get a session pubkey which could be used to encrypt a chaincode
included in an OP_RETURN.   Using the master private key the user can
derrive the session private key, which can then be used to recognize
transactions using the same session key.
In IBE (identity based encryption) this is all used a bit differently:
the master keys are held by a CA, and the session ID is your email
address, and now anyone can make a public key for you? but you need the
CA's help to get your private key)
Basically as Greg said your public key is your address (an email
address, a block hash, whatever is convenient and unique) and from that
and the master public key of the IBE server, the server can compute a
private key corresponding to that.  The master public is usually
considered to be a system-wide domain parameter.   Naturally enough
because a side effect of this is that the IBE server can decrypt
everyones email people were never too excited about the prospect.
However my 1998 NIFS observation is by acting as your own IBE server
(each user creates their own master public server key) they can create a
sequence (NIFS) or set (bitcoin reusable address) of public keys with
interesting and publicly derivable properties!
It is my conclusion from 1996 that to solve this with DL directly at
least in the NIFS case appears to be not possible.
So basically the reusable address becomes an IBE public key, the
existing public derivation via DH or EC Elgamal/ECIES or whatever
variant (bytecoins, mine, Peter Todd/Amir Taaki's) arrives at a factor
that can be recovered.  So with my variant (random sender generated
multiplication factor encrypted with ECIES) you could encrypt the factor
with a pub=IBE-extract(master pub, id=previous block hash) using the
previous block hash as the "identity" and the users own self-owned IBE
For Bytecoin & Peter Todd/Amir Taaki EC DH version using input or
auxilliary addresses to save space its not even necessary to send the
factor, its already sent.  So then you send a separate message to do
with secure delegatable filtering, a more secure/more space efficient
bloom filter/prefix replacement, and this is a more flexible structure.
So the secure delegatable filter is you separately add an encrypted
bloom bait Greg suggested (eg 1byte prefix communicated with public
address.)  And you can even combine that with Greg's extended bloom bait
above to add anonymity set within the block.
Consequently you can now securely and very network/space efficiently
securely delegate searching a block by computing the private key for the
IBE pub key that any sender would use for that block, and sending it as
a query to a random (or node-capture defended random selected node).
The node can decrypt the encrypted bloom baits with it, but remains
powerless to correlate with bloom baits to other payments received by
the same user in bother blocks.
(In practice you might need an epoch not block or overlapping test
because the user does not have full assurance of their tx ending up in
the pending block).
About weil pairing, and new hardness crypto risk, this is also the
hardness assumption under some ZK-SNARKs as I think used in zerocash,
and while ZK-SNARK introduces its own complexity/crypto system risk on
top; in my view weil pairing is slightly lower assurance/review not so
widely used relative to EC DL problem.  Anyway the interesting thing to
say about that is in the event this scheme got broken in the future it
falls back to the scheme that is being proposed using prefix.  Ie its no
worse than that from linkability and likely would retain some cost even
if broken-- asymmetric crypto breaks are usually somewhat gradual.
This looks more expensive and non-indexable though I didnt look to see
if there is any ciphertext only or batch precomputation that could be
squeezed out of it.
Obviously its more CPU intensive and some eg fee mechanism to prevent
node DoS could be nice, but it seems to demonstrate a proof by existence
that it is possible to do better.
Finally I think it maybe within possibility to do further than this
because it is not technically necessary to delegate decryption, only to
delegate filtering, which can be a simpler requirement.
(*) There was an earlier scheme by Maurer et al if I recall, but to get
a 1024-bit security margin you had to perform a discrete log attack on a
512-bit prime, so the key generation cost was immense, hence "sensible
trapdoor steepness" thats very shallow in tems of work difference
between setup cost and crypto system strength.
[quote author=adam3us link=topic=431756.msg4732503
[quote author=Mike Hearn link=topic=431756.msg4730986
You would need epochs for another reason. Recall that with Bloom
filtering the remote node is asked for blocks in batches of 500 at a
time and the remote end handles updating the filter as transactions are
matched. This is to avoid the performance hit of a network round-trip
for every block.
I see I dont think I realized that aspect of how bloom query works.  So
you then with IBE-based filtering could send multiple keys, one for each
block; but you are implicitly linked by being in one query, so you'd
just as well mark your key with your preferred epoch size and sender
uses epoch number in the query.
I think Greg is pointing out on IRC that by having a fairly small epoch
you can choose later to go down to that epoch size or scale up by
sending multiple epoch keys in a batch, a privacy/network round-trip
trade off.
Re my other problem with epochs ("In practice you might need an epoch
not block or overlapping test because the user does not have full
assurance of their tx ending up in the pending block") I think that
maybe fixable, if the blocknumber is chosen by the sender, and
communicated in enough bits to be mostly unambiguous in the message.
Then the node can index them by sen block num and no ambiguity.
It could be that another way to partly obscure ownership of queries
would be to relay queries and responses and mix other peoples queries
with your own in a batch, however as we are considering the SPV client
case relaying other peoples queries seems hard to gather query traffic
on demand and to use more bandwidth than it saves relative just issuing
smaller batches.
You could have relaying in the network eg using the embedded Tor but
waiting for queries to mix with adds latency, and suffers flood attacks
on mix-nets (send fake encrypted query traffic to flush out a tx, that
has no-anon set vs the person doing the flooding who can distinguish
their own queries).

@_date: 2014-02-02 06:55:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] (space) efficient reusable addr via weil 
You're explanation is a bit long-winded, so I'll start with a simplified
ECC-based version first and later explain how identity-based encryption
applies to the problem; I have a feeling not many non-crypt-experts
spent the time to figure out what you're talking about; do check if what
I've written below is correct:
So Alice wants to pay Bob, who is bandwidth constrained and frequently
offline. Meanwhile Ivan has a full node, but can't really be trusted.
Meanwhile Eve is busy trying to piece together everyones' financial
Bob publicly publishes three pubkeys, Filter, Recover, and Spend, along
with a short n-bit prefix p. When Alice needs to pay Bob she creates a
ephemeral keypair and uses ECDH *two* shared secrets, n_f and n_r, from
Bob's Filter and Recover pubkeys respectively. She makes a transaction
that pays Bob by deriving pubkey Spend_{n_f} from the Spend and n_r
nonce.  She also uses the Filter nonce and the prefix to derive a
encrypted prefix p'=n_f^p and puts that prefix and the cleartext
ephemeral pubkey in the transaction as data.
When Bob wants to find that transaction he gives the prefix and Filter
secret key to Ivan, who then scans the blockchain. For every transaction
he computes n_f=ECDH(Filter_sec, Ephm_pub), extracts the encrypted
prefix p' from the transaction, and checks if p'=n_f^p If so he gives
that transaction to Bob who can then use his Recover secret key to check
if the transaction was in fact for him. (note how the prefix can
actually always be simply a given length of zeros)
Because Bob's prefix is short Ivan only learns probabalistic information
about what transactions might be Bob's. Eve doesn't know the Filter
secret key, and thus learns nothing at all from the blockchain data. On
the other hand after getting the key once Ivan can forever after get
that probability information about what transactions might be Bob's.
What we'd really like is for there to be some way for Alice to derive a
time-limited Filter pubkey from some public random beacon with value
R_i, such as the Bitcoin blockchain, such that each defined time
interval uses a different key. Bob would then only give Ivan the secret
key(s) for the time interval(s) in question.
Unfortunately ECDSA doesn't have a way to do this. The closest thing
available is BIP32-style key derivation, however it has the property
that given a derived secret key and known master pubkey the master
secret key can be derived. Thus Ivan can simply try every public Filter
key/epoch tweak he knows about until he finds Q,d' st. (d+d')G=Q+d'G
From that he can recover d, reducing the security to where we started.
(or put another way, Ivan can store every (d+d') secret key he is asked
to search with, and test it against every public key he learns about
Identity-based cryptograhy however can do that. Bob publishes a (single)
master public key, and anyone can derive public keys based on that
master key and the random beacon value R_i. Bob can then derive the
corresponding secret key, but unlike with ECDSA, that secret key *can't*
be used to derive the master private key. Having said that, it can of
course be linked to that key, so every query that Bob makes gives Ivan
some knowledge about what transactions might be in Bob's wallet.
Problem is, who the hell has a production-ready Weil pairing library
kicking around? (is this read?  Also,
Weil pairing is not yet trustworthy:
    < gmaxwell> (IMO thats how we should be using pairing in
    cryptosystems: for lower value applications, and solving things that
    can't be solved any other way)

@_date: 2014-02-02 07:26:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] (space) efficient reusable addr via weil 
There needs to be two separate payor pubkeys, which I called elsewhere
the "Filter" and "Recover" pubkeys - the latter I think corresponds to
what you meant by identity key. From those two pubkeys two separate
shared secrets are derived.
The key idea is that you can encrypt a short string of zeros with the
"Filter" pubkey using ECDH and place the resulting "filter bait" in the
transaction. This lets the payor give the secret key corresponding to
that pubkey to a semi-trusted third party. That third party can then
trial decrypt all filter bait seen in transactions in the blockchain,
and every time the decrypted string has a sufficient number of zeros
it's considered a filter pass and the transaction is given to the payor.
For n zero bits one in 2^n transactions will match at random, which sets
your false positive rate.
Basically think of it as a way to outsource the work required for
zero-prefix stealth addresses, but with (less) of a sacrifice of
anonymity compared to just giving the third-party your recovery pubkey.
Identity-based encryption only comes into it because it's nice to be
able to further limit what transactions the server knows about to
specific time intervals rather than forver into the future.
Interestingly both schemes can be used at once - a short public prefix
combined with a second private filter. What's interesting there is that
the public prefix can do a first-pass filtering, with the second private
filter relatively long but still providing plausible deniability - you
can always claim 100% of the matching transactions were false positives
because you didn't receive any funds!
There's no bloom filters involved; as I said before "bloom bait" is a
misleading name. "Filter bait" is a better term given it's a generic
XOR with the ECDH-calculated nonce is fine. (run the nonce though a hash
function first)

@_date: 2014-02-04 08:03:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] bitcoinj 0.11 released, with p2sh, 
The above makes for a great homework problem for budding cryptographers:
Why did the three forms of signature, DKIM, long-lived bitcoin address,
and Official Swiss Government Identity fail to let you actually verify
you have the right code? (but make for great security theater)
Bonus question: Who has the smallest work-factor for such an attack?
Two rewards of 25mBTC for correct responses to each question from a
crypto newbie.
Soft-forking rule change.

@_date: 2014-02-04 08:17:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] bitcoinj 0.11 released, with p2sh, 
Heh, here's another 25mBTC while we're at it:
Why is that a bad idea?
Bonus question: What was I smoking? (hint: where do I live?)

@_date: 2014-02-04 09:46:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] bitcoinj 0.11 released, with p2sh, 
I think we have a winner; as you can see Jeff must be a great father.

@_date: 2014-02-04 11:04:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] bitcoinj 0.11 released, with p2sh, 
You're close, but not quite.
So, imagine you have a merkle tree, and you're trying to timestamp some
data at the bottom of the tree. Now you can successfully timestamp the
top digest in the Bitcoin blockchain right, and be sure that digest
existed before some time. But what about the digests at the bottom of
the tree? What can an attacker do exactly to make a fake timestamp if
the tree is using XOR rather than a proper hash function?

@_date: 2014-02-07 04:21:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] bitcoinj 0.11 released, with p2sh, 
Thanks for the great response! I had about a dozen or so people contact
me with solutions for one or more questions, and even a anonymous
donation of 75mBTC to cover the rewards.
I'll start with my summaries of those solutions:
So as most people correctly guessed, the problem here is that Mike
truncated the git commit hash; normally it's 160 bits long, but he only
gave 48 of those bits. To understand why this is a problem, recall that
what a cryptographic hash does is it takes a arbitrary block of data,
the message, and returns a fixed length bit string, the message digest
or simply digest. With git the message essentially your source code and
commit history, and the digest is the git commit hash. Critically for a
cryptographic hash to be secure the mapping between messages and digests
must be random - it must not be infeasible to find two messages with the
same digest. (this is called a preimage attack)
The problem is that 48 bits just isn't that many bits. An attacker can
take the bitcoinj sourcecode and modify it to do something malicious
like generate private keys insecurely. Then they can keep modifying it
until the last 48-bits of the commit hash match Mike's message. (this
called a partial preimage attack) Each modification has a 1 in 2^48
chance of succeeding. You can calculate the attackers chances exactly
with the Binominal distribution, but a good enough approximation is
they'd have to make about 2^48 attempts.
That's not a very big number! Here's a nice visual comparison of how
long 48 bits is, compared to the partial preimage the Bitcoin network
cracks every 10 minutes:
Literally tens of thousands of times harder. This problem is similar to
password cracking, and they're getting speeds like ten million attempts
per second per CPU core. Just do the math: 2^48/10million/second/core =
46 Core Weeks. Now I can rent 32-core servers at Amazon EC2 for as
little as $0.27 per hour (spot requests) which gives me a cost for the
attack of about $100; my time to actually do it will cost more than
But that calculation is missing the point; the extra bytes are really
cheap, so you can just use a simple rule of thumb: If a partial-preimage
attack is what you are trying to prevent, then in cryptography an
accepted number of bits to use is 128. Maybe just 80 bits would be
enough, or even just 64 bits, but pretty much everyone agrees 128 is
safe and conservative. But read on, because even 128 bits isn't safe
enough against another type of attack...
A second issue that a few people noticed was that Mike just said
"Andreas Schildbach's GPG key", rather than specifying the fingerprint
of the key. By now I'd expect Mike to be confident as to what PGP key
is actually the correct one for the human Andreas Schildbach, so there's
absolutely no reason not state what that key is, either in the release
notes, or by signing the key with Mike's (non-existant?) PGP key.
Preferably both.
No-one got this one correct or even tried!
What if Mike Hearn himself were the attacker? For instance, US officials
wanted to shutdown the gambling site SatoshiDice, which reportedly uses
the bitcoinj library. One way to do this would be to seize the funds
held by SatoshiDice, putting them out of business. If they could trick
SatoshiDice into using a version of bitcoinj with a broken PRNG, they
could simply wait until funds had moved into addresses generated by that
PRNG, and/or ECC signatures were created with a known k value. (leaking
the private key)
But how to pull that off? The bitcoinj sourcecode is public, so they
can't just backdoor bitcoinj directly - everyone would find out. What
they need is a way to trick SatoshiDice into installing a bugged
version, without leaving any evidence.
With Mike Hearn's help they can calculate a pair of hashes, each with a
n-bit prefix, but with only sqrt(2^n) work. This is called a
second-preimage attack, and takes advantage of the birthday paradox,
which as you may recall, is that in a room of just 23 people, there is a
50% chance that two them share the same birthday.
Now the US government continually generates pairs of slightly different
git commits, one being the honest code, the other with the backdoor.
Generating these pairs is simple enough, just change something
insignificant like the exact timestamp of last few commits. Every hash
generated is saved in a big hashtable, as well as compared with all
pre-existing hashes. In this case they'll just need to do about 2^24
tries to succeed, only 24 million attempts, which is frankly pretty
Now that they have two collissions they have Mike release bitcoinj as
before to the public, and at the same time they intercept the internet
connections of the people suspected to be the SatoshiDice developers.
For the latter a MITM attack is performed, secretly replacing the good
copy of bitcoinj they download with the backdoored copy. The developers
don't notice anything unusual, because both copies appear to have the
same commit hash!
The beauty of this technique is provided the disclosed hash isn't too
long, it's still plausible that a powerful government agency brute
forced the thing even if the backdoored code is leaked. With 48 bits
that's obviously trivial, but even a 64-bit collision could be made at
the cost of only a few million dollars. Thus, Mike Hearn has plausible
deniability and can claim innocence.
Moral of the story is if a second-pre-image attack is a threat, you need
to use a lot of bits. Even a full SHA-1 commit hash is only 160 bits,
which gives sqrt(2^160) or 2^80 security, so anything less than the full
git commit hash is risky. Industry standard is to use at least 160 bits,
and preferably you should always just use the full 256 bits that SHA-256
or similar provides unless you have a really good reason not too.
Brooks Boyd already posted a great writeup, so I'm going to reference
his instead:
 at lists.sourceforge.net/msg03882.html
In closing I think it's important that we all remember we're writing
software that handles money and the incentives to sneak backdoors into
said software are enormous, and every worse, universal. Everyone can
profit pretty directly from stealing cash, so the "Bad guys" we're up
against range from your "Russian hackers" to the US government and
everyone in between.
Fortunately the nature of attacks is that for an attack to succeed,
everything has to go right, but for it to fail, you only need a single
person to notice something is wrong. This is why the Bitcoin Core
development effort consists of multiple people, mutually verifying each
other's work, and signing code with OpenPGP keys that in turn are
verifiable via numerous different paths. Of course, many users will
naturally not bother with that effort and outsoruce their trust to a
single person or certificate authority, but the more advanced users with
more stringent security needs, such as developers at exchanges and big
merchants, can validate the code through the indepdent multiple
independent paths OpenPGP signatures provide. Bitcoinj would do well to
give their users that kind of security.
So in the spirit of community auditing, I'll give one last 25mBTC reward
out; I'll sneak in another obvious security flaw into something I write
in the future, and I want to see if you guys catch it.
As for the winners, I went by timestamp on the first email or other
contact I got, and rewarded better descriptions where it wasn't clear.
First of all I'm awarding the first bonus question to Vitalik, who in
person at the Toronto Bitcoin Meetup noticed the issue immediately. He's
no crypto-newbie, but at that point I had to give it to someone! Jeff
Garzik will receive nothing for his answer, as it would be morally wrong
to encourage further dad jokes.
Brooks Boyd wins for  and thanks for the solid write up! Finally, had a lot of submissions, but the earliest really clear answere was
privately emailed to me. Dunno if they want to be named publicly, but
here's the SHA256 hash of their email address for bragging rights:
Finally, it'd be really awesome to have some concrete examples of git
commits with these preimage and second-preimage attacks applied. So, I'm
pledging 250mBTC to anyone who creates a tool that can run on Ubuntu
Linux that takes two git commits, and brute-forces some not trivially
noticed nonce within those commits - I suggest the timestamp - to make
some subset of their hash collide.
A fast C or C++ inner loop would be ideal - being able to create
reasonably long collisions, best yet against arbitrary bit masks, would
be an excellent way to show people why they need to be careful. Contact
me if you want to take this on.

@_date: 2014-02-09 12:12:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] Embedded consensus system upgrade procedures 
The Problem
We have an embedded consensus system and we want to be able to upgrade
it with new rules. There inevitably will be a transition period where
some users use clients that interpret the new rules, while others only
interpret the old rules. Since we only rely on the host consensus system
for timestamped proof-of-publication the the miner-vote soft-fork
upgrade mechanism;(1) there are no validating miners in the system to
whome trust can be outsourced.
We have a problem: messages encoding actions, such as moving as asset
from one owner to another, can be published on the the blockchain
according to new and old rules simultaneously, double-spending the
asset. Potentially a user with the old v1 software may be tricked into
accepting an asset when the consensus of the v2 software is that the
asset has already been spent, and the v1-visible transaction is invalid.
Split actions into a separate "decrement" and "increment" operations,
and ensure that v1 software can see the "decrement" of a balance, spend
of a transaction output etc. even if it does not see the corresponding
increment operation. This solves the double-spend problem and ensures v1
users can't be ripped off. With obvious analogy to the PoW case, we will
refer to this general principle as a embedded consensus system
Note how with the Colored Coins technology this principle happens
implicitly and with miner validation: colored coins are valid
transaction outputs known to the host consensus system and moving them
from one owner to another is guaranteed to result in the desctruction of
the colored coin from the point of view of any older software version.
Older software that does not support the newer colored coin kernel
specified by the new asset definition will simply see the respective
coins be destroyed in invalid transactions. Note how this implies that
asset definitions created by issuers should be careful to ensure that
kernels chosen should be designed such that the actioned specified by
one kernel can-not be interpreted differently by another; kernels should
be clearly incompatible with each other.
Balance-based systems
Mastercoin is a balance-based system where transactions increment and
decrement balances. Being balance-based, and lacking pruning, an even
simplier "scorched earth" approach will be used where each address is
associated with a maximum version number seen by transactions signed by
the address. Addresses with a max version number higher than what the
software understands are considered to be null and have no value of any
kind. (counterparty would be wise to do the same)
Upgrading implementation
Implementations should record in their databases the blockhash
associated with transactions that were not recognized yet affected the
state of the consensus. For instance a colored coin implementation
should record the blockhash and transaction ID where a given coin was
destroyed in an invalid transaction; after upgrading these "last
transaction understood" markers can be used to replay blockchain data to
arrive at the new consensus.
Similarly in the case of the Mastercoin system balances associated with
addresses that have been frozen should be still allowed to increment so
that replaying blockchain data from the last recognized transaction
arrives at a upgraded consensus.
As an aside, any embedded consensus system would be wise to have a way
of generating a master digest representing the state of the consensus in
the database. The Bitcoin Core gettxoutsetinfo command is a good model,
which provides hash_serialized, a digest representing the entire UTXO
set. In all systems this is useful for ensuring that different
implementations and instances have in fact arrived at a consensus.
1) BIP-16, Pay to Script Hash,

@_date: 2014-02-09 13:04:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
Alex Mizrahi recently outlined a mechanism(1) based on SIGHASH_SINGLE
that allows colored coins and similar embedded consensus system assets
to be securely transferred to another party in exchange for Bitcoins
atomically. In summary his p2p 2-step-trade mechanism operates as
Alice controls a colored txout and wishes to sell it for 1BTC. Bob
wishes to buy that txout.
Alice signs a scriptSig using SIGHASH_SINGLE|ANYONECANPAY for a
transaction with a that time. (albeit a offer floor) single input, the
colored txout, and a single output with a scriptPubKey she controls and
nValue=1 This transaction is not valid as the value out is greater than
the value in.
She gives this partial transaction to Bob. He can now complete the
transaction by providing one or more inputs with a sum value >=1BTC, one
output for the colored coins to be directed to, and optionally any other
outputs required. (for instance for change)
Bob signs his inputs with SIGHASH_ALL and broadcasts the transaction,
completing the trade.
What Alice has signed, the first txin scriptSig, guarantees that if the
colored txout is spent she will receive 1BTC. Meanwhile what Bob has
signed, all other txin scriptSigs, sign the colored input and output,
guaranteeing that he will receive his coin in exchange for his money.
Thus the trade is trust free and atomic.
Decentralized markets and honest pricing
We can extend Mizrahi's 2-step-trade mechanism to create a decentralized
marketplace. First of all, remember that traders wishing to sell their
assets want to be sure that their assets offers reach the 100% of the
audience who may wish to buy said assets; an attacker may try to
manipulate the market to depress the price of an asset by hiding offers
from potential buyers. Similarly buyers want assurance that the offers
they are responding to represent all offers available.
Proof-of-publication(2) offers a solution. Alice can embed her
incomplete transaction as data in a second, valid, transaction. She
broadcasts this secondary transaction to some agreed upon blockchain,
either the one the colored coin is in, or potentially a secondary system
with suitable proof-of-publication security. Bidders such as Bob can now
scan the blockchain for offers with an acceptable price. (the offers can
make use of techniques like prefix filters to allow Bob to only scan
part of the blockchain, although Bob needs to know the status of all
assets of the type he is interested in anyway)
There is still some potential for manipulation with very recent offers,
particularly those embedded in unconfirmed transactions. However
typically markets have a large number of long-standing offers, which in
this case would be committed to the blockchain with one or more
Interestingly such a system can also provide honest historical pricing
information: any offer that goes unfilled for one or more blocks has (in
theory) been honestly published to 100% of those watching the blockchain
at that time. Thus we can assume the unfufilled offers at any
given block height are honest information about the market at that time
The overhead involved involved in Alice publishing the offer is roughly
a doubling of the overall transaction fees consumed. (remember that the
offer transaction is incomplete, and about half the size of the
acceptance transaction)
Application to other embedded consensus systems
Any embedded consensus system can make use of the 2-step-trade mechanism
so long as it is possible to create transactions where spending a single
transaction output moves an asset appropriately.
Unfortunately extending this to circumstances where more than one input
needs to be spent, or more than out output needs to be created, is
difficult. SIGHASH_SINGLE by itself results in a signature where the
index of the output is signed, but the contents - scriptPubKey and
nValue - of all other outputs is not signed. Meanwhile all transaction
inputs are signed and changes to that set, other than modifying the
nSequence value in each CTxIn, is not possible.
If there was a SIGHASH mode that merely truncated vin and vout based on
the index of the scriptSig we could commit to data in either, but
unfortunately we can't do that.
An alternative could be to create a mechanism where some embedded data
signified the creation of a temporary transfer txout, where spending
that txout made the underlying change desired in the consensus state
1) Alex Mizrahi, color kernel design considerations, Jan 7th 2014,
   Colored coins (BitcoinX) mailing list,
   2) Peter Todd, [Bitcoin-development] Disentangling Crypto-Coin Mining:
   Timestamping, Proof-of-Publication, and Validation, Nov 19 2013,
    at lists.sourceforge.net/msg03307.html

@_date: 2014-02-09 13:09:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Embedded consensus system upgrade 
Please, the rule change only can happen if users accept it.
If anything my proposed mechanism makes it even harder for developers to
impose anything by fiat: the spending your digital asset under new rules
decreases the amount available of it to trade with users who chose to
accept only the old rules. Since there is no safety concern involved,
the process is safe for both groups, developers can't plea to the
community that "OMG the sky will fall and you'll be all defrauded if you
don't upgrade right now!!!" Instead they'll be forced to make it clear
that if the community doesn't accept the new rules, whatever assets
you've moved to the new system may become forever worthless.

@_date: 2014-02-09 13:38:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Embedded consensus system upgrade 
Standard Disclaimer: Digital asset transfer systems are fundementally
fancy accounting systems; no amount of code can, by itself, make data
represent a physical or legal entity. Only consensus and/or authorities
in the "real world" can do that. Crypto-currencies are only a partial
exception to that rule, and only because a scarce asset that can be
transferred digitally appears to have potential to be broadly useful.
Those considering investing in or otherwise devoting resources to the
creation of digital asset transfer systems should be warned that their
value in general remains unproven and losing some or all of your
investment is very possible, even probable. I myself have doubts that
these systems serve real-world business needs, but the only way to find
out is to build them and see.
Peter Todd
Chief Scientist
Anyway, the best we can do is build good tools. Dwelling on the
underlying metaphysical nature of what those tools may or may not do
from a social perspective is frankly off-topic on this email list.

@_date: 2014-02-09 15:44:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
I'm told there's probably at least one if not more earlier
attributions/reinventions for the 2-step-trade protocol using
SIGHASH_SINGLE. Please reply with them if you have them so we can give
credit where credit is due.

@_date: 2014-02-09 22:00:48
@_author: Peter Todd 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
You should probably add making CHECKMULTISIG require the dummy value to
be exactly equal to OP_FALSE; verifying that in the transaction itself is
laborious. A more subtle example is we may want both CHECKSIG and
CHECKMULTISIG to fail the transaction if the signature is invalid but
not exactly equal to OP_FALSE; some transaction forms are significantly
more compact if you can have failed signatures, but that's a source of
malleability. (are there counter examples people can think of?)
But as I said on IRC, I'm a bit hesitant to bake in assumptions about
malleability when we have no solid idea if ECC signatures are or are not
malleable on a fundemental level; if "whack-a-mole" anti-malleability is
all we've got it could be ugly if a break is found. Similarly, we may
find we missed something, or some needed change makes the malleability
rules difficult to work with for some new script type that is required.
I'd rather see a new CHECKSIG mode for the case where malleability
absolutely must be eliminated - certain multi-party protocols - and fix
wallet software instead. (the malleability problems people see are
closely related to inability to handle double-spends and reorgs) But I
can easily see that being an impossible goal engineering wise...

@_date: 2014-02-10 14:23:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] MtGox blames bitcoin 
Your political conversations would be welcome at unsystem at lists.dyne.org
See you there.

@_date: 2014-02-10 14:32:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
Got this:
Organization: Monetize.io Inc.
If assets were tagged you could do a very limited form of pre-signed offers:
in: 10 btc SINGLE|ANYONECANPAY
out: 1 AAA
These are composable, in that you can append the inputs and outputs of
multiple offers together and result in a valid transaction. However this
is pretty much the limit of what is possible without adding new SIGHASH
modes, and if you're going to hard-fork to add tagging, then you might
as well go the whole distance with explicit hierarchical
sub-transactions as we did with Freimarkets.

@_date: 2014-02-10 14:40:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] MtGox blames bitcoin 
That's basically what appears to have happened with Mt. Gox.
Preventing the attack is as simple as training your customer service
people to ask the customer if their wallet software shows a payment to a
specific address of a specific amount at some approximate time. Making
exact payment amounts unique - add a few satoshis - is a trivial if
slightly ugly way of making sure payments can be identified uniquely
over the phone. That the procedure at Mt. Gox let front-line customer
service reps manually send funds to customers without a proper
investigation of why the funds didn't arrive was a serious mistake on
their part.
Ultimately this is more of a social engineering attack than a technical
one, and a good example of why well-thought-out payment protocols are
helpful. Though the BIP70 payment protocol doesn't yet handle busines to
individual, or individual to indivudal, payments a future iteration can
and this kind of problem will be less of an issue.
Similarly stealth addresses have an inherent per-tx unique identifier,
the derived pubkey, which a UI might be able to take advantage of.

@_date: 2014-02-14 00:20:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
You're assuming the seller cares about fairness - why should they? They
offered a price for an asset and someone bought it; exactly which buyer
willing to buy at that price was able to complete the trade is
irrelevant to them. What they do care about is being sure that at
whatever given price they offered 100% of the buyers willing to buy at
that price actually see the offer in a reasonable amount of time - at
the best price the seller will get there will be only a single buyer
after all so you need that solid proof that said buyer was actually able
to get the offer.

@_date: 2014-02-14 00:21:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
Cross-chain trading is a different thing entirely; it doesn't allow for
the clever 2-party-trade trick. (as far as I know)

@_date: 2014-02-18 16:47:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP70 proposed changes 
Note that merge-avoidance implemented in conjunction CoinJoin doesn't
have this problem - the CoinJoin'd transaction either does or doesn't
confirm. Meanwhile being able to avoid merges, or more precisely, being
able to be flexible with them, makes achiving good value-privacy much
Secondly merge-flexibility also makes cut-thru payments possible. For
example BitPay can direct customers paying for goods to pay to addresses
controlled by merchants and other parties who are owed money by BitPay.
This skips a step, saving on transction fees as well as increasing
privacy. Notably in this case the only parties that have to deal with
accounting complexity are BitPay and the merchants - consumers' wallet
software needs no changes beyond generic payment protocol support, and
notably you can even use this technique without the payment protocol.
See my post "DarkWallet Best Practices" for more info:
 at lists.sourceforge.net/msg03508.html
What specifically do you dislike about X.509? The technical standard or
the infrastructure around it? (IE the centralized authorities)

@_date: 2014-02-19 15:49:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
While we might be able to get away with a retroactive change in meaning right now in the future that won't be so easy. There are lots if proposed applications for nLockTime-using protocols that depend on transactions (or parts of transactions) being possible to mine as is. Making existing transactions impossible to mine in the future will break those types of applications. We might as well use this as a learning experience for what a version bump would look like infrastructures wise.
Note how the above is a particularly bad example of gmaxwell's generic "don't break things" objection. Equally, remember that lots of infrastructure *does* handle malleability just fine already.

@_date: 2014-02-21 06:06:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin Core trial balloon: splitting 
The seccomp mechanism would work well here - it's a syscall whitelister,
which makes ptrace useless, among other things. Used by Chrome as of v23
to sandbox the renderers.
We'd probably need to use it with chroot and whitelist the open() call
so that the existing code can create new blockfiles and do whatever
leveldb does.

@_date: 2014-02-24 23:41:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fee drop 
So, just to be clear, we're adding, say, a memory limited mempool or
something prior to release so this fee drop doesn't open up an obvious
low-risk DDoS exploit.... right? As we all know, the network bandwidth
DoS attack mitigation strategy relies on transactions we accept to
mempools getting mined, and the clearance rate of the new low-fee
transactions is going to be pretty small; we've already had problems in
the past with mempool growth in periods of high demand. Equally it
should be obvious to people how you can create large groups of low-fee
transactions, and then cheaply double-spend them with higher fee
transactions to suck up network bandwidth - just like I raised for the
equally foolish double-spend propagation pull-req.
Of course, there's also the problem that we're basically lying to people
about whether or not Bitcoin is a good medium for microtransactions.
It's not. Saying otherwise by releasing software that has known and
obvious DoS attack vulnerabilities that didn't exist in the previous
version is irresponsible on multiple levels.

@_date: 2014-02-25 09:49:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fee drop 
No it's not. The cost is only incurred in the transactions actually get
mined, and unlike before the drop appears to be well under the
break-even orphan cost of transactions; we've got no reason to think the
clearance rate of these low-fee transactions will be significant.
But anyway, mostly I'm writing this to register my strong opposition
knowing full well that I don't expect it to change your minds.

@_date: 2014-02-25 12:13:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fee drop 
Well, I've done my responsible disclosure, and I've got better things to
do than argue with wishful thinking.

@_date: 2014-02-27 20:37:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
As usual, you don't need a hardfork.
Anyway, one-sided trade is sufficient to get a functioning marketplace
up and running and test out the many other issues with this stuff prior
to forking anything.
You can make the same argument against Bitcoin itself you know...
A Bitmessage-like network would be trivial to front-run via a sybil
attack. It's the fundemental problem with marketplaces - the data
they're trying to publish has to be public.
And again, how do you know that record is honest? Fact is without
proof-of-publication you just don't.
You mean a reverse nLockTime that makes a transaction invalid after a
certain amount of time - that's dangerous in a reorg unfortunately as it
can make transactions permenantly invalid.

@_date: 2014-02-28 06:18:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fee drop 
That's exactly the problem.
Of course every time we make a new transaction type standard we also run
that risk, but at least it's a temporary situation and we can expect to
get hashing power on-board fairly quickly. With such a low MIN_RELAY
that's not true, and in an absolute sense, the funds required to DoS
attack the network are fairly low.
There's currently no expiration policy at all; that's the root of the
DoS problem I was referring too.
Have you seen the mempool superblock design that keeps getting
suggested? jgarzik has the most recent write-up here:
I was working on a relatively ambitious version of it last summer that
calculated the fee/KB for transactions, including depedencies, and then
simply ordered the mempool with highest fee/KB first. The idea was you
could then easily limit the total size of the mempool and drop
transactions with the lowest fee/KB first. Transactions that paid less
than the lowest fee/KB in a max-size mempool simply would not get
relayed at all. Pity had to put it off for higher-priority work.
What's interesting is how this makes zero-conf transactions even less
safe: all you have to do to double-spend one (or more!) that pay X
fee/KB is broadcast enough transactions paying X+e fee/KB to push out
the unconfirmed tx from mepools around the network, then broadcast your
double-spend. Obviously the economics of this are going to make attacks
frequently profitable, especially if you can attack multiple targets at
once. You can of course have schemes where you don't entirely drop
transactions, saving, say, the inputs they spend and a transaction id,
(so a rebroadcast can succeed) but that just reduces the effectiveness
of the attack by a constant factor and makes it possible to get into
complex situations where your funds are locked and unspendable.

@_date: 2013-12-31 23:53:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
You assume the value of a crypto-currency is equal to all miners, it's
Suppose I create a merge-mined Zerocoin implementation with a 1:1
BTC/ZTC exchange rate enforced by the software. You can't argue this is
a scamcoin; no-one is getting rich. There's a 1:1 exchange rate so the
only thing you can do with the coin is get some privacy. But inevitably
some miners won't agree that enabling better privacy is a good thing, or
their local governments won't. Either way, they can attack the Zerocoin
merge-mined chain with a marginal cost of nearly zero.
OTOH if the Zerocoin scheme was implemented by embedding ZTC
transactions within standard Bitcoin transactions - even without any
attempt at hiding them - the attackers would need a 50% majority of
hashing power to succeed. Of course potentially slow confirmations is a
trade-off, but that's likely a perfectly OK trade-off in this case.

@_date: 2014-01-01 00:25:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
Nope. Tying the alt-coin difficulty to the Bitcoin difficulty isn't some
magic way to avoid a 51% attack - you still need a majority of
consensus. The attackers can still mine a conflicting chain and there's
still no reasonable way to choose between the two chains other than
proof-of-something. Even worse, then can do a data-hiding attack by
mining a conflicting chain without publishing the blockchain data, then
revealing it some time in the future, or just sowing FUD by making it
clear that the mining is happening. Like it or not crypto-coins solve
double-spending with proof-of-publication, and that can't be done
without some kind of mathematically verifiable majority aligned with the
interests of the crypto-coin users.
Recall that my zookeyv(1) and zerocoin alt(2) proposals from last summer
was specifically designed to take that situation into account, and of
course could at best only make it clear that it was happening and how
many Bitcoins needed to be sacrificed to make the chain secure.
1)  2013-05-31
2)  at lists.sourceforge.net/msg02472.html

@_date: 2014-01-03 15:39:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] An idea for alternative payment scheme 
Lol, fourth if you include me, although my case is rather embarassing as
I had re-read Bytecoin's original post recently and completely missed
the main point of it!
Actually I think it has the potential to be *more* SPV compatible than
the alternative, as in conjunction with prefix filters it lets you
receive unlimited unrelated payments that you can find in the blockchain
with a single prefix query with a fixed bandwidth/anonymity set size
tradeoff. (obviously in conjunction with one of the many ways of tagging
transactions for more efficient search)
The BIP38 approach with UI's that make it easy to create a new address
for every payment on the other hand force you to either accept higher
bandwidth consumption, or decrease your anonymity set size, or lose
payments. (inclusive)
I've got a post talking about this in more detail as well as an overview
of bloom filters vs. prefix filters that I'll publish tomorrow. (tl;dr:
bloom filters have very poor O(n^2) scalability and should be

@_date: 2014-01-03 16:01:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
As I showed with my zerocoin example, often that is not the case, e.g. I
do not support anonymity, or *can't* support it because of the local
Or for that matter, really boring examples like there's two competing
implementations of some basic idea and we'd rather the winner be picked
on technical merits rather than "I have a grudge and a small pool so
I'll this upstart at birth"
It's a thought experiment; read my original post on how to make a
zerocoin alt-chain and it might make more sense:
 at lists.sourceforge.net/msg02472.html
Even better might be to use a merge-mined version of Mastercoin as an
example, where the initial distribution of coins is fixed at genesis and
forward from that is independent of the Bitcoin blockchain.
I'll give you a hint: "marginal cost"
You're rant has rather little to do with my argument.

@_date: 2014-01-05 21:53:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Privacy and blockchain data 
* Summary
CoinJoin, CoinSwap and similar technologies improve your privacy by
making sure information about what coins you own doesn't make it into
the blockchain, but syncing your wallet is a privacy risk in itself and
can easily leak that same info. Here's an overview of that risk, how to
quantify it, and how to reduce it efficiently.
* Background
In the most general sense a Bitcoin wallet is a collection of one or
more scriptPubKeys, often known as addresses.(*) The basic purpose of
the wallet is maintain the set of all transaction outputs (txouts)
matching the scriptPubKeys in the wallet.  Secondary to that purpose is
to maintain the set of all transactions associated with scriptPubKeys in
the wallet; almost all (all?) wallet software maintains transaction
information rather than only txout data. Usually, but not always, the
wallet will have some mechanism to spend transaction outputs, creating
new transactions. (if the wallet doesn't it is referred to as a
watch-only wallet)
Given a full set of blockchain data the task of keeping the set of all
relevant transactions and txouts up-to-date is simple: scan the
blockchain for the relevant data. The challenge is to devise systems
where wallets can be kept up to date without this requirement in a way
that is secure, efficient, scalable, and meets the user's privacy
*) Alternatively addresses can be thought of as instructions to the
   payor as to how to generate a scriptPubKey that the payee can spend,
   a subtlety different concept.
* Threat Model and Goals
Currently the Bitcoin network consists of a large (low thousands) number
of allegedly independent nodes. There is no mechanism to prevent an
attacker from sybil attacking the network other than the availability of
IP addresses. This protection is made even weaker by the difficulty of
being sure you have a non-sybilled list of nodes to connect too; IP
addresses are passed gossip-style with no authentication.
From a privacy perspective we are conservative and assume an active,
internal, and global attacker - using the terminology of Diaz et al.(1)
- that controls up to 100% of the nodes you are connected too. With
regard to retrieval of blockchain data we can use the Sweeney's notion
of k-anonymity(2) where the privacy-sensitive data for an individual is
obscured by it's inclusion in a data of a large set of individuals, the
anonymity set.
* Basic Functionality
With regard to blockchain data we have the following basic functions:
** Spending funds
The user creates a transaction and gets it to miners by some method,
usually the P2P network although also possibly by direct submission.
Either way privacy can be achieved through a mix network such as Tor
and/or relaying other users' transactions so as to embed yours within a
larger anonymity set. In some cases payment protocols can shift the
problem to the recipient of the funds. Using CoinJoin also helps
increase the anonymity set.
Usually the sender will want to determine when the transaction confirms;
once the transaction has confirmed modulo a reorganization the
confirmation count can only increase. Transaction mutability and
double-spends by malicious CoinJoin participants complicate the task of
detecting confirmation: ideally we could simply query for the presence
of a given txid in each new block, however the transaction could be
mutated, changing the txid. The most simple way to detect confirmation
is then to query for spends of the txouts spend by the transaction.
** Receiving new funds
While in the future payment protocols may give recipients transaction
information directly it is most likely that wallets will continue to
have to query peers for new transactions paying scriptPubKey's under the
user's control for the forseeable future.
** Detection of unauthorized spends
Users' want early detection of private key compromise, accomplished by
querying blockchain data for spends from txouts in their wallets. This
has implications for how change must be handled, discussed below.
* Scalability/Efficiency
The total work done by the system as a whole for all queries given some
number of transactions n is the scalability of the scheme. In addition
scalability, and privacy in some cases, is improved if work can be
easily spread out across multiple nodes both at a per-block and
within-block level.
* Reliability/Robustness
Deterministic wallets using BIP32 or similar, where all private keys are
derived from a fixed seed, have proven to be extremely popular with
users for their simple backup model. While losing transaction metadata
after a data-loss event is unfortunate, losing access to all funds is a
disaster. Any address generation scheme must take this into account and
make it possible for all funds to be recovered quickly and efficiently
from blockchain data. Preserving privacy during this recovery is a
consideration, but 100% recovery of funds should not be sacrificed for
that goal.
* Query schemes
** Bloom filters
BIP37 bloom filters are currently implemented by the Bitcoin reference
implementation and used by bitcoinj-based SPV clients. Bloom filters
achieve a privacy-bandwidth tradeoff by having probabalistic
false-positives; the false-positives comprise the anonymity set.
Boom filters have a number of problems, both in the specific BIP37
implementation, as well as fundemental to the idea. Scalability is a
serious problem: the client sends asks a peer with a copy of all
blockchain data to filter data sent to the client, limiting the client's
bandwidth to only the data they are interested in. In the typical case
of a SPV wallet syncronizing against m new blocks this requires the peer
to read those m blocks from disk in their entirety, apply the filter,
and send the client the subset of matching transactions. Obviously this
results in poor O(n^2) scaling for n clients each making some fixed
number of transactions.
Of course bloom filters are attractive in that they have very good
performance per match, but this performance is only really relevant for
the most recent blockchain information where the data is in RAM. For
older information they make possible the Bloom IO attack where an
attacker uses an inordinant amount of disk IO bandwidth at little cost
to themselves.(3)
The actual BIP37 standard, and existing implementations of it, have a
number of other flaws that reduce privacy. For instance the standard
lets the seed value of the hash function be tweaked with a 32-bit
integer, nTweak. However on the one hand if randomly chosen and rarely
changed, as suggested by BIP37, the 32-bit integer can be used by an
attacker to correlate multiple connections from the same wallet. On the
other hand if nTweak is changed an attacker that can link multiple bloom
filters can AND those filters together to greatly decrease the
false-positive rate and determine exactly what funds are in the user's
** Prefix filters
With a randomly distributed keyspace - common in cryptographic
applications - clients can query using variable length prefixes that
partially match the desired keys. A very simple format for a query of n
prefixes will look like the following:
    <1 byte length in bits> <1 to 256/8 bytes of prefix>
    ...
    ...
    0x00
The anonymity set is then the blockchain data whose key is the same
prefix, usually H(scriptPubKey) or scriptPubKey directly. An important
advantage of prefix filters is compatibility with the proposed (U)TXO
commitment schemes: the prefix maps directly to the committed
scriptPubKey lookup trees, and nodes simply return all entries matching
the prefix, as well the the rest of the merkle path to the blockchain
headers proving the data is valid.
While bloom filters have O(n) cost per lookup, or O(n^2) scalability
system-wide, prefix filters have significantly better O(log n) cost per
lookup, or O(n log n) system-wide. It's also worth noting that a naive
implementation can achieve very similar performance to bloom filters
without bothering to build key-value indexes by just scanning blockchain
data; once the data is hashed testing the hash against a prefix has a
minimal cost.
** Cryptographically blinded schemes
There are many blinded database query schemes in existence. While we do
not reject such schemes completely, technologies that rely on simple and
easy-to-understand cryptography have a significant advantage in their
simplicity. In addition such complex schemes are unlikely to ever be
made into miner commitments and thus are less trustworthy in the long
* Correlation attacks
It is often advantageous if blockchain queries can be efficiently spread
across multiple servers to avoid allowing the attacker to correllate the
information into a whole. If you have n addresses that need to be
watched for new transactions, splitting the queries across m nodes
reduces the information any one node may learn. With bloom filters doing
this is extremely costly as the full blockchain data needs to be read
from disk to apply the filter; with prefix filters if the nodes have
appropriate indexes there is little overhead to splitting the queries
and no performance loss.
* DoS attacks
A possible DoS attack on bandwidth is to insert a large amount of
blockchain data matching the target's filter; the BIP37 nTweak parameter
was an attempt to avoid this problem, although with privacy tradeoffs.
Blockchain data is an extremely expensive communications channel so we
do not consider this a serious issue. Implementations may wish to give
clients the ability to specify a filter for information they do not want
to avoid unintentional collisions, although hopefully in the future the
address reuse making this a potential problem will become less common.
* Address use, management, and generation
If privacy was not a consideration the most efficient mode of operation
would be to use a single address, as is done by many existing wallets,
notably the bitcoinj-derived Multibit and Android Wallet, both of which
use bloom filters. In addition to strongly encouraging address re-use,
neither provide the user any control over the privacy/bandwidth tradeoff
given by bloom filters; the default settings have an extremely low
false-positive rate that is a significant privacy risk.
Taking privacy into account better clients such as Electrum, Armory, and
Bitcoin Core discourage the re-use of addresses in their UIs, and send
change to new addresses. However this leads to problem with user
expectations: users expect it to be possible to be notified quickly of
new transactions paying any address ever generated by their wallet, as
well as unauthorized spends from any txout, yet for privacy each query
for transactions related to the address/txout must match false-positives
that consume bandwidth; for a fixed bandwidth budget the specificity and
size of the filter must increase over time.
We have two main avenues to solve this problem:
1) Txin-reuse: Continue to reinforce the idea that transaction inputs
   have no particular relationship to outputs. Using them for refunds or
   other purposes implying "ownership" must be strongly discouraged.
   CoinJoin will help here. If addresses associated with change txouts
   are truly one-time-use, we can reduce or eliminate queries associated
   with them. In particular, while the set of all change addresses ever
   used will grow linearly with time, the set of all change addresses
   with funds in them will remain roughly stable - it's this set that
   needs to be queried to detect unauthorized spends.
2) Common prefixes: Generate addresses such that for a given wallet they
   all share a fixed prefix. The length of that prefix determines the
   anonymity set and associated privacy/bandwidth tradeoff, which
   remainds a fixed ratio of all transactions for the life of the
   wallet.
With this approach change addresses continue to be generated randomly, a
requirement for CoinJoin privacy. There is some statistical information
leaked if many non-change txouts are spent in a single transaction in a
CoinJoin, but even that leak can be avoided with the authors
OP_RETURN-based stealth addresses proposal. (to be published)
The actual prefix-forcing scheme in many cases will have to be
brute-force search; fortunately the search space involved is reasonably
small, ~2 to ~16 bits, and can be done as a background task.
1) Towards Measuring Anonymity, Claudia Diaz and Stefaan Seys and Joris
   Claessens and Bart Preneel (April 2002)
2) k-Anonymity: A Model for Protecting Privacy, Latanya Sweeney, May
   2002
3) Private discussions with developers.

@_date: 2014-01-06 07:03:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
* Abstract
A Stealth Address is a new type of Bitcoin address and related
scriptPubKey/transaction generation scheme that allowers payees to
publish a single, fixed, address that payors can send funds efficiently,
privately, reliably and non-interactively. Payors do not learn what
other payments have been made to the stealth address, and third-parties
learn nothing at all. (both subject to an adjustable anonymity set)
* Acknowledgments
Credit goes to ByteCoin for the original idea.(1) Gregory Maxwell, Adam
Back, and others on  contributed valuable input on the
implementation. Finally thanks goes to Amir Taaki for input on the
general idea of stealth addresses and use-cases.
* Background
Viewed generally a Bitcoin address is a mechanism by which a payee
instructs a payor to create a transaction such that the payee can spend
one or more of the transaction outputs. Of course, typically the address
is simply the hash of a pubkey, and the mechanism by which the funds are
made available to the payee is to simply create a scriptPubKey of the
following form:
    DUP HASH160  EQUALVERIFY CHECKSIG
The problem however is address reuse: it is convenient for payees to
give one or more payor a single address and use it multiple times for
various purposes. This results in all those payments becoming trivially
linkable to each other by an attacker - a threat not only to the privacy
of the user, but also to all users of Bitcoin.(2)
BIP32 hierarchical deterministic wallets are frequently proposed as a
solution. Now an address is a chain code and the mechanism by which a
scriptPubKey is generated is to derive a one-time-use pubkey from that
chain code and some index i. However, this quickly runs into two main
1) Lack of privacy: While someone not in possession of the address can't
   link payments together, someone who is can.
2) State: If the index is not to be re-used wallets must either maintain
   per-address state, or somehow query for already used indexes, or
   somehow generate them in a sufficiently small range that the payee
   can recover the indexes. All these solutions are problematic.
A good example of where the BIP32-derivation solutions fails come up at
the Dark Wallet Hackathon where it was suggested by the author that for
the purpose of securing person-to-person payments OpenPGP public keys
and X.509 certificates be extended with a new user-id field containing a
Bitcoin address. Wallet software could then use either certificate
system to ensure funds were being sent to the intended recipients -
essentially a non-interactive way of solving what the BIP70 payment
protocol solves interactively. Of course, without stealth addresses the
scheme would likely have little or no privacy.
* Requirements
1) Generated scriptPubKey must be globally unique
2) Must be only spendable by payee
3) scriptPubKey and associated transaction must be indistinguishable to
   third-parties from other transactions in some anonymity set.
4) Method must be fully deterministic and funds recoverable from a
   wallet seed and blockchain data for both payee and payor.
5) Funds must be efficiently recoverable by payee with reasonable, and
   configurable, computation and bandwidth costs.
6) Must be compatible with CoinJoin/Must not leak information to payee
   about what txins were used to pay them.
7) Must be compatible with multisig-protected wallets.
8) Must not make assumptions about txin scriptSig form.
9) Must be possible to prove to third parties that payment was made in
   accordance to instructions without revealing any other information.
** Payment Reliability
Schemes for making payments by transmitting nonces to the recipient
through some other medium, such as Bitmessage, were discussed at the
Dark Wallet Hackathon. However using any medium but the blockchain
itself for the communication means that the reliability of the payment
getting to the recipient is less than that of a standard transaction.
For instance Bitmessage nodes only keep messages for two weeks. We
decided that anything less than reliable atomic transactions was
* Applying encryption to payments, simple explanation
Using Elliptic curve Diffie-Hellman (ECDH) we can generate a shared
secret that the payee can use to recover their funds. Let the payee have
keypair Q=dG. The payor generates nonce keypair P=eG and uses ECDH to
arrive at shared secret c=H(eQ)=H(dP). This secret could be used to
derive a ECC secret key, and from that a scriptPubKey, however that
would allow both payor and payee the ability to spend the funds. So
instead we use BIP32-style derivation to create Q'=(Q+c)G and associated
As for the nonce keypair, that is included in the transaction in an
additional zero-valued output:
    RETURN The payee recovers the funds by scanning the blockchain for candiate P's
in transactions, regenerating the scriptPubKey, and finally checking if
any txouts in the transactions match. Note the close similarity of this
technique to how the Bitmessage network functions - an initial
implementation of the idea will find the Bitmessage code a suitable
starting point.
* Trading off anonymity set size for decreased bandwidth/CPU
By taking advantage of prefix filters(3) we can choose a tradeoff
between anonymity set size and bandwidth/CPU usage if the payee
specifies that payments to them are to match some short prefix k. There
are a few possibilities for how the prefix is to the applied - the most
simple is if per-block indexes of scriptPubKeys are available:
    RETURN  Alternatively if per-block indexes of H(scriptPubKeys) are only
available the wallet software can grind the scriptPubKey with nonce i
until it matches the specified prefix:
    RETURN  Furthermore as symmetric ciphers are quite cheap we might as well hide
the purpose of the OP_RETURN txout and encrypt the pubkey P using H(Q)
as a symmetric key. This gives us a slightly larger anonymity set.
* Advantages of using a separate output
An alternative would be to either re-use a pubkey or signature nonce
value from a transaction input, saving about 45 bytes per txout. An
absolute minimum sized Bitcoin transaction is 166 bytes(4) so at best we
have a 27% savings in tx fees, and more typically around ~15%. (modulo
mass-payments from a single txin)
However using an explicit prunable OP_RETURN output to store the pubkey
rather than re-using one from a txin or txin signature has a number of
1) The txin's owned by the payor are not revealed to the payee. In fact,
   they could be held by a third-party who simply makes a transaction
   with the appropriate txouts on behalf of the payee.
2) Less information about the txouts is leaked. The statistical
   distribution of txouts remains unchanged - not possible in re-use
   schemes because they need to grind the payee scriptPubKey's for the
   sake of the prefix filters.
3) If required the nonce secret can be revealed to prove that a payment
   was made to a third-party, e.g. for dispute resolution.
* Bare CHECK(MULTI)SIG output alternative
An alternative with better efficiency could be to use bare
OP_CHECK(MULTI)SIG outputs to hold the nonce pubkey - generally a second
output is needed anyway for change. The most simple would be to use Jeff
Garzik's OP_DROP proposal(5) for the prefix:
   DROP n ... m CHECKMULTISIG
  or
   DROP  CHECKSIG
The payor pubkey is in the *change* txout, and the payee's ECDH-derived
pubkey in the other txout. By setting the prefix to be the same on both
txouts and using the same basic scriptPubKey form the relationship of
change and payment is still hidden; CoinJoin-using implementations can
adopt even more sophisticated approaches.
If IsStandard() rules remain the same and using OP_DROP is impractical,
we can also grind the change pubkey to match the prefix in a
deterministic manner so the wallet can still be recovered from a seed.
More costly, but maybe still acceptable for reasonably short prefixes.
Either way the result is transactions that are actually smaller and
cheaper than standard transactions, although without the advantage of
pushing scriptPubKey size payment to the receiver. (a pity we didn't
spend the extra time to adopt OP_EVAL)
A disadvantage is that revealing the nonce secret to prove a payment was
made is more problematic - either the txout needs to be spent first, or
we need a CHECKMULTISIG.
* Address format
To be decided. To support mulisig we probably want the ability to
specify n-of-m master pubkeys, using the nonce to generate derived ones.
For the single pubkey case the addresses will be a little longer than
standard Bitcoin addresses:
  s9KND3vfXjs3YqfZp86Acce3bM7Mhuptwh6mjeDnThsDei9Z2ZZcU
  vs.
  1LZn91ynrA6BCmoUKwnV3Ygk4FQMfPxLbg
1) ByteCoin, Untraceable transactions which can contain a secure message
   are inevitable, 2) Gregory Maxwell, Dark Wallet Certification discussions, also
   3) Peter Todd, [Bitcoin-development] Privacy and blockchain data,
    at lists.sourceforge.net/msg03612.html
4) Bitcoin Wiki, Maximum transaction rate,
   5) Jeff Garzik, Add small-data OP_DROP transactions as standard
   transactions,

@_date: 2014-01-06 10:44:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
It's not meant to prove anything - the proof-of-sacrificed-bitcoins
mentioned(*) in it is secure only if Bitcoin itself is secure and
functional. I referred you to it because understanding the system will
help you understand my thinking behind merge-mining.
*) It also mentions proof-of-sacrificed-zerocoins which *is* distinct
because you're sacrificing the thing that the chain is about. Now that
has some proof-of-stake tinges to it for sure - I myself am not
convinced it is or isn't a viable scheme.
You're argument is perfectly valid and correct, *if* the assumptions
behind it hold. The problem is you're assuming miners act rationally and
have equal opportunities - that's a very big assumption and I have
strong doubts it holds, particularly for alts with a small amount of
hashing power.
You know, something that I haven't made clear in this discussion is that
while I think merge-mining is insecure, in the sense of "should my new
fancy alt-coin protocol widget use it?", I *also* don't think regular
mining is much better. In some cases it will be worse due to social
factors. (e.g. a bunch of big pools are going to merge-mine my scheme on
launch day because it makes puppies cuter and kids smile)
All I'm saying is that if you can afford the transaction fees stuffing
your data into the Bitcoin blockchain has orders of magnitude better
security. I'm not saying it'll be cheap - if miners start trying to
block your protocol blacklists they can make it fairly expensive for
your alt - but it will be just as secure against reorganization attack
as Bitcoin itself.
Heh, my one line reply might have been a bit harsh because of that. :)

@_date: 2014-01-06 13:13:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Along the lines of my recent post on blockchain data:
Is it going to be possible to do partial prefix queries on that tree?
Also have you considered creating per-block indexes of all
scriptPubKeys, spent or unspent, queryable via the same partial prefix
It'd be very good to test this stuff thoroughly on Electrum first and
get a feel for the performance and usability before any soft-fork to
make it a miner commitment.
Similarly a C++ implementation should be simply added to Bitcoin Core as
a bloom filter replacement and made available over the P2P network.

@_date: 2014-01-10 05:10:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Privacy and blockchain data 
It's a trade-off. Most people are going to use public peers for their
SPV nodes - they're not going to run full nodes. They also are going to
want to limit how much bandwidth they use to sync their wallets; if they
don't care the can use a very short, or no, prefix and the problem goes
If you make that bandwidth/privacy trade-off by using very specific
filters and non-specific addresses then you have a situation where those
public peers are learning a lot of potentially valuable data. It's easy
to imagine, say, the IRS being willing to pay for data on how many
Bitcoins people have in their wallets to try to catch tax cheats for
instance, and that can easily fund a lot of fast and high-quality peers
that don't advertise the fact that they're selling data on the contents
of your wallet.
On the other hand if you use non-specific filters, and prefixed
addresses for incoming payments, then you're not leaking high-quality
information to anyone. I think this makes for a more robust Bitcoin
system, especially as we need things like CoinJoin for privacy that make
*everyones* privacy matter to you; CoinJoin could easily be defeated by
aquiring lots of good info on the contents of wallets through SPV
Actually UTXO isn't the right way to look at this; prefix filters would
be almost certainly matched against all txouts in blocks. Or put another
way, UTXO isn't the right way to look at it because the attacker will
have some rough idea of the time period, and wants to know about
transactions made.
Well what good, in your example, is it for the attacker to go from "I
know my target gets a paycheck every two weeks for $x" to "His wallet
prefix is abcd with y% probability"? Even once you learn the prefix of
your target's wallet, what funds they actually own is still embedded in
a much larger anonymity set of hundreds to thousands of transactions
that had nothing to do with them.
No, I specifically said that you don't want to use prefixes with change
txouts for that reason. Fortunately while the set of all scriptPubKey's
ever used for change txouts will grow over time, as long as you are not
watching for new payments on any key in that set you only need to query
for the ones that still have funds on them, and that's only because you
want to be able to detect unauthorized spends of them.

@_date: 2014-01-10 05:20:37
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
I think that's correct, but my ECC math is a bit shakey... In any case,
what's important is that you can derive a pubkey such that only the
recipient has the privkey, and without knowledge of the shared secret
you can't determine what the recipients master pubkey was.
Yup, you're understanding matches mine. (no guarantee if my
understanding is correct!)
Oh, sorry, I forgot to mention it in my first write-up but you can
easily make stealth addresses include a second pubkey for the purpose of
the communication that either isn't used in the scriptPubKey at all, or
is part of a n-of-m multisig. (n>=2) Interestingly that also means you
can give a third-party that key and out-source the effort of scanning
the blockchain for you.

@_date: 2014-01-10 06:11:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
Proof-of-sacrified-bitcoins is always a true sacrifice - provided
Bitcoin itself maintains consensus the proof is a guarantee that
something of value was given up.
Proof-of-sacrificed-"newcoins" means that within some consensus system I
created a signed statement that *within the system* means I lose
something of value. However that sacrifice is only valid if the
consensus of the system includes that sacrifice within the consensus,
and if the mechanism by which that consensus is maintained has anything
to do with those sacrifices you quickly find yourself on pretty shakey
Situations where decentralized consensus systems are competing for
market share in some domain certainely apply. For instance if I were to
create a competitor to Namecoin, perhaps because I thought the existing
allocation of names was unfair, and/or I had technical improvements like
SPV, it's easy to imagine Namecoin miners deciding to attack my
competitor to preserve the value of their namecoins and domain names
registered in Namecoin.
The problem here is that my new system has a substantial *negative*
value to those existing Namecoin holders - if it catches on the value of
their Namecoin investment in the form of coins and domain names may go
down. Thus for them doing nothing has a negative return, attacking my
coin has a positive return minus costs, and with merge-mining the costs
are zero.
Without merge mining if the value to the participants in the new system
is greater than the harm done to the participants in the old system the
total work on the new system's chain will still be positive and it has a
chance of surviving.
Of course, this is what Luke-Jr was getting at when he was talking about
scam-coins and merge mining: if you're alt-currency is a currency, and
it catches on, then it dilutes the value of your existing coins and
people who own those coins have an incentive to attack the competitor.
That's why merge-mined alt-coins that are currencies get often get
attacked very quickly.

@_date: 2014-01-10 06:25:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
Come to think of it, we've got that exact situation right now: the new
Twister P2P Microblogging thing has a blockchain for registering
usernames that could have been easily done with Namecoin, thus in theory
Namecoin owners have an incentive to make sure the Twister blockchain
gets killed at birth.
Pretty easy to do right now too as the hashing power behind Twister is
miniscule and probably will stay that way - the only incentive to mining
is that you get the right to make a "promoted post" - called a spam
message in the codebase - that in theory Twister clients are supposed to
show to their users. Of course, there's absolutely no way to guarantee
that clients actually do that.

@_date: 2014-01-10 07:00:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
Needs a prototype implementation first. The version with no prefix is
the simple one and doesn't have any other dependencies; the prefix
version is harder because it isn't clear yet what's the best way to
force the prefix, or for that matter whether scriptPubKey or
H(scriptPubKey) indexes will be available.
It's on my todo list, but as you've probably noticed my todo list is
rather long. :)

@_date: 2014-01-10 12:22:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] The insecurity of merge-mining 
Because there aren't that many pools out there and Ixcoin (and devcoin)
appear to have been lucky enough to servive long enough to get the
support of a reasonably big one. Once you do that, the potential
attackers have PR to think about. (namecoin especially has a PR
advantage) None of this stuff is hard and fast rules after all.
Those are all examples where the cost to the "bitcoiner defending their
currency" is high - I might get arrested trying to burn down a bank.
Anyway, I'm starting to think you're reading too much into my statement
"merge mining is insecure", which, keep in mind, I said in relation to a
guy who was trying to recruit devs to implement some unknown "altcoin"
Imagine you're one of the first US cave divers back in the early 70's.
You've been doing it for only a few years yourself, and you and your
buddies, some of them now late, realized pretty quick it's bloody
dangerous and there's all kinds of ways to get yourself killed. (caving
itself is bad enough) On the other hand, if you're careful and have good
training it *is* possible to reduce the risks significantly. Meanwhile
the media and public in general is starting to pick up on caving and
cave diving and there's a tonne of new people - most of whome don't seem
to know what they're doing - are getting into both sports. You just know
this is going to lead to a lot of people getting hurt and killed who
probably should have just stuck to caving. (IE, stuck to making
Bitcoin-using applications)
In that context I sure as heck would loudly yell "CAVE DIVING IS FUCKING
DANGEROUS, DON'T DO IT". Sure, that's not quite telling the whole story,
but the message is pretty close to the truth. The people that should be
in the sport are the ones that take a statement like that as a warning
to do their research; I have no reason to think the OP asking for
developers was one of those people.
Distributing harm among n people just reduces the harm for each person
by a factor of n. That may or may not make that harm smaller than
whatever tiny reward mining the chain would be.
You assume doing so has zero cost - it doesn't. Running namecoind
involves effort and bandwidth on my part.
Lets rephrase that "A secure chain is no more useful than a less secure
chain. A secure chain will not be more valuable than a less secure
chain, all other things being equal."
I don't think we're going to see eye to eye on this.

@_date: 2014-01-13 15:14:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
Basically stealth addresses *are* your scheme, using the blockchain as a
low or even no overhead communication channel for the payor to give the
payee that multiplier without bidirectional communication.
In the business card example I can't easily take your business card and
just send you some money without that transaction being linked to public
information. (your business card)
WoT is a perfect example of the problem: if you put BIP32 branch payment
info into my OpenPGP key I can't pay you securely and reliably without
making the transaction public. The best I can do is pick a nonce and pay
X=rootKey*nonce, communicating to you the nonce later - this isn't
reliable because if I or you lose the nonce the funds are lost.
With stealth addresses the user experience can be as simple as you
telling me on the phone "hey! send me that 0.234 BTC you owe me!", me
clicking on "Send to Alan Reiner (verified by PGP)" (perhaps again on my
off-line second factor device for a multi-sig wallet) and tellling you
"OK, sent".
Even if your phone has been wiretapped, the attacker still didn't learn
exactly what transaction was actually used to make the payment - a big
advantage over per-tx nonce stuff.

@_date: 2014-01-13 15:15:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
I mentioned it again in another email; I just forgot to include it in my
final write-up.

@_date: 2014-01-13 16:27:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
Well that's the thing: the cost to find all stealth-address-using
payments to you isn't O(n) transaction volume, it's O(n) anonymity set
size. I think we can make a pretty good argument that the anonymity set
people need is mostly fixed in size and has nothing to do with overall
tx volume, so really we've got O(1) scaling.
There is a catch however: if you need the prefix to be against
H(scriptPubKey) rather than scriptPubKey directly the sender needs to
grind the OP_RETURN output at 2^len(prefix) cost. Fortunately that
grinding can be done with hash operations rather than ECC - even if we
needed 32-bit prefixes eventually computing 32-bit hash collisions is
plausible, and more reasonable 8-bit is quite doable now.

@_date: 2014-01-14 07:10:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
That's exactly where you need to put it.
Incidentally a prefix nonce, either direct or grind-style, is a bit of a
privacy leak by suggestion how long the prefix was in the original
stealth address. Code should be written such that grinding routines
start at a random nonce, and nonces of any length are accepted. The
easiest way to do that is to just stick the grind nonce at the end after
the 33 bytes of pubkey.
I dunno yet what hashing algorithm to target for grinding. I'd assume
SHA256^2 on the basis that it's identical to what the merkle tree uses
and thus will have the same security properties in a committed index,
but I can see people pushing for the shorter 20-byte HASH160 too.
The idea was to make the anonymity set include other uses of OP_RETURN
txouts, however Gregory Maxwell pointed out that it'd easily lead to a
much reduced anonymity set because someone could trial decrypt the
encrypted P and check if it was a valid pubkey. If you encrypted the
full 33 bytes that'd be a total disaster - only 1/256 candidate stealth
keys would work. There are ways to do it right, but it's tricky and
there may be other attacks I don't know about, so I'm inclined to just
drop that idea for now unless a professional cryptographer wants to take
it on.
Yup. You can even use that pubkey to disambiguate/prove payments with
Timo Hanke's pay-to-contract ideas by deriving it from some root and a
contract hash.
Conversely Amir Taaki pointed out on the unsystem list that once a nonce
is agreed on, it can be used directly with BIP32 derivation so that
future payments don't have to use an OP_RETURN txout. Interesting idea,
although I worry that the statelessness advantage of stealth payments
gets lost if you do that. Probably best to look at that one after an
initial implementation happens and we get some experience with them in
the real world - adding that can be done in a backwards compatible
Will do.

@_date: 2014-01-14 09:15:17
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
I think what stealth addresses is showing is that the concept of an
address being "instructions on how to generate a txout/tx that results
in me getting Bitcoins" is actually quite valuable; it and
BIP32-derivation addresses with chaincodes are pretty clear cases where
just replacing address with scriptPubKey isn't sufficient.
Yeah, I don't see anything wrong with stealth addresses whatever length
they wind up being. It's a good intermediate step, and without them
people will just pass around unsigned payment requests and other stuff.
At the DarkWallet hackathon we had discussed how to integrate stealth
addresses into OpenPGP keys as a new user id type for instance, and
similarly into x.509 certs.
The big advantage here is the identity of *who* you are paying is
important, not just "I got this signed payment request". Basically the
concept becomes "identity signed payment address" and the signature
binding the identity to the address is a one time and offline thing; an
issue with the payment protocol as it stands is that it encourages
signing keys to be kept online to issue payment requests. If you have a
scheme where the private keys that bound the identity to the address can
be kept offline you're much better off, because the attacker can only
create a fake payment request, they can't divert the funds to
So with that in mind, I strongly suggest sticking with defining a
reasonable stealth address spec. But when you do, keep in mind that you
may want to upgrade it in the future, preferably in a backwards
compatible way. Also, it shouldn't be limited to exactly 2-of-2
CHECKMULTISIG, there's no reason why n and m can't be picked as needed.
Sure, it means the addresses are not fixed length, but for something
that is mostly an internal detail and only occasionally visible to
advanced users, I see no issues there.
Along those lines: what would a BIP32 chain code address look like? What
happens when you want to use that with a multisig-protected wallet?
I think you're missing the bigger picture here, not least of which is
that backwards compatibility is a bit of a misnomer for an unreleased
standard. :)
Why put this into the PaymentDetails? That a stealth address is to be
used for the payment is a property of the outputs being requested, not
the payment itself. We're better off if that goes into the Output
message, and further more it suggests that the Output message shouldn't
contain raw scriptPubKey's but rather addresses. After all, IsStandard()
means we have to inspect the scriptPubKey to see if we can even pay to
what the sender is requesting.
Once you establish that it's addresses that Outputs specify, then it's
easy enough to make a stealth address type, or a BIP32-chain-code
address type, or whatever else comes up in the future.

@_date: 2014-01-14 09:19:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
How would they trigger false positives? The payee recovers the nonce
with ECDH from the payor's ephemereal pubkey and their online detection
secret key. They use BIP32 public derivation with their offline spending
pubkey(s), if the derived pubkeys match the actual scriptPubKey they
know the output is spendable by them. I don't see how that can go wrong.

@_date: 2014-01-14 15:48:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
Good catch, yeah, use the master shared secret to derive per-pubkey
Well like I said, you shouldn't force the txout to be exactly a 2-of-2
multisig - the recipient might be using a multi-factor wallet for
instance. So, if I understand your code, what you want is the following:
byte[] Q = ;
byte[] Q_Scan = int m = <# of pubkeys required to redeem>;
byte[] S = EC.DH(e, Q_Scan);
byte[] qDerived[];
for (int = 0; i < len(Q); i++){
    qDerived[i] = EC.PointAdd(Q[i], Util.SingleSHA256(S || i));
qDerived = sorted(qDerived);
if (len(Q) > 1){
    stealthTx.Vout.Add(TxOut.PayToMultiSig(amount, m, len(Q), qDerived));
} else {
    stealthTx.Vout.Add(TxOut.PayToPubKeyHash(amount, qDerived[0]);
Finally, it would probably be better if the multisig output was wrapped
in a P2SH output to better match the behavior of other wallets for the
sake of a bigger anonymity set - seems that stuff that is implementing
multifactor wallets and escrow is using P2SH to do it rather than bare
multisig. Also there's quite a bit of support for making bare multisig
not IsStandard() to discourage data-storage applications.

@_date: 2014-01-16 16:28:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
I'm very against the name "reusable addresses" and strongly belive we
should stick with the name stealth addresses.
You gotta look at it from the perspective of a user; lets take standard
pay-to-pubkey-hash addresses: I can tell my wallet to pay one as many
times as I want and everything works just great. I also can enter the
address on blockchain.info's search box, and every transaction related
to the address, and the balance of it, pops up immediately.
What is that telling me? A: Addresses starting with "1" are reusable. B:
Transactions associated with them appear to be public knowledge.
Now I upgrade my wallet software and it says I now have a "reusable"
address. My reaction is "Huh? Normal addresses are reusable, what's
special about this weird reusable address thing that my buddy Bob's
wallet software couldn't pay." I might even try to enter in a "reusable"
address in blockchain.info, which won't work, and I'll just figure
"must be some new unsupported thing" and move on with my life.
On the other hand, suppose my wallet says I now have "stealth address"
support. I'm going to think "Huh, stealth? I guess that means privacy
right? I like privacy." If I try searching for a stealth address on
blockchain.info, when it doesn't work I might think twig on "Oh right!
It said stealth addresses are private, so maybe the transactions are
hidden?" I might also think "Maybe this is like stealth/incognito mode
in my browser? So like, there's no history being kept for others to
see?" Regardless, I'm going to be thinking "well I hear scary stuff
about Bitcoin privacy, and this stealth thing sounds like it's gonna
help, so I should learn more about that"
Finally keep in mind that stealth addresses have had a tonne of very
fast, and very wide reaching PR. The name is in the public conciousness
already, and trying to change it now just because of vague bad
associations is going to throw away the momentum of that good PR and
slow down adoption. Last night I was at the Toronto Bitcoin Meetup and I
based on conversations there with people there, technical and
non-technical, almost everyone had heard about them and almost everyone
seemed to understand the basic idea of why they were a good thing. That
just wouldn't have happened with a name that tried to hide what stealth
addresses were for, and by changing the name now we risk people not
making the connection when wallet software gets upgraded to support

@_date: 2014-01-17 09:46:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
I think we should paint it this colour:
    They had uncovered what seemed to be the side of a large coloured
    globule embedded in the substance. The colour, which resembled some
    of the bands in the meteor's strange spectrum, was almost impossible
    to describe; and it was only by analogy that they called it colour
    at all.  Its texture was glossy, and upon tapping it appeared to
    promise both brittle ness and hollowness. One of the professors gave
    it a smart blow with a hammer, and it burst with a nervous little
    pop. Nothing was emitted, and all trace of the thing vanished with
    the puncturing. It left behind a hollow spherical space about three
    inches across, and all thought it probable that others would be
    discovered as the enclosing substance wasted away.
I think it really gets to the core of my feelings about this naming
WOW! AWESOME KICK-ASS PICS!
Come to think of it, I could have called it "incognito addresses" - a
term nice enough that Google and Firefox use it in their browsers - but
what's done is done and any further discussion about this is just going
to confuse the public. Remember that in the long run all this stuff will
be hidden behind payment protocols anyway, and users *won't even know*
that under the hood a stealth address is being used, making the name
just a technical detail. For now though, lets use the good PR and get
some early adopters on board.
However, the term 'incognito' probably would be a good one to use within
wallet software itself to describe what it's doing when the user clicks
the "I want my transactions to be private" setting - there are after all
fundemental bandwidth-privacy trade-offs in the threat model supposed by
both prefix and bloom filters. In this instance the term isn't going to
go away.
Anyway, back to work: For the actual address format I strongly think we
need to ensure that it can be upgrading in a backwards compatible way.
This means we have to be able to add new fields - for instance if
Gregory's ideas for different ways of doing the SPV-bait came to
fruition. Given that "addresses" aren't something that should stay
user-visible forever, thoughts on just making the actual data a protocol
buffers object?
Second question: Any performance figures yet on how efficient scanning
the blockchain for matching transactions actually is? I'd like to get an
idea soon for both desktop and smartphone wallets so we can figure out
what kind of trade-offs users might be forced into in terms of prefix

@_date: 2014-01-17 09:52:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reality Keys trusted oracle service 
Finally seeing a more complex script-use-case being implemented:
    Enter Reality Keys, a new service by Tokyo-based startup Social
    Minds due for public launch on 20th January. Reality Keys provides
    real-world data in a form that can be used to complete or disregard
    bitcoin transactions, based on quantifiable facts.
    [...]
    Users then specify a date at which they would like to confirm the
    status or outcome of a particular event, and two cryptographic
    public keys are provided: one for if the event happens and another
    for if it doesn?t.
    [...]
    It is all, of course, anonymous. Reality Keys provides only the
    keys, and has no interest in or knowledge of the nature of the
    contract or the amounts of bitcoin at stake.

@_date: 2014-01-20 06:08:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Stealth Addresses 
Keep in mind that Bitmessage uses the same ECDH mechanism as what
stealth addresses will use. They seem to get decent enough performance
from it for a use-case not-unlike that of a Bitcoin wallet.
In any case I'm interested in knowing actual performance numbers for it;
last I talked to Kyle Drake he said he was working on getting ECDH
numbers on Javascript, probably the slowest possible implementation of
the idea. As for send to stealth addresses using prefixes, he's
confirmed that you'll be able to do that will well under a second to
brute-force the prefixes with the proposed OP_RETURN mechanism even with
rather long 8-bit prefixes.

@_date: 2014-01-20 06:11:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] unlinakble static address? & spv-privacy 
Actually the exact encoding is still undetermined - other encodings I
proposed in my original paper are the same size or even smaller than a
standard transaction.

@_date: 2014-01-20 17:35:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP0039: Final call 
That issue is more than enough to get a NACK from me on making the
current BIP39 draft a standard - I can easily see that leading to users
losing a lot of money.
Have any wallets implemented BIP39 this way already in released code?

@_date: 2014-01-24 04:02:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bait for reusable addresses 
Yes, but remember I proposed prefixing in my blockchain data query paper
because it's a trade-off between theoretical good privacy and
brittleness. The real world experience is that users, or to be exact
wallet authors, turn down SPV privacy parameters until bloom filters
have almost no privacy in exchange for little bandwidth usage. (though
load on the server is unchanged of course)
The brittleness comes in because the moment you connect to a malicious,
data-collecting peer, the contents of your wallet are all revealed.
Frankly that'd be a disaster for CoinJoin too, and I think it'd be a
bigger disaster than the poor specificity patterns leaked by prefix
usage. If anyone wants to deanonymize CoinJoin there will be a lot of
incentives to do so, and you only need wallet content data to do that.
Well, that's the big question: How much extra data do we need and what's
the chance that this will get turned into miner-committed indexes? Or
even just provided at all? We keep on saying that miner-commitments may
next happen at all because of performance issues, and adding n extra
indexes doesn't exactly help that situation. I really suspect that the
moment that gets implemented we'll see wallet software use that for
simple security reasons, so plan ahead for that.
In the short term without miner-commitments it's just a question of how
much extra load we subject servers to. Again, getting people to even
implement prefixes isn't a trivial argument to make, yet bloom has some
serious scalability problems. (though does do roughly what you're
In any case, your "bait" proposal is stealth address specific - how
would you propose applying the same principle to all addresses? Again,
it's a tradeoff between brittleness - connecting to a malicious peer
reveals your wallet - and blockchain stats data.

@_date: 2014-01-24 04:05:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP0039: Final call 
Is it? Surely the trezor can bruteforce, say, 8 bits == 0. How many
SHA256/sec can the trezor hardware do? Generating your seed is a
one-time thing after all - that taking 10-30s doesn't seem like a big
deal to me.
Even a 1/256th "checksum" will really cut down on the number of mistakes
made and money lost.

@_date: 2014-01-24 04:17:33
@_author: Peter Todd 
@_subject: [Bitcoin-development] unlinakble static address? & spv-privacy 
Something to keep in mind is that it's quite likely that the indexes
available will be over H(scriptPubKey). There's really good engineering
reasons for doing this: you need to be able to create succinct proofs of
fraud in indexes, miner committed and otherwise, and the only way they
are succinct is if you limit the length. Hashes naturally do that
because it's so expensive to generate partial collisions.
If you don't do this on the other hand now you have a situation where
the usual case - max 16 level deep tree -  and worst case - hundreds or
even thousands of levels deep - are vastly different. That's hard to
test for and likely to reveal implementation-specific limits in nasty
Anyway, grinding nonces isn't much of a burden given it's fast hash
functions. The prefixes in question are fairly small and will be small
for the forseeable future. As I said elsewhere in this thread, even
Javascript has performance that's perfectly adequate for the task.

@_date: 2014-01-24 10:26:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bait for reusable addresses 
Resolved for some users, not for all. The underlying trade-off will
always be there; less bandwidth makes it harder, more addresses to check
makes it harder; an HD wallet used properly without re-using addresses
will quickly lead to a fairly full bloom filter unless addresses are
expired, and expiration leads to scenarios where funds can be lost.
I think we need to provide users with better options than that.

@_date: 2014-01-24 11:13:30
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bait for reusable addresses 
You know, we've made this discussion rather confusing because we're
using the term "prefix" for both prefix filters - which are equivalent
to bloom filters but with better scalability - and the act of forcing a
scriptPubKey to match some given prefix. I suggest we call the latter
concept 'wallet clustering' as it can just as easily be applied to bloom
filters, as well as Gregory Maxwell's candidate bait scheme, and for
that matter, prefix filters with a tweak option, e.g. H(scriptPubKey |
So yeah, clustering schemes make network flow analysis easier if the
attacker only has blockchain data to work from. But they can also make
network flow analysis significantly harder for attackers that have query
logs from attackers running nodes, and as we know sybiling the network
to get query logs is very easy. I'd rather develop systems that don't
fail catastrophically against sybil attack.
The hostile network is likely to have a significant percentage of
hostile, query-logging nodes. For one thing, running nodes is expensive
and would be even more so in a blocksize limit raising scenario, and a
easy way to pay those costs is by selling query data.
Sure, in some cases you can use zero-length prefixes with trusted nodes;
not many users have access to such nodes.
Conversely, it'd be interesting if someone can dig up a proof showing
that doing much better than Gregory's ambiguity tradeoff is impossible.
My gut feeling is that it is, especially if you take into account the
desire for scalability - if we're to make the blocksize bigger assuming
all nodes have all data for every block just isn't going to happen.
Yes, and I think such schemes should be pursued. But in the near-term
what can we offer users?
Remember that making stealth addresses and similar clustering-using
schemes capable of backward compatible upgrades isn't hard; if the
crypto is found later it can be adopted.
What is harder is that people want miners to commit to various types of
indexes - changing those indexes would require a soft-fork and there's
much pressure for those indexes to have very good performance
Note how well the OpenPGP + bitcoin address UID ideas I and others have
been talking about meshes with TOFU: the logic for "Do I trust this
address to send money?" and "Do I trust this PGP key to send more
encrypted mail/verify signatures?" is just different questions about the
same human identity, so combining the two is synergistic. For instance I
might want to communicate securely with a friend via email and also send
funds to them securely.
An interesting nuance is ideally that UID can be used for more than just
a single address type, e.g. BIP32 derivation chains can the same root
pubkeys as stealth addresses. Though I don't know if the added
complexity is worthwhile vs. just adding another UID for the BIP32
derivation case.

@_date: 2014-01-28 12:23:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
Also users don't have infinite unspent txouts in their wallets - if they
need to make two payments in a row and run out their wallet software is
(currently) going to spend the change txout and either be forced to
broadcast both transactions anyway, or the second payment-protocol-using
recipient will do so on their behalf. (in the future they might also do
a replacement tx replacing the first with a single tx paying both to
save on fees, again with the same problem)
Anyway what you want is payment atomicity: the customer losing control
of the funds must be atomic with respect to the payment going through.
From that point of view it's unfortunate that Payment message contains
refund_to, memo, etc. That information should have been provided to the
merchant prior to them providing the list of addresses to pay.

@_date: 2014-01-28 16:12:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
That's the reason you use a model where things happen atomicly: the
funds either can or can't be transferred, so if the merchant screws up
due to a server failure at worst the wallet can always send the
original, signed, payment request and transaction details proving to the
merchant that they agreed. Since the asked for txouts exist in the
blockchain they must either refund the money, or ship the goods.
Wallet software can handle that kind of worst-case failure by
automatically sending the original payment request back to the merchant.
At worst all customer support has to do is tell the customer "Sorry
about that; we didn't get your payment. Please start your wallet up and
hit the 'resend transaction' button in your wallet and we'll clear that
right up."
Keep in mind that we're probably going to see fraudsters figuring out
ways to make payment servers fail. This means conversely that a customer
calling up a merchant and saying "Hey! Something didn work but the
wallet says I paid!" is going to be treated more suspiciously. By using
atomic protocols the issue of did or didn't they pay becomes much more
black and white, and failure resistant. That's exactly what we keep
saying Bitcoin offers that PayPal doesn't.

@_date: 2014-07-15 04:20:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin address TTL & key expiration? 
Note that "digitally signed" has no value here without some kind of
PKI/WoT/something else to know what key is doing the signing. I believe
Jeff is really referring to the checksum by "hash-sealed" here, which is
as good as is worth getting.
A few months ago I looked into what low-level details it'd take to add
Bitcoin addresses to OpenPGP keys a few months ago; one of the
requirements we came up with was to make sure the standard OpenPGP
expiration machinery would still work. Basically in that model the
Bitcoin address - most likely a stealth address for privacy - is added
to the key as signed data. All signatures in OpenPGP have optional
expiration times, and additionally they can be revoked after the fact if
needed as well.
Of course, such ideas aren't limited to OpenPGP - all payment protocols
should consider adopting them.
As for protocol level hacks, keep in mind that anything that makes a
transaction invalid because of the presence of a specific scriptPubKey
in a txout has the potential to make a whole chain of transactions
become invalid during a reorg. Adding such protection in the form of
IsStandard() policy would be ok, but as a protocol rule it'd be pretty
dangerous. IMO much better to just solve the problem at the UI level.

@_date: 2014-07-21 15:24:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Policy for DNS seeds 
Might be worthwhile to also write an "Expectations for DNSSeed users"
outlining what security properties the seeds actually have, and what
kind of attacks are possible. Many users would be better served with
seeds that offer authenticated and encrypted connections to the seeds
for instance. (esp. if they're using authed/encrypted connections to
nodes, e.g. Tor hidden services)
Along the lines of my above point, for Bitcoin Core users of the
DNSSeeds what constitutes a "functioning" Bitcoin node is much more
broad than what other users might need.
Note that singling out a group of hosts to receive different results
with DNS is especially difficult as you'll be usually singling out
different ISP's rather than hosts themselves. That said if we ever start
operating HTTPS or similar seeds this expectation will become even more
relevant for them.
I'll let others refine the exact wording. but I broadly agree with these
For the testnet DNS seeds - IE my one - my thoughts are the rules should
be identical. Most of the above is related to privacy rather than
security, which apply equally well on testnet. While there have been
suggestions to use the testnet seeds for testing vulnerabilities, the
public discussion clause should suffice to allow those exceptions. I
also suspect that vulnerabilities are likely to be dismissed by a large
part of the community if demonstrated with DNSSeed operator

@_date: 2014-07-27 18:22:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Time 
That's correct, but irrelevant for this application. The "gaming"
possible is only a few bits; gaming more bits than that either makes
blocks invalid due to being >2hr in the future, or < the median time in
the past. In addition doing the latter causes difficulty to rise.
Also see: "Re: [Bitcoin-development] 32 vs 64-bit timestamp fields" -
          Peter Todd - 08 May 2013
Only if the app is trying to use the blockchain non-interactively. The
right way to use the blockchain for determining the current time is to
create a nonce, timestamp it, wait for a confirmation, and get the
merkle path to the block header. This proves the attacker has spent at
least whatever resources it took to create a block considered valid by
your application. (you'll probably want to have a fairly high
See Reminds me: anyone know if tlsdate is able to produce timestamp proofs
verifiable by third-parties? If it could in conjunction with the
blockchain as a random beacon you could at least show dishonesty by
showing that google.com/etc. signed a HTTPS header with a time prior to
when some block was created. Right now unlike the blockchain these
independent servers can easily get away with timestamp fraud,
particularly if they manage to target your specific application. (use
Equally, the blockchain has the advantage that it's easy to show that
invalid blocks are being created for the purpose of creating fake
timestamps; it'd be reasonable for the P2P network to relay any block
header seen with a difficulty > some anti-DoS threshold. Gavin already
did something similar with relaying invalid blocks in pull-req It had the flaw of making network splits worse, but in conjunction with
a separate "invalid-block" inv type I think the issue goes away.

@_date: 2014-07-27 22:40:30
@_author: Peter Todd 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
Boring explanation: some mining pool wants to get a lower orphan rate by
connecting to the whole network simultaneously and has cleverly setup
their node as a Tor exit node to get some plausible deniability.
Of course, reducing orphan rates is indistinguishable from a sybil
attack; in general setting up such a node can be plausible deniability
cover for any type of attack. One possibility would be to sybil attack
the network to do logging; another would be DoS attacks. For the latter
we're pretty vulnerable to the Bloom IO attack(1). The former attack is
possible too, though I'd expect an attacker to want to do it in a less
obvious way and run more than one node. Also running one big Tor node is
less than ideal as it won't accept incoming connections, which lets you
attack SPV clients. Finally note how you can plausibly conduct the
attack directly from the node itself without bothering to actually use
the Tor network.
Anyway, just goes to show that we need to implement better incoming
connection limiting. gmaxwell has a good scheme with interactive
proof-of-memory - where's your latest writeup?
1)

@_date: 2014-07-28 07:28:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I've got a bitcoin-only exit running myself and right now there is absolutely no traffic leaving it. If the traffic coming from that node was legit I'd expect some to be exiting my node too.
Multiple people have confirmed the node is connected to an abnormally large % of the Bitcoin network. Looks like a Sybil attack to me, trying to hide behind a Tor exit node for plausible deniability.

@_date: 2014-06-04 08:54:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] Timelock: time-release encryption 
Decided to take a break yesterday and write some code...
Create a secret key that can be decrypted in a known amount of time
using parallel-serial hash chains. The creator can compute the timelock
in parallel, taking advantage of the large amount of cheap parallelism
available today, while others are forced to compute it serially,
constrained by the lack of scalar performance growth.
The chains are constructed such that Bitcoin addresses can be derived
from them and bounties placed, incentivizing third-parties to crack the
timelocks. This gives us a valuable chance to incentivize others to push
the envelope of scalar performance - important knowledge if we are going
to have any hope of knowing how soon our timelocked secrets will
actually be revealed! The Bitcoin secret keys and addresses are
constructed from the chains as follows:
    iv ->  -> privkey -> pubkey -> secret -> hashed_secret
    secret        = SHA256(pubkey)
    hashed_secret = RIPEMD160(secret)
Unlocking a given chain starting from the initialization vector gives
the person doing the work the private key, giving them an exclusive
opportunity to collect the bounty. Collecting that bounty forces them to
reveal the pubkey, from which the secret is derived. The hashed_secret
is then just a standard Bitcoin address, letting everyone see how large
the bounty is for unlocking the timelock.
Only a single algorithm - SHA256 - is supported by design: timelock
encryption works best if we're all on an even playing field.
Sourcecode: Credit goes to Amir Taaki for helping develop the idea.
To make things interesting I've made a ~256 hour timelock with 32
chains, 8 hours per chain. The addresses associated are as follows:
I've funded them with 10mBTC each, 320mBTC total. The full timelock
definition is as follows:
    "chains": [
        {
            "algorithm": "sha256",             "encrypted_iv": null,             "hashed_secret": "1ERvMr5J8FETF7zj4QM98u8ZANaL1o9XGZ",             "i": 0,             "iv": "353b124909ec8774325d3f2f6b0a01c839e79e3ce687ee6e893310368afdf336",             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "495c6092c9004dc23c2deefb1db1deba88f8895a319f5d9f7ce8a53b2a9ecfe9",             "hashed_secret": "18h7LwKpd9c6u8zJka3vMCASa8BfbiZFd4",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "8cbdf731374a71f22d59251890997ea001c2efa6e9c0bf808700c3d432ccf269",             "hashed_secret": "1DBJDp57QmbigLEbUsFeqJT3mkArGzH3gv",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "bc4d75fa5ec8f28e121cd73806ce058e82036577130cffb3bd596f2aa59b2a7f",             "hashed_secret": "1C1d6Tj7mZADurfj5yJ64p5BeRstquk7pu",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "80c622521c7aed318dac2b3c7d929e4e504d3c60650009b1abc0bae18fff979b",             "hashed_secret": "1FsH58jnq5Kc6D7hb7vmUhjh8fwnijiWss",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "1ba76f9a694f987e1758dbc65cf104984dbedfae01b134fe09135b64cb8c4034",             "hashed_secret": "1PDwPYStrrkKpGtV2zX71XCDYL3E2g3KM7",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "1cedd5a71a96f439c20e9f4f5957b8c159b23d472063fa07a05121333888cdf5",             "hashed_secret": "1BF6oYLeTG7WxNWJjC8p4SHz62q7vgZhXX",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "30d88e0fd113ce38d22a210012914e5331bd89dda408d85dfa9e9bfcbd40d8eb",             "hashed_secret": "1DxFiHr9ehVvi8JR2Cc5pcD4DEURUeFFda",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "5e654bfc1534b03e45548b59d407518b5915c3ae51c266e374ea8a368f72433c",             "hashed_secret": "1NKg3buX5BzhYgSY6Yvws9kMqZ6F1xnneD",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "10095f9aa85ceb221d00975a1db66fcffcae516c1d3d5dee66995567522f68ab",             "hashed_secret": "1Kptov9sgDqQcavz1rVsuQvfi8PcQcL4rc",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "6ecfd5282c650b1ebabbf39f5e72dd0e8e9724456e6b357e83c53a9a144b49eb",             "hashed_secret": "1P9k1HEfe3Z8LaiFejqsuDLjWXPZk1coDL",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "197796e5010f189165c2010cd7cfb0f3b933670276231292555b5540765bb6fa",             "hashed_secret": "1Mw25mU89Wp2b9zyMCSnjAVQ7x4AVWx3sf",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "58fe436839724b03f3ffd41e864d5dc030de2eeb5a3c4df19ed65ef9abe21cc2",             "hashed_secret": "1B8gdbd1StpVnV99Few1ae1XXfKH6iry1D",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "d2af3dd9cdfbb081149212ead19cbd85f6dcc31177de586d1c6dd6139c16141f",             "hashed_secret": "13bisx8T42CzmFA2oAm1evEytcijrHJ2iR",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "512f20d2a4f342cb635d5c73a0c3ad30272bdd84d73f2bb0f2849d17557f162f",             "hashed_secret": "1DyR6aNSbrJCzwqdeo9UN5obGR9L73Y2sa",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "75cb98d4abfd84318347220547f017f8285333dd55b41622c9220ece49e227e2",             "hashed_secret": "1Fx7j3gU3q7bQdoni2zZhGQx7BnLt8xNeK",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "d1d264674ba9b1c4baba189809a0d37ca4b8f402f91a145303907c3730ff890c",             "hashed_secret": "19q541m17opVcAxwisem7ak7YPuSpdS3Uj",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "31f2df6ae00e118f72063cb640c150bad5c0c015d12533826439d45f593ebb89",             "hashed_secret": "1DYWzBpLBDd7fpta2JYLf7QZ7nAYiZiamR",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "8113452a165b673957f6d707f19d6b407683499ef6a08e03ba35ca79306b8e3b",             "hashed_secret": "13sZYBMdyzffwvCkzhGTu49pCGGR5C5B9v",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "c0b8e246cd4a2a1abe760b1d2763931c8e04960e6a2e69f0f6723dd4892413ae",             "hashed_secret": "1DFSjViJLTNhNj5wvsqRwoqPyR9UTAzLgW",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "c4d0bc902093f21e1745193345909e080dee5b4cc410108a637ded75fcdc9e01",             "hashed_secret": "13Zi9eawpkzZwRC8RgHJausQmUS4ECBzho",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "588d778adb3180a9783781a175171790d8099d7942190fa853b235262c074a99",             "hashed_secret": "1BCQ5GVkAEzNYZ5WroAwXSspgpYfhQrojd",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "643b3afe4b674b44fee943507559d6218c5f53671ab01b09c8aaafa65f9f2e37",             "hashed_secret": "1K1gbE7qGLwbJDEoRyrCGxCsgJ4mHcFKwP",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "62e65d48eb215734c00165130027a6cb04842d95cc570a278968f7070d41936f",             "hashed_secret": "14Z4EawE1Kd3HRut43vWjgYUWYR3ZV5i2p",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "f89566d49c4765522cd382c16589486b90702cc99caff6801a4e8b504694ab6b",             "hashed_secret": "1NZTFVXVzKXvPMsFKZfkhZCBcbzqm7cGCj",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "3d9acd9785f12890b88f30576d81d583ba0ada2d98c9a17f6cc79a0ed1b7e027",             "hashed_secret": "1GFBBahQC5DzpLuJc6X1yVvvSKP2kCpUGf",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "4d886799cfef728add5c14b27e780f96511b78060d86d3b77b0d0ed5130f6f02",             "hashed_secret": "184PkQFdzhPR3TjoCxddsNK7sr3DVVhJ45",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "24b495c95129ca0062297b019af3684b12303382e4282e032ba5fd2ab805b127",             "hashed_secret": "14SLK5fXWdajoQZ2AX5WmAGbvRtLpdTgMq",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "0eec24b55713a85a41c996904def8f8b5b61a9efa2491dacf63f7161a71a6514",             "hashed_secret": "1LagrzYykb8w4NveKrw2SDcpKMezwxX72Z",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "1b3ecc11b7f6cd80a5111b11a15fa2cb8b1c006041f8ad5b7c7316947de2e3fa",             "hashed_secret": "16XdtV2U3ksdhKkByRpiq3VN61aB62Ndgh",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "7fed7012e601fb9431d48c072d060c391b1985fd9717d51b12bbcb7e271f656f",             "hashed_secret": "1Mu1SaUBu7aV4DHzEc4hxcgGzaYWdvuaAh",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        },         {
            "algorithm": "sha256",             "encrypted_iv": "7f3a0c156be2c08714b9296999dddca8c178385e145aa3156ea00ffd742075f8",             "hashed_secret": "1EJWWxwZckP9eLS2VzgbUwqzxFDZKZcF9b",             "i": 0,             "iv": null,             "midstate": null,             "n": 86400000000,             "seckey": null,             "secret": null
        }
    ],     "version": 1

@_date: 2014-06-06 04:19:33
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
BIP: Pull-req: Pretty simple really: service bit NODE_BLOOM is defined to allow nodes
to advertise to their peers that they support bloom filters. The network
protocol version number is also bumped. Recommended behavior for nodes
that do not support bloom is to simply disconnect peers who send a
filter* message anyway to let them quickly find another peer.
Rational: Bloom filters are not always supported or appropriate. For
instance no node implementation other than Bitcoin Core supports them,
e.g btcd and obelisk. (which ironically implement this BIP already,
modulo the version number bump) In the long term bloom filters will be
obsoleted eventually as they have poor scaling properties - problematic
with blocksize increases - and are incompatible with UTXO/TXO
commitments, which will use prefix-based filtering instead. (already
being implemented in electrum and obelisk)
In the short term bloom filters have high IO loads, which have lead to
DoS attacks, and are not an optimal use of resources for nodes which are
IO constrained rather than bandwidth constrained. (common in VPS setups
which could better help the network by serving full blocks)
Adding NODE_BLOOM as a service bit now will save us some hassle later
down the road, reflects what actual implementations do anyway, has been
already deployed on Litecoin, Dogecoin, and a zillion other alts with no
issues, (including SPV client support) and is a trivial patch.
Gregory Maxwell: Please assign a BIP #

@_date: 2014-06-06 05:04:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
That's assuming you're doing the proposed prefix brute forcing - if you
don't do that they have privacy equal or better than bloom filters, but
with better scalability. In particular that better scalability lets you
efficiently query multiple servers for blockchain data, only giving up
info on a subset of the addresses in your wallet to each server. This
can be a significant improvement to bloom filters if your attacker is
running logging nodes to try to, say, deanonymize CoinJoin transactions.
Indeed. But again, remember that the existing systems suck too;
prefix-brute forcing is a engineering tradeoff implementable with
existing and well understood technology.
Now if you want to come up with something better and write code, please
do! I'm sure the math exists; what doesn't exist is robust and well
tested code in multiple languages. Stealth addresses at least have been
designed so that future blockchain filter upgrades can be added in a
backwards compatible way.

@_date: 2014-06-06 05:11:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
Yup. Obelisk's indexing is sufficiently fast that they hadn't even
bothered making Dark Wallet store transaction information between
sessions until recently. Prefix brute-forcing isn't yet implemented,
although prefix filters is being implemented for lookups in general. (at
the very least it gives the server operators some valuable plausible

@_date: 2014-06-06 12:46:39
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
(changed subject line as this discussion has nothing to do with
I think you should re-read my original proposal; there's a whole host of
misunderstandings above, for instance I have no idea where you got the
idea that it has anything to do with "saving a byte" came from, or where
the number 128 came from.
Again, you have a misunderstanding. Both bloom filters and prefix
filters are just ways of giving a peer a probabalistic filter to match
transactions against. Where they differ is that bloom filters has O(n)
scaling, where n is the size of a block, and prefix filters have O(log n)
scaling with slightly(1) higher k. Again, if you *don't* use brute forcing
in conjunction with prefixes they have no different transactional graph
privacy than bloom filters, but the better scalability lets you do
things like split your queries across multiple peers that give you
better protection against hostile nodes.  Additionally prefix filters
are compatible with future miner committed indexes to make the data
1) see Amir's experience implementing prefix lookup in Obelisk
Maybe! If adversaries are operating a significant fraction of the peers
you are connecting to the current design of bloom filters + HD wallets
results in situations where those adversaries have better transactional
graph information than the alternative.
That's basically what Electrum and Obelisk already do - by default you
connect to a relatively small set of blockchain data servers operated by
well known people and use the same server repeatedly.
Applying that to the P2P network however is tricky as there is a huge
amount of churn in the nodes:
     < hearn> bitcoinj can't use
    [service bits] as it relies on DNS seeds and that is unlikely to change
    any time soon due to the general churn rate in the network making it
    hard to bootstrap quickly using just remembered sets of IPs.
I know, where can I find running code? Remember that a bug can easily
lose thousands of dollars worth of Bitcoins.

@_date: 2014-06-06 13:05:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
Again, you *don't* have to use brute-force prefix selection. You can
just as easily give your peer multiple prefixes, each of which
corresponds at least one address in your wallet with some false positive
rate. I explained all this in detail in my original blockchain data
privacy writeup months ago.

@_date: 2014-06-06 13:45:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
Because I was designing a system under the assumption that you were
highly likely to connect to an attacker at some point, and the trade-off
available with the available math was to either give very detailed info
to that attacker, or give away some probabalistic info to everyone.
Quite likely - I think most of this disagreement stems from the fact
that we have different starting assumptions. In particular my assumption
that you are likely to end up connecting to an attacker logging data,
and my desire to have a standard that can be implemented with existing
cryptographic primatives. Remember that I'm spending a lot of time
working with wallet authors; they have approximately zero interest in
standards that require crypto any more fancy than HD wallets do.
Scanning performance is different from bandwidth performance. Prefix
brute-forcing was designed to address the latter concern for cases where
you are bandwidth limited and don't have a trusted peer to do the
scanning for you.

@_date: 2014-06-08 17:35:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
As I explained in the email you're replying to and didn't quote, bloom
filters has O(n) cost per query, so sending different bloom filters to
different peers for privacy reasons costs the network significant disk
IO resources. If I were to actually implement it it'd look like a DoS
attack on the network.
Essentially with bloom filters you have to make a tradeoff between
scalability and privacy; with prefix filters you don't have to make that
ugly tradeoff. Notably that tradeoff gets worse if we ever increase the
Bitcoin blocksize.

@_date: 2014-06-08 17:45:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
You're completely correct. You can use the same nTweak value for each
filter and then slice up the filters bitwise, but then you end up
linking every query you make to your identity unless you just used a
fixed nTweak that everyone else uses.  (e.g. zero) If you do that, you
still have the problem that you're greatly increasing the load on the
In any case, all this shows is that in the future bloom filters will
very likely go away eventually, and to make that upgrade path smooth it
only makes sense to define a way for nodes to let others know whether or
not bloom is supported. A NODE_BLOOM service bit is a very reasonable
and simple way to do exactly that, and is defacto what implementations
that don't support bloom filters do anyway.
Note BTW that re: DNS seeds, once the NODE_BLOOM BIP is accepted and the
NODE_BLOOM patch merged into bitcoind, I'll write a patch for sipa's
seeder to make it only return seeds with bloom filter support.

@_date: 2014-06-10 13:08:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bloom bait 
Suppose I wrote an single address lookup tool for Android that connected
to multiple peers and used bloom filters to find the history of a
specific address. Of course, I don't want to use too much bandwidth
being on mobile, so I'll use as specific a bloom filter as possible. I
might even connect to multiple peers to speed up the lookup.
Is this any different from my bloom filter IO attack code? Nope. Hence,
splitting up bloom filter requests for better privacy will certainly
look like a DoS attack and will certainly greatly increase the load on
the network.
That's exactly the kinds of optimizations obelisk is implementing to
make its prefix lookup database fast. Also those optimizations are
situation dependent, for instance "packing the index next to each block"
is irrelevant if you put archival blockchain data on a slow HD, and
indexes on a fast SSD, something some obelisk servers do.
More to the point, your showing quite clearly there isn't just one
optimal way to do it. Applying a bloom filter, or a prefix filter, or
some as yet unknown filter, to blockchain data is a service and that
service has different tradeoffs compared to just serving up archival
block history. There is zero reason not to make that service something
you advertise with NODE_BLOOM - after all, you already have the code in
bitcoinj to do the exact same thing by checking the advertised protocol
Yup. I discussed this with Matt Corallo at the financial crypto
conference a few months back and he made the same point. Unfortunately
we'll need an upgrade to let nodes advertise ranges of blocks to begin
to fix that issue, and even then it still shows quite clearly how it's
not optimal if we force everyone to share blockchain data in the same

@_date: 2014-06-16 16:50:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fidelity bonds for decentralized instant 
You can always use fidelity bonds, or as I called it at the time(1),
"Trusted identities":
    Lets suppose Alice has some bitcoins held at bitcoin address A. She
    wants to establish trust in the "identity" associated with the ECC
    keypair associated with A, for instance for the purpose of having other
    users trust her not to attempt to double spend. Since the trust she
    seeks is financial in nature, she can do this by valuing the identity
    associated with A, by delibrately throwing away resources. A simple way
    to do this would of course be to transfer coins to a null address,
    provably incurring a cost to her.
    A more socially responsible way would be for her to create a series of
    transactions that happen to have large, and equal, transaction fees.
    Bitcoin makes the assumption that no one entity controls more than 50%
    of the network, so if she makes n of these transactions consecutively,
    each spending m BTC to transaction fees, there is a high probability
    that she has given up at least n/2 * m BTC of value. This of course is
    all public knowledge, recorded in the block chain. It also increases the
    transaction fees for miners, which will be very important for the
    network in the future.
    Now Bob can easily examine the block chain, and upon verifying Alice's
    trust purchase, can decide to accept a zero-confirmation transaction at
    face value. If Alice breaks that promise, he simply publishes her signed
    transaction proving that Alice is a fraudster, and future Bob's will
    distrust Alice's trusted identity, thus destroying the value needed to
    create it.
    In effect, we now have a distributed green address system.
Note that the second paragraph is seriously obsolete - better to either
use announce-commit sacrifices, or much preferably, simple destruction
of coins. (sacrifice to fees encourages mining centralization for
obvious reasons)
1) "[Bitcoin-development] Trusted identities", Apr 26th 2012, Peter Todd,
   Incidentally, my first post to this mailing list!

@_date: 2014-06-17 03:23:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: allocate 8 service bits for 
For my replace-by-fee implementation(1) I used service bit 26 to let
preferential peering work so that replace-by-fee nodes could easily find
each other. Of course, that's a temporary/experimental usage that can be
dropped after wider adoption, so I included the following comment:
    // Reserve 24-31 for temporary experiments
    NODE_REPLACE_BY_FEE = (1 << 26)
Service bits are never a guaranteed thing anyway, so occasional
collisions can and should be tolerated by applications using these
experimental service bits.
Alternately Wladimir J. van der Laan brought up elsewhere(2) the
possibility for a wider notion of an extension namespace. I'm personally
not convinced of the short-term need - we've got 64 service bits yet
NODE_BLOOM is the first fully fleshed out proposal to use one - but it's
worth thinking about for the long term.
1) 2)

@_date: 2014-06-17 20:15:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: relax the IsStandard rules for 
I'm also working on a very similar patch with some additional
protections to ensure forwards compatibility during soft-fork upgrades
and provide protections against transaction malleability.
The script execution code is probably in the 99.9th percentile of open
source code in terms of code review.
3. The risk that non-upgraded miners produce invalid blocks after a
   soft-fork due to them mining transactions that are now invalid.
4. Transaction malleability.
In addition to these changes the soft-fork-safe patch I'm working on
would do the following:
a) Define an opcode whitelist of soft-fork-safe opcodes.
This whitelist includes every opcode but the invalid opcodes, and most
importantly, the OP_NOPx opcodes that may be redefined in a future
soft-fork with new behavior. This rule, along with rejecting transations
with unknown nVersion's, ensures that a miner still running an old
version of Bitcoin Core will only mine transactions that the new version
of Bitcoin Core considers valid.
b) Consider scripts that leave extra items on the stack after execution
   to be non-standard.
As per Pieter Wuille's BIP62 "Dealing with malleability" extra scriptSig
pushes are a malleability source. If adding extra pushes causes a
transaction to be invalid, scriptPubKeys will in most cases be
automatically non-malleable. This change I've already submitted as a
separate pull-req: I'd be happy to add the above to your existing patch and submit the
pull-req for you.

@_date: 2014-06-19 06:09:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: relax the IsStandard rules for 
Well, just doing one and not the rest isn't necessarily a good idea. The
malleability protection definitely seems like a good idea, and has had
quite a bit of review.
Do we have consensus that future soft-forks to add new opcodes will
always be done in conjunction with a transaction nVersion bump? If so,
then that's ok, if not, then we should have a whitelist.
The code to restrict the opcodes to the softfork-safe subset is trivial,
a GetOp() loop and a switch statement. It can always be removed later.
Something that comes to mind is if we do always bump nVersion then
OP_NOPx always will have a parallel "do-nothing" behavior, which means
EvalScript() will always have to have code enabling that backwards
compatible behavior.

@_date: 2014-06-19 20:45:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: relax the IsStandard rules for 
Sounds like it could turn EvalScript() into a mess over time, but that's
a long way away. Anyway a BIP will be useful.
Yeah, that's what I implemented in

@_date: 2014-03-02 15:40:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] Procedure for non-tech contributions 
I proof-read rc1 and simply submitted my changes via pull-req:
    I'd say to encourage that method. If someone doesn't know how to use
git, yet still wants to proof-read, just send us a text-file with all
your corrections applied. We've got the tools to diff those changes
ourselves; no fancy software is required.
MtGox does host the bitcoin wiki, so yes, the funds probably do go to a
wallet held by MtGox in some fashion.

@_date: 2014-03-05 14:39:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] New side channel attack that can recover 
If you're following good practices you're not particularly vulneable to
it, if at all, even if you make use of shared hosting. First of all you
shouldn't be re-using addresses, which means you won't be passing that
~200 sig threshold.
More important though is you shouldn't be using single factor Bitcoin
addresses. Use n-of-m multisig instead and architect your system such
that that every transaction that happens in your service has to be
authorized by both the "online" server(s) that host your website as well
as a second "hardened" server with an extremely limited interface
between it and the online server. The hardened second factor *should*
use a separate codebase, ideally even a second language, to authenticate
actions that withdraw funds or generate new addresses based on data
given to it by the online server. In the best case your customers are
PGP-signing requests so you can verify their intent independently and
cryptographically on both servers. Mircea Popescu's MPEx exchange is an
example of this model, although I don't think they're doing any multisig
stuff. Failing that you can at least use the second server to do things
like limit losses by flagging higher-than-expected withdrawl volumes and
unusual events.
Since this second-factor server only deals with business logic - not the
website - you can certainly find a secure hosting arrangement for it
with physical control. I recommend you stick the machine in your
apartment and use tor + hidden services to connect to it from your VM
Note too that even if all you're doing is accepting Bitcoins from
customers, perhaps in exchange for goods, all of the above *still*
applies modulo the fact that the payment protocol is very incomplete.
With P2SH (finally!) supported in all the major Bitcoin wallets there
simply is no excuse not to have such an architecture other than lazyness
and transaction fees; if you fall into the latter category you're
business may very well be wiped out anyway by increased fees.

@_date: 2014-03-05 15:32:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] New side channel attack that can recover 
That's nice, but I wrote my advice to show people how even if they don't
know any crypto beyond what the "black boxes" do - the absolute minimum
you need to know to write any Bitcoin software - you can still defend
yourself against that attack and many others.
Point is you can architect systems that remain secure even when parts of
them fail, and you don't need any special cryptographic background to do
so - any competent programmer can.
Meanwhile, if you're not willing to take those simple steps, the Bitcoin
community damn well should look down on your amateur efforts, e.g.
Coinbase and EasyWallet.

@_date: 2014-03-11 17:12:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] Multisign payment protocol? 
Maybe never: you can implement a wallet that uses stealth addresses for
change, and doing appears to be advantageous in some scenarious with
regard to privacy.

@_date: 2014-03-12 05:44:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] New side channel attack that can recover 
I think you're misunderstanding me: I'm assuming one of the n parties
signing transactions in my multi-factor authentication scheme is
uncompromised - much easier to do when it's a low-bandwidth box sitting
in a secure location.
Not re-using keys is nice too of course, and while not perfect - your
above scenario - certainely helps limit losses.

@_date: 2014-03-12 12:24:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Multisign payment protocol? 
Multisig signing is an example of untrusted input from an adversary;
this is a good example where you'd be better off just handling it
correctly rather than trying to "make it easier". (although handling it
correctly may include internally converting every not-yet-signed dummy
signature into a 73 bytes pushdata prior to calculating the size)

@_date: 2014-03-12 12:47:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] Multisign payment protocol? 
It's the raw transaction API and Bitcoin Core wallet. What should be
carefully and loudly documented is the simple advice "Don't use to hold
customer funds; use Bitcoinj or something instead." followed by a
warning that SPV isn't secure enough for a business unless you run your
own full node.
The raw transaction API has no support to handle fees at all.

@_date: 2014-03-15 09:43:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] python-bitcoinlib v0.1 release - a low-level 
Hash: SHA256
I noticed that the ngccbase Colored Coin client(1) added a
python-bitcoinlib dependency, specifically my fork. In addition there is
also now a rudementary python-bitcoinlib package in archlinux.
So with that in mind I'm releasing v0.1, perhaps somewhat arbitrarily:
    This Python2/3 library provides an easy interface to the bitcoin data
structures and protocol. The approach is low-level and "ground up", with
a focus on providing tools to manipulate the internals of how Bitcoin
works in a Pythonic way, without straying far from the Bitcoin Core
The current status of the library as of v0.1 is that the support for
data-structures related to transactions, scripting, addresses, and keys
are all quite usable and the API is probably not going to change that
much. Bitcoin Core RPC support is included and automatically converts
the JSON to/from Python objects when appropriate.  EvalScript(),
VerifyScript(), and SignatureHash() are all functional and pass all the
Bitcoin Core unittests, as well as a few that are still yet to be
merged.(2) You'll find some examples for signing pay2script-hash and
p2sh txouts in the examples/ directory; I personally used the
transaction signing functionality to make up a set of unittests related
to OP_CODESEPARATOR and FindAndDelete() recently. Finally my dust-b-gone
script(3) is another good example, specifically of the RPC
I personally haven't had any need for the p2p network related code for
some time, so I'm sure it's not in a good state and it lacks unittests;
Bloom filters for one are missing the merkle-block support to actually
make them useful. But the RPC support makes up for that for many uses.
This release and others in the future are signed by my PGP key, as well
as every publicly pushed commit. You can verify the key via WoT, my
bitcointalk account, signing history in the Bitcoin Core repo, and
mailing list records among other sources.
Disclaimer: This is alpha code in a language not known for type-safety.
            I wouldn't personally use python-bitcoinlib for anything
            other than experiments and neither should you.
1) 2) 3)

@_date: 2014-03-15 10:34:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] python-bitcoinlib v0.1 release - a 
Also, for those who don't know the history of python-bitcoinlib, credit
where credit is due: my fork is based on Jeff Garzik's implementation(1)
and the bulk of the code structure is his work, modulo "pythonizing"
that I have done.
1)

@_date: 2014-03-15 13:22:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] python-bitcoinlib v0.1 release - a 
I've noticed it looks like people actually using my 'pythonize' code
have been linking directly to my tree in things like documentation and
build scripts, so the URL is probably not a problem.
I did open a pull-req on the bitcoin.org repo to change that URL
however:

@_date: 2014-03-22 04:47:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
There's been a lot of recent hoopla over proof-of-publication, with the
OP_RETURN  length getting reduced to a rather useless 40 bytes at
the last minute prior to the 0.9 release. Secondly I noticed a
overlooked security flaw in that OP_CHECKMULTISIG sigops weren't taken
into account, making it possible to broadcast unminable transactions and
bloat mempools.(1) My suggestion was to just ditch bare OP_CHECKMULTISIG
outputs given that the sigops limit and the way they use up a fixed 20
sigops per op makes them hard to do fee calculations for. They also make
it easy to bloat the UTXO set, potentially a bad thing. This would of
course require things using them to change. Currently that's just
Counterparty, so I gave them the heads up in my email.
To make a long story short, it was soon suggested that Bitcoin Core be
forked - the software, not the protocol - and miners encouraged to
support it. This isn't actually as trivial as it sounds, as you need to
add some anti-DoS stuff to deal with the fact that the hashing power
mining the transations you are relaying may be quite small. The second
issue is you need to add preferential peering, so the nodes in the
network with a broader idea of what is a "allowed" transaction can find
each other. (likely with a new service flag) It'd be a good time to
implement replace-by-fee, partly for its anti-DoS properties.
Which leaves us with a practical question: How do you gracefully handle
a switchover? First of all I suggest that proof-of-publication
applications adopt format flexibility, similar to how Mastercoin can
encode its data in pay-to-pubkeyhash, bare multisig, or op_return
outputs. Given the possibility of bare multisig going away, I'd suggest
that P2SH multisig scriptSig encoding be added as well. Note that a
really good implementation of all this is actually format agnostic, and
will let PUSHDATA's used for encoding data be specified arbitrarily. I
wrote up some code to do so awhile back as an experiment. It used the
LSB's of the nValue field in the txouts to specify what was and wasn't
data, along with some stenographic encryption of data and nValue. I'd be
happy to dig that up if anyone is interested.
All these methods have some overhead compared to just using OP_RETURN
and thus cost more. So I suggest you have your software simultaneously
double-spend the inputs to any proof-of-publication transaction with a
second transaction that just makes use of efficient OP_RETURN. That
second one can go to more enlightened miners. Only one or the other will
get mined of course and the cost to publish data will be proportional to
the relative % of hashing power in the two camps.
Finally I'll be writing something more detailed soon about why
proof-of-publication is essential and miners would be smart to support
it. But the tl;dr: of it is if you need proof-of-publication for what
your system does you're much more secure if you're embedded within
Bitcoin rather than alongside of it. There's a lot of very bad advise
getting thrown around lately for things like Mastercoin, Counterparty,
and for that matter, Colored Coins, to use a separate PoW blockchain or
a merge-mined one. The fact is if you go with pure PoW, you risk getting
attacked while your still growing, and if you go for merge-mined PoW,
the attacker can do so for free. We've got a real-world example of the
former with Twister, among many others, usually resulting in a switch to
a centralized checkpointing scheme. For the latter we have Coiledcoin,
an alt that made the mistake of using SHA256 merge-mining and got killed
off early at birth with a zero-cost 51% attack. There is of course a
censorship risk to going the embedded route, but at least we know that
for the forseeable future doing so will require explicit blacklists,
something most people here are against.
To MSC, XCP and others: Now I'm not going to say you shouldn't take
advice from people who call your finance 2.0 systems scams, or maybe if
they're nice, indistinguishable from a scam. But when you do, you should
think for yourself before just trusting that some authority figure has
your best interests in mind.
1) Yes, this was responsibly disclosed to the security mailing list. It
   was revealed to the public a few hours later on the -dev IRC channel
   without a fix.

@_date: 2014-03-22 14:21:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fake PGP key for Gavin 
Note that Bitcoin source and binary downloads are protected by both the
PGP WoT and the certificate authority PKI system. The binaries are
hosted on bitcoin.org, which is https and protected by a the PKI system,
and the source code is hosted on github, again, https protected. A MITM
attack would need to compromise the PKI system as well, at least
provided users aren't fooled into downloading over http.

@_date: 2014-03-22 15:08:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
Lol! Granted, I guess I should "disclose" that I'm working on tree
chains, which just improve the scalability of blockchains directly. I'm
think tree-chains could be implemented as a soft-fork; if applied to
Bitcoin the datacoin 1% might face market risk.  :P

@_date: 2014-03-22 15:34:35
@_author: Peter Todd 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
Well remember that my thinking re: UTXO is that we need to move to a
system like TXO commitments where storing the entirety of the UTXO set
for all eternity is *not* required. Of course, that doesn't necessarily
mean you can't have the advantages of UTXO commitments, but they need to
be limited in some reasonable way so that long-term storage requirements
do not grow without bound unreasonably. For example, having TXO
commitments with a bounded size committed UTXO set seems reasonable; old
UTXO's can be dropped from the bounded sized set, but can still be spent
via the underlying TXO commitment mechanism.
Like I said the real issue is making it easy to get those !IsStandard()
transactions to the miners who are interested in them. The service bit
flag I proposed + preferential peering - reserve, say, 50% of your
peering slots for nodes advertising non-std tx relaying - is simple
enough, but it is vulnerable to sybil attacks if done naively.
Right, but there's also a lot of the community who thinks
proof-of-publication applications are bad and should be discouraged. I
argued before that the way OP_RETURN was being deployed didn't actually
give any reason to use it vs. other data encoding methods.
Unfortunately underlying all this is a real ignorance about how Bitcoin
actually works and what proof-of-publication actually is:
    14-03-20.log:12:47 < gavinandresen> jgarzik: RE: mastercoin/OP_RETURN:
    what's the current thinking on Best Way To Do It?  Seems if I was to do
    it I'd just embed 20-byte RIPEMD160 hashes in OP_RETURN, and fetch the
    real data from a DHT or website (or any-of-several websites).
    Blockchain as reference ledger, not as data storage.
I think we're just going to have to agree to disagree on our
interpretations of the economics with regard to attacking merge-mined
chains. Myself, I'm very, very wary of systems that have poor security
against economically irrational attackers regardless of how good the
security is, in theory, against economically rational ones.
Again, what it comes down to in the end is that when I'm advising
Mastercoin, Counterparty, Colored Coins, etc. on how they should design
their systems I know that if they do proof-of-publication on the Bitcoin
blockchain, it may cost a bit more money than possible alternatives per
transaction, but the security is very well understood and robust. Fact
is, these applications can certainly afford to pay the higher
transaction fees - they're far from the least economically valuable use
of Blockchain space. Meanwhile the alternatives have, at best, much more
dubious security properties and at worse no security at all.
(announce/commit sacrifices is a great example of this, and very easy to

@_date: 2014-03-25 08:28:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
Bitcoin doesn't scale. There's a lot of issues at hand here, but the
most fundemental of them is that to create a block you need to update
the state of the UTXO set, and the way Bitcoin is designed means that
updating that state requires bandwidth equal to all the transaction
volume to keep up with the changes to what set. Long story short, we get
O(n^2) scaling, which is just plain infeasible.
So let's split up the transaction volume so every individual miner only
needs to keep up with some portion. In a rough sense that's what
alt-coins do - all the tipping microtransactions on Doge never have to
hit the Bitcoin blockchain for instance, reducing pressure on the
latter. But moving value between chains is inconvenient; right now
moving value requires trusted third parties. Two-way atomic chain
transfers does help here, but as recent discussions on the topic showed
there's all sorts of edge cases with reorganizations that are tricky to
handle; at worst they could lead to inflation.
So what's the underlying issue there? The chains are too independent.
Even with merge-mining there's no real link between one chain and
another with regard to the order of transactions. Secondly merge-mining
suffers from 51% attacks if the chain being merge-mined doesn't have a
majority of total hashing power... which kinda defeats the point if
we're worried about miner scalability.
Blocks and the TXO set as a binary radix tree
So how can we do better? Start with the "big picture" idea and take the
linear blockchain and turn it into a tree:
           ?????????????????
       ?????????       ?????????
     ?????   ?????   ?????   ?????
    ??? ??? ??? ??? ??? ??? ??? ???
Obviously if we could somehow split up the UTXO set such that individual
miners/full nodes only had to deal with subsets of this tree we could
significantly reduce the bandwidth that any one miner would need to
process. Every transaction output would get a unique identifier, say
txoutid=H(txout) and we put those outputs in blocks appropriately.
We can't just wave a magic wand and say that every block has the above
structure and all miners co-ordinate to generate all blocks in one go.
Instead we'll do something akin to merge mining. Start with a linear
blockchain with ten blocks. Arrows indicate hashing:
    a0 ? a1 ? a2 ? a3 ? a4 ? a5 ? a6 ? a7 ? a8 ? a9
The following data structure could be the block header in this scheme.
We'll simplify things a bit and make up our own; obviously with some
more effort the standard Satoshi structures can be used too:
    struct BlockHeader:
        uint256 prevBlockHash
        uint256 blockContentsHash
        uint256 target
        uint256 nonce
        uint time
For now we'll say this is a pure-proof-of-publication chain, so our
block contents are very simple:
    struct BlockContents:
        uint256 merkleRoot
As usual the PoW is valid if H(blockHeader) < blockHeader.target. Every
block creates new txouts, and the union of all such txouts is the txout
set. As shown previously(1) this basic proof-of-publication
functionality is sufficient to build a crypto-currency even without
actually validating the contents of the so-called transaction outputs.
The scalability of this sucks, so let's add two more chains below the
root to start forming a tree. For fairness we'll only allow miners to
either mine a, a+b, or a+c; attempting to mine a block with both the b
and c chains simultaneously is not allowed.
    struct BlockContents:
        uint256 childBlockHash # may be null
        bool childSide # left or right
        uint256 merkleRoot
Furthermore we shard the TXO space by defining txoid = H(txout) and
allowing any txout in chain a, and only txouts with LSB=0 in b, LSB=1 in
c; the beginning of a binary radix tree. With some variance thrown in we
get the following:
       b0 ?? b1 ????? b2 ? b3 ? b4 ? b5 ? b6 ? b7 ? b8
                     ?                        ?
    a0 ? a1 ? a2 ? a3 ?????? a4 ? a5 ? a6 ? a7 ? a8
           ?    ?              ?         ?         ?
       c0 ? c1 ? c2 ? c3 ?????? c4 ? c5 ? c6 ?????? c7
We now have three different versions of the TXO set: ?a, ?a + ?b, and
?a+?c. Each of these versions is consistent in that for a given txoutid
prefix we can achieve consensus over the contents of the TXO set. Of
course, this definition is recursive:
    a0 ? a1 ? a2 ? a3 ?????? a4 ? a5 ? a6 ? a7 ? a8
           ?    ?              ?         ?         ?
       c0 ? c1 ? c2 ? c3 ?????? c4 ? c5 ? c6 ?????? c7
               ?         ?         ?    ?              ?
           d0 ? d1 ?????? d2 ?????? d3 ? d4 ??? d5 ???? d6
Unicode unfortunately lacks 3D box drawing at present, so I've only
shown left-sided child chains.
Herding the child-chains
If all we were doing was publishing data, this would suffice. But what
if we want to syncronize our actions? For instance, we may want a new
txout to only be published in one chain if the corresponding txout in
another is marked spent. What we want is a reasonable rule for
child-chains to be invalidated when their parents are invalidated so as
to co-ordinate actions across distant child chains by relying on the
existance of their parents.
We start by removing the per-chain difficulties, leaving only a single
master proof-of-work target. Solutions less than target itself are
considered valid in the root chain, less than the target << 1 in the
root's children, << 2 in the children's children etc. In children that
means the header no longer contains a time, nonce, or target; the values
in the root block header are used instead:
    struct ChildBlockHeader:
        uint256 prevChildBlockHash
        uint256 blockContentsHash
For a given chain we always choose the one with the most total work. But
to get our ordering primitive we'll add a second, somewhat brutal, rule:
Parent always wins.
We achieve this moving the child block header into the parent block
    struct BlockContents:
       ChildBlockHeader childHeader # may be null (zeroed out)
       bool childSide # left or right
       bytes txout
Let's look at how this works. We start with a parent and a child chain:
    a0 ? a1 ? a2 ? a3
           ?         ?
       b0 ? b1 ? b2 ? b3 ? b4 ? b5
First there is the obvious scenario where the parent chain is
reorganized. Here our node learns of a2 ? a3' ? a4':
                 ? a3' ? a4'
    a0 ? a1 ? a2 ? a3 ? X
           ?         ?
       b0 ? b1 ? b2 ? b3 ? X
Block a3 is killed, resulting in the orphaning of b3, b4, and b5:
    a0 ? a1 ? a2 ? a3' ? a4'
           ?
       b0 ? b1 ? b2
The second case is when a parent has a conflicting idea about what the
child chian is. Here our node receives block a5, which has a conflicting
idea of what child b2 is:
    a0 ? a1 ? a2 ? a3' ? a4' ? a5
           ?                     ?
       b0 ? b1 ?????????????????? b2'
               ? b2 ? X
As the parent always wins, even multiple blocks can get killed off this
    a0 ? a1 ? a2 ? a3 ? a4
           ?
       b0 ? b1 ? b2 ? b3 ? b4 ? b5 ? b6 ? b7
    a0 ? a1 ? a2 ? a3 ? a4 ? a5
           ?                   ?
       b0 ? b1 ???????????????? b2'
               ? b2 ? b3 ? b4 ? b5 ? X
This behavior is easier to understand if you say instead that the node
learned about block b2', which had more total work than b2 as the sum
total of work done in the parent chain in blocks specifying the that
particular child chain is considered before comparing the total work
done in only the child chain.
It's important to remember that the parent blockchain has and validates
both childrens' block headers; it is not possible to mine a block with
an invalid secret of child headers. For instance with the following:
    a0 ? a1 ? a2 ? a3 ? a4
           ?         ?    ?
       b0 ? b1 ? b2 ? b3 ? b4 ? b5 ? b6 ? b7
I can't mine block a5 that says following b2 is b2' in an attempt to
kill off b2 through b7.
Token transfer with tree-chains
How can we make use of this? Lets start with a simple discrete token
transfer system. Transactions are simply:
    struct Transaction:
        uint256 prevTxHash
        script prevPubKey
        script scriptSig
        uint256 scriptPubKeyHash
We'll say transactions go in the tree-chain according to their
prevTxHash, with the depth in the tree equal to the depth of the
previous output. This means that you can prove an output was created by
the existance of that transaction in the block with prefix matching
H(tx.prevTxHash), and you can prove the transaction output is unspent by
the non-existance of a transaction in the block with prefix matching
With our above re-organization rule everything is consistent too: if
block b_i contains tx1, then the corresponding block c_j can contain a
valid tx2 spending tx1 provided that c_j depends on a_p and there is a
path from a_p to b_(i+k). Here's an example, starting with tx1 in c2:
       b0 ?????? b1
                ?
    a0 ? a1 ? a2
           ?
       c0 ? c1 ? c2
Block b2 below can't yet contain tx2 because there is no path:
       b0 ?????? b1 ? b2
                ?
    a0 ? a1 ? a2
           ?
       c0 ? c1 ? c2
However now c3 is found, whose PoW solution was also valid for a3:
       b0 ?????? b1 ? b2
                ?
    a0 ? a1 ? a2 ? a3
           ?         ?
       c0 ? c1 ? c2 ? c3
Now b3 can contain tx2, as b3 will also attempt to create a4, which
depends on a3:
       b0 ?????? b1 ? b2 ? b3
                ?
    a0 ? a1 ? a2 ? a3
           ?         ?
       c0 ? c1 ? c2 ? c3
Now that a3 exists, block c2 can only be killed if a3 is, which would
also kill b3 and thus destroy tx2.
Proving transaction output validity in a token transfer system
How cheap is it to prove the entire history of a token is valid from
genesis?  Perhaps surprisingly, without any cryptographic moon-math the
cost is only linear!
Remember that a transaction in a given chain has committed to the chain
that it can be spent in. If Alice is to prove to Bob that the output she
gave him is valid, she simply needs to prove that for every transaction
in the histroy of the token the token was created, remained unspent,
then finally was spent. Proving a token remained unspent between blocks
b_n and b_m is trivially possible in linear size. Once the token is
spent nothing about blocks beyond b_m is required. Even if miners do not
validate transactions at all the proof size remains linear provided
blocks themselves have a maximum size - at worst the proof contains some
invalid transactions that can be shown to be false spends.
While certainly inconvenient, it is interesting how such a simple system
appears to in theory scale to unlimited numbers of transactions and with
an appropriate exchange rate move unlimited amounts of value. A possible
model would be for the the tokens themselves to have power of two
values, and be split and combined as required.
The lost data problem
There is however a catch: What happens when blocks get lost? Parent
blocks only contain their childrens' headers, not the block contents.
At some point the difficulty of producing a block will drop sufficiently
for malicious or accidental data loss to be possible. With the "parent
chain wins" rule it must be possible to recover from that event for
mining on the child to continue.
Concretely, suppose you have tx1 in block c2, which can be spent on
chain b. The contents of chain a is known to you, but the full contents
of chain b are unavailable:
        b0 ? b1      (b)  (b)
           ?         ?    ?
    a0 ? a1 ? a2 ? a3 ? a4 ? a5
                ?              ?
       c0 ? c1 ? c2 ? c3 ? c4 ? c5
Blocks a3 and a4 are known to have children on b, but the contents of
those children are unavailable. We can define some ratio of unknown to
known blocks that must be proven for the proof to be valid. Here we
show a 1:1 ratio:
                ???????????????
        b0 ? b1      (b)  (b)   b2 ? b3 ? b4 ? b5 ? b6 ? b7
           ?         ?    ?         ?         ?    ?
    a0 ? a1 ? a2 ? a3 ? a4 ? a5 ? a6 ? a7 ? a8 ? a9
                ?              ?         ?
       c0 ? c1 ? c2 ? c3 ? c4 ? c5 ? c6 ? c7 ? c8 ? c9
The proof of now shows that while a3 and a4 has b-side blocks, by the
time you reach b6 those two lost blocks were in the minority. Of course
a real system needs to be careful that mining blocks and then discarding
them isn't a profitably way to create coins out of thin air - ratios
well in excess of 1:1 are likely to be required.
Challenge-response resolution
Another idea is to say if the parent blockchain's contents are known we
can insert a challenge into it specifying that a particular child block
be published verbatim in the parent. Once the challenge is published
further parent blocks may not reference that children on that side until
either the desired block is re-republished or some timeout is reached.
If the timeout is reached, mining backtracks to some previously known
child specified in the challenge. In the typical case the block is known
to a majority of miners, and is published, essentially allowing new
miners to force the existing ones to "cough up" blocks they aren't
publishing and allow the new ones to continue mining. (obviously some
care needs to be taken with regard to incentives here)
While an attractive idea, this is our first foray into moon math.
Suppose such a challenge was issued in block a2, asking for the contents
of b1 to be published. Meanwhile tx1 is created in block c3, and can
only be spent on a b-side chain:
        b0 ? b1
           ?
    a0 ? a1 ? (a2) ? a3
                       ?
         c0 ? c1 ? c2 ? c3
The miners of the b-chain can violate the protocol by mining a4/b1',
where b1' appears to contain valid transaction tx2:
        b0 ? b1              b1'
           ?                ?
    a0 ? a1 ? (a2) ? a3 ? a4
                       ?
         c0 ? c1 ? c2 ? c3
A proof of tx2 as valid payment would entirely miss fact that the
challenge was published and thus not know that b1' was invalid. While
I'm sure the reader can come up with all kinds of complex and fragile
way of proving fraud to cause chain a to be somehow re-organized, what
we really want is some sub-linear proof of honest computation.  Without
getting into details, this is probably possible via some flavor of
sub-linear moon-math proof-of-execution. But this paper is too long
already to start getting snarky.
Beyond token transfer systems
We can extend our simple one txin, one txout token transfer transactions
with merkle (sum) trees. Here's a rough sketch of the concept:
    input    ??output              ?? ??
    input  ???output               ???
    input  ???output              ?? ??
    input    ??output Where previously a transaction committed to a specific transaction
output, we can make our transactions commit to a merkle-sum-tree of
transaction outputs. To then redeem a transaction output you prove that
enough prior outputs were spend to add up to the new output's value. The
entire process can happen incrementally without any specific
co-operation between miners on different parts of the chain, and inputs
and outputs can come from any depth in the tree provided that care is
taken to ensure that reorganization is not profitable.
Like the token transfer system proving a given output is valid has cost
linear with history. However we can improve on that using
non-interactive proof techniques. For instance in the linear token
transfer example the history only needs to be proven to a point where
the transaction fees are higher than the value of the output. (easiest
where the work required to spend a txout of a given value is well
defined) A similar approach can be easily taken with the
directed-acyclic-graph of mutliple-input-output transactions. Secondly
non-interactive proof techniques can also be used, again out of the
scope of this already long preliminary paper.
1) "Disentangling Crypto-Coin Mining: Timestamping,
   Proof-of-Publication, and Validation",

@_date: 2014-03-25 08:50:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
BTW for those whose email clients have problems with unicode:
 at lists.sourceforge.net/msg04388.html
Also, I was in a bit of a rush - catching a flight - and know I should
have cited a few things, including, but not limited to, various peoples'
work on chain-to-chain transfers and SPV proofs.

@_date: 2014-03-25 09:49:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
Yeah, about that...
For someone with 'Chief Scientist' as their job title, I'm surprised you
think so little of hard evidence and so much of idol worshipping.
P.S. A year or so ago you complained that if I cared so much about
decentralization, I should make P2Pool better. Your homework: What do
tree-chains and Andrew Miller's non-outsourcable puzzles(1) have to do
with that? What about the cube-square law? And why don't I think TXO
commitments solve the blocksize problem?
1)

@_date: 2014-03-25 12:47:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
For the record, tree chains is designed to be a soft-fork upgrade to bitcoin, at least if we can get the economics to work out. Assuming it does, you would do this by defining bitcoin itself to be the top level chain, and carrying what appear to be anyone can spend txouts from block to block so that transaction outputs can be created when funds are redeemed in the top block chain from children lower in the tree. Very similar ideas as the chain to chain stuff via spv proofs that Mark and Adam were talking about here earlier, although I think the order and reorganisation guarantees is a big advantage over their unsynched chain model. Most of the other ideas are identical, and they deserve credit.
I'm on the currency design panel at the Princeton Bitcoin Research Conference this week - tree chains will be discussed informally if not on the panel itself.
Regarding cryptocurrency research related posts, the feedback I've gotten has always been quite positive. You are in the minority as far as I can tell, and anyway the volume of such posts is a small part of the total list volume.
As for the rest of your email, doctor, heal thyself. Gavin's constant namecalling of legit and well accepted scaling concerns as FUD has irritated many people for over a year now,  among many other things. Statements similar to what you claim are said about me are also often said to me about you and Gavin.
But anyway, reply off list please.

@_date: 2014-03-25 15:47:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
OK, deal. You guys stop calling my concerns FUD, accusing me of having ulterior motives, etc. and I'll pay the same respect to you.

@_date: 2014-03-25 18:05:07
@_author: Peter Todd 
@_subject: [Bitcoin-development] Privacy-Protecting Proof of Reserves without 
In the wake of the Mt. Gox debacle merkle-sum-trees for
proof-of-reserve(1) have been getting attention again. A serious
objection to using them is exchange privacy as the merkle-sum-tree
inherently reveals the sum total of all deposits held by a given
service. A second, lesser, consideration is the privacy of the users'
balances, as changes to those balances are reflected in the tree and
various levels of aggregate information can be extracted from the
solvency proofs. For instance consider how an attacker who had knowledge
of a few balance changes to a particular user's account - perhaps
because that attacker had deposited funds themselves - could then
corrolate those balance changes with changes in merkle path proofs to
determine an upper bound on the victim's total balance. With some effort
and/or luck that upper bound could be even improved to the exact account
balance by obtaining a solvency proof from an account adjacent to the
victim's account.
Real or imagined the privacy problems with merkle-sum-trees pose a
barrier to adoption. Jesse from the exchange Kraken stated recently(2)
on reddit:
    This is asking a lot of an exchange, and I don't think information
    is worth the price you're paying in security and privacy. Your
    interests would be better served by a private auditor's report.
While there has been much discussion recently on  and
other places about applying advanced cryptographic techniques - so
called "Moon Math" - generate zero-knowledge-proofs of blinded account
sum trees so as to not leak any information, these techniques all suffer
from implementation complexity. Fortunately private proof-of-solvency
without moon math is possible without significant increase in
First let's look at what exactly it is that our proof-of-solvency is
supposed to achieve. For expediency we'll refer to the third-parties
proving solvency as 'banks' and start with the big picture:
0) No more banks stealing everyone's money!
Of course, since the banks have the private keys to the bitcoins in
question the best we can actually do is much weaker:
1) Prove that at some point in the past, the bank had access to a
   private key that can be used to spend unspent txout(s) that still
   exists now.
Note how the bank may have since lost that key! But objective  isn't
good enough by itself; we also need to:
2) Prove that those txout(s) have no been re-used in any other proof of
   solvency or ownership.
Most discussions about merkle-sum-trees miss this critical point. To see
why it matters, consider the example of BigBank. They have a very simple
proof-of-solvency scheme where they simply allocate one address per
customer, holding at least their entire balance. To prove their solvency
to Alice they simply sign a message:
    $ btc verifymessage 13pPCfupiDhWadEXTZDnqSHm5Cy2rdUDho \
      ID6Wk3SDsg3os4cSWRtG13lODY84zoVYpfEC2Y4kfHqGqqZV9hy1xD5yRKCyjL0II3UwPirEVKxm5meJ3VVDW/0= \
      "Hi Alice"
    true
Alice checks that the txouts with that address sum up to at least as
many Bitcoins as her balance, sees that it does, and is satisfied
BigBank is solvent.
Meanwhile LittleBank is running short of funds, so they decide to
"borrow" some from BigBank. One of their customers, Bob, asks for a
proof-of-solvency for his balance, and LittleBank happily obliges:
    $ btc verifymessage 13pPCfupiDhWadEXTZDnqSHm5Cy2rdUDho \
      H9af7wCdJrVIPG5Z0qrSviwAsElPkGw9v5FrUBAdaBtpeEtP/G8UdwN6KxKOytqyU7ObzcQs3qa6urHceZIXDg4= \
      "Hi Bob"
    true
It's rather unlikely that Alice and Bob will compare notes so this
reuse-fraud goes undetected.
Solving txout reuse-fraud with per-txout commitments
Suppose BigBank gains a second customer, Bob. After depositing some
funds he asks for a proof-of-solvency. BigBank has since added
anti-reuse-fraud to their very simple one-address-per-customer scheme:
    $ btc verifymessage 1HHuBBExHYqPwfgmKiBEHAGFSaLSdVayh5 \
      H6IJztw/QM4WjbtHl51WFo5L8rXn5aONZZvpQIo/8ORz7Yx0puLD68Z2WOCmAEvFQfpz0wYSX3D28RhevYBexpQ= \
      "Hi Bob"
    true
Bob then goes and verifies that the address 1HHuBBE was derived from the
domain "bigbank.com", and finally verifies that the funds held at that
address are sufficient to cover his balance.
Alice does the same thing:
    $ btc verifymessage 1HHuBBExHYqPwfgmKiBEHAGFSaLSdVayh5 \
      H5Z1LEwagAx7s1Kj21sy98/i6/DEZpyyGDfauDVfwOUE2ewsuHqSAE1txRi5VltBs5zVoMExxMw/m4JAyXBSa+s= \
      "Hi Alice"
    true
Note that the addresses are the same! Again, BigBank has committed
re-use fraud, this time internal to the service. In our simplistic
example of one address per customer the domain the funds are committed
to could be extended to include Alice and Bob's usernames or email
addresses. Again, most discussions of merkle-sum-trees gloss over this
important point, and assume that "somehow" the bank will publish the
merkle root publicly, e.g. at a URL.
Merkle-sum-forests for proof-of-solvency
1) Customers request deposit addresses, but the exchange doesn't know in
   advance how much they are going to deposit. Those addresses should
   commit values and nonces for use in the solvency proof, so we need to
   define a merkle-sum-tree that operates on relative amounts rather
   than absolute.
2) We'd rather not have to spend a txout just to "make change" when a
   customer's balance changes internally. Thus rather than, say, a
   simple binary of two value decomposition in a txout, consider making
   available duplicate values. Q) What's optimal here? Real world data
   would help.
Deterministic nonces and backups
There needs to be care taken in how nonces are generated - losing a
nonce can mean losing the ability to spend the txout. What should be
done is for the merkle-sum-trees per txout be generated deterministicly
using "sufficient" sub values to allocate change... Which leads to a
curious final conclusion: we can in reality skip the actual
merkle-sum-trees, so to speak, and derive the actual nonces committed to
in the nonce->customer tree from some deterministic splitting algorithm
and the globally unique txout, specifically H("nonce" | txid:n).
Essentially the nonce->customer mapping is actually a "part of a
txout"->customer mapping, where every txout value is split into
convenient-sized change. We still get the privacy we want, because the
customer-containing tree is not a merkle-sum tree, and we completely
prevent fraudulent reuse, and we don't risk losing coins in the event of
a backup failure as all txout scriptPubKeys can be regenerated
deterministicly from a seed.
Future work
Implement this.
1) 2) 3) Homomorphic Payment Addresses and the Pay-to-Contract Protocol,
   Ilja Gerhardt, Timo Hanke, 13 Dec 2012
This document is placed in the public domain.

@_date: 2014-03-26 06:48:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
I see your point, but gmaxwell accurately guesses below that when I'm
talking about inflation, I'm including the inflation of the alt too.
With tree-chains that's particularly obvious as the scheme doesn't try
to privilege one chain over another beyond parent-child relationships.
Incidentally, I understand that the pegged chains are meant to be
merge-mined. To me this seems problematic and cheap to attack. Consider
a merge-mined zerocoin sidechain: Can you profit from depositing some
coins, taking them out again, then reorging the zerocoin chain to undo
that withdrawl on the zerocoin side, and performing it all over again?
It'd be easy to drain the pegging pool that way, and with merge-mining
there's no inherent cost to you to do so. Not unique to zerocoin either
of course, just in that case who actually double-spent is unknowable.
Well I'll certainly raid 2-way pegging for ideas. :) I think the big
difference between the two is how I'd like to see tree-chains reduce
dependence on miner validation - ideally miners wouldn't validate at all
if the efficiency can be regained with ZK-SNARKS or something. Dropping
validation from mining could also avoid the problem of how in Bitcoin
there is no explicit mechanism that actually forces miners to validate
the chain. Not unlike gmaxwell's "firedrill" ideas, you would be able to
"firedrill" clients at any point by just mining some invalid garage.
(not to say miners would certainly not do validation - you still want to
be able to pay them transaction fees, but in that case they're doing the
validation only for themselves)
Yup, and in the tree-chains model, every single chain is, from that
perspective, an altcoin.

@_date: 2014-03-26 06:58:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
Yeah, that's part of my fundemental disagreement with him: I draw a
sharp line between mining - the act of validating and constructing new
blocks - and hashing - the act of solving proof-of-work problems. The
latter definitely has incentives to decentralize due to simple physics:
it's cheaper per unit hashing power to get rid of a small amount of
waste heat than a large amount. The former requires a full node, and
that full node is a fixed cost overhead related to the number of
transactions per second. Any fixed cost overhead discourages
decentralization, and encourages centralization.
Yup. Quite importantly, the model is for any one miner to be able to
fully participate at the same level as any other miner by mining some
section of the tree. As your reward is linked to blocks mined, there
will always be some level at which you are mining blocks at a reasonably
low variance and you don't need to join a pool to achieve that low
varience. Equally your resources to keep up with that part of the tree
can be made reasonably low, and that cost only grows at the log of the
total transaction volume.

@_date: 2014-03-31 11:23:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP 70 refund field 
One of the main goals of steath addresses is actually scalability. In
particular in the refund address case you would use stealth addresses
with a per-order UUID so that refunds can be detected cheaply by just
scanning for payments to your (single) stealth address, then when those
payments are detected, check the UUID against a on-disk database. A
64-bit "UUID" is probably fine, although unfortunately with OP_RETURN
quite unexpectedly dropped to 40 bytes the standard needs to change;
might have to compromise on privacy and re-use a txin pubkey to make
things fit.

@_date: 2014-03-31 13:21:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] secure assigned bitcoin address directory 
Good timing! I'm at a hackathon right now working with a group to come
up with a standard for adding Bitcoin addresses to OpenPGP keys. You're
correct in thinking that doing so with standard Bitcoin addresses is a
privacy problem, however we can also define new types of Bitcoin
addresses that address the privacy issue; stealth addresses can handle
the case where you want to pay someone without a formal payment request,
and integrating OpenPGP into the payment protocol handles the scenario
where you want to send or pay to a formal payment request.
Incidentally on my todo list is to come up for a reasonable standard for
taking X.509 certificates and using them to sign OpenPGP user IDs.
Essentially the certificate authority is then making the statement that
a keypair is authorized to sign on behalf of a domain-name, and in turn
that keypair signs that the email address on the user ID is correct.
It's a best of both worlds option in the same spirit of keybase.io

@_date: 2014-05-02 21:43:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] moving the default display to mbtc 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Excellent point.
Also, I frequently hear statements referring to mili-bitcoins, mBTC, pronounced as "mili-bits" or "m-bits"; the term "bits" is very much already in use and not to refer to uBTC.

@_date: 2014-05-03 13:39:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bug with handing of OP_RETURN? 
Hash: SHA256
The standard format ended up being exactly:
    OP_RETURN <0 to 40-byte PUSHDATA>
You've split the data across two PUSHDATA's. The standard should have let the data be split up like that; pull requests accepted.

@_date: 2014-05-09 11:03:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
It's always interesting to see the reinvention cycle happen in the
Bitcoin space as ideas get proposed over and over again; I'm sure Amir
Taaki will be pleased to read this as it is a slightly less
sophisticated version of what he originally proposed to me for the
design of what became stealth addresses.
Of course we quickly rejected the idea of depending solely on a
communications backchannel to retrieve funds. Any communications medium
that isn't the blockchain makes the payment non-atomic, and thus creates
opportunities for it to fail. Fortunately we already have the necessary
ephemeral keys in standard Bitcoin transactions in pay-to-pubkey-hash
and pay-to-script-hash spending scriptSigs so you don't need to
compromise on reliability in exchange for transaction size as you're
mistakingly assuming. You should re-read my original stealth address
discussion with Gregory Maxwell on IRC if this is unclear.
In any case it's a mistake to argue that some times of data in the
blockchain are "bloat" by virtue of whether or not they happen to be
executed in a script. Multisig addresses use an extra ~107 bytes of data
per signature per txout spend to make it less likely for the user to
lose their funds under certain conditions; op-return-using stealth
addresses use an extra ~50 bytes of data per txout spend to make the
user less likely to lose their funds and make their transactions more
private under certain conditions.(1) Ultimately the resource being used
is the same, making it silly to try to dictate the right trade-offs by
brushing certain solutions as anti-social "bloat" and others not based
on top-down edict; let the free market for fees do its job.
1) Note that the recent advancements in ECDH multi-party signing are
   limited in the cases they can cover; there still is a strong need for
   discrete key multisig, e.g. for Greenaddress.it

@_date: 2014-05-09 11:27:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
Ah, you're still misunderstanding my point: You can get atomicity in the
worst-case where the communications medium fails *and* stealth payments
that use up no extra space in the blockchain. This gives you the best of
both worlds.
I haven't yet specified that mode of operation in the current draft
stealth address standard, however I do plan on adding it. Notably the
standard is designed to allow exactly that feature to be added in a
backwards compatible way - senders who don't implement that feature, or
choose not to use it, simply fall back to op-return.

@_date: 2014-05-09 11:43:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
The exact same way you're proposing: via the payment protocol.
If something goes wrong and a payment gets lost, that's where you
implement a last-ditch "scan for stealth payments" button or similar
that either just asks a semi-trusted server to scan the blockchain for
you, or accepts the bandwidth hit and does so itself. (note that the
scan pubkey used to find payments is unable to spend those payments)

@_date: 2014-05-09 14:13:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
I don't think we're going to find that's practical unfortunately due to
change. Every payment I make ties up txouts, so if we try to base the
atomicity of payments on whether or not the payee decides to broadcast
the transaction the payor is stuck with txouts that they can't use until
the payee makes up their mind. That leads to lots and lots of nasty edge
OTOH if we base the atomicity of payment on whether or not a specific
txout exists everything those edge cases don't exist. Yes, that might
force us to expose transaction fees to the user, but after all it's the
user who has control over those fees.
A separate issue is IsStandard() rules, and a near-term project for me
is to write a much relaxed version of them based on soft-fork safe
whitelisting/blacklisting of opcodes, version numbers, mutability etc.
We can definitely get to the point where those rules will change very

@_date: 2014-05-12 09:07:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
Yeah, with the receiver specifically signing off on the tx I think
that's fine. OTOH you still gotta ask if this process is really worth
it; do you really need this level of signing off for payments that are
only going to be considered fully valid after a confirmation? That's
always going to be the case for a large proportion of Bitcoin
transactions, and sticking to that model makes upgrades easier and
reduces the reasons why receivers would want to reimplement a bunch of
Bitcoin-related logic.

@_date: 2014-05-19 20:27:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] patents... 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Meh. The world is much bigger than the USA. Secondly that rule makes it difficult to educate people about why patents are as bad as they are.
Feel free to continue censoring your own discussion within closed corporate environments. But to say keeping patent discussion off mailing lists is appropriate or wise when the tech news is full of such discussion is silly.

@_date: 2014-05-19 20:39:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Paper Currency 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Might be worth looking into the recent RFC?7258: Pervasive Monitoring Is an Attack for some guidance on how to write such a social contract.
Re: Gavin, note the language in the foundation bylaws:
Section 2.2 The Corporation shall promote and protect both the decentralized, distributed and private nature of the Bitcoin distributed-digital currency and transaction system as well as individual choice, participation and financial privacy when using such systems.
You might want to do a pull-req to add fungibility and rejection of blacklists to that list; note Adam Back's comments on how fungibility and privacy are inherently linked.

@_date: 2014-05-19 20:46:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] patents... 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Interesting. Is that to say a viable strategy would be to apply for patents and let the application lapse?

@_date: 2014-05-23 03:25:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I've got a PGP smart card reader and card with a securely generated key and pin entered per signature.
Re: multisig, that's precisely why we want more than just a single maintainer signing commits.
PGP isn't perfect, but perfect is the enemy of good.

@_date: 2014-05-23 18:17:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is up again 
That said, keep in mind the github discussion(1) that was had: if all
the DNS seeds being down breaks your application, your application is
broken and insecure. The only exception is initial startup, and even
then you should have fallbacks such as hardcoded node lists and manual
peer entry. If for some reason you really do need instant startup, run
your own centralized high-availability/low-latency nodes; either way
you're depending on a centralized resource.
1)

@_date: 2014-05-26 12:37:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is up 
There are no logfiles for DNS requests.
I just checked on EC2 and my cellphone internet connection here in Tel
Aviv; both work fine. My best guess is that your DNS resolver locally or
at your ISP is unable to deal with the fact that the second DNS seed
serving the domain testnet-seed.bitcoin.petertodd.org happens to be down
right now. Note that some ISP's appear to both run buggy DNS servers,
and redirect traffic meant to go to Google's 8.8.8 and 8.8.4.4 DNS
servers to their own servers.
I'd suggest that someone setup an alternate HTTP(S) based DNS seed for
protocol redundency.
Dunno exactly. It appeared to be running fine when I logged into the
machine, but for whatever reason DNS requests just weren't getting
resolved. Restarted and it was ok again.

@_date: 2014-05-27 01:39:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Hey, really sorry I don't have the time to fix this issue, been travelling for a few weeks for my consulting job. If you want to step up and volunteer please feel free.

@_date: 2014-05-30 12:43:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I meant running a seed yourself. Note that I've only received funds to cover expenses and a trivial amount on top to cover some time - about one and a half hours at my usual rates.
Gavin: Speaking of, given it looks like my work will be frequently keeping me out of country and unable to provide any more than a "best effort" attempt at running a seed, I'd like to give back the grant funds for doing so. Email me privately with an address to send them too. I have no plans to take it down, however the expectations users have for it aren't something I can provide.
I checked via the same proxy both times; I believe the endpoint is located in Europe.

@_date: 2014-11-04 14:13:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
I agree with Luke: make the rules only apply to transactions with a
strict nVersion==3. If we want to extend that later we can do so in
another soft-fork.
On another topic, I'm skeptical of the choice of nVersion==3 - we'll
likely end up doing more block.nVersion increases in the future, and
there's no reason to think they'll have anything to do with
transactions. No sense creating a rule that'll be so quickly broken.

@_date: 2014-11-04 15:07:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
Keep in mind that we may even have a circumstance where we need to
introduce *two* different new tx version numbers in a single soft-fork,
say because we find an exploit that has two different fixes, each of
which breaks something.
I don't think we have any certainty how new features will be added in
the future - just look at how we only recently realised new opcodes
won't be associated with tx version number bumps - so I'm loath to setup
Besides, transactions can certainly be verified for correctness in a
stand-alone fashion outside a block; CHECKLOCKTIMEVERIFY was
specifically designed so that verifying scripts containing it could be
done in a self-contained manner only referencing the transaction the
script was within.

@_date: 2014-11-06 05:38:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] SCRIPT_VERIFY_STRICTENC and CHECKSIG NOT 
So right now git head will accept the following invalid transaction into
the mempool:
which spends the redeemScript:
CHECKSIG NOT
That pubkey is valid and accepted by OpenSSL as it's obscure "hybrid"
format. The transaction is invalid because the signature is correct,
causing CHECKSIG to return 1, which is inverted to 0 by the NOT.
However the implementation of the STRICTENC flag simply makes pubkey
formats it doesn't recognize act as through the signature was invalid,
rather than failing the transaction. Similar to the invalid due to too
many sigops DoS attack I found before, this lets you fill up the mempool
with garbage transactions that will never be mined. OTOH I don't see any
way to exploit this in a v0.9.x IsStandard() transaction, so we haven't
shipped code that actually has this vulnerability. (dunno about
I suggest we either change STRICTENC to simply fail unrecognized pubkeys
immediately - similar to how non-standard signatures are treated - or
fail the script if the pubkey is non-standard and signature verification

@_date: 2014-11-06 05:45:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] SCRIPT_VERIFY_STRICTENC and CHECKSIG NOT 
...and while we're at it, bitcoin-ruby's forked yet again...

@_date: 2014-11-06 06:04:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] SCRIPT_VERIFY_STRICTENC and CHECKSIG NOT 
I actually was thinking about SCRIPT_VERIFY_MINIMALDATA, CScript(), and
FindAndDelete() Specifically that if you were to change CScript() to
convert single-character PUSHDATA's to OP_ you'd be making a
consensus-critical change due to how FindAndDelete() is called with a a
CScript() signature. You didn't make that mistake, and I couldn't find a
way to exploit it anyway, but it reminded me of this STRICTENC stuff.
It should be enough to just duplicate the CheckInputs() call in
the AcceptToMemoryPool() function:
    if (!CheckInputs(tx, state, view, true, STANDARD_SCRIPT_VERIFY_FLAGS, true))
    {
        return error("AcceptToMemoryPool: : ConnectInputs failed %s", hash.ToString());
    }
    if (!CheckInputs(tx, state, view, true, MANDATORY_SCRIPT_VERIFY_FLAGS, true))
    {
        return error("AcceptToMemoryPool: : BUG FOUND Standard verify flags passed yet mandatory flags failed. %s", hash.ToString());
    }
Ok, then given we have to support hybrid encoding for awhile longer
anyway - I noticed your secp256k1 library supports it - lets do the
latter as a "least invasive" measure. I can't think of any case where
that'd be triggered other than delibrately. Doing that should make
STRICTENC a soft-fork-safe change, and we can decide at a later date if
we want to get rid of hybrid-encoded pubkeys in a further tightening of
the rules.

@_date: 2014-11-06 16:32:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus critical 
Recently wrote the following for a friend and thought others might learn
from it.
Exactly. tl;dr: if you accept a block as valid due to a bug that others reject,
you're forked and the world ends.
Long answer... well you reminded me I've never actually written up a good
example for others, and a few people have asked me for one. A great example of
this is the SIGHASH_SINGLE bug in the SignatureHash() function:
    uint256 SignatureHash(CScript scriptCode, const CTransaction& txTo, unsigned int nIn, int nHashType)
    {
        else if ((nHashType & 0x1f) == SIGHASH_SINGLE)
        {
            // Only lock-in the txout payee at same index as txin
            unsigned int nOut = nIn;
            if (nOut >= txTmp.vout.size())
            {
                printf("ERROR: SignatureHash() : nOut=%d out of range\n", nOut);
                return 1;
            }
        }
        // Serialize and hash
        CHashWriter ss(SER_GETHASH, 0);
        ss << txTmp << nHashType;
        return ss.GetHash();
    }
So that error condition results in SignatureHash() returning 1 rather than the
actual hash. But the consensus-critical code that implements the CHECKSIG
operators doesn't check for that condition! Thus as long as you use the
SIGHASH_SINGLE hashtype and the txin index is >= the number of txouts any valid
signature for the hash of the number 1 is considered valid!
When I found this bug? I used it to fork bitcoin-ruby, among others.
(I'm not the first; I found it independently after Matt Corallo) Those
alt-implementations handled this edge-case as an exception, which in
turn caused the script to fail. Thus they'd reject blocks containing
transactions using such scripts, and be forked off the network.
You can also use this bug for something even more subtle. So the
CHECKSIG* opcode evaluation does this:
    // Drop the signature, since there's no way for a signature to sign itself
    scriptCode.FindAndDelete(CScript(vchSig));
and CHECKMULTISIG* opcode:
    // Drop the signatures, since there's no way for a signature to sign itself
    for (int k = 0; k < nSigsCount; k++)
    {
        valtype& vchSig = stacktop(-isig-k);
        scriptCode.FindAndDelete(CScript(vchSig));
    }
We used to think that code could never be triggered by a scriptPubKey or
redeemScript, basically because there was no way to get a signature into a
transaction in the right place without the signature depending on the txid of
the transaction it was to be included in. (long story) But SIGHASH_SINGLE makes
that a non-issue, as you can now calculate the signature that signs '1' ahead
of time! In a CHECKMULTISIG that signature is valid, so is included in the list
of signatures being dropped, and thus the other signatures must take that
removal into account or they're invalid. Again, you've got a fork.
However this isn't the end of it! So the way FindAndDelete() works is as
    int CScript::FindAndDelete(const CScript& b)
    {
        int nFound = 0;
        if (b.empty())
            return nFound;
        iterator pc = begin();
        opcodetype opcode;
        do
        {
            while (end() - pc >= (long)b.size() && memcmp(&pc[0], &b[0], b.size()) == 0)
            {
                pc = erase(pc, pc + b.size());
                ++nFound;
            }
        }
        while (GetOp(pc, opcode));
        return nFound;
    }
So that's pretty ugly, but basically what's happening is the loop iterates
though all the opcodes in the script. Every opcode is compared at the *byte*
level to the bytes in the argument. If they match those bytes are removed from
the script and iteration continues. The resulting script, with chunks sliced
out of it at the byte level, is what gets hashed as part of the signature
checking algorithm.
As FindAndDelete() is always called with CScript(vchSig) the signature
being found and deleted is reserialized. Serialization of bytes isn't
unique; there are multiple valid encodings for PUSHDATA operations. The
way CScript() is called the most compact encoding is used, however this
means that if the script being hashed used a different encoding those
bytes are *not* removed and thus the signature is different.
Again, if you don't get every last one of those details exactly right, you'll
get forked.
...and I'm still not done! So when you call CScript(vchSig) the relevant code
is the following:
    class CScript : public std::vector
    {
        explicit CScript(const CScriptNum& b) { operator<<(b); }
        CScript& operator<<(const std::vector& b)
        {
            if (b.size() < OP_PUSHDATA1)
            {
                insert(end(), (unsigned char)b.size());
            }
            else if (b.size() <= 0xff)
            {
                insert(end(), OP_PUSHDATA1);
                insert(end(), (unsigned char)b.size());
            }
            else if (b.size() <= 0xffff)
            {
                insert(end(), OP_PUSHDATA2);
                unsigned short nSize = b.size();
                insert(end(), (unsigned char*)&nSize, (unsigned char*)&nSize + sizeof(nSize));
            }
            else
            {
                insert(end(), OP_PUSHDATA4);
                unsigned int nSize = b.size();
                insert(end(), (unsigned char*)&nSize, (unsigned char*)&nSize + sizeof(nSize));
            }
            insert(end(), b.begin(), b.end());
            return *this;
        }
    }
Recently as part of BIP62 we added the concept of a 'minimal' PUSHDATA
operation. Using the minimum-sized PUSHDATA opcode is obvious; not so obvious
is that there are few "push number to stack" opcodes that push the numbers 0
through 16 and -1 to the stack, bignum encoded. If you are pushing data that
happens to match the latter, you're supposed to use those OP_1...OP_16 and
OP_1NEGATE opcodes rather than a PUSHDATA.
This means that calling CScript(b'\x81') will result in a non-standard
script. I know an unmerged pull-req? related to sipa's BIP62 work has
code in the CScript() class to automatically do that conversion; had
that code shipped we'd have a potential forking bug between new and old
versions of Bitcoin as the exact encoding of CScript() is consensus
critical by virtue of being called by the FindAndDelete() code!
Even had we made that mistake, I'm not sure how to actually exploit it...
FindAndDelete() is only ever called in a consensus-critical way with valid
signatures; the byte arrays 01, 02, ..., 81 are all totally invalid signatures.
The best I could think of would be to exploit the script verification
flag SCRIPT_VERIFY_STRICTENC by using the little-known hybrid-pubkey
encoding?, which I spent the past two hours looking at. However it isn't
even soft-fork safe in the current implementation!  All I could find was
a new DoS attack?, and it's not exploitable in an actual release due to
the pre-v0.10 IsStandard() rules. :(
[?]: [?]: [?]: [?]:  at lists.sourceforge.net/msg06458.html

@_date: 2014-11-06 18:12:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
BIP62 is a less-than-ideal way of making contracts secure against
malleability as it relies on a "whack-a-mole" approach to security that
is insecure if any flaw is missed. If you only wanted to make contracts
secure, you'd either implement a new SignatureHash() that could leave
out the prevout field in favor of hashing the previous input's CTxOut()
structure, and/or implement the significantly more limited
Equally BIP62 fails at making more complex types of contracts secure.
For instance suppose I had a multi-step protocol that required more than
two transactions:
    tx1: Alice -> (Alice, Bob)
    tx1_refund: (Alice, Bob) -> Alice
    tx2: (Alice, Bob) -> Charlie
    tx2_refund: (Alice, Bob) -> Bob
tx1 can only be modified by Alice, so tx1_refund is secure. However the
second stage, where the output of tx1 is spent by tx2, with a refund
transaction giving the funds back to Bob, can't be made secure as BIP62
can't prevent Alice from changing her signature, getting tx2' mined
instead, and making tx2_refund invalid.
OTOH a new form of signature hash that was a signature on tx2.vout
structure rather than it's txid would be secure, as tx2_refund would be
valid regardless of tx2's actual txid.
Obviously there are good reasons to not use such signature hashes in the
general case, as they imply you can't reuse scriptPubKeys securely, but
that's a minor problem for purpose-built contract protocols. It's
certainly a much more minor problem then the huge number of holes
possible with BIP62.
BIP62 does make life easier for wallet authors as they don't have to
deal with malleability - maybe! - but for contracts it's a bad design.
FWIW I've done due-dilligence reviews for investors on projects and
companies that have re-implemented Bitcoin Core consensus-critical code,
and every time my review lists doing so as a major red flag.

@_date: 2014-11-06 18:19:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
I think people in this community often miss the serious political and
legal ramifications of hard-forks. Being in the social position of being
able to succesfully pull off hard-forks, particularly for new features,
is clear evidence that you have de-facto control over the system.
Regulators around the world appear to be going in directions that would
make that control subject to regulation and licensing, e.g. the European
Banking Association proposals, and initial Bitlicense proposals.
Equally, look how hard-forks - known as flag days elsewhere - are
generally considered to be dangerous and worth avoiding in other
contexts due to simple engineering reasons. It's just easier to upgrade
systems in backward compatible ways, especially when they incorporate
features specifically to make that possible. (as does bitcoin!)
This is a misconception; you can't prevent soft-forks from happening, so
you always have an SPV level of security by that standard.
People put *way* too much trust in small numbers of confirmations...

@_date: 2014-11-06 18:26:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
For the same reason we don't do hard-forking upgrades of basically every
protocol on the planet on a regular basis, even when we don't have
consensus problems to worry about.
Flag days are really rare in engineering, and for good reason.

@_date: 2014-11-06 19:03:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
The protocol is what the protocol is; the bugs are when you don't match
the protocol.
We can bring up politics if you want.
In the current model, the specification *is* the protocol, and the
Bitcoin Core team is scared to death of changing anything; they've got
very little real power. Soft-forks are the minimum-viable way of making
changes to the protocol, and it's very clear how they get adopted:
minerr consensus. They're also a fundemental way of changing the
protocol that is impossible to prevent, so you might as well use it.
Hard-forks require political consensus to achieve, and the way you
create that political consensus is by creating committes, groups,
associations... Foundations. Every last one of those things requires
centralization and political power.
You know, the smartest thing the Bitcoin Foundation could do if they
wanted to cement their place in the Bitcoin ecosystem as a power broker
would be to setup a program of periodic hard-forks, say every year or
two, and then manage the committees that decide what goes into those
hard-forks. That they haven't suggested that yet is a sign that they're
either not evil, or they don't understand Bitcoin very well.
I think programmers find this reality hard to accept, because they're
mostly interested in writing code that'll get widely used. To them it's
hard to accept that the Bitcoin protocol *is* a few thousand lines of
C++ code, and they're not good enough to write their own implementation
and make it match; if we replaced programmers with writers we might get
the equally bizzare and pointless situation of people taking perfectly
good RFCs and rewriting them in their own words.
If you do care about keeping the politics of Bitcoin development free
from centralized control you should do what I advised the Dark Wallet
team to do a year ago: fork Bitcoin Core and change the
non-consensus-critical code that implements policy. I've done this
myself in a minor way with my replace-by-fee(1) version. Luke-Jr has
also done this with his Eligius branch, a fork that something like 30%
of the Bitcoin hashing power appear to run. (Discus Fish has been mining
non-standard transactions(2) lately)
Multiple *forks* of the Bitcoin Core reference client that are actually
getting used by miners and other users ensures that no one group
maintaining such a fork has the ability to change anything without
strong consensus. Forking the codebase, rather than rewriting it, best
ensures that your code actually implements the protocol properly, is
safe to use for mining, and actually gets used.
Rewriting Bitcoin Core is a fun project, but it's terrible politics.
1) 2)

@_date: 2014-11-07 03:48:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
To a first approximation the consensus code *is* frozen; if we introduce
any consensus changes into it at this point it's due to a mistake, not
Of course, that's not including the two serious soft-fork proposals in
the air right now, Pieter Wuille's BIP62 and my CHECKLOCKTIMEVERIFY.
However dealing with proposed changes like those in an environment where
the competing implementations all use essentially the same
consensus-critical code is much easier than in an environment where they
don't; I say this on both a technical and political level.

@_date: 2014-11-07 06:47:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
That's a pretty old idea, and we're working on it. First step is a
stand-alone script verification library:
 H_BITCOIN_BITCOINCONSENSUS
 H_BITCOIN_BITCOINCONSENSUS
 defined(BUILD_BITCOIN_INTERNAL) && defined(HAVE_CONFIG_H)
 "config/bitcoin-config.h"
   defined(_WIN32)
     defined(DLL_EXPORT)
       defined(HAVE_FUNC_ATTRIBUTE_DLLEXPORT)
         EXPORT_SYMBOL __declspec(dllexport)
               EXPORT_SYMBOL
             defined(HAVE_FUNC_ATTRIBUTE_VISIBILITY)
     EXPORT_SYMBOL __attribute__ ((visibility ("default")))
   defined(MSC_VER) && !defined(STATIC_LIBBITCOINCONSENSUS)
   EXPORT_SYMBOL __declspec(dllimport)
 EXPORT_SYMBOL
   EXPORT_SYMBOL
 __cplusplus
extern "C" {
     bitcoinconsensus_SCRIPT_FLAGS_VERIFY_NONE      = 0,
    bitcoinconsensus_SCRIPT_FLAGS_VERIFY_P2SH      = (1U << 0), // evaluate P2SH (BIP16) subscripts
EXPORT_SYMBOL bool bitcoinconsensus_verify_script(const unsigned char *scriptPubKey, const unsigned int scriptPubKeyLen,
                                    const unsigned char *txTo        , const unsigned int txToLen,
                                    const unsigned int nIn, const unsigned int flags);
EXPORT_SYMBOL unsigned int bitcoinconsensus_version();
 __cplusplus
} // extern "C"
 EXPORT_SYMBOL
 // H_BITCOIN_BITCOINCONSENSUS

@_date: 2014-11-27 22:18:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP 65 and 
Hash: SHA256
It does; it is still a draft. That said I think writing up some actual working examples, in code, of CHECKLOCKTIMEVERIFY using protocols is a bigger priority. Micropayment channels comes to mind, as well as a greenaddress-style wallet.
When I get a chance I'm going to rebase the initial implementation and add to it a command-line-flag to verify CHECKLOCKTIMEVERIFY as an IsStandard() rule for testing purposes.

@_date: 2014-10-01 09:08:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent a 
I've written a reference implementation and BIP draft for a new opcode,
CHECKLOCKTIMEVERIFY. The BIP, reproduced below, can be found at:
    The reference implementation, including a full-set of unittests for the
opcode semantics can be found at:
      BIP:
  Title: OP_CHECKLOCKTIMEVERIFY
  Author: Peter Todd   Status: Draft
  Type: Standards Track
  Created: 2014-10-01
This BIP describes a new opcode (OP_CHECKLOCKTIMEVERIFY) for the Bitcoin
scripting system that allows a transaction output to be made unspendable until
some point in the future.
CHECKLOCKTIMEVERIFY re-defines the existing NOP2 opcode. When executed it
compares the top item on the stack to the nLockTime field of the transaction
containing the scriptSig. If that top stack item is greater than the transation
nLockTime the script fails immediately, otherwise script evaluation continues
as though a NOP was executed.
The nLockTime field in a transaction prevents the transaction from being mined
until either a certain block height, or block time, has been reached. By
comparing the argument to CHECKLOCKTIMEVERIFY against the nLockTime field, we
indirectly verify that the desired block height or block time has been reached;
until that block height or block time has been reached the transaction output
remains unspendable.
The nLockTime field in transactions makes it possible to prove that a
transaction output can be spent in the future: a valid signature for a
transaction with the desired nLockTime can be constructed, proving that it is
possible to spend the output with that signature when the nLockTime is reached.
An example where this technique is used is in micro-payment channels, where the
nLockTime field proves that should the receiver vanish the sender is guaranteed
to get all their escrowed funds back when the nLockTime is reached.
However the nLockTime field is insufficient if you wish to prove that
transaction output ''can-not'' be spent until some time in the future, as there
is no way to prove that the secret keys corresponding to the pubkeys controling
the funds have not been used to create a valid signature.
If Alice and Bob jointly operate a business they may want to
ensure that all funds are kept in 2-of-2 multisig transaction outputs that
require the co-operation of both parties to spend. However, they recognise that
in exceptional circumstances such as either party getting "hit by a bus" they
need a backup plan to retrieve the funds. So they appoint their lawyer, Lenny,
to act as a third-party.
With a standard 2-of-3 CHECKMULTISIG at any time Lenny could conspire with
either Alice or Bob to steal the funds illegitimately. Equally Lenny may prefer
not to have immediate access to the funds to discourage bad actors from
attempting to get the secret keys from him by force.
However with CHECKLOCKTIMEVERIFY the funds can be stored in scriptPubKeys of
the form:
    IF
         CHECKLOCKTIMEVERIFY DROP
         CHECKSIGVERIFY
        1
    ELSE
        2
    ENDIF
      2 CHECKMULTISIG
At any time the funds can be spent with the following scriptSig:
      0
After 3 months have passed Lenny and one of either Alice or Bob can spend the
funds with the following scriptSig:
      1
===Non-interactive time-locked refunds===
There exist a number of protocols where a transaction output is created that
the co-operation of both parties to spend the output. To ensure the failure of
one party does not result in the funds becoming lost refund transactions are
setup in advance using nLockTime. These refund transactions need to be created
interactively, and additionaly, are currently vulnerable to transaction
mutability. CHECKLOCKTIMEVERIFY can be used in these protocols, replacing the
interactive setup with a non-interactive setup, and additionally, making
transaction mutability a non-issue.
====Two-factor wallets====
Services like GreenAddress store Bitcoins with 2-of-2 multisig scriptPubKey's
such that one keypair is controlled by the user, and the other keypair is
controlled by the service. To spend funds the user uses locally installed
wallet software that generates one of the required signatures, and then uses a
2nd-factor authentication method to authorize the service to create the second
SIGHASH_NONE signature that is locked until some time in the future and sends
the user that signature for storage. If the user needs to spend their funds and
the service is not available, they wait until the nLockTime expires.
The problem is there exist numerous occasions the user will not have a valid
signature for some or all of their transaction outputs. With
CHECKLOCKTIMEVERIFY rather than creating refund signatures on demand
scriptPubKeys of the following form are used instead:
    IF
         CHECKSIGVERIFY
    ELSE
         CHECKLOCKTIMEVERIFY DROP
    ENDIF
     CHECKSIG
Now the user is always able to spend their funds without the co-operation of
the service by waiting for the expiry time to be reached.
====Micropayment Channels====
Jeremy Spilman style micropayment channels first setup a deposit controlled by
2-of-2 multisig, tx1, and then adjust a second transaction, tx2, that spends
the output of tx1 to payor and payee. Prior to publishing tx1 a refund
transaction is created, tx3, to ensure that should the payee vanish the payor
can get their deposit back. The process by which the refund transaction is
created is currently vulnerable to transaction mutability attacks, and
additionally, requires the payor to store the refund. Using the same
scriptPubKey from as in the Two-factor wallets example solves both these issues.
===Trustless Payments for Publishing Data===
The PayPub protocol makes it possible to pay for information in a trustless way
by first proving that an encrypted file contains the desired data, and secondly
crafting scriptPubKeys used for payment such that spending them reveals the
encryption keys to the data. However the existing implementation has a
significant flaw: the publisher can delay the release of the keys indefinitely.
This problem can be solved interactively with the refund transaction technique;
with CHECKLOCKTIMEVERIFY the problem can be non-interactively solved using
scriptPubKeys of the following form:
    IF
        HASH160  EQUALVERIFY
         CHECKSIG
    ELSE
         CHECKLOCKTIMEVERIFY DROP
         CHECKSIG
    ENDIF
The buyer of the data is now making a secure offer with an expiry time. If the
publisher fails to accept the offer before the expiry time is reached the buyer
can cancel the offer by spending the output.
===Proving sacrifice to miners' fees===
Proving the sacrifice of some limited resource is a common technique in a
variety of cryptographic protocols. Proving sacrifices of coins to mining fees
has been proposed as a ''universal public good'' to which the sacrifice could
be directed, rather than simply destroying the coins. However doing so is
non-trivial, and even the best existing technqiue - announce-commit sacrifices
- could encourage mining centralization. CHECKLOCKTIMEVERIFY can be used to
create outputs that are provably spendable by anyone (thus to mining fees
assuming miners behave optimally and rationally) but only at a time
sufficiently far into the future that large miners profitably can't sell the
sacrifices at a discount.
===Replacing the nLockTime field entirely===
As an aside, note how if the SignatureHash() algorithm could optionally cover
part of the scriptSig the signature could require that the scriptSig contain
CHECKLOCKTIMEVERIFY opcodes, and additionally, require that they be executed.
(the CODESEPARATOR opcode came very close to making this possible in v0.1 of
Bitcoin) This per-signature capability could replace the per-transaction
nLockTime field entirely as a valid signature would now be the proof that a
transaction output ''can'' be spent.
==Detailed Specification==
Refer to the reference implementation, reproduced below, for the precise
semantics and detailed rationale for those semantics.
    case OP_NOP2:
    {
        // CHECKLOCKTIMEVERIFY
        //
        // (nLockTime -- nLockTime )
        if (!(flags & SCRIPT_VERIFY_CHECKLOCKTIMEVERIFY))
            break; // not enabled; treat as a NOP
        if (stack.size() < 1)
            return false;
        // Note that elsewhere numeric opcodes are limited to
        // operands in the range -2**31+1 to 2**31-1, however it is
        // legal for opcodes to produce results exceeding that
        // range. This limitation is implemented by CScriptNum's
        // default 4-byte limit.
        //
        // If we kept to that limit we'd have a year 2038 problem,
        // even though the nLockTime field in transactions
        // themselves is uint32 which only becomes meaningless
        // after the year 2106.
        //
        // Thus as a special case we tell CScriptNum to accept up
        // to 5-byte bignums, which are good until 2**32-1, the
        // same limit as the nLockTime field itself.
        const CScriptNum nLockTime(stacktop(-1), 5);
        // In the rare event that the argument may be < 0 due to
        // some arithmetic being done first, you can always use
        // 0 MAX CHECKLOCKTIMEVERIFY.
        if (nLockTime < 0)
            return false;
        // There are two times of nLockTime: lock-by-blockheight
        // and lock-by-blocktime, distinguished by whether
        // nLockTime < LOCKTIME_THRESHOLD.
        //
        // We want to compare apples to apples, so fail the script
        // unless the type of nLockTime being tested is the same as
        // the nLockTime in the transaction.
        if (!(
              (txTo.nLockTime <  LOCKTIME_THRESHOLD && nLockTime <  LOCKTIME_THRESHOLD) ||
              (txTo.nLockTime >= LOCKTIME_THRESHOLD && nLockTime >= LOCKTIME_THRESHOLD)
             ))
            return false;
        // Now that we know we're comparing apples-to-apples, the
        // comparison is a simple numeric one.
        if (nLockTime > (int64_t)txTo.nLockTime)
            return false;
        // Finally the nLockTime feature can be disabled and thus
        // CHECKLOCKTIMEVERIFY bypassed if every txin has been
        // finalized by setting nSequence to maxint. The
        // transaction would be allowed into the blockchain, making
        // the opcode ineffective.
        //
        // Testing if this vin is not final is sufficient to
        // prevent this condition. Alternatively we could test all
        // inputs, but testing just this input minimizes the data
        // required to prove correct CHECKLOCKTIMEVERIFY execution.
        if (txTo.vin[nIn].IsFinal())
            return false;
        break;
    }
==Upgrade and Testing Plan==
Thanks goes to Gregory Maxwell for suggesting that the argument be compared
against the per-transaction nLockTime, rather than the current block height and
PayPub - Jeremy Spilman Micropayment Channels - This document is placed in the public domain.

@_date: 2014-10-01 13:06:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Yeah, there are lots of "upper-level" details to consider; I'm not going to pretend that BIP is complete yet. My thinking is that the first release should include my NOPx blacklist pull-req, and leave NOP2/CHECKLOCKTIMEVERIFY in that blacklist for another minor release or two.

@_date: 2014-10-01 17:05:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Better to create a GET-TXIN-BLOCK-(TIME/HEIGHT)-EQUALVERIFY operator. scriptPubKey would be:
    GET-TXIN-BLOCKHEIGHT-EQUALVERIFY
(fails unless top stack item is equal to the txin block height)
     ADD
(top stack item is now txin height + delta height)
    CHECKLOCKTIMEVERIFY
You'd want these sacrifices to unlock years into the future to thoroughly exceed any reasonable business cycle; that's so far into the future that miners are almost certain to just mine them and collect the fees.

@_date: 2014-10-01 17:12:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
No, the redeemScript has to be provably published to miners for the sacrifice to be valid; if not you can exploit the scheme by hiding the redeemScript and having a big miner mine it at lower-than-face-value cost when it unlocks.
Yes, you could do that in a followup tx containing the redeemScript in an OP_RETURN output to prove publication. That said as I said to Luke-Jr, the sacrifices need to unlock pretty far into the future, so I don't see miners bothering to do this.

@_date: 2014-10-01 18:06:06
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
Hash: SHA256
Agreed. People should keep in mind that leaving the deployment details as "TBD" was quite deliberate. There is some code in the repo to implement a softfork, but it's only meant to be illustrative.

@_date: 2014-10-01 18:09:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
For timestamps replace "height" with "time" in the above example; the minimum block time rule will prevent gaming it.
Very easy to incentivise mining centralisation with short maturities. I personally think just destroying coins is better, but it doesn't sit well with people so this is the next best thing.

@_date: 2014-10-03 17:38:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
Anyway the stuff Mike is saying about being able to detect upgrades is
incorrect - detecting an upgrade is *easier* with a soft-fork, just look
at the block header nVersion numbers and warn the user if they increase
beyond what you know is valid. Bitcoin Core implements this IIRC, and
bitcoinj should.
Someone with more time should write all this up for the bitcoin.org
developer docs BTW... There's extensive discussions on  and
others about all of this.

@_date: 2014-10-08 23:33:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
Speaking of, can anyone think of an example of a complex transaction
use-case that is affected by malleability which can't be fixed by
CHECKLOCKTIMEVERIFY? I'm sure they exist, but I'm scratching my head
trying to think of a good example.

@_date: 2014-10-14 04:09:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] Malleable booleans 
I noticed this awhile back myself. More interestingly, I remember
noticing some non-std scripts on mainnet that had opcodes that appeared
to be attempts to solve this issue with variations of the following:
    DUP
    IF
        1 EQUALVERIFY
    ELSE
        0 EQUALVERIFY
    ENDIF
I'll have to admit, I decided to keep quiet about it because it's a good
example of how relying on BIP62 for specialty contract applications that
absolutely need to avoid malleability for security reasons is a dubious
idea; it's hard to be sure that we've really gotten every relevant case
I think a decent argument *for* doing this is that if a script author
fails to properly 'bool-ize' every boolean-using path that can have
non-minimal encodings in normal execution, you can always create a
nVersion=1 transaction manually to spend the output, preventing funds
from getting lost. Meanwhile in the general case of a compenent script
author having the canonical bool testing in every boolean-using opcode
saves a lot of bytes.

@_date: 2014-10-14 15:45:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Malleable booleans 
I'm kinda inclined to agree, however there is an opposing argument too:
How often is BOOLAND and BOOLOR applied to unsanitised input from the
scriptSig? I can't think of a script type where that would be the case,
unlike OP_IF where the logical way of writing scripts is to have the
scriptSig select which brance you take. In every script I've ever
thought of BOOLAND and BOOLOR is applied to stuff generated within the
script itself, which isn't a malleability concern.

@_date: 2014-10-15 12:47:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP process 
I'll second that request. Something mailman based; don't particularly
care where it's hosted.
After all, one of the big advantages of open mailing lists is that
multiple third-parties can easily provide archives, for instance

@_date: 2014-10-15 15:40:04
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP process 
I used these guys for awhile to host a small mailman list with
absolutely no issues. Just $5/month for 1000 subscribers.

@_date: 2014-10-29 16:08:48
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reworking the policy estimation code (fee 
I don't have time to look at the details of your statistical methods
unfortunately due to some deadlines, but a quick comment:
You should think about the malleability of your estimates to attackers.
For instance the current fee estimation code has a serious issue where
it'll happily estimate ludicriously high fees based on very little date.
There is a 'insane fees' failsafe, but it's IIRC set to allow
transactions with fees of less than 100mBTC/tx, roughly $50 at current
exchange rates. It's relatively easy to get a wallet into a condition
where this happens as the estimations are considered valid even based on
very little data - a simple sybil attack suffices. (e.g. the recently
published paper(1) on Tor sybil attacks comes to mind as one example of
many ways to do this) Obviously this could empty someone's wallet pretty
quickly; an exchange that makes a few dozen transactions an hour could
easily lose tens of thousands of dollars due to this exploit. Someone
correct me if I'm wrong, but last I checked in git HEAD this exploit is
still unfixed.
A user-configurable failsafe limit is a pretty obvious solution here,
albeit a crude one; it'd be interesting to see if a plausible security
argument could be made for something more sophisticated, like taking
into account coin-age of observed transactions that estimates are based
1) "Bitcoin over Tor isn't a good idea",

@_date: 2014-09-13 14:55:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] Does anyone have anything at all signed by 
So far I have zero evidence that the common claim that "Satoshi PGP
signed everything" was true; I have no evidence he ever
cryptographically signed any communications at all.

@_date: 2014-09-14 07:28:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Does anyone have anything at all signed 
That's consistent with what everyone else is saying:
Doesn't seem like there's any evidence of that either. For instance the
archive.org Jan 31st 2009 capture of bitcoin.org with v1.3 has a link to
his PGP key, but the release itself is unsigned:
Similarly the Nov 29 2009 capture of the sourceforge download directory
has releases v0.1.0, v0.1.2, v0.1.3, and v0.1.5, none of which have
The earliest signature I can find is from v0.3.20 from Gavin Andresen:
Earliest sig in the git commit history is the v0.3.21 tag, again from
My best guess is Satoshi only created the PGP key in case
someone needed to send him a security-related bug report. Which leads to
a related question:
Do we have any evidence Satoshi ever even had access to that key? Did he
ever use PGP at all for anything?

@_date: 2014-09-15 17:20:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] Does anyone have anything at all signed 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
My original post was OT really, although obviously this was the right venue to be sure the required audience saw it and settle the question.

@_date: 2014-09-20 12:24:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] From block 0 to block 72499 the Merkle 
It's because of how the merkle tree algorithm works:
    uint256 CBlock::BuildMerkleTree() const
    {
        vMerkleTree.clear();
So here all the txids are pushed onto the vMerkleTree vector:
        BOOST_FOREACH(const CTransaction& tx, vtx)
            vMerkleTree.push_back(tx.GetHash());
For most of the early blocks there's just the coinbase transaction and
no other transactions.
        int j = 0;
        for (int nSize = vtx.size(); nSize > 1; nSize = (nSize + 1) / 2)
That means this for loop never executes! nSize = vtx.size() == 1, and
the loop terminates when nSize <= 1
        {
            for (int i = 0; i < nSize; i += 2)
            {
                int i2 = std::min(i+1, nSize-1);
                vMerkleTree.push_back(Hash(BEGIN(vMerkleTree[j+i]),  END(vMerkleTree[j+i]),
                                           BEGIN(vMerkleTree[j+i2]), END(vMerkleTree[j+i2])));
            }
            j += nSize;
        }
        return (vMerkleTree.empty() ? 0 : vMerkleTree.back());
    }
Thus the vMerkleTree still has only the coinbase txid in it, and and
vMerkleTree.back() returns that txid as the merkle root. There's no
problem with the merkle root algorithm working that way - to make a long
story short all this means is that the merkle tree algorithm
consistently uses the txid as the merkle root whenever there is only one
transaction. The contents of the block is still being securely committed
to by the merkleroot, which is the important thing, and there's no way
to lie about those contents.
There is however a serious flaw in the algorithm, unrelated to the case
of a single transaction, where the merkle tree is indistinguishable from
a merkle tree with duplicate txids if there are a non-power-of-two
number of items in the tree. For bitcoin we fixed this flaw with BIP30
and BIP34; for any other application you should *never* use the Satoshi
merkle root calculation code. Get it right on day one and do things

@_date: 2014-09-27 15:39:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] [ann] Bitcoin Core 0.9.3 has been released 
commit 84fe0ffd685627689bbbcd14cf419938f2a100b2
Author: Peter Todd     Increase IsStandard() scriptSig length
    Removes the limits on number of pubkeys for P2SH CHECKMULTISIG outputs.
    Previously with the 500 byte scriptSig limit there were odd restrictions
    where even a 1-of-12 P2SH could be spent in a standard transaction(1),
    yet multisig scriptPubKey's requiring more signatures quickly ran out of
    scriptSig space.
    From a "stuff-data-in-the-blockchain" point of view not much has changed
    as with the prior commit now only allowing the dummy value to be null
    the newly allowed scriptSig space can only be used for signatures. In
    any case, just using more outputs is trivial and doesn't cost much.
    1) See 779b519480d8c5346de6e635119c7ee772e97ec872240c45e558f582a37b4b73
       Mined by BTC Guild.
diff --git a/src/main.cpp b/src/main.cpp
index a0b6842..63b87b8 100644
--- a/src/main.cpp
+++ b/src/main.cpp
 -513,10 +513,14  bool IsStandardTx(const CTransaction& tx, string& reason)
     BOOST_FOREACH(const CTxIn& txin, tx.vin)
     {
-        // Biggest 'standard' txin is a 3-signature 3-of-3 CHECKMULTISIG
-        // pay-to-script-hash, which is 3 ~80-byte signatures, 3
-        // ~65-byte public keys, plus a few script ops.
-        if (txin.scriptSig.size() > 500) {
+        // Biggest 'standard' txin is a 15-of-15 P2SH multisig with compressed
+        // keys. (remember the 520 byte limit on redeemScript size) That works
+        // out to a (15*(33+1))+3=513 byte redeemScript, 513+1+15*(73+1)=1624
+        // bytes of scriptSig, which we round off to 1650 bytes for some minor
+        // future-proofing. That's also enough to spend a 20-of-20
+        // CHECKMULTISIG scriptPubKey, though such a scriptPubKey is not
+        // considered standard)
+        if (txin.scriptSig.size() > 1650) {
             reason = "scriptsig-size";
             return false;
         }

@_date: 2014-09-28 01:15:53
@_author: Peter Todd 
@_subject: [Bitcoin-development]  replace-by-fee v0.9.3 release 
Speaking of, I ported my replace-by-fee branch the recent v0.9.3
release: I actually ported it a few days ago; that release has been running on a
half-dozen or so nodes right now for a few days with no issues.
The v0.9.3 release's scriptSig size limit increase adds a new category
of double-spending exploit. I'm not going to get time to add that
exploit to my replace-by-fee toolkit(1) for at least another week or so
though - pull-reqs accepted.
1)

@_date: 2014-09-28 22:35:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] New opcodes and transaction version numbers 
That gist is mistaken. To see the problem consider the "re-define
OP_NOP1 to be OP_Q_CHECKSIGVERIFY" example. It says "Transactions that
use the new opcode are given a new version number." The problem is in
the definition of "use"
Lets first suppose that the evaluation of a scriptPubKey was done
according to the transaction version # of the transaction creating the
scriptPubKey. This is technically feasible as the UTXO set records the
version of the transaction creating the txout. However if I create a
P2SH address whose redeemScript makes use of a new opcode - say the new
OP_Q_CHECKSIGVERIFY - non-upgraded clients sending funds to that address
will be creating scriptPubKeys whose contents can be spent by anyone.
Not exactly ideal! This can be solved by upgrading the address format at
the same time to let senders know they must send the funds in a
transaction with an increased version number, but obviously needing new
addresses for every new opcode defeats the purpose of P2SH.
On the other hand suppose scriptPubKey evaluation is done according to
the version of the transaction spending the scriptPubKey. This is
insecure as now transaction outputs using the new opcode can be
trivially spent by just spending them in a transaction with the previous
version number; the OP_Q_CHECKSIGVERIFY is evaluated as OP_NOP and
checks nothing.
If txouts be spent only by transactions with nVersion >= the nVersion of
the transaction spending them, but again you're forced to upgrade the
address format for every new opcode. Interestingly this shows that the
common assertion that "P2SH should have been done by upgrading the tx
version  is in fact wrong except for the fact that P2SH required an
address format upgrade anyway; doing that for future opcode upgrades
would be a mistake.
With the above in mind the "Relax IsStandard rules for P2SH
transactions" pull-req(1) is incomplete as it doesn't blacklist usage of
the upgradable NOPx opcodes. After a future soft-fork redefining a NOPx
opcodes' behavior non-upgraded nodes will accept and mine transactions
that may now be invalid, the latter creating invalid blocks and thus
false confirmations. I've created a pull-req to fix this issue by
specifically blacklisting the NOPx opcodes if they are executed:
    Secondly the "Blockchain Rule Update Process" gist above should be
rewritten to say that new opcodes will be enabled for all scripts by the
block nVersion upgrade mechanism; scripts must never depend on a NOPx
opcode being executed least they be rendered unspendable by a future
By comparison BIP62 proposes that the transaction version # be increased
to indicate that the sender wants anti-malleability rules to be applied.
This is an appropriate usage of tx version numbers as in this case the
person creating the transaction wants the anti-malleability rules
applied; the creator of the scriptPubKey's being spent does not care
whether or not they are spent in a transaction that is or is not
malleable. Equally the new owners of the txouts being created don't in
general care how they were created. (modulo certain special-purpose
protocols where they would have the transaction anyway)
1)

@_date: 2014-09-29 01:35:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] New opcodes and transaction version 
============================== START ==============================
Keep in mind that when a *wallet* - not a node - creates a transaction
the scriptPubKeys in the transaction outputs have been specified by the
receiver(s) and aren't executed until they are spent. Modulo sigops(1)
there is absolutely no reason why the wallet should care what the
contents of those scriptPubKeys are at all.
This is particularly apparent when you remember that there may be
multiple recipients of a transaction. If I'm paying Alice and Bob, who
have specified that they want the transaction to have version number 2
and 3 respectively, now what? Do we take the highest of the two and
constrain ourselves for how scripts are interpreted for all eternity? It
just doesn't make very much sense.
Meanwhile the man-hours of effort that would be required to implement
that "one-time" address format change is huge - it took literally years
for everyone to update their software to just support P2SH addresses.
I'm working on a CHECKLOCKTIMEVERIFY implementation right now, and know
of exchanges who would like to use it ASAP. Why make them wait years for
everyone to upgrade?
On that basis alone I think the question ought to be why should we use
transaction version numbers to enable new opcodes rather than just
enabling them globally based on block version numbers.
1) Satoshi implemented a per-block sigop limit to prevent blocks from
causing an unreasonable number of signature checking operations, but
rather than computing that limit based on the scripts actually executed,
the limit is computed based on the contents of all scriptSigs and
scriptPubKeys in the block. This is bizzare given that the contents of
the latter are *not* executed, and the former misses the prevout
scriptPubKeys that *are* executed. In short this means you can create a
block that passes the sigop limit, yet executes millions of expensive
signature operations by being filled with scriptSigs spending txouts
with large numbers of sigops in their scriptPubKeys. P2SH improves on
this situation somewhat by counting the sigops in redeemScripts towards
the limit, but missed the opportunity to just count all sigops in all
scriptSigs directly.

@_date: 2015-04-09 13:28:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] Build your own nHashType 
Keep in mind though we can always make part of the soft-fork be to make
the hash operations in the new CHECKSIG mechanism consume sigops.
For the OP: Have you looked at how CODESEPARATOR allows the signature to
sign code to run as part of verifying the signature? E.g. my signature
can say "valid if you run these additional opcodes and they return true"
where those additional opcodes take the transaction, hash it in the
defined way, and verify that the ECC signature correctly signs that
hash and the hash of the additional opcodes. For instance in this case
making a signature that's only valid if the tx fee is less than the
defined amount would be a matter of GET_FEE  LESSTHAN VERIFY
This can be a much more general mechanism with easy to test modular
opcodes; for the consensus-critical codebase this can result in a much
easier and simpler to test CHECKSIG facility than a dozen new flags.

@_date: 2015-04-18 19:33:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Build your own nHashType 
I wrote up how to do this on  Dec 9th 2014:
17:13 < petertodd> hearn: even now you can use OP_CODESEPARATOR to
implement efficient payword schemes
17:14 < petertodd> hearn: early on you could have used it to do some
really useful after-the-fact signing delegation by wrapping a IF ENDIF
around the CODESEPARATOR introduced into the middle of the
scriptSig/scriptPubKey pair - shame we got rid of that without thinking
the design through
17:15 < petertodd> hearn: e.g. "create a signature that delegates
signing authority to another pubkey"
17:15 < petertodd> probably all 100% accidental... but a nice accident
17:16 < hearn> it's probably for the best. i can imagine such things
being a surprise for implementations not expecting them. a script 2.0
effort that incorporates lots of neat features but still lets script 1.0
work would be nice to have, one day
17:17 < petertodd> satoshi belived in 1 implementation, and by putting
CODESEPARATOR into the scriptSig/scriptPubKey concatenation you had to
opt-in to making that feature possible to use in any particular
17:17 < petertodd> w/o the mis-matched ENDIF you can't pull off that
trick because you can't turn CODESEPARATOR off
17:19 < petertodd> to be explicit: scriptPubKey: ENDIF CHECKSIG, then the normal case is scriptSig:  1 IF
17:19 < petertodd> they concatenate to   1 IF ENDIF CHECKSIG, CODESEPARATOR is evaluated, and the signature is evaluated on
the script ENDIF  CHECKSIG
17:20 < petertodd> to delegate signing authority after the fact sign a
signature on the script  0 IF ENDIF  CHECKSIG
17:21 < petertodd> (remember that CODESEPARATORS are removed by
17:22 < petertodd> oops, I mean:  CHECKSIGVERIFY 0 IF ENDIF
 CHECKSIG
17:22 < petertodd> anyway, to finally spend it, create another signature
with pubkey2 signing the script  CHECKSIGVERIFY 0 IF ENDIF
 CHECKSIG again, and finally spend it with the scriptSig:
  CODESEPARATOR  0 IF
17:24 < petertodd> after concatenation the script:  CODESEPARATOR  0 IF CODESEPARATOR ENDIF CHECKSIG is evaluated, the inner signature satisfies, and the outer
signature is satisfied only if the scriptPubKey was essentially changed
after the fact to also require the inner, second, pubkey2 to be
17:26 < petertodd> a nice use-case would, forinstance, have been to have
a signing robot be able to create signatures offline for a given txout
with SIGHASH_SINGLE such that you had a spending limit enforced, and
exactly who was then allowed to spend the funds - say a department of a
company - could be picked after the fact without re-spending the txout
17:33 < petertodd> gmaxwell: re: script validation state, a good model
would be to have the tx input to EvalScript() essentially be a
CMerkleTx, and the prevout scriptPubKey be the prevout CTxOut (*maybe*
the prevout tx itself... bit dubious there...)

@_date: 2015-04-21 03:59:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Depending on what you mean by "identical" this isn't actually reorg
safe. For instance consider this implementation:
    nLockTime = stack[-1] + prevout.nHeight
    if (nLockTime > txTo.nLockTime):
        return False
Used with this scriptPubKey:
    10 RCLTV DROP  CHECKSIG
If I create that output in tx1 which is mined at height 42 I can spend
it in a tx2 at height > 42+10 by setting tx2's nLockTime to >42+10, for
instance 53. However if a reorg happens and tx1 ends up at height 43
after the reorg I'm stuck - tx2's nLockTime is set at 42.
Thus RCTLV is only reorg safe if the height is compared against the
actual block height of the block containing the spending transaction,
not the spending transaction's nLockTime.
Yup, definitely kinda ugly.
If the above style of RCTLV was used, one possibility might be to make
the relative locktime difference be required to be at least 100 blocks,
same as the coinbase maturity, and just accept that it's probably not
going to cause any problems, but could in an extremely big reorg. But
re-orgs that big might be big enough that we're screwed anyway...
With the 100 block rule, during a sufficiently large reorg that
coinbases become unavailble, simply disconnect entire blocks - all
txouts created by them.
So to be clear, right now the minimal interface to script execution is
    int bitcoinconsensus_verify_script(const unsigned char *scriptPubKey, unsigned int scriptPubKeyLen,
                                       const unsigned char *txTo        , unsigned int txToLen,
                                       unsigned int nIn, unsigned int flags, bitcoinconsensus_error* err);
Where scriptPubKey is derived from the unspent coin in the UTXO set and
txTo is the transaction containing the script that is being executed.
The UTXO set itself currently contains CCoins entries, one for each
transaction with unspent outputs, which basically contain:
    nVersion - tx nVersion
    nHeight  - Height of the block the transaction is contained in.
    vout     - Unspent CTxOut's of the transaction.
The block nTime isn't directly available through the UTXO set, although
it can be found in the block headers. This does require nodes to have
the block headers, but at 4MB/year growth it's reasonable to assume the
UTXO set will grow faster.
Script execution does not have direct access to the current block
height/block time, however it does have indirect access via nLockTime.
Thus we have a few possibilities:
1) RCLTV against nLockTime
Needs a minimum age > COINBASE_MATURITY to be safe.
2) RCLTV against current block height/time
Completely reorg safe.
3) GET_TXOUT_HEIGHT/TIME  ADD CLTV
To be reorg safe GET_TXOUT_HEIGHT/TIME must fail if minimum age <
COINBASE_MATURITY. This can be implemented by comparing against
All three possibilities require us to make information about the
prevout's height/time available to VerifyScript(). The only question is
if we want VerifyScript() to also take the current block height/time - I
see no reason why it can't. As for the mempool, keeping track of what
transactions made use of these opcodes so they can be reevaluated if
their prevouts are re-organised seems fine to me.
Absolute CLTV
If we are going to make the block height/time available to
VerifyScript() to implement RCLTV, should absolute CLTV should continue
to have the proposed behavior of checking against nLockTime? If we go
with RCLTV against current block height/time, I'm going to vote no,
because doing so needlessly limits it to only being able to compare
against a block height or a block time in a single transaction.
Similarly it can complicate multi-party signatures in some
circumstances, as all parties must agree on a common nLockTime.
Time-based locks
Do we want to support them at all? May cause incentive issues with
mining, see  discussion, Jul 17th 2013:

@_date: 2015-04-21 07:37:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] Double spending and replace by fee 
Some questions:
1) Are you contractually obliged to accept zeroconf transactions with
   existing customers?
I keep hearing rumors of this, but would like some confirmation. In
particular, it would be good to know if you have the option of turning
zeroconf off at all, contractually speaking.
2) What are your double-spend losses to date?
3) Are you actively marketing zeroconf guarantees to new customers?
You're API is a bit unclear as to what exactly those guarantees are;
looks like they only apply if a merchant has "convert to fiat" turned
4) What are your short, medium, and long term plans to move away from
   dependency on "first-seen" mempool policy?
e.g. hub-and-spoke payment channels, Lightning network, off-chain, etc.
5) What is your plan for new Bitcoin Core releases that break zeroconf
   via changed tx acceptance rules?
Basically every release we've ever made has added a zeroconf exploit due
to different tx acceptance rules. (e.g. my 95% success rate last summer)
6) What are your plans for Bitcoin Core releases that fundementally
   break zeroconf?
For instance changes like limiting the mempool size create zeroconf
vulnerabilities that can't be avoided in many situations. Yet they may
also be unavoidably needed for, for instance, DoS protection. Will you
oppose these improvements?
7) If a mining pool adopts adopted policy that broke zeroconf, e.g.
   replace-by-fee, would you take any action?
8) Would you take legal action against a mining pool for adopting
   replace-by-fee publicly?
9) Would you take action against a mining pool who is mining
   double-spends without explanation?
e.g. one that claims not to be running non-Bitcoin Core policy, but
keeps on mining double-spends.

@_date: 2015-04-27 15:21:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
I strongly disagree.
There are exactly two cases where mutation matters to normal wallets:
1) Spending unconfirmed change. This can be more efficiently done by
   double-spending the first tx with a second that pays both recipients.
2) Large reorganizations. Making mutation impossible makes it more
   likely that after a large reorg all previously confirmed transactions
   will make it back to the blockchain succesfully.
Meanwhile, the "whack-a-mole" aspect of BIP62 is worrying - it's very
likely we'll miss a case. Even right now there are edge cases without
good solutions, like how in a multisig environment any of the key
holders can mutate transactions. Building wallets that make strong
assumptions about malleability and fail if those assumptions turn out to
be wrong is poor engineering.
While I think there are better ways to do 'Build your own nHashType'
than what was recently proposed, I strongly agree that for protocols
that really, truly, need malleability resistance it's far better to use
a purpose-built signature hashing algorithm.

@_date: 2015-04-27 15:35:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
So, seems to me that RCLTV opens up a whole rats nest of design
decisions and compromises that CLTV doesn't. Yet CLTV itself is a big
step forward, it's been implemented on Viacoin for the past few months
with no issues found, and has an extremely simple and easy to audit
I think I'm going to argue we implement it as-is in a soft-fork. Pieter
Wuille's been working on a new way to handle soft-fork upgrades in the
block nVersion field, so this would be a good opportunity to add
something simple and well tested, and also make sure the new nVersion
soft-fork mechanism works. Equally, doing both at the same time ensures
we don't burn yet another version bit.

@_date: 2015-04-28 10:49:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin core 0.11 planning 
Hash: SHA256
I'll point out that at this rate the soonest we'll see CHECKLOCKTIMEVERIFY implemented on Bitcoin will be something like summer 2016, a year and a half after it got adopted on Viacoin. (and a few other alts whose names I forget)
Right now the shortest path to adoption would be to release a v0.12 with just a CLTV soft-fork as soon as the BIP66 softfork triggers. While there's been proposal to change the way the upgrade mechanism triggers to a multiple parallel fork scheme, that is quite complex, stateful, and will need lots of review, probably a few months worth; faster would be to continue with the existing mechanism.
IMO the main reason to accelerate CLTV is scalability. The only viable scalability improvements possible in the short/medium term that don't entirely rely on trusting third parties are payment channel based. While we have a working payment channel scheme - Jeremy Spilman's refund tx based system - it is fairly complex, needs good and immediate backups, and is susceptible to tx malleability. CLTV fixes those issues robustly. Of course, payment channel schemes can start off with Spilman's scheme first and go to CLTV later, but that is a lot of extra code to be written and later depreciated - I'm sure many authors are dubious about going down that path.

@_date: 2015-04-28 09:42:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin core 0.11 planning 
The code changes for absolute CLTV are quite small, and easily ported to
any Bitcoin Core version.
What's the oldest version you think we need backports for?

@_date: 2015-08-04 21:18:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Quite correct; this paper is fatally flawed and at best rehashes what we already know happens in the "spherical cow" case, without making it clear that it refers to a completely unrealistic setup. It'd be interested to know who actually wrote it - "Peter R" is obviously a pseudonym and the paper goes into sufficient detail that it makes you wonder why the author didn't see the flaws in it.
For those wishing to do actual research, esp. people such as profs mentoring students, keep in mind that in Bitcoin situations where large miners have an advantage over small miners are security exploits, with severity proportional to the difference in profitability. A good example of the type of analysis required is the well known selfish mining paper, which shows how a miner adopting a "selfish" strategy has an advantage - more profit per unit hashing power - than miners who do not adopt that strategy, and additionally, that excess profits scales with increasing hashing power.
As for the OP, if this wasn't an attempt at misinformation, my apologies. But keep in mind that you're wading into a highly politically charged research field with billions hanging on the blocksize limit; understand that people aren't happy when flawed papers end up on reddit being used to promote bad ideas. You'd be wise to run future work past experts in the field prior to publishing widely if you dislike heated controversy.

@_date: 2015-08-04 21:46:22
@_author: Peter Todd 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I'd suggest you do more research into how Bitcoin and mining works as the above has a number of serious misunderstandings.
Or, I could just point out the obvious rather than try to be polite: you know exactly why the above makes no sense as a reply to this thread and are deliberately lying.
If the situation is the latter, your conduct is toxic to the development mailing list discussion, not to mention a waste of all our time, and you should leave.

@_date: 2015-08-04 21:29:56
@_author: Peter Todd 
@_subject: [bitcoin-dev] Consensus fork activation thresholds: Block.nTime 
Hash: SHA256
To be clear, without a strong supermajority of miner support the fork risks attack. Requiring 95% approval - which is actually just a 50% majority vote as the majority can squelch the minority - is an obvious minimum safety requirement.
Another option is Hearn's proposal of using centralised checkpoints to override PoW consensus; obviously that raises serious questions, including legal issues.
For forks without miner approval miners have a number of options to defeat them. For instance, they can make their own fork with a new consensus algorithm that requires miners to prove they're attacking the unwanted chain - Garzik's recent 2MB blocks proposal is a hilarious, and probably accidental, example of such a design, with the original Bitcoin protocol rules having the effect of attacking the Garzik 2MB chain.

@_date: 2015-08-06 21:56:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] Block size following technological growth 
Hash: SHA256
Incidentally, why is that competition good? What specific design goal is that competition achieving?

@_date: 2015-08-08 14:23:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Voting by locking coins 
Hash: SHA256
Yes, John Dillon proposed a very clever and viable blocksize vote scheme on this list awhile back:  at lists.sourceforge.net/msg02323.html

@_date: 2015-08-08 15:10:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] Voting by locking coins 
Hash: SHA256
John Dillon's proposal is essentially to have the economic majority give miners *permission* to raise the blocksize; making the vote costly is against the design intent of accurately capturing the broadest possible economic consensus.

@_date: 2015-08-17 06:42:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Even more direct: use coinbase outputs of  XT blocks to create those outputs, as they can't by definition be on the Bitcoin chain.
If you can't get those, using coinbase outputs of Bitcoin blocks to create "definitely Bitcoin-only" outputs, and then spend the inputs to those transactions again on the XT chain. This isn't quite as good, as a big reorg on the XT chain could in theory spend them, but it's a close second.

@_date: 2015-08-17 07:04:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
Hash: SHA256
The fun thing about this, is you only need >25% of hashing power running Not-BitcoinXT to screw over the miners running XT, as XT blocks are valid Bitcoin blocks if they're on a valid Bitcoin chain.
75% upgrade thresholds have a lot of issues...

@_date: 2015-08-17 15:01:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fees and the block-finding process 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Note that XT includes a patch that sets the soft limit to be the same as the hard limit by default, so if miners did use the defaults "as big as possible" blocks would be produced.

@_date: 2015-08-17 12:14:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Something I only recently realised is that Satoshi's apparent policy(1)
of never making any cryptographically secure signatures to link together
his posts - or indeed any communication at all - fits well with the
avoidance of creating a central authority figure. Currently every single
thing Satoshi ever apparently wrote can only be linked together by
trusting third parties - email archives could have been hacked,
bitcointalk might have fake messages, etc. Obviously in practice we have
reasonable assurance that the same person or group was behind most of
the messages we now consider to be "from Satoshi", but ultimately
strictly speaking we can only take each message individually, for the
arguments contained within.
As you've often said, the biggest achievement by Satoshi in the creation
of Bitcoin was to create a system where the identity of the creator is a
mere historical footnote. We can probably go further, and state that
while doing so, Satoshi quite counter-intuitively took steps to avoid
even creating a pseudoanonymous identity.
1) "Does anyone have anything at all signed by Satoshi's PGP key?",
   Peter Todd, Sept 13, 2014, Bitcoin-development mailing list,

@_date: 2015-08-17 14:29:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] Incentives to run full nodes 
Re: full nodes, my thinking along those lines has been:
1) Incentivising full-nodes is a red herring
We can look at this from multiple angles. From the point of view of a
wallet, it's not very secure to use Hearn-style SPV mode, and volunteers
running full nodes doesn't help things. Sybil attacking the IP address
space is pretty easy in comparison to aquiring hashing power sufficient
to create false confirmations, so any attacker able to do the former
will likely be running the full node you're connecting too anyway.
Ultimately, Hearn-style SPV is a close approximation to just trusting
anyone with a non-trivial amount of hashing power. (and getting that is
surprisingly easy, e.g. w/ SPV mining)
From the point of view of full node or miner, having your peers be
valiating nodes is at best just a bandwidth optimization; all you need
from the rest of the P2P network is flood-fill capability with
reasonable DoS resistance. This isn't a problem that strongly requires
validation, and if bandwidth needs started to get excessive, sharding
the flood-fill network to limit bandwidth of any one flood-fill peer
would be relatively easy.
2) The best incentive to validate is clear and immediate failure when you don't
Currently the game theory and attacks possible against non-validating
nodes is a very complex landscape, full of cases where small attacks are
infeasible, but larger attacks possible. In particular, in many cases
you have co-ordination problems, where an attack is only viable if you
can steal at least a block reward worth of Bitcoins to make up for your
opportunity cost. This risks lulling people into complacency as attacks
seem rare, even if the risk is still quite high as the few attacks that
will happen will be very high impact.
If the system as a whole made small-scale attacks easier, we wouldn't
see this complacency, and people would build stronger systems. A
concrete example is Gregory Maxwell's idea of having all blocks commit
to two separate merkle trees, one valid and one invalid. Determining
which was which would require active validation, and because the block
as a whole is still valid regardless, this gives the opportunity to run
constant "fire drills" to uncover flaws in validation. Notable, this
scheme would even be compatible with SPV clients provided that all
sources of invalidity can be proven with a compact fraud proof.
A more extreme version of this notion is my embedded consensus ideas,
where you rely on the PoW for only proof-of-publication and/or
anti-replay functionality. Determining if coins (or any other asset) are
real becomes a clear job of validating history yourself, and/or trusting
others to do that validation. For instance, my smartcolors colored-coin
protocol work implemented client-side validation of colored coins, with
a planned (but not fully implemented - client ran out of funds)
optimization/trust tradeoff of having the issuer periodically sign
merkle-trees committing to all valid proofs within the system on an
offline machine.

@_date: 2015-08-18 18:04:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Using Median time-past as endpoint for 
IIRC Gregory Maxwell came up with the actual idea in question, during a
discussion between myself and Luke-Jr about incentives related to nTime
on  or  He should get credit for it; I'll
see if I can dig up a link to the discussion for historical interest.

@_date: 2015-08-18 18:36:45
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XTs Tor IP blacklist downloading system 
That's not entirely correct.
The code does disable downloading of the Tor exit node list if fListen
is false, or if there is a proxy setup, this means the statement:
is false. However, in the common scenario of a firewalled node, where
the operator has neglected to explicitly set -listen=0, the code does
still download the Tor exit node list, revealing the true location of
the node. This is contrary to the previous behavior of not revealing any
IP information in that configuration.
FWIW Gregory Maxwell removed the last "call home" feature in pull-req
 by replacing the previous calls to getmyip.com-type services with
a local peer request. Similarly the DNS seeds use the DNS protocol
specifically to avoid leaking IP address information.
tl;dr: Yes, Bitcoin XT has a privacy problem with the automatic Tor exit
node list download.

@_date: 2015-08-18 19:25:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XTs Tor IP blacklist downloading system 
Oh, and I just checked, and Mike's original pull-req for the Tor
blacklist didn't include the proxy disable code; what's in master != the
pull-req, so the OP may have been looking at the wrong code by accident.
(I personally noticed this issue in the pull-req and didn't realise it
hadn't been merged into master w/o modifications)
Kinda sloppy of Mike to be making changes in master that don't
correspond to the peer-reviewed pull-req code...

@_date: 2015-08-18 22:50:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
Deployment of the proposed CLTV, CSV, etc. soft-forks has been recently
complicated by the existence of XT(1) and Not-Bitcoin-XT(2) miners. Both
mine blocks with nVersion=0x20000007, which would falsely trigger the
previously suggested implementation using the IsSuperMajority()
mechanism and nVersion=4 blocks. Additionally while the
XT/Not-Bitcoin-XT software claims to support Wuille/Todd/Maxwell's
nVersion soft-fork mechanism(3) a key component of it - fork
deadlines(3) - is not implemented.
XT/Not-Bitcoin-XT behavior
1) Plain IsSuperMajority() with nVersion=4
This option can be ruled out immediately due to the high risk of
premature triggering, without genuine 95% miner support.
2) nVersion mask, with IsSuperMajority()
In this option the nVersion bits set by XT/Not-Bitcoin-XT miners would
be masked away, prior to applying standard IsSuperMajority() logic:
    block.nVersion & ~0x20000007
This means that CLTV/CSV/etc. miners running Bitcoin Core would create
blocks with nVersion=8, 0b1000. From the perspective of the
CLTV/CSV/etc.  IsSuperMajority() test, XT/Not-Bitcoin-XT miners would be
advertising blocks that do not trigger the soft-fork.
For the perpose of soft-fork warnings, the highest known version can
remain nVersion=8, which is triggered by both XT/Not-Bitcoin-XT blocks
as well as a future nVersion bits implementation. Equally,
XT/Not-Bitcoin-XT soft-fork warnings will be triggered, by having an
unknown bit set.
When nVersion bits is implemented by the Bitcoin protocol, the plan of
setting the high bits to 0b001 still works. The three lowest bits will
be unusable for some time, but will be eventually recoverable as
XT/Not-Bitcoin-XT mining ceases.
Equally, further IsSuperMajority() softforks can be accomplished with
the same masking technique.
This option does complicate the XT-coin protocol implementation in the
future. But that's their problem, and anyway, the maintainers
(Hearn/Andresen) has strenuously argued(5) against the use of soft-forks
and/or appear to be in favor of a more centralized mandatory update
3) Full nVersion bits implementation
The most complex option would be to deploy via full nVersion bits
implementation using flag bit  to trigger the fork. Compliant miners
would advertise 0x20000008 initially, followed by 0x20000000 once the
fork had triggered. The lowest three bits would be unusable for forks
for some time, although they could be eventually recovered as
XT/Not-Bitcoin-XT mining ceases.
The main disadvantage of this option is high initial complexity - the
reason why IsSuperMajority() was suggested for CLTV/CSV in the first
place. That said, much of the code required has been implemented in XT
for the BIP101 hard-fork logic, although as mentioned above, the code
has had very little peer review.
1) 2) 3) "Version bits proposal",
    Pieter Wuille, May 26th 2015, Bitcoin-development mailing list,
        4) 5) "On consensus and forks - What is the difference between a hard and soft fork?",
   Mike Hearn, Aug 12th 2015,
   6) 2013 San Jose Bitcoin conference developer round-table

@_date: 2015-08-19 11:13:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
IMO that's a very reasonable request; lately I've spent a lot of time
having to educate journalists on how Bitcoin doesn't have a "chief
scientist" with any kind of authority. Having Gavin Andresen in that
position at the otherwise inactive and bankrupt Bitcoin Foundation
misleads the public about the true nature of how Bitcoin operates,
giving a misleading impression that it has the same centralized decision
making as conventional financial systems do. Among other things, this
harms the reputation of Bitcoin as a whole as it can confuse the public
into thinking there aren't major differences between Bitcoin and those
conventional financial systems.
As the email said "Regardless of your personal view on XT this is bad
for bitcoin." - a statement I agree with 100%

@_date: 2015-08-19 11:20:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Normal GitHub users submitting pull-reqs to Bitcoin Core can't delete
other users' comments on their own pull-reqs...
IMO that's an abuse of the pull-req process, and in turn, Gavin
Andresens's commit access rights for the Bitcoin Core repo.
That kind of comment is perfectly on topic in the pull-req review
process; deleting it harms that process by removing useful information
about the trade-offs of the pull-req, both for people now, as well as
future efforts investigating the history of Bitcoin's protocol
I think this should weigh in favor of Gavin Andresen not having commit
privileges for the Bitcoin Core repository.

@_date: 2015-08-19 14:03:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
Ah, I see your point now re: wasting bits; my post was a bit incorrect
on that point.
So a subtle thing with the IsSuperMajority() mechanism, and the nVersion
bits proposal alternative, is one of the main objectives of the latter
proposal proposal is to allow forks to *fail* to be adopted cleanly.
To illustrate the problem, consider a hypothetical CLTV soft-fork,
implemented with IsSuperMajority() nVersion >= 4. We release Bitcoin
Core with that code, call it Bitcoin Core v0.12.0, however some
substantial fraction of the mining community refuses to upgrade,
believing CLTV to be a bad idea. This forms the community into Bitcoin
Core and Bitcoin Not-CLTV camps. The Not-CLTV camp then wants to do a
new soft-fork upgrade, say for CHECKSIG2
What now? If CHECKSIG2 is implemented via IsSuperMajority, nVersion >=
5, that'll falsely trigger Core nodes to think the upgrade has gone
though. You could safely define >= 5 semantics to be "OP_CLTV is now
disabled", but that's pretty ugly and unnecessarily uses up a NOP.
You can avoid this problem by assigning one bit out of nVersion for
every soft-fork, but then you can only do ~29 more soft-forks - ugly!
Come to think of it, if you're Really Sure? the soft-fork will be
adopted, you can recycle those bits by using the following rule:
    if (IsFlagBitMaskSuperMajority(1 << 4, pindex->pprev) || block.nMedianTime > CLTV_SOFTFORK_DEADLINE) {
        flags |= SCRIPT_VERIFY_CLTV;
    }
IsFlagBitMaskSuperMajority() is a masked version of the existing
IsSuperMajority() logic. CLTV_SOFTFORK_DEADLINE would be set some
reasonable amount of time in the future, perhaps 3 years.
This would probably be ok for non-controversial forks - implementing
DERSIG this way would have been fine - and an unlimited number of
soft-forks can be done this way safely. (even in parallel)
However, this idea still causes problems if forks ever fail to get
adoption, something the nVersion bits proposal handles cleanly, albeit
at the cost of a significantly more complex implementation. With a
sufficiently far off fork deadline in practice that may not be a big
issue - nearly everyone would have upgraded their software anyway -  but
it's still technically creating hard-fork scenarios with respect to
older software whenever forks fail to get adoption.

@_date: 2015-08-19 16:23:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Actually not necessarily!
To my knowledge there aren't any SPV implementations that do address
caching; they all use the peer servers in a centralized fashion every
time they connect. If those peer servers are setup to only return nodes
on one side of the fork or the other, that's all they'll connect too and
they'll never see another chain.

@_date: 2015-08-20 02:13:34
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
No, I said there was no chance it'd be accepted "due to a number of
BIP-level issues in addition to debate about the patch itself. For
instance, Gavin has never given any details about testing; at minimum
we'd need a BIP16 style quality assurance document. We also frown on
writing software with building expiration dates, let alone expiration
dates that trigger non-deterministically. (Note how my recently merged
CLTV considered the year 2038 problem to avoid needing a hard fork at
that date)"
Of course no further review was done - issues were identified and they
didn't get fixed. Why would we do further review on something that was
broken whose author wasn't interested in fixing even non-controversial
and obvious problems?
The process is to do review, fix issues identified, and repeat until all
issues are fixed.

@_date: 2015-08-20 14:23:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
ACK on removing the inversion of nSequence from what would be human
I don't want to spend the rest of my life mentally having to subtrace
from 0xFFFFFFFF :)

@_date: 2015-08-20 17:37:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
You used 20% as the size of the large miner, with all the small miners
having good connectivity with each other.
That is *not* the scenario we're worried about. The math behind the
issue is that the a miner needs to get their blocks to at least 33% of
hashing power, but more than that is unnecessary and only helps their
competition; you simulated 20%, which is under that threshold. Equally,
why are you assuming the small miner group is well connected to each
You probably didn't get any replies because your experiment is obviously
wrong and misguided, and we're all busy.

@_date: 2015-08-20 17:58:20
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
You know, I've noticed you've spent a tremendous amount of time and
energy on this list promoting these kinds of metrics; obviously you're
somewhat of an expert on this compared to the rest of us.
Why don't you look into spearheading one of these analyses yourself to
show us how it's done?

@_date: 2015-08-20 22:38:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
That's fine by me.
I'd reference that paper on bloom filters re: the "little to no privacy"
issue. There's also a post in the bitcoinj mailing list somewhere IIRC
talking about the default settings, and how they don't provide any
Good to note Mike Hearn's Cartography seed protocol here.
Ah good! That solves the backwards compatibility quite nicely.

@_date: 2015-08-20 22:42:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Oh, and we should also point out that Bloom filters have scaling issues,
as each application of the filter has to scan the whole blockchain -
with future blocksize increases these issues increase, in some proposals
quite dramatically. The underlying idea also conflicts with some
proposals to "shard" the blockchain, again suggesting that we need a bit
to handle future upgrades to more scalable designs.

@_date: 2015-08-20 22:55:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
See my comment on the recently-opened issue, reproduced below. In short,
not all that much, especially if we adopt my suggestion of having the
Core implementation accept and respond to bloom filter requests from
non-upgraded clients regardless of whether or not NODE_BLOOM was set
until some fixed upgrade deadline in the future.
    Note that since the last time NODE_BLOOM was proposed, the landcape for
    (lite-)SPV clients has changed significantly in a few key ways:
    1)  [Cartographer](
    seed protocol has been created and deployed in production to allow
    (lite-)SPV clients to find nodes supporting arbitrary service bits,
    notable NODE_GETUTXOs.
    2) Bloom filter usage has declined significantly, as lite-SPV clients
    are moving towards using centralized, trusted, servers run by the wallet
    authors. For instance
    [Mycelium](
    [GreenBits](
    [AirBitz](
    and [Electrum]( all fall in this category.
    3) Bloom filters [have been found]( to
    have severe privacy issues, offering essentially no privacy at all.
    Under many threat models a small number of trusted servers pose less
    privacy security risk than connecting to random, sybil-attackable, peers
    using unencrypted connections and giving those peers very accurate
    wallet contents information.
    4) Finally, Bloom filters still have [unsolved DoS attack
    issues](
    that will get significantly worse under upcoming blocksize increase
    proposals.
    Re: service bit identifier, I'd just pick 1<<3
    -

@_date: 2015-08-20 23:07:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
I run a number of high speed nodes and while I don't have historical
logs handy over time, I've noticed a drop from about %5-%10 SPV clients
at any one time to closer to %1 (Matt: you have a few TB of logs saved
don't you?)
Also, as I mentioned, just look at the popularity of wallets such as
Mycelium that are not adopting bloom filters, but going with SPV
verification of block headers w/ lookup servers.
Anyway, look at the analogous implementation of NODE_GETUTXO's, which
helpfully has provided the infrastructure for wallets that need bloom
filters to find appropriate nodes to connect too - we certainely aren't
seeing any shortages of nodes for those wallets to use.

@_date: 2015-08-21 09:29:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Core Devs : can you share your thoughts about all 
Hash: SHA512
I'll second that, which is why I've mostly not commented on whether or not particular proposals are good ideas, except in the case where they're obviously broken due to reasons other than the blocksize itself. For instance both of Garzik's proposals and Andresen's BIP101 have serious flaws regardless of your thoughts on the blocksize, which is why I've commented on them. Wuille's OTOH is implemented well, which is why I have not commented about it.
What might be valuable is to ask devs to explain what their threat models are, what should be at the root of their thinking about the blocksize.

@_date: 2015-08-21 09:32:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] Core Devs : can you share your thoughts about all 
Hash: SHA512
Given the strong consensus that blockchains simply don't scale well - even Andersen describes a blocksize increase as "kicking then can down the road" - it'd be good to ask service providers what their long term plans for growth are.

@_date: 2015-08-21 09:35:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Core Devs : can you share your thoughts about all 
Hash: SHA512
What if could be used for; theres value in being more explicit.

@_date: 2015-08-21 15:06:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Well, in general relying on seeds every time you start your node is a
really bad idea; doing so much be carefully weighed against the
downsides and should be used only as a last resort. Nodes should be
doing caching and proper gossip protocol participation whenever
possible. (note how bitcoinj nodes *do* rely on centralized servers,
implemented with an unauthenticated, unencrypted, protocol - the worst
of all possible solutions with many possible MITM vectors and privacy
security holes)
To that end, I'd be inclined to leave the DNS seed protocol as it is and
let others solve the centralized server use-case, for which Cartographer
isn't all that bad of a load balancing mechanism. Also as gmaxwell noted
on IRC, adding flag bits does have privacy implications.
Any protocol change that would split blocks themselves into multiples.
Not an easy problem to solve, but given the inherent O(n^2) scaling of
global consensus blockchains, it's the only kind of solution that could
in the future make the blockchain itself have reasonable scalability.
Well actually, we can reference the DoS attacks that Bitcoin XT nodes
are undergoing right now - part of the attack is repeated Bloom filter
requests to soak up disk IO bandwidth. I've CC'd Gavin and Mike - as far
as I know they haven't published details of those attacks - a write-up
would be very helpful.
While so far those are being directed only at XT nodes, obviously this
is a potential issue for Core nodes as well. Like I mentioned last time
around, it's critical that miners aren't affected by these attacks -
nodes simply serving SPV wallet clients are much less latency sensitive,
so a good DoS attack mitigation strategy would be to have the two
classes of nodes out there "in the wild"

@_date: 2015-08-21 15:21:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
This is a security issue: if you can find a likely scenario where the
system fails, that's a problem and we need to fix it.
You've taken the scenario where the system fails, and changed the
conditions to create a scenario where it works. That's not particularly
interesting or noteworthy.
To use a car analogy, Pieter Wuille has shown that the brake cylinders
have a fatigue problem, and if used in stop-and-go traffic regularly
they'll fail during heavy braking, potentially killing someone. You've
countered with a study of highway driving, showing that if the car is
only used on the highway the brakes have no issues, claiming that the
car design is perfectly safe.

@_date: 2015-08-21 15:25:46
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
That'd be a foolish design decision to move exclusively over; their
wallet was safe to use during the recent fork, unlike Android Wallet,
precisely because of their existing design.
In any case, regardless of whether we're wrong about the popularity
issue, I've yet to see any issues raised with implementing NODE_BLOOM
that will adversely affect such wallets - we've certainly got no
shortage of node capacity to go around.

@_date: 2015-08-21 17:01:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
Incidentally, what's your acceptable revenue difference between a small
(1% hashing power) and large (%30 hashing power) miner, all else being
equal? (remember that we shouldn't preclude variance reduction
techniques such as p2pool and pooled-solo mode)
Equally, what kind of attacks on miners do you think we need to be able to
resist? E.g. DoS attacks, hacking, etc.
That would let me know if you're definition of "the brakes are bad"
corresponds to normal usage, or something that's not reasonable to
design for.

@_date: 2015-08-21 17:57:49
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Using Median time-past as endpoint for 
Found it! From  2013-07-16:
23:57 < petertodd> See, it'd be possible for nLockTime w/ time-based locks to create some really ugly incentives for miners to mine blocks at thelimit of the 2hr window - a timestamping chain could provide a way for nodes to at least detect that their clocks are off, especially given how peers can mess with them.
23:58 < petertodd> It's still dodgy though... I was thinking if nLockTime-by-time inclusion was based on the previous block timestamp it'd be ok, but that still leaves large miners with incentives to screw with the 2hr window, never mind how it can reduce competition if there exists clock skew in the mining nodes.
--- Log closed Wed Jul 17 00:00:57 2013
--- Log opened Wed Jul 17 00:00:57 2013
00:01 < petertodd> (remember that if this is a timestamping facility any node wanting to know the current time simply gets a nonce timestamped, and then they know what time it is!)
00:11 < Luke-Jr> I don't see how nLockTime can discourage forward-dating blocks
00:11 < Luke-Jr> and there is no 2hr window backward..
00:12 < Luke-Jr> well, I guess if miners are behaving there is <.<
00:19 < petertodd> The problem is a block being created with nTime > actual time, and the incentive is to get a head start on other miners to put, say, a high-fee nLockTime in the block you are creating.
00:21 < Luke-Jr> petertodd: but nLockTime only sets a minimum time, it cannot set a maximum
00:22 < petertodd> but that's it, if I have a 1BTC fee tx, with nLockTime expiring in two hours, why not take the increased orphan chance and set nTime on my block to two hours ahead/
00:22 < petertodd> ?
00:22 < petertodd> yet if we allow that incentive, it's very bad for consensus
00:23 < gmaxwell> ha. We can fix.
00:23 < gmaxwell> it's a soft forking fix.
00:23 < gmaxwell> use the last blocks ntime, not this one.
00:23 < Luke-Jr> is sipa's secp256k1 branch reasonably stable?
00:23 < petertodd> gmaxwell: that's what I said...
00:24 < gmaxwell> petertodd: sorry I just read the last couple lines.
00:24 < Luke-Jr> petertodd: AFAIK we already don't relay transactions with time in the future?
00:24 < gmaxwell> petertodd: well I agree. (or not even the last block? it could use the minimum time)
00:24 < petertodd> gmaxwell: The problem is, that's only a fix if mining power is well distributed, it actually makes things worse because if there is a lot of profit to be gained the miners with a lot of hashing power still have the incentive, and it's to a much greater degree. (their orphan rate is less)
00:24 < Luke-Jr> gmaxwell: the minimum time will be earlier than the last block's :p
00:25 < gmaxwell> Luke-Jr: sure, but that doesn't change it really. Presumably if people start locking in the future miners will run nodes that take what they get and selfishly horde them, creating incentives for all miners to run good collection networks.
00:25 < petertodd> Luke-Jr: sure, but there are lots of ways to learn that a tx exists
00:26 < gmaxwell> petertodd: one of the reasons that the min is important there is because (1) it's hard to advance, and (2) when you advance it you raise the difficulty.
00:26 < petertodd> gmaxwell: I was working on figuring out the expected return - the math is really ugly
00:27 < gmaxwell> petertodd: a worst case expected return may be easier.
00:27 < petertodd> gmaxwell: Worst case is easy - your block is orphaned.
00:28 < petertodd> gmaxwell: See the issue is that once I find a block, the other side needs to find two blocks to beat me. As time goes on more of the other sides hashing power will accept my from the future block as valid, so then you get the next level where the remainder needs three blocks and so on.
00:28 < petertodd> gmaxwell: Pretty sure it can't be done as a closed-form equation.
00:30 < petertodd> gmaxwell: I don't think minimum time works either, because you still get to manipulate it by creating blocks in the future, although the ability too is definitely less. If I could show you'd need >50% hashing power to do anything interesting I'd be set.
00:31 < Luke-Jr> petertodd: hmm, is block-uneconomic-utxo-creation basically just an older revision of what Gavin did in 0.8.2?
00:31 < gmaxwell> petertodd: moving the minimum time forward needs the coperation of >50% of the hashpower over the small median window.
00:32 < petertodd> Luke-Jr: It's what Gavin did but non-hardcoded. I'd emphasize the better, not the older. :P
00:32 < Luke-Jr> petertodd: will you be rebasing it despite its closed status?
00:32 < Luke-Jr> actually, what about Gavin's is hardcoded? <.<
00:33 < petertodd> gmaxwell: Yeah, but you have to assume a steady stream of these incentives.
00:33 < gmaxwell> petertodd: right, so you have some force that turns all miners into a conspiracy.
00:34 < petertodd> gmaxwell: exactly
00:34 < petertodd> gmaxwell: nLockTime by time should have never been added in the first place, but it's such a nice idea on the face of it
00:35 -!- realazthat is now known as rudeasthat
00:35 -!- rudeasthat is now known as rudest
00:35 < Luke-Jr> softfork so nLockTime requires data on what block a transaction was created at, and enforces the 10 min per block <.<
00:36 -!- rudest is now known as heh
00:36 < petertodd> Luke-Jr: ?
00:36 -!- heh is now known as realz
00:36 < Luke-Jr> petertodd: for example, if you nLockTime for 1 day from now, it also enforces 144 blocks passing too
00:37 < Luke-Jr> so block count must be >now+144 AND time must be >now+24h
00:37 < Luke-Jr> not perfect, but might help
00:37 < petertodd> Still doesn't help in the usual case where mean interval is < 10 minutes, because you're back to only caring about time.
00:38 < Luke-Jr> usual now, but not eventually
00:38 < petertodd> Right, you've solved half the problem, when measured over the entire lifespan of Bitcoin, and only approximately half. :P
00:39 < Luke-Jr> theory is so much nicer than practice <.<
00:39 < gmaxwell> I'm forgetting why this is a problem again?  If miners mine blocks early, people will just artifically inflate their times or switch to height locking.
00:39 < petertodd> The problem is you're incentivising miners to make the 2hr window for block acceptance effectively shorter.
00:39 < petertodd> Thus requiring accurate clocks for consensus.
00:39 < gmaxwell> if miners do this consistently they'll drive difficulty up too which wouldn't be in their interest.
00:39 < Luke-Jr> ^
00:40 < petertodd> gmaxwell: It's only a fixed 2hr offset, that just drives difficulty up by 0.5%
00:40 < Luke-Jr> and on top of that, you'd just end up treating nTime with a minus-2-hours :p
00:41 < Luke-Jr> if everyone does it, it's predictable.
00:41 < petertodd> More to the point for any individual miner the marginal difference if they do it is effectively zero.
00:41 < gmaxwell> consider, why why cant the 2 hour window be 24 hours?
00:41 < petertodd> Luke-Jr: But that's the problem, if everyone does it, and people respond, then you can extend the interval even further!
00:41 < Luke-Jr> petertodd: how?
00:41 < petertodd> gmaxwell: It should have been more like 24 hours in the first place...
00:42 < Luke-Jr> you don't change the 2h rule
00:42 < Luke-Jr> you just assume miner times will always be up against it
00:42 < gmaxwell> Luke-Jr: move your clock/window forward so you dont reject stupid blocks.
00:42 < petertodd> Luke-Jr: Again, the issue is the effect on *consusus*. I don't care when the tx gets mined, I care that miners are incentivised to break consunsus for anyone without NTP.
00:43 < petertodd> The problem is no matter *what* the window is, there is an incentive to mine as close to the window as possible to accept a tx sooner than your competitors.
00:43 < petertodd> It could be a week and people would still have an incentive to set nTime + 1 week - 1 second
00:44 < Luke-Jr> if nTime is future, wait until that time before relaying it? <.<
00:44 -!- realz is now known as pleasedont
00:44 < gmaxwell> and once people did that, you'd want to start accepting blocks that where nTime + 1 week because god knows you don't want to reject a block if your clock was 2 seconds slow and most hashpower accepted it.
00:44 < petertodd> About the only thing that might change that is if the rule was nLockTime > nTime of last block, and then after that being allowed to include a tx was based on H(txhash, last hash) or similar
00:45 < petertodd> gmaxwell: exactly, the fundemental issue is there is no good incentive to set nTime accurately other than miners rejecting your blocks, and nLockTime sabotages that
00:45 -!- pleasedont is now known as realzies
00:45 < petertodd> gmaxwell: (timestamping could do, but the cause->effect is less obvious)
00:45 < Luke-Jr> I guess I just incentivized always setting nTime to the minimum then
00:45 < Luke-Jr> [04:32:26]  petertodd: will you be rebasing it despite its closed status? (block-uneconomic-utxo-creation)
00:46 < petertodd> Luke-Jr: again, relaying does nothing - consider the case of nLockTime'd fidelity bonds where it's guaranteed 100% of the hashing power know (why I wrote the spec as by-block-height in the first place)
00:46 < petertodd> Luke-Jr: sure
00:46 < Luke-Jr> petertodd: I mean delaying relaying the BLOCK
00:46 < Luke-Jr> ie, increasing the risk of it being stale
00:47 < petertodd> Luke-Jr: then you have your mining pool connect directly to other mining pools playing the same game
00:47 < petertodd> you have to assume perfect information knowledge in this stuff, at least if you're writing worst-case academic papers
00:48 < gmaxwell> petertodd: so ... prior block vs minimum time.
00:48 < petertodd> see, that's why I was talking about timestamping, because it provides a way for all users to set their clocks to what the majority of hashing power thinks nTime is, sidestepping the problem
00:48 < gmaxwell> petertodd: what are your arguments there?
00:48 < petertodd> gmaxwell: minimum time is definitely stronger because it involves more hashing power
00:49 < petertodd> gmaxwell: users would prefer minimum time - easier to understand why the tx isn't getting mined
00:49 < gmaxwell> sidestepping the problem < that doesn't sidestep the problem, it would allow the majority of hashpower to mine difficulty down to 1; also moots nlocktime as _time_ being more reliable than a height.
00:49 < gmaxwell> petertodd: plus, you can just add a constant offset to your nlocktime to adjust for the expected minimum lag.
00:51 < petertodd> gmaxwell: yes, it creates a new problem, but it did sidestep the existing one :P
00:51 < gmaxwell> petertodd: yea, lol, creates an inflation attack. Keep it up and you'll be qualified to create an altcoin. :P
00:52 < gmaxwell> (sorry, hah, I'm not poking fun at you, I'm poking fun at all the altcoins that "solved the Foo problem" where foo is something no one else thinks is a problem and they totally broke security as a side effect)
00:52 < petertodd> gmaxwell: yup, now you see how it only sidesteps the problem truly when there is enough hashing power setting their clocks back, IE 50% honest, which is better
00:53 < petertodd> gmaxwell: without the timestamping, nodes have the consensus failures, which can be attacked, likely it trades off one risk for a more existential risk
00:53 < petertodd> gmaxwell: and it's a good excuse for timestamping, lol
00:54 < gmaxwell> I thin the min solves the consensus failure so long as hashpower is well distributed.
00:54 < petertodd> yeah, I'm thinking min is probably the best we can do

@_date: 2015-08-21 18:48:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
One of the oldest mentions is the to-be-published-later portion of my
Litecoin Audit report; attached.
(see for the original report/timestamping/verification)

@_date: 2015-08-22 06:26:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Actually no. I thought revenue would be a less subjective question to ask, with more focus on the underlying orphan rate question; part of the answer might include an assumed profit margin.

@_date: 2015-08-25 13:16:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
What would you think of an approach like John Dillon's proposal to
explicitly give the economic majority of coin holders a vote for the max
blocksize? Miners could still vote BIP100 style for what max they were
willing to use, limited in turn by the vote of the economic majority.
I think in principle that would give all parties a clear voice in the
matter, which in turn makes whatever the result is more legitimit and
less controversial.

@_date: 2015-08-25 13:37:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
Note that you can make the % of voters required adapt dyanmically to voter
interest. Also, your example is rather misleading, as car buyers *do*
make those kinds of decisions though various market mechanisms. Equally,
you can make the same criticism of democracy in general.
An interesting idea would be to design a voting mechanism such that only
users with access to validating nodes be able to vote - a fundemental
requirement for users to fully participate in Bitcoin's goverance.

@_date: 2015-08-25 17:29:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
To implement a vote where only users with access to a full node can
vote, you'd force part of the vote to be determined by a
non-miner-committed value calculatable by anyone with a full node. For
instance, a very simple toy example that would work is just XORing your
vote with SHA256(the entire blockchain)

@_date: 2015-08-27 16:19:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Using Median time-past as endpoint for 
I thought we had decided that the masking thing doesn't work as
To recap, XT nodes are producing blocks with nVersion=0b001...111
You're suggesting that we apply a mask of ~0b001...111 then trigger the
soft-fork on nVersion >= 0b0...100 == 4, with miners producing blocks with
That will work, but it still uses up a version bit. The reason why is
blocks with nVersion=0b001...000 - the intended deployment of the
nVersion bits proposal - will be rejected by the nVersion >= 4 rule,
hard-forking them off the network. In short, we have in fact "burnt" a
version bit unnecessarily.
If you're going to accept hard-forking some people off the network, why
not just go with my stateless nVersion bits w/ time-expiration proposal
instead? The only case where it leads to a hard-fork is if a soft-fork
has been rejected by the time the upgrade deadline is reached. It's easy
to set this multiple years into the future, so I think in practice it
won't be a major issue for non-controversial soft-forks.
Equally, spending the time to implement the original stateful nVersion
bits proposal is possible as well, though higher risk due to the extra
complexity of tracking soft-fork state.

@_date: 2015-12-02 17:27:39
@_author: Peter Todd 
@_subject: [bitcoin-dev] Opt-in Full Replace-By-Fee (Full-RBF) 
Again, you're giving the whole world information about what's your
change address; that's simply unacceptable for privacy.
The only way to solve this is by a scheme where you pre-commit via a
hash, and reveal that later, which is extremely complex and not easily
feasible given the current tx data structure.

@_date: 2015-12-17 11:44:08
@_author: Peter Todd 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
If Bitcoin remains decentralized, miners have veto power over any
blocksize increases. You can always soft-fork in a blocksize reduction
in a decentralized blockchain that actually works.

@_date: 2015-12-18 04:18:45
@_author: Peter Todd 
@_subject: [bitcoin-dev] On the security of softforks 
To clarify, because the 95% of upgraded hashing power is creating valid
blocks from the point of view of the remaining 5%, that 95% majority
will continually reorg the 5% non-upgrading chain. This ensures that the
invalid chain remains short, and thus the # of invalid confirmations
possible remains small. For instance, the chance of getting one invalid
confirmation is 0.05^1 = 5%, two invalid confirmations 0.05^2 = 0.25%, three
0.05^3 = 0.01% etc.
Whereas with a hard fork, the 5% of miners will continue mining on their
own chain. While that chain's length will increase more slowly than
normal, the # of confirmations that non-upgraded clients will see on it
are unbounded.
Anyway, we should write this up as a BIP - there's been a tremendous
amount of misinformation, even flat out lies, floating around on this

@_date: 2015-12-18 20:43:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] The increase of max block size should be 
Hash: SHA512
FWIW all these median time based schemes should be using median time past: the point is to use a time that the block creator has no direct control of, while still tying the rule to wall clock time for planning purposes.

@_date: 2015-12-19 09:43:10
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segregated witness softfork with moderate 
Note how the fact that segwit needs client-side adoption to enable an
actual blocksize increase can be a good thing: it's a clear sign that
the ecosystem as a whole has opted-into a blocksize increase.
Not as good as a direct proof-of-stake vote, and somewhat coercive as a
vote as you pay lower fees, but it's an interesting side-effect.

@_date: 2015-12-19 10:20:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] The increase of max block size should be 
If size is calculated from the median time past, which is fixed for a
given block and has no dependency on the block header's nTime field,
does that solve your problem?
By "median time past" I mean the median time for the previous block.

@_date: 2015-12-19 10:42:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
At the recent Scaling Bitcoin conference in Hong Kong we had a chatham
house rules workshop session attending by representitives of a super
majority of the Bitcoin hashing power.
One of the issues raised by the pools present was block withholding
attacks, which they said are a real issue for them. In particular, pools
are receiving legitimate threats by bad actors threatening to use block
withholding attacks against them. Pools offering their services to the
general public without anti-privacy Know-Your-Customer have little
defense against such attacks, which in turn is a threat to the
decentralization of hashing power: without pools only fairly large
hashing power installations are profitable as variance is a very real
business expense. P2Pool is often brought up as a replacement for pools,
but it itself is still relatively vulnerable to block withholding, and
in any case has many other vulnerabilities and technical issues that has
prevented widespread adoption of P2Pool.
Fixing block withholding is relatively simple, but (so far) requires a
SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
do this hard-fork in conjunction with any blocksize increase, which will
have the desirable side effect of clearly show consent by the entire
ecosystem, SPV clients included.
Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
witholding attacks are a good thing, as in their model they can be used
by small pools against larger pools, disincentivising large pools.
However this argument is academic and not applicable to the real world,
as a much simpler defense against block withholding attacks is to use
anti-privacy KYC and the legal system combined with the variety of
withholding detection mechanisms only practical for large pools.
Equally, large hashing power installations - a dangerous thing for
decentralization - have no block withholding attack vulnerabilities.
1)

@_date: 2015-12-19 10:48:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segregated witness softfork with moderate 
Note that English-language measures of Bitcoin usage/activity are very
misleading, as a significant - probably super majority - of economnic
activity happens outside the English language, Western world.
Centralized forums such as twitter and reddit are easily censored and
manipulated. Finally, we can't discount the significant amount of
non-law-abiding Bitcoin economic activity that does happen, and I do not
believe we should adopt consensus-building processes that shut those
stakeholders out of the discussion.
As an aside, I have a friend of mine who made a Bitcoin related product
with non-culturally-specific appeal.  I asked where she was shipping her
product, and it turned out that a super majority went to
non-English-speaking countries. (she might be willing to go on public
record about this; I can ask)

@_date: 2015-12-19 20:44:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
There is no known way for pools - especially ones that allow anonymous
hashers - to effectively prevent block withholding attacks without
changing the Bitcoin protocol.

@_date: 2015-12-20 03:24:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
That's incorrect terminology - what I proposed are "TXO commitments". I
proposed that a MMR of all prior transaction outputs's, spent and
unspent, be committed too in blocks along with a spentness flag, not
just spent transaction outputs.
That's why I often use the term (U)TXO commitments to refer to both
classes of proposals.
What I proprosed is that a consensus-critical maximum UTXO age be part
of the protocol; UTXO's younger than that age are expected to be cached.
For UTXO's older than that age, they can be dropped from the cache,
however to spend them you are required to provide the proof, and that
proof counts as blockchain space to account for the fact that they do
need to be broadcast on the network.
The proofs are relatively large, but not so much larger than a CTxIn as
to make paying for that data infeasible.

@_date: 2015-12-20 05:28:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Ah, thanks for the correction.
Now to be clear, I'm not saying any of the above isn't true - it's a
fascinating result. But the hashing/mining ecosystem is significantly
more complex than just pools.
There are a number of techniques that can be used to detect block
withholding attacks that you are not aware of. These techniques usually
have the characteristic that if known they can be avoided, so obviously
those who know about them are highly reluctant to reveal what exactly
they are. I personally know about some of them and have been asked to
keep that information secret, which I will.
In the context of KYC, this techniques would likely hold up in court,
which means that if this stuff becomes a more serious problem it's
perfectly viable for large, well-resourced, pools to prevent block
withholding attacks, in part by removing anonymity of hashing power.
This would not be a positive development for the ecosystem.
Secondly, DRM tech can also easily be used to prevent block withholding
attacks by attesting to the honest of the hashing power. This is being
discussed in the industry, and again, this isn't a positive development
for the ecosystem.
GHash.io was not a pure pool - they owned and operated a significant
amount of physical hashing power, and it's not at all clear that their %
of the network actually went down following that 51% debacle.
Currently a significant % of the hashing power - possibly a majority -
is in the form of large hashing installations whose owners individually,
and definitely in trusting groups, have enough hashing power to solo
mine. Eyal's results indicate those miners have incentives to attack
pools, and additionally they have the incentive of killing off pools to
make it difficult for new competition to get established, yet they
themselves are not vulnerable to that attack.
Moving to a state where new hashing power can't be brought online except
in large quantities is not a positive development for the ecosystem.
This is also way I described the suggestion that Eyal's results are a
good thing as academic - while the idea hypothetically works in a pure
open pool vs. open pool scenario, the real world is significantly more
complex than that simple model.
Basically you have the pool pick a secret k for each share, and commit
to H(k) in the share. Additionally the share commits to a target divider
D. The PoW validity rule is then changed from H(block header) < T, to be
H(block header) < T * D && H(H(block header) + k) < max_int / D
Because the hasher doesn't know what k is, they don't know which shares
are valid blocks and thus can't selectively discard those shares.

@_date: 2015-12-20 16:35:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Exactly! The information will be out there - "just backup the seed" requires someone to have the exact same data needed to generate the TXO-unspent proof that my proposal requires to spend an old txout.
tl;dr: jgarzik is incorrect; theres no difference at all from the status quo.

@_date: 2015-12-22 17:31:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segregated witnesses and validationless mining 
# Summary
1) Segregated witnesses separates transaction information about what
coins were transferred from the information proving those transfers were
2) In its current form, segregated witnesses makes validationless mining
easier and more profitable than the status quo, particularly as
transaction fees increase in relevance.
3) This can be easily fixed by changing the protocol to make having a
copy of the previous block's (witness) data a precondition to creating a
# Background
 Why should a miner publish the blocks they find?
Suppose Alice has negligible hashing power. She finds a block. Should
she publish that block to the rest of the hashing power? Yes! If she
doesn't publish, the rest of the hashing power will build a longer chain
than her chain, and she won't be rewarded. Right?
Well, can other miners build on top of Alice's block? If she publishes
nothing at all, the answer is certainely no - block headers commit to
the previous block's hash, so without knowing at least the hash of
Alice's block other miners can't build upon it.
 Validationless mining
Suppose Bob knows the hash of Alice's new block, as well as the height
of it. This is sufficient information for Bob to create a new, valid,
block building upon Alice's block. The hash is needed because of the
prevhash field in the block header; the height is needed because the
coinbase has to contain the block height. (technically he needs to know
nTime as well to be 100% sure he's satisfying the median time rule) What
Bob is doing is validationless mining: he hasn't validated Alice's
block, and is assuming it is valid.
If Alice runs a pool her stratum or getblocktemplate interfaces give
sufficient information for Bob to figure all this out. Miners today take
advantage of this to reduce their orphan rates - the sooner you can
start mining on top of the most recently found block the more money you
earn. Pools have strong incentives to only publish work that's valid to
their hashers, so as long as the target pool doesn't know who you are,
you have high assurance that the block hash you're building upon is
Of course, when this goes wrong it goes very wrong, greatly amplifying
the effect of 51% attacks and technical screwups, as seen by the July
4th 2015 chain fork, where a majority of hashing power was building on
top of an invalid block.
 Transactions
However other than coinbase transactions, validationless mined blocks
are nearly always empty: if Bob doesn't know what transactions Alice
included in her block, he doesn't know what transaction outputs are
still unspent and can't safely include transactions in his block. In
short, Bob doesn't know what the current state of the UTXO set is. This
helps limit the danger of validationless mining by making it visible to
everyone, as well as making it not as profitable due to the inability to
collect transaction fees. (among other reasons)
# Segregated witnesses and validationless mining
With segregated witnesses the information required to update the UTXO
set state is now separate from the information required to prove that
the new state is valid. We can fully expect miners to take advantage of
this to reduce latency and thus improve their profitability.
We can expect block relaying with segregated witnesses to separate block
propagation into four different parts, from fastest to propagate to
1) Stratum/getblocktemplate - status quo between semi-trusting miners
2) Block header - bare minimum information needed to build upon a block.
Not much trust required as creating an invalid header is expensive.
3) Block w/o witness data - significant bandwidth savings, (~75%) and
allows next miner to include transactions as normal. Again, not much
trust required as creating an invalid header is expensive.
4) Witness data - proves that block is actually valid.
The problem is  is optional: the only case where not having the
witness data matters is when an invalid block is created, which is a
very rare event. It's also difficult to test in production, as creating
invalid blocks is extremely expensive - it would be surprising if an
anyone had ever deliberately created an invalid block meeting the
current difficulty target in the past year or two.
# The nightmare scenario - never tested code ~never works
The obvious implementation of highly optimised mining with segregated
witnesses will have the main codepath that creates blocks do no
validation at all; if the current ecosystem's validationless mining is
any indication the actual code doing this will be proprietary codebases
written on a budget with little testing, and lots of bugs. At best the
codepaths that actually do validation will be rarely, if ever, tested in
Secondly, as the UTXO set can be updated without the witness data, it
would not be surprising if at least some of the wallet ecosystem skips
witness validation.
With that in mind, what happens in the event of a validation failure?
Mining could continue indefinitely on an invalid chain, producing blocks
that in isolation appear totally normal and contain apparently valid
transactions. It's easy to imagine this happening from an engineering
perspective: a simple implementation would be to have the main mining
codepaths be a separate, not-validating, process that receives "invalid
block" notifications from another process containing a validating
implementation of the Bitcoin protocol. If a bug/exploit is found that
causes that validation process to crash, what's to guarantee that the
block creation codepath will even notice? Quite likely it will continue
creating blocks unabated - the invalid block notification codepath is
never tested in production.
# Easy solution: previous witness data proof
To return segregated witnesses to the status quo, we need to at least
make having the previous block's witness data be a precondition to
creating a block with transactions; ideally we would make it a
precondition to making any valid block, although going this far may
receive pushback from miners who are currently using validationless
mining techniques.
We can require blocks to include the previous witness data, hashed with
a different hash function that the commitment in the previous block.
With witness data W, and H(W) the witness commitment in the previous
block, require the current block to include H'(W)
A possible concrete implementation would be to compute the hash of the
current block's coinbase txouts (unique per miner for obvious reasons!)
as well as the previous block hash. Then recompute the previous block's
witness data merkle tree (and optionally, transaction data merkle tree)
with that hash prepended to the serialized data for each witness.
This calculation can only be done by a trusted entity with access to all
witness data from the previous block, forcing miners to both publish
their witness data promptly, as well as at least obtain witness data
from other miners. (if not actually validate it!) This returns us to at
least the status quo, if not slightly better.
This solution is a soft-fork. As the calculation is only done once per
block, it is *not* a change to the PoW algorithm and is thus compatible
with existing miner/hasher setups. (modulo validationless mining
optimizations, which are no longer possible)
# Proofs of non-inflation vs. proofs of non-theft
Currently full nodes can easily verify both that inflation of the
currency has no occured, as well as verify that theft of coins through
invalid scriptSigs has not occured. (though as an optimisation currently
scriptSig's prior to checkpoints are not validated by default in Bitcoin
It has been proposed that with segregated witnesses old witness data
will be discarded entirely. This makes it impossible to know if miner
theft has occured in the past; as a practical matter due to the
significant amount of lost coins this also makes it possible to inflate
the currency.
How to fix this problem is an open question; it may be sufficient have
the previous witness data proof solution above require proving posession
of not just the n-1 block, but a (random?) selection of other previous
blocks as well. Adding this to the protocol could be done as soft-fork
with respect to the above previous witness data proof.

@_date: 2015-12-23 07:41:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segregated witnesses and validationless mining 
Note that this fix can be designed to retain the possibility of
validationless mining, by allowing empty blocks to be created if the
previous witness data proof is omitted. This would achieve the same goal
as Gregory Maxwell's blockchain verification flag(1) but with
significantly less ability/reason to lie about the status of that flag.
1) [bitcoin-dev] Blockchain verification flag (BIP draft),
   Gregory Maxwell, Dec 4th 2015,

@_date: 2015-12-28 11:12:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
That's certainly be a good place to be, but the design of Bitcoin
currently makes achieving that goal fundementally difficult. Do you specifically mean selfish mining as defined in Emin G?n
Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant
issue in a scenario - one malicious miner with >30% hashing power -
where you're already very close to the margins anyway; the difference
between a 50% attack threshold and a 30% attack threshold isn't very
Far more concerning is network propagation effects between large and
small miners. For that class of issues, if you are in an environemnt
where selfish mining is possible - a fairly flat, easily DoS/sybil
attacked network topology - the profitability difference between small
and large miners even *without* attacks going on is a hugely worrying
problem. OTOH, if you're blocksize is small enough that propagation time
is negligable to profitability, then selfish mining attacks with <30%
hashing power aren't much of a concern - they'll be naturally defeated
by anti-DoS/anti-sybil measures.
I think the latter was assumed as well, although ruling out of the
former is impossible.
Note though that Eligius is *not* the only pool to have had problems
with block withholding, though AFAIK Eligius is the only one who has
gone on record so far. (as I said in my original post, I'm relaying
information given to me under condition of confidentiality)

@_date: 2015-12-28 12:02:21
@_author: Peter Todd 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
I mean Knowing Your Customer. The only way to know that a customer is
*not* coming from a competitor's data center is to know their identity,
which is precisely what KYC is.
In the financial world, KYC is used to refer to any time you take steps
to determine the real identity/deanonymize your customers.
That's a terrible outcomes for decentralization; we *want* people to be
able to contribute hashing power to the network even if they don't
already have personal connections with existing miners. That's how we
attract new players to the mining industry whose backgrounds are not
correlated with the backgrounds of other miners - exactly what we want
for decentralization.
Keep in mind that access to cheap power and cheap opportunities to get
rid of waste heat is naturally decentralized by physics, economics, and
politics. Basically, the cheapest power, and cheapest ways to get rid of
waste heat, is in the form of unique opportunities that don't have
economies of scale. For example, much of the Chinese hashing power takes
advantage of stranded hydroelectric plants that are located far away
from markets that would otherwise buy that power. These plants are
limited in size by the local rivers and there's no way to make them any
bigger - there's a natural diseconomy of scale involved.
Now, support if you have access to such a hydro plant - maybe a mine in
the middle of nowhere just closed and there's no-one else to sell the
power too. Right now you can buy some hashing equipment(1) and start
earning money immediately by pointing it at a pool of your choice. If
that pool fucks up, it's really easy for you to change a few lines in
your configs and point that hashing power to a different pool.
However if block withholding attacks continue and kill off open access
pools the process becomes much more arduous. Chances are you won't even
bother, and Bitcoin will end up with one less decentralized
1) If access to hashing equipment becomes a limiting factor/fails to
improve, Bitcoin itself will likely have to switch PoW functions to
succeed as a decentralized system.
Again, lets look at it from the perspective of someone with access to
cheap power.
With DRM tech a likely implementation is the equipment manufacturer/pool
operator sells you a locked down, tamper-resistant, box that only can
send hashing power to a specific pool. 21 for example has been
investigating this model. If such equipment is common, even though the
guy with a hydro plant in Siberia is physically and politically highly
decentralized, the control of the blocks created is highly centralized,
rendering his contribution to the network's decentralization moot.
At best we might get general purpose attestation, but implementing that
vs. locked down, single pool, boxes is more expensive and slower to
market. Even then, we'd be much more likely to get fragile and
difficult-to-reverse-engineer hashing equipment that's always going to
be easier to add backdoors too.
We're better off with an ecosystem where DRM tech like attestation isn't
needed at all.
As for cloud hashing... those scams have mostly died off as the market
has become more efficient.
What evidence do you have for them being "clearly quite effective"? Is
there any evidence that they were used against GHash.io for example?
Remember that block withholding attacks give an advantage to those with
access to large amounts of physical hashing power, like GHash.IO did at
that time.
It's not a change to the PoW, just a change to the definition of block
validity; mining hardware does *not* need to change to implement
Luke-Jr's idea. Also, as mentioned elsewhere in this thread, it can be
implemented slowly as a pseudo-soft-fork.

@_date: 2015-12-28 21:35:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a simple 
Occured to me that this hasn't been mentioned before...
We can trivially fix the quadratic CHECK(MULTI)SIG execution time issue
by soft-forking in a limitation on just SignatureHash() to only return
true if the tx size is <100KB. (or whatever limit makes sense)
This fix has the advantage over schemes that limit all txs, or try to
count sigops, of being trivial to implement, while still allowing for a
future CHECKSIG2 soft-fork that properly fixes the quadratic hashing
issue; >100KB txs would still be technically allowed, it's just that
(for now) there'd be no way for them to spend coins that are
cryptographically secured.
For example, if we had an issue with a major miner exploiting
slow-to-propagate blocks(1) to harm their competitors, this simple fix
could be deployed as a soft-fork in a matter of days, stopping the
attack quickly.
1)  at lists.sourceforge.net/msg03200.html

@_date: 2015-12-30 06:19:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] An implementation of BIP102 as a softfork. 
Your fear is misplaced: it's trivial to avoid recursion with a bit of
For instance, if Bitcoin was redesigned to incorporate the forced fork
concept, instead of block headers committing to just a merkle root,
they could instead commit to H(version + digest)
For version == 0, digest would be a merkle root of all transactions. If
the version was > 0, any digest would be allowed and the block would be
interpreted as a NOP with no effect on the UTXO set.
In the event of a major change - e.g. what would otherwise be a
hard-forking change to the way the merkle root was calculated - a
soft-fork would change the block validity rules to make version == 0
invalid, and verison == 1 blocks would interpret the digest according to
the new merkle root rules. Again, version > 1 blocks would be treated as
A good exercise is to apply the above to the existing Bitcoin ecosystem
as a soft-fork - it certainely can be done, and done right is
technically very simple.
Regardless of how it's done - existing Bitcoin compatible or clean sheet
redesign - you get the significant safety advantages soft-forks have
over hard-forks in nearly all situations where you'd have to do a
hard-fork. OTOH, it's kinda scary how this institutionalizes what could
be seen as 51% attacks, possibly giving miners significantly more
control over the system politically. I'm not sure I agree with that
viewpoint - miners can do this anyway - but that has made people shy
away from promoting this idea in the past. (previously it's been often
referred to as an "evil" soft-fork)

@_date: 2015-12-30 06:28:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] An implementation of BIP102 as a softfork. 
It's very dangerous to simply send multiple transactions in such a way
that they don't double-spend each other; you have no good way of knowing
for sure that you're seeing the longest block chain with software alone.
Competently designed software with fee-bumping wouldn't allow that
mistake to be made; the UX should make it clear that txs sent are still
pending until confirmed or clearly double-spent.
That can mess up pre-signed transations, e.g. refunds.

@_date: 2015-12-30 06:31:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] An implementation of BIP102 as a softfork. 
Actually, a better name is probably "forced soft-fork", making this
clear we're using the soft-fork mechanism to force everyone to upgrade.

@_date: 2015-12-30 20:16:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] How to preserve the value of coins after a fork. 
Hash: SHA512
Note how transaction malleability can quickly sabotage naive notions of this idea.
Equally, if this looks like it might ever be implemented, rather than using a hard fork, using a forced soft-fork to deploy changes becomes attractive.

@_date: 2015-12-30 20:32:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] How to preserve the value of coins after a fork. 
Hash: SHA512
You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.
Look into what a segwit transaction hashes - that's a better notion of non-malleable transaction. But even then lots of transactions are malleable, and its easy to trigger those cases intentionally by third parties.
Most likely any Bitcoin United scheme would quickly diverge and fail; much simpler and more predictable to achieve convincing consensus, e.g. via proof of stake voting, or Adam Bank's extension blocks suggestions. (or of course, not trying to force controversial forks in the first place)

@_date: 2015-12-31 15:14:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP numbers 
You ever noticed how actually getting a BIP # assigned is the *last*
thing the better known Bitcoin Core devs do? For instance, look at the
segregated witness draft BIPs.
I think we have problem with peoples' understanding of the Bitcoin
consensus protocol development process being backwards: first write your
protocol specification - the code - and then write the human readable
reference explaining it - the BIP.
Equally, without people actually using that protocol, who cares about
the BIP?
Personally if I were assigning BIP numbers I'd be inclined to say "fuck
it" and only assign BIP numbers to BIPs after they've had significant
adoption... It'd might just cause a lot less headache than the current

@_date: 2015-12-31 15:48:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segregated witnesses and validationless mining 
============================== START ==============================
Updates from IRC discussion:
1) There was some debate about what exactly should be required from the
current block to calculate the previous block posession proof. For
instance, requiring the coinbase outputs potentially restricts some
mining setups; requiring a commitment to the current block's
(non-coinbase) transaction outputs restricts tx selection outsourcing
However, it appears that we can allow the nonce to be picked
arbitrarily. Equally, if the nonce is arbitrary, then a future soft-fork
can be add commitments to current block contents. Thus the previous
block proof can be simple H( + )
2) Pieter Wuille brought up fraud proofs in relation to previous block
content proofs - specifically how the simplest H( +
) construction requires a large fraud proof to
prove incorrect. This followed a bunch of debate over what exactly fraud
proofs would be - a proof that some data is fraudulent, or a unmet
challenge that some data is correct?
Regardless, if the posession proof is structured as a merkle tree, then
fraud can be easily proven with a merkle path. In that model we'd take
the previous block contents and rehash it in its entirety with the
nonce. The fraud proof then becomes two merkle paths - one in the
original block with the original hash, and the second with the same
data, and same structure, but with the nonce mixed into the hashing
Todo: writeup the difference between the fraud proof model, and the
validity challenge model, to provide background to making this decision.
Incidentally, based the positive response to fixing this issue w/
segregated witnesses - my main objection to the plan - I've signed the
Bitcoin Core capacity increases statement:

@_date: 2015-02-05 17:17:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] determining change addresses using the 
Have you looked at Armory? IIRC they do this kind of stuff.

@_date: 2015-02-05 17:34:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Merged mining a side chain with proof of 
Oh, where I was saying OP_DEPTH, I was referring to a *hypothetical*
opcode; I'd forgotten when I wrote that post that OP_DEPTH is an real
These days I'd suggest you use the (upcoming on BTC/live on Viacoin)
OP_CHECKLOCKTIMEVERIFY opcode instead. Pretty simple really:
     CHECKLOCKTIMEVERIFY

@_date: 2015-02-09 00:32:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Update to Double-Spend Deprecation 
Hash: SHA256
This is an incredibly dangerous and foolish proposal that opens up the Bitcoin network to serious vulnerabilities, both from attackers outside the network, as well as miners trying to gain an advantage over their competition.
Ultimately it's flawed for the same root problem that proof-of-stake proposals suffer from: the p2p network just isn't a reliable broadcast medium. Seeing a transaction is not a guarantee that any other node has seen it; not seeing a transaction is not a guarantee other nodes have not seen a spend.
You can measure "propagation times" and other metrics all you want, but you're measuring a network that isn't under attack; Bitcoin must be robust against attacks, and it must not create incentives to launch them. Institutionalising the punishment of miners being they did not have perfect connectivity - an unattainable goal in a trust less, decentralised system - is athema to the goals of having a decentralised systmem and will only lead to smaller mining operations being punished for being the victim of attacks on their network connectivity that are only made profitable by this proposal.
Equally your proposal actually makes it *easier* to pull off apparently single-confirm double-spend attacks - any miner who ignores a block containing the apparent double-spend is just as likely to be aiding an attacker trying to get a 1-conf transaction double-spent. This forces *everyone* to waiting *longer* before accepting a transaction because now even a single-confirmation is no longer good evidence of an accepted transaction. In an ecosystem where hardly anyone relies on zeroconf anyway your putting a much larger group of people at risk who weren't at risk before.
Frankly if this idea gets traction it should serve as a warning to all miners that it's time they adopt replace-by-fee to set a clear precedent that they have no obligations other than the same economic self-interest- not vague notions of "honesty" - that makes Bitcoin work in the first place.
BTW you quote Hal Finney and Satoshi in your proposal to try to lend support to it. Don't do that - appealing to authority is a surefire way to get people to ignore you. Its particularly bad when the authorities being appealed too haven't participated in consensus research for years; you're referencing stuff from a time when Bitcoin was barely understood.

@_date: 2015-02-12 01:47:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
My replace-by-fee patch is now available for the v0.10.0rc4 release:
    Along with demo scripts of the functionality:
    New to this version is a comprehensive set of unittests under
Additionally the preferential peering support now preferentially peers
with Bitcoin XT? nodes that support Andresen/Harding's double-spend
relaying? patch. While Bitcoin XT nodes don't accept double-spends into
their mempool, they do relay them perfectly well and thus are an asset
to those doing replace-by-fee mining.?
I've had a number of requests from miners for a version of
replace-by-fee against Luke-Jr's Eligius patches?; I'll be also
releasing that shortly once this release has undergone some more
What's replace-by-fee?
Currently most Bitcoin nodes accept the first transaction they see
spending an output to the mempool; all later transactions are rejected.
Replace-by-fee changes this behavior to accept the transaction paying
the highest fee, both absolutely, and in terms of fee-per-KB. Replaced
children are also considered - a chain of transactions is only replaced
if the replacement has a higher fee than the sum of all replaced
Doing this aligns standard node behavior with miner incentives: earn the
most amount of money per block. It also makes for a more efficient
transaction fee marketplace, as transactions that are "stuck" due to bad
fee estimates can be "unstuck" by double-spending them with higher
paying versions of themselves. With scorched-earth techniques? it gives
a path to making zeroconf transactions economically secure by relying on
economic incentives, rather than "honesty" and alturism, in the same way
Bitcoin mining itself relies on incentives rather than "honesty" and
Finally for miners adopting replace-by-fee avoids the development of an
ecosystem that relies heavily on large miners punishing smaller ones for
misbehavior, as seen in Harding's proposal? that miners collectively 51%
attack miners who include doublespends in their blocks - an unavoidable
consequence of imperfect p2p networking in a decentralized system - or
even Hearn's proposal? that a majority of miners be able to vote to
confiscate the earnings of the minority and redistribute them at will.
Once you've compiled the replace-by-fee-v0.10.0rc4 branch just run your
node normally. With -debug logging enabled, you'll see messages like the
following in your ~/.bitcoin/debug.log indicating your node is replacing
transactions with higher-fee paying double-spends:
    2015-02-12 05:45:20 replacing tx ca07cc2a5eaf55ab13be7ed7d7526cb9d303086f116127608e455122263f93ea with c23973c08d71cdadf3a47bae45566053d364e77d21747ae7a1b66bf1dffe80ea for 0.00798 BTC additional fees, -1033 delta bytes
Additionally you can tell if you are connected to other replace-by-fee
nodes, or Bitcoin XT nodes, by examining the service bits advertised by
your peers:
    $ bitcoin-cli getpeerinfo | grep services | egrep '((0000000000000003)|(0000000004000001))'
            "services" : "0000000000000003",
            "services" : "0000000004000001",
            "services" : "0000000004000001",
            "services" : "0000000000000003",
            "services" : "0000000004000001",
            "services" : "0000000004000001",
            "services" : "0000000000000003",
            "services" : "0000000000000003",
Replace-by-fee nodes advertise service bit 26 from the experimental use
range; Bitcoin XT nodes advertise service bit 1 for their getutxos
support. The code sets aside a certain number of outgoing and incoming
slots just for double-spend relaying nodes, so as long as everything is
working you're node should be connected to like-minded nodes a within 30
minutes or so of starting up.
If you *don't* want to advertise the fact that you are running a
replace-by-fee node, just checkout a slightly earlier commit in git; the
actual mempool changes are separate from the preferential peering
commits. You can then connect directly to a replace-by-fee node using
the -addnode command line flag.
1) 2) 3) 4) 5) 6)  at lists.sourceforge.net/msg06970.html
7)

@_date: 2015-02-12 02:45:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
Absolutely nothing in the replace-by-fee patch is consensus critical;
your objection is entirely an artifact of the poor modularity of the
Bitcoin Core codebase, something that is being actively improved on as
we speak.
Anyway, the logic of dealing with double-spends and keeping mempools
synced is pretty trivial:
    for i in range(len(tx.vout)):
        for double_spent_tx in mempool.mapNextTx[COutPoint(tx.GetHash(), i)]:
            mempool.remove(double_spent_tx, recursive=True)
    mempool.add(tx)
IOW, assume every transaction your "border router" gives you is now the
one and only true transaction, and everything conflicting with it must
All the complexity of replace-by-fee is in deciding when one transaction
should replace another(s). Other than that the code is simple and very
similar to block handling logic.

@_date: 2015-02-12 03:49:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
Wait, what the heck do you mean by "only if it is actually replacing an
How does my replace-by-fee patch *not* do that?

@_date: 2015-02-12 14:45:17
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
None of those solutions are compatible with decentralized networks for a
lot of reasons. Given the inability to prevent sybil attacks your
suggestions lead to people being unfairly punished for poor connectivity
that may be entirely out of their control. They also make maintaining a
Bitcoin node and mining the blockchain require a significant amount of
hands on maintenance, again incompatible with a decentralized system.

@_date: 2015-02-12 15:06:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
Speaking of, a relatively simple thing that would help dispel these
notions would be if some wallets supported replace-by-fee-using
fee-bumping and an "attempt undo" button. Armory is an (unfortunately!)
special case because it uses a full node and has good privacy
guarantees, but most wallets could implement this by just sending the
doublespend transactions to any node advertising either the
replace-by-fee or GETUTXO's service bits.
1)

@_date: 2015-02-12 15:18:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
That's actually incorrect now, as a mechanism for implementing
scorched-earth without child-pays-for-parent using SIGHASH_ANYONECANPAY
is available:
I greatly prefer this mechanism as it's an opt-in mechanism - many
wallets double-spend on occasion by accident - and can have the
incentives be adjusted to suit the % of hashing power that actual
supports replace-by-fee. (and the % probability you'll see the
My patch implements 90% of the logic required for the above to work,
however I've intentionally limited the total depth of recursion when the
replacement is being evaluated as an interm anti-DoS measure in the
spirit of belt-and-suspenders engineering. This can certainly be
improved on, e.g. by limiting total mempool size.

@_date: 2015-02-13 02:53:14
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP for deterministic pay-to-script-hash 
It might be enough to rewrite this BIP to basically say "all pubkeys
executed by all CHECKMULTISIG opcodes will be in the following canonical
order", followed by some explanatory examples of how to apply this
simple rule.
OTOH we don't yet have a standard way of even talking about arbitrary
scripts, so it may very well turn out to be the case that the above rule
is too restrictive in many cases - I certainly would not want to do a
soft-fork to enforce this, or even make it an IsStandard() rule.

@_date: 2015-02-14 08:13:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: [Libbitcoin] 
I haven't bothered reading the thread, but I'll put this out there:
The consensus critical Satoshi-derived sourcecode is a protocol
*specification* that happens to also be machine readable and executable.
Rewriting it is just as silly as as taking RFC 791 and rewriting it
because you wanted to "decentralize control over the internet"
My replace-by-fee fork of Bitcoin Core is a perfect case in point: it
implements different non-consensus-critical policy than Bitcoin Core
does, while adhering to the same Bitcoin protocol by virtue of being the
same sourcecode - the same protocol specification. When I went to miners
asking them to implement it, the biggest concern for them is "Will it
stay in consensus with other miners?" If I had rewritten the whole thing
from scratch the fact is the honest answer to them would be no way in
hell - reimplementing Bitcoin and getting it right is software
engineering's Apollo Project and none of us have the resources to pull
that off. But I didn't, which means we might soon have a significant
chunk of hashing power implementing a completely different mining policy
than what is promoted by the Bitcoin Core maintainers.
By reimplementing consensus code - rewriting the protocol spec - you
drop out of the political process that is Bitcoin development. You're
not decentralizing Bitcoin at all - you're contributing to its
centralization by not participating, leaving behind a smaller and more
centralized development process. Fact is, what you've implemented in
libbitcoin just isn't the Bitcoin protocol and isn't going to get
adopted by miners nor used by serious merchants and exchanges - the
sources of real political power.
Right now we could live in a world where a dozen different groups
maintain Bitcoin implementations that are actually used by miners. We
could have genuine innovation on the p2p networking layer, encryption,
better privacy for SPV clients, better resistance to DoS attacks. We
could have diverse tx acceptance policies rather than wasting hundreds
of man hours bitching about how many bytes OP_RETURN should allow. We
could have voices from multiple groups at the table when the community
discusses how to scale Bitcoin up.
Instead we have a world with a half dozen teams wasting hundreds if not
thousands of of man hours dicking around trying to rewrite consensus
critical *specifications* because they happen to be perfectly good
executable code, and the first thing a programmer thinks when they see
perfectly good battle-hardened code is "Hey! Let's rewrite that from
You know you does have significant political power over the development
of the Bitcoin protocol *other* than the Bitcoin Foundation?
Luke Dashjr.
Because he maintains the Eligius fork of Bitcoin Core that something
like %30 of the hashing power run. It Actually Works because it uses the
Actual Protocol Specification, and miners know if they run it they
aren't going to lose tens of thousands of dollars. It's why it's easy to
get transactiosn mined that don't meet the Bitcoin Core's IsStandard()
rules: they aren't part of the protocol spec, and Luke-Jr has different
views on what transactions should and should not be allowed into the
And when Gavin Andresen starts negotiating with alt-implementations to
get his bloat coin proposals implemented, you know who's going to be at
the table? Luke-Jr again! Oh sure, the likes of btcd, libbitcoin, toshi,
etc. will get invited, but no-one's going to really care what they say.
Because at best only a tiny - and foolish - sliver of hashing power will
be using their implementations of Something Almost But Not Quite
Bitcoin?, and any sane merchant or exchange will be running at least one
or two Bitcoin Foundation Genuine Bitcoin Core? nodes in front of any
from-scratch alt-implementation.
So stop wasting your time. Help get the consensus critical code out of
Bitcoin Core and into a stand-alone libconsensus library, wrap it in the
mempool policy, p2p networking code, and whatever else you feel like,
and convince some hashing power to adopt it. Then enjoy the fruits of
your efforts when the next time we decide to soft-fork Bitcoin the
process isn't some secretive IRC discussion by a half-dozen "core
developers" - and one guy who finds the term hilarious - but a full on
DIRECT DEMOCRACY OCCUPY WALL STREEET MODIFIED CONSENSUS POW-WOW,
complete with twinkle fingers. A pow-wow that you'll be an equal part
of, and your opinions will matter.
Or you can be stereotypical programmers and dick around on github for
the next ten years chasing stupid consensus bugs in code no-one uses.
The choice is yours.

@_date: 2015-02-15 12:02:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Wallet and RPC server are definitely not consensus critical code.
P2P service rules are weakly consensus critical, in that a failure to
relay valid blocks can in practice cause a loss of consensus. But
relaying valid blocks is very easy, and you only need sone relay
mechanism out of many to work for consensus to be maintained.
OpenSSL is getting replaced by libsecp256k1, a library designed for
consensus-critical applications.
As for databases, look at the good  discussion yesterday
for strategies to make databases less relevant to consensus.
Are you referring to feature extensions to consensus critical code -
like my own CHECKLOCKTIMEVERIFY? - or extensions to code that isn't
consensus critical?
Yes you are dicking around. The effort you're going to spend recreating
the core consensus code and getting it right is orders of magnitude more
work than figuring out how to use the foreign function interface in your
chosen language, or at worse, just running Bitcoin Core to do validation
and using RPC or the p2p protocol locally to track that state.
Don't assume your prior experience with other commercial projects has
any bearing on Bitcoin: consensus-critical crypto is a brand new field
within software engineering with very unique requirements, pioneered by
Bitcoin itself.

@_date: 2015-02-15 12:11:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
It's worth remembering that one of the goals in writing - or to be more
precise, separating - libconsensus from the Bitcoin Core codebase is to
make it easier to maintain strict consensus between Bitcoin Core
The necessity of it isn't a political or emotive issue, but the
consequences are definitely political. Just not in the way that most of
the ecosystem appears to think.

@_date: 2015-02-15 12:21:09
@_author: Peter Todd 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Ah, sorry, that wasn't clear to me.
Indeed, which is why I've done a lot of work on a reimplementation of
the Bitcoin scripting system as well:
Which has this cheery warning at the top:
"""Script evaluation
Be warned that there are highly likely to be consensus bugs in this
code; it is unlikely to match Satoshi Bitcoin exactly. Think carefully
before using this module.
I'll be adding a FFI interface to libconsensus in the future... and I
probably should make that warning scarier...

@_date: 2015-02-19 03:44:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
...in the same way going to university may be one of the more valuable things you ever do. But using the code resulting from that process over Satoshi Bitcoin/libconsensus is foolish.
I suggest you actually look at the git commit history for the consensus-critical part of the Bitcoin Core codebase - so much work cleaning it up and refactoring has been done for v0.10.0/libconsensus that I think we're risking the introduction of a consensus bug unnecessarily and should slow down a little.
"holy scripture" it ain't.
Again, this is exactly what people are working towards, at a speed that if anything is probably a bit too rapid.
-----BEGIN PGP SIGNATURE-----

@_date: 2015-02-22 01:15:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I think you guys are reading too much into the name... Replace-by-fee is called "replace-by-fee" because it considers whether to replace or not based on fee; the idea came about in an discussion about replacement based on nSequence.
I forget whether it was myself or John Dillon who came up with the name "scorched earth", but it just refers to the game theory behind the *specific* idea of the receiver combating a zeroconf double-spend by sending all the funds to fees. Scorched earth as in "You're trying to defraud me, so I'm not going yo play this game or negotiate, I'm just going to immediately do what is most likely to make you lose the maximum amount of money to punish you for your vandalism."
I'm not so convinced, precisely because we've seen zeroconf fail in pretty bad ways; the people most vulnerable to losses have generally changed the way they operate. (e.g. ATM's that no longer rely on zeroconf security, instead waiting for confirmations, installing cameras, etc.)
My  concern right now is person-to-person trading, and people doing that tend to wait for confirmations or otherwise protect themselves. (e.g. reputation systems)
Agreed. Deploying it has been something I've made into a long, drawn out, protracted process for precisely that reason. OTOH I sometimes wonder if I've gone too far with that - the services that themselves try to guarantee zeroconf right now through metrics are themselves highly centralised, and there's a big risk of them driving mining centralisation itself when they fail.

@_date: 2015-02-22 07:34:28
@_author: Peter Todd 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
FWIW I've been advocating this kind of thing in various forms for
literally years, including to hold fidelity bonded banks honest - what
you now call 'federated sidechains' - and most recently Feb 12th on
19:56 < petertodd> leakypat: now, do note that an advanced version [of replace-by-fee scorched earth] could be to make another tx that alice and bob setup in advance such that if alcie doublespends, bob gets the money *and* alice pays a bunch of cash to miners fees
19:57 < petertodd> leakypat: this would work espectially well if we improved the scripting system so a script could evaluate true based on proof-of-doublespend
19:58 < leakypat> Yeah, proof of double spend would ideally be done at the protocol level
19:59 < petertodd> leakypat: if satoshi hadn't make the multiple things that CHECKSIG does into one opcode it'd be really easy, but alas...
Implementing it as a general purpose scripting language improvement has
a lot of advantages, not least of which is that you no longer need to
rely entirely on inherently unreliable P2P networking: Promise to never
create two signatures for a specific BIP32 root pubkey and make
violating that promise destroy and/or reallocate a fidelity bond whose
value is locked until some time into the future. Since the fidelity bond
is a separate pool of funds, detection of the double-spend can happen
Equally, that *is* what replace-by-fee scorched-earth does without the
need for a soft-fork, minus the cryptographic proof and with a bit less
Is releasing a version of Bitcoin Core with different IsStandard() rules
than the previous version vandalism? Is mining with a different policy
than other people vandalism? Is mining at a pool that gets sybil
attacked vandalism? Are my replace-by-fee tools an act of vandalism?
Because every one of those things causes people to get double-spent in
the real world, even losing tens of thousands of dollars until they get
some sense and stop treating unconfirmed transactions as confirmed.
Is it vandalism if you decide to host a wedding right next to a hairpin
corner at a rally race and complain to me that mud is getting on the
pretty white dresses? Is it vandalism if I tell that wedding party to
fuck off before someone gets hurt? Is it vandalism if some racers take
the mudguards off for a few laps to see if we can encourage them to
leave before someone gets *actually* hurt? Or someone decides that the
solution is to pave the track over and hold a bicycle race instead...

@_date: 2015-02-22 13:53:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
That's a really old idea - I proposed it about two years ago. The optimal way is to allow any txout to be replaced with one with an equal or greater nValue and same scriptPubKey, as well as additional txouts added. (obviously so long as none are removed)
Alas, there's lots of situations where this restricts you from doing useful things, for instance collapsing multiple payments into one by repeated updating to reduce tx size. Equally the benefit is marginal at best given how insecure unconfirmed transactions are - breaking what is already broken isn't a negative.

@_date: 2015-02-22 14:07:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] alternate proposal opt-in miner 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Great example! Systems that appear more secure than they really are to uninformed users are dangerous. Same reason why brain wallets are such scary technology, and equally, why I like to give a few dollars away every so often to the guys brute forcing weak ones.
In my experience there's a pattern of "accept unconfirmed; get burned badly/see someone else get burned; stop relying on them" Although of course, there's some bias in that people contact me asking what to do after they get burned. :)

@_date: 2015-02-22 09:33:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
Indeed, which is why I wrote some easy-to-use and highly effective tools
to pull off double-spends and made sure to publicise them and their
effectiveness widely. They've had their desired effect and very few
people are relying on unconfirmed transactions anymore. As for the
remaining, next week alone I'll be volunteering one or two hours of my
consulting time to discuss solutions with a team doing person-to-person
trading for instance.
Like I've said repeatedly, the current "weaker" 0-conf transactions gets
people new to Bitcoin - both individuals and companies - burnt over and
over again because inevitably someone eventually gets motivated and
breaks them, and suddenly they lose stacks of money.
Keeping *that* kind of "security" around rather than depreciating it
ASAP and being honest about what Bitcoin can do does no-one any good.
Anyway, there is no one magic solution to this stuff - the best
solutions vary greatly on the situation.

@_date: 2015-02-22 10:41:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
You lot spend so much time trying to claim I'm working for people I'm
not that I have a bad feeling I'm going to end up having to explain what
an internet troll is to "friendly" Revenue Canada tax auditor...
That tool was intentionally shipped with unclear instructions and nearly
all the double-spend strategies turned off by default; you can easily
increase that number with a little understanding.
"[Bitcoin-development] Eliminating double-spends with two-party
self-escrow for high value transactions",
Peter Todd, Apt 26th 2014,
(note that the above should be updated to use CHECKLOCKTIMEVERIFY)

@_date: 2015-02-22 12:12:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
No, OTOH if they don't then the situation is no difference from what we
have now, and replace-by-fee does no harm. Meanwhile, relaying of bare
double-spend signatures can be implemented in the future, as I suggested
last year for your/Andresen's double-spend relaying patch.
Did you notice the even more obvious way to defeat ANYONECANPAY scorched
earth with that patch?
So? RBF nodes will.
I suspect many won't, because few people need to rely on unconfirmed
transactions anyway.
If you're going to consider replacement, conflict processing will
definitely be more expensive. :)
An actual DoS attacker would do their DoS attack in a way where conflict
processing has nothing to do with it, so this change does no actual
What exact git commit were you looking at? I did have an early one that
did have a bug along those lines, now fixed.
The current version ensures every replacement pays at least as much
additional fees as would normally cost to broadcast that much data on
the network, and additionally requires the fees/KB to always increase;
under all circumstances it should be no more of a DoS threat than
low-fee transactions are otherwise. I'd like to know if there is a flaw
in that code however!

@_date: 2015-02-22 21:50:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Not that issue - that's both easily avoidable, and has nothing to do with the replace-by-fee patch. I'm talking about something in the specific patch - good test to see if you've fully reviewed it.

@_date: 2015-01-02 22:48:29
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP: Voluntary deposit bonds 
It definitely does change the incentives as it makes it easy and secure
to pay miners to mine specific blocks rather than specific transactions.
For instance I could securely pay a miner to mine a re-org in a specific
way, something I can't do right now. From the perspective of "the
blockchain must move forward" this is worrying. I have proposed this
idea before myself for my PowPay(1) micropayments scheme, but on
reflection I don't think it's a good idea anymore.
PowPay in general is an idea I'm now rather dubious about: it works much
better with large mining pools, which would further incentivise pools to
get bigger. In general we want mining to be dumber, not smarter, to keep
the overhead of it as small as possible to getting into it is as easy as
re: hard-fork vs. soft-fork, Gregory Maxwell's comments elsewhere in the
thread are what I'd say myself.
1) [Bitcoin-development] Coinbase TxOut Hashcash,
   Peter Todd, May 10th 2013,
    at lists.sourceforge.net/msg02159.html

@_date: 2015-01-10 00:40:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] OpenSSL 1.0.0p / 1.0.1k incompatible, 
As an aside, it's interesting to note that this issue is not entirely
unique to miners.
For example in micropayment channel protocols the receiver must validate
signatures from the sender to ensure that they will be able to broadcast
transactions containing those signatures in the near-future. If they
accept a signature as valid that the majority of hashing power rejects
as invalid the sender can simply wait until the micropayment channel
timeout expires to recover 100% of their funds, ripping off the
receiver. There's many other advanced Bitcoin protocols with similar
vulnerabilities; I'd be interested to hear if anyone can come up with a
similar vulnerability in a non-Bitcoin protocol, and wouldn't be that
surprised if they did.
While I have often cautioned people before to avoid using libsecp256k1
for verification on the grounds that consensus trumps correctness, the
above incompatibility does strongly suggest that OpenSSL may not itself
have very good consensus-critical design. Along with Maxwell and
Wuille's recent findings? CVE-2014-3570 - strong evidence of the
excellent testing the library has undergone - I personally am now of the
opinion that migrating Bitcoin Core to libsecp256k1 in the near future
is a good idea on the grounds that it provides us with a well-written,
and well-understood library designed with consensus in mind that'll
probably give us fewer consensus problems than our existing OpenSSL
dependency. It'll also help advanced protocol implementations by giving
them a clear dependency to use when they need consensus-critical
signature evaluation.
1)

@_date: 2015-01-11 17:24:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bi-directional micropayment channels with 
Would you mind doing up some actual scriptPubKeys/transactions using
this idea as an example? I think it'd make the review process a lot
easier for everyone if there was something more concrete. (equally,
sorry I haven't had a chance to look at this, very busy for the rest of
the month)
You may find my CLTV-using micropayment channel demo useful reference
material too:

@_date: 2015-01-19 12:48:26
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP70: why Google Protocol Buffers 
Hash: SHA256
Protocol buffers isn't any more hashable than XML or json - round trips aren't deterministic with standard protobuf libraries. To make it deterministic you end up creating a new standard.
I have this problem for an asset representation standard for one of my clients, and I've reluctantly had to roll my own format.

@_date: 2015-01-19 13:06:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP70: why Google Protocol Buffers for 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
That's 100% true: BIP70 passes around serialized protobuf data that it signs directly for this reason; it could just as easily be a byte array with json in it. (not that json/XML/etc. doesn't have other flaws)

@_date: 2015-01-20 10:46:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] The legal risks of auto-updating wallet 
I was talking to a lawyer with a background in finance law the other day
and we came to a somewhat worrying conclusion: authors of Bitcoin wallet
software probably have a custodial relationship with their users,
especially if they use auto-update mechanisms. Unfortunately this has
potential legal implications as custodial relationships tend to be
pretty highly regulated.
Why is this? Well, in most jurisdictions financial laws a custodial
relationship is defined as having the ability, but not the right, to
dispose of an asset. If you have the private keys for your users'
bitcoins - e.g. an exchange or "online" wallet - you clearly have the
ability to spend those bitcoins, thus you have a custodial relationship.
However if you can trivially obtain those private keys you can also
argue you have a custodial relationship. For instance StrongCoin was
able to seize funds stolen from OzCoin? with a small change to the
client-side Javascript their users download from them every time they
visit the site. Portraying that as "the ability to dispose of an asset"
in a court of law would be pretty easy. Equally on a technical level
this isn't much different from how auto-updating software works.
Now I'm sure people in this audience will immediately point out that by
that logic your OS vendor is also in a custodial relationship - they
after all can push an update that steals everyones' bitcoins regardless
of what local wallet you use. But the law isn't a deterministic
algorithm, it's a political process. Circle is easy to portray as having
a custodial relationship, StrongCoin and Blockchain.info are a little
harder, Android Wallet harder still, Bitcoin Core's multi-party
deterministicly compiled releases even harder.
But ultimately we're not going to know until court cases start
happening. In the meantime probably the best advice - other than getting
out of the wallet business! - is to do everything you can to prevent
losses through malicious auto-updates. Create systems where as many
people as possible have to sign off and review an update before it has
the opportunity to spend user funds. Not having auto-updates at all is a
(legally) safe way to achieve that goal; if you do have them make sure
the process by which an update happens is controlled by more than one
person and there are mechanisms in place to create good audit logs of
how exactly an update happened.
Finally keep in mind that one of the consequences of a custodial
relationship is that some legal authority might try to *force* you to
seize user funds. StrongCoin made it 100% clear to authorities that they
and sites like them are able to seize funds at will - I won't be
surprised if authorities use that power in the future. The more
automatic and less transparent an update is, the higher the chance some
authority will lean on you to seize funds. So don't make it easy for
yourself to meet those demands.
1)

@_date: 2015-01-20 12:15:57
@_author: Peter Todd 
@_subject: [Bitcoin-development] The legal risks of auto-updating wallet 
Heh, well, courts tend not to have the narrow-minded pedantic logic that
programmers do; quite likely that they'd see having the ability to give
themselves the ability as equivalent to simply having the ability. What
matters more is intent: the authors of an operating system had no intent
to have a custodial relationship over anyones' BTC, so they'd be off the
hook. The authors of a Bitcoin wallet on the other hand, depends on how
you go about it.
For instance Lighthouse has something called UpdateFX, which allows for
multi-signature updates. It also supports deterministic builds, and
allows users to chose whether or not they'll follow new updates
automatically, or only update on demand. In a court that could be all
brought up as examples of intent *not* to have a custodial relationship,
which may be enough to sway judge/jury, and certainly will help avoid
ending up in court in the first place by virtue of the fact that all
those protections help avoid theft, and increase the # of people that an
authority need to involve to seize funds via an update.

@_date: 2015-01-20 12:40:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] The legal risks of auto-updating wallet 
Posessing a private key certainly does not give you an automatic legal
right to anything. As an example I could sign an agreement with you that
promised I would manage some BTC on your behalf. That agreement without
any doubt takes away any legal right I had to your BTC, enough though I
may have have the technical ability to spend them. This is the very
reason why the law has the notion of a custodial relationship in the
first place.
Don't assume the logic you'd use with tech has anything to do with the
logic courts use.

@_date: 2015-01-20 12:49:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] The legal risks of auto-updating wallet 
The law concerns itself with what should be done, not what can be done.
Bitcoin the technology doesn't have a concept of "ownership" - that's a
legal notion, not a mathematical one.

@_date: 2015-01-21 11:10:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I read this and it's boring, now that all my objections have been met. :)
I'll try get a chance to actually test/review this in detail; in SF for
the next three weeks with some ugly deadlines and a slow laptop. :(

@_date: 2015-01-23 09:49:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
That's what P2SH is for; the senders will just be sending to a P2SH address.
Hard-forks aren't hard for directly political issues, they're politically hard because they're risky by requiring everyone yo upgrade at once. In the case of signature validation, that touches a *lot* of third party code that people rely on to avoid being defrauded.
FWIW I've actually got a half-finished writeup for how to use OP_CODESEPARATOR and a CHECKSIG2 soft-fork to have signatures sign fees and so forth.

@_date: 2015-07-03 17:56:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] REQ BIP # / Discuss - Sweep incoming unconfirmed 
CPFP probably needs changes to the P2P layer to be able to support RBF
scorched earth well unfortunately, as currently transactions are
processed individually and out of context. In the RBF case you'd need to
keep previously removed transactions in a buffer and evaluate new
transactions against that buffer - relatively complex.
The other big issue is that existing wallets don't appear to be very
good at preventing double-spends. There's lots of edge cases where
transations aren't recorded correctly, like crashes, shutting down
unexpected etc. and in those cases there's a high chance of the wallet
sending a double-spend by accident. There's also coinjoin to consider -
plainly incompatible. With scorched-earth this will lead to losses.
Fortunately you can implement scorched-earth using SIGHASH_ANYONECANPAY
instead on an opt-in basis, which wallets could add only if they've
taken the special engineering considerations into account first:
    "Replace-by-fee scorched-earth without child-pays-for-parent",
    Peter Todd, Bitcoin-development mailing list, Apr 28th 2014
    For the OP: I'd be interested in pursuing this further.

@_date: 2015-07-03 18:07:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] Mailing List Administrivia - GPG, Archives, 
I'd support making the raw archives public for archiving as well.

@_date: 2015-07-03 23:30:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
As for what "SPV mining" is:
While blocks are propagating throughout the network, frequently it's
possible for miners to get the header of the block before they get and
fully validate the block itself. This is just a few seconds to tens of
seconds, but that's a big deal for profitability. So miners have been
running custom patches that mine on top of the longest chain they know
about, even if they haven't actually verified the blocks in it due to
propagation delays.
Unfortunately the Bitcoin protocol lets you do that, and the extra % of
revenue makes a big difference when you take into account the low profit
margins of mining these days. BIP66 happened to trigger this issue this
time, but actually *any* miner creating an invalid block for *any*
reason would have done so with the software miners are running right

@_date: 2015-07-04 01:22:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
Yeah, I was really surprised when I found out today that bitcoinj
doesn't implement any of the soft-fork code. There's no excuse for not
doing that frankly. :(

@_date: 2015-07-04 01:44:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
Nodes can and do lie about what version they are all the time.
Fact is, SPV means you're trusting other people to check the rules for
you. In this particular case bitcoinj could have - and should have -
checked the BIP66 soft-fork rules, but in general there's no easy
solution to this problem.

@_date: 2015-07-13 12:04:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
You're missing something really critical about what F2Pool/AntPool were
(are?) doing: They're finding out about new blocks not by getting block
headers from just anywhere, but by connecting to other pools' via
stratum anonymously and determining what block hash they're telling the
hashers at the pool to work on. (e.g. what prevblockhash is in the block
header of shares being generated)
If other pools try to fake this information they're immediately and
directly losing money, because they're telling their own hashers to make
invalid blocks. This of course has a high chance of being detected, and
can easily be FUDed into "STOP MINING AT FOO POOL!" reardless of what
the ivory tower game theory might say. The only hope the pools have is
to somehow identify which connections correspond to other pools with
high reliability and target just those connections - good luck on that.
Anyway, all this concern about SPV mining is misguided: relying purely
on SPV w/ low  of confirmations just isn't very smart. What SPV can
do - at least while the inflation subsidy is still high - is give
reasonable protection against your third-party-run trusted full nodes
from lying to you, simply because doing so has well-defined costs in
terms of energy to create fake blocks. Targetting enough people at once
to make a fake block a worthwhile investment is difficult, particularly
when you take into account how timing works in the defenders favor - the
attacker probably only has a small % of hashing power, so they're going
to wait a long time to find their fake block. Between that and a trusted
third party-run full node you're probably reasonably safe, for now.

@_date: 2015-07-15 11:18:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
Actually, I was looking at what I believe was (part of?) this attack
yesterday in the logs on my full-RBF nodes and the txs involved *did*
have good fees and were highly relayable/minable - the double-spent txs
had near 100% propagation on blockchain.info (who has unfortunately
purged the relevant data already)
Shapeshift.io depends on Blockcypher's "confidence factor" model(1)
under the hood - yet another one of those sybil attacking network
monitoring things - to estimate tx confirmation probability by looking
at the % of nodes a tx has propagated too. But miners frequently use
customized Bitcoin Core codebases that don't follow normal policies, so
those measurements don't actually tell you what you need to know.
hapeshift confirmed(2) the attack - confirming that they disabled
unconfirmed tx acceptance - said they're going to "improve" their
system... It'll be interesting to see what that actually entails.
1) 2)

@_date: 2015-07-15 11:59:03
@_author: Peter Todd 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
For Blockcypher to succesfully do what they claim to do they need to
connect to a large % of nodes on the network; that right there is a
sybil attack. It's an approach that uses up connection slots for the
entire network and isn't scalable; if more than a few services were
doing that the Bitcoin network would become significantly less reliable,
at some point collapsing entirely.

@_date: 2015-07-16 04:32:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
"In a Sybil attack the attacker subverts the reputation system of a
peer-to-peer network by creating a large number of pseudonymous
identities, using them to gain a disproportionately large influence."
Quoting your API docs:
"[Blockcypher is] always connected to a statistically significant number
of nodes on the network - we target anywhere between 10 to 20% of the
active nodes on any given blockchain"
In the case of Bitcoin, there's something like 6,000 nodes, so if that
20% is achived via outgoing connections you'd have 600 to 1200 active
outgoing connections using up network resources.  Meanwhile, the default
is 8 outgoing connections - you're using about two orders of magnitude
more resources.
If you are achieving that via incoming connections, you're placing a big
part of the relay network under central control. As we've seen in the
case of Chainalysis's sybil attack, even unintentional confirguation
screwups can cause serious and widespread issues due to the large number
of nodes that can fail in one go. (note how Chainalysis's actions were
described(1) as a sybil attack by multiple Bitcoin devs, including
Gregory Maxwell, Wladimir van der Laan, and myself)
Right now the P2P network has relatively weak protections against sybil
attacks, but efforts are being made to find ways to defend against them.
As anti-sybil attack technology improves, you'll be able to
simultaneously connect to a smaller and smaller % of the network, and
your confidence factor technology will degrade further.
Questions: How exactly does your monitoring network work? Do you make
incoming, outgoing, or both types of connections? What subnet(s) do the
connections come from? What software makes those connections?
What you are doing is inherently incompatible with decentralization.
Your service simply doesn't scale; it's a server only a small number of
centralized entities can provide without causing the P2P network to
collapse due to resource exhaustion.
Question: Do you have relationships with mining pools? For instance, are
you looking at contracts to have transactions mined to guarantee
1)

@_date: 2015-07-17 07:59:20
@_author: Peter Todd 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
Then are you willing to tell us what IP addresses your nodes connect
from? This is important network stability information due to how nodes
prevent a lack of diversity in their outgoing connections.
The Bitcoin P2P network's primary concern is reliability through
diversity; you are harming that resource.
So to be clear, you have both a high level of outgoing and incoming
connections? Given that Bitcoin nodes only connect to eight outgoing
peers, how do you manage to connect to your claimed 10%-20% of all
reachable nodes? Obviously you can't be doing that with just incoming
connections, unless you're running hundreds of nodes, or doing an addr
spamming attack.
It's actually marginally better for the network if you're using hundreds
of distinct nodes rather than just a few to do this sybil attack - the
chance of your small number of nodes suddenly going off-line and causing
propagation issues is more than hundreds of nodes all going off-line
suddenly. Additionally it's easier for bad actors to survail your few
internet connections to easily get tx propagation information from the
network than it is to survail Chainalysis's setup. (ironic I know)
"Control attempts"? Care to explain?
Re: "gatekeeping" - fact is your business model and technology can only
be succesfully run by a small number of entities at once, resulting in a
situation where those few companies act as gatekeepers for access to
zeroconf confirmation probability information.
Nice cheap shot there. My "relationships" are nothing more than people
being willing to talk to me, ask me for advice, and warn me about
possible threats. They're not legal contracts.

@_date: 2015-07-18 11:09:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
Based on what I saw in my logs, the double-spends were mainly being done
by exploiting the fact that much of the hashing power has reverted your
10x relay fee drop as it makes wasting bandwidth and mempool RAM easy.
(so much so that crashing nodes with OOM's is fairly cheap)

@_date: 2015-07-19 03:52:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] Do we really need a mempool? (for relay nodes) 
As in, do relay nodes need to keep a record of the transactions they've
relayed? Strictly speaking, the answer is no: one a tx is relayed modulo
DoS concerns the entire thing can be discarded by the node. (unconfirmed
txs spending other unconfirmed txs can be handled by creating packages
of transactions, evaluated as a whole)
To mitigate DoS concerns, we of course have to have some per-UTXO limit
on bandwidth relayed, but that task can be accomplished by simply
maintaining some kind of per-UTXO record of bandwidth used. For instance
if the weighted fee and fee/KB were recorded, and forced to - say -
double for each additional tx relayed that spent a given UTXO you would
have a clear and simple upper limit of lifetime bandwidth. Equally it's
easy to limit bandwidth moment to moment by asking peers for highest
fee/KB transactions they advertise first, stopping when our bandwidth
limit is reached.
You probably could even remove IsStandard() pretty much entirely with
the right increasingly expensive "replacement" policy, relying on it
alone to provide anti-DoS. Obviously this would simplify some of the
debates around mining policy! This could even be re-used for scalable a
general-purpose messaging network paid by coin ownership if the UTXO set
is split up, and some kind of expiration over time policy is
Miners of course would still want to have a mempool, but that codebase
may prove simpler if it doesn't have to work double-duty for relaying as

@_date: 2015-07-21 09:04:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
The Bitcoin protocol fundementally uses time in a consensus-critical
manner anyway; miners vote on what time it is via the median time
Triggering events based on median time is compatable with consensus and
gives more human scale predictability as to when events may happen. In
addition the median time is guaranteed to be monotonic by the consensus
See the version bits proposal for an example of its use:
Having said that, in general triggering events without verifying a
supermajority of miner support can be very dangerous. Without miner
support the chain is insecure, and can be attacked. For instance a
blocksize limit increase that a majority of miners choose not to
implement raises huge risks of reorg for any miners who attempt to
create large blocks, and huge risks of payment reversal for any
merchants accepting transactions in such blocks. Note how with BIP102,
extending the original Bitcoin chain is inherently an attack on the
Garzik chain.
For that reason I think BIP102 is extremely poorly designed. I can only
conclude that Jeff Garzik is either deliberately trolling us and/or
manipulating discussion with a badly designed proposal that he doesn't
actually expect to be adopted verbatim, or is incompetent.

@_date: 2015-07-21 09:58:46
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
Expanding on that a bit:
I don't agree with you at all.
This is a case where if Jeff doesn't understand that issue, he's
proposing changes that he's not competent enough to understand, and it'd
save us a lot of review effort if he left that discussion. Equally, Jeff
is in a position in the dev community where he should be that competent;
if he actually isn't it does a lot of good for the broader community to
change that opinion.
I personally *don't* think he's doing that, rather I believe he knows
full well it's a bad patch and is proposing it because he wants to push
discussion towards a solution. Often trolling the a audience with bad
patches is an effective way to motivate people to respond by writing
better ones; Jeff has told me he often does exactly that.
I think in this case we shouldn't do anything, so short-circuiting that
process by pointing out what he's doing publicly makes sense.
Re: BIP  we explicitly have a policy of allocating them for stupid
ideas, to avoid having to be gatekeepers. Ironically that makes it
harder to get a BIP # if you know what you're doing, because Gregory
Maxwell will argue against you in private and delay actually allocating
one if he knows you should know better. :)

@_date: 2015-07-22 22:30:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Sorry, but I think you need to re-read my first message. What you've written below has nothing to do with what I actually said re: how you're BIP102 and associated pull-req doesn't measure miner consensus.

@_date: 2015-07-23 16:05:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Node Speed Test 
Hash: SHA256
Note how due to bandwidth being generally asymetric your findings are probably optimistic - you've measured download capacity. On top of that upload is further reduced by the fact that multiple peers at once need to be sent blocks for reliability.
Secondly you're measuring a network that isn't under attack - we need significant additional margin to resist attack as performance is consensus-critical.

@_date: 2015-07-23 19:29:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] Process for BIP number allocation 
Hash: SHA256
To be clear, where is an implementation of your proposed BIP?
The philosophy of the process - particularly for non-consensus BIPs - is running code, preferably in production. An actual number for the standard that code implements isn't a barrier to that process.
Remember that it's convenient for all if the number of BIPs out there isn't significantly higher than the number of actual standards in place that are being used.

@_date: 2015-07-24 00:53:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
As mining is a random, poisson process, obviously giving guarantees without a majority of hashing power isn't possible.

@_date: 2015-07-23 22:26:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] Process for BIP number allocation 
Now get people to actually use this and get back to us.
Remember that once there's actual adoption, BIP  will and have been
assigned even for standards that are dangerous and badly designed; the
barrier for non-consensus-stuff is the desire to avoid assigning a bunch
of numbers for things that never get used.

@_date: 2015-07-24 13:40:39
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
FWIW, blockchain.info is obviously *not* running a full node as their
wallet was accepting invalid confirmations on transactions caused by the
recent BIP66 related fork; blockchain.info has $30m in funding.
Coinbase also was not running a full node not all that long ago, instead
running a custom Ruby implementation that caused their service to go
down whenever it forked. (and would have also accepted invalid
confirmations) I believe right now they're running that implementation
behind a full node however.
Actually I've been trying to get the CCSS standard to cover full nodes,
and have been getting push-back:
tl;dr: Running a full node is *not* required by the standard right now
at any certification level.
This is of course completely ridiculous... But I haven't had much much
time to put into getting that changed so maybe we just need some better
explanations to the others maintaining the standard. That said, if the
standard stays that way, obviously I'm going to have to ask to have my
name taken off it.
I would point out that lack of understanding of how Bitcoin works, as
well as a lack of understanding of security engineering in general, is
probably a significant contributor to these problems. Furthermore
Bitcoin and cryptocurrencies in general are still small enough that many
forseeable low probability but high impact events haven't happened,
making it difficult to explain to non-technical stakeholders why they
should be listening to experts rather than charlatans and fools.
After a few major centralization related failures have occured, we'll
have an easier job here. Unfortunately there's also a good chance we
only get one shot at this due to how easy it is to kill PoW systems at

@_date: 2015-07-24 13:43:20
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
We can test the fact that blockchain.info's wallet and block explorer
has behaved in a way consistent with not running a full node - they have
shown invalid data that any full node would reject on multiple
occasions, most recently invalid confirmations during the BIP66 fork.

@_date: 2015-07-27 08:08:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
It's worth reminding people that Bitcoin Core, Bitcoin XT, my own
Bitcoin RBF, Luke-Jr's Bitcoin distribution, etc. are all software
packages that implement the Bitcoin protocol. Like many protocols,
changing the Bitcoin protocol isn't easy, and requires a broad consensus
among many players for any change to proceed smoothly. Conversely,
changing non-protocol aspects of any of those software packages is easy,
and requires little to no coordination.
Of course, in practice the Bitcoin Core dev team does have a lot of
influence, to the point where soft-forks proposed by them are adopted
pretty much blindly by most users. This is essentially a meta-consensus:
the community is assuming what the Bitcoin Core team releases will be a
good idea to run as well as non-controversial without necessarily
investigating too closely. The Core dev team has a strong track record
of making good decisions with very few mistakes, while still adding new
features, fixing security bugs, and improving performance significantly.
That leads to a fairly strong meta-consensus of "Just run Bitcoin Core"
Of course, if the Core team was taking changes and making controversial
changes, I suspect that meta-consensus would quickly break down! So it's
not as strong as it looks - the Core team doesn't really have the
ability to push through controversial changes, and the Core team acts
If you don't agree with that "meta-consensus", running an alternative
Bitcoin protocol implementation such as Bitcoin XT is a logical way of
showing your support for a different way of coming to consensus on
protocol changes. It's not totally clear yet what that way actually is,
but it's certainly shaping up to have a lot less emphasis on broad
consensus among the technical community. (of course, the XT team to date
has much less experience with the Bitcoin protocol and codebase than the
combined Core team does)
The ugly thing is I think everyone in this process recognises the
meta-consensus nature of the debate already. Notice how Gavin Andresen's
initial blocksize posts were in the form of a non-technical blog, making
non-technical arguments to the public - not the Core dev team - in ways
not conducive to open response.  A rather annoying example is Jeff
Garzik's recent efforts: a fundementally broken troll pull-req raising
the blocksize to 2MB that simply can't be merged for reasons unrelated
to the blocksize, followed by very public and loud efforts to spin a
non-issue - closing a pull-req that had no real impact on blockchain
capacity - into a broader reddit furor over a "changed" policy on
scaling. As a PR effort to the public this was fairly effective: framing
the Core dev team's actions as a change and raising the blocksize as a
default action puts the team on the defensive. As a way of building
consensus among the Core dev team, Garzik's actions are very
I personally have a fairly high tolerance to trolling, but I wouldn't be
surprised if other devs start getting tired of this stuff and just leave
Bitcoin development to focus on more productive stuff. To many it's
discouraging when the other side gets to "promise ponies" - we've got a
fundamentally uphill PR battle in arguing for the development of
scalability tech.
For the long term, I think it'd be useful for research to be done on how
to better manage these social issues. I suspect a lot of the problem -
at least for non-scalable blockchain designs - stems from how
centralization failures aren't gradual, and the ease of relying on trust
rather than verification. While we get a lot of warning of issues, the
warning isn't directly associated with losses at first, making the
problem hard to explain to the general public.
You know, those promoting the idea of a "one-time-only" blocksize
increase would do well to get the stakeholders affected to publicly
explain what exactly are their plans with regard to scalability in the
long run. If they don't have any, then it's a strong sign that said
stakeholders don't actually intend to have a "one-time-only" blocksize
increase. Remember that there's no guarantee that the technology
limiting the blocksize will improve as fast as desired, or even for that
matter, improve at all. (bandwidth is limited by politics far more than
it is limited by technology)
There's strong parallels to zeroconf safety, where as far as I can tell
the relevant stakeholders - pretty much all large payment providers -
have no plans at all to move to genuine decentralized zeroconf
technology. Rather they have backup plans to get into dangerous and
centralizing mining contracts if zeroconf security gets any worse,
something Coinbase even publicly admitted on this list.

@_date: 2015-06-01 13:26:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Fwd: Block Size Increase Requirements 
It's important to remember that the service Bitcoin miners are providing
us is *not* transaction validation, but rather decentralization.
Validation is something every full node does already; there's no
shortage of it. What's tricky is designing a Bitcoin protocol that
creates the appropriate incentives for mining to remain decentralized,
so we get good value for the large amount of money being sent to miners.
I've often likened this task to building a robot to go to the grocery
store to buy milk for you. If that robot doesn't have a nose, before
long store owners are going to realise it can't tell the difference
between unspoilt and spoilt milk, and you're going to get ripped off
paying for a bunch of spoiled milk.
Designing a Bitcoin protocol where we expect "competition" to result in
smaller miners in more geographically decentralized places to get
outcompeted by larger miners who are more geographically centralized
gets us bad value for our money. Sure it's "self-correcting", but not in
a way that we want.
Note how that VPN, and likely VPS it's connected too, immediately adds
another one or two points of failure to the whole system. Not only does
this decrease reliability, it also decreases security by making attacks
significantly easier - VPS security is orders of magnitude worse than
the security of physical hardware.

@_date: 2015-06-06 11:32:47
@_author: Peter Todd 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
Just set nLockTime to 500000000-1 and nSequence appropriately to make
the transaction impossible to mine for the next 9500 years.
Though I agree that this whole idea seems a bit dubious to me.

@_date: 2015-06-06 22:35:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] Lexicographical Indexing of Transaction 
In general I think this is a good idea, and should be implemented; we've
had a depressing number of wallets fail to implement randomization
properly, if at all.
Why mention SIGHASH_SINGLE at all? Its use-case is highly specialized
protocols; you haven't taken into account the needs of those protocols.
For BIP's it's better to stick to the use-cases where the need is clear
and there exists running code that to speculate too much on future uses.
With signature hashing in particular it's not yet clear at all what
future OP_CHECKSIG's will look like, let alone the various ways people
will use sighash for smart contract type stuff.
You'd be better off presenting the BIP in terms of a generic statement
that "except when otherwise prevented by advanced signature hashing
requirements, wallet software must emit transactions sorted according to
the following" You can then specify the two common cases in detail:
1) SIGHASH_ALL: input and output order signed, so sort appropriately
2) SIGHASH_ANYONECANPAY: input order not signed, so software should emit
   transactions sorted, recognising that the actual mined order may be
   changed.
As for IsStandard() rules - let alone soft forks - better to leave
discussion of them out for now. In particular, for the soft-fork case
mandating certain transaction orders will very likely cause problems in
the future for future OP_CHECKSIG upgrades. For SIGHASH_ANYONECANPAY, it
might be appropriate for nodes to enforce a certain ordering, but that
can be a separate BIP. (actually implementing that in Bitcoin Core would
be annoying and ugly right now; without replace-by-fee ANYONECANPAY has
a silly DoS attack (adding low-fee inputs) so I can't recommend wallets
use it in the general case yet)
"and a sequence number currently set to 0xFFFFFFFF." <- Actually, this
will be changed in Bitcoin Core as of v0.11.0, which implements
anti-fee-sniping w/ nLockTime.(1) (I need to write up a full BIP
describing it)
Do you have a patch implementing deterministic tx ordering for Bitcoin
Core yet?
1)

@_date: 2015-06-08 17:33:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
Actually they don't, which is an unfortunate problem with the existing
mempool implementation; the only way a transaction can be removed from a
Bitcoin Core mempool is through it getting mined, double-spent, or the
node restarting.
The protection that we have against that attack is that you need access
to a lot of bitcoins to pay enough fees. With the 0.01mBTC/KB minimum
relay fee and $230 USD/BTC that works out to about $2.3kUSD/GB of ram
consumed, and furthermore, actually getting that many transactions to
propagate over the network is non-trivial. (no, I'm not going to tell
you how)
The obvious solution is to cap the size of the mempool and evict
transactions lowest fee/KB first, but if you do that they you (further)
break zeroconf security. On the other hand, if you don't break zeroconf
security an attacker can prevent reasonable fee transactions from
I probably should get around to fixing this...

@_date: 2015-06-08 17:36:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Timestamping is another case where order matters: if you put the digest
in the last vout you can use SHA256 midstate's to reduce the size of the
timestamp proof.
Anyway, there's no reason to rush re: changes to IsStandard()

@_date: 2015-06-08 17:44:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
There will always be a blocksize limit based on technological
considerations - the network has a finite bandwidth limit.
Without a blocksize limit the attacker would just flood the network
until the bandwidth usage became so great that consensus would fail,
rendering Bitcoin both worthless, and insecure.
The worst an attacker flooding the network with transactions with a
blocksize limit can do is raise costs, without harming security. Keep in
mind, that at some point it'd be cheaper to just 51% attack the network.
Based on the current block subsidy of 25BTC/MB that's at the point where
transaction fees are 25mBTC/KB, which corresponds to <$2/tx fees - not
that cheap, but still quite affordable for a large percentage of
Bitcoin's users right now. And that's the *absolute worst-case* attack

@_date: 2015-06-08 18:18:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
Bitcoin is a global consensus system - if you're bandwidth isn't
sufficient to keep up you are not part of the consensus.
The blocksize limit *is* what determines the minimum bandwidth required
to stay in consensus.
Again, in your scenario if the bandwidth consumed by those transactions
was sufficiently high, the network would collapse because consensus
would fail.
Why wouldn't that bandwidth be high enough to cause that collapse?
Because of the blocksize limit! (combined with an intelligent mempool
that increases the minimum fee/KB appropriately - we don't have that
I already did the math for you on that: the maximum transaction fee
you'd see in that kind of attack is around $2.5 USD/tx. That definitely
is not high enough to make Bitcoin non-viable - I personally could
easily afford fees like that for about 90% of my transactions this year
by value, as I mainly use Bitcoin to get paid by my clients around the
world. In fact, just today O'Reilly paid $15 USD to send me a wire
transfer for expenses related to a conference I was invited too.
A much more realistic transaction flood scenario - one that didn't raise
serious questions about whether or not the attacker could afford to 51%
attack Bitcoin - would raise tx fees to something more like $0.25/tx

@_date: 2015-06-08 18:26:22
@_author: Peter Todd 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
Mike Hearn reduced the minimum relay fee to 0.01mBTC/KB:

@_date: 2015-06-08 18:28:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
You know, you can think of Bitcoin as a system that maintains a ledger
for transferrable hashcash... Which means transaction fees *are* paid in

@_date: 2015-06-09 16:14:36
@_author: Peter Todd 
@_subject: [Bitcoin-development] Lexicographical Indexing of Transaction 
Two other things:
I'd keep it even simpler than that, and just say for now that such
use-cases are out of the scope of this BIP, however those standards
should come up with some kind of deterministic standard that meets the
needs of the protocol. Again, there's a bunch of possible use-cases here
and we just can't predict them; focus on the fact that the *spirit* of
what this BIP is about is applicable and future standards should be
So I'd change the "Applicability" section to:
This BIP applies to all transactions where the order of inputs and
outputs does not matter. This is true for the vast majority of
transactions as they simply move funds from one place to another.
Currently this generally refers to transactions where SIGHASH_ALL is
used, in which case the signatures commit to the exact order of input
and outputs. In the case where SIGHASH_ANYONECANPAY and/or SIGHASH_NONE
has been used (e.g. crowdfunds) the order of inputs and/or outputs may
not be signed, however compliant software should still emit transactions
with sorted inputs and outputs, even though they may later be modified
by others.
In the event that future protocol upgrades introduce new signature hash
types, compliant software should apply the lexographic ordering
principle analogously.
While out of scope of this BIP, protocols that do require a specified
order of inputs/outputs (e.g. due to use of SIGHASH_SINGLE) should
consider the goals of this BIP and how best to adapt them to the
specifics needs of those protocols.
Then remove the "handling input/output deps" section.
re: the actual ordering algorithm, having txids be sorted by with the
hex-based algorithm is odd. I'd simply say they're sorted as
little-endian byte arrays, or in other words, with the bytearr_cmp()
function, but with the order of bytes reversed. You also should say that
we're doing that to make the user see them in visually sorted order to
match expectations because txids are displayed as little-endian.
For outputs, don't say "locking script", say "scriptPubKey". Secondly,
scriptPubKeys are not in little-endian representation - they have no
endianness to them. With output amount, there's no need to say that
they're unsigned or little-endian satoshies, just say they're sorted
largest/smallest amount first.
"For the sake of efficiency, amounts will be considered first for
sorting, since they contain fewer bytes of information (7 bytes)
compared to a standard P2PKH locking script (800 bytes)." <- where the
heck did you get these numbers from? Amounts are 8 bytes, and P2PKH
scriptPubKeys are 25 bytes.
"Backwards Compatibility" <- I'd just remove this whole section; we're
unlikely to make this an IsStandard() rule anytime soon.

@_date: 2015-06-10 05:10:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee patch against 
First-seen-safe Replace-by-Fee is now available as a patch against
    I've also had a pull-req against git HEAD open for a few weeks now:
    I've got some hashing power interested in running this patch in the near
future, so I'm offering a bounty of up to 1 BTC to anyone who can find a
way to attack miners running this patch. Specifically, I'm concerned
about things that would lead to significant losses for those miners. A
total crash would be considered very serious - 1 BTC - while excess
bandwidth usage would be considered minor - more like 0.1 BTC. (remember
that this would have to be bandwidth significantly in excess of existing
For reference, here's an example of a crash exploit found by Suhas
Daftuar: If two people report the same or overlapping issues, first person will
get priority. Adding a new test that demos your exploit to the unit
tests will be looked upon favorably. That said, in general I'm not going
to make any hard promises with regards to payouts and will be using my
best judgement. I've got a bit over 2BTC budgetted for this, which is
coming out of my own pockets - I'm not rich! All applicants are however
welcome to troll me on reddit if you think I'm being unfair.
Suhas: speaking of, feel free to email me a Bitcoin address! :)

@_date: 2015-06-10 15:03:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] Is SourceForge still trustworthy enough 
Please keep it that way; HTML messages have no place on a technical
mailing list.
What type of digital signatures specifically? What email client?

@_date: 2015-06-10 15:20:04
@_author: Peter Todd 
@_subject: [Bitcoin-development] Is SourceForge still trustworthy enough 
It might be that Thunderbird doesn't properly handle messages with both
signed and unsigned content. I use mutt myself, which handles it just
fine. (the sigs on your emails verify just fine for instance)

@_date: 2015-06-10 15:43:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Is SourceForge still trustworthy enough 
It has perfectly valid signatures, as do your earlier messages to the
PGP/MIME definitely does support partially signed content.

@_date: 2015-06-10 16:03:23
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: SPV Fee Discovery mechanism 
Fee stats can always be fabricated by individual miners because fees can
be paid out-of-band.

@_date: 2015-06-11 09:10:48
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposal: SPV Fee Discovery mechanism 
Hence the need for ways to increase fees on transactions after initial
broadcast like replace-by-fee and child-pays-for-parent.
Re: "dropped in an unpredictable way" - transactions would be dropped
lowest fee/KB first, a completely predictable way.

@_date: 2015-06-11 20:40:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Miners: You'll (very likely) need to upgrade 
The BIP66 soft-fork recently passed the 75% support threshold. This
means that 75% of the hashing power has upgraded to support BIP66; 25%
of the hashing power has not. Once 95% of the hashing power has
upgraded, blocks created by the 5% who have not upgraded will be
If you operate a pool, solo-mine, or mine on p2pool you'll very likely
need to upgrade your Bitcoin Core node to support the BIP66 soft-fork,
or your blocks will be rejected. If you only sell your hashing power to
a centralized pool you do not need to do anything.
How does the Bitcoin protocol measure BIP66 support?
BIP66 - "Strict DER signatures" - is a soft-fork that tightens the rules
for signature verification, specifically the way that signatures are
encoded. The Bitcoin Core implementation currently relies on OpenSSL for
signature validation, which means it implicitly defines Bitcoin's block
validity rules. Unfortunately, OpenSSL is not designed for
consensus-critical behaviour (it does not guarantee bug-for-bug
compatibility between versions), and thus changes to it can - and have -
affected Bitcoin software. (see CVE-2014-8275)
By tightening these rules BIP66 reduces the risk that changes to OpenSSL
will cause forks in the Bitcoin blockchain, as seen previously by the
March 2013 fork. Secondly reducing our dependency on OpenSSL is a step
towards replacing OpenSSL with libsecp256k1, a signature validation
library from Pieter Wuille and Gregory Maxwell, that is designed for
consensus-critical applications, as well as being significantly faster
than OpenSSL.
Is it possible that the BIP66 soft-fork will not happen?
In theory yes, though it is unlikely and rejection of BIP66 would be a
very ugly process. Unfortunately the existing soft-fork mechanism
provides no mechanism for a soft-fork to expire, so once set in motion
there is no clean way to stop a soft-fork.
There is a proposal from Wuille/Maxwell/Todd, to reform how soft-forks
are adopted that aims to fix this issue, as well as allow multiple
soft-forks be adopted in parallel:
 at lists.sourceforge.net/msg07863.html

@_date: 2015-06-12 14:01:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Mining centralization pressure from 
To be clear, when you say 8% of their income, you mean revenue, not
Actual profit margins of something like 5%-10% are likely, so that's an
enormous hit that could make their mining operation completely

@_date: 2015-06-12 14:11:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Jeff Garzik recently proposed that the upper blocksize limit be removed
entirely, with a "soft" limit being enforced via miner vote, recorded by
hashing power.
This mechanism within the protocol for users to have any influence over
the miner vote. We can add that back by providing a way for transactions
themselves to set a flag determining whether or not they can be included
in a block casting a specific vote.
We can simplify Garzik's vote to say that one of the nVersion bits
either votes for the blocksize to be increased, or decreased, by some
fixed ratio (e.g 2x or 1/2x) the next interval. Then we can use a
nVersion bit in transactions themselves, also voting for an increase or
decrease. Transactions may only be included in blocks with an
indentical vote, thus providing miners with a monetary incentive via
fees to vote according to user wishes.
Of course, to cast a "don't care" vote we can either define an
additional bit, or sign the transaction with both versions. Equally we
can even have different versions with different fees, broadcast via a
mechanism such as replace-by-fee.
See also John Dillon's proposal for proof-of-stake blocksize voting:
 at lists.sourceforge.net/msg02323.html

@_date: 2015-06-12 19:30:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] Mining centralization pressure from 
Then simulate first the relay network assuming 100% of txs use it, and
secondly, assuming 100%-x use it.
For instance, is it in miners' advantage in some cases to sabotage the
relay network? The analyse say yes, so lets simulate that. Equally even
the relay network isn't instant.

@_date: 2015-06-12 19:34:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Sounds like a good encoding to me. Taking the median of the three
options, and throwing away "don't care" votes entirely, makes sense.
Thanks! I personally expect disaster to ensue with this kind of
proposal, but I'm less concerned if the disaster is something users
explicitly allowed to happen in a consensual way.

@_date: 2015-06-12 19:36:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Exactly. I very explicitly am proposing that we consider giving users a
mechanism to pay for votes to give them a way to directly influence the

@_date: 2015-06-12 19:44:51
@_author: Peter Todd 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Median ensures that voting "no change" is meaningful. If "double" + "no
change" = 66%-1, you'd expect the result to be "no change", not "halve""
With a plurality vote you'd end up with a halving that was supported by
a minority.

@_date: 2015-06-12 19:47:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Read John Dillon's proposal then, which via proof-of-stake explicitly
approportions control of increases via % of Bitcoin owned.
Anyway, representing everyone is never going to be easy, but at least
this nVersion thing is very easy to implement.

@_date: 2015-06-15 05:11:49
@_author: Peter Todd 
@_subject: [Bitcoin-development] comments on BIP 100 
...and can there be? The goal of validation after all is finding if a
mistake has been made, and current production cryptography doesn't have
any way to prove you have done that honestly. You need "moon math" like
recursive SNARKS to do that, and it's unknown when they'll be available
for production usage.
When we say "compensating validators", if we're being honest with
outselves what we really mean is the much more boring task of
compensating servers who are giving us blockchain data. That has nothing
to do with validation.
A useful task would be to make an SPV archival node implementation that
did no validation at all, while distributing the blockchain data linked
to the longest chain. Such an implementation can and should serve SPV
clients, as this is what their actual security model usually is given
the lack of authentication of the identity of the server they're
connecting too. Actually implementing this would be a simple matter of
patching Bitcoin Core to turn off block validation.
Concretely, 20MB blocks lead to 20GB/week of blocks. On my 1MB/second
down internet, turning on my node after a week away would take five
hours; starting up a new node after two years of 20MB blocks would take
23 days - likely longer in practice.
There's serious unsolved and undiscussed devops and development issues
with this. For instance, after changes to the validation code, it's
routine to resync/reindex Bitcoin Core to ensure starting up a new node
actually works. Even now we haven't really come to grips with what
consistent 1MB blocks looks like from this point of view after a few
years of usage, let another order of magnitude longer sync times.
Note how with 20MB blocks it would take up to 1TB of IO per year-synced
for a bloom-filter-using wallet to sync the blockchain. We already have
a bloom IO DoS attack issue - what are the consequences of making that
issue 20x worse? Nobody has analysed it yet.

@_date: 2015-06-15 05:43:42
@_author: Peter Todd 
@_subject: [Bitcoin-development] comments on BIP 100 
StrawPay hasn't published any details of their work publicly; if they
wanted credit on the mailing list they should have done that.
I couldn't even find any screenshots of that GUI wallet when I learned
what they were doing; I went to the trouble of reaching out to them
recently because I have multiple clients with a need for their
technology. I'm sure we all would have appreciated and welcomed them
taking the time to let us know what they were doing; it would have saved
me personally a lot of time; their lack of recognition on this mailing
list is both unfortunate, and a product of their actions alone.
In any case, StrawPay and Lightning are complementary projects: StrawPay
has limited functionality in exchange for faster deployment; Lightning
has significantly more functionality in exchange for a longer deployment
schedule. Both projects can and should be developed in parallel.
Equally, note efforts like my own CHECKLOCKTIMEVERIFY, which will be
part of StrawPay in due time.
Note for instance how we're discussing what standards we need in the
CryptoCurrency Security Standard for requirements for compliant
companies to run full nodes for transaction verification; failure to run
a full node will be considered non-compliant in much the same way that
failure to secure your private keys is non-compliance. Pedantically, if
you assume a diverse, decentralized ecosystem, these security standards
by themselves do create fixed linear relationships between those
variables, giving O(n^2) scaling.
Equally, not running full nodes bears little resemblance to the Bitcoin
we use today. Either way, something must change for the number of
Bitcoin users to grow.
I'm genuinely looking forward to a concrete fork proposal. Any ETA on
when the blocksize increase code will go in Bitcoin XT?

@_date: 2015-06-16 14:33:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
FWIW there Constance Choi and Primavera De Filippi (CC'd) are holding a
blockchain-tech conference October 14th-15th in Hong Kong as well;
coordinating your summit with that conference could be useful.
This workshop series has been attracting audiences of people looking to
use blockchain tech in general; many of these use-cases will likely
involve the Bitcoin blockchain in unpredictable ways. Importantly, these
ways can drive demand significantly beyond our current assumptions based
on most demand being consumer-merchant transactions.
In addition, many of the attendees have significant experience with
regulatory issues and interacting with governments on regulation of
blockchain tech. Bitcoin faces existential risks to its existence by
these regulation efforts, which include things like efforts to setup
industry wide Anti-Money-Laundering/Know-Your-Customer programs,
including programs that would tie on-chain transactions to identity
information. Any blocksize discussion needs to be informed by these
potential threats to usage of the technology, as well as challenges to
using scaling solutions.
Agreed. Pieter Wuille's recent work is a great example of the kind of
science-driven investigations that need to be done - and haven't been
done very much - to get us some hard data to make decisions on.

@_date: 2015-06-16 18:46:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reusable payment codes 
Sorry, but I'm looking at the huge amount of work that I'll likely have
responding to the blocksize issue, so I think I'm inclined to shelve
work on BIP63 for now.
Feel free to take it up; a (>=2)-part standard describing the resuable
codes aspect, and separately how the ephemeral key is transmitted to the
recipient makes sense to me.

@_date: 2015-06-16 19:17:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
To be clear, it depends on what kind of sidechain.
My off-chain transaction notions are federated sidechains with an
economic incentive to not commit fraud using fidelity bonds; they were
definitely proposed as a scaling solution.
Merge-mined sidechains are not a scaling solution any more than SPV is a
scaling solution because they don't solve the scaling problem for
Some kind of treechain like sidechain / subchains where what part of the
tree miners can mine is constrained to preserve fairness could be both a
scaling solution, and decentralized, but no-one has come up with a solid
design yet that's ready for production. (my treechains don't qualify for
transactions yet; maybe for other proof-of-publication uses)
Keep in mind that other than preserving mining
decentralization/resisting censorship, we've known how to scale up
blockchains for ages w/ things like (U)TXO commitments and fraud proofs.

@_date: 2015-06-17 04:54:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
BTW it's worth working out how many $ in fees you need for a given
amount of MB worth of mempool.
At the current 10uBTC/KB minimim relay fee 1MB of txs requires just $2.5
worth of fees - kinda ridiculous when a block earns a miner $6250 in
revenue. Pretty much all txs pay significantly higher rates - more like
100uBTC/KB, or $25/MB. At that rate the 288MB max mempool size proposed
by Patrick Strateman's pull-req requires at least $7.2k worth of BTC to
fill to pay the fees, and in practice will probably quickly get higher.
See above - filling the mempool like that will be both a slow process,
and require lots of funds. Equally, once full, the sensible thing to do
is raise the minimum relay fee appropriately, so those transactions that
pay too low a fee will simply be rejected.
It'd be reasonable to tell peers that, and what the minimum fee needed
for acceptance would be for that particular node.
For an interactive, mobile wallet, the best thing to do is estimate the
fee correctly the first time, using RBF as a follow up mechanism only if
needed. For other users - e.g. exchanges handling customer withdrawals -
using RBF more agressively to get the minimum possible fee may make
In any case, the *existance* of RBF makes no difference to any of these
problems; RBF does make solving the easier. You can always choose to not
use it after all, resulting in the same "send-and-forget" process.
Having it available allows mistakes to be fixed after the fact, always
an improved user experience over not being able to re-bid for block
Incidentally, if my FSS-RBF bug bounty isn't collected in the next week
or two, we'll likely have a major double-digits % of hashing power
mining FSS-RBF soon after.
 at lists.sourceforge.net/msg08122.html
Have you looked at the fee estimation code in Bitcoin Core? I have no
reason to think it doesn't basically speaking work. Of course, SPV
wallets will need a semi-trusted third party to securely get that data,
but this seems to be a fundemental problem in a decentralized network -
the purpose of the blockchain itself is to prove that some data was
published to some audience, an analogous problem to proving to the SPV
wallet that their transaction actually reached miners and they actually
are considering it for inclusion.
Guaranteed reliable transaction processing is only possible in
centralized environments that can make service guarantees.
Few if any of those mechanisms can be deployed in a consensus-critical
way that is resistant to attack; the blocksize limit is needed to -
among other things - resist attacks by one miner on another to reduce
the competitors profitability. Without an explicit limit tx selection
and propagation rule changes can be gamed.

@_date: 2015-06-17 04:59:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
Great! They're excited to see this happen. I'm in London right now
actually for the conference they were holding this week; the blocksize
issue was being discussed a fair bit there among attendees. (notably,
with rather different views than seen on reddit!)
Yup, though keep in mind the regulatory question is more than just how
your local jurisdiction views Bitcoin, but rather how your customers'
jurisdictions view Bitcoin.
Of course, when I say "customers" above, I mean the entire Bitcoin
community that is ultimately buying the new coins produced by miners and
paying fees to them!

@_date: 2015-06-19 06:39:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Yesterday F2Pool, currently the largest pool with 21% of the hashing
power, enabled full replace-by-fee (RBF) support after discussions with
me. This means that transactions that F2Pool has will be replaced if a
conflicting transaction pays a higher fee. There are no requirements for
the replacement transaction to pay addresses that were paid by the
previous transaction.
I'm a user. What does this mean for me?
Whether full or first-seen-safe? RBF support (along with
child-pays-for-parent) is an important step towards a fully functioning
transaction fee market that doesn't lead to users' transactions getting
mysteriously "stuck", particularly during network flooding
events/attacks. A better functioning fee market will help reduce
pressure to increase the blocksize, particularly from the users creating
the most valuable transactions.
Full RBF also helps make use of the limited blockchain space more
efficiently, with up to 90%+ transaction size savings possible in some
transaction patterns. (e.g. long payment chains?) More users in less
blockchain space will lead to higher overall fees per block.
Finally as we'll discuss below full RBF prevents a number of serious
threats to the existing level playing field that miners operate in.
Why can't we make accepting unconfirmed txs from untrusted people safe?

@_date: 2015-06-19 09:44:08
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Specifically the following is what I told them:
Basically both let you replace one transaction with another that pays a
higher fee. First-seen-safe replace-by-fee adds the additional criteria
that all outputs of the old transaction still need to be paid by the new
transaction, with >= as many Bitcoins. Basically, it makes sure that if
someone was paid by tx1, then tx2 will still pay them.
I've written about how wallets can use RBF and FSS-RBF to more
efficiently use the blockchain on the bitcoin-development mailing list:
 at lists.sourceforge.net/msg07813.html
 at lists.sourceforge.net/msg07829.html
Basically, for the purpose of increasing fees, RBF is something like %50
cheaper than CPFP, and FSS-RBF is something like %25 cheaper.
In addition, for ease of implementation, my new FSS-RBF has a number of
other restrictions. For instance, you can't replace multiple
transactions with one, you can't replace a transaction whose outputs
have already been spent, you can't replace a transaction with one that
spends additional unconfirmed inputs, etc. These restrictions aren't
"set in stone", but they do make the code simpler and less likely to
have bugs.
In comparison my previous standard RBF patch can replace multiple
transactions with one, can replace long chains of transactions, etc.
It's willing to do more computation before deciding if a transaction
should be replaced, with more complex logic; it probably has a higher
chance of having a bug or DoS attack.
You've probably seen the huge controversy around zeroconf with regard to
standard replace-by-fee. While FSS RBF doesn't make zeroconf any safer,
it also doesn't make it any more dangerous, so politically with regard
to zeroconf it makes no difference. You *can* still use it doublespend
by taking advantage of how different transactions are accepted
differently, but that's true of *every* change we've ever made to
Bitcoin Core - by upgrading to v0.10 from v0.9 you've also "broken"
zeroconf in the same way.
Having said that... honestly, zeroconf is pretty broken already. Only
with pretty heroic measures like connecting to a significant fraction of
the Bitcoin network at once, as well as connecting to getblocktemplate
supporting miners to figure out what transactions are being mined, are
services having any hope of avoiding getting ripped off. For the average
user their wallets do a terrible job of showing whether or not an
unconfirmed transaction will go through. For example, Schildbach's
Bitcoin wallet for Android has no code at all to detect double-spends
until they get mined, and I've been able to trick it into showing
completely invalid transactions. In fact, currently Bitcoin XT will
relay invalid transactions that are doublepsends, and Schildbach's
wallet displays them as valid, unconfirmed, payments. It's really no
surprise to me that nearly no-one in the Bitcoin ecosystem accepts
unconfirmed transactions without some kind of protection that doesn't
rely on first-seen-safe mempool behavior. For instance, many ATM's these
days know who their customers are due to AML requirements, so while you
can deposit Bitcoins and get your funds instantly, the protection for
the ATM operator is that they can go to the police if you rip them off;
I've spoken to ATM operators who didn't do this who've lost hundreds or
even thousands of dollars before giving up on zeroconf.
My big worry with zeroconf is a service like Coinbase or Shapeshift
coming to rely on it, and then attempting to secure it by gaining
control of a majority of hashing power. For instance, if Coinbase had
contracts with 80% of the Bitcoin hashing power to guarantee their
transactions would get mined, but 20% of the hashing power didn't sign
up, then the only way to guarantee their transactions could be for the
80% to not build on blocks containing doublespends by the 20%. There's
no way in a decentralized network to come to consensus about what
transactions are or are not valid without mining itself, so you could
end up in a situation where unless you're part of one of the big pools
you can't reliably mine at all because your blocks may get rejected for
containing doublespends.
One of my goal with standard replace-by-fee is to prevent this scenario
by forcing merchants and others to implement ways of accepting zeroconf
transactions safely that work in a decentralized environment regardless
of what miners do; we have a stronger and safer Bitcoin ecosystem if
we're relying on math rather than trust to secure our zeroconf
transactions. We're also being more honest to users, who right now often
have the very wrong impression that unconfirmed transactions are safe to
accept - this does get people ripped off all too often!
Anyway, sorry for the rant! FWIW I updated my FSS-RBF patch and am
waiting to get some feedback:
    Suhas Daftuar did find a pretty serious bug in it, now fixed. I'm
working on porting it to v0.10.2, and once that's done I'm going to put
up a bounty for anyone who can find a DoS attack in the patch. If no-one
claims the bounty after a week or two I think I'll start feeling
confident about using it in production.

@_date: 2015-06-19 09:48:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
No worries, let me know if you have any issues. You have my phone
While my own preference - and a number of other devs - is full-RBF,
either one is a good step forward for Bitcoin.

@_date: 2015-06-19 09:52:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Do you mean just full-RBF, or FSS-RBF as well?
Speaking of, could we get a confirmation that Coinbase is, or is not,
one of the merchant service providers trying to get hashing power
contracts with mining pools for guaranteed transaction acceptance? IIRC
you are still an advisor to them. This is a serious concern for the
reasons I outlined in my post.
Equally if anyone else from Coinbase would like to chime in that'd be

@_date: 2015-06-19 10:08:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
You're mistaking cause and effect: the contracts will drive
centralization of mining, as only the larger, non-anonymous, players
have the ability to enter into such contracts.
What happens if the mining pools who are mining double-spends aren't
doing it delibrately? Sybil attacking pools appears to have been done
before to get double-spends though, equally there are many other changes
the reduce the reliability of transaction confirmations. For instance
the higher demands on bandwidth of a higher blocksize will inevitably
reduce the syncronicity of mempools, resulting in double-spend
opportunities. Similarly many proposals to limit mempool size allow
zeroconf double-spends.
In that case would you enter into such contracts?

@_date: 2015-06-19 10:59:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Unless you're sybil attacking the network and miners, consuming valuable
resources and creating systemic risks of failure like we saw with
Chainalysis, I don't see how you're getting "very small" double-spend
You realise how the fact that F2Pool is using full-RBF right now does
strongly suggest that the chances of a double-spend are not only low,
but more importantly, vary greatly? Any small change in relaying policy
or even network conditions creates opportunities to double-spend.
You know, you're creating an interesting bit of game theory here: if I'm
a miner who doesn't already have a mining contract, why not implement
full-RBF to force Coinbase to offer me one? One reason might be because
other miners with such a contract - a majority - are going to be asked
by Coinbase to reorg you out of the blockchain, but then we have a
situation where a single entity has control of the blockchain.
For the good of Bitcoin, and your own company, you'd do well to firmly
state that under no condition will Coinbase ever enter into mining

@_date: 2015-06-19 11:11:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
If you ask me to pay you 1BTC at address A and I create tx1 that pays
1BTC to A1 and 2BTC of chain to C, what's wrong with me creating tx2
that still pays 1BTC to A, but now only pays 1.999BTC to C? I'm not
defrauding you, I'm just reducing the value of my change address to pay
a higher fee. Similarly if I now need to pay Bob 0.5BTC, I can create
tx3 paying 1BTC to A, 0.5BTC to B, and 1.498BTC to C.
Yet from the point of view of an external observer they have no idea why
the transaction outputs reduced in size, nor any way of knowing if fraud
did or did not occur.
Equally, maybe you tell me "Actually, just give me 0.5BTC to cancel out
that debt", in which case I'm not breaking any contract at all by giving
you less money than I first promised - the contract has changed.
Again, none of this can or should be observable to anyone other than the
parties directly involved.
What do you think of Bitcoin XT then? It relays double-spends, which
makes it much easier to get double-spends to miners than before. In
particular you see a lot of zero-fee transactions being replaced by
fee-paying transactions, relayed through Bitcoin XT nodes and then
mined. Is that encouraging fraud?

@_date: 2015-06-19 11:40:54
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Well it is actually; that's why myself, Wladimir van der Laan, and
Gregory Maxwell all specifically? called Chainalysis's actions a sybil
The Bitcoin P2P network is resilliant to failure when the chance of any
one node going down is uncorrelated with others. For instance if you
accidentally introduced a bug in your nodes that failed to relay
transactions/blocks properly, you'd simultaneously be disrupting a large
portion of the network all at once.
How many nodes is Coinbase connecting too? What software are they
running? What subnets are they using? In particular, are they all on one
subnet or multiple?
You realise that Hearn/Andresen/Harding's double-spend-relaying patch,
included in Bitcoin XT, relays double-spend transactions right? Do you
consider that harmful?
But of course, you'd never 51% the network right? After all it's not
possible to guarantee that your miner won't mine double-spends, as there
is no single consensus definition of which transaction came first, nor
can there be.
Or do you see things differently? If I'm a small miner should I be
worried my blocks might be rejected by the majority with hashing power
contracts because I'm unable to predict which transactions Coinbase
believes should go in the blockchain?
Well, I think I've shown how dangerous mining contracts can be to the
overall health of the Bitcoin ecosystem; I'm simply asking you to
promise not to make use of this dangerous option regardless of what
happens. Like I said, if for whatever reason the first-seen mempool
behavior proves to be insufficient at preventing double-spends from your
perspective, you did suggest you might use mining contracts to ensure
txs you want mined get mined, over others.
1) "Chainalysis CEO Denies 'Sybil Attack' on Bitcoin's Network",
   March 14th 2015, Grace Caffyn, Coindesk,

@_date: 2015-06-19 12:15:53
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Incidentally, because someone asked that message was sent two weeks ago.
Also, a shout-out to Marshal Long of FinalHash for his help with
(FSS)-RBF deployment and for getting F2Pool and myself in touch, as well
as his work in talking getting pools on board with BIP66.

@_date: 2015-06-19 12:37:46
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Ah, seems you misunderstand the problem.
By properly we're concerned that things do get relayed, not that they do
not. In particularl with blocks a fairly to relay valid blocks will
quickly lead to a loss of consensus.
Right, so those dozen nodes, how many outgoing connections are they
While your goals may be reasonable, again, the question is how are you
going to achieve them? Do you accept that you may be in a position where
you can't guarantee confirmations? Again, what's your plan to deal with
this? For instance, I know Coinbase is contractually obliged to accept
zeroconf payments with at least some of your customers - how strong are
those agreements?
What we're worried about is your plan appears to include nothing
concrete beyond the possibility of getting contracts with hashing power,
maybe even just a majority of hashing power. This is something that
should concern everyone in the Bitcoin ecosystem, and it'd help if you
clearly stated what your intentions are.

@_date: 2015-06-19 12:53:19
@_author: Peter Todd 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
For instance, one of the ideas behind my Proofchains work is that you
could hind all details of a smartcontract-whatchamacallit protocol
behind single-use-seals in a consensus blockchain. Closing those seals,
that is spending the appropriate txouts, represents things in the
protocol which are absolutely unobservable to anyone without the data
behind those hashes, an extreme version of the above.
Incidentally, some patent prior-art exposure:

@_date: 2015-06-21 22:06:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] Just FYI 
As for how you can do that, on stock gnupg:
    gpg --send-key 934023AE18144354
Or possibly:
    gpg --keyserver hkp://keys.gnupg.net --send-key 934023AE18144354
If your gpg.conf doesn't have a keyserver set.
Note that if you add a new subkey (as well as other changes to your key)
you need to resend your key to the keyservers or people won't be able to
verify your signatures.
Disclaimer: OpenPGP kinda sucks. Deal with it.

@_date: 2015-06-22 15:23:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
It's important that we see a wide range of realistic testing of what an
8MB limit could look in the near future. An important part of that
testing is load testing.
As of writing the BIP above has no mention of what switchover rules will
be used for testnet; code floating around has August 1st 2015 as that
date. I propose we use August 1st 2013.
This switch over date should be set in the _past_ to allow for the
creation (via reorg) of a realistic full-load blockchain on testnet to
fully test the real-world behavior of the entire infrastructure
ecosystem, including questions like the scalability of block explorers,
SPV wallets, feasibility of initial syncronization, scalability of the
UTXO set, etc. While this is of course inconvenient - 2 years of 8MB
blocks is 840GB worth of data - the Bitcoin ecosystem can-not afford to
make a change like this blindly.
I'm sure with a $3.5 billion market cap at stake we can scrape together
the resources to voluntarily run a few hundred full-load full-nodes for
testing a change with the potential to destroy that market cap.

@_date: 2015-06-22 16:12:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
It would be useful if you replied directly to the emails in question as
opposed to breaking the flow of conversation and taking replies out of
context for other readers.

@_date: 2015-06-22 16:54:21
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
In the nVersion bits proposal that I co-authored we solved that issue by
comparing the timestamp against the median time, which is guaranteed by
the protocol rules to monotonically advance.

@_date: 2015-06-23 15:16:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
Also, as a few people have pointed out to me, the BIP proposal has no
information at all about testing, reproducable or not.
As much of the discussion about the acceptability of this BIP will be
heavily influenced by real world test results we should expect
sufficient information available to understand and reproduce those
tests; the Quality Assurance Test Plan done for BIP16 is a model worth
looking at:

@_date: 2015-06-23 15:28:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
Wladimir noted that 'The original presented intention of block size
increase was a one-time "scaling" to grant time for more decentralizing
solutions to develop'
In particular, if bandwidth scaling doesn't go according to your plan,
e.g. the exponential exponent is too large, perhaps due to technological
growth not keeping pace, or the political realities of actual bandwidth
deployment making theoretical technological growth irrelevant, what
mechanism will prevent centralization? (if any)

@_date: 2015-06-23 16:46:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
Pieter Wuille showed with simulations that miners with bad connectivity
are negatively affected by other miners creating larger blocks.
Similarly I showed that with equation-based analysis. I've seen no
response to either argument, and it's a centralization pressure.
Note how propagation times are important enough to miners that they
already mine on top of unverified headers from other miners to increase
profitability, a grave threat to the security of the Bitcoin network.
These block propagation improvements are both already implemented (Matt
Corallo's relay network, p2pool) and require co-operation.
For instance, notice the recent full-RBF debate where Coinbase said
they'd consider getting contracts directly with miners to get
transactions they desired mined even when they otherwise would not be
due to double-spends. This is one of many scenarios where block
propagation improvements fail. Thus for a safety engineering
analysis we need to talk about worst-case scenarios.
Equally, I don't see any analysis from anyone of that % of non-optimized
transactions need to fail for what kind of centralizing pressure.
In any case, this ponts to the need for your proposal to explictly talk
about what kind of resources are needed by miners for what kind of
profitability, including the case where other miners are sabotaging
their profitability.

@_date: 2015-06-23 16:50:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
In particular, note how this bump is being proposed at a time when
blockchain space demand is so low that transactions usually cost well
under a penny each, a insignificant amount of money for almost all
In that regard Jeff Garzik's proposal of a blocksize increase with a
miner vote feedback mechanism is a huge improvement over Gavin's

@_date: 2015-06-23 22:43:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Mempool size consensus + dynamic block size 
It might help you to answer the following: If your mempool consensus
idea worked, could you use it to replace proof-of-work? Why? Why not?

@_date: 2015-06-25 18:33:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP65 / CHECKLOCKTIMEVERIFY deployment 
BIP66 adoption is quite close to 95% and will likely be enforced for all
blocks in a few more days; now is time to think about how CLTV will be
deployed, particularly given its benefits to much-needed scalability
solutions such as payment channels.
While I'm both a fan and co-author of the Version bits BIP(1) proposal,
it hasn't been implemented yet, and the implementation will be
relatively complex compared to the previous soft-fork mechanism. I think
there is good reason to get CLTV deployed sooner, and I don't think we
have any lack of consensus on it. The CLTV code itself has been
extensively reviewed in the form of the "mempool-only" pull-req, has
been included in the Elements sidechain prototype by Mark Friedenbach,
has been running in production on Viacoin for six months, and has a few
working demos of its functionality implemented. It's also been famously
described as "What you thought nLockTime did until you actually tried to
use it."
To that end I'm proposing that we simply use the existing median block
version mechanism previously used for the nVersion=2 and nVersion=3
soft-forks for CLTV. This mechanism is well-tested and understood, and
would allow CLTV to be easily backported to v0.10.x (even 0.9.x) with
little risk for rapid deployment. In the event that another soft-fork is
proposed prior to BIP65, nVersion=4, enforcement, we do have the option
of setting in motion yet another soft-fork as the median mechanism only
requires forks to be serialized in sequence - it does not prevent
multiple soft-forks from being "in-flight" at the same time.
Thoughts? If there are no objections I'll go ahead and write that code,
using the same thresholds as BIP66.
1)  at lists.sourceforge.net/msg07863.html

@_date: 2015-06-26 15:08:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
"Just rent a server" forces miners into deploying insecure hosted
infrastructure that's vulnerable to hacking and seizure; that we
encourage this already is worrying; requiring it for miners to be
profitable isn't acceptable.
See above. The obvious thing to do if connectivity matters is keep your
hashing in the cheapest possible place and sell that hashing power to
centralized miners, an effect we're already seeing. Making this effect
about an order of magnitude worse, then doubling the problem every two
years is dangerous.
As mining and hashing can be trivially separated that theory just
doesn't work.
Again, what concretely works against centralization of mining control?
A proper proposal would discuss this issue, and explain what the
expected effect will be.
The co-operation comes form the fact that mempool policies have to be
syncronized, not the protocol itself.

@_date: 2015-06-26 15:25:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
You're the one proposing a change here; we're evaluating the safety of
that change.
An analogous situation is imagine we have two islands, with a suspension
bridge between them. The bridge has just two lanes in either direction,
so obviously there's a limited amount of traffic that can flow across
it. It used to be used to little that people would joyride back and
forth as the toll booths at either end just charged a hundreth of a
penny per trip, or even not at all, but as demand has been increasing
tolls are going up as well.
You've come along with a bold new plan to add fifteen more lanes to that
bridge by expanding the bridge deck, then hire contractors in advance to
double the number of lanes every two years after that with no clear way
of terminating their contract if anything goes wrong. (in just over a
decade our two lane bridge will be a mile wide!)
Of course, obviously if we add enough lanes the cables holding it up
will snap, so we've better carefully analyse the carrying capacity of
the brdige and the threats it faces. For instance, earthquakes happen
every so often - the last one even snapped a few strands in the main
cables, which people claim were fixed... but we don't really know for
sure as a thick layer of paint was quickly slapped over the fix and
no-one's been able to inspect it.
It's perfectly reasonable to ask what kind of earthquakes you expect the
bridge to withstand, as well as peer-reviewed and peer-reproducable
figures about the strength of the cables and the weight of the traffic.
Similarly, we've got the funds to make a test bridge of the same
dimensions and see if it collapses. Any bridge widening proposal that
doesn't have this data is simply incomplete, end of story.
As for the other side, the worst that happens if nothing changes is
usage of this bridge gets proportioned to the most valuable users by the
supply and demand toll system. Some people might decide to take the bus
across rather than an inefficient individual car, some of the
advertising companies running trucks back and forth with billboards on
the side are going to stop doing that. But traffic is still going to get
across. It's not a politically easy position to be in - there's enormous
pressure to quickly "do something" however dangerous - but the actual
effects of doing nothing are ultimately not a big deal.
In civil engineering we have enough experience with disasters to know
that you can't give into political pressure to do potentially dangerous
things until the consequences are well understood; hopefully we'll learn
that in the consensus cryptography space before a big disaster rather
than after.

@_date: 2015-06-26 15:30:31
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Bitcoin-development] questions about bitcoin-XT 
IMO any change to the blocksize needs explicit mechanisms to let all
Bitcoin holders have a say in it.
Great! Glad to hear.
Are you thinking this more technical meeting should be before or after
the October event? Perhaps a better question, is what exactly do you see
being discussed at a technical meeting?

@_date: 2015-06-26 15:36:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] The need for larger blocks 
The supply and demand fee market means that there is a range of
reliability levels depending on what fee you pay; regardless of how high
demand is if you pay a sufficiently high fee that outbids less
important/lower fee transactions you'll get reliable transaction
The perceived lack of reliability is a function of the poor state of
wallet software, not an inherent problem with the system. Fixing that
software is much easier and much less risky than any hard-fork ever will
From my article on transaction fees during the CoinWallet.eu flood:
What needs to be done
Transaction fees aren't going away, blocksize increase or not. CoinWallet.eu is
only spending $5k flooding the network; even an 8MB blocksize increase can only
raise the cost of that attack to $40k, which is still very affordable. For
instance an attacker looking to manipulate the Bitcoin price could probably
afford to spend $40k doing it with the right trading strategy; let alone
governments, banks, big businesses, criminal enterprises, etc. to whom $40k is
chump-change. Wallets need to become smarter about fees, as does the rest of
the Bitcoin community.
What we need to do:
* Add fee/KB displays to block explorers.
* Change wallets to calculate and set fees in fee/KB rather than fixed fees regardless of tx size.
* Make websites with easy to understand displays of what the current mempool
  backlog is, and what fee/KB is needed to get to the front of the queue. We've
  done a great job for Bitcoin price charts, let's extend that to transaction
  fees.
* Add the ability to set any fee/KB to wallets, rather than be stuck with
  predefined options that may not be high enough.
* Add support for fee-bumping via (FSS)-RBF to wallets and Bitcoin Core.
Capacity limits are just a fact of life in the design of the Bitcoin protocol,
but that doesn't mean we can't give users the tools to deal with them

@_date: 2015-06-27 15:13:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] The need for larger blocks 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
You're assuming a receiver who is accepting a zeroconf transaction; most receivers don't.
For instance, when I deposit funds on my exchange they don't credit those funds until 4 confirmations, so I very much cafe about how long it takes to get the first confirmation.

@_date: 2015-06-27 15:21:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Hash: SHA256
It's not a reasonable path forward right now given the lack of testing done with 8MB+ blocks, among many other problems. A way to help make that appear more reasonable would be to setup a 8MB testnet as I suggested, with two years or so of 8MB blocks in history as well as a large UTXO set to test performance characteristics.
Of course, that'll be a 840GB download - if that's unreasonable you might want to ask why 8MB blocks are reasonable...

@_date: 2015-06-27 15:32:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
You know, if doing that is imprudent, then people are using Bitcoin in a recklessly dangerous way.

@_date: 2015-06-27 12:37:31
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Keep in mind that Andresen and Hearn both propose that the majority of
Bitcoin users, even businesses, abandon the global consensus technology
aspect of Bitcoin - running full nodes - and instead adopt trust
technology instead - running SPV nodes.
We're very much focused on meeting the demand for global consensus
technology, but unfortunately global consensus is also has inherently
O(n^2) scaling with current approaches available. Thus we have a fixed
capacity system where access is mediated by supply and demand
transaction fees.
Solutions like (hub-and-spoke) payment channels, Lightning, etc. allow
users of the global consensus technology in Bitcoin to use that
technology in much more effcient ways, leveraging a relatively small
amount of global consensus to do large numbers of transactions

@_date: 2015-06-27 13:20:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
The two main things you need to show is:
1) Small, anonymous, miners remain approximately as profitable as large
miners, regardless of whether they are in the world, and even when
miners are under attack. Remember I'm talking about mining here, not
just hashing - the process of selling your hashpower to someone else who
is actually doing the mining.
As for "approximately as profitable", based on a 10% profit margin, a 5%
profitability difference between a negligable ~0% hashing power miner
and a 50% hashing power miner is a good standard here.
The hard part here is basically keeping orphan rates low, as the %5
profitability different on %10 profit margin implies an orphan rate of
about 0.5% - roughly what we have right now if not actually a bit lower.
That also implies blocks propagate across the network in just a few
seconds in the worst case, where blocks are being generated with
transactions in them that are not already in mempools - circumventing
propagation optimization techniques. As we're talking about small
miners, we can't assume the miners are directly conneted to each other.
(which itself is dangerous from an attack point of view - if they're
directly connected they can be DoS attacked)
2) Medium to long term plan to pay for hashing power. Without scarcity
of blockchain space there is no reason to think that transaction fees
won't fall to the marginal cost of including a transaction, which
doesn't leave anything to pay for proof-of-work security. A proposal
meeting this criteria will have to be clever if you don't keep the
blocksize sufficiently limited that transaction fees are non-negligable.
One possible approach - if probably politically non-viable - would be to
change the inflation schedule so that the currency is inflated

@_date: 2015-06-27 13:34:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Hub-and-spoke payment channels and the Lightning network are not
off-chain solutions, they are ways to more efficiently use on-chain
transactions to achive the goal of moving assets from point a to point
b, resulting in more economic transactions being done with fewer - but
not zero! - blockchain transactions.
Off-chain transaction systems such as Changetip allow economic
transactions to happen with no blockchain transactions at all.
No, Bitcoin the network scales with O(n^2) with your above criteria, as
each node creates k transactions, thus each node has to verify k*n
transactions, resulting in O(n^2) total work.
For Bitcoin to have O(n) scaling you have to assume that the number of
validation nodes doesn't scale with the number of users, thus resulting
in a system where users trust others to do validation for them. That is
not a global consensus system; that's a trust-based system.
There's nothing inherently wrong with that, but why change Bitcoin
itself into a trust-based system, when you can preserve the global
consensus functionality, and built a trust-based system on top of it?

@_date: 2015-06-27 13:37:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
That's exactly how Bitcoin works already. See my article on how
transaction fees work for more details:

@_date: 2015-06-27 13:54:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
There's lots of markets where there is no assured quality of service,
and where the bids others are making aren't known. Most financial
markets work that way - there's only ever probabalistic guarantees that
for a given amount of money you'll be able to buy a certain amount of
gold at any given time for instance. Similarly for nearly all
commodities the infrastructure required to mine those commodities has
very little room for short, medium, or even long-term production
increases, so whatever the production supply is at a given time is
pretty much fixed.

@_date: 2015-06-27 14:47:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Which is a huge problem.
Concretely, what O(n^2) scaling means is that the more Bitcoin is
adopted, the harder it is to use in a decentralized way that doesn't
trust others; the blocksize limit puts a cap on how centralized Bitcoin
can get in a given technological landscape.

@_date: 2015-06-28 15:50:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP65 / CHECKLOCKTIMEVERIFY deployment 
I've opened a pull-req to deploy CHECKLOCKTIMEVERIFY via the
IsSuperMajority() mechanism:
        Final step towards CLTV deployment on mainnet.
    I've copied the logic and tests from the previous BIP66 (DERSIG)
    soft-fork line-by-line for ease of review; any code review applicable to
    BIP66 should be applicable to BIP65.
    Once merged I'll prepare a backport of the soft-fork logic for the
    v0.10.x branch as well.

@_date: 2015-06-29 01:07:26
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
Gregory: Please assign a BIP #
  BIP: ??
  Title: Full Replace-by-Fee Deployment Schedule
  Author: Peter Todd   Status: Draft
  Type: Standards Track
  Created: 2015-06-29
This BIP proposes a deployment schedule for full replace-by-fee (full-RBF)
functionality, with an automatic activation of Tuesday April 5th, 2016, at 3pm
UTC upon which supporting relay nodes and miners will enable full-RBF mempool
behavior on mainnet. Prior to the activation deadline supporting nodes and
miners will support first-seen-safe(1) replace-by-fee (FSS-RBF) mempool behavior.
Full-RBF has significant efficiency advantages(2) over alternatives such as
FSS-RBF and Child-Pays-For-Parent for a wide variety of common transaction
patterns such as fee-bumping and multiple sequential payments, as well as smart
contract protocols such as payment channels and auctions. Miner support would
let the wider Bitcoin community use the blockchain more efficiently, supporting
more transactions per second in less blockchain space.
While full-RBF does allow users to "undo" transactions after they have been
sent, the ability of decentralized wallets to protect users from double-spends
has proven to be near-zero.(3) Centralized services have had some success in
doing so, albeit at the cost of having to sybil attack the network, a strategy
that cannot scale to more than a small handful of payment processing
Even then success is not assured. Worryingly large payment providers have shown
willingness(4) to consider extreme measures such as entering into legal
contracts directly with large miners to ensure their transactions get mined.
This is a significant centralization risk and it is not practical or even
possible for small miners to enter into these contracts, leading to a situation
where moving your hashing power to a larger pool will result in higher profits
from hashing power contracts; if these payment providers secure a majority of
hashing power with these contracts inevitably there will be a temptation to
kick non-compliant miners off the network entirely with a 51% attack.
It does not make sense for the whole Bitcoin community to incur higher
transaction costs, sybil attacks, and centralization risk for the sake of a
small handful of companies. However an orderly, planned, upgrade is still
As full-RBF usage patterns, unlike first-seen-dependent zeroconf, does not
depend on mempool syncronization this BIP won't specify detailed relay node
behavior. However the following implementation is reasonable and well-tested
with considerations such as DoS attacks taken into account:
    To maximize engineer availability the deadline date was chosen to be towards,
but not at, the start of the week, and away from any public holidays. 3pm UTC
was chosen as a compromise between Pacific West Coast and European timezones;
miners in the Asian timezones may choose to manually set their exact switchover
date a few hours ahead with little risk to themselves. Nine months into the
future was chosen on the basis of allowing time for affected companies to plan
for the upgrade, without pushing the upgrade unnecessarily far into the future.
Thanks goes to Jeff Garzik for suggesting the idea of a full-RBF deployment
1) "First-Seen-Safe Replace-by-Fee",
Peter Todd, Bitcoin-development mailing list, May 25th 2015,
2) "Cost savings by using replace-by-fee, 30-90%",
Peter Todd, Bitcoin-development mailing list, May 25th 2015,
3) "F2Pool has enabled full replace-by-fee",
Peter Todd, Bitcoin-development mailing list, June 19th 2015,
 at lists.sourceforge.net/msg08422.html
4) "F2Pool has enabled full replace-by-fee",
Adrian Macneil, Director of Engineering, Coinbase,
Bitcoin-development mailing list, June 19th 2015,
 at lists.sourceforge.net/msg08443.html
This document is placed in the public domain.

@_date: 2015-06-29 01:53:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
For example, consider Luke-Jr's own BIP19, M-of-N Standard Transactions,
a non-consensus-critical suggested policy change!
    Anyway, full-RBF has significant impacts for wallet authors and many
other stakeholders. At minimum it changes how you will want to author
and (re)author transactions, much like BIP19 does.

@_date: 2015-06-29 01:56:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
Remember that one of the goals of full-RBF is to explicitly reject the
idea that miners should have any obligation with regard to what they're
mining. I perhaps should say that explicitly in my BIP proposal; I say
it implicitly by pointing out how the BIP *doesn't* define an exact
standard, but rather only an suggests an implementation as a starting

@_date: 2015-06-29 21:37:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
To be clear, full-RBF is a change that broadens what the P2P network
relays - transactions previously not relayed are now relayed. Under no
circumstance will full-RBF result in transactions *not* being relayed
that previously were relayed. This makes the P2P network more useful
rather than less, as it gives a predictable and uniform method to get
transactions to a wider variety of miners with a wider variety of
Note how even if no miners ever supported full-RBF, supporting full-RBF
on relay nodes would still be useful to users as it provides an easy and
cost-effective mechanism to rebroadcast transactions. In fact,
supporting full-RBF by default and disabling it if getblocktemplate is
called would be reasonable, if more than a bit of a hack!
In any case, my pull-req lets you set -fullrbfactivationtime=0 as a
simple and easy way to disable full-RBF functionality. Miners and relay
nodes who choose not to support it can easily do so, similar to how
OP_RETURN transactions can be disabled with -datacarrier=0

@_date: 2015-06-30 10:53:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
The disadvantage can be calculated compensated for by higher fees; if
the disadvantage is so large that the higher fees are unaffordable we're
in big trouble as the guarantees that mempools are in sync are pretty
poor. (why doublespending zeroconf txs is easy!) For instance, that'd
imply that sending two simultaneous transactions will cause profit
losses to all but the largest miners - who are unaffected - and that
upgrades that change IsStandard() will cause profit losses, among many
other problems.
Bitcoin just doesn't work if blocks aren't relayed quickly in the worst

@_date: 2015-06-30 12:05:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
Well, as you know I have good reason to believe those contracts are
being actively worked on right now. I've been talking about this issue
for something like two years now, and rather than seeing a shift away
from use of zeroconf, we're seeing new services adopting it, always
large, centralized, startups in the payment space. Meanwhile the story
for decentralized wallets is if anything even worse, and most such
wallets don't even have code to detect double-spends at all.
From the point of view of large companies like Coinbase, getting hashing
power contracts and sybil attacking the network is relatively easy. Why
would they invest in genuine improvements when they can take the easy
way out? Especially when the easy way is something their smaller
competitors simply have no access too? Working on those contracts now
only makes sense, especially as the reliability of the P2P network in
providing zeroconf guarantees continues to decline as transaction volume
increases, and uniformity of nodes decreases.
By acting sooner rather than later in adopting full-RBF I think we have
a shot at changing the direction of the industry; if we wait I think we
stand a real chance of that dangerous infrastructure being put into
place. Equally, when you ask who is benefiting from the status quo, it
isn't decentralized wallets, but a small number of centralized startups
who have an advantage that the former can't match.
You know, if the status quo didn't have the downsides I mention above,
I'd probably agree with you on that point. But the risks outweigh it
Note how relaying proof of double-spent status is only useful if you can
do something about it; the only method available without a scripting
language soft-fork is the scorched earth concept, which ironically
relies on full-RBF.
I'd suggest using nSequence for that purpose by defining non-maxint
nSequence as allowing RBF. (as well as non-maxint - 1 for nLockTime
usage to discourage fee sniping) Mark Friedenbach's sequence number BIP
is going to make use of transaction replacement anyway after all, so
doing that would be forward-compatible with it.

@_date: 2015-06-30 12:25:26
@_author: Peter Todd 
@_subject: [bitcoin-dev] Block size increase oppositionists: please 
and help do it
Which of course raises another issue: if that was the plan, then all you
can do is double capacity, with no clear way to scaling beyond that.
Why bother?

@_date: 2015-03-16 15:12:59
@_author: Peter Todd 
@_subject: [Bitcoin-development] My thoughts on the viability of the Factom 
Repost of for archival/disclosure purposes:
I'm very skeptical about the long-term viability of Factom and the value of the
Factom token.
The idea behind Factom is to create a proof-of-publication medium where facts
for some purpose can be published; the token incentivises people to maintain
the infrastructure required for that medium. You can think of Factom as a two
layer system, with Factom servers provide a layer that in turn is used by
secondary proof-of-publication ledgers. By publishing records in the Factom
layer you prove that the secondary layer of actual facts is being maintained
For instance one such secondary layer might be the property records of a
city using a digital Torrens title system? to maintain land titles.
Let's look at how this works step by step:
* You would know your digitally represented land title was valid because
  it was in the city's ledger and the digital signatures verify.
* You in turn know the *copy* of that ledger that you posess is the
  official version because you can inspect the ledger maintained by the
  Factom servers.
* You know that ledger is the official Factom layer - not a fork of that
  ledger - because you can run the Factom consensus protocol against the
  Bitcoin blockchain.
* You know you have the only Bitcoin blockchain and not a fork because
  of the Bitcoin Proof-of-Work consensus algorithm.
That's four steps in total. The problem is step  - the Factom
consensus layer - requires you to trust the Factom servers. The issue is
if the Factom servers ever publish a Factom ledger anchor in the Bitcoin
blockchain but don't make the data available you have no way of proving
that your Factom-secured ledger - e.g. the city's property title records
- is the only copy out there and you're not trying to defraud someone.
Those servers are voted in by a (quite complex) consensus algorithm, but
ultimately they are trusted third parties that can break your ability to
prove your Factom-secured records are honest.
Of course in practice if this happens people will just accept it and
patch their software to ignore the failure... but then why use Factom at
all? You can do the exact same thing with *far* less complexity by just
securing your ledger directly in the Bitcoin blockchain, skipping step
 and the dependency on trusted third parties. (I don't mean putting
the ledger itself in the chain, just hashes of it)
The only disadvantage to securing your records directly in the Bitcoin
blockchain is you have to pay transaction fees to do it. However
currently those fees are very small - they'll always be about the cost
to make a transaction - and if they do increase it's easy to create
"meta-ledgers" based on explicit trust relationships. For instance a
bunch of cities can get together to establish a meta-ledger for all
their per-city property title systems, perhaps using efficient
threshold-signature? multisig to ensure that a majority of those cities
have to sign off on any updates to the meta-ledger.
Of course all these Factom alternatives can be argued to "bloat the
blockchain" - but how are we going to force people to use less secure
alternatives to just using the blockchain? It's impossible to stop
people from securing ledgers in the blockchain technically; our only way
to do it is via social pressure like writing angry posts on reddit and
tl;dr: For the Facom token to rise in value we need Bitcoin transaction
fees to rise greatly, and/or people to choose to use much more complex
and less secure systems in preference to much more simple systems.
Disclaimer: I've been hired by Factom to review the Factom protocol. I
also am working on a competing library called Proofchains that among
other things can be used to secure ledgers using Bitcoin directly. That
funding model for that effort is to convince individual clients that
they need the technology and should pay me to develop it.
1) 2)

@_date: 2015-03-26 14:33:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
Again, along the lines of what Gregory Maxwell is saying, if the payment
instructions you have given to the sender say "don't make funds
spendable with scriptPubKey after this date" why are you scanning those
"old" bitcoin addresses for future payments? That makes no more sense
than taking your p2pkh addresses and scanning for the same scriptPubKey
embedded within a p2sh address - you haven't told anyone to pay you via
that method so why expect anyone to do so?
The sender is free to bury their Bitcoins in a safe in your neighbors
front yard; you have no reason to accept such silly behavior as payment
and every reason to ignore it.

@_date: 2015-03-28 14:22:27
@_author: Peter Todd 
@_subject: [Bitcoin-development] Double spending and replace by fee 
Hash: SHA256
Would you so us all a favor and make a list of companies *actually* relying on "first-seen" mempool behaviour. Because I've been having a hard time actually finding anyone who does who hasn't given up on it. Not very useful to talk about attacks against hypothetical defences.

@_date: 2015-03-29 07:20:43
@_author: Peter Todd 
@_subject: [Bitcoin-development] My thoughts on the viability of the 
============================== START ==============================
You know, looking though your writeup, I think we're talking past each
other. I've found with a lot of other projects a good way to start is to
explicitly list what you think Factom *prevents* from happening. It is
after all security software - the most important thing it does is what
it prevents the attacker from doing. Be specific - you really need to
nail down exactly what kind of guarantees you're trying to get out of
the Factom system.

@_date: 2015-05-04 00:36:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] New release of replace-by-fee for Bitcoin 
My replace-by-fee patch is now available for the v0.10.1 release:
    No new features in this version; this is simply a rebase for the Bitcoin
Core v0.10.1 release. (there weren't even any merge conflicts) As with
the Bitcoin Core v0.10.1, it's recommended to upgrade.
The following text is the copied verbatim from the previous release:
What's replace-by-fee?
Currently most Bitcoin nodes accept the first transaction they see
spending an output to the mempool; all later transactions are rejected.
Replace-by-fee changes this behavior to accept the transaction paying
the highest fee, both absolutely, and in terms of fee-per-KB. Replaced
children are also considered - a chain of transactions is only replaced
if the replacement has a higher fee than the sum of all replaced
Doing this aligns standard node behavior with miner incentives: earn the
most amount of money per block. It also makes for a more efficient
transaction fee marketplace, as transactions that are "stuck" due to bad
fee estimates can be "unstuck" by double-spending them with higher
paying versions of themselves. With scorched-earth techniques? it gives
a path to making zeroconf transactions economically secure by relying on
economic incentives, rather than "honesty" and alturism, in the same way
Bitcoin mining itself relies on incentives rather than "honesty" and
Finally for miners adopting replace-by-fee avoids the development of an
ecosystem that relies heavily on large miners punishing smaller ones for
misbehavior, as seen in Harding's proposal? that miners collectively 51%
attack miners who include doublespends in their blocks - an unavoidable
consequence of imperfect p2p networking in a decentralized system - or
even Hearn's proposal? that a majority of miners be able to vote to
confiscate the earnings of the minority and redistribute them at will.
Once you've compiled the replace-by-fee-v0.10.1 branch just run your
node normally. With -debug logging enabled, you'll see messages like the
following in your ~/.bitcoin/debug.log indicating your node is replacing
transactions with higher-fee paying double-spends:
    2015-02-12 05:45:20 replacing tx ca07cc2a5eaf55ab13be7ed7d7526cb9d303086f116127608e455122263f93ea with c23973c08d71cdadf3a47bae45566053d364e77d21747ae7a1b66bf1dffe80ea for 0.00798 BTC additional fees, -1033 delta bytes
Additionally you can tell if you are connected to other replace-by-fee
nodes, or Bitcoin XT nodes, by examining the service bits advertised by
your peers:
    $ bitcoin-cli getpeerinfo | grep services | egrep '((0000000000000003)|(0000000004000001))'
            "services" : "0000000000000003",
            "services" : "0000000004000001",
            "services" : "0000000004000001",
            "services" : "0000000000000003",
            "services" : "0000000004000001",
            "services" : "0000000004000001",
            "services" : "0000000000000003",
            "services" : "0000000000000003",
Replace-by-fee nodes advertise service bit 26 from the experimental use
range; Bitcoin XT nodes advertise service bit 1 for their getutxos
support. The code sets aside a certain number of outgoing and incoming
slots just for double-spend relaying nodes, so as long as everything is
working you're node should be connected to like-minded nodes a within 30
minutes or so of starting up.
If you *don't* want to advertise the fact that you are running a
replace-by-fee node, just checkout a slightly earlier commit in git; the
actual mempool changes are separate from the preferential peering
commits. You can then connect directly to a replace-by-fee node using
the -addnode command line flag.
1) 2) 3) 4) 5) 6)  at lists.sourceforge.net/msg06970.html
7)

@_date: 2015-05-04 01:07:15
@_author: Peter Todd 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
Matt Corallo brought up? the issue of OP_NOP scarcity on the mempool
only CLTV pull-req?:
    "I like merging this, but doing both CLTV things in one swoop would be
    really nice. Certainly if we're gonna use one of the precious few
    OP_NOPs we have we might as well make it more flexible."
I have two lines of thought on this:
1) We're going to end up with a Script v2.0 reasonably soon, probably
   based on Russel O'Connor and Pieter Wuille's Merkelized Abstract Syntax
   Tree? idea. This needs at most a single OP_NOPx to implement and mostly
   removes the scarcity of upgradable NOP's.
2) Similarly in script v1.0 even if we do use up all ten OP_NOPx's, the
   logical thing to do is implement an  OP_EXTENDED.
3) It's not clear what form a relative CLTV will actually take; the BIP
   itself proposes a OP_PREVOUT_HEIGHT_VERIFY/OP_PREVOUT_DATA along with
   OP_ADD, with any opcode accessing non-reorg-safe prevout info being made
   unavailable until the coinbase maturity period has passed for
   soft-fork safeness.
That said, if people have strong feelings about this, I would be willing
to make OP_CLTV work as follows:
     1 OP_CLTV
Where the 1 selects absolute mode, and all others act as OP_NOP's. A
future relative CLTV could then be a future soft-fork implemented as
     2 OP_CLTV
On the bad side it'd be two or three days of work to rewrite all the
existing tests and example code and update the BIP, and (slightly) gets
us away from the well-tested existing implementation. It also may
complicate the codebase compared to sticking with just doing a Script
v2.0, with the additional execution environment data required for v2.0
scripts cleanly separated out. But all in all, the above isn't too big
of a deal.
Interested in your thoughts.
1) 2) 3)

@_date: 2015-05-06 21:49:52
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
I'm not sure if you've seen this, but a good paper on this topic was
published recently: "The Economics of Bitcoin Transaction Fees"
    Abstract
    --------
    We study the economics of Bitcoin transaction fees in a simple static
    partial equilibrium model with the specificity that the system security
    is directly linked to the total computational power of miners. We show
    that any situation with a fixed fee is equivalent to another situation
    with a limited block size. In both cases, we give the optimal value of
    the transaction fee or of the block size. We also show that making the
    block size a non binding constraint and, in the same time, letting the
    fee be fixed as the outcome of a decentralized competitive market cannot
    guarantee the very existence of Bitcoin in the long-term.
In short, without either a fixed blocksize or fixed fee per transaction
Bitcoin will will not survive as there is no viable way to pay for PoW
security. The latter option - fixed fee per transaction - is non-trivial
to implement in a way that's actually meaningful - it's easy to give
miners "kickbacks" - leaving us with a fixed blocksize.
I think this lack of understanding of the limitations of blockchain tech
is very dangerous, never mind, downright misleading. I keep running into
startups at conferences with completely unrealistic ideas about how
large they'll be able to grow their on-blockchain businesses. For
example, a few weeks ago at the Stanford blockchain conference I spoke
to a company planning on using multisig escrow contracts to settle
financial instruments, and expected to be doing about as many
transactions/day on the blockchain for their business within a year or
so as all other Bitcoin users currently do combined. These guys quite
frankly had no understanding of the issues, and had apparently based
their plans on the highly optimistic Bitcoin wiki page on
scalability.(1) (I'd fix this now, but the wiki seems to not be allowing
We'd do a lot of startups a lot of good to give them accurate, and
honest, advice about the scalability of the system. The wiki definitely
isn't that. Neither is the bitcoin.org developer documentation(2), which
doesn't mention scalability at all.
Even a relatively small increase to 20MB will greatly reduce the number
of people who can participate fully in Bitcoin, creating an environment
where the next increase requires the consent of an even smaller portion
of the Bitcoin ecosystem. Where does that stop? What's the proposed
mechanism that'll create an incentive and social consensus to not just
'kick the can down the road'(3) and further centralize but actually
scale up Bitcoin the hard way? The only proposal that I've seen that
attempts to do this is John Dillon's proof-of-stake blocksize vote(4),
and that is far from getting consensus.
1) 2) 3) 4)  at lists.sourceforge.net/msg02323.html

@_date: 2015-05-06 22:16:44
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
I'll also point out that miners with the goal of finding more blocks
than their competition - a viable long-term strategy to increase market
share and/or a short-term strategy to get more transaction fees -
actually have a perverse incentive(1) to ensure their blocks do *not*
get to more than ~30% of the hashing power. The main thing holding them
back from doing that is that the inflation subsidy is still quite high -
better to get the reward now than try to push your competition out of
It's plausible that with a limited blocksize there won't be an
opportunity to delay propagation by broadcasting larger blocks - if
blocks propagate in a matter of seconds worst case there's no
opportunity for gaming the system. But it does strongly show that we
must build systems where that worst case propagation time in all
circumstances is very short relative to the block interval.
1)  at lists.sourceforge.net/msg03200.html

@_date: 2015-05-07 06:12:50
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
Care to be specific?
We've made lots of protocol related changes, as well as non-consensus
policy changes, often in quite short timeframes, and with little drama.
For instance BIP66 adopting is progressing smoothly, and itself was very
quickly developed as part of a broader response to a serious OpenSSL
flaw. My own BIP65 is getting wide consensus with little drama and good
peer review, and that's happening even without as much attention paid to
it from myself as I should have been giving it. The BIP62 malleability
softfork is going more slowly, but that's because peer review is finding
issues and fixing them - something to be expected in an environment
where we simply must be cautious.
As for the v0.11 release, it will have pruning, perhaps the biggest
change to the way Bitcoin Core works that we've ever made. Equally it's
notable how many people collaborated on the implementation of pruning,
again with little drama.
Sure, some stuff has been hard to get consensus on. But those things
carry high risks, and involve code and practices known to be dangerous.
In most cases we've found out the lack of consensus was spot on, and
controversial changes turn out later to have severe security
vulnerabilities. I read that as a sign that the peer review and
consensus building process works just fine.

@_date: 2015-05-07 09:02:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
If Gavin had good points to make, he'd probably eventually change
everyone's mind.
But if he fails to do that at some point he'll just get ignored and for
all practical purposes won't be considered part of the consensus. Not
unlike how if someone suggested we power the blockchain via perpetual
motion machines they'd be ignored. Bitcoin is after all a decentralized
system so all power over the development process is held only by social
consent and respect.
At that point I'd suggest Gavin fork the project and go to the next
level of consensus gathering, the community at large; I'm noticing this
is exactly what you and Gavin are doing.
Speaking of, are you and Gavin still thinking about forking Bitcoin
Core? If so I wish you the best of luck.
Sent: Wednesday, July 23, 2014 at 2:42 PM
Forking Bitcoin-Qt/Core has been coming up more and more often lately in conversation (up from zero not that long ago). Gavin even suggested me and him fork it ... I pointed out that maintainers don't normally fork their own software :)
The problem is that the current community of developers has largely lost interest in supporting SPV wallets. Indeed any protocol change that might mean any risk at all, for anyone, is now being bogged down in endless circular arguments that never end. The Bitcoin developers have effectively become the new financial regulators: restricting options within their jurisdiction with "someone might do something risky" being used as the justification.
If alternative low-risk protocols were always easily available this would be no problem, but often they require enormous coding and deployment effort or just don't exist at all. Yet, wallets must move forward. If we carry on as now there simply won't be any usable decentralised wallets left and Bitcoin will have become an energy-wasting backbone for a bunch of banks and payment processors. That's so far from your original vision, it's sad.
I know you won't return and that's wise, but sometimes I wish you'd left a clearer design manifesto before handing the reigns over to Gavin, who is increasingly burned out due to all the arguments (as am I).
Source:

@_date: 2015-05-07 10:22:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
As you know I was forwarded that email first, and because I *do* respect
your privacy I consulting with you via private IRC chat first, and as
you wished I didn't publish it. The hacker presumably gave up waiting
for me to do so and published it themselves seven months ago; to make
that clear I linked the source(1) of the email in my message. Those
emails simply are no longer private.
Frankly personal attacks like this - "your hypocrisy really is
bottomless, isn't it?", "Satoshi's hacker had no illusions about your
horrible personality" - simply don't belong on this mailing list and I
think we would all appreciate an apology.
1)

@_date: 2015-05-07 10:32:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
The hard part here will be including the players who aren't individually
"major", but are collectively important; who is the community?
How do you give the small merchants a voice in this discussion? The
small time traders? The small time miners? The people in repressive
countries who are trying to transact on thier own terms?
Legality? Should people involved in 3rd world remittances be
included? Even if what they're doing is technically illegal? What about
dark markets? If DPR voiced his opinion, should we ignore it?
Personally, I'm dubious about trying to make ecosystem-wide decisions
like this without cryptographic consensus; fuzzy human social consensus
is easy to manipulate.

@_date: 2015-05-07 10:40:12
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
I quite liked Wladimir's description of what someone with the ability
to merge pull requests into Bitcoin Core is:
     github.com/bitcoin/bitcoin repository admin, or maybe just "janitor"
In any case, we can't force people to run Bitcoin Core - an unpopular
patch that fails to reach consensus is a strong sign that it may not get
user acceptance either - so we might as well accept that centralized
authority over the development process isn't going to fly and deal with
the sometimes messy consequences.
Like I said, you're welcome to fork the project and try to get user
acceptance for the fork.

@_date: 2015-05-07 10:49:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
Well, basically you're asking if we shouldn't assume the people in this
discussion have honest intentions. If you want to go down that path,
keep in mind where it leads.
I think we'll find an basic assumption of civility to be more
productive, until proven otherwise. (e.g. NSA ties)

@_date: 2015-05-07 10:56:58
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
Would you please explain what you mean by "a soft-fork to start
producing bigger blocks"

@_date: 2015-05-07 11:25:03
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
I'm not.
I'm saying dealing with someone with proven NSA ties is one of the few
times when I think the assumption of honest intent should be ignored in
this forum.
Altcoins and non-Bitcoin-blockchain tx systems? Assuming anything other
than honest intent isn't productive in this forum.

@_date: 2015-05-07 13:17:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
There's no good way to implement that option - when the OP_NOPx is
executed all that's available to it without a lot of complex work is
what's already been pushed to the stack, not what will be pushed to the
stack in the future.

@_date: 2015-05-07 13:29:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
I've spent a lot of time talking to companies about this, and the
problem is telling them that isn't actually very useful; knowing the
supply side of the equation isn't all that useful if you don't know the
demand side. Problem is we don't really have a good handle on what
Bitcoin will be used for in the future, or even for that matter, what
it's actually being used for right now.
As we saw with Satoshidice before and quite possibly will see with smart
contracts (escrows, futures, etc) it's easy for a relatively small
number of use cases to drive a significant amount of transaction volume.
Yet, as Wladimir and others point out, the fundemental underlying
architecture of the blockchain has inherently poor O(n^2) scaling, so
there's always some level of demand where it breaks, and/or incentivizes
actors in the space to push up against "safety stops" like soft
blocksize limits and get them removed.
Note how the response previously to bumping up against soft policy
limits was highly public calls(1) at the first hint of touble: "Mike
Hearn: Soft block size limit reached, action required by YOU"
1)

@_date: 2015-05-07 13:42:20
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
Sounds like you're saying we are bumping up against a 1MB limit. However
other than the occasional user who has sent a transaction with an
extremely low/no fee, what evidence do we have that this is or is not
actually impacting meaningful usage form the user's point of view?
Do we have evidence as to how users are coping? e.g. do they send time
sensitive transactiosn with higher fees? Are people conciously moving
low value transactions off the blockchain? Equally, what about the story
with companies? You of course are an advisor to Coinbase, and could give
us some insight into the type of planning payment processors/wallets are
doing.  For instance, does Coinbase have any plans to work with other
wallet providers/payment processors to aggregate fund transfers between
wallet providers - an obvious payment channel application.

@_date: 2015-05-07 20:05:56
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
It's really important that we remember that we're building security
software: it *must* hold up well even in the face of attack. That means
we need to figure out how it can be attacked, what the cost/profits of
such attacks are, and if the holes can be patched.  Just testing the
software with simulated loads is insufficient.
Also, re: breaking, don't forget that this may not be a malicious act.
For instance, someone can send contradictory transactions to different
parts of the network simultaneously to prevent mempool consistency -
there's no easy way to fix this. There are also cases where miners have
different policy than others, e.g. version disagreements, commercial
contracts for tx mining, etc.
Finally, remember that it's not in miners' incentives in many situations
for their blocks to propagate to more than ~30% of the hashing power.(1)
Personally, I'm really skeptical that we'll ever find a block
propagation latency reduction technique that sucesfully meets all the
above criteria without changing the consensus algorithm itself.
* How do we ensure miners don't cheat and stop validating blocks fully
before building on them? This is a significant moral hazard with larger
blocks if fees don't become significant, and can lead to dangerous
forks. Also, think of the incentives: Why would a miner ever switch from
the longest chain, even if they don't actually have the blocks to back
it up?
* We need a clear understanding of how we expect new full nodes, pruned
or not, to sync up to the blockchain. Obviously 20MB blocks
significantly increases the time and data required to sync. Are we
planning on simply giving up on full validation and trusting others for
copies of UTXO sets? Are we going to rely on UTXO commitments? What
happens if the UTXO set size itself increases greatly?
A good start would be for those players to commit to the general
principles of these systems; if they can't commit explain why.
For instance I'd be very interested in knowing if services like Coinbase
see legal issues with adopting technologies such as payment channels
between hosted wallet providers, payment processors, etc. I certainly
wouldn't be surprised if they see doing anythign not on-blockchain as a
source of legal uncertainty - based on discussions I've had with
regulatory types in this space it sounds like there's a reasonable
chance protocol details such as requiring that transactions happen on a
public blockchain will be "baked into" regulatory requirements.
FWIW I've got some funding to implement first-seen-safe replace-by-fee.
1)  at lists.sourceforge.net/msg03200.html

@_date: 2015-05-07 23:41:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase 
Your corporate clients, *why* do they want to use Bitcoin and what for

@_date: 2015-05-08 12:37:01
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
The soft-limit is there miners themselves produce smaller blocks; the
soft-limit does not prevent other miners from producing larger blocks.
As we're talking about ways that other miners can use 20MB blocks to
harm the competition, talking about the soft-limit is irrelevant.
Similarly, as security engineers we must plan for the worst case; as
we've seen before by your campaigns to raise the soft-limit(1) even at a
time when the vast majority of transaction volume was from one user
(SatoshiDice) soft-limits are an extremely weak form of control.
For the proposes of discussing blocksize increase requirements we can
stop talking about the soft-limit.
1)

@_date: 2015-05-08 12:43:10
@_author: Peter Todd 
@_subject: [Bitcoin-development] Assurance contracts to fund the network 
You mean anyone-can-spend?
I've got code that does this actually:
Needs to have a feature where it replaces the txout set with simply
OP_RETURN-to-fees if the inputs don't sign the outputs though.
(SIGHASH_NONE for instance)

@_date: 2015-05-08 12:51:45
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
The problem with gating block creation on Bitcoin days destroyed is
there's a strong potential of giving big mining pools an huge advantage,
because they can contract with large Bitcoin owners and buy dummy
transactions with large numbers of Bitcoin days destroyed on demand
whenever they need more days-destroyed to create larger blocks.
Similarly, with appropriate SIGHASH flags such contracting can be done
by modifying *existing* transactions on demand.
Ultimately bitcoin days destroyed just becomes a very complex version of
transaction fees, and it's already well known that gating blocksize on
total transaction fees doesn't work.

@_date: 2015-05-08 23:08:33
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
Makes it trivial to find miners and DoS attack them - a huge risk to the
network as a whole, as well as the miners.
Right now pools already get DoSed all the time through their work
submission systems; getting DoS attacked via their nodes as well would
be a disaster.
That'd be an excellent way to double-spend merchants, significantly
increasing the chance that the double-spend would succeed as you only
have to get sufficient hashing power to get the lucky blocks; you don't
need enough hashing power to *also* ensure those blocks don't become the
longest chain, removing the need to sybil attack your target.

@_date: 2015-05-09 12:39:24
@_author: Peter Todd 
@_subject: [Bitcoin-development] Bitcoin-development Digest, Vol 48, 
Mr Gomez may find my thesis paper on the creation of imitations of
reality with the mathematical technique of Bolshevik Statistics (BS) to
be of aid:

@_date: 2015-05-09 14:30:31
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Like many things, the fact that they need to negotiate the right at all
is a *huge* barrier to smaller mining operations, as well as being an
attractive point of control for regulators.
If you want to allow stakeholders influence you should look into John Dillon's
proof-of-stake blocksize voting scheme:
 at lists.sourceforge.net/msg02323.html

@_date: 2015-05-09 14:45:18
@_author: Peter Todd 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
You can't assume that UTXO growth will be driven by walles at all; the
UTXO set's global consensus functionality is incredibly useful and will
certainly be used by all manner of applications, many having nothing to
do with Bitcoin.
As one of many examples, here's a proposal - with working code - to use
the UTXO set to get consensus over TLC certificate revocations. The
author, Christopher Allen, was one of the co-authors of the SSL
There's nothing we can do to stop these kinds of usages other than
forcing users to identify themselves to get permission to use the
Bitcoin blockchain. Using the Bitcoin blockchain gives their users
significantly better security in many cases than any alternatives, so
there's strong incentives to do so. Finally, the cost many of these
alternative uses are willing to pay pre UTXO/transaction is
significantly higher than the fees many existing Bitcoin users can pay
to make transactions.

@_date: 2015-05-10 16:51:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] A way to create a fee market even without 
It's not possible to create consensus rules enforcing anything about
fees because it's trivial to pay miners out of band.
For instance, you can pay transaction fees by including anyone-can-spend
outputs in your transactions. The miner creating the block then simply
adds a transaction at the end of their block collecting all the
anyone-can-spend outputs. Equally, if you try to prohibit that - e.g. by
preventing respending of funds in the same block - they can simply
publish fee addresses and have people put individual outputs for those
addresses in their transactions. (IIRC Eligius gave people the option to
pay fees that way for awhile)

@_date: 2015-05-11 06:34:02
@_author: Peter Todd 
@_subject: [Bitcoin-development] Reducing the block rate instead of 
It's *way* easier to buy more bandwidth that it is to get lower latency.
After all, getting to the other side of the planet via fiber takes at
*minimum* 100ms simply due to the speed of light; routing overheads
approximately double or triple that for all but highly specialized and
very, very expensive, networking services. Bandwidth simply can't fix
the speed of light.
It's also not at all realistic or desirable to assume connectivity in a
single hop, so you can again multiply that base latency by 2-5 times.
And on top of *that* you have to take into account latency from hasher
to mining pool - time that the hashing power isn't working on the new
block because they're work unit hasn't been updated matters just as much
as the time to get that block to the pool in the first place. Being
forced to reduce that latency is very damaging to the ecosystem as
you're making it more profitable to keep hashing power centralized.
In any case, even with 10 minute blocks pools already pay a lot of
attention to latency... Why make that problem 10x worse?
Can you explain your reasoning here in detail?
Actually the correct figure is less than ~30%:
 at lists.sourceforge.net/msg03200.html
They dynamically change? Source?
Remember that the strategy still gives you a benefit if you simply
target, say, 75% rather than the minimum threshold.
How do you see that blacklisting actually being done?
Equally, it's easy to portray such mining as being "for the good of
Bitcoin" - "we're just making transaction cheap! tough luck if your
shitty pool can't keep up" This is quite unlike selfish mining.
GHOST works radically differently than a linear blockchain, and it's not
clear that it actually has the correct economic incentives.
Keep in mind that miners already use optimized propagation techniques,
like p2pool's implementation or Matt Corallo's block relaying network.

@_date: 2015-05-12 13:16:40
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
Lots of people are tossing around ideas for partial archival nodes that
would store a subset of blocks, such that collectively the whole
blockchain would be available even if no one node had the entire chain.

@_date: 2015-05-12 17:01:25
@_author: Peter Todd 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
Doing this makes handling the year 2038 problem a good deal more
The CLTV codebase specifically fails on negative arguments to avoid any
ambiguity or implementation differences here.

@_date: 2015-05-20 02:26:11
@_author: Peter Todd 
@_subject: [Bitcoin-development] ChainDB Whitepaper 
I'm quite disappointed to see that you still haven't fixed the problem
that transaction fees prove nothing at all. Among other things, you risk
creating a system where miners can much more cheaply sell the service of
including the requisite "high-fee" transactions in blocks they mine for
the much lower cost of the risk of the blocks being orphaned and other
miners getting those fees. In particular, the more hash power you have,
the lower that cost is - exactly the opposite kind of incentive than we
want. As described this is an extremely dangerous project, both to its
users, and Bitcoin as a whole; in general the idea of anything that
tries to use transaction fees as "proof" is highly dangerous.
You should implement this with direct provably unspendable OP_RETURN
sacrifices for now, and perhaps in the future, sacrifice to
any-one-can-spend-in-the-future scriptPubKeys once CLTV is deployed.  If
you do this the interval needs to be long enough to robustly get past
business cycles - e.g. 1 year - to avoid the well-known problem that
large miners can sell these proofs cheaply.
Other comments:
* Bitcoin does not securely store data; Bitcoin proves the publication of
data.(1) This can be seen by the recently added(2) pruning functionality
which allows full nodes to discard all blockchain data other than the
UTXO set and some number of recent blocks. (to handle reorganizations
efficiently) Additionally even the UTXO set can be discarded in
principle if my TXO commitments proposal is implemented.  Between both
proposals there's no guarantee that data published to the Bitcoin
blockchain will be stored by anyone at all, let alone be readily made
* The paper lacks a clear statement about what exactly the ChainDB
proposal is attempting to accomplish, and what ChainDB attempts to
prevent from happening. Are we trying to prove that data existed before
a certain time? (timestamping) Are we trying to prove that data reached
a certain audience? (proof-of-publication) Are we trying to come to
consensus on some type of mapping? (key:value consensus) What are we
assuming from miners? Might miners attempt to censor ChainDB
transactions? For instance you say "In the second rule, applying an
unpredictable order for selecting the best chain mitigates certain
attacks by Bitcoin miners" but you don't say what those attacks are.  A
key question related to that is whether or not the ChainDB chains are or
are not private, both recent and historical history.
* "A comprehensive ordering of all transactions also makes it possible
to select a block even when some blocks are being withheld." Keep in
mind that what has been "withheld" depends on what blocks you have
access too; from the point of view of one ChainDB user the withheld
blocks may be the blocks another ChainDB user has access too and
vice-versa. Again, the Bitcoin consensus is a way to prove publication
of data with strong sybil attack detection - the cost to sybil attack
ChainDB will be quite low in many situations as miners have the
priviledged position of having very low costs to include a transaction.
* "To minimize the risk that a builder loses bitcoin in the bidding
process, builders coordinate to select a common UTXO that all bid
transactions use as an input. In so doing, bid transactions are created
such that they deliberately conflict." This is a clever idea; I believe
Jeff Garzik deserves credit for this. (his auction proposal) Note too
that with SIGHASH_ANYONECANPAY the consensus scheme could be arranged
such that anyone can also add additional funds to a proposed consensus
that they agree with. Better yet, with SIGHASH_SINGLE by "stacking"
additional inputs to the transaction you would ensure all bids end up in
the same transaction, simplifying the consensus logic. (otherwise the
total bid is the sum of potentially multiple transactions sacrificing
funds in support of the same consensus)
* Speaking of, proof-of-sacrifice or proof-of-burn is the common term
used for a cryptographically provable expenditure. (e.g. for a fidelity
bond(3)) Although in this case, it's not a true sacrifice as fee-paying
transactions by themselves can be trivially collected by miners.
* "And Factom [8] has advanced the concept of using the Bitcoin block
chain directly for timestamping data" Factom goes well beyond simply
timestamping data. (something my much earlier OpenTimestamps project did
among many others) Rather Factom acts as a proof-of-publication layer
that allows the proving of the negative.(4)
ChainDB has a lot of similarities with my Zookeyv(5) proposal, as well
as some key differences. To recap the idea was to come to consensus on a
key:value mapping, such that there was a well-defined cost to change any
particular k:v pair, and such that 'uncontroversial' key:value pairs
would become more expensive to change over time as latter k:v pairs
would add to the cost to change of previous ones.
My original proposal was create a DAG of sacrifices, each committing a
key:value pair, and one or more previous nodes. (the case where n=1
being a linear chain) Nodes that set a key:value already assigned would
be considered invalid. For any tip you'd be able to determine a sum
sacrifice, and equally, a sum sacrificed on top of any key:value pair.
In hindsight, the rule set could be extended to all kinds of situations
akin to a blockchain. (as you propose)
A key question I came up with was whether or not the minimal data
required to prove the shape of the graph be published directly in the
blockchain. e.g. if a node consists of {H(key), H(value),
prev_node_hash[]} do you require those values to be themselves published
in the blockchain, or are they hidden behind a hash? The latter is more
efficient and censorship resistant, while the former makes it possible
to detect possible 51% attacks and outspend them. (Note how this notion
of "reactive security" can be efficiently used to fend off attackers by
outspending them after the fact, while keeping sacrifices low in the
general case; the sacrifice could even be crowdfunded with
SIGHASH_ANYONECANPAY transactions)
1) "[Bitcoin-development] Disentangling Crypto-Coin Mining: Timestamping, Proof-of-Publication, and Validation"
   Peter Todd, Nov 19th, 2013,
    at lists.sourceforge.net/msg03307.html
2) 3) 4) "Factom - Business Processes Secured by Immutable Audit Trails on the Blockchain"
   Paul Snow et. al, Nov 17th 2014,
   5)  discussion, May 31st 2013

@_date: 2015-05-23 14:26:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Replace-by-fee v0.10.2 - Serious DoS attack 
Bitcoin Wallet
My replace-by-fee patch is now available for the Bitcoin Core v0.10.2
    This release fixes a serious DoS attack present in previous releases.
Upgrading is strongly recommended for relay nodes, and mandatory for
miners. Users of Luke-Jr's gentoo distribution should either disable RBF
until a patch is released, or run their node behind a patched node.
Previously replacements that spent outputs the transactions they
conflicted with would be accepted. This would lead to orphaned
transactions in the mempool, a potential bandwidth DoS attack for relay
nodes, and even worse, on mining nodes would cause Bitcoin to crash when
CreateNewBlock() was called.
Thanks goes to to Suhas Daftuar for finding this issue.
Additionally, while investigating this issue I found that
Andresen/Harding's relay doublespends patch?, included in Bitcoin XT?,
also fails to verify that doublespends don't spend outputs of the
transactions they conflict with. As the transactions aren't accepted to
the mempool the issue is simply a variant of the bandwidth DoS attack
that's a well-known issue of Bitcoin XT. However, interestingly in
testing I found that Schildbach's Android Bitcoin Wallet? fails to
detect this case, and displays the transaction as a valid unconfirmed
transaction, potentially leading to the user being defrauded with a
doublespend.  While a well-known issue in general - Schildbach's
implementation trusts peers to only send it valid transactions and
doesn't even detect doublespends it receives from peers - it's
interesting how in this case the attacker doesn't need to also do a
sybil attack.
1) 2) 3)

@_date: 2015-05-25 17:05:41
@_author: Peter Todd 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
Odd, I just tried the above as well - with multiple peers connected -
and had the exact same problem.

@_date: 2015-05-25 17:26:38
@_author: Peter Todd 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
This can cause problems as until those transactions confirm, even more
of the user's outputs are unavailable for spending, causing confusion as
to why they can't send their full balance. It's also inefficient, as in
the case where the user does try to send a small payment that could be
satisfied by one or more of these small UTXO's, the wallet has to use a
larger UTXO.
With replace-by-fee however this problem goes away, as you can simply
double-spend the pending defragmentation transactions instead if they
are still unconfirmed when you need to use them.

@_date: 2015-05-25 20:10:34
@_author: Peter Todd 
@_subject: [Bitcoin-development]  Cost savings by using replace-by-fee, 30-90% 
CPFP is a significantly more expensive way of paying fees than RBF,
particularly for the use-case of defragmenting outputs, with cost
savings ranging from 30% to 90%
Case 1: CPFP vs. RBF for increasing the fee on a single tx
My wallet has a two transaction outputs that it wants to combine into
one for the purpose of UTXO defragmentation. It broadcasts transaction
t1 with two inputs and one output, size 340 bytes, paying zero fees.
Prior to the transaction confirming I find I need to spend those funds
for a priority transaction at the 1mBTC/KB fee level. This transaction,
t2a, has one input and two outputs, 226 bytes in size. However it needs
to pay fees for both transactions at once, resulting in a combined total
fee of 556uBTC. If this situation happens frequently, defragmenting
UTXOs is likely to cost more in additional fees than it saves.
With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374
bytes in size, paying 374uBTC. Even better, if one of the two inputs is
sufficiently large to cover my costs I can doublespend t1 with a
1-in-2-out tx just 226 bytes in size, paying 226uBTC.
Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF
              costs you more than you save

@_date: 2015-05-26 01:13:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
First-seen-safe replace-by-fee (FSS RBF) does the following:
1) Give users effective ways of getting "stuck" transactions unstuck.
2) Use blockchain space efficiently.
3) Changing the status quo with regard to zeroconf.
The current Bitcoin Core implementation has "first-seen" mempool
behavior. Once transaction t1 has been accepted, the transaction is
never removed from the mempool until mined, or double-spent by a
transaction in a block. The author's previously proposed replace-by-fee
replaced this behavior with simply accepting the transaction paying the
highest fee.
FSS RBF is a compromise between these two behaviors. Transactions may be
replaced by higher-fee paying transactions, provided that all outputs in
the previous transaction are still paid by the replacement. While not as
general as standard RBF, and with higher costs than standard RBF, this
still allows fees on transaction to be increased after the fact with
less cost and higher efficiency than child-pays-for-parent in many
common situations; in some situations CPFP is unusable, leaving RBF as
the only option.
For reference, standard replace-by-fee has the following criteria for
determining whether to replace a transaction.
1) t2 pays > fees than t1
2) The delta fees pay by t2, t2.fee - t1.fee, are >= the minimum fee
   required to relay t2. (t2.size * min_fee_per_kb)
3) t2 pays more fees/kb than t1
FSS RBF adds the following additional criteria to replace-by-fee before
allowing a transaction t1 to be replaced with t2:
1) All outputs of t1 exist in t2 and pay >= the value in t1.
2) All outputs of t1 are unspent.
3) The order of outputs in t2 is the same as in t1 with additional new
   outputs at the end of the output list.
4) t2 only conflicts with a single transaction, t1
5) t2 does not spend any outputs of t1 (which would make it an invalid
   transaction, impossible to mine)
These additional criteria respect the existing "first-seen" behavior of
the Bitcoin Core mempool implementation, such that once an address is
payed some amount of BTC, all subsequent replacement transactions will
pay an equal or greater amount. In short, FSS-RBF is "zeroconf safe" and
has no affect on the ability of attackers to doublespend. (beyond of
course the fact that any changes what-so-ever to mempool behavior are
potential zeroconf doublespend vulnerabilities)
Case 1: Increasing the fee on a single tx
All wallets should treat conflicting incoming transactions as equivalent
so long as the transaction outputs owned by them do not change. In
addition to compatibility with RBF-related practices, this prevents
unnecessary user concern if transactions are mutated. Wallets must not
assume TXIDs are fixed until confirmed in the blockchain; a fixed TXID
is not guaranteed by the Bitcoin protocol.

@_date: 2015-05-26 21:25:21
@_author: Peter Todd 
@_subject: [Bitcoin-development] Cost savings by using replace-by-fee, 
You're a bit mistaken there: standard RBF lets you change anything, and
FSS RBF lets you modify inputs and add outputs and/or make the value of
outputs higher.
Any significant change to mempool policy like RBF is very unlikely to be
incorporated in the Bitcoin Core v0.10.x branch, simply because it'd be
too large a change for a minor, mostly bugfix, release.
Having said that, I already maintain a standard RBF branch for v0.10.x,
and have been asked by a major minor to backport FSS RBF for v0.10.x as

@_date: 2015-05-27 03:30:32
@_author: Peter Todd 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
To clarify a point raised(1) on the pull-req itself:
The replacement transaction is allowed to not only add new txin's, but
also replace txins. Suppose t1 is a 2-in-2-out P2PKH using transaction,
374 bytes in size. With CPFP accomplished by a 1-in-1-out tx, 192 bytes,
you have 566 bytes total. With FSS RBF if you have an unspent output
greater in value than one of the outputs spent by t1, you can replace
that output in t1's vin txin set and rebroadcast the transaction, still
374 bytes in size. This gives you a 34% cost savings vs. CPFP.
Similarly in the multiple recipients case, if sufficiently large
outputs are available the additional funds can be obtained by swapping
one input for another.
For instance if Alice has three outputs, 1.0, 0.5, and 0.2 BTC, and
needs to pay Bob 1.1 BTC, she can create t1:
    1.0 -> Bob   1.1
    0.2 -> Alice 0.1
If she then needs to pay Charlie 0.2 BTC she can doublespend that with:
    1.0 -> Bob     1.1
    0.5 -> Charlie 0.2
        -> Alice   0.2
Note that care does need to be taken to ensure that multiple rounds of
this always leave at least one input unchanged.
1)

@_date: 2015-05-27 03:47:13
@_author: Peter Todd 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Can you provide a worked example of this in use? I think I see a major
flaw, but I'd like to see a worked example first.
Keep in mind that there's absolutely no reason to have pending
transactions in mempools until we actually expect them to be mined.
Equally this proposal is no more "consensus enforcement" than simply
increasing the fee (and possibly decreasing the absolute nLockTime) for
each replacement would be; increasing the fee for each mempool
replacement is a hard requirement as an anti-DoS anyway. (this was all
discussed on the mailing list two years ago when RBF was first proposed)

@_date: 2015-05-27 06:15:16
@_author: Peter Todd 
@_subject: [Bitcoin-development] Version bits proposal 
The median time mechanism is basically a way for hashing power to show
what time they think it is. Equally, the nVersion soft-fork mechanism is
a way for hashing power to show what features they want to support.
Block counts are inconvenient for planning, as there's no guarantee
they'll actually happen in any particular time frame, forward and back.
There's no particular incentive problems here - the median time clearly
shows support by a majority of hashing power - so I don't see any reason
to make planning more difficult.
If you assume no large reorganizations, your table of known BIPs can
just as easily be a list of block heights even if the median time
mechanism is used.
If you do assume there may be large reorganizations you can't have a
"simple table"

@_date: 2015-05-27 06:58:05
@_author: Peter Todd 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
I understand that part.
I'm just saying it's not clear to me what's the functional difference in
practice between it and having both parties sign a decreasing absolute
nLockTime. For instance, you and I could setup a payment channel using
the following transaction t0:
    1.0 BTC: PT -> 1.0 BTC: PT && (GM ||  CLTV)
    1.0 BTC: GM -> 1.0 BTC: GM && (PT ||  CLTV)
After  both of us are guaranteed to get our funds back
regardless. I can then give you funds by signing my part of t1a:
    t0.vout[0]   -> 0.5 BTC: PT
    t0.vout[1]   -> 1.5 BTC: GM
    nLockTime = You can then give me funds with t1b:
    t0.vout[0]   -> 1.5 BTC: PT
    t0.vout[1]   -> 0.5 BTC: GM
    nLockTime = etc. etc. We can close the channel by signing a non-nLockTime'd tx at
any time. If you don't co-operate, I have to wait, and hope I get my tx
mined before you get yours.
What I'm not seeing is how the relative nLockTime that nSequence
provides fundamentally changes any of this.

@_date: 2015-05-28 08:04:34
@_author: Peter Todd 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
For that matter, we probably don't want to treat this as a *version*
change, but rather a *feature* flag. For instance, nSequence is
potentially useful for co-ordinating multiple signatures to ensure they
can only be used in certain combinations, a use-case not neccesarily
compatible with this idea of a relative lock. Similarly it's potentially
useful for dealing with malleability.
nSequence is currently the *only* thing in CTxIn's that the signature
signs that can be freely changed; I won't be surprised if we find other
uses for it.
Of course, all of the above is assuming this proposal is useful; that's
not clear to me yet and won't be without fleshed out examples.

@_date: 2015-05-28 13:50:00
@_author: Peter Todd 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Note that the majority of hashing power is using Matt Corallo's block
relay network, something I confirmed the other day through my mining
contacts. Interestingly, the miners that aren't using it include some of
the largest pools; I haven't yet gotten an answer as to what their
rational for not using it was exactly.
Importantly, this does mean that block propagation is probably fairly
close to optimal already, modulo major changes to the consensus
protocol; IBLT won't improve the situation much, if any.
It's also notable that we're already having issues with miners turning
validation off as a way to lower their latency; I've been asked myself
about the possibility of creating an "SPV miner" that skips validation
while new blocks are propagating to shave off time and builds directly
off of block headers corresponding to blocks with unknown contents.

@_date: 2015-05-31 09:05:30
@_author: Peter Todd 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
Great to hear from you!
Yeah, I'm pretty surprised myself that Gavin never accepted the
compromises offered by others in this space for a slow growth solution,
rather than starting with over an order of magnitude blocksize increase.
This is particularly surprising when his own calculations - after
correcting an artithmetic error - came up with 8MB blocks rather than
Something important to note in Gavin Andresen's analysises of this issue
is that he's using quite optimistic scenarios for how nodes are
connected to each other. For instance, assuming that connections between
miners are direct is a very optimistic assumption that depends on a
permissive, unregulated, environment where miners co-operate with each
other - obviously that's easily subject to change! Better block
broadcasting logic helps this in the "co-operation" case, but there's
not much it can do in the worst-case.
Unrelated: feel free to contact me directly if you have any questions
re: the BIP66 upgrade; I hear you guys were planning on upgrading your
mining nodes soon.

@_date: 2015-11-03 23:00:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
FWIW my replace-by-fee fork does preferential peering with other RBF
nodes to ensure that you'll always be connected to at least some
full-RBF peers. In practice this works very well, and I'm sure a similar
scheme could be used in this situation as well.
Basically, conceptually unless you're connected to peers that advertise
that they relay the new data, you treat the situation as though you're
not connected to any peers at all. No different than if for some reason
none of your peers were advertising NODE_NETWORK.

@_date: 2015-11-16 18:24:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] request to use service bit 28 for testing 
Go for it!
AFAIK the only testing service bit in use right now is bit 26, used to
indicate full-RBF support by my replace-by-fee branch:
Speaking of, you may find the preferential peering code in the above to
be useful. It's a bit of a hack, but it does work.

@_date: 2015-11-16 19:42:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Opt-in Full Replace-By-Fee (Full-RBF) 
Opt-In Full-RBF allows senders to opt-into full-RBF semantics for their
transactions in a way that allows receivers to detect if the sender has
done so. Existing "first-seen" mempool semantics are left unchanged for
transactions that do not opt-in.
At last week's IRC meeting(1) we decided to merge the opt-in Full-RBF
pull-req(2), pending code review and this post, so this feature will
likely make it into Bitcoin Core v0.12.0
A transaction is considered to have opted into full-RBF semantics if
nSequence < 0xFFFFFFFF-1 on at least one input. Nodes that respect the
opt-in will allow such opt-in transactions (and their descendents) to be
replaced in the mempool if they meet the economic replacement criteria.
Transactions in blocks are of course unaffected.
To detect if a transaction may be replaced check if it or any
unconfirmed ancestors have set nSequence < 0xFFFFFFFF-1 on any inputs.
nSequence is used for opting in as it is the only "free-form" field
available for that purpose. Opt-in per output was proposed as well by
Luke-Jr, however the CTxOut data structure simply doesn't contain any
extra fields to use for that purpose. nSequence-based opt-in is also
compatible with the consensus-enforced transaction replacement semantics
in BIP68.
Allowing replacement if any input opts in vs. all inputs opting in is
chosen to ensure that transactions authored by multiple parties aren't
held up by the actions of a single party. Additionally, in the
multi-party scenario the value of any zeroconf guarantees are especially
Replacement is allowed even if unconfirmed children did not opt-in to
ensure receivers can't maliciously prevent a replacement by spending the
funds. Additionally, any reasonable attempt at determining if a
transaction can be double-spent has to look at all unconfirmed parents
Feedback from wallet authors indicates that first-seen-safe RBF isn't
very useful in practice due to the limitations inherent in FSS rules;
opt-in full-RBF doesn't preclude FSS-RBF from also being implemented.
Opt-in RBF transactions are currently mined by 100% of the hashing
power. Bitcoin Core has been producing transactions with non-maxint
nSequence since v0.11.0 to discourage fee sniping(3), and currently no
wallets are known that display such transactions yet do not display
opt-in RBF transactions.
1) 2) 3)

@_date: 2015-11-23 23:36:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP68: Second-level granularity doesn't make sense 
BIP68 currently represents by-height locks as a simple 16-bit integer of
the number of blocks - effectively giving a granularity of 600 seconds
on average - but for for by-time locks the representation is a 25-bit
integer with granularity of 1 second. However this granularity doesn't
make sense with BIP113, median time-past as endpoint for lock-time
calcualtions, and poses potential problems for future upgrades.
There's two cases to consider here:
1) No competing transactions
By this we mean that the nSequence field is being used simply to delay
when an output can be spent; there aren't competing transactions trying
to spend that output and thus we're not concerned about one transaction
getting mined before another "out of order". For instance, an 2-factor
escrow service like GreenAddress could use nSequence with
CHECKSEQUENCEVERIFY (CSV) to guarantee that users will eventually get
their funds back after some timeout.
In this use-case exact miner behavior is irrelevant. Equally given the
large tolerances allowed on block times, as well as the poisson
distribution of blocks generated, granularity below an hour or two
doesn't have much practical significance.
2) Competing transactions
Here we are relying on miners prefering lower sequence numbers. For
instance a bidirectional payment channel can decrement nSequence for
each change of direction; BIP68 suggests such a decrement might happen
in increments of one day.
BIP113 makes lock-time calculations use the median time-past as the
threshold for by-time locks. The median time past is calculated by
taking median time of the 11 previous blocks, which means when a miner
creates a block they have absolutely no control over what the median
time-past is; it's purely a function of the block tip they're building
This means that granularity below a block interval will, on average,
have absolutely no effect at all on what transaction the miner includes
even in the hypothetical case. In practice of course, users will want to
use significantly larger than 1 block interval granularity in protocols.
The downside of BIP68 as written is users of by-height locktimes have 14
bits unused in nSequence, but by-time locktimes have just 5 bits unused.
This presents an awkward situation if we add new meanings to nSequence
if we ever need more than 5 bits. Yet as shown above, the extra
granularity doesn't have a practical benefit.
Recommendation: Change BIP68 to make by-time locks have the same number
of bits as by-height locks, and multiply the by-time lock field by the
block interval.

@_date: 2015-11-24 00:58:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP68: Second-level granularity doesn't make sense 
Ha, that's awesome! Looks like we're pretty much on the same page re:

@_date: 2015-11-24 07:20:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
Definitely this one.
Although I wouldn't rush to make the change just yet - I for one am busy
writing some test programs to actually use BIP112, and in theory they
might say the more general CSV concept is better.
Whatever we call it, deciding on that is a simple s/FOO/BAR/ prior to

@_date: 2015-11-25 16:37:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] Why sharding the blockchain is difficult 
The following was originally posted to reddit; I was asked to repost it here:
In a system where everyone mostly trusts each other, sharding works great! You
just split up the blockchain the same way you'd shard a database, assigning
miners/validators a subset of the txid space. Transaction validation would
assume that if you don't have the history for an input yourself, you assume
that history is valid. In a banking-like environment where there's a way to
conduct audits and punish those who lie, this could certainly be made to work.
(I myself have worked on and off on a scheme to do exactly that for a few
different clients: [Proofchains](
But in a decentralized environment sharding is far, far, harder to
accomplish... There's an old idea we've been calling "fraud proofs", where you
design a system where for every way validation can fail, you can create a short
proof that part of the blockchain was invalid. Upon receiving that proof your
node would reject the invalid part of the chain and roll back the chain. In
fact, the original Satoshi whitepaper refers to fraud proofs, using the term
"alerts", and assumed SPV nodes would use them to get better guarantees they're
using a valid chain. (SPV as implemented by bitcoinj is sometimes referred to
as "non-validating SPV") The problem is, how do you guarantee that the fraud
will get detected? And How do you guarantee that fraud that is detected
actually gets propagated around the network? And if all that fails... then
The nightmare scenario in that kind of system is some miner successfully gets
away with fraud for awhile, possibly creating hundreds of millions of dollars
worth of bitcoins out of thin air. Those fake coins could easily "taint" a
significant fraction of the economy, making rollback impossible and shaking
faith in the value of the currency. Right now in Bitcoin this is pretty much
impossible because everyone can run a full node to validate the chain for
themselves, but in a sharded system that's far harder to guarantee.
Now, suppose we *can* guarantee validity. zk-SNARKS are basically a way of
mathematically proving that you ran a certain computer program on some data,
and that program returned true. *Recursive* zk-SNARKS are simply zk-SNARKS
where the program can also recursively evaluate that another zk-SNARK is true.
With this technology a miner could *prove* that the shard they're working on is
valid, solving the problem of fake coins. Unfortunately, zk-SNARKS are bleeding
edge crypto, (if zerocoin had been deployed a the entire system would have been
destroyed by a recently found bug that allowed fake proofs to be created) and
recursive zk-SNARKS don't exist yet.
The closest thing I know of to recrusive zk-SNARKS that actually does work
without "moon-math" is an idea I came up with for treechains called coin
history linearization. Basically, if you allow transactions to have multiple
inputs and outputs, proving that a given coin is valid requires the entire coin
history, which has quasi-exponential scaling - in the Bitcoin economy coins are
very quickly mixed such that all coins have pretty much all other coins in
their history.
Now suppose that rather than proving that all inputs are valid for a
transaction, what if you only had to prove that *one* was valid? This would
linearize the coin history as you only have to prove a single branch of the
transaction DAG, resulting in O(n) scaling. (with n <= total length of the
blockchain chain)
Let's assume Alice is trying to pay Bob with a transaction with two inputs each
of equal value. For each input she irrevocable records it as spent, permanently
committing that input's funds to Bob. (e.g. in an irrevocable ledger!) Next she
makes use of a random beacon - a source of publicly known random numbers that
no-one can influence - to chose which of the two inputs' coin history's she'll
give to Bob as proof that the transaction is real. (both the irrevocable ledger
and random beacon can be implemented with treechains, for example)
If Alice is being honest and both inputs are real, there's a 100% chance that
she'll be able to successfully convince Bob that the funds are real. Similarly,
if Alice is dishonest and neither input is real, it'll be impossible for her
convince prove to Bob that the funds are real.
But what if one of the two inputs is real and the other is actually fake? Half
the time the transaction will succeed - the random beacon will select the real
input and Bob won't know that the other input is fake. However, half the time
the *fake* input will be selected, and Alice won't be able to prove anything.
Yet, the real input has irrevocably been spent anyway, destroying the funds! If
the process by which funds are spent really is irrevocable, and Alice has
absolutely no way to influence the random beacon, the two cases cancel out.
While she can get away with fraud, there's no economic benefit for her to do
so. On a macro level, this means that fraud won't result in inflation of the
currency. (in fact, we want a system that institutionalizes this so-called
"fraud" - creating false proofs is a great way to make your coins more private)
(FWIW the way zk-SNARKS actually work is similar to this simple linearization
scheme, but with a lot of very clever error correction math, and the hash of
the data itself as the random beacon)
An actual implementation would be extended to handle multiple transaction
inputs of different sizes by weighing the probability that an input will be
selected by it's value - merkle-sum-trees work well for this. We still have the
problem that O(n) scaling kinda sucks; can we do better?
Yes! Remember that a genesis transaction output has no history - the coins are
created out of thin air and its validity is proven by the proof of work itself.
So every time you make a transaction that spends a genesis output you have a
chance of reducing the length of the coin validity proof back to zero. Better
yet, we can design a system where every transaction is associated with a bit of
proof-of-work, and thus every transaction has a chance of resetting the length
of the validity proof back to zero. In such a system you might do the PoW on a
per-transaction basis; you could outsource the task to miners with a special
output that only the miner can spend. Now we have O(1) scaling, with a k that
depends on the inflation rate. I'd have to dig up the calculations again, but
IIRC I sketched out a design for the above that resulted in something like 10MB
or 100MB coin validity proofs, assuming 1% inflation a year. (equally you can
describe that 1% inflation as a coin security tax) Certainly not small, but
compared to running a full node right now that's still a *huge* reduction in
storage space. (recursive zk-SNARKS might reduce that proof to something like
1kB of data)
Regardless of whether you have lightweight zk-SNARKS, heavyweight linearized
coin history proofs, or something else entirely, the key advantage is that
validation can become entirely client side. Miners don't even need to care
whether or not their *own* blocks are "valid", let alone other miners' blocks.
Invalid transactions in the chain are just garbage data, which gets rejected by
wallet software as invalid. So long as the protocol itself  works and is
implemented correctly it's impossible for fraud to go undetected and destroy
the economy the way it can in a sharded system.
However we still have a problem: censorship. This one is pretty subtle, and
gets to the heart of how these systems actually work. How do you prove that a
coin has validly been spent? First, prove that it hasn't already been spent!
How do you do that if you don't have the blockchain data? You can't, and no
amount of fancy math can change that.
In Bitcoin if everyone runs full nodes censorship can't happen: you either have
the full blockchain and thus can spend your money and help mine new blocks, or
that alternate fork might as well not exist. SPV breaks this as it allows funds
to be spent without also having the ability to mine - with SPV a cartel of
miners can prevent anyone else from getting access to the blockchain data
required to mine, while still allowing commerce to happen. In reality, this
type of cartel would be more subtle, and can even happen by accident; just
delaying other miners getting blockchain data by a few seconds harms those
non-cartel miners' profitability, without being obvious censorship. Equally, so
long as the cartel has [>30% of hashing power it's profitable in the long run
for the cartel if this
happens]( at lists.sourceforge.net/msg03200.html).
In sharded systems the "full node defense" doesn't work, at least directly. The
whole point is that not everyone has all the data, so you have to decide what
happens when it's not available.
Altcoins provide one model, albeit a pretty terrible one: taken as a whole you
can imagine the entire space of altcoins as a series of cryptocurrency shards
for moving funds around. The problem is each individual shard - each altcoin -
is weak and can be 51% attacked. Since they can be attacked so easily, if you
designed a system where funds could be moved from one shard to another through
coin history proofs every time a chain was 51% attacked and reorged you'd be
creating coins out of thin air, destroying digital scarcity and risking the
whole economy with uncontrolled inflation. You can instead design a system
where coins can't move between shards - basically what the altcoin space looks
like now - but that means actually paying someone on another "shard" requires
you to sell your coins and buy their coins - a inefficient and expensive
logistical headache. (there's a reason the Eurozone was created!)
If you want to transfer value between shards with coin history proofs, without
risking inflation, you need all the shards to share some type of global
consensus. This is the idea behind treechains: every part of the tree is linked
to a top-level timestamp chain, which means we have global consensus on the
contents of all chains, and thus spending a coin really is an immutable
one-time act.
Let's go into a bit more detail. So what is a coin in a treechains system?
First and foremost it's a *starting point* in some part of the tree, a specific
subchain. When Alice wants to prove to Bob that she spent a coin, giving it to
Bob, she inserts into that subchain the data that proves that someone *could
have* spent that coin - a valid signature and the hash of the transaction
output it was spending. But the actual proof that she gives to Bob isn't just
that spend data, but rather proof that all the blocks in that chain between the
starting point and the spend did *not* have a valid spend in them. (easiest way
to do that? give Bob those blocks) That proof must link back to the top-level
chain; if it doesn't the proof is simply not valid.
Now suppose Alice can't get that part of the subchain, perhaps because a cartel
of miners is mining it and won't give anyone else the data, or perhaps because
everyone with the data suffered a simultaneous harddrive crash. We'll also say
that higher up in the tree the data is available, at minimum the top-level
chain. As with Bitcoin, as long as that cartel has 51% of the hashing power,
Alice is screwed and can't spend her money.
What's interesting is what happens after that cartel disbands: how does mining
restart? It's easy to design a system where the creation of a block doesn't
require the knowledge of previous blocks, so new blocks can be added to extend
the subchain. But Alice is still screwed: she can't prove to Bob that the
missing blocks in the subchain didn't contain a valid spend of her coin. This
is pretty bad, on the other hand the damage is limited to just that one
subchain, and the system as a whole is unaffected.
There's a tricky incentives problem here though: if a miner can extend a
subchain without actually having previous blocks in that chain, where's the
incentive for that miner to give anyone else the blocks they create? Remember
that exclusive knowledge of a block is potentially valuable if you can extort
coin owners for it. (Bitcoin suffers from this problem right now with
validationless "SPV" mining, though the fact that a block can be invalid in
Bitcoin helps limit its effects)
Part of the solution could be mining reward; in Bitcoin, coinbase outputs can't
be spent for 100 blocks. A similar scheme could require that a spend of a
coinbase output in a subchain include proof that the next X blocks in that
subchain were in fact linked together. Secondly make block creation dependent
on actually having that data to ensure the linkage actually means something,
e.g. by introducing some validity rules so blocks can be invalid, and/or using
a PoW function that requires hashers to have a copy of that data.
Ultimately though this isn't magic: like it or not lower subchains in such a
system are inherently weaker and more dangerous than higher ones, and this is
equally true of any sharded system. However a hierarchically sharded system
like treechains can give users options: higher subchains are safer, but
transactions will expensive. The hierarchy does combine the PoW security of all
subchains together for the thing you can easily combine: timestamping security.
There's a big problem though: holy !@ is the above complex compared to
Bitcoin! Even the "kiddy" version of sharding - my linearization scheme rather
than zk-SNARKS - is probably one or two orders of magnitude more complex than
using the Bitcoin protocol is right now, yet right now a huge % of the
companies in this space seem to have thrown their hands up and used centralized
API providers instead. Actually implementing the above and getting it into the
hands of end-users won't be easy.
On the other hand, decentralization isn't cheap: using PayPal is one or two
orders of magnitude simpler than the Bitcoin protocol.

@_date: 2015-11-26 17:25:56
@_author: Peter Todd 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
...and CRLTV is hard to visually distinguish from CLTV. :(
You know, calling it AGEVERIFY is short and sweet.

@_date: 2015-10-03 16:30:56
@_author: Peter Todd 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
BIP68 and BIP112 collectively define the CHECKSEQUENCEVERIFY semantics,
which can be summarized conceptually as a relative CHECKLOCKTIMEVERIFY.
However, CSV does define behavior for the previously undefined nSequence
field, which is the only "free-form" field we currently have in the
transaction serialization format that can be used for future upgrades -
we should justify this new behavior carefully as it limits our options
in the future. Adding new fields to the serialization format is very
difficult, due to the very broad system-wide impact of the hard-fork
required to do so.
So we need to make the case for two main things:
1) We have applications that need a relative (instead of absolute CLTV)
2) Additionally to RCLTV, we need to implement this via nSequence
To show we need RCLTV BIP112 provides the example "Escrow with Timeout",
which is a need that was brought up by GreenAddress, among others; I
don't think we have an issue there, though getting more examples would
be a good thing. (the CLTV BIP describes seven use cases, and one
additional future use-case)
However I don't think we've done a good job showing why we need to
implement this feature via nSequence. BIP68 describes the new nSequence
semantics, and gives the rational for them as being a
"Consensus-enforced tx replacement" mechanism, with a bidirectional
payment channel as an example of this in action. However, the
bidirectional payment channel concept itself can be easily implemented
with CLTV alone. There is a small drawback in that the initial
transaction could be delayed, reducing the overall time the channel
exists, but the protocol already assumes that transactions can be
reliably confirmed within a day - significantly less than the proposed
30 days duration of the channel. That example alone I don't think
justifies a fairly complex soft-fork that limits future upgrades; we
need more justification.
So, what else can the community come up with? nSequence itself exists
because of a failed feature that turned out to not work as intended;
it'd be a shame to make that kind of mistake again, so let's get our
semantics and use-cases in the BIPs and documented before we deploy.

@_date: 2015-10-06 13:09:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
You should write that up in the BIP, along with a description of how
exactly that would go; I suspect the most obvious way of dong that
upgrade - just increase precision for everyone - would break
compatbility too much to be practical.

@_date: 2015-10-07 14:59:22
@_author: Peter Todd 
@_subject: [bitcoin-dev] Public Debate Challenge 
This is very off-topic for a development mailing list.
Go away.

@_date: 2015-10-08 19:41:20
@_author: Peter Todd 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
There's three that immediately come to mind:
Gregory Maxwell has proposed it as a way of discouraging miners from
reorging chains, by including some of the low-order bits of a previous
block header in nSequence.
A few people have proposed implementing proof-of-stake blocksize voting
with nSequence.
Well, a few low-order bits, if you want to use RCLTV functionality; pure
RCLTV would save a lot more bits.
Indeed! But lets make sure we have a good argument in the BIP.

@_date: 2015-10-08 19:43:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Another issue that came to mind re: CSV review is that there isn't
actually any one pull-req with all the code changes together, making it
hard to be sure what the final effect will be once all three BIPs are
While evaluating stuff separately is often good, I think this is a case
where the overall design needs to be evaluated as a single unit to fully
understand the behavior.

@_date: 2015-10-22 21:22:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Bitcoin-development] Reusable payment codes 
FWIW multi-push OP_RETURN outputs will be standard in v0.12.0:

@_date: 2015-09-01 02:54:42
@_author: Peter Todd 
@_subject: [bitcoin-dev] Short review of previously-proposed exotic 
There's also my "meta sighash" idea of using code to build up the
signature with OP_CODESEPARATOR:
 at lists.sourceforge.net/msg07384.html

@_date: 2015-09-01 03:56:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] ERRATA CORRIGE + Short Theorem 
FWIW I did a quick math proof along those lines awhile back too using
some basic first-year math, again proving that larger miners earn more
money per unit hashing power:
 at lists.sourceforge.net/msg03272.html

@_date: 2015-09-03 13:52:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 100 specification 
Solid NACK on making string parsers part of the consensus critical
codebase. (WTF?)

@_date: 2015-09-03 23:50:45
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 100 specification 
The IEC standard is to use the prefix MiB for 2^20 bytes:

@_date: 2015-09-04 16:31:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP/Draft] BIP Acceptance Process 
It depends on a case-by-case basis.
E.g. for soft-forks miners can do what they want with little ability for
other parties to have a say. For non-consensus-related standards - e.g.
address formats - it's quite possible for a BIP to be "accepted" even if
only a small group of users use the standard. For hard-forks almost
everyone is involved, though who can stop a fork isn't as well defined.
IMO trying to "set up a system" in that kind of environment is silly,
and likely to be a bureaucratic waste of time. Let the market decide, as
has happened previously. If you're idea isn't getting acceptance, do a
better job of convincing the people who need to adopt it that it is a
good idea.
No amount of words on paper will change the fact that we can't force
people to run software they don't want to run. The entire formal part of
the BIP process is simply a convenience so we have clear, short, numbers
that we can refer to when discussing ideas and standards. The rest of
the process - e.g. what Adam Back and others have been referring to when
attempting to dissuade Hearn and Andresen - is by definition always
going to be a fuzzy, situation-specific, and generally undefined
Or put another way, even if you did create your proposed process, the
first time those committees "approved" a BIP that relevant stakeholders
disagreed with, you'd find out pretty quickly that "clear acceptance" of
your 4% sample would fall apart the moment the other 96% realized what a
tiny minority was intending to do. Particularly if it was one of the
inhernet cases where the underlying math means a particular group - like
miners - has the ability to override what another group wants out of

@_date: 2015-09-06 20:43:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] python-bitcoinlib-v0.5.0rc1 - OpenSSL crashes on OSX 
FWIW if you've been experienceing OpenSSL related crashes on OSX or Arch
Linux this release should fix your issues. I don't have any way of
testing this myself, so if I could get some confirmation that this new
release candidate fixes things that'd be really helpful!
Other release notes:
Major fix: Fixed OpenSSL related crashes on OSX and Arch Linux. Big thanks to
everyone who helped fix this!
Breaking API changes:
* Proxy no longer has ``__getattr__`` to support arbitrary methods. Use
  RawProxy or Proxy.call instead. This allows new wrappers to be added safely.
  See docstrings for details.
New features:
* New RPC calls: getbestblockhash, getblockcount, getmininginfo
* Signing and verification of Bitcoin Core compatible messages. (w/ pubkey recovery)
* Tox tests
* Sphinx docs
Notable bugfixes:
* getinfo() now works where disablewallet=1

@_date: 2015-09-16 18:29:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
Incidentally, it'd help if we got some insight into why those branches
are being maintained; what features are in those branches that Bitcoin
Core doesn't have?
I've run into a number of cases where companies were maintaining forks
of Bitcoin Core unnecessarily, where a different, loosely coupled,
architecture could do what they needed to do without including the new
logic in the codebase itself.

@_date: 2015-09-17 22:44:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Hash: SHA512
Why wouldn't that work with p2sh? It can be implemented by a "treat like Coinbase" flag in the UTXO set, set when the output is created.

@_date: 2015-09-18 04:19:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
Hash: SHA512
My schedule is chaotic, but I'll try to attend.

@_date: 2015-09-18 18:47:10
@_author: Peter Todd 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
We still seem to be in a possition where there is fundemental
disagreements about the threat model we should design for, and
ultimately, what we want Bitcoin to be. For instance, yesterday I was on
a blocksize panel, and Valery Vavilov - CEO of the ASIC manufacturer and
miner BitFury - stated that he thought we needed to setup a system of
large, high-bandwidth, high-powered, Bitcoin nodes at institutions such
as universities and large companies to allow the Bitcoin blocksize to be
raised multiple orders of magnitude. (e.g. hundreds of megabytes, or
even multiple gigabytes) In discussion with him he seemed to expect that
we'd have just a few hundred Bitcoin nodes at most, with SPV being the
standard way of using Bitcoin.
While to many of us that sounds crazy, if you're threat model assumes
Bitcoin is a legal/regulated service provided by a highly trusted mining
community it's a reasonable design. Mike Hearn recently posted his
threat model, which specifically argues we should assume governments are
not a threat. (and Hearn has previously argued that the design of
Bitcoin assumes a majority of miners are "honest" rather than merely
economically rational) Similarly Gavin Andresen was also on that panel,
and stated that he believes the idea that Bitcoin has O(n^2) scaling is
wrong, implying he doesn't think a large % of the Bitcoin user base will
continue to run fully validating nodes. (note that there are other
possibilities he could be referring to here, although again with
different security assumptions and/or unproven tech)
The main objection I raised during the committer/contributor discussions
to the idea of a "short term bump" was messaging. I think it's fair to
say that nearly all the support for a small blocksize increase stemmed
from the (perceived) need to give Bitcoin users and Bitcoin
infrastructure some more time to adapt to a world where the blocksize
does not grow sufficiently to meet demand, resulting in higher
transaction fees and the practical requirement to use the Bitcoin
blockchain more efficiently. (or of course the development of genuinely
scalable blockchain technology) With that in mind, it's important that
we properly communicate that fact, or as Hearn replied, we'll run into
the same problem all over again in a few years, but with even less
safety margin in the system.
My second objection was one of science. Any bump should be accompanied
by some kind of model describing scientifically what we were trying to
achieve and where the numbers chosen came from. For instance, Pieter
Wuille's BIP103 proposes 17% per year based on a bandwidth growth model,
the assumption that bandwidth is the bottleneck we're trying to keep
constant, and the design criteria to keep centralization roughly
constant. (all else being equal) Sure there's lots of potential flaws in
that proposal, but the _message_ that we're basing it on science rather
than political "horse-trading" is very important.
As for the disagreements, it's quite likely that we can't come to
genuine consensus in the fact of those fundemental disagreements about
what Bitcoin should be. I don't have any good way to resolve that, and
I'm open to suggestions!

@_date: 2015-09-24 18:41:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Proposal] New "sendheaders" p2p message 
Hash: SHA512
You can enable the behaviour based on advertised p2p network version.

@_date: 2015-09-24 19:27:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Proposal] New "sendheaders" p2p message 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Block headers are so small - 80 bytes - that it may be reasonable to just stop using the inv mechanism for them in favor of always sending headers. IIRC a inv is 32 bytes of digest and another four bytes or something of the inv string itself - that's already nearly half of the header.
Meanwhile reducing the amount of state in the protocol does have some value, and decreasing overall latency for headers to get around the network certainely isnt a bad thing.

@_date: 2015-09-27 10:51:41
@_author: Peter Todd 
@_subject: [bitcoin-dev] python-bitcoinlib-v0.5.0rc1 - OpenSSL crashes on 
No issues have been reported with the release candidate, so I've
released v0.5.0 officially pretty much as-is:

@_date: 2015-09-27 14:50:31
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
It's time to deploy BIP65 CHECKLOCKTIMEVERIFY.
I've backported the CLTV op-code and a IsSuperMajority() soft-fork to
the v0.10 and v0.11 branches, pull-reqs  and  respectively. A
pull-req for git HEAD for the soft-fork deployment has been open since
June 28th,  - the opcode implementation itself was merged two
months ago.
We should release a v0.10.3 and v0.11.1 with CLTV and get the ball
rolling on miner adoption. We have consensus that we need CLTV, we have
a well tested implementation, and we have a well-tested deployment
mechanism. We also don't need to wait for other soft-fork proposals to
catch up - starting the CLTV deployment process isn't going to delay
future soft-forks, or for that matter, hard-forks.
I think it's possible to safely get CLTV live on mainnet before the end
of the year. It's time we get this over with and done.
Detailed Rational
1) There is a clear need for CLTV
Escrow and payment channels both benefit greatly from CLTV. In
particular, payment channel implementations are made significantly
simpler with CLTV, as well as more secure by removing the malleability
Why are payment channels important? There's a lot of BTC out there
vulnerable to theft that doesn't have to be. For example, just the other
day I was talking with Nick Sullivan about ChangeTip's vulnerability to
theft, as well as regulatory uncertainty about whether or not they're a
custodian of their users' funds. With payment channels ChangeTip would
only be able to spend as much of a deposit as a user had spent, keeping
the rest safe from theft. Similarly, in the other direction - ChangeTip
to their users - in many cases it is feasible to also use payment
channels to immediately give users control of their funds as they
receive them, again protecting users and helping make the case that
they're not a custodian. In the future I'm sure we'll see fancy
bi-directional payment channels serving this role, but lets not let
perfect be the enemy of good.
2) We have consensus on the semantics of the CLTV opcode
Pull-req  - the implementation of the opcode itself - was merged
nearly three months ago after significant peer review and discussion.
Part of that review process included myself(1) and mruddy(2) writing
actual demos of CLTV. The chance of the CLTV semantics changing now is
3) We have consensus that Bitcoin should adopt CLTV
The broad peer review and discussion that got  merged is a clear
sign that we expect CLTV to be eventually adopted. The question isn't if
CLTV should be added to the Bitcoin protocol, but rather when.
4) The CLTV opcode and IsSuperMajority() deployment code has been
   thoroughly tested and reviewed
The opcode implementation is very simple, yet got significant review,
and it has solid test coverage by a suite of tx-(in)valid.json tests.
The tests themselves have been reviewed by others, resulting in Esteban
Ordano's pull-req  by Esteban Ordano which added a few more cases.
As for the deployment code, both the actual IsSuperMajority() deployment
code and associated unit-tests tests were copied nearly line-by-line
from the succesful BIP66. I did this deliberately to make all the peer
review and testing of the deployment mechanism used in BIP66 be equally
valid for CLTV.
5) We can safely deploy CLTV with IsSuperMajority()
We've done two soft-forks so far with the IsSuperMajority() mechanism,
BIP34 and BIP66. In both cases the IsSuperMajority() mechanism itself
worked flawlessly. As is well-known BIP66 in combination with a large %
of the hashing power running non-validating "SPV" mining operations did
lead to a temporary fork, however the root cause of this issue is
unavoidable and not unique to IsSuperMajority() soft-forks.
Pragmatically speaking, now that miners are well aware of the issue it
will be easy for them to avoid a repeat of that fork by simply adding
IsSuperMajority() rules to their "SPV" mining code. Equally turning off
SPV mining (temporarily) is perfectly feasable.
6) We have the necessary consensus to deploy CLTV via IsSuperMajority()
The various "nVersion bits" proposals - which I am a co-author of - have
the primary advantage of being able to cleanly deal with the case where
a soft-fork fails to get adopted. However, we do have broad consensus,
including across all sides of the blocksize debate, that CLTV should be
adopted. The risk of CLTV failing to get miner adoption, and thus
blocking other soft-forks, is very low.
7) Using IsSuperMajority() to deploy CLTV doesn't limit or delay other upgrades
It _is_ possible for multiple IsSuperMajority() soft-forks to coexist,
in the sense that if one soft-fork is "in flight" that doesn't prevent
another soft-fork from also being deployed simultaneously.
In particular, if we deploy CLTV via IsSuperMajority() that does _not_
impact the adoption schedule for other future soft-forks, including
soft-forks using a future nVersion bits deployment mechanism.
For instance, suppose we start deployment of CLTV right now with
nVersion=4 blocks. In three months we have 25% miner support, and start
deploying CHECKSEQUENCEVERIFY with nVersion=5 blocks. For miners
supporting only OP_CLTV, the nVersion=5 blocks still trigger OP_CLTV;
miners creating nVersion=5 blocks are simply stating that they support
both soft-forks. Equally, if in three months we finish a nVersion bits
proposal, those miners will be advertising nVersion=(1 << 29) blocks,
which also advertise OP_CLTV support.
8) BIP101 miners have not proved to be a problem for CLTV deployment
While there was concern that BIP101's use of nVersion would cause
issues with a IsSuperMajority() softfork, the % of blocks with BIP101
nVersion's never reached more than 1%, and currently is hovering at
around 0.1%
As Gavin Andresen has stated that he is happy to add CLTV to BIP101, and
thus Bitcoin XT, I believe we can expect those miners to safely support
CLTV well before soft-fork enforcement happens. Secondly, the 95%
enforcement threshold means we can tolerate a fairly high % of miners
running pre-CLTV BIP101 implementations without fatal effects in the
unlikely event that those miners don't upgrade.
9) Doing another IsSuperMajority() soft-fork doesn't "burn a bit"
This is a common myth! All nVersion bits proposals involve permanently
setting a high-order bit to 1, which results in nVersion >= all prior
IsSuperMajority() soft-forks. In short, we can do a nearly unlimited
number of IsSuperMajority() soft-forks without affecting future nVersion
bits soft-forks at all.
10) Waiting for nVersion bits and CHECKSEQUENCEVERIFY will significantly
    delay deployment of CLTV
It's been proposed multiple times that we wait until we can do a single
soft-fork with CSV using the nVersion bits mechanism.
nVersion bits doesn't even have an implementation yet, nor has solid
consensus been reached on the exact semantics of how nVersion bits
should work. The stateful nature of nVersion bits soft-forks requires a
significant amount of new code compared to IsSuperMajority() soft-forks,
which in turn will require a significant amount of testing. (again I'll
point out I'm a co-author to all the nVersion bits proposals)
CSV has an implementation, but there is still debate going on about what
the exact semantics of it should be. Getting the semantics right is
especially important as part of CSV includes changing the meaning of
nSequence, restricting future uses of that field. There have been many
proposals to use nSequence, e.g. for proof-of-stake blocksize voting,
and it has the unique capability of being a field that is both unused,
and signed by scriptSigs. We shouldn't take potentially restricting
future uses of it lightly.
CSV is also significantly more complex and invasive than CLTV in terms
of code changes. A large % of the mining power is running forks
of Bitcoin Core with custom changes - modifying these forks with new
features is a labor intensive and slow process.
If CLTV is ready now, why delay it - potentially for 6-12 months - for
other proposals to catch up? Equally if they do catch up, great! As
explained above an in-flight CLTV soft-fork won't delay future upgrades.
11) Even if CLTV is broken/obsoleted there is very little carrying cost
    to having it
Suppose we decide in two years that CLTV was botched and we need to fix
it. What's the "carrying cost" of having implemented CLTV in the first
We'll have used up one of our ten soft-forkable NOPs, but if we ever
"run out" it's easy to use extension NOPs(3). Similarly, future script
improvements like OP_MAST - or even a hard-fork - can easily expand the
range of NOPs to the point where this is a non-issue.
If you don't use OP_CLTV in your scripts there is zero effect on your
transactions; we're not limiting future improvements to Bitcoin in any
way other than using up a NOP by implementing CLTV.
1) 2) 3) 4)

@_date: 2015-09-27 16:27:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I certainly could, though there's good reasons to move to v0.10.x; I'd
want to first hear from miners as to why they're still on v0.9.x

@_date: 2015-09-28 09:21:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Hmm? You didn't quote any of my email, so I'll remind you what I did say
we had consensus about:
    2) We have consensus on the semantics of the CLTV opcode
    3) We have consensus that Bitcoin should adopt CLTV
    The broad peer review and discussion that got  merged is a clear
    sign that we expect CLTV to be eventually adopted.  __The question isn't
    if CLTV should be added to the Bitcoin protocol, but rather when.__
(emphasis mine)
Both those statements of consensus are *not* about how CLTV is to be
deployed. I did discuss deployment later:
    6) We have the __necessary consensus__ to deploy CLTV via IsSuperMajority()
    The various "nVersion bits" proposals - which I am a co-author of - have
    the primary advantage of being able to cleanly deal with the case where
    a soft-fork fails to get adopted. However, we do have broad consensus,
    including across all sides of the blocksize debate, that CLTV should be
    adopted. __The risk of CLTV failing to get miner adoption, and thus
    blocking other soft-forks, is very low.__
I probably could have worded this section a bit more clearly; when I say
"necessary consensus" I'm referring to the consensus required for a
soft-fork deployment. At minimum a simple majority of hashing power -
your approval isn't required.
For a safe soft-fork, we'd like a super majority of miners to be on
board. For a IsSuperMajority() soft-fork - as opposed to nVersion bits -
we also need the probability of the soft-fork being rejected to be very
low. To achieve that, having consensus that CLTV is a good idea is the
best situation to be in. But that's not to say that a few dissenting
voices should be seen as a blocker to progress - rather is just makes
the deployment a bit more risky, being a sign that the consensus may
change in the future, with the soft-fork being later rejected. For
example strong objections by a respected Bitcoin developer who has made
significant contributions to the consensus codebase and protocol
development would be a strong sign that a IsSuperMajority() soft-fork
might fail, and deployment via nVersion bits is probably a better
approach. Fortunately we're not in that situation.
Hard-forks are a very different situation, with significantly more need
for very broad consensus, but that's been well discussed elsewhere.
I have three questions to you:
1) Do you agree that CLTV should be added to the Bitcoin protocol?
Ignoring the question how exactly it is added, hard-fork or soft-fork.
2) Will you add a IsSuperMajority() CLTV soft-fork to Bitcoin XT if it
   is added to Bitcoin Core?
If you refuse to do this the risk of the soft-fork is increased a bit,
although miner support for XT has remained extremely low, and the 95%
switch-over threshold has a significant margin for error. (there's a 75%
threshold to consider as well, however as XT has adopted my pull-req
 - Discourage NOPs reserved for soft-fork upgrades - those miners
will only produce valid blocks under CLTV rules)
3) Will you add soft-fork detection to bitcoinj, to allow SPV clients to
   detect advertised soft-forks and correctly handle them?
Notably, if you do this your objections against soft-forks will be met,
as the behavior of a SPV client with soft-fork detection during a
soft-fork will be identical to that client during a hard-fork. In
particular, the SPV client will correct reject invalid blocks, and
continue to follow only the longest valid chain. (modulo unadvertised
forks of course, an inherently unavoidable problem with the SPV security
model) Secondly, that code should also detect forks it doesn't know
about - as is done in Bitcoin Core already - and warn the user.

@_date: 2015-09-28 09:28:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Do you have such a document for your BIP101? That would save me a lot of
time, and the need for that kind of document is significantly higher
with BIP101 anyway.
Re: mempool, CLTV-using transactions are non-standard and are not
relayed in all Bitcoin Core releases. (see my pull-req  -
Discourage NOPs reserved for soft-fork upgrades - for why) I believe
that meets your suggestion of deploying mempool-only first.

@_date: 2015-09-28 10:14:41
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I don't remember what you are referring to at all. Was this a private
email? IRC chat? In person discussion?
"unless you were lying"
Please keep the discussion on the development mailing list civil and
Actually, that sounds like the kind of thing that should be in the
bitcoin.org developer documentation; IMO for the audience of competent
full node developers the comments in the pull-req code itself and
associated discussion covers everything they need to know. Without that
background though, this is something that'd fit well in the category of
general education to get new developers to a good state of competence.
As for wallets specifically, that's pretty much all covered by SPV
wallets based on bitcoinj, and Mike Hearn has different views on the
subject which need to be resolved first.

@_date: 2015-09-28 10:29:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Could you elaborate on what exatly you mean by this.
SPV wallets can't detect hard-forks, so in both cases you will have
invalid blocks be accepted by SPV clients; there's no deployment
scenario for either hard or soft forks that guarantees all miners adopt
a fork.
What does prevent invalid blocks being accepted by SPV clients is
checking the block nVersion field and applying forking logic. Of course,
that only works for advertised forks, but again, that's equally true for
soft and hard forks.
Again, in neither case do you get the "correct outcome" of SPV clients
accepting no invalid blocks without nVersion field checking.
However, in the hard-fork case, because the non-adopting miners reject
the fork, they build a chain which could be used to attack SPV clients
with false confirmations by sybil attacking those clients. In the
soft-fork case, the non-adopting miners keep accepting the longer chain
built by the adopting miners, preventing the creation of a chain that
could be used to attack SPV miners.
BTW, what's the other widely used SPV implementation you're thinking of?
I'll contact them directly and help them implement proper SPV fork
protections if they haven't already; if bitcoinj is unwilling to do this
at least we could have an alternative implementation that does.
(equally, if anyone wants to fork bitcoinj and correct this flaw I'd be
happy to help advise)

@_date: 2015-09-28 10:43:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Ok, so again, if that's your security criteria, what's the issue with
soft-forks? With soft-forks, the result of a SPV wallet following the
highest work chain is the same: eventually invalid blocks are reorged
However, because soft-forks make it less likely that a long invalid
chain will be generated, an attacker sybil attacking your SPV wallet has
a much harder time tricking it into accepting a transaction. (they might
get one or two confirmations, rather than dozens)
What's the scenario where soft-forks are worse than hard-forks from a
SPV wallet's perspective?

@_date: 2015-09-28 11:05:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I have read your article. In fact we reviewed it at a NY BitDevs meetup
that I attended.
Can you explain exactly how you think wallets will "know" how to ignore
the invalid chain?
With an advertised soft-fork, e.g. the IsSuperMajority() mechanism,
ignoring the invalid chain is easy: use nVersion to detect invalid
blocks when you know what soft-forks are coming up, and if presented
with an unknown - but advertised - soft-fork at minimum loudly warn the
user. In the case of a hard-fork identical logic can be used. (BIP101
being an example of a hard-fork triggered in a way that can be detected
by SPV clients, both explicitly (BIP101 specific) and implicitly
(general unknown block nVersion warnings))
How so? Miners can always choose to create invalid blocks, thus
attacking SPV wallets; my statement with regard to pull-req  comes
from a risk-based approach, knowing that every invalid block is
expensive and the new concern created by a soft-fork is whether or not
miners will create them accidentally; miners can always create invalid
blocks delibrately.
That's incorrect: Miners bypassing IsStandard() risk creating invalid
blocks in the event of a soft-fork. Equally, we design soft-forks to
take advantage of this.
We seem to be in strong disagreement about which option has "clear,
explicit downsides"

@_date: 2016-04-01 05:00:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] AsicBoost 
What steps are you going to take to make sure that this improvement is
available to all ASIC designers/mfgs on a equal opportunity basis?
The fact that you've chosen to patent this improvement could be a
centralization concern depending on the licensing model used. For example, one
could imagine a licensing model that gave one manufacture exclusive rights.

@_date: 2016-08-05 12:54:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Progress On Hardfork Proposals Following The Segwit 
Repost by request from my blog, apologies for the somewhat screwy formatting!
layout: post
title:  "Progress On Hardfork Proposals Following The Segwit Blocksize Increase"
date:   2016-08-05
- bitcoin
- hardfork
- segwit
With segwit getting close to its initial testnet release in Bitcoin Core
v0.13.0 - expected to be followed soon by a mainnet release in Bitcoin Core
v0.13.1 - I thought it'd be a good idea to go over work being done on a
potential hard-fork to follow it, should the Bitcoin community decide to accept
the segwit proposal.
First of all, to recap, in addition to many other improvements such as fixing
transaction malleability, fixing the large transaction signature verification
DoS attack, providing a better way to upgrade the scripting system in the
future, etc. segwit increases the maximum blocksize to 4MB. However, because
it's a soft-fork - a backwards compatible change to the protocol - only witness
(signature) data can take advantage of this blocksize increase; non-witness
data is still limited to 1MB total per block. With current transaction patterns
it's expected that blocks post-segwit won't use all 4MB of serialized data
allowed by the post-segwit maximum blocksize limit.
Secondly, there's two potential upgrades to the Bitcoin protocol that will
further reduce the amount of witness data most transactions need: [Schnorr
signatures]( and [BLS aggregate signatures](
Basically, both these improvements allow multiple signatures to be combined,
the former on a per-transaction level, and the latter on a per-block level.
[Last February](
some of the mining community and some of the developer community got together to discuss potential
hard-forks, with the aim of coming up with a reasonable proposal to take to the
wider community for further discussion and consensus building. Let's look at
where that effort has lead.
 Ethereum: Lessons to be learned
But first, Ethereum. Or as some have quipped, the Etherea:
The Battle for Etherea.  Samson Mow ( July 31, 2016
If you've been following the crypto-currency space at all recently, you
probably know that the Ethereum community has split in two following a very
controversial hard-fork to the Ethereum protocol, To make a long story short, a
unintended feature in a smart-contract called "The DAO" was exploited by a
as-yet-unknown individual to drain around $50 million worth of the Ethereum
currency from the contract. While "white-hat attackers" did manage to recover a
majority of the funds in the DAO, a hard-fork was proposed to rewrite the
Ethereum ledger to recover all funds - an action that many, [including myself](/2016/ethereum-dao-bailout-vote),
have described as a bailout.
The result has been a big mess. This isn't the place to talk about all the
drama that's followed in depth, but I think it's fair to say that the Ethereum
community found out the hard way that just because you give a new protocol the
same name as an existing protocol, that doesn't force everyone to use it. As of
writing, what a month ago was called "Ethereum" - Ethereum Classic - has 20% of
the hashing power as the bailout chain, and peaked only two or three days ago
at around 30%. As for market cap, while the combined total for the two chains
is similar to the one chain pre-fork, this is likely misleading: there's
probably a lot of coins on both chains that aren't actually accessible and
don't represent liquid assets on the market. Instead, there's a good chance a
significant amount of value has been lost.
In particular, both chains have suffered significantly from transaction replay
issues. Basically, due to the way the Ethereum protocol is designed - in
particular the fact that Ethereum isn't based on a UTXO model - when the
Ethereum chain split transactions on one chain were very often valid on another
chain. Both attacks and accidents can lead to transactions from one chain
ending up broadcast to others, leading to unintentional spends. This wasn't an
unexpected problem:
. we knew it would happen weeks before launch, we didn't want to implement replay-protection b.c. of implementation complexity Vlad Zamfir ( July 31, 2016
...and it's lead to costly losses. Among others Coinbase has lost [an unknown amount of
funds]( that they may [have to buy back]( Even worse, BTC-e [lost pretty much their entire balance](
of original Ethereum coins - apparently becoming insolvent - and instead of
returning customer funds, they decided to [declare the original Ethereum chain a scam]( instead.
A particularly scary thing about this kind of problem is that it can lead to
artificial demand for a chain that would otherwise die: for all we know
Coinbase has been scrambling behind the scenes to buy replacement ether to make
up for the ether that it lost due to replay issues.
More generally, the fact that the community split shows the difficulty - and
unpredictability - of achieving consensus, maintaining consensus, and
measuring consensus. For instance, while the Ethereum community did do a coin
vote [as I suggested](/2016/ethereum-dao-bailout-vote), turnout was extremely
low - around 5% - with a significant minority in opposition (and note that
exchanges' coins were blacklisted from the vote due to technical reasons).
Additionally, the miner vote also had low turnout, and again, significant
minority opposition.
With regard to [drama]( resulting
from a coin split, something I think not many in the technical community had
considered, is that exchanges can have perverse incentives to encourage it. The
split resulted in significant trading volume on the pre-fork, status quo,
Ethereum chain, which of course is very profitable for exchanges. The second
exchange to list the status-quo chain was Poloniex, who have over 100
Bitcoin-denominated markets for a very wide variety of niche currencies - their
business is normally niche currencies that don't necessarily have wide appeal.
Finally, keep in mind that while this has been bad for Ethereum, it'd be even
worse for Bitcoin: unlike Ethereum, Bitcoin actually has non-trivial usage in
commerce, by users who aren't necessarily keeping up to date with the latest
drama^H^H^H^H^H news. We need to proceed carefully with any
non-backwards-compatible changes if we're to keep those users informed, and
protect them from sending and receiving coins on chains that they didn't mean
 Splitting Safely
So how can we split safely? Luke Dashjr has written both a
[BIP]( and
[preliminary code](
to do a combination of a hard-fork, and a soft-fork.
This isn't a new idea, in fact Luke [posted it](
to the bitcoin-dev mailing list last February, and it's been known as an option
for years prior; I personally mentioned it [on this blog](/2016/forced-soft-forks) last January.
The idea is basically that we do a hard-fork - an incompatible rule change - by
"wrapping" it in a soft-fork so that all nodes are forced to choose one chain
or the other. The new soft-forked rule-set is simple: no transactions are
allowed at all. Assuming that a majority of hashing power chooses to adopt the
fork, nodes that haven't made a decision are essentially 51% attacked and will
follow an empty chain, unable to make any transactions at all.
For those who choose not to adopt the hard-fork, they need to themselves do a
hard-fork to continue transacting. This can be as simple as blacklisting the
block where the two sides diverged, or something more complex like a
proof-of-work change.
On the plus side, Luke's proposal maximizes safety in many respects: so long as
a majority of hashing power adopts the fork no-one will accidentally accept
funds from a chain that they didn't intend too.
 Giving Everyone A Voice
It's notable that what Luke calls a "soft-hardfork" has also been called a
"forced soft-fork" by myself, as well as an "evil fork" by many others - what
name you give it is a matter of perspective. From a technical point of view,
the idea is a 51% attack against those who choose not to support the new
protocol; it's notable that when I pointed this out to some miners they were
very concerned about the precedent this could set if done badly.
Interestingly, due to implementation details Ethereum hard-fork was similar to
Luke's suggestion: pre-fork Ethereum clients would generally fail to start due
to an implementation flaw - in most cases - so everyone was forced to get new
software. Yet, Ethereum still split into two economically distinct coins.
This shows that attempting to kill an unwanted side of a split chain via 51%
attack isn't a panacea: people can and will choose to use the chain they want
to if there's controversy. So we'd be wise to try to achieve broad community
consensus first.
Interestingly, Tom Harding's [Hard fork opt-out bits](
proposal can also be used to measure consent. Basically, as an anti-replay
mechanism, wallets could begin (un)setting a nSequence bit in transaction
inputs that a hard-fork would make _invalid_, while simultaneously a soft-fork
would make (un)setting a different bit invalid already; the hard-fork would
make that second bit _valid_ (users of nLockTime would (un)set neither bit,
making their transactions valid on both chains). This allows us to easily
measure readiness for a fork by looking at what percentage of transactions are
setting the anti-replay bit correctly - a sign that their running software that
is ready for a future hard-fork.
Secondly, I've been working on coming up with more concrete mechanisms based on
signaling/voting proportional to coins held, in particular, out-of-band
mechanisms based on signed messages and probabilistic sampling that could
potentially offer better privacy and censorship resistance, and give "hodlers"
who aren't necessarily doing frequent transactions a voice as well. My recent
work on [making TXO commitments more practical](/2016/delayed-txo-commitments)
is part of that effort.
 UTXO Size
Segwit's witness-data-discount has the important effect of discouraging the
creation of new UTXOs, in favor of spending existing ones, hopefully resulting
in [reduced UTXO set growth](
As a full copy of the UTXO set is (currently) a mandatory requirement for
running a full node, even with pruning, it's important that we keep UTXO growth
rates sustainable.
Matt Corallo has been doing work on finding better ways to properly account for
the cost to the network as a whole of UTXO creation, and he has told me he'll
be publishing that work soon. In addition, I've been working on a longer-term
solution in the form of [TXO commitments](/2016/delayed-txo-commitments), which
hopefully can eliminate the problem entirely, by allowing UTXO's to be
archived, shifting the burden of storing them to wallets rather than all
Bitcoin nodes. Additionally, Bram Cohen has been [working on](
making the necessary data structures for TXO commitments faster in of
themselves by optimizing cache access patterns.
 Block Propagation Latency
A significant concern with any blocksize increase - including segwit - is that
the higher bandwidth requirements will encourage centralization of hashing
power due to the disproportionate effect higher latency has on stale rates
between large and small miners. Matt Corallo has done a significant amount of
work recently on mitigating this effect with his [compact blocks]( - to be
released in v0.13.0 - and his next-gen block relay network
Additionally, I've been [doing research]( to better
understanding the limitations of these approaches in adversarial,
semi-adversarial, and "uncaring" scenarios.
 Anti-Replay
I mentioned Tom Harding's work, above; I'll also mention that Gregory Maxwell
proposed a generic - and very robust - solution to anti-replay: have
transactions commit to a recent but not too recent (older than ~100 blocks or so) block hash.
While this has some potential downsides in a large reorg - transactions may
become permanently invalid due to differing block hashes - so long as the block
hashes committed too aren't too recently the idea does very robustly fix replay
attacks across chains, in a way that's completely generic no matter how many
forks happen. For example, a reasonable way to deploy would be to have wallet's
refuse to make transactions for the first day or two after a hard-fork, and
then use a post-fork blockhash in all transactions to ensure they can't be
replayed on an unwanted chain.

@_date: 2016-08-08 14:53:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] Hiding entire content of on-chain transactions 
The fact that miners verify transactions is just an optimisation:
    Preventing double-spending however is a fundemental requirement of Bitcoin, and
this proposal does prevent double-spending perfectly well (although there may
be better ways to do it).
The OP's proposal sounds quite similar to my earlier one along similar lines:

@_date: 2016-08-08 19:21:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] *Changing* the blocksize limit 
The largest output on testnet is a bit under 1MB, and encodes a certain
well-known love song...
In many circumstances(1) miners have an incentive to create larger blocks that
take their competitors longer to receive and validate, so protocol-level block
limits need to take all these potential DoS vectors into account; serialized
size is one of the most fundemental things that needs to be limited.
As mentioned above, and explained in detail in my recent blog post(1),
restrictions are needed to keep a level playing field between all miners.
1)

@_date: 2016-08-16 12:43:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
SegWit txids aren't malleable, but segwit transactions as a whole still are.
For instance, I could mess with a segwit transaction by replacing part of the
witness that is used as an argument to an OP_IF with a much larger push,
potentially making the transaction larger, thus making it not get mined due to
the higher fee. There are also potential legal issues if someone replaces a
push with data where posession in your jurisdiction is illegal.
Having said that, a better approach may be a separate CHECKBOOLVERIFY opcode
that fails unless the top item on the stack is a minimally encoded true or
false value, to allow script writers to opt into this behavior; it's not always

@_date: 2016-08-16 17:14:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] Hardware Wallet Standard 
I'm not aware of any ECC-enabled smart-cards that can sign the specific curve
that Bitcoin uses, not to mention the fact that those smartcards generally only
speak higher level protocols than raw signature generation, precluding the
signing of bitcoin transactions.
The other serious problem - and this is a problem with smartcards in general
anyway - is that without Bitcoin-specific logic you're just signing blindly; we
recently saw the problems with that with the Bitfinex/BitGo hack. And even
then, without a screen most of the hardware wallets in are still just signing
blindly, with at best hard-to-use limits on maximum funds moved
per-transaction. Also note how even hardware wallets with a screen, like
Trezor, aren't yet able to authenticate who you are paying.

@_date: 2016-08-16 19:30:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
At least some types of malleability are less harmful than others: changing a
few bits with some weird ECC transformation isn't as likely to cause problems
as being able to append arbitrary data to a transaction's input script. And of
course, we do prevent the latter with the cleanstack rule - consensus enforced
in segwit.

@_date: 2016-08-17 20:00:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
An important part of the design of segwit is that resource constained devices
doing lite-client verification don't need to get witness data at all to verify
lite-client merkle-path proofs.
Remember that lite-clients can't verify anything useful in witnesses anyway, so
for them to have witness data is useless (unless they're doing some kind of
embedded consensus protocol with data published in witnesses, but few people
here care about that use-case).

@_date: 2016-08-24 01:46:34
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
Bitcoin-based honeypots incentivise intruders into revealing the fact they have
broken into a server by allowing them to claim a reward based on secret
information obtained during the intrusion. Spending a bitcoin can only be done
by publishing data to a public place - the Bitcoin blockchain - allowing
detection of the intrusion.
The simplest way to achieve this is with one private key per server, with each
server associated with one transaction output spendable by that key. However
this isn't capital efficient if you have multiple servers to protect: if we
have N servers and P bitcoins that we can afford to lose in the compromise, one
key per server gives the intruder only N/P incentive.
Previously Piete Wuille proposed(1) tree signatures for honeypots, with a
single txout protected by a 1-N tree of keys, with each server assigned a
specific key. Unfortunately though, tree signatures aren't yet implemented in
the Bitcoin protocol.
However with a 2-of-2 multisig and the SIGHASH_SINGLE feature we can implement
this functionality with the existing Bitcoin protocol using the following
    2   2 CHECKMULTISIG
The honeypot secret key is shared among all N servers, and left on them. The
distriminator secret key meanwhile is kept secret, however for each server a
unique signature is created with SIGHASH_SINGLE, paying a token amount to a
notification address. For each individual server a pre-signed signature created
with the distriminator secret key is then left on the associated server along
with the honeypot secret key.
Recall the SIGHASH_SINGLE flag means that the signature only signs a single
transaction input and transaction output; the transaction is allowed to have
additional inputs and outputs added. This allows the thief to use the honeypot
key to construct a claim transaction with an additional output added that pays
an address that they own with the rest of the funds.
Equally, we could also use SIGHASH_NONE, with the per-server discriminator
being the K value used in the pre-signed transaction.
Note that Jeff Coleman deserves credit as co-inventor of all the above.
Censorship Resistance
A potential disadvantage of using non-standard SIGHASH flags is that the
transactions involved are somewhat unusual, and may be flagged by
risk analysis at exchanges and the like, a threat to the fungibility of the
We can improve on the above concept from Todd/Coleman by using a pre-signed
standard transaction instead. The pre-signed transaction spends the honeypot
txout to two addresses, a per-server canary address, and a change address. The
private key associated with the change addres is also left on the server, and
the intruder can then spend that change output to finally collect their reward.
To any external observer the result looks like two normal transactions created
in the process of someone with a standard wallet sending a small amount of
funds to an address, followed by sending a larger amount.
A subtlety in the the two transactions concept is that the intruder doesn't
have the necessary private keys to modify the first transaction, which means
that the honeypot owner can respond to the compromise by doublespending that
transaction, potentially recovering the honeypot while still learning about the
compromise. While this is possible with all honeypots, if the first transaction
is signed with the opt-in RBF flags, and CPFP-aware transaction replacement is
not implemented by miners, the mechanics are particularly disadvantageous to
the intruder, as the honeypot owner only needs to increase the first
transaction's fee slightly to have a high chance of recovering their funds.
With CPFP-aware transaction replacement the intruder could in-turn respond with
a high-fee CPFP second transaction, but currently no such implementation is
Scorched Earth
We can use the "scorched earth" concept to improve the credibility of the
honeypot reward by making it costly for the honeypot owner to doublespend. Here
a second version of the honeypot pre-signed transaction would also be provided
which sepnds the entirety of the honeypot output to fees, and additionally
spends a second output to fees. An economically rational intruder will publish
the first version, which maximizes the funds they get out of the honeypot. If
the owner tries to dishonestly doublespend, they can respond by publishing the
"scorched earth" transaction, encouraging the honeypot owner's honesty and
making CPFP-aware transaction replacement irrelevant.
Of course, miner centralization adds complexity to the above: in many instances
honeypot owners and/or intruders will be able to recover funds from altruistic
miners. Equally, the additional complexity may discourage intruders from making
use of the honeypot entirely.
Note that as an implementation consideration CHECKSEQUENCEVERIFY can be used to
ensure the honeypot output can only be spent with transaction replacement
enabled, as CSV requires nSequence to be set in specific ways in any transation
spending the output.
1)

@_date: 2016-08-24 19:18:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
I think it's very related; to be absolutely clear the idea of a Bitcoin
honeypot is 100% not my idea! Also, if anyone else had previously invented the
techniques I (and Jeff Coleman) invented, I'd love to hear about it so I can
give appropriate credit.

@_date: 2016-08-24 19:22:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
Remember that it's _always_ possible for the owner to redeem the coins at any
time, and there's no way to prevent that.
The incentive for the intruder to collect the honeypot in a timely manner is
simple: once they've broken in, the moment the honeypot owner learns about the
compromise they have every reason to attempt to recover the funds, so the
intruder needs to act as fast as possible to maximize their chances of being

@_date: 2016-08-28 04:42:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
Also, having a overt tripwire doesn't preclude having covert tripwires as well.
In any case, this all deserves a Standard? to make sure intruders know where to look to find the funds. Maybe /var/honeypot...

@_date: 2016-08-28 04:37:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
Not at all. Most servers have zero reason to have any Bitcoin's accessible via them, so the presence of BTC privkeys is a gigantic red flag that they are part of a honeypot.
Re-read my last section on the "scorched earth" disincentive to doublespend the intruder.

@_date: 2016-08-31 20:01:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
============================== START ==============================
Ah, yeah, I think you have a point re: naming - this isn't quite the
traditional honeypot, as we uniquely have the ability to give the attackers a
reward in a way where it's ok for the intruder to know that they've been
detected; with traditional non-monetary honeypots it's quite difficult to come
up with a scenario where it's ok for an intruder to gain something from the
intrusion, so you're forced to use deception instead.
Perhaps a better term for this technique would be a "compromise canary"? Or
"intruder bait"? After all, in wildlife animal research it's common to use bait
as a way of attracting targets to discover that they exist (e.g. w/ wildlife
cameras), even when you have no intention of doing any harm to the animal.

@_date: 2016-02-02 12:03:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Draft] Allow zero value OP_RETURN in Payment 
I'll point out that getting a BIP for a feature is *not* a hard
requirement for deployment. I'd encourage you to go write up your BIP
document, give it a non-numerical name for ease of reference, and lobby
wallet vendors to implement it.
While I'll refrain from commenting on whether or not I think the feature
itself is a good idea, I really don't want people to get the impression
that we're gatekeepers for how people choose use Bitcoin.

@_date: 2016-02-02 12:07:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Draft] Allow zero value OP_RETURN in Payment 
Note that because the dust limit is ignored completely for OP_RETURN
outputs, you can work around this by setting the OP_RETURN outputs to 1
satoshi instead.

@_date: 2016-02-02 12:38:49
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Draft] Allow zero value OP_RETURN in Payment 
Everyone is on moderation only in this mailing list, myself included.

@_date: 2016-02-02 14:12:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Draft] Allow zero value OP_RETURN in Payment 
Keep in mind that actual human beings need to hit the approve button on
your posts; quite likely Luke happened to respond when those humans were
available, and you didn't. I personally had to do the exact same thing
the other day with one of my posts.
Moderation is an unfortunate thing to need, but this list is read by
literally hundreds of busy people, many of whome have had to unsubscribe
at various points in the past due to a lack of moderation. I wish we had
a better solution, but that's what we have. We're also not along in
using fairly agressive moderation, for example the
cryptography at metzdowd.com mailing list where Bitcoin was originally
announced uses manual approval moderation on all messages as well;
there's also an unmoderated offshoot of it, cryptography at randombit.net
(and feel free to start an unmoderated version of bitcoin-dev!)

@_date: 2016-02-04 13:19:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Hardfork bit BIP 
1) There is no way to guarantee that nodes will see those blocks, and
the current network behavior works against such guarantees even in the
non-adversarial case.
2) I know of no currently deployed SPV wallet software that warns users
about unknown block versions anyway.

@_date: 2016-02-06 16:11:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
I covered the security considerations unique to hard-forks on my blog:
How do we know any of this testing is actually being performed? I don't
currently know of any concrete testing actually done.
Are you unaware of Not Bitcoin XT?
See above.
Also, as the two coins are separate currencies and can easily trade
against each other in a 75%/25% split, it would be easy for the price to
diverge and hashing power to move.
In fact, I've been asked multiple times now by exchanges and other
players in this ecosystem for technical advice on how to split coins
across the chains effectively (easily done with nLockTime). Notably, the
exchanges who have asked me this - who hold customer funds on their
behalf - have informed me that their legal advice was that the
post-hard-fork coins are legally speaking separate currencies, and
customers must be given the opportunity to transact in them separately
if they choose too.  Obviously, with a 75%/25% split, while block times
on the other chain will be slower, the chain is still quite useful and
nearly as secure as the main chain against 51% attack; why I personally
have suggested a 99% threshold:
(remember that the threshold can always be soft-forked down)
It's also notable that millions of dollars of Bitcoin are voting agsast
the fork on the proof-of-stake voting site Bitcoinocracy.com While
obviously not comprehensive, the fact that a relatively obscure site
like it can achieve participation like that, even without an easy to use
user friendly interface.
Please provide details on exactly how that's going to happen.

@_date: 2016-02-06 16:24:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
Oh, and to be 100% clear, I should say those are only *some of* the
unique security considerations - for starters the article is mainly
talking about uncontroversial hard-forks, and doesn't even delve into
economic attacks among other omissions. It's just an introductory

@_date: 2016-02-06 17:22:21
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
Why wouldn't an attacker be able to counter-sybil-attack that effort?
Who are these people?
I'll remind everyone that Bitcoin Core does not condone participation in
network attacks to push controversial protcol changes through. I also
checked with Adam Back, who confirmed Blockstream as a company shares
those views.
For those readers unfamiliar with Sybil attacks, basically what the
above does is prevents nodes from being able to finding peers with
accurate information about what blockchains exist - the above can be
used to prevent nodes from learning about the longest chain for
instance, or the existance of substantial support for a minority chain.
This is why we've advocated giving users sufficient time to actively
opt-in to protocol changes.

@_date: 2016-02-07 11:54:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
Eh, lets not put words into people's mouths. I personally don't
understand why Gavin is using 75% in the manner that he is, given there
are many better alternatives, even if you don't think you can get ~100%
hashing power support.
Note that the grace period adds a significant amount of complexity to
the implementation; a much simpler alternative is to just use a hashing
power activated change with a very high threshold - 99% or so - with a
minimum activation date some point reasonably far into the future.
Also the way the grace period is implemented means that if support
*drops* after 75% is reached, the hardfork still activates (I haven't
actually tested this, so I may be misunderstanding the code). Obviously,
this is a dangerous situation, and an easy way for miners to "poison the
well" and disruptively force the fork to be rescheduled without actually
attacking the coin (nothing wrong with changing your mind! and pool
distribution may change anyway).
Again, a simple high % miner consensus fork with a reasonable minimum
activation time avoids all these problems, with far less code

@_date: 2016-02-08 17:41:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP Final status 
It may be good to update BIP 50 with the new information that calling it
a "hard fork" misses subtleties about what happened during that fork. In
particular, 0.7 rejection of the chain was non-deterministic, based on
having seen a re-org in a specific way.

@_date: 2016-02-08 17:54:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
UTXO set space is significantly more expensive for the network as all
full nodes must keep the entire UTXO set.
Additionally, transaction input/output data in general is argued by some
to be less expensive than signatures, as you have more options with
regard to skipping validation of signatures (e.g. how Bitcoin Core skips
validation of signatures prior to checkpoints).
So, something to keep in mind in general in all these discussions is
that at best engineering always has "magic numbers" involved, the
question is where?
For example, I've proposed that we use a 99% miner vote threshold for
hard-forks (remember that the threshold can always be soft-forked down
later). The rational there is, among other things, you want to ensure
that the non-adopting miners' chain is useless for transacting due to
extremely long block times, as well as we want it to receive
confirmations slowly to prevent fraud. (of course, there's also the
non-technical argument that we want to adopt hard-forks with extremely
wide adoption) At 99% the 1% remaining chain will have a block interval
of about 16 hours.
Now, I've been asked "why 99%? isn't that a magic number?"
I could have instead said my goal was to increase the block interval to
24 hours, in which case I'd have used a 99.3% threshold. But again,
isn't 24 hours a magic number? Why not 25hrs?
The answer is 24 hours *is* a magic number - but trying to eliminate
that with yet another meta level of engineering analysis becomes a game
of diminishing returns.

@_date: 2016-02-12 10:34:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Soft fork fix for block withholding attacks 
So, while you're technique I believe works, it's not a soft-fork, at
least under the definition most of the Bitcoin dev/research community
have been using.
The reason is if it's adopted by a majority of hashing power, less than
a majority of hashing power can create a chain that appears to be the
most-work chain, from the perspective of non-adopting nodes. Those nodes
would then be following a weaker chain.
A better term for what you're proposing might be a "pseudo-soft-fork",
given that you don't quite meet the requirements for a true soft-fork.
Having said that, it may be the case that overall your technique still
reduces risk compared to a simpler hard-fork implementation of the idea;
more analysis is needed there.

@_date: 2016-01-08 10:41:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
I'll point out that I personally raised an issue with segpregated
witnesses quite recently - my concern that it could make validationless
mining easier and more profitable(1). Neither Pieter Wuille nor Gregory
Maxwell believed my concern to be important at first in private
communication. However, it was still discussed on IRC, with Pieter,
Greg, and others contributing valuable input on the problem and my
proposed fix. Right now I think the next step for me is to write the
code to implement my fix and submit a pull-req against the segwit
I certainly wouldn't describe that experience as "Here's the spec, take
it or leave it; We don't what what you think."
1) "Segregated witnesses and validationless mining",
    Peter Todd, Dec 23 2015, Bitcoin-dev mailing list,

@_date: 2016-01-08 10:52:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks 
Note how this is a good example where trying to avoid the relatively
small amount of complexity of having two different segregated witness
schemes to allow for 128bit security could lead to a significant amount
of upper level complexity trying to regain security. I wouldn't be
surprised at all if this upper level complexity leads to exploits; at
the very least it'll lead to a lot of wasted mental effort from
cryptographers concerned about the potential weakness, both within and
external to the Bitcoin development community.

@_date: 2016-01-11 06:57:42
@_author: Peter Todd 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
On 10 January 2016 22:57:15 GMT-05:00, Rusty
Don't get too caught up in Moore's law here - more likely the attack will become feasible because SHA2 is partially weakened, as happened with SHA1. Having industry standard safety margins would make such a weakening be an academic problem rather than an emergency.

@_date: 2016-01-23 15:59:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Three Month bitcoin-dev Moderation Review 
I would extend this to say that the technical explanation also should
contribute uniquely to the conversation; a +1 with an explanation
the last +1 gave isn't useful.

@_date: 2016-01-28 13:51:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] Segwit Upgrade Procedures & Block Extension Data 
A few notes on upgrade procedures associated with segregated witnesses:
Initial Deployment
While segregated witnesses is a soft-fork, because it adds new data
blocks that old nodes don't relay segwit nodes can't sync from
non-segwit nodes and still be fully validating; once the segwit softfork
has activated full nodes need witness data to function. This poses a
major problem during deployment: if full node adoption lags miner
adoption, the segwit-supporting P2P network can partition and lose
While Pieter Wuille's segwit branch(1) doesn't yet implement a fix for
the above problem, the obvious thing to do is to add a new service bit
such as NODE_SEGWIT, and/or bump the protocol version, and for outgoing
peers only connect to peers with segwit support. Interestingly, a
closely related problem already exists in Bitcoin Core: neither addrman
nor the outgoing connection thread takes what service bits a peer
advertises into account. So if a large number of non-block-relaying
nodes joined the network and advertised their addresses the network
could, in theory, partition even without an explicit attack. (My own
full-RBF fork of Bitcoin Core does fix(2) this issue, though by
Note how because of this the segwit soft-fork has properties not unlike
hard-forks in terms of the need for nodes to upgrade with regard to the
P2P layer. Even with the above fix, the worst case would be for segwit
to not be adopted widely by full node operators, resulting in a network
much more vulnerable to attacks such as DoSing nodes. This is one of the
(many) reasons why hard-forks are generally significantly more dangerous
than soft-forks.
Future Upgrades
Segwit isn't going to be the last thing that adds new block data. For
example, my own prev-block-proof proposal(3) requires that blocks commit
to another tree, which itself is calculated using a nonce that must be
passed along with the block data. (U)TXO commitments are another
possible future example.
BIP141 (currently) suggests an Extensible Commitment Structure(4)
consisting of a hashed linked list of consensus-critical commitments,
with a redefinable nonce at the end of the list for future soft-forks.
Currently this nonce is put into the otherwise useless, and non-hashed,
witness for the coinbase transaction(6) and a block is invalid if its
witness contains more than that single nonce.(7)
Unfortunately, this means that the next soft-fork upgrade to add
additional data will have the above relaying problem all over again!
Even a minimal upgrade adding a new commitment - like my
prev-block-proof proposal - needs to at least add another nonce for
future upgrades. In addition to having to upgrade full nodes, this also
requires systems like the relay network to upgrade, even though they may
not themselves otherwise need to care about the contents of blocks.
A more subtle implication of this problem is how do you handle parallel
upgrades, as proposed by BIP9? Splitting the P2P network into
non-upgraded nodes, and a much smaller group of upgraded nodes, is bad
enough when done every once in a awhile. How does this look with more
frequent upgrades, not necessarily done by teams that are working
closely with each other?
Proposal: Unvalidated Block Extension Data
1) Remove the restriction that the coinbase witness contain exactly one
   32byte value.
2) Hash the contents of the coinbase witness (e.g. as a merkle tree) and
   commit them in place of the current nonce commitment.
3) Include that data in the blocksize limit (to prevent abuse).
Now future soft-forks can simply add additional data, which non-upgraded
nodes simply see as extension data that they don't know how to fully
validate. All nodes can however validate that data came from the miner,
and thus they can freely propagate that data without risk of attack
(Bitcoin Core used to allow additional data to be included with
transactions, which was used in a DoS attack (CVE-2013-4627)).
This is more efficient than it may appear at first glace. As most future
upgrades are expected to be additional commitments where full nodes can
deterministically recalculate the commitment, the additional data for
each new commitment is just 32 bytes.
A significant design consideration is that if arbitrary data can be
added, it is very likely that miners will make use of that ability for
non-Bitcoin purposes; we've already run into problems deploying segwit
itself because of pools using the coinbase space for advertising and
merge-mining. Avoiding this problem is easiest with a merkelized
key:value mapping, with the ability to use collision-resistant ID's as
keys (e.g. UUID).
Secondly, does using the coinbase witness for this really make sense?
Logically it'd make more sense to change the way blocks are serialized,
much the same way transaction serialization was changed to accomodate
segwit; stuffing this in the coinbase witness smells like a hack. (along
those lines, note how witnesses themselves could have been implemented
this way - probably too late to change now)
1) 2) 3) 5) 6) 7)

@_date: 2016-01-29 14:11:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] Best (block nr % 2016) for hard fork activation? 
I wrote up some of those risks in my "Soft Forks Are Safer Than Hard
Forks" post the other week:
I was writing mainly in terms of technical risks for deployment
non-controversial forks; for controversial forks there's many more
failure scenarios. In any case, on technical grounds alone it's obvious
that hard-forks without very high - 95% or so - activation thresholds
are quite dangerous.
In general, it should be remembered that high activation thresholds for
hard-forks can always be soft-forked down after the fact. For instance,
suppose we initially used 100% support over the past one month of blocks
as a hard-fork threshold, but can't get more than 96% support. A
soft-fork with the following rule can be implemented:
    If 95% of the past blocks vote yes, voting against the hard-fork is
    not allowed.
As soft-forks can be rolled out quite quickly, implementing this in the
event that a hard-fork isn't getting sufficient support won't add much
delay to the overall process; as it is a soft-fork, only miners need to
adopt it for it to take effect.
For this reason I'd suggest any hard fork use 99%+ activation
thresholds, measured over multi-week timespan. Hard-forks should not be
controversial for good social/political reasons anyway, so there's
little harm in most cases to at worst delaying the fork by two or three
months if stragglers won't upgrade (in very rare cases like security
issues there may be exceptions; blocksize is certainly not one of those

@_date: 2016-07-02 14:43:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] Code Review: The Consensus Critical Parts of 
Sure - equally you could say you could add additional commitments as other
coinbase txouts.
My point is that the extensible commitment - specifically the thing described
in the BIP - can't be easily used for the purpose of extending segwit due to a
design flaw.
It will lead to a special case in code that does things with witness
transactions, as we can spend a witness output without a witness.
I'm aware of that - there are many P2SH scripts where birthday attacks are not
an issue. In fact, _most_ usage of P2SH right now - multifactor wallets -
doesn't have a birthday attack problem.
Huh? That still another level of indirection.
Anyway, the right argument against my proposal for pay-to-pubkey-hash
functionality, is that taking into account the witness discount, my proposal is
slightly less efficient. In P2WPKH in P2SH the witness program in the
redeemScript is 22 bytes:
    <1-byte version> <1-byte length> <20 byte pubkey hash>
And the witness len(sig) + 34 bytes:
     <1 byte length> <33 bytes pubkey>
Taking into account the discount, that results in 22*4 + 34 + len(sig) = 122 bytes + len(sig)
My proposal would have a 37 byte redeemScript:
    <1-byte version> <1-byte witness script length> {<1-byte pubkey length> <33 byte pubkey> OP_CHECKSIG}
and a len(sig) length witness:
Taking into account the discount, that results in 37*4 + len(sig) = 148 bytes + len(sig)
Meanwhile for any more complex script, you'd certainly want to use the 256-bit
hash instead, due to the witness discount.
This suggests an obvious alternative: let users choose to use 160-bit security,
and make 256-bit and 160-bit witness programm commitments just be different
hash functions. P2PKH functionality implemented this way would be a single
extra byte vs. special-casing it.
Thus you could summarize the argument for the P2PKH special case as "We don't
want to make it possible to use 160-bit commitments for multisig, which _might_
need 256-bit security. But we do want to special-case pubkeys to save a few
No, you're quite confused at my point: the witness script is otherwise
constrained to 10,000 bytes, as the first item in the witness is special-cased
for version 0 to be not be subject to the 520 byte rule.
Nope. The problem is it might not be a hash collission, if the actual bytes
signed can be interpreted in two different ways, by different types of
signature hashes.
This is the same reason the signmessage functionality prepends the message
being signed with the "Bitcoin Signed Message:\n" magic string.

@_date: 2016-07-04 19:27:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] Code Review: The Consensus Critical Parts of 
The Bitcoin Core codebase, no, but it does reduce the number of special cases
other codebases have to contend with.
Probably not worth changing now, but it was I think a weird design choice to
How short? 128 bits? 80 bits? 64 bits?
It's hard to know what's the point where you're going to risk massive losses
due to theft... and as we've seen with the DAO, those kinds of losses can lead
to very undesirable pressure for devs to act as a central authority and
Why isn't this carefully documented in the BIPs then?
Again, as I said in my summary:
    In a number of places we either have a belt, or suspenders, when given the
    importance of this code we?d rather have both a belt and suspenders.
Tagged hashing is an excellent way to absolutely sure that signatures can't be
reused in different contexts; if it happens to be overkill in a specific
context, the overhead of hashing another few bytes is trivial; the gain of
being absolutely sure you haven't missed a vulnerability can't be easily
Equally, I think in most cases simply XORing the digest obtained by hashing
with a magic tag prior to using it (e.g. by signing it) should be sufficient
for signature applications, and the overhead of doing that is ~zero.
Essentially you can think of the magic tag that's XORed with the raw digest as
making clear the intent of the signature: "this is why I think I'm signing this
However, the XOR option does have one potentially big downside in other
contexts, like general use in committed data structures: it's incompatible with
timestamping schemes like OpenTimestamps that rely on all operations being
cryptographically secure.

@_date: 2016-07-05 13:46:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP Number Request: Open Asset 
What's the status of this BIP? Will it be assigned?

@_date: 2016-07-15 12:31:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] Status updates for BIP 9, 68, 112, and 113 
As of writing the text of BIP68 says:
    'This BIP is to be deployed by "versionbits" BIP9 using bit 0.'
Essentially including BIP9 as part of the BIP68 standard; BIP68 could have
equally been written by including some or all of the text of BIP9. If it had
done that, that text would be part of a "Standard BIP" rather than
"Informational BIP", thus I'll argue that BIP9 should also be a "Standard BIP"
Also, note that if we ever modified BIP9, we'd most likely do so with a new
BIP, and in soft-forks using that new standard, would refer to the new BIP ACK "Final" status.

@_date: 2016-07-20 01:46:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP draft: HTLC transactions 
Note that because you're hashing the top item on the stack regardless
scriptSig's that satisfy HTLC's are malleable: that top stack item can be
changed anything in the digest-not-provided case and the script still passes.

@_date: 2016-06-15 20:10:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle trees and mountain ranges 
No, that's incorrect - I'm only proposing TXO commitments, not UTXO nor STXO
Again, I'm not proposing STXO commitments precisely because the set of _spent_
transactions grows without bound. TXO commitments with committed sums of
remaining unspent TXO's and with pruning of old history are special in this
regard, because once spent the data associated with spent transactions can be
discarded completely, and at the same time, data associated with old history
can be pruned with responsibility for keeping it resting on the shoulders of
those owning those coins.
TXO commitments allows you to do all of this without requiring miners to have
unbounded storage to create new blocks.
Agreed - regardless of approach adding latency to commitment calculations of
all kinds is something I think we all agree can work in principle, although
obviously it should be a last resort technique when optimization fails.
It'd help if you specified exactly what type of merkle tree you're talking
about here; remember that the certificate transparency RFC appears to have
reinvented merkle mountain ranges, and they call them "merkle trees".  Bitcoin
meanwhile uses a so-called "merkle tree" that's broken, and Zcash uses a
partially filled fixed-sized perfect tree.
I'm having a hard time understanding this paragraph; could you explain what you
think is happening when things are "merged into larger hills"?
As UTXO/STXO/TXO sets are all enormously larger than L1/L2 cache, it's
impossible to get CPU cache misses below one for update operations. The closest
thing to an exception is MMR's, which due to their insertion-ordering could
have good cache locality for appends, in the sense that the mountain tips
required to recalculate the MMR tip digest will already be in cache from the
previous append. But that's not sufficient, as you also need to modify old
TXO's further back in the tree to mark them as spent - that data is going to be
far larger than L1/L2 cache.
Have you looked at the pruning system that my proofchains work implements?

@_date: 2016-06-15 23:26:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle trees and mountain ranges 
I think you need to re-read my original post on TXO commitments, specifically
where I say:
    # TXO Commitments
    A merkle tree committing to the state of __all transaction outputs, both spent
    and unspent__, we can provide a method of compactly proving the current state of
    an output.
Nope, MMR's are completely unlike what you just described.
Ok, but then if you're concerned about that risk, why introduce a data
structure - the STXO set - that's _guaranteed_ to grow without bound?
Which codebase exactly? I have both a insertion-ordered list (MMR) and a
key:value mapping (referred to as a "merbinner tree" in the codebase) in the
proofchains codebase. They're very different data structures.
I'm rather confused, as the above sounds nothing like what I've implemented,
which only has leaf nodes, inner nodes, and the special empty node singleton,
for both the MMR and merbinner trees.
Yeah, we're definitely not...
In MMR's append operations never need to modify mountain contents.
Again, see above.
I'm very confused as to why you think that's possible. When you say "practical
Bitcoin updates", what exactly is the data structure you're proposing to
update? How is it indexed?

@_date: 2016-06-17 00:34:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle trees and mountain ranges 
Ah, yeah, I misunderstood you there; as expected absolutely no-one is proposing
STXO set commitments. :)
Ah, I see what you mean now.
So above you said that in merbinner trees each node "hash[es] in a record of
its depth" That's actually incorrect: each node commits to the prefix that all
keys below that level start with, not just the depth.
This means that in merbinner trees, cases where multiple keys share parts of
the same prefix are handled efficiently, without introducing extra levels
unnecessarily; there's no need for the ONLY0/1 nodes as the children of an
inner node will always be on different sides.
When keys are randomly distributed, this isn't a big deal; OTOH against
attackers who are choosing keys, e.g. by grinding hashes, merbinner trees
always have maximum depths in proportion to log2(n) of the actual number of
items in the tree. Grinding is particularly annoying to deal with due to the
birthday attack: creating a ground prefix 64 bits long only takes 32 bits worth
of work.
In my deterministic expressions work one of the ideas I've been tossing around
is rather than always using hash digests directly for when you need to commit
to some data, we could instead extend the idea of a digest to that of a
"commitment", where a commitment is simply some short, but variable-sized,
string that uniquely maps to a given set of data. Secondly, commitments do
*not* always guarantee that the original data can't be recovered from the
commitment itself.
By allowing commitments to be variable sized - say 0 to ~64 bytes - we get a
number of advantages:
1) Data shorter than the length of a digest (32 bytes) can be included in the
commitment itself, improving efficiency.
2) Data a little longer than a digest can have hashing delayed, to better fill
up blocks.
In particular, case  handles your leaf node optimizations generically,
without special cases and additional complexity. It'd also be a better way to
do the ONLY0/1 cases, as if the "nothing on this side" symbol is a single byte,
each additional colliding level would simply extend the commitment without
hashing. In short, you'd have nearly the same level of optimization even if at
the cryptography level your tree consists of only leaves, inner nodes, and nil.
Another advantage of variable sized commitments is that it can help make clear
to users when it's possible to brute force the message behind the commitment.
For instance, digest from a hashed four byte integer can be trivially reversed
by just trying all combinations. Equally, if that integer is concatenated with
a 32 byte digest that the attacker knows, the value of the integer can be brute
Your estimate of updates requiring 32 bytes of data is *way* off.
Each inner node updated on the path to a leaf node will itself require 32 bytes
of data to be fetched - the digest of the sibling. As of block 416,628, there
are 39,167,128 unspent txouts, giving us a tree about 25 levels deep.
So if I want to update a single leaf, I need to read:
    25 nodes * 32 bytes/node = 800 bytes
of data. Naively, that'd mean our 2,000 updates needs to read 1.6MB from RAM,
which is 6.4x bigger than the L2 cache - it's just not going to fit.
Taking into account the fact that this is a batched update improves things a
little bit. For a node at level i with random access patterns and N accesses
total our amortised cost is 1/(1 + N/2^i) Summing that over 2,000 leaf updates
and 25 levels gives us ~29,000 total updates, 0.9MB, which is still a lot
larger than L2 cache.
While this might fit in L3 cache - usually on the order of megabytes - this is
a rather optimistic scenario anyway: we're assuming no other cache pressure and
100% hit rate.
Anyway hashing is pretty slow. The very fast BLAKE2 is about 3 cycles/byte
(SHA256 is about 15 cycles/byte) so hashing that same data would take around
200 cycles, and probably quite a bit more in practice due to overheads from our
short message lengths; fetching a cache line from DRAM only takes about 1,000
cycles. I'd guess that once other overheads are taken into account, even if you
could eliminate L2/L3 cache-misses it wouldn't be much of an improvement.
I think it's safe to say that given our working set is significantly larger
than the L2/L3 cache available, none of the above optimizations are likely to
help much. Better to just keep the codebase simple and use standard techniques.

@_date: 2016-06-18 18:09:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle trees and mountain ranges 
So, to be clear you're assuming that blocks commit to key:value maps of the
block contents, specifically a pre-block "UTXO deletion/things that this block
spent" set? First of all, it's interesting how the much smaller dataset of a
pre-block key:value map would make L2/L3 caching optimizations much more likely
to be relevant. :)
That type of solution would be very similar to the solutions treechains would
need to prove coins haven't been doublespent. Basically, in treechains the
system as a whole is a datastructure indexed by time and prefix. So, if you
want to prove a valid spend you need to convince me of three things:
1. The coin existed as of time t1 at prefix p
2. At t2, p, a valid spend was published.
3. Between t1 and t2 at prefix p no other valid spend was published.
Paths to any prefix p as of time t, will have about log2(len(p)) size (beyond
the top-level chain), similar to your above suggestion. Of course, unlike your
above suggestion, in treechains it's not clear if step  can be done without
another n*log(N)-ish sized proof in a truly trustless environment!
I'm _not_ of the optinion that validation before propagation needs to be done
at all - I think it's perfectly reasonable to propgate blocks that you have not
validated at all (beyond checking PoW as an anti-DoS measure).  The time it
takes miners to start mining the next block - and collecting fees - is however
very important.
In practice, I think we're mostly in agreement here, but because I'm happy to
propagate prior to validating I'd be ok with protocol designs that required
miners to have relatively large amounts of RAM - say 32GB - dedicated to UTXO
lookup because that wouldn't require relay nodes to also have those kinds of
resources available to them once validationless propagation was implemented.

@_date: 2016-06-18 19:01:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle trees and mountain ranges 
Wait, are you saying you think committing to the prefix is a "trick"? It's just
a very simple - and possibly not-optimal - way of committing to what data
should be accessible under a given node. An alternative would have been ensure
that in terms of _cryptographic_ tree position.
By "position", are you talking about position within RAM? That may or may not
be a viable optimization, but it's quite separate from the question of the
cryptographic structure of the data.
Have you seen how BLAKE2 omits padding when the data to be hashed happens to be
exactly one block in size? It's significantly faster than SHA256, and that's a
standard part of the algorithm already.
Have you benchmarked the cost of a hash operation vs. the cost of a cache miss?
What are the actual numbers?
A single....?
So, is this also how the data structure looks cryptographically, or is the way
it's hashed separate from the above description?
Page as in 4096 bytes? But L1/L2/L3 cache is arranged in terms of 64 byte cache
lines - where do pages come in here?
At Bitcoin UTXO set scale, how large do you think these data structures are?
"Sorted order" - what exact type of sorting do you mean here?
But that's assuming the dataset in question fits in cache; I don't see how it
does. Since it doesn't, I'm argung the total % improvement by _any_ cache
optimization on the subset that does fit in cache will be relatively small.
Again, how large a dataset do you think you're working with here?

@_date: 2016-06-20 04:56:49
@_author: Peter Todd 
@_subject: [bitcoin-dev] Building Blocks of the State Machine Approach to 
In light of Ethereum's recent problems with its imperative, account-based,
programming model, I thought I'd do a quick writeup outlining the building
blocks of the state-machine approach to so-called "smart contract" systems, an
extension of Bitcoin's own design that I personally have been developing for a
number of years now as my Proofchains/Dex research work.
# Deterministic Code / Deterministic Expressions
We need to be able to run code on different computers and get identical
results; without this consensus is impossible and we might as well just use a
central authoritative database. Traditional languages and surrounding
frameworks make determinism difficult to achieve, as they tend to be filled
with undefined and underspecified behavior, ranging from signed integer
overflow in C/C++ to non-deterministic behavior in databases. While some
successful systems like Bitcoin are based on such languages, their success is
attributable to heroic efforts by their developers.
Deterministic expression systems such as Bitcoin's scripting system and the
author's Dex project improve on this by allowing expressions to be precisely
specified by hash digest, and executed against an environment with
deterministic results. In the case of Bitcoin's script, the expression is a
Forth-like stack-based program; in Dex the expression takes the form of a
lambda calculus expression.
 Proofs
So far the most common use for deterministic expressions is to specify
conditions upon which funds can be spent, as seen in Bitcoin (particularly
P2SH, and the upcoming Segwit). But we can generalize their use to precisely
defining consensus protocols in terms of state machines, with each state
defined in terms of a deterministic expression that must return true for the
state to have been reached. The data that causes a given expression to return
true is then a "proof", and that proof can be passed from one party to another
to prove desired states in the system have been reached.
An important implication of this model is that we need deterministic, and
efficient, serialization of proof data.
 Pruning
Often the evaluation of an expression against a proof doesn't require all all
data in the proof. For example, to prove to a lite client that a given block
contains a transaction, we only need the merkle path from the transaction to
the block header. Systems like Proofchains and Dex generalize this process -
called "pruning" - with built-in support to both keep track of what data is
accessed by what operations, as well as support in their underlying
serialization schemes for unneeded data to be elided and replaced by the hash
digest of the pruned data.
# Transactions
A common type of state machine is the transaction. A transaction history is a
directed acyclic graph of transactions, with one or more genesis transactions
having no inputs (ancestors), and one or more outputs, and zero or more
non-genesis transactions with one or more inputs, and zero or more outputs. The
edges of the graph connect inputs to outputs, with every input connected to
exactly one output. Outputs with an associated input are known as spent
outputs; outputs with out an associated input are unspent.
Outputs have conditions attached to them (e.g. a pubkey for which a valid
signature must be produced), and may also be associated with other values such
as "# of coins". We consider a transaction valid if we have a set of proofs,
one per input, that satisfy the conditions associated with each output.
Secondly, validity may also require additional constraints to be true, such as
requiring the coins spent to be >= the coins created on the outputs. Input
proofs also must uniquely commit to the transaction itself to be secure - if
they don't the proofs can be reused in a replay attack.
A non-genesis transaction is valid if:
1. Any protocol-specific rules such as coins spent >= coins output are
   followed.
2. For every input a valid proof exists.
3. Every input transaction is itself valid.
A practical implementation of the above for value-transfer systems like Bitcoin
could use two merkle-sum trees, one for the inputs, and one for the outputs,
with inputs simply committing to the previous transaction's txid and output #
(outpoint), and outputs committing to a scriptPubKey and output amount.
Witnesses can be provided separately, and would sign a signature committing to
the transaction or optionally, a subset of of inputs and/or outputs (with
merkle trees we can easily avoid the exponential signature validation problems
bitcoin currently has).
As so long as all genesis transactions are unique, and our hash function is
secure, all transaction outputs can be uniquely identified (prior to BIP34 the
Bitcoin protocol actually failed at this!).
 Proof Distribution
How does Alice convince Bob that she has done a transaction that puts the
system into the state that Bob wanted? The obvious answer is she gives Bob data
proving that the system is now in the desired state; in a transactional system
that proof is some or all of the transaction history. Systems like Bitcoin
provide a generic flood-fill messaging layer where all participants have the
opportunity to get a copy of all proofs in the system, however we can also
implement more fine grained solutions based on peer-to-peer message passing -
one could imagine Alice proving to Bob that she transferred title to her house
to him by giving him a series of proofs, not unlike the same way that property
title transfer can be demonstrated by providing the buyer with a series of deed
documents (though note the double-spend problem!).
# Uniqueness and Single-Use Seals
In addition to knowing that a given transaction history is valid, we also want
to know if it's unique. By that we mean that every spent output in the
transaction history is associated with exactly one input, and no other valid
spends exist; we want to ensure no output has been double-spent.
Bitcoin (and pretty much every other cryptocurrency like it) achieves this goal
by defining a method of achieving consensus over the set of all (valid)
transactions, and then defining that consensus as valid if and only if no
output is spent more than once.
A more general approach is to introduce the idea of a cryptographic Single-Use
Seal, analogous to the tamper-evidence single-use seals commonly used for
protecting goods during shipment and storage. Each individual seals is
associated with a globally unique identifier, and has two states, open and
closed. A secure seal can be closed exactly once, producing a proof that the
seal was closed.
All practical single-use seals will be associated with some kind of condition,
such as a pubkey, or deterministic expression, that needs to be satisfied for
the seal to be closed. Secondly, the contents of the proof will be able to
commit to new data, such as the transaction spending the output associated with
the seal.
Additionally some implementations of single-use seals may be able to also
generate a proof that a seal was _not_ closed as of a certain
 Implementations
 Transactional Blockchains
A transaction output on a system like Bitcoin can be used as a single-use seal.
In this implementation, the outpoint (txid:vout  is the seal's identifier,
the authorization mechanism is the scriptPubKey of the output, and the proof
is the transaction spending the output. The proof can commit to additional
data as needed in a variety of ways, such as an OP_RETURN output, or
unspendable output.
This implementation approach is resistant to miner censorship if the seal's
identifier isn't made public, and the protocol (optionally) allows for the
proof transaction to commit to the sealed contents with unspendable outputs;
unspendable outputs can't be distinguished from transactions that move funds.
 Unbounded Oracles
A trusted oracle P can maintain a set of closed seals, and produce signed
messages attesting to the fact that a seal was closed. Specifically, the seal
is identified by the tuple (P, q), with q being the per-seal authorization
expression that must be satisfied for the seal to be closed. The first time the
oracle is given a valid signature for the seal, it adds that signature and seal
ID to its closed seal set, and makes available a signed message attesting to
the fact that the seal has been closed. The proof is that message (and
possibly the signature, or a second message signed by it).
The oracle can publish the set of all closed seals for transparency/auditing
purposes. A good way to do this is to make a merkelized key:value set, with the
seal identifiers as keys, and the value being the proofs, and in turn create a
signed certificate transparency log of that set over time. Merkle-paths from
this log can also serve as the closed seal proof, and for that matter, as
proof of the fact that a seal has not been closed.
 Bounded Oracles
The above has the problem of unbounded storage requirements as the closed seal
set grows without bound. We can fix that problem by requiring users of the
oracle to allocate seals in advance, analogous to the UTXO set in Bitcoin.
To allocate a seal the user provides the oracle P with the authorization
expression q. The oracle then generates a nonce n and adds (q,n) to the set of
unclosed seals, and tells the user that nonce. The seal is then uniquely
identified by (P, q, n)
To close a seal, the user provides the oracle with a valid signature over (P,
q, n). If the open seal set contains that seal, the seal is removed from the
set and the oracle provides the user with a signed message attesting to the
valid close.
A practical implementation would be to have the oracle publish a transparency
log, with each entry in the log committing to the set of all open seals with a
merkle set, as well as any seals closed during that entry. Again, merkle paths
for this log can serve as proofs to the open or closed state of a seal.
Note how with (U)TXO commitments, Bitcoin itself is a bounded oracle
implementation that can produce compact proofs.
 Group Seals
Multiple seals can be combined into one, by having the open seal commit to a
set of sub-seals, and then closing the seal over a second set of closed seal
proofs. Seals that didn't need to be closed can be closed over a special
re-delegation message, re-delegating the seal to a new open seal.
Since the closed sub-seal proof can additionally include a proof of
authorization, we have a protcol where the entity with authorization to close
the master seal has the ability to DoS attack sub-seals owners, but not the
ability to fraudulently close the seals over contents of their choosing. This
may be useful in cases where actions on the master seal is expensive - such as
seals implemented on top of decentralized blockchains - by amortising the cost
over all sub-seals.
 Atomicity
Often protocols will require multiple seals to be closed for a transaction to
be valid. If a single entity controls all seals, this is no problem: the
transaction simply isn't valid until the last seal is closed.
However if multiple parties control the seals, a party could attack another
party by failing to go through with the transaction, after another party has
closed their seal, leaving the victim with an invalid transaction that they
can't reverse.
We have a few options to resolve this problem:
 Use a single oracle
The oracle can additionally guarantee that a seal will be closed iff some other
set of seals are also closed; seals implemented with Bitcoin can provide this
guarantee. If the parties to a transaction aren't already all on the same
oracle, they can add an additional transaction reassigning their outputs to a
common oracle.
Equally, a temporary consensus between multiple mutually trusting oracles can
be created with a consensus protocol they share; this option doesn't need to
change the proof verification implementation.
 Two-phase Timeouts
If a proof to the fact that a seal is open can be generated, even under
adversarial conditions, we can make the seal protocol allow a close to be
undone after a timeout if evidence can be provided that the other seal(s) were
not also closed (in the specified way).
Depending on the implementation - especially in decentralized systems - the
next time the seal is closed, the proof it has been closed may in turn provide
proof that a previous close was in fact invalid.
# Proof-of-Publication and Proof-of-Non-Publication
Often we need to be able to prove that a specified audience was able to receive
a specific message. For example, the author's PayPub protocol[^paypub],
Todd/Taaki's timelock encryption protocol[^timelock], Zero-Knowledge Contingent
Payments[^zkcp], and Lightning, among others work by requiring a secret key to
be published publicly in the Bitcoin blockchain as a condition of collecting a
payment. At a much smaller scale - in terms of audience - in certain FinTech
applications for regulated environments a transaction may be considered invalid
unless it was provably published to a regulatory agency.  Another example is
Certificate Transparency, where we consider a SSL certificate to be invalid
unless it has been provably published to a transparency log maintained by a
Secondly, many proof-of-publication schemes also can prove that a message was
_not_ published to a specific audience. With this type of proof single-use
seals can be implemented, by having the proof consist of proof that a specified
message was not published between the time the seal was created, and the time
it was closed (a proof-of-publication of the message).
 Implementations
 Decentralized Blockchains
Here the audience is all participants in the system. However miner censorship
can be a problem, and compact proofs of non-publication aren't yet available
(requires (U)TXO commitments).
The authors treechains proposal is a particularly generic and scalable
implementation, with the ability to make trade offs between the size of
audience (security) and publication cost.
 Centralized Public Logs
Certificate Transparency works this way, with trusted (but auditable) logs run
by well known parties acting as the publication medium, who promise to allow
anyone to obtain copies of the logs.
The logs themselves may be indexed in a variety of ways; CT simply indexes logs
by time, however more efficient schemes are possible by having the operator
commit to a key:value mapping of "topics", to allow publication (and
non-publication) proofs to be created for specified topics or topic prefixes.
Auditing the logs is done by verifying that queries to the state of the log
return the same state at the same time for different requesters.
 Receipt Oracles
Finally publication can be proven by a receipt proof by the oracle, attesting
to the fact that the oracle has successfully received the message. This is
particularly appropriate in cases where the required audience is the oracle
itself, as in the FinTech regulator case.
# Validity Oracles
As transaction histories grow longer, they may become impractical to move from
one party to another. Validity oracles can solve this problem by attesting to
the validity of transactions, allowing history prior to the attested
transactions to be discarded.
A particularly generic validity oracle can be created using deterministic
expressions systems. The user gives the oracle an expression, and the oracle
returns a signed message attesting to the validity of the expression.
Optionally, the expression may be incomplete, with parts of the expression
replaced by previously generated attestations. For example, an expression that
returns true if a transaction is valid could in turn depend on the previous
transaction also being valid - a recursive call of itself - and that recursive
call can be proven with a prior attestation.
 Implementations
 Proof-of-Work Decentralized Consensus
Miners in decentralized consensus systems act as a type of validity oracle, in
that the economic incentives in the system are (supposed to be) designed to
encourage only the mining of valid blocks; a user who trusts the majority of
hashing power can trust that any transaction with a valid merkle path to a
block header in the most-work chain is valid. Existing decentralized consensus
systems like Bitcoin and Ethereum conflate the roles of validity oracle and
single-use seal/anti-replay oracle, however in principle that need not be true.
 Trusted Oracles
As the name suggests. Remote-attestation-capable trusted hardware is a
particularly powerful implementation - a conspiracy theory is that the reason
why essentially zero secure true remote attestation implementations exist is
because they'd immediately make untraceable digital currency systems easy to
implement (Finney's RPOW[^rpow] is a rare counter-example).
Note how a single-use seal oracle that supports a generic deterministic
expressions scheme for seal authorization can be easily extended to provide a
validity oracle service as well. The auditing mechanisms for a single-use seal
oracle can also be applied to validity oracles.
# Fraud Proofs
Protocols specified with deterministic expressions can easily generate "fraud
proofs", showing that claimed states/proof in the system are actually invalid.
Additionally many protocols can be specified with expressions of k*log2(n)
depth, allowing these fraud proofs to be compact.
A simple example is proving fraud in merkle-sum tree, where the validity
expression would be something like:
    (defun valid? (node)
        (or (== node.type leaf)
            (and (== node.sum (+ node.left.sum node.right.sum))
                 (and (valid? node.left)
                      (valid? node.right)))))
To prove the above expression evaluates to true, we'll need the entire contents
of the tree. However, to prove that it evaluates to false, we only need a
subset of the tree as proving an and expression evaluates to false only
requires one side, and requires log2(n) data. Secondly, with pruning, the
deterministic expressions evaluator can automatically keep track of exactly
what data was needed to prove that result, and prune all other data when
serializing the proof.
 Validity Challenges
However how do you guarantee it will be possible to prove fraud in the first
place? If pruning is allowed, you may simply not have access to the data
proving fraud - an especially severe problem in transactional systems where a
single fraudulent transaction can counterfeit arbitrary amounts of value out of
thin air.
A possible approach is the validity challenge: a subset of proof data, with
part of the data marked as "potentially fraudulent". The challenge can be
satisfied by providing the marked data and showing that the proof in question
is in fact valid; if the challenge is unmet participants in the system can
choose to take action, such as refusing to accept additional transactions.
Of course, this raises a whole host of so-far unsolved issues, such as DoS
attacks and lost data.
# Probabilistic Validation
Protocols that can tolerate some fraud can make use of probabilistic
verification techniques to prove that the percentage of undetected fraud within
the system is less than a certain amount, with a specified probability.
A common way to do this is the Fiat-Shamir transform, which repeatedly samples
a data structure deterministically, using the data's own hash digest as a seed
for a PRNG. Let's apply this technique to our merkle-sum tree example. We'll
first need a recursive function to check a sample, weighted by value:
    (defun prefix-valid? (node nonce)
        (or (== node.type leaf)
            (and (and (== node.sum (+ node.left.sum node.right.sum))
                      (> 0 node.sum)) ; mod by 0 is invalid, just like division by zero
                                      ; also could guarantee this with a type system
                 (and (if (< node.left.sum (mod nonce node.sum))
                          (prefix-valid? node.right (hash nonce))
                          (prefix-valid? node.left (hash nonce)))))))
Now we can combine multiple invocations of the above, in this case 256
    (defun prob-valid? (node)
        (and (and (and .... (prefix-valid? node (digest (cons (digest node) 0)))
             (and (and ....
                            (prefix-valid? node (digest (cons (digest node) 255)))
As an exercise for a reader: generalize the above with a macro, or a suitable
types/generics system.
If we assume our attacker can grind up to 128 bits, that leaves us with 128
random samples that they can't control. If the (value weighted) probability of
a given node is fraudulent q, then the chance of the attacker getting away with
fraud is (1-q)^128 - for q=5% that works out to 0.1%
(Note that the above analysis isn't particularly well done - do a better
analysis before implementing this in production!)
 Random Beacons and Transaction History Linearization
The Fiat-Shamir transform requires a significant number of samples to defeat
grinding attacks; if we have a random beacon available we can significantly
reduce the size of our probabilistic proofs. PoW blockchains can themselves act
as random beacons, as it is provably expensive for miners to manipulate the
hash digests of blocks they produce - to do so requires discarding otherwise
valid blocks.
An example where this capability is essential is the author's transaction
history linearization technique. In value transfer systems such as Bitcoin, the
history of any given coin grows quasi-exponentially as coins are mixed across
the entire economy. We can linearize the growth of history proofs by redefining
coin validity to be probabilistic.
Suppose we have a transaction with n inputs. Of those inputs, the total value
of real inputs is p, and the total claimed value of fake inputs is q. The
transaction commits to all inputs in a merkle sum tree, and we define the
transaction as valid if a randomly chosen input - weighted by value - can
itself be proven valid. Finally, we assume that creating a genuine input is a
irrevocable action which irrevocable commits to the set of all inputs, real and
If all inputs are real, 100% of the time the transaction will be valid; if all
inputs are fake, 100% of the time the transaction will be invalid. In the case
where some inputs are real and some are fake the probability that the fraud
will be detected is:
    q / (q + p)
The expected value of the fake inputs is then the sum of the potential upside -
the fraud goes detected - and the potential downside - the fraud is detected
and the real inputs are destroyed:
    E = q(1 - q/(q + p)) - p(q/(q + p)
      = q(p/(q + p)) - p(q/(q + p)
      = (q - q)(p/(q + p))
      = 0
Thus so long as the random beacon is truly unpredictable, there's no economic
advantage to creating fake inputs, and it is sufficient for validity to only
require one input to be proven, giving us O(n) scaling for transaction history
 Inflationary O(1) History Proofs
We can further improve our transaction history proof scalability by taking
advantage of inflation. We do this by occasionally allowing a transaction proof
to be considered valid without validating _any_ of the inputs; every time a
transaction is allowed without proving any inputs the size of the transaction
history proof is reset. Of course, this can be a source of inflation, but
provided the probability of this happening can be limited we can limit the
maximum rate of inflation to the chosen value.
For example, in Bitcoin as of writing every block inflates the currency supply
by 25BTC, and contains a maximum of 1MB of transaction data, 0.025BTC/KB. If we
check the prior input proof with probability p, then the expected value of a
transaction claiming to spend x BTC is:
    E = x(1-p)
We can rewrite that in terms of the block reward per-byte R, and the transaction size l:
    lR = x(1-p)
And solving for p:
    p = 1 - lR/x
For example, for a 1KB transaction proof claiming to spending 10BTC we can omit
checking the input 0.25% of the time without allowing more monetary inflation
than the block reward already does. Secondly, this means that after n
transactions, the probability that proof shortening will _not_ happen is p^n,
which reaches 1% after 1840 transactions.
In a system like Bitcoin where miners are expected to validate, a transaction
proof could consist of just a single merkle path showing that a single-use seal
was closed in some kind of TXO commitment - probably under 10KB of data. That
gives us a history proof less than 18.4MB in size, 99% of the time, and less
than 9.2MB in size 90% of the time.
An interesting outcome of thing kind of design is that we can institutionalize
inflation fraud: the entire block reward can be replaced by miners rolling the
dice, attempting to create valid "fake" transactions. However, such a pure
implementation would put a floor on the lowest transaction fee possible, so
better to allow both transaction fee and subsidy collection at the same time.
# References
[^paypub] [^timelock] [^zkcp] [^rpow]

@_date: 2016-06-21 18:10:08
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
I'll second that statement.
Ease of use isn't a very good criteria for security-critical software handling
money, and the JSON standard has a very large amount of degrees of freedom in
how people have implemented it historically. Even protobuf I'd personally avoid
using on that basis, as protobuf encoding isn't deterministic: you can encode
the same data in multiple ways.
Unfortunately there isn't a viable alternative, so we're probably stuck with
protobuf right now for standards that want to see wide adoption in the near
future; I've got a few projects that need an alternative, which I'm working on,
but that's a ways off.

@_date: 2016-06-21 18:13:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
Note that "client supplied identification" is being pushed for AML/KYC
compliance, e.g. Netki's AML/KYC compliance product:
This is an extremely undesirable feature to be baking into standards given it's
negative impact on fungibility and privacy; we should not be adopting standards
with AML/KYC support, for much the same reasons that the W3C should not be
standardizing DRM.

@_date: 2016-06-21 18:19:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
How else would you have keybase accomplish what they're accomplishing, with the
same security model?

@_date: 2016-06-21 18:42:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Building Blocks of the State Machine Approach to 
I think you've misunderstood what I'm proposing. The state machine approach I
described doesn't necessarily require blocks or even miners to exist at all.
Rather, it assumes that a single-use seal primitive is available, and a random
beacon primitive for tx linearization, and then builds a system on top of those
primitives. Transaction data - the proofs that certain states have been reached
in the system - does not need to be broadcast publicly; if Alice wants to
convince Bob that she has given him money, the only person who needs that
transaction (and transactions prior to it in the tx history) is Bob.
So as to your question about miners assembling blocks, and what blocks contain:
there doesn't need to be blocks at all! Transaction history linearization is
something your wallet would do for you.

@_date: 2016-06-21 19:02:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
It's easy to confirm who is sending you money: give out different addresses to
different people, and keep those addresses private.

@_date: 2016-06-22 07:10:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] Closed Seal Sets and Truth Lists for Better Privacy 
At the recent coredev.tech meetup in Zurich I spent much of my time discussing
anti-censorship improvements with Adam Back, building on his idea of blind
symmetric commitments[^bsc], and my own ideas of client-side verification. Our
goal here is to combat censorship by ensuring that miners do not have the
information needed to selectively censor (blacklist) transactions, forcing them
to adopt a whitelist approach of allowed transactions if they choose to censor.
Back's work achieves that by changing the protocol such that users commit to
their transaction in advance, in such a way that the commitment doesn't contain
the information necessary to censor the transaction, although after commitment
all transactional information becomes available. Here we propose a similar
scheme with using "smart contract" state machine tooling, with the potential
for an even better Zerocash-like guarantee that only a subset of data ever
becomes public, without requiring "moon math" of uncertain security.
# The Closed Seal Maps
To implement Single-Use Seals we propose that miners attest to the contents of
a series of key:value maps of true expressions, with the keys being the
expressions, and the values being commitments, which along with (discardable)
witnesses make up the argument to the expression. Once an expression is added
to the closed seal map, the value associated with it can't be changed.
Periodically - perhaps once a year - the most recent map is archived, and the
map is started fresh again. Once archived a closed seal map is never changed.
Miners are expected to keep the contents of the current map, as well as the
most recent closed seal map - the contents of older maps are proven on demand
using techniques similar to TXO commitments.
A single-use seal[^sma] implemented with the closed seal maps is then
identified by the expression and a block height. The seal is open if the
expression does not exist in any closed seal maps between the creation block
height and the most recent block height. A witness to the fact that the seal
has been closed is then a proof that the seal was recorded as closed in one of
the closed seal maps, and (if needed) proof that the seal was still open in any
prior maps between its creation and closing.
Similar to the logic in Bitcoin's segregated witnesses proposal, separating the
commitment and witness arguments to the seal expression ensures that the
witness attesting to the fact that a given seal was closed does not depend on
the exact signature used to actually close it.
Here's a very simple example of such a seal expression, in the author's
Dex[^dex] expression language, for an application that can avoid reusing
     (checksig   (hash ))
This desugars to the following after all named arguments were replaced by
explicit destructuring of the expression argument, denoted by the arg symbol:
    (and          (checksig  (cdr arg) (digest (car arg))))
The arguments to the expression are the closed seal map's commitment and
witness, which are our committed value and signature respectively:
    ( . )
 The Truth List
We implement an expression validity oracle by having miners attest to the
validity of a perpetually growing list of true predicate expressions, whose
evaluation can in turn depend on depend on previously attested expressions in
the truth list. SPV clients who trust miners can use the truth list to skip
validation of old history.
Similar to TXO commitments, we expect miners to have a copy of recent entries
in the truth list, perhaps the previous year. Older history can be proven on an
as-needed basis. Unlike TXO commitments, since this is a pure list of valid
expressions, once an item is added to the list it is never modified.
As the truth list can include expressions that reference previously
evaluated expressions, expressions of arbitrary depth can be evaluated. For
example, suppose we have an extremely long linked list of numbers, represented
as the following sexpr:
    (i_n i_n-1 i_n-2 ... i_1 i_0)
We want to check that every number in the list is even:
    (defun all-even? (l)
        (match l
            (nil true)
            ((n . rest) (if (mod n 2)
                            false
                            (all-even? rest)))))
In any real system this will fail for a sufficiently long list, either due to
stack overflow, or (if tail recursion is supported) due to exceeding the
anti-DoS limits on cycles executed in one expression; expressing the above may
even be impossible in expression systems that don't allow unbounded recursion.
A more subtle issue is that in a merkelized expression language, an expression
that calls itself is impossible to directly represent: doing so creates a cycle
in the call graph, which isn't possible without breaking the hash function. So
instead we'll define the special symbol self, which triggers a lookup in the
truth map instead of actually evaluating directly. Now our expression is:
    (defun all-even? (l)
        (match l
            (nil true)
            ((n . rest) (if (mod n 2)
                            false
                            (self rest)))))
We evaluate it in parts, starting with the end of the list. The truth list only
attests to valid expressions - not arguments - so we curry the argument to form
the following expression:
    (all-even? nil)
The second thing that is appended to the truth list is:
    (all-even? (0 . Note how we haven't actually provided the cdr of the cons cell - it's been
pruned and replaced by the digest of nil. With an additional bit of metadata -
the index of that expression within the trust list, and possibly a merkle path
to the tip if the expression has been archived - we can show that the
expression has been previously evaluated and is true.
Subsequent expressions follow the same pattern:
    (all-even? (1 . Until finally we reach the last item:
    (all-even? (n_i . Now we can show anyone who trusts that the truth list is valid - like a SPV
client - that evaluating all-even? on that list returns true by extracting a
merkle path from that item to the tip of the list's MMR commitment.
# Transactions
When we spend an output our goal is to direct the funds spent to a set of
outputs by irrovocably committing single-use seals to that distribution of
outputs. Equally, to validate an output we must show that sufficient funds have
been directed assigned to it. However, our anti-censorship goals make this
difficult, as we'll often want to reveal some information about where funds
being spend are going immediately - say to pay fees - while delaying when other
information is revealed as long as possible.
To achieve this we generalize the idea of a transaction slightly. Rather than
simply having a set of inputs spent and outputs created, we have a set of
_input splits_ spent, and outputs created. An input split is then a merkle-sum
map of nonces:values that the particular input has been split into; the
transaction commits to a specific nonce within that split, and is only valid if
the seal for that input is closed over a split actually committing to the
Secondly, in a transaction with multiple outputs, we don't want it to be
immediately possible to link outputs together as seals associated with them are
closed, even if the transaction ID is known publicly. So we associate each
output with a unique nonce.
Thus we can uniquely identify a specific transaction output - an outpoint - by
the following data (remember that the tx would usually be pruned, leaving just
the digest):
    (struct outpoint
        (tx     :transaction)
        (nonce  :digest))
An transaction output is defined as:
    (struct txout
        (value     :int)    ; value of output
        (nonce     :digest)
        (authexpr  :func))  ; authorization expression
An input:
    (struct txin
        (prevout :outpoint) ; authorization expression
        (split   :digest)   ; split nonce
        (value   :int))     ; claimed value of output spent
And a transaction:
    (struct transaction
        ; fixme: need to define functions to extract sums and keys
        (inputs   :(merkle-sum-map  (:digest :txin))
        (outputs  :(merkle-sum-map  (:digest :txout))
        ; and probably more metadata here)
 Spending Outputs
Our single-use seal associated with a specific output is the expression:
    (  . arg)
When the seal is closed it commits to the merkle-sum split map, which is
indexed by split nonces, one per (tx, value) pair committed to.  This means
that in the general case of an spend authorization expression that just checks
a signature, the actual outpoint can be pruned and what actually gets published
in the closed seal set is just:
    (  . arg)
Along with the commitment:
    #
With the relevant data hidden behind opaque digests, protected from
brute-forcing by nonces, external observers have no information about what
transaction output was spent, or anything about the transaction spending that
output. The nonce in the seal commitment prevents that multiple spends for the
same transaction from being linked together.  Yet at the same time, we're still
able to write a special-purpose spend auth expressions that do inspect the
contents of the transaction if needed.
 Validating Transactions
When validating a transaction, we want to validate the least amount of data
possible, allowing the maximum amount of data to be omitted for a given
recipient. Thus when we validate a transaction we _don't_ validate the outputs;
we only validate that the inputs spent by the transaction are valid, and the
sum of (split) inputs spent is correct. We only need to validate outputs when
they're spent - until then an invalid output is of no relevance. We also don't
need to validate any outputs other than the ones we're trying to spend - the
merkle sum tree guarantees that regardless of what's going on with other
outputs, the funds we're spending are uniquely allocated to us.
This means our function to check that a transaction is valid won't check the
outputs of the transaction itself, but will check outputs of previous
    (defun valid-tx? (tx)
        (map-reduce tx.inputs
            (lambda (txin)
                (and                      (valid-output? txin.prevout)))))
# Censorship Resistant Usage
To make use of the separation between seal closure and validation we need to
pass transaction information from peer to peer. Let's look at what happens when
Alice pays Bob:
1. Alice picks one or more inputs to spend.
2. For each input she constructs a split, paying part of the funds to a
per-input fee transaction with no outputs, and committing part of the funds to
the transaction paying Bob. If she has change left over she'll construct a
third transaction with just that change as an input.
3. She signs each input, creating valid signatures for the corresponding
output's seal's authorization expression.
4. She broadcasts the subset of data corresponding to just the fee paying
transactions and related signatures individually, with a time delay between
each one. All other data is pruned, leaving just opaque digests.
5. Once all inputs are confirmed, she gives Bob the data corresponding to his
transaction, including the relevant parts of the merkle trees, and relevant
closed seal witnesses.
At this point, a whole bunch of seals have been closed, but there's absolutely
nothing on chain that links them together. Now let's suppose Bob pays Charlie,
using the funds Alice gave him, and a different input to pay mining fees:
1. Bob constructs a fee paying transaction, splitting some funds from a
previously revealed output, and depending on the seal for the output Alice gave
him, but without spending any of that output's funds.
2. Bob broadcasts the above publicly. Miners have to add both seals to the
closed seal set to collect the fees.
3. Once confirmed, Bob gives Charlie the corresponding transaction information
for his output, as well as the still-private information it depends on to prove
that the output Alice created for Bob is itself valid.
Again, nearly none of the information related to the transaction is public, yet
the funds have moved twice.
 Pruning Old History
Over time the proofs that a coin is valid will grow as each additional
transaction adds more data. We shorten these proofs by publishing some of the
data in the form of additions to the truth list of valid expressions,
specifically the is-valid-tx? expressions that determine whether or not a
transaction (and prior transactions) are valid. This allows SPV clients who
trust miners to stop validating once they reach that old history.
Secondly, with transaction history linearization[^sma] we can avoid ever
revealing most of the transaction data, greatly improving privacy. Only one
input per transaction needs to be proven, so all data related to other inputs
can be discarded permanently; in practice this will lead to either one or two
public inputs, including the input made public to pay mining fees.
# "Smart Contracts"
Privacy aside, the combination of single-use seal and true expressions list
enables all known "smart contract" applications, such as the ones Ethereum
currently targets. After all, the accounts-based Ethereum architecture can
always be simulated with a series of single-use seal's that explicitly keeps
track of of an account balance based on actions taken.
# Open Questions
1. How does the above architecture interact with scaling proposals, like
sharding? Fraud proofs?
2. How does the statistical inflation protection of transaction history
linearization work in a real economy, e.g. if people use it gamble with their
3. PoW isn't a perfect random beacon; how do we take that into account when
designing linearization?
4. How do wallets pass proof data between each other, e.g. offline?
5. How do wallets backup proof data? (similar problem that Lightning has)
# References
[^bsc]: "blind symmetric commitment for stronger byzantine voting resilience",
        Adam Back, May 15th 2013, bitcoin-dev mailing list,
        [^sma]: "Building Blocks of the State Machine Approach to Consensus",
        Peter Todd, Jun 20th 2016,
                [^dex]: "Dex: Deterministic Predicate Expressions for Smarter Signatures",
        Peter Todd, May 25th 2016,

@_date: 2016-06-23 06:56:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
In some (most?) countries, it is illegal to offer telecoms services without
wiretap facilities. Does that mean Tor builds into its software "open source"
"open standards" wiretapping functionality? No. And interestingly, people
trying to add support for that stuff is actually a thing that keeps happening
in the Tor community...
In any case, I'd strongly argue that we remove BIP75 from the bips repository,
and boycott wallets that implement it. It's bad strategy for Bitcoin developers
to willingly participate in AML/KYC, just the same way as it's bad for Tor to
add wiretapping functionality, and W3C to support DRM tech. The minor tactical
wins you'll get our of this aren't worth it.

@_date: 2016-06-23 07:11:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] Building Blocks of the State Machine Approach to 
Useful yes, but actually implementing that often results in systems that are
too tightly coupled to scale well.
What do you mean by "new data"?
The point I'm making is simply that to be useful, when you close a seal you
have to be able to close it over some data, in particular, another seal. That's
the key thing that makes the idea a useful construct for smart contacts, value
transfer/currency systems, etc.
I did describe some seal authorization condition functions in my more recent
post; the key thing is you'd have some kind of "checksig" operator that checks
a cryptographic signature.
Thanks for the links! Not at all surprising to me that there's a whole bunch of
projects working along those same lines; it's the obvious way to build this
kind of stuff once you realise that the imperative, stateful, model isn't

@_date: 2016-06-23 07:21:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Building Blocks of the State Machine Approach to 
You know, I'm kinda regretting not making it sufficiently clear that Dex isn't
Lisp... It may look like it with all the braces, but expressions in it are
evaluated without any global state (they can be evaluated in parallel) and I've
got a lot of work ahead of me in type safety.
I'd be surprised if you could find a scheme interpreter that's sufficiently
well defined to be suitable for that; starting with an existing one and
whipping it into shape would very likely be more work than starting from
Yeah, in general I'd expect most of these systems to be layered to a degree;
after all even in something like MAST you need tooling to manage the fact that
the opcodes that end up public, on-chain, are only a subset of the script.
It's probably the best of a lot of bad alternatives... We use C++ not because
it's good, but because there's no other option.
In particular, we have enormous cost and risk in moving to other things due to
consensus, so making use of other languages is very difficult; my work with
dex/proofchains does not have that constraint.
No, I think you've very much misunderstood things. The abstract notion of a
single-use seal doesn't even need global consensus on anything to implement; it
does not require transactions to have "indexes"

@_date: 2016-06-23 07:39:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
For the record, I think the idea of the bips repo being a pure publication
platform isn't a good one and doesn't match reality; like it or not by
accepting bips we're putting a stamp of some kind of approval on them.
For example, I suspect I wouldn't be able to get a BIP for a decentralized
assassination market protocol standard into the repository, regardless of
whether or not it was used - it's simply too distastful and controversial for
us to want to merge that. Would you call that rejection censorship?
I have zero issues with us exercising editorial control over what's in the bips
repo; us doing so doesn't in any way prevent other's from publishing elsewhere.

@_date: 2016-06-23 08:10:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
That's simply not how the rest of the community perceives bips, and until we
move them elsewhere that's not going to change.
No matter how much we scream that we don't have authority, the fact of the
matter is the bips are located under the github.com/bitcoin namespace, and we
do have editorial control over them.
Right, so you accept that we'll exert some degree of editorial control; the
question now is what editorial policies should we exert?
My argument is that rejecting BIP75 is something we should do on
ethical/strategic grounds. You may disagree with that, but please don't troll
and call that "advocating censorship"

@_date: 2016-06-23 08:43:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
In the future we're likely to see a lot of BIPs around AML/KYC support, e.g.
adding personal identity information to transactions, blacklist standards, etc.
Should we accept those BIPs into the bips repo?

@_date: 2016-06-24 18:23:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Building Blocks of the State Machine Approach to 
Not necessarily. In my writeup I outlined two ways that those chains can be
shortened: trusted validity oracles and the probabalistic, inflationary,
history proof concept.
Equally, even if history grows over time, that's no worse than Bitcoin.
Alice isn't _creating_ a tokne, she's _defining_ a token.
In Alice's token definition, the genesis state of the token is defined to be
associated with a specific single-use seal. To transfer the token to Bob, she
asks Bob for the seal he wishes to use, and then closes the genesis seal over a
new state committing to Bob's seal.
Now Alice could construct the seal for Bob, in which case she'd just need to
know the auth expression Bob wants to use, but that's not the most fundamental
way of implementing this.
Regardless, the seal oracle doesn't need to know that any of the above is
happening; all it needs to do is spit out seal closed witnesses when the
authorization expressions are satisfied appropriately; the oracle does not and
should not know what the seals have been closed over. Whether or not the oracle
stores anything when seals are closed is an implementation decision - see my
original writeup on the unbounded vs. bounded oracle case. And of course, seals
implemented with decentralized blockchains are a different matter entirely.
Yes, as I mentioned above, there exists multiple techniques that can shorten
history proofs in a variety of ways, depending on what kinds of tradeoffs your
application needs.

@_date: 2016-06-28 14:22:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 
Being able to easily detect MITM attacks is a _huge_ step forward that
shouldn't be underestimated; even if 99% of users aren't in a position to
detect the MITM you only need a small subset of users that do the necessary
checks to alert the wider community, who can then respond with stronger
security measures. Those measures are likely to be more costly - authenticated
systems are significantly harder than not - so better to save your efforts
until the need for them is more obvious.
Also the fact that an attack has a reasonable probability of detection is a big
disincentive for many types of attackers - note how one of the things revealed
in the Snowden leaks was the fact that the NSA generally tries quite hard to
avoid tipping off targets to the fact that they're being surveilled, with a
myriad of carefully scripted policies to control when and how exploits are used
against targets.

@_date: 2016-06-28 16:14:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 
BIP151 gives users the tools to detect a MITM attack.
It's kinda like PGP in that way: lots of PGP users don't properly check keys,
so an attacker won't have a hard time MITM attacking those users. But some
users do check keys, a labor intensive manual process, but not a process that
requires any real cryptographic sophistication, let alone writing any code.
It's very difficult for widescale attackers to distinguish the users who do
check keys from the ones that don't, so if you MITM attack _any_ user you run
the risk of running into one of the few that does check, and those users can
alert everyone else.
The key thing, is we need to get everyones communications encrypted first: if
we don't the MITM attacker can intercept 99% of the communications with 0% risk
of detection, because the non-sophisticated users are trivially distinguishable
from the sophisticated users: just find the users with unencrypted

@_date: 2016-06-28 16:36:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 
Easy: anonymous peers aren't always actually anonymous.
A MITM attacker can't easily distinguish communications between two nodes that
randomly picked their peers, and nodes that are connected because their
operators manually used -addnode to peer; in the latter case the operators can
check whether or not they're being attacked with an out-of-band key check.

@_date: 2016-06-29 16:13:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 use of HMAC_SHA512 
What's the rational for doing that "directly" rather than with two SHA256
operations? (specifcially SHA256(0 . thing), SHA256(1 + thing) for the two
parts we need to derive)
Reducing the # of basic cryptographic primitives you need to implement a
standard needs is a good thing.

@_date: 2016-06-30 12:52:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 
To be clear, are you against Bitcoin Core's tor support?
Because node-to-node connections over tor are encrypted, and make use of onion
addresses, which are self-authenticated in the exact same way as BIP151
proposes. And we're shipping that in production as of 0.12.0, and by default
Tor onion support is enabled and will be automatically setup if you have a
recent version of Tor installed.
Does that "create pressure to expand node identity"?

@_date: 2016-06-30 15:06:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 151 
I don't think you answered my question.
Again, we _already have_ the equivalent of BIP151 functionality in Bitcoin
Core, shipping in production, but implemented with a Tor dependency.
BIP151 removes that dependency on Tor, enabling encrypted connections
regardless of whether or not you have Tor installed.
So any arguments against BIP151 being implemented, are equally arguments
against our existing Tor onion support. Are you against that support? Because
if you aren't, you can't have any objections to BIP151 being implemented

@_date: 2016-03-02 13:20:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
Changing the difficulty adjustment algorithm significantly changes the
security of the whole system, as it lets attackers create fake chains
with a lot less hashing power.
Given as tx fees rise this problem will hopefully be a one-time issue, a
simple fixed difficulty adjustment probably makes sense. No need to
bring in new algorithms here with controversial new security tradeoffs.

@_date: 2016-03-02 18:02:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
You know, I do agree with you.
But see, this is one of the reasons why we keep reminding people that
strictly speaking a hardfork *is* an altcoin, and the altcoin can change
any rule currently in Bitcoin.
It'd be perfectly reasonable to create an altcoin with a 22-million-coin
limit and an inflation schedule that had smooth, rather than abrupt,
drops. It'd also be reasonable to make that altcoin start with the same
UTXO set as Bitcoin as a means of initial coin distribution.
If miners choose to start mining that altcoin en-mass on the halving,
all the more power to them. It's our choice whether or not we buy those
coins. We may choose not to, but if 95% of the hashing power decides to
go mine something different we have to accept that under our current
chosen rules confirmations might take a long time.
Of course, personally I agree with Gregory Maxwell: this is all fairly
unlikely to happen, so the discussion is academic. But we'll see.

@_date: 2016-03-03 10:28:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] consensus rule change for TX fee safety 
Bitcoin Core already implements this safety limit with the "absurd fee"
limit of 10000 * the minimum relay fee. This limit is active in both the
wallet and the sendrawtransaction RPC call. Additionally for the wallet
there is a user configurable -maxtxfee option to limit fees set by the
wallet which currently defaults to 0.1 BTC.

@_date: 2016-05-02 22:05:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] segwit subsidy and multi-sender (coinjoin) 
Note how this is obviously an unsustainable situation - at some point that
change needs to be combined again, or you're throwing away money in the form of
UTXO's that aren't ever getting spent.
Meanwhile, if you put it another way the segwit discount is an obvious
advantage for coinjoin: by making spending UTXO's cheaper, we can recover those
funds that would otherwise get lost to dust, becoming ever more difficult to

@_date: 2016-05-09 13:40:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
Can you please explain why you moved the above part of gmaxwell's reply to here, when previously it was right after:
Editing gmaxwells reply like that changes the tone of the message significantly.

@_date: 2016-05-10 14:57:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
As part of the hard-fork proposed in the HK agreement(1) we'd like to make the
patented AsicBoost optimisation useless, and hopefully make further similar
optimizations useless as well.
What's the best way to do this? Ideally this would be SPV compatible, but if it
requires changes from SPV clients that's ok too. Also the fix this should be
compatible with existing mining hardware.
1) 2)

@_date: 2016-05-11 18:50:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
That's why we're asking the market right now, and any actual hard-fork to make
AsicBoost irrelevant would be voted on by miners themselves and in turn, the
economic majority, again letting the market collectively decide.

@_date: 2016-05-11 19:01:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
First of all, we can easily do this in a way where miners show their support
for this change, say with the usual 95% approval threshold we've been using for
soft-forks. That gets the % of hashing power on a AsicBoost chain fork down to
5% at most.
Secondly, we can probably make the consensus PoW allow blocks to be mined using
both the existing PoW algorithm, and a very slightly tweaked version where
implementing AsicBoost gives no advantage. That removes any incentive to
implement AsicBoost, without making any hardware obsolete (such as 21inc's
hardware). This means that no hashing power at all needs to use the AsicBoost
Obviously, the fact that miners can support such a change (assuming of course
the economic majority approves it as well) changes the negotiation position re:
licensing fees; the actual outcome may simply be you guys make the patent 100%
public for all to use at a much reduced price, given you're lack of negotiation
I think _patented_ optimizations where one party has a monopoly are very
different than optimizations that anyone can independently rediscover -
AsicBoost itself looks to be something that two or three parties independently
...which is a scenario that may result in a dozen patented optimizations, with
new ASIC manufacturers needing a dozen licenses, from potentially hostile
For instance, it's not clear to me if you actually own this patent, or
Cointerra's creditors. Obviously in the latter case, it'd be quite possible
that some kind of bankrupcy court ruling results in the patent getting sold to
a hostile entity who will use it against all of Bitcoin. Equally, even if it is
100% owned by you and Sergio, it'd be very easy for a personal bankrupcy to
result in the same scenario (suppose you get into a car accident and lose a
negligence lawsuit over it).

@_date: 2016-05-12 01:58:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
If others are found that are significant I think we'd definitely consider fighting them as well.

@_date: 2016-05-12 02:33:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
The optimisation has been independently discovered two or three times (Spondoolies and maybe Bitmain).

@_date: 2016-05-17 09:23:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With Low-Latency 
# Motivation
UTXO growth is a serious concern for Bitcoin's long-term decentralization. To
run a competitive mining operation potentially the entire UTXO set must be in
RAM to achieve competitive latency; your larger, more centralized, competitors
will have the UTXO set in RAM. Mining is a zero-sum game, so the extra latency
of not doing so if they do directly impacts your profit margin. Secondly,
having possession of the UTXO set is one of the minimum requirements to run a
full node; the larger the set the harder it is to run a full node.
Currently the maximum size of the UTXO set is unbounded as there is no
consensus rule that limits growth, other than the block-size limit itself; as
of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,
which expands to significantly more in memory. UTXO growth is driven by a
number of factors, including the fact that there is little incentive to merge
inputs, lost coins, dust outputs that can't be economically spent, and
non-btc-value-transfer "blockchain" use-cases such as anti-replay oracles and
We don't have good tools to combat UTXO growth. Segregated Witness proposes to
give witness space a 75% discount, in part of make reducing the UTXO set size
by spending txouts cheaper. While this may change wallets to more often spend
dust, it's hard to imagine an incentive sufficiently strong to discourage most,
let alone all, UTXO growing behavior.
For example, timestamping applications often create unspendable outputs due to
ease of implementation, and because doing so is an easy way to make sure that
the data required to reconstruct the timestamp proof won't get lost - all
Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay
use-cases like using the UTXO set for key rotation piggyback on the uniquely
strong security and decentralization guarantee that Bitcoin provides; it's very
difficult - perhaps impossible - to provide these applications with
alternatives that are equally secure. These non-btc-value-transfer use-cases
can often afford to pay far higher fees per UTXO created than competing
btc-value-transfer use-cases; many users could afford to spend $50 to register
a new PGP key, yet would rather not spend $50 in fees to create a standard two
output transaction. Effective techniques to resist miner censorship exist, so
without resorting to whitelists blocking non-btc-value-transfer use-cases as
"spam" is not a long-term, incentive compatible, solution.
A hard upper limit on UTXO set size could create a more level playing field in
the form of fixed minimum requirements to run a performant Bitcoin node, and
make the issue of UTXO "spam" less important. However, making any coins
unspendable, regardless of age or value, is a politically untenable economic
# TXO Commitments
A merkle tree committing to the state of all transaction outputs, both spent
and unspent, we can provide a method of compactly proving the current state of
an output. This lets us "archive" less frequently accessed parts of the UTXO
set, allowing full nodes to discard the associated data, still providing a
mechanism to spend those archived outputs by proving to those nodes that the
outputs are in fact unspent.
Specifically TXO commitments proposes a Merkle Mountain Range? (MMR), a
type of deterministic, indexable, insertion ordered merkle tree, which allows
new items to be cheaply appended to the tree with minimal storage requirements,
just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
never removed; if an output is spent its status is updated in place. Both the
state of a specific item in the MMR, as well the validity of changes to items
in the MMR, can be proven with log2(n) sized proofs consisting of a merkle path
to the tip of the tree.
At an extreme, with TXO commitments we could even have no UTXO set at all,
entirely eliminating the UTXO growth problem. Transactions would simply be
accompanied by TXO commitment proofs showing that the outputs they wanted to
spend were still unspent; nodes could update the state of the TXO MMR purely
from TXO commitment proofs. However, the log2(n) bandwidth overhead per txin is
substantial, so a more realistic implementation is be to have a UTXO cache for
recent transactions, with TXO commitments acting as a alternate for the (rare)
event that an old txout needs to be spent.
Proofs can be generated and added to transactions without the involvement of
the signers, even after the fact; there's no need for the proof itself to
signed and the proof is not part of the transaction hash. Anyone with access to
TXO MMR data can (re)generate missing proofs, so minimal, if any, changes are
required to wallet software to make use of TXO commitments.
 Delayed Commitments
TXO commitments aren't a new idea - the author proposed them years ago in
response to UTXO commitments. However it's critical for small miners' orphan
rates that block validation be fast, and so far it has proven difficult to
create (U)TXO implementations with acceptable performance; updating and
recalculating cryptographicly hashed merkelized datasets is inherently more
work than not doing so. Fortunately if we maintain a UTXO set for recent
outputs, TXO commitments are only needed when spending old, archived, outputs.
We can take advantage of this by delaying the commitment, allowing it to be
calculated well in advance of it actually being used, thus changing a
latency-critical task into a much easier average throughput problem.
Concretely each block B_i commits to the TXO set state as of block B_{i-n}, in
other words what the TXO commitment would have been n blocks ago, if not for
the n block delay. Since that commitment only depends on the contents of the
blockchain up until block B_{i-n}, the contents of any block after are
irrelevant to the calculation.
 Implementation
Our proposed high-performance/low-latency delayed commitment full-node
implementation needs to store the following data:
1) UTXO set
    Low-latency K:V map of txouts definitely known to be unspent. Similar to
    existing UTXO implementation, but with the key difference that old,
    unspent, outputs may be pruned from the UTXO set.
2) STXO set
    Low-latency set of transaction outputs known to have been spent by
    transactions after the most recent TXO commitment, but created prior to the
    TXO commitment.
3) TXO journal
    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
    must be low-latency; removals can be high-latency.
4) TXO MMR list
    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,
    backed by a reference counted, cryptographically hashed object store
    indexed by digest (similar to how git repos work). High-latency ok. We'll
    cover this in more in detail later.
 Fast-Path: Verifying a Txout Spend In a Block
When a transaction output is spent by a transaction in a block we have two
1) Recently created output
    Output created after the most recent TXO commitment, so it should be in the
    UTXO set; the transaction spending it does not need a TXO commitment proof.
    Remove the output from the UTXO set and append it to the TXO journal.
2) Archived output
    Output created prior to the most recent TXO commitment, so there's no
    guarantee it's in the UTXO set; transaction will have a TXO commitment
    proof for the most recent TXO commitment showing that it was unspent.
    Check that the output isn't already in the STXO set (double-spent), and if
    not add it. Append the output and TXO commitment proof to the TXO journal.
In both cases recording an output as spent requires no more than two key:value
updates, and one journal append. The existing UTXO set requires one key:value
update per spend, so we can expect new block validation latency to be within 2x
of the status quo even in the worst case of 100% archived output spends.
 Slow-Path: Calculating Pending TXO Commitments
In a low-priority background task we flush the TXO journal, recording the
outputs spent by each block in the TXO MMR, and hashing MMR data to obtain the
TXO commitment digest. Additionally this background task removes STXO's that
have been recorded in TXO commitments, and prunes TXO commitment data no longer
Throughput for the TXO commitment calculation will be worse than the existing
UTXO only scheme. This impacts bulk verification, e.g. initial block download.
That said, TXO commitments provides other possible tradeoffs that can mitigate
impact of slower validation throughput, such as skipping validation of old
history, as well as fraud proof approaches.
 TXO MMR Implementation Details
Each TXO MMR state is a modification of the previous one with most information
shared, so we an space-efficiently store a large number of TXO commitments
states, where each state is a small delta of the previous state, by sharing
unchanged data between each state; cycles are impossible in merkelized data
structures, so simple reference counting is sufficient for garbage collection.
Data no longer needed can be pruned by dropping it from the database, and
unpruned by adding it again. Since everything is committed to via cryptographic
hash, we're guaranteed that regardless of where we get the data, after
unpruning we'll have the right data.
Let's look at how the TXO MMR works in detail. Consider the following TXO MMR
with two txouts, which we'll call state       0
     / \
    a   b
If we add another entry we get state         1
       / \
      0   \
     / \   \
    a   b   c
Note how it 100% of the state  data was reused in commitment  Let's
add two more entries to get state             2
           / \
          2   \
         / \   \
        /   \   \
       /     \   \
      0       2   \
     / \     / \   \
    a   b   c   d   e
This time part of state  wasn't reused - it's wasn't a perfect binary
tree - but we've still got a lot of re-use.
Now suppose state  is committed into the blockchain by the most recent block.
Future transactions attempting to spend outputs created as of state  are
obliged to prove that they are unspent; essentially they're forced to provide
part of the state  MMR data. This lets us prune that data, discarding it,
leaving us with only the bare minimum data we need to append new txouts to the
TXO MMR, the tips of the perfect binary trees ("mountains") within the MMR:
            2
           / \
          2   \
               \
                \
                 \
                  \
                   \
                    e
Note that we're glossing over some nuance here about exactly what data needs to
be kept; depending on the details of the implementation the only data we need
for nodes "2" and "e" may be their hash digest.
Adding another three more txouts results in state                   3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
                         / \
                        /   \
                       /     \
                      3       3
                     / \     / \
                    e   f   g   h
Suppose recently created txout f is spent. We have all the data required to
update the MMR, giving us state  It modifies two inner nodes and one leaf
                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
                         / \
                        /   \
                       /     \
                      4       3
                     / \     / \
                    e  (f)  g   h
If an archived txout is spent requires the transaction to provide the merkle
path to the most recently committed TXO, in our case state  If txout b is
spent that means the transaction must provide the following data from state             2
           /
          2
         /
        /
       /
      0
       \
        b
We can add that data to our local knowledge of the TXO MMR, unpruning part of
                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
         /               / \
        /               /   \
       /               /     \
      0               4       3
       \             / \     / \
        b           e  (f)  g   h
Remember, we haven't _modified_ state  yet; we just have more data about it.
When we mark txout b as spent we get state                   5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               / \
        /               /   \
       /               /     \
      5               4       3
       \             / \     / \
       (b)          e  (f)  g   h
Secondly by now state  has been committed into the chain, and transactions
that want to spend txouts created as of state  must provide a TXO proof
consisting of state  data. The leaf nodes for outputs g and h, and the inner
node above them, are part of state  so we prune them:
                  5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               /
        /               /
       /               /
      5               4
       \             / \
       (b)          e  (f)
Finally, lets put this all together, by spending txouts a, c, and g, and
creating three new txouts i, j, and k. State  was the most recently committed
state, so the transactions spending a and g are providing merkle paths up to
it. This includes part of the state  data:
                  3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
         / \               \
        /   \               \
       /     \               \
      0       2               3
     /       /               /
    a       c               g
After unpruning we have the following data for state                   5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         / \             / \
        /   \           /   \
       /     \         /     \
      5       2       4       3
     / \     /       / \     /
    a  (b)  c       e  (f)  g
That's sufficient to mark the three outputs as spent and add the three new
txouts, resulting in state                         6
                       / \
                      /   \
                     /     \
                    /       \
                   /         \
                  6           \
                 / \           \
                /   \           \
               /     \           \
              /       \           \
             /         \           \
            /           \           \
           /             \           \
          6               6           \
         / \             / \           \
        /   \           /   \           6
       /     \         /     \         / \
      6       6       4       6       6   \
     / \     /       / \     /       / \   \
   (a) (b) (c)      e  (f) (g)      i   j   k
Again, state  related data can be pruned. In addition, depending on how the
STXO set is implemented may also be able to prune data related to spent txouts
after that state, including inner nodes where all txouts under them have been
spent (more on pruning spent inner nodes later).
 Consensus and Pruning
It's important to note that pruning behavior is consensus critical: a full node
that is missing data due to pruning it too soon will fall out of consensus, and
a miner that fails to include a merkle proof that is required by the consensus
is creating an invalid block. At the same time many full nodes will have
significantly more data on hand than the bare minimum so they can help wallets
make transactions spending old coins; implementations should strongly consider
separating the data that is, and isn't, strictly required for consensus.
A reasonable approach for the low-level cryptography may be to actually treat
the two cases differently, with the TXO commitments committing too what data
does and does not need to be kept on hand by the UTXO expiration rules. On the
other hand, leaving that uncommitted allows for certain types of soft-forks
where the protocol is changed to require more data than it previously did.
 Consensus Critical Storage Overheads
Only the UTXO and STXO sets need to be kept on fast random access storage.
Since STXO set entries can only be created by spending a UTXO - and are smaller
than a UTXO entry - we can guarantee that the peak size of the UTXO and STXO
sets combined will always be less than the peak size of the UTXO set alone in
the existing UTXO-only scheme (though the combined size can be temporarily
higher than what the UTXO set size alone would be when large numbers of
archived txouts are spent).
TXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum
overhead per entry: a unique merkle path to a TXO commitment (by "unique" we
mean that no other entry shares data with it). On a reasonably fast system the
TXO journal will be flushed quickly, converting it into TXO MMR data; the TXO
journal will never be more than a few blocks in size.
Transactions spending non-archived txouts are not required to provide any TXO
commitment data; we must have that data on hand in the form of one TXO MMR
entry per UTXO. Once spent however the TXO MMR leaf node associated with that
non-archived txout can be immediately pruned - it's no longer in the UTXO set
so any attempt to spend it will fail; the data is now immutable and we'll never
need it again. Inner nodes in the TXO MMR can also be pruned if all leafs under
them are fully spent; detecting this is easy the TXO MMR is a merkle-sum tree,
with each inner node committing to the sum of the unspent txouts under it.
When a archived txout is spent the transaction is required to provide a merkle
path to the most recent TXO commitment. As shown above that path is sufficient
information to unprune the necessary nodes in the TXO MMR and apply the spend
immediately, reducing this case to the TXO journal size question (non-consensus
critical overhead is a different question, which we'll address in the next
Taking all this into account the only significant storage overhead of our TXO
commitments scheme when compared to the status quo is the log2(n) merkle path
overhead; as long as less than 1/log2(n) of the UTXO set is active,
non-archived, UTXO's we've come out ahead, even in the unrealistic case where
all storage available is equally fast. In the real world that isn't yet the
case - even SSD's significantly slower than RAM.
 Non-Consensus Critical Storage Overheads
Transactions spending archived txouts pose two challenges:
1) Obtaining up-to-date TXO commitment proofs
2) Updating those proofs as blocks are mined
The first challenge can be handled by specialized archival nodes, not unlike
how some nodes make transaction data available to wallets via bloom filters or
the Electrum protocol. There's a whole variety of options available, and the
the data can be easily sharded to scale horizontally; the data is
self-validating allowing horizontal scaling without trust.
While miners and relay nodes don't need to be concerned about the initial
commitment proof, updating that proof is another matter. If a node aggressively
prunes old versions of the TXO MMR as it calculates pending TXO commitments, it
won't have the data available to update the TXO commitment proof to be against
the next block, when that block is found; the child nodes of the TXO MMR tip
are guaranteed to have changed, yet aggressive pruning would have discarded that
Relay nodes could ignore this problem if they simply accept the fact that
they'll only be able to fully relay the transaction once, when it is initially
broadcast, and won't be able to provide mempool functionality after the initial
relay. Modulo high-latency mixnets, this is probably acceptable; the author has
previously argued that relay nodes don't need a mempool? at all.
For a miner though not having the data necessary to update the proofs as blocks
are found means potentially losing out on transactions fees. So how much extra
data is necessary to make this a non-issue?
Since the TXO MMR is insertion ordered, spending a non-archived txout can only
invalidate the upper nodes in of the archived txout's TXO MMR proof (if this
isn't clear, imagine a two-level scheme, with a per-block TXO MMRs, committed
by a master MMR for all blocks). The maximum number of relevant inner nodes
changed is log2(n) per block, so if there are n non-archival blocks between the
most recent TXO commitment and the pending TXO MMR tip, we have to store
log2(n)*n inner nodes - on the order of a few dozen MB even when n is a
(seemingly ridiculously high) year worth of blocks.
Archived txout spends on the other hand can invalidate TXO MMR proofs at any
level - consider the case of two adjacent txouts being spent. To guarantee
success requires storing full proofs. However, they're limited by the blocksize
limit, and additionally are expected to be relatively uncommon. For example, if
1% of 1MB blocks was archival spends, our hypothetical year long TXO commitment
delay is only a few hundred MB of data with low-IO-performance requirements.
 Security Model
Of course, a TXO commitment delay of a year sounds ridiculous. Even the slowest
imaginable computer isn't going to need more than a few blocks of TXO
commitment delay to keep up ~100% of the time, and there's no reason why we
can't have the UTXO archive delay be significantly longer than the TXO
commitment delay.
However, as with UTXO commitments, TXO commitments raise issues with Bitcoin's
security model by allowing relatively miners to profitably mine transactions
without bothering to validate prior history. At the extreme, if there was no
commitment delay at all at the cost of a bit of some extra network bandwidth
"full" nodes could operate and even mine blocks completely statelessly by
expecting all transactions to include "proof" that their inputs are unspent; a
TXO commitment proof for a commitment you haven't verified isn't a proof that a
transaction output is unspent, it's a proof that some miners claimed the txout
was unspent.
At one extreme, we could simply implement TXO commitments in a "virtual"
fashion, without miners actually including the TXO commitment digest in their
blocks at all. Full nodes would be forced to compute the commitment from
scratch, in the same way they are forced to compute the UTXO state, or total
work. Of course a full node operator who doesn't want to verify old history can
get a copy of the TXO state from a trusted source - no different from how you
could get a copy of the UTXO set from a trusted source.
A more pragmatic approach is to accept that people will do that anyway, and
instead assume that sufficiently old blocks are valid. But how old is
"sufficiently old"? First of all, if your full node implementation comes "from
the factory" with a reasonably up-to-date minimum accepted total-work
threshold? - in other words it won't accept a chain with less than that amount
of total work - it may be reasonable to assume any Sybil attacker with
sufficient hashing power to make a forked chain meeting that threshold with,
say, six months worth of blocks has enough hashing power to threaten the main
chain as well.
That leaves public attempts to falsify TXO commitments, done out in the open by
the majority of hashing power. In this circumstance the "assumed valid"
threshold determines how long the attack would have to go on before full nodes
start accepting the invalid chain, or at least, newly installed/recently reset
full nodes. The minimum age that we can "assume valid" is tradeoff between
political/social/technical concerns; we probably want at least a few weeks to
guarantee the defenders a chance to organise themselves.
With this in mind, a longer-than-technically-necessary TXO commitment delay?
may help ensure that full node software actually validates some minimum number
of blocks out-of-the-box, without taking shortcuts. However this can be
achieved in a wide variety of ways, such as the author's prev-block-proof
proposal?, fraud proofs, or even a PoW with an inner loop dependent on
blockchain data. Like UTXO commitments, TXO commitments are also potentially
very useful in reducing the need for SPV wallet software to trust third parties
providing them with transaction data.
i) Checkpoints that reject any chain without a specific block are a more
   common, if uglier, way of achieving this protection.
j) A good homework problem is to figure out how the TXO commitment could be
   designed such that the delay could be reduced in a soft-fork.
 Further Work
While we've shown that TXO commitments certainly could be implemented without
increasing peak IO bandwidth/block validation latency significantly with the
delayed commitment approach, we're far from being certain that they should be
implemented this way (or at all).
1) Can a TXO commitment scheme be optimized sufficiently to be used directly
without a commitment delay? Obviously it'd be preferable to avoid all the above
complexity entirely.
2) Is it possible to use a metric other than age, e.g. priority? While this
complicates the pruning logic, it could use the UTXO set space more
efficiently, especially if your goal is to prioritise bitcoin value-transfer
over other uses (though if "normal" wallets nearly never need to use TXO
commitments proofs to spend outputs, the infrastructure to actually do this may
3) Should UTXO archiving be based on a fixed size UTXO set, rather than an
age/priority/etc. threshold?
4) By fixing the problem (or possibly just "fixing" the problem) are we
encouraging/legitimising blockchain use-cases other than BTC value transfer?
Should we?
5) Instead of TXO commitment proofs counting towards the blocksize limit, can
we use a different miner fairness/decentralization metric/incentive? For
instance it might be reasonable for the TXO commitment proof size to be
discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.
thinblocks) is used to ensure all miners have received the proof in advance.
6) How does this interact with fraud proofs? Obviously furthering dependency on
non-cryptographically-committed STXO/UTXO databases is incompatible with the
modularized validation approach to implementing fraud proofs.
# References
1) "Merkle Mountain Ranges",
   Peter Todd, OpenTimestamps, Mar 18 2013,
   2) "Do we really need a mempool? (for relay nodes)",
   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,
   3) "Segregated witnesses and validationless mining",
   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,

@_date: 2016-05-18 19:53:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
log2(n) operations.
I wrote a full MMR implementation with pruning support as part of my
proofchains work:
Documentation is a bit lacking, but I'd suggest reading the above source code
and the unit tests(1) to understand what's going on. As of writing item
retrieval by index is implemented(2), and if you follow how that works you'll
see it's log2(n) operations; changing elements in-place isn't yet
implemented(3) but would be a fun homework problem. I'll bet anyone a beer that
you'll find it can be done in k*log2(n) operations, with a reasonably small k. :)
Additionally, I also have a merkelized key:value prefix tree implementation
called a "merbinner tree" in the same library, again with pruning support. It
does implement changing elements in place(4) with log2(n) operations.
Incidentally, something I probably should have made more clear in my TXO
commitments post is that the original MMR scheme I developed for OpenTimestamps
(and independently reinvented for Certificate Transparency) is insufficient:
while you can easily extract a proof that an element is present in the MMR,
that inclusion proof doesn't do a good job of proving the position in the tree
very well. OpenTimestamps didn't need that kind of proof, and I don't think
Certificate Transparency needs it either. However many other MMR applications
do, including many types of TXO commitments.
My proofchains MMR scheme fixes this problem by making each inner node in the
MMR commit to the total number of elements under it(5) - basically it's a
merkle-sum-tree with the size of the tree being what's summed. There may be
more efficient ways to do this, but a committed sum-length is easy to
implement, and the space overhead is only 25% even in the least optimised
implementation possible.
1) 2) 3) 4) 5) Nope. The reason why this doesn't work is apparent when you ask how will the
STXO be indexed?
If it's indexed by outpoint - that is H(txid:n) - to update the STXO you need
he entire thing, as the position of any new STXO that you need to add to the
STXO tree is random.
OTOH, if you index the STXO by txout creation order, with the first txout ever
created having position  the second  etc. the data you may need to update
the STXO later has predictable locality... but now you have something that's
basically identical to my proposed insertion-ordered TXO commitment anyway.
Incidentally, it's interesting how if a merbinner tree is insertion-order
indexed you end up with a datastructure that's almost identical to a MMR.

@_date: 2016-05-20 04:45:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
The MMR proofs provided by transactions aren't proofs of *how* the MMR should
be be changd; they're just proofs that the MMR is in a certain state right now.
You're situation is just an example of a double-spend, that miners have to
detect if they don't want to create invalid blocks. Specifically, if I
understand your example correctly, they'd be rejected by the STXO set.

@_date: 2016-05-22 04:55:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
We're working along the same lines, but my proposal is much better fleshed out;
I think you'll find you missed a few details if you flesh out yours in more
detail. For instance, since your dormant UTXO list is indexed by UTXO
expiration order, it's not possible to do any kind of verification that the
contents of that commitment are correct without the global state of all UTXO
data - you have no ability to locally verify as nothing commits to the contents
of the UTXO set.

@_date: 2016-11-16 16:01:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
So, conceptually, another way to deal with this is to hardcode a blockhash
where we allow blocks in a chain ending with that blockhash to _not_ follow
BIP65, up until that blockhash, and any blockchain without that blockhash must
respect BIP65 for all blocks in the chain.
This is a softfork: we've only added rules that made otherwise valid chains
invalid, and at the same time we are still accepting large reorgs (albeit under
stricter rules than before).
I'd suggest we call this a exemption hash - we've exempted a particular
blockchains from a soft-forked rule that we would otherwise enforce.

@_date: 2016-11-17 03:44:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
If there is a tx hash collision it is almost certainly going to be because
SHA256 has become weak through advances in cryptography, much like MD5. If that
is the case, Bitcoin is fundementally broken because the blockchain no longer
can be relied upon to commit to a unique transaction history: miners would be
able to generate blocks that have SHA256 collisions in transactions and even
the merkle tree itself, making it possible to simultaneously mine two (or more)
contradictory transaction histories at once.
Meanwhile the probability of SHA256 _not_ being broken and a collision being
found is low enough that we should be more worried about earth-killing
asteroids and mutant sharks, among other things.
Quoting Bruce Schneier:
    These numbers have nothing to do with the technology of the devices; they are
    the maximums that thermodynamics will allow. And they strongly imply that
    brute-force attacks against 256-bit keys will be infeasible until computers are
    built from something other than matter and occupy something other than space.

@_date: 2016-10-02 12:17:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
I think your history of patenting(1) Bitcoin consensus relevant technology is
sufficient by itself to be extremely dubious of any proposals coming from you
or your colleagues; patents on Bitcoin consensus technology are a serious
threat to decentralization. Personally, I'm NACKing this proposal on that basis
You need to rectify this dangerous and unethical behavior in a convincing,
legally binding way. I'd suggest looking into Blockstream's patent pledges as a
way forward:
    I see no reason to have any further discussion of your proposal until this is
1) "AsicBoost is a patent-pending method to improve the efficiency and cost of Bitcoin mining by approximately 20%"

@_date: 2016-10-02 13:11:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
I've suggested a way that you can rectify this situation so we can continue to
collaborate: Have Rootstock adopt a legally binding patent pledge/license. I'd
suggest you do as Blockstream has done and at minimum adopt the Defensive
Patent License (DPL); I personally will be doing so in the next week or two for
my own consulting company (I'm discussing exactly how to do so with my lawyer
right now).
If Rootstock is not planning on getting any patents for offensive purposes,
then there is no issue with doing so - the DPL in particular is designed in a
minimally intrusive way.
Please fix this issue so we can in fact continue to collaborate to improve

@_date: 2016-10-02 13:24:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
I don't know if it is; that's the problem.
Given Sergio's prior behavior of attempting to use patents offensively, it's
perfectly reasonable to suspect that Rootstock does in fact intend to encumber
this proposal with patents. So the obvious thing to do, is for Rootstock to
give us all a legally binding guarantee that they will not be using patents
offensively, eliminating the problem and allowing us to return to productive
Remember that this kind of requirement is very common in standards bodies, e.g.
by having all companies contributing to the standards in question join a patent
pool, or by making legally binding pledges/licenses to ensure any patents they
hold can't be used offensively.

@_date: 2016-10-02 13:34:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
Thanks, please let us all know when this is done so we can continue our
collaborations constructively.
I'll likewise prioritise my own adoption of the DPL and will announce it on
this mailing list.

@_date: 2016-10-13 04:51:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] Defensive Patent License Offer Notice 
Also published at and Bitcoin txid b4bf94f5c457d080924aa163106d423670373cfe3b10f8ec00742c2234b01b72
    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA256
    I, Peter Todd, hereby declare myself and all technology companies that I
    control as "Defensive" by committing to offer a Defensive Patent License,
    version 1.1, for any of my patents either existing or future, to any DPL User.
    Neither I nor any companies that I control have any patents at this time.
    My contact address is pete at petertodd.org
    -----BEGIN PGP SIGNATURE-----
    iQEcBAEBCAAGBQJX/t11AAoJEGOZARBE6K+yR00H/0xp3oO7FiMvM4pjfoHZPPOa
    m3KjT4RSbFQLa9uniz0u/9bkc5I70CggkY3jtNLtDMbMBTwcMP61ABsvx+5y2gGD
    zE6VZ9DPcHVg/Eup6WSBlQO3HQKuFVz7vXSMuaidG7A+fpkU71SjDpB4M6hdvWnS
    +L9XBQ1GtQe0lSM73s4mld/IvB1giwPN1bOheQ9koYcQjj+B8PWyt2gIUwctxyvA
    7bC+KtCQT4RJPsQHbHx569CDkyIi3dNt0rTjCo5bOeUKrJF7eA3YktYdTJefZ+Rf
    00dbRZMslrg3dW9VWECfC0xC/kn+heStJ7WqJJKqYWo4apm6IiKPZxlwIcVscF0=
    =xrPk
    -----END PGP SIGNATURE-----
# Notes
* On the advice of my lawyer, I'm currently offering only a specific version of
  the DPL. I probably will offer licenses under subsequent versions in the
  future, but I'd prefer to see how the DPL evolves and whether or not the
  license stewards behind it prove trustworthy before committing to doing so.
* The language "all technology companies I control" is there to avoid any
  complications with non-technology companies that I may control in the future,
  e.g. family real-estate holding companies, and the non-profit caving group
  I'm a part of. To my knowledge, I only control one company as of writing, the
  numbered company I do all my consulting through; I consider that company a
  "technology company", and thus the above offer applies to it.
* Equally, if by some stroke of luck I do end up in control of any other
  technology companies - maybe Bill Gate's blockchain smart-contract will
  mysteriously gives me control of Microsoft - then the above offer will apply.

@_date: 2016-10-14 06:57:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] DPL is not only not enough, 
Glad to hear you're taking a conservative approach.
So I assume Rootstock is going to do something stronger then, like
Blockstream's DPL + binding patent pledge to only use patents defensively?
    Because if not, the DPL is still better than the status quo.
Indeed. However, you're also free to adopt the DPL irrevocably by additionally
stating that you will never invoke that 180-day notice provision (or more
humorously, make it a 100 year notice period to ensure any patents expire!).
If you're concerned about this problem, I'd suggest that Rootstock do exactly
To be clear, modulo the revocability provision, it's a danger mainly to those
who are unwilling to adopt the DPL themselves, perhaps because they support
software patents.
Again, lets remember that you personally proposed a BIP[1] that had the effect
of aiding your ASICBOOST patent[2] without disclosing that fact in your BIP nor
your pull-req[3]. The simple fact is we can't rely solely on voluntary
disclosure - your own behavior is a perfect example of why not.
[1]: BIP: [2]: ASICBOOST PATENT [3]: Extra nonce pull request: A serious problem here is the definition of "Bitcoin users". Does Bitcoin
Classic count? Bitcoin Unlimited? What if Bitcoin forks?
Better to grant _everyone_ a irrevocable license.
Along those lines, it'd be reasonable to consider changing the Bitcoin Core
license to something like an Apache2/LGPL3 dual license to ensure the copyright
license also has anti-patent protections.

@_date: 2016-10-14 08:31:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] DPL is not only not enough, 
Agreed. That's also one of the reasons (lesser) reasons why I didn't adopt a
patent pledge like blockstream has done. Though frankly the main reason is I'm
unlikely to be able to afford to get any patents anytime soon anyway, so it's
all symbolic and I'd rather spend as little as possible on lawyers. :) Also, my
standard contract that I use with clients prohibits me from getting patents on
work I do (and imposes financial penalties on clients who in turn try to apply
for patents on work derived from mine).
Indeed. For a codebase that is in large part both a reference implementation
and the very definition of the Bitcoin protocol, we do want a permissive
license to ensure that commercial users are able to use the Bitcoin protocol.
However there is no reason to extend that permissivity to allowing others to
attempt to restrict others' rights to use the Bitcoin protocol via patents.
Ah, actually I think I misremembered: it'd be Apache2.0/LGPL_v2_ where a dual
license would make sense; Apache2.0 is compatibile with (L)GPL3:
        (L)GPLv2 doesn't have the patent protections that (L)GPLv3 does, so my
suggestion is wrong; Apache2.0 by itself is perfectly good.
Yup, that'd be perfectly possible to do. Basically new contributions would be
licensed under the new, MIT-compatible, licenses. I did that myself with
python-bitcoinlib, as part of the codebase was licensed MIT, and part LGPLv2 or
later; to comply with the latter I changed the license for all new work to
LGPLv3 or later. Interestingly, this has lead to the Bitcoin Core unit tests
using an older version of python-bitcoinlib; kudo's goes to Suhas Daftuar for
dilligently respecting the new license.

@_date: 2016-10-16 17:49:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
Speaking as maintainer of python-bitcoinlib, ACK.
Currently python-bitcoinlib doesn't have any support for segwit, although Bob
McElrath has had a pull-req open for it since July:
    I may or may not get time to finishing reviewing and merging that pull-req
before segwit activates - I've been a rather distracted maintainer. But either
way, as has been explained elsewhere ad nauseam, segwit is backwards compatible
with existing nodes and wallets so there's no rush to upgrade.
For example, another project of mine - OpenTimestamps - also makes use of
python-bitcoinlib for the relatively complex and hairy low-level code that
extracts timestamp proofs from blocks, among other things. In fact, in the
development of OpenTimestamps I had to fix a few minor bugs in
python-bitcoinlib, because it exercised parts of the codebase that few other
projects do.
Yet the impact on segwit for OpenTimestamps will be zero - since segwit is a
softfork it's 100% backwards compatible with existing software. Of course, at
some point in the future I'll probably get around to adding segwit support to
the software to reduce transaction fees, but there's no rush to do so. All I'll
be doing for segwit in the near future is upgrading the full nodes on the two
redundant OpenTimestamps calendar servers to v0.13.1, and even there I'll be
able to stagger the upgrades to protect against the unlikely occurance of
v0.13.1 having a bug that v0.13.0 doesn't. Again, staggering full-node upgrades
is only possible because segwit is a soft-fork.

@_date: 2016-10-17 09:09:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
Please don't misleadingly reference/quote me.
I made it quite clear in my last post that because segwit is a backwards
compatible soft-fork, the vast majority of code out there will not have to
change; my own OpenTimestamps being a good example. All I'll have to do to
prepare for segwit is upgrade the (pruned) full nodes that the OpenTimestamps
servers depend on to determine what's the most-work valid chain, and in the
event I was concerned about compatibility issues, I could easily run my
existing nodes behind updated segwit-supporting (pruned) nodes.
Like most users, my OpenTimestamps code doesn't "fully understand" transactions
at all - I rely on my full node to do that for me. What it does understand
about transactions and blocks remains the same in segwit. I can receive
transactions from segwit users with lite-client security without any action at
all, and full-node security once I upgrade my full nodes (or run them behind
upgraded nodes).
Your proposed alternative to segwit - flexible transactions - has none of these
beneficial properties. And as Matt Corallo reported, it's no-where near ready
for deployment: three buffer overflows in 80 lines of code is a serious

@_date: 2016-09-02 05:47:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] New BIP: Dealing with dummy stack element 
This should say "for all scriptPubKey types in actual use, non-compliant
signatures can trivially be converted into compliant ones"
You can of course create a scriptPubKey where that's not possible, but
fortunately no-one appears to do that.
Also, as original author of NULLDUMMY, thanks for finally making it into a

@_date: 2016-09-10 00:58:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
Good to do this sooner rather than later, as alert propagation on the P2P
network is going to continue to get less reliable as nodes upgrade to software
that has removed alert functionality; better that the final alert key
retirement message is reliably seen by the remaining software out there in a
predictable way than this be something that happens unpredictably.

@_date: 2016-09-10 02:19:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
I think that's a good idea, and it's a simple way to document that final alert
as well.

@_date: 2016-09-18 00:20:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] Interpreting nTime for the purpose of 
As part of my recent work(1) on OpenTimestamps I've been putting some thought
towards how to interpret the nTime fields in block headers, for the purpose of
timestamping. I'd like to get some peer review on the following scheme I've
come up with.
# Motivation
We want to use the Bitcoin blockchain to provide evidence (the "attestation")
that a message M existed prior to some point in time T. Exactly how we do this
is beyond the scope of this post, but suffice to say we show that some block
header b cryptographically commits to the message, e.g. via a commitment
operation path proof, as implemented by OpenTimestamps.
A valid timestamp is simply one where T is a point in time where the message
did in fact exist. Of course, if a timestamp for time T is valid, all
subsequent T+d are also valid; such timestamps are simply more conservative
versions of the same statement.
A naively approach - as is implemented by most (all?) existing Bitcoin
timestamping schemes - is to assume that the block header's nTime field was
perfectly accurate, and thus M exists prior to the block's nTime. But that
doesn't take into account malicious miners, who may backdate their blocks.
# Threat Model
We assume miners are divided into two categories:
1) Dishonest Miners --- These miners are actively conspiring to create invalid
timestamps for time's prior to when the message existed. A dishonest miner will
set the nTime field in blocks they create to the minimum possible value.
2) Honest Miners --- These miners set nTime in blocks they create to
approximately the current true time. An honest miner may use techniques such as
nTime-rolling. Additionally, all honest miners may be simultaneously affected
by systematic misconfigurations.
 nTime Rolling
Prior to BIP113, reducing a block's nTime from the work given by a pool by even
a second could easily render it invalid, as the pool may have included
nLockTime'd transactions in the block. Thus hashing software was designed to
only roll nTime in the forward direction, not reverse, even though rolling
could be done in the reverse direction, up to the limit of the median-time-past
+ 1.
The Stratum mining protocol doesn't even have a way to tell clients what the
minimum allowed time is, just a single field, nTime, which is defined as "the
current time". Thus Stratum hashers will only ever increase nTime, which can
never result in an invalid timestamp if the original, unrolled, nTime would
have been a valid timestamp.
The getblocktemplate protocol does support telling hashers the minimum time via
the mintime field, which Bitcoin Core sets to the median-time-past. Regardless,
it appears that the pools supporting GBT (Eligius) return a much tighter limit
on mintime than the median-time-past, just 180 seconds, and as of writing,
don't actually declare that the ntime field is mutable anyway.
From an implementation point of view, relying on being able to roll nTime
backwards is unwise anyway, as the amount you can roll it back may be minimal
(e.g. if multiple blocks were recently found).
Since all miners have an incentive for time to move forward to keep difficulty
down it's reasonable to assume that the above observed behavior will continue,
and nTime rolling in the reverse direction will be a minimal effect; we'll
assume no miner rolls nTime backwards more than 1 hour.
 Systematic Errors
1) Botched daylight savings time changes --- While internal clocks should be
unaffected by timezone changes, it's plausible that some kind of mistake
related to daylight savings could result in the time being set incorrectly +- 1
hour. For example, multiple large miners might manually set their clocks, based
on an incorrect understanding of what time it was.
2) Broken NTP servers --- It's reasonable to assume that many miners are using
NTP to set their clocks, and it's plausible that they're using the same NTP
servers. Of course, a broken NTP server could return any time at all! The
Bitcoin protocol considers blocks to be valid if nTime is set up to 2 hours in
the future (from the perspective of the local node) so we'll say instead that
we expect systematic NTP errors to be corrected with high probability if
they're more than 2 hours in magnitude - more than that and the Bitcoin network
is broken in a very visible way anyway.
Thus, we'll assume honest miners always create blocks with nTime greater than
the true time minus two hours, which accounts for both likely daylight savings
time misconfigurations, and likely NTP server misconfigurations. Additionally,
two hours is greater than any expected effects from nTime rolling.
# Proposed Algorithm
For a timestamp anchored at a block of height x we'll define the time T it
represents as:
    T = max(block[i].nTime for i in {x, ..., x + N-1}) + max_offset
In short, T is the maximum nTime out of the N blocks that confirmed the
timestamp, including first block that actually committed the timestamp;
max_offset is the maximum nTime offset we expect from a block created by an
honest miner, discussed above.
The dishonest miners can successfully create an invalid timestamp iff all N
blocks are found by them; if any block is found by an honest miner, the nTime
field will be set correctly. Of course T may not be the minimum possible value,
but the timestamp will be at least valid.
So how big should N be? Let q be the ratio of dishonest miners to total hashing
power. The probability that all N blocks are found by dishonest miners is q^N,
and thus the probability P that at least one block is found by an honest miner
    P = 1 - q^N  =>  N = log(1 - P)/log(q)
If the dishonest miners have q>0.5, the situation is hopeless, as they can
reject blocks from honest miners entirely; the only limit on them setting nTime
is the median-time-past rule, which only requires blocks timestamps to
increment by one second per block (steady state). Thus we'll assume q=0.5, the
worst possible case where a Bitcoin timestamp can still be meaningful evidence:
    P = 97%      => N = 5
    P = 99%      => N = 7
    P = 99.9%    => N = 10
    P = 99.99%   => N = 14
    P = 99.999%  => N = 17
    P = 99.9999% => N = 20
The reliability for the higher N is higher than the actual reliability of
Bitcoin itself. On the other hand, there's no known instance where miners have
ever created blocks with nTime's significantly less than true time on a wide
scale; even in the well-known cases where the Bitcoin network has temporarily
failed due to forks, timestamps produced during those times would be valid, if
delayed by a few hours.
Similarly, if we assume a lower q, say a single "rogue" 20% mining pool, we
    q = 0.20, P = 99.99% => N = 6
Another way to think about the choice of N is to compare its contribution to
how conservative the timestamp is - T as compared to the true time - to the
effect of the max-honest-miner-offset we choose earlier. For example, 98% of
the time at least 6 blocks will be found within 2 hours, which means that if we
pick N=7, 98% of the time the conservatism added by N will be less than the
contribution of the max offset.
# UI Considerations
One problem with the above algorithm is that it will frequently return
timestamps in the future, from the perspective of the user. A user who sees a
message like the following at 2:00 pm, immediately after their timestamp
confirms, is understandably going to be confused:
   Bitcoin: 99% chance that  existed prior to 4:00 pm, Jan 1st 2016
A reasonable approach to this problem might just to refrain from displaying
timestamps at all until the local time is after the timestamp; the UI could
describe the timestamp as "Not yet confirmed"
It may also be reasonable to round the timestamp up to the nearest day when
displaying it. However what timezone to use is a tricky issue; people rarely
expect to see timezones specified alongside dates.
Of course, in some cases a less conservative approach to interpreting the
timestamp is reasonable; those users however should be reading and
understanding the calculations in this post!
# References
1)

@_date: 2016-09-18 12:05:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] Interpreting nTime for the purpose of 
Well that's the thing: timestamps are simply proofs that something existed
prior to some time, nothing more, nothing less.
So it doesn't make sense for there to be any notion of the "wrong order" in a
timestamp proof; the proof either is or is not valid, but that has nothing to
do with other proofs. Additionally, the architecture of OpenTimestamps doesn't
and can't make any 100% guarantees about the apparent order of timestamps,
because it's always possible for an earlier timestamp to end up committed in
the blockchain after a later timestamp gets committed. It's not all that likely
of an event, but it is possible.
If you don't want that to be possible, you're going to need a dedicated chain
of transactions for your particular purpose, which adds a lot of complexity,
cost, and makes it much harder to achieve the same level of availability for
the service as a whole.
Remember that for many use-cases the user experience is that there's two or
more claimed dates, and OpenTimestamps simply verifies that those dates are
plausible. Take for example, timestamped git commits:
    commit 536411e73b8c23dc2fdfd78052c893f578444926
    ots: Got 2 attestation(s) from cache
    ots: Success! Bitcoin attests data existed as of Thu Sep 15 01:07:08 2016 EDT
    ots: Good timestamp
    gpg: Signature made Thu 15 Sep 2016 12:10:25 AM EDT
    gpg:                using RSA key 6399011044E8AFB2
    gpg: Good signature from "Peter Todd "
    gpg:                 aka "[jpeg image of size 5220]"
    Author: Peter Todd     Date:   Thu Sep 15 00:10:20 2016 -0400
        Release v0.2.0
Here we have the date on the git commit, another date a few seconds later for
the PGP signature, and a third date an hour later for the Bitcoin timestamp,
attesting to the fact that the two other dates for that one git commit are

@_date: 2016-09-19 13:56:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] Interpreting nTime for the purpose of 
Ah! That's a good point; my analysis only applies to the case where you're
assuming the dishonest miners aren't willing to lose revenue from the attack by
mining a less-work chain with blocks that won't end up in the main chain. I
should state that assumption more clearly.
If the dishonest miners are willing to spend money to create an invalid
timestamp the analysis is quite different. In OpenTimestamps a timestamp
doesn't contain the actual block headers - just a block height - so verifiers
are expected to have a working Bitcoin node. If that Bitcoin node is in sync
with the most-work work chain there's no risk: the blocks created by the
dishonest miners won't be part of the most-work chain, and validation of the
timestamp will fail.
In the case where the verifier is not in sync with the most-work chain, an
attacker can sybil attack the verifier's node and cause them to think that the
blocks committing the invalid timestamp are in fact the most-work chain. This
case is no different than a payee being sybil attacked, so we can use the same
analysis we would in that circumstance.
This also means that timestamps definitely shouldn't contain the block headers
of the blocks allegedly confirming them - that's an extremely weak proof given
the relative ease of creating a block, particularly when you take into account
that the same block could be used to create an unlimited number of fake
timestamps. OpenTimestamps doesn't do this, but it wouldn't hurt to make this
point 100% clear.

@_date: 2016-09-20 17:56:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Requesting BIP assignment; Flexible Transactions. 
If the order of the tokens is fixed, the tokens themselves are redundant
information when tokens are required; when tokens may be omitted, a simple
"Some/None" flag to mark whether or not the optional data has been omitted is
Also, if you're going to break compatibility with all existing software, it
makes sense to use a format that extends the merkle tree down into the
transaction inputs and outputs.

@_date: 2016-09-22 14:26:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Requesting BIP assignment; Flexible Transactions. 
That argument is not applicable to required fields: the code to get the fields
from the extensible format is every bit as complex as the very simple code
required to deserialize/serialize objects in the current system.
In any case your BIP needs to give some explicit examples of hypothetical
upgrades in the future, how they'd take advantage of this, and what the code to
do so would look like.
See my arguments re: segwit a few months ago, e.g. the hardware wallet txin
proof use-case.

@_date: 2016-09-22 14:27:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Requesting BIP assignment; Flexible Transactions. 
CSV uses per-input sequence numbers; you only have a per-tx equivalent.

@_date: 2016-09-23 12:18:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
Your BIP is a bit confusing: you say "In some circumstances, users may wish to
spend received bitcoins before they have confirmed on the blockchain", but what
you're really referring to isn't spending unconfirmed outputs - which
OP_CHECKBLOCKATHEIGHT can't protect - but rather spending outputs with a small
number of confirmations.
In the existing ecosystem, if multi-block reorgs were a regular event Bitcoin
would be in a lot of trouble; since they're rare, advising wallet authors to
simply refuse to make transactions for some time after such a reorg may be a
better solution. After all, a multi-block reorg is a strong indication that
there's somehting very wrong with the network, and it'd be safer to stop using
Bitcoin for awhile until things settle down.

@_date: 2016-09-23 16:02:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
However, by doing that we'd also make the functionality not all that useful for
this application; by the time you waited 100 blocks for the tx to be minable,
the chance of a reorg happening is low enough that I can't imagine many - if
any - wallets would bother using the opcode in the first place, and would
instead just rely on the fact that a reorg that deep which resulted in the
double-spent transaction ending up back in the chain is very unlikely.
Specifically I'm referring to the following scenario:
1) Alice pays Bob with tx1a
2) tx1a gets N confirmations, where N is some small number of confirmations.
2) Bob pays Charlie from tx1a's output in tx2a
3) A reorg eliminates the block that tx1a existed, and a conflicting tx1b is
   mined instead, making tx1a and tx2a invalid.
4) Bob pays Charlie again with tx2b, whose inputs do not conflict with tx2a
5) Another reorg eliminates tx1b, allowing tx1a, tx2a, and tx2b to all be
   mined.
6) Charlie has now been paid twice.
Since you need _two_ reorgs for this scenario to be applicable, it's much
easier to just wait for tx1b to be confirmed suffiently deeply in the chain
that a reorg undoing it - thus allowing tx1a and tx2a to exist - is
sufficiently unlikely; 100 blocks is a lot more than  most wallets are going to
consider "sufficiently unlikely", so the featureu just won't get used (assuming
wallets even bother to handle this case of course!).
Unfortunately I think this is an inherent catch-22 of the idea.

@_date: 2016-09-26 14:41:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] Proposed BIP-1 change removing OPL licensing 
Note how the OPL is significantly more restrictive than the Bitcoin Core
license; not good if we can't ship documentation with the code.

@_date: 2016-09-27 15:17:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] Proposed BIP-1 change removing OPL licensing 
The issue isn't that the licenses are different, it's that the OPL is
significantly more restrictive (with the additional clauses that you opted
Indeed, using a different license for documentation is common advise, although
if the documentation also includes example code you may want to dual-license
the documentation with a code-oriented license as well if the documentation
license isn't maximally permissive.
Thanks, CC-BY-SA is a perfectly good license for that purpose.

@_date: 2017-04-05 22:31:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
While I'm in favour of blocking covert usage of ASICBOOST, there's every reason
to block non-covert usage of it as well. In a low margin business like mining,
the advatange it gives is enormous - quite possibly 10x your profit margin -
and given that barrier free access to being able to purchase ASICs is already
an archilles heal for Bitcoin there is every reason to eliminate this legal
vulnerability. Additionally, it's a technical vulnerability as well: we want
getting into the ASIC manufacturing and design business to have as low barriers
to entry as is feasible, and the ASICBOOST exploit significantly increases the
minimum capital requirements to do so.
Remember that the whole purpose of PoW is to destroy value on a level playing
field. Anything that inhibits a level playing field is an exploit. While this
isn't standard crypto - we can't fix every exploit completely - since we're
going to do a technical change to partially mitigate the ASCIBOOST exploit
there is every reason to fully mitigate it.

@_date: 2017-04-05 22:49:10
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
Agreed! There's no benefit to Bitcoin for having it - one way or the other
miners are going to destroy ~12BTC/block worth of energy. Meanwhile it appears
to have lead to something like a year of stupid political bullshit based on a
secret advantage - there's no reason to invite a repeat of this episode.

@_date: 2017-04-05 23:23:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
You're talking about proof-of-stake here.
At best it's very difficult for such a "proof-of-burn" to _actually_ be a
proof, as the burn only happens if the consensus mechanism ultimately includes
that burn. Contrast that to proof-of-work's incredibly simple proof: you _know_
energy was destroyed to find a PoW solution, regardless of what consensus is
ultimately reached.
It's the difference between a computer secured from hackers with an anti-virus
scanner, and a computer secured by the fact that it's not connected to the
internet at all.

@_date: 2017-04-05 23:42:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
One of the things going for us here is that Bitmain has been keeping ASICBOOST
from their own customers - as far as we know they haven't been sharing it, and
thus they're the only ones you can actually use it.
So while we're pissing off Bitmain in disabling it, we wouldn't be affecting
anyone else.
Equally, mining is a zero-sum game: if no-one can use ASICBOOST, miners are in
the same position as before. ASICBOOST is only relevant to miners like Bitmain
who have access to it while other miners don't.

@_date: 2017-12-05 05:15:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] Scalable Semi-Trustless Asset Transfer via 
I recently wrote this up for a client, and although the material has been
covered elsewhere, I thought being a worked example it might be of interest,
particularly while sidechains are being discussed again.
As per (1) I've perhaps foolishly committed to making an even more fleshed out
example, so peer review here before it gets to an even wider audience would be
appreciated. :)
1) tl;dr: We can do trustless with respect to validity, trusted with respect to
censorship resistance, indivisible asset transfer with less than 5MB/year/token
of proof data, assuming token ownership is updated every two hours, at a rate
of ~500,000 transfers per second. The scalability of this scheme is linear with
respect to update interval, and logarithmic with respect to overall transfer
 Single-Use-Seal Definition
Analogous to the real-world, physical, single-use-seals used to secure shipping
containers, a single-use-seal primitive is a unique object that can be closed
over a message exactly once. In short, a single-use-seal is an abstract
mechanism to prevent double-spends.
A single-use-seal implementation supports two fundamental operations:
    Close(l,m) -> w_l
        Close seal l over message m, producing a witness w_l
    Verify(l,w_l,m) -> bool
        Verify that the seal l was closed over message m
A single-use-seal implementation is secure if it is impossible for an attacker
to cause the Verify function to return true for two distinct messages m_1, m_2,
when applied to the same seal (it _is_ acceptable, although non-ideal, for
there to exist multiple witnesses for the same seal/message pair).
Practical single-use-seal implementations will also obviously require some way
of generating new single-use-seals. Secondly, authentication is generally
useful. Thus we have:
    Gen(p) -> l
        Generate a new seal bound to pubkey p
    Close(l,m,s) -> w_l
        Close seal l over message m, authenticated by signature s valid for pubkey p
Obviously, in the above, pubkey can be replaced by any cryptographic identity
scheme, such as a Bitcoin-style predicate script, zero-knowledge proof, etc.
Finally, _some_ single-use-seal implementations may support the ability to
prove that a seal is _open_, e.g. as of a given block height or point in time.
This however is optional, and as it can be difficult to implement, it is
suggested that seal-using protocols avoid depending on this functionality
 Indivisible Token Transfer
With a secure single-use-seal primitive we can build a indivisible token
transfer system, allowing the secure transfer of a token from one party to
another, with the seals preventing double-spends of that indivisible token.
Each token is identified by its genesis seal l_0. To transfer a token, the most
recent seal l_n is closed over a message committing to a new seal, l_{n+1},
producing a witness w_{l_n} attesting to that transfer. This allows a recipient
to securely verify that they have received the desired token as follows:
1. Generate a fresh, open, seal l_{n+1} that only they can close.
2. Ask the sender to close their seal, l_n, over the seal l_{n+1}
3. Verify that there exist a set of valid witnesses w_0 .. w_n, and seals
   l_0 .. l_n, such that for each seal l_i in i = 0 .. n, Verify(l_i, w_i, l_{i+1})
   returns true.
Since a secure single-use-seal protocol prohibits the closure of a single seal
over multiple messages, the above protocol ensures that the token can not be
double-spent. Secondly, by ensuring that seal l_{n+1} can be closed by the
recipient and only the recipient, the receipient of the token knows that they
and they alone have the ability to send that token to the next owner.
 Divisible Asset Transfer
In the case of a divisible asset, rather than transferring a single, unique,
token we want to transfer a _quantity_ of an asset. We can accomplish this in a
manner similar how Bitcoin's UTXO-based transactions, in which one or more
inputs are combined in a single transaction, then split amongst zero or more
We define the concept of an _output_. Each output x is associated with a seal l
and value v. For each asset we define a set of _genesis outputs_, X_G, whose
validity is assumed.
To transfer divisible assets we further define the concepts of a _spend_ and a
_split_. A spend, D, is a commitment to a set of outputs x_i .. x_j; the value
of a spend is simply the sum of the values of all outputs in the spend. A split
commitments to a set of zero or seal/value, (l_i,v_i), tuples, with the sum
value of the split being the sum of a values in the split.
Spends and splits are used to define a _split output_. While a genesis output
is simply assumed valid, a split output x is then the tuple (D,V,i), committing
to a spend D, split V, and within that split, a particular output i.
A split output is valid if:
1. Each output in the spend set D is a valid output.
2. The sum value of the spend set D is >= the sum value of the split V.
3. i corresponds to a valid output in the split.
4. There exists a set of witnesses w_i .. w_j, such that each seal in the spend
   set closed over the message (D,V) (the spend and split).
As with the indivisible asset transfer, a recipient can verify that an asset
has been securely transferred to them by generating a fresh seal, asking the
sender to create a new split output for that seal and requested output amount,
and verifying that the newly created split output is in fact valid. As with
Bitcoin transactions, in most transfers will also result in a change output.
Note how an actual implementation can usefully use a merkle-sum-tree to commit
to the split set, allowing outputs to be proven to the recipient by giving only
a single branch of the tree, with other outputs pruned. This can have both
efficiency and privacy advantages.
 Single-Use-Seal Implementation
An obvious single-use-seal implementation is to simply have a trusted notary,
with each seal committing to that notary's identity, and witnesses being
cryptographic signatures produced by that notary. A further obvious refinement
is to use disposable keys, with a unique private key being generated by the
notary for each seal, and the private key being securely destroyed when the
seal is closed.
Secondly Bitcoin (or similar) transaction outputs can implement
single-use-seals, with each seal being uniquely identified by outpoint
(txid:n), and witnesses being transactions spending that outpoint in a
specified way (e.g. the first output being an OP_RETURN committing to the
 Proof-of-Publication Ledger
For a scalable, trust-minimized, single-use-seal implementation we can use a
proof-of-publication ledger, where consensus over the state of the ledger is
achieved with a second single-use-seal implementation (e.g. Bitcoin).
Such a ledger is associated with a genesis seal, L_0, with each entry M_i in
the ledger being committed by closing the most recent seal over that entry,
producing W_i such that Verify(L_i, (L_{i+1}, M_i), W_i) returns true.
Thus we achieve consensus over the state of the ledger as we can prove the
contents of the ledger.
Specifically, given starting point L_i we can prove that the subsequent ledger
entries M_i .. M_j are valid with witnesses W_i .. W_j and seals L_{i+1} .. L_{j+1}.
A proof-of-publication-based seal can then be constructed via the tuple (L_i,
p), where L_i is one of the ledger's seals, and p is a pubkey (or similar). To
close a proof-of-publication ledger seal a valid signature for that pubkey and
message m is published in the ledger in entry M_j.
Thus the seal witness is proof that:
1. Entry M_j contained a valid signature by pubkey p, for message m.
2. All prior entries M_i .. M_{j-1} (possibly an empty set) did _not_ contain
   valid signatures.
Finally, for the purpose of scalability, instead of each ledger entry M_i
consisting of a unstructured message, we can instead commit to a merkelized
key:value tree, with each key being a pubkey p, and each value being an
alleged signature (possibly invalid). Now the non-publication condition is
proven by showing that either:
1. Tree M_i does not contain key p.
2. Tree M_i does contain key p, but alleged signature s is invalid.
The publication condition is proven by showing that tree M_j does contain key
p, and that key is associated with valid signature s.
A merkelized key:value tree can prove both statements with a log2(n) sized
proof, and thus we achieve log2(n) size scalability, with the constant factor
growing by the age of the seals, the ledger update frequency, the rate at which
seals are closed, and the maximum size allowed for signatures.
Note how a number of simple optimizations are possible, such as preventing the
creation of "spam" invalid signatures by blinding the actual pubkey with a
nonce, ensuring only valid signatures are published, etc. Also note how it is
_not_ necessary to validate all entries in the ledger form a chain: the
single-use-seals guarantees that a particular range of ledger entries will be
unique, regardless of whether all ledger history was unique.
Proof-of-Publication ledgers are trustless with regard to false seal witnesses:
the ledger maintainer(s) are unable to falsify a witness because they are
unable to produce a valid signature. They are however trusted with regard to
censorship: the ledger maintainer can prevent the publication of a signature
and/or or withhold data necessary to prove the state of the seal.
# Performance Figures
Assume a indivisible token transfer via a PoP ledger using Bitcoin-based
single-use-seals, with the ledger updated 12 times a day (every two hours).
Assume each ledger update corresponds to 2^32, 4 billion, transfers.
The data required to prove publication/non-publication for a given ledger
update is less than:
    lite-client BTC tx proof:                            = ~1KB
    merkle path down k/v tree: 32 levels * 32bytes/level =  1KB
    key/value: 32 bytes predicate hash + 1KB script sig  = ~1KB
                                                   Total = ~3KB/ledger update
        * 356 days/year * 12 updates/day = 13MB/year
Now, those are *absolute worst case* numbers, and there's a number of ways that
they can be substantially reduced such as only publishing valid signatures, or
just assuming you're not being attacked constantly... Also, note how for a
client with multiple tokens, much of the data can be shared amongst each token.
But even then, being able to prove the ownership status of a token, in a
trustless fashion, with just 13MB/year of data is an excellent result for many
With these optimizations, the marginal cost per token after the first one is
just 1KB/ledger update, 4.4MB/year.

@_date: 2017-12-11 18:16:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Scalable Semi-Trustless Asset Transfer via 
It's centralized in that writeup basically because centralizing it is
*significantly* easier; it's not obvious how to maintain a proof-of-publication
ledger in a decentralized, scalable, way.
In the centralized version it's obvious how to scale process by which the
ledger is built via sharding: split the key range up as needed and assign each
range to a separate server (be it an actual server, or a fault-tolerate cluster
acting as a single server) that reports back to a master co-ordinator who
builds the tree from the per-range sub-tips reported back by the shards. If
required due to extreme scale, do this on multiple levels. Similarly, once the
tree is built, storage and distribution can obviously be done via sharding.
In short, no matter how much the transaction rate on a PoP ledger grows, it's
possible to meet demand by simply buying more hardware, and distributing the
key space over a larger number of smaller shards.
But that simple architecture only works with trust: the coordinator is trusting
the shards to build valid trees and distribute the results. Without trust, how
do you ensure that actually happens? How do you pick who is assigned to what
shard? How do you incentivise correct behavior?
That's not to say this is impossible - in fact my prior work on Treechains(1)
is an attempt to do just this - but it's an orders of magnitude more difficult
1)    "[Bitcoin-development] Tree-chains preliminary summary", Mar 25th 2014,
   Peter Todd

@_date: 2017-02-14 07:33:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Pre-BIP] Community Consensus Voting System 
...and note how, like blocksize, the roots of the DRM argument at W3C aren't a
technical disagreement, but rather a political disagreement.

@_date: 2017-02-19 16:30:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] replace-by-fee-v0.14.0rc1 available 
My full-RBF patched branch of Bitcoin Core v0.14.0rc1 is now available:
As with replace-by-fee-v0.13.2, this version uses the nRelevantServices
machinery to do preferential peering, so it's just a few lines of code changed
between it and Bitcoin Core v0.14.0rc1.
The relevant services machinery is less agressive at connecting to full-RBF
peers than the earlier custom code previous versions used. But it seems to work
well enough to keep RBF peers connected to each other, so I'm inclined to keep
using it as doing so makes maintaining this patched branch pretty trivial every
time a new upstream version is released.

@_date: 2017-02-22 20:11:47
@_author: Peter Todd 
@_subject: [bitcoin-dev] TXO commitments do not need a soft-fork to be useful 
Something I've recently realised is that TXO commitments do not need to be
implemented as a consensus protocol change to be useful. All the benefits they
provide to full nodes with regard to allowing for old UTXO data to be pruned -
and thus solving the UTXO bloat problem - can be implemented even without
having miners commit to the TXO commitment itself. This has a significant
deployment advantage too: we can try out multiple TXO commitment schemes, in
production, without the need for consensus changes.
# Reasoning
1) Like any other merkelized data structure, a TXO commitment allows a data set
- the TXO set - to be securely provided by an untrusted third party, allowing
the data itself to be discarded. So if you have a valid TXO commitment, you can
discard the TXO data itself, and rely on untrusted entities to provide you that
data on demand.
2) The TXO set is a super-set of the UTXO set; all data in the UTXO set is also
present in the TXO set. Thus a TXO commitment with spent TXO's pruned is
equivalent to a UTXO set, doubly so if inner nodes in the commitment tree
commit to the sum-unspent of their children.
3) Where a outpoint-indexed UTXO set has a uniform access pattern, an
insertion-ordered TXO set has a delibrately *non-uniform* access pattern: not
only are new entries to the TXO set always appended to the end - an operation
that requires a known, log2(n), sized set of merkle tips - but due to lost
coins alone we can guarantee that older entries in the TXO set will be less
frequently updated than newer entries.
4) Thus a full node that doesn't have enough local storage to maintain the full
UTXO set can instead keep track of a TXO commitment, and prune older UTXO's
from it that are unlikely to be spent. In the event those UTXO's are spent,
transactions and blocks spending them can trustlessly provide the necessary
data to temporarily fill-in the node's local TXO set database, allowing the
next commitment to be calculated.
5) By *not* committing the TXO commitment in the block itself, we obsolete my
concept of delayed TXO commitments: you don't need to have calculated the TXO
commitment digest to validate a block anyway!
# Deployment Plan
1) Implement a TXO commitment scheme with the ability to efficiently store the
last n versions of the commitment state for the purpose of reorgs (a
reference-counted scheme naturally does this).
2) Add P2P support for advertising to peers what parts of the TXO set you've
3) Add P2P support to produce, consume, and update TXO unspentness proofs as
part of transaction and block relaying.
4) Profit.
# Bootstrapping New Nodes
With a TXO commitment scheme implemented, it's also possible to produce
serialized UTXO snapshots for bootstrapping new nodes. Equally, it's obviously
possible to distribute those snapshots, and have people you trust attest to the
validity of those snapshots.
I argue that a snapshot with an attestation from known individuals that you
trust is a *better* security model than having miners attest to validity: the
latter is trusting an unknown set of unaccountable, anonymous, miners.
This security model is not unlike the recently implemented -assumevalid
scheme(1), in that auditing the validity of the assumed valid TXO commitments
is something anyone can do provided they have a full node. Similarly, we could
ship Bitcoin nodes with an assumed-valid TXO commitment, and have those nodes
fill in the UTXO data from their peers.
However it is a weaker security model, in that a false TXO commitment can more
easily be used to trick a node into accepting invalid transactions/blocks;
assumed valid blocks requires proof-of-work to pull off this attack. A
compromise may be to use assumed valid TXO commitments, extending my partial
UTXO set(2) suggestion of having nodes validate the chain backwards, to
eventually validate 100% of the chain.
# References
1) 2) [Bitcoin-development] SPV bitcoind? (was: Introducing BitcoinKit.framework),
   Peter Todd, Jul 17th 2013, Bitcoin development mailing list,

@_date: 2017-02-22 20:15:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Reposting something that came up recently in a private discussion with some
Concretely, let's define a prunable MMR with the following grammar. This
definition is an improvement on whats in the python-proofmarshal by committing
to the number of items in the tree implicitly; an obvious max-log2(n)-sized
proof-of-tree-size can be obtained by following the right-most nodes:
    Maybe(T) := UNPRUNED  | PRUNED     FullNode(0) :=     FullNode(n) :=      PartialNode(0) := SOME  | NONE
    PartialNode(n) :=      MMR := FULL   | PARTIAL  Basically we define it in four parts. First we define Maybe(T) to represent
pruned and unpruned (hash only) data. Secondly we define full nodes within 2^n
sized trees. Third we define partial nodes. And finally we define the MMR
itself as being either a full or partial node.
First of all, with pruning we can define a rule that if any operation (other
than checking commitment hashes) attempts to access pruned data, it should
immediately fail. In particular, no operation should be able to determine if
data is or isn't pruned. Equally, note how an implementation can keep track of
what data was accessed during any given operation, and prune the rest, which
means a proof is just the parts of the data structure accessed during one or
more operations.
With that, notice how proving the soundness of the proofs becomes trivial: if
validation is deterministic, it is obviously impossible to construct two
different proofs that prove contradictory statements, because a proof is simply
part of the data structure itself. Contradiction would imply that the two
proofs are different, but that's easily rejected by simply checking the hash of
the data.

@_date: 2017-02-22 20:26:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Generalized Commitments 
Note that this is a use-case specific concept of an idea I'm calling a
"generalized commitment"
A commitment scheme needs only have the property that it's not feasible to find
two messages m1 and m2 that map to the same commitment; it is *not* required
that it be difficult to find m given the commitment. Equally, it's not required
that commitments always be the same size.
So a perfectly reasonable thing to do is design your scheme such that the
commitment to short messages is the message itself! This adds just a single bit
of data to the minimum serialized size(1) of the commitment, and in situations
where sub-digest-sized messages are common, may overall be a savings.
Another advantage is that the scheme becomes more user-friendly: you *want*
programmers to notice when a commitment is not effectively hiding the message!
If you need message privacy, you should implement an explicit nonce, rather
than relying on the data to not be brute-forcable.
1) The more I look at these systems, the more I'm inclined to consider
bit-granularity serialization schemes... Heck, sub-bit granularity has
advantages too in some cases, e.g. by making all possible inputs to the
deserializer be valid.

@_date: 2017-02-23 02:23:10
@_author: Peter Todd 
@_subject: [bitcoin-dev] TXO commitments do not need a soft-fork to be 
Thinking about this a bit more, by not being forced to calculate a TXO
commitment for every block, we may be able to do significantly better than
delayed TXO commitments by lazily hashing.
Suppose we have the following perfect merkle tree, which we're using as a
key-value map. We'll represent inner nodes for which we've calculated digests
with "0"'s to represent what version of the tree they correspond too:
               0
              / \
             /   \
            /     \
           /       \
          /         \
         0           0
        / \         / \
       /   \       /   \
      0     0     0     0
     / \   / \   / \   / \
    a   b c   d e   f g   h
If a value is updated, digests above it become out of date and need to be
               1
              / \
             /   \
            /     \
           /       \
          /         \
         0           1
        / \         / \
       /   \       /   \
      0     0     0     1
     / \   / \   / \   / \
    a   b c   d e   f g   H
               2
              / \
             /   \
            /     \
           /       \
          /         \
         0           2
        / \         / \
       /   \       /   \
      0     0     2     1
     / \   / \   / \   / \
    A   b c   d e   F g   H
               3
              / \
             /   \
            /     \
           /       \
          /         \
         0           3
        / \         / \
       /   \       /   \
      0     0     2     3
     / \   / \   / \   / \
    a   b c   d e   F G   H
Suppose however that your implementation does lazy hashing; after the 3rd
update your state will be:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
        / \         / \
       /   \       /   \
      0     0     .     .
     / \   / \   / \   / \
    a   b c   d e   F G   H
Basically all the digests on the right side is out of date and need to be
recalculated. Now, first of all it's obviously possible for your implementation
to keep updating values in the tree given their keys - you've essentially
regressed to a bog standard binary tree.
But what happens if you discard part of your dataset? Let's suppose you've
discarded the left half:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
                    / \
                   /   \
                  .     .
                 / \   / \
                e   F G   H
Note how you still have sufficient information to calculate the current merkle
tip commitment: the left side hasn't changed yet. But what happens when someone
gives you an update proof? Specifically, suppose they want to change b -> B.
That requires them to provide you with the part of the merkle tree proving that
position  is b. Now you might think that's this data:
               3
              / \
             /   \
            /     \
           /       \
          /         \
         0           3
        / \
       /   \
      0     0
     / \
    a   b
But the inner node digests marked "3" are useless to you: you haven't
calculated those digests yet so you can't compare them to anything. What you
can compare is the following:
         0
        / \
       /   \
      0     0
     / \
    a   b
With that extra data your local knowledge is now:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
        / \         / \
       /   \       /   \
      0     0     .     .
     / \         / \   / \
    a   b       e   F G   H
Allowing you to apply the update:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         .           .
        / \         / \
       /   \       /   \
      .     0     .     .
     / \         / \   / \
    a   B       e   F G   H
If you want to again prune that data, simply recalculate the digests so you
can verify a copy given to you by a peer in the future:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         4           .
        / \         / \
       /   \       /   \
      4     0     .     .
     / \         / \   / \
    a   B       e   F G   H
And prune, leaving you with:
               .
              / \
             /   \
            /     \
           /       \
          /         \
         4           .
                    / \
                   /   \
                  .     .
                 / \   / \
                e   F G   H
So tl;dr: the reason this works is that we can substitute commitments for
pointers: our merkle tree can also be viewed as a binary tree. So a reasonable
real-world implementation would be to delay computation of digests for anything
we have in RAM, and only compute digests as in-RAM data is flushed to disk.
Equally, on disk we can use standard time-space tradeoffs to only store a
subset of the digests, recalculating the rest on the fly. Given that'd we could
effectively combine both a cryptographic data structure and a standard
pointer-based data structure in one, I suspect we can get good performance out
of this.
The main subtlety of this approach will be how exactly to handle the proofs:
the level of verification possible depends on what digests a given node has
calculated, and we want to avoid making network splitting attacks possible by
attackers deliberately giving nodes proofs with upper digests that are
incorrect, something only some nodes can detect. Not sure yet exactly what's
the right approach there.
Finally, notice how this entire approach depends on schemes like MMR's where
the overall structure of the tree does not change as nodes are added and
updated; it would be much harder to implement this idea for something like a
merklized red-black tree where the structure changes as the tree is rebalanced.

@_date: 2017-02-23 02:41:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
That's an improvement, but I think we can do even better if we think of missing
pruned data as analogous to virtual memory: pruned data is the same as a page
that has been swapped to disk, with the magical property that hashing allows us
to swap it back in from an untrusted source.
Thus a proof should actually be whatever data we expect our counterparty to
have flushed, ranging from none at all, to 100% (modulo a root hash). An
implementation should then do operations as normal, using parts of the proof on
an as-needed basis where pruned data is encountered.
Thus if you have a key-value map and do a get() operation, you'd expect the
proof to *not* be what the get operates on, but rather be a *context* argument
to the get() operation. The other way around is actually an example of doing
computations on untrusted data, and bad API design!
I'm talking about these MMR's: Notably I'm talking about an insertion ordered list, indexed by position, that
supports append and update operations, but *not* insertions; this is different
than what you've recently published re: UTXO commitments. That's a full
key-value map, something MMR's are delibrately are not doing.
Draw out a MMR based on the formal definition you're replying too and you'll
see the new structure.
Like I say above, you're solving a different problem than MMR's solve.

@_date: 2017-02-23 13:14:09
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
Worth noting: the impact of the SHA1 collison attack on Git is *not* limited
only to maintainers making maliciously colliding Git commits, but also
third-party's submitting pull-reqs containing commits, trees, and especially
files for which collisions have been found. This is likely to be exploitable in
practice with binary files, as reviewers aren't going to necessarily notice
garbage at the end of a file needed for the attack; if the attack can be
extended to constricted character sets like unicode or ASCII, we're in trouble
in general.
Concretely, I could prepare a pair of files with the same SHA1 hash, taking
into account the header that Git prepends when hashing files. I'd then submit
that pull-req to a project with the "clean" version of that file. Once the
maintainer merges my pull-req, possibly PGP signing the git commit, I then take
that signature and distribute the same repo, but with the "clean" version
replaced by the malicious version of the file.

@_date: 2017-02-23 13:19:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
See the discussion on TXO commitments for how MMR's could be used; a better MMR
makes for a better TXO commitment.

@_date: 2017-02-23 13:31:40
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Why do you want a non-existance proof?
It supports an efficient *spentness* proof, which is sufficient for what we
need in Bitcoin, and much more scalable.

@_date: 2017-02-23 16:28:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
Thinking about this a bit more, the most concerning avenue of attack is likely
to be tree objects, as I'll bet you you can construct tree objs with garbage at
the end that many review tools don't pick up on. :(

@_date: 2017-02-23 18:51:05
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
In what way do you see MMRs as redundant?
Remember that with UTXO commitments because access patterns are uniform, you'll
over time have a lot more "redundancy" in the form of lost-coins evenly spread
out across the whole keyspace.
That statement is incorrect with pruning: you can maintain a commitment to the
TXO set, without actually storing the entire TXO set, because you don't need to
store anything for nodes that have already been spent.
Concretely, this can be done with nothing more than adding a FullySpent node
type to the MMR definition I published earlier, with the rule being that only a
left or right child of an inner node be a FullySpent node, not both; if both
sides are spent, the inner node itself becomes FullySpent. Equally, I think you
can re-use the Empty node for this, but I need to think a little about the
implications re: partial inner nodes.
Regardless, with a generalized commitment scheme, the serialization/commitment
to an Empty node is simply '0', the encoding of an unspent txout surrounded by
spent txouts will be similar in size to a position integer followed by the
A subtlety of this construction is that you can only prove that a specific
txout # is unspent, but that's actually sufficient, as you can also prove what
# a txout txid corresponds too with a previous version of the MMR.
Well, I think at this point there's still discussion over whether or not a UTXO
set commitment is the right approach to begin with; if it's not your
implementation isn't relevant.

@_date: 2017-02-23 20:09:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
I think you've misunderstood what TXO commitments are. From my article:
"A merkle tree committing to the state of all transaction outputs, both spent
and unspent, can provide a method of compactly proving the current state of an
I'm proposing that we commit to not just the set of transaction outputs, but
also the current *state* of those outputs, with the same commitment structure.
Concretely, each leaf node in the TXO commitment tree needs to commit to - at
minimum - the outpoint (txid:n) and spent/unspent status (possibly structurally
as mentioned elsewhere in this thread). It's probably also valuable to commit
to the scriptPubKey, nValue, as well, though technically that's redundant as
the txid already commits to that (there's some implementation options here).
Why would you commit to a balanced version of the TXO set? I'm proposing
committing to an insertion-ordered list, indexed by txout Hmm? That's exactly what I'm doing. Also, as per the above, I think you've
misunderstood what my TXO commitment proposal is.

@_date: 2017-02-23 21:58:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Sorry, but I was replying to your statement:
So to be clear, do you agree or disagree with me that you *can* extract a
compact proof from a MMR that a given output is unspent?
I just want to make sure we're on the same page here before we discuss
performance characteristics.

@_date: 2017-02-23 22:15:31
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Glad we're on the same page with regard to what's possible in TXO commitments.
Secondly, am I correct in saying your UTXO commitments scheme requires random
access? While you describe it as a "merkle set", obviously to be merkelized
it'll have to have an ordering of some kind. What do you propose that ordering
to be?
Maybe more specifically, what exact values do you propose to be in the set?

@_date: 2017-02-23 23:36:13
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
The hash of what? The values in the set?
Ok, so let's assume the values in the set are the unspent outpoints.
Since we're ordering by the hash of the values in the set, outpoints will be
distributed uniformly in the set, and thus the access pattern of data in the
set is uniform.
Now let's fast-forward 10 years. For the sake of argument, assume that for
every 1 UTXO in the set that corresponds to funds in someone's wallet that are
likely to be spent, there are 2^12 = 4096 UTXO's that have been permanently
lost (and/or created in spam attacks) and thus will never be spent.
Since lost UTXO's are *also* uniformly distributed, if I'm processing a new
block that spends 2^12 = 4096 UTXO's, on average for each UTXO spent, I'll
have to update log2(4096) = 12 more digests than I would have had those "dead"
UTXO's not existed.
Concretely, imagine our UTXO set had just 8 values in it, and we were updating
two of them:
               #
              / \
             /   \
            /     \
           /       \
          /         \
         #           #
        / \         / \
       /   \       /   \
      #     .     .     #
     / \   / \   / \   / \
    .   X .   . .   . X   .
To mark two coins as spent, we've had to update 5 inner nodes.
Now let's look at what happens in an insertion-ordered TXO commitment scheme.
For sake of argument, let's assume the best possible case, where every UTXO
spent in that same block was recently created. Since the UTXO's are recently
created, chances are almost every single one of those "dead" UTXO's will have
been created in the past. Thus, since this is an insertion-ordered data
structure, those UTXO's exist in an older part of the data structure that our
new block doesn't need to modify at all.
Concretely, again let's imagine a TXO commitment with 8 values in it, and two
of them being spent:
               #
              / \
             /   \
            /     \
           /       \
          /         \
         .           #
        / \         / \
       /   \       /   \
      .     .     .     #
     / \   / \   / \   / \
    .   . .   . .   . X   X
To mark two coins as spent, we've only had to update 3 inner nodes; while our
tree is higher with those lost coins, those extra inner nodes are amortised
across all the coins we have to update.
The situation gets even better when we look at the *new* UTXO's that our block
creates. Suppose our UTXO set has size n. To mark a single coin as spent, we
have to update log2(n) inner nodes. We do get to amortise this a bit at the top
levels in the tree, but even if we assume the amortisation is totally free,
we're updating at least log2(n) - log2(m) inner nodes "under" the amortised
nodes at the top of the tree for *each* new node.
Meanwhile with an insertion-ordered TXO commitment, each new UTXO added to the
data set goes in the same place - the end. So almost none of the existing data
needs to be touched to add the new UTXOs. Equally, the hashing required for the
new UTXO's can be done in an incremental fashion that's very L1/L2 cache
tl;dr: Precisely because access patterns in TXO commitments are *not* uniform,
I think we'll find that from a L1/L2/etc cache perspective alone, TXO
commitments will result in better performance than UTXO commitments.
Now it is true that Bitcoin's current design means we'll need a map of
confirmed outpoints to TXO insertion order indexes. But it's not particularly
hard to add that "metadata" to transactions on the P2P layer in the same way
that segwit added witnesses to transactions without modifying how txids were
calculated; if you only connect to peers who provide you with TXO index
information in blocks and transactions, you don't need to keep that map
Finally, note how this makes transactions *smaller* in many circumstances: it's
just a 8-byte max index rather than a 40 byte outpoint.

@_date: 2017-02-24 20:01:22
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.
AFAIK there aren't any known weaknesses in RIPEMD160, but it also hasn't been
as closely studied as more common hash algorithms. That said, Bitcoin uses
RIPEMD160(SHA256(msg)), which may make creating collisions harder if an attack
is found than if it used RIPEMD160 alone.

@_date: 2017-02-24 23:12:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Yes, exactly.
So to be clear, what you're proposing there is to use the insertion order as
the index - once you go that far you've almost entirely re-invented my
In fact, when I was working my proofchains/proofmarshal libraries I put some
thought into whether or not I could leave out the MMR merkelized list
implementation and use only the key-value map I also wrote. I decided to
include both as they aren't quite the same datastructure - using a list for a
list has advantages.
Your merkle-set implementation is 1500 lines of densely written Python with
almost no comments, and less than a 100 lines of (also uncommented) tests. By
comparison, my Python MMR implementation is 300 lines of very readable Python
with lots of comments, a 200 line explanation at the top, and 200 lines of
(commented) tests. Yet no-one is taking the (still considerable) effort to
understand and comment on my implementation. :)
Fact is, what you've written is really daunting to review, and given it's not
in the final language anyway, it's unclear what basis to review it on anyway. I
suspect you'd get more feedback if the codebase was better commented, in a
production language, and you have actual real-world benchmarks and performance
In particular, while at the top of merkle_set.py you have a list of advantages,
and a bunch of TODO's, you don't explain *why* the code has any of these
advantages. To figure that out, I'd have to read and understand 1500 lines of
densely written Python. Without a human-readable pitch, not many people are
going to do that, myself included.
Lost coins alone guarantees that access patterns will be biased towards new
coins being more likely to be spent. That basis alone is sufficient to justify
an insertion-ordered data structure. Additionally, people have done graphs of
the average age of UTXO's when spent, and that data clearly shows that newer
coins are more likely to be spent than older coins.
Like I mentioned in the email you're replying to, that extra lookup can be
easily avoided with a change to how transactions/blocks are serialized; if all
your peers support TXO commitments you can even discard the lookup database
entirely, as it's only a backwards compatibility measure.
Optimization is itself extra complexity. If you're data structure has worse
inherent performance, and you have to make up the different with a highly
optimized implementation, that's likely to lead to more overall complexity than
using a data structure with better inherent performance.
Your current merkle-set implementation definitely _is_ very complex. An
apples-to-apples comparison is with my merkelized key:value tree(1), also a
patricia tree, which like the MMR is only about 300 lines of well-commented and
straight-forward code.
1) To be clear, "insertion ordering" isn't a simple trick, it's a fundamental
change to what the data structure is. Once you do that, you're talking about my

@_date: 2017-02-25 14:12:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
That's something that we're well aware of; there have been a few discussions on
this list about how P2SH's 160-bits is insufficient in certain use-cases such
as multisig.
However, remember that a 160-bit *security level* is sufficient, and RIPEMD160
has 160-bit security against preimage attacks. Thus things like
pay-to-pubkey-hash are perfectly secure: sure you could generate two pubkeys
that have the same RIPEMD160(SHA256()) digest, but if someone does that it
doesn't cause the Bitcoin network itself any harm, and doing so is something
you choose to do to yourself.
In any case, segwit will provide a 256-bit pay-to-witness-script-hash(1), which
provides a 128-bit security level against collision attacks.
1)

@_date: 2017-02-25 15:57:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
That's what I said: "P2SH's 160-bits is insufficient in certain use-cases such
as multisig"
Obviously any usecase where multiple people are creating a P2SH redeemScript
collaboratively is potentially vulnerable. Use-cases where the redeemScript was
created by a single-party however are _not_ vulnerable, as that party has
complete control over whether or not collisions are possible, by virtue of the
fact that they're the ones who have to make the collision happen!
Similarly, even in the multisig case, commit-reveal techniques can mitigate the
vulnerability, by forcing parties to commit to what pubkeys/hashlocks/etc.
they'll use for the script prior to pubkeys/hashlocks/etc. being revealed.
Though a better long-term approach is to use a 256-bit digest size, as segwit

@_date: 2017-02-25 16:04:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
I'm very aware of that, in fact I think I may have even been the first person
to post on this list the commit-reveal mitigation.
Note how I said earlier in the message you're replying to that "P2SH's 160-bits
is insufficient in certain use-cases such as multisig"

@_date: 2017-02-25 16:40:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
I can't agree with you at all there: we're still at the point where the
computational costs of such attacks limit their real-world impact, which is
exactly when you want the *maximum* exposure to what they are and what the
risks are, so that people develop mitigations.
Keeping details secret tends to keep the attacks out of public view, which
might be a good trade-off in a situation where the attacks are immediately
practical and the need to deploy a fix is well understood. But we're in the
exact opposite situation.
Deploying segwit's 256-bit digests is a response that's already fully coded and
ready to deploy, with the one exception of a new address format. That address
format is being actively worked on, and could be deployed relatively quickly if

@_date: 2017-01-12 14:58:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] Transaction Replacement by Fee 
Additionally some miners mine full replace-by-fee, which has no limitations on
nSequence. My implementation (for v0.13.2) is here:
    and is identical to Bitcoin Core modulo the nSequence stuff being removed, and
a special service bit added to allow full-rbf nodes to preferentially peer with
each other to make sure replacement transactions get propagated.
In practice full-RBF works fairly well, so while it's even faster to use the
nSequence signalling specified in BIP-125, doing so is not mandatory so long as
you can et your replacement transaction to a full-RBF node.

@_date: 2017-01-28 18:29:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
That's a perfect example of why fraud proofs aren't as secure as expected: the miner who created such a block wouldn't even give you the data necessary to prove the fraud in the first place.
What you actually need are validity challenges, where someone makes a challenge claiming that part of the block is invalid. A failure to meet the challenge with proof that the rules are followed is considered defacto evidence of fraud.
But validity challenges don't scale well and pose DoS attacks issues; it's far from clear that they can be implemented in a useful way. Even if validity challenges work, they also don't solve censorship: a world of nodes in large datacenters is a world where it's very easy to force the few Bitcoin nodes remaining to follow AML/KYC rules for instance, a risk we wouldn't be able to mitigate with a PoW change.

@_date: 2017-01-28 18:22:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
Satoshi also thought that SPV clients would be able to use fraud proofs (called "alerts" in the white paper) to detect fraudulent behavior by miners, and thus not have to completely trust those nodes in those datacenters. Unfortunately it turns out that fraud proofs are both a very difficult engineering challenge to implement, and also offer much less security than once thought. In fact, as per Satoshi's vision, SPV clients don't currently exist; what's called SPV isn't what Satoshi was envisioning.
Of course, this wouldn't be the first time that aspects of Satoshi's vision for Bitcoin turned out to be wrong: the white paper also refers to the "longest chain" rather than most-work chain, something that had to be fixed in what's technically a hardfork after Bitcoin's initial release.

@_date: 2017-01-28 16:54:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
So, in that particular type of case, the ZK proof may show that the block
itself is valid and follows all the rules; there'd be no need to get the block
data to know that.
The issue here is other miners being able to mine. Exactly what happens here
depends on the exact construction of the ZK proofs, but at best the missing
data will mean that part of the UTXO state can no longer be updated by other
miners, and thus they can't mine all transactions; at worst they'd be
completely preventing from mining at all.
This is why part of the economic pressure that users exert on miners is
subverted by SPV/lite-clients: users that can transact without sufficient
blockchain data to allow others to mine aren't exerting pressure on miners to
allow other miners to mine - particularly new entrants to mining. In that
respect, ZK proofs are in fact quite harmful to the security of the system if
applied naively.
Equally, I'll point out that if ZK proofs can be made sufficiently powerful to
do all the above, genuinely scalable sharded systems like my own Treechains are
far easier to implement, changing the discussion entirely. Currently it is far
from proven that ZK proofs can in fact accomplish this; I hear that Zcash will
soon have to upgrade their ZK-SNARK scheme due to advances in cryptographic
analysis that may result in a full system break in the near future. We really
don't want to be depending on that technology for Bitcoin's security until
events like that become much less common.

@_date: 2017-07-17 17:50:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap [Update] 
A closely related example is my own Treechains work, which got a bunch of
excitement when I first published the idea. But would I have wanted it on a
roadmap? Hell no: sure enough, as it got more peer review others (and myself!)
found that it was going to be a harder than it initially looked to actually get
into production.
Drivechains is definitely in that situation right now.
Also don't forget that proper security peer review takes a *lot* of work. I
myself have a todo list item to respond to Paul's post on Drivechains, but I
need to spend a few days to do that and just haven't had the time (not to
mention that no-one is paying me to do general Bitcoin dev work right now).

@_date: 2017-06-19 14:31:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] An alternative way to protect the network from 
You have to specify what you mean by "PoS" - there's dozens of variations.
Equally, existing pure PoS schemes probably don't make sense as a "bolt-on"
add-on, as once you introduce PoW to it you should design something that uses
the capabilities of both systems.
FWIW, I've heard that the Ethereum guys are leaning towards abandoning pure PoS
and are now trying to design a PoW + staking system instead.
To be clear, you mean such a scheme would protect the multi-billion dollar
investments non-malicious miners have made in SHA256^2 hardware by ensuring it
remains useful, right?
Note that if those PoS blocks are *pure* PoS, you'll create a significant risk
of double-spend attacks, as there's zero inherent cost to creating a pure-PoS
block. Such blocks can't be relied on for confirmations; even "slasher" schemes
have significant problems with sybil attacks.
The scaling problem is one of scalability; PoS does nothing to improve
scalability (though many in the ETH community have been making dishonest
statements to the contrary).
As a sidechain yes, but in what you propose above the extra blocks wouldn't
contain transactions that non-PoS-aware nodes could understand in a
backwards-compatible way.
All the above aside, I don't think it's inherently wrong to look at adding PoS
block *approval* mechanisms, where a block isn't considered valid without some
kind of coin owner approval. While pure-PoS is fundamentally broken in a
decentralized setting, it may be possible to mitigate the reasons it's broken
with PoW and get a system that has a stronger security model than PoW alone.
FWIW there's some early discussions by myself and others about this type of
approach on the  IRC channels, IIRC from around 2014 or so.

@_date: 2017-06-27 00:13:08
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Relevant: fixed points can be found for the SHA256 compression function, if the
attacker can control the IV:

@_date: 2017-02-28 20:56:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
Can you explain in more detail what you mean there?

@_date: 2017-03-01 17:31:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Better MMR Definition 
But do you see my point here? Even though I spent some time reading through
that code, I didn't realise you had a 300 line reference implementation
embedded within those 1500 lines. This makes it less likely for you to get any
review on either.
A better way to present your work would have been to at least explain that at
the top of the file, and perhaps even better, split the reference
implementation and optimized implementation into two separate files. If you did
this, you'd be more likely to get others to review your work.
Yes, and it's good that you have those comments. But the codebase itself could
definitely use more, and adding those comments would help get more people
reviewing your work.
Great! But you see how without comments, it'll take a tremendous amount of work
for an external reviewer like myself to determine what is being tested, and
what edge cases you're targeting.
In fact, I'd suggest that for things like edge cases, you test edge cases in
separate unit tests that explain what edge cases you're trying to catch.
To be clear, I gave my implementation as an example of how hard it is to get
external review, not to suggest it's going to be a part of Bitcoin; I've
pointed a lot of people to it when they asked for a MMR implementation, and I'm
sure if some of those people had reviewed it carefully they would have
suggested changes. Yet they haven't, because doing good review is a lot of
That's good, but that paragraph should be part of your MerkleSet git repo,
preferably in the README, where reviewers will immediately find it and get
excited about reviewing your code.

@_date: 2017-03-21 15:14:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
Doing that only makes addresses a few % shorter, at the cost of significant
downsides.  For example, not everyone knows what those additional characters
are called, particularly for non-English-speaking users. Non-alphanumeric
characters also complicate using the addresses in a variety of contexts ('/'
in particularly isn't valid in filenames).
I'd suggest you review the "Rational" section of the BIP for more details:

@_date: 2017-05-07 18:34:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
Exactly - knowledge of the English language isn't a binary. Equally, I don't
remember ever learning names of special characters in French class back in
elementary school, but I do recall us drilling the alphabet and especially
numbers repeatedly.
If I were trying to tell a French speaker a BTC address, I'd probably be able
to succesfully do it with bech32, but not with any encoding using special
FWIW, I also did a partial rust implementation of just the Bech32 encoding for
a prototype non-BTC use-case. Other than the version number being it's own
"chunk" I found it very straight-forward to implement and I think it'll make
for a nice replacement for what otherwise would have been hex digests.

@_date: 2017-05-12 18:22:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
I strongly disagree with this proposal.
nVersion signaling is already technically unenforceable, in the sense that we
don't have good ways of ensuring miners actually adopt the rules they're
claiming to signal. Equally, it's users who ultimately adopt rules, not miners,
and attempting to pay miners to signal certain bits will further confuse this
Quite likely the outcome of users trying to anonymously pay anonymous miners to
signal certain bits will be the complete breakdown of the honesty of the
nVersion signalling system, currently enforced only by "gentlemans agreement".
A more productive direction would be a direct coin-owner signalling process,
with users taking action based on what provable coin-ownership has signalled.
Also, as an aside, this "specification" again shows the inadequacy and
unreadability of English language specifications. I'd strongly suggest you
delete it and instead mark the "reference implementation" as the specification.

@_date: 2017-05-13 08:48:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
I'm not arguing that it changes that; I'm arguing that it further confuses the
I think you're assuming that the users paying for soft-fork signalling will
represent an economic majority; that's not necessarily the case.
For example, if miners decide there's no downside to false signalling, they may
take the extra fees provided by 1% of the users paying to signal a fork, while
the other 99% don't participate, resulting in a situation where we have blocks
violating the nVersion protocol, and an unknown % of that 99% rejecting those
blocks. At best that'd be no worse than a UASF, and at wost you're wrecked the
validity of the nVersion "gentlemans agreement"
Just read it: you have ten separate lines of dense English text describing
something that could have been specified instead by ten lines of much more
formally defined C++. In particular, note how many of those lines of English
text refer to C++ code anyway, like the sentence "minimal-length 40-bit
I don't want to have to learn another language - formally defined English that
still fails to be formally defined - just to read Bitcoin's specification.

@_date: 2017-05-16 07:01:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
To be clear, *none* of the previous (U)TXO commitment schemes require *miners*
to participate in generating a commitment. While that was previously thought to
be true by many, I've seen no counter-arguments to the argument I published I
few months ago(1) that (U)TXO commitments did not require a soft-fork to
1) "[bitcoin-dev] TXO commitments do not need a soft-fork to be useful",
   Peter Todd, Feb 23 2017,

@_date: 2017-05-16 08:23:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] TXO commitments do not need a soft-fork to be 
Lol, good job! And you even figured out that lovely "distributed file system"
explanation first.
Though, it does look like I'm still the person who made it 100% *clear* the
first time - you're explanation is easy to read the wrong way, particularly
when you say:
"Next time I will teach you how to implement a blockchain-based cryptocurrency
in such a way that new miners can start mining right away without downloading
whole blockchain, stay tuned..."
After all, at the time UTXO commitments had been already discussed. Also,
talking about a DHT in relation to this stuff probably made the explanation get
missed by some people.
Unfortunately, I think this is a good example of how important coming up with
good explanations and analogies is. :/

@_date: 2017-05-22 02:27:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] Barry Silbert segwit agreement 
It's interesting how changing the bit used to signal could be used as a way to
try to trick people into changing node software ASAP to support the hard-fork
code. Basically, the narrative would be that other software *doesn't* support
segwit, so you have to upgrade right away.
In contrast this proposal wouldn't have that effect, because as you point out
it's compatibel with the existing segwit protocol once activated.
Smells like political engineering to me.

@_date: 2017-05-22 09:33:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
Thanks for the credit, although I think the security properties of what you're
proposing are very different - and much weaker - than what I proposed in
As you state in [2] "if miners never validate sidechains at all, whoever bids
the most for the chain (on a continuous basis), can spam a 3-month long stream
of invalid headers, and then withdraw all of the coins deposited to the
sidechain." and "Since the mining is blind, and the sidechain-withdrawal
security-level is SPV, miners who remain blind forever have no way of telling
who ?should? really get the funds."
Finally, you suggest that in this event, miners *do* have to upgrade to a full
node, an expensive and time-consuming operation (and one that may be impossible
for some miners if necessary data isn't available).
It's unclear to me what the incentive is for miners to do any of this. Could
you explain in more detail what that incentive is?

@_date: 2017-05-22 10:05:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
To be clear, what math operations do you mean by "?" and "?"?

@_date: 2017-05-22 10:09:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] A proposal to reintroduce the disabled script 
It'd help your case if you gave us some examples of such scripts being used.
See the CHECKSEQUENCEVERIFY and my own CHECKLOCKTIMEVERIFY bips for examples of
how to write up such use-cases.

@_date: 2017-05-22 12:14:04
@_author: Peter Todd 
@_subject: [bitcoin-dev] A proposal to reintroduce the disabled script 
Great! That's exactly the type of justifying use-case we need for a BIP.
An OP_CAT will have to have limits on maximum output size; how big an output
does your application need?

@_date: 2017-05-27 13:41:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Cartesian product can mean a lot of things.
What specifically do you mean by "cartesian product" here?

@_date: 2017-05-28 04:26:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
This doesn't hold true in the case of pruned trees, as for the pruning to be
useful, you don't know what produced the left merkleRoot, and thus you can't
guarantee it is in fact a midstate of a genuine SHA256 hash.

@_date: 2017-05-28 17:07:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
That's not at all true. If I'm a miner with a better capability than another
miner to prevent that theft, I have reasons to induce it to happen to give me
political cover to pushing that other miner off the network.
This is a very similar problem to what we had with zeroconf double-spending,
where entities such as Coinbase tried to pay off miners to guarantee something
that wasn't possible in a geninely decrentralized system: safe zeroconf
Why are you forcing miners to run this code at all?
Equally, you're opening up miners to huge political risks, as rejecting all
withdrawals is preventing users' from getting their money, which gives other
miners a rational for kicking those miners off of Bitcoin entirely.
Why do you think this will be infrequent? Miners with a better ability to
validate the drivechain have every reason to make these events more frequent.
This is also a very dubious security model - I would argue that Bitcoin is much
*more* valuable if miners do everything they can to ensure that drivechains
fail, given the huge risks involved. I would also argue that users should do
user-activated-soft-forks to ensure they fail.
By comparison, note Adam Back and my own efforts to ensure miners have a
smaller part in the ecosystem, with things like committed (encrypted)
transactions and my closed-seal-set/truth-list approach(1). We want to involve
miners as little as possible in the consensus, not more.
I have to ask: What use-cases do you actually see for drivechains? Why can't
those use-cases be done in the much safer client-side validation fashion?
1)

@_date: 2017-05-29 12:10:59
@_author: Peter Todd 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Well, it's an easy thing to forget, unless like me, you've written a general
purpose library to work with pruned data. :) (actually, soon to be two of
I also ran into the midstate issue with my OpenTimestamps protocol, as I was
looking into whether or not it was safe to have a SHA256 midstate commitment
operation, and couldn't find clear evidence that it was.
Notice how what you're proposing here is almost the same thing as using SHA256
directly, modulo the fact that you skip the final block containing the message
Similarly, you don't need to compute sha256(t) - you can just as easily compute
the midstate sha256Compress(IV, t), and cache that midstate if you can reuse
tags. Again, the only difference is the last block.
I think a better question to ask is why you want that property in the first
My earlier python-proofmarshal(1) library had a scheme of per-use-case tags,
but I eventually realised that depending on tags being unique is a footgun. For
example, it's easy to see how two different systems could end up using the same
tag due to designers forgetting to create new tags while copying and pasting
old code. Similarly, if two such systems have to be integrated, you'll end up
with tags getting reused for two different purposes.
Now, if you design a system where that doesn't matter, then by extension it'll
also be true that collisions between the sha256 and merkleroot functions don't
matter either. And that system will be more robust to design mistakes, as tags
only need to be unique "locally" to distinguish between different sub-types in
a sum type (enum).
FWIW what I've done with my newer (and as yet unpublished) rust-proofmarshal
work is commitments are only valid for a specific type. Secondly, I use
blake2b, whose compression function processes up to 128 bytes of message on
each invocation.  That's large enough for four 32 byte hashes, which is by
itself more than sufficient for a summed merkle tree with three 32 byte hashes
(left right and sum) and a per-node-type tag.
Blake2b's documentations don't make it clear if it's resistant to collision if
the adversary can control the salt or personalization strings, so I don't
bother using them - the large block size by itself is enough to fit almost any
use-case into a single block, and it hashes blocks significantly faster than
SHA256. This also has the advantage that the actual primitive I'm using is 100%
standard blake2b, an aid to debugging and development.
1)

@_date: 2017-11-06 14:50:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] Introducing a POW through a soft-fork 
Some quick thoughts...
First of all, I don't think you can really call this a soft-fork; I'd call it a
My reasoning being that after implementation, a chain with less total work than
the main chain - but more total SHA256^2 work than the main chain - might be
followed by non-supporting clients. It's got some properties of a soft-fork,
but it's security model is definitely different.
Note how you're basically proposing for the block interval to be decreased,
which has security implications due to increased orphan rates.
Exactly! Not really a soft-fork.

@_date: 2017-11-14 04:11:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] Updates on Confidential Transactions efficiency 
Re: section 4.6, "For cryptocurrencies, the binding property is more important
than the hiding property. An adversary that can break the binding property of
the commitment scheme or the soundness of the proof system can generate coins
out of thin air and thus create uncontrolled but undetectable inflation
rendering the currency useless.  Giving up the privacy of a transaction is much
less harmful as the sender of the transaction or the owner of an account is
harmed at worst."
I _strongly_ disagree with this statement and urge you to remove it from the
The worst-case risk of undetected inflation leading to the destruction of a
currency is an easily quantified risk: at worst any given participant loses
whatever they have invested in that currency. While unfortunate, this isn't a
unique or unexpected risk: cryptocurrencies regularly lose 50% - or even 90% -
of their value due to fickle markets alone. But cryptocurrency owners shrug
these risks off. After all, it's just money, and diversification is an easy way
to mitigate that risk.
But a privacy break? For many users _that_ threatens their very freedom,
something that's difficult to even put a price on.
Furthermore, the risk of inflation is a risk that's easily avoided: at a
personal level, sell your holdings in exchange for a less risky system; at a
system-wide level, upgrade the crypto.
But a privacy leak? Once I publish a transaction to the world, there's no easy
way to undo that act. I've committed myself to trusting the crypto
indefinitely, without even a sure knowledge of what kind of world I'll live in
ten years down the road. Sure, my donation to Planned Parenthood or the NRA
might be legal now, but will it come back to haunt me in ten years?
Fortunately, as section 4.6 goes on to note, Bulletproofs *are* perfectly
hiding. But that's a feature we should celebrate! The fact that quantum
computing may force us to give up that essential privacy is just another
example of quantum computing ruining everything, nothing more.

@_date: 2017-11-14 05:07:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Updates on Confidential Transactions efficiency 
Re: the unprunable accumulators, that doesn't need to be an inherent property
of Zcash/Monero style systems.
It'd be quite feasible to use accumulator epochs and either make unspent coins
in a previous epoch unspendable after some expiry time is reached - allowing
the spent coin accumulator data to be discarded - or make use of a merkelized
key-value scheme with transaction provided proofs to shift the costs of
maintaining the accumulator to wallets.
The disadvantage of epoch schemes is of course a reduced k-anonymity set, but
if I understand the Confidential Transactions proposals correctly, they already
have a significantly reduced k-anonymity set per transaction than Zcash
theoretically could (modulo it's in practice low anonymity set due to lack of
actual usage). In that respect, epoch size is simply a tradeoff between state
size and k-anonymity set size.

@_date: 2017-11-28 05:48:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP159 - NODE_NETWORK_LIMITED service bits, 
FWIW, I run all my pruned nodes with the prune parameter set to about a month
worth of blocks (a few GB). And come to think of it, I should bump that up even
higher now that segwit has increased the blocksize.

@_date: 2017-09-04 10:06:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fwd:  "Compressed" headers stream 
Note that I'm already planning on OpenTimestamps having infrastructure for
trusted validity attestations; log scaling proofs alone only prove total work,
not validity. Timestamping has all kinds of very dubious security properties
when done via proof-of-work, due to various ways that miners can get away with
inaccurate block times. In particular, setting a block time backwards is
something that miners can do, particularly with majority hashing power, which
is the exact thing we're trying to prevent in a timestamp proof.
This all makes me dubious about risking further weakening of this already weak
security with compact SPV proofs; we'd need a lot more analysis to understand
what we're risking. Also note that we can ship a known-good
sum-merkle-tree tip hash with the software, which further reduces initial
download bandwidth needed to get the block headers on top of this obviously
safe eliding of redundant hashes.

@_date: 2017-09-04 10:10:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] "Compressed" headers stream 
Timestamping can easily be *more* vulnerable to malicious miners than financial
applications for a number of reasons, including the fact that there's no
financial feedback loop of miners destroying the value of the coins they
produce - timestamping is a non-financial piggy-back application that doesn't
directly interact with the Bitcoin economy, beyond a trival number of timestamp

@_date: 2017-09-04 09:51:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] P2WPKH Scripts, P2PKH Addresses, 
It's not a theoretical use-case: the two OpenTimestamps calendar servers I run
- {alice,bob}.btc.calendar.opentimestamps.org - use native P2WPKH segwit
outputs to keep transaction size to the absolute minimum possible; previously
they used bare CHECKSIG  output scripts for the same reason.
I enabled support for it the moment segwit activated, so I'm probably the first
ever production user of P2WPKH on mainnet, and quite possibly, the first person
to create P2WPKH outputs on mainnet for any reason.

@_date: 2017-09-07 01:55:57
@_author: Peter Todd 
@_subject: [bitcoin-dev] Fast Merkle Trees 
Note that in general, designs should *not* create new hash functions by using
custom IVs, but rather use bog-standard SHA256, and make a fixed first block.
That allows unoptimised implementations to just hash a block with the second
initialization value, and optimized implementations to start with the fixed

@_date: 2017-09-07 14:00:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
Given that this has a very minimal cost for spammers - just a single satoshi -
I don't think this is worth the risk of making future upgrades more complex as
other posters have brought up.
Secondly, I think we have good reason to think that things like my own TXO
commitments and Bram's related work will make UTXO growth a non-issue in the
So, I'd NACK such a proposal myself.

@_date: 2017-09-07 14:02:56
@_author: Peter Todd 
@_subject: [bitcoin-dev] Proposal: Extended serialization format for 
More to the point, even for the blockchains that don't contain timestamps in
their blocks, their blocks do exist in our spacetime continum and thus are
created at a specific point in time. :)
If someone does however come up with an example of a blockchain that does not
occupy our spacetime continum, I'd love to hear about it!

@_date: 2017-09-07 14:09:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] Proposal: Extended serialization format for 
Note how private key birthday is a potential privacy issue in certain cases.
Rare of course, because usually you don't release your private keys! But users
will on occasion have those keys be compromised.
Personally, I'd advocate rounding down to month-level resolution, as that
should be both enough to handle any likely reorg scenario, and it's still a
precision that won't add much extra scanning ot any reasonably old (~1yr)
Note also how if transactions created with private keys in a seed use
nLockTime, you can ensure coins won't exist in a block older than the seed
birthday by simply ensuring that nLockTime is set to a more recent date then
that birthday under all circumstances.

@_date: 2017-09-13 05:24:34
@_author: Peter Todd 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
So with Confidential Transactions, the only thing that's changed relative to a
normal Bitcoin transaction is that fact that the sum of input values is >= the
sum of output values is proven via a CT proof, rather than revealing the actual
sums. Other than that, CT transactions don't need to be any different from
regular transactions.
For CT to be a softfork, we have to ensure that each CT transaction's sum of
inputs and outputs is valid. An obvious way to do this is to have a pool of
"shielded" outputs, whose total value is the sum of all CT-protected outputs.
Outputs in this pool would appear to be anyone-can-spend outputs to pre-CT
This gives us three main cases:
1) Spending unshielded outputs to CT-shielded outputs
Since the CT-shielded output's value is unknown, we can simply set their value
to zero. Secondly, we will add the newly CT-shielded value to the pool with an
additional output whose value is the sum of all newly created CT-shielded
2) Spending CT-shielded outputs to unshielded outputs
Here one or more CT-shielded outputs will be spent. Since their value is zero,
we make up the difference by spending one or more outputs from the CT pool,
with the change - if any - assigned to a CT-pool output.
3) Spending CT-shielded outputs to CT-shielded outputs
Since both the inputs and outputs are zero-valued, to pre-CT nodes the
transaction is perfectly valid: the sum of coins spent is 0 BTC, and the sum of
coins created is also 0 BTC. We do have the problem of paying miners fees, but
that could be done with an additional CT output that the miner can spend, a
child-pays-for-parent transaction, or something else entirely that I haven't
thought of.
Suppose zero-valued outputs are prohibited. In case  above, if there are more
outputs than inputs, we need to add an additional input from the CT-shielded
pool to make up the difference, and an additional change output back to the
CT-shielded pool.
If shielded-to-shielded transactions are common, these extra outputs could
consume a significant % of the total blockchain space - that's a significant
cost. Meanwhile the benefit is so small it's essentially theoretical: an
additional satoshi per output is an almost trivial cost to an attacker.
Quite simply, I just don't think the cost-benefit tradeoff of what you're
proposing makes sense.

@_date: 2017-09-13 05:41:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
Ethereum does something quite like this; it's a very bad idea for a few
1) If you bailed out of verifying a script due to wasted ops, how did you know the
transaction trying to spend that txout did in fact come from the owner of it?
2) How do you verify that transactions were penalized correctly without *all*
nodes re-running the DoS script?
3) If the DoS is significant enough to matter on a per-node level, you're going
to have serious problems anyway, quite possibly so serious that the attacker
manages to cause consensus to fail. They can then spend the txouts in a block
that does *not* penalize their outputs, negating the deterrent.

@_date: 2017-09-13 06:03:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Minutia in CT for Bitcoin. Was: SF proposal: 
So to be clear, we have two versions of this problem:
1) CT signatures do *not* sign which pool input they're using
Here, obviously the inputs can be changed at will by miners. An implementation
could have the exact CT pool input be something miners add; the CT transactions
broadcast on the P2P network wouldn't actually need them.
2) CT signatures *do* sign which pool input they're using
Wallets would pick the input at random. This is required if you want to have a
transaction spending both CT and legacy inputs. This reduces the reorg risk to
double-spends. While double-spends are always a potential problem, the problem
is somewhat worse here, as even regular wallets are spending inputs that anyone
can choose to spend.
So basically, you're essentially observing that in the event that everyone uses
CT, this isn't actually a problem; you're allowing everyone to "use" CT, by
trying to allow even unshielded outputs to "use" it.
Which means by "unshielded output", what you *actuall* mean is creating a CT
transaction where the output - even though it's a zero-valued CT output - is
constructed such that the value is public information.
Or do you mean trying to have non-CT outputs in the pool somehow? I don't think
that makes sense, because the whole point of the pool is that the outputs in it
are anyone-can-spend, and thus any CT transaction may spend them; which CT
transaction spends them gives no information about the ownership of the coins.
This is incompatible with anything but anyone-can-spend outputs.
Note that the order in which outputs in the pool are spent can be
deterministic. For example, you could say that each transaction must spend the
oldest outputs in the pool (that sum to the value needed). You could probably
come up with a scheme where the outputs that will be spent in the future in the
event that the output is spent back to an unshielded output is fixed when the
output was created, for example, by picking a random index. While this wouldn't
prevent all collisions, it'd may be possible to make reorgs relatively safe, by
constraining how miners could txids.
Specifically, you could imagine a scheme where if a given input set can only be
satisified by unspent pool outputs with index's >= i, then the miner would need
to have the ability to mine a conflicting transaction that also happened to
have the same pool output set. Given a sufficiently large set of pool outputs,
this may be an impractical attack most of the time.

@_date: 2017-09-27 12:06:54
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to BIP-173 
Re-use of old addresses is a major problem, not only for privacy, but also
operationally: services like exchanges frequently have problems with users
sending funds to addresses whose private keys have been lost or stolen; there
are multiple examples of exchanges getting hacked, with users continuing to
lose funds well after the actual hack has occured due to continuing deposits.
This also makes it difficult operationally to rotate private keys. I personally
have even lost funds in the past due to people sending me BTC to addresses that
I gave them long ago for different reasons, rather than asking me for fresh
To help combat this problem, I suggest that we add a UI-level expiration time
to the new BIP173 address format. Wallets would be expected to consider
addresses as invalid as a destination for funds after the expiration time is
Unfortunately, this proposal inevitably will raise a lot of UI and terminology
questions. Notably, the entire notion of addresses is flawed from a user point
of view: their experience with them should be more like "payment codes", with a
code being valid for payment for a short period of time; wallets should not be
displaying addresses as actually associated with specific funds. I suspect
we'll see users thinking that an expired address risks the funds themselves;
some thought needs to be put into terminology.
Being just an expiration time, seconds-level resolution is unnecessary, and
may give the wrong impression. I'd suggest either:
1) Hour resolution - 2^24 hours = 1914 years
2) Month resolution - 2^16 months = 5458 years
Both options have the advantage of working well at the UI level regardless of
timezone: the former is sufficiently short that UI's can simply display an
"exact" time (though note different leap second interpretations), while the
latter is long enough that rounding off to the nearest day in the local
timezone is fine.
Supporting hour-level (or just seconds) precision has the advantage of making
it easy for services like exchanges to use addresses with relatively short
validity periods, to reduce the risks of losses after a hack. Also, using at
least hour-level ensures we don't have any year 2038 problems.

@_date: 2017-09-27 17:15:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
My concern is not primarily people re-using addresses, but rather people using
stale addresses that the recipient would rather not be used anymore. This
situation often happens even if the stale address has never been used.

@_date: 2017-09-27 17:20:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
Why should addresses have a birthdate? I don't see why that information would
be relevant to the person sending funds, and it could pose a privacy risk.

@_date: 2017-09-27 17:33:07
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
Just remembered: it's notable how Coinbase has a 10 minute timeout on their
payment window, which is in effect a 10 minute expiry time for the address.
Presumably they'd make use of this feature if it existed.

@_date: 2017-09-28 21:45:43
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
I'm well aware. As the payment protocol hasn't caught on - and doesn't fully
overlap all the usecases that addresses do anyway - I think we should consider
bringing this important feature to Bitcoin addresses too.

@_date: 2017-09-28 21:50:48
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
Notably, even something as simple as adding a new type of confirmation window
that might be needed is a big chance to UI logic.
Yeah, I just noticed Pieter Wuille's BIP173-including segwit pull-req - that's
a lot of code that would get touched by this proposal, so it's likely too late
in the process.
What do you mean by "an embedded amount"?
I'm inclined to agree as well. Also, Bitcoin payments themselves are inherently
imprecise, because blocks aren't found on a regular interval - Coinbase's "10
minute" payment expiry window is odd from that point of view.
Having said that, you'd want a resolution more precise than what you'd expect
timeouts to be set at, to avoid UI "fencepost" oddity; if I want to set a 1 day
timeout, users shouldn't see either 1 or 2 days depending on exactly which way
it happened to be rounded that particular time..

@_date: 2017-09-28 21:52:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revising BIP 2 to expand editorial authority 
As part of this, we may want to say that the BIP editor should
cryptographically sign (and ideally timestamp) all their changes as a secondary
measure to make it clear who actually made the change.

@_date: 2017-09-28 22:02:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
This proposed fix is itself broken, because the miner can easily include *only*
transactions paying out-of-band, at which point the fee can be anything.
Equally, miners can provide fee *rebates*, forcing up prices for everyone else
while still allowing them to make deals.
Also, remember that you can pay fees via anyone-can-spend outputs, as miners
have full ability to control what transactions end up spending those outputs.
The fact these countermeasures are all likely to be implemented - all of which
harm the overall ecosystem by reducing visibility of fees and making it harder
to compete with centralized miners - makes me very dubious about that proposal.

@_date: 2017-09-28 22:10:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
Note too that the fee users are willing to pay often changes over time.
My OpenTimestamps service is a perfect example: getting a timestamp confirmed
within 10 minutes of the previous one has little value to me, but if the
previous completed timestamp was 24 hours ago I'm willing to pay significantly
more money because the time delay is getting significant enough to affect the
trustworthyness of the entire service. So the fee selection mechanism is
nothing more than a RBF-using loop that bumps the fee every time a block gets
mined w/o confirming my latest transaction.
This kind of time sensitivity is probably true of a majority of Bitcoin
use-cases, with the caveat that often the slope will be negative eventually:
after a point in time completing the transaction has no value.

@_date: 2017-09-28 22:18:46
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
This has been discussed many times before; there are *severe* downsides to
making it possible for transactions to become invalid after the fact.
Note that "large range" is a requirement driven by the fact that expiry times
will inevitably be specified absolutely, not relatively: when the range runs
out you need to upgrade the standard. Better to use another character and use a
range that won't run out any time soon.
This wouldn't create a need for more checksum space.

@_date: 2017-09-28 22:55:38
@_author: Peter Todd 
@_subject: [bitcoin-dev] Why the BIP-72 Payment Protocol URI Standard is 
The BIP-70 payment protocol used via BIP-72 URI's is insecure, as payment qr
codes don't cryptographically commit to the identity of the merchant, which
means a MITM attacker can redirect the payment if they can obtain a SSL cert
that the wallet accepts.
For example, if I have a wallet on my phone and go to pay a
merchant, a BIP-72 URI will look like the following(1):
    bitcoin:mq7se9wy2egettFxPbmn99cK8v5AFq55Lx?amount=0.11&r=
A wallet following the BIP-72 standard will "ignore the bitcoin
address/amount/label/message in the URI and instead fetch a PaymentRequest
message and then follow the payment protocol, as described in BIP 70."
So my phone will make a second connection - likely on a second network with a
totally different set of MITM attackers - to In short, while my browser may have gotten the correct URL with the correct
Bitcoin address, by using the payment protocol my wallet is discarding that
information and giving MITM attackers a second chance at redirecting my payment
to them. That wallet is also likely using an off-the-shelf SSL library, with
nothing other than an infrequently updated set of root certificates to use to
verify the certificate; your browser has access to a whole host of better
technologies, such as HSTS pinning, certificate transparency, and frequently
updated root certificate lists with proper revocation (see Symantec).
As an ad-hoc, unstandardized, extension Android Wallet for Bitcoin at least
supports a h= parameter with a hash commitment to what the payment request
should be, and will reject the MITM attacker if that hash doesn't match. But
that's not actually in the standard itself, and as far as I can tell has never
been made into a BIP.
As-is BIP-72 is very dangerous and should be depreciated, with a new BIP made
to replace it.
1) As an aside, it's absolutely hilarious that this URL taken straight from
   BIP-72 has the merchant using PHP, given its truly terrible track record for
   security.

@_date: 2017-09-28 23:02:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
I specifically outlined a scenario where that text isn't relevant: *all*
transaction in a block can be paying out of band.
You're making the incorrect assumption that all transactions have to be
broadcast publicly; they don't.
It certainly does not. It simply adds another level of complexity and overhead
to the out-of-band payment situation, which is not desirable. If we can't
eliminate out of band payments entirely, we do not want to make the playing
field of them even more unbalanced than it already is.
This is a typical academic proposal that only considers first order effects
while ignoring second order effects.

@_date: 2017-09-29 05:55:37
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
"Virtually all"?
I regularly pay with Bitcoin, and I haven't seen the payment protocol used in
Can you name some users of it?

@_date: 2017-09-29 09:52:03
@_author: Peter Todd 
@_subject: [bitcoin-dev] Address expiration times should be added to 
Lol, interesting mistake I made w/ Coinbase: my mobile wallets are all setup in
ways that don't support the payment protocol w/ Coinbase, probably because come
to think of it they were (still are?) rejecting payment protocol requests over
proxies and Tor. And on my desktop setups payment protocol URLs don't work for
various reasons, and I'd forgotten I'd manually disabled them ages ago.
Just checked and Bitfinex, BTCC, and Shapeshift all don't seem to use the
payment protocol.
Other than BitPay and Coinbase, do you have an example of a service supporting
the payment protocol?
User-to-user payments pretty much always use naked addresses.

@_date: 2018-04-11 03:52:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Few questions regarding ListTransaction 
Or via full replace-by-fee, which appears to be used by a significant minority
of miners:
In practice transaction replacement by the sender for any transaction is very

@_date: 2018-04-11 05:37:24
@_author: Peter Todd 
@_subject: [bitcoin-dev] Few questions regarding ListTransaction 
My full-replace-by-fee tree ignores that. It also does preferential peering to
ensure it's well connected with likewise peers, and thus the whole network.

@_date: 2018-08-05 23:57:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] Claiming an OP_RETURN Prefix 
It's better if you don't use a prefix at all from a censorship resistance and anonymity perspective; you're application should not require a prefix for technical reasons.

@_date: 2018-08-15 21:46:18
@_author: Peter Todd 
@_subject: [bitcoin-dev] Claiming an OP_RETURN Prefix 
If you're *actually* just doing timestamping you're better off using OpenTimestamps. But many times people think they're just doing timestamping in reality mere timestamps are insufficient for the task.
Notably, this is something the Satoshi Bitcoin white paper gets wrong, incorrectly describing Bitcoin as a timestamping system: timestamping is insufficient to prevent double-spends.

@_date: 2018-08-17 19:18:19
@_author: Peter Todd 
@_subject: [bitcoin-dev] Brock Pierce? 
Hash: SHA512
Note how this message isn't PGP signed, and the headers show it coming from a different server than my usual one.
Not at a computer to check, but likely the SPF filtering on the list mail server isn't working.

@_date: 2018-08-30 16:02:39
@_author: Peter Todd 
@_subject: [bitcoin-dev] Testnet3 Reest 
Actually I'd advocate the opposite: I'd want testnet to be a *larger*
blockchain than mainnet to find size-related issues first.
Note that for testing regtest is often a better alternative, and you can setup
private regtest blockchains fairly easily and with good control over exactly
when and how blocks are created.

@_date: 2018-12-17 23:22:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
This isn't really a security issue, it's a software reliability issue. And
you're making a trade-off between complexity of the core protocol and
complexity of wallet software.
A core protocol failure has high costs for every single Bitcoin user; a wallet
software failure affects a much smaller number of people. So I'd be inclined to
prioritise core protocol simplicity rather than stamping out one of many, many,
ways that wallet software can screw up and lose money.

@_date: 2018-02-12 13:12:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] Total fees have almost crossed the block reward 
Does shabang.io say anywhere how it determines whether or not a transaction
funded a Lightning channel?

@_date: 2018-02-12 17:58:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
I don't actually see where the problem is here. First of all, suppose we have a
transaction T_a that already pays Alice with a feerate sufficiently high that
we expect it to get mined in the near future. If we want to pay Bob, we can do
that by simply creating a double-spend of T_a that pays both Bob and Alice,
T_{ab}. BIP125 only requires that double-spend to have an absolute fee higher
than the minimum relay feerate * size of the transaction.
I just checked one of my nodes, and the absolute minimum relay fee is about
1/5th that of what estimatefee returns for the longest possible estimate, 48
blocks. Depends on the exact circumstances, but it'll likely be worth it to pay
Bob with a replacement of T_a rather than create a second transaction due to
that difference.
Secondly, if for some reason you need to broadcast a separate transaction
paying Bob before you do the replacement, again I don't see an issue: just make
a minimum fee T_b that pays Bob, and replace both with T_{ab}. Again, the big
difference between minimum fee and what you might actually pay in fees means
that you'll still save money in most cases, so long as your wallet is
intelligent enough to pick a low feerate for T_b.
I think what you mean here should be the effective fee rate of the maximum
feerate package that can be built from the set of transactions that begins with
the candidate replacement. But actually calculating this is I believe
non-trivial, which is why I didn't implement it this way when RBF was first
So the previous version of condition  does this implicitly because the
absolute fee isn't allowed to go down; you're effectively re-adding this
condition. But as I've shown above, you can get the same *behavior* by simply
ensuring that the transactions you broadcast that you'll want to double-spend
have a minimum feerate in the first place.
I think this is very important. For example, without this condition I could do
a DoS attack by repeatedly broadcasting a transaction, then spending the
outputs of that transaction with a very large number of child transactions, all
of minimum fee. With up to 100 transactions allowed for consideration, and a
100KB max transaction size, that could be up to ~10MB of transactions.
Next I double spend the root, increasing it's feerate but *not* paying for the
child transactions. Those ~10MB are now evicted from the mempool, and I can
repeat the cycle again. The cost is whatever the root tx replacement cost,
which will be much less than the cost of broadcasting 10MB should have been.
A better way to solve this class of problems may be diffed tx replacement
propagation: basically broadcast a diff between the original tx and the
proposed replacement, allowing you to do the minimum bandwidth accounting based
on the size of the diff instead.

@_date: 2018-02-12 18:42:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Ah ok, I misunderstood and didn't realise you were talking about the case where
Alice re-spends her unconfirmed payment. Unfortunately I don't think that case
is possible to solve without putting some kind of restriction on spending
unconfirmed outputs; with a restriction it's fairly simple to solve.
True, maybe we can just reuse the CPFP calculation now. That said, AFAIK that's
only done in the miner code, not the mempool, so that may not be trivial to
actually do.

@_date: 2018-02-13 13:40:34
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Yeah, sorry, I just misread what scenario you guys were talking about. IIRC the
term "pinned" may have even been invented by myself, as IIRC I noticed the
issue when the RBF patch was being developed years ago. I don't think I had a
solution at the time so I just punted on it.
I'm not sure that's actually true, as you're only creating transactions sets
that are reorg safe. Though I don't have a detailed design in mind so I may be
missing something.
Yes, the diff approach doesn't help for the pinned case.
Unfortunately the only solution I have is basically the same as what you
proposed(1) months ago: limit spends of unconfirmed outputs in some way.
So here's a question: how many wallets have actually implemented CPFP fee bumps
for incoming transactions?
1)

@_date: 2018-02-13 14:03:17
@_author: Peter Todd 
@_subject: [bitcoin-dev] Total fees have almost crossed the block reward 
Sounds plausible; it'd be good if they documented that on the site!

@_date: 2018-01-08 07:45:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Can you explain _exactly_ what scenario the "plausible deniability" feature
refers to?

@_date: 2018-01-08 14:37:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
This sounds very dangerous. As Gregory Maxwell pointed out, the key derivation
function is weak enough that passphrases could be easily brute forced, at which
point the bad guys have cryptographic proof that you tried to lie to them and
cover up funds.
What model of human memory are you assuming here? What specifically are you
assuming is easy to remember, and hard to remember? What psychology research
backs up your assumptions?

@_date: 2018-01-08 19:37:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
We're talking about seeds here, not hardware wallets.
For a hardware wallet theft scenario, if you're worried about muggers you can
make the hardware have secret accounts with different seeds, *without* risking
user funds getting lost - a much more likely scenario - due to mistyped
In any case, even if you were to do this type of design, a much better idea is
to use a checksum by default to reject invalid passwords, while having an
advanced-use-only option to override that checksum. The virtual file encryption
filesystem encfs does exactly this with its --anykey flag. This allows advanced
users to do their thing, while protecting the majority of users for whome this
feature is dangerous.

@_date: 2018-01-08 20:13:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Trezor's "plausible deniability" scheme could very well result in you going to
jail for lying to border security, because it's so easy for them to simply
brute force alternate passwords based on your seeds. With that, they have proof
that you lied to customs, a serious offense.
I would strongly advise you not to use it in that situation.

@_date: 2018-01-12 03:54:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] New Bitcoin Core macOS signing key 
Note that you may need to add -noverify as well if your openssl doesn't have
the Apple Certificate Authority in the CA list.
While a clunky way to do it, you can use the `-signer` option to tell OpenSSL
to write the signer's certificate to a file. That certificate can then be
compared to the one from the repo, which was still in the repo as of the
(signed!) v0.15.1 tag.
Fun fact: OpenTimestamps has git integration, which means you can extract a OTS
proof from 2016 for that certificate from the repo:
    $ git checkout v0.15.1
    $ ots git-extract share/certs/BitcoinFoundation_Apple_Cert.pem share/certs/BitcoinFoundation_Apple_Cert.pem.ots 36f60a5d5b1bc9a12b87d6475e3245b8236775e4
    $ ots verify share/certs/BitcoinFoundation_Apple_Cert.pem.ots
    Assuming target filename is 'share/certs/BitcoinFoundation_Apple_Cert.pem'
    Success! Bitcoin attests data existed as of Thu Oct 13 14:08:59 2016 EDT
Homework problem: write a paragraph explaining how the proof generated by the
above three commands are crypto snakeoil that proved little. :)
Ha! Fortunately even the mailing list archives at lists.linuxfoundation.org
seem to contain the attachment just fine.
But anyway, I'd suggest using base64:
On Linux, the `base64 -d` command will decode the above just fine.
The _real_ issue is that asking the user to cut-n-paste that PKCS7-encoded
message is problematic, as differences in whitespace and line endings will make
the verification fail. Works fine on Linux, but would probably have failed on
What's nice about OpenPGP's "clearsigned" format is how it ignores whitespace;
a replica of that might be a nice thing for OTS to be able to do too. Though
that's on low priority, as there's some tricky design choices(1) to be made about
how to nicely nest clearsigned PGP within OTS.
1) For example, I recently found a security hole related to clearsigned PGP
recently. Basically the issue was that gpg --verify will return true on a file
that looks like the following:
    1d7a363ce12430881ec56c9cf1409c49c491043618e598c356e2959040872f5a  foo-v2.0.tar.gz
    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA256
    e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  foo-v1.0.tar.gz
    -----BEGIN PGP SIGNATURE-----
    -----END PGP SIGNATURE-----
The system I was auditing then did something like this to verify that the file
was signed:
    set -e # exit immediately on error
    gpg --verify SHA256SUMS.asc
    cat SHA256SUMS.asc | grep foo-v2.0.tar.gz
While it makes it a bit less user friendly, the fact that PKCS7's encoding made
it impossible to see the message you signed until it's been properly verified
is a good thing re: security.
And yes, I checked: Bitcoin Core's contrib/verifybinaries/verify.sh isn't
vulnerable to this mistake. :)

@_date: 2018-01-12 04:50:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
While passphrases *can* be long, most user's aren't going to understand the
risk. For example, Trezors blog(1) doesn't make it clear that the passphrases
could be bruteforced and used as evidence against you, and even suggests the
    Since the passphrase is never saved on the device, this means that there is no
    wrong passphrase. The device does not know which one you have chosen, and
    therefore all of them are correct! Given the same seed, for each and every
    letter combination used as a passphrase, a different wallet will be generated.
    Since there is no way to prove that there is any wallet beyond the ones
    that you have admitted to, the ?attacker? will have to be satisfied with
    the revealed ones.
Also note how this blog doesn't mention anti-forensics: the wallet software
itself may leave traces of the other wallets on the computer. Have they really
audited it sufficiently to be sure this isn't the case?
1)

@_date: 2018-01-13 01:11:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] Plausible Deniability (Re: Satoshilabs secret 
It's very common for disks to be filled with pseudorandom data; this is not
suspicious at all. For example:
1) An encrypted partition that is filled, and later reformatted, will be left
full of random bytes. Even if you give border security your passphrase, the
unused space in the encrypted partition will be random data. (an exception
being most - but not all! - SSD's where TRIM has been used)
2) Modern drives (SSD and HD) often implement fast secure erasure with
encryption, which means that the actual data stored to disk or FLASH is
*always* encrypted. If such a drive is wiped, the encryption keys are replaced,
which means whatever data was stored becomes random noise (the encrypted data
is usually not authenticated). This also means that such drives can arrive from
the factory filled with random noise.
3) Software disk encryption schemes have the same property: reformatting
results in a drive filled with random noise.
The latter is particularly interesting with LUKS, as you can do all kinds of
things like erase the drive with luksErase, while keeping a backup of the LUKS
header elsewhere.

@_date: 2018-01-18 11:36:44
@_author: Peter Todd 
@_subject: [bitcoin-dev] Upgrading PoW algorithm 
There's no reason to think Moore's law will last for 400 years; if it did
mining Bitcoin blocks would require astronomical energy levels. I haven't
actually done the math, but having to convert a mass-energy equivalance of a
planet or two per block is probably an accurate lower-bound even with quantum
computers. Once we're at that point, the problem is the speed of light: we'll
run out of energy in our 10 minute light radius, and thus need to get it from
farther away, at which point the 10 minute block interval forces a hard fork
anyway because mining no longer is in consensus.
tl;dr: This is a topic for sci-fi writers, not bitcoin-dev
Also:

@_date: 2018-01-22 15:00:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
It would definitely introduce DoS vectors by making it much cheaper to use
relay bandwidth. You'd also be able to push others' txs out of the mempool.
Most transactions don't have change?! Under what circumstance? For most
use-cases the reverse is true: almost all all transactions have change, because
it's rare for the inputs to exactly math the requested payment.

@_date: 2018-01-24 02:28:35
@_author: Peter Todd 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
Oh, Bitcoin Core doesn't already do that? I though that was what the (rather
complex) knapsack code was supposed to be doing.
In any case, you're assuming that there actually are a large number of outputs.
That's not likely to be the case in most "consumer-like" use-cases where the
number of deposits into the wallet is relatively low compared to the number of
withdrawls as coins are spent in smaller amounts; that's the pattern most of my
Bitcoin usage follows, particularly as I keep the amount of funds in my hot
wallets low.
Having said that, Rhavar's usage patterns could easily be different; I'd be
completely wrong in the case of a payment service for instance where a large
number of deposits are aggregated into a smaller number of payments; that
use-case happens to be a particularly interesting one for using tx replacement
to add outputs, so my criticism was definitely premature.

@_date: 2018-01-24 02:44:53
@_author: Peter Todd 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
In fact I considered only requiring an increase in fee rate, based on the
theory that if absolute fee went down, the transaction must be smaller and thus
miners could overall earn more from the additional transactions they could fit
into their block. But to do that properly requires considering whether or not
that's actually true in the particular state the mempool as a whole happens to
be in, so I ditched that idea early on for the much simpler criteria of both a
feerate and absolute fee increase.

@_date: 2018-01-30 02:22:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] NIST 8202 Blockchain Technology Overview 
That's absolutely right; this is why segwit is a blocksize increase first and
foremost rather than some kind of transaction size optimization.
It'd be good to get that corrected as well.

@_date: 2018-01-30 02:27:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] How accurate are the Bitcoin timestamps? 
That is incorrect. The OpenTimestamps servers are specifically designed not to
be trusted, and thus do not make any cryptographically verifiable attestations
as to when timestamps were created.
In the future I expect to add a trusted timestamping scheme via disposable keys
to the OpenTimestamps protocol, but that work isn't yet complete:

@_date: 2018-07-03 01:21:00
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP sighash_noinput 
The problem with that name is `SIGHASH_REUSE_VULNERABLE` tells you nothing
about what the flag actually does.
What name are we going to give a future flag that does something different, but
is also replay vulnerable?

@_date: 2018-07-09 05:41:39
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP sighash_noinput 
I think you have a good point there. But that's not the only way that reuse
could be a vulnerability: consider hash-based signatures.
I'm happy with adding a suffix or prefix to the term SIGHASH_NOINPUT, e.g.
SIGHASH_NOINPUT_UNSAFE to re-use Rust terminology.

@_date: 2018-06-05 20:43:26
@_author: Peter Todd 
@_subject: [bitcoin-dev] Disallow insecure use of SIGHASH_SINGLE 
SIGHASH_NONE is important as it's the only way that a multisig signers can
relinquish the need for them to sign without giving up the private key.
FWIW the SIGHASH_SINGLE bug can be used in similar ways too.

@_date: 2018-06-05 20:49:01
@_author: Peter Todd 
@_subject: [bitcoin-dev] Disallow insecure use of SIGHASH_SINGLE 
I don't see why we should bother to soft fork this out on the basis of
security, given that there are many other ways to insecurely use private keys
(e.g. reused nonces). Maybe soft-fork it out on the basis of code complexity,
but this sounds like a lot of work.
Also, I have to wonder if it's just as likely the devs might think the
non-standardness means it is secure.

@_date: 2018-06-07 13:13:11
@_author: Peter Todd 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
It's well known that the Bitcoin merkle tree algorithm fails to distinguish
between inner nodes and 64 byte transactions, as both txs and inner nodes are
hashed the same way. This potentially poses a problem for tx inclusion proofs,
as a miner could (with ~60 bits of brute forcing) create a transaction that
committed to a transaction that was not in fact in the blockchain.
Since odd-numbered inner/leaf nodes are concatenated with themselves and hashed
twice, the depth of all leaves (txs) in the tree is fixed.
It occured to me that if the depth of the merkle tree is known, this
vulnerability can be trivially avoided by simply comparing the length of the
merkle path to that known depth. For pruned nodes, if the depth is saved prior
to pruning the block contents itself, this would allow for completely safe
verification of tx inclusion proofs, without a soft-fork; storing this data in
the block header database would be a simple thing to do.
Lite client verification without a trusted source of known-valid headers is
dangerous anyway, so this protection makes for a fairly simple addition to any
lite client protocol.
# Brute Force Cost Assuming a Valid Tx
Consider the following 64 byte transaction:
    tx = CTransaction([CTxIn(COutPoint(b'\xaa'*32,0xbbbbbbbb),nSequence=0xcccccccc)],[CTxOut(2**44-1,CScript([b'\xdd\xdd\xdd']))],nLockTime=2**31-1)
If we serialize it, the last 32 bytes are:
    aaaaaaaaaa bbbbbbbb 00 cccccccc 01 ffffffffff0f0000 04 03dddddd ffffff7f
    ?prevhash? ? n    ?    ? seq  ?    ? nValue       ?    ? pubk ? ?lockt ?
                        ? sig_len   ?num_txouts         ?scriptPubKey_len
Of those fields, we have free choice of the following bits:
prevhash:  40 - prev tx fully brute-forcable, as tx can be created to match
prev_n:    16 - can create a tx with up to about 2^16 outputs
seq:       32 - fully brute-forcable in nVersion=1 txs
nValue:    44 - assuming attacker has access to 175,921 BTC, worth ~1.3 billion right now
pubk:      32 - fully brute-forcable if willing to lose BTC spent; all scriptPubKeys are valid
nLockTime: 31 - valid time-based nLockTime
Total: 195 bits free choice ? 61 bits need to be brute-forced
Additionally, this can be improved slightly by a few more bits by checking for
valid scriptSig/scriptPubKey combinations other than a zero-length scriptSig;
the attacker can also avoid creating an unspendable output this way, and
recover their funds by spending it in the same block with a third transaction.
An obvious implementation making use of this would be to check that the high
bits of prevout.n are zero first, prior to doing more costly checks.
Finally, if inflation is not controlled - and thus nValue can be set freely -
note how the brute force is trivial. There may very well exist crypto-currencies
for which this brute-force is much easier than it is on Bitcoin!

@_date: 2018-06-07 18:20:28
@_author: Peter Todd 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
^^^^^^^^^^^^^^^^^^^
Re-read my post: I specifically said you do not need a soft-fork to implement
this. In fact, I think you can argue that this is an accidental feature, not a
bug, as it further encourages the use of safe full verifiaction rather than
unsafe lite clients.

@_date: 2018-06-09 08:45:16
@_author: Peter Todd 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
That doesn't make any sense. Against a SPV wallet you don't need that attack;
with that kind of budget you can fool it by just creating a fake block at far
less cost, along with a sybil attack. Sybils aren't difficult to pull off when
you have the budget to be greating fake blocks.
That's technically incorrect: txouts can only be spent once, so you'll need to
do 2^40 work each time you want to repeat the attack to grind the matching part
of the prevout again.

@_date: 2018-06-09 08:50:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
It's been known so long ago that I incorrectly thought the attack was ok to
discuss in public; I had apparently incorrectly remembered a conversation I had
with Greg Maxwell over a year ago where I thought he said it was fine to
discuss because it was well known.
My apologies to anyone who thinks my post was jumping the gun by discussing
this in public; cats out of the bag now anyway.
My post is arguing that we *don't* need to fix the attack, because we can make
pruned nodes invulerable to it while retaining the ability to verify merkle
path tx inclusion proofs.
As for SPV, there is no attack to fix: they can be attacked at much lower cost
by simply generating fake blocks.

@_date: 2018-06-09 09:02:55
@_author: Peter Todd 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
There's hardly any cases where "thousands of confirmations" change anything.
Anyway, SPV is a discredited concept and we shouldn't be concerning ourselves
with it.
Indeed it does: between the number of txouts, scriptSig length, scriptPubKey
length, and the upper bits of nValue we have ~32 known bits that we can use to
distinguish between inner nodes and transactions. That's a false positive rate
of under one in a billion, so no issues there.

@_date: 2018-03-01 10:11:29
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
I mean, I think in general solving this problem is probably not possible.
Basically, the fundamental problem is someone else has consumed network
bandwidth that should be paid for with fees. What you're trying to do is
replace a transaction without paying those fees, which is identical to what an
attacker is trying to do, and thus any such scheme will be as vulnerable to
attack as not having that protection in the first place.
...which does give you an out: maybe the attack isn't important enough to
matter. :)

@_date: 2018-03-08 13:34:26
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
But that's not a good argument: whether or not normal users are trying to
attack each other has nothing to do with whether or not you're opening up an
attack by relaxing anti-DoS protections.
Equally, how often are normal users who aren't attacking each other creating
issues anyway? You can always have your wallet code just skip use of RBF
replacements in the event that someone does spend an unconfirmed output that
you sent them; how often does this actually happen in practice? Not many
wallets let you spend unconfirmed outputs that you didn't create.

@_date: 2018-03-09 13:28:03
@_author: Peter Todd 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Fair: you're not removing them entirely, but you are weakening them compared to
the status quo.
Pity, that does sound like a problem. :(
Miners and full nodes have slightly different priorities here; it's not clear
to me why it matters that they implement slightly different policies.
Still, re-reading your initital post, I'm convinced that the weakening of the
DoS protections is probably not a huge problem, so maybe lets try this in a
release and see what happens.
Notably, if people actually use this new replacement behavior, the institutions
doing these sweeps of unconfirmed outputs might stop doing that! That's
probably a good thing, as respends of potentially conflicted unconfirmed
outputs can be dangerous in reorgs; we're better off if outputs are buried
deeply before being spent again.

@_date: 2018-03-13 20:37:52
@_author: Peter Todd 
@_subject: [bitcoin-dev] Data structure for efficient proofs of 
CCing bitcoin-dev because this is of general interest...
For background, Daniel is asking about my client-side verified single-use-seals
via proof-of-publication model, previously published here?, which creates an
anti-double-spend primitive via a proof-of-publication, and many
tl;dr: A seal is closed by publishing a valid signature for the seal to a
ledger. The first signature is the valid one, so if Alice want to convince Bob
you she closed a seal correctly (e.g. to pay Bob), she has to supply Bob with a
proof that the signature _was_ published - a proof-of-publication - as well as
proof that prior to being published, no valid signature was previously
published - a proof-of-non-publication.
It's the proofs-of-non-publication that take up the most space.
So remember that the system I proposed? used sorted merkle trees only within a
block; for blocks themselves you mostly can't do any better than a linear list.
Though I think there may be some exceptions which deserves another email. :)
As you know, asymptotically merkle trees have excellent log2(n) proof size
growth. But as you correctly suggest, their high overhead in the small-n case
suggests that we can do better. In fact, Bram Cohen previously proposed? a "TXO
Bitfield" for the somewhat similar use-case of committing to the spentness of
outputs efficiently.
# Naive Analysis
So suppose at an intermediate node you commit to a simple bitfield where each
possible value under that node is represented by a single bit. Thus for m
values under that point in the tree, the marginal size of the non-inclusion
proof is m bits. By comparison a naive merkle tree built from a hash function
with k bits takes approximately k*log2(m) bits to prove non-inclusion. For an
rough, unsophisticated, analysis just solve:
    m = k * log2(m)
Apparently you can do this analytically, but as this analysis is only
approximate a much better idea is to just plot it on a graph: for k=256bits the
crossover point is roughly m=3000.
# Merkle Tree Structure
But what is the bitfield competing against exactly? Just saying "merkle tree"
isn't very precise. Most designs along these lines use something like a
merkelized patricia tree, where each bit of the key is compared individually
and each node is a two-way (left vs right) branch. Better yet is the radix
tree, where each inner node that has only one child is merged with its parent.
Regardless, the logic of these kinds of trees can be thought of a recursive
query, where each type of node has a `get(key)` operation that returns either a
value or None.
So let's define a new type of inner node that commits to a
merkle-mountain-range (MMR) tip and a 2^j element bitfield. `get(key)` is then
this pseudo-rust:
    fn get(self, prefix) -> Option {
        let idx = Int::from(prefix[0 .. j]);
        if self.bitfield[idx] {
            let mmr_idx = node.bitfield[0 .. idx].count_ones() - 1;
            Some(node.mmr[mmr_idx].get(prefix)
        } else {
            None
        }
    }
The hard part with this is choosing when to use a bitfield-based inner node
instead of a standard one. Assuming keys are randomly distributed, it makes
sense to do so when the bitfield table is partially empty, but how empty? It's
a trade-off between multiple parameters, including the size of
proofs-of-publication - although the latter may be OK to ignore as the number
of proof-of-non-publication needed should usually greatly outnumber
Question: is it OK for this choice to not be part of the deterministic
consensus? Is that even possible to enforce?
# Security
For a proof-of-publication to be secure, it must ensure that any attempt to
construct a false proof-of-non-publication will fail. In the pruning model,
that means that a proof-of-publication is simply the data necessary for the
proof-of-non-publication verifier to return false. Concretely:
    fn verify_pop(tree, key) -> bool {
        !verify_non_pop(tree, key)
    }
However note the subtle difference in trust model with respect to censorship
between the following two possible non-pop verifiers:
    fn verify_non_pop(tree, key) -> bool {
        !tree.exists(key)
    }
    fn verify_non_pop(tree, key) -> bool {
        match tree.get(key) {
            Some(value) => !verify_signature(value),
            None => true,
        }
    }
 False Positives
Note how if we use the second `verify_non_pop()` function shown above we can
also use probabilistic data structures such as bloom filters in place of a
bitfield. This works because a false-positive is acceptable, as it will still
fail signature verification (or sooner if the key is committed in the leaf
For example, it's plausible that a compressed bloom filter would be more space
efficient than a bitfield, as the multiple hashing steps might use the bits in
the filter more efficiently. Investigating this further would be a good
research topic.
# References
1) "[bitcoin-dev] Scalable Semi-Trustless Asset Transfer via Single-Use-Seals and Proof-of-Publication",
   Peter Todd, Dec 5th 2017, 2) "[bitcoin-dev] The TXO bitfield",
   Bram Cohen, Mar 31st 2017, 3) "Bloom filters",
    Wikipedia, Jan 27th 2018,

@_date: 2018-05-09 15:27:33
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
I don't think that will work, as a zero-fee tx won't get relayed even with
CPFP, due to the fact that we haven't yet implemented package-based tx

@_date: 2018-05-09 16:59:14
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
I don't see any reason why UTXO pollution would be a special concern so long as
those outputs are subject to the same dust rules as any other output is.

@_date: 2018-05-17 11:43:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
So I think we have two cases where wallets want to find txs spending from their
1) Waiting for a confirmation
2) Detecting theft
The former can be turned off once there are no expected unconfirmed
As for the latter, this is probably a valuable thing for wallets to do. Modulo
reorgs, reducing the frequency that you check for stolen funds doesn't decrease
total bandwidth cost - it's one filter match per block regardless - but perhaps
the real-world bandwidth cost can be reduced by, say, waiting for a wifi
connection rather than using cellular data.

@_date: 2018-05-20 23:56:58
@_author: Peter Todd 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
We've discussed this before: that rule prevents bandwidth usage DoS attacks on
the mempool; it's not a "heuristic". If you drop it, an attacker can repeatedly
broadcast and replace a series of transactions to use up tx relay bandwidth for
significantly lower cost than otherwise.
Though these days with relatively high minimum fees that may not matter.

@_date: 2019-03-31 21:11:12
@_author: Peter Todd 
@_subject: [bitcoin-dev] Softfork proposal for minimum price of $50k 
I believe I've found a serious vulnerability in your proposal: there's no limit
on the maximum supply of USD.

@_date: 2019-04-13 15:09:25
@_author: Peter Todd 
@_subject: [bitcoin-dev] assumeutxo and UTXO snapshots 
As hashest smaller than the target have no significance to the Bitcoin
consensus I'd suggest not basing any features on that property. It's just as
arbitrary as picking whole decimal number block heights, yet has the additional
downsides of being harder to compute, and being likely to confuse people as to
how the Bitcoin consensus works.

@_date: 2019-04-26 23:32:27
@_author: Peter Todd 
@_subject: [bitcoin-dev] Improving Pre and Post Merging Abilities With 
Speaking as maintainer of the Python library python-bitcoinlib, I don't think
using Python for security critical codebases is a good idea. It's just too easy
to make mistakes; I wouldn't recommend new projects use python-bitcoinlib.
Currently I'm doing 100% of my new projects on Rust. That's not to say Rust is
the only language usable for this kind of work. But it fits my style of
programming well and the type system - esp good handling of immutability -
appears to offer significant benefits.

@_date: 2019-08-12 10:40:23
@_author: Peter Todd 
@_subject: [bitcoin-dev] Single-use-Seal Implementation 
I'm not sure what you're getting at here; single-use-seals are really boring
and simple. To recap, they're akin to a pubkey that has the "magical" property
that it can only be signed once. This of course is impossible with math alone,
but can be implemented with beyond-math mechanisms like trust or PoW (physics).
Thus you have a globally unique seal, which can be closed over a message,
producing a witness attesting to the fact that the seal was closed over that
message. A single-use-seal protocol is secure if it is impossible (in your
chosen security model) to trick the validation function into thinking a single
seal was closed over two different messages.
The obvious implementation with Bitcoin is to define the seal to be a specified
txout, and the witness to be a transaction (and lite client proof) that spends
that txout in a transation with an OP_RETURN output committing to the hash of
the message as the first output. A fancier implementation could use a
pay-to-pubkey-style commitment (RGB? uses something along these lines).
For applications requiring a chain of single-use-seals, you can easily keep two
txouts for seals in your wallet, and alternate them as the chain is extended.
Do you mean to say there didn't previously exist a practical way to implement
them? Or that you've found another way? I'm curious what you mean here.
1)

@_date: 2019-08-12 11:01:10
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
So, I'll point out that I'd describe this a little bit differently:
    The vault is a tx setup scheme that binds coins in such a way that they can
    only be spent via a proof-of-publication *notification*, followed by a delay
    period, during which coins can be recovered/clawed back.
The key difference being it's not important that this be a *public*
notification: that the public can see just happens to be an (unfortunate)
implementation detail. For example, you could imagine a system where the
"prepare to spend" tx is indistinguishable from any other transaction.
It's important to note the reason this is possible is because any coin bound by
a convenant simply isn't a coin in the normal sense of the word, and is only
acceptable as payment directly if the receiver chooses to accept it.
To use an analogy many others have used, if you owe me $100, it's not
acceptable for you to pay me that $100 by dumping a time-locked safe on my
front lawn containing that $100 unless I've agreed to accept payment that way.
So to be clear, you're spending to a proof-of-burn _key_ because of the use of
adapter signatures for multisig? I'm not sure where the 0x00 is coming from
Obviously normally to provably destroy coins you'd spend to an OP_RETURN
output, or if miner censorship was an issue, a pay-to-script-hash of an
OP_RETURN  script.
I think this could use a bit more analysis here: why can't delete the *keys*
work, with each party deleting a separate private key that's used in an m-of-n
fashion? So long as at least n-m+1 parties actually deleted their keys IIUC it
should be secure.
Could you explain in more detail why you're deriving this from a blockhash?

@_date: 2019-08-13 10:15:32
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
But why does this specifically need to be entropy?
If I understand the scheme correctly, the important thing is for the ECDSA
private key to be unknown. Under the standard assumption that hash functions
are random oracles, hashing anything should be sufficient to create a pubkey
whose private key is unknown.
Secondly, there's probably better slightly privacy if a random nonce is chosen
(perhaps by concatenating a nonce from each party) rather than picking pubkeys
unique to this use-case.

@_date: 2019-08-16 12:06:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] Burying CSV and segwit soft fork activations 
I just wanted to elaborate on this excellent point:
This is debatable because Bitcoin is a decentralized, soft-forks are backwards
compatible, and it's very difficult if not impossible to measure the
preferences of economically significant nodes. Both the BIP9 version bits
signalling and the BIP 148 UASF had the same basic effect: enforce segwit.
Furthermore, the BIP 148 UASF rejected blocks that didn't signal via the BIP9
version bits.
We can observe the fact that 100% of known blocks produced after Aug 1st 2017
have complied with segwit rules, and the BIP9 signalling protocol for segwit.
But strictly speaking we don't really know why that happened. It's possible
that miners were running the BIP9 signalling Bitcoin Core release around that
time. It's also possible that miners were running UASF enforcing software.
It's possible there was a combination of both. Or even entirely different
software - remember that some miners produced segwit-valid blocks, but didn't
actually mine segwit transactions. Each scenario leads to the same externally
observable outcome.
Furthermore there's the question as to why miners were producing
segwit-compliant blocks: perhaps they thought the vast majority of economically
significant nodes would reject their blocks? Perhaps they just wanted to
enforce segwit?
These are all questions that have plausible answers, backed by evidence and
argument. But because Bitcoin is a decentralized network no authority can tell
you what the answers are.

@_date: 2019-01-25 00:16:30
@_author: Peter Todd 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Note that CSV using transactions are always RBF as CSV disables the opt-out.

@_date: 2019-07-22 04:32:15
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
Can you specify exactly which wallets those are?
Secondly, this doesn't stop people from running NODE_BLOOM nodes, and the DNS
seed infrastructure among others can easily direct wallets to those nodes. This
is of course not very secure, but bloom filters have never been very secure.

@_date: 2019-07-23 16:36:50
@_author: Peter Todd 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
With that patch people are still free to choose to provide bloom filtering
services by setting -peerbloomfilters=1
There really isn't due to sybil attacks; we already have good reason to believe
that the Bitcoin network is subject to them by deanonymization/chainanalysis
Indeed there's a good argument that creating services that are vulnerable to
sybil attacks encourages them by making them succesful at something. That
creates its own risks, for instance the risk that the sybil attacker will
themselves screw up and cause a bunch of nodes to go offline at once.

@_date: 2019-06-16 16:25:06
@_author: Peter Todd 
@_subject: [bitcoin-dev] testnet4 
Remember that the size of testnet itself is an important test; I've argued in
that past that we should consider making testnet *larger* than mainnet. There's
good arguments against that too, but I personally think the current size is a
reasonable compromise.
Of course, I personally tend to do all my testing on either internal regtest
nodes, or directly on mainnet. But the fact that works for me is specific to
the exact type of development I do and may not be applicable to you.

@_date: 2019-03-19 14:01:51
@_author: Peter Todd 
@_subject: [bitcoin-dev] Notice: List Infrastructure Migration 
Where will that git archive actually be created/maintained? As this will be a
Git repo, looks like getting it timestamped could be as simple as installing
OpenTimestamps appropriately on whatever server is actually maintaining it.
Equally, it'd be good to have the archive PGP signed.

@_date: 2019-10-04 07:15:36
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
One issue with this is the simplest implementation where the state is just raw
bytes would expose raw SHA256 midstates, allowing people to use them directly;
preventing that would require adding types to the stack. Specifically I could
write a script that rather than initializing the state correctly from the
official IV, instead takes an untrusted state as input.
SHA256 isn't designed to be used in situations where adversaries control the
initialization vector. I personally don't know one way or the other if anyone
has analyzed this in detail, but I'd be surprised if that's secure. I
considered adding midstate support to OpenTimestamps but decided against it for
exactly that reason.
I don't have the link handy but there's even an example of an experienced
cryptographer on this very list (bitcoin-dev) proposing a design that falls
victim to this attack. It's a subtle issue and we probably don't want to
encourage it.

@_date: 2019-10-05 11:49:02
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Obviously with care you can get the computation right. But at that point what's
the actual advantage over OP_CAT?
We're limited by the size of the script anyway; if the OP_CAT output size limit
is comparable to that for almost anything you could use SHA256STREAM on you
could just as easily use OP_CAT, followed by a single OP_SHA256.

@_date: 2019-10-06 05:12:21
@_author: Peter Todd 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
In even that degenerate case allocators also free memory.
Anyway, every execution step in script evaluation has a maximum output size,
and the number of steps is limited. At worst you can allocate the entire
possible stack up-front for relatively little cost (eg fitting in the MB or two
that is a common size for L2 cache).
256 bytes is more than enough for even the most complex summed merkle tree with
512-byte hashes and full-sized sum commitments. Yet that's still less than the
~500byte limit proposed elsewhere.
