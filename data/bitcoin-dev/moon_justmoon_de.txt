
@_date: 2011-08-05 00:10:59
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Double spend detection to speed up 
Since nobody else has mentioned it: There is another (more pragmatic?) way to detect double spends:
1. Connect to lots of clients
2a. If they all send you the same transaction -> double spend unlikely
2b. If some don't send you the transaction (or send a conflicting one) -> double spend in progress
Obviously not everyone will run a double spend detector - it's much more easily realized as a service (just like mining.) Jan put up a proof of concept: Would network support like a MSG_DOUBLESPEND be better? I used to think yes, but looking at the reality of Transaction Radar, I'm not so sure. Nothing stops such a service from scaling up and connecting to thousands of random nodes (especially when the network itself grows bigger), pushing the probabilities of missing a double spend "in the wild" to near zero. It could also connect directly to important miners/pools as others have suggested.
Of course this doesn't help against double spends where the attacker does his own mining*, but neither would MSG_DOUBLESPEND. Given the added network load I'd argue that network support for double spends is unnecessary and potentially damaging. DoS is more scary to me than non-instant transactions.
* In this case of course the hacker will be exposed to some randomness, and I doubt many attackers will buy 100 televisions, newspaper subscriptions or MP3s to get one for free. So this is only a problem for liquid goods with tiny spreads (any investment or stored value instrument.)

@_date: 2011-08-05 02:14:31
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Double spend detection to speed up 
To the transaction radar it doesn't much matter whether its connections are outgoing or incoming (assuming it can keep its nodes' IPs secret and it has reasonable uptime). So you could say that this is an argument *for* this kind of double spend protection if it means the creation of nodes/clusters accepting 10000+ incoming connections while making few outgoing connections. My point is, the amount of connections a node has has nothing to do with its effect on the in/out balance.
Some words on the shortage of listeners itself:
Could this be because the network right now consists largely of end users with residential type networks? With BitTorrent a lot of users go through the trouble of opening up ports in their router manually in order to get more peers and better download speeds - this is not (yet?) a widespread practice with Bitcoin. (I know Bitcoin has UPnP support, but I haven't found any numbers on how widely the IGD protocol is actually deployed. Wikipedia says that "some NAT routers" support it and that it's not an IETF standard. All routers I've actually seen in real life had it disabled by default.)
In the long term all the trends favor more clients allowing incoming connections: End users will tend to move towards lighter clients and the ones that stick with full nodes will tend to configure them better - meaning opening ports etc. - as documentation improves.
As for downright malicious nodes: It should be possible to come up with some sensible policies to temp ban nodes that don't relay any useful messages or try to flood you. This is an ongoing optimization problem in any peer-to-peer network and I expect us to make progress with this over I had only seen you mention a "miner backbone" which is sort of a more long-term vision, whereas Transaction Radar exists today. I didn't read everything though, so if you mentioned this idea specifically, please just consider my post as further support for your position.

@_date: 2011-12-08 23:43:24
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Lowering confirmation requirements and 
Hey Andy,
Bitcoin already does something which in practice has exactly this effect: If a transaction is reversed, any transactions based on its outputs are rejected.
Hosted wallets can make use of this - but as you correctly point out, depending on the service, it can get tricky. What if I exchange the money to USD and back before withdrawing? You could have an algorithm where MtGox prefers to spend outputs from your own deposits as the inputs for your withdrawals, it's not trivial though and never 100% secure.
I have trouble thinking of a good example where you need an explicit block dependency as you describe. The only times you'd want to use this dependency of transactions on specific previous transactions is when you can clearly and easily associate the money. But if you can clearly and easily associate the money, you might as well just relate the transactions (use the outputs from the deposit transaction as the inputs of the withdrawal transaction.)
This is btw something that would strongly agree with: Hosted wallets should absolutely keep each account as separate public keys. With that you lose free and instant internal transactions, but you gain instant deposits and much better risk isolation.
This is just my view. Thanks and keep the thought-provoking stuff coming!

@_date: 2011-12-13 03:39:42
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] [BIP 15] Aliases 
Grabbing is no more an issue than mining Bitcoins is an issue. Sure, domain grabbers will have the domains first, but they want to profit and therefore are willing to sell them for whatever price they can get. Just like the trading of any other limited resource, this process sounds like somebody is getting rich for nothing, but it does tend to put the limited resources to good use as people who waste good domains can't afford them in the long run. The problem with Firstbits is that the names already grabbed have fixed private keys that are known by their originators. That makes the names untradable. This may be fixable with split keys, but a lot of "good" 1firstbits are already made useless in this way.
Names in Namecoin can be transferred/traded securely, strong cryptography is built in and it shares mining without bloating the Bitcoin block chain. I see it as a decentralized DNS alternative at a time when domain seizures are on the rise, even absent any court order.
So I would use one of the DNS-based solutions that Amir suggested and simply require standard-compliant clients to be able to look up .bit (i.e. Namecoin) domains as well. That way we have a pragmatic solution, but one that also provides security and true decentralization for the more paranoid of our users.

@_date: 2011-12-18 22:19:06
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Protocol extensions 
Hey Chris,
This type of intermediate routing makes Tor slow . Bitcoin does not and imho should not make anonymity guarantees. Many users do not need them.
Let those who want anonymity connect through Tor, Freenet, etc. It's easy to add anonymity via an extra layer, but it is impossible to add performance on top of a slow system.
That's really the only thing I wanted to point out - if you do DHTs, focus on performance, not anonymity. :)

@_date: 2011-12-29 20:54:28
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Alternative to OP_EVAL 
How about releasing it on testnet first? If you want "less smart" people such as myself to help test, well I don't think I would get anywhere if I tried to abstractly reason about every possibility. Low-level testing is certainly important, but for me "thorough testing" requires an actual network of nodes (running different clients) and applications capable of creating and verifying real OP_EVAL transactions.
My suggestion would be: Deploy OP_EVAL on testnet quickly, let's build some real-life applications and if it works well, /then /let's pull the trigger for mainnet. If some issues or improvements arise, we'll have a chance to adjust it and try again.
I don't think this is too conservative or paranoid. I think this is a textbook use case for testnet.

@_date: 2011-07-08 08:36:23
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Version bytes 
Hey Pieter,
We could use the XOR hack for now and remove it the next time we reset testnet. But I do think the 111 is baggage we want to get rid of. Using the lsb as a simple flag is much cleaner.

@_date: 2011-10-27 19:18:02
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Bitcoin Wiki 
+1 for something based on git. Github has a dedicated wiki feature that is git-backed:

@_date: 2011-09-15 15:00:06
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Request review: drop misbehaving peers 
A few thoughts:
Should the DoS protection auto-disable if the node has less than a minimum number of connections? The idea being that if our node seems to be kicking /everybody /off the roster maybe there is something wrong with the protections.
It would be nice if the node sent a message to the banned peer with a code indicating the reason for the ban, specifically the offense that put Bitcoin over the edge. Logging the reason is probably fine for most cases, but I wanted to put the idea out there, because it might make debugging easier if there are some weird bans happening in the wild and we can't figure out why.
Should sending lots of messages that don't pass the protocol-level checksum test be a bannable offense? Or generally sending garbage data? The attacks I'm thinking of are cross-protocol attacks. So as rough example: The attacker puts an iframe on a website with a url like  so lots of people's browsers connect to it. Maybe he could even use something like [magic-bytes]tx\0[...][valid orphan transaction] in the URL, so the browser would send GET /[magic-bytes] etc. and the Bitcoin node would interpret it.
Strongly disagree. What is a non-standard transaction today may /be /a standard transaction tomorrow.

@_date: 2012-08-13 09:41:50
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] BIP: Custom Services 
Hey everyone,
I was working on some custom protocol extensions for Bitcoin that I
wanted to experiment with and I noticed that in order to enable nodes to
announce these services the only mechanism the protocol currently
provides is to use one of the 64 bits of the services field. This is
obviously a resource that will run out quickly if we all just help
ourselves, so I set out to come up with a standardized way to announce
custom protocol extensions, without using up NODE_* flags.
Please kindly review my solution:
Thanks to Amir Taaki, Mike Hearn and Pieter Wuille who provided
invaluable feedback while writing the draft.
Note: Earlier drafts of this BIP contained a description of a mechanism
for peer exchange for these custom services. However, since that part of
the BIP was (1) just a recommendation and (2) rather complex, Amir and I
agreed to split it off into a separate BIP [1] that will be refined some
more and submitted later.

@_date: 2012-08-13 22:00:36
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] BIP: Custom Services 
I'm implementing a DHT, rather than storing the whole blockchain index
locally, a future version of BitcoinJS will store only a user defined
percentage (anywhere from 0-100%.) Any misses will be resolved by
querying the network.
Thanks to ultraprune, we no longer need a full index for verification.
And for all the other use cases like lightweight server queries and
block explorer queries a bit of latency is acceptable in exchange for
scalability and decentralization. This feature will give people to
option to run anywhere from a large BitcoinJS node (100% index) to a
small one (20% index) to a lightweight one (0% index.) All of them are
equally queryable, so if you're just trying out BitcoinJS you won't have
to download the block chain just to run the block explorer example. Only
when your block explorer's traffic grows will you need to contribute
some query services back to the network in order not to get rate-limited.
Pieter brought up this very point when he reviewed an earlier draft.
This prompted the creation of the second BIP I mentioned:
The basic principle is quite simple - prefix the standard addr message
with a service-specific message to mark off service support. It's easy
to implement and very efficient (without compression it's 1.288 bits per
node, with compression it's quite possibly more efficient than the
services field.) Because this stuff is a bit more complex and because it
requires no changes to the Bitcoin protocol, Amir and I chose to split
it off into a separate BIP and I want to work on it a little more. But
if you are wondering how peer exchange might work for these custom
services, please do take a look at it.
When I designed the DHT, I did just that. Later I was working on a
concept for a decentralized pool and I noticed I was solving the same
problems again. And with all three services running a node might be
maintaining three separate TCP connections to the exact same peer.
So then I considered making the DHT network extensible, so that the
decentralized pool protocol could live in there. But, well if I'm doing
that, why not just make the regular Bitcoin protocol extensible and let
both extensions live in there.
For a custom service you DO need the following:
- service-specific DNS seeds
- service announcement (BIP: CS)
- service-specific messages (BIP: CS)
- service-specific peer exchange (BIP: CSD)
And those four things cover a lot of what Bitcoin does. But once you're
thinking about n custom services it starts to look easier to add
semantics for "some nodes support some things" in one network rather
than instantiating n networks.
On the opposite end of the spectrum there are very simple services.
Consider a WebSocket transport. Some nodes might offer
Bitcoin-over-WebSocket, for example to implement an SPV client in the
browser. But they don't connect to each other via WebSocket, since they
prefer plain TCP. So in this case you need peer exchange, but not much
else. You could create a new P2P network for the sole purpose of
exchanging peers, but again it seems much easier if there were ways to
do this on the Bitcoin network.
One final point: A major focus of this BIP is to make it easy to
canonicalize custom services if we choose to do so. The idea is that
custom services get to prove themselves in the wild - those that work
well may be added to the standard protocol. That's a good reason to 1)
encourage custom services to live in-band and 2) recommend compatibility
with Bitcoin's standard mechanisms (12 byte command names, 1 bit service
announcement, 1 bit peer exchange etc.)

@_date: 2012-08-16 21:21:14
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] BIP 35: add mempool message 
BitcoinJS will implement it, it's a useful feature and there is no
reason not to support it.
Two comments from my end:
- This is just a thought, but I wouldn't mind using a new inv_type for
this, e.g. MSG_MEMTX. I could conceivably see a future where broadcast
and relay txs are stored in a very fast local cache whereas the general
mempool is stored in a slower data structure. By being able to
distinguish incoming getdata requests I can save a few milliseconds by
querying the right storage right away. Might also help with things like
telling apart broadcast/relayed transactions from the response to a
mempool request for purposes like DoS scoring etc.
Not a big deal by any means, but I also don't see a downside to it.
inv_types are not a scarce resource, we have four billion of them available.
For now clients would just treat MSG_TX and MSG_MEMTX interchangeably.
- If a node doesn't have anything in it's mempool it sends back an empty
inv message. This is either ambiguous (if other things also send empty
inv messages in the future) or arbitrary (why should an empty inv be
associated with a mempool request of all things.) Instead why not
respond with an inv message that contains a single element of type
MSG_MEMTX and hash 0. That would a very direct way to indicate that this
response is associated with a mempool request.
I'm not married to either suggestion, just trying to add my perspective.
One thing you notice when reimplementing Bitcoin is that Bitcoin's
protocol leaves out a lot of information not for space reasons, but
because the reference client's implementation doesn't happen to need it.
Sometimes however this locks other clients into doing things the same
way. If we can make the protocol a bit richer, especially if this
doesn't cost any extra bytes, then we should consider it as it might
help some implementation down the road make a neat optimization.

@_date: 2012-02-29 22:00:44
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
Looks good to me. I also second the notion that we should deploy this quickly, given that it's a bug fix.

@_date: 2012-01-02 16:14:32
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Alternative to OP_EVAL 
The OP_EVAL discussion went into some private discussion for a bit, so here is a summary of what we talked about.
Roconnor pointed out that the currently proposed OP_EVAL removes the ability to statically reason about scripts. Justmoon pointed out that this is evidenced by the changes to GetSigOpCount:
Currently, the client first counts the number of sigops and if it is over a certain limit, it doesn't execute the script at all. This is no longer possible with OP_EVAL, since OP_EVAL can stand for any number of other operations, which might be part of some piece of data. The script that is executed by OP_EVAL can be changed (polymorphic code). Gavin's patch deals with this, by counting the sigops at runtime and aborting only after the limit has been reached.
Here is an example for a script that based on naive counting contains no sigops, but in fact contains 20:
[20 signatures] 20 [pubkey] OP_DUP OP_DUP OP_2DUP OP_3DUP OP_3DUP     OP_3DUP OP_3DUP OP_3DUP 20 "58959998C76C231F" OP_RIPEMD160 OP_EVAL
RIPEMD160( 58 95 99 98 C7 6C 23 1F )
hashes to
which OP_EVAL interprets as
OP_CHECKMULTISIG "400B7DF3A56FE2B32B9906BCF1B1AFE9" OP_DROP
The nonce 58959998C76C231F was generated using this code: Gavin and Amir argued that it is possible to "dry run" the script, avoiding the expensive OP_CHECKSIG operation and running only the other very cheap operations. However, sipa pointed out that in the presence of an OP_CHECKSIG a dry runner cannot predict the outcome of conditional branches, so it has to either do the OP_CHECKSIG (and become just a regular execution) or it has to follow both branches. Roconnor and justmoon suggested the following script to illustrate this point:
[sig] [pubkey]
[some data]
[sig] [pubkey] OP_CHECKSIG OP_IF OP_HASH160 OP_ELSE OP_HASH256 OP_ENDIF
(previous line repeated 33 times with different sigs/pubkeys)
This script is valid assuming that the resulting hash from the branch that is chosen based on what signatures are valid contains an OP_CHECKSIG. (And the initial [sig] and [pubkey] are valid.) But a dry runner trying to count how many OP_CHECKSIGs this script contains would run into the first OP_CHECKSIG OP_IF and have to run both branches. In both branches it would again encounter a OP_CHECKSIG OP_IF and run all four branches, etc. In total it has to run (2^33 - 2) * 1.5 SHA256 operations (8 GHash) and 2^32 - 1 RIPEMD160 operations. Therefore we now believe a dry runner is not possible or at least too complicated to be involved in protocol rules such as the sigops limit.
As a result people are now on a spectrum from those who feel strongly that static analysis is an important property and not something to give up easily all the way to those who think it's superfluous and the other side is just unnecessarily delaying OP_EVAL deployment.
One thing I want to note is that static analysis is a property for which there is a better argument than for other, weaker properties, such as limited recursion depth. Bitcoin currently allows you to:
* Tell if a script contains a specific opcode or not
* Count how many times a script will execute an operation at most
* Count how many total operations a script will execute at most
* Count how many signatures a script will execute at most
* Find the maximum length of a datum pushed onto the stack
* Find the maximum number of items that can be pushed onto the stack
* Find the maximum size (in bytes) of the stack
* Calculate how long a script will run at most
OP_EVAL as proposed makes these upper bounds almost meaningless as it can contain, indirectly, up to 32 instances of any other opcode. (About 3-6 instances are currently practical.) The only way to answer these questions would then be to fully execute the script.
Suppose we want to one day allow arbitrary scripts as IsStandard, but put constraints on them, such as enforcing a subset of allowed opcodes. (See list above for other possible restrictions.) If we want to include OP_EVAL in the set of allowed opcodes, it's important that OP_EVAL is implemented in a way that allows static analysis, because we can then allow it while still maintaining other restrictions.
If proponents of the current implementation want to argue that we don't need static analysis now, the burden is on them to show how we could retrofit it when/if we get to this point or why they think we will never want to allow some freedom in IsStandard that includes OP_EVAL.
There are several proposals for OP_EVAL that allow static analysis:
* Using a fixed position reference prefix (sipa)
* Using an execute bit on data set by an opcode (justmoon)
* Using OP_CODEHASH (roconnor)
* Using OP_CHECKEDEVAL (sipa)
* Using OP_HASH160 OP_EQUALVERIFY as a special sigPubKey (gavinandresen)
Let's fully develop these proposals and see how much of a hassle it would actually be to get a statically verifiable OP_EVAL. I think that's a prerequisite for having the argument on whether it is *worth* the hassle.
(Update: Gavin's latest proposal looks *very* good, so that may settle the debate quickly.)

@_date: 2012-01-02 18:10:25
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Alternative to OP_EVAL 
+1. I love this proposal.
It's two less bytes than OP_EVAL even.
It allows static analysis.
It doesn't require any change to the script interpreter. (You can do a static replacement step between parsing and execution.)
It allows all urgent use cases.
It doesn't consume a NOP. If we ever want recursion or something else, we can still add OP_EVAL,... then.
I disagree. If people actually do mean HASH160  EQUAL, let *them* add a NOP. Or better to avoid NOP let them use HASH160  EQUALVERIFY 1. Point is, if you don't want code replacement you can easily break the pattern. But code replacement will be overwhelmingly more common, so it should be as small as possible. Every byte matters.

@_date: 2012-01-17 10:25:27
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] bitcoin.org SOPA/PIPA blackout 
I'm firmly against a *blackout* - it would harm users' trust in Bitcoin since people looking to download the client or to get information about Bitcoin may end up in the wrong place. I constantly have to delete YouTube spam advertising this or that "miracle GPU miner" or "secure Bitcoin client", which of course are all just the same trojan.
As for making a statement or putting up a banner - that's ok with me.

@_date: 2012-07-09 18:39:24
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Random order for clients page 
I am and I'm going to have to agree with Greg. Information about clients
is bound to be transient and controversial.
My relatively naive suggestion would be to move it to the Wiki. If it
can handle the controversies involved with the Trade page, it should
easily be able to handle the controversies involved with a Clients page
like this one. A link to that page could be added under Bitcoin Wiki on
On the subject of randomization, I think that's a bad idea. Randomness
does not equal fairness and more importantly it does not serve the
users, which should be the overriding concern. As a user I don't want to
be recommended a random client but the most sensible choice. As
alternative client implementors we should not be overly concerned about
Bitcoin.org recommending the wrong client, truly good clients will
benefit from word-of-mouth and eventually rise to the top. If you want a
"fair" ordering, then I'd order by number of downloads for downloadable
clients and Alexa rank for any hosted / online services if it were
decided that such should be listed at all.

@_date: 2012-07-10 00:26:06
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Random order for clients page 
I strongly agree, but this is *why* I suggested moving it to the wiki. I
recently had to choose an XMPP client and I looked on xmpp.org - after a
frustrating experience with their listing [1], I went to Wikipedia who
have an decent feature-based matrix [2].
(There may be better examples, but I'm using this one, because this
actually did happen.)
This is just anecdotal, but there are some reasons why wikis tend to do
a better for this kind of thing is because they are:
- more up-to-date (anyone can update them)
- more in touch with users:
  -> Users can edit the page and add a column to a feature matrix for
  -> The editing discussions include users. I guarantee there are more
Bitcoin end users with a wiki account than a Github account.
-  immediately recognizable as a wiki (thanks to Mediawiki/Wikipedia.)
As such many users will correctly treat and interpret the information
presented as community-generated and fallible.
So they are more user-oriented in the sense that they will be influenced
by a diverse set of backgrounds and views vs. a Github based page which
will be dominated by developers. If you want to see "the result of
internal politics", the current client page is a good example. We
couldn't agree on the columns for a feature matrix, so now we just have
walls of text. Some of the options that are de-facto the most popular
with users like BlockChain.info or just using your MtGox account are not
mentioned at all. When analyzing client security, Greg discussed
counterparty risks but ignored other risk factors like default backup
behavior and the usability of security features.
But even if I grant you that those clients' overall risk profile is
worse than Bitcoin-Qt's, maybe I'm happy to take that risk in exchange
for less setup/maintenance effort. Based on our support requests at
WeUseCoins I know that there are tons of users with < 1 BTC in their
wallets. If my hourly wage is 20$ and I have 20$ in my Bitcoin wallet
then spending one hour per month downloading/updating/figuring-out the
client is equivalent to a total loss.
The list is obviously designed by open-source developers and that's
fine, it's bitcoin.org, arguably we *should* try to push users in a
specific direction, arguably we *should* err on the side of caution in
order to not be caught recommending a hosted wallet that gets hacked.
But if user orientation is supposed to be the focus, then the wiki will
both allow us (because it's less "official") and force us (because users
will have a say) to include even clients we personally wouldn't use. :)
[1]

@_date: 2012-07-10 04:36:39
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Random order for clients page 
The point is to expand the circle of contributors. I'm pretty sure there
are more people who can edit a wiki than people who know HTML and how to
create a git pull request. :)
My apologies, I vaguely remembered Luke's original proposal and that it
got rejected, but you're correct, the reason wasn't a debate on the
columns but that people didn't like the feature matrix at all.
I didn't really mean to argue on the details of what the page should
look like, but just to briefly respond to Mike's point:
A well-designed feature matrix can quite useful and user-friendly.
Prose is better to get a sense of the philosophy and basic idea of a
client. If it was between having only a feature matrix or only prose,
I'd probably go for the prose as well.
What a feature matrix is good at though is it allows you to very quickly
find the specific feature or general criteria you're looking for without
reading through all of the text. So it might be a useful addition maybe
not on Bitcoin.org, but certainly on the wiki.

@_date: 2012-07-10 11:11:59
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Random order for clients page 
True, it's those compromises that people should base their decision on.
To make that easier was the motivation for me to suggest feature
matrices in the first place.
Right now if I read Electrum's description, it doesn't say anything
about the tradeoffs with a lightweight client like the slightly weaker
privacy guarantees. At best I could deduce that from the fact that
unlike Bitcoin-Qt it doesn't explicitly list privacy as an advantage.
So applying the same "MyBitcoin test" to the current Bitcoin Clients
page and if you want to be fair, we'd have to assume that if it was
indeed included it would also just be a short pitch listing only pros
and no cons. So it would say something like: "MyBitcoin starts instantly
and is really easy to use and great for beginners." etc.
Obviously if you compare a bad matrix to good short descriptions and
vice versa you'll get the conclusion you're trying to get.
I think Alan had the best idea - let's have the Clients page as it is
and have it link to the wiki for those who want a more detailed
comparison. On the wiki page we can then have explanations of the basic
client types, separate matrices for features and for security/privacy
and whatever else might be useful to know when choosing a client. Then
users who don't really care aren't bothered by "too much information"
and users who do can easily click through and find out about the
different tradeoffs.

@_date: 2012-07-29 18:33:25
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] script tests - invalid script in 
OP_WITHIN is lower-bound-inclusive, but upper bound exclusive, so 1 0 1 WITHIN is false.
bool fValue = (bn2 <= bn1 && bn1 < bn3);

@_date: 2012-07-29 19:35:12
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] script tests - invalid script in 
I was baffled by this exact script, too. :)
Big props to Gavin for adding those data-driven test cases. I can't
overstate how useful they are.
Is there interest to port more tests (P2SH, checksig, checkmultisig,
block verification, maybe even DoS rules) into data-driven format? It
might be something that I'd like to help with if pull requests in that
direction are welcome.

@_date: 2012-06-15 18:56:38
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Near-term scalability 
Thanks Mike for the writeup - I'm very sad to have missed the discussion
on IRC since fee economics are probably my favorite topic, but I'll try
to contribute to the email discussion instead.
Fees are a product of both real and artificial limits to transaction
The artificial limits like the block size limit are essentially putting
a floor on prices by limiting supply beyond what it would otherwise be.
E.g. the network could confirm more transactions theoretically, but the
block size limit prevents it.
The real limits are the bandwidth, computing and memory resources of
participating nodes. For the sake of argument suppose a 1 TB block was
released into the network right now and we'll also assume there was no
block size limit of any kind. Many nodes would likely not be able to
successfully download this block in under 10-30 minutes, so there is a
very good chance that other miners will have generated two blocks before
this block makes its way to them.
What does this mean? The miner generating a 1 TB block knows this would
happen. So in terms of economic self interest he will generate the
largest possible block that he is still confident that other miners will
accept and process. A miner who receives a block will also consider
whether to build on it based on whether they think other miners will be
able to download it. In other words, if I receive a large block I may
decide not to mine on it, because I believe that the majority of mining
power will not mine on it - because it is either too large for them to
download or because their rules against large blocks reject it.
It's important to understand that in practice economic actors tend to
plan ahead. In other words, if there is no block size limit that doesn't
mean that there will be constant forks and total chaos. Rather, no miner
will ever want to have a block rejected due to size, there is plenty of
incentive to be conservative with your limits. Even if there are forks,
this simply means that miners have decided that they can make more money
by including more transactions at the cost of the occasional dud.
Therefore, from an economic perspective, we do not need a global block
size limit of any kind. As "guardians of the network" the only thing we
need to do is to let miners figure out what they wanna do.
HOWEVER, the existing economic incentives won't manifest unless somebody
translates them into code. We have to give our users (miners & endusers)
the tools to create a genuine fee-based verification market.
On the miner side: I would make the block size limit configurable with a
relatively high default. If the default is too low few people will
bother changing it, which means that it is not worth changing (because a
majority uses the default anyway), which means even fewer people will
change it and so on.
The block size limit should also be a soft rather than a hard limit -
here are some ideas for this:
- The default limit for accepting blocks from others should always be
significantly greater than the default limit for blocks that the client
itself will generate.
- There should be different size limits for side chains that are longer
than the currently active chain. In other words, I might reject a block
for being slightly too large, but if everyone else accepts it I should
eventually accept it too, and my client should also consider
automatically raising my size limit if this happens a lot.
The rationale for the soft limit is to allow for gradual upward
adjustment. It needs to be risky for individual miners to raise the size
of their blocks to new heights, but ideally there won't be one solid
wall for them to run into.
On the user side: I would display the fee on the Send Coins dialog and
allow users to choose a different fee per transaction. We also talked
about adding some UI feedback where the client tries to estimate how
long a transaction will take to confirm given a certain fee, based on
recent information about what it observed from the network. If the fee
can be changed on the Send Coins tab, then this could be a red, yellow,
green visual indication whether the fee is sufficient, adequate or
dangerously low.
A criticism one might raise is: "The block size limit is not to protect
miners, but to protect end users who may have less resources than miners
and can't download gigantic block chains." - That's a viewpoint that is
certainly valid. I believe that we will be able to do a lot just with
efficiency improvements, pruning, compression and whatnot. But when it
comes down to it, I'd prefer a large network with cheap
microtransactions even if that means that consumer hardware can't
operate as a standalone validating node anymore. Headers-only mode is
already a much-requested feature anyway and there are many ways of
improving the security of various header-only or lightweight protocols.
(I just saw Greg's message advocating the opposite viewpoint, I'll
respond to that as soon as I can.)
+1 Very good change. This would allow miners to maximize their revenue
and in doing so better represent the existing priorities that users
express through fees.
Discouraging address reuse will not change the amount of transactions, I
think we all agree on that. As for whether it improves the
prioritization, I'm not sure. Use cases that we seek to discourage may
simply switch to random addresses and I don't agree in and of itself
this is a benefit (see item 4 below). Here are a few reasons one might
be against this proposal:
1) Certain use cases like green addresses will be forced to become more
complicated than they would otherwise need to be.
2) It will be harder to read information straight out of the block
chain, for example right now we can pretty easily see how much volume is
caused by Satoshi Dice, perhaps allowing us to make better decisions.
3) The address index that is used by block explorers and lightweight
client servers will grow unnecessarily (an address -> tx index will be
larger if the number of unique addresses increases given the same number
of txs), so for people like myself who work on that type of software
you're actually making our scalability equation slightly worse.
4) You're forcing people into privacy best practices which you think are
good, but others may not subscribe to. For example I have absolutely
zero interest in privacy, anyone who cares that I buy Bitcoins with my
salary and spend them on paragliding is welcome to know about it.
Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other
people want to use mixing services and randomize their addresses and
communicate through Tor that's fine, but the client shouldn't force me
to do those things if I don't want to by "deprioritizing" my transactions.
5) We may not like firstbits, but the fact remains that for now they are
extremely popular, because they improve the user experience where we
failed to do so. If you deprioritize transactions to reused addresses
you'll for example deprioritize all/most of Girls Gone Bitcoin, which
(again, like it or not) is one of the few practical, sustainable niches
that Bitcoin has managed to carve out for itself so far.
Free is just an extreme form of cheap, so if we can make transactions
very cheap (through efficiency and very large blocks) then it will be
easier for charitable miners to include free transactions. In practice,
my prediction is that free transactions on the open network will simply
not be possible in the long run. Dirty hacks aside there is simply no
way of distinguishing a spam transaction from a charity-worthy
transaction. So the way I envision free transactions in the future is
that there may be miners in partnership with wallet providers like
BlockChain.info that let you submit feeless transactions straight to
them based on maybe a captcha or some ads. (For the purist, the captcha
challenge and response could be communicated across the bitcoin network,
but I think we agree that such things should ideally take place
That way, the available charity of miners who wish to include feeless
transactions would go to human users as opposed to the potentially
infinite demand of auto-generated feeless transactions.

@_date: 2012-06-15 19:52:10
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] SatoshiDice and Near-term scalability 
I do agree that changing/lifting the block size limit is a hard fork
measure, but Mike raised the point and I do believe that whatever we
decide to do now will be informed by our long term plan as well. So I
think it is relevant to the discussion.
According to BlockChain.info we seem to have lots of small blocks of
0-50KB and some larger 200-300 KB blocks. So in terms of near term
measure one thing I would like to know is why miners (i.e. no miners at
all) are fully exhausting the available block size despite thousands of
transactions in the memory pool. I'm not too familiar with the default
inclusion rules, so that would certainly be interesting to understand.
There are probably some low hanging fruit here.
The fact that SatoshiDice is able to afford to pay 0.0005 BTC fees and
fill up the memory pool means that either users who care about speedy
confirmation have to pay higher fees, the average actual block size has
to go up or prioritization has to get smarter. If load increases more
then we need more of any of these three tendencies as well. (Note that
the last one is only a very limited fix, because as the high priority
transactions get confirmed faster, the low priority ones take even longer.)

@_date: 2012-06-19 21:22:15
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] LevelDB benchmarking 
Here are my 2 cents after using LevelDB as the default backend for
BitcoinJS for about a year.
LevelDB was written to power IndexedDB in Chrome which is a JavaScript
API. That means that LevelDB doesn't really give you a lot of options,
because they assume that on the C++ layer you don't know any more than
they do, because the actual application is on the JavaScript layer. For
example whereas BDB supports hashtables, b-trees, queues, etc., LevelDB
uses one database type, LSM trees which is an ordered data structure
that is pretty good at everything.
Another gotcha was the number of file descriptors, LevelDB defaults to
1000 per DB. We originally used multiple DBs, one for each of the
indices, but it was easy enough to combine everything into one table,
thereby solving the fd issue. (Lowering the file descriptor limit also
works of course, but if you lower it too much, LevelDB will start to
spend a lot of time opening and closing files, so I believe combining
your tables into one is the better option.)
Overall, LevelDB is a fantastic solution for desktop software that is
faced with multiple use cases that aren't known at compile time. It
isn't really designed for something like Bitcoin which doesn't need
ordered access, has relatively predictable characteristics and - at
least some of the time - runs on servers.
That said, it does seem to work well for the Bitcoin use case anyway.
Thanks to the LSM trees, It's very quick at doing bulk inserts and we
don't seem to need any of the bells and whistles that BDB offers. So I
can't think of a reason not to switch, just make sure you all understand
the deal, LevelDB unlike Tokyo/Kyoto Cabinet is *not* intended as a
competitor or replacement for BDB, it's something quite different.

@_date: 2012-06-22 08:39:24
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Berlin Bitcoin Hackathon 
Flights booked. Mike Hearn and I will be there. :)

@_date: 2012-06-24 18:51:26
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Enforcing inflation rules for SPV clients 
Very interesting for you to bring this up. I had a similar idea for a
totally different use case. Greg recently pointed out an interesting
dilemma saying that (significantly) larger blocks would lead to
centralization. So I've been working on a design for a decentralized
pool that can handle gigabyte sized blocks by splitting up the work
among its members.
At the moment P2Pool nodes all verify all transactions in all blocks.
But it seems feasible to create a system where miners who have over the
last 10000 blocks contributed to the pool's proof-of-work are allocated
a proportional piece of verification work with redundancy and
deterministic randomness that makes manipulation of the allocation
extremely difficult. Such a pool would be very unlikely to accept an
invalid block or transaction in practice.
However, with these block sizes obviously non miners are going to have
to be SPV, so even just a 0.0001% chance of an invalid block being
accepted has profound implications for the network. If a decentralized
pool like that had more than 50% of the hashing power and it accepted a
single invalid block, that tainted chain would be forever regarded as
valid by SPV clients. There needs to be some way to recover once an
invalid block has been accidentally accepted by an imperfect miner.
Based on that I also started to think about proofs of invalidity that
would circulate. Basically you would add a new network message that
would contain the proof that a specific signature and therefore the
whole block is invalid.
As long as the block's proof-of-work is valid and the block's parent is
one of the last n = 50000 blocks, the message is relayed (subject to a
cooldown, warnings would be less frequently relayed the older the
offending block is.)
The mechanism works in exactly the way Mike mentions: It allows even SPV
clients to punish any miner who is dishonest or negligent with their
verification work. That gives miners a good reason not to be dishonest
or negligent in the first place.
Processing more transactions means that hashing is a smaller part of the
overall cost for miners. For example, paying for 50 BTC worth of hashing
per block costs 0.05 BTC per tx at 1000 tx/block, but only 0.0005 BTC at
100000 tx/block.
Number of transactions is a lever that lets us have lower fees and more
network security at the same time. Like Greg correctly pointed out, this
is not worth having if we have to sacrifice decentralization. But if we
don't, it becomes a no-brainer.
My IMTUO proposal [1] showed a way where miners don't need a copy of the
set of unspent outputs at all. This means the minimum storage
requirements per node no longer grow with the number of transactions.
However, the price for this was about five times greater bandwidth usage
per verified transaction. Since every miner still had to verify every
transaction it looked like bandwidth would become an even bigger problem
with IMTUO than storage would have been without. However, if a small
miner can do less than 100% verifications and still contribute, suddenly
IMTUO may become viable. That would accomplish the holy grail of Bitcoin
scalability where the network successfully runs on trust-atomic entities
all of which can choose to store only a small fraction of the block
chain, verify a small fraction of transactions and perform a small
fraction of the hashing.)
[1]

@_date: 2012-03-03 14:44:45
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] JSON-RPC is BIP territory or not? 
Since several independent clients (I know at least libcoin and BitcoinJS ) aim to implement JSON-RPC APIs which are either a superset of the original client's or have at least some compatible functions, I think you can make a case for including JSON-RPC API calls within the domain of BIPs.
In this instance the BIP aims to create a common protocol between different clients, miners, mining proxies and pools. That's a lot of software, so standardization definitely seems like a good idea and I can't think of a reason not to use the BIP process.
I have some comments on the content of the BIP, but since this thread is more of a meta-discussion I'll wait until the BIP is officially proposed.

@_date: 2012-03-03 15:23:08
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] getmemorypool BIP process 
From what I understand the BIP uses a polling model, e.g. a miner would use getmemorypool to request new work from a pool in intervals. Would it make sense to specify a version of the API supporting long polling?
For BitcoinJS specifically, since we also support JSON-RPC over TCP, I'd even be interested in a genuine "push" API. Something like C->S "listenmemorypool", S->C "updatememorypool" (continually).

@_date: 2012-03-04 01:18:09
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] getmemorypool BIP process 
As for an explicitly standard way, there is none. The JSON-RPC 1.0 spec  says "The specifications do not require a certain transport protocol. The use of TCP/IP socket streams is encouraged. The serialized request and response objects are sent to the peers through the byte streams. " The JSON-RPC 2.0 spec  goes out of its way to say "It is transport agnostic in that the concepts can be used within the same process, over sockets, over http, or in many various message passing The de-facto standard for bidirectional JSON-RPC is plain TCP sockets. BitcoinJS currently implements this - we detect whether an incoming connection is HTTP or raw JSON-RPC based on the first character. (HTTP must start with an uppercase letter, raw JSON-RPC must start with an opening curly bracket.)
There are two things to watch out for with JSON-RPC over plain TCP:
1. Plain TCP sockets (unlike HTTP) have no standardized authentication mechanism, so I added an extra RPC call auth("username", "password").
2. The TCP packets may or may not correspond to JSON-RPC messages. You can either use a streaming JSON parser (yajl in ANSI C, Jackson in Java, etc.), or you can just count (non-string-literal) curly braces to detect when a complete message has arrived.
Many JSON-RPC libraries come with TCP socket support out of the box: We're planning to add more features to our JSON-RPC API in the future, such as:
- JSON-RPC over TLS sockets
- Challenge-response authentication
- TLS client handshake (certificate authentication)
As for HTTP Keep-Alive: It works, but I don't think it's very widely supported among client libraries and HTTP isn't really made for this type of thing, so my gut instinct would be to avoid it. That said, it doesn't hurt to offer the option.

@_date: 2012-05-26 13:52:33
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Punishing empty blocks? 
Zooko is spot on - slower confirmations will give people a reason to set
higher fees. As soon as fees reach a level where they matter, even
botnet operators will be looking into ways of including transactions for
some extra profit.
In the meantime slightly slower confirmations aren't a problem. Consider
that even if it takes four blocks to get your transaction included
instead of one, once it is included, you still benefit from every new
block in terms of security. So if you're looking for six confirmations
for example, even a three block delay will only be a 50% delay for you.
And of course there are techniques for instant transactions which
continue to be refined and improved.
As for the proposed solutions: Punishing 1-tx blocks is complete and
utter nonsense. It's trivial to include a bogus second transaction.
Any additional challenges towards miners like hashes of the previous
block are at best useless. If I was running a botnet, I'd just grab that
hash from a website (pretty good chance Blockchain.info will have it :P)
or mining pool or wherever and keep going undeterred. At worst they may
affect scalability one day. You might imagine a peer-to-peer network of
miners who for cost reasons don't download all blocks anymore, but
verify only a percentage of them at random. They might then exchange
messages about invalid blocks including a proof (invalid tx, merkle
branch) why the block is invalid. This is just one idea, the point is
that assumptions about what a legitimate miner looks like may not always
hold in the future.
Finally, there is an ethical aspect as well. If a miner wishes not to
include my transaction that is his choice. He has no more an obligation
to sell his service to me than I have to buy it from him. If I really,
really want him to include my transaction I will have to offer to pay more.
If we as developers think that confirmations are too slow or that more
blocks should include transactions, then the right measures would be:
- Educating users about the relationship between confirmation speed and fees
- Raising the default transaction fee
Every market has a supply curve, so it is economically to be expected
that there will be some miners who don't include transactions, simply
because they are at that end of the supply curve where it is not worth
it for them to sell their service. All markets must have a certain
tension - there must be miners who don't include transactions for there
to be users who want their transactions included more quickly. In other
words there must be somebody not confirming if confirmations are to have
value. If you interfere with that all you'll accomplish is keep
transaction fees below market level, which will make the transition from
inflation-financed hashing to transaction-financed hashing more painful
and disruptive.

@_date: 2012-10-21 20:48:15
@_author: Stefan Thomas 
@_subject: [Bitcoin-development] Public key and signature malleability 
Sounds good to me. I think it's important to give people a chance to fix
their software, but Pieter's proposal does that.
