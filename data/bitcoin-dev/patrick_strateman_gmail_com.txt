
@_date: 2015-08-09 14:57:57
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] What Lightning Is 
The costs of operating a hub are as follows:
Time value of the funds the Hub has locked up in payment channels.
Enhanced risk of loss of control of private keys (the keys necessarily
need to be on an internet connected system).
Operating costs (I expect this will be minimal).
The hub can charge a fee for it's services to recoup these costs.

@_date: 2015-08-09 15:06:03
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] What Lightning Is 
I suspect there is some amount of confusion here on terms.
The hub is essentially swapping funds between payment channels.
The hub's entire business is centered around having payment channels
open with other hubs/users.
If the hub requires user funds to open these channels... then the users
have no reason to pay the hub anything in fees.
A hub that doesn't use it's own funds to open payment channels to other
hubs/merchants is useless.

@_date: 2015-08-09 15:36:28
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] What Lightning Is 
On the contrary those costs are clearly very low.
Both the time value of money and operating expenses will be trivial with
even a small volume of transactions.
The true cost of operating a hub is clearly in the enhanced risk of loss.
It's clear that risk of loss will be moderated by market forces.
The hubs which are better at securing their systems will reduce their
risk of loss and obtain a competitive advantage.
It's also important to note that the risk of loss is the same whether
the hub is doing 1 transaction/second or 1 million transactions/second.
At 1 transaction/second the cost (but not necessarily the fees) is going
to be quite high.
At 1 million transactions/second the cost is going to be very very low.

@_date: 2015-08-09 20:31:56
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] What Lightning Is 
aren't much closer to today's BTC borrowing rates.
The borrowing rates you're talking about involve the risk of default.
In lightning the hubs funds are not at risk so long as they maintain
control of the private keys.
The rates charged by hubs will almost certainly be orders of magnitude
below the rates charged on the various p2p lending sites....
But that seems fairly obvious... did I miss something?

@_date: 2015-08-10 01:36:49
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] What Lightning Is 
If a path cannot be built to the recipient through the lightning network
then a standard transaction should be used.

@_date: 2015-08-16 18:20:49
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Minimum Block Size 
The first question to answer here is simple:
What value would there be in requiring a minimum block size?
I see no value.

@_date: 2015-08-18 17:00:09
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Bitcoin XTs Tor IP blacklist downloading system 
First of all I would like to say... LOL
Second Andrew LeCody is correct, this is off topic.

@_date: 2015-12-02 10:45:23
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and 
If compression is to be used a custom compression algorithm should be
Bitcoin data is largely incompressible outside of a tiny subset of fields.

@_date: 2015-12-08 12:50:08
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Scaling by Partitioning 
Payment recipients would need to operate a daemon for each chain, thus
guaranteeing no scaling advantage.
(There are other issues, but I believe that to be enough of a show
stopper not to continue).

@_date: 2015-12-08 13:29:13
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Scaling by Partitioning 
If partition is selected from a random key (the hash of the output for
example) then payment recipients would need to operate a full node on
each of the chains.
What's the point of partitioning if virtually everybody needs to operate
each partition?
The mining aspect has it's own set of issues, but I'm not going to get
into those.

@_date: 2015-07-18 12:46:01
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Do we really need a mempool? (for relay nodes) 
Relay nodes do not need a mempool, but do need some mechanism to avoid
DoS issues.
Wallet nodes can use the mempool for fee estimation (in addition to
looking at past blocks).

@_date: 2015-06-22 14:39:07
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
If you truly have a consensus then the rational behavior is to
permanently change the nodes behavior after the trigger.

@_date: 2015-06-26 11:47:37
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] The need for larger blocks 
Planning for a hard forks which change the consensus rules (including
the blocksize limit) is something we can all agree is worthy of time and
However there is clearly not consensus sufficient today to deploy a hard
fork that changs the blocksize without there being serious and
potentially experiment ending consequences.
For a proposed hard fork to reach a level of consensus necessary to be
safe requires that there be a clear and self evident course of action.
That simply does not exist on the blocksize limit question.

@_date: 2015-06-27 19:13:16
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Original Vision 
organizations operating full network nodes would provide connectivity to
light clients and these light clients would make up the majority of the
user base.
Satoshi also believed that fraud proofs would be widely available and
If fraud proofs were practical SPV client security would be much closer
to full node security than it is today.
Unfortunately no design for fraud proofs which is both efficient and
secure has been proposed; much less implemented and deployed.
In building a system as new and innovative as bitcoin certain things
will be wrong.
The perception that SPV clients could be made nearly as secure as full
nodes is one example of something that was wrong.

@_date: 2015-06-27 22:29:24
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Original Vision 
Fraud proofs need to be at least more efficient than full node validation.
Currently they are not.

@_date: 2015-05-27 18:05:08
@_author: Patrick Strateman 
@_subject: [Bitcoin-development] Version bits proposal 
There is absolutely no reason to do this.
Any reasonable micro-controller can build merkle tree roots
significantly faster than is necessary.
1 Th/s walks the nonce range once every 4.3ms.
The largest valid merkle trees are 14 nodes high.
That translates to 28 SHA256 ops per 4.3ms or 6511 SHA256 ops/second.
For reference an RPi 1 model B does 2451050 SHA256 ops/second.

@_date: 2015-09-18 12:43:13
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] Hash of UTXO set as consensus-critical 
Full nodes using UTXO set commitments is a change to the bitcoin
security model.
Currently an attacker with >50% of the network hashrate can rewrite history.
If full nodes rely on UTXO set commitments such an attacker could create
an infinite number of bitcoins (as in many times more than the current
21 million bitcoin limit).
Before we consider mechanisms for UTXO set commitments, we should
seriously discuss whether the security model reduction is reasonable.

@_date: 2016-02-07 11:03:54
@_author: Patrick Strateman 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
I would expect that custodians who fail to produce coins on both sides
of a fork in response to depositor requests will find themselves in
serious legal trouble.
Especially if the price moves against either fork.
