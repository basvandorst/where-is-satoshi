
@_date: 2014-12-21 12:25:36
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] The relationship between 
So let's go through an example to see in which ways
non-proof-of-publication orders are "insecure".
Alice the seller wants to sell 1 unit of A for 100 units of B.
Bob is willing to pay up to 200 Bs for 1 A.
Let's assume a proof of publication system first, in which the
execution price is the mean between bid and ask.
Alice publishes her order.
Bob could publish his order at price 200 Bs and the order would
execute at 150 Bs.
But after seeing Alice's order he knows he doesn't need to pay that
much, so he publishes and order buying for 100 Bs.
Alice gets 100 Bs (what she signed she wanted) and Bob pays less than
he was wiling to pay, he pays 100 Bs. Everybody happy.
Now let's assume native assets and sighash_single.
Alice publishes her order (out of band, using various channels).
Bob could publish his order at price 200 Bs and then a miner would
execute at 100 Bs for Alice, at 200 Bs for Bob and pocket 100 Bs as
mining fees.
But after seeing Alice's order he knows he doesn't need to pay that
much, so he publishes and order buying for 100 Bs.
Again, Alice gets 100 Bs (what she signed she wanted) and Bob pays pays 100 Bs.
The main difference is that Alice didn't had to pay a fee to publish
her binding order.
Now let's try to articulate your concerns.
Your concern is that Carol, isolates Bob preventing him from seeing
Alice's order.
Then maybe Bob publishes his own order at 200 Bs.
If Carol sees both orders while preventing the other participants from
seeing them, she can build a tx in which Alice sells at 100, Bob buys
at 200, and Carol pockets the difference.
But...any smart miner will replace Carol's address with his own when
processing the trade, so Carol cannot win this way.
Another thing Carol can do is to buy the A herself for 100 Bs, leaving
Bob without them.
If Alice cares about Bob getting the deal instead of Carol she can do
two things:
1) Establish a direct communication channel with Bob
2) Move to a proof of publication system and start paying fees for
publishing binding orders.
So again, what's the advantage that proof-of-publication provides TO
ALICE so that she will be so eager to pay the higher costs to get the
same deal?
If this example is not enough to be able to explain the advantage of
proof-of-publication markets feel free to write a more complex one.

@_date: 2014-12-21 20:39:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] The relationship between 
And like last time we discussed this on the mailing list my opinion
differs from yours.
You talk about "real world sellers and buyers" but ignore Alice the
seller and Bob the buyer in my example.
You failed to explain how sybil attackers (Carol) get all those
margins. In my example I claim miners get them, what am I missing?
How is the same example with a proof-of-publication market any better?
Miners can reorder the orders with proof of publication too.
If getting orders into mined blocks faster has an advantage miners can
charge privileged traders for privileged connections (just like it
happens today with "perfectly fair" centralized markets today that
feature the high-frequency trading you mention).
They could even charge for moving transactions around within the same
block if that has any effect on the execution rules.
I prefer that miners can get the difference between bids and asks
directly to compensate for their hashing power.
The point is that there's more models for p2p markets beyond those
that require proof of publication for their orders, and you're
claiming that only those using proof of publication are secure.
That's incorrect.
Can you define "Accurate unbiased price information"?
Traders want to trade. The primary function of markets is exchange,
not price discovery.
"Provably fair price discovery" is probably impossible. But I can
imagine how many people could get excited about such a technology.
Can you formally define what you mean by this?
You see, "fair" implies morality and therefore it's a very subjective
term, so it's not obvious to me what you may mean by that.

@_date: 2014-12-30 11:47:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] BIP: Voluntary deposit bonds 
On Mon, Dec 29, 2014 at 10:34 PM, Justus Ranvier
What services?
I must be missing something obvious about the motivation.
I understand the difference between "paying to myself only when I mine
the next block" and "offering fees to whoever mines this tx".
But how does allowing miners to pay to themselves in this way help
with security and future lower subsidies at all?

@_date: 2014-11-03 15:14:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] side-chains & 2-way pegging (Re: is there 
rather than proving secret knowledge as
is typical for digital signatures, we refer to them as miners. To
achieve stable consensus on the
blockchain history, economic incentives are provided where miners are
rewarded with fees and
subsidies in the form of coins that are valuable only if the miners
form a shared valid history,
incentivising them to behave honestly.[...]"
Ignoring altrustic miners, the irreversibility of a DMMS chain
obviously depends on the rewards received by miners on that chain.
Nobody is claiming that sidechains will be "as secure bitcoin", any 2
way pegged assets is always "more secure" (probably too vague of a
term in this context) in its original chain.
Since all seigniorage from Bitcoin's initial distribution is spent on
mining subsidies for the main chain, it is not available to subsidize
sidechains too. Thus sidechains, in principle, reward their miners
with the same Bitcoin will use in the future: only transaction fees.
Since some people claim that won't be enough (is not always clear to
me if they believe that won't be enough for sidechains or also for
bitcoin when the subsidies are gone), we included this section with
other ideas we have explored to further. Some of them, like
"time-shifted fees" could be interesting for Bitcoin itself in the
Reorganizations are both a naturally occurring phenomenon and
something that an attacker may cause to revert history.
Section "11. Calculations" of the Bitcoin whitepaper gives you this
formula (in C code):
 double AttackerSuccessProbability(double q, int z)
    double p = 1.0 - q;
    double lambda = z * (q / p);
    double sum = 1.0;
    int i, k;
    for (k = 0; k <= z; k++)
    {
        double poisson = exp(-lambda);
        for (i = 1; i <= k; i++)
            poisson *= lambda / i;
        sum -= poisson * (1 - pow(q / p, z - k));
    }
    return sum;
Also says "Given our assumption that p > q, the probability drops
exponentially as the number of blocks the
attacker has to catch up with increases."
In this case, the contest period determines z, the number of blocks
the attacker has to catch up from the honest chain.
So the longer the contest period is, the harder it is to succeed with
a fraudulent transfer.
For example, if a given sidechain chooses 52560 as the contest period
(1 year assuming 10 min blocks), it will be very hard for an attacker
to produce a fake alternative longest-than-the-last-year-of-history
fork to steal coins.
A sidechain with such an extreme contest period would probably not be
very practical though, since honest users would have to wait more than
a year to complete transfers from the parent chain to the sidechain
and viceversa.
I hope this clarifies our assumptions.

@_date: 2014-11-03 18:32:31
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] side-chains & 2-way pegging (Re: is there 
But if the majority of the sidechain miners keep working on the honest
chain, anyone can submit a reorg proof during the contest period that
invalidates this "unlockment" on the parent chain.
Honest sidechain miners will get rewarded in the sidechain, and those
rewards will only be valuable if they form a shared valid history.
This is correct. There's many variables at play.
As explained many times, sidechains and merged mining are orthogonal:
pegged sidechains don't need to use merged mining just as merged
mining altchains don't need to be sidechains.
Anyway, I think you're somehow assuming that deciding to mine against
the sidechain instead of mining for its rewards.
This is simply not true. No matter how big the attack incentive is, if
you're assuming my 52560 contest period example and that the attacker
doesn't control the majority of the hashing power on the sidechain,
the probability of achieving a one-year reorg is negligible. In the
meantime honest nodes are getting some reward, let's say 0.1 BTC per
block. That's 5256 btc/year to the honest nodes vs 0 btc/year for the
If the attacker controls, say, 10% of the network, he's losing 525.6
btc/year in opportunity costs for an extremely small chance of getting
1000 btc.
We're not claiming that the security model is the same, we just
compare it to Bitcoin's because it's similar in many senses.
Yes, that's precisely the kind of reorganizations the BITCOIN
WHITEPAPER is talking about in section "11 Calculations":
reorganizations caused intentionally by an attacker. Please read it
"q_z = probability THE ATTACKER will ever catch up from z blocks behind".
If it sounds to you like we're claiming that attacker-induced
reorganizations are not likely, maybe we could have expressed it some
other way. That was certainly not the intention.
That's not true for Bitcoin itself and that's not what we're claiming.
Exponentially harder with the number of blocks is good enough for me.
That would be a reorganization too, you can't create a completely fake
history for a sidechain, the attacker will share some of the chain's
Yes, the attacker can create an SPV proof of a fake chain and in that
sense, this is different from a regular double-spend.
If honest miners control the majority of the hashing power, they will
produce a valid chain longer than the fake chain. And then anyone can
use that reorg proof to stop the attacker before the contest period.
You see, "SPV security" is not the same as "SPV security with more
than 52560 confirmations of the transaction I'm receiving".
Proof of work is not free, that's the whole point of proof of work.
As said, sidechains, like Bitcoin itself, relies on the assumption
that an attacker won't control a majority of the network. Satoshi's
paper just says that p must be greater than q.
We go beyond that precisely at the beginning of the "6.1 Hashpower
attack resistance" section:
"The main thrust of this paper surrounds two-way peg using SPV proofs,
which are forgeable by a
51%-majority and blockable by however much hashpower is needed to
build a sufficiently-long
proof during the transfer?s contest period. (There is a tradeoff on
this latter point ? if 33%
hashpower can block a proof, then 67% is needed to successfully use a
false one, and so on.)"
I'm happy to keep trying to clarify things. But I think we will
advance faster if you first tell me what do you think the contest
period is for.
Because that's I think the source of your misunderstandings. From what
you're saying, I don't think you're having the contest period into
account at all.

@_date: 2014-11-16 19:44:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
I agree with Luke, we can endlessly discuss the "best defaults" like
the default size allowed for OP_RETURN, minimum fees, anti-dust
policies, first-seen vs replace-by-fee, etc; but the fact is that
policies depend on miners. Unfortunately most miners and pools are
quite apathetic when it comes to configure their own policy.
In my opinion the best we can do is to make it easier for miners to
implement their own policies by abstracting out those parts of the
code. Pull requests like  and  are steps in that direction.
So if you're interested in having more miners accepting 80 bytes
OP_RETURN transactions, I suggest you invest some time reviewing and
testing those PRs.
Although this wasn't its main purpose, separating script/standard was
also a little step in the same direction.

@_date: 2014-11-16 20:04:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
As an aside, the decision to make it 40 bytes made sense because it is
enough for timestamping. In fact, you can do cheaper and even secret
(and thus impossible to censor by miners) timestamping using
pay-to-contract [1], which uses exactly 0 extra bytes in your
transaction and the blockchain.
I remember people asking in  "Does anyone know any use
case for greater sizes OP_RETURNs?" and me answering "I do not know of
any use cases that require bigger sizes".
I'm aware that so called "proof of publication" is not equivalent to
timestamping, but I wasn't aware at the moment (and I don't think it's
very interesting but that's obviously only my opinion, "embedded
systems" developers will disagree).
[1] Here's a video explaining pay-to-contract in the context of
invoicing as a use case: Here's a generic working implementation:

@_date: 2014-11-17 13:22:39
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
On Mon, Nov 17, 2014 at 12:43 PM, Flavien Charlon
No, storing only a hash is enough for ALL timestamping applications.
If you need to broadcast more data then we're not talking about
timestamping anymore, but rather proof of publication.
Unfortunately (and as it has been already mentioned) many applications
don't need proof of publication and yet they are just using the
blockchain as a convenient transport mechanism, but that's highly
It's like if you sent all your mails to all the existing email
addresses with the metadata "to be read by: destination at yourhost.com".
It wouldn't make any sense and it wouldn't scale.
A url definitely looks like something that doesn't belong in the chain.

@_date: 2015-04-24 10:55:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
s7r you may be interested in this video explaining several aspects of
malleability: It is pre BIP62, but I believe it is very relevant and will hopefully
clear some of your doubts.
The signer of TX1 will always be able to change the signature and thus
the tx ID.

@_date: 2015-04-26 13:35:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Yes, can we call this one OP_MATURITY to distinguish it from RCLTV?
Mhmm, interesting.
I'm totally fine with changing the interface to:
     int bitcoinconsensus_verify_script(const unsigned char
*scriptPubKey, unsigned int scriptPubKeyLen,
                                        const unsigned char *txTo
  , unsigned int txToLen, unsigned nHeight,
                                        unsigned int nIn, unsigned int
flags, bitcoinconsensus_error* err);
I prefer op_maturity over RCLTV and there are also gains for absolute
CLTV as you explain later.
When you validate the script inputs of a transaction you already have
a height, either the real final nHeight in ConnectBlock and the miner,
or nSpendHeight in AcceptToMemoryPool.
The costs are meaningless in my opinion, specially when we will
already have to change the interface to add libsecp256k1's context.
I'm infinitely more worried about the other assumption that the 3
solutions are already making.
Changing to
     int bitcoinconsensus_verify_script(const unsigned char
*scriptPubKey, unsigned int scriptPubKeyLen,
                                        const unsigned char *txTo
  , unsigned int txToLen, const CCoinsViewCache& inputs,
                                        unsigned int nIn, unsigned int
flags, bitcoinconsensus_error* err);
Is simply not possible because CCoinsViewCache is a C++.
You could solve it in a similar way in which you could solve that
dependency for VerifyTransaction.
For example:
typedef const CTxOut& (*TxOutputGetter)(const uint256& txid, uint32_t n);
      int bitcoinconsensus_verify_script(const unsigned char
*scriptPubKey, unsigned int scriptPubKeyLen,
                                        const unsigned char *txTo
  , unsigned int txToLen, TxOutputGetter utxoGetter,
                                        unsigned int nIn, unsigned int
flags, bitcoinconsensus_error* err);
Of course, this is assuming that CTxOut becomes a C struct instead of
a C++ class and little things like that.
In terms of code encapsulation, this is still 100 times uglier than
adding the nHeight so if we're doing it, yes, please, let's do both.
There's another possibility that could keep the utxo out of Script verification:
class CTxIn
    COutPoint prevout;
    CScript scriptSig;
    uint32_t nSequence;
could turn into:
class CTxIn
    COutPoint prevout;
    CScript scriptSig;
    uint32_t nHeight;
And a new softfork rule could enforce that all new CTxIn set nHeight
to the correct height in which its corresponding prevout got into the
That would remove the need for the TxOutputGetter param in
bitcoinconsensus_verify_script, but unfortunately it is not reorg safe
(apart from other ugly implementation details).
So, in summary, I think the new interface has to be something along these lines:
      int bitcoinconsensus_verify_script(const unsigned char
*scriptPubKey, unsigned int scriptPubKeyLen,
                                        const unsigned char *txTo,
unsigned int nIn,
                                        unsigned int txToLen,
TxOutputGetter utxoGetter, unsigned nHeight, secp256k1_context_t *ctx
                                        unsigned int flags,
bitcoinconsensus_error* err);
I'm totally fine not supporting time-based locks for the new operators.
Removing them from the regular nLockTime could be more complicated but
I wouldn't mind either.
Every time I think of a contract or protocol that involves time, I do
it in terms of block heights.
I would prefer to change all my clocks to work in blocks instead of
minutes over changing nHeights for timestamps in any of those

@_date: 2015-04-26 14:20:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Wait, wait, this can be made reorg-safe and more backards compatible.
The new validation rule at the tx validation level (currently in
main::CheckInputs()) would be
for (unsigned int i = 0; i < tx.vin.size(); i++) {
            if (tx.vin.nHeight + 100 > tx.nLockTime)
                return state.Invalid(false, REJECT_INVALID,
            if (coins->nHeight > tx.vin.nHeight)
                return state.Invalid(false, REJECT_INVALID,
Existing transactions that have used the deprecated CTxIn::nSequence
for something else will be fine if they've used low nSequences.
The only concern would be breaking some colored coins kernels, but
there's many others implemented that don't rely on CTxIn::nSequence.
Transactions that want to use OP_MATURITY just have to set the
corresponding CTxIn::nHeight and CTransaction::nLockTime properly.
This way op_maturity wouldn't require anything from the utxo and the
final interface could be:
 int bitcoinconsensus_verify_script(const unsigned char* scriptPubKey,
unsigned int scriptPubKeyLen,
                                        const unsigned char* txTo,
unsigned int txToLen,
                                        unsigned int nIn, unsigned int nHeight,
                                        unsigned int flags,
secp256k1_context_t* ctx,
                                        bitcoinconsensus_error* err);

@_date: 2015-04-28 09:23:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Proof of Payment 
So at the low level, how does a "proof of payment" differ from just proving
that a given transaction is in a given block (what SPV nodes take as proof
of payment today)?

@_date: 2015-04-28 09:44:14
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Even if it's new and has not received any feedback, I think my solution to
op_maturity is quite clean.
But anyway, yes, the non-relative cltv is much simpler in design and
doesn't have to wait for the other. On the other hand, I would upgrade it
to absolute cltv like you suggested and take the current height as a
parameter to verifyScript instead of using the nLockTime as reference.
If we know we're going to use it for rcltv/op_maturity, better put add soon
rather than later, specially if that will give us a more powerful cltv.
If we don't want that height param, we can leave it out of for op_maturity
too, but that's the wingle decision about rcltv/maturity that affects cltv
so better solve that first.

@_date: 2015-04-28 14:53:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Proof of Payment 
Forget it, sorry, I misunderstood the proposal entirely, re-reading
with more care...

@_date: 2015-04-30 10:08:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Where do I start? 
As Mike says it depends on your interests. But one thing that is almost
always welcomed is improving the tests, and it is unlikely that it
conflicts with other people's PRs (unless they're changing that part of the
code and need to update those tests. Improving documentation is also good
and you can do that while reading the code. Usually I just start cloning,
compiling and changing things as I read, "if I understand this correctly,
this change should not break the tests, if I understand this, this other
change should break the build", etc.
But again, is up to you.

@_date: 2015-04-30 12:28:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Where do I start? 
Well, if you're interested in learning java while learning bitcoin,
probably you should be looking at or one of its related project (like the android bitcoin wallet based
on it).
There's a getting sterted page: These links my be useful too:
On Thu, Apr 30, 2015 at 11:35 AM, Telephone Lemien

@_date: 2015-08-04 12:35:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
As explained by Venzen, this is a non-sequitur.
I don't know the answer either, that's fine. It's the opposite
question that I've been insistently repeating and you've been
(consciously or not) consistently evading.
But that's also fine because I believe you finally answer it a few lines below.
The emergence of a btc price enabled the emergence of professional
miners, which in turn enabled the emergence of sha256d-specialized
hardware production companies.
Nothing surprising there.
By no means it consitutes an example of how a bigger consensus sizes
can cause less mining centralization.
Correlation does not imply causation. I will better leave it at that...
No, I think 20 MB was chosen very optimistically, considering 3rd
party services rates (not the same service as self-hosting) in the
so-called "first world". And then 20 MB goes to 20 GB, again with
optimistic and by no means scientific expectations.
But where the number comes from it's not really what I'm demaning,
what I want is some criterion that can tell you that a given size
would be "too centralized" but another one isn't.
I haven't read any analysis on why 8GB is a better option than 7GB and
9GB for a given criterion (nor one declaring 20 GB a winner over 19 GB
or 21 GB).
A simulation test passing 20 GB but not 21 GB would make it far less arbitrary.
I'm of course talking about consensus maximum blocksize, not about
actual blocksize.
Yes, again, when mining becomes profitable, economic actors tend to
appear and get those profits.
But don't confuse total hashrate improvements with an "increase in the
number of miners" or with mining decentralization.
Finally, I think you finally answered my repetitive question here.
If I say "Mike Hearn understands that the consensus block size maximum
rule is a tool for limitting mining centralization" I'm not putting
words in your mouth, right?
I think many users advocating for an increase in the consensus limit
don't understand this, which is extremely unfortunate for the debate.
Great! Maybe after 2 mining centralization improves so much that we're
confortable not only not lowering it but rather increasing it.
Did the fact that you "understand that the consensus block size
maximum rule is a tool for limitting mining centralization" influenced
your rejection of that idea at all?
The only way that "not caring much whther we have a consensus limit or
not" and "understand that the consensus block size maximum rule is a
tool for limitting mining centralization" at the same time is by not
caring about mining centralization at all.
Is that your position?
If you don't care about having a limit but you don't want to limit
transaction volume, then ++current_size will ALWAYs be your
"compromise position" and no blocksize increase will ever be enough
until the limit is completely removed.
Is that your position?
Yes, and I believe the same points stand.
You keep talking about "high-value-transactions-only" like if
non-urgent transaction fees rising from zero to, say, 1 satoshi, would
automatically result in that "high-value-transactions-only" Bitcoin.
Please, stop talking as if someone was proposing a
"high-value-transactions-only" Bitcoin. That may happen but nobody
really knows. If it happens it may not be bad thing necessarily (ie
bitcoin microtransactions can still happen using trustless payment
channels and x is still cheaper than x% for any transacted value
higher than 100) but that's really not what we're talking about here
so it seems distraction that can only help further polirizing this
What we're talking about here is that hitting the limit would
(hopefully) make miners start caring about fees. Enough that they stop
being irrational about free transactions. If both things happen,
non-urgent transaction fees will likely rise (as said, above zero).
You think that would be a catastrophe for adoption and I disagree.
But (as Pieter has repeatedly explained) for any size there will be
use cases that will be eventually priced out.
So when rising this consensus limit, not increasing centralization
should be the priority and the potential impact in market fees a much
more secondary concern.
Do you agree with this?
I'm sure there are many intermediate positions between "caring more
about mining centralization than market fees when deciding about a
consensus rule that limits mining centralization" and "not caring
about mining centralization at all".
I really don't want to put words in your mouth, but I honestly don't
know what your position is.
I don't really know how else can I ask the same question: you don't
care the consensus maximum blocksize rule being here at all or not
(you just said that).
Is it because you don't think it limits mining centralization or
because you don't care about limiting mining centralization with
consensus rules at all?

@_date: 2015-08-04 12:53:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
It's interesting how realizing that the blocksize consensus limit does
the opposite of what you initially thought when starting the thread
didn't changed your conclusion from
"If you're truly worried about larger blocks causing centralization,
think about how, by restricting blocksize, you're enabling the
Communist Chinese government to maintain centralized control over 57%
of the Bitcoin hashing power."
"If you're truly worried about larger blocks causing centralization,
think about how, by INCREASING blocksize, you're enabling the
Communist Chinese government to POTENTIALLY INCREASE ITS centralized
control over 57% of the Bitcoin hashing power."
The new conclusion is just "somebody should mine from Venezuela and
Iceland" instead.
If you were so concerned about mining centralization, now that you
understand how the blocksize maximum influences it (by being the only
consensus rule that limits it) and if you were consequent, now you
would warn about the dangers of increasing the blocksize consensus
limit in this particular moment in time when mining centralization
looks already really bad (ie 57% hashrate in the same jurisdiction).
Another possibility is that you don't really care about mining
centralization and you were only looking for an argument in favor of
increasing the blocksize, which for some other reason you have already
concluded that must be done as soon as possible.

@_date: 2015-08-04 13:59:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
I prefer to wait and let him talk by himself.
That is not my position. Again, I don't know what the right blocksize
for the short term is (I don't think anybody does).
But I know that the maximum block size limit consensus rule (no more
artificial than any other consensus rule, like, say, the one that
prohibits double-spends) serves to limit mining centralization.
Therefore how the change can affect mining centralization must be the
main concern, instead of (also artificial) projections about usage
growth (no matter how organic their curves look).
Also I don't think "hitting the limit" must be necessarily harmful and
if it is, I don't understand why hitting it at 1MB will be more
harmful than hitting it at 2MB, 8MB or 8GB.
I don't know where you get your "majority" from or what it even means
(majority of users, majority of the coins, of miners?)
But there's something I'm missing something there...why my position
doesn't matter if it's not a majority?
How is what the the majority has been told it's best an objective argument?
And if we can "break things" in simulations first before we "break
things" in production, maybe we don't need the later hardfork to "fix
things" (if it's still possible to fix them without completely
restarting the ASIC market).
The fact is that we don't have a single simulation that can tell you
"too centralized/shouldn't affect mining centralization much" for a
given block size.
So if you say 8, I must ask, why not 9?
Why 9 MB is not safe for mining centralization but 8 MB is?
There is NO criterion based on mining centralization to decide between
2 sizes in favor of the small one.
It seems like the rationale it's always "the bigger the better" and
the only limitation is what a few people concerned with mining
centralization (while they still have time to discuss this) are
willing to accept. If that's the case, then there won't be effectively
any limit in the long term and Bitcoin will probably fail in its
decentralization goals.
I think its the proponents of a blocksize change who should propose
such a criterion and now they have the tools to simulate different
block sizes.
I want us to simulate many blocksizes before rushing into a decision
(specially because I disagree that taking a decision there is urgent
in the first place).

@_date: 2015-08-04 15:13:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
Nobody is preventing anyone from claiming anything. Some developers
are encouraging users to ask for bigger blocks.
Others don't want to impose consensus rule changes against the will of
the users (even if they're 10% of the users).
Still, "Things apparently aren't bad enough" is just your opinion.
1) I don't care what the so-called "majority" thinks: I don't want to
impose consensus rule changes against the will of a reasonable
2) It doesn't matter who is to blame about the current centralization:
the fact remains that the blocksize maximum is the only** consensus
rule to limit mining centralization.
3) In fact I think Luke Dashjr proposed to reduced it to 400 KB, but I
would ask the same thing: please create a simulation in which the
change is better (or at least no much worse) than the current rules by
ANY metric.
Please read the point 2 with special attention because it's not the
first time I say this in this thread.
** There's also the maximum block sigops consensus rule to limit
mining centralization.

@_date: 2015-08-04 15:37:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
No, I think 1 MB is just as arbitrary as any other size proposed.
All I want is for consensus change proponents to try harder to
convince other users (including me)
No, if the changed rule only serves to limit centralization, then how
that limitation to centralization is affected should be the first
thing to consider.
If miners' concern was only the amount of profit they make they
wouldn't mine free transactions already.
You cannot possibly know what all users' are concern about, so I will
just ignore any further claim in that direction.
Talk for yourself: your arguments won't be more reasonable just
because you claim that all users think like you do.
I disagree with this wild prediction as well.
And I assume the way that vaguely defined "economic majority"
communicates with you through a crystal ball or something
No more, but not less either.
Nobody can't control the implementation that I (or other people
concerned with centralization) run either.
How is allowing fees from rising above zero "fighting the market"?
The system is currently designed with a 1 MB limit. I don't think
that's sacred or anything, but I really don't feel like I'm fighting
"the market" or "the way the system is designed".
In any case, what do "the market" and "the way the system is designed"
have to do with what the majority have been told it's best (which you
seem to think should be a source of truth for some reason I'm still
Why 9 MB is safe but 10 MB isn't?
The "conflict" won't be resolved by evading hard questions...
Will there ever be a debate that results in "further blocksize
increases at this point are very risky for mining centralization"?
How will we tell then? Can't we use the same criteria now?

@_date: 2015-08-04 19:59:25
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
But reading it 10 times may help you understand the claim, you will
never find out until you try.
"Miners buying each other out" is not the only way in which mining
centralization can get even worse.
A Blocksize limit may not be able to prevent such a scenario, but it's
still the only consensus tool to limit mining centralization.
If you want to prove that claim wrong you need to find a
counter-example of another consensus rule that somehow limits mining
You could also prove that this rule doesn't help with mining
centralization at all. But that's much more difficult and if you just
claim it (and consequently advocate for the complete removal of the
consensus rule) we will have already advanced a lot.
But you denying that the limits serves limiting mining centralization
and at the same time advocating for keeping a limit at all doesn't
seem very consistent.
If you don't want that rule to limit mining centralization pressures,
what do you want it for?

@_date: 2015-08-04 22:02:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus fork activation thresholds: Block.nTime 
To be clear, all options can use the version.
All advantages shared with the height option.
I'm talking about the non-reorg case. Without reorg, you know what the
next height or current median time is, but you don't know what the
next block time is.
How does bitcoin-tx know about the next block time?
It doesn't. You would need to use the current time as a proxy for the
median time or the block.nTime which you don't know either.
Or just keep the sanity check as it is. Note that this case is
blocksize-specific: other hardforks (like my previous example, or the
code proposal in BIP99) don't share that concern.
One thing I've noticed there seems to be disagreement on is whether
miners' upgrade confirmation (aka voting) is necessary for
uncontroversial hardforks or not.
In BIP99 the advice for uncontroversial hardforks is setting a
threshold (based on height, but we can change it) and then wait for
95% of the hashrate to upgrade to enforce the chain.
But maybe the "voting" can happen first and then the threshold is
added to the "miners' confirmation height/time".
I think that may influence which of the three discussed options
(height, block time and median time) is better, so maybe we should
discuss that first.

@_date: 2015-08-05 21:29:41
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus fork activation thresholds: Block.nTime 
I'm not sure how bip102 is less secure than other blocksize proposal
but please let's keep defects specific to each proposal in their own
In any case, I understand that you agree that 95% confirmation is a
good idea for uncontroversial hardforks (like in uncontroversial
I'm not sure if you prefer that to happen before or after the time
threshold, but I guess you're fine with doing it after the threshold
since you didn't complained about that specifically (you can always
clarify your preferences of course).

@_date: 2015-08-06 01:24:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Superluminal communication and the consensus block 
There is a common meme that block propagation times is the only metric
that matters when it comes to value the block size maximum consensus
rule's usefulness in limiting mining centralization.
Here is an extremely optimist thought experiment for those who think
that is the case:
Imagine that superluminal communication has somehow been invented and
validity of mined blocks can be checked in constant time thanks to
some sort of snarks magic. This doesn't mean that block propagation is
O(1) with respect to time because each node needs to repeat that cheap
validation before relaying the block.
Still, this is the best situation we can imagine with respect to block
propagation, right?
No, wait, due to some technical or economical miracle this
superluminal communication is free for everyone:
better-than-physically-possible (our understanding of physics changes
with time as well, right?) communication and infinite bandwidth for
At this point, does the consensus block size maximum still help
limiting mining centralization or we can just remove it entirely?
The answer is yes, it can help limit mining centralization.
Let's imagine that these amazing advancements have happened in less
than 22 years and we only had 6 more subsidy halvings, that's only 7
halvings in total, so the subsidy is still as high as 50 * (0.5 ^ 7) =
0.390625 btc/block
Although the orphan-block-probability cost for a miner to include an
extra transaction has been completely minimized, it is still not null.
Let's assume that while all these technical miracles were
happening...that 22 more years was enough for miners to realize this
fact, they have removed the special-cased-for-free-transaction policy
code that currently comes with Bitcoin Core (or it has been removed
from Bitcoin Core) and they don't mine transactions with fees lower
than 1 satoshi anymore.
I hope this last assumption doesn't turn out to be more wild
than superluminal communication...
But there must be a physical limit: in our example, miners will have
different CPU constraints (to further simplify, genetically-engineered
and super-fast memory also grows in the streets everywhere after an
accident in a Monsanto Lab; or better downloadmoreram.com actually
works and I just hadn't tried from windows or mac).
Miner A is able to process 100 M tx/block while miner B is only able
to process 10 M tx/block.
Will miner B be able to maintain itself competitive against miner B?
The answer is: it depends on the consensus maximum block size.
How so? Let's imagine that it has been completely removed.
Assuming a fee of 1 satoshi per transaction and no shortage of
unconfirmed transactions, miner A's block reward will be 0.390625 + 1
= 1.390625 btc vs miner B's 0.390625 + 0.1 = 0.390625 + 0.1 = 0.490625
Difficulty will tend to increase until the cost to produce a block
(including interest in all the capital needed, paid or not) is equal
to 1.390625 btc and therefore miner B will stop mining or go bankrupt.
But maybe 100 M and 10 M were too high numbers. What about 10 M and 1
M? Still, 0.400625 btc can't compete with 0.490625 btc.
You think 10x is too much of a difference? Fine, 2M vs 1M: still
0.400625 btc can't compete with 0.410625 btc
In summary, there will always be some physical limitation that may
benefit big mining players, so the block size maximum will always be
useful to limit mining centralization.
In other words (and I don't intend this to sound rude), if you want to
eventually remove the block size maximum consensus rule entirely, I
will never be able to agree with you: not even in your wildest dreams.

@_date: 2015-08-06 03:26:09
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
This is a much more reasonable position. I wish this had been starting
point of this discussion instead of "the block size limit must be
increased as soon as possible or bitcoin will fail".
If the only fear about not increasing the block size fast enough is
that fees may rise, pieter wouldn't had to repeatedly explain that for
any reasonable block size some use case may fill the blocks rapidly.
This sounds good overall but I'm afraid you are oversimplifying some things.
Centralization pressure not only comes from global average bandwidth
costs and block propagation times is not the only concern.
Here's an extreme example: [1]
But anyway, yes, I agree, ANY metric would be better than nothing (the
current situation).
Given that for any non-absurdly-big size some transactions will
eventually be priced out, and that the consensus rule serves for
limiting mining centralization (and more indirectly centralization in
general) and not about trying to set a given average transaction fee,
I think the current level of mining centralization will always be more
relevant than the current fee level when discussing any change to the
consensus rule to limit centralization (at any point in time).
In other words, the question "can we change this without important
risks of destroying the decentralized properties of the system in the
short or long run?" should be always more important than "is there a
concerning rise in fees to motivate this change at all?".
As said, I would always consider the centralization risks first: I'd
rather have a $5/tx decentralized Bitcoin than a Bitcoin with free
transactions but effectively validated (when they validate blocks they
mine on top of) by around 10 miners, specially if only 3 of them could
easily collude to censor transactions [orphaning any block that
doesn't censor in the same manner]. Sadly I have no choice, the later
is what we have right now. And reducing the block size can't guarantee
that the situation will get better or even that fees could rise to
$5/tx (we just don't have that demand, not that it is a goal for
anyone). All I know is that increasing the block size *could*
(conditional, not necessarily, I don't know in which cases, I don't
think anybody does) make things even worse.
On the other hand, I could understand people getting worried if fees
where as high as $5/tx or even 20 cent/tx but we're very far away from
that case. How can low subsidies (a certainty) be "too far in the
future to worry about it" but $5/tx, 20 cent/tx or even 5 cent/tx an
urgent concern? For all I know, 5 cent/tx may not happen in the next
25 years: it may never happen. And if it happens, to me it will be a
symptom of Bitcoin success, even for others it means that Bitcoin has
become a "high value settlement network".
To the question
- At which minimum mining fee rate will you urge others to change the
consensus rules to increase the block size?
I'm very sorry, but my answer is:
- I honestly don't know, that may never happen.
What I can tell you is this: I will never be worried about "too high
fees" while the fees remain at 0 (null, zero, nothing, cero, nada,
That's right, no matter what wallet's defaults chose for their users,
no matter what the minimum relay fee policy does to the "fee market"
and how much urgent transactions pay in fees; the fact remains that in
practice non-urgent transactions usually (when the raw transaction's
structure it's appropriate) don't have to pay fees.
I'm still missing an answer from the "big blocks size side" to the
following question (which I have insistently repeated with various
If "not now" when will it be a good time to let fees rise above zero?
After the next subsidy halving? After 4 more subsidy halvings (ie
about 13 years from now, subsidy = 1.5625 btc/block )? After your
grandmother abandons her national currency and uses Bitcoin for
everything? Never?
ANY answer (maybe with the exception of the last one) would be less
worrying than silence.
I really appreciate your efforts to mediate in this dispute and I
honestly hope that my previous answer is useful as it is.
Just replace 1MB with ANY size.
But, yes, please, when will you consider a size to be too dangerous
for centralization?
Why 20 GB would have been safe but 21 GB wouldn't have been (or the
respective maximums and respective +1)?
Note that "Never, 20GB was just a number closer to infinity than 1MB
and I hoped that could had been voluntarily accepted by users. What I
really want is to completely remove this consensus rule forever." is a
perfectly valid answer. It just means that I will agree with people
that think this way as explained in [1] (yes, not even after
superluminal communication).
As an less relevant note, I feel extremely uncomfortable about being
included in the "1MB advocates" group. As I've tried to explain
several times, 1 is to me an arbitrary number like any other (at most,
the canonical arbitrary number), and so it is 1000000
(MAX_BLOCK_SIZE). I rarely like being grouped or labelled, but maybe
something more accurate like "not-recklessly-change-consensus-rules
advocates" would help.
There's nothing special about 1MB apart from being the current rule,
so I don't think the number is that much relevant to the discussion.
Grouping anyone that has raised any concern about rising the consensus
block size limit as "the 1MBers" is about as fair as grouping anyone
that has ever proposed a rise in the maximum size as "the 20GBers"
(specially when there's people that belong to both groups
simultaneously, like Pieter Wuille who started this thread, and whose
proposal we're supposed to be discussing).
[1]

@_date: 2015-08-06 04:33:11
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Superluminal communication and the consensus 
Yes, of course.
I didn't say "it only depends", just "it depends".
The example is focused on CPU as the "centralizing factor".
And still, all costs are already included in the example:
"Difficulty will tend to increase until the cost to produce a block
(including interest in all the capital needed, paid or not) is equal
to [...]"
I'm focusing on gains but I didn't forget to subtract costs at the end.
I may have "weird" economic ideas, but that will usually just mean
that I mention "interest" in contexts where you may think it is not
In other words, expect me to "sin" by excess rather than omission when
it comes to economic costs.
I agree that mining profitability can radically change in this case
(ie a home heater miner is competing with other heaters, not with
other bitcoin miners).
But until such an economic breakthrough happens I would rather not
rely on it happening.
This could certainly change the mining centralization dynamics in a radical way.
Note that if I buy a heater for 20 usd and expect to mine 5 usd worth
of btc this winter, I will consider it cheaper than an
equivalently-energy-consuming non-mining heater sold for 16 usd.
Maybe next year a more mining-efficient heater will be sold that will
still mine 5 usd worth of btc in its first winter, while my old one
will only mine 0.5 usd the second winter. That's completely fine, it's
0.5 usd extra savings and I was already happy with 1 usd savings in
the first year!
This can be applied to small home heaters, full-building heaters...
Apparently the future doesn't look so bright when it comes to
industrial heating because higher temperatures are needed, but I'm
really optimistic about mining as a byproduct of human-heating
This would also mean that part of the total hashrate would travel the
globe with the winter, which would also have its own benefits (and
maybe new risks?) to decentralization.
When/If this happens, I think everybody should carefully reconsider
all their assumptions about mining centralization.
By "this", I mean production of mining devices whose primary purpose
it's not mining but rather heating (I don't think many people realize
about the huge economic consequences of this seemingly-small
difference, not even companies specialized in bitcoin mining ASIC
It would also save me a lot of discussions with some ecologist friends
(the fact is that I'm much more worried about bitcoin's huge subsidies
on bitcoin's mining and what that means to the environment than I
usually let them know), but that's another topic...
At the same time we want mining to be (I was going to say "remain" but
I can't help with being pessimistic about the current mining
situation) decentralized, don't we?

@_date: 2015-08-06 17:25:47
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
I'm pretty sure that I can quote Mike Hearn with a sentence extremely
similar to that in this forum or in some of his blog posts (not in
 at a first
But yeah, what people said in the past is not very important: people
change their minds (they even acknowledge their mistake some times).
What interests me more it's what people think now.
I don't want to put words in your mouth and you are more than welcome
to correct what I think you think with what you really think.
All I'm trying to do is framing your fears properly.
If I say "all fears related to not raising the block size limit in the
short term can be summarized as a fear of fees rising in the short
Am I correct? Am I missing some other argument?
Of course, problems that need to be solved regardless of the block
size (like an unbounded mempool) should not be considered for this
If you pay high enough fees your transactions will be likely mined in
the next block.
So this seems to be reducible to the "fees rising" concern unless I am
missing something.
I think I would have a much better understanding of what "the other
side" thinks if I ever got an answer to a couple of very simple
questions I have been repeating ad nausea:
1) If "not now" when will it be a good time to let fees rise above zero?
2) When will you consider a size to be too dangerous for centralization?
In other words, why 20 GB would have been safe but 21 GB wouldn't have
been (or the respective maximums and respective +1 for each block
increase proposal)?
3) Does this mean that you would be in favor of completely removing
the consensus rule that limits mining centralization by imposing an
artificial (like any other consensus rule) block size maximum?
I've been insistently repeating this question too.
Admittedly, it would be a great disappointment if your answer to this
question is "yes": that can only mean that either you don't understand
how the consensus rule limits mining centralization or that you don't
care about mining centralization at all.
If you really want things to move forward, please, prove it by
answering these questions so that we don't have to imagine what the
answers are (because what we imagine is probably much worse than your
actual answers).
I'm more than willing to stop trying to imagine what "big block
advocates" think, but I need your answers from the "big block
Asking repeatedly doesn't seem to be effective. So I will answer the
questions myself in the worse possible way I think a "big block
advocate" could answer them.
Feel free to replace my stupid answers with your own:
I'm quite confident that you will have better answers than those.
Please, let me know what you think.

@_date: 2015-08-06 19:15:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
First of all, thank you very much for answering the questions, and
apologies for not having formulated them properly (fortunately that's
not an irreparable mistake).
When we talk about "fees" we're talking about different things. I
should have been more specific.
Average fees are greatly influenced by wallet and policy defaults, and
they also include extra fees that are included for fast confirmation.
I'm not talking about fast confirmation transactions, but about
non-urgent transactions.
What is the market minimum fee for miners to mine a transaction?
That's currently zero.
If you don't want to directly look at what blocks contain, we can also
use a fee estimator and define a "non-urgent period", say 1 week worth
of blocks (1008 blocks).
The chart in your link doesn't include a 1008 blocks line, but the 15
blocks (about 2.5 hours) line seems to already show zero fees:
So I reformulate the question:
1) If "not now", when will it be a good time to let the "market
minimum fee for miners to mine a transaction" rise above zero?
This just shows where the 20 GB come from, not why you would reject 21 GB.
Let me rephrase.
2) Do you have any criterion (automatic or not) that can result in you
saying "no, this is too much" for any proposed size?
Since you don't think the consensus block size maximum limits mining
centralization (as you later say), it must be based on something else.
In any case, if you lack a criterion that's fine as well: it's never
too late to have one.
Would you agree that blocksize increase proposals should have such a
Ok, this is an enormous step forward in the discussion, thank you.
In my opinion all discussions will be sterile while we can't even
agree on what are the positive effects of the consensus rule that
supposedly needs to be changed.
It's not that you don't care about centralization, it's that you don't
believe that a consensus block size maximum limits centralization at
This means that if I can convince you that the consensus block size
maximum does in fact limit centralization in any way, you may change
your views about the whole blocksize consensus rule change, you may
even take back or change your own proposal.
But let's leave that aside that for now.
Regardless of the history of the consensus rule (which I couldn't care
less about), I believe the only function that the maximum block size
rule currently serves is limiting centralization.
Since you deny that function, do you think the (artificial) consensus
rule is currently serving any other purpose that I'm missing?
If the answer is something along the lines of "not really, it's just
technical debt", then I think you should be honest and consequent, and
directly advocate for the complete removal of the consensus rule.
I really think conversations can't really advance until we clarify the
different positions about the discussed consensus rule current

@_date: 2015-08-06 23:51:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
Really, thanks again for replying and not getting mad when I get your
thoughts wrong.
I believe that I've learned more about your position on the subject
today than in months of discussion and blogs (that's not a critique to
your blog post, it's just that they didn't answer to some questions
that I personally needed responded).
I'm very happy to have made the stupid question then. It has revealed
another big difference in the fundamental assumptions we're using.
My assumption is that for any reasonable size, free transactions will
eventually disappear (assuming Bitcoin doesn't "fail" for some other
Maybe I'm being too optimistic about the demand side of the market in
the long term.
In contrast, your assumption seems to be (and please correct me on
anything I get wrong) that...
"The limit will always be big enough so that free transactions are
mined forever. Therefore fees just allow users to prioritize their
urgent transactions and relay policies to protect their nodes against
DoS attacks.
Well, obviously, they also serve to pay for mining in a low-subsidy
future, but even with the presence of free transactions, fees will be
enough to cover mining costs, or a new mechanisms will be developed to
make a low-total-reward blockchain safe or expensive proof of work
will be replaced or complemented with something else that's cheaper.
The main point is that fees are not a mechanism to decide what gets
priced out of the blockchain, because advancements in technology will
always give as enough room for free transactions."
- jtimon putting words in Gavin's mouth, with the only intention to
understand him better.
I'm using "free transactions" even though you said "zero or very close to zero".
To you, "zero or very close to zero" may be the same thing, but to me
zero and very close to zero are like...different galaxies.
To me, entering the "very close to zero galaxy" is a huge step in the
development of the fee market.
I've been always assuming that moving from zero to 1 satoshi was
precisely what "big block advocates" wanted to avoid.
What they meant by "Bitcoin is going to become a high-value only
network" and similar things.
Knowing that for "big block advocates" zero and "very close to zero"
are equally acceptable changes things.
I completely agree, but the block size limit is a consensus rule that
doesn't adapt to the market. The market will adapt to whatever limit
is chosen by the consensus rules.
I would really like a more formal criterion, ideally automatic (like
any other test, the parameters can be modified as technology
But fair enough, even though your criterion is too vague or not
future-proof enough, I guess it is still a criterion.
It seems that this is a matter of disagreements and ideal ways of
doing things and not really a disagreement on fundamental assumptions.
So it seems this question wasn't so interesting after all.
That's what you think you are discussing, but I (and probably some
other people) think we are discussing something entirely different.
Because we have a fundamentally different assumption on what the block
size limit is about.
I really hope that identifying these "fundamental assumption
discrepancies" (FAD from now own) will help us avoid circular
discussions so that everything is less frustrating and more productive
for everyone.
This could be prevented in some other ways. If this is the only
concern, it doesn't need to be a consensus rule.
Sorry, another try:
You think the maximum block size rule serves to limit centralization
by limiting how hard it is to run a full node.
I agree with that, but I would add something more and you wouldn't:
The maximum block size consensus rule limits how hard it is to be a
competitive miner.
In other words, you think the last statement is false or incorrect.
Meta: I think we should try to collect and list more of this "FADs"
(we have at least 2 of them already). If you think it can be useful,
I'm more than happy to repeat this process in the opposite direction:
you make the questions and I give the answers, you write what you
think I think and I correct you in iterations. Probably we should
finish with you correcting what I think you think first. I am really
excited about understanding your point of view better.
Stupid humor (hopefully not out of context and not offensive): I'm
happy to discover that what I thought it was FUD was just FAD.
More seriously, I'm really happy for your interest in understanding
and being understood.
Let's worry about where do we think differently first and about who is
right on each point later.
In the end, only the conclusions on each point will matter and not who
claimed the final conclusions (in the points where we find them) first
(if we get to final common conclusions on that point at all).

@_date: 2015-08-07 19:33:34
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
yes, fear of Bad Things Happening as we run up against the 1MB limit is one
of the reasons.
What are the other reasons?
and have seen what happens when networks run out of capacity very seriously.
When "the network runs out of capacity" (when we hit the limit) do we
expect anything to happen apart from minimum market fees rising (above
Obviously any consequences of fees rising are included in this concern.

@_date: 2015-08-10 13:55:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Gavin, I interpret the absence of response to these questions as a
sign that everybody agrees that  there's no other reason to increase
the consensus block size other than to avoid minimum market fees from
rising (above zero).
Feel free to correct that notion at any time by answering the
questions yourself.
In fact if any other "big block size advocate" thinks there's more
reason I would like to hear their reasons too.

@_date: 2015-08-10 14:45:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
Here's some related commits from  :
And a related PR (closed for now, at least until  is merged) :
I definitely add the chainID field, and support regtest in bip70 too
(code is more complex by not supporting it that it could be while
supporting it). And if we want to maintain the chain petname, I would
change "test" to "testnet3".
While "main" and "regtest" are always used for those chains, we
currently have 3 different strings for testnet3:
"testnet3": for the default data directory.
"testnet": for the GUI style, and strings showed to the user.
"test": for bip70
I really want to simplify this and I think the simplest way to do so
is by unifying everything to always use "testnet3", although that
would require modifying bip70.
Altchains aren't just altcoins and sidechains: there's also testchains
like testnet3, regtest and sizetestN (  ). Since there's so many
possible instances for sizetest, testchains are already more numerous
than altcoins (not that this last thing matters much for anything).
Just forget about altcoins and sidechains: do it for the testchains
(that's the reason why bitcoin has chainparams and multi-chain support
in the first place).
We should make things easier to add new testchains, not harder.
It is sad to see that some times things are "the wrong way" because
doing them "the right way" could "simplify things to altcoins too
Such a design criterion seems so ridiculous and sad to me...
Those lookups can but just to a map in memory, like in
Alternatively we can maintain the chain petname field, but those are
just "standard petnames", not unique and immutable ids like the
genesis hash.

@_date: 2015-08-10 15:03:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I don't think that's necessarily true. Theoretically urgent
transactions could fund hashing power on their own while there are
still some free non-urgent transactions being mined from time to time.

@_date: 2015-08-10 15:06:51
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
Well, yes, I guess it's modifying that in the extension BIP.
The point is not having exceptions and treating all supported chains
in the same way in the code.
Having a special case for regtest makes the code more complex, not simpler.

@_date: 2015-08-10 16:55:40
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
unreliable.  Unreliable is bad.
without an increase to the max block size.
I'm not trying to be obstinate but I seriously can't see how they are
When you say unreliable I think you mean "unreliable for cheap fee
transactions". Transactions with the highest fees will always confirm
reliably. For example, a 1 btc fee tx will probably always confirm very
reliably even if capacity never increases and demands increases a lot.

@_date: 2015-08-10 21:28:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
Agreed. I would just like that there was an attempt to automatically
estimate those risks before taking those risks.
Some function we're trying to optimize with simulations (based on
 ) to find an ideal (according to that imperfect metric) maximum
consensus block size.
Maybe the function/simulations just take some minimum hardware
specifications and returns an block size, I don't know.
The most important thing to understand in this discussion is that it
is about a trade-off between lower fees (more maximum tx volume) and
mining (and general) centralization.
I don't know what the costs and gains curves are here (for 4MB, 1 MB
or any other number, and I don't think anybody does).
But if we can't even agree on what the advantages and disadvantages of
increasing the consensus block size maximum, it is very hard that we
can agree on a universally acceptable point or range in this trade-off
This is all probably right, but IMO we're very far away from $5/tx.
My main point about fees is that minimum mining fees rising above zero
(theri current level) is not necessarily a bad thing or an urgent
On the other hand, we have much more data about current mining
centralization, which should be very relevant information when
discussing a block size increase.
As Pieter has explained repeatedly, a big block count is not a goal in
itself, just a metric.
And if you ask me, I don't think it's all that interesting as a
metric. For all I know there could be a lot more full nodes being run
that for whatever reason are not seen by people collecting this data.
The block size maximum consensus rule limits mining centralization,
not just full node centralization. Gavin, for example, disagrees with
Fortunately I believe at least 2 mathematical proofs can be produced
to demonstrate Gavin and those who think like him are wrong.
I believe that even with the relay network, and even assuming all
miners are connected using something like IBLT, a mathematical proof
can be constructed to demonstrate that bigger block sizes can prevent
the worse connected miners from being profitable.
It is important to note that the worse connected miners aren't
necessarily those with less bandwidth: maybe you have the best
bandwidth but you are poorly connected to the majority of the hashrate
(for example, because the majority of the hashrate is within the same
country but that country is not very well connected to the rest of the
We're certainly far away from this being a concern in practice.
But I'm working on a mathematical proof that at some scale CPU
requirements could become a discriminating factor making the smallest
mining operations unprofitable.
I don't think there's such a thing as a "dangerous full node count level".
It's just data that can be useful to build centralization metrics.
Probably hashrate distribution by pool is much more interesting (and
if you ask me that looks really bad right now without increasing the
block size consensus maximum).
I agree.
Great. I don't think that minimum mining fees will rise above 1 usd
cent/tx anytime soon even if we maintain the limit of 1MB.
Maybe that's why I'm not worried at all about "hitting the limit".
But I'm sorry, I don't have those concrete numbers because it is a
trade-off I don't think we've studied in enough detail.
Well, I can also say I wouldn't be worried at all about moving to,
say, 1.01 MB (because the difference in centralization pressure should
be minimal) and I would just take it as a "let's proof hardforks are
possible" change similar to the one proposed in bip99.
That would be interesting to read and I have totally missed it.
Do you have a link?
And for any size something similar could happen with some use case.
But this is a great example of a situation where I would understand
people panicking and clamoring to change the consensus rule as soon as
Even with much lower fees, say 1 usd/tx.
I think it would be a great problem to have and admittedly I'm not
worried about having it in the short term.
And if it happened overnight we could always deploy an emergency hardfork.
I think it's helping by determining who is to be served first, and
that is those who benefit more from Bitcoin (and are therefore willing
to pay higher costs for using it), in this case, people doing
international remittances.
This is what I mean by "market minimum fee".
I think the code that miners use to select which transactions to
include first needs a lot of work.
As said miners are subsidizing free transactions, increasing their own
costs for nothing in exchange.
Also, yes, there is something special about this market: it is
supposed to pay for most of the global hashrate in the not-so-far
If we take too long to start moving away from total seigniorage
subsidy dependence, it may be too late when we do.
I think you are underestimating the software costs.
And you not only have to adapt the software that we have now, but also
the software that hasn't been written yet and will be written assuming
free transactions and an underdeveloped market.
Also you are over-estimating the costs: you could hit the limit but
then rise the block size maximum as soon as you reach, say 0.00001
Even if minimum fees go again to zero after rising the block size
maximum, the software improvements will remain there.
And when do you think "Bitcoin will need to rely heavily on txn fees
for security"?
How many more halvings is that?
That's not what I've been arguing, I've just being saying that I would
be completely ok with minimum fees rising above zero (say, to 1
satoshi/tx) tomorrow.
I don't think that's necessarily a bad thing (in fact, it has some
advantages) and certainly not something we should fear to the point of
rushing hardforks to avoid it.

@_date: 2015-08-10 21:44:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
That's a fatal design in Feathercoin, not a mistake all altchains have
done and certainly irrelevant to Bitcoin.
Regtest is a testchain just like testnet2 and testnet3. Testchains are
the only reason why Bitcoin Core supports multiple chains using
For the payment protocol testchains, sidechains and altcoins are all
quite similar.
But it is fine to just focus on testchains if sidechains and altcoins
are out of scope.

@_date: 2015-08-10 21:49:23
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
On Mon, Aug 10, 2015 at 9:19 PM, Ross Nicoll via bitcoin-dev
No, the chain ID needs to be unique, that's the whole point.

@_date: 2015-08-10 22:17:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] trust 
On Sat, Aug 8, 2015 at 2:37 PM, Thomas Zander via bitcoin-dev
Adam, I think he means a multisig escrow transaction where the escrow
is trusted by both parties, and other examples like that.
But I don't see how that is relevant, allowing trust to be involved in
different ways is a feature, but it's optional.
I think the point "you don't need to trust anyone to use Bitcoin" remains.

@_date: 2015-08-11 19:03:27
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Aug 9, 2015 10:44 PM, "Dave Scotese via bitcoin-dev" <
bigger blocks will help to reduce the occurrence of that problem, I propose
a user-configurable default limit to the size of the mempool as a permanent
solution regardless of block size.  "This software has stopped consuming
memory necessary to validate transactions.  You can override this by ..."
If anyone feels that protecting those running full nodes from bitcoind
eating more and more memory this way is a good idea, I can make a BIP out
of it if that would help.
You are completely right: this problem has nothing to do with the consensus
block size maximum and it has to be solved regardless of what the maximum
is. No BIP is necessary for this. The "doing nothing side" has been working
on this too:

@_date: 2015-08-11 19:47:56
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Aug 11, 2015 12:14 AM, "Thomas Zander via bitcoin-dev" <
I've read them. I have read gavin's blog posts as well, several times.
I still don't see what else can we fear from not increasing the size apart
from fees maybe rising and making some problems that need to be solved
rewardless of the size more visible (like a dumb unbounded mempool design).
This discussion is frustrating for everyone. I could also say "This have
been explained many times" and similar things, but that's not productive.
I'm not trying to be obstinate, please, answer what else is to fear or
admit that all your feas are just potential consequences of rising fees.
With the risk of sounding condescending or aggressive...Really, is not that
hard to answer questions directly and succinctly. We should all be friends
with clarity. Only fear, uncertainty and doubt are enemies of clarity. But
you guys on the "bigger blocks side" don't want to spread fud, do you?
Please, prove paranoid people like me wrong on this point, for the good of
this discussion. I really don't know how else to ask this without getting a
link to something I have already read as a response.

@_date: 2015-08-11 21:27:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
network -- which is a network where all the participating nodes are aware
of and agree upon every transaction. Constraining Bitcoin capacity below
the limits of technology will only push users seeking to participate in a
global consensus network to other solutions which have adequate capacity,
such as BitcoinXT or others. Note that lightning / hub and spoke do not
meet requirements for users wishing to participate in global consensus,
because they are not global consensus networks, since all participating
nodes are not aware of all transactions.
Even if you are right, first fees will raise and that will be what pushes
people to other altcoins, no?
Can we agree that the first step in any potentially bad situation is
hitting the limit and then fees rising as a consequence?

@_date: 2015-08-11 21:45:35
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
transaction, and those transactions can and will probably be moved onto
offchain solutions in order to avoid paying the cost of achieving global
consensus. But you still don't get to set the cost of global consensus
artificially. Market forces will ensure that supply will meet demand there,
so if there is demand for access to global consensus, and technology exists
to meet that demand at a cost of one cent per transaction -- or whatever
the technology-limited cost of global consensus happens to be -- then
that's what the market will supply.
Assuming we maintain any block size maximum consensus rule, the market will
adapt to whatever maximum size is imposed by the consensus rules.
For example, with the current demand and the current consensus block size
maximum, the market has settled on a minimum fee of zero satoshis per
transaction. That's why I cannot understand the urgency to rise the maximum
In any case, yhe consensus maximum shouldn't be based on current or
projected demand, only on centralization concerns, which is what the
consensus rule serves for (to limit centralization).
For example, Gavin advocates for 20 MB because he is not worried about how
that could increase centralization because he believes it won't.
I can't agree with that because I believe 20 MB could make mining
centralization (and centralization in general) much worse.
But if I have to chose between 2 "centralization safe" sizes, sure, the
bigger the better, why not.
In my opinion the main source of disagreement is that one: how the maximum
block size limits centralization.

@_date: 2015-08-11 21:53:56
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
question at hand is whether we should constrain that limit below what
technology is capable of delivering. I'm arguing that not only we should
not, but that we could not even if we wanted to, since competition will
deliver capacity for global consensus whether it's in Bitcoin or in some
other product / fork.
You didn't answer the 2 questions...
Anyway, if we don't care about centralization at all, we can just remove
the limit: that's what "technology can provide".
Maybe in that case it is developers who move to a decentralized
network -- which is a network where all the participating nodes are aware
of and agree upon every transaction. Constraining Bitcoin capacity below
the limits of technology will only push users seeking to participate in a
global consensus network to other solutions which have adequate capacity,
such as BitcoinXT or others. Note that lightning / hub and spoke do not
meet requirements for users wishing to participate in global consensus,
because they are not global consensus networks, since all participating
nodes are not aware of all transactions.
pushes people to other altcoins, no?
hitting the limit and then fees rising as a consequence?

@_date: 2015-08-12 10:51:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
And again, you dodge the question...
It's not that I don't remember, it's that for all your "reasons" I can
always say one of these:
1) This could only be an indirect consequence of rising fees (people will
move to a competitive system, cheap transactions will become unreliable,
2) This problem will appear with other sizes too and it needs to be solved
permanently no matter what (dumb mempool design, true scalability, etc)
Whatever, even suggesting you may want to just spread fud and that's why
you don't respond directly to the questions made you respond directly to
the question: you answered with "[]".
I just give up trying that people worried about a non-increase in the short
term answer to me that question. I will internally think that they just
want to spread fud, but not vey vocal about it.
It's just seems strange to me that you don't want to prove to me that's not
the case when it is so easy to do so: just answer the d@ question.
I'm not so sure, people keep talking about the need to scale the system by
increasing the consensus maximum...
But I'm happy that, indeed, many (possibly most?) people understand this.
I disagree with this.
In any case, how can future demand be easier to predict than software
development times?

@_date: 2015-08-12 11:00:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Aug 12, 2015 10:11 AM, "Thomas Zander via bitcoin-dev" <
Don't fear this happening at 1 MB, fear this happening at any size. This
needs to be solved regardless of the block size.
Don't worry, the "doing nothing side" is already taking care of this. I
will give the link for the second time...

@_date: 2015-08-12 11:45:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Wed, Aug 12, 2015 at 11:23 AM, Thomas Zander via bitcoin-dev
I feel people aren't being respectful with me either, but what I feel
doesn't matter.
I really feel I am very close to exhaust all possible avenues for that
question getting directly answered.
Suggesting that the answer doesn't come because the goal it's just to
spread FUD was one of my last hopes. And it didn't work!
This question had been dodged repeatedly (one more time in this last response).
I could list all the times I have repeated the question in various
forms in the last 2 weeks and the "answers" I received (when I
received any answer at all) but I'm afraid that will take too much
Then we could go one by one and classify them as:
1) Potential indirect consequence of rising fees.
2) Software problem independent of a concrete block size that needs to
be solved anyway, often specific to Bitcoin Core (ie other
implementations, say libbitcoin may not necessarily share these
If you think there's more "problem groups", please let me know.
Otherwise I don't see the point in repeating the question. I have not
received a straight answer but you think you've given it.
Seems like a dead end.
On Wed, Aug 12, 2015 at 11:25 AM, Thomas Zander via bitcoin-dev
I don't think everybody knows, but thank you for saying this
explicitly! Now I know for sure that you do.
Now I know that you are ok with classifying this concern under group 2
in my above list.

@_date: 2015-08-12 11:59:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not rising 
I believe all concerns I've read can be classified in the following groups:
- Lowest fee transactions (currently free transactions) will become
more unreliable.
- People will migrate to competing systems (PoW altcoins) with lower fees.
- Bitcoin Core's mempool is unbounded in size and can make the program
crash by using too much memory.
- There's no good way to increase the fee of a transaction that is
taking too long to be mined without the "double spending" transaction
with the higher fee being blocked by most nodes which follow Bitcoin
Core's default policy for conflicting spends replacements (aka "first
seen" replacement policy).
I have started with the 3 concerns that I read more often, but please
suggest more concerns for these categories and suggest other
categories if you think there's more.

@_date: 2015-08-12 12:28:39
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to rising the 
We've identified a fundamental disagreement in:
- The block size maximum consensus rule serves to limit mining centralization
But as said I believe at least 2 different formal proofs can be
produced that in fact this is the case. One of them (the one I'm
working on) remains true even after superluminal communication, free
unlimited global bandwidth and science fiction snarks.
But let's just list the concerns first.
I believe there's 2 categories:
1) Increasing the block size may increase centralization.
- Mining centralization will get worse (for example, China's aggregate
hashrate becomes even larger)
   - Government control in a single jurisdiction could enforce
transaction censorship and destroy irreversibility of transactions
      - Some use cases that rely on a decentralized chain (like
trustless options) cannot rely on Bitcoin anymore.
      - Reversible transactions will have proportional fees rather
than flat ones.
         - Some use cases that rely on flat fees (like remittance) may
not be practical in Bitcoin anymore
- The full node count will decrease, leaving less resources to serve SPV nodes.
2) Trying to avoid "hitting the limit" permanently minimizes minimum
fees (currently zero) and fees in general
- If fees' block reward doesn't increase enough, the subsidy block
reward may become insufficient to protect the irreversibility of the
system at some point in time, and the system is attacked and destroyed
at that point in time
- Miners will continue to run noncompetitive block creation policies
(ie accepting free transactions)
- More new Bitcoin businesses may be created based on unsustainable
assumptions and consequently fail.
- "Free transactions bitcoin marketing" may continue and users may get
angry when they discover they have been lied about the sustainability
of that property and the reliability of free transactions.
Please suggest more concerns or new categories if you think they're needed.

@_date: 2015-08-15 00:01:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to rising 
On Wed, Aug 12, 2015 at 6:41 PM, Thomas Zander via bitcoin-dev
Yes, concerns or risks. Feel free to bike-shed the word.
I'm not the manager of Bitcoin Core nor the Bitcoin consensus rules.
I'm just trying to separate "dogs from cats" (if you allow me the
analogy) in another random attempt to move the discussion into more
productive territories.
I wouldn't be doing this if I didn't felt we have advance identifying
and understanding each other's concerns better.
If you think this thread is useless you don't need to participate.
This thread is for "guardian dogs". To discuss to mouses catches we're
missing, I've created the other almost-identically-titled thread:
The title of this one doesn't contain a "not" like the other one. You
missed (imagined?) that little detail.
In other words, if you want to contribute to this thread, think about
"things that could go wrong if we rise the consensus block size
Judging from your strong conviction about the need of an urgent size
rise, probably you have thought hard about this before concluding that
there's not risk for your favorite proposed size (2MB, 8MB, 8GB,
Note that even if you are certain that those risks aren't a concern at
this scale, identifying and somehow estimating or measuring (if that's
even possible) the risks will be useful for future increases.

@_date: 2015-08-15 00:24:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I extremely dislike the inversion to preserve the "previous nSequence
semantics". The "previous nSequence semantics" were
consensus-unenforceable but we can cover the same use cases (or the
realistic ones at least) with nMaturity. Let's face it and move on
without technical debt we don't need and may regret. If we do this
inversion we will likely carry it for very long if not forever.
As a side effect, I believe documentation can become much clearer
(maybe even shorter simultaneusly).

@_date: 2015-08-15 00:35:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
No, sorry it's 2 concerns/risks with 2 sub-concerns/risks each:
1) Potential indirect consequence of rising fees.
1.1) Lowest fee transactions (currently free transactions) will become
more unreliable.
1.2) People will migrate to competing systems (PoW altcoins) with lower fees.
2) Software problem independent of a concrete block size that needs to
be solved anyway, often specific to Bitcoin Core (ie other
implementations, say libbitcoin may not necessarily share these
2.1) Bitcoin Core's mempool is unbounded in size and can make the program
crash by using too much memory.
2.2) There's no good way to increase the fee of a transaction that is
taking too long to be mined without the "double spending" transaction
with the higher fee being blocked by most nodes which follow Bitcoin
Core's default policy for conflicting spends replacements (aka "first
seen" replacement policy).
I believe thisbelongs in 2, not really sure if 2.1 or 2.2 since it's
related to both.
I believe "fear of exchange rate declining" can probably be added to
any concern/risk "leaf", so we should probably leave that for the end
or just omit it.
I'm not sure I understood this but seems related to the exchange rate.

@_date: 2015-08-15 00:55:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
I think potential is more general since it allows us to list uncertain
consequences (aren't all consequences just projections and thus
uncertain anyway?).
It is a big difference to me (it may mean policy code has improved a
lot in the process).
Anyway, I'm heppy to hear again that this is not a concern, at some
point I thought this was the ONLY concern, so I was clearly
misinterpreting people's arguments.
Let's list them all first and then identify which are more worrying in
the short term.
"Utility" like "value" is always subjective and very vague. I prefer
to identify more concrete ways in which "utility is reduced".
It is clear that not all use cases fit the blockchain, but it's still
unclear which ones don't fit yet.
But the amount of use cases supported is not a valid metric for
In any case, it would be interesting if we could list some concrete
cases that would be lost.
This is correct. Layer 2 can become more expensive in total as well
(it doesn't mean layer 2 doesn't scale though).
I wil add it as 1.3
Experimentation can be done with worthless testchains. I'm not sure
I'm following on this one.
1.4) Less users than we could have had with a bigger size
1.4.1) More regulation pressure
Related to exchange rate.
1.4.2) Not enough fees when subsidy is lower
Resulting list:
1) Potential indirect consequence of rising fees.
1.1) Lowest fee transactions (currently free transactions) will become
more unreliable.
1.2) People will migrate to competing systems (PoW altcoins) with lower fees.
1.3) Layer 2 settlements become more expensive
1.4) Less users than we could have had with a bigger size
1.4.1) More regulation pressure
1.4.2) Not enough fees when subsidy is lower
2) Software problem independent of a concrete block size that needs to
be solved anyway, often specific to Bitcoin Core (ie other
implementations, say libbitcoin may not necessarily share these
2.1) Bitcoin Core's mempool is unbounded in size and can make the program
crash by using too much memory.
2.2) There's no good way to increase the fee of a transaction that is
taking too long to be mined without the "double spending" transaction
with the higher fee being blocked by most nodes which follow Bitcoin
Core's default policy for conflicting spends replacements (aka "first
seen" replacement policy).

@_date: 2015-08-15 00:57:18
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
I believe your concerns are included in:
1) Potential indirect consequence of rising fees.
1.4) Less users than we could have had with a bigger size
1.4.2) Not enough fees when subsidy is lower

@_date: 2015-08-15 00:59:12
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
On Fri, Aug 14, 2015 at 12:01 AM, Geir Harald Hansen via bitcoin-dev
I believe this is included in "btc exchange rate may fall".

@_date: 2015-08-15 01:12:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
I was tempted to add it as a sub-sub-risk in both, since both things
need to be solved before that stop being a concern. But they may be
more things to do to solve that so I'm listing it as a separate
sub-risk in 2:
2.3) Big list of valid unconfirmed transactions
resulting list:
1) Potential indirect consequence of rising fees.
1.1) Lowest fee transactions (currently free transactions) will become
more unreliable.
1.2) People will migrate to competing systems (PoW altcoins) with lower fees.
1.3) Layer 2 settlements become more expensive
1.4) Less usage than we could have had with a bigger size
1.4.1) More regulation pressure
1.4.2) Not enough fees when subsidy is lower
2) Software problem independent of a concrete block size that needs to
be solved anyway, often specific to Bitcoin Core (ie other
implementations, say libbitcoin may not necessarily share these
2.1) Bitcoin Core's mempool is unbounded in size and can make the program
crash by using too much memory.
2.2) There's no good way to increase the fee of a transaction that is
taking too long to be mined without the "double spending" transaction
with the higher fee being blocked by most nodes which follow Bitcoin
Core's default policy for conflicting spends replacements (aka "first
seen" replacement policy).
2.3) Big list of valid unconfirmed transactions

@_date: 2015-08-15 01:57:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A summary list of all concerns related to not 
To be clear, these two are just my personal lists of arguments on
"each side" to clear my mind and try to be perfectly objective.
The resulting lists may not have any practical value but I'm going to
maintain them locally nonetheless. If that's is not useful for the
participants they can just leave the thread.
I'm not going to be impartial: I will only add to my lists the
arguments that I consider reasonable and, more importantly, I
The lists are in the public domain and everybody is free to use it and
modify it in any way, feel free to fork the threads with other
criteria different from "whatever jtimon thinks is reasonable", but if
you want to participate, face it, it's what these 2 threads are about.
Here's the updated both-thread lists:

@_date: 2015-08-17 13:44:59
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
On Aug 17, 2015 1:40 PM, "Oliver Egginger via bitcoin-dev" <
Why should we block any email address?

@_date: 2015-08-17 17:54:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
BIP99 (still a draft too) currently recommends a minimum height plus
95% mining upgrade confirmation (aka "miner voting") after that for
uncontroversial hardforks:
But general hardfork activation discussion is still inconclusive in
The code for the example uncontroversial hardfork proposed in bip99 is
at: But I haven't created a PR for either the code or the bip yet.

@_date: 2015-08-17 17:58:18
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
You may be interested in this patch/PR:
Why only one more testchain when you can add
std::numeric_limits::max() new testchains with approximately
the same code?

@_date: 2015-08-17 18:11:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
I don't think is all that interesting to make miners vote on lower
limit. Say the community wants to reduce the size to limit mining
centralization, it's not unthinkable that the hashrate majority (which
may have to face more competition or harvest less transactions after
the change) may oppose to that and then the community is forced to
deploy an anti-miner's hardfork (maybe even asic-reset hardfork?)
instead of a softfork.
Yes, uncontroversial sofforks are easier and less risky to deploy than
uncontroversial hardforks, but anti-miner hardforks are not.
Not only I don't think it's a good idea for miners to vote on the
block size (which is there to control them), I don't even buy the
assumption that "we can always just softfork a smallwer size later".
If you give something to miners they may not want to give it back later.
We could hardfork to 42 M supply and then "just softfork back to 21 M", right?
Or what's the same, we could "just softfork supply to 15 M". Such a
change would be clearly controversial among miners, so it wouldn't be
an uncontroversial softfork anymore. Some of these cases are discussed
in BIP99.

@_date: 2015-08-17 18:18:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Minimum Block Size 
One could think that this could help with things like SPV mining, but
miners can just pay to themselves to follow the minimum size block
rule without risking anything.
As long as they have a singled matured satoshi they can just pay to
themselves with it as many times as they need in the same block.
On Mon, Aug 17, 2015 at 3:20 AM, Patrick Strateman via bitcoin-dev

@_date: 2015-08-17 18:32:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
You mean to avoid discussions about his authenticity?
Should that matter at all?
Does the content of the post matter less than its author?

@_date: 2015-08-17 19:03:09
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP [104]: Replace Transaction Fees with Data, 
Did you self-assigned a bip number?
Anyway, from your document:
"The existing (2015) Big Data analysis market is valued at around
$125bn according to market
research firm IDC. As the Bitcoin block chain replaces existing
payment systems and attracts
millions or hundreds of millions of users, the intrinsic value of the
data in the block chain
will increase and be attractive to data analytics businesses and block
chain start-ups."
Without even entering about details related to technical feasibility,
do you realize that you are proposing Bitcoin to become the most
orwellian money ever?
That this is the opposite of what many proposals and designs like
coinjoin try to achieve?
I think (hope?) that this idea doesn't have any chance of being
accepted by the Bitcoin community.

@_date: 2015-08-17 19:15:39
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Yes, people have been abusing his name in the block size debate to
present their own personal views, almost from the beginning, and that
has been very annoying.
But I don't remember you proposing to block their emails from the list
in those occasions.
For all I know this could have been the real Satoshi. But I just
maintain what I've said when arguments of authority (an old fallacy)
have been used: only the arguments matter, not who makes them (which
is also what logic says).
Maybe the people using the arguments of authority actually care about
whether the author is Satoshi or not to determine what they think
about what the content says.
But I personally don't care: I can say that I agree with what the post
says no matter if it is written by Satoshi or someone else (because
the identity of the author doesn't change what I think of the

@_date: 2015-08-17 21:39:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Pools could be somehow required to do p2pool between them, but there
would still be pools to further reduce variance, no?

@_date: 2015-08-19 10:59:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Separated bitcoin-consensus mailing list (was Re: 
Apparently that existed already: But technical people run away from noise while non-technical people
chase them wherever their voices sounds more loud.
One thing that I would like though, is separating Bitcoin
Core-specific development from general bips and consensus discussions.
I know, the bitcoin-consensus mailing list will probably still be
noisy, but at least we will have a non-noisy one and the ability to
say things like "Bitcoin Core's default policy is off-topic in
bitcoin-consensus" in the noisy one...
Also developers of alternative implementations may not be interested
in Bitcoin Core-specific things, so they may want to subscribe to
bitcoin-consensus and unsubscribe from bitcoin-dev.
I already told this to some people and everybody seemed to be positive
about this change, at most sometimes skeptics about the potential

@_date: 2015-08-19 11:24:11
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
Apart from classifying all potential consensus rule changes and
recommend a deployment path for each case, deploying an
uncontroversial hardfork is one of the main goals of bip99:
The uncontroversial hardfork doesn't need to change the maximum block
size: there's plenty of hardfork proposals out there, some of them
very well tested (like the proposed hardfork in bip99).
I disagree with this. I think it should be schedule at least a year
after it is deployed in the newest versions.
Maybe there's something special about June 2016 that I'm missing.

@_date: 2015-08-19 11:29:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
I would expect any uncontroversial hardfork to be deployed in testnet3
before it is deployed in bitcoin's main chain.
In any case, you can already do these tests using
Note that even if the new testchains are regtest-like (ie cheap proof
of work) you don't need to test them "in-a-box": you can run them from
many different places.
Rusty's test (  ) could have been
perfectly made using  it just didn't existed at the time.

@_date: 2015-08-19 11:34:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
Seems like 3 is something we want to do no matter what and therefore
is the "most future-proof" solution.
I wonder if I can help with that (and I know there's more people that
would be interested).
Where's the current "non-full" nVersion bits implementation?
Why implement a "non-full" version instead of going with the full
implementation directly?
On Wed, Aug 19, 2015 at 8:10 AM, Mark Friedenbach via bitcoin-dev

@_date: 2015-08-19 11:47:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
On Tue, Aug 18, 2015 at 11:46 AM, NxtChg via bitcoin-dev
Nobody has complained about Bitcoin-XT (nor libbitcoin, nor libcoin,
nor against any other of the multiple alternative implementations of
Please, understand that people are worried about the schism hardfork,
not about the software fork (which happened long ago when some of
Hearn's changes were reverted due to security concerns). If Bitcoin-XT
didn't had a schism hardfork, nobody would be calling it "an altcoin".
For consensus rules we use "the implementation is the specification"
as a principle for multiple reasons. By separating libconsensus (a
work in progress [far less progress than I would like]) we remove
Bitcoin Core's privileged position: Bitcoin Core wouldn't be "the
specification of the consensus rules" anymore, just a reference
implementation that is not "consensus-safer" compared to alternative
implementations (since they can use libconsensus directly [or a
software fork of it in the case of a reasonable schism hardfork]).
We have many reasons to fear schism hardforks (
), even though they may be unavoidable at some point (ie for an
ASIC-reset hardfork).

@_date: 2015-08-19 12:09:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
On Mon, Aug 17, 2015 at 1:02 AM, Cameron Garnham via bitcoin-dev
Some XTBTC advocates may sell all their BTC for XTBTC and viceversa.
But I'm afraid that what most currency speculators (thus most Bitcoin
holders) will do is just sell both all their BTC and XTBTC for fiat,
and wait for things to settle before deciding whether to re-enter or
This could result in both currencies' prices going down to 1 usd cent,
nobody knows.
Unfortunately it also puts Bitcoin core in an extremely weaker
position than it was before the Schism hardfork.
Even if XT fails in making blocks bigger, it may destroy Bitcoin.
That's probably not the goal of Bitcoin XT, but I don't think Andresen
and Hearn fully undesrtand the risks of a Schism hardfork (not to
mention their "followers" in the interwebs).
Since we want to discard the assumption that Hearn and Andresen want
to make Bitcoin centralized or destroy it, it's reasonable to conclude
that have serious misunderstandings on how the global consensus works.
This is consistent with some of their strong positions on Bitcoin Core
policy defaults (like maintaining the first seen spending-conflict
replacement policy [the dumbest possible one after "last seen"]
On Mon, Aug 17, 2015 at 2:33 PM, Eric Lombrozo via bitcoin-dev
Yes, it seems the simplest way to permanently separate your BTC from
your XTBTC is to move them all in transactions bigger than 1MB. You
may need too many outputs to increase the size (thus also hurting the
utxo size in Bitcoin XT), but that's just a side effect.

@_date: 2015-08-19 12:21:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Separated bitcoin-consensus mailing list (was Re: 
I don't disagree with anything you have said. But I think that having
a list specific to Bitcoin Core development will make defining the
"clear rules" easier.
Not sure if necessary but not opposed to this either.
As said, that list already exists, it's just that nobody uses it:

@_date: 2015-08-19 12:31:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
I don't think just using version=4 for cltv and friends would be a
problem if it wasn't for the XT/nonXT issue.

@_date: 2015-08-19 12:37:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I repeated my nit on On Mon, Aug 17, 2015 at 9:58 PM, Btc Drak via bitcoin-dev

@_date: 2015-08-19 12:53:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
One of your goals is "show the world that reaching consensus for a
Bitcoin hardfork is possible", right?
BIP99 can achieve that goal without touching the block size (thus
probably less controversial).
The data can be collected using testchains. See That makes the data less interesting, doesn't it?
It wouldn't defeat the "show the world that reaching consensus for a
Bitcoin hardfork is possible" objective though. But again, we don't
need to touch the block size to achieve that goal.
But that wouldn't be uncontroversial (unless it was accompanied with
data that somehow quantifies the risks, in which case maybe another
bigger size is acceptable).
Yes, emergency hardforks are a different case as explained in BIP99's draft.
In any case, " we all agree that some kind of block size hardfork will
happen on June 2016" it's clearly false since I can find many
counter-examples besides myself.
I don't think giving 1 year for "everybody in the world" to upgrade is
an "ultra-long" period.
I'm proposing 5 years for the hardfork proposed in bip99.
I do think softforks are safer than hardforks, that doesn't mean
softforks don't have serious risks as well.
I disagree with this conclusion as well (it doesn't follow from the
first sentence, non sequitur).

@_date: 2015-08-19 13:06:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
XT it's just a software fork.
BIP101 (as currently implemented in Bitcoin XT) is a Schism hardfork
(or an altcoin), but BIP101 could be modified to be deployed like an
uncontroversial hardfork (in current bip99's draft, a given height
plus 95% mining upgrade confirmation after that).
I'm not defending the Schism hardfork being proposed. I am very
worried about it and I have publicly said so several times.
If Bitcoin XT didn't contained the Schism bip101 hardfork I wouldn't
be so worried: users are free to use software that is less reviewed at
their own risk.
It's users and not miners who decide the consensus rules.
You may want to read BIP99 to understand that I know this, but still
think that Schism hardforks may be necessary in some situations (I
don't think this one is reasonable though).
All I'm saying is that Bitcoin XT the software fork is totally fine
(like other alternative Bitcoin implementations). The big problem is
BIP101 being deployed as a Schism hardfork.

@_date: 2015-08-19 17:25:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
And I'm not trying to be a dumdum (whatever that is) when saying that
Bitcoin XT the softfork and BIP101 implemented as a schism hardfork in
Bitcoin XT are different things. One existed before the other. And if
there wasn't a schism hardfork in the making there wouldn't be any
warning about Bitcoin XT in bitcoin.org because Bitcoin XT implemented
the same consensus rules (and I think was largely ignored).
When we say "Bitcoin XT is bad" some users read "Bitcoin Core devs
think any code fork to Bitcoin Core is back because they lose control
over it".
The second is not the case: nobody complained or cared about Bitcoin
XT when it implemented the same consensus rules.
I'm not sure I will learn anything by reading this link (I didn't
reading the previous link).
Have you read BIP99 already? Can you tell me where you disagree with
what's in BIP99 in one of its 2 threads?
Again, I'm against this Schism hardfork but maybe I'm in favor of an
Schism hardfork in the future, I don't know.
We're both against the Schism hardfork but somehow you think I'm in
favor and are trying to change my mind.
What are we even discussing about?
Right now it shouldn't be downloaded because it contains this quite
irrational Schism hardfork.
When it was only a not-up-to-date-bitcoin/master + the commits
(non-consensus changes) Hearn would like to see in Bitcoin Core nobody
was specially worried about it.
So what are we discussing about?

@_date: 2015-08-19 19:28:38
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I think calling it "miner vote" was the first mistake: miner's
shouldn't have a "voting power" the rest of the users lack.
I prefer to call it "miner upgrade confirmation" and in BIP99 the
recommendation is to use 95% for both uncontroversial softforks and
uncontroversial hardforks (the uncontroversial harforks also have a
minimum height before starting the miner confirmation/voting to give
users additional time to upgrade).
To me it's no different that the mechanism is used for uncontroversial
softforks or hardforks, the main question is that it is NOT a "miners'
If you expect everyone (including all miners) to upgrade, I don't
think any less than 95% makes sense. On the other hand, 100% makes it
relatively cheap for an attacker to block uncontroversial consensus
For a Schism hardfork, bip99 doesn't recommend to use miner's
confirmation/vote at all. Miners could be against the change, for
example in an ASIC-reset Schism hardfork or in a "hardfork" (it cannot
be a softfork if miners oppose to it) to reduce the block size), but
that shouldn't stop the hardforkers if they think dividing the
currency in 2 is the best solution to whatever is the problem at hand
(which I don't think it's the case now).
Of course, BIP99 is still a draft and can still be changed. But I
would really like that we focused on "how to do hardforks in general"
first and only then focus on how to make a blocksize hardfork

@_date: 2015-08-19 20:22:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I don't know the timeline for this, but maybe he was referring to
(where he is one of the few people that have participated).

@_date: 2015-08-19 20:30:31
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Introduce N testnet chains to test different 
I don't have the time to use the code to write tests and simulations
myself right now but I would be really happy about someone else doing
Even though they share the same port and magic numbers, each of the N
testchains in  has a different genesis block, so they will reject
blocks from any other testchain from the start.
But you will likely connect the nodes directly and manually to get the
network topology you want to test anyway.
I hope this answers your questions but I'm happy to answer any other
questions you may have.

@_date: 2015-08-19 20:33:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
I have (hopefully) answered your questions in the other thread.
Review/testing of  would be also appreciated (to hopefully
eventually merge the full  branch into bitcoin/master).

@_date: 2015-08-19 21:58:55
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
On Wed, Aug 19, 2015 at 9:48 PM, Eric Lombrozo via bitcoin-dev
And this is precisely why we should make perfectly clear that we're
not against a code fork where Hearn or anyone else acts as a
"benevolent dictator", just against the controversial hardfork it is
attempting to deploy.
Otherwise the PR battle is probably lost (which may mean users sell
all their BTC for XTBTC [or just forget about their BTC and only care
about their XTBTC]).

@_date: 2015-08-19 23:32:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
That policy code should be simple to change, but thank you for pointing it out.
Also thank you for declaring your position (indifference) on the subject.

@_date: 2015-08-20 00:00:09
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I think that effort is in progress (again, much slower that I would
like it to be) and it's called libconsensus.
Once we have libconsensus Bitcoin Core it's just another
implementation (even if it is the reference one) and it's not "the
specification of the consensus rules" which is a "privileged" position
that brings all sorts of misunderstandings and problems (the block
size debate is just one example).

@_date: 2015-08-20 00:28:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
I think everyone is in that position (we just don't have enough data
about the proposed sizes) or it's just too optimistic.
Bitcoin XT is just a software fork and nobody seem to have a problem
with that (as repeated in other threads), people are worried about the
way bip101 is going to be attempted to be deployed using Bitcoin XT.
We already have more than 5000 software forks and that's totally fine.
A Schism fork may not kill Bitcoin but it will certainly create 2
different coins.
The claim that "there will be a winner and everybody will just move
there" is incredibly naive and uninformative.
Many people will sell their xtbtc and reject the hardfork
independently of its support by miners.
Nobody knows what the result will be, but both currencies' prices
dropping near zero is certainly a possibility that Gavin and Mike are
not aware about or are not informing their followers about.
Here's something a little bit longer about this topic:
Note the last part:
+This is very disruptive and hopefully will never be needed. But if
+it's needed the best deployment path is just to activate the rule
+changes after certain block height in the future. On the other hand,
+it is healthy decentralization-wise that many independent software
+projects are ready to deploy a schism hardfork.
Again, no problem with the code fork, but the Schism hardfork is very
risky regardless of their intentions.
If they don't extensively lobby Bitcoin companies, they don't start a
massive PR campaign labbeling other developers as "obstructionists"
and don't misinform a big part of the Bitcoin users (often using
logical fallacies, intentionally or not), probably those 5 new
currencies will be ignored and nothing bad will happen.
Unfortunately in this case a great division between users is being created.
Can you please stop conflating "Bitcoin Core as a project" and
"Bitcoin consensus rules".
They are different things and nobody is or can be "in charge" of the
later, face it.
Can you please also stop conflating software fork and
"Schism/controversial/contentious hardfork"? Nobody has anything
against the former and as you point out it is allowed by its free
software license.
Why should miners have a voting power that the rest of the users lack?
All this sounds reasonable.
But you cannot know this will happen this way!
If the threshold is reached (let's forget about noXT for now), the
remaining miners cannot be forced to adopt bip101.
And users can never be forced to adopt hardforks.
It is possible that 75% of the hashrate moves to the bip101 chain
while 99% of the users remain in the old Bitcoin chain. Or 50/50,
40/60...nobody knows.
No, it doesn't rely on that. It just relies on the majority of the
miners not attacking the network for too long (ie the number of
confirmations people are waiting).
If I'm running a full node, I'm not isolated from the network and the
majority of the hashrate is not reorging the chain I am safe no matter
how dishonest the "majority" (whatever that means in this context) is.

@_date: 2015-08-20 01:13:59
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] bitcoin-dev list etiquette 
On Thu, Aug 20, 2015 at 12:25 AM, Gary Mulder via bitcoin-dev
The potential impacts of Schism/controversial/contentious hardforks
are shortly covered in
It is still a BIP draft so improvements are welcomed.

@_date: 2015-08-20 01:27:51
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
No, as previously explained, once libconsensus is complete it can be
moved to a separate repository like libsecp256k1.
At first it will need to be a subtree/subrepository of Bitcoin Core
(like libsecp256k1 currently is), but I still don't undesrtand how
that can possibly be a problem for alternative implementations (they
can use a subtree as well if they want to). Depending on a separated
libconsensus doesn't "make Bitcoin Core a dependency" more than
depending on libsecp256k1 currently does.
I believe the simplest option would be to fork the libconsensus
project and do the schism/controversial/contentious hardfork there.
But of course modifying libconsensus will be much easier than
modifying Bitcoin Core (if anything, because the amount of code is
much smaller).
Unfortunately I only directly contacted libbitcoin because I was
subscribed to the list at the time (maybe I'm still subscribed, not
really sure).
The other attempts to get feedback from other alternative
implementations have been just mostly-ignored threads in bitcoin-dev.
So, no, I cannot facilitate such a discussion, but I'm more than happy
to collaborate to achieve our mutual goal.

@_date: 2015-08-20 01:56:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
By the way, now that I remember why I subscribed to the libbitcoin
list I want to share it with you.
I met Amir Taaki in person in a spanish hackmeeting and had the chance
to talk a lot with him, very interesting person whose input in this
blocksize matter I would greatly appreciate. He explained some of his
concerns with Bitcoin Core (Bitcoin-qt at the time) and he
specifically named 2 persons: Mike Hearn and Gavin Andresen. If I
remember correctly, Hearn had recently proposed a blacklisting scheme
for Bitcoin.
I remember I said something along the lines:
"Mike Hearn has certainly proposed some nasty things but I don't think
other devs will ever accept that kind of changes in Bitcoin-qt.
Regarding Gavin, I believe he is someone that can be trusted even if
he visited the CIA. If anything, I think he is overly conservative
about some changes, but that's very understandable given how fragile
Bitcoin is (specially at this early stage)".
Looking back, I now realize that his concerns were not exaggerated at
all and I was clearly wrong thinking Gavin was overly conservative.
He was also worried about the payment protocol and we agreed to
disagree there (maybe I should read all the payment protocol stuff
more deeply).
I don't want this to be taken as an argument of authority "Mike and
Gavin cannot be trusted because Amir didn't trust them", just as a
curious anecdote.
Amir, I wouldn't like to put words in your mouth: that's why I cc'ed
you so you can correct me in case my memory is failing.

@_date: 2015-08-20 02:14:28
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Separated bitcoin-consensus mailing list (was Re: 
On Thu, Aug 20, 2015 at 1:44 AM, NxtChg via bitcoin-dev
Certainly I have talked to much this month, my apologies.
I believe most of my posts (if not all) were on-topic but I could
still had repeated myself much less.
I've been trying to concentrate my usual points in documents or
threads that I can link to so that my comments can be shorter.
But, yes, most of my posts have been related to general consensus
topics and not specific to Bitcoin Core development (that's part of
why I think the bitcoin-consensus and bitcoin-dev lists would be a
good separation).
In any case, my apologies for this unplanned record.

@_date: 2015-08-20 02:53:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
Moving it here from the other thread.
Yes, unfortunately I don't see this happening any time soon either, at
least not with the amount of review I'm getting.
My initial hope was to complete libconsensus by 0.12 (one year should
be enough time, right?) but I was being too optimistic.
By "wait for it" I assume you mean waiting for libconsensus to be
complete before we separate it to a different repository.
The reason is just simplicity.
Mhmm, not sure I understand this point.
Yes, you need to operate as if it can happen at any time. I now
understandbetter  your position of having your own repository until a
complete libconsensus is separated.
In the meantime you will have to keep using your re-implementation of
the rest of the consensus rules (besides the script checks), but
fortunately the most risky and harder reimplementation is the part of
the script validation.
Yes, I want a separated repository. I just wanted to start with a
separated folder first. Right now there's consensus code all over the
place, specially in main.cpp.
I think changing the order (separated repository first, moving code
from Bitcoin Core to libconsensus later) would increase the total
amount of work.
Here's another option that has recently crossed my mind:
1) Finish the libconsensus separation in an independent branch on top
of a given version, for example 0.11.
2) Separate a repository from that. Alternative implementations can
start using a full libconsensus
3) Rebase that branch on top of bitcoin/master and start to PR small
groups of commits. Once the whole branch has been merged, Bitcoin Core
can replace the consensus folder with the libconsensus subtree, so
that Bitcoin Core itself can start using a full libconsensus.
Ironically with this plan Bitcoin Core may not be the full node first
implementation to use a full libconsensus.
There will be some consensus fork bug risks during 3 (which at the
current speed I estimate it could easily take 3 or 4 years) and there
would be some redundant work (replicating every consensus change in
both Bitcoin Core and libconsensus).
On the bright side, we may be able to have a full libconsensus this
year (which was my goal after we exposed VerifyScript in the first

@_date: 2015-08-20 03:17:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I should have posted that just on libbitcoin but not in bitcoin-dev.
My apologies.

@_date: 2015-08-20 09:31:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
For the 73th time or so this month on this list:
The maximum block size consensus rule limits mining centralization
(which is currently pretty bad).
But don't worry about not being an authority on the subject: Gavin
(who has written extensively on the subject) doesn't seem to
understand this either.
He thinks it only limits full node centralization (by limiting how
expensive it can be to run a full node).
I thought the later would be quite obvious for everyone, but this
month I've discovered that I've been extremely optimistic about
people's understanding of the effects of the consensus rule they want
to change.
For the later reason (the one Gavin and I agree on) there's an old but
very clear video explaining it:

@_date: 2015-08-20 10:06:28
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
It wasn't just me: I didn't had the idea of creating a libconsensus
with a C API (thank Matt Corallo for that), I didn't removed all the
undesired dependencies or prepared the building part (thank Cory
Fields) and also thank at least Wladimir and Pieter who also
contributed in some ways I don't remember.
And of course also thank all the reviewers that made the PR merges possible.
I'm really happy to hear that libconsensus is being used, thank you
for your effort there too.
I don't understand what you mean by "quality" in this context. One of
the goals is to have as little dependencies as possible (so "more
modern tool sets" may not be suitable for libconsensus). libsecp256k1
will keep on being a dependency (highly optimized C code) and that's
about it.
Ideally I would like to slowly move libconsensus from C++ to C too,
but it seems other people would prefer to move to C++11 instead.
Yes, they are simpler and thus there's less risks of consensus fork
bugs, but it still exists.
It is true that the consensus code is currently spread all around
(specially in main.cpp), but completing libconsensus would solve that.
Lastly, since for consensus rules "the code is the specification", it
is unfortunate that the specification is coupled with a concrete
implementation (Bitcoin Core) and we should fix that.
But the goal is not reimplementing the consensus rules but rather
extract them from Bitcoin Core so that nobody needs to re-implement
them again.
It is not only exposing it but also separating it from Bitcoin Core so
that they can be changed without having to also change/take into
account non-consensus Bitcoin Core specific things.
A single PR would certainly be unacceptable, I was making many little
more acceptable ones (some of them already merged):
* [1/9] Consensus
** MERGED or DELETED
*** MERGED Consensus: Decouple pow from chainparams  [consensuspow]
*** MERGED MOVEONLY: Move constants and globals to consensus.h *** DELETED Refactor: Create CCoinsViewEfficient interface for
CCoinsViewCache  [coins]
*** MERGED Chainparams: Refactor: Decouple IsSuperMajority from
Params()  [params_consensus]
*** MERGED Remove redundant getter
CChainParams::SubsidyHalvingInterval()  [params_subsidy]
*** MERGED Separate CValidationState from main  [consensus]
*** DELETED Consensus: Refactor: Separate CheckFinalTx from
main::IsFinalTx  [consensus_finaltx]
*** MERGED Consensus: Decouple ContextualCheckBlockHeader from
checkpoints  [consensus_checkpoints]
*** MERGED Separate Consensus::CheckTxInputs and GetSpendHeight in
CheckInputs  [consensus_inputs]
*** MERGED Bugfix: Don't check the genesis block header before
accepting it  [5975-quick-fix]
** REBASE Chainparams: Explicit Consensus::Params arg in consensus
functions  [params_consensus2]
** REBASE Optimizations: Consensus: In AcceptToMemoryPool,
ConnectBlock, and CreateNewBlock  [consensus-txinputs-0.12.99]
** REBASE MOVEONLY: Move most of consensus functions (pre-block) ** REBASE Consensus: Refactor: Turn CBlockIndex::GetMedianTimePast
into independent function  [consensus_mediantime]
** DEPENDENT Consensus: Refactor: Consensus version of
CheckBlockHeader()  [consensus_checkblockheader]
** DEPENDENT Consensus: Consensus version of pow functions [consensus_pow2]
** DEPENDENT API: Expose bitcoinconsensus_verify_header() in
libconsensus  [consensus_header]
** DEPENDENT API: Expose bitcoinconsensus_verify_block() in
libconsensus  [consensus_tip]

@_date: 2015-08-21 21:46:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
But you don't want something better, you want something functionally identical.
You may want to watch sipa's explanation on why "the implementation is
the specification" and the reasons to separate libconsensus:
Since you already depend on libconsensus for VerifyScript, wouldn't it
be nice that it also offered VerifyTx, VerifyHeader and VerifyBlock?
You would still have complete control over storage, concurrency,
networking, policy...
My plan is for the C API to interface with the external storage by
passing a function pointer to it.

@_date: 2015-08-21 22:09:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
The rule is already there. My goal is to make sure we understand the
potential consequences of changing that rule in the "less limitation
to mining centralization" better before changing it.

@_date: 2015-08-21 22:28:31
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
On Thu, Aug 20, 2015 at 12:23 PM, Milly Bitcoin via bitcoin-dev
Please start with the centralization metrics we both agree are
necessary instead of keeping insulting me publicly and privately.
I'm not inventing this, he recently said so himself publicly on this
mailing list:
"I don't believe that the maximum block size has much at all to do with
mining centralization"
It is therefore not surprising that non-developers and developers with
less experience in Bitcoin than Gavin have similar misunderstandings.
That claim seems in contradiction with his earlier analysis:
"I ran some simulations, and if blocks take 20 seconds to propagate, a
network with a miner that has 30% of the hashing power will get 30.3%
of the blocks."
That's why I was surprised when he denied the relation between the
consensus maximum size and mining centralization, but hey, people
change their minds and that's completely fine. I change my mind about
many things quite often myself. For example, I will change my mind
about not touching the maximum blocksize consensus rule as soon as I
see some data that convinces that the proposed sizes are not very

@_date: 2015-08-22 05:21:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
Don't you mean profits instead of revenue?
On Aug 21, 2015 5:01 PM, "Peter Todd via bitcoin-dev" <

@_date: 2015-08-23 08:40:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size possible solution - to set minimum size 
A minimum block size does nothing to prevent the problems that come
from schism hardforks.
But also a minimum block size can be trivially cheated as recently
explained on this list:
"[...] miners can just pay to themselves to follow the minimum size
block rule without risking anything.
As long as they have a single matured satoshi they can just pay to
themselves with it as many times as they need in the same block."
It is good to search previous post before proposing or asking
something (it could have been proposed/asked earlier):
On Sun, Aug 23, 2015 at 1:30 AM, Bdimych Bdimych via bitcoin-dev

@_date: 2015-08-23 08:48:38
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Censorship 
This is off-topic here.
Consensus critical changes (those aren't changes that Bitcoin Core
developers can make unilaterally against the will of alternative
implementations or users) and Bitcoin Core development are independent
from bitcointalk.org and /r/bitcoin.
And as you say, people can create competing without moderation or
"censorship" (ie /r/bitcoin_uncensored or bitcontalkuncensred.org
On Sat, Aug 22, 2015 at 3:39 PM, David Vorick via bitcoin-dev

@_date: 2015-08-23 08:51:12
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 104: Replace Transaction Fees with Data, 
Again, did you got a bip number asigned or did you self-assigned it yourself?
On Sat, Aug 22, 2015 at 1:01 PM, Ahmed Zsales via bitcoin-dev

@_date: 2015-08-24 04:23:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
On Mon, Aug 24, 2015 at 3:01 AM, Gregory Maxwell via bitcoin-dev
No, I don't think anybody thought about this. I just explained this to
Pieter using "for example, 10 instead of 1".
He suggested 600 increments so that it is more similar to timestamps.

@_date: 2015-08-24 04:27:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
On Mon, Aug 24, 2015 at 1:41 AM, Tom Harding via bitcoin-dev
As far as I know, "his conclusions" were that there was an effect,
while suspending judgement on whether that effect was high enough to
be important for a given size or not.

@_date: 2015-08-27 02:48:42
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Unlimited Max Blocksize (reprise) 
Maybe this helps undesrtanding the risks of a contentious/schism
hardfork: That section can be greatly improved though.
I have only skimmed the document, but I believe its conclusions (or
some of them) are right for the current propagation code.
The assumption may not stand if we move to something like IBLT though.
I believe that document also proves that it is
irrational/noncompetitive for miners to include ANY free transactions
at all (like they currently do).
But the analysis is about the effect of the maximum block size on
fees: there's more effects of that consensus rule.
The most important one being that it limits mining centralization (and
centralization in general).
This is true for at least 2 reasons: one is related to block
propagation and it is what is usually described.
At a bigger scale (even with crazy assumptions like constant time
infinite bandwidth, instant [superluminal] communication, zkSNARKS
block validity proofs ) minimum CPU costs will be something to limit
through the maximum block size consensus rule. There's a scale at
which the minimum CPU costs for a miner to be competitive may be so
high that some small miners without the resources to meet that minimum
will become unprofitable.
Admittedly we're not near that scale yet, but if something to take into account.
There are some simulations. See:
The goal of  is to allow
people to do more realistic simulations (by using real full nodes).
That doesn't mean that more simplified simulations are worthless, but
I didn't want people to have to create their own testchain every time
they want to simulate a different size, like rusty had to do for:
So the "incapacity to quantify the advantages that large miners have
over smaller ones" doesn't really exist.
It would be nice to have more data about this (more sizes, more
network topologies, etc) though.
Although smaller subsidies will remove some problems we currently
have, for example, SPV mining (there's no incentive to SPV mine
worthless empty blocks), I don't understand your claim that they will
also solve mining centralization problems related to block
I really dislike basing the consensus rules on predictions about
future technology. For all I know, a terrible war could destroy half
of the global internet infrastructure in the next 5 years.
I prefer simpler increments like in bip102 (although I don't have the
data to know if 2MB is safe mining-centralization-wise at this point
[when mining centralization is pretty bad]).
Arguments against that kind of change are usually along the lines
"then we will have to repeat this same discussion in 1 or 2 years".
I believe that with the proper simulation tools being deployed and a
better general understanding of what the concerns are, the
conversation should be much simpler the next time.
Actually some simulations show they in fact have incentive to do just
that in some cases.
But more importantly, we shouldn't assume that all attackers are
rational miners. Maybe a potential attacker is a secret service or a
financial cartel attempting to destroy Bitcoin for whatever reason.
I truly hope that the discussion can move forward into more productive
territories after the workshop, and I'm particularly interested in
Peter R's presentation, even if I haven't found the time to read his
paper yet. Even if fees are not the main reason why we want to have a
block size maximum, fees are certainly very relevant and anything that
he has mathematically proven in that regard will be useful to this

@_date: 2015-08-28 22:45:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Uniquely identifying forked chains 
On Fri, Aug 28, 2015 at 10:15 PM, Matt Whitlock via bitcoin-dev
If it's a new chain, we're talking about a "spinoffs"
Yes, this seems like the best solution in the schism hardfork case.
What both sides of a schism hardfork would want is to avoid hurting
bystander users who can't tell the difference between the old and the
new currency/chain.
I should extend BIP99's section on schism hardforks.
Anybody else is welcomed to propose changes to the BIP draft, just PR
to this branch:

@_date: 2015-08-29 01:44:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
On Sat, Aug 29, 2015 at 1:36 AM, Btc Drak via bitcoin-dev
Can we please not discuss an ideal deployment mechanism in 4+
different proposals and discuss the same deployment mechanism (for all
proposals) in BIP99's thread instead?

@_date: 2015-08-29 02:00:11
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
On Sat, Aug 29, 2015 at 1:38 AM, Mark Friedenbach via bitcoin-dev
I realize now that this is not what Greg Maxwell proposed (aka
flexcap): this is just miner's voting on block size but paying with
higher difficulty when they vote for bigger blocks.
As I said several times in other places, miners should not decide on
the consensus rule to limit mining centralization.
People keep talking about miners voting on the block size or
"softforking the size down if we went too far". But what if the
hashing majority is perfectly fine with the mining centralization at
that point in time?
Then a softfork won't be useful and we're talking about an "anti-miner
fork" (see and  I believe miner's voting on the rule to limit mining centralization is
a terrible idea.
It sounds as bad as letting pharma companies write the regulations on
new drugs safety, letting big food chains deciding on minimum food
controls or car manufacturers deciding on indirect taxes for fuel.
That's why I dislike both this proposal and BIP100.

@_date: 2015-08-29 19:30:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
On Aug 29, 2015 9:43 AM, "Daniele Pinna via bitcoin-dev" <
to lag behind on a lot of work. I apologize for typos which I may not have
seen. I stand by for any comments the community may have and look forward
to reigniting consideration of a block size scaling proposal (BIP101)
which, due to the XT fork drama, I believe has been placed hastily and
undeservedly on the chopping block.
I don't like relying on exponential growth (that's why I don't like neither
Gavin's 101 nor Pieter's 103).
But I don't think it's too late to turn bip101 into just another proposal
for an uncontroversial hardfork (changing the 75% to 95% would be the first
step) and xt into just another software fork.
My favorite one so far is bip102 (even though I still consider "2mb now"
arbitrary and I'm worried about making mining centralization even worse
than it is now), but if it was framed as a schism hardfork like bip101 I
would also warn about the dangers of a schism hardfork for it.
I'll read it to try to understand your claims. Are you presentung this in
the scaling workshop?

@_date: 2015-08-29 22:10:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
On Sat, Aug 29, 2015 at 9:01 PM, Matt Whitlock via bitcoin-dev
I would really prefer chain= over network=
By chainID I mean the hash of the genesis block, see
I'm completely fine with doing that using an optional parameter (for
backwards compatibility).
I agree with Andreas Schildbach that respecting the most commonly used
schemes is desirable.
So my preference would be:
(a tx in testnet)
(a block in bitcoin's mainnet)

@_date: 2015-08-29 22:41:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
Then I only care about the hard cap (for example, to me bip100 is
practically equivalent to just raise the limit to 32 MB directly).
Miners can always produce smaller blocks by modifying their local policy.
So if we need a maximum that cannot be altered by miners anyway, why
take the additional complexity of miners voting on a lower and
changing maximum size?
How expensive it is depends on the concrete function f(extra_nBits) =
But the goal of that proposal is not to raise the size maximum
permanently, but rather temporarily allow bigger blocks when there are
spikes in demand (ie many fees to collect in unconfirmed
Yes miners will ask that question to themselves, and the answer will
depend on the concrete function and on the fees of those extra
The miner paying for the costs will get the gains: no tragedy of the
commons here.
I believe the tragedy of the commons actually happens with your
proposal. Why would I pay alone for something that benefits all
This seems to solve the tragedy of the commons problem with your
current proposal.
It would be like flexcap but instead of the change in size being
temporary, it affects the next maximum size permanently.
One thing to worry about is miners filling blocks with
pay-to-themselves garbage to avoid reducing the size when they don't
have enough attractive transactions to include (ie it may not be free
for the network for miners to vote on "maintain current size").
I still don't see the point in having a lower moving size maximum.
If 8 MB is mining-centralization-safe, let's move directly to 8 MB
without adding this seemingly useless extra complexity.
If it's not, mining voting on a lower moving maximum won't make it safer.
Once we have more objective tools (centralization metrics, simulators,
etc...) to determine whether or not a block size is
mining-centralization-safe for a given point in time (looking at
current centralization and current technology available), I don't see
the problem with repeating the equivalent of bip102 periodically
(every 2 years?) to adapt the size to better technology or lower
mining centralization.
It would be also helpful to have a tool to somehow measure "size
increase urgency" (ie right now free transactions get mined and blocks
aren't full or close to be full, I don't think the current general
sense of urgency on this matter is justified).
With all respect, I believe bip100 and this proposal are
over-engineering; and bip101 and bip103 (pieter's) are
overly-optimistic (in their exponential technological growth

@_date: 2015-08-29 22:59:27
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Variable Block Size Proposal 
O(1) propagation can never be achieved without a centralized topology.
No matter how efficient of an IBLT (or similar) we implement, O(1) can
only be achieved for transmitting a block between 2 nodes. The
propagation time of a block across the whole network (or even to the
miner's sub-network) will always depend on the concrete network
That's not to say that transmitting a block between to peers in
constant time wouldn't greatly help with mining centralization
concerns related the maximum block size, but I'm concerned about this
incorrect "O(1) block propagation" meme keeps spreading.
Even with ansibles [1] and safe zk-SNARKS [2] for constant time block
validation (somehow removing the trusted setup), both of which are
science fiction right now you need to verify the snark proof for every
node receiving and relaying the block.
At that point block propagation would be meaningless as a
miner-centralization concern for not raising the maximum block size
though: the minimum CPU costs for being able to mine profitably would
be the next concern or "bottleneck".
I completely agree with the minimum block size being inappropriate
though. I don't even believe that the stated goals of the size minimum
can be accomplished with it.
[1] [2]

@_date: 2015-08-29 23:21:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [Bitcoin-development] [BIP draft] Motivation and 
On Wed, Aug 26, 2015 at 1:20 AM, Andy Chase via bitcoin-dev
As specified in BIP001, there's an optional field to link to the
discussion on the mailing list, which in this case links to this
thread (that's why I'm replying here):
You are probably right, but that is too vague for me to take any action.
Can you propose something more concrete as a PR to my branch?
Can you explain why?
I think they're helpful as examples for the explanations (even though
the concrete texts can probably improved/summarized).
I'm not sure I understand this.
What do you mean by "code forks"?
If you mean "software fork" (like libcoin or bitcoin xt
[pre-controversial-bip101]) those are completely fine and out of scope
for this BIP, since they don't require coordination by the different
users/implementations to upgrade/re-implement the consensus changes.
I don't think the spell checking had been followed at all for this or
any other BIP, but yes, Greg assigned the number 99 (he did so
privately instead of here on this thread, which I find very annoying
because you are the second person who complains about this).
That is a good question. The proposal currently says "informational |
process": But I wasn't really convinced about this so I'm happy to change it to
whatever it's more appropriate.
The contained code is an example of an uncontroversial hardfork to
create a precedent. I'm not sure I understand your proposal for a
"patch into core as a module that hard forks can use in the future".
Can you elaborate what would go in that patch?

@_date: 2015-08-30 00:08:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
In fact I have been accused in the past (by at least Peter Todd) of
having "too many cases in mind" or "doing refactors that are good for
That's why I'm very cautious about proposing changes that are not
strict improvments in maintainability to bitcoin itself.
But I actually have freicoin, sidechains and private chains (defined
in freimarkets, used in elements alpha as "block signing") in mind.
Some of the consensus changes I have in mind are support for multiple
assets or interest-bearing assets, for example.
But if you need to change the consensus rules you need to change the
code, there's no way around that.
It will be much simpler to only adapt libconsensus to other chains
than it is to adapt the whole Bitcoin Core code base.
Libconsensus can free you from the need of running "border routers"
(which you need to adapt if you depend on them and are supporting
chains with different rules).
When libconsensus has it's own independent repository, will I fork the
project to have a multi-consensus library supporting multiple
different chains (apart from bitcoin and its testchains)? Maybe, I'm
not sure it makes sense, maybe it's just simpler to maintain a
different project for each different chain (ie libfreicoinconsensus,
libbetaconsensus, etc).
That is very sad to hear. The main reason to integrate libconsensus is
to avoid consensus fork bugs (or to not depend on the "border routers"
to avoid those bugs).
That's out of scope for libconsensus which will be stateless and whose
only API would be in C.
But the refactors in Bitcoin Core will hopefully make it easier to
support such a minimal node in it (you know you can "./configure
--disable-wallet --without-gui" already, right?, about RPC, that's the
remaining API!).
Yes, and the wheel it's an invention used in pre-historic times: that
doesn't make it less useful.
Do you have any other suggestion for interfacing with external storage
using a C API?

@_date: 2015-08-30 01:25:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
I completely agree and I share your frustration.
The importance of modularization is often disregarded but in my
opinion it has a deep positive impact in the long term: more people
are able to contribute with code and review (in the areas they know
better), the risks associated with each change become more clear
(there was a time when almost any change implied consensus risks),
more alternative code bases can be implemented on top of the basic
ones without fear of consensus bugs, etc.
When I first read some of the code in 2011, I concluded that almost
everything was in main.cpp (which I found ridiculous from a software
engineering perspective). When I started to contribute with code in
2014, main.cpp was still (and still is in my opinion) giant, but the
modularization had greatly improved thanks to changes like moving the
serialization code out of main (thank you very much for that). We
still have a lot of work ahead, but we've certainly advanced a lot.
Unfortunately we cannot force reviewers to pay more attention to
modularization PRs, many of them are usually more interested in
changes that add or remove functionality in the short term. This
problem gets exacerbated when modularization changes are required to
be done in small increments to make them more easily reviewable and
less disruptive to other open PRs, since it's harder for people to see
the big picture and the rationale for those small changes (that often
don't hcange functionality or performance at all).
I know we are not alone on this and people like Wladimir, Pieter, Cory
and Jonas Schnelli (at least, probably more people do) deeply care
about modularization, even if I subjectively and selfishly interpret
the lack of review on some of my PRs as a symptom of the opposite.
So I suggest that people who think this is a high priority join and
review each other's PRs on the subject.
Currently I focus on 3 modularization areas:
1) Chainparams: supporting multiple chains (ie multiple testchains is
all what Bitcoin cares about) is a great goal but there's still many
barriers to create a new testchain. I started this work with but even after  there are still more things to do.
2) Consensus: separating the consensus code, Matt Corallo had the idea
of also exposing it in libconsensus. I started with  the latest
things I still have open are  and  please review.
3) Policy: separate node local policy code. Luke Dashjr started with
 I started with  (after several failed attempts), the next
little step blocking many other changes I have ready for way too long
is  ( also helps), please review.
I know Jonas Schnelli is focusing on the wallet. Cory Fields has
recently focused on checkpoints and chainparams.
Now that I know that you also care about modularization I will ask you
for review as well, I hope not to be too annoying like I've been with
Wladimir and Cory some times (and I usually am with some of my
coworkers at blockstream). Please do the same with me: point me to any
modularization PR you have opened.
Regarding your next post, I agree that an additional "Layer" field in
BIPs could be useful. Maybe you should start a BIP for that?

@_date: 2015-08-30 01:30:39
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
I plan to replicate the RPC API (or a subset of it) using ZMQ's
req/rep pattern, but  comes first.
Well, the RPC is the API. For libconsensus, its C API is the API.
We've been talking about separating the wallet and qt to a different
repository for long, but modularization is a prerequisite.

@_date: 2015-08-30 01:37:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Splitting BIPs 
Concept ACK. As suggested in the other thread, maybe it is worth to
start a new BIP draft for this?
On Thu, Aug 27, 2015 at 10:51 PM, Eric Lombrozo via bitcoin-dev

@_date: 2015-08-30 04:20:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
On Aug 29, 2015 7:02 PM, "Chun Wang via bitcoin-dev" <
That's obviously a design mistake in FTC, but it's not unsolvable. FTC
could move their genesis block to the next block (or the first one that is
not identical to LTC's).
Bitcoin and all its test chains have different genesis blocks, so I'm not
sure FTC should be a concern for a BIP anyway...

@_date: 2015-08-30 20:56:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
No, I'm not making such assumption. I'm focusing on what they CAN do,
while suspending judgement on their good will and not trying to
predict their future behavior from historic behaviour.
With 60% of the hashrate, you can easily get 100% by orphaning
everybody else's blocks. More importantly, being under the same
jurisdiction they can be forced to behave in certain way (for example,
censor transactions) by law.
I'm very worried about the current situation no matter how benevolent
current miners are. Thus weakening the only limit to mining
centralization that we have at the consensus rule level seems
extremely risky at this point.
My point is, a "soft cap" determined by miners clearly doesn't protect
us from mining centralization: the "hard cap" does.
Knowing that, and given that miners can currently set their own policy
block size maximum, what does this "voting on a lower limit" achieve?
What are the gains? Why are we "lucky" if they keep the lower one as
low as possible?
Thank you for admitting it is not urgent!
I suggested 5 years for the concrete hardfork in bip99 because it's
clearly non-urgent and I wanted to be very conservative. I'm happy to
reduce that to say, 1 year (specially given that the change is very
simple to implement).
For a simple block size change (like, say bip102) 1 year (maybe 6
months + miner's confirmation) is probably more than enough as well.
And we can always deploy an urgency hardfork if it is necessary.
Fortunately we haven't been discussing this for 5 years, I don't know
where you get that from.
A schism fork it's certainly always a possibility but I would only
consider it after an urgency hardfork (once the issue becomes urgent)
fails due to not being uncontroversial.
Would you agree with me on that?
What would be your criterion for considering an increase in block size urgent?
Mine is: we should consider a block increase only when minimum market
fees for transactions to be mined (currently zero satoshis) increase
above a high fee (admittedly undefined, but certainly greater than
Even if it's "urgent", I think we should only increase the maximum if,
at the same time, the new size can be considered safe
mining-centralization-wise (unfortunately we don't have any metric to
measure that nor enough tools to realistically simulate different
sizes in different network topologies at the moment). But once we have
them, the next discussion will be much simpler, so I don't see the
need for block size maximum that changes over time (neither
exponentially nor linearly).
Would you agree with me that mining centralization should be the most
important criterion when changing the block size maximum rule rather
than the level of minimum fees?
If the community can't agree on this, I'm afraid there will be a
schism hardfork eventually. Another possibility is that those who
aren't concerned with mining centralization start their own altcoin
(centralizedcoin? ), maybe a spinoff [
 ] if they want to
keep Bitcoin's utxo at the moment of the separation.
But if the community agrees with this and just disagrees on the
maximum block size consensus rule having any effect on mining
centralization (like Gavin and I disagree), we should calm down and
use scientific processes to find out what the relation between the two
actually is (if there's any relation at all).
Would you agree with me on this?

@_date: 2015-12-08 12:14:32
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
On Dec 8, 2015 7:08 PM, "Wladimir J. van der Laan via bitcoin-dev" <
can test as well.
Testnet4 ?

@_date: 2015-12-09 00:50:35
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
On Dec 9, 2015 7:41 AM, "Jonathan Toomim via bitcoin-dev" <
size of fraud proofs considerably, makes the whole design more elegant and
less kludgey, and is safer for clients who do not upgrade in a timely
I agree, although I disagree with the last reason.
assumptions of non-upgraded clients (including SPV wallets). I think that
for these clients, no data is better than invalid data. Better to force
them to upgrade by cutting them off the network than to let them think
they're validating transactions when they're not.
I don't undesrtand. SPV nodes won't think they are validating transactions
with the new version unless they adapt to the new format. They will be
simply unable to receive payments using the new format if it is a softfork
(although as said I agree with making it a hardfork on the simpler design
and smaller fraud proofs grounds alone).

@_date: 2015-12-09 01:58:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
It would be also a nice opportunity to move the height to a more
accessible place.
For example CBlockHeader::hashMerkleRoot (and CBlockIndex's) could be
replaced with a hash of the following struct:
struct hashRootStruct
uint256 hashMerkleRoot;
uint256 hashWitnessesRoot;
int32_t nHeight;
But then all wallet software will need to adapt their software twice.
Why introduce technical debt for no good reason?
Uncontroversial hardforks can also be deployed with small risks as
described in BIP99.

@_date: 2015-12-09 02:02:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Or better, for forward compatibility (we may want to include more
things apart from nHeight and hashWitnessesRoot in the future):
struct hashRootStruct
 uint256 hashMerkleRoot;
 uint256 hashWitnessesRoot;
 uint256 hashextendedHeader;
For example, we may want to chose to add an extra nonce there.

@_date: 2015-12-09 08:54:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
cleanup hardfork later" earlier you didn't really meant it. And that
you will oppose to that hardfork later just like you are opposing to
it now.
As said I disagree that making a softfork first and then move the
commitment is less disruptive (because people will need to adapt their
software twice), but if the intention is to never do the second part
then of course I agree it would be less disruptive.
How long after the softfork would you like to do the hardfork?
1 year after the softfork? 2 years? never?

@_date: 2015-12-10 06:38:01
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness 
On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
would allow a function to pay its creator.
I don't understand what you mean by "a function" in this context, I assume
you mean a scriptSig, but then "paying its creator" doesn't make much sense
to me .
Could you provide some high level examples of the use cases you would like
to support with this?

@_date: 2015-12-11 16:36:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness 
used as voting shares (This will be difficult as I do not know FORTH).
That seems like a fairly simple, useful example that will require loops and
reused functions.  I'll add a fee that goes to the creator.
If it's voting for something consensus, you will need something special. If
it's not consensus (ie external) thw voting doesn't have to hit the chain
at all.
I don't see how "loops and reused functions" are needed in the scripting
language for this use case, but I'm probably missing some details. Please,
the more concrete you make your example, the easiest it will be for me to
it makes sense to charge a fee for its usage.
But each scriptSig is only executed once with its corresponding
scriptPubKey. Are you proposing we change that?
a small fee on each trade.
I've been researching the topic of decentralized exchange from before the
term "colored coins" was first used (now there's multiple designs and
implementations); contributed to and reviewed many designs: none of them
(colored coins or not) required turing completeness.
I'm sorry, but what you are saying here is too vague for me to concretely
be able to refute the low level "needs" you claim your use cases to have.
would allow a function to pay its creator.
assume you mean a scriptSig, but then "paying its creator" doesn't make
much sense to me .
like to support with this?

@_date: 2015-12-11 16:38:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness 
well "only executed once" (every time someone verifies that transaction)...

@_date: 2015-12-11 17:18:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
This is basically what I meant by
struct hashRootStruct
uint256 hashMerkleRoot;
uint256 hashWitnessesRoot;
uint256 hashextendedHeader;
but my design doesn't calculate other_root as it appears in your tree (is
not necessary).
Since stop requiring bip34 (height in coinbase) is also a hardfork (and a
trivial one) I suggested to move it at the same time. But thinking more
about it, since BIP34 also elegantly solves BIP30, I would keep the height
in the coinbase (even if we move it to the extented header tree as well for
That should be able to include future consensus-enforced commitments (extra
back-refs for compact proofs, txo/utxo commitments, etc) or non-consensus
data (merged mining data, miner-published data).
Greg Maxwell suggested to move those later and I answered fair enough. But
thinking more about it, if the extra commitments field is extensible, we
don't need to move anything now, and therefore we don't need for those
designs (extra back-refs for compact proofs, txo/utxo commitments, etc) to
be ready to deploy a hardfork segregated witness: you just need to make
sure that your format is extensible via softfork in the future.
I'm therefore back to the "let's better deploy segregated witness as a
hardfork" position.
The change required to the softfork segregated witnesses implementation
would be relatively small.
Another option would be to deploy both parts (sw and the movement from the
coinbase to the extra header) at the same time but with different
activation conditions, for example:
- For sw: deploy as soon as possible with bip9.
- For the hardfork codebase to extra header movement: 1 year grace + bip9
for later miner upgrade confirmation.

@_date: 2015-12-12 21:00:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness 
Well, as said, if it's for consensus, you will need to adapt the
system in a special way anyway, but I still don't see why turing
completeness is required.
This type of idea is not new. Since miners can censor votes (and
that's undetectable for consensus), several solutions have been
proposed, time lock the votes, for example.
What you call "recursion" seems similar to what we usually call "covenants", see
Although the thread says "an amusingly bad idea", I think it's
actually a great idea and there's some use cases that are very hard to
support without covenants.
Again, no Turing completeness required for this.

@_date: 2015-12-16 22:24:38
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
On Dec 16, 2015 10:08 PM, "Jeff Garzik via bitcoin-dev" <
sign a
choice - failing to ACK a MAX_BLOCK_SIZE increase still creates very real
Economic Change Event risk.
Unless the community is going to always avoid this "economic change event"
forever (effectively eliminating MAX_BLOCK_SIZE), this is going to happen
at some point. I assume those concerned with the "economic change" are only
scared about it because "nitcoin is still very young" of something like
Since you advocate for delaying this event from happening, can you be
clearer about when do you think it would be ok to let the event happen?
What other event makes this event ok?
actors to a notable degree.  Maintaining a short term economic policy of
fixed 1M supply in the face of rising transaction volume carries risks that
should be analyzed and communicated.
Assuming we adopt bip102, eventually you will be able to say exactly the
same about 2 MB. When does this "let's not change the economics" finishes
(if ever)?

@_date: 2015-12-17 14:09:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
Although I agree that how safe a pre-hardfork upgrade period is depends on
the complexity of the changes (we should assume everyone may need time to
reimplementat it themselves in their own implementations, not just upgrade
bitcoin core) and bip102 is indeed a very simple hardfork, I think less
than 6 months for a hardfork is starting to push it too much.
For a more complex hardfork (say, a SW hardfork or a collection of many
little fixes) I believe 1 year or more would make more sense.
BIP99 recommends a time threshold (height or median time) + 95% miner
upgrade confirmation with BIP9 (version bits).
So how about the following plan?
1) Deploy BIP102 when its ready + 6 median time months + 95% miner upgrade
2) Deploy SW when it's ready + 95% miner upgrade confirmation via bip9.
Note that both "when it's ready" depend on something we are not paying a
lot of attention to: bip9's implementation (just like bip113, bip68-112,
bip99, the coinbase-commitments-cleanup post-SW uncontroversial hardfork,
Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
equivalent to the 2-4-8 "compromise" proposal (which by the way I never
liked, because I don't think anybody should be in a position to
"compromise" anything and because I don't see how "let's avoid an
unavoidable economic change for a little bit longer" arguments can
reasoably claim that "we need to kick the can down the road exactly 3 more
times" or whatever is the reasoning behind it).

@_date: 2015-12-18 06:23:25
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
On Thu, Dec 17, 2015 at 8:44 PM, Peter Todd via bitcoin-dev
You can always schism hardfork miners out...

@_date: 2015-12-18 06:32:31
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] On the security of softforks 
To me it's getting clearer and clearer that th frintier between
softforks and hardforks it's softer than we thought.
Aoftforks should start having a minimum median time deplayment day (be
it height or median time, I don't care, just not header.nTime).
On Fri, Dec 18, 2015 at 4:10 AM, jl2012 via bitcoin-dev

@_date: 2015-12-18 20:52:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The increase of max block size should be 
I agree that nHeight is the simplest option and is my preference.
Another option is to use the median time from the previous block (thus you
know whether or not the next block should start the miner confirmation or
not). In fact, if we're going to use bip9  for 95% miner upgrade
confirmation, it would be nice to always pick a difficulty retarget block
(ie block.nHeight % DifficultyAdjustmentInterval == 0).
Actually I would always have an initial height in bip9, for softforks too.
I would also use the sign bit as the "hardfork bit" that gets activated for
the next diff interval after 95% is reached and a hardfork becomes active
(that way even SPV nodes will notice when a softfork  or hardfork happens
and also be able to tell which one is it).
I should update bip99 with all this. And if the 2 mb bump is
uncontroversial, maybe I can add that to the timewarp fix and th recovery
of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
follow bip99's recommendations and doesn't want to give 6 full months as
the pre activation grace period).
On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <

@_date: 2015-12-18 21:10:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The increase of max block size should be 
Well, if it's not going to be height, I think median time of the previous
block is better than the time of the current one, and would also solve Chun
Wang's concerns.
But as said I prefer to use heights that correspond to diff recalculation
(because that's the window that bip9 will use for the later 95%
confirmation anyway).

@_date: 2015-12-18 21:20:13
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The increase of max block size should be 
I believe the attacks are the same for height or median time of the prev
block are equal, only the time of the current block has more edge cases.

@_date: 2015-12-18 23:58:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The increase of max block size should be 
past: the point is to use a time that the block creator has no direct
control of, while still tying the rule to wall clock time for planning
Well, if after the "planned clock time" you need to wait for the next diff
retarget and then wait for 95% (bip9) I think the value of being able to
use "human friendly clock time" is very dubious (specially since median
time is different from real-world time anyway).
But yeah, not giving the creator of the current block direct control over
whether its block starts the activation process or not is achieved with
median time of the previous block just as well as nHeight does.
So even if I disagree with the value that median time brings over the
simpler height approach, let's please decide on one and always use that for
both hardforks and softforks as part of bip9 (which we would need to
An initial time threshold is not necessary for uncontroversial softforks,
but it doesn't hurt (you can always set it in the past if you want to not
use it) and in fact it simplifies bip9's implementation.
Let's please decide once and for all, update bip9 and bip99 and stop doing
something different on every hardfork patch we write.

@_date: 2015-12-21 10:56:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
To clarify, although I have defended the deployment of segwit as a
hardfork, I have no strong opinion on whether to do that or do it as a
softfork first and then do a hardfork to move things out of the
coinbase to a better place.
I have a strong opinion against never doing the later hardfork though.
I would have supported segwit for Bitcoin even if it was only possible
as a hardfork, but there's a softfork version and that will hopefully
accelerate its deployment.
Since the plan seems to be to do a softfork first and a hardfork
moving the witness tree (and probably more things) outside of the
coinbase later, I support the plan for segwit deployment.
In fact, the plan is very exciting to me.

@_date: 2015-12-26 16:33:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
On Dec 26, 2015 9:24 AM, "Eric Lombrozo via bitcoin-dev" <
throughput, and lower miner revenue. Note, however, that confirmations
would (on average) represent more PoW, so fewer confirmations would be
required to achieve the same level of security.
I'm not sure I understand this. If mining revenue per unit of time drops,
total pow per unit of time should also drop. Even if the inter-block time
is increased, it's not clear to me that the pow per block would necessarily
be higher.
What am I missing?

@_date: 2015-12-26 18:20:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
On Dec 26, 2015 5:45 PM, "Pieter Wuille via bitcoin-dev" <
whether consensus for a hard fork exists, and is technically necessary and
safe. We don't need a hashpower vote to decide whether a hardfork is
accepted or not, we need to be sure that full noded will accept it, and
adopt it in time. A hashpower vote can still be used to be sure that miners
_also_ agree.
To clarify, that's the role of Bitcoin Core maintainers because they decide
what goes into Bitcoin Core, not because they decide the consensus rules of
Bitcoin. Other full node implementations (say, libbitcoin) will have to
decide on their own and Bitcoin Core mainteiners don't have any authority
over libbitcoin (or other alternative implementations). Nobody has such
authority (not even the creator of the system if he was still maintaining
Bitcoin Core).

@_date: 2015-12-26 19:01:59
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
The hashpower is a function of the block reward (subsidy + fees): it's
economically irrational to have costs greater than the reward (better just
turn off your miners) and in a perfect competition (a theoretical model)
profits tend to zero. That is, the costs tend to equal revenue (block

@_date: 2015-12-26 20:34:32
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
On Dec 26, 2015 7:30 PM, "Eric Lombrozo via bitcoin-dev" <
remains constant.
But that's not reasonable if you are assuming that the total reward per
unit of time drops, that's what confused me.

@_date: 2015-02-14 21:00:51
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Right now libconsensus' only dependency is openSSL. Most of the
testing in libsecp256k1 has been in signing rather than verifying
signatures (please, anyone with more knowledge in the library don't
hesitate to correct me or clarify things). But eventually openSSL will
be completely replaced by libsecp256k1.
It does not store anything, 0.1 is just a dynamic library with a c API
to a single function: VerifyScript().
This function saves the hassle of reimplementing signature checking
(which is a really hard part) and reimplementing an interpreter that
must function in exactly the same way in many as many other nodes with
different software and/or hardware.
Guido van Rossum can say "some behaviours in python the language are
not specified, so it is ok if cpython and pypy do different things,
they're still both running python which is more abstract than any of
its implementation".
But a consensus system like bitcoin doesn't have the luxury of leaving
consensus rules unspecified. And the simplest way to fully specify a
language interpreter is by implementing it.
But coupling the consensus rules specification with a bigger project
like bitcoin core can result in implementation details of that bigger
project accidentally and unexpectedly becoming consensus rules. This
is what happened with bdb and nobody wants that to happen again,
that's the whole point.
Note that many parts of the bitcoin protocol (like the p2p messages)
are NOT part of the consensus rules.
You can have a look at
 and maybe you
would be surprised about how small they actually are. This branch is
incomplete and still a mess that needs to be cleaned up. And none of
that is included in libconsensus yet.
I was planning on writing a post here asking for feedback on the
interfaces for these higher level checks. I'm just putting the code
together in the same module, but obviously class CCoinsViewCache
cannot be an argument in functions of a c API.
Nobody is attacking alternative implementations. This tool was created
mostly with alternative implementations in mind.
So input from them it's very welcomed on how to continue libconsensus
(or of course correct any flaws in verifyScript if there's any).
I just wanted to wait to have some more code to make things easier to
explain (and have a clearer idea of it myself).
There's a more limited branch on "next steps for libconsensus" in Sure, I think he is complaining that at the moment that's probably the
only safe way to operate with alternative implementations and still
have full node guarantees.
Sidechains are completely orthogonal to this discussion and, in fact,
it would be good to have libconsensuses for sidechains too, since
their nodes also need to come to consensus.

@_date: 2015-02-19 18:16:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
He didn't said "a project for all possible language bindings", just
java bindings. Other languages' bindings would be separate projects.

@_date: 2015-02-20 04:47:41
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Oh, I didn't knew that. Thanks for the clarification.

@_date: 2015-02-21 20:09:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
I agree "scorched hearth" is a really bad name for the 0 conf protocol
based on game theory. I would have preferred "stag hunt" since that's
basically what it's using (see but I like the protocol and I think it would be interesting to
integrate it in the  payment protocol.
Even if that protocol didn't existed or didn't worked, replace-by-fee
is purely part of a node's policy, not part of consensus.
of miners was never an assumption, and it is clear to me that the
system cannot provide those guaranties based on such a weak scheme. I
believe thinking otherwise is naive.
As to consider non-standard policies "an attack to bitcoin" because
"that's not how bitcoin used to work", then I guess minimum relay fee
policies can also be considered "an attack to bitcoin" on the same
Lastly, "first-seen-wins" was just a simple policy to bootstrap the
system, but I expect that most nodes will eventually move to policies
that are economically rational for miners such as replace-by-fee.
Not only I disagree this will be "the end of bitcoin" or "will push
the price of the btc miners are mining down", I believe it will be
something good for bitcoin.
Since this is apparently controversial I don't want to push for
replace-by-fee to become the new standard policy (something that would
make sense to me). But once the policy code is sufficiently modular as
to support several policies I would like bitcoin core to have a
CReplaceByFeePolicy alongside CStandardPolicy and a CNullPolicy (no
policy checks at all).
One step at a time I guess...

@_date: 2015-02-22 04:25:32
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
When I posted this: Peter Todd clarified that the concept was referred to as "scorched earth"
Like I said I don't like the name and would prefer "stag hunting"
which is more formal and precise.
Some people seem to use the same term for "the potential undesirable
consequences of widely deployed replace-by-fee policies".
I'm not sure that concept deserves its own term.
And maybe by maintaining first seen policies we're harming the system
in the long term by encouraging people to widely deploy systems based
on extremely weak assumptions.

@_date: 2015-07-06 18:50:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Introduce N testnet chains to test different block 
I have created the following PR that simplifies testing of different
block sizes and (if it were merged) would also slightly simplify a
future block size change hardfork.
I hope someone finds this useful. Please, post to github if you find
any issues. But, please, don't discuss the block size issue itself in
this post or the PR, the size is simply -blocksize.
I repeat the text here:
It would be generally good to have more people collecting data and
conduction simulations related to different consensus maximum block sizes.
This PR attempts to simplify that work.
Even if it may take long until it is merged (because it requires many
little steps to be taken first), this branch (or a fork of it) can be
used right now for
testing purposes.
One can use it, for example, like this: ```./src/qt/bitcoin-qt
-chain=sizetest -debug -printtoconsole -gen=1 -genproclimit=20
I will rebase and update the list of dependencies accordingly as
things get merged.
- Chainparams: Translations: DRY: options and error strings - CTestNetParams and CRegTestParams extend directly from CChainParams

@_date: 2015-07-11 11:24:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
All miners should validate transactions precisely because of the latest
attack you've described. Full miners can gain a lot from this attack to
leverage their full validation against spv miners who blindly spend energy
hashing on top of something that may be worthless crap. SPV mining makes no
sense, but some miners claim they're doind it for very short periods of
time, which shouldn't be as bad as doing it all the time.
I think it would be more rational for them to keep mining on top of the old
block until they've fully validated the new block (which shouldn't take so
long anyway), even if this slightly increases the orphan rate.

@_date: 2015-07-12 20:37:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
If miners aren't validating the blocks they mine on top of, an
attacker can do more nasty things I think.
As long as miners switch back to the new longest chain after they
validate the block, mining on top of the
non-most-work-but-surely-valid may be less risky than mining on top of
a most-work-but-potentially-invalid block.
This has risks too. In both cases, if they don't mine a block during
the block validation, everything is fine.
If they successfully SPV mine, they risk having mined on top of an
invalid block, which not only means lost coins for them but high risk
for regular SPV users.
If they successfully mine on top of the previous block, they start a
mini-race that they can win or not, but the impact to regular SPV
users is much lower.
The later may be slightly less profitable, but I bet the difference is
negligible. It would be interesting to know if miners actually did
this numbers and how (in case their model is incomplete or flawed).
It is important to note that while SPV mining requires you to produce
empty blocks, mining on the previous on top of the previous block
allows you to include transactions and earn fees.
In a future where block rewards aren't so overwhelmingly dominated by
subsidies, the numbers will run against SPV mining.
In a future without (or with negligible) subsidy, SPV mining is always
inferior to just keep mining on top of the same block you were mining
until you fully validate the next one.
This seems correct (for both cases).
It's also less worrying the shorter the full validation time of a block is.

@_date: 2015-07-21 00:14:41
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Do we really need a mempool? (for relay nodes) 
Exactly, so an anti-DoS mechanism that would be sufficient for a
non-mempool node would be also useful for small values in -maxmempool.
I think a simple cache for transaction validations should be enough.
Please, review a draft for that in the newest I would be happy to rebase it back to 0.11 and even 0.10.

@_date: 2015-07-21 11:26:35
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
I still disagree. Using height instead of time may make the
implementation more complex by requiring some additional preparations
but using height is in fact a simpler design. Why relay on clocks that
we know will differ in different computers and places when we have a
universal tick with every block?
Btw, BIP16 and BIP34 could be changed to height-based activation
already. BIP16 simply should have used height instead of time from the
On Mon, Jul 20, 2015 at 12:51 AM, Ross Nicoll via bitcoin-dev

@_date: 2015-07-23 13:10:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [Bitcoin-development] [BIP draft] Motivation and 
Discussions about whether to get miner's confirmation on
uncontroversial hardforks or not, and about whether to use nHeight,
nMedianTime or just use nTime are spreading all around. Hopefully
getting a BIP number (even though this is still a draft) will help
concentrating discussions about deployment of uncontroversial
hardforks to a single place.
Greg, can I get a BIP number for this?

@_date: 2015-07-23 13:24:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
Peter Todd, as discussed on IRC, I'm not opposed to median time, which
has many of the properties nheight has, I'm just opposed to just using
nTime which is what all "hardfork proposals" I've seen so far
(including this one) do.
No, the height is in the current block after bip34, no context required.
In any case, you already have the nHeight in most functions that would
require it (for example, main::ConnectBlock).
The median time actually needs a context (the last 10 headers), but
it's not hard to calculate and pass around either.
But simply using nTime is not a good idea. Leaving aside time zones,
einstein and all that it introduces edge cases and weird incentives
for no good reason.
If the goal is to make it "human-schedule-friendly", median time
should be good enough.
If we're going to make miners 95% confirm after the date/height, I
still prefer the height, but as said median time seems a reasonable
Can we move the "height/medianTime/nTime" and "is it good to confirm
that the change is uncontroversial to miners by requiring 95% to
activate the consensus change, like we do with uncontroversial
softforks?" discussions to the thread with my bip draft (
) on precisely this subject?
I have requested a bip number.
Let's please have an uncontroversial hardfork to set a precedent.
Hopefully that way we may decouple some parts of the blocksize

@_date: 2015-07-23 14:17:14
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
That's because demand for space (transactions) was always lower than
the supply (block space) and no market price (fees) arose.
Now the market (not the supply) has changed: demand has increased.
With a fixed supply, it was perfectly reasonable to expect that fees
would rise (from zero).
If the user expectation is that a price would never arise because
supply is going to be increased ad infinitum and they will always be
able to send fast in-chain bitcoin transactions for free, just like
breath air (an abundant resource) for free, then we should change that
expectation as soon as possible.
It is not a new economic policy, it is a new market situation. Please,
stop saying that.
If "not now", then when?
I've been asking that question repeatedly and the closest to an answer
that I got from the "not now side" was "the hashrate being paid by
fees instead of subsidy it's too far away in the future to worry about
it now".
That answer is not very satisfying to me.
Yes, business plans that rely on free in-chain transactions may fail,
business plans that are planning for a future with fees and without
subsidies may get the advantage they deserve. But "kicking the can" is
just picking winners and losers in opposite way.
You seem to imply that rewarding inertia and laziness is the best
option for short-term bitcoin adoption and you may be right.
I simply think these arguments shouldn't be considered at all: the
criteria for the consensus block size should be purely based on
technological capacity (propagation benchmarking, etc) and
centralization concerns (those in the "not now side" have already seen
this 2-year-old video[1], right?).
But it seems to me that the "not now side" has no centralization
concerns at all and their true position is "not ever hit the blocksize
limit", that's the only explanation I can find to their lack of
answers to the "when do you think we should allow users to notice that
there's a limit in the blocksize to guarantee that the system can be
decentralized?". I've even read that the consensus limit "was just a
temporary measure". Then Gavin lowers his 32 GB limit to an 8 GB
Maybe I'm being paranoid, but I'm really afraid that when the  "not
now side" wins this battle (like they've won for 6 years, as you say)
they will simply advance the front and start another battle, because
their true hidden faction is the "not ever side".
Please, Jeff, Gavin, Mike, show me that I'm wrong on this point.
Please, answer my question this time.
If "not now", then when?
[1]

@_date: 2015-07-23 16:30:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin Core 
I think there were some misunderstandings in our previous conversation
about this topic.
I completely agree with having a separated repository for libconsensus
(that's the whole point, alternative implementations can be
consensus-safe by using it, and in the event of a schism fork[1], they
can fork just that smaller project without having to relay on Bitcoin
Core [satoshi] at all).
But I thought you also wanted Bitcoin Core to use libconsensus instead
of just having a subtree/subrepository like it currently does with
I'm not sure if that would ever be accepted, but in any case we're
certainly far away from that goal. Here are some things that need to
happen first:
1) Finish encapsulating consensus code so that it can be built without
any (we've done it only with script-related code so far). Here are
some related PRs (other people havee done other things that help with
this as well):
** MERGED or DELETED
*** MERGED Consensus: Decouple pow from chainparams  [consensuspow]
*** DELETED MOVEONLY: Move constants and globals to consensus.h and
policy.o  [consensus_policy0]
*** DELETED Refactor: Create CCoinsViewEfficient interface for
CCoinsViewCache  [coins]
*** MERGED Chainparams: Refactor: Decouple IsSuperMajority from
Params()  [params_consensus]
*** MERGED Remove redundant getter
CChainParams::SubsidyHalvingInterval()  [params_subsidy]
*** MERGED Separate CValidationState from main  [consensus]
*** DELETED Consensus: Refactor: Separate CheckFinalTx from
main::IsFinalTx  [consensus_finaltx]
*** MERGED Consensus: Decouple ContextualCheckBlockHeader from
checkpoints  [consensus_checkpoints]
*** MERGED Separate Consensus::CheckTxInputs and GetSpendHeight in
CheckInputs  [consensus_inputs]
*** MERGED Bugfix: Don't check the genesis block header before
accepting it  [5975-quick-fix]
** REVIEW Optimizations: Consensus: In AcceptToMemoryPool,
ConnectBlock, and CreateNewBlock  [consensus-txinputs-0.12.99]
** REBASE MOVEONLY: Move most of consensus functions (pre-block) ** REBASE Consensus: Refactor: Turn CBlockIndex::GetMedianTimePast
into independent function  [consensus_mediantime]
** DEPENDENT Consensus: Refactor: Consensus version of
CheckBlockHeader()  [consensus_checkblockheader]
** DEPENDENT Consensus: Consensus version of pow functions [consensus_pow2]
2) Finish libconsensus's API: expose more things than VerifyScript, at
the very least, also expose VerifyTx, VerifyHeader and VerifyBlock.
Feedback from alternative implementations like libbitcoin is extremely
valuable here. Some related closed-for-now PRs:
** DEPENDENT API: Expose bitcoinconsensus_verify_header() in
libconsensus  [consensus_header]
** DEPENDENT API: Expose bitcoinconsensus_verify_block() in
libconsensus  [consensus_tip]
** REBASE Chainparams: Explicit Consensus::Params arg in consensus
functions  [params_consensus2]
3) Move libconsensus to a separate repository as a
subtree/subrepository of Bitcoin Core.
Only after all that we can discuss whether Bitcoin Core itself should
include libconsensus' code or just use its API directly.
I hope that after all this, libbitcoin also reconsiders whether to
reimplement its own libconsensus or use the "official" one directly
I completely agree. That's the goal of libconsensus (and an
alternative implementation like libbitcoin being able to use it
without sacrificing any of its current or future design differences
from Bitcoin Core would be a sign of success in this reward).
Unfortunately any changes that touch consensus code are risky and
therefore slow. And when consensus encapsulation changes conflict with
other changes (not because the other changes need to change consensus
but because consensus code is still coupled with policy and other
bitcoind-specific code), refactors are never prioritized. Ironically,
you need to encapsulate the consensus code to avoid such conflicts,
which would make all non-consensus changes far less risky (reducing
the consensus-critical review development bottleneck).
Unfortunately and ironically again, safer, small and incremental
changes are less interesting for reviewers.
For example, I've been trying to move consensus code to the consensus
folder for a long time. The correctness of a MOVEONLY change is
trivial to review for anyone who knows how to copy/paste in its
favorite editor and how to use git diff, but will I ever get answers
to my questions in [1]?
I know there's many people who really care about this, Cory Fields,
Wladimir and Pieter Wuille to name a few have reviewed many of this
changes (I've just got used to publicly whine about lack of review on
this front and policy encapsulation [very related fronts] as an
attempt to get some attention: not always, but begging for review
actually works some times).
Another unfortunate fact is that although a script-only libconsensus
allows you to avoid a big part of all possible consensus fork bugs,
there cannot be users of a finished libconsensus to ask things to util
a finished libconsensus actually exists. At the same time, the future
users (alternative implementations, since bitcoin core is already
"using libconsensus") are the most relevant people to listen when it
comes to the C API. That's why I beg you to comment on [2], even if
 is currently closed. Your input on [1] would be very appreciated
as well (maybe you think it's better to expose verifyTx before
exposing verifyHeader, even if exposing verifyHeader is something that
could be done faster).
 > To make choice regarding consensus an actual choice (and thereby actual
Would you agree that asking people to fork an independent libconsensus
project instead of having to fork the full Bitcoin-qt is much more
I mean, I agree with your points. If "the specification of the
consensus rules is an implementation", then that implementation
shouldn't be coupled with a bunch of policy and non-consensus
technical choices (storage, dependencies, p2p protocol...). But I
still think that "the specification of the consensus rules should be a
concrete implementation" rather than based purely on a natural
language like English.
I believe that's the only point where we fundamentally disagree, but
it shouldn't be a barrier in our common goal of taking "power" away
from Bitcoin Core development. If we're successful Bitcoin Core won't
have any privileged position with regards to, say, libbitcoin when it
comes to deciding consensus rules changes.
You see, people like Mike Hearn believe that "uncontroversial
acceptance by Bitcoin Core devs" is the same as "uncontroversial
acceptance by all users of the system" (for a libbitcoin developer
like you, obviously a superset of Bitcoin Core's users). He thinks
that Gavin proposal is only a schism consensus fork[3] because the
code is in github/bitcoinxt/bitcoinxt instead of
github/bitcoin/bitcoin, not because PeterTodd-the-user-of-the-system
(he doesn't care about him) opposes it.
But let's imagine a different situation:
1) libconsensus us finished and used by libbitcoin
2) Bitcoin Core was unanimously in favor of Gavin's 32 GB initial
proposal and the changes are applied to bitcoin/bitcoin and
bitcoin/libconsensus (or Bitcoin Core has a dictator like Mike
wants[4] and he accepts it, it doesn't really matter for this
But let's also assume that X% of the users and 10% of the miners are
against that Schism hardfork, and they don't want to be forced to
change the rules by any influential group, mining, economic or user
Libbitcoin cannot be forced to accept the next, controversial version
of bitcoin/libconsensus, so you guys fork libbitcoin/libconsensus out
of the last ok version.
Centralized-bitcoin and old-bitcoin would become 2 separated
currencies and some people would likely lose money in the transition
from one currency to 2 of them, but the users of old-bitcoin have the
right of keeping the rules they signed up for and the only responsible
people for this likely-catastrophic schism would be the Bitcoin core
developers for trying to impose consensus changes into others against
their will. Trying to impose consensus changes against the will of
some users is wrong, and it is irrelevant if that happens in Bitcoin
Core or Bitcoin Tx (if it is uncontroversial, it's also irrelevant
where it gets implemented first).
I really believe bitcoin needs competitive alternative implementations
and I believe libconsensus is a tool to help that happen and reduce
the "gatekeeping" friction that there's (unfortunately) around Bitcoin
Core. I look forward to any potential collaboration on this front.
Even if you still want to maintain a reimplementation of libconsensus
(which I humbly think it's a mistake, but I don't think there's any
point on keep discussing that, since we know we disagree) we can
collaborate on the future common API of a complete libconsensus (with
verifyBlock and all). I really hope we can do that.
[1] [2] (kind of outdated, but the API is what matters here)
[3] [4]

@_date: 2015-07-23 19:51:11
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
On Thu, Jul 23, 2015 at 6:17 PM, Tom Harding via bitcoin-dev
We know perfectly well that the system will need to eventually be
sustained by fees.
We should stop misinforming new users talking them about how bitcoin
transactions "are free", because they're clearly not.
Am I "artificially manipulating expectations" ?
Timestamping data using the blockchain is not the same as including
that the data in the blockchain itself because the later is a scarce
The "timestamping space" is already unlimited today with no changes.
You can use a bitcoin transaction to timestamp an unbounded amount of
external data using exactly 0 extra bytes in your transaction!
Here's the code: And I'm very interested in scaling Bitcoin, I just disagree that
changing a constant is a "scaling solution".
On Thu, Jul 23, 2015 at 6:28 PM, Gavin Andresen via bitcoin-dev
I extremely disagree that having a block limit is failure. It's a
design decision to protect the system against centralization (which we
will be able to rise as we solve technical and centralization problems
we have today).
But thank you for being more clear about it now, Gavin. You won't stop
on a 8GB or 32GB limit because you think having ANY limit would be a
Is that correct?
If not, can you please answer clearly when and why you think the
blocksize should be lower than demand (when you will be ok with
bitcoin users having to pay fees for the service they're enjoying)?
If your answer is "never", I would prefer to hear it from you than
just concluding it by the lack of an answer.

@_date: 2015-07-23 20:12:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
For what is worth, here's yet another piece of code from the "doing
nothing" side:
It allows you to create a regtest-like testchain with a maximum block
size chosen at run time.
Rusty used a less generic testchain for testing 8 MB blocks:
Unfortunately I don't know of anybody that has used my patch to test
any other size (maybe there's not that much interest in testing other
sizes after all?).
I'm totally in favor of preemptively adapting the code so that when a
new blocksize is to be deployed, adapting the code is not a problem.
Developers can agree on many changes in the code without users having
to agree on a concrete block size first.
I offer my help to do that. That's what I'm trying to do in  and
but to my surprise that gets disregarded as "doing nothing" and as
"having a negative attitude", when not simply ignored.

@_date: 2015-07-23 20:47:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
On Thu, Jul 23, 2015 at 7:14 PM, Robert Learney via bitcoin-dev
His proposals actually end up with 20 GB and 8 GB respectively. I'm
not sure if you count me on the ?1Mb or bust? group, but I'm not
firmly stuck anywhere.
I've never said that the block size should never be increased, that it
shouldn't change now, that 8 MB is too much or anything like that
because I simply don't have the data (and I don't think anybody has
it). I invite people to collect that data and I've written a patch to
bitcoin to facilitate that task.
Do you really think that's an obstructionist attitude?
My position could be summarized like this:
- We're going to hit the limit tomorrow, and Bitcoin will fail when we do.
- I'm not so sure we will hit the limit tomorrow but even accepting
the premise, this is a non sequitur. Fees will probably rise, but
that's not necessarily a bad thing. A limit that is meaningful in
practice must happen eventually, mustn't it? If not now, when are we
planning to let that "disaster" happen?
- That's too far in the future to worry about it.
- Does that mean waiting, say, 4 more subsidy halvings, 8? 10?
- Just don't worry about it
I'm not opposing to anything, I'm just patiently waiting for some
answers that never seem to arrive.
If people interpret questions or the fact that when people use
fallacious arguments I like to identify the concrete fallacy they're
using and state it so publicly (I do it for sport and "against all
sides") as "opposition", I don't really think I can do anything about

@_date: 2015-07-23 22:26:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
On Thu, Jul 23, 2015 at 9:52 PM, Jameson Lopp via bitcoin-dev
Although I don't have a concrete proposals myself, I agree that
without having any common notion of what the "minimal target hardware"
looks like, it is very difficult to discuss other things that depend
on that.
If there's data that shows that a 100 usd raspberry pi with a 1 MB
connection in say, India (I actually have no idea about internet
speeds there) size X is a viable full node, then I don't think anybody
can reasonably oppose to rising the block size to X, and such a
hardfork can perfectly be uncontroversial.
I'm exaggerating ultra-low specifications, but it's just an example to
illustrate your point.
There was a thread about formalizing such "minimum hardware
requirements", but I think the discussion simply finished there:
- Let's do this
- Yeah, let's do it
- +1, let's have concrete values, I generally agree.

@_date: 2015-07-23 23:02:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
On Thu, Jul 23, 2015 at 4:57 PM, Milly Bitcoin via bitcoin-dev
Mike has sincerely said that he would like "Bitcoin Core to have a
benevolent dictator like other free software projects", and I wanted
to make clear that I wasn't putting words in his mouth but it's
actually something very easy to find on the internet. But I now
realize that the search can be interpreted as me calling him dictator
or something of the sort. That wasn't my intention. In fact, Mike's
point of view on Bitcoin Core development wasn't even relevant for my
example so I shouldn't even have mentioned him in the first place. I
apologize for both mistakes, but please let's keep this thread focused
on libconsensus.
I actually don't spend much time on reddit: I don't particularly like
it. But I do spend some time in reddit so, I agree: I spend too much
time on reddit.

@_date: 2015-07-24 11:24:01
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Why do you expect users to "increase the number of fee-paying transactions"
if their free transactions reliably get mined relatively fast?
And if it's good that they pay fees, why is not good when the reason they
do it is because there's limited space in the block? Is user's paying fees
soon a good thing or the "catastrophe" we need to avoid by rising the block
size, what is it? Or is there something else wrong with having a limit
other than "fees will hurt short-term adoption"? I'm confused about your
position now...
Regarding "increasing the exchange rate" it would be really nice to just
push a button and double bitcoin's price just before the next subsidy
halving, but unfortunately that's something out of our control.

@_date: 2015-07-24 11:42:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin, Perceptions, and Expectations 
Well, I think "fast international transactions" is still true. An hour is
pretty fast when you compare it with several days. But yeah, "free" and
"instant" are misleading words.
Low fees may be ok. One thing that is not mentioned often is that the fact
that the system is p2p is what makes transactions irreversible (otherwise a
court order can tell any centralized server to cancel any transaction).
Irreversible transactions don't need proportional fees, because there's
nothing being ensured and the amount being moved is irrelevant. So even if
we have a future with 5 usd fees, that's still a very low fee for moving,
say 1 M usd worth of btc. So I'm not opposed to talking about low fees,
just not free and not instant (although lightning can actually provide free
and instant transactions).
On Jul 24, 2015 10:48 AM, "Jonas Schnelli via bitcoin-dev" <

@_date: 2015-07-26 00:05:28
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Draft: Minimum Viable TXIn Hash 
"It could also more easily, ignoring the difficulties of a hard-fork
period, be rolled out as a hard fork to avoid hokey-pokey.[1]
[1] Because someone asked... The Txid Hokey Pokey: you put the tail
end in, you put the tail end out, you put the tail end in and you hash
it all about you do the hokey pokey and you solve the block difficulty
bound, that's what it's all about!"
Reading this, the first thing that comes to mind is "What the h is
a hokey pokey?"
English-speaking countries.".
That explains why I haven't heard about it in my whole life.
It may things clearer for people in these countries, but at least to
me, it just makes things more complicated: the analogy (that I still
don't understand after skimming the wikipedia article) doesn't allow
me to understand the actual explanation.
Can you please rewrite that with a more culturally-neutral analogy (or
just no analogy and just leave the explanation)?
On Sat, Jul 25, 2015 at 9:51 PM, Luke Dashjr via bitcoin-dev
I think his goal is to make it a consensus change so that confirmed
transactions can also use less space in blocks.
But, yes, I don't think it gives you anything to enforce it as a
consensus rule (all you care about is the savings when transmitting
the transactions and blocks).
In fact, I'm not sure how would that work, would the "compact tx"
produce a different hash than the non-compact one?

@_date: 2015-07-27 11:08:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [Bitcoin-development] BIP for Proof of Payment 
I don't know, it wasn't me that proposed a bigger nonce. I just wanted to
point out that the policy limit shouldn't be a concern.

@_date: 2015-07-28 11:58:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
Ok, I'm going to separate terms: current-libconsensus from theoretical
future-libconsensus (implementing ALL consensus rules).
We want to complete future-libconsensus (decouple all the consensus
rules from the rest of the bitcoin core code) first.
Then we can move future-libconsensus to a subrepository/subtree like
libsecp256k1 and I believe everybody wants this to eventually happen.
Separating current-libconsensus now may make completing
future-libconsensus harder.
What I mean is that once it is separated to a subtree, there's one more step:
Make Bitcoin Core use future-libconsensus' API instead of a subtree.
Decoupling future-libconsensus from Bitcoin Core is one thing, and
Decoupling Bitcoin Core from future-libconsensus is another thing: you
need to decouple Bitcoin Core from all future-libconsensus
implementation internals. For example, script/sign (part of Bitcoin
Core) depends on individual non-API-exposed classes in
Moving from a subtree to a completely separated library is what I
don't know will ever happen, but I don't think this is "unfairly
advantageous" for Bitcoin Core or anything like that: other
implementations can also use future-libcosensus as a subtree instead
of the C API as well.
You have accomplished the goal of separating curren-libconsensus, not
In fact, if you complete the equivalent of future-libconsensus in
libbitcoin and separate that, maybe that's a better place to start
drafting a full API.
future-libconsensus will not have significant changes *once it is
completed*. Currently future-libconsensus is spread around many places
inclusing src/main, so that obviously needs to change before it can be
separated to an independent repo.
Well, Bitcoin Core is "currently the only user of future-libconsensus"
since bitcoin core and future-libconsensus are currently mutually
Bitcoin Core will always keep using future-libconsensus. The only
question is whether it will use it through the C API or as a
subtree/subrepository (both options are also available to other
implementations). I don't know if decoupling Bitcoin Core from
future-libconsensus' implementation details enough to be able to
directly use the API is worth it or if anyone will be interested in
doing so. For me this last step is not all that interesting: if we
have an independent repo with a full API that other implementations
can use, I don't really care about Bitcoin Core not going through the
API and using including all the code directly instead.
As I told you before the reason why current-libconsensus is using
OpenSSL instead of libsecp256k1 is that the very authors of
libsecp256k1 warned that using libsecp256k1 for validation was
consensus risky. As Wladimir said, Pieter Wuille will make an
announcement about this soon.
In any case, as I told you in previous conversations, the plan is to
move from OpenSSL to libsecp256k1 for validation too (so libconsensus
wil drop the OpenSSL dependency and this is just a temporary concern).
Well, the questions about the API are just in english, no need to
deeply know Bitcoin Core's (satoshi client) internals.
But maybe we should have an independent mailing list for
consensus-only things. Not all future-libconsensus users will be
interested in Bitcoin Core-specific discussions, and making them
subscribe and filter seems like an unnecessary burden to
I think that's precisely what makes it a high priority in the eyes of
all the people working on it or reviewing related changes.
But, yes, I guess "evil-thinking", maybe that's what make it a low
priority for someone evil that wants Bitcoin Core's implementation
have more importance than it shold forever. I prefer not to evil-think
and just attribute it to having other priorities or just apathy about
By "finished" I mean a future-libconsensus that implements ALL
consensus rules. We don't have that yet.
Bitcoin Core is the ONLY "user" of future-libconsensus (which actually
only exists inside Bitcoin Core and it's not exposed).
Current-libconsensus is used by Bitcoin Core and also exposed as an
independent build (not a separated repository yet).
Once future-Bitcoin's API is completed and the code in a different
repo, how is Bitcoin Core using the API instead of the sources
directly of any importance to other implementations?
That's really the part that I cannot understand. It will be a problem
Bitcoin Core, but if other implementations want to have (and maybe
solve later) the same problem they can use a subtree too and start
coupling their code with implementations details from
Why would they want to do that? Again, I have no idea. I don't
understand what the complain is here.
Great. I mean, I wasn't asking about reviewing the commits themselves
(which is also great if you do), but rather on answering the questions
I'm making there, ie: what to expose next (ie VerifyTx or
VerifyHeader)? would this be an acceptable way to expose VerifyHeader
? Which of he step-checks functions is worth exposing too (Bitcoin
Core is currently using some to prevent DoS attacks, for example)?
Well, neither libbitcoincosnensus nor libbitcoin-consensus implements
all the consensus rules.
That's what makes them different from future-libconsensus.
But great, we're confirming more views that we share.
Well, this is where I fear we will never agree. I think "Bitcoin is
different" in this reward and you disagree.
Maybe Pieter's explanation is more convincing to you:
Otherwise, I think I'll stop trying convincing you.
But one last try:
If there's 2 "specification implementations":
1) widely deployed but containing 1 bug
2) not deployed anywhere but more readable and without the bug.
When the bug is found, is it a consensus rule or not?
If it is, it turns out the second implementation wasn't an specification at all.
If it's not, nobody has been ever following consensus rules!!
1) Working on it
2) The Satohi client has been using all along and it will use it
forever (maybe not through the API, but I don't get what the problem
with that is).
3) There will be an announce about this soon.
Once future-libconsensus exists as a separated repository, I don't
think you want to preemptively fork it unless you're actually changing
Ideally schism consensus-rules changes will never happen, since they
effectively divide the currency in 2 and force the users to chose
(apart from other complications).
But if someone is trying to impose a schism fork (from Bitcoin Core,
from the new future-lbiconsensus repo or from anywhere else) it
becomes trivial to protect your implementation against forced changes.
Ideally all consensus changes will be uncontroversial or not happen at all.
But I think about non-ideal cases too.
1) Reasonable.
2) You mean use it through the API? Seriously, why do you care?
To be clear, Bitcoin Core's using future-libconsensus through a
subtree instead of the API is not a preference or a goal: it is just
how things will be just after completing and separating
future-libconsensus. Making it use the API instead of the subtree will
be additional work. I'm not sure I will want/have time to do it and I
don't know of anyone planning to do that (which seems very reasonably
given that a separated future-libconsensus is a dependency for such a
Is it because "fear of consensus bugs is what keeps people on the
satoshi client" and you want to keep things this way?
Sorry, just joking about your previous "cynic" comment. See? Don't
attribute to malice what can be attributed to lack of time.

@_date: 2015-07-28 12:09:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
On Tue, Jul 28, 2015 at 10:43 AM, Wladimir J. van der Laan
As explained to Eric, it's not that I don't want Bitcoin Core to use
future-libconsensu through the API instead of a subtree: it's just
that that's more long-term and more work. And I don't see why other
implementations should really care about it.
Well, pure movements will not be enough, parameters will have to
change, incompatible dependencies have to be removed (ie util.h which
contains globals), etc.
But yes, I think we can do it with only low-risk and easy-to-review commits.
And still, this doesn't require Bitcoin Core to use the API, a subtree
is enough at first.
This "easy step" doesn't guarantee that Bitcoin Core is using
future-libconsensus' API.
I really think these code separations help with this (ie there are
many more people in the world with enough knowledge to review the qt
or even policy parts than there's people able to review consensus
I know and I said so.

@_date: 2015-07-28 13:29:36
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
My level of contributions are irrelevant to this discussion. But
still, I feel I should clarify some of the metrics you are looking to.
Most of my contributions so far are code refactors (with a small
change on the command-line options and a small optimization here and
there). This type of changes is usually better done incrementally to
be less risky and disruptive (this is specially important for
consensus-related code), and that makes my total commit count
unusually high, even when some people have contributed more in new
functionality than me in a single commit.
Also code movements (often required as part of these refactors)
produce unusually high total diff counts even when they require little
less than copy and paste (once you know what you want to move and
where, of course).
If I didn't thought this work is extremely important in the long term
(among other things, to make the code more accessible to new
reviewers/developers) I wouldn't be doing it, but you can't just
compare contributions counting commits or lines changed, that's not
how it works.
Github may say that I'm  with 96 commits / 9,371 ++ / 8,962 --, but
it's obvious to me that, say, gmaxwell  with 71 commits / 807 ++ /
707 -- has done a lot more for Bitcoin Core than I have.
Even if it was true that I'm the person currently coding more for
Bitcoin Core, I wouldn't write any of that if I had no hope of getting
review, so review is certain sense much more important than coding.
Who cares?
If my work is good for the software, my motivations are irrelevant. If
I accidentally PR a bug, my motivations are again: the bug should not
be accepted no matter how pure and noble my intentions are.
But, no, making Bitcoin's price (no offense taken, but there's an
spanish say that goes like this "S?lo un necio confunde valor y
precio" which translates to "Only a fool confuses value and price")
rise is not one of my main motivations.
I'm much more concerned about the long term success of the currency
(for which turnover is a much more interesting metric than market cap
IMO) and about learning a technology that I believe will revolutionize
the world, but maybe you don't believe me. There's a Bitcoin incentive
as part of my Blockstream's contract, so I have a financial incentive
for Bitcoin's price to increase, but, in fact, when I started
contributing to bitcoin core my bitcoin holdings where extremely low.
It bothers me that so many people seem to assume that Bitcoin
developers are also hardcore currency speculators and are also good at
it (I can say Bitcoin has teach me that I'm a terrible day-trader
There's many reasons to contribute to Bitcoin core and none of them
are relevant to this discussion.
The fact is that there's no "bitcoin developer dance" that makes it
rain and also raises bitcoin's market price 100 usd. And suggesting
"rising the price" as a solution to any problem just cannot be
considered a serious proposal.
No, we can't just ACK a "double the price" PR when the next halving comes.
If that's what you're doing as a currency speculator, that's fine.
It's just off-topic to this list.
And, no, that's not "what I am doing" as a software developer.
I want the system to improve, like that "Jessie J" singer said, forget
about the priceeeeeeeeee, yeah.

@_date: 2015-07-28 19:33:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
That's not what I said. We don't seem able to communicate with each other
efficiently, probably my fault since English is not my native language. But
I don't want to use more of my time (or yours) in this discussion, since
it's clearly unproductive.

@_date: 2015-07-29 20:00:10
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Actually validating blocks IS the equilibrium strategy. When the
subsidy is completely gone (or at least when the block reward is not
almost exclusively composed of subsidy [a future where fees are not a
completely negligible part of the total reward]), miners will
re-calculate their estimations and they will find out that mining
empty blocks won't be so profitable in a future with less subsidy. In
fact, with the incentives they currently have (negligible fees)
actually bothering about including transactions at all it's not really
worth it for them. They may just do it because they're nice people,
meta-incentives...whatever the reason is, they users are enjoying a
service they're not paying for.
Only subsidy and no fees creates other incentive problems, not just
SPV mining. But apparently some people think that scaring some users
with unreasonable expectations away because they have to pay fees
(still, non-proportional [to the amount you're moving] fees due to the
irreversibility of the payments: something the reversible payments
based on the banking industry can't simply compete with) it's much
worse than perpetuating big incentive problems that could break the
system. And, of course, short term convenience for users is much more
important than figuring out the long term viability of the system once
the seigniorage (spent on the miner's subsidy) goes away.
The pattern seems clear to be: decentralization and long term
viability don't matter too much to some people.
For some people, short term market cap seems to be the most important
priority and everything else is secondary.

@_date: 2015-07-29 22:27:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Consensus fork activation thresholds: Block.nTime vs 
When it comes to define thresholds for consensus fork activation there
are 3 options that I know of and each of them has at least a
disadvantage that the other 2 lack:
-Block.nTime: It's not monotonic
-median time: You cannot validate it without context (in contrast,
nTime is contained in the block header and nHeight in the coinbase)
-block.nHeight: You cannot know the exact time when a given height
will be reached.
I personally think that nHeight's disadvantage is the less worse, but
others will likely disagree. The point is we need to find a solid
criteria to decide.
When combining the threshold with a later miner's "voting" (upgrade
confirmation) on top of it, not being monotonic may be a real problem.
Doing that on top of height seems straight forward: check if
(prevBlockIndex.nHeight > threshold &&
With median time, seems safe too: if
(prevBlockIndex.CalculateMedianTime > threshold &&
Just a little bit less efficient.
It would look more like if (IsSuperMajority(...,prevBlockIndex,...) &&
GetFirstBlockUsedInVoting(prevBlockIndex).nTime > threshold)
But in some cases (say, an emergency consensus fork) you won't combine
the mining confirmation, so you may not have the prevBlockIndex
available and you may need to pass the height or medianTime down.
If the current block is not accessible from wherever the check is
being made, you would need an additional blockTime parameter as well.
Are there any example cases where a rule activation check doesn't have
the block available?
Of course, let's consider the following hardfork example:
before the hardfork: consensus_size(tx) = real_size(tx)
after the hardfork: after consensus_size(tx) = real_size(tx) +
that would allow miners to create bigger blocks if the transactions
help reducing the size of the utxo (and penalize transactions that
make the utxo grow by considering bigger when it comes to block
Well, at the block validation level (the most important one), you
obviously have block.nTime available.
But what if you're checking an unconfirmed transaction?
It's size (and thus it's validity and the policy relay decisions)
depends on whether the hardfork is activated or not.
So to check an unconfirmed transaction, you would need the block.nTime
of the next block, which is unpredictable (unless you're a miner)
because miners set those.
AcceptToMemoryPool already uses the nHeight (in fact, there's nHeight
and nSpendHeight there, not sure why we need to of them yet), so this
case would be trivial to implement.
Calculating the median time there wouldn't be difficult either: even
if globals weren't so heavily-used in AcceptToMemoryPool, the
prevIndex can always be passed down as parameter.
Some people may think that I'm discussing tiny details, but I would
really like that we can chose whatever is more generic for any type of
consensus fork and always use that from now on (instead of risking of
having to use 2 of them if we find out later that the chosen option is
not general enough).
It would be also nice to have only one uniform type of threshold in
Consensus::Params, and height seems to be the choice for softforks
that have been accepted long ago via miners'
voting/upgrade_confirmation, like in :
That doesn't mean it needs to necessarily be height: in a rebased
version of  we could replace consensus.nBIP34Height = 227931 with
consensus.nBIP34Time = .
But I would really like to have a uniform threshold mechanism instead
of using the 3 options depending on the fork.
I had assumed that height was the preferred option for everyone and
that's why I used it in
But judging from the existing blocksize hardfork proposals (using
block.nTime, the option I like less ) I was too fast there and clearly
I need to reopen the discussion.

@_date: 2015-07-29 23:46:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
Ok, let's assume we want to expose verifyHeader first (which I think
will be easier).
In  I had one (probably
stupid) proposal.
But it had so many preparations commits that I had to close it.
In the last commit
you can see that I'm adding a new function in
script/bitcoinconsensus.cpp with the following declaration:
int bitcoinconsensus_verify_header(const unsigned char* blockHeader,
unsigned int blockHeaderLen,
 const Consensus::Params& params, int64_t nTime, CBlockIndexBase*
pindexPrev, PrevIndexGetter indexGetter,
 bitcoinconsensus_error* err)
The ugly parts that you may not like are the CBlockIndexBase struct
(or maybe it's not so unreasonable) and the function pointer
To see their "ugliness" you can look at:
The PrevIndexGetter function pointer that Bitcoin Core would use
internally would be:
const CBlockIndexBase* GetPrevIndex(const CBlockIndexBase* pindex)
    return ((CBlockIndex*)pindex)->pprev;
with an ugly casting. But, well, I guess that's only ugly for Bitcoin
Core, not necessarily for other libconsensus users, which can define
their own function pointer, provided that it's of the form:
typedef const CBlockIndexBase* (*PrevIndexGetter)(const CBlockIndexBase*);
The struct that I think needs more refinement (and I just used what I
considered easier to implement at the time) is the CBlockIndexBase
struct itself:
+struct CBlockIndexBase
+ //! pointer to the hash of the block, if any. Memory is owned by
this CBlockIndexBase
+ const uint256* phashBlock;
+ //! block header
+ int32_t nVersion;
+ uint256 hashMerkleRoot;
+ uint32_t nTime;
+ uint32_t nBits;
+ uint32_t nNonce;
+ //! height of the entry in the chain. The genesis block has height 0
+ int nHeight;
I don't like phashBlock being a pointer instead of just a ref or even an object
Should that struct have a CBlockIndexBase* pprev; field (moving it
down from CBlockIndex)?
That's the kind of question where your feedback seems very important
from other-implementations developers (because you won't necessarily
take into account the difficulty of the refactors required in Bitcoin
Core to expose the right interface, and "libconsensus shouldn't care"
either, all we want is the best interface).
Agreed, and I would say all of the checkpoint check separation has
been done already.
What I mean by step functions is...look at verfyHeader internals, for example:
It internally calls Consensus::CheckBlockHeader (quite cheap with no
context required) and Consensus::ContextualCheckBlockHeader (not so
Bitcoin Core never calls (yet) the full verifyHeader at once. It does
the cheap tests first and the expensive later. For example,
call CheckBlockHeader, then CheckBlock (also cheap), then
ContextualCheckBlockHeader and then ContextualCheckBlock.
The question is, will other implementations want access to these
not-full-but-cheap tests?
In other words, apart from exposing VerifyHeader that fully validates
all consensus rules for a header, do we also want to expose
CheckBlockHeader and ContextualCheckBlockHeader to give more
flexibility to libconsensus' users?
I think, yes, other implementations will want this for the same DoS
reasons that Bitcoin Core currently wants them. But it would be nice
to know what a second person thinks about this.
In fact, one thing does: never changing the code again (but the cure
would be worse than the illness).
Agreed, any software changes in the consensus code can cause consensus
forks (and that's why you don't want to touch libconsensus that much
once it's separated).
Well, the "one true library" will be much better than the current "one
true full node".
The "one true library" would be the specification of the consensus
rules, but that doesn't mean you can't fork and modify it however you
I get this point, even if the current satoshi client contains the
consensus rules specification (and many other things, obviously), that
doesn't mean is somehow protected from forking with itself if the
consensus code is changed in the wrong way accidentally. But the more
separated libconsensus and Bitcoin Core (satoshi client) are, the less
likely that changes in Bitcoin Core that weren't supposed to change
consensus rules actually do it by accident (like last time with the
migration out of bdb).
I think alternative implementations using a full libconsensus can
increase their adoption a lot, since they become just as vulnerable to
consensus forks as Bitcoin Core (instead of more vulnerable like now).
You mean libbitcoin's code is better organized than Bitcoin Core's?
I don't doubt it. Maybe we can create a full-libbitcoin-libconsensus
first and work on the API there.
Oh, I see, you don't like that libsecp256k1 is currently a subtree of
Bitcoin Core either for the same reasons, right?
To not need to know when the changes in libconsensus are applied in
Bitcoin Core.
Mhmm, once libconsensus is complete, why would you care about it?
You just care about the libconsensus version (which doesn't have to
coincide with Bitcoin Core versions anymore).
For the sake of clarity, please say "use the library's API". It's
going to use the library one way or another.
To be clear, I don't oppose to "dogfooding", it's just clear to me
that it will take even longer.
So what I don't understand is "once libbitcoin is complete and ready
for us to use, we will keep using our reimplementation of consensus
until Bitcoin Core uses the API as well. If Bitcoin core doesn't use
the API, we prefer not to use the library at all and keep having the
same consensus risk. We will do what we think it's worse for us until
Bitcoin Core uses the library through the API".
And we will hopefully migrate the current libconsensus from openSSL to
libsecp256k1 soon. So we will be able to enjoy libsecp256k1's
performance improvements without risking consensus. One problem less.
This was just a joke because you said something similar earlier.
Don't take it seriously.

@_date: 2015-07-30 11:38:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
It is important ro note that even if lightning was never developed, the
block size remains at 1 MB forever and fees rise to 10 usd per transaction,
such "high fees" are still extremely competitive with non-decentralized
payment systems that have proportional fees. For example, 10 usd is still
lower than 1% when you are moving more than 1000 usd. I know, this doesn't
work for micro-transactions, but I don't think Bitcoin can be useful for
micro-transactions in the long term unless something like lightning payment
channels is deployed. Until we accept the second fact, it will be very hard
to discuss any projection of future usage. I think that believing that all
the transactions of the entire world population can be made in-chain while
keeping bitcoin decentralized is incredibly naive. Not even nasdaq has that
capacity (and if full node's require nasdaq's capacity, I don't think we
can talk about a decentralized system anymore).
On Jul 30, 2015 11:16 AM, "Venzen Khaosan via bitcoin-dev" <

@_date: 2015-07-30 16:10:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
On Thu, Jul 30, 2015 at 2:29 PM, Gavin via bitcoin-dev
I agree that having a "minimal hardware requirements" specification
would greatly help with this discussions.
These tests by Rusty (strong advocate of IBLT and working on it) seem
to indicate otherwise: On Thu, Jul 30, 2015 at 2:50 PM, Pieter Wuille via bitcoin-dev
I think the risks of a controversial deployment in consensus rules
changes, outweigh by far potential benefits of ANY consensus forks, no
matter how amazing the potential benefits may seem. Bitcoin may not
survive a controversial hardfork or go 3 years back in adoption,
nobody knows.
I agree. Unfortunately, technological and economic growth is very hard
to predict.
I just explained why I disagree with this point. Bitcoin fees depend
on transaction sizes rather than amounts moved. Even ignoring
script-based signatures and all the other advantages in Bitcoin, that
fact alone makes it extremely competitive with "traditional systems"
for many use cases (say, sending 1000 usd from the US to M?xico).
I agree overall with your other points.
Extremely cheap and instant transactions can be provided by lightning,
but cannot be provided by Bitcoin in-chain alone in the long term (it
can't even provide instant irreversible transactions).
Of all blocksize proposals, bip102 (the one with the single doubling
to 2MB) is the one I dislike less because it doesn't make any
assumptions about future technological or economic growth (I loved
your Bohr cite).
But it still has something that I dislike from all proposals: the
numbers just seem pulled out of a hat.
But I already created that testnet you propose (and
std::numeric_limits::max() -1 more testnets for other sizes)
in You can run it with the following runtime options: -chain=sizetest
Unfortunately, nobody seems interested in running some tests for
several sizes before proposing a concrete size.
As far as I know, nobody has used that branch to test different sizes.

@_date: 2015-07-30 17:12:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
1) Unlike previous blocksize hardfork proposals, this uses median time
instead of block.nTime for activation. I like that more but my
preference is still using height for everything. But that discussion
is not specific to this proposal, so it's better if we discuss that
for all of them here:
2) I think uncontroversial hardforks should also take miner
confirmation into account, just like uncontroversial softforks do. We
cannot make sure other users have upgraded before activating the
chain, but we can know whether miners have upgraded or not. Having
that tool available, why not use it. Of course other hardforks may not
care about miners' upgrade state. For example "anti-miner hardforks,
see But again, this is common to all uncontroversial hardforks, so it
would probably better to discussed it in
(gmaxwell assigned to bip99 to my bip draft).
3) As commented to you privately, I don't like to make any assumptions
about technological advancements (much less on economical growth). I
don't expect many people to agree with me here (I guess I've seen too
many "peak oil" [or more generally, peak energy production] plus I've
read Nietzsche's "On the utility and liability of history for life"
[1]; so considering morals, technology or economics as "monotonic
functions" in history is simply a ridiculous notion to me), but it's
undeniable that internet connections have improved overall around the
world in the last 6 years. I think we should wait for the
technological improvements to happen and then adapt the blocksize
accordingly. I know, that's not a "definitive solution", we will need
to change it from time to time and this is somewhat ugly.
But even if I'm the only one that considers a "technological
de-growth" possible, I don't think is wise to rely on pseudo-laws like
Moore's or Nielsen?s so-called "laws".
Stealing a quote from another thread:
"Prediction is difficult, especially about the future." - Niels Bohr
So I would prefer a more limited solution like bip102 (even though I
would prefer to have some simulations leading to  a concrete value
(even if it's bigger) rather than using 2MB's arbitrary number.
Those are my 3 cents.
[1] On Thu, Jul 30, 2015 at 4:25 PM, Pieter Wuille via bitcoin-dev

@_date: 2015-07-30 17:36:13
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Thu, Jul 30, 2015 at 4:05 PM, Gavin Andresen via bitcoin-dev
Apparently lightning doesn't require Segregated Witnesses: cltv and
rcltv may be enough (although I'm not up to date to the latest
designs). I definitely don't think we should wait to have SW ready to
be deployable in Bitcoin to have other hardforks. I think we should
have an uncontroversial hardfork as soon as possible, if anything, to
set a precedent and show the world that hardforks are possible in
Bitcoin, see Any scaling up innovation that happens in sidechains can be adopted by
Bitcoin too.
In fact, some of those changes (like op_maturity/rcltv/scv) are needed
in Bitcoin for a fully p2p Bitcoin sidechain to be even possible.
I really think lightning should be possible in Bitcoin main (and not
just sidechains) as soon as possible.
Not necessarily. How are older payment channels designs (different
from lightning) that don't even require cltv riskier than a hardfork?
In any case, yes, both things are kind of orthogonal and we can work
on both (and more) at the same time.

@_date: 2015-07-30 17:41:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
On Thu, Jul 30, 2015 at 4:52 PM, Thomas Zander via bitcoin-dev
I'm just saying that rational economic actors will prefer to pay 10
usd over 11 usd in fees.
My example was: 10 usd flat fee vs 1% fee (both numbers pulled out of a hat).
Well, 10 usd fees is cheaper than 1% fees for any transacted amount
greater than 1000 usd.
Take into account that this is just an extreme example to make my
point: hopefully fees will never rise to a value as high as 10 usd.

@_date: 2015-07-30 19:46:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
When considering "too conservative" options, let's not forget that,
say, scheduling 2MB for 2020 doesn't preclude us from deciding that
was too conservative and schedule, say 4MB for 2018 later. The first
hardfork would be "useless", but it would set a "minimum increase"
that would have been useful if the second one never happened.
To be clear, for this concrete case block.nTime would just work just
fine. I just want us to decide on one of the options and uniformly
recommend that options for all cases in BIP99 [just renamed the file,
 ].
But, yes, please, let's discuss this in the other thread.

@_date: 2015-07-30 19:51:12
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
Great! No, I'm not running any node right now at any size.
Take into account that it's not a regular testnet (ie like testnet3),
it's a regtest-like testnet to make mining and simulations cheap.
That also means that anybody can trivially create reorgs, so it is
expected to be used in a controlled environment (you can control which
node creates a new block and when).

@_date: 2015-07-30 20:00:21
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
Gavin, Pieter, Mark, Gary, can we move the median time discussion to
its own thread?
I really don't want to fill this thread with that discussion.

@_date: 2015-07-30 20:14:47
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] 
On Thu, Jul 30, 2015 at 3:14 PM, Tom Harding via bitcoin-dev
The blocksize limit (your "production quota") is necessary for
decentralization, not for having a functioning fee market.
But I have the hope that hitting the limit would help with getting rid
of all that special-case or at least would encourage miners to
implement their own policies.
If we can agree that hitting the limit will JUST cause higher fees and
not bitcoin to fail, puppies to die or the sky to turn purple I think
that's a great step forward in this debate.
(as said, I think it can even have positive consequences; for example,
higher fees may be just what is needed for more scalable solutions
[like payment channels] to be adopted by bitcoin companies). Hitting
the limit may produce a more healthy market, but it is true that a
market for fast confirmations already exists.
Unless we want to completely get rid of the blocksize limit (which I
would consider another debate entirely), we will eventually hit the
limit anyway. Why not now so that we can make sure the software is
completely compatible with having a limit?
Why we can hit the limit eventually but not now?
(As said, unless you think the limit should be completely removed forever).

@_date: 2015-07-30 20:16:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] 
s/all that special-case/s/all that special-case policy code for free

@_date: 2015-07-31 03:21:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] 
Yes, Adam Back and I some times see things differently, and that's fine.
Many times, we realize later that we're saying the same thing with
different words and we're just discussing about terminology. That's
not an exclusive problem we have, it's a universal communication
problem. That's why math (which is nothing but a language) was
invented: to never discuss about terminology, to force any used
concept to be defined beforehand.
Sorry for the distraction, but I think this is one of those times.
Whether "hitting the limit" is "necessary" (I bet he never said
"strictly necessary") or just "helpful" is not very relevant. I think
Adam and I agree that hitting the limit wouldn't be bad, but actually
good for an young and immature market like bitcoin fees.
Apart from the dubious time-preference premium (dubious because in
most cases is just wallet's defaults and not users in a hurry),
transactions are basically free if you are willing to wait (and
apparently not that much).
If I was a miner and you want me to include your transaction for free,
you're asking me to give you money, which I would prefer to do
directly if you are a friend or a non-profit organization that I like
or whatever rather than giving you money through bitcoin fee
By including your transaction, I'm increasing the probability of my
mined block being orphaned and you're not willing to give me even a
single satoshi in exchange.
Today, in practice, one satoshi fee and no fee at all are treated
exactly the same by most (maybe all?) miners, which if you ask me, I
find very ~~unfair~~ economically absurd.
Are all miners just stupid?
Not necessarily, they just don't care about fees or transactions at all.
Who is to blame? Certainly not the value chosen for the block size
limit, it's clearly the subsidy's fault: subsidy is all miners care
about (by the way, that's also the illness behind the SPV-mining
symptom). I am very worried about excessive mining subsidies (if you
knew how worried the freicoin community was [and still is] about this
problem, even if freicoin probably has one of the lowest mining
subsidies out there [currently and perpetually annual 5% of the
monetary base]...).
And I think that "hitting the limit" is not a catastrophe at all, but
rather an opportunity to motivate miners to start caring more about
transactions and fees (in proportion to what they care about).
And if the limit is increased later and fees fall again, that's fine,
because miners' will already be more prepared for the next time we
"hit the limit".
Anyway, maybe that hope is irrelevant, but what I'm convinced about is
that rising non-fast-confirmation mining fees above zero is not a bad
To be honest, I've only followed those were assuming the worse case
for optimization: bitcoin global monetary monopoly.
If I remember correctly, they were aiming for something around 170 MB,
but in any case, any value for the constant is completely arbitrary to
me at this point, including 1 MB. I'm deeply offended when I feel
included in the "1MBers group" because I don't feel like that at all.
To be honest, I have no idea what the correct value should be, all I
know it's a trade-off in a monotonic function:
f(blocksize) = decentralization
You are completely right: there's no defined measurable unit for
"decentralization" ("p2pness", whatever bitcoin has that wasn't
possible before pow-based distributed consensus).
And I'm afraid we will never have such a measurable unit. Maybe the
best definition of the property we're trying to capture is just "the
opposite of centralization", assuming centralization is easier to
The best we have now are pool percentages, number of nodes,
subsidy/fee ratio (as said, this influences things like SPV mining)
How all that gets to...?
g(many unrelated matrics) = centralization
I don't really think anybody knows, but no matter what your
interpretation of some Japanese-named dude on the internet's words
(aka bitcoin sacred history) are, if you think 3 validating nodes is
enough for a "p2p" monetary network.
It is very possible that decentralization(blocksize) =
decentralization(blocksize+1) for many values of blocksize, but I
think the burden of the proof that decentralization(current_blocksize)
~= decentralization(current_blocksize+1) is on those who propose
But I think ANY metric for centralization would be welcomed right now.
In fact, it doesn't need to be a function of blocksize, it can be a
function of maxBlockSigops or maybe even maxBlockInputs or
But if we don't want to have any consensus limit to centralization
bitcoin has already fail (and doesn't need expensive proof of work).
It is a certainty that fees will be necessary someday: bitcoin's
seigniorage is limited to 21 M to subsidize mining, and we know that
won't last forever. Expensive proof of work (that centralized systems
lack) must be paid for somehow.
Who's child am I asking to work in a factory? I feel I'm missing
something there.

@_date: 2015-07-31 03:29:21
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] 
I'm re-reading and I have many spelling errors, sorry.

@_date: 2015-07-31 13:51:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
He is not saying that. Whatever the reasons for centralization are, it
is obvious that increasing the size won't help.
In the best case, it will only make it slightly worse. How big of a
"slightly worse" are we willing to risk to increase the size is the
open question.
As far as I know, people just want to change an arbitrary number for
another arbitrary number.
But this arbitrary cap is a cap to centralization, not a tool to make
Bitcoin-Qt more important or to attack concrete Bitcoin companies like
you seem to think.
If you don't think the blocksize cap helps limiting centralization and
you think it would be fine to completely remove it, then it would be
better for the conversation that you said that directly instead of
supporting other arbitrary caps like 8GB (bip101).
I think it would be nice to have some sort of simulation to calculate
a "centralization heuristic" for different possible blocksize values
so we can compare these arbitrary numbers somehow. Even if the first
definition of this "centralization heuristic" is stupid, it would be
better than keep rolling dices and heatedly defend one result over
To reiterate, If you don't think the blocksize cap helps limiting
centralization, please, say so.
If we can't agree on what the limit is for, we will never be able to
agree on whether 1MB (current situation) or 8GB (bip101) is the most
appropriate value to have at a given point in time.
Lightning is nothing more than a better design for trustless payment
channels, but it's really good that you agree that if we want to scale
not everything can be broadcast in-chain.
What he means is that if Bitcoin needs to support a scale that is only
feasible with high degrees of centralization (say, supporting 1 M tx/s
right now), then it has already failed in its decentralization goals.
In fact, with only a few miners, I'm not sure regulators will still
agree Bitcoin transactions are irreversible...
But you are right, we haven't tried to destroy bitcoin by removing the
only available consensus tool to limit centralization yet.
I don't want to try, do you?
Let's go to "most people use bitcoin" first and then think about "many
people ONLY use Bitcoin" later, please.
I believe everybody here thinks that the more people are able to use
Bitcoin, the better.
But that doesn't
Risking destroying Bitcoin through centralization to be able to keep
free transactions for longer it's a very risky gamble.
Doing so explicitly against the will of some of the users by promoting
schism hardfork, and thus risking to economically destroy both Bitcoin
and Bitcoin_new_size (different currencies) in the process is also a
very risky gamble.
So may want to give some example of responsibility yourself to make
these calls to responsibility more credible.
You certainly cannot know what "all the payment processors and
startups plans" are based on, and spreading conspiracy theories about
the evil secret plans of Blockstream (or any other Bitcoin company)
doesn't help in keeping this discussion civilized, contaminates
bitcoin development in general and unhealthily polarizes the whole
Bitcoin ecosystem. Also, I believe is doing a disservice to your
reputation among technical people, but since you don't seem worried
about that, why should I be?
Are you suggesting that bitcoin consensus rules should be designed to
maximize the profits of Bitcoin exchanges?
I assume not, but I'm really having troubles trying to read the
question with another meaning.
Can you rephrase this, please?

@_date: 2015-07-31 16:33:14
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Block size following technological growth 
How more users or more nodes can bring more miners, or more
importantly, improve mining decentralization?
I don't think anybody is defending that position so you can stop refuting it.
Well, negatiations don't make the number less arbitrary. As far as I
know, the sequence of events was this:
1) Gavin proposes 20MB to 20GB
2) Some chinese miners say they can securely handle at most 8 MB.
3) Gavin proposes 8 MB to 8 GB
In any case, history is irrelevant for this point: if party 1 proposes
arbitrary value A and party 2 proposes arbitrary value B, any
"compromise" value between those 2 is still an arbitrary value. For
example, A + ((B-A)/2) is still an arbitrary value.
I'm sorry, but until there's a simulation that I can run with
different sizes' testchains (for example using  to somehow
compare them, I will consider any value arbitrary. A "I run this with
blocksize=X and nothing seems to have broken" doesn't help here.
We need to compare, and a criterion to compare.
Agreed on the first sentence, I'm just saying that the influence of
the blocksize in that function is monotonic: with bigger sizes, equal
or worse mining centralization.
About the second sentence, yes, I could destroy Bitcoin by changing
one single constant if I could somehow force users to adopt my version
of the software. I'm sure I can actually find several examples if
necessary. "Through centralization" is harder, but say we chose
std::numeric_limits::max() as the maximum block size (in
practice, entirely removing the block size limit), then the consensus
rules don't have any rule to limit mining centralization.
Sacrificing that tool, and doing so this early on could certainly turn
Bitcoin into an effectively centralized system, destroying Bitcoin (or
at least the "p2p currency" part of it, which is the most interesting
one for many Bitcoin users including me).
So, once it's clear that nobody is saying that centralization depends
ONLY on the block size, can you tell us please if you think it's
useful for limiting mining centralization or not?
If you think the blocksize consensus limit does nothing to limit
centralization then there's no tradeoff to consider and you should be
consequently advocating for full removal of the limit rather than
changes towards bigger arbitrary values.
Otherwise you may want to explain why you think 8 GB is enough of a
limit to impede further centralization.
Sorry, I don't know about Pieter, but I was mostly talking about
mining centralization, certainly not about payment services.
No, that's not what we are doing.
It's good that you talk about your fears but, please, let other people
talk about theirs on their own.
I can imagine a non-for-profit exchange but there's no point in
finding edge cases: no general disagreement here.
My first post on the bitcoin forums (and vague hardfork proposal, I
started reading in December 2010) was January 21, 2011 (vs yours Dec
14th 2010, as Greg pointed out in the other thread). I bought my first
bitcoins (and also sold most of them shortly after, stupid me) using
some web that used paypal and was closed down not too long after that.
At first I couldn't participate in exchanges because I had no Liberty
Reserve account...
Look, I'm sure there's many stories about how we met Bitcoin that we
can share having a beer in a bar or something. But probably most of
the subscribers to this list don't really care, and if they care they
can ask us privately, or you can create a new thread (probably better
in bitcointalk or somewhere else than here): they are completely
irrelevant in this technical discussion.
So, back on-topic: do I agree that exchanges are extremely important
for the Bitcoin ecosystem?
Yes, of course I do.
But that doesn't mean that their "potential for future profit" should
be a primary concern when deciding consensus rules changes that affect
ALL users.
But even before that, I disagree with the premise that "not rising the
consensus blocksize as soon as possible" will ruin the exchanges or
"remove their potential for future profit".

@_date: 2015-07-31 17:24:31
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] 
On Fri, Jul 31, 2015 at 11:56 AM, Thomas Zander via bitcoin-dev
As a rhetorical exercise, I just asked myself those questions (with
other words) in the very post you are replying to.
Please, read again.
If miners have a cost in including transactions (which they have) but
there's no gain, why are miners including free transactions?
Is it because they are stupid or because they don't care enough about
fees (and thus blindly use whatever default policy that comes with
Bitcoin Core)?
On Fri, Jul 31, 2015 at 2:32 PM, Oleg Andreev via bitcoin-dev
You are completely right, this is what matters in the end. To correct
myself, what I'm worried about is how low the fees/profits ratio is,
fees/total_reward is just an easier-to-calculate approximation when
you don't know costs = total_reward - profits.
Don't forget a rise in fees paid as another potential factor. That was
my whole point: higher fees may help reducing problems related to a
low fees/profits ratio.
And that's why I don't think a rise in fees is necessarily a bad thing.
Let's not forget that we're just talking about market fees for
non-urgent transfers rising above zero!
There may be a fee market for fast confirmations already, but there's
certainly none for non-urgent transfers.
In my opinion, rising from zero to anything, it's a great step
forwards. I can perfectly understand that maintaining that anything
low is good for adoption, but insisting in maintaining it at zero
doesn't seem very reasonable to me, given that we know for a fact that
is not sustainable in the long term.
We don't want business plans to fail because they're relaying on free
transactions. We don't want new users to be lied about the real
properties of the system.
And I'm sure that any ridiculously low value will be so marginally
worse for adaption when compared to a plain zero, that I'm not worried
about it at all.
Users starting to pay SOMETHING for a service they're enjoying and
that actually has quite big operational costs (energy-demanding proof
of work, currently subsidized by the finite initial seigniorage) it's,
by no means, the end of Bitcoin.
To me is really more of a start, a tiny first step towards a viable
system that doesn't depend on subsidies (with expiration date).

@_date: 2015-07-31 22:37:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [Bitcoin-development] [BIP draft] Motivation and 
On Fri, Jul 31, 2015 at 7:40 PM, Thomas Kerin via bitcoin-dev
There was a document from the start, but after I got the BIP number, I
was renaming the file, moving from org-mode to mediawiki and getting
the code ready.
I'm sorry, I broke the old link to the document, here's the new one:
Maybe I should create a PR already to have a permanent link, I don't know.
As said in the document, the code is now here:
Also, I should mention that one particular discussion related to this
BIP (whether we should use Block.nTime, median time or block.nHeight
for the activation thresholds) is being discussed in:
The BIP is currently assuming that the preferred choice for all
non-emergency uncontroversial hardforks is defining a starting
block.nHeight after which miners start confirming their upgrade. Once
the 95% threshold is reached the hardfork takes effect.
Long after that, after that first block enforcing the new rules is
deeply buried, that check can simply replaced by re-defining the
threshold height not with the height when miners started voting, but
simply with the height in which the rules started being enforced for
the first time (see

@_date: 2015-07-31 23:30:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Fri, Jul 31, 2015 at 2:15 AM, Milly Bitcoin via bitcoin-dev
All this sounds very reasonable and useful.
And if a formal organization owns this "process", that's fine as well.
I still think hardforks need to be uncontroversial (using the vague "I
will know it when I see it" defintion) and no individual or
organization can be an "ultimate decider" or otherwise Bitcoin losses
all it's p2p nature (and this seems the point where you, Milly, and I
But metrics and data tend to help when it comes to "I will know it
when I see it" and "evidences".
So, yes, by all means, let's have an imperfect decentralization metric
rather than not having anything to compare proposals. Competing
decentralization metrics can appear later: we need a first one first.
I would add that we should have sets of simulations being used to
calculate some of those metrics, but maybe I'm just going too deep
into details.

@_date: 2015-06-16 09:10:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
What about commitments that don't use op_return (ie pay2contract
In any case, if the motivation is ordering in multi-party transactions
there should be ways to do it without any consequences for other
transaction types' privacy. For example you could have a deterministic
method that depends on a random seed all parties in the transaction
previously share. That way the ordering is deterministic for all parties
involved in the transaction (which can use whatever channel they're using
to send the parts to also send this random seed) while at the same time the
order looks random (or at least not cannonical in a recognisable way) to
everyone else.

@_date: 2015-06-18 17:23:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
adequately handle the situation when blocks fill up
This will have to eventually be done in addition to any other "alternative"
unless the plan is to keep rising the limit until it is removed or
Maybe this should be the priority? Maybe this is the "alternative" that
some no-block-size-limit proponents (meaning people who think that
centralization is not a concern when deciding the block size limit) claim
nobody was putting forward?
Anyway, it's sad that we're always mixing 2 different topics: hardfork
deployment and blocksize limit. I wish we talked more about the former, I
wish we would have talked about it it long before the block size debate
became "urgent" (or at least before it was being perceived as urgent).
We've had plenty of time to deploy non-emergency hardforks but apparently
no one was interested (say, for fixing miner but known bugs like the
timetravel attack).
In fact, I plan to eventually propose such a fork, I agree with gavin that
"hardforks aren't possible" is not an answer, though finding opposition to
a concrete hardfork in a concrete point in time doesn't mean that
"hardforks aren't possible". I believe I have proposed many more hardforks
than Gavin, all of them rejected and I still hope some of them will
eventually make it into bitcoin main.
When it was clear that wouldn't be the case I'm afraid the only answer is
creating an altcoin (like Mark and I did with Freicoin and "xtcoin" could
become [hopefully not destroying bitcoin main in the process]).

@_date: 2015-06-18 20:49:35
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
But according to Alex's explanation (which I think is very good
leaving asides some cases like change of the pow hashing function, for
example) there's no individual that can force or veto a change. It is
the decision of each individual user and their own "pretty much
everybody" may vary. But this "pretty much everybody" is what Mark
referred to with the "I know it when I see it."
If you recommend users to apply changes when this criterion is met but
you know there's still many users who don't agree with the change,
then you're acting irresponsibly by promoting a chaotic consensus fork
where coins can be spent in both chains at once.
Well...unless you're promoting it as an altcoin that simply happens to
distribute part of the initial monetary based to bitcoin holders at
block X and whose genesis block is equal to bitcoin's genesis block at
block X. I guess in that case you wouldn't necessarily be
"Miner's vote" is irrelevant here since it cannot tell you anything
about users adoption (besides miner's adoption of course).
But this is only relevant for the point 1. Software projects can have
dictators, forks and everything else other free software projects
have. But consensus-based p2p blockchains cannot change their rules in
the same way, otherwise they're centralized.
THERE CANNOT BE A VOTING PROCESS FOR CONSENSUS CHANGES.
If anybody can vote, hod do you prevent the sibyls from outvoting everyone?
If not everybody can vote, how is the voters' list determined without
centralizing the system?
If we had a technical solution to this problem we wouldn't need proof
of work in the first place!!

@_date: 2015-06-19 13:31:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
He doesn't mean that: he means solving the mempool and crashes and
hitting the limit would have.
If the chain has limited size it is a scarce resource and people have
to bid for it: nothing unexpected or wrong about that.
Of course, people that believe the limit should be completely removed
eventually because they don't care about mining being decentralized
(or fail to see the relation between the two) may have a very
different view about this.
Do you think that this ratio is unrelated to an abundant (non-scarce)
block size?
When is the right time to allow space pressure to rise that ratio?
When the subsidy is at 1.5625, for example, it may be too late to
start a non-catastrophic transition from subsidies to fees.
I don't claim to know that, but it's something that worries me.
No matter how many people say "that's too far away in the future to
worry about it", I still worry about it and I'm sure more people do.
What if "when it's time to care about it" we discover that we should
have started to do things about it long ago to minimize the risks of
this transition?

@_date: 2015-06-20 23:22:41
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] [BIP draft] Motivation and deployment of 
This is an attempt to unify views on why and how hardforks can be
deployed. I would like to turn this into an informational BIP later
after gathering some feedback.
Please, do not discuss block size issues here: there's plenty of
threads to do so. The scope of this one is only hardforks and
softforks in a more abstract way. Sometimes block size changes are
used as examples because no other example came to mind
(non-blocksize-related examples for the same cases [or others] are
a user should be just ignored. But what if the welcomed), and
if we go into too much detail they stop being useful as examples. So
please, try to avoid going into too much detail about the concrete
examples when possible.
Please, feel free to make suggestions or bike-shed some of the terms.

@_date: 2015-06-21 01:16:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
I disagree with this premise. Please, don't take this as an argument
from authority fallacy, but I will cite Satoshi to express what I
think the assumptions while using the system should be:
"As long as a majority of CPU power is controlled by nodes that are
not cooperating to attack the network, they'll generate the longest
chain and outpace attackers."
I can't say for sure what was meant by "attacking the network" in this
context but I personally mean trying to rewrite valid and
proof-of-work-timestamped history.
Unconfirmed transactions are simply not part of history yet. Ordering
unconfirmed transactions in a consensus compatible way without a
universal clock is impossible, that's why we're using proof of work in
the first place.
Alternative policies are NOT attacks on the network.

@_date: 2015-06-21 01:20:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Non-repudiation can be built on top of the payment protocol layer.

@_date: 2015-06-21 09:27:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Fwd: Re: [RFC] Canonical input and output 
---------- Forwarded message ----------
On Tue, Jun 16, 2015 at 10:06 AM, Rusty Russell Here's a short explanation and the code:
Here's a longer explanation with a concrete use case (the contract is
the invoice):
Great. Well, then all I'm saying is that I like this as plan A.

@_date: 2015-06-21 12:31:34
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] [BIP draft] Motivation and deployment of 
You mean the timewarp fix can be coded as a softfork instead of a
hardfork? How so?
If that's the case, do you have a better candidate?

@_date: 2015-06-26 13:13:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process and Votes 
This statement seems "cultish" by your own definition.
I'm going to make the opposite statement:  the consensus on code
changes is almost always 100%.
Mark has already given a couple examples of changes to consensus rules
(the most risky type of change), here's a few thousand other examples
of changes to the bitcoin core's code that had no opposition:
Can you please point us to a few examples were changes were made with
opposition to them?
In those cases (which you assure is what happens almost always), would
you say that the result of letting a decider decide instead of fixing
or addressing all the concerns (either by changing the proposed code
or explaining it) better in restrospective?

@_date: 2015-06-27 12:13:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
Obviously those who claim that you or "committers" or "developers" are
in control of the consensus rules are far from understanding this
life-threatening part. If you, Gavin or anyone becomes "the president
of bitcoin" he will likely get killed, or kidnapped, or get his family
kidnapped, or tortured...
I fully agree with what you've said but there's an argument I
sympathize with: "hardforks must be possible". Otherwise it seems that
the system is "eventually obsolete by design".
Provided they're also uncontroversial, they don't need to be that
different (in terms of deployment) from softforks. Since they risks
are bigger you just need to give more time for users and alternative
software to upgrade.
I would really like deploying an uncontroversial hardfork to prove
nobody wants them to be impossible, as explained in:
I hear people claiming that "hardforks must be possible" here and
there, see this example:

@_date: 2015-06-27 13:04:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
But that option is not unknown, that's the point of this thread.
"Doing nothing" would require to fix the mempool to scale with the
number of unconfirmed transaction. This is something that we will
eventually have to fix unless the plan is to eventually remove the
blocksize limit.
What will happen with full blocks is that fees will likely rise and
the transactions with bigger fees will get confirmed first. This is
something that will eventually happen unless the blocksize limit is
removed before ever being hit.
What this thread is saying is that this option (the so-called "doing
nothing" option, which actually requires more work than any of the
current proposals for increasing the blocksize) is perfectly valid,
which is in contradiction to a perceived "need to increase the
blocksize limit soon". Increasing the block size is only an option,
not a "need". And changing the consensus rules and forcing everybody
to adapt their software to the changes is certainly not "maintaining
the status quo", I'm getting tired of hearing that absurd notion.

@_date: 2015-06-27 13:28:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process and Votes 
Yes, I understand that it may be difficult to define
"uncontroversial", as I explain in
Can you provide anything to back your claim?
Note that even if that's true, still, Bitcoin core != Bitcoin consensus rules.
Well, yes, github is centralized and so it is bitcoin core development.
But bitcoin core developers don't decide hardfork changes.
So far, softfork changes have been made because they have been
considered "uncontroversial", not because there's any centralized
negotiating table or voting process to decide when to force every user
to adapt their software to new consensus rules.

@_date: 2015-06-27 13:43:55
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
Ok, so then the decision is to either change a policy or change a
consensus rule and then maintaining the status quo is simply not
Since per-node and per-wallet policies weren't universal anyway
(nobody can be forced to run the standard policy), making some changes
to the policy code of the different implementations that weren't
prepared for the current consensus rules (that includes bitcoin core)
seems orders of magnitude closer to "maintaining the status quo" than
a hardfork.
It's interesting to note that increasing the blocksize without fixing
the underlying problems that make it a perceived "need" will leave the
implementations unprepared for the new rules too (it is just
unprepared for ANY block size limit, not specifically unprepared for
1MB blocks).
So increasing the block size is actually the "lazy option" regardless
of how the "doing nothing option" is perceived by many uninformed
participants in the discussions.
Then I guess by "maintaining the status quo" some people just mean
"not fixing the known problems we have yet, leave it for later". Not
only some people propose to delay solving this problems: they don't
even want to be forced to fix them in 20 years!
That...or they just want to remove the block size limit entirely
forever, don't fear centralization, and are not being clear about it.

@_date: 2015-06-28 14:03:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
On Sat, Jun 27, 2015 at 2:09 PM, Wladimir J. van der Laan
Yes, I specificalyl say that here
 (just
with 4 years instead of 5).

@_date: 2015-06-28 14:13:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
Fortunately we have a lower limit in the standard mining policy to see
if the skies turn purple when we hit that limit like some people
But this is NOT a way to see the majority of anything. I can run 1000
nodes, have you heard of sybil attacks?
There's simply no decentralized way of voting that works. Otherwise we
could vote on each block instead of using proof of work.
Miners voting on size is also ridiculous since big miners have an
incentive to completely remove the limit and make smaller miners
No, this is very important. The majority has no right to dictate on
the minority.
If the majority of bitcoiners wanted demurrage (and we actually had a
working method for "measuring majorities"), the minority would still
say "these are not the rules we signed up for, go make freicoin as a
separate chain".
And that is very reasonable. If some people want a more centralized
version of Bitcoin they can create an altcoin too. Doesn't dogecoin
already have big blocks?

@_date: 2015-06-28 14:30:55
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process and Votes 
I wasn't asking for an example of something that was rejected, there's
plenty of those.
You were saying people were opposing a change and jgarzik unilaterally added it.
When did that happen?
There are many pieces of software and many maintainers, libbitcoin,
for example, is another full node implementation different from
Bitcoin core.
Also, to change Bitcoin core I don't need to convince anyone, I do it
all the time here Maybe Bitcoin core devs have more influence, but still, they don't
have the power to decide for everyone else what the consensus rules
Your analogy is ridiculous, it literally takes seconds to fork bitcoin
and is as simple as clicking a button.
Wladimir has explained many times that he hasn't decided anything
because he can't decide that.
You keep insisting that he has control over consensus rules. Are you
doing it because you want him to be threaten, tortured, kidnapped or
If you don't, please stop making false claims about powers he doesn't
have because some bad guy could believe you.
For the last time, they may have control over Bitcoin core (one
implementation of the Bitcoin protocol), not the consensus rules.
Why are anyone's bitcoin holdings relevant in any technical discussion?
Please, keep this kind of offtopic comments out.
As said several times, yes, it is hard to define "uncontroversial"
without giving veto powers to any random guy on the internet.
But this is clearly not what is happening now. Most Bitcoin core devs
are against the current proposals, that cannot be considered
uncontroversial for any sane definition of it.

@_date: 2015-06-28 17:05:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
I think you're not contradicting me: ff there's not rights built into
the system, the majority has no "right to dictate" anything.

@_date: 2015-06-28 17:35:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process and Votes 
Ok, I misunderstood.
Well, the fact is that the number of capable reviewers is quite small.
If more companies hired and trained more developers to become bitcoin
core developers that situation could change, but that's where we are
Only if you change the consensus rules (which are, in fact, a
relatively small part of the code).
Mike mantains Bitcoin XT and that's fine, Peter Todd maintains patches
with the replace by fee policy, libbitcoin also changes many
non-consensus things, there's code written in other languages...
There's multiple counter-examples to your claim of that argument being faulty.
Seriously, forking the project is just one click. You should try it
out like at least 9627 other people have done.
code yourself) and maybe they're also fine being insulted by you as
part of the job.
What you still can't do is unilaterally change the consensus rules of
a running p2p consensus system, because you cannot force the current
users to run any software they don't want to run.
Please, stop the cultist crap. Maybe insulting people like that is how
you got people to call you a troll.
But, yes, you are right: there's no known mechanism for safely
deploying controversial changes to the consensus rules
Well, if you don't think he has control over the consensus rules we're
I think that was implied from some of your previous claims. He is no
"decider" on consensus changes.
Insisting on it can indeed get him hurt, so I'm happy that you're
taking that back (or clarifying that really wasn't your position).
Influence is very relative and not only core devs have "influence".
Maybe Andreas Antonopolous has more "influence" than I have because he
is a more public figure?
Well, that's fine I think. I don't see the point in discussing who has
how much influence.
Please, don't generalize. I don't think I put myself in any kind of pedestal.
That is insulting to me and many others (you may not even know and
you're insulting them).
And I think my Bitcoin holdings are completely irrelevant when judging
my contributions to the software: either they're good or not, and who
I am or how many Bitcoins I have at any given time shouldn't matter.
Again, nobody forces you to use our software, as said there's
alternatives (including forking the project right now).
Well, for now the process we have is seeking consensus, and although
our definition of "uncontroversial" is very vague, I think it is quite
obvious when a proposed change is not "uncontroversial" (like in the
block size debate).
It seems to me that any other "formal process" would imply
centralization in the decision making of the consensus rules (and from
there you only have to corrupt that centralized organization to
destroy Bitcoin).

@_date: 2015-06-28 17:45:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The need for larger blocks 
If the Schism fork goes wrong (ie 2 chains coexist in parallel for
long) the result may as well be that NOBODY will be left any value.
Both the majority and the minority can lose simultaneusly.
See That kind of hardfork is basically like forcing the users to go to war
against each other.
Really, I don't think civil war is an exaggerated analogy.
That sounds great. Do you have any proposal in mind?
I really want hardforks to be made, I just don't want to kill the
system attempting it.

@_date: 2015-06-28 17:51:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Original Vision 
What about a TXO and a STXO O(1)-append commitment? That shouldn't
cause that much overhead and you can build UTXO from TXO - STXO.
I know it's not so efficient in some respects but it scales better I think.

@_date: 2015-06-28 19:47:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] TXO + STXO vs UTXO Re:  Original Vision 
But you can prune them.
No, no.
You don't need a non-constant update of any spent flag (because
there's none), that's the whole point of having 2 separated trees for
everything on one side, and only spent outputs on the other side.
This proposal is not useful for SPV wallets but it lets you build the
UTXO at any height from the committed txo + stxo trees and update it
yourself from there. You could have a fast synchronization mode in
which you're not really a full node from the beginning but you end up
validating the older blocks later, when you have time after
synchronizing to the tip of the chain.
For the SPV use case you would need a committed UTXO (or the TXO with
a fIsSpent bit) but that seems to be more complicated and can be done
separately later.

@_date: 2015-06-28 19:53:40
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Unlike other payment channels designs, the lightning payment channel
network allows you to pay to people that you haven't sent a pre-fund
There's must be a path in the network from you to the payee.
That's simpler with only a few hubs although too few hubs is bad for privacy.
Worried about financial institutions using Bitcoin? No. Who said that?
Remember the hubs cannot steal any coins.
I don't see how people could pay coffees with bitcoin in the long term
Bitcoin IOUs from a third party (or federation) maybe, but not with
real p2p btc.

@_date: 2015-03-24 13:08:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] BIP70: why Google Protocol Buffers for 
That case is very unlikely IMO, but still you can solve it while keeping
hash of the genesis block as the chain id. If a community decides to accept
a forking chain with new rules from block N (let's call it bitcoinB), the
original chain can maintain the original genesis block and the new
community can define N (which is not accepted by bitcoin due to the new
rules) as the genesis block for bitcoinB for the purposes of chain ID. As
said forking bitcoins and  bitcoinsB with the same owners doesn't make much
sense to me. If you're creating a new currency you can just as well define
a new chain. If you want to start an initial utxo giving the new coins to
bitcoin holders...I still don't see the point, but you can also do that in
a new chain.
In summary, your example is not a good reason not to adopt a hash of the
genesis block as chain ID.
On Mar 14, 2015 5:22 PM, "Isidor Zeuner"

@_date: 2015-05-04 13:24:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
What I was describing was an attempt to fix a similar proposal by Mark
Friedenbach, but it didn't needed fixing: I was simply
misunderstanding it.
Mark's RCLTV is completely reorg safe, so there's no need for the 100
block restriction. It also keeps the script validation independent
from the utxo.
Here's is how it works:
The operator takes a relative_height parameter and it checks that the
nSequence of the input is lower than that parameter.
Additionally, a new check at the transaction level:
for (unsigned int i = 0; i < tx.vin.size(); i++) {
            if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)
                return state.Invalid(false, REJECT_INVALID,
Well, this is assuming that we're only using it with heights and not timestamps.
Mark, feel free to elaborate further.

@_date: 2015-05-05 21:19:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Well, apparently the timestamp can be make compatible with Mark's
nSequence-based RCLTV by adding an additional check at the block level
but I was only explaining the concept using heights (which is the most
interesting part IMO).
I'm also not sure I understood the details and I don't want to confuse
people again, so I'll wait for someone else to explain that part.
ACLTV can work with timestamps too unless I'm missing something. It's
just more complexity and I was never convinced that there's enough use
cases relying on timestamps to justify them. But the timestamp
discussion is quite orthogonal to the nSequence-based RCLTV proposal

@_date: 2015-05-06 09:37:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
Yes, sorry, I changed it just before sending from "what needs to be
satisfied for the validation error to trigger" to "what needs to be
satisfied for the tx to be valid".
You're right.
Yes, this would be the simplest solution. Another option would be to
have a new tx version in which IsFinal(CTransaction) doesn't check the
inputs sequences to be 0xFFFFFFFF for the tx to be final.
Well, the semantics of nSequence don't really change completely. In
fact, one could argue that this put it closer to its original
But in any case, yes, already signed transaction should remain valid.
No transaction would become invalid, just non-final.
As soon as the height of its inputs plus their respective nSquences
get higher than current height they will become final again.
I cannot think of any use case where a tx becomes invalid forever.
Also, probably most people have usedrelatively low values for
nSequence given the original semantics, just like the relative lock
nSquence will likely be used as well.
To be clear, this proposal is supposed to replace RCLTV, so there
would still be 2 options. But please let's imagine we have infinite
opcodes in this thread and let the "should we design an uglier
scripting langues to save opcodes?" question in the other one.
This gives you less flexibility and I don't think it's necessary.
Please let's try to avoid this if it's possible.

@_date: 2015-05-07 12:52:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Can you please elaborate on what terrible things will happen if we
don't increase the block size by winter this year?
I assume that you are expecting full blocks by then, have you used any
statistical technique to come up with that date or is it just your
Because I love wild guesses and mine is that full 1 MB blocks will not
happen until June 2017.
We've successfully reached consensus for several softfork proposals already.
I agree with others that hardfork need to be uncontroversial and there
should be consensus about them.
If you have other ideas for the criteria for hardfork deployment all I'm ears.
I just hope that by  "What we need to see right now is leadership" you
don't mean something like "when Gaving and Mike agree it's enough to
deploy a hardfork" when you go from vague to concrete.
Oh, so your answer to "bitcoin will eventually need to live on fees
and we would like to know more about how it will look like then" it's
"no bitcoin long term it's broken long term but that's far away in the
future so let's just worry about the present".
I agree that it's hard to predict that future, but having some
competition for block space would actually help us get more data on a
similar situation to be able to predict that future better.
What you want to avoid at all cost (the block size actually being
used), I see as the best opportunity we have to look into the future.
Free transactions are a gift from miners that run an altruistic policy.
That's great but we shouldn't rely on them for the future. They will
likely disappear at some point and that's ok.
In any case, he's not complaining about the lack of free transactions,
more like the opposite.
He is saying that's very easy to get free transactions in the next
block and blocks aren't full so there's no incentive to include fees
to compete for the space.
We can talk a lot about "a fee market" and build a theoretically
perfect fee estimator but we won't actually have a fee market until
there's some competition for space.
Nobody will pay for space that's abundant just like people don't pay
for the air they breath.
Ok, this is my plan: we wait 12 months, hope that your estimations are
correct (in case that my guess was better than yours, we keep waiting
until June 2017) and start having full blocks and people having to
wait 2 blocks for their transactions to be confirmed some times.
That would be the beginning of a true "fee market", something that
Gavin used to say was his  priority not so long ago (which seems
contradictory with his current efforts to avoid that from happening).
Having a true fee market seems clearly an advantage.
What are supposedly disastrous negative parts of this plan that make
an alternative plan (ie: increasing the block size) so necessary and
I think the advocates of the size increase are failing to explain the
disadvantages of maintaining the current size. It feels like the
explanation are missing because it should be somehow obvious how the
sky will burn if we don't increase the block size soon.
But, well, it is not obvious to me, so please elaborate on why having
a fee market (instead of just an price estimator for a market that
doesn't even really exist) would be a disaster.

@_date: 2015-05-07 14:26:10
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Ok, thanks.
Yes, Peter Todd gave more details.
Well, yes, it is true that "universally uncontroversial" (which is
what I think the requirement should be for hard forks) is a vague
qualifier that's not formally defined anywhere.
I guess we should only consider rational arguments. You cannot just
nack something without further explanation.
If his explanation was "I will change my mind after we increase block
size", I guess the community should say "then we will just ignore your
nack because it makes no sense".
In the same way, when people use fallacies (purposely or not) we must
expose that and say "this fallacy doesn't count as an argument".
But yeah, it would probably be good to define better what constitutes
a "sensible objection" or something. That doesn't seem simple though.
Well, there's two different things here.
One thing is the Bitcoin core project where you could argue that the 5
committers decide (I don't know why Wladimir would have any more
authority than the others).
But what the bitcoin network itself does it's very different because
unlike the bitcoin core software project, the Bitcoin network is
If the people with commit access go nuts and decide something that's
clearly stupid or evil, people can just fork the project because it is
free software.
You cannot be forced to use specific features of free software, you
can always remove them and recompile, that's the whole point.
So, no, there's no authority to decide on hardforks and that's why I
think that only clearly uncontroversial things can get through as
Ok, so in simple terms, you expect people to have to pay enormous fees
and/or wait thousands of blocks for their transactions to get included
in the chain.
Is that correct?
As said above there's no authority to decide on what Bitcoin the p2p
network does. Again, that's the whole point.
But, yes, I agree that both sides understanding each other better is progress.
I'm sure he wants a fee market to eventually exist as well.
But it seems that some people would like to see that happening before
the subsidies are low (not necessarily null), while other people are
fine waiting for that but don't want to ever be close to the scale
limits anytime soon.
I would also like to know for how long we need to prioritize short
term adoption in this way. As others have said, if the answer is
"forever, adoption is always the most important thing" then we will
end up with an improved version of Visa.
But yeah, this is progress, I'll wait for your more detailed
description of the tragedies that will follow hitting the block
limits, assuming for now that it will happen in 12 months.
My previous answer to the nervous "we will hit the block limits in 12
months if we don't do anything" was "not sure about 12 months, but
whatever, great, I'm waiting for that to observe how fees get
But it should have been a question "what's wrong with hitting the
block limits in 12 months?"

@_date: 2015-05-07 15:40:23
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Looking at this graph and in retrospective, we shouldn't have removed
the standard policy limit without observing the supposedly disastrous
effects of hitting the limit first.
Removing the standard limit would have been trivial (bdb issues aside)
at any point after seeing the effects.
Well, this is only for first confirmations of free transaction.
A higher fee should increase your probabilities, but if you're sending
free transactions you may not care about them taking longer to be
Well, maybe "instant and free" it's not a honest form of bitcoin
marketing and it just has to disappear.
Maybe we just need to start being more honest about pow being good for
processing micro-transactions: it is not.
Hopefully lightning will be good for that.
Free and fast in-chain transactions is something temporary that we
know will eventually disappear.
If people think it would be a adoption disaster that it happens soon,
then they could also detail an alternative plan to roll that out
instead of letting it happen.
But if the plan is to delay it forever...then I'm absolutely against.
I take this as an argument for increasing fee competition and thus,
against increasing the block size.
No blacklisting, please, that's centralized.
In any case, a related known: bigger blocks give competitive advantage
to bigger miners.

@_date: 2015-05-07 17:33:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
That is not what I said. Then you demonstrated that it was absurd.
That's called a straw man argument and it's a well known fallacy, it
is precisely the example of arguments that can be safely ignored.
It is an argument against my admittedly vague definition of
"non-controversial change".
More importantly, I never said anything about "removing anyone", I was
always talking about arguments and not people.
One person could use fallacious arguments to attack or defend a given
proposal and use perfectly valid ones in another, a person can even
mix valid and invalid arguments in the same mail.
Yes, the maintainer of the Bitcoin core free software project (I
cannot stressed this enough, that can be forked by anyone), not the
president of Bitcoin the p2p network.
I'm sure that if they become that stupid, developers would move to a
fork of the project in no time.
Ok, thanks in advance.
Again, I didn't say any of that. My point is that a network that
becomes too "centralized" (like visa, that is centralized vs p2p, not
vs distributed) doesn't offer any security or decentralization
advantage over current networks (and of course I meant that could
happen with larger blocks, not 1 MB blocks).
I'm sure that's not what the proponents of the size increase want, and
I'm not defending 1 MB as a sacred limit  or anything, but my question
is "where is the limit for them?"
Even a limitless block size would technically work because miners
would limit it to limit the orphan rate. So "no hardcoded consensus
limit on transaction volume/block size" could be a valid answer to the
question "what is the right consensus limit to block size?" for which
there's no real right answer because there is a tradeoff between
transaction volume and centralization.
Should we maintain 1 MB forever? Probably not.
Is 20 MB a bad size? I honestly don't know.
Is this urgent? I don't think so.
Should we rush things when we don't have clear answers to many related
questions? I don't think so.
You think that it is too soon to start restricting transaction volume
in any way. You will answer why in your post.
When is the right time and what is the right limitation then?
I want to have fee competition as soon as possible, at least
temporarily. But you say that it can wait for later.
Ok, when do you think we should make that happen then?
When 20 MB are full, will that be the right time to let the fee market
develop then or will it be urgent to increase the block size again?
Should we directly remove the limit then and let miners handle it as
they want?
If so, why not now?
Maybe we can increase to 2 MB, then wait for fee competition, then
wait for 2 more subsidy halvings and then increase to 11 or 20 MB?
There's so many possibilities that I don't understand how can be
surprising that "20 MB, as soon as possible" is not the obvious answer
to everyone...

@_date: 2015-05-07 18:21:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Mhmm, I hadn't thought about this. This makes sense and actually
explains the urgency on taking a decision better than anything else
I've heard.
This, on the other hand, is a non sequitur [1], another type of fallacy.
Well, several of them, actually:
- If it's not raised, then bitcoin cannot become popular
- If it's not raised, then users will leave
- Businesses built on the assumption that Bitcoin could become popular
were also assuming that it's going to be risen.
These statements may even be true, but they're no logical conclusions
even if they seem obvious to you.
I don't think those claims are strictly true, specially because they
involve predictions about what people will do.
But if they're true they require some proof or at least some explanation.
[1]

@_date: 2015-05-07 18:47:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Yes, but it was an argument against something I didn't said ;)
Yes, that's why I drafted a definition for "uncontroversial change"
rather than "change accepted by consensus".
It will still be vague and hard to define, but consensus seems much harder.
And, yes, you're right, it is more like giving power to anyone with
valid arguments to veto hardfork changes.
But as you say, that could lead to make hardforks actually impossible,
so we should limit what constitutes a valid argument.
I later listed some examples of invalid arguments: logical fallacies,
unrelated arguments, outright lies.
Certainly I don't think technical merits should count here or that we
could veto a particular person from vetoing.
We should filter the arguments, not require an identity layer to
blacklist individuals.
We should even accept arguments from anonymous people in the internet
(you know, it wouldn't be the first time).
Some research at all about fee market dynamics with limited size that
hasn't happened at all.
If we're increasing the consensus max size maybe we could at least
maintain the 1MB limit as a standard policy limit, so that we can
study it a little bit (like we could have done instead of removing the
initial policy limit).
I don't know yet, but I understand now that having a clearer roadmap
is what's actually urgent, not the change itself.
What about 2 MB consensus limit and 1 MB policy limit for now? I know
that's arbitrary too.
As others have explained, the number of full nodes is not the
improtant part, but how easy it is to run one.
I think a modest laptop with the average internet connection of say,
India or Brazil, should be able to run a full node.
I haven't made those numbers myself but I'm sure that's possible with
1 MB blocks today, and probably with 2 MB blocks too.
This is an excellent question for both sides.
Unfortunately I don't know the answer to this. Do you?

@_date: 2015-05-07 20:05:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase 
Ok, the fact that the fee increases the probability of getting
included faster already is a good thing, the graphs with the
probability of getting included in the next block were less important
to me.
Although scarce space (beyond what miners chose to limit by
themselves) would increase the fee competition, I didn't knew that
there is actually some competition happening already.
So I guess this diminishes the argument for maintaining the limits
longer to observe the results of more scarce space.
Still, I think maintaining a lower policy limit it's a good idea, even
if we decide not to use it to observe that soon.
For example, say we chose the 20 MB consensus limit, we can maintain
the policy limit at 1 MB or move it to 2 MB, and slowly moving it up
later as needed without requiring everyone to upgrade.
Of course, not all miners have to follow the standard policy, but at
least it's something.
So please take this as a suggestion to improve your proposal. You can
argue it like this "if we want to maintain the limits after the
hardfork or increase them slowly, for observing fee dynamics with more
scarce space or for any other reason, those limits can be partially
enforced by the standard policy". I mean, I think that could be a
reasonable compromise for that concrete line of arguments.

@_date: 2015-05-12 21:16:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
This saves us ocodes for later but it's uglier and produces slightly
bigger scripts.
If we're convinced it's worth it, seems like the right way to do it,
and certainly cltv and rclv/op_maturity are related.
But let's not forget that we can always use this same trick with the
last opcode to get 2^64 brand new opcodes.
So I'm not convinced at all on whether we want   or But it would be nice to decide and stop blocking  this.

@_date: 2015-05-13 02:38:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
I like the reuse with negative numbers more than the current proposal
because it doesn't imply bigger scripts. If all problems that may
arise can be solved, that is.
If we went that route, we would start with the initial CLTV too.
But I don't see many strong arguments in favor of using the current
trick later when we're actually running out of opcodes, just that
"CLTV and RCLTV/op_maturity are semantically related". How
semantically related depends on the final form of RCLTV/op_maturity,
but I don't think anybody wants to block CLTV until RCLTV is ready.
So we could just deploy the initial CLTV ( now and then decide
whether we want to reuse it with negatives for RCLTV or if we use an
independent op.
Can the people that don't like that plan give stronger arguments in
favor of the parametrized version?
I've missed IRC conversations, so I may be missing something...

@_date: 2015-05-14 02:11:47
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Long-term mining incentives 
Or never, nobody knows at this point.
I think is very healthy to worry about that since we know it's
something that will happen.
The system should work without subsidies.
Lightning payment channels may be a new idea, but payment channels are
not, and nobody is using them.
They are the best solution to scalability we have right now,
increasing the block size is simply not a solution, it's just kicking
the can down the road (while reducing the incentives to deploy real
solutions like payment channels).
Not worrying about 10 years in the future but asking people to trust
estimates and speculations about how everything will burn in 2 years
if we don't act right now seems pretty arbitrary to me.
One could just as well argue that there's smart hard-working people
that will solve those problems before they hit us.
It is true that the more distant the future you're trying to predict
is, the more difficult it is to predict it, but any threshold that
separates "relevant worries" from "too far in the future to worry
about it" will always be arbitrary.
Fortunately we don't need to all share the same time horizon for what
is worrying and what is not.
What we need is a clear criterion for what is acceptable for a
hardfork and a general plan to deploy them:
-Do all the hardfork changes need to be uncontroversial? How do we
define uncontroversial?
-Should we maintain and test implementation of hardfork whises that
seem too small to justify a hardfork on their own (ie time travel fix,
allowing to sign inputs values...) to also deploy them at the same
time that other more necessary hardforks?
I agree that hardforks shouldn't be impossible and in that sense I'm
glad that you started the hardfork debate, but I believe we should be
focusing on that debate rather than the block size one.
Once we have a clear criteria, hopefully the block size debate should
become less noisy and more productive.

@_date: 2015-05-14 01:46:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Long-term mining incentives 
If you get isolated from the network you may not get the longest valid
chain. I don't think any other consensus mechanism deals with this
better than Bitcoin.
Checkpoints are NOT part of the consensus rules, they're just an
optimization that can be removed.
Try keeping the genesis block as your only checkpoint and rebuild: it
will work. You can also define your own checkpoints, there's no need
for everyone to use the same ones.
In a future with committed utxo the optimization could be bigger, but
still, we shouldn't rely on checkpoints for consensus, they're just an
optimization and you should only trust checkpoints that are buried in
the chain. Trusting a committed utxo checkpoint from 2 years ago
doesn't seem very risky. If the code is not already done (not really
sure if it was done as part of auto-prune), we should be prepared for
reorgs that invalidate checkpoints.
So, no, Bitcoin does NOT rely on that "weak subjectivity" thing.

@_date: 2015-05-27 05:51:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Version bits proposal 
It would also help to see the actual code changes required, which I'm sure
will be much shorter than the explanation itself.

@_date: 2015-05-27 12:15:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Version bits proposal 
to activate and then must wait at least 1000 for implication?
You need 75% to start applying it, 95% to start rejecting blocks that don't
apply it.

@_date: 2015-05-27 19:07:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
This allows the implementation of a rcltv that doesn't make script depend
on the current height, in a similar way that cltv uses the nLockTime (which
has been compared with the current height already when checking the script).
In fact, the implementation could be simpler if the goal of maintaining the
original nSequence semantics was ignored ( although not that simpler, but
you wouldn't need to use ~ (bitwise not).
I'm still not sure whether there should be 2 BIPs for this or just one.

@_date: 2015-05-31 16:46:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
out of business.
transaction fees.
No, the later determines who can be profitable.
Here's a thought experiment:
Subsidy is gone, all the block reward comes from fees.
Miner A has great connectivity and mines 20 MB blocks, with an average of
20 btc per block.
Miner B has a connectivity such that 2 MB blocks puts it on a reasonable
orphan rate, so it gets an average of 2 btc per block mined.
But the difficulty is the same for all and it can rise up to miner A
breaking even after energy costs.
Will miner B be profitable with this setup? The answer is no and miner B
will just go out of business. In that sense too, bigger blocks mean more
mining centralization.

@_date: 2015-05-31 16:59:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
Whatever...let's use the current subsidies, the same argument applies, it's
just 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.
Miner B would still go out of business, bigger blocks still mean more
mining and validation centralization. The question is how far I we willing
to go with this "scaling by sacrificing decentralization", but the answer
can't be "that's to far away in the future to worry about it, right now as
far as we think we can using orphan rate as the only criterion".

@_date: 2015-05-31 17:45:01
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
it's just 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.
mining and validation centralization
connectivity, then they need to pay for better connectivity.
Well, I was assuming they just can't upgrade their connection (without
moving thei operations to another place). Maybe that assumption is
ridiculous as well.
from the middle of the Sahara" then we're going to have to agree to
No, I'm not suggesting that.
connectivity to run a full node? The 20MB number comes from estimating
costs to run a full node, and as my back-and-forth to Chang Wung shows, the
costs are not excessive.
Well, you were I think assuming a new desktop connecting from somewhere in
the US. I would be more confortable with an eee pc from a hotel in India,
for example. But yeah, targeting some concrete minimum specs seems like the
right approach for deciding "how far to go when increasing centralization".
But "hitting the limit will be chaos" seems to imply that completely
removing the consensus maximum blocksize is the only logical solution. What
happens when we hit the limit next time? When do we stop kicking the can
down the road? When do we voluntarily get that "chaos"?
Again, "that's too far away in the future to worry about it" is not a very
conving answer to me.

@_date: 2015-11-05 16:27:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
On Tue, Nov 3, 2015 at 11:01 PM, Luke Dashjr via bitcoin-dev
I think this is just a terminology confusion.
There's conflicting spends of the same outputs (aka unconfirmed
double-spends), and there's signature malleability which Segregated
Witnesses solves.
If we want to define malleability as signature malleability +
conflicting spends, then that's fine.
But it seems Christian is mostly interested in signature malleability,
which is what SW can solve.
In fact, creating conflicting spends is sometimes useful for some
contracts (ie to cancel the contract when that's supposed to be
Maybe it is "incorrect" that people use "malleability" when they're
specifically talking about "signature malleability", but I think that
in this case it's clear that we're talking about transactions having
an id that cannot be changed just by signing with a different nonce
(what SW provides).
Please, Christian, correct me if you mean something else.

@_date: 2015-11-05 21:25:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
I disagree. Segregated witnesses, for example, doesn't solve all kinds
of malleability and is very useful in some practical cases by solving
all signature malleability.
As said, we don't want to eliminate all forms of malleability (for
example, replace by fee), although we may want to "address them" at
some level.
As you have said, wallets should be looking at scriptPubKeys, not
transaction ID, but that is orthogonal to SW, a normalized tx ID and
signature malleability.

@_date: 2015-11-12 21:43:17
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Upcoming Transaction Priority Changes 
The ease of implementation is not gained if it's maintained optionally.
I agree changing policy defaults is meaningless, but in this case it
is supposed to signal deprecation of the policy option.
On Thu, Nov 12, 2015 at 9:20 PM, Chun Wang via bitcoin-dev
I am in favor of having customizable cost (currently tx size but it
has been proposed to also include sigoprate) and reward (currently
feerate). The main problem I see for keep maintaining the code is that
priority is not integrated in the reward function and cannot easily be
with its current functionality unchanged (which slows down other very
necessary improvements in the mempool limits).

@_date: 2015-11-14 11:52:12
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP - Block size doubles at each reward halving 
Currently bip99 recommends 95% miner upgrade confirmation with version bits
(bip9) for uncontroversial hardforks just like it does for uncontroversial
softforks. It is true that in the case of hardforks miners don't decide and
it's the whole economy who has to upgrade before activation, but "the whole
economy" and "all users" includes miners, so why not use the only upgrade
confirmation mechanism that we have available?
The way I see it, uncontroversial softforks are also expected to be
upgraded to by everyone eventually. The advantage of softforks is that
non-miners don't need to do it before activation like with hardforks.
That's the only important difference I see between uncontroversial
softforks and hardforks (unilateral softforks and schism hardforks are
another thing though).
Please let's discuss this generally within the context of bip99 instead of
discussing different deployment details with every proposal. There's a
couple of threads in the ml, a couple of now merged bip99 prs in
On Nov 14, 2015 10:31 AM, "Adam Back via bitcoin-dev" <

@_date: 2015-11-14 14:48:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] How to evaluate block size increase suggestions. 
I agree with the usefulness of at least trying to define a formal set
of criteria.
I'm afraid that with proposals that schedule future increases to the
blocksize consensus maximum or leave it for miners to decide (like
BIP100, BIP101 and BIP103) cannot be evaluated without making
assumptions about the future (or what miners will decide in the
Since limiting resource consumption and mining centralization dynamics
are the reasons to have blocksize consensus maximum in the first
place, I think it would be ideal to have some simulation +
benchmarking software that is able to analyze a given proposal, give
resource consumption benchmark data about average and worst cases, and
also give some kind of metric from "mining centralization dynamics
We could start with just a metric for concrete block sizes (for
arbitrary maximum blocksizes testchains see Note that this is unrelated to the deployment mechanism, proposed
activation date and other details.
On Fri, Nov 13, 2015 at 5:52 PM, Angel Leon via bitcoin-dev
That depends on block space demand on a particular moment in time, the
fee paid by the example user and local relay and mining policies in
the network (and how they treat transactions with the specific form of
the transaction example) and even the network topology.
There's no consensus rule that can guarantee that all transaction from
all users will be included at most 3 blocks after they are relayed.
For starters, any user can create infinite transactions (without fee)
for free while the network will never have infinite computing
What we have is a fee estimator that observes the chain and can
estimate the market situation to tell you the average number of blocks
for a given transaction with a given feerate. I know that number was
only 14 blocks or so for free (not a single satoshi in fees)
transactions, but that has probably changed with the recent attacks...

@_date: 2015-11-15 11:12:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
On Nov 15, 2015 5:10 AM, "Peter R via bitcoin-dev" <
Thanks to the worl of many people, part of the consensus rules are finally
encapsulated in the libbitcoinconsensus library. I'm currently writing a
document to complete the encapsulation of the specification of the
consensus rules.
that it can enforce one against the invisible hand of the market.
You keep insisting that some consensus rules are not consensus rules while
others "are clearly a very different thing". What technical difference is
there between the rule that impedes me from creating transactions bigger
than X and the rules that prevent me frm creatin new coins (not as a miner,
as a regular user in a transaction with more coins in the outputs than in
the inputs)? What about property enforcement? If the invisible hand of the
market is what decides consensus rules instead of their (still incomple)
specification (aka libconsensus), then the market could decide to stop
enforcing ownership.
Will you still think that Bitcoin is a useful system when/if you
empirically observe the invisible hand of the market taking coins out of
your pocket?
You also keep assuming that somehow it is a universal law that users must
eventually converge under the most-work chain. People follow the most-work
VALID chain, but if they consciously decide to implement different rules
(different definitions of "valid block") then their chains can diverge, and
once they do they won't converge again (unless/until one group decides to
implement the rules of the other exactly again), just like when the
implementation of the rules diverge in a unintentional consensus fork. But
in this case they could decide to never implement the same rules.
See bip99 and specially the "schism hardforks" section for more details.
Please, read the thread again. I think it is pretty clear that you did.
Nothing wrong with that, just move it to the discussion ml.

@_date: 2015-11-15 12:28:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Going back on topic, I believe libconsensus shouldn't depend on any
particular database because assuming it will continue to be stateless
(the current libbitcoinconsensus is stateless) end therefore has no
storage. I know some people disagree in various degrees.
At the same time, the parts of the consensus rules verification that
depends on storage has not been encapsulated out to
libbitcoinconsensus yet, and I agree that changing the database is
unnecessarily risky at this point.
Even when the consensus rules are encapsulated, that doesn't mean that
Bitcoin Core should be DB agnostic or that we can guarantee that it
will follow the longest valid chain with databases that have not been

@_date: 2015-11-15 12:42:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
Thank you for incorporating the feedback, specifically thank you for
using the genesis block hash as the unique chain ID.
I wen't through the BIP draft and left a few of comments, but I really
like its simplicity and focus. Good work!
On Sun, Nov 15, 2015 at 3:14 AM, Marco Pontello via bitcoin-dev

@_date: 2015-11-15 13:16:56
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP - Block size doubles at each reward halving 
Maybe we should define "the bitcoin economy" is first. In my
definition, miners are definitely part of the economy (and also users
of the system).
Miners accept bitcoins as payment for a real service (with real costs
like electricity) to the network: extending the longest valid chain
with their proof of work.
In the context of BIP99, there's no concept of "an economic majority"
deciding hardforks. Hardforks are either uncontroversial, in which
case BIP99 recommends 95% miner upgrade confirmation in addition to a
time threshold, or are schism hardforks (for example, an anti-miner
hardfork), in which case BIP99 recommends using a time threshold
alone. But no majority can force the dissenting users to use one
validation rule set or the other: users will always be free to run
whatever free software they like.
That alone seems like a very good reason in favor to confirm that
miners have upgraded in addition to a minimal activation block median
time, not a reason against it
I'm not sure I understand this. The trigger mechanism must be uniform
for each rule change, it cannot be optionally different or consensus
can fail.
How are miners supposed to "perceive" adoption?
The time threshold must be set enough in the future to give users time
to upgrade. But we can perceive miners' adoption, so if the system
knows they haven't upgraded, it should wait for them to upgrade (it
would be nice to have an equivalent mechanism to wait for the rest of
the users, but unfortunately there's none).
Please, remember that this is in the context of uncontroversial
hardforks for which all users (including all miners) are expected to
upgrade to.
To reiterate, schism hardforks are treated differently and the miner
upgrade confirmation becomes completely irrelevant.

@_date: 2015-11-16 13:06:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP99 and Schism hardforks lifecycle (was Switching 
Sorry for going out of topic on that thread, I have just created
another thread to discuss this particular point (whether schism
hardforks can be universally predicted to collapse into a single chain
or not), which is a fundamental part of BIP99 discussion and I believe
technical enough for this list (assuming that we stay on topic). But
the moderation thinks it's not relevant enough for this list, we can
move it to the discussion mailing list or private emails.
Jorge Tim?n said:
Of course, their technical difference come from the fact they are
technically different. That's not what I meant.
There's no technical argument that lets you predict whether
eliminating one rule or the other will be more or less acceptable to
There's no technical difference that I can see in that reward.
I think these two examples strike people as "obviously different" just
because they are morally different, but I want to avoid moral
judgments in BIP99.
Those were unintentional hardforks. There's an example of a failed
schism hardfork: when some people changed the subsidy/issuance rules
to maintain the 50 btc block subsidy constant.
It didn't failed because of "tremendous pressure": it failed because
the users and miners of the alternative ruleset abandoned it. If they
hadn't, the two incompatible chains would still grow in parallel.
Yes, there could be arbitrage and speculators selling "on both sides"
is also a possibility.
At some point we would arrive to some kind of price equilibrium,
different for each of the coins. BIP99 states that those prices are
unpredictable (or at least there's no general method to predict the
result without knowing the concrete case, the market, etc) and in fact
states that the resulting price for both sides could be going to close
to zero market capitalization.
That still doesn't say anything about one side having to "surrender".
The coin that ends up with the lowest price (and consequently, the
lowest block reward and hashrate) can still continue, maybe even for
longer than the side that appeared to be "victorious" after the
initial arbitrage.
I haven't heard any convincing arguments about schism hardforks having
to necessarily collapse into a single chain and until I do I'm not
going to adapt BIP99 to reflect that.
Yes, I have control: all users (including miners) have direct control
over the rules that software they run validates.
You cannot ever have your coins stolen in the longest valid chain you
follow if the validity rules you use enforce property ownership.
No majority can force you to move to the new non-ownership rules, just
like no majority can force you to move to any different set of rules.
If we accept the notion that a groups of users could resist to
deploying this particular rule changes and keep operating under the
old rules, we have to accept that this can happen for any
controversial hardfork, and that we cannot predict a common lifecycle
for all schism hardforks.

@_date: 2015-11-16 15:43:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
Not a native english speaker myself, so I may have missed some things...
Yes, sorry about the link. I guess you can point to  . I can
rebase it if needed but I would close it again because I don't want to
have too many things from  opened at the same time (is noisy and
worse for review). My plan was to not open it independently at least
until after  (and actually after 0.12 assuming  gets in by
0.12). But then I would maybe open a new one and reference the old one
rather than reopening  (which tends to be confusing).
I'm not really sure what's the best answer here...but  is
certainly going to need rebase and the link will be broken again.
Maybe one answer is to copy some text from  or the commit and add
it directly to the BIP instead of referencing to that commit (which
will be, at least until  is merged, a moving target).

@_date: 2015-11-18 12:29:13
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
I can always link to the BIP when I reopen that commit as independent
instead of the other way around.
Btw, the PR needs rebase (probably the conflict is in the README).

@_date: 2015-11-18 11:15:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP - Block size doubles at each reward halving 
Assuming it was deployed as an uncontroversial hardfork as recommended
in BIP99, the deployment would use versionbits (BIP9) and the hardfork
would timeout.
But this timeout would clearly signal that either the minimum
activation threshold wasn't giving enough time for all users to
upgrade (apparently miners didn't had time) or the hardfork is not
really an uncontroversial hardfork but rather a schism one. Then,
assuming some people still want to deploy it as a schism hardfork,
bip99 recommends using only a mediantime threshold without versionbits
nor miner upgrade confirmation.

@_date: 2015-11-20 15:15:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
On Tue, Nov 17, 2015 at 11:17 PM, telemaco via bitcoin-dev
Yes, but we're only testing levelDB and we couldn't assure that it
won't produce unintentional consensus forks with other databases
behind the whatever db-agnostic interface.
I believe Bitcoin Core should officially support only one database at
a time. And if that is to change in the future, I don't think it
should be before a storage-agnostic libconsensus is encapsulated (and
after that there will still be risks and costs in officially
supporting several several databases simultaneously).
As has been said, these kind of experiments are welcomed outside of
bitcoin/master though.

@_date: 2015-11-24 13:31:55
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
I agree, I believe the first name that an op with equivalent functionality
had was simply op_maturity.
At least I remember we discussed such an opcode when discussing pegged
sidechains' design.
I kind of dislike the check_x_verify naming pattern. We want all new
operands to return if whatever they're checking/verifying fails, fine. Do
we have to repeat this redundant naming pattern forever due to that
I hope not, but if that's the case my vote is for CMV.
As said before, I believe the documentation and code comments can become
much more clear with this change.

@_date: 2015-11-24 13:35:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
On Nov 24, 2015 1:21 PM, "Peter Todd via bitcoin-dev" <
While I agree we're not in a hurry, the more we wait, the longer docs (to
be modified later) will accumulate making the assumption that the name is
csv rather than op_maturity.

@_date: 2015-11-27 11:14:10
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
On Nov 26, 2015 12:06 AM, "Mark Friedenbach via bitcoin-dev" <
think the nSequence name is better...
I suggested to rename nSequence to nMaturity on this list even before the
bips and implementations were started, probably too late now.
Before the implementation "let's think about those naming details later".
After the implementation "now it's too late, now we would need to change
the implementation, this renaming is now unnecessarily disruptive".
Reminds me of refactors and major releases:
At the beginning of the release "not now, this will disrupt development of
feature X"
After feature X is merged or replaced by feature Y: "too late in the
release cycle, refactors should be done only at the beginning, at the end
is 'too risky' ".
Sigh, I hope I find the "right time" (not both too soon and too late like
this time), next time...

@_date: 2015-11-29 12:55:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Use CPFP as consensus critical for Full-RBF 
Both CPFP and RBF are relay/mining policy and cannot be made consensus
rules because you cannot know which transactions have been received by a
givrn peer and which have not (or at what time). Consensus rules can only
validate information that's in the blockchain.
On Nov 29, 2015 5:33 AM, "Vincent Truong via bitcoin-dev" <

@_date: 2015-10-01 02:11:49
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Actually, no, sorry, the second paragraph is not correct as explained by
Greg Maxwell.

@_date: 2015-10-02 10:20:55
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
On Oct 2, 2015 10:03 AM, "Daniele Pinna via bitcoin-dev" <
optimization be found.
This is demonstrably impossible: anything that can be done with software
can be done with hardware. This is computer science 101.
And specialized hardware can always be more efficient, at least
On the other hand, BIP99 explicitly contemplates "anti-miner hardforks"
(obviously not for so called "ASIC-resistance" [an absurd term coined to
promote some altcoins], but just for restarting the ASIC and mining market
in case mining becomes too centralized).

@_date: 2015-10-02 13:00:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
to promote some altcoins]
the Core camp.
doesn't mean you can't make it _arbitrary_hard_ in practice.
ASIC-RESISTANCE is simply not possible, I'm sorry if that position strikes
you as arrogant. Note that I didn't say anything about memory-hard, which
is possible (but not necessarily preferrable to
simple-to-implement-in-hardware pow algorithms).

@_date: 2015-10-05 14:04:12
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
On Oct 5, 2015 1:28 PM, "Mike Hearn via bitcoin-dev" <
starts skipping bits then I'd say it's not really "working" according to
its original design goals
But assuming the hashrate majority has upgraded (and we're using 95% as the
miner upgrade confirmation threshold to start activation, so that
assumption seems pretty safe), a non-upgraded full node and an upgraded
full will converge on what they see: "the most-work valid chain" will be
the same for both. A non-upgraded full node wallet waiting for several
confirmations (for example, 6 confirmations) will be just as safe as an
upgraded one. In that sense, it keeps working. On top of that, nodes (of
any kind) can use unknown block version numbers to notify the user or even
stop working (the same notification mechanism you would use with hardforks).
I agree that hardforks are necessary and we should deploy a hardfork asap
to show the world they are indeed possible (bip99 proposes a likely
uncontroversial one), but I still believe that is clear that softfork
deployment is preferrable in many cases like this one.
Are you going to produce a bip65 hardfork alternative to try to convince
people of its advantages over bip65 (it is not clear to me how you include
a new script operand via hardfork)?

@_date: 2015-10-05 14:16:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Given the assumptions above, only of transactions without enough
Not if the wallet waits for enough confirmations.

@_date: 2015-10-05 15:24:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
"Consensus" it's a term we use for consensus critical code and we
refer to different machines (potentially with different software)
validating in exactly the same way.
I think also using the term for people agreeing on what those
consensus rules is confusing, so in BIP99 I used the term
"uncontroversial" instead.
Uncontroversial consensus upgrades
"Uncontroversial" is something though to define in this context. What
if a single user decides he won't upgrade no matter what and he
doesn't even attempt to explain his decision? Obviously, such a user
should be just ignored. But what if the circumstances are slightly
different? What if they're 2, 10 users? Where's the line? It is
possible that we can never have a better definition than "I know it
when I see it" [citation].
The fact that there's at least 3 different proposals for a blocksize
increase, that there's not a lot of data comparing different possible
block sizes and its potential effects on block propagation and that
the development progress has enormously slowed down during months of
discussion are, in my opinion, clear signs that none of the current
proposals are "uncontroversial", even by this vague definition.
I believe BIP65 is uncontroversial since no reasonable objections to
the feature itself have been raised, it has been widely reviewed and
tested. The only complain is about it is it's softfork deployment
Was deployment of bip16, bip30 or bip66 controversial (which were
deployed via softforks, some of them even with people [ie Mike Hearn]
preferring always hardforks over softforks) uncontroversial?
I believe they were all (maybe with the exception of bip16)
uncontroversial. That's the story bip99 is telling, but bip99 is not
finished so we can change that if it makes sense.
We could say that they have been "Unilateral softforks", but I don't
think that would be fair for the miners who helped deploy it. Or we
could always create a new category in bip99 (please, propose a new
category of softforks if you think there's some potential case that's
not covered).
This is not about Mike Hearn or you or any person in particular.
"Uncontroversial" is so far defined in a vague way, if you think you
can put a more formal definition forward, please do so (provided that
it's not an absurd definition which allows any individual to block
everything without reasonable arguments). I'm more than happy
improving bip99 before we move it from its current "draft" status.
If Mike Hearn (and you) are right, I should update bip99 to NEVER
recommend softforks for consensus rule changes.
But I still believe it is uncontroversial that softforks have great
advantages in many cases (even if not everybody understand this).
I want bip99 itself to be uncontroversial, so please nit/nack fast,
nit/nack often and please please please nit/nack on time (while bip99
is still a draft).

@_date: 2015-10-05 15:29:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
This has already been discussed. The recommended risk mitigation
mechanism for softforks it's just the same as the one for hardforks:
unknown block version notifications.

@_date: 2015-10-05 17:33:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
As Greg explained to you repeatedly, a softfork won't cause a
non-upgraded full node to start accepting blocks that create more
subsidy than is valid.
It's only the new rule (in this case, BIP65) that they won't validate.
That's very different security from an SPV node, and as Greg also
explained, SPV nodes could be much more secure than bitcoinj nodes
(they could, for example, validate the coinbase transaction of every
If a non-upgraded node it's not a "full node" for you, that's fine,
but it is for everyone else. So please stop confusing other people.
Assuming the majority of the hashrate upgraded, there's almost no risk
for non-upgraded full nodes.
Thanks, I wasn't aware that there was room for new opcodes that
weren't noops already.
Can you give an example of an attack in which a non-upgraded full node
wallet is defrauded with BIP65 but could not with the hardfork
alternative (that nobody seems to be willing to implement)?
Please, don't assume 0 confirmation transactions or similar
unreasonable assumptions (ie see section 11 "Calculations" of the
Bitcoin whitepaper).

@_date: 2015-10-05 17:42:56
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Why would you care about payments to other people?
The scriptPubKey's that you give to your payers certainly have meaning to you.
What is it important that you are able to calculate balances of
wallets that aren't yours?
Why would anyone "pay you" to a scriptPubKey you don't understand?
I can "pay" the bill of my internet services by burying cash in a park
nearby my house for my provider to pick up later.
But if I don't tell my provider, it will never know. If I inform it, I
will get an answer: "no, sorry, we won't accept this new 'form of
payment' of yours as payment".

@_date: 2015-10-19 12:43:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
On Fri, Oct 16, 2015 at 3:26 AM, Rusty Russell via bitcoin-dev
Once more functions (specially consensus-critical functions) take
nTime explicitly as parameter instead of relying on the
library-unfriendly GetAdjustedTime(), then SetMockTime() will be less
necessary for testing. For example, see

@_date: 2015-09-01 11:25:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
On Aug 31, 2015 3:01 PM, "Justus Ranvier via bitcoin-dev" <
I believe he explained very well what he meant by decentralized, please
stop suggesting he doesn't understand his own thoughts: it is extremely
For starters, a third party (or a recuded group of miners controlling the
majority of the hashrate) can censor transactions. It doesn't matter how
benevolent that party is: it can be forced to do it by the laws of its
If you don't care about this, I suggest you start a new system without
expensive proof of work, you can replace it with block signing (it can
still be multisig). It is already coded, just fork the alpha or the
blocksigning branch in elementsProject (github).

@_date: 2015-09-02 00:46:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
I'm fine with each explorer using whatever chain they prefer as default.
It would need to be a different argument, for example chainPetName.
But who is the central authority that registers the mnemonic names?
That's why I say petname, because no dictionary of supported chains
should be considered universally accepted and thus it will always be
just a local registry.
If we're chainPetName is supported, there should be an additional call
to query that local list. For example:
JSON response:
{ "main": "000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f",
  "test": "000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943",
  "regtest": "0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206"}
It may be problematic when too many chains are supported. For example,
 introduces std::numeric_limits::max() new chains.
Obviously 4 bytes is not "as distinct" as 32 bytes. In std::numeric_limits::max() new chains share the same magic
And again, there's no central authority to register unique magic
bytes. In contrast, producing a unique genesis block is trivial (look
how I produced std::numeric_limits::max() new unique genesis
blocks in There's many altcoins that call "testnet" to their own testnet. In
Bitcoin itself, we've been using "testnet" to refer to the original
testnet, testnet2 and testnet3.
But again, the main issue is that we don't want a central authority to
register unique unique and memorable chain name strings.
Relevant links:

@_date: 2015-09-02 01:57:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC - BIP: URI scheme for Blockchain exploration 
I agree. That's why we don't need to account for altchains other than
testchains (ie sidechains and altcoins).
We can do it the right way from now on (and as you say altcoins can
trivially adapt to this).
Sorry for having missed bip44 for review but that section is horrible
in my opinion (see the links above). And it seems to be incompatible
with bip001 which says are immutable once accepted (assuming that
document is expected to be the central registry of registered chains).
Schism hardforks are explicitly renouncing to reach consensus with all
previous users. You're intentionally divorcing 2 chains, and you can
do that without confusing users.
In BIP99 the recommended deployment path for a schism fork is to
simply use the nHeight for activation.
The 95% miner's upgrade confirmation is not used here (like in
uncontroversial hardforks and softforks) because you don't necessarily
expect all miners to move to your side of the schism (and you don't
want to wait for them, specially if it's an "anti-miner" hardfork).
To avoid confusing users, you can define a new "genesis block" to use
for the chain ID, for example, 1000 blocks before the activation
The same applies for potentially pre-mined altcoins that haven't had
the decency/competency of even changing the string in pszTimestamp.
For example, FTC, coins generated with coingen (Matt Corallo or the
current owner may want to correct me on this point) or elements alpha
Fortunately alpha has a unique chain ID because it was changing both
the block and transaction serialization formats anyway. But hopefully
we will fix that for beta and later sidechains.
All chains that want a unique chain ID can have it retroactively. At
worst, they may need to use the hash of a block that is not the
genesis block.
In other words, they may need to move their "genesis checkpoint" upwards.
Terminology may make things more clear. We can replace:
"The chain ID is the hash of the genesis block"
"The chain ID is the hash of the genesis checkpoint".
If we want a unique chain ID we can have it: it just cannot be
memorable at the same time.
And each chain and implementation can start using them (in addition to
petname -> chain ID local dictionaries) at any point in time: this is
retroactively (and obviously forwards) compatible.
There can be many competing registries for the name -> chainID
dictionaries (maybe one of them based on namecoin?) but bitcoin/bips
shouldn't maintain one.

@_date: 2015-09-03 18:13:34
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 100 specification 
On Sep 3, 2015 5:58 PM, "Btc Drak via bitcoin-dev" <
In fact, that discussion can happen in parallel. But it is more efficient
to do so in one place instead of in each of the 5+ hardfork proposals
(bip99 itself has a hardfork proposal with its code ready).

@_date: 2015-09-03 20:23:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] block size - pay with difficulty 
Greg, I believe Jeff is focusing on BtcDrak's proposal (
 ) where the
increased nBits are used to vote for the block size to raise
permanently ( or until it gets voted down).
His arguments don't seem to apply to your original proposal (where the
size is only increased for that block).
On Thu, Sep 3, 2015 at 7:57 PM, Gregory Maxwell via bitcoin-dev

@_date: 2015-09-05 13:17:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC: HD Bitmessage address derivation based on 
On Sep 4, 2015 7:56 PM, "Justus Ranvier via bitcoin-dev" <
The "namespace" defined in BIP43 is acceptable. BIP44's is not:
It defines a centralized registry controlld by a single company instead of
having a way for different companies (or p2p chains like namecoin?) to
maintain competing registries.
Even better, it could use a code deterministically generated from the chain
ID (the hash of the genesis block), completely removing the need for a
registry in the first place.

@_date: 2015-09-06 04:09:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] RFC: HD Bitmessage address derivation based on 
On Sat, Sep 5, 2015 at 6:48 PM, Christophe Biocca
This is in fact useful. The centralized registries themselves are fine
provided that we don't rely on having only one of them or in them
having the same values for the same chains.
Trezor can maintain its own too.
Future versions of Trezor could support full chain IDs instead of
these integers (or keep using these integers forever, whatever they
chose to do).
Can you read my reasoning here?
What I propose is retro-compatible, even for carelessly designed
chains (that allowed pre-mining) like FTC.
And provides securely unique IDs that don't require a centralized registry.
Maybe I should start a Chain IDs BIP...

@_date: 2015-09-09 20:51:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Yet another blocklimit proposal / compromise 
On Sep 9, 2015 8:36 PM, "Marcel Jamin via bitcoin-dev" <
driving up costs of running a node up too much. Most systems currently
running a fullnode probably have some capacity left.
What about the risk of further increasing mining centralization?

@_date: 2015-09-11 18:22:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Yet another blocklimit proposal / compromise 
Unfortunately the relation between block maximum size and mining
centralization is much more complex than that.

@_date: 2015-09-11 18:32:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Days Destroyed as block selection 
On Sep 11, 2015 12:27 PM, "Dave Scotese via bitcoin-dev" <
pretending to) use the first-seen block, I propose that a more
sophisticated method of choosing which of two block solutions to accept.
There's already a criterion to chose: the one with more work (in valid
blocks) on top of it.

@_date: 2015-09-11 20:17:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Yet another blocklimit proposal / compromise 
worldwide." Increasing network connection requirements might even decrease
mining centralization right now.
No. People seem to think "Chinese have slow connections? Screw them, free
But not being well connected with the other miners is not a problem for the
Chinese miners (who are the hashrate majority), it's a problem for the rest
of the miners!!
It's not about being well connected to the "global internet", it's about
being well connected to the hashrate majority.
operation and

@_date: 2015-09-11 20:37:34
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Days Destroyed as block selection 
On Sep 11, 2015 1:18 PM, "Christophe Biocca" I thought he was proposing a new consesnsus rule. I see, this would be just
a policy validation that everybody would be free to ignore (like the "first
seen" spend conflict tx replacement policy).
I don't see how miners would benefit from running this policy so I would
not expect them to run it in the long run (like the "first seen" spend
conflict tx replacement policy).
If miners don't use it, I don't see how users can benefit from running that
policy themselves.
They will still have to keep waiting some block confirmation to
exponentially reduce the chances of a successful double-spend attack with
each new confirmation (as explained in the bitcoin white paper).
How do you know which of 2 blocks with the same height is "newer"?

@_date: 2015-09-16 22:27:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
For enforcing new restrictions on your own blocks (thus at the policy
level, not consensus) you don't need to wait for 75%. You can do it from
the start (this way all miners setting the bit will enforce the new
On Sep 16, 2015 4:20 PM, "Rusty Russell via bitcoin-dev" <

@_date: 2015-09-16 22:38:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
No, 95% is safer and will produce less orphaned blocks.
0%is fine to do it in your own blocks.
I agree on using height vs time. Rusty, what do you mean by being easier
for bip writers? How is writing "block x" any harder than writing "date y".
On Sep 16, 2015 4:32 PM, "Tier Nolan via bitcoin-dev" <

@_date: 2015-09-16 22:54:56
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
On Sep 16, 2015 4:49 PM, "Tier Nolan via bitcoin-dev" <
until 95%.
blocks (under the rule).
You shouldn't rely on that, some may start applying the restrictions in
their own blocks at 0% and others only at 90%. Until it becomes a consensus
rule it is just part of the standard policy (and we shouldn't rely on nodes
following the standard policy).

@_date: 2015-09-16 23:03:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
I understand your proposal, but I don't see what it accomplishes compared
to applying the new rule from the start (in your own blocks) and wait for
95% for consensus activation (which is my preference and it's much simpler
to implement).
What are the disadvantages of my approach? What are the advantages of yours?
On Sep 16, 2015 4:57 PM, "Tier Nolan via bitcoin-dev" <

@_date: 2015-09-17 15:59:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
On Thu, Sep 17, 2015 at 12:38 PM, Tier Nolan via bitcoin-dev
I'm still unconvinced, but thanks, this is what I was asking for.

@_date: 2015-09-17 21:14:38
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Fill or kill us normally used for trades and I think it can be confusing.
Previous times this has been discussed it has been discussed under
nExpiryTime or op_height (which enables expiration), for example, in the
freimarkets white paper.
As Mark points out this can be made safe by requiring that all the outputs
of a transaction that can expire have op_maturity/csv/rcltv of 100. That
makes them as reorg-safe as coinbase transactions. Unfortunately this
doesn't play very well with p2sh...
On Sep 17, 2015 3:08 PM, "Mark Friedenbach via bitcoin-dev" <

@_date: 2015-09-18 15:08:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Coinbase" flag in the UTXO set, set when the output is created.
I said require all scrptPubkeys to have op_maturity/rcltv/csv 100+, but
yeah, that would work.
Regarding nKillTime, please call it nExpiryTime. And instead of fill or
kill transactions, ttansactions that expire. It is not only more accurate
(ie fill or kill is for market orders that complete in their full amount
now or are cancelled, not for transfers) and it is the term we have been
using for years.
Reinventing the wheel by changing its name it's something we do often (for
example, rcltv was op_maturity in February 2014 and was "reinvented" as
rcltv recently. This makes it harder for people to learn and follow up.
Please don't insist in fok, that's for market orders and works differently
than expiries. Expiry is the old name and it's also much more accurate.

@_date: 2015-09-18 22:37:05
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hash of UTXO set as consensus-critical 
Well, with utxo commitments at some point maybe is enough to validate the
full headers history but only the last 5 years of ttansaction history
(assuming utxo commitments are buried 5 years worth of blocks in the past).
This scales much better than validating the full history and if we get a 5
year reorg something is going really wrong anyway...
Maybe after validating the last 5 years you also want to validate the rest
of the history backards to get the "fully-full node" security.
Of course 5 years it's just an arbitrary number: 2 or maybe even 1 would
probably be secure enough for most people. I've referred to this idea as
"hard checkpoints" or "moving the genesis block forward" in the past.
On Sep 18, 2015 4:18 PM, "Rune Kj?r Svendsen" <

@_date: 2015-09-18 22:38:06
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hash of UTXO set as consensus-critical 
s/move the genesis block forward/move your genesis checkpoint forward/

@_date: 2015-09-19 07:04:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
I disagree with the importance of this concern and old soft/hardforks will
replace this activation mechanism with height, so that's an argument in
favor of using the height from the start. This is "being discussed" in a
thread branched from bip99's discussion.
Anyway, is this proposing to use the block time or the median block time?
For some hardforks/softforks the block time complicates the implementation
(ie in acceptToMemoryPool) as discussed in the mentioned thread.

@_date: 2015-09-19 07:09:23
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
How them being expensive to generate make them less likely to be reorged?
Would an op_return output used as a nonce to make the hash of the
transaction contain some proof of work make the non-coinbase expirable
transaction more secure against reorgs?
I'm afraid your point is irrelevant.

@_date: 2015-09-21 10:24:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
The initial thread is linked to from the BIP document (which is in the
bitcoin/bips PR).

@_date: 2015-09-22 19:45:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
That would work. I was thinking about requiring OP_MATURITY 100 (or
greater than 100) in all the scriptPubKey's of the expiry transaction.

@_date: 2015-09-22 20:12:41
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
Just because you don't understand the changes proposed it doesn't mean
that they are random.
I may have done a poor job in communicating "my plan for libconsensus"
but I have tried many times and in many ways.
 logs show that I have not worked "in the dark" at all, on
the contrary, I've been very tenacious when asking for review and
opinions, to the point that several people (at least  and
 have complained about their github inboxes being full of
This is a relatively recent thread where I describe my plan:
Not my first attempt on this list.
It is very frustrating that everybody seems to agree that separating
libconsensus is a priority to maximize the number of people that can
safely contribute to the project, but at the same time, nobody thinks
that reviewing the necessary refactors to do so is a priority.
I tried creating big PRs for people to see "the big picture"  but
those were too many commits and nobody wanted to read it. Gavin asked
for an API.
So I tried a smaller step: exposing just VerifyHeader in libconsensus
and leave VerifyTx and VerifyBlock for later Again, this was "too big" and "a moving target". In the meantime I
always had smaller one-little-step PRs that were part of a longer
** [8/8] MERGED Consensus
- [X] Consensus: Decouple pow from chainparams  [consensuspow]
- [X] MOVEONLY: Move constants and globals to consensus.h - [X] Chainparams: Refactor: Decouple IsSuperMajority from Params()
 [params_consensus]
- [X] Remove redundant getter CChainParams::SubsidyHalvingInterval()
 [params_subsidy]
- [X] Separate CValidationState from main  [consensus]
- [X] Consensus: Decouple ContextualCheckBlockHeader from checkpoints
 [consensus_checkpoints]
- [X] Separate Consensus::CheckTxInputs and GetSpendHeight in
CheckInputs  [consensus_inputs]
- [X] Bugfix: Don't check the genesis block header before accepting it
 [5975-quick-fix]
** [5/5] DELETED
*** DELETED Refactor: Create CCoinsViewEfficient interface for
CCoinsViewCache  [coins]
*** DELETED Chainparams: Explicit Consensus::Params arg in consensus
functions  [params_consensus2]
*** DELETED MOVEONLY: Move most of consensus functions (pre-block)
 [consensus_moveonly] (depends on consensus-blocksize-0.12.99)
*** DELETED Consensus: Refactor: Separate CheckFinalTx from
main::IsFinalTx  [consensus_finaltx]
*** DELETED Consensus: Refactor: Turn CBlockIndex::GetMedianTimePast
into independent function  [consensus_mediantime]
*** DELETED Consensus: Adapt declarations of most obviously consensus
functions  [consensus-params-0.12.99]
*** DELETED Consensus: Move blocksize and related parameters to
consensusparams ...without removing consensus/consensus.h [
alternative]  [consensus-blocksize-0.12.99]
After a while I stop rebasing the longer branches and just maintained
a few small consensus-related PRs at a time.
Now I consolidated 3 of them in
*** REVIEW Optimizations: Consensus: In AcceptToMemoryPool,
ConnectBlock, and CreateNewBlock  [consensus-txinputs-0.12.99]
with the hope that it would be merged relatively fast.
After that it will be much simpler to start talking about potential C
APIs for VerifyHeader, VerifyTx and VerifyBlock; as well as separating
the library to a subtree.
I'm more than happy to answer any questions anyone may have about any
of the PRs or commits, until everybody interested is convinced that
there's nothing random in the proposed changes.
I'm also more than happy to get advice on how to better communicate my
plans and structure my PRs.

@_date: 2015-09-22 20:36:14
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Long-term vision for bitcoind (was libconsensus and 
I agree with this long term vision.
Here's how I think it could happen:
1) Libconsensus is completed and moved to a subtree (which has libsecp
as an internal subtree)
2) Bitcoind becomes a subtree of bitcoin-wallet (which has
bitcoin-wallet and bitcoin-qt)
Without aggressively changing it for this purpose, libconsensus should
tend to become C, like libsecp, which is better for proving
Hopefully at some point it won't take much to move to C.
Upper layers should move to C++11
Don't focus on the git subtrees, the basic architecture is bitcoin-qt
on top of bitcoin-wallet, bitcoin-wallet on top of bitcoind (and
friends like bitcoin-cli and bitcoin-tx), bitcoind on top of
libconsensus on top of libsecp256k1.
I believe this would maximize the number of people who can safely
contribute to the project.
I also believe this is the architecture most contributors have in mind
for the long term, but I may be wrong about it.
Criticisms to this plan?

@_date: 2015-09-23 18:58:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
Separating the consensus code is extremely important for less risky
and wider contributions regardless of what is exposed.
But once a complete libconsensus is exposed, alternative
implementations should use it (SPV implementations may not use all of
it though) and Bitcoin Core should eventually use it through its API
as well.
It will provide full consensus validation (verification) for the
following structures:
- Script (done, VerifyScript is already exposed)
- Block Headers
- Transactions
- Blocks (including headers and transactions)
The user of the library has to manage storage by itself. This library
will be stateless (apart from libsecp256k1's context) and won't
provide storage.
This library won't tell you which is the longest chain, the highest
level function is VerifyBlock() that just tells you whether a block is
valid or not.
Like the existing libconsensus, a complete libconsensus will have a C API.
The concrete API of each function is to be determined. The exact
concrete way to expose CCoinsViewCache and CBlockIndex (which are not
stateless) will require some discussion.
My preference is using function pointers combined with structs but
there's several possibilities there.
Once the code is separated and the rest of the undesired dependencies
are eliminated, people will be able to propose concrete final APIs
with a few commits.
At the very least:
- VerifyScript
- VerifyHeader
- VerifyTx
- VerifyBlock
To allow users of the library to intertwine policy or DoS checks with
the full verification of a structure (like Bitcoin core does today), I
would also expose at least:
- CheckTransaction/Consensus::CheckTx
- Consensus::CheckTxInputs
- Consensus::CheckTxInputsScripts (doesn't exist yet in master)
- CheckBlockHeader
- ContextualCheckBlockHeader
- CheckBlock
- ContextualCheckBlock
Nobody has the time to review a PR with the many commits necessary to
propose a final independently buildable and complete C API.
This is a work in progress and there's more people participating, not just me.
There's many possible roads that lead to Rome, but let's not allow
perfection be the enemy of walking the very first step.
Can we at least agree on most of the functions that are clearly
consensus critical and separate those so it's easy to build them
separately from main.cpp ?
Can we agree on some of the dependencies that are obviously undesired
and relatively easy to remove?

@_date: 2015-09-29 01:17:15
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
On Sep 28, 2015 7:14 PM, "Mike Hearn via bitcoin-dev" <
we may have 2 chains co-exist for a long time.
That depends only on how fast people upgrade, which is unaffected by the
rollout strategy used.
Yes, there is a difference. Assuming the hashrate majority upgrades, in the
case of a softfork non-upgraded miners will try to build on top of the
longest chain (the upgraded one) but their blocks will get consistently
orphaned for having a too old block version (and if they just increment the
version without implementing the new restrictions, then their blocks will
be orphaned when they fail to enforce the new restrictions). In the case of
a hardfork, the non-upgraded miners will keep on building their own longest
valid chain (the upgraded chain is not valid in their eyes), potentially
That's not to say softforks are always preferrable. There's cases when a
feature can be implemented as a softfork or a hardfork, but the softfork
solution is clearly inferior and introduces technical debt.
In those cases I prefer a hardfork, but this is not one of those cases.
In any case, maybe you want to provide some feedback to bip99, which is
about possible consensus rule changes scenarios and a recommended
deployment path for each of them (softforks and hardforks are subdivided in
several types). This discussion about the general desirability of softforks
seems offtopic for the concrete cltv deployment discussion, which assumes
softforks as deployment mechanism (just like bip66 assumed it).

@_date: 2015-09-30 17:55:52
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I didn't talked about how fast miners would upgrade, please read again
because I believe I was extremely precise.
In both cases I'm assuming there's a minority of the hasrate which
doesn't upgrade.
In the softfork case, the minority will always build on top of the
longest chain (which is valid to them). There may be many little
alternative chains that are ignored (and orphaned) by the upgraded
miners, but non-upgraded miners will always build on top of the
longest chain.
In the hardfork case, non-upgraded miners will reject the upgraded
chain because it is invalid to them, so they will build on top of the
longest non-upgraded chains.
Two alternative chains will continue growing forever unless the
non-upgraded miners eventually upgrade.
In contrast, there won't be 2 alternative chains growing forever in
the softfork case even if the minority miners never upgrade.
BIP99 recommends an uncontroversial softfork for this kind of case.
You seem to be contradicting BIP99 in many other places. Maybe you
want to complain about some of the recommendations in BIP99 (instead
of everywhere else):
On Wed, Sep 30, 2015 at 2:30 PM, Mike Hearn via bitcoin-dev
You seem to be the only one who thinks that softforks have "numerous
downsides" over hardforks.
So everybody just basically disagrees with the assumption in your
question and thus nobody can answer it.

@_date: 2015-09-30 18:14:42
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Is it possible for there to be two chains after a 
Gavin, you assume that users must necessarily always follow the
hashrate majority, but this is not true.
In fact, it is the opposite: market forces make the hashrate follow the users.
Not following the hashrate majority is not necessarily insane.
If some users aren't happy with the new hardfork rules, they may never
upgrade. This is discussed (although I want to improve the text) under
the "Schism hardforks" section of BIP99 (which you may have some
complaints against, so please review
It is true that users of chain A may sell or their B-coins, but the
opposite is also true: users in chain B may sell all their A-coins.
Speculators will likely sell both and probably not buy again until the
initial uncertainty is gone (or they may never buy again, nobody can
predict this).
Let's use an example. Let's assume that a hardfork is rolled out to
completely remove the blocksize limit (I believe you would be against
that from previous conversations with you).
As long as there are users creating demand for the old-coins, there
will be miners mining the old coins.
This is not insane for neither users or miners no matter how big the
majority of users and/or miners in the new rules chain.
Again, probably the best place to discuss this kind of thing is
 or the bitcoin-dev thread
linked from the BIP (
On Tue, Sep 29, 2015 at 8:23 PM, Allen Piscitello via bitcoin-dev

@_date: 2015-09-30 19:58:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
As previously explained, the biggest advantage of softforks is that
assuming the hasrate majority upgrades, network convergence is
I don't know of anyone else (apart from you) that believes that the
advantages of softforks are generally worse than those of hardforks.
I'm attempting to clarify everything related to consensus rule changes
in BIP99.
But your argument is flawed because it assumes softforks are more
risky than hardforks.
You've been explained why this is not the case, so unless you can
explain what's more important for a consensus system than network
convergence I think we can still consider this consensus rule change
uncontroversial, just like BIP66 was (even if you were also unable to
understand the advantages of softforks back then, just like you are
unable to understand them now, as you just proved in your answer to my
explanation). Using BIP99's terminology, this is an "uncontroversial
softfork" and it's therefore the safest option for consensus rule
changes deployment.
I should definitely improve my explanation on why uncontroversial
softforks are preferrable to uncontroversial hardforks in most cases
(and maybe try to come up with an example in which a hardfork is
preferable). I should also explain the disadvantages of
uncontroversial softforks that you have pointed out several times. So
I will mention you in BIP99's PR once I update it with a new section
that talks about the trade offs of uncontroversial softforks vs
uncontroversial hardforks.
In the meantime I believe that we can safely move forwards with BIP65
(again, just like we did with BIP66 ) and I also believe that you, as
an expert in Bitcoin, will eventually be able to understand the
advantages of uncontroversial softforks.
With all due respect, I don't think we need to wait for you to
understand the advantages of softforks to move forward with BIP65,
just like we didn't need to wait for every developer and user to
understand BIP66 to deploy it.
You don't have specific complaints against the new script operator,
and you don't have an uncontroversial hardfork alternative design (or
This is a feature that enables new contracts that are important to
Bitcoin. Please don't try to block it just to make a point about what
"uncontroversial" means.

@_date: 2015-09-30 20:10:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core 0.12.0 release schedule 
Yes, I believe consensus rule changes don't need to be couple with
major releases, there's no problem that I can see in them being minor
releases if they're not ready on time for a major release.
On Wed, Sep 30, 2015 at 7:57 PM, Luke Dashjr via bitcoin-dev

@_date: 2015-09-30 22:37:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
On Sep 30, 2015 9:56 PM, "Mike Hearn via bitcoin-dev" <
don't. You get constant mini divergences until everyone has upgraded, as
opposed to a single divergence with a hard fork (until everyone has
upgraded). The quantity of invalid blocks mined, on the other hand, is
identical in both types.
Exactly, all those "mini divergences" eventually disappear (because we're
assuming the hashrate majority has upgraded and non-upgraded miners accept
upgraded blocks as valid), even if the hashrate minority never upgrades.
On the other hand, the "single divergence" in the hardfork keeps growing
forever (unless all miners evetually upgrade.
With softforks, we maintain eventual consistency, with hardforks we don't.

@_date: 2015-10-01 00:14:21
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
That is correct. But doesn't seem to contradict anything I said.
Assuming it is an uncontroversial hardfork (unlike bip101 in its
current form), miners will eventually upgrade because all users will
eventually upgrade as well.
Softfork-caused forks will live shortly because non-upgraded miners
will build on top of the longest upgraded chain.
In contrast, non-upgraded miners will not build on top of the longest
chain (the upgraded one assuming hashrate majority) and a parallel
chain will be built for some time. This chain can be used to defraud
non-upgraded or SPV users by isolating them and showing them only the
non-upgraded chain, which keeps growing but will eventually be
In the case of a Schism hardfork, some users may never want to
"upgrade" and if there's demand for the "old coins" there will be
miners for the "old chain".
I think my argument makes a lot of sense, it's just that for some
reason you don't think guaranteed eventual consistency has any value
because you are ok with miners abandoning the old rules chain only
eventually (and you don't believe that "eventually" can be far in the
future in practice).
On Wed, Sep 30, 2015 at 9:56 PM, Mike Hearn via bitcoin-dev
BIP99 doesn't talk about "developer consensus", but rather
"uncontroversial consensus rule changes".
Obviously a patch in which developers steal everybody else's coins
wouldn't be "uncontroversial" even if "developer consensus" is
We don't need to ignore anyone to consider BIP65 an uncontroversial
softfork: we just need to ignore fallacious and unreasonable
As far as I can tell, you are the only person opposing BIP65 (even if
you keep talking about "several people") and I would like to think
that you are aren't being obstinate on purpose only to make your point
about "developer consensus not meaning anything", but you are making
it very hard.
On Wed, Sep 30, 2015 at 11:01 PM, Mike Hearn via bitcoin-dev
No, you didn't. "Simplified Payment Verification" is section 8 in the
Bitcoin whitepaper that you like to cite so much.
Please study this page carefully and hopefully one day you will stop
using logical fallacies as often as you currently do:
In this case you manage to combine ad hominem and appeal to authority
(maybe false authority is more accurate?).
Once again, please, stop using fallacies to try to convince people
that you are right. No offense, but being warned publicly about the
use of logical fallacies so often would be extremely embarrassing to

@_date: 2016-08-12 17:49:23
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
No, anyone with the bip32 public seed can do the same as the receiver as
"watch only". The only difference is rhat the receiver can actually spend
the coins. As gmaxwell explained, if it's expensive for everyone, it will
be also expensive for the receiver (assuming no interaction after the bip32
public seed is transfered).
Something different would be to give a different bip32 public seed to each
payer.  That way they can simply start with zero an increment with each new
payment. With those assumptions, the receiver could start listening to new
addresses only after they receive something in the previous address.
Probably not useful for this case, just thinking out loud about using bip32
public seeds instead of one use addresses when there's going to be several
payments from the same payer to the payee.
On Aug 12, 2016 2:37 PM, "Erik Aronesty via bitcoin-dev" <
to it.
possible addresses would be a barrier
blockchain fairly efficiently and determine which addresses he has the keys
compute all

@_date: 2016-12-02 02:42:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] New BIP: Hardfork warning system 
We can already warn users of a hardfork when a block is invalid (at
least) because of the highest bit in nVersion (as you say, because it
is forbidden since bip34 was deployed). It seems the softfork serves
only to warn about soft-hardforks, assuming it chooses to use this
mechanism (which a malicious soft hardfork may not do). In fact, you
could reuse another of the prohibited bits to signal a soft-hardfork
while distinguishing it from a regular hardfork. And this will also
serve for old nodes that have not upgraded to the softfork. But, wait,
if you signal a soft-hardfork with an invalid bit, it's not a
soft-hardfork anymore, is it? It's simply a hardfork.
Your softfork would result in soft hardforks being hardforks for nodes
that upgraded to this softfork, but softforks for older nodes.
Is this the intended behaviour? if so, why?
I would rather have a simpler BIP that doesn't require a softfork
(whether it recommends soft-hardforks to use one of the currently
invalid bits, but a different one than from hardforks or not, but I
also don't see the reason why soft-hardforks should appear as invalid
blocks for older nodes instead of using regular softfork warning
[besides, in this case, after the "unkown softfork" warning you will
get only empty blocks, which may make you suspicious]).

@_date: 2016-12-02 07:35:36
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] New BIP: Hardfork warning system 
Ok, so the goal of the softfork then is to hold on is there is to
"hold on" on the most-work valid chain while there's an even-more-work
invalid chain and for new nodes force a response from the user. This
could be clearer in the motivation section.
We can still notify and force a response from the user with a single
invalid block (or N, or W accumulated work). Note that we don't need
to "hold on" while waiting for the user's response. Therefore I insist
there could be a PIB with all the recommended warnings but not
softfork (which this one could extend from).
Thinking as a full node user, I'm not sure I want my node to "hold on"
on validating new valid blocks just because there's some "longest"
invalid chain out there and I may chose to follow that instead later.
Before paying or copying another address to receive, I would
definitely would want the warning though.
Particularly as a miner (not that I'm one), I think validationless
mining shows us that some miners prefer to throw energy to the abyss
of validation uncertainty rather than stop their mining hardware.
What if when I give the response to the system I decide to pass on the
HF but it means my hardware have been not mining in the valid chain
for hours?
I would account that as "money lost thanks to a 'friendly' interface".
Specially if we're talking about a controversial hardfork.
If we're talking about an uncontroversial hardfork, I would definitely
prefer BIP9 coordination.
I would prefer to receive the warning when, say, 30% of the hashrate
is supporting an unknown change to the consensus rules (regardless of
it being a softfork or a hardfork, which I don't know yet until the hf
bit is used because the change is unknown to me), way before I need to
decide what branch to mine.
In fact, if I was a miner but not a user at the same time, after
knowing about the unknown hardfork and if I consider the hardfork to
be potentially controversial, I would try to coordinate with exchanges
(and pools if no solo mining) to be able to write a program that
chooses the chain likely to be most profitable depending on difficulty
and price feeds for every block.
In the case of a SHF, even more reason to keep mining on the old
chain, perhaps I mine one empty block (assuming that's a rule in the
SHF) out of luck, or maybe I should just start mining empty blocks
whenever I see the SHF bit active for a block whose chain keeps
Perhaps for a SHF we should use a valid bit instead of an invalid one
(clearing all possible fears with old and older nodes perceiving SHFs
differently as HFs and SFs respectively). We can trivially make old an
older nodes coincide in their perception of good-willing SHFs as
either HFs or SFs as we wish. Choosing divergence of perception from
the 2 versions we're considering makes no sense to me.
I'm reserving my judgement for which one I prefer just in case there's
actually an advantage in this divergence, but I've missed it.
Terminology, I think you get my point. I'm all for formalizing
definitions but please let's not slow down discussion unnecessarily.
I'm fine saying "evil fork" instead of "malicious SHF" if you prefer
that, but they're just the same thing.
Right, and those same mechanisms could be implemented using one of the
already prohibited bits (for example, just like the higher weight bit
in nHeight, the lowest value one was prohibited when BIP34 was
There's no need to invalidate another bit in the softfork (repeat: bit
1 got invalidated when bip34 was activated as well; or we could just
use a valid bit for SHFs).
Right, I'm also under the assumption that a HF (or a SHF) would give
plenty of/enough time (to be defined, I suggest at least 1 year but we
really shouldn't get into this in this thread) in their
BIP9Deployment::nStartTime (or equivalent if BIP9 is not reused for
HF/SHF). Otherwise I would consider any HF or SHF controversial in
itself regardless of what it does (for not giving enough time to users
to adapt).
Therefore we can assume that all the warnings would be deployed in
advance to any HF or SHF, with or without a previous softfork.
I strongly disagree that the proposed softfork "[makes] it easier to
resist an un-consented-to hardfork". If anything, it makes it easier
to disrupt the old network if it doesn't fully consent to the HF. For
"un-consented-to SHFs" (or "evil HFs" if you prefer) I don't think
it's a safe to assume they will use an invalid bit to signal their
intend. At least if I was an "evil softhardforker" just interested in
disruption, I wouldn't do it (just like if I was an "evil hardforker"
I wouldn't use the normal hardfork bit).

@_date: 2016-12-15 19:48:47
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Planned Obsolescence 
Hard to predict or not, you can't force people to run newer software.
When you introduce anti-features like this in free software they can
be trivially removed and they likely will.
There's a simpler solution for this which is what is being done now:
stop maintaining and giving support for older versions.
There's limited resources and developers are rarely interested in
fixing bugs for very old versions. Users shouldn't expect things to be
backported to old versions (if developers do it and there's enough
testing, there's no reason not to do more releases of old versions, it
is just rarely the case).

@_date: 2016-02-02 18:38:59
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process: Status, comments, 
In the section Can we please find another term for the "consensus" here (which is
often confused with "consensus rules", "consensus code" etc)?
In BIP99 I used the term "uncontroversial", but I'm happy to change it
to something else if that helps us moving away from consistently using
the same term for two related but very different concepts.
"nearly universal acceptance", "ecosystem-harmonious"...seriously,
almost anything would be better than keep overloading "consensus"...

@_date: 2016-02-03 01:59:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP Process: Status, comments, 
It  is true that are many levels of consensus and that term itself is
not incorrect for any of the meanings.
Maybe we should try to start distinguishing between different types of
In BIP99 the only concepts that are needed are "consensus rules" and
"adoption consensus" (aka "community consensus", "full node runners
consenusus", "monetary users consenusus", "economic
super-ultra-majority", not sure if any of them or all of them, that's
still a placeholder in bip99 for
[ie safe deployment requirements for an uncontroversial hardfork, just
like we have for uncontroversial softforks]).
Whatever term and defintion we chose for this concept, it has to be
neutral to whether the consensus rule changes are can be deployed as a
softfork or only as a hardfork [although we have had many
hardfork-to-softfork re-designs in the past and I agree that there
will be more, some people including  suspect that SF=HF, but
haven't been able to prove it yet], or otherwise we're implicitly
giving miners a power of unilaterally changing some consensus rules
that they don't have, for users can't never be denied from the right
to validate whatever rules they chose, just like an old radio receiver
machine owner cannot be forced to listen any channel in particular.
The "consensus rules" are in some sense the id of a theoretical
communication channel, and should not be confused with a real-life
process for how users should coordinate to "upgrade" to a new channel
(which is what BIP99 is about) or how we can objectively know whether
a proposed changed has had "adoption consensus" or not, which is what
this BIP is about.
But yeah, suggestions totally welcomed for a replacement for "adoption
I'm afraid this would be horribly expensive in development hours for
not good enough reason and I must nack.

@_date: 2016-02-05 10:58:36
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hardfork bit BIP 
Concept ACK. I've been talking about adding this to BIP99 since before
scaling bitcoin Hong Kong, so it will be nice to have a BIP to just point
to. Also I hadn't thought about concurrent deployment of 2 hardforks, nice.
On Feb 4, 2016 23:30, "Gavin Andresen via bitcoin-dev" <
negative version number will, indeed, fork them off the the chain, in
exactly the same way a block with new hard-forking consensus rules would.
And with the same consequences (if there is any hashpower not paying
attention, then a worthless minority chain might continue on with the old
Additionally, a warning or special error could be thrown when a block is
rejected due to the hardfork bit being activated.
recommends that SPV clients to pay attention to block version numbers in
the headers that they download, and warn if there is a soft OR hard fork
that they don't know about.
Although I agree this PR should include such warning/error recommendations,
SPV nodes can't tell whether a change is a hardfork or a softfork just by
looking at the version bits, even in the case of uncontroversial hardforks
deployed with bip9 as recommended by bip99. For controversial hardforks
where bip9 should NOT be used for deployment, setting the hardfork bit is
even more important.
timestamps in the block headers that the receive, and to warn if blocks
were generated either much slower or faster than statistically likely.
Doing that (as Bitcoin Core already does) will mitigate Sybil attacks in
This seems out of the scope of this PR.

@_date: 2016-02-05 11:20:28
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hardfork bit BIP 
On Feb 4, 2016 19:29, "Luke Dashjr via bitcoin-dev" <
I don't interpret this as "treating version bits as a number" it is just
being explained which bit we're talking about. Could you propose some
concrete rephrasing instead of leaving the task of somehow solving this
vague and subtle concern to the author?
I very much disagree with "significant" and in any case it depends on the
hardfork: the changes required can still be quite minimal in all cases and
it should never be a problem, even for emergency hardforks. In emergency,
we could for example just a new global (we have many already anyway),
although activeChain.tip () is already there and one can simply get the
last height or median time from there.
This is consistent with bip99, which recommends bip9 for deploying
uncontroversial hardforks.
Controversial hardforks (as defined bip9) always have the potential to
create two chains that survive for unbounded amounts of time (maybe
forever) as discussed in one of the few threads of the bitcoin discuss
mailing list.
Of course, BIP99 cannot say anything general about the "legitimacy" of all
controversial hardforks since ASIC-reset hardforks, for example, are
controversial hardforks by definition in the context of bip99 (and the
definitions in bip99 seem to apply to this bip). BIP99 can only warn about
the dangers and risks of controversial hardforks but at some point (let's
hope never) a controversial hardfork may be required to save the system
from some evil (say, evil miners blacklisting via softforking out the
miners that don't  blacklist or something) and that controversial hardfork
would be legitimate (at least to the eyes of some).

@_date: 2016-02-06 04:14:03
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
If it is to be uncontroversial and everybody will upgrade, there's no
fear of a "veto power" and there's no good reason not to wait for 95%
block version signaling for deployment coordination, ideally using
But that's for chosing the exact block where to start. The grace
period to give time to all users to upgrade should be before and not
after miner's final confirmation: that simplifies and accelerates
things. Assuming we chose a grace period that is really adequate,
nearly 100% of miners will have likely upgraded long before everyone
(since miners are a subset of "everyone"). If that is not the case and
miners happen to be the latest to upgrade, using bip9 after the grace
period (aka starting median-time/height) will make sure the hardfork
doesn't get activated without 95% of the miners having upgraded.
28 days seems extremely short (specially if the grace period comes
first), some people have suggested one year for simple hardforks like
this one.
On Sat, Feb 6, 2016 at 1:12 AM, Luke Dashjr via bitcoin-dev

@_date: 2016-02-06 18:09:21
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
Any thoughts on the "95% better than 75%" and "grace period before miner
coordination instead of after" comments ?
nodes-- probably 30 to 40%. Losing those nodes will not be a problem, for
three reasons:
None of the reasons you list say anything about the fact that "being lost"
(kicked out of the network) is a problem for those node's users.
grace period.
I didn't say that.
arguments why an upgrade would take a business or individual longer than 28
Their own software stack may require more work to integrate the new rules
or their resources may not be immediately available to focus on this within
28 days they hadn't planned.
I believe it wold be less controversial to chose something that nobody can
deny is more than plenty of time for everyone  to implement the changes
like, say, 1 year. I wouldn't personally oppose to something shorter like 6
months for really simple changes, but I don't see how 28 can ever be
considered uncontroversial and safe for everyone. Just trying to help in
removing controversy from the PR, but if you still think 28 can be safe and
uncontroversial, feel free to ignore these comments on the concrete length
and please let me know what you think about the other points I raised.

@_date: 2016-01-02 17:37:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segregated witnesses and validationless mining 
Is there a link to the IRC discussion?
On Jan 1, 2016 12:49 AM, "Peter Todd via bitcoin-dev" <
Is there a link to the IRC discussion?

@_date: 2016-01-11 21:32:15
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
On Fri, Jan 8, 2016 at 4:50 PM, Gavin Andresen via bitcoin-dev
If the crypto code is properly encapsulated, the code complexity costs
of choosing one hashing function over another should be non-existent.
You made the space argument which is valid, but in my opinion code
complexity shouldn't be a valid concern in this discussion.
As a maybe uninteresting anecdote, I proposed the asset IDs in
to do the same ```ripemd160 . sha256``` choice that Mark Friedenbach
had proposed and I had approved for
. More humble than me, he admitted he had made a design mistake much
earlier than me, who (maybe paradoxically) probably had less knowledge
for making crypto choices at the low level. In the end I was convinced
with examples I failed to write down for documentation and can't
That's not to say I have anything to say in this debate other than
code complexity (which I do feel qualified to talk about) shouldn't be
a concern in this debate. Just want to focus the discussion on what it
should be: security vs space tradeoff.
Since I am admittedly in doubt, I tend to prefer to play safe, but
neither my feelings nor my anecdote are logical arguments and should,
therefore, be ignored for any conclusions in the ```ripemd160 .
sha256``` vs sha256d debate. Just like you non-sequitor "sha256d will
lead to more code complexity", if anything, sha256d should be simpler
than ```ripemd160 . sha256``` (but not simpler enough that it matters

@_date: 2016-01-12 18:53:24
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus phase 2 
After talking to some people about libconsensus in the last Hong Kong
conference I realized that my initial plan of exposing one more thing
at a time would actually probably slow things down.
There's still a promised pdf with pictures that will be released, and
actually drafting the UML pictures helped realize that the whole
explanation could be much simpler if  was merged first as the
last step in phase 1 (that phase has so many contributors that I will
probably never get finished documenting it). Matt Corallo's idea of
exposing VerifyScript() through a C API certainly helped a lot in
cementing the more-minimal-than-earlier dependencies (thanks to Cory
Fields among many other people before him) that are not part of the
incomplete but existing libbitcoinconsensus library.
Given this success in protecting encapsulation by exposing things in a
new library, my instinct was to expose more things: VerifyHeader(),
VerifyTx() and VerifyBlock() [in that order].
But all those three new functions depend on storage in one way or
another. That was part of my reasoning to expose VerifyHeader() first,
because I believe there will be less discussion on a common interface
for the stored longest chain than for the utxo view (which may depend
on other transactions spent within the same block).
In any case, I realized we should finish putting all the consensus
critical code in the libconsensus lib and then worry about its "final"
Therefore I changed the goal of the phase 2 in my libconsensus
encapsulation planning from "expose VerifyHeader() in the existing
libconsensus library" to "build all the consensus critical code within
the existing libconsensus library, even if we don't expose anything
else". I believe this is much feasible for a single Bitcoin Core
release cycle and also more of a priority. Other implementations
experimenting with libconsensus like
 will have the
chance to compare their reimplementations with the future complete
libbitcoinconsensus without having to worry about the C API, which
ideally they will help to define.
I repeat, the goal of phase 2 in my upcoming libconsensus
encapsulation plan is to fully decouple libconsensus from Bitcoin
In phase 3, we can refine the storage interfaces and focus on a
quasi-final C API.
In phase 4, we can refine and take out code that doesn't belong in
libconsensus like CTxOut::GetDustThreshold() in
primitives/transaction.h and move all those consensus files to the
consensus folder before creating a separate sub-repository like for
libsecp256k1. Note that most of the file-moving work can be in
parallel to phases 2 and 3 and, in fact, by any new developer that is
willing to exchange rebase-patience for meaningless github stats (I'll
do it if nobody else wants, but I'm more than happy to delegate there:
I have more than enough github meaningless stats already).
As said, the document with pictures and the update to  are still
promised, but until they're ready, merging/reviewing   and  could do a great deal to make later steps in
libconsensus phase 2 more readable.
Most reviewers probably don't need to see any "big picture" to tell
whether certain functions on Bitcoin Core are consensus-critical or
not, or whether consensus critical code needs to depend on util.o or
But I wouldn't be writing to the mailing list without a plan with
further words nor pictures if I didn't had what I believe is a
complete implementation of what I just defined as "libconsensus phase
Phase 3 should finish long pending discussions like "should
libconsensus be C++14 or plain C" which should NOT delay phase 2.
Phase 4 should be mostly trivial: rename files to the target dir and
move the remaining unused code out of libconsensus.
Phase 5 should make Bitcoin Core eat its own dog food and use
libbitcoinconsensus oonly by its generic C API (I'm sorry if this
looks too far away for me to even think about detailing it).
The work in progress branch (but hopefully being finished, nit and
merged within the 0.12.99 cycle) can be found in:
Before sipa asks, signing code may make it into a new library but
SHOULDN'T BE PART OF LIBBITCOINCONSENSUS. Ideally, all exposed
functions will return true or false and an error string. It is based
on last-0.12.99 3cd836c1 but by popular demand I can open it as a
"DEPENDENT-tagged" PR linking to smaller steps and keeping track of
steps done. Analogous to the about to be replaced (for a simpler and
more maintainable example of testchain)  If people like
Wladimir, Cory and Pieter cannot see that I've been able to reduce my
overall cry-for-review noise thanks to github adoption of emacs'
org-mode's [ ] vs [X] I can alwways leave those "big picture" branches
as "private" branches out of the pull request count.
I expect to publish a phase 3 branch very shortly. But as said I
expect a lot of discussion on the API part, so I don't expect big
movements in phase 3 until phase 2 is done (as said phase 4 is
orthogonal to anything, this time git will say "verified MOVEONLY" for
To finish this long mail, if you are new to free software and would
like to get familiarized with Bitcoin Core development in particular,
moving one file is a simple task that you can always besure you can do
The way I plan to hand this to you, you won't need to convince anyone
to publicly confirm that your "MOVEONLY" commit being legit, because
all your remaining work will be to build on one platform (ideally you
should do a gitian build, but embarrassingly enough for someone
touching consensus code I just trust travis ) and trust travis (as
said, that's what I do from my laptop, but I plan to buy my own
building machine [and maybe outsource it for free in some protocol
that hasn't been invented, sorry again for the distraction]) and fix
the includes that have stopped working.
I intend to create an issue to move all the files in this list one by one:
But don't hesitate to contact me if are eager for moving some files,
because I believe we can save a few lines of total diff if we chose
the order of the movements properly.
Sorry, I forgot many people read this list again.
Happy to answer any question.
Specially about

@_date: 2016-01-13 09:37:46
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus phase 2 
The interface will exist but it will be a C++ interface that fits
better with Bitcoin Core internals.
See an initial draft of what could be the storage interface:
Phase 3 will consist on discussing and refining that interface to also
define the C interfaces using structs of function pointers instead of
classes (see for an early draft) that is needed for the "final" C API.
But since I think there will be more discussion and work defining
those interfaces, I would rather start with ANY interface that allows
us to decouple the consensus code from chain.o and coins.o, which we
don't want to be built as part of the consensus building package
(which is used for building both libbitcoinconsensus and Bitcoin
Future potential users are more than welcomed to draft their own C
APIs and that experience should be useful for phase 3.
I was expecting you, for example, to include the whole consensus code
(even if it lacks a C API) in
 for better testing
of the equivalent code in libbitcoin. You are kind of taking the C API
part out already, so this time you will just have less things to
This is a concern that has been risen repeatedly.
I am aware that faithful reproduction of the stored data is a
prerequisite for consensus validity. On the other hand, my presumption
is that a libbitcoinconsensus that forces its users to a given unifrom
storage will likely had much less users and any alternative
implementation that wants to implement its own custom storage would
have to necessarily reimplement the consensus validation code.
Doing it this way is more flexible. We can relatively easily implement
another library (if I remember correctly, last time we talked about it
we reffered to it as "libconsensus plus", but there's probably better
names) also takes care of storage for the users that don't want to
take the risks of reimplementing the storage (probably just using
Bitcoin Core's structures).
Unlike me, Luke Dashjr, for example, advocated for the
storage-dependent version, but I believe that implementing both
versions was an acceptable solution to him.
It is certainly an acceptable solution for me. I don't want to force
anyone that doesn't want or need to take the risks reimplementing the
consensus storage part to do so. But at the same time I really believe
that it would be a mistake to not allow it optionally.
Does that make sense?

@_date: 2016-01-29 19:50:14
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Best (block nr % 2016) for hard fork activation? 
The reason why BIP9 (versionbits) only checks for new activations
during difficulty retargettings is a simple optimization to only check
1/2016 of the blocks.
I suspect the check itself is not that costly for Bitcoin Core, which
has all the block headers in memory anyway, but I don't think we
should assume that will be the case for all implementations.
As an aside, BIP99 never recommends a 75% mining signaling activation
threshold: it recommends 95% for uncontroversial rule changes and no
miner signaling at all for controversial hardforks.
I still have to update BIP99 with some later changes I commented at
Scaling Bitcoin HK like signaling hardfork activation with the
"negative int32_t bit" so that old clients are forced to
upgrade/decide. We could start deploying better ways to inform users
about a hardfork event, but of course those changes cannot be applied
to older software that is already deployed (but hopefully they will
still notice something is weird is happening if the longest chain that
keeps growing is invalid because it contained a block with a negative
version in it).
But I'm yet to see a single hardfork proposal that follows BIP99's
recommendations besides the hardfork proposed in BIP99 itself, which
should consist on a manageable list of very simple to deploy fixes
like the timewarp fix forward-ported from Freicoin 0.8 for the BIP. I
haven't seen much interest in growing that little list of "a few fixes
nobody disagrees are bugs or sub-optimal design decisions, plus the
changes are easy to implement both separately and as a whole" either.
I cannot say I have seen any opposition at all to BIP99 as a hardfork
either, but I naively expected people would ask me to implement more
things for BIP99 besides
or even contribute the patches themselves. For all that, I don't
consider BIP99 a priority to work on and I plan to complete it at some
point later, unless there's a time limit for a BIP to be in the
"draft" state or something.
If someone else considers completing BIP99 a priority, I'm happy to
review and integrate things, though. Thanks again to all the reviewers
and contributors to the BIP at this time and I'm sorry that it has
been stuck for some time. Maybe the classification/recommendations
should have been a BIP without code and the hardfork proposal itself
should have been another one and that would have been clearer. I just
wanted to have some code on my first BIP (and as said the plan is
still to put more code at some point).

@_date: 2016-03-03 16:36:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] consensus rule change for TX fee safety 
There's  an absurd fee (non-consensus) check already. Maybe that check can
be improved, but probably the wallet layer is more appropriate for this.
On Mar 3, 2016 16:23, "Henning Kopp via bitcoin-dev" <

@_date: 2016-03-10 16:46:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 2 promotion to Final 
On Mar 10, 2016 02:04, "Mustafa Al-Bassam via bitcoin-dev" <
No, the hardfork can still happen, but if a small group remains using the
old chain (a single person will likely abandon it very soon), then it
cannot be said that deployment was universal and thus the hardfork BIP
doesn't move to the final state. As long as there's users using the old
chain, a hardfork BIP shouldn't become final if I understood BIP2
In other words,  uncontroversial hardfork bips can make it to the final
state once deployed, controversial hardforks may never become universally

@_date: 2016-03-10 16:59:32
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 2 promotion to Final 
On Mar 10, 2016 16:51, "Mustafa Al-Bassam via bitcoin-dev" <
And all the attacker will achieve is preventing a field on a text file on
github from moving from "active" to "final".
Seems pretty stupid. Why would an attacker care so much about this? Is
there any way the attacker can make gains or harm bitcoin with this attack?

@_date: 2016-03-10 19:30:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP 2 promotion to Final 
reaching final status, means that in an base of millions of users it's
guaranteed that some disgruntled or bored person out there will attack it,
even if it's for the lulz.
I still fail to see the harm caused by this attack. At some point the
attacker will get bored of laughing even if the attack has a small costs
(which I'm not that sure it is).
one - will be adapted by every single person in a ecosystem of millions of
people, is wishful thinking and the BIP may as well say "hard fork BIPs
shall never reach final status."
This is what seem to have happened with uncontroversial softforks in the
past. Why is wishful thinking to expect the same for uncontroversial

@_date: 2016-05-11 16:07:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
On May 11, 2016 05:15, "Timo Hanke via bitcoin-dev" <
hardfork that is only contentious but doesn?t change the mining algorithm,
the kind of hardfork you are proposing would guarantee the persistence of
two chains.
If all users abandon the old rules, why would asicboost miners continue to
spend energy on a chain that everybody else is ignoring?
validation ruleset S you have to make sure that every hardware that was
capable of mining R-valid blocks is also capable of mining S-valid blocks.
No, this proposal, for example, may make patented asicboost hardware
I don't accept this claim as true, this is just your opinion.
optimizations as possible to the point where there are no more
optimizations left to do, or hopefully getting very close to that point.
What do you mean by "embrace" in the context of a patented optimization
that one miner can prevent the rest from using?

@_date: 2016-05-12 13:05:51
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
On May 12, 2016 00:43, "Timo Hanke via bitcoin-dev" <
inevitably lead to the creation of an altcoin. Simply because the hardware
exists and can't be used for anything else both chains will survive. I was
only comparing the situation to a contentious hardfork that does not fork
out any hardware. If the latter one is suspected to lead to the permanent
existence of two chains then a hardfork that forks out hardware is even
more likely to do so (I claim it's guaranteed).
You are wrong. Whether 2 chains survive in parallel or not depends SOLELY
in whether both chains maintain demand (aka users).
Anyway, this is a discussion I had with Gavin and Rusty on bitcoin-discuss
already. I suggest we move this particular point there since it is more
philosophical than technical.

@_date: 2016-05-18 13:14:59
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
On May 17, 2016 15:23, "Peter Todd via bitcoin-dev" <
merkle path
How expensive it is to update a leaf from this tree from unspent to spent?
Wouldn't it be better to have both an append-only TXO and an append-only
STXO (with all spent outputs, not only the latest ones like in your "STXO")?

@_date: 2016-05-19 11:31:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
tip of the tree.
Updating a leaf is just as expensive as adding a new one?
That's not what I expected.
Or is adding a new one O (1) ?
Anyway, thanks, I'll read this in more detail.
Just the same way the TXO is (you just stop updating the txo leafs from
unspent to spent.
Yeah, that's what I want. Like your append only TXO but for STXO (that way
we avoid ever updating leafs in the TXO, and I suspect there are other
advantages for fraud proofs).
No complain with MMR. My point is having 2 of them separated: one for the
TXO (entries unmutable) and one for the STXO (again, entries unmutable).
Maybe it doesn't make sense, but I would like to understand why.

@_date: 2016-11-17 00:48:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
On Wed, Nov 16, 2016 at 2:58 PM, Eric Voskuil via bitcoin-dev
Checkpoints are not necessary for consensus and work is being done to
remove them completely from Bitcoin Core in particular.
I personally think uncontroversial hardforks are ok.
Good question, specially if we plan to do this with future buried
deployments. What about 1 year reorg?
Not sure I understand this question.
If this is a hardfork, it is one that will only be visible if/when
there's a very deep reorg , one of the kind where we can practically
consider Bitcoin done (and only if some nodes keep the ISM code).
But I could accept that definition. Another way to see it (even though
other said the optimization part was not important) as such an
optimization and simplification.
The way I see it, ISM and BIP9 are just coordination mechanisms for
uncontroversial rule changes.
Once the coordination happened and is long in the past, I really don't
see the problem with replacing the mechanism with a simpler height.
Mhmm, I disagree on the notion that any hardfork necessarily
represents an altcoin.
It is certainly an improvement in the sense that it simplifies
implementations and optimizes validation. You may argue that you don't
consider the improvement important though.
These changes to Bitcoin Core could be rolled back (and obviously
other implementations don't need to adopt them unless they want to
benefit from the simplification/optimization or fear such a long
reaorg), but I really hope we don't.
Trying to understand you better...
Accepting your definition of this as a hardfork, do you oppose to it
simply because it is a hardfork, or because you consider this
"hardfork" a bad idea for some reason I am missing?

@_date: 2016-11-17 00:58:19
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
On Wed, Nov 16, 2016 at 3:18 PM, Thomas Kerin via bitcoin-dev
This is not really the same. BIP30 is not validated after BIP34 is
active because blocks complying with BIP34 will always necessarily
comply with BIP30 (ie coinbases cannot be duplicated after they
include the block height).

@_date: 2016-11-17 01:06:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP Proposal] 
Sorry for moving the topic, but isn't duplication of tx hashes
precisely what BIP30 prevents?
That was my undesrtanding but should read it again.
Since regular txs take inputs, the collision is extremely unlikely
(again, this is my understanding, please correct me when wrong), the
worrying case is coinbase txs (which don't have input to take entropy
from). By introducing the committed height, collisions on coinbase txs
are prevented too.
If I'm wrong on any of this I'm more than happy to learn why.

@_date: 2016-10-03 08:17:25
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The use OP_COUNT_ACKS for paying for a common 
When would miners vote no to receive more funds?
Also, why would they spend the funds buying X once they get them?
On Oct 3, 2016 00:58, "Sergio Demian Lerner via bitcoin-dev" <

@_date: 2016-10-10 16:58:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Libconsensus completion plan document (with pictures) 
Hello, since trying to encapsulate consensus code without exposing
anything else (see my post from january
) wasn't succesful in getting review, I decided to turn "phase 2" into
"expose verifyHeader" again. I was previously starting the document
with pictures but since things we're changing and the pictures were
already deprecated, I decided to wait after segwit was merged and
include those changes in the pictures too.
This time I created a repository so that people can look at it, even
if it's less advanced than previous versions have been:
Here's a branch with the resulting images, latex file and pdf:
And here's the pdf:
Any questions or comments are welcomed. If some of the images are
wanted for some other more general documentation or you want me to
create a specific diagram to document Bitcoin Core I'm happy to do so
as well.
Note that some phases can be done in different order or in parallel
(ie phase 3 and phase 4 could happen before phase 2, although I
strongly doubt it because phase 2 is the simplest to review and I've
been harassing different people to do it for a while with little
success [thanks to those who reviewed it and gave feedback] ).
An implementation of phase 2 (expose verifyHeader()) can be seen in

@_date: 2016-10-16 20:41:34
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
As has been mentioned there have been a lot of time to upgrade
software to support segwit. Furthermore, since it is a softfork, there
will be plenty of time after activation too for those taking a "wait
and see" approach.
You keep insisting on "2 months after activation", but that's not how
BIP9 works. We could at most change BIP9's initial date, but if those
who haven't started to work on supporting segwit will keep waiting for
activation, then changing the initial date won't be of any help to
them can only delay those who are ready and waiting.
The new features are not a requirement after activation. And although
it may take some time after activation for the new features to really
get to the users, that's just a fact of life that won't change by
changing the initial BIP9 date.
On Sun, Oct 16, 2016 at 8:20 PM, Tom Zander via bitcoin-dev

@_date: 2016-10-17 15:31:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
On Mon, Oct 17, 2016 at 1:17 PM, Tom Zander via bitcoin-dev
Please, nobody is asking for this.
Nobody should produce segwit transactions until the softfork is
activated, after which those transactions aren't anyone-can-spend
After activation, nobody can be forced to use the new format
immediately (or ever) if they don't want to reduce their tx fees.
Maybe because they want to be additionally cautious or maybe because
they haven't implemented the new features yet.
Either way, it is fine that some people upgrade later since, as
repeated by many, this is a backward compatible change.

@_date: 2017-04-01 14:33:18
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Segwit replaces the 1 mb size limit with a weight limit of 4 mb. After
segwit there's no need for MAX_BLOCK_BASE_SIZE anymore, let alone
Thus, by "hf to 2 mb" it seems you just really mean hardforking from 4
mb weight to 8 mb weight.
I would also use the hardfork bit (sign bit in block.nNersion) as matt comments.
If segwit is controversial the way it is (I still don't understand why
despite having insistently asking to users and miners who claim to
oppose it), adding more consensus rule changes won't make it any less
controversial. If anything, it would be removing consensus rule
changes, not adding them that could make it less controversial.
By no means I want to dissuade you from working on this bip proposal,
but I really don't see how it helps getting out of the deadlock at
On Sat, Apr 1, 2017 at 1:44 PM, Sergio Demian Lerner via bitcoin-dev

@_date: 2017-04-01 16:07:32
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
No, because of the way the weight is calculated, it is impossible to
create a block that old nodes would perceive as bigger than 1 mb
without also violating the weight limit.
After segwit activation, nodes supporting segwit don't need to
validate the 1 mb size limit anymore as long as they validate the
weight limit. The weight is also the only notion of cost miners need
to consider when comparing txs by feerate (fee per cost, before segwit
tx_fee/tx_size, post-segwit tx_fee/tx_weight).
This is important to remember, because having 2 separated limits or
costs would make block creation and relay policies much harder to
Therefore a hardfork after segwit can just increase the weight limit
and completely forget about the pre-segwit 1 mb size limit.

@_date: 2017-04-02 06:57:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
It's not odd, it's just counter-intuitive. How can "< 4 mb weight" be
a more restrictive rule than "< 1 mb size"? Well, it is, that's why
segwit's size increase is a softfork.
It is not that hard once you look at the actual weight formula:
segregated_sigs_sise + (other_size * 4) < 4 "mb"
It is impossible to produce to produce a block that violates the 1 mb
size limit but doesn't violate the 4 mb weight limit too.
There can be block that are < 1 mb size but 20 mb in weight, but those
are invalid according to the new 4 mb weight rule.
At the same time, any block that violates the < 1 mb rule for old
nodes will be invalid not only to old nodes but also to any node
validating the new 4mb rule. This is not by chance but a design choice
for any block size increase within segwit to remain a softfork, which
is what can be deployed faster.
One extreme example would be any 1 mb block today. 1 "mb" of a block
today times 4 is 4 mb, so it complies with the new 4 mb weight rule.
The opposite extreme example would be 4 mb of signatures and 0 mb of
"other data", but this example is not really possible in practice
because signatures need some tx to be part of to be part of the block
The most extreme examples I have seen on testnet are 3.7 mb blocks,
but those don't represent the average usage today (whenever you read
One common misunderstanding is that users who aren't using payment
channels (that includes lightning but also other smart contracts) or
users that aren't using mutlisig can't enjoy the so called "discount":
there's no reasonable argument for rejecting the "discount" on your
own transactions once/if segwit gets activated.
I would prefer to call the absence of "discount" *penalization*.
Signatures are unreasonable penalized pre-segwit, and there's more
things that remain unreasonably penalized with respect to their
influence on the current utxo after segwit. But signatures are by far
the biggest in data space and validation time, and the most important
unreasonable yet unintended penalization pre-segwit.
Exactly, once one maximum limit is defined, no need for two limits.
But the current max is 1 mb size, not 4 mb weight until/unless segwit
is activated.
Some people complain about 4 mb weight not being as much as 4 mb size,
and that is correct, but both are bigger than 1 mb size.
If the single ratio needs to be modified, it can be modified now
before any rule changes are activated, no need to change the consensus
rules more than needed.
If you don't see any disadvantage on having one single limit if/when
segwit gets activated, I don't see the point of maintaining two
limits, but if you're happy to maintain the branch with the redundant
one you may get my ack: I don't see any disadvantage on checking the
same thing twice besides performance,
That's precisely why it's good segwit has been designed to be backward
compatible as a bip9 softfork.

@_date: 2017-04-02 13:43:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Just saying that we can talk in terms of weight alone after segwit. 8 mb
weight is much more clear than 2 mb size to me. 2 mb size seems to
obfuscate the actual new limit with the proposed hf, which simply 8 mb

@_date: 2017-04-06 17:15:10
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
This is simply a non sequitur. These optimizations benefit users. On
the other hand, asicboost doesn't benefit users in any way, it only
benefits some miners if and only if not all miners use it. It
obviously harms the miners that aren't using it by making them less
profitable (maybe to the point that they lose money).
If all miners use it or if no one of them uses it is equivalent from
the point of view of the user. In fact, the very fact of allowing it
makes the network less secure unless every single honest miner uses
it, for an attacker could use it against the network.
Even if asicboost was good for users in any way (which as explained
isn't), this proposal doesn't disable it, only the covert form that
cannot be proven to be used.
Therefore there's no rational arguments to oppose this proposal unless
you are (or are invested in):
A) A Miner currently using the covert form of asicboost.
B) A Miner planning to use the covert form of asicboost soon.
C) An attacker using or planning to use the covert form of asicboost.
Asicboost doesn't seek this and doesn't help with this in any way.

@_date: 2017-04-06 19:51:37
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
On Thu, Apr 6, 2017 at 4:39 AM, Bram Cohen via bitcoin-dev
This is an interesting point.
If you have a precise description why it makes an incentive to make
blocks smaller I would love to read it.
Somebody asked and I didn't have an answer.
I imagine you try several reorderings sometimes excluding certain
branches of the merkle tree, permuting the branches you exclude or
something similar, but I really don't know the algorithm in detail and
I didn't want to say something inaccurate.

@_date: 2017-04-08 18:27:48
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
On 8 Apr 2017 5:06 am, "Jimmy Song via bitcoin-dev" <
Praxeology Guy,
Why would the actual end users of Bitcoin (the long term and short term
Certainly, if only one company made use of the extra nonce space, they
would have an advantage. But think of it this way, if some newer ASIC
optimization comes up, would you rather have a non-ASICBoosted hash rate to
defend with or an ASICBoosted hash rate? Certainly, the latter, being
higher will secure the Bitcoin network better against newer optimizations.

@_date: 2017-04-08 19:22:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
To be more specific, why "being higher will secure the Bitcoin network
better against newer optimizations"?
Or, to be more clear, let's forget about future "optimizations", let's
just think of an attacker. Does asicboost being used by all miners
make the system more secure against an attacker? No, for the attacker
can use asicboost too.
What about the case when not all the miners are using asicboost? Then
the attacker can actually get an advantage by suing asicboost.
Sometimes people compare asicboost with the use of asics in general as
both providing more security for the network and users. But I don't
think this is accurate. The existence of sha256d asics makes an attack
with general purpose computing hardware (or even more specialized
architectures like gpgpu) much more expensive and unlikely. As an
alternative the attacker can spend additional resources investing in
asics himself (again, making many attacks more expensive and
But as far as I know, asicboost can be implemented with software
running on general purpose hardware that integrates with regular
sha256d asics. There is probably an advantage on having the asicboost
implementation "in the same box" as the sha256d, yet again the
attacker can invest in hardware with the competitive advantage from
having asicboost more intergrated with the sha256d asics too.
To reiterate, whether all miners use asicboost or only a subset of
them, I remain unconvinced that provides any additional security to
the network (to be more precise whether that makes "tx history harder
to rewrite"), even if it results on the hashrate charts looking "more

@_date: 2017-04-09 13:46:22
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
On 8 Apr 2017 8:31 pm, "praxeology_guy via bitcoin-dev" <
There is the equation:
Power Cost + Captial Rent + Labor ~= block reward + fees
I don't know why many people insist on calling the subsidy the blick
reward. Thw block reward is both the block subsidy plus the block fees.

@_date: 2017-04-09 13:48:27
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
Why won't the attacker use asicboost too? (Please don't say because of

@_date: 2017-04-10 11:16:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
Why won't the attacker use asicboost too? (Please don't say because of
We're assuming the ASIC optimization in my example is incompatible with
ASICBoost. But if the new optimization were compatible with ASICBoost,
you're right, the network would be in an equivalent situation whether
ASICBoost was banned or not.
Only if all honest miners use asicboost, otherwise the situation for an
attack is not equivalent but worse with asicboost.
I want to point out again that overt ASICBoost can be used on the network
today. My proposal is to bring ASICBoost usage out into the open vs hiding
it. Banning ASICBoost via protocol changes is another issue completely.
Doesn't greg's proposal of disabling covert asicboost "bring asicboost
usage into the open vs hiding it" too? It also does it without making any
assumptions on whether we want to completely disable it later (I want)
while your proposal assumes we do not.

@_date: 2017-04-11 15:00:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
The discussion is going offtopic. Can we please take vague discussions
about changing pow, so called "asic resistance", the environment etc
to bitcoin-disscuss or some other forum?

@_date: 2017-04-11 23:25:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
On Tue, Apr 11, 2017 at 4:40 PM, Jimmy Song via bitcoin-dev
See bip2, specifically
"Following a discussion, the proposal should be submitted to the BIPs
git repository as a pull request. This draft must be written in BIP
style as described below, and named with an alias such as
"bip-johndoe-infinitebitcoins" until the editor has assigned it a BIP
number (authors MUST NOT self-assign BIP numbers)."
But I think it's kind of late to modify bip141, given that there's
code out there with the current specification.
I guess you can propose extensions or alternatives to replace it. I'm
really not sure what's the next step, but I don't think you have
provided enough motivation as to why we would want to maintain
asicboost. You said it makes the network more secure, but that's not
the case, as explained, not even if all honest miners use it.

@_date: 2017-08-27 13:33:04
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Solving the Scalability Problem Part II - Adam 
Regarding storage space, have you heard about pruning? Probably you should.
On 27 Aug 2017 12:27 am, "Adam Tamir Shem-Tov via bitcoin-dev" <

@_date: 2017-01-04 11:13:02
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Committed bloom filters for improved wallet 
There were talks about implementing spv mode for bitcoin core without using
bloom filters. Less efficient because it downloads full blocks, but better
for privacy. Perhaps other spv implementations should consider doing the
same instead of committing the filters in the block?
Now I feel I was missing something. I guess you can download the whole
block you're interested in instead of only your txs and that gives you
But how do you get to know which blocks are you interested in?
If the questions are too basic or offtopic for the thread, I'm happy
getting answers privately  (but then maybe I get them more than once).
On 4 Jan 2017 09:57, "Aaron Voisine via bitcoin-dev" <
It's easy enough to mark a transaction as "pending". People with bank
accounts are familiar with the concept.
Although the risk of accepting gossip information from multiple random
peers, in the case where the sender does not control the receivers network
is still minimal. Random node operators have no incentive to send fake
transactions, and would need to control all the nodes a client connects to,
and find a non-false-positive address belonging to the victims wallet.
It's not impossible, but it's non trivial, would only temporarily show a
pending transaction, and provide no benefit to the node operator. There are
much juicier targets for an attacker with the ability to sybil attack the
entire bitcoin p2p network.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-01-04 15:45:54
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Script Abuse Potential? 
I would assume that the controversial part of op_cat comes from the fact
that it enables covenants. Are there more concerns than that?
On 4 Jan 2017 04:14, "Russell O'Connor via bitcoin-dev" <

@_date: 2017-07-06 19:20:47
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Height based vs block time based thresholds 
I'm all for using height instead of time. That was my preference for
bip9 all along, but my arguments at the time apparently weren't
Regarding luke's proposal, the only advantage I see is that it would
allow nodes that don't know a deployment that gets activated to issue
a warning, like bip9 always does when an unknown deployment is locked
But there's a simpler way to do that which doesn't require to add
consensus rules as to what versionbits should be.
I'm honestly not worried about it being "coersive" and I don't think
it's inherently reckless (although used with short deployment times
like bip148 it can be IMO). But it adds more complexity to the
consensus rules, with something that could merely be "warning code".
You can just use a special bit in versionbits for nodes to get the warning.
My proposal doesn't guarantee that the warning will be signaled, for
example, if the miner that mines the block right after lock in doesn't
know about the deployment, he can't possibly know that he was supposed
to signal the warning bit, even if he has the best intentions. Miners
can also intentionally not signal it out of pure malice. But that's no
worse than the current form, when deployments activated by final date
instead of miner signaling never get a warning.
Shaolinfry had more concerns with my proposed modification, but I
think I answered all of them here:
The implementation of the proposal is there too. I'm happy to reopen
and rebase to simplify ( was merged and there's at least 1
commit to squash).
on an as-needed basis.
You can also do
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].bit = 0;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].nStartHeight = 500000;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].nTimeoutHeight = 510000;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].lockinontimeout = false;
and "if needed", simply add the following at any time (before the new
nStartHeight, obviously):
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].bit = 0;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].nStartHeight = 510000;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].nTimeoutHeight = 515000;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].lockinontimeout = true;
On Wed, Jul 5, 2017 at 9:44 PM, Hampus Sj?berg via bitcoin-dev

@_date: 2017-07-07 11:51:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Height based vs block time based thresholds 
What if you want height based but lockinontimeout = false ?
On 7 Jul 2017 8:09 am, "shaolinfry via bitcoin-dev" <

@_date: 2017-07-10 20:38:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Segwit2x BIP 
This is correct. If you are trying to imply that makes the short
timeline here right, you are falling for a "tu quoque" fallacy.
There's no logical reason I can think of (and I've heard many attempts
at explaining it) for miners to consider segwit bad for Bitcoin but
segwitx2 harmless. But I don't see 80% hashrate support for bip141, so
your claim doesn't seem accurate for the segwit part, let alone the
more controversial hardfork part.
I read some people controlling mining pools that control 80% of the
hashrate signed a paper saying they would "support segwit
immediately". Either what I read wasn't true, or the signed paper is
just a proof of the signing pool operators word being something we
cannot trust.
So where does this 80% figure come from? How can we trust the source?
It would be unfortunate to split the network into 2 coins only because
of lack of patience for deploying non-urgent consensus changes like a
size increase or disagreements about the right time schedule.
I think anything less than 1 year after release of tested code by some
implementation would be irresponsible for any hardfork, even a very
simple one.

@_date: 2017-07-12 19:38:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] A Segwit2x BIP 
On 12 Jul 2017 2:31 pm, "Tom Zander via bitcoin-dev" <
Good news!
Code to support 2x (the hard fork part of the proposal) has been out and
tested for much longer than that.
Not true. It's different code on top of segwit. The first attempt in btc1
(very recent) didn't even increased the size (because it changed the
meaningless "base size" without touching the weight limit. As for the
current code, I don't think it has been properly tested today, let alone
"for mucj longer than 1 year.
Anyway, I said, one year from tested release. Segwitx2 hasn't been
released, has it? If so, too late to discuss a bip imo, the bip may end up
being different from what has been released due to feedback (unless it is
ignored again, of course).
Tom Zander
Blog: Vlog: bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-06-11 15:17:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
The current proposal assumes that bip149 would only be merged and
released after nov15, so there's not time in one day.
My preference would be a bip149 proposal that could be merged and
released now, but some people complain that would require more
testing, because if you deploy bip149 and then sw gets activated pre
nov15, then you want bip149 nodes to use the old service bit for
segwit, not the new one (you would use that one if it activates post
nov15, so that pre-bip149 nodes don't get confused).
I was slowly modifying shaolinfry's code to try to code that, but I'm
currently not working on it because there doesn't seem there's a lot
of interest in releasing bip149 before nov15...
On Sun, Jun 11, 2017 at 7:48 AM, Ryan Grant via bitcoin-dev

@_date: 2017-06-11 16:29:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
Right, that would be part of it, as well as not removing the BIP141
deployment with bip9.
See No, if segwit activates pre nov15, bip149 nodse can detect and
interpret that just fine.
The problem if it activates post nov15, then you need a separate
service bit in the p2p network, for pre-BIP149 will think sw hasn't
activated while post-BIP149 would know it has activated.
If you release it only after nov15, you don't need to test
compatibility between the two for neither of this two cases.
Or do you? Actually you only save testing the easier case of pre-nov15

@_date: 2017-06-11 17:06:01
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The BIP148 chain split may be inevitable 
This is not needed, if segwit is locked in by aug 1 (with or without
bip91), no split will happen even if segwit is not active yet.
So the hashrate majority could avoid the split that way (or adopting bip148).
But it doesn't seem like they are planning to do this (with or without
bip91), the last thing I've heard, it's they will wait until
"immediately" before they signal sw (but there must be some language
barrier here, perhaps "immediately" and "inmediatamente" are false
friends). The reason why they will wait until "immediately" instead of
just starting to signal sw today, it's still unclear to me.
The other way to prevent the split is if bip148 users abort bip148
deployment, but unfortunately that seems increasingly unlikely.

@_date: 2017-06-11 19:11:53
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] The BIP148 chain split may be inevitable 
This still doesn't prevent the split if 45% or more of the hashrate
keeps blocking segwit, so I don't see how this help.
Miners could start signaling bit 1 today, before they use bip91 too
and signal bit 4 in addition.
But they aren't doing it, it seems they prefer to block segwit. I
don't see why changing using bit 4 or reducing the threshold would
change their mind.
Or you can replace this whole plan with the step 3, convincing miners
to stop blocking segwit, upgrade to segwit capable code if they
haven't already and signal bit 1 to activate it.
If you don't get that, there's going to be a split. Unless bip148 is
aborted in favor of bip149, which seems unlikely.
If we had 51%+ of the hashrate currently signaling segwit, I believe
there would be no problem convincing people to move from bip148 to
bip91, but we don't have that.
To me the lesson is not rushed deployments but bip8 and never commit
the mistake of giving miners the ability to block changes again, like
we did with csv and segwit, but using bip8 instead of bip9 from now

@_date: 2017-06-27 18:31:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
First the implementation, then the technical design (BIP)... will the
analysis come after that?
Will there be any kind of simulations of tje proposed size or will thag
come only after activation on mainnet?
I assume the very last step will be activation on testnet 3 ?
On 27 Jun 2017 8:44 am, "Sergio Demian Lerner via bitcoin-dev" <
Currently the only implementation that fulfills the requirements of the NYA
agreement is the segwit2x/btc1 implementation, which is being finalized
this week.
Segwit2mb does not fulfill the NYA agreement.
I'm asking now the segwit2x development team when a BIP will be ready so
that Core has the opportunity to evaluate the technical proposal.
On Wed, Jun 21, 2017 at 1:05 AM, Jacob Eliosoff via bitcoin-dev <
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-03-17 11:39:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Quadratic hashing solution for a post-segwit hard 
Segwit allows old -> old, old -> new, new -> old and of course new -> new
On 17 Mar 2017 1:47 a.m., "Erik Aronesty via bitcoin-dev" <
Yeah, it does make things harder, and it's easy enough to soft fork to
handle arbitrary opt-in protocol improvements, new much larger block sizes,
whatever you want.   Even OK to migrate to a new system by not allowing
old->old or new->old transactions.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-03-23 19:27:39
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Fraud proofs for block size/weight 
Great stuff, although the ordering of the sections seems a little bit confusing.
I think it would be clearer to put the "Creation of proofs" section
before "Proof verification", maybe even before "Proof format" if a
high level defintion of "full tx size proof" is provided before.
Also, in "For the full-size proof, each transaction should be assumed
to be at a minimum the stripped-size rather than the fixed 60 bytes."
it seems you are referring to a "full-size block proof" as opposed to
a "full size tx proof", perhaps a better term could be "full-weight
block proof" if what you are referring to is the proof of the weight
instead of only the pre-segwit size.
Perhaps some short definitions for "stripped-size proof", "full tx
size proof", "full-size proof" and maybe also "size component" at the
beginning would be enough.
In "Network protocol", "It should not recheck blocks known to be
valid, " does "known to be valid" include the blocks that the peer
told us where valid (with their hash and 0 in the enumerated varint)?
Those could be invalid too if the peer was lying, no?
Do you mean "It should not recheck blocks known to be invalid,"?
Why do you need to have at least one full tx size?
In Rationale you have:
Why must a full tx size proof be included?
This is necessary to establish that the claimed block transaction
count is correct.
Why do you need to establish that? If you can establish that the
number of transactions is at least N and that N * 60 bytes is greater
than the size/weight limit, isn't it that enough?
On Wed, Mar 22, 2017 at 10:51 PM, Matt Corallo via bitcoin-dev

@_date: 2017-03-29 11:37:08
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
While Segwit's change from 1 mb size limit to 4 mb weight limit seems to be
controversial among some users (I find that very often it is because they
have been confused about what segwit does or even outright lied about it) I
don't think it's very interesting to discuss further size increases.
I find more interesting to talk to the users and see how they think Segwit
harms them, maybe we missed something in segwit that needs to be removed
for segwit to become uncontroversial, or maybe it is just disinformation.
On the other hand, we may want to have our first uncontroversial hardfork
asap, independently of block size. For example, we could do something as
simple as fixing the timewarp attack as bip99 proposes. I cannot think of a
hf that is easier to implement or has less potential for controversy than
On 29 Mar 2017 8:32 am, "Bram Cohen via bitcoin-dev" <
On Tue, Mar 28, 2017 at 9:59 AM, Wang Chun via bitcoin-dev <
Much as it may be appealing to repeal the block size limit now with a grace
period until a replacement is needed in a repeal and replace strategy, it's
dubious to assume that an idea can be agreed upon later when it can't be
agreed upon now. Trying to put a time limit on it runs into the possibility
that you'll find that whatever reasons there were for not having general
agreement on a new setup before still apply, and running into the
embarrassing situation of winding up sticking with the status quo after
much sturm and drang.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-05-10 07:37:33
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
If there's a better factor than 0.25 I would change it now before deploying
segwit instead of leaving it to be changed later with a hf.
On 9 May 2017 10:59 pm, "Sergio Demian Lerner via bitcoin-dev" <

@_date: 2017-05-23 15:20:10
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
On Tue, May 23, 2017 at 2:55 PM, Luke Dashjr via bitcoin-dev
Well, it's putting users at more risk only if for those users who
actively decided to put themselves at risk.
I also feel bip148 is rushed and that makes it more risky. I don't
want to reiterate points other have made but I don't fully agree with
all of them.
I prefer the way it is over the way it was (just activating at a given
date without forcing mining signaling), but I still think it's rushed
and unnecessarily risky (unless activating segwit was urgent, which I
think it's not, no matter how much I want it to become active as soon
as possible).
On the other hand, I support uasf and bip8 to replace bip9 for future
deployments, since bip9 made assumptions that weren't correct (like
assuming miners would always signal changes that don't harm any user
and are good for some of them).
Perhaps bip149 can be modified to activate earlier if the current
proposal is perceived as unnecessarily cautious.
Luke, I've seen you say in other forums that "bip148 is less risky
than bip149", but I think that's clearly false.
As a reminder, one of my complains about bip109 was precisely that it
was also rushed in how fast it could activate.

@_date: 2017-05-25 21:53:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Suggested changes to bip8 
Hi, I didn't want to comment on
because it seemed to me that thread was more broad.
I like bip8 very much as an extension to bip9, but I think it could be better.
With bip9, a bip9-ready node that sees a softfork activated that he is
not aware of will see a warning. See the implementation:
But with bip8, if a deployment is made at the end of the period
instead of through 95% signaling, nodes that implement bip8 but don't
implement a certain deployment that is activated can't receive such a
The solution that comes to mind is to reserve one of the nVersion for
the specific purpose of requiring that the bit is active for one block
when a deployment is locked in in this way (or maybe also when it's
activated with miners' signaling too, maybe that can be used to
simplify the way the current warnings are checked).
I expect the code changes to do this to be simple, and I'm happy to
help with it.

@_date: 2017-05-30 15:27:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hypothetical 2 MB hardfork to follow BIP148 
Why not simply remove the (redundant after sw activation) 1 mb size
limit check and increasing the weight limit without changing the
discount or having 2 limits?
On Wed, May 24, 2017 at 1:07 AM, Erik Aronesty via bitcoin-dev

@_date: 2017-05-31 00:26:20
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hypothetical 2 MB hardfork to follow BIP148 
My understanding is that you cannot possibly violate the 1 MB block
size rule without also violating the 4 MB weight rule.
Regarding size alone, the only check we care about if we accept segwit is:
 [size4]
If that doesn't fail due to excessive non-witness data, then there's no way that
 [size1]
would have failed before due to excessive non-witness data.
If I understood it correctly when I was explained, if I remember
correctly, that last check is really just an optimization or a
protection against DoS invalid blocks. If the size without any witness
data is bigger than 1/4 the max_weight, then the max_weight check is
certain to fail as well without having to look at any witness data at
that validation stage (assuming the failure is due to excessive
non-witness data).
I think you are not referring to the 1 mb size limit but to related
one for sigops:
whose segwit parallel is in:
I believe the situation is similar in checking before knowing anything
about the witness data just in case that's already too much. In fact,
here is clearer because MAX_BLOCK_SIGOPS_COST is used for both (and
WITNESS_SCALE_FACTOR is used for the optimization case).
So what I would do in a hardfork after segwit activation would be to
simply equal MAX_BLOCK_BASE_SIZE=MAX_BLOCK_WEIGHT/WITNESS_SCALE_FACTOR
for size1, and increase MAX_BLOCK_WEIGHT and MAX_BLOCK_ SIGOPS_COST
proportionally for size4 and sigops4 respectively (well, the sigops
const for sigops1 as well).
If I understood segwit correctly, I believe that even though it is not
activated yet, you could remove both the size1 and sigops1 checks and
your node would still not accept invalid blocks by pre-bip141 rules,
your node would just spend more time on invalid blocks due to
currently excessive size/sigops, because it would only realize at a
later validation stage. Sorry for the redundancy about the validation
But it is not unlikely that I'm missing something. If I am wrong about
this I am spreading misinformation about segwit in several channels,
so I'm very interested in corrections to my statements in this mail.

@_date: 2017-05-31 03:22:44
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Hypothetical 2 MB hardfork to follow BIP148 
Why is it not enough at this point?
Why the need for a transaction size limit?

@_date: 2017-09-05 23:51:45
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
This is not a priority, not very important either.
Right now it is possible to create 0-value outputs that are spendable
and thus stay in the utxo (potentially forever). Requiring at least 1
satoshi per output doesn't really do much against a spam attack to the
utxo, but I think it would be slightly better than the current
Is there any reason or use case to keep allowing spendable outputs
with null amounts in them?
If not, I'm happy to create a BIP with its code, this should be simple.

@_date: 2017-09-09 23:11:57
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
Tier Nolan, right, a new tx version would be required.
I have to look deeper into the CT as sf proposal.
What futures upgrades could this conflict with it's precisely the
question here. So that vague statement without providing any example
it's not very valuable.
Although TXO commitments are interesting, I don't think they make UTXO
growth a "non-issue" and I also don't think they justify not doing
Yeah, the costs for spammers are very small and doesn't really improve
things all that much, as acknowledged in the initial post.

@_date: 2017-09-30 05:53:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
I really don't see how this "outlier behaviour" can be prevented. I think
it would be the norm even with your proposed "fix". Perhaps I'm missing
something too.
On 29 Sep 2017 5:24 pm, "Mark Friedenbach via bitcoin-dev" <

@_date: 2017-09-30 05:55:58
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
Gmaxwell I think what's new is that in this case, with a single tx you
would take out all txs with fee below 1 btc. With current rules, you would
only remove enoguh txs for that one to fit, not empty the whole block and
mine only a block with that single tx.

@_date: 2018-08-15 22:33:43
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Claiming an OP_RETURN Prefix 
op_return outputs can be pruned because they are not spendable.
putting a hash on in the witness script data won't make things better
(it would actually make them worse) and it definitely doesn't help
"block size bloat".
I think I'm missing some context, but if you're using op_return purely
for timestamping I would recommend using pay 2 contract  instead.
On Tue, Aug 14, 2018 at 8:34 PM, Christopher Allen via bitcoin-dev

@_date: 2018-08-22 15:48:16
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Getting around to fixing the timewarp attack. 
I only knew about ArtForz's fix, which isn't backwards compatible.
On Mon, Aug 20, 2018 at 10:14 PM, Gregory Maxwell via bitcoin-dev

@_date: 2018-03-28 14:55:26
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling? 
Yes, you can activate softforks at a given height.
I don't see any reason why you couldn't rebase to 0.16 directly.
The block version bumping was a mistake in bip34, you don't really
need to bump the version number. In any case, I would recommend
reading bip34 and what it activates in the code. IIRC the last thing
was bip65.
On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev

@_date: 2018-03-30 22:52:50
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling? 
Yes, in fact, you don't need to lose those bits like bitcoin by
imposing that the version is greater than that. But I guess just doing
the same is simpler.
On Thu, Mar 29, 2018 at 7:14 AM, Samad Sajanlal

@_date: 2018-05-10 09:33:29
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
I fail to see what's the practical difference between sending to op_true
and giving the coins are fees directly. Perhaps it is ao obvious to you
that you forget to mention it?
If you did I honestlt missed it.
On Wed, 9 May 2018, 01:58 Rusty Russell via bitcoin-dev, <

@_date: 2018-05-10 09:33:30
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
But in prnciple I don't oppose to making it stardard, just want to
understand what's the point.

@_date: 2018-10-04 01:53:07
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Bitcoin Core 0.17.0 released 
It seems we forgot
since getblockstats is only mentioned in the commits.
On Wed, Oct 3, 2018 at 12:32 PM Wladimir J. van der Laan via

@_date: 2019-05-23 21:03:09
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] OP_DIFFICULTY to enable difficulty hedges (bets) 
The complains I could imagine about this, (apart from being a very
specific use case) are the same complains I heard about op_expiry.
Namely, that in a reorg, the same tx, having been valid in a given
block could potentially become invalid in some other block mining it.
I guess in this case the situation is less likely in this case than
with op_expiry, but it is still possible.
Another complain I could imagine is this kind of forces the
implementation to break some existing encapsulations, but I guess
those are just implementation details not that relevant here.
I personally don't have strong feelings towards this proposal one way
or the other, I'm just imagining what other people may complain about.
On Thu, May 23, 2019 at 8:33 PM Tamas Blummer via bitcoin-dev

@_date: 2020-01-10 23:21:51
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
Well, bip9 doesn't only fall apart in case of unreasonable objection,
it also fails simply with miners' apathy.
Anyway, your proposed plan should take care of that case too, I think.
Overall sounds good to me.
Regarding bip8-like activation, luke-jr suggested that instead of
simply activating on date x if failed to do so by miners' signaling, a
consensus rule could require the blocks to signal for activation in
the last activation window.
I see 2 main advantages for this:
1) Outdated nodes can implement warnings (like in bip9) and they can
see those warnings even if it's activated in the last activation
window. Of course this can become counterproductive if miners' squat
signaling bits for asicboost again.
2) It is easier for users to actively resist a given change they
oppose. Instead of requiring signaling, their nodes can be set to
ignore chains that activate it. This will result in a fork, but if
different groups of users want different things, this is arguably the
best behaviour: a "clean" split.
I assume many people won't like this, but I really think we should
consider how users should ideally resist an unwanted change, even if
the proponents had the best intentions in mind, there may be
legitimate reasons to resist it that they may not have considered.
On Fri, Jan 10, 2020 at 10:30 PM Matt Corallo via bitcoin-dev

@_date: 2020-01-11 00:07:00
@_author: =?UTF-8?B?Sm9yZ2UgVGltw7Nu?= 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
I see how your approach doesn't lose goal 3 while "mine" does.
Regarding goal 4, I don't think any of the approaches loses it. "Use
hashpower enforcement to de-risk the upgrade process, wherever
Well, in the case of activation while there's "many" non upgrade
miners, they simply can't help to reduce upgrade risks unless they
upgrade. It doesn't matter if the activation is silent or with
mandatory signaling. Am I missing something?
That's not the goal at all. All my arguments have been focused on
users, not miners.
Well, it's not that you want to fork yourself off the network, is that
you don't want change X. Ideally change X wouldn't be activated, but
if it is, you prefer to be in a chain without change X.
Let's say we're using your system to deploy change X you oppose for
legitimate reasons.
What easier thing would you do as a user to resist change X with all
other users who also oppose it?
If there are simpler and better ways to do this, great. It's just
something to think about.
