
@_date: 2015-10-07 01:07:48
@_author: Ryan Grant 
@_subject: [bitcoin-dev] on rough consensus 
Bitcoin's participants can improve their ability to stay on a valuable
and censorship resistant blockchain by individually and informally
absorbing cultural wisdom regarding "rough consensus".  This does not
require writing any formal rules about what rough consensus is.  It is
a matter of participation with an understanding.
      In many ways, the IETF runs on the beliefs of its participants.
    One of the "founding beliefs" is embodied in an early quote about
    the IETF from David Clark: "We reject kings, presidents and
    voting.  We believe in rough consensus and running code".
A June 2015 bitcoin-dev thread, arguing about consensus, included the
usual range of responses; ranging from claims that any objection must
block consensus to a definition based on US Justice Stewart's "I'll
know it when I see it".  (It's funny because it's true.  We can
explain it better, though.)
  "Concerns Regarding Threats by a Developer to Remove Commit Access
  from Other Developers"
  An August 2015 cryptography-list thread presents the idea that rough
consensus can be used as a tool for hindering progress.  The specific
threat was that two protocol options could be made to seem equally
good.  To solve this example, identify that as the problem, then
engage a judgement to pick one solution "good enough" (but that does
not lead to a dead-end for other goals of the project), and go with
it.  There is room, within "rough consensus", for such action to
defend against the attack; as you can see from other excerpts in this
  "[Cryptography] asymmetric attacks on crypto-protocols - the rough
  consensus attack"
  To learn about forming a useful "rough consensus", see the very
readable "Tao of the IETF", and RFC 7282.
  "The Tao of the IETF"
      (previously RFC 4677)
  RFC 7282
  "On Consensus and Humming in the IETF"
  Strong objections don't block rough consensus:
      Rough consensus has been defined in many ways; a simple version is
    that it means that strongly held objections must be debated until
    most people are satisfied that these objections are wrong.
      Having full consensus, or unanimity, would be ideal, but we don't
    require it: Requiring full consensus allows a single intransigent
    person who simply keeps saying "No!" to stop the process cold.  We
    only require rough consensus: If the chair of a working group
    determines that a technical issue brought forward by an objector
    has been truly considered by the working group, and the working
    group has made an informed decision that the objection has been
    answered or is not enough of a technical problem to prevent moving
    forward, the chair can declare that there is rough consensus to go
    forward, the objection notwithstanding.
The working group chair's responsibility is different from that of
either a vote counter or a benign dictator:
      Note that 51% of the working group does not qualify as "rough
    consensus" and 99% is better than rough.  It is up to the Chair to
    determine if rough consensus has been reached.
      3.  Rough consensus is achieved when all issues are addressed, but
         not necessarily accommodated
      [...]
      If the chair finds, in their technical judgement, that the issue
      has truly been considered, and that the vast majority of the
      working group has come to the conclusion that the tradeoff is
      worth making, even in the face of continued objection from the
      person(s) who raised the issue, the chair can declare that the
      group has come to rough consensus.  (And even though this is
      framed in terms of a "vast majority", even that is not
      necessarily true.  This point is discussed in more detail in
      Sections 6 and 7.)
      [...]
      The chair of a working group who is about to find that there is
      only rough consensus is going to have to decide that not only
      has the working group taken the objection seriously, but that it
      has **fully examined the ramifications** of not making a change
      to accommodate it, and that the outcome does not constitute a
      failure to meet the technical requirements of the work.
      [...]
    6.  One hundred people for and five people against might not be
         rough consensus
      [...] one of the great strengths of using consensus over voting:
      It isn't possible to use "vote stuffing" (simply recruiting a
      large number of people to support a particular side, even people
      who have never participated in a working group or the IETF at
      all) to change the outcome of a consensus call.  As long as the
      chair is looking for outstanding technical objections and not
      counting heads, vote stuffing shouldn't affect the outcome of
      the consensus call.
    7.  Five people for and one hundred people against might still be
         rough consensus
      [...Sybil attack] it is within bounds for the chair to say, "We
      have objections, but the objections have been sufficiently
      answered, and the objectors seem uninterested in participating
      in the discussion.  Albeit rough in the extreme, there is rough
      consensus to go with the current solution."
      [...] it is likely that if a working group got this
      dysfunctional, it would put the whole concept of coming to rough
      consensus at risk.  But still, the correct outcome in this case
      is to look at the very weak signal against the huge background
      noise in order to find the rough consensus.
Working group chairs can help direct discussion:
      Sometimes discussions get stuck on contentious points and the
    chair may need to steer people toward productive interaction and
    then declare when rough consensus has been met and the discussion
    is over.
Some working groups segregate the role of forming a consensus from
communicating the consensus:
      Another method that some Working Groups adopt is to have a Working
    Group "secretary" to handle the juggling of the documents and the
    changes.  The secretary can run the issue tracker if there is one,
    or can simply be in charge of watching that all of the decisions
    that are made on the mailing list are reflected in newer versions
    of the documents.
Bitcoin Core is neither an IETF working group, nor should it aim to
curate its network protocol ruleset as one.  The IETF uses a steering
group, formal variance procedures, an appeals board, and a director
(to send even higher appeals to).  All of those positions could become
points of attack, if Bitcoin were to attempt to use or copy them.
That said, most IETF appeal routes are merely authorized to undo a
prior ruling of consensus, opening for reconsideration prior dismissed
points of argument (on their technical merits).  In Bitcoin, if
developers know what to work on, and can speak clearly enough to the
economic majority, then the system is working; regardless of whether
any role exists taking all the responsibility that an IETF working
group chair would take.
It is absolutely the case that resolving excessive roughness in shared
consensus takes more work than either votes or dictatorship.  It is
also the case that rough consensus is a good defense against
committing to decisions with subtle undesirable long-term effects.
That is why the IETF cares about it, and that same long-term threat is
important in Bitcoin's ecosystem as well.
  "The Tao of the IETF"
      A 2012 continuation of 2006's RFC 4677, itself first published in
    1994.
  BCP 25
      (1998)
    3.3. Session management
      Working groups make decisions through a "rough consensus"
      process.  IETF consensus does not require that all participants
      agree although this is, of course, preferred.  In general, the
      dominant view of the working group shall prevail.  (However, it
      must be noted that "dominance" is not to be determined on the
      basis of volume or persistence, but rather a more general sense
      of agreement.)  Consensus can be determined by a show of hands,
      humming, or any other means on which the WG agrees (by rough
      consensus, of course).  Note that 51% of the working group does
      not qualify as "rough consensus" and 99% is better than rough.
      It is up to the Chair to determine if rough consensus has been
      reached.
      In the case where a consensus, which has been reached during a
      face-to-face meeting, is being **verified on a mailing list**,
      the people who were in the meeting and expressed agreement must
      be taken into account.  If there were 100 people in a meeting
      and only a few people on the mailing list disagree with the
      consensus of the meeting then the consensus should be seen as
      being verified.  Note that enough time should be given to the
      verification process for the mailing list readers to understand
      and consider any objections that may be raised on the list.  The
      normal two week last-call period should be sufficient for this.
      [...]
      To facilitate making forward progress, a Working Group Chair may
      wish to decide to reject or defer the input from a member, based
      upon the following criteria:
        - Old
          The input pertains to a topic that already has been resolved
          and is redundant with information previously available;
        - Minor
          The input is new and pertains to a topic that has already
          been resolved, but it is felt to be of minor import to the
          existing decision;
        - Timing
          The input pertains to a topic that the working group has not
          yet opened for discussion; or
        - Scope
          The input is outside of the scope of the working group
          charter.
    [...]
  RFC 2026
  "The Internet Standards Process -- Revision 3"
      6.5 Conflict Resolution and Appeals
    [...]
  RFC 7282
  "On Consensus and Humming in the IETF"
      1.  Introduction
      [...] our credo is that we don't let a single individual dictate
      decisions (a king or president), nor should decisions be made by
      a vote, nor do we want decisions to be made in a vacuum without
      practical experience.  Instead, we strive to make our decisions
      by the consent of all participants, though allowing for some
      dissent (rough consensus), and to have the actual products of
      engineering (running code) trump theoretical designs.
      Having full consensus, or unanimity, would be ideal, but we
      don't require it: Requiring full consensus allows a single
      intransigent person who simply keeps saying "No!" to stop the
      process cold.  We only require rough consensus: If the chair of
      a working group determines that a technical issue brought
      forward by an objector has been truly considered by the working
      group, and the working group has made an informed decision that
      the objection has been answered or is not enough of a technical
      problem to prevent moving forward, the chair can declare that
      there is rough consensus to go forward, the objection
      notwithstanding.
    2.  Lack of disagreement is more important than agreement
      [...] **determining** consensus and **coming to** consensus are
      different things than **having** consensus [emphasis in
      original].
      [...]If at the end of the discussion some people have not gotten
      the choice that they prefer, but they have become convinced that
      the chosen solution is acceptable, albeit less appealing, they
      have still come to consensus.  Consensus doesn't require that
      everyone is happy and agrees that the chosen solution is the
      best one.  Consensus is when everyone is sufficiently satisfied
      with the chosen solution, such that they **no longer have
      specific objections** to it.
      [...] "Can anyone not live with choice A?" is more likely to
      only hear from folks who think that choice A is impossible to
      engineer given some constraints.  Following up with, "What are
      the reasons you object to choice A?" is also essential.
      [...]
      There is also an important point to be made about reaching
      consensus and "compromising": Unfortunately, the word
      "compromise" gets used in two different ways, and though one
      sort of compromising to come to consensus is good (and
      important), the other sort of compromising in order to achieve
      consensus can actually be harmful.  As mentioned earlier,
      engineering always involves balancing tradeoffs, and figuring
      out whether one engineering decision makes more sense on balance
      compared to another involves making engineering "compromises":
      We might have to compromise processor speed for lower power
      consumption, or compromise throughput for congestion resistance.
      Those sorts of compromises are among **engineering choices**,
      and they are **expected and essential**.  We always want to be
      weighing tradeoffs and collectively choosing the set that best
      meets the full set of requirements.
      However, there is another sense of "compromise" that involves
      compromising between people, not engineering principles.  For
      example, a minority of a group might object to a particular
      proposal, and even after discussion still think the proposal is
      deeply problematic, but decide that they don't have the energy
      to argue against it and say, "Forget it, do what you want".
      That surely can be called a compromise, but a chair might
      mistakenly take this to mean that they agree, and have therefore
      come to consensus.  But really all that they've done is
      capitulated; they've simply given up by trying to appease the
      others.  That's not coming to consensus; there still exists an
      outstanding unaddressed objection.  Again, if the objection is
      only that the choice is not ideal but is otherwise acceptable,
      such a compromise is fine.  But **conceding** when there is a
      real outstanding technical objection **is not coming to
      consensus**.
      [...]
      Coming to consensus is when everyone (including the person
      making the objection) comes to the conclusion that either the
      objections are valid, and therefore make a change to address the
      objection, or that the objection was not really a matter of
      importance, but **merely a matter of taste**.  Of course, coming
      to full consensus like that does not always happen.  That's why
      in the IETF, we talk about "rough consensus".
    3.  Rough consensus is achieved when all issues are addressed, but
not necessarily accommodated
      [...]
      If the chair finds, in their technical judgement, that the issue
      has truly been considered, and that the vast majority of the
      working group has come to the conclusion that the tradeoff is
      worth making, even in the face of continued objection from the
      person(s) who raised the issue, the chair can declare that the
      group has come to rough consensus.  (And even though this is
      framed in terms of a "vast majority", even that is not
      necessarily true.  This point is discussed in more detail in
      Sections 6 and 7.)
      [...]
      The chair of a working group who is about to find that there is
      only rough consensus is going to have to decide that not only
      has the working group taken the objection seriously, but that it
      has **fully examined the ramifications** of not making a change
      to accommodate it, and that the outcome does not constitute a
      failure to meet the technical requirements of the work.
      In order to do this, the chair will need to have a good idea of
      the purpose and architecture of the work being done, perhaps
      referring to the charter of the working group or a previously
      published requirements document, or even consulting with other
      experts on the topic, and then the chair will use **their own
      technical judgement** to make sure that the solution meets those
      requirements.  It is possible that the chair can come to the
      wrong conclusion, and the chair's conclusion is always
      appealable should that occur, but the chair must use their
      judgement in these cases.  What can't happen is that the chair
      bases their decision solely on hearing a large number of voices
      simply saying, "The objection isn't valid."  That would simply
      be to take a vote.  A **valid justification needs to me made**.
      [...] Indeed, RFC 2418 adds on to [old talk of balloting] by
      stating, "Note that 51% of the working group does not qualify as
      'rough consensus' and 99% is better than rough."  This document
      actually disagrees with the idea that simply balloting or
      otherwise looking at percentages can "determine" consensus.
      While counting heads might give a good guess as to what the
      rough consensus will be, doing so can allow important minority
      views to get lost in the noise.  One of the strengths of a
      consensus model is that minority views are addressed, and using
      a rough consensus model should not take away from that.  That is
      why this document talks a great deal about looking at open
      issues rather than just counting the number of people who do or
      do not support any given issue.  Doing so has some interesting
      and surprising implications that are discussed in subsequent
      sections.
      Any finding of rough consensus needs, at some level, to provide
      a **reasoned explanation** to the person(s) raising the issue of
      why their concern is not going to be accommodated.  A good
      outcome is for the objector to **understand the decision taken
      and accept the outcome**, even though their particular issue is
      not being accommodated in the final product.
      Remember, if the objector feels that the issue is so essential
      that it must be attended to, they always have the option to file
      an appeal.  A technical error is always a valid basis for an
      appeal. [...]
    4.  Humming should be the start of a conversation, not the end
      [...] a show of hands might leave the impression that the number
      of people matters in some formal way.
    5.  Consensus is the path, not the destination
      We don't try to reach consensus in the IETF as an end in itself.
      We use consensus-building as a tool to get to the best technical
      (and sometimes procedural) outcome when we make decisions.
      Experience has shown us that traditional voting leads to gaming
      of the system, "compromises" of the wrong sort as described
      earlier, important minority views being ignored, and, in the
      end, worse technical outcomes.
    6.  One hundred people for and five people against might not be
rough consensus
      [...] one of the great strengths of using consensus over voting:
      It isn't possible to use "vote stuffing" (simply recruiting a
      large number of people to support a particular side, even people
      who have never participated in a working group or the IETF at
      all) to change the outcome of a consensus call.  As long as the
      chair is looking for outstanding technical objections and not
      counting heads, vote stuffing shouldn't affect the outcome of
      the consensus call.
      [...]
      Even if no particular person is still standing up for an issue,
      that doesn't mean an issue can be ignored.  As discussed
      earlier, simple capitulation on an issue is not coming to
      consensus.  But even in a case where someone who is not an
      active participant, who might not care much about the fate of
      the work, raises a substantive issue and subsequently
      disappears, the issue needs to be addressed before the chair can
      claim that rough consensus exists.
    7.  Five people for and one hundred people against might still be
rough consensus
      [...Sybil attack] it is within bounds for the chair to say, "We
      have objections, but the objections have been sufficiently
      answered, and the objectors seem uninterested in participating
      in the discussion.  Albeit rough in the extreme, there is rough
      consensus to go with the current solution."
      [...] it is likely that if a working group got this
      dysfunctional, it would put the whole concept of coming to rough
      consensus at risk.  But still, the correct outcome in this case
      is to look at the very weak signal against the huge background
      noise in order to find the rough consensus.
    9.  Security Considerations
      "He who defends with love will be secure." -- Lao Tzu

@_date: 2016-02-02 02:35:07
@_author: Ryan Grant 
@_subject: [bitcoin-dev] BIP Process: Status, comments, 
For section "Formally defining consensus",
Where objections were not deemed substantiated by the community, clear
reasoning must be offered.
For section "BIP Comments",
Comments should be solicited on the bitcoin-dev mailing list, and
summarized fairly in the wiki; with notice of summarization and time
for suggesting edits on the mailing list.  Wiki registration and
monitoring should not be a required hurdle to participation.

@_date: 2016-02-04 13:45:38
@_author: Ryan Grant 
@_subject: [bitcoin-dev] BIP Process: Status, comments, 
On Thu, Feb 4, 2016 at 12:15 AM, Luke Dashjr via bitcoin-dev
These recent edits definitely guide us towards less hard feelings when
comments are offered, without excessive policy structure.
[BIP 2:]
Is this mix of wiki and mailing list intentional?  If so, the wiki
talk page is meant to be a self-curated permanent record of support
and dissent, but second-order reply commentary might fall either on
the wiki or the mailing list?
Mediawiki offers watchlists on a polling model, and there is some
email support [1], but it would be nice of a BIP author to at least
gather new/edited comment titles and report them to bitcoin-dev once a
week, during review.  Someone has to stare at the diffs.
  [1] BIP 2 should ask that all current and future forums that BIP authors
might choose for review have indisputable records of moderation and
user edits.
Is dump.bitcoin.it a sufficient public record of contentious
moderation or user cross-comment editing?  It seems like as long as
the wiki as a whole is verifiable, it would suffice.

@_date: 2016-02-04 20:09:09
@_author: Ryan Grant 
@_subject: [bitcoin-dev] BIP Process: Status, comments, 
Ahh, much better.  Thank you.
FWIW, this is the phrase that confused me:
[BIP 2:] If a BIP is not yet completed, reviewers should [...]

@_date: 2016-11-03 13:42:22
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Implementing Covenants with 
I know of a good business case that could benefit from two nice
As an example:
  Two parties have initiated a transaction designed with
  counterparty-minimization in mind.  It uses MAST and has many
  different payout distributions.  Both parties enter expecting to
  gain from the transaction, but both take on risk due to external
  factors.
  Because of the risks involved, there exist possible times when one
  party may wish to renegotiate the exit distribution, and might
  threaten to block any exit.  Or, either party might get hit by the
  proverbial bus.  During such times, the other party's eventual exit
  is protected by using a multisig which includes an oracle
  determination.  The oracle's trusted role is bound to this example's
  unstated "external factors" in a very limited sense, and does not
  include broader concerns, such as determining whether a party to the
  transaction is of "sound mind and body".
  The singular term "oracle" hides a set of entities participating in
  m-of-n multisig, which we can name the "oracle-set".
  Transaction terms include a CLTV lasting perhaps several years,
  applied whenever the exit requires the oracle-set's signatures.
  Both parties may mutually select and sign one of the payout
  distributions, to exit early.
The example, as I've described it so far, doesn't need anything other
than MAST.  It isn't a covenant, because it doesn't impose any forward
restrictions when satisfied; despite the contractual complications of
executing the oracle-set's signatures.  As covenant features are
considered across updated instances of what is otherwise a singular
transaction, it's important that none carry into the final payout
distribution, and that this is easy to verify.
Features desired:
  - One party would like to unilaterally sell their participation in
    the transaction, to a previously unknown recipient, before the
    CLTV becomes valid.
    The other originating party's stored MAST should either continue
    to function, or require minimal replacements that can be
    deterministically applied using data visible on the blockchain.
    It should not be necessary to ask permission from - or coordinate
    online communication with - the other originating party.
    (This can also be viewed as a key rotation problem for any
    long-lasting multisig transaction.)
  - Both parties would like to mutually revoke rouge oracle-entities
    from the oracle-set, without exposing each other to any possible
    renegotiation of other terms.
Note that these features affect each other, since if one party sells
their participation after any oracle-entities have been revoked, then
the revocations should not reset, but rather remain in effect, until a
proper payout executes the final agreement in the contract.
Of course, if there's a way to achieve these features with less risk
than evaluating covenant logic, I would very much like to hear how to
do so.

@_date: 2016-10-09 22:31:54
@_author: Ryan Grant 
@_subject: [bitcoin-dev] 1 Year bitcoin-dev Moderation Review 
Maybe bitcoin-discuss should have been opt-out rather than opt-in.
Dear moderators, what is the subscription count to bitcoin-discuss,
and bitcoin-dev?

@_date: 2017-04-07 08:55:57
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Draft BIP: Version bits extension with guaranteed 
The primary failure mode of a user's misconfiguration of nTimeout will
be a stopped chain.
If less-sophisticated users are offered these configuration settings
then chaintip progress failures that result from them should be
prominently displayed.

@_date: 2017-04-07 23:48:34
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Draft BIP: Version bits extension with guaranteed 
Praxeology Guy,
On Fri, Apr 7, 2017 at 12:56 PM, praxeology_guy
If our rule change timing is different from changes on the chain with
most work, then (extending Johnson Lau's terminology a bit) we may
experience subjective hardfork-ness; due to miners creating blocks
which the economic majority goes on to accept, though they have a less
restrictive ruleset than ours.
Correct for the segwit soft fork, which is narrowing the definition
of a nonstandard transaction.  It's safe to say that if a block with a
tx violating cleanstack were to occur on a non-segwit chain, that it
was for malicious reasons.
However, some future forks - that a full node experiences as
low subjective hardfork-ness (i.e. soft forks) - might restrict
more common things.
Sure, a nice-to-have would be a SetfLargeWorkInvalidChainFound() that
was aware as well, though clients can make these decisions themselves.

@_date: 2017-04-14 15:12:34
@_author: Ryan Grant 
@_subject: [bitcoin-dev] extended BIP9 activation of segwit, for legacy nodes 
Segwit has proven more contentious to activate than anticipated
(although my read has long been that the technical consensus is clear,
despite noisy objections).  No matter which method is used to
eventually activate segwit, or on what timeline, it would be
beneficial if validating nodes already capable of supporting segwit
could, without further upgrades, eventually participate to their
fullest capacity.
BIP9 assignments should reserve a backward compatibility bit which all
yet-unknown segwit-compatible proposals may utilize.  These future
proposals must be consensus compatible with BIPs 141, 143, & 147,
except that they may use different deployment logic.
The motivation is so that any validating node software released after
this BIP9 assignment can eventually understand if segwit is activated
by alternate means, even when the node is itself a legacy version.
This is important because the realities of system administration on
the Bitcoin network are that upgrades occur slowly (which is inherent
in the security choice of not presenting an auto-upgrade feature).
Even though segwit in particular is backwards compatible with old
validating nodes, there are still distinct advantages to validating
and generating segregated witness transactions.
For example, future BIP9-compatible deployment attempts might
additionally include a date-dependent UASF fallback.  If, either
during or after activation, deployment rules also require signaling
for segwit using the backwards-compatible bit here proposed, then
(after 95% of recent blocks signal for the alternate segwit
deployment) more legacy nodes would understand and validate
transactions using segregated witnesses.
An expiration time of five years seems conservative:
  // Alternate Deployment 1 of SegWit (BIP141, BIP143, and BIP147)
  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].bit = 2;
  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nStartTime
= 1510704000; // November 15th, 2017.
  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nTimeout =
1668470400; // November 15th, 2022.
Segwit deployment logic would then look like:
  bool IsWitnessEnabled(const CBlockIndex* pindexPrev,
                        const Consensus::Params& params)
  {
      LOCK(cs_main);
      return    (VersionBitsState(pindexPrev,
                                  params,
                                  Consensus::DEPLOYMENT_SEGWIT,
                                  versionbitscache)
                 == THRESHOLD_ACTIVE)
 (VersionBitsState(pindexPrev,
                                  params,
                                  Consensus::DEPLOYMENT_SEGWIT_ALT1,
                                  versionbitscache)
                 == THRESHOLD_ACTIVE);
  }

@_date: 2017-04-15 09:54:00
@_author: Ryan Grant 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
A proposal from yesterday would separate this concern; though not
retroactively.  One way to name this proposal would be "Catch-All
Segwit Activation".
  "extended BIP9 activation of segwit, for legacy nodes"
  If this release valve exists, then discussions (such as this thread)
can get back to focusing on finding the safest incentive-compatible
transitions, with time improving the situation instead of making it worse.

@_date: 2017-06-11 01:48:39
@_author: Ryan Grant 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
Is there any reason that BIP149 activation on November 16th would
cause a problem?

@_date: 2017-06-11 15:31:00
@_author: Ryan Grant 
@_subject: [bitcoin-dev] extended BIP9 activation of segwit, 
This[1] idea from April would assist in a BIP149-like segwit
activation on November 16th.
Its goal is to be incredibly easy to test and deploy, right now, even
before a decision on revisions to BIP149 is made, and well before such
"BIP149ish" testing is itself complete.
UASFs don't need time for most legacy nodes to upgrade - that's the
point of a soft fork.  UASFs simply need to have inevitability,
which is provided by some nodes more than others.  But for the node
less instrumental in that inevitability, and more relaxed about
scheduling upgrade work, being moved to a miner-protected consensus
ruleset is not as desirable a position as the opportunity to
participate fully.  As a courtesy, the plan for soft forks has
always been to allow legacy nodes time to upgrade to full
participation.  How much time should rollouts allow for this
Extended BIP9 activation of segwit (for legacy nodes) separates
concerns between intending to activate segwit and its method of
deployment, allowing "semi-legacy" nodes that have upgraded to
include this proposal to participate immediately in a successful
segwit activation, without needing any courtesy time to upgrade to
the particular deployment logic.
Code for deployment is included in the original email[1].  There's
nothing missing from the logic shown.  The whole intent of the
proposal is that other deployment specifics are left to be defined
by future proposals.  In the same block that THRESHOLD_ACTIVE is
reached for segwit, require Consensus::DEPLOYMENT_SEGWIT_ALT1 to
also reach THRESHOLD_ACTIVE, and the burden of the future proposal
is fulfilled.
If the idea proves broken or of no benefit, when actually
implementing and testing future deployments, then we can avoid using
DEPLOYMENT_SEGWIT_ALT1 until it expires, and zero nodes get hurt.
Let's look at how this affects options for how to deploy segwit...
  - BIP149ish debated (handles BIP141 success, unlike BIP149)
  - BIP149ish tested
  - BIP149ish courtesy timeout debated
  - BIP148    fails
  - BIP149ish released
  - BIP141    fails
  - BIP149ish courtesy timeout expires
  - BIP149ish activates
  - segwit activates
  - legacy nodes must upgrade before recognizing BIP149 activation
If we try to restrict ourselves to only the original service bit, we
see a tension between activating soon and leaving legacy nodes on a
miner-protected consensus.  We also see a tension between *planning*
how long it will take to debate and test UASF logic, and setting
expectations for when a reasonable activation date is.
These seemingly small tensions complicate the solution, and push it
out of developer's visions of what is feasible.  It's not clear how
soon it can happen, so it doesn't get started in an urgent manner.
It doesn't get started in an urgent manner, so it's not clear how
soon it can happen.
  - this proposal debated
  - this proposal tested
  - this proposal released (courtesy begins)
  - BIP149ish     debated  (handles BIP141 success, unlike BIP149)
  - BIP149ish     tested
  - BIP148        fails
  - BIP149ish     released
  - BIP141        fails
  - BIP149ish     activates
  - segwit activates
  - semi-legacy nodes *immediately* use segwit, via this proposal
We can remove the courtesy timeout problem, because the courtesy
begins as soon as this proposal goes live.  This simplifies debate on
how BIP149ish should deploy, and helps make reasonable a much quicker
segwit activation should BIP141 fail.
With a clear route to quick activation for semi-legacy nodes, the
UASF can be planned for activation in as short a window as the key
nodes can upgrade.  Not even all the key nodes need to upgrade: it's
still a soft fork, and UASFs simply need to have inevitability.
These differences can help us aim for activating segwit on November
16th, if BIP141 and BIP148 do not succeed earlier.  Since BIP149 as
originally conceived is slow as molasses, BIP149ish still needs
debate, BIP141 has steadfast enemies, and the community is slow to
adapt to BIP148's complicated commitment requirements, it is prudent
to take this intermediate step allowing quicker BIP149ish activation.
[1]

@_date: 2017-05-18 15:28:38
@_author: Ryan Grant 
@_subject: [bitcoin-dev] 
On Thu, May 18, 2017 at 9:44 AM, Cameron Garnham via bitcoin-dev
On Thu, May 18, 2017 at 10:59 AM, Tier Nolan via bitcoin-dev
One principled way to proceed would be to fault not the exploit, but
the protocol design.
Bits in the block header have been discovered which could be used for
dual meanings, and at least one meaning does not preserve the
incentive balances intended and assumed by others.  This unexpectedly
creates an incentive to block protocol improvements.  The protocol
must be repaired.
In this view, which focuses on covert-ASICBOOST, how work is done is
up to the implementation.  But if the hashing work specified possibly
could gain from blocking development work, then we have a
I believe this is clear grounds for taking action without any delay.

@_date: 2018-08-16 17:32:25
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Claiming an OP_RETURN Prefix 
The hash of the file is deterministic and `ipfs add` tells us what it
is whether the network is connected or disconnected.  We don't upload
files to IPFS until the transaction has settled with several

@_date: 2018-02-05 10:56:23
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
Am I reading correctly that this allows unilateral key rotation (to a
previously unknown key), without invalidating the interests of other
parties in the existing multisig (or even requiring any on-chain
transaction), at the cost of storing the signed delegation?

@_date: 2018-02-22 12:19:36
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
Tagging this thread with "nExpiryTime".  Search archives for more.

@_date: 2018-01-02 06:31:51
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Decoupling BIP70 Payment Protocol from Wallets 
As a reminder, there is a W3C Payments API, currently proceeding along
the W3C Recommendation track, which registers "payment handlers" in
the browser, and selects one to complete a transaction:
  The purpose of the payments API is to automate all data entry and
handle choices related to common transactions on the Web.  Payment
requests will often ask for information that Bitcoin wallets have no
current need to provide, such as a shipping address.  If shipping
options or other personally identifying information (such as an email
address and a return payment address) are involved, then it is the
chosen payment type's *handler* that is tasked with negotiating with
the user how to reveal the supposedly necessary information.
  Although it may seem early for wallet makers to consider integration
with a mere W3C Recommendation, it would not be early to choose the
right architecture to build code on, given that this is in the works
for the major browsers.  Development can proceed even in browsers that
have not implemented anything, through an HTML5 Javascript polyfill.
A demonstration which includes payment in bitcoins is already
available, although it leaves as an exercise for the reader exactly
how the txid would be made known to the handler (whether manually
input by paste buffer after copying from an external app, or returned
through IPC):
    that would preclude the workflow of a Bitcoin transaction, whether
on-chain (with the seller's backend marking off confirmations) or
using the Lightning Network.  It even allows the seller to offer a
discount on certain payment methods:

@_date: 2020-12-08 17:39:23
@_author: Ryan Grant 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
It looks like a good strategy for a bech32 library that is external to
Bitcoin Core would be:
  - Default to the new M, under the same bech32 brand.
  - Provide an interface to explicitly use both M=1 and M=0x2bc830a3.
  - If decoding fails, throw an error; but in constructing that error
    inform whether the other M would have succeeded.
  - Provide an interface for a BIP173 implementation to peek at the
    witness version byte of the data part, which may also involve
    sanity-checking that byte for errors using a BIP173-specific
    understanding of the appropriate checksum.
    Return values for this special interface might currently be:
      "it's version zero, based on a clean decoding",
      "it's version one,  based on a clean decoding",
      "it's version zero, based on an auto-corrected byte",
      "it's version one,  based on an auto-corrected byte",
      "no result, due to a decoding error on this byte", and
      "too many errors to say anything more about decoding".
Although the reasoning is clear for doing so, looking into the data
that is supposed to be checksummed to determine which checksum to use
is not very elegant.  There are two trips into a bech32 library for a
BIP173 decoding, and an indeterminate result on the version byte would
require heuristics for deciding what to do with the rest of the data
part to even advise the user on the error.  Because of this, as a
library writer I would be tempted to auto-correct the witness version
byte (against the "SHOULD NOT" advice of BIP173's current version), if
it were the only one corrupted, as per the example return values
above.  Please advise.
Some of the libraries that will be contemplating these steps include:
  Here are three existing uses of bech32 that are external to Bitcoin Core:
      Of the above, I think BIP136 can be unconditionally moved to
M=0x2bc830a3 due to having little legacy burden.
