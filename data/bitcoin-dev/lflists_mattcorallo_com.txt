
@_date: 2015-08-05 19:27:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
See-also: Bitcoinrelaynetwork.org. It's already in use my the majority of large miners, is publicly available to anyone, and the protocol is rather simple and the client could be tweaked easily to keep exactly it's block ready to quickly relay to the nearest server (ie only have to relay the header, the coinbase transaction, and only small other data... Experience shows this is really easy to fit into one packet on the wire). It's not nearly as complicated as your suggestion, but may still marginally favor well-connected miners, but hopefully not much (when you're taking about single packets, it should all be latency, and the servers are well distributed). If you feel so inclined, there are some todos to make it really meet is efficiency limits filled on github.com/TheBlueMatt/RelayNode, feel free to rewrite the protocol if you really want :).

@_date: 2015-08-06 20:38:41
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
No, don't think so, the protocol is, essentially, relay transactions, when you get a block, send header, iterate over transactions, for each, either use two bytes for nth-recent-transaction-relayed, use 0xffff-3-byte-length-transaction-data. There are quite a few implementation details, and lots of things could be improved, but that is pretty much how it works.

@_date: 2015-08-06 20:50:32
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
No, just one queue, but it has a count-of-oversize-txn-limit, in addition to a size.
Except it doesn't really work :( (see Patches welcome :) (read the issues list first... Rewriting the protocol from scratch is by far not the biggest win here).

@_date: 2015-08-06 20:55:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
The version check was only added hours after the initial fork, so it should have (assuming BTC Nuggets or anyone who accepted it is running a client)
The reason other miners mined on that fork is because they were watching each other's stratum servers, so the relay network should not have had a significant effect. Still, even in a different fork, miners already aggressively relay around the network/between each other, so I'm not so worried.

@_date: 2015-08-13 16:36:07
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I think all such competing proposals are dropped because this seemed like a better idea. Feel free to revive one/come up with one, though.

@_date: 2015-08-14 18:53:59
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
My concern is how the hell do you automate this? Having a threat of
"well, everyone could update their software to a new version which will
destroy all coins right now" is kinda useless, and trying to come up
with a reasonable set of metrics as to how much and when you move from
just paying the fee to destroying coins is really hard, especially if
you assume the attacker is a miner with, say, enough hashrate (maybe
rented) to get one or three blocks in the next day (the timeout period).
Now users are coordinating quickly in an attack scenario?
Yea, implementation is really tricky here. I do not at all think we
should be thinking about implementing this any time soon, and should
assume Lightning will have to stand reasonably on its own without it
first, and only if it gains a lot of traction will there be enough
motivation for making such a change at the Bitcoin protocol level for
Doesnt that defeat the purpose of Lightning?
I'm not even sure if sufficient coordination is a sufficient solution.
If you assume a hub just shut down, and everyone is trying to flush to
the chain, with a backlog of a few days worth of transactions (with
timeouts of a day or so), and users are even paying huge fees (99% of
what they'd get back), if the former-hub is a miner, it can claim that
last 1% of many of the transactions that take longer than a day to confirm.

@_date: 2015-08-17 21:42:29
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Bitcoin Core doesnt have to do this. It is rational if you have >25% of
hash power (or if you believe 25% of hash power is doing this) to do this.
If a 75% hardfork target is reached, and >25% of the hashpower doesnt
allow the hardfork, and the hardfork is strictly more permissive than
the original (ie it is essentially a reverse softfork - there are no
previously valid blocks which are not still valid), then the miners who
voted for the fork would be constantly generating blocks which are
soft-forked-out by the >25% and non-supporting miners.
I believe this was pointed out to the Bitcoin XT folks weeks ago, but
apparently did not sway the decision to use 75% and a (as far as I can
tell?) strictly more permissive hardfork.

@_date: 2015-08-19 12:36:33
@_author: Matt Corallo 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
Wait, why did Bitcoin-XT use that nVersion???
Definitely option 3 is much cleaner technically, and it would be nice to have that code implemented, but I'd be rather concerned about the size of the fork ballooning. It's already four separate features in one fork, which seems pretty big, even if the code isn't too huge. I'd probably prefer slightly to waste another two version bits than add a bunch of code to the fork now (maybe we can take just a part of the bit-based approach and allow lower version numbers again after the fork reaches 95%?). Either way, a proper version of the bit-based soft forking mechanism should be implemented sooner rather than later (maybe merged unused, even).
Still, it is entirely possible that there is relatively little uptake on XT and a competing proposal has a lot of support before we want to ship a LTV fork, so it may all be moot (or we could choose option 1 to fix the XT fork for them - easily forking off XT miners when either fork happens so XT isn't vulnerable to the 25% attack without needing to mine a massive transaction on Bitcoin first :p).

@_date: 2015-08-20 17:44:28
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
I dont think a libconsensus would have any kind of networking layer, nor
is C++ an antique tool set (hopefully libconsensus can avoid a boost
dependency, though thats not antique either). Ideally it would have a
simple API to give it blocks and a simple API for it to inform you of
what the current chain is. If you really want to get fancy maybe it has
pluggable block storage, too, but I dont see why you couldnt use this in
~any client?

@_date: 2015-08-20 21:35:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
I'm not suggesting pluggable networking, I'm suggesting (and I think
everyone thinks the design should be) NO networking. The API is
ValidationResult libconsensus.HeyIFoundABlock(Block) and
ListOfBlocksToDownloadNext libconsensus.HeyIFoundAHeaderList(ListOfHeaders).
Are you suggesting to support altcoins? I dont think anyone cares about
supporting that.
I think you'd be very pleasantly surprised. It sounds like you havent
dug into Bitcoin Core validation code in years.
Hmm? The result would be an obviously correct consensus implementation
that everyone could use, instead of everyone going off and writing their
own and either being wrong, or never updating in the case of forks. Its
a huge deal to allow people to focus on making their libraries have good
APIs/Wallets/etc instead of focusing on making a working validation
engine (though maybe for that the p2p layer needs to also be in a library).
We have one, it just needs a few already obvious performance improvements.
There are a number of good development tools for C++ that allow this....

@_date: 2015-08-21 04:46:17
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Peter: Since I stole most of this text from your old BIP, should I leave
you as an author?
BIP: ?
Title: NODE_BLOOM service bit
Author: Matt Corallo , Peter Todd Type: Standards Track (draft)
Created: 20-08-2015
This BIP extends BIP 37, Connection Bloom filtering, by defining a
service bit to allow peers to advertise that they support bloom filters
explicitly. It also bumps the protocol version to allow peers to
identify old nodes which allow bloom filtering of the connection despite
lacking the new service bit.
BIP 37 did not specify a service bit for the bloom filter service, thus
implicitly assuming that all nodes that serve peers data support it.
However, the connection filtering algorithm proposed in BIP 37, and
implemented in several clients today, has been shown to provide little
to no privacy, as well as being a large DoS risk on some nodes. Thus,
allowing node operators to disable connection bloom filtering is a
much-needed feature.
The following protocol bit is added:
    NODE_BLOOM = (1 << 2)
Nodes which support bloom filters should set that protocol bit.
Otherwise it should remain unset. In addition the protocol version is
increased from 70002 to 70011 in the reference implementation. It is
often the case that nodes which have a protocol version smaller than
70011, but larger than 70000 support bloom filtered connections without
the NODE_BLOOM bit set, however clients which require bloom filtered
connections should avoid making this assumption.
NODE_BLOOM is distinct from NODE_NETWORK, and it is legal to advertise
NODE_BLOOM but not NODE_NETWORK (eg for nodes running in pruned mode
which, nonetheless, provide filtered access to the data which they do have).
If a node does not support bloom filters but receives a "filterload",
"filteradd", or "filterclear" message from a peer the node should
disconnect that peer immediately. For backwards compatibility, in
initial implementations, nodes may choose to only disconnect nodes which
have the new protocol version set and attempt to send a filter command.
While outside the scope of this BIP it is suggested that DNS seeds and
other peer discovery mechanisms support the ability to specify the
services required; current implementations simply check only that
NODE_NETWORK is set.
Design rational
A service bit was chosen as applying a bloom filter is a service.
The increase in protocol version is for backwards compatibility. In
initial implementations, old nodes which are not yet aware of NODE_BLOOM
and use a protocol version < 70011 may still send filter* messages to a
node without NODE_BLOOM. This feature may be removed after there are
sufficient NODE_BLOOM nodes available and SPV clients have upgraded,
allowing node operators to fully close the bloom-related DoS vectors.
Reference Implementation
This document is placed in the public domain.

@_date: 2015-08-21 17:53:34
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
The proposal will not break any existing clients in the first release.
After sufficient time to upgrade SPV clients, a new version will be
released which will result in older SPV clients finding themselves
disconnected from peers when they send filter* commands, so they can go
find other peers which do support bloom filtering.

@_date: 2015-08-21 17:55:58
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Revised copy follows. re: mentioning the HTTP seeding stuff, I'm not
sure we want to encourage more people aside from bitcoinj to use
that...I thought about adding a DNS seed section to this bip, but
decided against it...still, I think we should add the option to select
service bits to DNS seeds ASAP.
Re: need to "shard" the blockchain: not sure what you're referring to
here. The bloom filter stuff requires you to download the chain
in-order, sure, but you have to do that for headers anyway, and
hopefully your total data isnt too much more than headers alone.
Anyone have the best reference for the DoS issues?
BIP: ?
Title: NODE_BLOOM service bit
Author: Matt Corallo , Peter Todd Type: Standards Track (draft)
Created: 20-08-2015
This BIP extends BIP 37, Connection Bloom filtering, by defining a
service bit to allow peers to advertise that they support bloom filters
explicitly. It also bumps the protocol version to allow peers to
identify old nodes which allow bloom filtering of the connection despite
lacking the new service bit.
BIP 37 did not specify a service bit for the bloom filter service, thus
implicitly assuming that all nodes that serve peers data support it.
However, the connection filtering algorithm proposed in BIP 37, and
implemented in several clients today, has been shown to provide little
to no privacy[1], as well as being a large DoS risk on some nodes[2].
Thus, allowing node operators to disable connection bloom filtering is a
much-needed feature.
The following protocol bit is added:
    NODE_BLOOM = (1 << 2)
Nodes which support bloom filters should set that protocol bit.
Otherwise it should remain unset. In addition the protocol version is
increased from 70002 to 70011 in the reference implementation. It is
often the case that nodes which have a protocol version smaller than
70011, but larger than 70000 support bloom filtered connections without
the NODE_BLOOM bit set, however clients which require bloom filtered
connections should avoid making this assumption.
NODE_BLOOM is distinct from NODE_NETWORK, and it is legal to advertise
NODE_BLOOM but not NODE_NETWORK (eg for nodes running in pruned mode
which, nonetheless, provide filtered access to the data which they do have).
If a node does not support bloom filters but receives a "filterload",
"filteradd", or "filterclear" message from a peer the node should
disconnect that peer immediately. For backwards compatibility, in
initial implementations, nodes may choose to only disconnect nodes which
have the new protocol version set and attempt to send a filter command.
While outside the scope of this BIP it is suggested that DNS seeds and
other peer discovery mechanisms support the ability to specify the
services required; current implementations simply check only that
NODE_NETWORK is set.
Design rational
A service bit was chosen as applying a bloom filter is a service.
The increase in protocol version is for backwards compatibility. In
initial implementations, old nodes which are not yet aware of NODE_BLOOM
and use a protocol version < 70011 may still send filter* messages to a
node without NODE_BLOOM. This feature may be removed after there are
sufficient NODE_BLOOM nodes available and SPV clients have upgraded,
allowing node operators to fully close the bloom-related DoS vectors.
Reference Implementation
This document is placed in the public domain.
[1] [2] ???? is one example where the issues were found, though others
independently discovered issues as well. Sample DoS exploit code
available at

@_date: 2015-08-21 19:22:39
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Proposed new policy for transactions that depend 
I dont see any problem with such limits. Though, hell, if you limited
entire tx dependency trees (ie transactions and all required unconfirmed
transactions for them) to something like 10 txn, maximum two levels
deep, I also wouldnt have a problem.

@_date: 2015-08-22 01:08:13
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Had a discussion on IRC and with Pieter, and I kinda agree that the more
optimal way is for DNS seeds to, instead of returning NODE_NETWORK
nodes, return any node which responds to getaddr, allowing clients to
connect to a few DNS seeds by name, do a getaddr, then disconnect (like
Bitcoin Core does now if you're using Tor). They can then select the
peers they want based on nServices.
Meh, whatever, justification is already provided well enough without
having to go into "but if we did this long into the future"  arguments.
Ehh, I was going more for the oldest mention.

@_date: 2015-08-24 17:37:51
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Its more of a statement of "in the future, we expect things to happen
which would make this an interesting thing to do, so we state here that
it is not against spec to do so". Could reword it as "NODE_BLOOM is
distinct from NODE_NETWORK, and it is legal to advertise NODE_BLOOM but
not NODE_NETWORK (though there is little reason to do so now, some
proposals may make this more useful in the future)"?

@_date: 2015-08-24 17:39:12
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
I'll just quote what I said on github:
Neither this pull nor the BIP has any stated intention of phasing out
bloom filtering support in the protocol. As much as I'd love to, I 100%
agree with  here, that would break any ability of SPV clients
to operate on the P2P network (either as a way to double-check
centralized servers, or otherwise), and that is really not a good idea
without a replacement in place. This pull/BIP DOES suggest we phase out
REQUIRED bloom filtering support in the protocol - thereby fixing the
peer selection of SPV clients in the face of btcd with some flags/many
patched versions of Core/etc peers, providing a remedy for a potential
DoS attack, etc.

@_date: 2015-08-24 18:07:05
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
BIP 111 was assigned, pull request (with the proposed changes) available
at

@_date: 2015-08-24 18:28:59
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
I'm not sure what your reasoning for this is? If your concern is that
someone starts DoS attacking you with bloom-based attacks, you should
just disconnect them as an attacker, and announce that you support bloom
filtering globally. If you want to serve your own nodes, then I dont
think this BIP doesnt allow you to do so, just needs an implementation.
Trustless (and non-privacy-losing) proposals welcome :)

@_date: 2015-08-29 21:07:58
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
I believe it was pointed out previously in the discussion of the Peter R
paper, but I'll repeat it here so that its visible - this seems to
ignore the effects of transaction validation caches and block
compression protocols. Many large miners already have their own network
to relay blocks around the globe with only a few bytes on the wire at
block-time, and there is also the bitcoinrelaynetwork.org network, which
does the same for smaller miners, albeit with slightly less efficiency.
Also, transaction validation time upon receiving a block can be rather
easily made negligible (ie the only validation time you should have is
the DB modify-utxo-set time). Thus, the increased orphan risk for
including a transaction can be reduced to a very, very tiny amount,
making the optimal blocksize, essentially, including everything that
you're confident is in the mempool of other reasonably large miners.

@_date: 2015-08-30 02:33:38
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
It is not a purely academic scenario that blocks contain effectively no
information (that was not previously relayed). I'm not aware of any
public code to do so, but I know several large miners who pre-relay the
block(s) they are working on to other nodes of theirs around the globe.
This means at announce-time you have only a few bytes to broadcast (way
less than a packet, and effects of using smaller packets to relay things
vs larger packets are very small, if anything). After you've broadcast
to all of your nodes, hops to other mining nodes are probably only a
handful of ms away with very low packet loss, so relay time is no longer
connected to transaction inclusion at all (unless you're talking about
multi-GB blocks). Of course, this is relay time for large miners who can
invest time and money to build such systems. Small miners are completely
screwed in such a system.
Thus, the orphan risk for including a transaction is related to the
validation time (which is only DB modify-utxo-set time, essentially,
which maybe you can optimize much of that away, too, and only have to
pass over mempool or so). Anyway, my point, really, is that though
miners will have an incentive to not include transactions which will
trigger validation by other nodes (ie things not already in their
mempool), the incentive to not include transactions which have already
been relayed around sufficiently is, while not theoretically zero, as
near to zero in practice as you can get.

@_date: 2015-08-30 02:35:10
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
Of course this assumes the network does not change any as a result of
such a system. But such a system provides strong incentives for the
network to centralize in other ways (put all the mining nodes in one DC
for all miners, etc).

@_date: 2015-08-30 03:56:47
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
You missed one key point - information transmitted across networks is
not slowed down on a per-bit basis, its (largely) slowed down on a
per-packet basis! You have some 1400+ bytes to work with before you
notice the increase in information in relay cost. Of course this only
applies if you are talking about long-haul links (across oceans, between
countries, etc), if you're talking about short links it is a per-byte
cost, but your time is probably negligible for anything but VERY large
Very fast is an understatement. My point was that it is literally always
a single packet, and a very small packet, at that.
I think you missed it, this mail was not about the relay network. Let's
ignore the relay network entirely for now. Several miners have nodes
well distributed all over the globe which peer either directly with
other miners in the area, or just connect to a ton of nodes, probably
hitting other miners. The cost to transmit from their pool server to
their relay server is one packet, always. They pre-relay everything
they're gonna include, so, at max, they might have to pick between a few
pre-relayed blocks (what, like, 2/5 bits?). From there, you assume they
are transmitting to a node in the same DC, or at least very close (say,
local within AWS-US-East/maybe between AWS-US-East and somewhere in
NYC), where transmitting even 100MB is really fast.

@_date: 2015-12-01 05:28:42
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks 
I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea. If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much. The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks. Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).
All that said, I'd love a proposal that allows clients to download compressed blocks via an external daemon, especially during IBD. This could help people with very restrictive data caps do IBD instead of being pushed to revert to SPV. Additionally, I think we need more chain sync protocols so that the current P2P protocol isn't consensus-critical anymore.

@_date: 2015-12-02 22:23:47
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and 
My issue is more that its additional complexity and attack surface, and for a very minor gain which should disappear with further optimization elsewhere and less that we absolutely shouldn't add compression because we're definitely gonna have issues.

@_date: 2015-12-04 13:30:33
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and 
Ok. It wasn't clear to me that you weren't also claiming at latency reduction as a result. In any case, the point I was making is that the p2p protocol isn't for every use-case. Indeed, I agree (as noted previously) that we should support people who have very restrictive data usage limits, but I don't think we need to do this in the p2p protocol. Considering we're in desperate need of more ways to sync, supporting syncing over slow and/or very restrictive connections is something maybe better addressed by a sync-over-http-via-cdn protocol than the p2p protocol.
My point is that, with limited further optimization, and especially after the first hundred thousand blocks, block download should nearly never be the thing limiting IBD speed.
No matter how easily you can implement something, complexity always has cost. This is especially true in complicated, incredibly security critical applications exposed to the internet.

@_date: 2015-12-16 20:50:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
A large part of your argument is that SW will take longer to deploy than a hard fork, but I completely disagree. Though I do not agree with some people claiming we can deploy SW significantly faster than a hard fork, once the code is ready (probably a six month affair) we can get it deployed very quickly. It's true the ecosystem may take some time to upgrade, but I see that as a feature, not a bug - we can build up some fee pressure with an immediate release valve available for people to use if they want to pay fewer fees.
On the other hand, a hard fork, while simpler for the ecosystem to upgrade to, is a  1-2 year affair (after the code is shipped, so at least 1.5-2.5 from today if we all put off heads down and work). One thing that has concerned me greatly through this whole debate is how quickly people seem to think we can roll out a hard fork. Go look at the distribution of node versions on the network today and work backwards to get nearly every node upgraded... Even with a year between fork-version-release and fork-activation, we'd still kill a bunch of nodes and instead of reducing their security model, lead them to be outright robbed.

@_date: 2015-12-16 22:29:39
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
We should probably start by defining "economically important". To me, it's pretty clear that every, or at least around 99% of, " economically important" node have upgraded by the time the fork kicks in, with way more than sufficient time given to everyone to upgrade (minding that this is not an emergency situation and that people have lives and many Bitcoin services are hobby projects and upgrading isn't always as easy as just restarting your node). I'd define "economically important" as any node that is used for anything more than simply "being a node" (ie people who started a node to provide resources to the network, and only using their node for that). Note, of course, that we should avoid breaking all such "non-economically important" nodes, but breaking many of them is not a big deal. Note that my proposal includes nodes such as the one doing transaction selection for the relay network. Though it is not used for payments, if it is not upgraded, things will break.
With this definition in mind, I think a year is an aggressive timeline.

@_date: 2015-12-16 22:32:57
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
As for "the ecosystem waiting around for laggards", yes, it is absolutely the ecosystems y responsibility to not take actions that will result in people losing money without providing them far more than enough opportunity to fix it. One of the absolute most important features of Bitcoin is that, if you're running a full node, you are provided reasonable security against accepting invalid transactions.

@_date: 2015-12-20 03:36:10
@_author: Matt Corallo 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Peter was referring to pool-block-withholding, not selfish mining.

@_date: 2015-07-24 02:48:03
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Node Speed Test 
You may see much better throughput if you run a few servers around the
globe and test based on closest-by-geoip. TCP throughput is rather
significantly effected by latency, though I'm not really sure what you
should be testing here, ideally.

@_date: 2015-11-12 19:47:50
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Upcoming Transaction Priority Changes 
On the IRC meeting today there was a long discussion on how to handle
the old "transaction priority" stuff in 0.12. Over time the "transaction
priority" stuff has added a huge amount of code and taken a bunch of
otherwise-useful developer effort. There is still some debate about its
usefulness going forward, but there was general agreement that it will
either be removed entirely or replaced with something a bit less costly
to maintain some time around 0.13.
With the mempool limiting stuff already in git master, high-priority
relay is disabled when mempools are full. In addition, there was
agreement to take the following steps for 0.12:
 * Mining code will use starting priority for ease of implementation
 * Default block priority size will be 0
 * Wallet will no longer create 0-fee transactions when mempool limiting
is in effect.
What this means for you is, essentially, be more careful when relying on
priority to mine your transactions. If mempools are full, your
transactions will be increasingly less likely to be relayed and more
miners may start disabling high-priority block space. Make sure you
analyze previous blocks to determine if high-priority mining is still
enabled and ensure your transactions will be relayed.

@_date: 2015-11-14 04:11:01
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core 0.11.2 released 
Heh, though mine doesnt since I mangled the line breaks...that should
have been...
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Strange, the signature validates for me. If I copy the entire mail
(including the signature and PGP armor, but not the ads) I see the hash
of the mail as

@_date: 2015-11-26 22:23:38
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
Actually, with this argument I think CHECKSEQUENCEVERIFY is more appropriate. To an app developer, you're enforcing maturity by enforcing sequence. I think it's much more clear to app devs to say sequence here since it makes explicit how to create the transaction which passes the check, whereas saying maturity night be confusing.

@_date: 2015-10-08 03:33:07
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Proposed new policy for transactions that depend 
There is a PR up for this change at  which is getting some discussion for those following along.

@_date: 2015-10-14 18:57:08
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
That conversation missed a second issue. Namely that there is no way to punish people if there is a double spend in a micro block that happens in key block which reorg'd away the first transaction. eg one miner mines a transaction in a micro block, another miner (either by not having seen the first yet, or being malicious - potentially the same miner) mines a key block which reorgs away the first micro block and then, in their first micro block, mines a double spend. This can happen at any time, so you end up having to fall back to regular full blocks for confirmation times :(.
Also, Greg Slepak brought up a good point on twitter at  Noting that this model means users could no longer pick transactions in a mining pool which was set up in such a way (it could be tweaked to do so with separate rewards and pubkeys, but now the user can commit fraud at a much lower cost - their own pool reward, not the block's total reward).

@_date: 2015-10-15 01:59:24
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
Huh? No... This is not a Bitcoin Core issue, it is a Bitcoin protocol one and should be discussed here, not on github.
I really appreciate Ittay and Emin's efforts in this space and their willingness to work with the Bitcoin community on it! It seems it still needs some tuning, but seems like if the pool-mining issues were resolved it could make block relay times irrelevant, at least.

@_date: 2015-10-28 02:08:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
Oops, just realized I never responded to this...
I think you're overstating how short the wait times can be. They need to
be much longer than the network propagation delay.
The attacker does not need to withold their keyblock at all. All the
attacker does is, for every transaction they ever send, after it is
included in a microblock, set their hashpower to start mining a keyblock
immediately prior to this microblock. When they find a keyblock, they
immediately announce it and start creating microblocks, the first of
which double-spends the previous transaction. If they dont win the key
block, oh well, their payment went through normally and they couldn't
In chatting with Glenn about this, we roughly agreed that the
confirmation time for microblocks possibly doesn't need to be a full
key-block, but it needs to be a reasonable period after which such an
attacker would lose more in fees than the value of their double-spend
(ie because the key-block afterwards gets 20% more in fees than the
key-block before hand). In any case, the game theory here starts to get
rather complicated and it doesn't make me want to suggest accepting
microblocks as confirmations is safe.
It is not a practical issue today because no one does it, but it is a
massive issue in that the splitting of pool rewards and transaction
selection is one of the few easy wins we have left in the fight against
mining centralization. Mining centralization today is absolutely awful,
and closing off our only big win would be tragic.

@_date: 2015-09-16 21:51:57
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
I only have one "correction", included inline.
I would be careful to point out that hard numbers were deliberately NOT
discussed. Though some general things were thrown out, they were not
extensively discussed nor agreed to. I personally think 2-4 is "in
range", though 8 maybe not so much. Of course it depends on exactly how
the non-blocksize limit accounting/adjusting is done.
Still, the "greatest common denominator" agreement did not seem to be
agreeing to an increase which continues over time, but which instead
limits itself to a set, smooth increase for X time and then requires a
second hardfork if there is agreement on a need for more blocksize at
that point.

@_date: 2015-09-18 20:06:23
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
I did not intend to imply that there was agreement on a desire to
schedule a second hardfork. My wording may have been a bit too loose.
Instead, I believe there was much agreement that doing a short-term
hardfork now, with many agreeing that a second would hopefully be
entirely unnecessary/impossible, while others thought that a second
would be necessary and would have to happen. While this may set up a
similar controversy again in several years, I think everyone agreed that
we cannot predict the future and I, personally, think none of us should
be committing to a viewpoint for what should be done at that time.
Personally, I think it is also critical that there be no messaging that
people should rely on or assume there will be a future increase after a
short-term bump (which I also do not believe people should be relying on

@_date: 2015-09-18 20:11:33
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Hash of UTXO set as consensus-critical 
I believe the discussion here is on improving initial-sync time by
simply skipping initial-sync and getting a committed-to utxo set. This
is obviously a new security model in between SPV and full-node (I would
call it SPV with future validation). Still, I'm not convinced it buys us
anything, we really should just tweak Bitcoin Core to do spv mode at
startup and validate backwards in the background. I think this would
alleviate most of the concerns raised, given the chain growth is not
entirely unreasonable going forward.

@_date: 2015-09-18 20:14:29
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
Generally in favor, but for practical purposes can we select a timezone
that is available in Google Calendar? It appears it does not directly
support UTC...

@_date: 2015-09-18 20:27:00
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
Google Calendar is localized, but has an option to change the timezone
of an event, it just doesnt have UTC in its options. So, yes, we should
use something that observes DST in roughly the same way as everyone else
- CEST/PDT/EST/etc.

@_date: 2015-09-18 20:36:41
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
Yes, I'm aware, however they are closer to each other than UTC is to either :p.

@_date: 2015-09-18 20:34:43
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
I believe that is out of date. I see neither UTC nor GMT on the website nor on Android.

@_date: 2016-12-18 10:34:43
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Planned Obsolescence 
One thing which hasn't been addressed yet in this thread is developer centralization. Unlike other applications we want to ensure that it's not only possible for users to refuse an upgrade, but easy. While this by no means lessens the retirement that users run up to date software for security reasons, finding the right line to draw is difficult.

@_date: 2016-12-19 02:22:21
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Planned Obsolescence 
Please do report bugs to  . If you never report them of course they won't get fixed. I'm not aware of test suite failures and know a bunch of folks who use CentOS, though not sure how many develop on it.

@_date: 2016-02-08 19:26:48
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Hi all,
I believe we, today, have a unique opportunity to begin to close the
book on the short-term scaling debate.
First a little background. The scaling debate that has been gripping the
Bitcoin community for the past half year has taken an interesting turn
in 2016. Until recently, there have been two distinct camps - one
proposing a significant change to the consensus-enforced block size
limit to allow for more on-blockchain transactions and the other
opposing such a change, suggesting instead that scaling be obtained by
adding more flexible systems on top of the blockchain. At this point,
however, the entire Bitcoin community seems to have unified around a
single vision - roughly 2MB of transactions per block, whether via
Segregated Witness or via a hard fork, is something that can be both
technically supported and which adds more headroom before second-layer
technologies must be in place. Additionally, it seems that the vast
majority of the community agrees that segregated witness should be
implemented in the near future and that hard forks will be a necessity
at some point, and I don't believe it should be controversial that, as
we have never done a hard fork before, gaining experience by working
towards a hard fork now is a good idea.
With the apparent agreement in the community, it is incredibly
disheartening that there is still so much strife, creating a toxic
environment in which developers are not able to work, companies are
worried about their future ability to easily move Bitcoins, and
investors are losing confidence. The way I see it, this broad
unification of visions across all parts of the community places the
burden of selecting the most technically-sound way to achieve that
vision squarely on the development community.
Sadly, the strife is furthered by the huge risks involved in a hard fork
in the presence of strife, creating a toxic cycle which prevents a safe
hard fork. While there has been talk of doing an "emergency hardfork" as
an option, and while I do believe this is possible, it is not something
that will be easy, especially for something as controversial as rising
fees. Given that we have never done a hard fork before, being very
careful and deliberate in doing so is critical, and the technical
community working together to plan for all of the things that might go
wrong is key to not destroying significant value.
As such, I'd like to ask everyone involved to take this opportunity to
"reset", forgive past aggressions, and return the technical debates to
technical forums (ie here, IRC, etc).
As what a hard fork should look like in the context of segwit has never
(!) been discussed in any serious sense, I'd like to kick off such a
discussion with a (somewhat) specific proposal.
First some design notes:
* I think a key design feature should be taking this opportunity to add
small increases in decentralization pressure, where possible.
* Due to the several non-linear validation time issues in transaction
validation which are fixed by SegWit's signature-hashing changes, I
strongly believe any hard fork proposal which changes the block size
should rely on SegWit's existence.
* As with any hard fork proposal, its easy to end up pulling in hundreds
of small fixes for any number of protocol annoyances. In order to avoid
doing this, we should try hard to stick with a few simple changes.
Here is a proposed outline (to activate only after SegWit and with the
currently-proposed version of SegWit):
1) The segregated witness discount is changed from 75% to 50%. The block
size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
maximum block size of 3MB and a "network-upgraded" block size of roughly
2.1MB. This still significantly discounts script data which is kept out
of the UTXO set, while keeping the maximum-sized block limited.
2) In order to prevent significant blowups in the cost to validate
pessimistic blocks, we must place additional limits on the size of many
non-segwit transactions. scriptPubKeys are now limited to 100 bytes in
size and may not contain OP_CODESEPARATOR, scriptSigs must be push-only
(ie no non-push opcodes), and transactions are only allowed to contain
up to 20 non-segwit inputs. Together these limits limit
total-bytes-hashed in block validation to under 200MB without any
possibility of making existing outputs unspendable and without adding
additional per-block limits which make transaction-selection-for-mining
difficult in the face of attacks or non-standard transactions. Though
200MB of hashing (roughly 2 seconds of hash-time on my high-end
workstation) is pretty strongly centralizing, limiting transactions to
fewer than 20 inputs seems arbitrarily low.
Along similar lines, we may wish to switch MAX_BLOCK_SIGOPS from
1-per-50-bytes across the entire block to a per-transaction limit which
is slightly looser (though not too much looser - even with libsecp256k1
1-per-50-bytes represents 2 seconds of single-threaded validation in
just sigops on my high-end workstation).
3) Move SegWit's generic commitments from an OP_RETURN output to a
second branch in the merkle tree. Depending on the timeline this may be
something to skip - once there is tooling for dealing with the extra
OP_RETURN output as a generic commitment, the small efficiency gain for
applications checking the witness of only one transaction or checking a
non-segwit commitment may not be worth it.
4) Instead of requiring the first four bytes of the previous block hash
field be 0s, we allow them to contain any value. This allows Bitcoin
mining hardware to reduce the required logic, making it easier to
produce competitive hardware [1].
I'll deliberately leave discussion of activation method out of this
proposal. Both jl2012 and Luke-Jr recently begun some discussions about
methods for activation on this list, and I'd love to see those continue.
If folks think a hard fork should go ahead without SPV clients having a
say, we could table  or activate  a year or two after 1-3 activate.
[1] Simpler here may not be entirely true. There is potential for
optimization if you brute force the SHA256 midstate, but if nothing
else, this will prevent there being a strong incentive to use the
version field as nonce space. This may need more investigation, as we
may wish to just set the minimum difficulty higher so that we can add
more than 4 nonce-bytes.
Obviously we cannot reasonably move forward with a hard fork as long as
the contention in the community continues. Still, I'm confident
continuing to work towards SegWit as a 2MB-ish soft-fork in the short
term with some plans on what a hard fork should look like if we can form
broad consensus can go a long way to resolving much of the contention
we've seen.

@_date: 2016-02-09 21:54:01
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Thanks for keeping on-topic, replying to the proposal, and being civil!
Replies inline.
It's intended to activate when we have clear and broad consensus around
a hard proposal across the community.
The goal isnt really to get a "gain" here...its mostly to decrease the
worst-case (4MB is pretty crazy) and keep the total size in-line with
what the network could handle. Getting 1MB blocks through the network in
under a second is already incredibly difficult...2MB is pretty scary and
will take lots of work...3MB is over the bound of "yea, we can pretty
for sure get that to work pretty well".
Hmmmmmm...you make a valid point. I was trying to avoid breaking old
transactions, but didnt think too much about time-locked ones.
Hmmmmmm...we could apply the limits to txn that dont have at least one
"newer than the fork input", but I'm not sure I like that either.
There is a clear goal here of NOT using block-based limits and switching
to transaction-based limits. By switching to transaction-based limits we
avoid nasty issues with mining code either getting complicated or
enforcing too-strict limits on individual transactions.

@_date: 2016-02-09 22:00:44
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Oops, forgot to reply to your last point.
Indeed, we could push for more place by just always having one 0-byte,
but I'm not sure the added complexity helps anything? ASICs can never be
designed which use more extra-nonce-space than what they can reasonably
assume will always be available, so we might as well just set the
maximum number of bytes and let ASIC designers know exactly what they
have available. Currently blocks start with at least 8 0-bytes. We could
just say minimum difficulty is now 6 0-bytes (2**16x harder) and reserve
those? Anyway, someone needs to take a closer look at the midstate
optimization stuff to see what is reasonable required.

@_date: 2016-02-09 22:15:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] A roadmap to a better header format and bigger 
As for your stages idea, I generally like the idea (and mentioned it may
be a good idea in my proposal), but am worried about scheduling two
hard-forks at once....Lets do our first hard-fork first with the things
we think we will need anytime in the visible future that we have
reasonable designs for now, and talk about a second one after we've seen
what did/didnt blow up with the first one.
Anyway, this generally seems reasonable - it looks like most of this
matches up with what I said more specifically in my mail yesterday, with
the addition of timewarp fixes, which we should probably add, and Luke's
header changes, which I need to spend some more time thinking about.

@_date: 2016-02-09 22:39:34
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Did you read the footnote on my original email? There is some potential
for optimization if you can brute-force the midstate, which today
requires using the nVersion space as nonce. In order to fix this we need
to add nonce space in the first compression function, so this is an
ideal place. Even ignoring that reducing complexity of mining control
stuff is really nice. If we could go back to just providing block
headers to miners instead of having to provide the entire
transaction-hash-list we could move a ton of complexity back into
Bitcoin Core from mining setups, which have historically been pretty
poorly-reviewed codebases.
Meh, my point was less that its a really bad idea and more that it adds
compexity that I dont see much need for.

@_date: 2016-01-07 19:13:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
We absolutely should be worried about 80-bit collision resistance.
Collisions only take 2**80 work if the hash is theoretically perfect,
which is never the case, not to mention that collision resistance is
almost always the first thing to go for hash functions, and often starts
to get easier slowly long, long before anyone is truly worried about the
security of the hash function.
I would never assume RIPEMD160's collision resistance is 2**80, and
would definitely never wager a significant amount of money that this
remains true for, say, five years.

@_date: 2016-01-08 01:26:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
So just because other attacks are possible we should weaken the crypto
we use? You may feel comfortable weakening crypto used to protect a few
billion dollars of other peoples' money, but I dont.

@_date: 2016-01-08 03:41:34
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision 
Indeed, anything which uses P2SH is obviously vulnerable if there is an attack on RIPEMD160 which reduces it's security only marginally. While no one thought hard about these attacks when P2SH was designed, we realized later this was not such a good idea to reuse the structure from P2PKH. Hence why this discussion came up.

@_date: 2016-05-02 22:13:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Hi all,
The following is a BIP-formatted design spec for compact block relay
designed to limit on wire bytes during block relay. You can find the
latest version of this document at
There are several TODO items left on the document as indicated.
Additionally, the implementation linked at the bottom of the document
has a few remaining TODO items as well:
 * Only request compact-block-announcement from one or two peers at a
time, as the spec requires.
 * Request new blocks using MSG_CMPCT_BLOCK where appropriate.
 * Fill prefilledtxn with more than just the coinbase, as noted by the
spec, up to 10K in transactions.
Luke (CC'd): Can you assign a BIP number?
  BIP: TODO
  Title: Compact block relay
  Author: Matt Corallo   Status: Draft
  Type: Standards Track
  Created: 2016-04-27
Compact blocks on the wire as a way to save bandwidth for nodes on the
P2P network.
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
Historically, the Bitcoin P2P protocol has not been very bandwidth
efficient for block relay. Every transaction in a block is included when
relayed, even though a large number of the transactions in a given block
are already available to nodes before the block is relayed. This causes
moderate inbound bandwidth spikes for nodes when receiving blocks, but
can cause very significant outbound bandwidth spikes for some nodes
which receive a block before their peers. When such spikes occur, buffer
bloat can make consumer-grade internet connections temporarily unusable,
and can delay the relay of blocks to remote peers who may choose to wait
instead of redundantly requesting the same block from other, less
congested, peers.
Thus, decreasing the bandwidth used during block relay is very useful
for many individuals running nodes.
While the goal of this work is explicitly not to reduce block transfer
latency, it does, as a side effect reduce block transfer latencies in
some rather significant ways. Additionally, this work forms a foundation
for future work explicitly targeting low-latency block transfer.
===Intended Protocol Flow===
TODO: Diagrams
The protocol is intended to be used in two ways, depending on the peers
and bandwidth available, as discussed [[
The "high-bandwidth" mode, which nodes may only enable for a few of
their peers, is enabled by setting the first boolean to 1 in a
"sendcmpct" message. In this mode, peers send new block announcements
with the short transaction IDs already, possibly even before fully
validating the block. In some cases no further round-trip is needed, and
the receiver can reconstruct the block and process it as usual
immediately. When some transactions were not available from local
sources (ie mempool), a getblocktxn/blocktxn roundtrip is neccessary,
bringing the best-case latency to the same 1.5*RTT minimum time that
nodes take today, though with significantly less bandwidth usage.
The "low-bandwidth" mode is enabled by setting the first boolean to 0 in
a "sendcmpct" message. In this mode, peers send new block announcements
with the usual inv/headers announcements (as per BIP130, and after fully
validating the block). The receiving peer may then request the block
using a MSG_CMPCT_BLOCK getdata reqeuest, which will receive a response
of the header and short transaction IDs. In some cases no further
round-trip is needed, and the receiver can reconstruct the block and
process it as usual, taking the same 1.5*RTT minimum time that nodes
take today, though with significantly less bandwidth usage. When some
transactions were not available from local sources (ie mempool), a
getblocktxn/blocktxn roundtrip is neccessary, bringing the best-case
latency to 2.5*RTT, again with significantly less bandwidth usage than
today. Because TCP often exhibits worse transfer latency for larger data
sizes (as a multiple of RTT), total latency is expected to be reduced
even when full the 2.5*RTT transfer mechanism is used.
===New data structures===
Several new data structures are added to the P2P network to relay
compact blocks: PrefilledTransaction, HeaderAndShortIDs,
BlockTransactionsRequest, and BlockTransactions. Additionally, we
introduce a new variable-length integer encoding for use in these data
For the purposes of this section, CompactSize refers to the
variable-length integer encoding used across the existing P2P protocol
to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.
====New VarInt====
TODO: I just copied this out of the src...Something that is
wiki-formatted and more descriptive should be used here isntead.
Variable-length integers: bytes are a MSB base-128 encoding of the number.
The high bit in each byte signifies whether another digit follows. To make
sure the encoding is one-to-one, one is subtracted from all but the last
Thus, the byte sequence a[] with length len, where all but the last byte
has bit 128 set, encodes the number:
(a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))
* Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
* Every integer has exactly one encoding
* Encoding does not depend on size of original integer type
* No redundancy: every (infinite) byte sequence corresponds to a list
  of encoded integers.
0:         [0x00]  256:        [0x81 0x00]
1:         [0x01]  16383:      [0xFE 0x7F]
127:       [0x7F]  16384:      [0xFF 0x00]
128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]
Several uses of New VarInts below are "differentially encoded". For
these, instead of using raw indexes, the number encoded is the
difference between the current index and the previous index, minus one.
For example, a first index of 0 implies a real index of 0, a second
index of 0 thereafter refers to a real index of 1, etc.
A PrefilledTransaction structure is used in HeaderAndShortIDs to provide
a list of a few transactions explicitly.
differentially encoded since the last PrefilledTransaction in a
list||The index into the block at which this transaction is
which is in the block at index index.
A HeaderAndShortIDs structure is used to relay a block header, the short
transactions IDs used for matching already-available transactions, and a
select few transactions which we expect a peer may be missing.
by the encoding used by "block" messages||The header of the block being
transaction ID calculations
encode array lengths||The number of short transaction IDs in shortids
Endian||The short transaction IDs calculated from the transactions which
were not provided explicitly in prefilledtxn
elsewhere to encode array lengths||The number of prefilled transactions
in prefilledtxn
size*prefilledtxn_length||As defined by PrefilledTransaction definition,
above||Used to provide the coinbase transaction and a select few which
we expect a peer may be missing
A BlockTransactionsRequest structure is used to list transaction indexes
in a block being requested.
the block header, as used elsewhere||The blockhash of the block which
the transactions being requested are in
VarInt]]||The number of transactions being requested
[[ VarInt]], differentially encoded||The indexes of the
transactions being requested in the block
A BlockTransactions structure is used to provide some of the
transactions in a block, as requested.
the block header, as used elsewhere||The blockhash of the block which
the transactions being provided are in
[[ VarInt]]||The number of transactions provided
messages||The transactions provided
====Short transaction IDs====
Short transaction IDs are used to represent a transaction without
sending a full 256-bit hash. They are calculated by:
# single-SHA256 hashing the block header with the nonce appended (in
# XORing each 8-byte chunk of the double-SHA256 transaction hash with
each corresponding 8-byte chunk of the hash from the previous step
# Adding each of the XORed 8-byte chunks together (in little-endian)
iteratively to find the short transaction ID
===New messages===
A new inv type (MSG_CMPCT_BLOCK == 4) and several new protocol messages
are added: sendcmpct, cmpctblock, getblocktxn, and blocktxn.
# The sendcmpct message is defined as a message containing a 1-byte
integer followed by a 8-byte integer where pchCommand == "sendcmpct".
# The first integer SHALL be interpreted as a boolean (and MUST have a
value of either 1 or 0)
# The second integer SHALL be interpreted as a little-endian version
number. Nodes sending a sendcmpct message MUST currently set this value
to 1.
# Upon receipt of a "sendcmpct" message with the first and second
integers set to 1, the node SHOULD announce new blocks by sending a
cmpctblock message.
# Upon receipt of a "sendcmpct" message with the first integer set to 0,
the node SHOULD NOT announce new blocks by sending a cmpctblock message,
but SHOULD announce new blocks by sending invs or headers, as defined by
# Upon receipt of a "sendcmpct" message with the second integer set to
something other than 1, nodes SHOULD treat the peer as if they had not
received the message (as it indicates the peer will provide an
unexpected encoding in cmpctblock, and/or other, messages)
# Nodes SHOULD check for a protocol version of >= 70014 before sending
sendcmpct messages.
# Nodes MUST NOT send a request for a MSG_CMPCT_BLOCK object to a peer
before having received a sendcmpct message from that peer.
# getdata messages may now contain requests for MSG_CMPCT_BLOCK objects.
# Upon receipt of a getdata containing a request for a MSG_CMPCT_BLOCK
object with the hash of a block which was recently announced and after
having sent the requesting peer a sendcmpct message, nodes MUST respond
with a cmpctblock message containing appropriate data representing the
block being requested.
# MSG_CMPCT_BLOCK inv objects MUST NOT appear anywhere except for in
getdata messages.
# The cmpctblock message is defined as as a message containing a
serialized HeaderAndShortIDs message and pchCommand == "cmpctblock".
# Upon receipt of a cmpctblock message after sending a sendcmpct
message, nodes SHOULD calculate the short transaction ID for each
unconfirmed transaction they have available (ie in their mempool) and
compare each to each short transaction ID in the cmpctblock message.
# After finding already-available transactions, nodes which do not have
all transactions available to reconstruct the full block SHOULD request
the missing transactions using a getblocktxn message.
# A node MUST NOT send a cmpctblock message unless they are able to
respond to a getblocktxn message which requests every transaction in the
# A node MUST NOT send a cmpctblock message without having validated
that the header properly commits to each transaction in the block, and
properly builds on top of the existing chain with a valid proof-of-work.
A node MAY send a cmpctblock before validating that each transaction in
the block validly spends existing UTXO set entries.
# The getblocktxn message is defined as as a message containing a
serialized BlockTransactionsRequest message and pchCommand == "getblocktxn".
# Upon receipt of a properly-formatted getblocktxnmessage, nodes which
recently provided the sender of such a message a cmpctblock for the
block hash identified in this message MUST respond with an appropriate
blocktxn message. Such a blocktxn message MUST contain exactly and only
each transaction which is present in the appropriate block at the index
specified in the getblocktxn indexes list, in the order requested.
# The blocktxn message is defined as as a message containing a
serialized BlockTransactions message and pchCommand == "blocktxn".
# Upon receipt of a properly-formatted requested blocktxn message, nodes
SHOULD attempt to reconstruct the full block by:
 Taking the prefilledtxn transactions from the original cmpctblock and
placing them in the marked positions.
 For each short transaction ID from the original cmpctblock, in order,
find the corresponding transaction either from the blocktxn message or
from other sources and place it in the first available position in the
# Once the block has been reconstructed, it shall be processed as
normal, keeping in mind that short transaction IDs are expected to
occasionally collide, and that nodes MUST NOT be penalized for such
collisions, wherever they appear.
===Implementation Notes===
# For nodes which have sufficient inbound bandwidth, sending a sendcmpct
message with the first integer set to 1 to up to three peers is
RECOMMENDED. If possible, it is RECOMMENDED that those peers be selected
based on their past performance in providing blocks quickly. This will
allow them to receive some blocks in only 0.5*RTT between them and the
sending peer. It will also reduce their block transfer latency in other
cases due to the smaller amount of data transmitted. Nodes MUST NOT send
such sendcmpct messages to all peers, as it encourages wasting outbound
bandwidth across the network.
# All nodes SHOULD send a sendcmpct message to all appropriate peers.
This will reduce their outbound bandwidth usage by allowing their peers
to request compact blocks instead of full blocks.
# Nodes with limited inbound bandwidth SHOULD request blocks using
MSG_CMPCT_BLOCK/getblocktxn requests, when possible. While this
increases worst-case message round-trips, it is expected to reduce
overall transfer latency as TCP is more likely to exhibit poor
throughput on low-bandwidth nodes.
# Nodes sending cmpctblock messages SHOULD make an attempt to not place
too many transactions into prefilledtxn (ie should limit prefilledtxn to
only around 10KB of transactions). When in doubt, nodes SHOULD only
include the coinbase transaction in prefilledtxn.
# Nodes MAY pick one nonce per block they wish to send, and only build a
cmpctblock message once for all peers which they wish to send a given
block to. Nodes SHOULD NOT use the same nonce across multiple different
# Nodes MAY impose additional requirements on when they announce new
blocks by sending cmpctblock messages. For example, nodes with limited
outbound bandwidth MAY choose to announce new blocks using inv/header
messages (as per BIP130) to conserve outbound bandwidth.
# Note that the MSG_CMPCT_BLOCK section does not require that nodes
respond to MSG_CMPCT_BLOCK getdata requests for blocks which they did
not recently announce. This allows nodes to calculate cmpctblock
messages at announce-time instead of at request-time. Thus, nodes MUST
NOT request blocks using MSG_CMPCT_BLOCK getdatas unless it is in
response to an inv/headers block announcement (as per BIP130), and MUST
NOT request blocks using MSG_CMPCT_BLOCK getdatas in response to headers
messages which were, themselves, responses to getheaders requests.
# While the current version sends transactions with the same encodings
as is used in tx messages and elsewhere in the protocol, the version
field in sendcmpct is intended to allow this to change in the future.
For this reason, it is recommended that the code used to decode
PrefilledTransaction and BlockTransactions messages be prepared to take
a different transaction encoding, if and when the version field in
sendcmpct changes in a future BIP.
====Protocol design====
There have been many proposals to save wire bytes when relaying blocks.
Many of them have a two-fold goal of reducing block relay time and thus
rely on the use of significant processing power in order to avoid
introducing additional worst-case RTTs. Because this work is not focused
primarily on reducing block relay time, its design is much simpler (ie
does not rely on set reconciliation protocols). Still, in testing at the
time of writing, nodes are able to relay blocks without the extra
getblocktxn/blocktxn RTT around 90% of the time. With a smart
compact-block-announcement policy, it is thus expected that this work
might allow blocks to be relayed between nodes in 0.5*RTT instead of
1.5*RTT at least 75% of the time.
====Use of New VarInts====
Bitcoin has long had a variable-length integer implementation (referred
to as CompactSize in this document), making a second a strange protocol
quirk. However, in this protocol most of our variable-length integers
are between 0 and 2000. For both encodings, small numbers (<100) are
encoded as 1-byte. For numbers over 250, the CompactSize encoding begins
to use 3 bytes instead of 1, whereas the New VarInt encoding uses 2.
Because the primary motivation for this work is to save bytes during
block relay, the extra byte of saving per transaction-difference is
considered worth the extra design complexity.
====Short transaction ID calculation====
The short transaction ID calculation is designed to take absolutely
minimal processing time during block compaction to avoid introducing
serious DoS vulnerabilities such as those introduced by the
bloom-filtering in BIP 37. As such, it is possible for a node to
construct one compact-block representation of a block for relay to
multiple peers. Additionally, only one cryptographic hash (2 SHA rounds)
is used when calculating the short transaction IDs for an entire block.
The XOR-and-add method is used for calculating short transaction IDs
primarily because it is fast and is reasonably able to limit the ability
of an attacker who does not know the block hash or nonce to cause
collisions in short transaction IDs. If an attacker were able to cause
such collisions, filling mempools (and, thus, blocks) with them would
cause poor network propagation of new (or non-attacker, in the case of a
miner) blocks.
The 8-byte nonce in short transaction ID calculation is used to
introduce additional entropy on a per-node level. While the use of 8
bytes is sufficient for an attacker to maliciously cause short
transaction ID collisions in their own block relay, this would have less
of an effect than if such an attacker were relaying headers/invs and not
responding to requests for the full block.
==Backward compatibility==
Older clients remain fully compatible and interoperable after this change.
Thanks to Gregory Maxwell for the initial suggestion as well as a lot of
back-and-forth design and significant testing.
This document is placed in the public domain.

@_date: 2016-05-06 03:09:14
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Thanks Greg for the testing!
Note that to those who are reviewing the doc, a few minor tweaks to
wording and clarification have been made to the git version, so please
review there.

@_date: 2016-05-08 03:24:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
(This response was originally off-list as moderators were still
deciding, here it is for those interested).
Hi Tom,
Thanks for reading the draft text and commenting! Replies inline.
Nodes don't need to predict much in advance, and the cost for predicting
wrong is 0 if your peers receive blocks with a few hundred ms between
them (as we should expect) and you haven't set the announce bit on more
than a few peers (as the spec requires for this reason). As for
complexity of keeping state, think of it as a version flag in much the
same way sendheaders operates.
It seems I forgot to add a suggested peer-preforwarding-selection
algorithm in the text, but the intended use-case is to set the bit on
peers which recently provided you blocks faster than other peers, up to
only one or three peers. This is both simple and should be incredibly
[This has now been clarified in the BIP text]
In line with recent trends, neither service bits nor protocol versions
are particularly well-suited for this purpose. Protocol versions are
impossible to handle sanely across different nodes on the network, as
they cannot indicate optional features. Service bits, while somewhat
more appropriate for this purpose, are a very limited resource which is
generally better suited to indicating significant new features which
nodes might need for correct operation, and thus might wish to actively
seek out when making connections. I'm not sure anyone is suggesting that
here, and absent that recent agreement preferred message-based feature
indication instead of version-message-extension.
Hmm? There is no UTF anywhere in this protocol. Indeed this section
needs to be rewritten, as indicated. I'd recommend you read the code
until I update the section with better text if you're confused.
I'm confused as to what, specifically, you're proposing this be changed
to. I'm pretty sure the proposed protocol is about as simple as you can
get while retaining some reasonable collision resistance. I might,
however, decide to switch to siphash with a very low round count, given
that it's probably faster than the cache-fill-time taken by just
iterating over the mempool. Needs a bit further investigation.
Greg was the only large contributor to the document (and was a very
large contributor, as mentioned - the work is based hugely on a protocol
recommendation he wrote up several years ago) don't see why this should
mean he doesn't get credit.
[For those interested, I'm referring here to
 This
BIP/the implementation is a precursor to an implementation that looks
similar to what Greg proposes there which can be found on my udp-wip
branch, which is based on and uses the data structures involved here.]

@_date: 2016-05-10 21:35:39
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Yea, I think in any hardfork that we should be talking about, a part of
it should include 1) fix the version field so its a static constant, 2)
the merkle root becomes hash of the real block header 3) swap first 2
bytes of the merkle root with the timestamp's two high-order bits, 4)
swap the next 4 bytes of the merkle root with the difficulty field.
I believe this should be compatible with all existing ASICs, with the
exception, possibly, of some 21 Inc hardware. I believe this fixes
AsicBoost (without thinking about it tooo much, so please critique).
While this is somewhat nasty, the risks of AsicBoost and the precedent
that should be set necessitates a response, and it should be included in
any hardfork.

@_date: 2016-05-10 22:59:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Replies inline.
One is patented and requires paying a license fee to a group, or more
likely, ends up with it being impossible to import hardware from other
jurisdictions into the US/western world. The other requires more
investment in R&D, and over the long run, there is no guaranteed
advantage to such groups.
To some extent, this is the case, but there is a strong difference
between a guaranteed advantage enforced by the legal system and one that
is true due to intellectual superiority. In the long run, I am confident
the second will not remain the case. For example, AsicBoost was
independently discovered by at least two companies/individuals within a
year or two.
As far as I'm aware neither of these are patented. Is this not the case?

@_date: 2016-05-11 01:12:32
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Replies inline.
Yea, there's already an ongoing discussion of that, and the UDP stuff will definitely want something different than the current proposals.
Not at all. The goal with the UDP stuff I've been working on is not to provide reliable transport. Like the relay network, it is assumed some percent of blocks will fail to transit properly, and you will use some other transport to figure out how to get the block. Indeed, a big part of my desire for diversity in network protocols is to enable them to make tradeoffs in reliability/privacy/etc.
I assume what Greg was referring to the idea that if there is a conflict, a given block will require an extra round trip when being broadcast between roughly each peer, compounding the effect across each hop.
... Assuming different encoding lengths aren't just truncated, but ok :).

@_date: 2016-05-11 20:50:10
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
That's the reason for this post! All current major ASIC manufacturers
have made warrants that they are not using AsicBoost (with the exception
of the 21 Inc Bitcoin computer).
The fact that the optimization was patented is what has required that we
work to hardfork it out, not that people might have such private
optimizations. The fact that AsicBoost was independently discovered by
at least two (if not three) organizations seems to lend credence to the
idea that private optimizations will only provide a temporary win over

@_date: 2016-05-11 21:01:57
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Indeed, I think the "ASICs are bad, because 1-CPU-1-vote" arguments
mostly died out long ago, and, indeed, the goal that many making those
arguments had of building "unoptimizeable" ASICs failed with them.
I think everyone understands that there will always be some ability to
iterate on ASIC designs, however, a patented optimization breaks that
assumption. Instead of being freely able to optimize their ASIC design,
patented optimizations require that people who discover such
optimizations themselves do not use them, giving one
manufacturer/licenser a huge influence in who is successful in a market
that we're all relying on remaining rather flat. Indeed, with AsicBoost,
we saw Spondoolies independently discover the same optimization, but
with the current legal system they would not have been able to sell such
systems without licensing AsicBoost.

@_date: 2016-05-12 01:58:42
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Aside from patents related to the silicon manufacturing process itself and patents not yet published, yes, the process is unencumbered, and setting the correct precedent (that the community will fight large centralization risks) is important in the first case.

@_date: 2016-05-18 01:49:10
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Implemented a few of your suggestions.
Also opened a formal pull request for the BIP at
 and the code at
Fixed, the whole thing now uses New Varints.
I switched to 6-byte short txids.
Switched to SipHash2-4.
Additionally we should compare to the orphan pool (which apparently
helps a lot).

@_date: 2016-10-02 22:51:06
@_author: Matt Corallo 
@_subject: [bitcoin-dev] About ASICBoost 
Replies to comments inline.
This is a very misleading comparison. I am not aware of any other
patents on Bitcoin-specific ASIC technology which are practically
enforceable or which the owners have indicated they wish to enforce. Of
the two patents which you point out which were filed on essentially the
same optimization that ASICBoost covers, yours predates both of them,
invalidating both the Spondoolies one (which Guy had indicated he wished
to use only defensively) and the AntMiner one. Of course, as China is
notorious for ignoring international patent law, AntMiner's could
possibly still be enforced in China. Still, AntMiner has, like
Spondoolies did, indicated they have no intention of enforcing their
patent to limit competition, though without any legally-enforceable
commitment. This leaves only your patent as practical and likely to be
enforced in the vast majority of the world.
If you had acted in a way which indicated even the slightest regard for
centralization pressure and the harm it can do to Bitcoin in the
long-term, then I dont think many would be blaming you. Instead of any
kind of open or transparent licensing policy, with price structures
designed to encourage competition, you chose to hide behind an opaque
website, asking people to simply email you and Timo to negotiate
Optimizations to the hashing algorithm are not, themselves, "attacks" on
Bitcoin, as you claimed in your post at the time. Only when they are
used in a rent-seeking fashion to push for more centralization and lower
miner revenue do they become so. One of the biggest advantages of SHA256
in the context of mining is exactly that it is a relatively simple
algorithm, allowing for fewer large algorithmic optimizations (or, when
there are, more people are capable of finding them, as happened with
ASICBoost). This opens the doors to more competition in the ASIC market
than if only few people had the knowledge (or a patent) to build
efficient ASICs. While it is certainly true that the high-end
ASIC-manufacturing industry is highly-centralized, making it worse by
limiting those who can build Bitcoin ASICs from anyone with access to
such a fab to only those who can, additionally, negotiate for patent
rights and navigate the modern patent system, is far from ideal.
You claim that Bitcoin should have fixed the problem at the time, but
you posted a proposal for a hard fork, with the only argument given as
to why it should happen being that you thought you had an attack, but
cant yet "really tell if they could affect Bitcoin". Instead of
following up with more information, as you indicated you would, you went
and patented the optimizations and have gone on rent-seeking behavior since.

@_date: 2016-10-02 23:19:08
@_author: Matt Corallo 
@_subject: [bitcoin-dev] About ASICBoost 
Even if the Bitcoin Foundation decided to recklessly disregard Bitcoin's
future centralization, I'm not sure going to them and asking them to pay
a license fee in order to keep from holding the rest of the Bitcoin
mining community hostage counts as "regard for centralization pressure".
It also doesn't excuse the lack of transparent licensing being available
today, or the lack of transparency when discussing it in public after
the patent had been filed.

@_date: 2016-10-16 17:04:59
@_author: Matt Corallo 
@_subject: [bitcoin-dev] On the security of soft forks 
I highly recommend you read the excellent thread on soft fork risks at
and respond there instead of getting off topic for this thread.

@_date: 2016-10-16 17:52:09
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
This start time seems reasonable to me. It is mostly in line with BIP 9's proposed defaults, which seems like an appropriate choice.

@_date: 2016-10-16 19:35:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] (no subject) 
You keep calling flexible transactions "safer", and yet you haven't
mentioned that the current codebase is riddled with blatant and massive
security holes. For example, you seem to have misunderstood C++'s memory
model - you would have no less than three out-of-bound, probably
exploitable memory accesses in your 80-LoC deserialize method at
if you were to turn on flexible transactions (and I only reviewed that
method for 2 minutes). If you want to propose an alternative to a
community which has been in desperate need of fixes to many problems for
several years, please do so with something which would not take at least
a year to complete given a large team of qualified developers.
You additionally have not yet bothered to address the discussion of
soft-fork security at
which I believe answers all of your claims about upgrades required in a
much more detailed discussion than I will include here. Please take your
off-topic discussions there instead of this thread.

@_date: 2016-10-25 17:47:37
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core 0.13.1 release candidate 3 
Due to a relatively trivial fix for an out-of-place assertion in rc2
0.13.1rc3 was tagged and is now available on github either via git or at
Because of the simplicity of the change, it was decided that 0.13.1
should not be delayed as a result, and thus no official binaries will be
made available for rc3.
However, any additional testing which folks have time for before final
tag on Thursday would be significantly welcomed.
As usual please report bugs using the issue tracker on github at

@_date: 2016-09-01 00:56:08
@_author: Matt Corallo 
@_subject: [bitcoin-dev] ScalingBitcoin 2015: Retarget - Call For 
Hi all,
For those who missed it, the deadline for submissions has been extended
to Sept 9th so be sure to submit before then! We will be doing rolling
acceptance this time around to try to get most responses out before the
Because a few folks seemed to have some confusion, the definition of
"scaling" here is pretty broad - while we definitely will have a lot of
talks on the usual tx-volume-throughput things, the topics of interest
also include things like fungibility. The full list from the site (for
inspiration purposes, this is by no means exhaustive) is:
Improving Bitcoin throughput
Layer 2 ideas (i.e. payment channels, etc.)
Security and privacy
Incentives and fee structures
Testing, simulation, and modeling
Network resilience and latency
Anti-spam measures
Block size proposals
Mining concerns

@_date: 2017-04-07 10:14:07
@_author: Matt Corallo 
@_subject: [bitcoin-dev] A different approach to define and 
Random misreadings of your post aside (maybe it's time to moderate this list a bit more again), I think this is a reasonable model, and certainly more terminology/understanding is useful, given I and many others have been making arguments based on these differences.
One thing you may wish to further include may be that many soft forks do not require any miner upgrade at all due to standardness rules. Eg OP_CSV and SegWit both only require miners upgrade if they wish to receive the additional fees from new transactions using these features.

@_date: 2017-12-03 16:32:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Two Drivechain BIPs 
Process note: It looks like the BIPs have never been posted to
bitcoin-dev, only high-level discussion around the idea. As I understand
it, this is *not* sufficient for BIP number assignment nor
(realistically) sufficient to call it a hard "proposal" for a change to
consensus rules.
Would love to get feedback from some others who are looking at deploying
real-world sidechains, eg the RSK folks. We can't end up with *two*
protocols for sidechains in Bitcoin.
Comments on BIP 1:
At a high level, I'm rather dissapointed by the amount of data that is
going into the main chain here. Things such as a human readable name
have no place in the chain, IMO. Further, the use of a well-known
private key seems misplaced, why not just track the sidechain balance
with something that looks like `OP_NOPX genesis_block_hash`?
I'm not convinced by the full semantics of proposal/ack of new
sidechains. Given the lack of convincing evidence of that "Risk of
centralisation of mining" drawback in section 4.3 of the sidechains
paper has been meaningfully addressed, I'd say its pretty important that
new sidechains be an incredibly rare event. Thus, a much simpler system
(eg a version-bits-based upgrade cycle with high threshold) could be
used to add new sidechains based on well-known public parameters.
The semantics of the deposit process seem very suboptimal. You note that
only one user can deposit at a time, but this seems entirely
unnecessary. As implemented in the first Elements Alpha release (though
I believe subsequently removed in later versions due to the nature of
Elements of targeting asymmetric "federated" sidechains), if you have
outputs that look like `OP_NOPX genesis_block_hash` as the sidechain
deposit/storage address, deposit can be fully parallel. To reduce
blockchain bloat, spending them for the purpose of combining such
outputs is also allowed. You could even go further and allow some new
sighash type to define something like SIGHASH_ALL|SIGHASH_ANYONECANPAY
which further specifies some semantics for combining inputs which all
pay into the same output.
Finally, you may also want to explore some process for the removal of
sidechain balances from the main chain. As proposed it seems like a
sidechain might, over time, fade into an insecure state as mining power
shifts and new miners no longer consider it worth the value to mine an
old sidechain (as has happened over time with namecoin, arguably).
Comments on BIP 2:
I may be missing something, but I find the security model here kind of
depressing...Not only do hashpower-secured sidechains already have a
significantly reduced security level, but now you're proposing to
further (drastically) reduce it by requiring users to potentially pay in
excess of the value an attacker is willing to pay to keep their chain
secure, on a recurring basis? It seems like if a chain has 10 BTC stored
in it, and I wish to reorg it for a potential gain of, lets say, 6 BTC,
I can pay 6 * 1 BTC (1 per block) to reorg it, and users on the chain
would be forced to pay >6 BTC to avoid this?
While I appreciate the desire to implement the proposed mitigation in
section 4.3 of the sidechains paper (delegating the mining effort of a
merge-mined sidechain to an external entity), I believe it was primarily
referencing pooling the sidechain work, not blindly taking the highest
bidder. I suppose, indeed, that, ultimately, as long as the sidechain is
of relatively small value in comparison to BTC, miners do not risk the
value of their BTC/mining investment in simply taking the highest bidder
of a merge-mined block, even if its a clear attack, but I don't think
thats something to be celebrated, encouraged, or designed to be possible
by default. Instead, I'd, in line with Peter Todd's (and others')
objection to merged mining generally, call this one of the most critical
issues with the security model.
Ultimately, I dont believe your proposal here really solves the drawback
in section 4.3 of the paper, and possibly makes it worse. Instead, it
may be more useful to rely on a high threshold for the addition of new
sidechains, though I'd love to see discussion on this point specifically
on this list. Further, I'd say, at a minimum, a very stable
default-available low-bandwidth implementation of at least the
pool-based mitigation suggested in the paper must exist for something
like this to be considered readily stable enough to be deployed into the
Bitcoin ecosystem.

@_date: 2017-12-23 16:25:08
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP-21 amendment proposal: -no125 
While the usability of non-RBF transactions tends to be quite poor, there are some legitimate risk-analysis-based reasons why people use them (eg to sell BTC based on a incoming transaction which you will need to convert to fiat, which has low cost if the transaction doesn't confirm), and if people want to overpay on fees to do so, no reason not to let them, including if the merchant is willing to CPFP to do so.
Honestly, I anticipate very low usage of such a flag, which is appropriate, but also strongly support including it. If things turn out differently with merchants reducing the usability of BTC without taking over the CPFP responsibility we could make the option imply receiver-pays-fee, but no reason to overcomplicate it yet.

@_date: 2017-02-13 10:16:13
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP151 protocol incompatibility 
For the reasons Pieter listed, an explicit part of our version handshake and protocol negotiation is the exchange of otherwise-ignored messages to set up optional features.
Peers that do not support this ignore such messages, just as if they had indicated they wouldn't support it, see, eg BIP 152's handshake. Not sure why you consider this backwards incompatible, as I would say it's pretty clearly allowing old nodes to communicate just fine.

@_date: 2017-02-13 11:11:11
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP151 protocol incompatibility 
I believe many, if not all, of those messages are sent irrespective of version number.
In any case, I fail to see how adding any additional messages which are ignored by old peers amounts to a lack of backward compatibility.

@_date: 2017-02-13 13:04:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP151 protocol incompatibility 
Sorry, I'm still missing it...
So your claim is that a) ignoring incoming messages of a type you do not recognize is bad, and thus b) we should be disconnecting/banning peers which send us messages we do not recognize (can you spell out why? Anyone is free to send your host address messages/transactions they are generating/etc/etc, we don't ban nodes for such messages, as that would be crazy - why should we ban a peer for sending us an extra 50 bytes which we ignore?), and thus c) this would be backwards incompatible with software which does not currently exist?
Usually "backwards incompatible" refers to breaking existing software, not breaking theoretical software. Note that, last I heard, BIP 151 is still a draft, if such software actually exists we can discuss changing it, but there are real wins in sending these messages before VERSION.

@_date: 2017-01-26 03:29:14
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
"A. For users on both existing and new fork, anti-replay is an option,
not mandatory"
To maximize fork divergence, it might make sense to require this. Any
sensible proposal for a hard fork would include a change to the sighash
anyway, so might as well make it required, no?

@_date: 2017-01-26 17:41:55
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
Excuse me, yes, for previously-signed transactions this is required. We might consider some limits on UTXO-chain-from-before-the-fork-length and likely something like move towards only allowing one transaction per block from the old mode over time.
I highly disagree that compatibility with existing transaction signing software should be considered (but for hardware which cannot be upgraded easily we do need to consider it). Wallets which can upgrade should, as much as possible, upgrade to a new form to maximize chain divergence and are going to end up having to upgrade to know a new header format anyway, so am extra few lines of code to change a transaction version should be trivial.

@_date: 2017-01-28 00:35:55
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Extension block softfork proposal 
Hey Johnson,
As you know I've always been a rather large critic of this approach.
First a bit of background. Pieter's excellent post on the security of
soft forks [1] covers pretty well why soft forks are preferable to hard
forks by debunking much of the "soft forks are less secure" arguments.
While those arguments apply readily to your proposal, what wasn't
covered are the "soft forks are coercive" arguments. Indeed, many of
those arguments are also bogus. After all, soft forks are not "forks"
without buy-in from the economically relevant community running nodes
which enforce the new rules (ie fork-by-miner-censorship isn't all that
much of a fork at all, and has security properties which I would be
hesitant to use for anything but the smallest of value).
That said, when we start talking about extension blocks, I believe we
start to rapidly enter this "coerciveness" territory. With segwit, we've
seen pretty clearly that the community, much to its detriment, can be
easily made unwilling to speak up for or against a fork, making
consensus an incredibly murky thing.
Luckily, as noted in Pieter's original post, there isn't much harm in
the passive observer not making their voice heard and going along and
enforcing SegWit. SegWit maintains UTXO compatibility and transactions
continue to work as normal, only hiding information necessary to apply
the soft fork's rules from old nodes. This is not significantly
different from any other softfork, where declining to enforce its rules
results in you missing information (only in this case in the form of
additional validity rules instead of signatures themselves, which you
otherwise don't know what to do with). Even better, the bandwidth
increases for fully-validating nodes have been more than offset by other
technology upgrades.
Much of this goes out the window with extension blocks. Instead of the
extra data being reasonable to ignore if you choose to not enforce the
soft fork's rules, all of a sudden a majority (or at least significant
chunk) of transactions on the network are happening in the data you've
chosen to ignore. Instead of being able to reasonably walk back
transaction history to identify risk based on potential
censorship-enforced-transactions (ie transactions in a soft fork you're
not aware of, potentially that only miners are enforcing), all
transactions will look risky. Instead of being able to enforce
fundamental network rules like the 21 million coin limit, you're left to
trust that what is happening on the extension block (which all miners
are largely forced to mine due to the fee revenue opportunity cost).
This ultimately makes it a social cost, not an individual trust problem
- instead of opting into a soft fork's security (or lack thereof) for
your own transaction, the entire network is forced to trust the
extension block.
Finally, this sets us up for some pretty terrible precedent. As we noted
in a footnote of the original sidechains paper, the idea that miners
will start soft-forking in sidechains is a massive risk - it allows
individual large miners and individual economic users to force others to
switch to new consensus rules, with potentially little consensus or review.
On January 26, 2017 4:39:43 AM EST, Johnson Lau via bitcoin-dev

@_date: 2017-01-28 02:32:26
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
Looks cool, though I have a few comments inline.
One general note - it looks like you're letting complexity run away from
you a bit here. If the motivation for something is only weak, its
probably not worth doing! A hard fork is something that must be
undertaken cautiously because it has so much inherent risk, lets not add
tons to it.
In order of appearance:
First of all lets try to minimize header size. We really dont want any
more space taken up here than we absolutely need to.
I'm super unconvinced that we need more than one merkle tree for
transactions. Lets just have one merkle tree who's leaves are
transactions hashed 2 ways (without witnesses and only witnesses).
Why duplicate the nBits here? shouldn't the PoW proof be the
responsibility of the parent header?
I have to agree with Tadge here, variable-length header fields are evil,
lets avoid them.
Why have merkle branches to yet another header? Lets just leave it as an
opaque commitment header (32).
Finally, lets not jump through hoops here - the transaction merkle root
of the "old-style" (now PoW) header should simply be the hash of the new
header. No coinbase transaction, just the hash of the secondary header.
This saves space without giving up utility - SPV nodes are already not
looking at the coinbase transaction, so no harm in not having one to give.
Will comment on the anti-replay post.
This is definitely too much. On the one hand its certainly nice to be
able to use max() for limits, and nice to add all the reasonable limits
we might want to, but on the other hand this can make things like coin
selection super complicated - how do you take into consideration the 4
different limits? Can we do something much, much simpler like
max(serialized size with some input discount, nSigOps * X) (which is
what we effectively already have in our mining code)?
Hum, not sure this is sufficient. Its still stair-stepping at big enough
jumps that we could conceivably see super slow block times around
halvings in the distant future. Maybe instead of 100%-75%-75%-50% (I
believe that's what you're proposing here?),
100%-87.5%-75%-75%-62.5%-50% might be smoother?
I'm not necessarily opposed to this, but what is the justification for it?
Sounds good.
If we cant build wholesale proofs, then lets not jump through hoops and
add special bits to build partial ones? Its not clear to me that it
would be any reduction in soft-fork-ability later down the road to not
have this - if you're changing the definition of tx weight, you're
likely doing something like segwit where you're adding something else,
not trying to re-adjust weights.

@_date: 2017-01-28 03:02:21
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
Oops, forgot to mention, in the "parent" (ie old) block header, we should:
1) fix the version field so its a static constant
2) swap first 2 bytes of the merkle root with the timestamp's two
high-order bytes (preferably more, I'm not sure how much ASIC hardware
has timestamp-rolling in it anymore, but if there is none left we should
take all 4 bytes from the timestamp field).

@_date: 2017-01-28 17:14:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
Replies inline.
Agreed. Would like 1, dont care about 2, not a fan of 3. 2 could even be
implemented easily as a softfork if we allow the
spend-other-coinbase-outputs from 1.
Hmm? I'm saying that "the header" should be viewed as both the
"top-level" PoW-proving header, and the sub-header. There is no need to
have nBits in both?
We can safely disable SPV clients post-fork by just keeping the header
format sufficiently compatible with PR without caring about the
coinbase transaction, which I think should be the goal.
Regarding firmware upgrade, you make a valid point. I suppose we need
something that looks sufficiently like a coinbase transaction that
miners can do nonce-rolling using existing algorithms. Personally, I'd
kinda prefer something like a two-leaf merkle tree root as the merkle
root in the "primary 80-byte header" (can we agree on terminology for
this before we go any further?) - the left one is a
coinbase-transaction-looking thing, the right one the header of the new
block header.
Yes, I got the max() being at the transaction level (at the block level
would just be stupid) :).
This does not, however, make UTXO selection trivial, indeed, the second
you start having not-completely-homogeneous UTXOs in your wallet you
have to consider "what if the selection of this UTXO would switch my
criteria from one to another", which I believe makes this nonlinear.
So lets apply this only to non-Segwit-hashed transactions (incl
transactions which opted into the new sighash rules using the
anti-replay stuff)?
Yes, I tend to agree that there isnt much way around a sigops limit (as
we have now).
Totally agree there, but we can easily discount inputs more than outputs
to accomplish this for most potential outputs, I believe.
Did I miss a justification for there being a separate b (witness
serialized size) and c (txweight-with-discounts?).
Or by allowing coinbase txn to spend previous coinbase outputs. This
seems to not be an issue at present, though is something to consider in
future soft forks, so, agreed, lets table this and make sure we're set
up to do it in a soft fork if we need to.
Hmm, cant we accomplish this with a future sighash mode in which you
simply include the block's hash in the sighash and then spend to an
Hmm, I think you missed my point - if we're soft-forking in a new limit,
we can trivially add a new merkle tree over only that limit, which is
sufficient to make fraud proofs for the new limit.
Sure, but now miners can disable fraud proofs using a simple majority,
which really, super sucks.
Yes, lets skip it for now, I dont see much value in debating it for a HF

@_date: 2017-07-07 18:44:21
@_author: Matt Corallo 
@_subject: [bitcoin-dev] A Segwit2x BIP 
This is horribly under-specified (ie not possible to implement from what
you've written, and your implementation doesn't match at all, last I heard).
This is not a protocol change. I have no idea why you included it in the
"specification" section.
This is not a hard fork, simply adding a new limit is a soft fork. You
appear to be confused - as originally written, AFAIR, Jeff's btc1 branch
did not increase the block size, your specification here matches that
original change, and does not increase the block size.
There is no hard fork, and this would violate consensus rules. Not sure
what you mean. If you do add a hard fork to this BIP, you really need to
flip the hard fork bit.
This is far from sufficient to protect from DoS attacks, you really
should take a look through the mailing list archives and read some of
the old discussions on the issues here.

@_date: 2017-06-01 21:33:43
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP Proposal: Compact Client Side Filtering for 
Quick comment before I finish reading it completely, looks like you have no way to match the input prevouts being spent, which is rather nice from a "watch for this output being spent" pov.

@_date: 2017-03-22 21:51:08
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Fraud proofs for block size/weight 
It works today and can be used to prove exact size: the key observation is that all you need to show the length and hash of a transaction is the final SHA256 midstate and chunk (max 64 bytes). It also uses the observation that a valid transaction must be at least 60 bytes long for compression (though much of that compression possibility goes away if you're proving something other than "too large").

@_date: 2017-03-23 23:01:09
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Issolated Bitcoin Nodes 
I haven't investigated, but you may be seeing segwit-invalid blocks...0.13.0+ nodes will enforce segwit as it activated some time ago on testnet, 0.12.X nodes will not.

@_date: 2017-03-27 19:32:30
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segregated witness p2p layer compatibility 
Just to expand a tiny bit here, while the testnet setup of only a few nodes acting as "bridges", mainnet already has many systems which act as effective bridges today - there are several relay networks in use which effectively bypass the P2P network, including my legacy relay network (which many miners historically have used, and I'd expect those who aren't paying attention and don't upgrade will not turn off, fixing the issue for them), ViaBTC's super aggressive bandwidth-wasting block announcement network which pushes blocks from several pools to many nodes globally, and Bitcoin.com's private relay network. (Of course many other miners and pools have private relay networks, but the several other such networks I'm aware of are already segwit-compatible, even for pools not signaling segwit).

@_date: 2017-03-28 17:13:09
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Not sure what "last week's meeting" is in reference to?
Agreed that the hard fork should be well-prepared, but I think its
dangerous to think that a hard fork as agreed upon would be a simple
relaxation of the block size. For example, Johnson Lau's previous
proposal, Spoonnet, which I think is probably one of the better ones,
would be incompatible with these rules.
I, of course, worry about what happens if we cannot come to consensus on
a number to soft fork down to, potentially significantly risking miner
profits (and, thus, the security of Bitcoin) if a group is able to keep
things "at the status quo". That said, for that to be alleviated we
could simply do something based on historical transaction growth (which
is somewhat linear, with a few inflection points), but that number ends
up being super low (eg somewhere around 2MB at the next halving, which
SegWit itself already provides :/.
We could, of course, focus on designing a hard fork's activation and
technical details, with a very large block size increase in it (ie
closer to 4/6MB at the next halving or so, something we at least could
be confident we could develop software for), with intention to soft fork
it back down if miner profits are suffering.

@_date: 2017-03-28 22:35:05
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Fraud proofs for block size/weight 
I dont think thats true? Sure, you have to assume the block is valid
aside from a too-large size, but it seems sane.
You don't strictly need to show that a leaf is a parseable transaction,
as long as you can assume that the block is valid and that you cannot
forge a SHA256 midstate which, when combined with data with a given
length tag, would result in a hash of a given value (this is a pretty
strong assumption, IMO, IIRC this was not a studied nor a claimed
feature of SHA256).
The only issue is that, since parts of the merkle tree are repeated, you
need to be sure that the counting for minimum number of transactions is
accurate, though I did not review your proposal text to check that.
 - snip -

@_date: 2017-03-31 21:22:10
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request 
Hey Sergio,
You appear to have ignored the last two years of Bitcoin hardfork
research and understanding, recycling instead BIP 102 from 2015. There
are many proposals which have pushed the state of hard fork research
much further since then, and you may wish to read some of the posts on
this mailing list listed at and make further edits based on what you learn. Your goal of "avoid
technical changes" appears to not have any basis outside of perceived
compromise for compromise sake, only making such a hardfork riskier
At a minimum, in terms of pure technical changes, you should probably
consider (probably among others):
a) Utilizing the "hard fork signaling bit" in the nVersion of the block.
b) Either limiting non-SegWit transactions in some way to fix the n**2
sighash and FindAndDelete runtime and memory usage issues or fix them by
utilizing the new sighash type which many wallets and projects have
already implemented for SegWit in the spending of non-SegWit outputs.
c) Your really should have replay protection in any HF. The clever fix from
Spoonnet for poor scaling of optionally allowing non-SegWit outputs to
be spent with SegWit's sighash provides this all in one go.
d) You may wish to consider the possibility of tweaking the witness
discount and possibly discounting other parts of the input - SegWit went
a long ways towards making removal of elements from the UTXO set cheaper
than adding them, but didn't quite get there, you should probably finish
that job. This also provides additional tuneable parameters to allow you
to increase the block size while not having a blowup in the worst-case
block size.
e) Additional commitments at the top of the merkle root - both for
SegWit transactions and as additional space for merged mining and other
commitments which we may wish to add in the future, this should likely
be implemented an "additional header" ala Johnson Lau's Spoonnet proposal.
Additionally, I think your parameters here pose very significant risk to
the Bitcoin ecosystem broadly.
a) Activating a hard fork with less than 18/24 months (and even then...)
from a fully-audited and supported release of full node software to
activation date poses significant risks to many large software projects
and users. I've repeatedly received feedback from various folks that a
year or more is likely required in any hard fork to limit this risk, and
limited pushback on that given the large increase which SegWit provides
itself buying a ton of time.
b) Having a significant discontinuity in block size increase only serves
to confuse and mislead users and businesses, forcing them to rapidly
adapt to a Bitcoin which changed overnight both by hardforking, and by
fees changing suddenly. Instead, having the hard fork activate technical
changes, and then slowly increasing the block size over the following
several years keeps things nice and continuous and also keeps us from
having to revisit ye old blocksize debate again six months after activation.
c) You should likely consider the effect of the many technological
innovations coming down the pipe in the coming months. Technologies like
Lightning, TumbleBit, and even your own RootStock could significantly
reduce fee pressure as transactions move to much faster and more
featureful systems.
Commitments to aggressive hard fork parameters now may leave miners
without much revenue as far out as the next halving (which current
transaction growth trends are indicating we'd just only barely reach 2MB
of transaction volume, let alone if you consider the effects of users
moving to systems which provide more features for Bitcoin transactions).
This could lead to a precipitous drop in hashrate as miners are no
longer sufficiently compensated.
Remember that the "hashpower required to secure bitcoin" is determined
as a percentage of total Bitcoins transacted on-chain in each block, so
as subsidy goes down, miners need to be paid with fees, not just price
increases. Even if we were OK with hashpower going down compared to the
value it is securing, betting the security of Bitcoin on its price
rising exponentially to match decreasing subsidy does not strike me as a
particularly inspiring tradeoff.
There aren't many great technical solutions to some of these issues, as
far as I'm aware, but it's something that needs to be incredibly
carefully considered before betting the continued security of Bitcoin on
exponential on-chain growth, something which we have historically never

@_date: 2017-03-31 21:18:25
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request 
Hey Sergio,
You appear to have ignored the last two years of Bitcoin hardfork
research and understanding, recycling instead BIP 102 from 2015. There
are many proposals which have pushed the state of hard fork research
much further since then, and you may wish to read some of the posts on
this mailing list listed at and make further edits based on what you learn. It seems your goal of
"avoid any technical changes" doesn't have any foundation aside from a
perceived compromise for compromise sake, only making for fork riskier
in the process.
At a minimum, in terms of pure technical changes, you should probably
consider (probably among others):
a) Utilizing the "hard fork signaling bit" in the nVersion of the block.
b) Either limiting non-SegWit transactions in some way to fix the n**2
sighash and FindAndDelete runtime and memory usage issues or fix them by
utilizing the new sighash type which many wallets and projects have
already implemented for SegWit in the spending of non-SegWit outputs.
c) Your replay protection isn't really ideal - XXX. The clever fix from
Spoonnet for poor scaling of optionally allowing non-SegWit outputs to
be spent with SegWit's sighash provides this all in one go.
d) You may wish to consider the possibility of tweaking the witness
discount and possibly discounting other parts of the input - SegWit went
a long ways towards making removal of elements from the UTXO set cheaper
than adding them, but didn't quite get there, you should probably finish
that job. This also provides additional tuneable parameters to allow you
to increase the block size while not having a blowup in the worst-case
block size.
e) Additional commitments at the top of the merkle root - both for
SegWit transactions and as additional space for merged mining and other
commitments which we may wish to add in the future, this should likely
be implemented an "additional header" ala Johnson Lau's Spoonnet proposal.
Additionally, I think your parameters here pose very significant risk to
the Bitcoin ecosystem broadly.
a) Activating a hard fork with less than 18/24 months (and even then...)
from a fully-audited and supported release of full node software to
activation date poses significant risks to many large software projects
and users. I've repeatedly received feedback from various folks that a
year or more is likely required in any hard fork to limit this risk, and
limited pushback on that given the large increase which SegWit provides
itself buying a ton of time.
b) Having a significant discontinuity in block size increase only serves
to confuse and mislead users and businesses, forcing them to rapidly
adapt to a Bitcoin which changed overnight both by hardforking, and by
fees changing suddenly. Instead, having the hard fork activate technical
changes, and then slowly increasing the block size over the following
several years keeps things nice and continuous and also keeps us from
having to revisit ye old blocksize debate again six months after activation.
c) You should likely consider the effect of the many technological
innovations coming down the pipe in the coming months. Technologies like
Lightning, TumbleBit, and even your own RootStock could significantly
reduce fee pressure as transactions move to much faster and more
featureful systems.
Commitments to aggressive hard fork parameters now may leave miners
without much revenue as far out as the next halving (which current
transaction growth trends are indicating we'd just only barely reach 2MB
of transaction volume, let alone if you consider the effects of users
moving to systems which provide more features for Bitcoin transactions).
This could lead to a precipitous drop in hashrate as miners are no
longer sufficiently compensated.
Remember that the "hashpower required to secure bitcoin" is determined
as a percentage of total Bitcoins transacted on-chain in each block, so
as subsidy goes down, miners need to be paid with fees, not just price
increases. Even if we were OK with hashpower going down compared to the
value it is securing, betting the security of Bitcoin on its price
rising exponentially to match decreasing subsidy does not strike me as a
particularly inspiring tradeoff.
There aren't many great technical solutions to some of these issues, as
far as I'm aware, but it's something that needs to be incredibly
carefully considered before betting the continued security of Bitcoin on
exponential on-chain growth, something which we have historically never

@_date: 2017-05-03 22:03:43
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Full node "tip" function 
If we ever have a problem getting blocks, we could consider adding something to pay to receive historical blocks but luckily that isn't a problem we have today - the available connection slots and bandwidth on the network today appears to be more than sufficient to saturate nearly any fully-validating node.

@_date: 2017-05-09 18:15:45
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I'm not sure who wrote segwit.org, but I wouldn't take it as
authoritative reasoning why we must do X over Y.
You seem to be claiming that there is not cost for a miner to fill
"extra witness space", but this is very untrue - in order to do so they
must forgo fees on other transactions. Your analysis on worst-case vs
normal-case blocks also seems flawed - there is a single limit, and not
a separate, secondary, witness limit.
You suggested "If the maximum block weight is set to 2.7M, each byte of
non-witness block costs 1.7", but these numbers dont work out - setting
the discount to 1.7 gets you a maximum block size of 1.7MB (in a soft
fork), not 2.7MB. If you set the max block weight to 2.7 with a 1.7x
discount, you have a hard fork. If you set the discount to 2.7x with a
2.7 weight limit, you dont get 2.7MB average-sized blocks, but smaller,
and still have the potential for padding blocks with pure-witness data
to create larger blocks.
Additionally, note that by padding blocks with larger witness data you
lose some of the CPU cost to validate as you no longer have as many
inputs (which have a maximal validation cost).
Further, I'm not sure why you're arguing for a given witness discount on
the basis of a future hardfork - it seems highly unlikely the community
is in a position to pull something like that off, and even if it were,
why set the witness discount with that assumption? If there were to be a
hardfork, we should probably tweak a bunch of parameters (see, eg, my
post from February of last year at
Maybe you could clarify your proposal a bit here, because the way I read
it you seem to have misunderstood SegWit's discount system.

@_date: 2017-05-09 19:42:56
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
There is something in between throwing the SegWit goals out the window
(as Sergio seems to be advocating for) and having a higher discount
ratio (which is required for the soft fork version to be useful).
When I first started looking at the problem I very much wanted to reduce
the worst-case block size (though have come around to caring a bit less
about that thanks to all the work in FIBRE and other similar systems
over the past year or two), but rapidly realized that just reducing the
Segwit discount wasn't really the right solution here.
You might as well take the real win and reduce the cost of the input
prevout itself so that average inputs are less expensive than outputs
(which SegWit doesn't quite achieve due to the large prevout size - 40
bytes). This way you can reduce the discount, still get the SegWit goal,
and get a lower ratio between worst-case and average-case block size,
though, frankly, I'm less interested in the last one these days, at
least for reasonable parameters. If you're gonna look at hard forks,
limiting yourself to just the parameters that we can tweak in a soft
fork seems short-sighted, at beast.

@_date: 2017-05-10 14:05:30
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I'm highly unconvinced of this point. Sure, you can change fewer lines
of code, but if the result is, lets be honest, shit, how do you believe
its going to have a higher chance of getting acceptance from the broader
community? I think you're over-optimizing in the wrong direction.

@_date: 2017-05-10 16:39:00
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I highly disagree about the "not shit" part.  You're advocating for throwing away one of the key features of Segwit, something that is very important for Bitcoin's long-term reliability! If you think doing so is going to somehow help get support in a divided community, I don't understand how - more likely you're only going to make things significantly worse.

@_date: 2017-05-22 22:43:00
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Reduced signalling threshold activation of 
Given the overwhelming support for SegWit across the ecosystem of businesses and users, this seems reasonable to me.

@_date: 2017-05-26 20:02:41
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Barry Silbert segwit agreement 
Your proposal seems to be simply BIP 91 tied to the
as-yet-entirely-undefined hard fork Barry et al proposed.
Using James' BIP 91 instead of the Barry-bit-4/5/whatever proposal, as
you propose, would make the deployment on the incredibly short timeline
Barry et al proposed slightly more realistic, though I would expect to
see hard fork code readily available and well-tested at this point in
order to meet that timeline.
Ultimately, due to their aggressive timeline, the Barry et al proposal
is incredibly unlikely to meet the requirements of a
multi-billion-dollar system, and continued research into meeting the
spirit, not the text, of their agreement seems warranted.

@_date: 2017-05-26 20:04:58
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
A more important consideration than segwit's timeout is when code can be
released, which will no doubt be several months after SegWit's current
Greg's proposed 6 months seems much more reasonable to me, assuming its
still many months after the formal release of code implementing it.

@_date: 2017-05-26 22:44:44
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Barry Silbert segwit agreement 
While I'm not 100% convinced there are strict technical reasons for needing to wait till after segwit is active before a hard fork can be started (you can, after all, activate segwit as a part of the HF), there are useful design and conservatism reasons (not causing massive discontinuity in fee market, handling major system changes one at a time, etc).
Still, totally agree that attempting to design, code, and test a new hard fork in six months, let alone deploy it, let alone simultaneously with segwit, is a joke and fails to take seriously the investment many have made in the bitcoin system. Previous, rather simple, soft forks required similar if not more development time, not counting deployment and activation time.
If the community is unable to form consensus around segwit alone for political reasons, further research into hard fork design may help, but even forks tied together would nearly certainly need to activate months apart.

@_date: 2017-11-27 11:33:07
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making OP_CODESEPARATOR and FindAndDelete in 
I strongly disagree here - we don't only soft-fork out transactions that
are "fundamentally insecure", that would be significantly too
restrictive. We have generally been willing to soft-fork out things
which clearly fall outside of best-practices, especially rather
"useless" fields in the protocol eg soft-forking behavior into OP_NOPs,
soft-forking behavior into nSequence, etc.
As a part of setting clear best-practices, making things non-standard is
the obvious step, though there has been active discussion of
soft-forking out FindAndDelete and OP_CODESEPARATOR for years now. I
obviously do not claim that we should be proposing a soft-fork to
blacklist FindAndDelete and OP_CODESEPARATOR usage any time soon, and
assume that it would take at least a year or three from when it was made
non-standard to when a soft-fork to finally remove them was proposed.
This should be more than sufficient time for folks using such weird (and
largely useless) parts of the protocol to object, which should be
sufficient to reconsider such a soft-fork.
Independently, making them non-standard is a good change on its own, and
if nothing else should better inform discussion about the possibility of
anyone using these things.

@_date: 2017-11-27 16:33:37
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Making OP_CODESEPARATOR and FindAndDelete in 
Indeed, the PR in question does *not* change the semantics of
OP_CODESEPARATOR within SegWit redeemScripts, where it is still allowed
(and Nicolas Dorier pointed out that he was using it in TumbleBit), so
there are still ways to use it, but only in places, like SegWit, where
the potential validation complexity blowup is massively reduced.
I am not sure that OP_CODESEPARATOR is entirely useless in pre-SegWit
scripts (I believe Nicolas' construction may still be relevant
pre-SegWit), though I strongly believe FindAndDelete is.
I don't think CODESEPARATOR rises to the threshold of it being "widely
known to be useless", but certainly the historical use of it (to
separate the scriptSig and the scriptPubKey in the scriptCode, which was
run as a single concatenated thing in the original design is no longer
relevant). FindAndDelete is equally irrelevant if not significantly more

@_date: 2017-10-30 21:42:44
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
I admittedly haven't had a chance to read the paper in full details, but I was curious how you propose dealing with "jets" in something like Bitcoin. AFAIU, other similar systems are left doing hard-forks to reduce the sigops/weight/fee-cost of transactions every time they want to add useful optimized drop-ins. For obvious reasons, this seems rather impractical and a potentially critical barrier to adoption of such optimized drop-ins, which I imagine would be required to do any new cryptographic algorithms due to the significant fee cost of interpreting such things.
Is there some insight I'm missing here?

@_date: 2017-10-30 18:14:44
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
Are you anticipating it will be reasonably possible to execute more
complicated things in interpreted form even after "jets" are put in
place? If not its just a soft-fork to add new script operations and
going through the effort of making them compatible with existing code
and using a full 32 byte hash to represent them seems wasteful - might
as well just add a "SHA256 opcode".
Either way it sounds like you're assuming a pretty aggressive soft-fork
cadence? I'm not sure if that's so practical right now (or are you
thinking it would be more practical if things were

@_date: 2017-10-30 18:50:04
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
OK, fair enough, just wanted to make sure we were on the same page.
"Thorny issues there and there hasn't been a ton of effort put into what
Bitcoin integration and maintainability looks like" is a perfectly fair
response :)

@_date: 2017-09-10 19:02:36
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
I believe there continues to be concern over a number of altcoins which
are running old, unpatched forks of Bitcoin Core, making it rather
difficult to disclose issues without putting people at risk (see, eg,
some of the dos issues which are preventing release of the alert key).
I'd encourage the list to have a discussion about what reasonable
approaches could be taken there.

@_date: 2017-09-29 01:53:55
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
I'm somewhat curious what the authors envisioned the real-world implications of this model to be. While blindly asking users to enter what they're willing to pay always works in theory, I'd imagine in such a world the fee selection UX would be similar to what it is today - users are provided a list of options with feerates and expected confirmation times from which to select. Indeed, in a world where users pay a lower fee if they paid more than necessary fee estimation could be more willing to overshoot and the UX around RBF and CPFP could be simplified greatly, but I'm not actually convinced that it would result in higher overall mining revenue.
The UX issues with RBF and CPFP, not to mention the UX issues involved in optimizing for quick confirmation are, indeed, quite significant, but I believe them to be solveable with rather striaght-forward changes. Making the market more useable (for higher or lower overall miner revenue) may be a sufficient goal, however, to want to consider something like this.

@_date: 2018-01-14 22:41:55
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Ivy: a higher-level language targeting Bitcoin 
I'm curious if you've considered adding some form of compiler-time enforcement to prevent witness malleability? With that, Ivy could help to resolve for it's users one of the things that can make Bitcoin scripts more complicated to write, instead of simply type-checking and providing a high-level language mapped 1-to-1 with Bitcoin script.

@_date: 2018-01-18 05:00:28
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Or make it a part of your secret-split logic... Gotta love how fast GF(2^8) is:

@_date: 2018-01-23 02:51:51
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Thanks Greg!
I'd be hesitant to deploy a MAST proposal without this clever application of pay-to-contract-hash now! Looks like the overhead over a more-naive MAST construction is rather trivial, too!

@_date: 2018-01-23 21:23:21
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
The issue with that approach without support for the privacy-encouraging wrapper proposed by Greg here is that it encourages adoption halfway and destroys a lot of the value of the apparent-script monoculture for privacy preservation. Greg's proposal here doesn't change the format of any specific MAST implementation, but instead adds the privacy wrapper that I always felt was missing in existing proposals, without any real additional overhead in many use-cases!
Indeed, permissionless innovation is important, but the huge advantage of providing the privacy wrapper by default here is absolutely massive to the ecosystem and should not be handwaved away for vague possibly-advantages.

@_date: 2018-01-27 17:23:12
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Gah, please no. I see no material reason why cross-input signature aggregation shouldn't have the signatures in the first n-1 inputs replaced with something like a single-byte push where a signature is required to indicate aggregation, and the combined signature in the last input at whatever position the signature is required.

@_date: 2018-06-05 14:44:57
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP Proposal] BetterHash Mining Protocol Replacements 
Been working on this one for a while, so its already been through a few
rounds of feeback (thanks to all those who already have provided feedback)!
At a high level, this meets a few goals:
1) Replace getblocktemplate with something that is both more performant
(no JSON encoding, no full transactions sent over the wire to update a
job, hence we can keep the same CTransactionRef in Bitcoin Core making
lots of validation things way faster), more robust for consensus changes
(no need to add protocol changes to add commitments ala SegWit in the
future), and moves more block-switching logic inside of the work
provider (allowing Bitcoin Core to better optimize work switching as it
knows more than an outside pool server, specifically we can play more
games with how we do mempool eviction, empty block mining, and not
mining fresh transactions more easily by moving to a more "push" model
from the normal "pull" getblocktemplate implementation).
2) Replace Stratum with something more secure (sign messages when
applicable, without adding too much overhead to the pool), simpler to
implement (not JSON-wrapped-hex, no 32-byte-swapped-per-4-byte-byteorder
insanity), and better-defined (a clearly written spec, encompassing the
various things shoved backwards into stratum like suggested difficulty
in the password field and device identification by setting user to
"user.device") with VENDOR_MESSAGEs provided for extensibility instead
of conflicting specifications from various different vendors.
3) Provide the ability for a pool to accept work which the users of the
pool selected the transactions for, providing strong decentralization
pressure by removing the network-level centralization attacks pools can
do (or be compromised and used to perform) while still allowing them
full control of payout management and variance reduction.
While (1) and (2) stand on their own, making it all one set of protocols
to provide (3) provides at least the opportunity for drastically better
decentralization in Bitcoin mining in the future.
The latest version of the full BIP draft can be found at
and implementations of the work-generation part at
 and
pool/proxy parts at  (though
note that both implementations are currently on a slightly out-of-date
version of the protocol, I hope to get them brought up to date in the
coming day or two and make them much more full-featured. The whole stack
has managed to mine numerous testnet blocks on several different types
of hardware).

@_date: 2018-06-06 15:16:09
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [BIP Proposal] BetterHash Mining Protocol 
Clients "inspecting and modifying the transactions" is explicitly *not*
supported. There should be more than enough features for clients to get
bitcoind to generate the exact block they want already available via
Bitcoin Core. The only reason transactions are exposed over the work
protocol at all, really, is so that clients can generate weak blocks to
be sent to the pool for efficient client -> pool block relay, not sure
that's worth bothering to add a whole new endpoint for, sounds
needlessly complicated (and the spec is already more than complicated
enough, sadly).

@_date: 2018-06-26 14:44:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BetterHash status 
Things go into production when people decide to adopt them, not before. You're welcome to contribute to the implementation at

@_date: 2018-05-17 11:25:12
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
BIP 158 currently includes the following in the "basic" filter: 1)
txids, 2) output scripts, 3) input prevouts.
I believe (1) could be skipped entirely - there is almost no reason why
you'd not be able to filter for, eg, the set of output scripts in a
transaction you know about and (2) and (3) may want to be split out -
many wallets may wish to just find transactions paying to them, as
transactions spending from their outputs should generally be things
they've created.
In general, I'm concerned about the size of the filters making existing
SPV clients less willing to adopt BIP 158 instead of the existing bloom
filter garbage and would like to see a further exploration of ways to
split out filters to make them less bandwidth intensive. Some further
ideas we should probably play with before finalizing moving forward is
providing filters for certain script templates, eg being able to only
get outputs that are segwit version X or other similar ideas.

@_date: 2018-05-17 11:28:28
@_author: Matt Corallo 
@_subject: [bitcoin-dev] UHS: Full-node security without maintaining a 
Hey Cory,
I'm generally a fan of having an option to "prove a block is valid when
relaying it" instead of "just relay it", but I am concerned that this
proposal is overfitting the current UTXO set. Specifically, because UTXO
entries are (roughly) 32 bytes per output plus 32 bytes per transaction
on disk today, a material increase in batching and many-output
transactions may significantly reduce the UTXO-set-size gain in this
proposal while adding complexity to block relay as well as increase the
size of block data relayed, which can have adverse effects on
propagation. I'd love to see your tests re-run on simulated transaction
data with more batching of sends.

@_date: 2018-05-17 15:46:18
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
(1) can be accomplished by filtering for the set of outputs in the transaction you created. I agree (2) would ideally be done to avoid issues with a copied wallet (theft or not), but I am worried about the size of the filters themselves, not just the size of the blocks downloaded after a match.

@_date: 2018-05-17 12:59:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
Yea I generally would really prefer something like that but it
significantly complicates the download logic - currently clients can
easily cross-check a filter in case they differ between providers by
downloading the block. If we instead went with the script being spent
they would have to be provided all previous transactions (potentially
compressed via midstate) as well, making it potentially infeasible to
identify the offending node while remaining a lightweight client. Maybe
there is some other reasonable download logic to replace it with, however.

@_date: 2018-11-29 19:37:54
@_author: Matt Corallo 
@_subject: [bitcoin-dev] CPFP Carve-Out for Fee-Prediction Issues in 
(cross-posted to both lists to make lightning-dev folks aware, please take lightning-dev off CC when responding).
As I'm sure everyone is aware, Lightning (and other similar systems) work by exchanging pre-signed transactions for future broadcast. Of course in many cases this requires either (a) predicting what the feerate required for timely confirmation will be at some (or, really, any) point in the future, or (b) utilizing CPFP and dependent transaction relay to allow parties to broadcast low-feerate transactions with children created at broadcast-time to increase the effective feerate. Ideally transactions could be constructed to allow for after-the-fact addition of inputs to increase fee without CPFP but it is not always possible to do so.
Option (a) is rather obviously intractible, and implementation complexity has led to channel failures in lightning in practice (as both sides must agree on a reasonable-in-the-future feerate). Option (b) is a much more natural choice (assuming some form of as-yet-unimplemented package relay on the P2P network) but is made difficult due to complexity around RBF/CPFP anti-DoS rules.
For example, if we take a simplified lightning design with pre-signed commitment transaction A with one 0-value anyone-can-spend output available for use as a CPFP output, a counterparty can prevent confirmation of/significantly increase the fee cost of confirming A by chaining a large-but-only-moderate-feerate transaction off of this anyone-can-spend output. This transaction, B, will have a large absolute fee while making the package (A, B) have a low-ish feerate, placing it solidly at the bottom of the mempool but without significant risk of it getting evicted during memory limiting. This large absolute fee forces a counterparty which wishes to have the commitment transaction confirm to increase on this absolute fee in order to meet RBF rules.
For this reason (and many other similar attacks utilizing the package size limits), in discussing the security model around CPFP, we've generally considered it too-difficulty-to-prevent third parties which are able to spend an output of a transaction from delaying its confirmation, at least until/unless the prevailing feerates decline and some of the mempool backlog gets confirmed.
You'll note, however, that this attack doesn't have to be permanent to work - Lightning's (and other contracting/payment channel systems') security model assumes the ability to get such commitment transactions confirmed in a timely manner, as otherwise HTLCs may time out and counterparties can claim the timeout-refund before we can claim the HTLC using the hash-preimage.
To partially-address the CPFP security model considerations, a next step might involve tweaking Lightning's commitment transaction to have two small-value outputs which are immediately spendable, one by each channel participant, allowing them to chain children off without allowng unrelated third-parties to chain children. Obviously this does not address the specific attack so we need a small tweak to the anti-DoS CPFP rules in Bitcoin Core/BIP 125:
The last transaction which is added to a package of dependent transactions in the mempool must:
  * Have no more than one unconfirmed parent,
  * Be of size no greater than 1K in virtual size.
(for implementation sanity, this would effectively reduce all mempool package size limits by 1 1K-virtual-size transaction, and the last would be "allowed to violate the limits" as long as it meets the above criteria).
For contracting applications like lightning, this means that as long as the transaction we wish to confirm (in this case the commitment transaction)
  * Has only two immediately-spendable (ie non-CSV) outputs,
  * where each immediately-spendable output is only spendable by one   * and is no larger than MAX_PACKAGE_VIRTUAL_SIZE - 1001 Vsize,
each counterparty will always be able to independantly CPFP the transaction in question. ie because if the "malicious" (ie transaction-delaying) party bradcasts A with a child, it can never meet the "last transaction" carve-out as its transaction cannot both meet the package limit and have only one unconfirmed ancestor. Thus, the non-delaying counterparty can always independently add its own CPFP transaction, increasing the (A, Tx2) package feerate and confirming A without having to concern themselves with the (A, Tx1) package.
As an alternative proposal, at various points there have been discussions around solving the "RBF-pinning" problem by allowing transactors to mark their transactions as "likely-to-be-RBF'ed", which could enable a relay policy where children of such transactions would be rejected unless the resulting package would be "near the top of the mempool". This would theoretically imply such attacks are not possible to pull off consistently, as any "transaction-delaying" channel participant will have to place the package containing A at an effective feerate which makes confirmation to occur soon with some likelihood. It is, however, possible to pull off this attack with low probability in case of feerate spikes right after broadcast.
Note that this clearly relies on some form of package relay, which comes with its own challenges, but I'll start a separate thread on that.
See-also: lightning-dev thread about the changes to lightning spec required to incorporate this:

@_date: 2018-11-30 19:33:56
@_author: Matt Corallo 
@_subject: [bitcoin-dev] CPFP Carve-Out for Fee-Prediction Issues in 
============================== START ==============================
Hmm, you may be correct that this doesn't (striclty speaking) imply a change to the BIP 125 itself, though the high-level protocol here is likely of interest to the list, as well as likely to generate feedback. Note that in your example, output Z must be CSV-delayed (ie you cannot construct a packeg using that output as it must be spent in a different block than TX0 is confirmed in) in order for the proposal to be secure as otherwise Alice could use output A to pin the transaction, and then "use up" the proposed "last-transaction" rule by spending output Z, leaving Bob unable to spend output B without meeting the (expensive) RBF It was further pointed out to me that while the original mail states that this relies on package relay, this isn't really entirely true. The status quo today may leave a commitment transaction unable to be broadcast if feerates spike much higher than the feerate negotiated at the time of construction. Under this proposal this is not changed, it is only the implementation proposal which implies the commitment transaction feerate negotiation will simply be replaced with a 1sat/vbyte constant which relies on some form of package relay.

@_date: 2018-09-06 13:31:58
@_author: Matt Corallo 
@_subject: [bitcoin-dev] A BIP proposal for transactions that are 
I think a simple approach to what you want to accomplish is to simply have a multisig option with a locktime pre-signed transaction which is broadcastable at the 24h mark and has different spendability. This avoids introducing reorg-induced invalidity.

@_date: 2018-09-06 12:33:38
@_author: Matt Corallo 
@_subject: [bitcoin-dev] A BIP proposal for transactions that are 'cancellable' 
I think you misunderstood my proposal. What you'd do is the transaction
is spendable by either Bob OR (Bob AND Alice) and before
broadcast/during construction/whatever sign a new transaction that
spends it and is only spendable by Alice, but is timelocked for 24
hours. At the 24h mark, Alice broadcasts the transaction and once it is
confirmed only Alice can claim the money.

@_date: 2019-08-14 11:07:19
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
You very clearly didn't bother to read other mails in this thread. To make it easy for you, here's a few links:

@_date: 2019-02-05 12:21:45
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Interrogating a BIP157 server, 
- snip -
 > 1) Introduce a new P2P message to retrieve all prev-outputs for a given
 > block (essentially the undo data in Core), and verify the scripts
 > against the block by executing them. While this permits some forms of
 > input script malleability (and thus cannot discriminate between all
 > valid and invalid filters), it restricts what an attacker can do. This
 > was proposed by Laolu AFAIK, and I believe this is how btcd is I'm somewhat confused by this - how does the undo data help you without seeing the full (mistate compressed) transaction? In (the realistic) thread model where an attacker is trying to blind you from some output, they can simply give you "undo data" where scriptPubKeys are OP_TRUE instead of the real script and you'd be none the wiser.
- snip -
- snip -
Huh? I don't think we should seriously consider only-one-codebase-has-deployed-anything-with-very-limited-in-the-wild-use as "too late into the current deployment"?

@_date: 2019-01-07 15:18:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
Sorry for the late reply.
Hmm, I included the old RBF-pinning proposal as a comparison. Personally, I find it both less clean and less convincingly secure.
Ultimately, defining a "near the top of the mempool" criteria is fraught with issues. While it's probably OK for the original problem (large batched transactions where you don't want a single counterparty to prevent confirmation), lightning's requirements are very different. Instead is wanting a high probability that the transaction in question confirms "soon", we need certainty that it will confirm by some deadline.
Thus, even if you imagine a steady-state mempool growth, unless the "near the top of the mempool" criteria is "near the top of the next block" (which is obviously *not* incentive-compatible), its easy to see how the package would fail to confirm within a handful of blocks given block time variance. Giving up the ability to RBF/CPFP more than once in case the fee moves away from us seems to be a rather significant THe original proposal is somewhat of a hack, but its a hack on the boundary condition where packages meet our local anti-DoS rules in violation of the "incentive compatible" goal anyway (essentially, though miners also care about anti-DoS). This proposal is very different and, similar to how it doesn't work if blocks randomly come in a bit slow for an hour or two, isn't incentive compatible if blocks come in a bit fast for an hour or two, as all of a sudden that "near the top of the mempool" criteria makes no sense and you should have accepted the new As for package relay, indeed, we can probably do soemthing simpler for this specific case, but itdepends on what the scope of that design is. Suhas opened an issue to try to scope it out a bit more at

@_date: 2019-01-08 09:46:45
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I responded to a few things in-line before realizing I think we're out of sync on what this alternative proposal actually implies. In my understanding is it, it does *not* imply that you are guaranteed the ability to RBF as fees change. The previous problem is still there - your counterparty can announce a bogus package and leave you unable to add a new transaction to it, the difference being it may be significantly more expensive to do so. If it were the case the you could RBF after the fact, I would likely agree with you.
I strongly disagree. If you're someone sending a batched payment, 5% chance it takes 13 blocks is perfectly acceptable. If you're a lightning operator, that quickly turns into "5% chance, or 35% chance if your counterparty is malicious and knows more about the market structure than you". Eg in the past it's been the case that transaction volume would spike every day at the same time when Bitmex proceed a flood of withdrawals all at once in separate transactions. Worse, it's probably still the case that, in case is sudden market movement, transaction volume can spike while people arb exchanges and move coins into exchanges to sell.
My point was, because of block time variance, even that criteria doesn't hold up. If you assume a steady flow of new transactions and one or two blocks come in "late", suddenly "top 4MWeight" isn't likely to get confirmed until a few blocks come in "early". Given block variance within a 12 block window, this is a relatively likely scenario.

@_date: 2019-07-20 17:46:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
Just a quick heads-up for those watching the list who may be using it -
in the next Bitcoin Core release bloom filter serving will be turned off
by default. This has been a long time coming, it's been an option for
many releases and has been a well-known DoS vector for some time.
As other DoS vectors have slowly been closed, this has become
increasingly an obvious low-hanging fruit. Those who are using it should
already have long been filtering for NODE_BLOOM-signaling nodes, and I
don't anticipate those being gone any time particularly soon.
See-also PR at The release notes will liekly read:
P2P Changes
- The default value for the -peerbloomfilters configuration option (and,
thus, NODE_BLOOM support) has been changed to false.
  This resolves well-known DoS vectors in Bitcoin Core, especially for
nodes with spinning disks. It is not anticipated that
  this will result in a significant lack of availability of
NODE_BLOOM-enabled nodes in the coming years, however, clients
  which rely on the availability of NODE_BLOOM-supporting nodes on the
P2P network should consider the process of migrating
  to a more modern (and less trustful and privacy-violating) alternative
over the coming years.

@_date: 2019-07-22 05:01:58
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
Hey Andreas,
I think maybe some of the comments here were misunderstood - I don't
anticipate that most people will change their defaults, indeed, but
given the general upgrade cycles we've seen on the network over the
entire course of Bitcoin's history, there's little reason to believe
that many nodes with NODE_BLOOM publicly accessible will be around for
at least three or four years to come, though obviously any conscious
effort by folks who need those services to run nodes could extend that
As for the DoS issues, a super old Proof-of-Concept of the I/O variant
is here:  though CPU DoS
attacks are also possible that use high hash counts to fill a node's CPU
usage (you can pretty trivially see when a bloom-based peer connects to
you just by looking at top...).
Finally, regarding alternatives, the filter-generation code for BIP
157/158 has been in Bitcoin Core for some time, though the P2P serving
side of things appears to have lost any champions working on it. I
presume one of the Lightning folks will eventually, given they appear to
be requiring their users connect to a handful of their own servers right
now, but if you really need it, its likely not a ton of work to pipe
them through.

@_date: 2019-07-27 12:10:03
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bitcoin Core to disable Bloom-based Filtering by 
This conversation went off the rails somewhat. I don't think there's any immediate risk of NODE_BLOOM peers being unavailable. This is a defaults change, not a removal of the code to serve BIP 37 peers (nor would I suggest removing said code while people still want to use them - the maintenance burden isn't much). Looking at historical upgrade cycles, ignoring any other factors, there will be a large number of nodes serving NODE_BLOOM for many years.
Even more importantly, if you need them, run a node or two. As long as no one is exploiting the issues with them such a node isn't *too* expensive. Or don't, I guarantee you chainanalysis or some competitor of theirs will very very happily serve bloom-filtered clients as long as such clients want to deanonymize themselves. We already see a plurality of nodes on the network are clearly not run-of-the-mill Core nodes, many of which are likely deanonimization efforts.
In some cases BIP 137 is a replacement, in some cases, indeed, it is not. I agree at a protocol level we shouldn't be passing judgement about how users wish to interact with the Bitcoin system (aside from not putting our own, personal, effort into building such things) but that isn't what's happening here. This is an important DoS fix for the average node, and I don't really understand the argument that this is going to break existing BIP 37 wallets, but if it makes you feel any better I can run some beefy BIP 37 nodes.

@_date: 2019-06-03 11:48:31
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
I think this needs significantly improved motivation/description. A few areas I'd like to see calculated out:
1) wrt rule 3, for this to be obviously-incentive-compatible-for-the-next-miner, I'd think no evicted transactions would be allowed to be in the next block range. This would probably require some significant additional tracking in today's mempool logic.
2) wrt rule 4, I'd like to see a calculation of worst-case free relay. I think we're already not in a great place, but maybe it's worth it or maybe there is some other way to reduce this cost (intuitively it looks like this proposal could make things very, very, very bad).
3) wrt rule 5, I'd like to see benchmarks, it's probably a pretty nasty DoS attack, but it may also be the case that is (a) not worse than other fundamental issues or (b) sufficiently expensive.
4) As I've indicated before, I'm generaly not a fan of such vague protections for time-critical transactions such as payment channel punishment transactions. At a high-level, in this context your counterparty's transactions (not to mention every other transaction in everyone's mempool) are still involved in the decision about whether to accept an RBF, in contrast to previous proposals, which makes it much harder to reason about. As a specific example, if an attacker exploits mempool policy differences they may cause your concept of "top 4M weight" to be bogus for a subeset of nodes, causing propogation to be limited.
Obviously there is also a ton more client-side knowledge required and complexity to RBF decisions here than other previous, more narrowly-targeted proposals.
(I don't think this one use-case being not optimal should prevent such a proposal, i agree it's quite nice for some other cases).

@_date: 2019-03-06 21:39:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP Proposal: The Great Consensus Cleanup 
The following is a proposed BIP to soft-fork out some oddities in the current Bitcoin consensus rules, resolving several vulnerabilities, in addition to fixing the timewarp vulnerability. I'd like to ask the BIP editor to assign a BIP number.
The latest version of the BIP can be found at  (a text copy is included below).
Some things that may be worth discussing:
  * Note that the activation times in this BIP may result in the activation of the new soft-fork rules on the same block as the scheduled block-subsidy halving. Sadly, avoiding this either requires a significantly compressed BIP activation time (which may result in the rules not activating for benign reasons) or beginning the activation process significantly into the future.
  * The BIP proposes allowing timestamps on the difficulty-adjustment block to go backwards by 600 seconds which has the nice property of making the difficulty-adjustment algorithm target almost exactly one block per 600 seconds in the worst-case (where miners are attempting to exploit the timewarp attack), while avoiding any potential hardware bricking (assuming upgrades on the part of mining pools). Alternatively, some have proposed allowing the time to go backwards 7200 seconds, which introduces some small level of inflation in the case of a miner attack (though much less than we've had historically simply due to the rapidly growing hashrate) but avoids any requirements for upgrades as the existing 7200-second-in-the-future check implies miners will only ever build on blocks for which they can set the next timestamp to their current time.
  * The 4th change (making non-standard signature hash types invalid) may be worth discussing. In order to limit the number of potential signature hashes which could be used per-input (allowing us to cache them to avoid re-calculation), we can disable non-standard sighash types. Alternatively, however, most of the same effect could be achieved by caching the just-before-the-last-byte sighash midstate and hashing only the last byte when a checking signatures. Still, them having been non-standard for many years makes me doubt there is much risk involved in disabling them, and I don't see much potential use-case for keeping them around so I'd like to just remove them.
As for why the timewarp vulnerability should (IMO rather obviously) be fixed, it seems rather clear that the only potential use for exploiting it would be either to inflate the currency supply maliciously by miners or to fork in what amounts to extension blocks. As for why extension blocks are almost certainly not the right approach to such changes, its likely worth reading this old post: BIP: XXXX
Layer: Consensus (soft fork)
Title: The Great Consensus Cleanup
Author: Matt Corallo
Status: Draft
Type: Standards Track
Created: 2019-01-28
License: PD
This BIP defines a set of consensus changes which reduce the complexity of Bitcoin implementations and improve worst-case validation times, fixing a number of long-standing vulnerabilities.
BIP 143 significantly improved certain aspects of Bitcoin's consensus rules, key to this being changes to the format of the data which is hashed and signed in CHECKSIG operations during script execution. However, several improvements were left for later forks to avoid bloating the original activation with unrelated changes. This BIP seeks to make some of these changes as well as a few other simplifications. Specifically, this BIP proposes the following changes:
* Worst-case validation time for non-BIP 143 transactions has long been considered a significant vulnerability. To address this, both OP_CODESEPARATOR in non-BIP 143 scripts and FindAndDelete fail script validation, among other cleanups. This drastically reduces worst-case validation time for non-BIP 143 transactions by enabling Signature Hash caching on a per-input basis. While validation time of large, simple non-BIP 143 transactions can still be excessively high on their own, removing these multipliers goes a long way towards resolving the issue.
* By further restricting nTime fields on difficulty adjustment blocks, we propose fixing the long-standing "timewarp" inflation vulnerability in Bitcoin's difficulty adjustment without risking existing mining hardware becoming unusable. This limits the worst-case difficulty adjustment target in case of attack from the current exponential growth, to once every roughly 600 seconds. Note that no change in default behavior is proposed, keeping the existing target of one block every ~600.6 seconds[1] in the common case (ie we limit the attack scenario to about a 0.1% inflation rate, much smaller than the historical inflation rate due to rapid hashrate growth).
* Several vulnerabilities where Bitcoin clients needed to check for specific cases of malleation in the merkle tree construction are resolved by making certain transaction sizes invalid.
Upon activation, the following rules will be enforced on all new blocks:
* scriptSigs which contain non-push opcodes fail the script validation. Push opcodes are OP_0 - OP_1NEGATE and OP_1 - OP_16. Note that this implies any opcodes in scriptSigs greater than 0x60 will fail script validation, in addition to OP_RESERVED (0x50, which already fails script execution in executed branches, though all branches are now guaranteed to execute).
* OP_CODESEPARATOR in non-BIP 143 scripts fails the script validation. This includes OP_CODESEPARATORs in unexecuted branches of if statements, similar to other disabled opcodes, but unlike OP_RETURN.
* When validating signatures in non-BIP 143 scripts, if the scriptPubKey being executed contains, pushed as a single element using minimal PUSHDATA, a signature stack element being validated, the script fails validation. For the avoidance of doubt, any FindAndDelete matches result in script execution failure.
* If the sighash type byte (ie last byte in a signature being evaluated during the execution of OP_CHECKSIG[VERIFY] or OP_CHECKMULTISIG[VERIFY]) is anything other than 1, 2, 3, 0x81, 0x82, or 0x83, the script execution fails. This does not apply to 0-length signature stack elements.
* Transactions smaller than 65 bytes when serialized without witness data are invalid.
* The nTime field of each block whose height, mod 2016, is 0 must be greater than or equal to the nTime field of the immediately prior block minus 600. For the avoidance of doubt, such blocks must still comply with existing Median-Time-Past nTime restrictions.
This BIP will be deployed by "version bits" BIP9 with the name "cleanups" and using bit (0-indexed) 3.
For Bitcoin mainnet, the BIP9 starttime will be midnight August 1st, 2019 UTC (Epoch timestamp 1564617600) and BIP9 timeout will be midnight August 1st, 2020 UTC (Epoch timestamp 1596240000).
For Bitcoin testnet, the BIP9 starttime will be midnight June 1st, 2019 UTC (Epoch timestamp 1559347200) and BIP9 timeout will be midnight June 1st, 2020 UTC (Epoch timestamp 1590969600).
* There are very few known uses for OP_CODESEPARATOR and none for FindAndDelete. None of these uses enable new functionality, and any efficiency gains are better made by switching to BIP 141. Further, there is no known use of either on the chain today, and both have been non-standard in Bitcoin Core since version 0.16.1, making them much more difficult to have mined. Both changes, together, allow for signature hash caching within each input script in a non-BIP 143 transaction today. Note that due to their non-standardness, miners using Bitcoin Core version 0.16.1 or later will not mine blocks which violate these rules today.
* Reducing valid scriptSigs to the minimal set of operations which can generate any stack state removes the requirement that scriptCodes need to be generated for scriptSig execution, reducing the possible set of scriptCodes which must be cached per input by 2x. Because any stack state can be created using only push opcodes, this does not reduce spendability except for pessimal scriptPubKeys which require a significant number of identical stack elements (ie created using OP_DUP). Note that such transactions have been non-standard in Bitcoin Core since before git history (SVN 197) and thus miners running Bitcoin Core will not mine such transactions today.
* Further, disabling non-canonical sighash types allows caching of the sighash themselves instead of midstates (as the sighash type byte is included in the sighash itself). Avoiding applying this rule to 0-length signatures avoids breaking deliberate OP_CHECKSIG failures while still avoiding having to ever calculate such sighashes. Such sighashes have been non-standard and thus miners using Bitcoin Core version 0.8 or higher will not mine blocks containing such transactions today.
* While there are no known attempts to exploit the "timewarp" vulnerability on Bitcoin's mainnet today, and the authors do not believe it is likely to occur in the immediate future, removing the possibility has long been on various wishlists and greatly simplifies potential attack analysis.
** Sadly, some deployed mining hardware relies on the ability to roll nTime forward by up to 600 seconds[3]. Thus, only requiring that the nTime field move forward during difficulty adjustment would allow a malicious miner to prevent some competitors from mining the next block by setting their timestamp to two hours in the future. Thus, we allow nTime to go backwards by 600 seconds, ensuring that even a block with a timestamp two hours in the future allows for 600 seconds of nTime rolling on the next block.
** Note that miners today only enforce increasing timestamps against the median-timestamp-of-last-11-blocks, so miners who do not upgrade may mine a block which violates this rule at the beginning of a difficulty window if the last block in a difficulty window has a timestamp in the future. Thus, it is strongly recommended that SPV clients enforce the new nTime rules to avoid following any potential forks which occur.
* The issues involved in having leaf nodes in the transaction merkle tree which can be confused for inner nodes are well documented. [4][5][6] While there are workarounds for the pitfalls, there are many SPV-proof-validators which do not implement them. Further, the limited use-cases for very small transactions does not suffice as reason to force the added complexity onto clients. Note that any transactions smaller than 83 bytes have been considered non-standard since Bitcoin Core version 0.17.0, so miners will not mine blocks which validate this rule by default.
* There are several early-stage proposals which may affect the execution of scripts, including proposals such as Schnorr signatures, Taproot, Graftroot, and MAST. These proposals are not expected to have any interaction with the changes in this BIP, as they are likely to only apply to SegWit scripts, which are not covered by any of the new rules except for the sighash type byte rule. Thus, the sighash type byte rule defined above only applies to *current* signature-checking opcodes, as any new signature-checking is likely to be implemented via the introduction of new opcodes.
* In spite of some suggestion that other activation methods be used, BIP 9 is proposed as ensuring miners have upgraded to enforce new rules is an important part of minimizing disruption. While previous BIP 9 soft-forks have resulted in political contention, this comparatively-unimportant soft-fork provides a good opportunity to attempt to return to utilizing BIP 9 to ensure miner upgrade prior to activation, which the authors believe is a critical goal. However, if there is broad agreement to activate these rules when the BIP 9 expiry time is reached, and miners have not yet signaled sufficient level of readiness, a later flag-day activation may be merited. For this reason, implementations may wish to provide a compatibility option which allows flag-day enforcement of these rules without an update.
==Reference Implementation==
[ Bitcoin Core Pull [1] The difficulty adjustment algorithm in Bitcoin multiplies the previous difficulty by (2016 / time taken to mine the last 2015 blocks). Intuitively[2], this implies the actual Inter-Block-Time (IBT) target is 2016/2015*600, or about 600.3 seconds. However, the expected value of the inverse of an Erlang distribution (which the above is effectively sampling from) is actually 1/(N-1), not 1/N. Thus, the above expression actually targets an IBT of 2016/2014*600, or about 600.6 seconds, ie E(2016*600/X) = 1 where X~ErlangDistribution(k=2015, ?=1/IBT) when IBT is 2016/2014*600. This is equivalent to 600*E(2016*600/X) where X~ErlangDistribution(k=2015, ?=1/600). In the case of a miner deliberately reducing timestamps by 600 seconds on the difficulty-retargeting block, we are effectively changing the difficulty multiplier to (2016 / (time taken to mine the last 2016 blocks + 600)), or 600*E(2016*600/(X + 600)) where X~Erlang Distribution(k=2016, ?=1/600), which is effectively targeting an inter-block time of ~599.9999 seconds.
[2] See [ for most peoples' intuition. For more info see Pieter's writeup at [3] While no official stratum specification exists, the btc.com pool server (one of the most popular pool servers today) rejects shares with timestamps more than 600 seconds in the future at [ While there are few resources describing hardware operation today, timestamp rolling can be observed on the chain (in some rare cases) as block timestamps go backwards when a miner rolled one block nTime forward and the next does not, but only incredibly rarely more than 600 [4] [5] [6] Thanks (in alphabetical order) to Suhas Daftuar, James Hilliard, Johnson Lau, Steve Lee, Greg Maxwell, John Newberry, and Pieter Wuille for their helpful feedback at various stages as well as the entire Bitcoin Protocol Development Community.

@_date: 2019-03-07 19:44:23
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP Proposal: The Great Consensus Cleanup 
Replies inline.
Oops, I guess most of the "Discussion" section can just be moved into a "Backwards Compatibility" section. Will do before PR'ing.
This refers to the following spec change:
If the sighash type byte (ie last byte in a signature being evaluated during the execution of OP_CHECKSIG[VERIFY] or OP_CHECKMULTISIG[VERIFY]) is anything other than 1, 2, 3, 0x81, 0x82, or 0x83, the script execution fails. This does not apply to 0-length signature stack elements.
I agree they are somewhat separate ideas, but the arguments in that thread apply equally to timewarp-based inter-block-time reductions. If you want to discuss it further, I'd suggest a new thread.
Will add.
Note that you inherently have to use a new opcode for such things - the non-standard type bytes *are* defined and define a sighash/signature, they can't be simply redefined to a new sighash/signature type in a soft

@_date: 2019-03-07 19:50:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Replies inline.
This is true, and yet it does not appear to actually be practically usable. Thus far, despite a ton of effort, I have not yet seen a practical use-case for OP_CODESEPARATOR (except for one example of it being used to make SegWit scripts ever-so-slightly more effecient in TumbleBit, hence why this BIP does not propose disabling it for SegWit).
(1) It has been well documented again and again that there is desire to remove OP_CODESEPARATOR, (2) it is well-documented OP_CODESEPARATOR in non-segwit scripts represents a rather significant vulnerability in Bitcoin today, and (3) lots of effort has gone into attempting to find practical use-cases for OP_CODESEPARATOR's specific construction, with no successes as of yet. I strongly, strongly disagree that the highly-unlikely remote possibility that someone created something before which could be rendered unspendable is sufficient reason to not fix a vulnerability in Bitcoin today.
You could equally argue, however, that any such limit could render some moderately-large transaction unspendable, so I'm somewhat skeptical of this argument. Note that OP_CODESEPARATOR is non-standard, so getting them mined is rather difficult in any case.

@_date: 2019-03-07 19:57:29
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Sighash Type Byte; 
I can't say I'm particularly married to this idea (hence the alternate proposal in the original email), but at the same time the lack of existing transactions using these bits (and the redundancy thereof - they don't *do* anything special) seems to be pretty strong indication that they are not in use. One could argue a similarity between these bits and OP_NOPs - no one is going to create transactions that require OP_NOP execution to be valid as they are precisely the kind of thing that may get soft-forked to have a new meaning. While the sighash bits are somewhat less candidates for soft-forking, I don't think "someone may have shoved random bits into parts of their locked-for-more-than-a-year transactions" is sufficient reason to not soft-fork something out. Obviously, actually *seeing* it used in practice or trying to fork them out in a fast manner would be unacceptable, but neither is being proposed here.

@_date: 2019-03-08 18:35:42
@_author: Matt Corallo 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Replies inline.
The common way to set that up is to have a separate key, but, ok, fair enough. That said, the argument that "it may be hidden by P2SH!" isn't sufficient here. It has to *both* be hidden by P2SH and have never been spent from (either on mainnet or testnet) or be lock-timed a year in the future. I'm seriously skeptical that someone is using a highly esoteric scheme and has just been pouring money into it without ever having tested it or having withdrawn any money from it whatsoever. This is just a weird argument.
You're already arguing that someone has such an esoteric use of script, suggesting they aren't *also* creating pre-signed, long-locktimed transactions with many inputs isn't much of a further stretch (especially since this may result in the fee being non-standardly low if you artificially increase its weight).
Note that "just limit number of OP_CODESEPARATOR calls" results in a ton of complexity and reduces the simple analysis that fees (almost) have today vs just removing it allows us to also remove a ton of code.
Further note that if you don't remove it getting the efficiency wins right is even harder because instead of being able to cache sighashes you now have to (at a minimum) wipe the cache between each OP_CODESEPARATOR call, which results in a ton of additional implementation complexity.
Huh?! The whole point of non-standardness in this context is to (a) make soft-forking something out safer by derisking miners not upgrading right away and (b) signal something that may be a candidate for soft-forking out so that we get feedback. Who is getting things disabled who isn't bothering to *tell* people that their use-case is being hurt?!

@_date: 2019-03-08 15:14:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Aside from the complexity issues here, note that for a user to be adversely affect, they probably have to have pre-signed lock-timed transactions. Otherwise, in the crazy case that such a user exists, they should have no problem claiming the funds before activation of a soft-fork (and just switching to the swgwit equivalent, or some other equivalent scheme). Thus, adding additional restrictions like tx size limits will equally break txn.

@_date: 2019-03-08 15:20:49
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Signet 
To make testing easier, it may make sense to keep the existing block header format (and PoW) and instead apply the signature rules to some field in the coinbase transaction. This means SPV clients (assuming they only connect to honest/trusted nodes) work as-is.
A previous idea regarding reorgs (that I believe Greg came up with) is to allow multiple keys to sign blocks, with one signing no reorgs and one signing a reorg every few blocks, allowing users to choose the behavior they want.

@_date: 2019-03-11 22:23:33
@_author: Matt Corallo 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
I think you may have misunderstood part of the motivation. Yes, part of the motivation *is* to remove OP_CODESEPARATOR wholesale, greatly simplifying the theoretical operation of checksig operations (thus somewhat simplifying the implementation but also simplifying analysis of future changes, such as sighash-caching code).
I think a key part of the analysis here is that no one I've spoken to (and we've been discussing removing it for *years*, including many attempts at coming up with reasons to keep it) is aware of any real proposals to use OP_CODESEPARATOR, let alone anyone using it in the wild. Hiding data in invalid pubic keys is a long-discussed-and-implemented idea (despite it's discouragement, not to mention it appears on the chain in many places).
It would end up being a huge shame to have all the OP_CORESEPARATOR mess left around after all the effort that has gone into removing it for the past few years, especially given the stark difference in visibility of a fork when compared to a standardness change.
As for your specific proposal of increasing the weight of anything that has an OP_CODESEPARATOR in it by the cost of an additional (simple) input, this doesn't really solve the issue. After all, if we're assuming some user exists who has been using sending money, unspent, to scripts with OP_CODESEPARATOR to force signatures to commit to whether some other signature was present and who won't see a (invariably media-covered) pending soft-fork in time to claim their funds, we should also assume such a user has pre-signed transactions which are time-locked and claim a number of inputs and have several paths in the script which contain OP_CODESEPARATOR, rendering their transcription invalid.

@_date: 2019-03-12 17:08:39
@_author: Matt Corallo 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Note that even your carve-outs for OP_NOP is not sufficient here - if you were using nSequence to tag different pre-signed transactions into categories (roughly as you suggest people may want to do with extra sighash bits) then their transactions could very easily have become un-realistically-spendable. The whole point of soft forks is that we invalidate otherwise-unused bits of the protocol. This does not seem inconsistent with the proposal here.

@_date: 2019-05-21 19:41:22
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
If we're going to do covenants (and I think we should), then I think we
need to have a flexible solution that provides more features than just
this, or we risk adding it only to go through all the effort again when
people ask for a better solution.

@_date: 2019-11-07 19:41:54
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Given the issue is in the address format, not the consensus/standardness layer, it does seem somewhat strange to jump to addressing it with a consensus/standardness fix. Maybe the ship has sailed, but for the sake of considering all our options, we could also redefine bech32 to not allow such addresses.

@_date: 2019-11-11 01:02:15
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Seems good to me, though I'm curious if we have any (even vaguely)
immediate need for non-32/20-byte Segwit outputs? It seems to me this
can be resolved by just limiting the size of bech32 outputs and calling
it a day - adding yet another address format has very significant
ecosystem costs, and if we don't anticipate needing it for 5 years (if
at all)...lets not jump to pay that cost.

@_date: 2019-11-17 18:42:03
@_author: Matt Corallo 
@_subject: [bitcoin-dev] v3 onion services 
There is effort ongoing to upgrade the Bitcoin P2P protocol to support other address types, including onion v3. There are various posts on this ML under the title ?addrv2?. Further review and contributions to that effort is, as always, welcome.

@_date: 2019-10-14 22:33:53
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Is Signet Bitcoin? 
Indeed, Signet is no less (or more) Bitcoin than a seed format or BIP 32. It?s ?not Bitcoin? but it?s certainly ?interoperability for how to build good testing for Bitcoin?.

@_date: 2019-10-24 21:25:14
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I may be missing something, but I'm not sure how this changes anything?
If you have a commitment transaction, you always need at least, and
exactly, one non-CSV output per party. The fact that there is a size
limitation on the transaction that spends for carve-out purposes only
effects how many other inputs/outputs you can add, but somehow I doubt
its ever going to be a large enough number to matter.

@_date: 2019-10-25 07:30:41
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I don?te see how? Let?s imagine Party A has two spendable outputs, now they stuff the package size on one of their spendable outlets until it is right at the limit, add one more on their other output (to meet the Carve-Out), and now Party B can?t do anything.

@_date: 2020-04-20 22:43:14
@_author: Matt Corallo 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing Interest 
[Hi bitcoin-dev, in lightning-land we recently discovered some quite frustrating issues which I thought may merit
broader discussion]
While reviewing the new anchor outputs spec [1] last week, I discovered it introduced a rather nasty ability for a user
to use RBF Pinning to steal in-flight HTLCs which are being enforced on-chain. Sadly, Antoine pointed out that this is
an issue in today's light as well, though see [2] for qualifications. After some back-and-forth with a few other
lightning folks, it seems clear that there is no easy+sane fix (and the practicality of exploitation today seems
incredibly low), so soliciting ideas publicly may be the best step forward.
I've included lots of background for those who aren't super comfortable with lightning's current design, but if you
already know it well, you can skip at least background 1 & 2.
Background - Lightning's Transactions (you can skip this)
As many of you likely know, lightning today does all its update mechanics through:
 a) a 2-of-2 multisig output, locking in the channel,
 b) a "commitment transaction", which spends that output: i) back to its owners, ii) to "HTLC outputs",
 c) HTLC transactions which spend the relevant commitment transaction HTLC outputs.
This somewhat awkward third layer of transactions is required to allow HTLC timeouts to be significantly lower than the
time window during which a counterparty may be punished for broadcasting a revoked state. That is to say, you want to
"lock-in" the resolution of an HTLC output (ie by providing the hash lock preimage on-chain) by a fixed block height
(likely a few hours from the HTLC creation), but the punishment mechanism needs to occur based on a sequence height
(possibly a day or more after transaction broadcast).
As Bitcoin has no covanents, this must occur using pre-signed transactions - namely "HTLC-Success" and "HTLC-Timeout"
transactions, which finalize the resolution of an HTLC, but have a sequence-lock for some time during which the funds
may be taken if they had previously been revoked. To avoid needless delays, if the counterparty which did *not*
broadcast the commitment transaction wishes to claim the HTLC value, they may do so immediately (as there is no reason
to punish the non-broadcaster for having *not* broadcasted a revoked state). Thus, we have four possible HTLC
resolutions depending on the combination of which side broadcast the HTLC and which side sent the HTLC (ie who can claim
it vs who can claim it after time-out):
 1) pre-signed HTLC-Success transaction, providing the preimage in the witness and sent to an output which is sequence-
    locked for some time to provide the non-broadcasting side the opportunity to take the funds,
 2) pre-signed HTLC-Timeout transaction, time-locked to N, providing no preimage, but with a similar sequence lock and
    output as above,
 3) non-pre-signed HTLC claim, providing the preimage in the witness and unencumbered by the broadcaster's signature,
 4) non-pre-signed HTLC timeout, OP_CLTV to N, and similarly unencumbered.
Background 2 - RBF Pinning (you can skip this)
Bitcoin Core's general policy on RBF transactions is that if a counterparty (either to the transaction, eg in lightning,
or not, eg a P2P node which sees the transaction early) can modify a transaction, especially if they can add an input or
output, they can prevent it from confirming in a world where there exists a mempool (ie in a world where Bitcoin works).
While this is somewhat unintuitive, there are any number of good anti-DoS reasons for this, eg:
 * (ok, this is a bad reason, but) a child transaction could be marked 'non-RBF', which would mean allowing the parent
   be RBF'd would violate the assumptions those who look at the RBF opt-in marking make,
 * a parent may be very large, but low feerate - this requires the RBF attempt to "pay for its own relay" and include a
   large absolute fee just to get into the mempool,
 * one of the various package size limits is at its maximum, and depending on the structure of the package the
   computational complexity of calculation evictions may be more than we want to do for a given transaction.
Background 3 - "The RBF Carve-Out" (you can skip this)
In today's lightning, we have a negotiation of what we expect the future feerate to be when one party goes to close the
channel. All the pre-signed transactions above are constructed with this fee-rate in mind, and, given they are all
pre-signed, adding additional fee to them is not generally an option. This is obviously a very maddening prediction
game, especially when the security consequences for negotiating a value which is wrong may allow your counterparty to
broadcast and time out HTLCs which you otherwise have the preimage for. To remove this quirk, we came up with an idea a
year or two back now called "anchor outputs" (aka the RBF carve-out for those in Bitcoin-land) - a neat trick to allow
both counterparties to add fees to a transaction which is being broadcast without getting into the quagmire that is RBF
pinning. Specifically, we added a rule to Bitcoin Core which allows for transactions which have a narrow structure to be
CPFP'd trivially by either counterparty, irrespective of what the other counterparty does! In order to meet this
structure, the commitment transaction (b) must have two (potentially-)additional outputs, each which only one side can
spend, and every other output must have a CSV lock associated with it. This is great and there is (finally) movement to
deploy this.
RBF Pinning HTLC Transactions (aka "Oh, wait, I can steal funds, how, now?")
You'll note that in the discussion of RBF pinning we were pretty broad, and that that discussion seems to in fact cover
our HTLC outputs, at least when spent via (3) or (4). It does, and in fact this is a pretty severe issue in today's
lightning protocol [2]. A lightning counterparty (C, who received the HTLC from B, who received it from A) today could,
if B broadcasts the commitment transaction, spend an HTLC using the preimage with a low-fee, RBF-disabled transaction.
After a few blocks, A could claim the HTLC from B via the timeout mechanism, and then after a few days, C could get the
HTLC-claiming transaction mined via some out-of-band agreement with a small miner. This leaves B short the HTLC value.
You'll note that B would be just fine if they had a way to safely monitor the global mempool, and while this seems like
a prudent mitigation for lightning implementations to deploy today, it is itself a quagmire of complexity, especially
when you consider differences in relay policy during an upgrade cycle and how those may effect propagation through the
P2P network. Further, this is a really obnoxious assumption to hoist onto lightning nodes - having an active full node
with an in-sync mempool is a lot more CPU, bandwidth, and complexity than most lightning users were expecting to face.
It seems highly likely we could come up with some kind of variant of of the RBF Carve-Out to solve this problem, though
probably much more specific to this particular transaction structure - imagine a rule which allowed B to RBF C's low-fee
HTLC claim transaction, without ever seeing it. This could be accomplished by locking down the transaction types in (3)
and (4) by pre-signing them (just, like (1) and (2)) and then using some kind of policy rule to allow only the addition
of additional confirmed inputs and one (small) output. This would mean that B knows that either C's transaction has high
fee, or B's reasonably-higher-fee transaction will meet the RBF rules and replace C's maliciousness.
While the original RBF Carve-Out was a little awkward, its structure was sufficiently generic that other off-chain
protocols could reasonably (need to) take advantage of it, however a rule to address this issue seems like it would need
to be highly tailored to lightning, which doesn't seem acceptable (there appears to be a way to shoehorn the existing
carve-out, but it results in a ton of extra on-chain volume).
Strategies involving full-RBF for transactions not at the top of the mempool, (slow-)full-mempool-sync allowing
mempool-total-fee decreases and relaxations of the RBF rules would be welcome, but without a ton of legwork to include
things like package relay I'm not convinced they would suffice. This of course doesn't even account for the possibility
of similar issues given rely policy differences.
PS For Lightning-Dev Folks (aka "An Alternative Anchor Proposal")
Given the anchor outputs proposal seeks to expand lightning's security in a world where Bitcoin is running at
steady-state and the mempool reliably has transactions in it, not fixing this issue seems to render the whole exercise
somewhat useless (not to mention that the current design makes this attack more obvious and provides several alternative
paths to exploitation).
An alternative, albeit not ideal anchor outputs proposal is as follows:
 * Instead of making the HTLC output spending more free-form with SIGHASH_ANYONECAN_PAY|SIGHASH_SINGLE, we clearly need
   to go the other direction - all HTLC output spends need to be pre-signed.
 * Sadly, and this really hurts from an on-chain-tx-size perspective, we have to include anchor outputs in the HTLC
   transactions (intuitively I think all of them, but at least HTLC-fulfilling transactions definitely).
 * Our poor B, being exploited, above, will attempt to spend their anchor output with a CPFP even if they aren't sure C
   has broadcast the HTLC-Success transaction! This is fine as B already knows the txid, and just wants to learn whats
   in the witness (assuming there is one).
For those from bitcoin-dev still reading who are thinking "blah, you clearly don't need anything else, you have a
solution!" we're talking about extra outputs out the wazoo for hopefully-unnecessary edge cases involving transactions
entering the mempool which a user wants to avoid confirming! This severely cuts into the lowest-value HTLCs which can be
sent "safely" and adds a significant social cost of extra low-value, possibly-uneconomical outputs in the chain.
Still, lacking a better idea, and with a strong desire to make lightning's security more practical in a world where
Bitcoin miners are paid to operate, we should probably start considering moving forward with this.
Thanks for reading,
[1] [2] I'll note that while it is a "severe issue", given the general issues with fee-prediction described in background 3,
its pretty hard to argue its really in the scope of the security model of lightning today. If there were an easy fix to
it, we'd have deployed it by now in response to private discussion, but, sadly, there is not.

@_date: 2020-04-22 12:50:46
@_author: Matt Corallo 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
A few replies inline.
This seems like a somewhat unnecessary drive-by insult of a project you don't contribute to, but feel free to start with
a concrete suggestion here :).
Because watching your own mempool is not guaranteed to work, and during upgrade cycles that include changes to the
policy rules an attacker could exploit your upgraded/non-upgraded status to perform the same attack.
If mempool-watching were practical, maybe, though there are a number of folks who are talking about designing
partially-offline local lightning hubs which would be rendered impractical.
Hmm, maybe the proposal wasn't clear. The idea isn't to add signatures to braodcasted transactions, but instead to CPFP
a maybe-broadcasted transaction by sending a transaction which spends it and seeing if it is accepted. You only need to
know the transaction's exact format (ie txid, which we do, since we sent a signature for it long ago) to do this, you
don't have to actually *have* the fully-signed transaction (and you don't).

@_date: 2020-04-22 12:56:38
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Hmm, indeed, though note that (IIRC) you can break this by adding children or parents which are *not* RBF-enabled and
then the package may lose the ability to be RBF'd.
No. The whole point of this attack is that you keep a transaction in the mempool but unconfirmed via RBF pinning, which
prevents an *alternative* transaction from being confirmed. You then have plenty of time to go get it confirmed later.
Right, I think I didn't explain clearly enough. The point is that, here, B tries to broadcast the timeout transaction
but cannot because there is an in-mempool conflict.
This does not solve the issue because you can add as many fees as you want, as long as the transaction is RBF-pinned,
there is not much you can do in an automated fashion.

@_date: 2020-04-22 15:53:37
@_author: Matt Corallo 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
?Hmm, that's an interesting suggestion, it definitely raises the bar for attack execution rather significantly. Because lightning (and other second-layer systems) already relies heavily on uncensored access to blockchain data, its reasonable to extend the "if you don't have enough blocks, aggressively query various sources to find new blocks, or, really just do it always" solution to "also send relevant transactions while we're at it".
Sadly, unlike for block data, there is no consensus mechanism for nodes to ensure the transactions in their mempools are the same as others. Thus, if you focus on sending the pinning transaction to miner nodes directly (which isn't trivial, but also not nearly as hard as it sounds), you could still pull off the attack. However, to do it now, you'd need to
wait for your counterparty to broadcast the corresponding timeout transaction (once it is confirmable, and can thus get into mempools), turning the whole thing into a mempool-acceptance race. Luckily there isn?t much cost to *trying*, though it?s less likely you?ll succeed.
There are also practical design issues - if you?re claiming multiple HTLC output in a single transaction the node would need to provide reject messages for each input which is conflicted, something which we?d need to think hard about the DoS implications of.
In any case, while it?s definitely better than nothing, it?s unclear if it?s really the kind of thing I?d want to rely on for my own funds.

@_date: 2020-04-22 16:20:03
@_author: Matt Corallo 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
Indeed, that is what I?m suggesting. Anchor output and all. One thing we could think about is only turning it on over a certain threshold, and having a separate ?only-kinda-enforceable-on-chain-HTLC-in-flight? limit.
It does seem like my cached recollection of RBF opt-in was incorrect but please re-read the intro email. There are a bunch of ways of doing pinning - just opting into RBF isn?t even close to enough.

@_date: 2020-04-22 21:10:47
@_author: Matt Corallo 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
Hmm, indeed, it does seem to require a change to the state machine, but I don't think a very interesting one. Because B
providing A an HTLC signature spending a commitment transaction B will broadcast does not allow A to actually broadcast
said HTLC transaction, B can be rather liberal with it. Indeed, however, it would require that B provide such a
signature before A can send the commitment_signed that exists today.
Right, you'd have to use anchor outputs just like we do on the commitment transaction :).
No? Even if we assume there are no tricks that you can play with, eg, the package limits duri eviction, which I'd be
surprised about, the "absolute fee/feerate" thing still screws you. The attacker here gets to hold something at the
bottom of the mempool and the poor honest party is going to have to pay an absurd (likely more than the HTLC value) fee
just to get it unstuck, whereas the attacker never would have had to pay said fee.

@_date: 2020-04-22 23:21:50
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Great summary, a few notes inline.
Quick point of clarification, due to the mempool lacking a consensus system (that?s the whole point, after all :p), there are several reasons to that just running a full node/having a mempool isn?t sufficient.
Another: this is the simplest example. There are also games around the package size limits if I recall correctly.
Or they can just wait. For example in today?s mempool it would not be strange for a transaction at 1 sat/vbyte to wait a day but eventually confirm.
Note that no query is required. The problem has been solved and the preimage-containing transaction should now confirm just fine.
As noted, such transactions today are profit in 10 hours. Just because they?re big doesn?t mean they don?t pay.
I believe the limit is 25, though the point stands, mostly from a total-size perspective.
Sadly, it?s very very easy for this to be a huge amount of CPU + bandwidth.
There have been several proposals before around considering a transactions position in the mempool for various similar criteria. The extreme version being simply heavily rate-limiting transaction relay at low feerates and allowing much more liberal replacement of such packages. It isn?t quite perfect for this issue, though, as it may be easy for the attacker to just fill that rate-limit bucket.
It?s not clear to me that tightening the acceptance rules wouldn?t break other existing uses. Historically the 25 package size limit has proven to be an issue for users doing (somewhat na?ve) centralized wallet withdraws. Sadly many users want that ?payment pending? notification instantly, even if we know it to be somewhat lacking in security. Wallets which have over-compacted their UTXOs are thus stuck making long chains.
To revive an old discussion, on the original thread proposing the CPFP Carve-Out, I said this:
To which Rusty responded (and I may be paraphrasing here): ?Fuck Yea?. I?m still not much of a fan of this idea as it introduces too many constants (what is ?the top of the mempool?, anyway?), and it?s unclear to me what you do as the mempool prevailing feerate changes, but it seems more along the lines or what you?re looking for here.

@_date: 2020-04-23 18:47:46
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Correct, once it makes it into the mempool we can CPFP it and all the regular sub-package CPFP calculation will pick it
and its descendants up. Of course this relies on it not spending any other unconfirmed inputs.

@_date: 2020-08-04 00:02:21
@_author: lf-lists@mattcorallo.com 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
While I admit I haven?t analyzed the feasibility, I want to throw one additional design consideration into the ring.
Namely, it would ideally be trivial, at the p2p protocol layer, to relay a transaction to a full node without knowing exactly which input transaction that full node has in its mempool/active chain. This is at least potentially important for systems like lighting where you do not know which counterparty commitment transaction(s) are in a random node?s mempool and you should be able to describe to that node that you are spending then nonetheless.
This is (obviously) an incredibly nontrivial problem both in p2p protocol complexity and mempool optimization, but it may leave SIGHASH_NOINPUT rather useless for lighting without it.
The least we could do is think about the consensus design in that context, even if we have to provide an external overlay relay network in order to make lighting transactions relay properly (presumably with miners running such software).

@_date: 2020-08-04 09:10:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Hmm, apologies that little context was provided - this was meant in the context of the current crop of relay-based attacks that have been discovered. As we learned in those contexts, ?just handle it when it confirms? doesn?t provide the types of guarantees we were hoping for as placing commitment transactions in mempools can be used to prevent honest nodes from broadcasting the latest state. This implies that HTLC security may be at risk.

@_date: 2020-08-06 11:58:53
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Yep! That is the attack I had in mind - just in general any time you have a non-relative time limit (ie an HTLC) for
confirmation, relay attacks become critical and its no longer just about revocation (which is fine when your time limit
is CSV-based).
In general, SIGHASH_NOINPUT makes these issues much, much simpler to address, but only if we assume that nodes can
somehow be "smart" about replacement when they see a SIGHASH_NOINPUT spend which can spend an output that something else
in the mempool already spends (potentially a different input than the relaying node thinks the transaction should
spend). While ideally we'd be able to shove that (significant) complexity into the Bitcoin P2P network, that may not be
feasible, but we could imagine a relay network of lightning nodes doing that calculation and then passing the
transactions to their local full nodes.
Given such an overlay network would represent an increase in local mempool fees, it is not unreasonable to expect at
least some miners to run a local node which can submit such transactions to their template-generating nodes.

@_date: 2020-08-10 20:14:29
@_author: Matt Corallo 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
I was assuming, largely, that Bitcoin Core will eventually get what you describe here (which is generally termed "package relay", implying we relay, and process, groups of transactions as one).
What we'd need for SIGHASH_ANYPREVOUT is a relay network that isn't just smart about fee calculation, but can actually rewrite the transactions themselves before passing them on to a local bitcoind.
eg such a network would need to be able to relay
"I have transaction A, with one input, which is valid for any output-idx-0 in a transaction spending output B".
and then have the receiver go look up which transaction in its mempool/chain spends output B, then fill in the input with that outpoint and hand the now-fully-formed transaction to their local bitcoind for processing.

@_date: 2020-08-18 10:59:00
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
This sounds like a great idea!
Bitcoin is no longer a homogeneous network of one client - it is many, with different features implemented in each. The Bitcoin protocol hasn't (fully) evolved to capture that reality. Initially the Bitcoin protocol had a simple numerical version field, but that is wholly impractical for any diverse network - some clients may not wish to implement every possible new relay mechanic, and why should they have to in order to use other new features?
Bitcoin protocol changes have, many times in recent history, been made via new dummy "negotiation" messages, which take advantage of the fact that the Bitcoin protocol has always expected clients to ignore unknown messages. Given that pattern, it makes sense to have an explicit negotiation phase - after version and before verack, just send the list of features that you support to negotiate what the connection will be capable of. The exact way we do that doesn't matter much, and sending it as a stream of messages which each indicate support for a given protocol feature perfectly captures the pattern that has been used in several recent network upgrades, keeping consistency.

@_date: 2020-08-18 13:26:36
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
There are several cases where a new message has been sent as a part of a negotiation without changing the protocol version. You may chose to ignore that, but that doesn't mean that it isn't an understood and even relied upon feature of the Bitcoin P2P protocol. If you wish to fail connections to new nodes (and risk network splits, as Suhas points out), then you may do so, but that doesn't make it a part of the Bitcoin P2P protocol that you must do so. Of course there is no "official document" by which we can make a formal appeal, but historical precedent suggests otherwise.
Still, I think we're talking pedantics here, and not in a useful way. Ultimately we need some kind of negotiation which is flexible in allowing different software to negotiate different features without a global lock-step version number increase. Or, to put it another way, if a feature is fully optional, why should there be a version number increase for it - the negotiation of it is independent and a version number only increases confusion over which change "owns" a given version number.
I presume you'd support a single message that lists the set of features which a node (optionally) wishes to support on the connection. This proposal is fully equivalent to that, instead opting to list them as individual messages instead of one message, which is a bit nicer in that they can be handled more independently or by different subsystems including even the message hashing.

@_date: 2020-08-18 14:25:26
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
- snip -
It means that part of the discussion is not useful, and not worth bothering to go back and figure out what was shipped before the version increase and what wasn't, lets talk about what makes sense for the future :).
I think the point is, this doesn't work today, bumping the protocol version requires everyone agreeing on which features make sense, and as we can see from this email thread alone, that isn't a common result in this community. People happily ignore BIPs that make no sense, of which there are a lot, and they should totally be able to do that!
You can say that the current world works, but there's a reason over time we've shifted away from the original "shove another bit on the end of the version message, and everyone agrees on the order of those bits for new feature negotiation." Version bumping is an extension of that, really.
Some things may need further negotiation. eg compact blocks sends multiple redundant messages with different versions and then deduces the correct version based on the message ordering and version set supported. Doing this via verack locks you into a very specific possible negotiation protocols. You could extend it further and suggest a verack K-V list which allows for more flexible negotiation, but I'm not sure that it isn't more complicated than just shoving more messages on the wire.
I think we agree here - the current method of protocol version bumping isn't scalable and something more flexible is definitely a better world.
This is true, there is some ownership requirement, we could switch to hashes or something of the like, but human-readable names have shown to be relatively non-colliding in past Bitcoin protocol changes.

@_date: 2020-08-21 10:15:10
@_author: lf-lists@mattcorallo.com 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
Sure, we could do a new message for negotiation, but there doesn?t seem to be a lot of reason for it - using the same namespace for negotiation seems fine too. In any case, this is one of those things that doesn?t matter in the slightest, and if one person volunteers to write a BIP and code, no reason they shouldn?t just decide and be allowed to run with it. Rough consensus and running code, as it were :)

@_date: 2020-08-21 16:45:26
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
This seems to be pretty overengineered. Do you have a specific use-case in mind for anything more than simply continuing the pattern we've been using of sending a message indicating support for a given feature? If we find some in the future, we could deploy something like this, though the current proposal makes it possible to do it on a per-feature case.
The great thing about Suhas' proposal is the diff is about -1/+1 (not including tests), while still getting all the flexibility we need. Even better, the code already exists.

@_date: 2020-08-21 18:16:02
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
Hmm, could that not be accomplished by simply building this into new messages? eg, send "betterprotocol", if you see a verack and no "betterprotocol" from your peer, send "worseprotocol" before you send a "verack".

@_date: 2020-02-02 19:41:21
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Characterizing orphan transaction in the Bitcoin 
The orphan pool has nontrivial denial of service properties around transaction validation. In general, I think the goal has been to reduce/remove it, not the other way around. In any case, this is likely the wrong forum for software-level discussion of Bitcoin Core. For that, you probably want to open an issue on github.com/bitcoin/bitcoin.

@_date: 2020-02-09 20:40:27
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity 
Responding purely to one point as this may be sufficient to clear up
lots of discussion:
Its not just about the frequency and likelihood, no. If there is a
clearly-provided optimization for this common case in the protocol, then
it becomes further more likely that developers put in the additional
effort required to make this possibility a reality. This has a very
significant positive impact on user privacy, especially those who wish
to utilize more advanced functionality in Bitcoin. Further, yes, it is
anticipated that the N of N case is possible to take in the vast
majority of deployed use-cases for advanced scripting systems, ensuring
that it is maximally efficient to do so (and thereby encouraging
developers to do so) is a key goal in this work.

@_date: 2020-01-10 21:30:09
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
There are a series of soft-fork designs which have recently been making
good progress towards implementation and future adoption. However, for
various reasons, activation methods therefor have gotten limited
discussion. I'd like to reopen that discussion here.
It is likely worth revisiting the goals both for soft forks and their
activation methods to start. I'm probably missing some, but some basic
1) Avoid activating in the face of significant, reasonable, and directed
objection. Period. If someone has a well-accepted, reasonable use of
Bitcoin that is working today, have no reason to believe wouldn't work
long into the future without a change, and which would be made
impossible or significantly more difficult by a change, that change must
not happen. I certainly hope there is no objection on this point (see
the last point for an important caveat that I'm sure everyone will jump
to point out).
2) Avoid activating within a timeframe which does not make high
node-level-adoption likely. As with all "node" arguments, I'll note that
I mean "economically-used" nodes, not the thousand or so spy nodes on
Google Cloud and AWS. Rule changes don't make sense without nodes
enforcing them, whether they happen to be a soft fork, hard fork, or a
blue fork, so activating in a reduced timeframe that doesn't allow for
large-scale node adoption doesn't have any value, and may cause other
unintended side effects.
3) Don't (needlessly) lose hashpower to un-upgraded miners. As a part of
Bitcoin's security comes from miners, reducing the hashpower of the
network as a side effect of a rule change is a needless reduction in a
key security parameter of the network. This is why, in recent history,
soft forks required 95% of hashpower to indicate that they have upgraded
and are capable of enforcing the new rules. Further, this is why recent
soft forks have not included changes which would result in a standard
Bitcoin Core instance mining invalid-by-new-rules changes (by relying on
the standardness behavior of Bitcoin Core).
4) Use hashpower enforcement to de-risk the upgrade process, wherever
possible. As a corollary of the above, one of the primary reasons we use
soft forks is that hashpower-based enforcement of rules is an elegant
way to prevent network splits during the node upgrade process. While it
does not make sense to invest material value in systems protected by new
rules until a significant majority of "economic nodes" is enforcing said
rules, hashpower lets us neatly bridge the gap in time between
activation and then. By having a supermajority of miners enforce the new
rules, attempts at violating the new rules does not result in a
significant network split, disrupting existing users of the system. If
we aren't going to take advantage of this, we should do a hard fork
instead, with the necessarily slow timescale that entails.
5) Follow the will of the community, irrespective of individuals or
unreasoned objection, but without ever overruling any reasonable
objection. Recent history also includes "objection" to soft forks in the
form of "this is bad because it doesn't fix a different problem I want
fixed ASAP". I don't think anyone would argue this qualifies as a
reasonable objection to a change, and we should be in a place, as a
community (never as developers or purely one group), to ignore such
objections and make forward progress in spite of them. We don't make
good engineering decisions by "bundling" unrelated features together to
enable political football and compromise.
I think BIP 9 (plus a well-crafted softfork) pretty effectively checks
the boxes for  here, and when done carefully with lots of community
engagement and measurement, can effectively fulfill  as well.  is,
as I'm sure everyone is aware, where it starts to fall down pretty hard.
BIP 8 has been proposed as an alternative, largely in response to issues
with  However, a naive deployment of it, rather obviously, completely
fails   and  and, in my view, fails  as well by both giving
an impression of, setting a precedent of, and possibly even in practice
increasing the ability of developers to decide the consensus rules of
the system. A BIP 8 deployment that more accurately measures community
support as a prerequisite could arguably fulfill  and  though I'm
unaware of any concrete proposals on how to accomplish that. Arguably, a
significantly longer activation window could also allow BIP 8 to fulfill
 and  but only by exploiting the "needlessly" and "wherever
possible" loopholes.
You may note that, from the point of view of achieving the critical
goals here, BIP 8 is only different from a flag-day activation in that,
if it takes the "happy-path" of activating before the flag day, it looks
like BIP 9, but isn't guaranteed to. It additionally has the
"nice-to-have" property that activation can occur before the flag-day in
the case of faster miner adoption, though there is a limit of how fast
is useful due to node adoption.
Thus, to strike a balance between the drawbacks of BIP 8 and BIP 9, the
Great Consensus Cleanup softfork proposal included this text in the
discussion section (with the spec describing a BIP 9 deployment):
Ultimately, through admittedly rather limited discussion, I still like
this model (though I cannot claim it as my own, the original proposal
came from Greg Maxwell). BIP 9 only falls apart in case of unreasonable
objection, which, naturally, should carry a high bar to ignore, given we
have to have some level of agreement that it is, in fact, unreasonable
(or untargeted). While I admit this is a possibility, I both find it
less likely than in previous soft-forks, and even if it is the case, it
only slows down the process, it doesn't necessarily stop it. In the case
that it does fail, BIP 9 process, in fact, provides a good learning
opportunity as to the level of community readiness and desire for a
given change. While we can (and should, and are) learning a lot about
community readiness for, and acceptability of a change through outreach
and discussion, there is something about a change with a timeframe that
forces people to more carefully consider it.
Thus, as something a bit more concrete, I think an activation method
which sets the right precedent and appropriately considers the above
goals, would be:
1) a standard BIP 9 deployment with a one-year time horizon for
activation with 95% miner readiness,
2) in the case that no activation occurs within a year, a six month
quieting period during which the community can analyze and discussion
the reasons for no activation and,
3) in the case that it makes sense, a simple command-line/bitcoin.conf
parameter which was supported since the original deployment release
would enable users to opt into a BIP 8 deployment with a 24-month
time-horizon for flag-day activation (as well as a new Bitcoin Core
release enabling the flag universally).
This provides a very long time horizon for more standard activation,
while still ensuring the goals in  are met, even if, in those cases,
the time horizon needs to be significantly extended to meet the goals of
 Developing Bitcoin is not a race. If we have to, waiting 42 months
ensures we're not setting a negative precedent that we'll come to regret
as Bitcoin continues to grow.
Thanks also to AJ for feedback on an earlier version of this rant.

@_date: 2020-01-10 17:43:35
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
I went back and forth with a few folks on this one. I think the fact that we lose goals 3/4 very explicitly in order to nudge miners seems like a poor trade off. I?ll note that your point 2 here seems a bit disconnected to me. If you want to fork yourself off the network, you can do it in easier ways, and if miners want to maliciously censors transactions to the detriment of users, rejecting a version bit doesn?t really help avoid that.
Your point about upgrade warnings is well-made, but I?m dubious of it?s value over the network chaos many large forks might otherwise cause.

@_date: 2020-01-14 19:22:47
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
Good thing no one is proposing a naive BIP 9 approach :). I'll note that
BIP 9 has been fairly robust (spy-mining issues notwithstanding, which
we believe are at least largely solved in the wild) in terms of safety,
though I noted extensively in the first mail that it failed in terms of
misunderstanding the activation parameters. I think the above proposal
largely solves that, and I don't see much in the way of arguing that
point from you, here.
As an aside, BIP 9 is also the Devil We Know, which carries a lot of
value, since we've found (and addressed) direct issues with it, whereas
all other activation methods we have ~0 experience with in the modern
Bitcoin network.

@_date: 2020-01-14 19:42:07
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
In general, your thoughts on the theory of how consensus changes should
work I strongly agree with. However, my one significant disagreement is
how practical it is for things to *actually* work that way. While I wish
ecosystem players (both businesses and users) spent their time
interacting with the Bitcoin development community enough that they had
a deep understanding of upcoming protocol change designs, it just isn't
realistic to expect that. Thus, having an "out" to avoid activation
after a release has been cut with fork activation logic is quite a
compelling requirement.
Thus, part of the goal here is that we ensure we have that "out", and
can observe the response of the ecosystem once the change is "staring
them in the face", as it were. A BIP 9 process is here not only to offer
a compelling activation path, but *also* to allow for observation and
discussion time for any lingering minor objections prior to a BIP 8/flag
day activation.
As for a "mandatory signaling period" as a part of BIP 8, I find this
idea strange both in that it flies in the face of all recent soft fork
design work, and because it doesn't actually accomplish its stated goal.
Recent soft-fork design has all been about how to design something with
minimal ecosystem impact. Certainly in the 95% activation case I can't
say I feel strongly, but if you actually *hit* the BIP 8 flag day,
deliberately causing significant network forks for old clients has the
potential to cause real ecosystem risk. While part of the reason for a
24-month time horizon between BIP 8 decision and flag-day activation
endeavors to de-risk the chance that major players are running on
un-upgraded nodes, you cannot ignore the reality of them, both full-,
and SPV-clients.
On the other hand, in practice, we've seen that version bits are set on
the pool side, and not on the node side, meaning the goal of ensuring
miners have upgraded isn't really accomplished in practice, you just end
up forking the chain for no gain.

@_date: 2020-07-14 16:46:06
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Thoughts on soft-fork activation 
Thanks Anthony for this writeup!
I find it incredibly disappointing that the idea of naive flag day fork activation is being seriously discussed in the
form of BIP 9. Activation of forks is not only about the included changes but also around the culture of how changes to
Bitcoin should be and are made. Whether we like it or not, how Taproot activates will set a community understanding and
future norms around how many changes are made.
Members of this list lost sleep and years off their life from stress fighting to ensure that the process by which
Bitcoin changes is not only principled in its rejection of unilateral changes, but also that that idea was broadly
understood, and broadly *enforced* by community members - the only way in which it has any impact. That fight is far
from over - Bitcoin's community grows and changes daily, and the history around what changed and how has been rewritten
time and time again. Worse still, the principled nature of Bitcoin's change process is targeted constantly as untrue in
an attempt by various alternative systems to pretend that their change process of "developers ship new code, users run
it blindly" is identical to Bitcoin.
While members of this list may be aware of significant outreach efforts and design work to ensure that Taproot is not
only broadly acceptable to Bitcoin users, but also has effectively no impact on users who wish not to use it, it is
certainly not the case that all Bitcoin users are aware of that work, nor seen the results directly communicated to them.
Worse still, it is hard to argue that a new version of Bitcoin Core containing a fixed future activation of a new
consensus rule is anything other than "developers have decided on new rules" (even if it is, based on our own knowledge,
not the case). Indeed, even the proposal by Anthony, which makes reference to my previous work has this issue, and it
may not be avoidable - there is very legitimate concern over miners blocking changes to Bitcoin which do not harm them
which users objectively desire, potentially purely through apathy. But to dismiss the concerns over the optics which set
the stage for how future changes are made to Bitcoin purely because miners may be too busy with other things to upgrade
their nodes seems naive at best.
I appreciate the concern over activation timeline given miner apathy, and to some extend Anthony's work here addresses
that with decreasing activation thresholds during the second signaling period, but bikeshedding on timeline may be merited.
To not make every attempt to distance the activation method from the public perception unilateral activation strikes me
as the worst of all possible outcomes for Bitcoin's longevity. Having a quieting period after BIP 9 activation failure
may not be the best way to do that, but it seems like a reasonable attempt.

@_date: 2020-06-24 01:32:52
@_author: Matt Corallo 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Given transaction relay delays and a network topology that is rather transparent if you look closely enough, I think this is very real and very practical (double-digit % success rate, at least, with some trial and error probably 50+). That said, we all also probably know most of the people who know enough to go from zero to doing this practically next week. As for motivated folks who have lots of time to read code and dig, this seems like something worth fixing in the medium term.
Your observation is what?s largely led me to conclude there isn?t a lot we can do here without a lot of creativity and fundamental rethinking of our approach. One thing I keep harping on is maybe saving the blind-CPFP approach with a) eltoo, and b) some kind of magic transaction relay metadata that allows you to specify ?this spends at least one output on any transaction that spends output X? so that nodes can always apply it properly. But maybe that?s a pipedream of complexity. I know Antoine has other thoughts.

@_date: 2020-09-13 22:11:32
@_author: Matt Corallo 
@_subject: [bitcoin-dev] Default Signet, 
[resent with correct source, sorry Michael, stupid Apple]
Yes, a ?default? signet that regularly reorgs a block or two all the time and is ?compatible? with testnet but a faster block target (eg so that it is trivial to mine but still has PoW) and freshly-seeded genesis would be a massive step-up in testing usability across the space.
I don?t have strong feelings about the multisig policy, but probably something that is at least marginally robust (ie 2-of-N) and allows valid blocks to select the next block?s signers for key rollovers is probably close enough.
There are various folks with operational experience in the community, so let?s not run stuff on DO/AWS/etc, please.
