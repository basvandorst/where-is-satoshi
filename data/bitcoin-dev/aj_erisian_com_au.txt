
@_date: 2015-08-03 08:35:45
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Block size following technological growth 
I haven't seen anyone do the (trivial) maths on this. Have I just
missed it? By my count:
 - blocks in 25 btc in rewards and about 0.5 btc in fees per block
 - at ~$300 USD per btc, that's ~$7,650 per block
 - current hashrate is ~400 PH/s; so ~240,000 PH/block works out
   to having to spend about 30PH per dollar earnt.
 - for comparison,
          quotes $1.99 per GH/s for 12 months, which by my count is
      60*60*24*365 / 1.99 GH/$ = 15.8 PH per dollar spent
 - hashrate growth has slowed from about x4/quarter to x2/year:
     sep '13: ~1PH/s
     dec '13: ~4PH/s
     mar '14: ~20PH/s
     jun '14: ~80PH/s
     sep '14: ~200PH/s
     aug '15: ~400PH/s
 - so, as far as I understand it, miners don't make absurd profits compared
   to capital investment and running costs
 - presumably, then, miners will stop mining bitcoin if the revenue/block
   drops significantly at some point
 - less miners means a lower hashrate; a lower hashrate makes
   50% attacks easier, and that's a bad thing (especially if there's lots
   of pre-loved ASIC mining hardware available cheap on ebay or alibaba)
 - in about a year, the block reward halves, cutting out 12.5 btc or
   ~$3750 USD per block. without an increase in fees per block, miners
   will just get ~$3900 USD per block
 - the last time the reward for mining a block was under $4000 per block
   was around oct '13, with a hashrate of ~2PH/s
 - 13 btc in fees per block compared to .5 btc in fees per block is a
   25x increase; which could be either an increase in fee/txn or
   txns/block
 - with ~500 bytes/transaction, that's ~2000 transactions per MB
 - 13 btc in fees ($3900) per block means per transaction fees of
   about
     $2 for 1MB blocks
     $1 for 2MB blocks
     25c for 8MB blocks
     10c for 20MB blocks
   (assuming full blocks, 500 byte txns)
 - comparing that to credit card or paypal fees at ~2.5% that's:
    $2 -> minimum transaction  $80
    $1 -> minimum transaction  $40
    25c -> minimum transaction $10
    10c -> minimum transaction  $4
 - those numbers only depend on the USD/BTC exchange rate in so far as
   the more USD for a BTC, the more likely the block reward will pay
   for hashrate without transaction fees, even with the reward reduced
   to 12.5 btc/block. otherwise it's just USD/txn paying for USD/hashrate
 - the reference implementation fee of 0.1mBTC/kB equates to about
   3c per transaction (since it rounds up). Even 10c/transaction is more
   than a 3x increase on that.
What the above says to me is that even assuming everyone starts paying
fees, the lightning network works great, and so do sidechains and whatever
else, you /still/ want to up the volume of bitcoin txns by something like
an order of magnitude above what's currently allowed within a year or so.

@_date: 2015-08-05 00:22:19
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Eli Dourado on "governance" 
On 4 August 2015 at 01:22, Gavin Andresen via bitcoin-dev <
?The scicast prediction market is shutdown atm (since early July?) so those
numbers aren't live. But...
Network hash rate
3,255.17 PH/s  (same block size)
5,032.64 PH/s  (block size increase)
4,969.68 PH/s  (no replace-by-fee)
3,132.09 PH/s  (replace-by-fee)
Those numbers seem completely implausible: that's ~2.9-3.6 doublings of the
current hashrate (< 400PH/s) in 17 months, when it's taken 12 months for
the last doubling, and there's a block reward reduction due in that period
too. (That might've been a reasonable prediction sometime in the past year,
when doublings were slowing from once every ~45 days to once a year; it
just doesn't seem a supportable prediction now)
That the PH/s rate is higher with bigger blocks is surprising, but given
that site also predicts USD/BTC will be $280 with no change but $555 with
bigger blocks, so I assume that difference is mostly due to price. Also,
12.5btc at $555 each is about 23 btc at $300 each, so if that price
increase is realistic, it would compensate for almost all of the block
reward reduction.
Daily transaction volume
168,438.22 tx/day  (same block size)
193,773.08 tx/day  (block size increase)
192,603.80 tx/day  (no replace-by-fee)
168,406.73 tx/day  (replace-by-fee)
That's only a 15% increase in transaction volume due to the block size
increase; I would have expected more? 168k-194k tx/day is also only a
30%-50% increase in transaction volume from 130k tx/day currently. If
that's really the case, then a 1.5MB-2MB max block size would probably be
enough for the next two years...
(Predicting that the node count will drop from ~5000 to ~1200 due to
increasing block sizes seems quite an indictment as far as centralisation
risks go; but given I'm not that convinced by the other predictions, I'm
not sure I want to give that much weight to that one either)

@_date: 2015-08-07 12:16:50
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Block size implementation using Game Theory 
On 7 August 2015 at 09:52, Wes Green via bitcoin-dev <
The equilibrium for that game is just keeping the same block size, isn't
it? Assume there's a huge backlog of fee-paying transactions, such that you
can trivially fill 1MB, and easily fill 1.25MB for the forseeable future;
fee per MB is roughly stable at "f". Then at any point in time, a miner has
the choice between receiving 25 + f btc, or increasing the blocksize to 1+p
MB and earning (25+f+pf) * (1-p) = f-ppf = 25(1-p) + f(1-pp) < 25+f. Even
if you think bigger blocks are good long term, wouldn't you reason that
other people will think so too, so why pay the price for it yourself,
instead of waiting for someone else to pay it and just reaping the benefit?
An idea I've been pondering is having the block size adjustable in
proportion to fees being paid. Something like "a block is invalid if
a(B-c)*B > F" where B is the block's size, F is the total fees for the
block, and a and c are scaling parameters -- either hardcoded in bitcoin,
or dynamically adjusted by miner votes. ATM, bitcoin's behavour is
effectively the same as setting c=1MB, a>=21M BTC/byte.
Having a more reasonable value for a would make it much easier to produce a
fee market for bitcoin transactions -- if the blocksize is currently around
some specific "B", then the average cost per byte of a transaction is just
"a(B-c)". If you pay more than that, then a miner will include your txn
sooner, increasing the blocksize beyond average if necessary; if you pay
less, you may have to wait for a lull in transactions so that the blocksize
ends up being smaller than average and miners can afford to include your
transaction (or someone might send an unnecessarily high fee paying txn
through, and yours might get swept along with it).
To provide some real numbers, you need to make some assumptions on fee
levels. If you're happy with:
 - 1 MB blocks are fine, even if no one's paying any fees
 - if people are paying 0.1 mBTC / kB (=0.1 BTC/MB) in fees on average then
8MB is okay
then a(1-c)=0, so c=1MB, and a(8-1)=0.1, so a=0.0143 and the scaling works
out like:
 - 1MB blocks: free transactions, no fees
 - 2MB blocks: 0.0143 mBTC/kB, 0.02 btc in fees/block
 - 4MB blocks: 0.043 mBTC/kB, 0.17 btc in fees/block
 - 8MB blocks: 0.1 mBTC/kB, 0.8 btc in fees/block
 - 20MB blocks: 0.27 mBTC/kB, 5.4 btc in fees/block
 - 40MB blocks: 0.56 mBTC/kB, 22 btc in fees/block
In the short term, miners can just maximise fees for a block -- ie, add the
highest fee/byte txns in order until adding the next one would invalidate
the block.
Over the long term, you'd still want to be able to adjust a and c -- as the
price of bitcoin in USD/real terms goes up, a should decrease
proportionally; as hardware/software improve, a should decrease and/or c
should increase.  Essentially miners would want to choose a,c such that the
market for block space clears at a price of some $x/byte, where $x is
determined by their costs -- ie, hardware/software constraints. If they set
a too high, or c too low, then they'll be unable to accept some
transactions offering $x/byte, and thus lose out. If they set a too low or
c too high, they'll be mining bigger blocks for less reward, and lose out
that way too. At the moment, I think it's a bit of both problems -- c is
too low (meaning some transactions get dropped), but a is too high (meaning
fees are too low to pay for the effort of bigger blocks).
Note that, as described, miners could try cheating this plan by making a
high fee transaction to themselves but not publishing it -- they'll collect
the fee anyway, and now they can mine arbitrarily large blocks. You could
mitigate this by having a(B-c) set the /minimum/ fee/byte of every
transaction in a block, or alternatively by enforcing each miner pay a
significant %ge of collected fees to the miner of the next block(s).

@_date: 2015-08-08 04:22:50
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On 8 August 2015 at 00:57, Gavin Andresen via bitcoin-dev <
Every once in a while the network will get lucky and we'll find six blocks
All the higher-fee transactions waiting to be confirmed will get confirmed
?That depends a bit on how ra?tional miners are, doesn't it? Once the block
subsidy is retired, hashpower is only paid for by fees -- and if there's no
fee paying transactions in the queue, then there's no reward for applying
hashpower, so mining a block won't even pay for your costs. In that case,
better to switch to hatching something else (an altcoin with less fees than
bitcoin has on average but more than nothing, eg), or put your hashing
hardward into a low power mode so you at least cut costs.
That will only be needed for a short while though -- presumably enough
transactions will come in in the next five or ten minutes for a block to be
worth mining again, so maybe implementing that decision process is more
costly than the money you'd save.
onversely, when the queue is over-full because there's been no blocks found
for a while, that should mean you can fill a block with higher-than-average
fee transactions, so I'd expect some miners to switch hashpower from
altcoins and sidechains to catch the temporary chance of higher revenue
?Both tendencies would help reduce the variance in block time, compared to
a steady hashrate, which would probably be a good thing for the network as
a whole)?
I think the same incentives apply with mining being paid for by assurance
contracts rather than directly by transaction fees -- if you get a bunch of
blocks done quickly, the existing assurance contracts are dealt with just
as well as if it had taken longer; so you want to wait until new ones come
in rather than spend your hashpower for no return.
?All of this only applies once fees make up a significant portion of the
payment for mining a block, though.?

@_date: 2015-08-11 02:50:31
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Off-chain transactions and miner fees 
That's not a universally held belief. See for example:
  It's also not clear to me what amount of security people actually "want".
 "Currently the Bitcoin community is being effectively taxed about
  $832,000 per day ... just to support mining! [...]
  We?re not spending so much on mining because we really need it. It?s
  because printing money distorts behaviour."
  -- If $832k (25*240 btc/day * $231 USD/btc) is too much, maybe $475k/day
(reward halved, at current price) will still be too much in a year's
time? If bitcoin's price rises at just 19% pa on average (which doesn't
seem like much if you're thinking in startup/VC terms?), then the block
reward will still be worth about $475k/day after it halves again to 6.25
coins per block. So maybe the block reward pays for bitcoin transactions
and fees are effectively zero right up until the day the block reward
goes away entirely?
In any event, the lightning network offers three potential benefits over
on-chain transactions:
 - lower fees
 - shorter confirmation times
 - no ongoing costs once the channel is closed
If you have zero fees, lightning is still interesting for quick
transactions (since they offer better assurance of payment than
zero-confirmation transactions), and also for microtransactions (where
spamming the blockchain and the UTXO db with thousands of transactions
just to move $1 from here to there isn't appealing).
Every lightning transaction happens through a series of channels (at least
one, but realistically at least two; with any amount of decentralisation,
probably more likely somewhere in the range of three to twenty). Each
of those channels requires at least two blockchain transactions (one or
two to create the channel; one or three to close the channel and spend
the balances).
It's not 100% clear at this point, but keeping a lightning channel open
will probably have (hardware) costs that grow linearly in the number of
transactions [0]; in which case keeping them open forever won't be an
option, and they'll be closed when the cost of keeping it open is less
than the cost of resetting it on the blockchain (only one blockchain
transaction required). So in that case even if lightning is crazy popular,
there'll still be activity on the blockchain at whatever fee rate there
is, just by people trading off storage costs for blockchain fees.
[0] Even if it's not the case, closing a channel eventually is probably
good practice in order to rollover keys. Channels also have a maximum
theoretical number of transactions, but that's likely on the order
of exa-transactions, so is probably irrelevant. Channel profitability
likely varies over time, and since channels lock up bitcoin, closing
less profitable channels so the funds can be used elsewhere is likely
also valuable. With all those things together, ballpark max lifetime of a random channel
(IMHO) is somewhere in the range of two weeks to two years. If lightning
is the only thing doing transactions on the blockchain and only using
250B/txn, 8M channels with an average lifetime of 2 weeks would fill
1MB blocks; as would 210M channels with an average lifetime of a year,
or 420M channels with an average lifetime of two years. Those sort
of numbers probably roughly cover lots of Americans having access to
a lightning based point-of-sale network to buy Starbucks, eg, but not
much more than that. (You need at least one channel per customer, plus
one per business, plus something on the order of log(N) hubs to connect
them all; having multiple channels is probably about as good an idea as
having multiple credit cards).
Lightning transactions will have to pay for several things:
 - the blockchain fees for opening/closing the channel
 - the time value of the funds being used to keep the channels open,
   for each channel in the route from payer to payee
 - the maintenance costs of the hardware/software to run a lightning
   channel
By contrast, blockchain transactions just have to pay miners the
blockchain fee for the transaction; there's no other intermediaries
who have to be compensated. At some point, the latter will certainly
be cheaper than the former -- since the lightning network has to pay
for third parties' time value of bitcoin there should certainly be some
"sufficiently large" amount whose time value is higher than the bitcoin
txn fee, even for a very short txn time.
It's all a bit hypothetical though -- not only is lightning still
unimplemented as yet, but I think at present the time value of bitcoin is
effectively zero (ie, afaik people recommend "just buy and hold bitcoin
and wait for the next bubble", rather than "buy bitcoin and put it in
AwesomeBank's Term Deposit product and gain 3% pa"), and most of the
time fees seem to be basically zero too.
I think the general answer is that lightning relies on the blockchain --
if the blockchain doesn't work, neither does lightning. So whatever level
of txn fees it takes to make the blockchain work; whether that's $0/txn,
1c/txn or $50/txn, nodes in the lightning network will pay that fee,
and, presumably, pass it on to the lightning network's end users in the
form of txn fees on the lightning network.

@_date: 2015-08-11 03:26:20
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Off-chain transactions and miner fees 
Sure, that's a way to increase your net worth in real terms, but it only
works if your interest rate on your fiat account is greater than the
price rise in bitcoin over the same term. If you pull out a BTC today at
$300, put it in a bank account earning 3% interest for a year and then
buy $309 worth of bitcoin when the price has risen to $400 per BTC,
and only get 0.7725 of a bitcoin, that's not a winning proposition.
I'd call that earning a -22.75% rate (in bitcoin terms), while a 0%
rate would just be ending up with as many bitcoin after a year as you
started with. Note that in USD (and real) terms, in this scenario 77%
of a bitcoin is actually worth more after a year than 1 bitcoin is now.
You might get a positive rate of return on bitcoin invested today by
running an exchange or a gambling service of some sort; but I think
mostly, people are just sitting on their coins hoping they appreciate. If
so, (in my terminology at least) they're earning 0%, denominated in
bitcoin, and have a time-value of bitcoin of zero.

@_date: 2015-08-11 05:02:40
@_author: Anthony Towns 
@_subject: [bitcoin-dev] What Lightning Is 
1. User swipes their phone over merchant's NFC device
    (or scans a QR code displayed by the register or whatever)
 2. Dialog pops up on phone:
     "Pay Infinitea $5.20? [yes] [no]"
 3. User presses [yes]
 4. Brief pause
 5. "Payment confirmed" appears on both user's phone and merchant's POS
    device
The backend bits that need to happen:
 1. Merchant passes on their identity and public key, an amount, and a
    hash for the payment.
 2. User's phone goes online to see if a route to the vendor can be
    worked out, and to work out what lightning network fees are needed.
    Also translates the bitcoins requested into the user's preferred
    currency so they don't have to do maths in their head.
 3. User's phone prepares a lightning transaction to the vendor, signed
    with the corresponding lightning channel keys, using the hash the
    merchant provided, and sends it through one of the channels the user
    already has open and funded (at >$5.20 worth of bitcoins).
 4. The transaction makes its way through the lightning network, and the
    confirmation makes its way back. It's not clear to me how long this
    realistically takes (either how many hops are likely, or how long
    a single hop will actually take; I assume it should be a couple
    of seconds).
 5. UIs at both ends update. User gets a nice cup of tea.
There's a few potential problems with this:
 - what if the merchant says "no you didn't pay me, your phone is lying,
   you're a liar, I hate you, no tea for you" despite your phone saying
   you paid?
     a) you could mitigate this by having the payments be incremental
        (here's 1c, 520 times) with both terminals visibly updating;
        but that would take up to 520x longer than sending a single
        transaction, and mightn't really be any better anyway
     b) you could also have the initial negotiation involve signing
        something that could be adjudicated independently later (hey,
        here's a QR code saying he owed me a tea, here's a QR code showing
        I paid for it, and here's a video of him saying "no tea for you").
     c) Or maybe you just bite the $5 and never shop there again; just
        as you would if you handed over $5.20 in cash and then they told
        you you hadn't paid them.
 - what if the transaction's unroutable? then you get a "service
   unavailable" notice on your phone and pay by other means -- blockchain,
   cash, etc. Same as if your credit card won't register. ditto if your
   phone can't get on the internet.
 - what if the fees are large? (eg, the coffee costs $5, and the fees are
   20c?)
     a) I think they'll actually be small -- even for a 10% pa interest
        rate denominated in bitcoin, the time-value of $5.20 in bitcoin
        for 7 days is just under 1c (.35%). If so, that's noise, and the
        user legitimately doesn't care. OTOH, it does get multiplied by
        number of hops, and maybe the user cares about $5.20 vs $5.26.
     b) Alternatively, the app could just indicate the fees ("Pay $5.20 +
        <1c in fees") and/or the user could have an explicit setting for
        fee info ("Only warn me when fees are greater than either 5c/1%")
     c) Or you could have some UI magic, so the vendor's POS device
        initially says "advertised price is $5.20, but I actually
        expect just $5.05, call the rest a discount", the phone says
        "fees are 5c, so I'll display "Pay just $5.10 for a $5.20 cup of
        coffee!". That's closer to how Visa/Mastercard do it and might
        be reasonable in many cases.
 - what happens if the user presses "yes" but the lightning transaction
   then fails? you don't want to wait for the 7 day timeout to know if
   you can have a cup of tea; and you don't want the payment to go through
   after you've paid for your tea in cash, drank it, and gone home.
     a) Maybe the lightning network hubs just reply early cancelling the
        transactions, and your phone can say "failed". You can't force
        them to do this, but maybe the incentives work out okay so that
        they'll want to anyway. (I think they will) If so, everything's
        fine. As far as the merchant's concerned, you may as well have
        just pressed "No".
     b) The vendor could issue a conditional refund, eg an on-blockchain
        transaction for the amount of the coffee from them to you,
        payable if you ever receive the hash token. (And if you don't,
        redeemable by them after a timeout). That doesn't work real well
        if the fee for a blockchain txn is more than the price of a cup
        of tea of course.
IMHO, most of the complexity isn't in doing the transaction, it's in
maintaining the channels. For example:
 - what if the tea shop has a sudden run of customers, and all their
   payment channels fill up?
 - how do you close the till at the end of the day (ie, put your
   earnings into a cold wallet so they can't be hacked as easily and
   clear your channel so you can accept more payments tomorrow?) Do you
   do this on the blockchain or do you have a different lightning
   network channel to your "bank account"?
 - inversely; if you do all your weekly shopping and impulse buys (tea,
   coffee, beer, meals, groceries, fuel, road tolls, ...) on lightning,
   how do you reset your channels once a day/week/fortnight/month with
   some money from your salary/savings, so they don't run out?
 - do refunds work, even after the original txn has finished? ("Oh,
   actually we're out of tea")
 - you have to watch the blockchain once a week or so in case your
   counterparty tries stealing your balance by replaying old states and
   hoping you don't notice
 - how do you keep the channel keys/secrets sufficiently available
   and secure?
 - how do you figure out who to make channels with in the first place,
   and if/when to change them?
 - what happens if your phone breaks, is stolen or gets lost; have you
   lost your channel secrets, and potentially all the coins in your
   balance?
 - etc.
(I don't think they're unsolvable or anything, though)

@_date: 2015-08-11 05:19:01
@_author: Anthony Towns 
@_subject: [bitcoin-dev] What Lightning Is 
Sigh, kanzure on irc points out I misread this -- I read "on-channel"
not "on-chain".
In that case, I think the answer is "the customer doesn't pay for tea
via lightning". They have to setup a channel with someone first, and
to do that, they'll need a "sufficiently confirmed" anchor transaction,
and I don't think zero confirmations would be enough for that.
 -1 "Oh, how did that guy pay for tea with his phone? That's cool!"
    "Scan the QR code, yeah, where the lightning logo is"
    "Cool, I'll try it tomorrow"
 0 go home, "open a lightning channel", sleep, look forward to getting
   tea tomorrow
For step 0, I guess it's:
 0.1 "Choose a hub to connect to" (could be randomly selected)
 0.2 "Choose an amount to fund the channel" (0.5 btc would be a lot)
 0.3 "Are you sure?"
 0.4 [wait briefly while counterparty signs]
 0.5 [wait for 10 confirmations?]
I don't think it's at all clear how 0.1 works in practice yet -- routing
has barely been spitballed, and without knowing how routing works, it's
hard to say who to connect to.
Hard to say how much to put in in step 0.2 too -- if it takes a while to
refresh funds in a channel (you have to do a blockchain txn eg), then
that you should put more in; if you have multiple channels for whatever
reason, maybe you can put less in each.
The "Are you sure?" might require some legal T&Cs in practice.
You need to have some realtime communication with your channel
counterpary when creating the anchor; but that should be fairly quick.
You also need to establish some random numbers and keys, but those could
be done in advance.
I think you (or your counterparty) will want to wait for a few
confirmations before your channel is actually usable. So I think it'll
take over an hour for a channel to be "open" in even the best case.

@_date: 2015-08-14 16:20:35
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Adjusted difficulty depending on relative 
?That would mean that as usage grew, blocksize could increase, but
confirmation times would also increase (though presumably less than
linearly). That seems like a loss?
If you also let the increase in confirmation time (due to miners finding
harder blocks rather than a reduction in hashpower) then get reflected back
as decreased difficulty, it'd probably be simpler to just dynamically
adjust the max blocksize wouldn't it?

@_date: 2015-08-14 17:00:25
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Adjusted difficulty depending on relative 
On 14 August 2015 at 16:48, Jakob R?nnb?ck <
?If you're waiting for one confirmation, something like that works -- you
might from 95% chance of 10 minutes 5% chance of 20 minutes to 100% chance
of 10m30s. But if you want 144 confirmations (eg) you go from 95% chance of
1 day, 5% chance of 1 day 10 minutes; to 100% chance of 1 day 72 minutes.
Once blocksizes had normalised as much larger than 1MB with a corresponding
higher average hashrate, a bad actor could easily mine a raft of valid
empty/small blocks at the minimum hash rate and force a reorg (and do
doublespends, etc).

@_date: 2015-08-16 17:44:42
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
?That's been really unclear to me. Personally, I'd love to see a vote from
the core and XT developers on:
 - what should the block size soft limit be in 12 months (min and max)
 - what should the block size hard limit be in 12 months (min and max)
 - at what rate should the hard limit grow over the next 10 years? (min and
 - what mechanism should be used to update the soft limit
   (manual code change, time based, blockchain history, something else)
? - what me?chanism should be used to update the hard limit
   (hard fork code change, time based, blockchain history, something else)
? - what should the
?transaction ?
fee level be in 12 months (after the reward halves)??
 - what's a good measure of "(de)centralisation" and what value should
everyone aim for in 12 months?
As an interested newbie, I can't actually tell what most people think the
answers to most of those questions are. FWIW, mine would be:
 - soft limit in 12 months: 1MB-4MB
 - hard limit in 12 months: 2MB-20MB
 - hard limit grows at 17-40% a year (and should be >4x average txn volume)
 - update the soft limit by code changes or blockchain history
 - update the hardlimit by (1) fee level, (2) miner vote, (3) hard coded
time updates at a conservative (low) rate, (4) hard fork every couple of
 - transaction fees should in 12months should be lower per kB than today's
defaults, say 20%-50% of today's defaults in USD
 - number of bitcoin nodes, should be 20% higher in 12 months than it is now

@_date: 2015-12-08 12:42:24
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Bit ambiguous what "worst" means here; lots of people would say the
smallest increase is the worst option. :)
By my count, P2PKH transactions get 2x space saving with segwit [0],
while 2-of-2 multisig P2SH transactions (and hence most of the on-chain
lightning transactions) get a 3x space saving [1]. An on-chain HTLC (for
a cross-chain atomic swap eg) would also get 3x space saving [2]. The most
extreme lightning transactions (uncooperative close with bonus anonymity)
could get a 6x saving, but would probably run into SIGOP limits [3].
So I think it's fair to say that on its own it gives up to a 2x increase
for ordinary pay to public key transactions, and a 3x increase for 2/2
multisig and (on-chain) lightning transactions (which would mean lightning
could scale to ~20M users with 1MB block sizes based on the estimates

@_date: 2015-12-08 14:58:03
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
So from IRC, this doesn't seem quite right -- capacity is constrained as
  base_size + witness_size/4 <= 1MB
rather than
  base_size <= 1MB and base_size + witness_size <= 4MB
or similar. So if you have a 500B transaction and move 250B into the
witness, you're still using up 250B+250B/4 of the 1MB limit, rather than
just 250B of the 1MB limit.
In particular, if you use as many p2pkh transactions as possible, you'd
have 800kB of base data plus 800kB of witness data, and for a block
filled with 2-of-2 multisig p2sh transactions, you'd hit the limit at
670kB of base data and 1.33MB of witness data.
That would be 1.6MB and 2MB of total actual data if you hit the limits
with real transactions, so it's more like a 1.8x increase for real
transactions afaics, even with substantial use of multisig addresses.
The 4MB consensus limit could only be hit by having a single trivial
transaction using as little base data as possible, then a single huge
4MB witness. So people trying to abuse the system have 4x the blocksize
for 1 block's worth of fees, while people using it as intended only get
1.6x or 2x the blocksize... That seems kinda backwards.
Having a cost function rather than separate limits does make it easier to
build blocks (approximately) optimally, though (ie, just divide the fee by
(base_bytes+witness_bytes/4) and sort). Are there any other benefits?
But afaics, you could just have fixed consensus limits and use the cost
function for building blocks, though? ie sort txs by fee divided by [B +
S*50 + W/3] (where B is base bytes, S is sigops and W is witness bytes)
then just fill up the block until one of the three limits (1MB base,
20k sigops, 3MB witness) is hit?
(Doing a hard fork to make *all* the limits -- base data, witness data,
and sigop count -- part of a single cost function might be a win; I'm
just not seeing the gain in forcing witness data to trade off against
block data when filling blocks is already a 2D knapsack problem)

@_date: 2015-12-08 16:54:48
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Isn't that solvable in the short term, if miners just agree to order
transactions via a cost function, without enforcing it at consensus
level until a later hard fork that can also change the existing limits
to enforce that balance?
(1MB base + 3MB witness + 20k sigops) with segwit initially, to something
like (B + W + 200*U + 40*S < 5e6) where B is base bytes, W is witness
bytes, U is number of UTXOs added (or removed) and S is number of sigops,
or whatever factors actually make sense.
I guess segwit does allow soft-forking more sigops immediately -- segwit
transactions only add sigops into the segregated witness, which doesn't
get counted for existing consensus. So it would be possible to take the
opposite approach, and make the rule immediately be something like:
  50*S < 1M
  B + W/4 + 25*S' < 1M
(where S is sigops in base data, and S' is sigops in witness) and
just rely on S trending to zero (or soft-fork in a requirement that
non-segregated witness transactions have fewer than B/50 sigops) so that
there's only one (linear) equation to optimise, when deciding fees or
creating a block. (I don't see how you could safely set the coefficient
for S' too much smaller though)
B+W/4+25*S' for a 2-in/2-out p2pkh would still be 178+206/4+25*2=280
though, which would allow 3570 transactions per block, versus 2700 now,
which would only be a 32% increase...
Sure, but, at least for now, there's already two limits that are being
hit. Having one is *much* better than two, but I don't think two is a
lot better than three?
(Also, the ratio between the parameters doesn't necessary seem like a
constant; it's not clear to me that hardcoding a formula with a single
limit is actually better than hardcoding separate limits, and letting
miners/the market work out coefficients that match the sort of contracts
that are actually being used)
Sure; it just seems to be halving the increase in block space (60% versus
100% extra for p2pkh, 100% versus 200% for 2/2 multisig p2sh) for what
doesn't actually look like that much of a benefit in fee comparisons?
I mean, as far as I'm concerned, segwit is great even if it doesn't buy
any improvement in transactions/block, so even a 1% gain is brilliant.
I'd just rather the 100%-200% gain I was expecting. :)

@_date: 2015-12-09 14:51:39
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
So the worst case script I can come up with is:
       1 0 {2OVER CHECKSIG ADD CODESEP} OP_EQUAL
which (if I didn't mess it up) would give you a redeem script of about
36B plus 4B per sigop, redeemable via a single signature that's valid
for precisely one of the checksigs.
Maxing out 20k sigops gives 80kB of redeemscript in that case; so you
could have to hash 19.9GB of data to fully verify the script with
current bitcoin rules.
Segwit with the 75% factor and the same sigop limit would make that very
slightly worse -- it'd up the hashed data by maybe 1MB in total. Without
a sigop limit at all it'd be severely worse of course -- you could fit
almost 500k sigops in 2MB of witness data, leaving 500kB of base data,
for a total of 250GB of data to hash to verify your 3MB block...
Segwit without the 75% factor, but with a 3MB of witness data limit,
makes that up to three times worse (750k sigops in 3MB of witness data,
with 1MB of base data for 750GB of data to hash), but with any reasonable
sigop limit, afaics it's pretty much the same.
However I think you could add some fairly straightforward (maybe
soft-forking) optimisations to just rule out that sort of (deliberate)
abuse; eg disallowing more than a dozen sigops per input, or just failing
checksigs with the same key in a single input, maybe. So maybe that's
not sufficiently realistic?
I think the only realistic transactions that would cause lots of sigs and
hashing are ones that have lots of inputs that each require a signature
or two, so might happen if a miner is cleaning up dust. In that case,
your 1MB transaction is a single output with a bunch of 41B inputs. If you
have 10k such inputs, that's only 410kB. If each input is a legitimate
2 of 2 multisig, that's about 210 bytes of witness data per input, or
2.1MB, leaving 475kB of base data free, which matches up. 20k sigops by
475kB of data is 9.5GB of hashing.
Switching from 2-of-2 multisig to just a single public key would prevent
you from hitting the sigop limit; I think you could hit 14900 signatures
with about 626kB of base data and 1488kB of witness data, for about
9.3GB of hashed data.
That's a factor of 2x improvement over the deliberately malicious exploit
case above, but it's /only/ a factor of 2x.
I think Rusty's calculation  was that
the worst case for now is hashing about 406kB, 3300 times for 1.34GB of
hashed data [0].
So that's still almost a factor of 4 or 5 worse than what's possible now?
Unless I messed up the maths somewhere?
[0] Though I'm not sure that's correct? Seems like with a 1MB
    transaction with i inputs, each with s bytes of scriptsig, that you're
    hashing (1MB-s*i), and the scriptsig for a p2pkh should only be about
    105B, not 180B.  So maximising i*(1MB-s*i) = 1e6*i - 105*i^2 gives i =
    1e6/210, so 4762 inputs, and hashing 500kB of data each time,
    for about 2.4GB of hashed data total.

@_date: 2015-12-17 13:52:22
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
You're right that the miner selection algorithm doesn't force it to be
the way Pieter describe. But your claim is still incorrect. :)
With Pieter's segwit proposal (as it stands), there are two
consensus-limited resources: number of signature ops in the base block
must be no more than 20k, and the virtual block size must be no more
than 1MB (where virtual block size = base block size plus a quarter of
the witness data size).
Nodes and miners have other constraints -- bandwidth, storage, CPU, etc,
such that they might not want to max out these limits for whatever reason,
but those limits aren't enforced by consensus, so can be adjusted as
technology improves just by individual miner policy.
(modulo sigop constraints, same as today for fee per base block size)
That's on the "supply" side (ie, miners are forced to be a single group
of economic actors with alighned constraints due to consensus rules).
On the demand side, there might be people who are able to trade off
witness data for base data at different ratios. For most, it's just 1:1
up to a limit as they move scriptsig to witness data, and obviously if
you have to trade 1B of base data for more than 4B of witness data it's
uneconomic. But since the ratio is fixed, there's no bartering to be
done, it's just the same simple calculation for everyone -- does 1B of
base convert to <4B of witness? then do it; otherwise, don't. But once
they've selected a tradeoff, all they can do is choose an absolute fee
value for their transaction, and then you're just back to having some
people who are willing to pay higher fees per virtual block size, and
others who are only willing to pay lower fees.

@_date: 2015-12-17 20:57:13
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
I think there's a few variants of (2) -- there's "just 2MB", "2MB now,
4MB in a while, 8MB after that", "1MB for a while longer, then 2MB,
then 4MB" (halved from 2/4/8 since segwit gives 1.6x-2x benefit), and
variations of those with different dates, whether or not to smooth out
the increases to avoid economic shocks, and how to determine activation
(flag day? miner consensus? combination?).
That's true on the benefit side (both give about double the number of
ordinary transactions per block; though segregated witness has other
benefits). On the cost side, the limits are different:
 * worst case block data size is 2x for BIP102, 4x for segwit (affecting
   bandwidth, latency and storage costs for nodes)
 * worst case sigops is 2x for BIP102, but the same as today for segwit
   (affecting block validation time)
 * worst case bytes to hash a block is 4x for BIP102 (doubling block
   size and sigops), but the same as today for segwit (again affecting
   block validation time)
 * worst case UTXO bloat is 2x for BIP102, but the same as today for
   segwit (affecting memory usage, and validation time)
In the "expected" case (where people aren't attacking the blockchain)
I think they're the same on all these metrics. But increasing the
limits could easily make attacks more common, especially if it makes
them more effective.
I think the main attack vector of these is that increasing block
validation time via too many (active) sigops or too many bytes hashed
allows a selfish mining attack, but I'm not clear enough on how that
would work exactly to estimate where the boundary between acceptable and
unacceptable risk is (and how feasible non-consensus-level countermeasures
might be).
But at 1x sigops, you can already (accidently!) construct blocks that
take minutes to verify; and at 4x you can probably already construct a
block that takes 10 minutes to verify, which would probably be pretty
bad... But I'm not sure this isn't already exploitable, so maybe we're
already assuming a certain level of altruism and making things worse
doesn't actually make them worse?
I think it would be good for BIP102 or similar to include an evaluation
of that risk before being deployed [0].
Yes. That doesn't mean it's not worth it, though.
(The 2-month timeline for the BIP50 accidental hardfork to be accepted
on the network seems persuasive to me that it's possible to roll out a
deliberate, SPV-compatible, hardfork on today's network in 3-6 months)
I think it makes sense to just do both of these independently; ie:
 * release segwit via softfork ASAP (perhaps targetting March or April
   to get it included in bitcoin, activation a month or three
   afterwards?), with virtual block size calculated as proposed and
   capped by MAX_BLOCK_SIZE [1]
 * increase MAX_BLOCK_SIZE via hardfork to 2MB after block 420,000
   (phased in gradually? with future scheduled increases to 4MB or 8MB?)
If segwit gets delayed because it's complicated, that's okay; if
it comes out earlier, that's okay too. If the hardfork gets delayed
because miners aren't ready or because it's better to introduce it in a
staggered fashion, or because there's no clear evaluation of the risks,
that's okay too.
But more importantly, it allows evaluate the pros and cons of each
implementation separately and on its own merits, rather than arguing
against working on one just because you're in favour of doing the
other ASAP.
They have benefits if you combine them too; for instance, if the
MAX_BLOCK_SIZE increase is phased in rather than done as a step increase
(ie block x's limit is 1MB, block x+1's limit is 1.005MB or similar,
and block x+2's limit is 1.01MB, etc) having segwit available in parallel
could provide a helpful escape valve: if an individual bitcoin user has
been dying for more capacity, they can spend the time/effort to update
their software for segwit and get it immediately without having to wait
as the consensus limits rise.
Conversely, having both segwit and a phased increase to MAX_BLOCK_SIZE
means that miners generally won't be immediately mining 2MB (or 4MB)
blocks halfway through the year, which should avoid both technological
shocks (bandwidth just doubled!) or economic shocks (supply has increased
so fees have plummeted), which could be good.
FWIW, the worst case scenarios are:
 * block data size:
     BIP102:  2x   (worst/avg)
     segwit:  4x   (worst, ~2x avg)
     both:    8x   (worst, ~4x avg)
     BIP101:  8x   (worst/avg)
 * sigops per block:
     BIP102:  2x
     segwit:  1x
     both:    2x
     BIP101:  8x
 * bytes hashed per block:
     BIP102:  4x
     segwit:  1x
     both:    4x
     BIP101:  64x
 * UTXO rate of increase:
     BIP102:  2x
     segwit:  1x
     both:    2x
     BIP101:  8x
Compared to the (expected, eventual, near-term) benefits:
 * transactions per block:
     BIP102:  2x
     segwit:  1.6x-2x
     both:    3.2x-4x
     BIP101:  8x
 * misc:
     BIP101/2: planned hardforks are possible, bitcoin community governance
       is demonstrably working, etc
     segwit: malleability fixes, script improvements, lightning
       enablement, etc
The block data is the only case where the average case is already just
about the worst case; for the others, as long as the worst case doesn't
inspire new attacks, the future average case should just increase in
proportion to the additional transactions.
[0] (and segwit should actually account for sigops in witness data before
     being deployed...)
[1] If segwit warrants a hardfork to clean up data structures, I think
    that should be deferred until well after the MAX_BLOCK_SIZE hardfork,
    rather than trying to do it at the same time. As such, doing segwit by
    soft fork in the short term seems to make sense, since it also helps
    with transaction malleability and further improvements to script.

@_date: 2015-12-18 03:55:41
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
Segwit as proposed gives a 75% *discount* to witness data with the
same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up
of 650kB of base block data plus 1.4MB of witness data; where 650kB +
1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus
2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.
With segregated witness, 2-2 multisig transactions are made up of 94B
of base data, plus about 214B of witness data; discounting the witness
data by 75% gives 94+214/4=148 bytes. That compares to about 301B for
a 2-2 multisig transaction with P2SH rather than segwit, and 301/148
gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed
to get the numbers above.
You get further improvements with, eg, 3-of-3 multisig, but to get
the full, theoretical 4x gain you'd need a fairly degenerate looking
Pay to public key hash with segwit lets you move about half the
transaction data into the witness, giving about a 1.6x improvement by
my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,
where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is
a reasonable expectation to have for the proposed segwit scheme overall.

@_date: 2015-12-18 16:12:23
@_author: Anthony Towns 
@_subject: [bitcoin-dev] On the security of softforks 
For it to be a "safe" soft fork, the "invalid segwit transaction" should
look non-standard to Carol, and as such she should refuse to mine it.
I think the attack has to go like this:
 * segwit activates; 5% of miners fail to upgrade however
 * Mallory creates a transaction paying to a segwit script
   (ie scriptPubKey is just a 33 byte push) [0]
 * non-upgraded nodes and miners will refuse to forward or mine
   this transaction (a non-p2sh scriptPubKey that just pushes data is
   non-standard) but the upgraded nodes and miners will forward and mine
   it. it will be included in the blockchain by upgraded miners fairly
   soon, and will then be in the UTXO set of non-upgraded miners and
   nodes too.
 * Mallory creates a segwit-invalid spend back to himself (or directly
   to Bob for the 1BTC), ie provides empty scriptSig, but no
   witness data. Upgraded miners and nodes reject the transaction,
   but non-upgraded nodes will relay and mine it afaics.
I *think* that transaction will fail the AreInputsStandard() test on
non-upgraded nodes, and thus still won't be accepted to the mempool
or mined by non-upgraded nodes, and thus no one will see it, or any
descendent transactions. (Upgraded nodes will reject it because it's
segwit invalid, of course)
If it is accepted by some old nodes, that transaction won't ever get many
confirmations -- if Carol mines it, her block will be orphaned by the
upgraded mining majority after the next two or three blocks are found.
With only 5% of hashpower, it will take around three hours for Carol
and friends to find a block in general.
Also, the fact that segwit outputs are "anyone can spend" maybe mitigates
this further -- you could have a vigilante node that creates invalid
segwit txns for every segwit output that just spends the entire thing
to fees. Even if the vigilante's transactions get rejected by nodes who
see Mallory's attempt first, that should still be enough to trigger any
sort of double-spend alerts I can think of, at least if anyone at all
is altruistic enough to run a vigilante node like that in the first place.
So I think the only way Mallory gets free beer from you with segwit
soft-fork is if:
 - you're running out of date software and you're ignoring warnings to
   upgrade (block versions have bumped)
 - you've turned off standardness checks
 - you're accepting low-confirmation transactions
 - you're not using any double-spend detection service
If you're not accepting zero-confirmation transactions straight from
the mempool, (ie you require 1 or 2 confirmations) you also need:
 - some non-upgraded miners who have turned off standardness checks
 - your business is setup that an attacker can happily wait hours for
   the transaction to be included in a block before trying to get beer
   from you
In general (IMO), just leaving standardness checks turned on (and waiting
for 6 confirmations before accepting any non-standard transaction) should
be enough to keep you safe from any attack a soft-fork might enable.
Upgrading your software regularly should also be enough to keep you safe
for any soft-fork, and also for any hard-fork, obviously.
[0] Actually, for this attack Mallory could use *any* segwit payment, it
    doesn't have to be his bitcoins to start with, he just has to make
    it look like they finish up with him, which is trivial if segwit
    looks like anyone can spend. Having it be his segwit payment in the
    first place makes it a little easier to ensure his payment is seen
    as the original and not the doublespend though.

@_date: 2015-12-21 18:07:47
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
+1's are off-topic, but... +1. My impression is that each of libsecp256k1,
versionbits, segregated witness, IBLT, weak blocks, and OP_CSV have
been demonstrated to be significant improvements that are implementable,
and don't introduce any new attacks or risks [0]. There's some freaking
awesome engineering that's gone into all of those.
I think the following proposed features are as yet missing from Pieter's
segwit branch, and I'm guessing patches for them would be appreciated:
 - enforcing the proposed base+witness/4 < 1MB calculation
 - applying limits to sigops seen in witness signatures
I guess there might be other things that still need to be implemented
as well (and presumably bugs of course)?
I think I'm convinced that the proposed plan is the best approach (as
opposed to separate base<1MB, witness<3MB limits, or done as a hard fork,
or without committing to a merkle head for the witnesses, eg), though.
jl2012 already pointed to a draft segwit BIP in another thread, repeated
here though:
 aj (hoping that was enough content after the +1 to not get modded ;)
[0] I'm still not persuaded that even a small increase in blocksize
    doesn't introduce unacceptable risks (frankly, I'm not entirely
    persuaded the *current* limits don't have unacceptable risk) and that
    frustrates me no end. But I guess (even after six months of reading
    arguments about it!) I'm equally unpersuaded that there's actually
    more to the intense desire for more blocksize is anything other than
    fear/uncertainty/doubt mixed with a desire for transactions to be
    effectively free, rather than costing even a few cents each... So,
    personally, since the above doesn't really resolve that quandry
    for me, it doesn't really resolve the blocksize debate for me
    either. YMMV.

@_date: 2015-11-30 09:41:43
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [BIP] OP_CHECKPRIVPUBPAIR 
This wouldn't be a softfork -- a single prefixed 0x01 byte would just
push "OP_CRYPTO" onto the stack... If you had OP_CRYPTO look at the top
item on the stack to determine what to do, you could have:
  OP_[0-16] OP_CRYPTO
  OP_PUSHDATA1 [0x11-0xFF] OP_CRYPTO
  OP_PUSHDATA2 [0x00-0xFF] [0x01-0xFF] OP_CRYPTO
  ...
to get 17 different crypto ops in two bytes, and the next 238 in three,
and be arbitrarily expandable from there with multibyte pushes.
I think that's good enough to try them out in prototypes though -- and
presumably if they're demonstrably useful in prototypes that's a good
argument for adding a dedicated op code to script?
Are there any other crypto ops that might be worth adding into a BIP for
a check-verify crypto toolkit op like this? The only ones that come to mind
as having practical uses in the near term are:
 Base-point multiply on secp256k1 (ie, CHECKPUBPRIVPAIR)
 Schnorr-signature of transaction with secp256k1 curve (smaller,
   faster, more-anonymous N-of-N multisig)
But perhaps there's also uses for some of:
 General point addition on secp256k1
 General point multiply on secp256k1
 SHA3-256 / SHA2-512 / SHA3-512
 ECDSA/Schnorr signature of value from stack
 ...?
Then again, I gather that if the segregated witness soft-fork turns out
to be plausible, re-enabling/changing/adding *any* sort of opcode could
be done as a soft-fork, not just turning a NOP into CHECK_foo_VERIFY...
So it might be better to wait and see how that goes before putting too
much time into drafting a BIP or similar?

@_date: 2015-10-04 18:35:25
@_author: Anthony Towns 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Do you mean "with RCLTV alone" here?
RCLTV/OP_CSV is used in lightning commitment transactions to enforce a
delay between publishing the commitment transaction, and spending the
output -- that delay is needed so that the counterparty has time to
prove the commitment was revoked and claim the outputs as a penalty.
Using absolute CLTV instead would mean that once the effective delay a
commitment transaction has decreases over time -- initially it will be
longer than desirable, causing unwanted delays in claiming funds when no
cheating is going on; but over time it will become too short, which
means there is not enough time to prove cheating (and the channel has to
be closed prematurely). You can trade those things off and pick
something that works, but it's always going to be bad.
Compared to using a CLTV with 30 days duration, With RCLTV a channel
could be available for years (ie 20x longer), but in the case of problems
funds could be reclaimed within hours or days (ie 30x faster).
But that's all about RCLTV vs CLTV, not about RCLTV vs nSequence/OP_CSV.
ie, it needs BIP 112 (OP_CSV) but not necessarily BIP 68 (nSequence
relative locktime), if they could be disentangled.
You could do all that with " OP_CHECK_HEIGHT_DELTA_VERIFY"
that ignores nSequence, and directly compares the height of the current
block versus the input tx's block (or the diff of their timestamps?)
though, I think?
I think the disadvantage is that (a) you have to look into the input
transaction's block height when processing the script; and (b) you don't
have an easy lookup to check whether the transaction can be included in
the next block.
You could maybe avoid (b) by using locktime though. Have "
OP_CHECK_RELATIVE_LOCKTIME_VERIFY" compare the transactions locktime
against the input's block height or time; if the locktime is 0 or too low,
the transaction is invalid. (So if nLockTime is in blockheight, you can
only spend inputs with blockheight based OP_CRLTV tests; and if it's in
blocktime, you can only spend inputs with blocktime based OP_CRLTV. "n"
does need to encode whether it's time/block height though).
That way, when you see a txn:
 - run the script. if you see  RCLTV, then
    + if the tx's locktime isn't set, it's invalid; drop it
    + if the input txn is unconfirmed, it's invalid; try again later
    + workout "locktime - n" if that's >= the input tx's block
      height/time, it's good; keep it in mempool, forward it, etc
 - if you're mining, include the tx when locktime hits, just like you
   would any other valid tx with a locktime
I think the use cases for BIP68 (nSequence) are of the form:
 1) published input; here's a signed tx that spends it to you, usable
    after a delay. might as well just use absolute locktime here, though.
 2) here's an unpublished input, you can build your own transaction to
    spend it, just not immediately after it's published. BIP112 is
    required, and OP_RCLTV as defined above works fine, just include it
    in the published input's script.
 3) here's an unpublished input, and a signed transaction spending it,
    that you can use to spend it after a delay. BIP68 is enough; but
    OP_RCLTV in the second transaction works here. however without
    normalised tx ids, the input could be malleated before publication,
    so maybe this use case isn't actually important anyway.
So I think OP_CRLTV alone works fine for them too...
(Does that make sense, or am I missing something obvious?)

@_date: 2015-10-06 16:20:31
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Old nodes running bitcoind will see it as OP_NOP2, and will reject it
unless they've manually disabled SCRIPT_VERIFY_DISCOURAGE_UPGRADABLE_NOPS,
which (aiui) has been available since bitcoin 0.10 [0], but not backported
to 0.8 or 0.9.
[0] That covers about 4700/5880 nodes going by bitnodes.21.co; but I can't
tell how many miners it covers.
Further, AIUI, nodes running 0.8 or 0.9 will still apply IsStandard()
checks to scripts attempting to spend p2sh outputs [1], so will also
fail to either mine or relay your OP_NOP2 payment.
[1] My understanding is that this isn't supposed to be a problem because you
won't be able to find an old miner that will do that; released versions
of bitcoin already block it by default.
Sure, someone could disable those checks and not pay attention to a soft
fork that will cause their blocks to be orphaned, but I'm not seeing why
that's any different a threat compared to someone deliberately mining
invalid blocks to do 1-confirmation doublespends against merchants not
running a full node.
At least, that's my understanding, and I'm not an expert, so corrections

@_date: 2015-10-08 01:00:14
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I think I finally understand this objection.
For a hard fork, activated by a majority of nodes/hashpower upgrading
to a new bitcoin release, the behaviour is:
 - upgraded bitcoin nodes: everything works fine
 - non-upgraded bitcoin nodes: total breakage. there will be a push
   alert telling you to upgrade. anyone who doesn't will think they're
   tracking "bitcoin" but will actually be tracking a new "bitcoin-old"
   altcoin. most non-upgraded miners will presumably realise they're
   wasting hashpower and stop doing this pretty quick; and remaining
   miners will only create blocks very slowly due to sudden reduced
   hashpower, without possibility of difficulty adjustment. users who
   don't uprade will try to do transactions, but won't see them confirm
   for hours or days due to lack of hashpower.
 - SPV nodes: they track the upgraded majority, everything works fine
   even if they don't upgrade
For a soft fork, again activated by the majority of upgraded hashpower,
the behaviour is:
 - upgraded bitcoin nodes: everything works fine
 - non-upgraded bitcoin miners willing to mine newly unacceptable txs:
   may produce orphaned blocks; may be able to be forced into producing
   blocks that will be orphaned
 - other non-upgraded bitcoin nodes: everything works fine
 - SPV nodes: partial breakage -- may track invalid blocks for 1-2
   confirmations until the set of "non-upgraded bitcoin miners willing
   to produce newly unacceptable txs" becomes vanishingly few.
In the hard fork case, all non-upgraded nodes get a DoS attack, but
aren't likey to be hit by doublespends. That's inconvenient, but it's
not too bad.
In the soft fork case, if there's likely to be old nodes mining
previously invalid transactions, SPV clients become very unreliable,
to the point of possibly seeing semi-regular double-spends with 1 or
2 confirmation, until miners that aren't paying attention notice their
blocks are getting orphaned and upgrade. That is pretty bad IMHO; and
there are a lot more *people* running SPV clients than bitcoin nodes,
so its impact is potentially worse in both ways.
Comparing generic hard forks versus generic soft forks, the above says
to me that a hard fork would be less harmful to users in general, and
thus a better approach.
*But* a soft fork that only forbids transactions that would previously
not have been mined anyway should be the best of both worlds, as it
automatically reduces the liklihood of old miners building newly invalid
blocks to a vanishingly small probability; which means that upgraded
bitcoin nodes, non-upgraded bitcoin nodes, /and/ SPV clients *all*
continuing to work fine during the upgrade.
AFAICS, that's what BIP65 achieves, as will similar OP_NOP* replacements
like BIP112.
But that only applies to a subset of potential soft forks, not every
soft fork.
Maybe a good way to think about it is something like this.  Consensus
(IsValid) is always less restrictive than (default) policy (previously
IsStandard, not sure how to summarise it now, maybe it's just OP_NOP
redefinition?).  So choosing a new consensus rule will be one of:
  * even less restrictive than consensus (hard fork)
  * more restrictive than consensus, but less restrictive than policy
    (safe soft fork)
  * more restrictive than IsStandard etc (damaging soft fork)
Hmm, in particular, following this line of thinking it's not clear to
me that BIP68 is actually less restrictive than current policy? At
least, I can't see anything that prevents txs with nSequence set to
something other than 0 or ~0 from being relayed?
If it's not, and nodes currently happily mine and relay transactions
with nSequence set without caring what it's set to, doesn't this mean
BIP68 is of the "damaging soft fork" variety? That is, if it activated
as a soft-fork with a majority of miners using it, but a minority of ~5%
not upgraded, then
 - someone could construct an tx with nSequence set to sometime in
   the future, but not using OP_CSV
 - this tx would get relayed by old nodes (but not upgraded nodes
   due to CheckLockTime)
 - non-upgraded miners would mine it into a block immediately, which
   would then get orphaned by majority hashpower
 - before it got orphaned, non-upgraded nodes and SPV clients would
   be misled and vulnerable to double spend attacks of txs with 0, 1 or
   maybe 2 confirmations
(BIP65 with OP_CLTV and BIP112 with OP_CSV don't have that problem as
they both redefine a non-standard opcode and would not get relayed or
mined by old, non-upgraded nodes, and are thus "safe soft forks" per
above terminology. This is just BIP68)
Can anyone confirm or refute the above?

@_date: 2015-10-08 02:38:37
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
The point of that case is that there aren't such miners, so that exploit
doesn't apply.
In particular, AIUI, you'll have a hard job right now finding someone to
mine an OP_NOP2 transaction -- eligius might do it, but I don't think many
others will. And you also need your currently OP_NOP2-friendly miner not
to upgrade to an OP_CLTV-validating codebase, so I don't think eligius
will qualify there.
If you want XOR, you'd need something more like:
 OP_IF OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIGVERIFY
 OP_ELSE  OP_CLTV
 OP_ENDIF
But that' still fail IsStandard and DISCOURAGE_UPGRADABLE_NOPS checks
if you tried spending without a valid sig, so wouldn't be mined by
current nodes. (Not having a sig would also allow anyone to spend it to
themselves, so that might make it hard to use as a basis for double
spends anyway...)

@_date: 2015-10-10 17:23:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
As was discussed on the weekly meeting [0], turns out it *is* less
restrictive than current policy. IsStandardTx currently returns a failure
if the tx version is greater than 1, and per BIP68, nSequence will only
be inforced with tx version of 2 or greater.
So afaics, BIP 65 (OP_CLTV), BIP 68 (nSequence) and BIP 112 (OP_CSV)
are all "safe soft forks", and if activated won't cause SPV nodes to
see a significant uptick in reorgs, double-spends etc. (They'll still
be vulnerable to people deliberately spending hashpower to mine invalid
blocks, but that's a problem at any point, independent of whether a
soft-fork is underway)
[0]

@_date: 2015-10-13 02:33:47
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Glad you found it useful; there's a lot of subtleties in how this
stuff works, and I'm certainly still figuring it out.
Absolutely. So I think there are three scenarios where SPV clients get
affected by orphans, where full nodes wouldn't be worried:
 1. Independent of any soft/hard fork, someone just mines a completely
    invalid block containing invalid transactions (eg, spending a
    non-existant transaction) based of the best current valid block,
    and presents it to SPV clients.
    An SPV client will accept this as valid until it's orphaned by the
    rest of the network building two valid blocks.
    This is expensive since you could have mined a valid block and
    got 25+ bitcoin legitimately, and it also doesn't last very long
    (around 20m on average, but less if you're unlucky), and the timing
    is unpredictable. It also won't get relayed by nodes, so you have
    to do a Sybil attack against the SPV client as well. It's also only
    good for places that accept 1-confirmation transactions; so you're
    probably better off finding somewhere that accepts 0-confirmation
    transactions and doing a Finney attack, where you at least get to keep
    the 25+ bitcoin from the subsidy/fees.
    So it's possible in theory, but seems pretty unlikely to be worth
    the hassle in practice.
 2. There's a "damaging soft-fork", with lock-in occurring immediately
    after 95% of blocks claim to support the soft-fork.
    In this case upgraded miners will only build on blocks from other
    upgraded miners, but non-upgraded miners will build on blocks from
    upgraded miners or non-upgraded miners. But with a ratio of 95:5,
    upgraded miners will tend to find a block every 10.5 minutes, while
    non-upgraded miners will only find one every 3h20m. SPV clients will
    accept the latest block from whichever miner most recently found a
    block, upgraded or not.
    If one of the blocks mined by a non-upgraded node includes a
    transaction that was valid under the old rules, but is not under the
    new rules (which is entirely likely in the damaging soft-fork case),
    that transaction will be trivially vulnerable to double-spends. So
    until the remaining 5% of hashpower upgrades, SPV clients will
    easily be able to be spoofed roughly once every 3h20m (when a
    non-upgraded miner finds a block) for about 20m (until two upgraded
    blocks are found, and the non-upgraded block is orphaned).
    The cost of this attack is borne entirely by the non-upgraded
    miners, who are mining blocks that will always be orphaned -- ie,
    they're still spending electricity on maintaining 5% of hashpower,
    but their "blocks" are all orphaned so they're getting a return of
    0 BTC per day, instead of roughly 180 BTC per day. So presumably
    they won't continue wasting power too long, and this will only be a
    problem for a few days at most, but during that time SPV clients are
    quite vulnerable.
    Also, that assumes that the soft-fork activates immediately on
    hitting 95%. The versionbits proposal will add two weeks' delay
    between seeing 95% of nodes supporting the soft-fork, and enforcing
    the rules, which gives the 5% additional time to upgrade, which
    probably means there won't be much of a window left at all.
 3. AIUI, without versionbits soft-forks are done by bumping the
    block nVersion field (ie, from "2" currently to "3"); then enforcing
    the new rules on blocks with a bumped version; and finally orphaning
    blocks with the old version once 95% of the last 1000 blocks use
    the bumped version. (See BIP 34)
    This means that even with a "safe soft-fork", non-upgraded miners
    will have their blocks orphaned by upgraded miners immediately after
    the soft-fork is activated, and SPV clients will see a similar orphan
    rate (up to 1 in 20 blocks, seeing an block that will get orphaned
    for about ~20 minutes every 3h20m).
    However, in the "safe soft-fork" case, all the transactions included
    in the invalid blocks will also be acceptable to upgraded miners,
    and will likely be included in the replacement blocks anyway. So
    this should be an annoyance, rather than an actual problem. (In
    a safe soft-fork, any transaction you attempt to get only mined
    by non-upgraded nodes will be picked up by upgraded nodes anyway;
    and any transaction you attempt to get mined only be new nodes will
    be mined very quickly so non-upgraded nodes won't have a chance to
    mine a block that double-spends)
    Further while I don't think they actually do it currently, SPV
    clients *could* monitor the block version (they download it anyway),
    and simply decline to accept new version "2" blocks once version "3"
    is seen for 95% of the last 1000 blocks. Then they would not see any
    blocks that will get orphaned, in either safe or damaging soft-forks.
    They couldn't do something similar with versionbits in place, since
    the corresponding bit is cleared once the soft-fork activates. However
    this also means that with versionbits in use, upgraded miners have
    no reason to orphan blocks built by non-upgraded miners; so this
    isn't a problem in the first place...
This might have been confusing. A hard fork creates two separate
blockchains both starting from the genesis block. The old one that
obeys the old rules, call it "bitcoin-old" and the new one obeying the
new rules, call it "bitcoin-new". The first block making use of the new
features will be unacceptablee on "bitcoin-old", and will be the point
of divergence. At that point, with 95% of hashpower on bitcoin-new,
it will see new blocks every 10.5 minutes, while with 5% of hashpower
bitcoin-old will only see new blocks every 3h20m. (With 75% hashpower,
bitcoin-new would see new blocks every 13m20s, while bitcoin-old would
see new blocks every 40m)
I'm assuming that as far as almost everyone is concerned, the blockchain
with the most hashpower (bitcoin-new in this case) would be called
"bitcoin", but I'm sure people would argue over it.
Since a majority of hashpower switched to a different chain, anyone
running non-upgraded nodes after a hard fork was activated would see far
fewer blocks being found (ie, with 5% of hashpower, that would be every 3
hours, rather than every 10 minutes). This would resolve itself when the
difficulty next reset, but that would be after 2016 blocks, which at 3h20m
per block would take about 9 months rather than the standard 2 weeks.
(With 25% of hashpower, bitcoin-old would see new blocks every 40
minutes, and difficulty would be reset after about 8 weeks)
Until the difficulty reset, if they were mining, they'd also see more
blocks being found by them (eg, if they had 2% of hashpower previously,
instead of finding 2% of blocks, they'd now be finding 40% of blocks,
ie 2/5 instead of 2/100).
Because a hard fork doesn't invalidate any transactions, non-upgraded
nodes would still see almost all the transactions intended for bitcoin-new
(excepting those from miners working on the fork or that explicitly use
new features enabled by the fork) so the mempool would grow pretty large,
given the low rate at which blocks are mined. (If the new feature is
something like a bigger blocksize, which then increases the rate of
transactions in bitcoin-new, then that's even worse for bitcoin-old
And, of course, those numbers get worse if the 5% of hashpower mining
bitcoin-old reduces as miners write it off as not-profitable.
(Maybe full bitcoin nodes should emit a warning that you've probably
been hard-forked off the main chain if they see, say, 4 or fewer blocks
in 5 hours -- with normal hashpower, I think that should only happen
once in something like 6000 years, but if you've got less than about
13% of hashpower left on your chain will happen about 50% of the time.
I don't know if that would actually be helpful compared to a pushed GPG
signed announcement of the hard-fork though)
Hmm. Depends what you mean by "users". If the user is running an
SPV wallet, they'll be following the most hashpower and will see
bitcoin-new. With 12 random connections (bitcoinj's default), I think
you'd just need ~1350 of 6000 nodes to have upgraded to have a 95%
chance of seeing bitcoin-new somewhere. So I don't see SPV users having
any problems.
But in the quote above I was talking about users who are running bitcoin
core rather than an SPV client. *They* would have the same problems
mentioned above, because non-upgraded bitcoin core would just totally
ignore the bitcoin-new blockchain.
So if they published a transaction, it would get confirmed quickly on
bitcoin-new, sure, but they wouldn't see that confirmation because their
software is deliberately ignoring bitcoin-new. They'd instead see it as
unconfirmed until it was included in a bitcoin-old block, but those only
come every 40m (25% hashpower) or every 3h20m (5% hashpower).
Worse, most of the bitcoin-new transactions are still valid for
bitcoin-old, so those transactions might get included by the remaining
miners in bitcoin-old -- so you'd have to pay higher fees to get
confirmed in 3h40m in bitcoin-old than you would to get confirmed in
13m in bitcoin-new...
TL;DR: I think my conclusions are:
 - gads this stuff is complicated
 - "safe soft-forks" really are safe (this covers BIP 65 (OP_CLTV) and
   BIPs 68 and 112 (OP_CSV))
 - as currently proposed, versionbits will actually make "damaging
   soft-forks" pretty safe too
 - if there's a hard fork in the wind, and you're running a full node,
   make sure you're on the latest version. maybe run an SPV client as well
   and check you're getting the same answers from both, just to be safe.

@_date: 2015-10-13 03:06:37
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Actually, just to take this scenario more explicitly...
Say you've got 5% of hashpower running on old software, along with,
say, 1500 nodes; and meanwhile you've got 95% of hashpower running new
software, along with 4000 nodes.
There's still about 750 nodes running 0.9 or 0.8 of 5400 total according
to bitnodes.21.co/nodes, so those numbers seems at least plausible to
me for the first week or two after a soft-fork is activated.
Eventually an old-rules block gets found by the 5% hashpower. The 4000
new nodes and 95% of hashpower ignore it, of course. With 8 random
connections, old nodes should have 92% chance of seeing an old node
as a peer, so I think around ~1300 of them should still be a connected
subgraph, and the old-rules block should get propogated amongst them
(until two new-rules blocks come along and orphan it).
An SPV client with 12 random connections here has 96% chance of having one
of the ~1300 old nodes as a peer, and if so, will see the old-rules block,
that will be orphaned, and may be at risk from double-spends as a result.
So I think even with just 5% hashpower and ~30% of nodes left running
the old version, a "damaging soft fork" still poses a fairly high risk to
someone receiving payments via an SPV client, and trusting transactions
with few confirmations.

@_date: 2016-02-07 21:37:57
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
To me, it seems absurd to have a hardfork but not take the opportunity
to combine these limits into a single weighted sum.
I'd suggest:
   0.5*blocksize + 50*accurate_sigops + 0.001*sighash < 2,000,000
That provides worst case blocksize of 4MB, worst case sigops of 40,000
and worst case sighash bytes of 2GB. Given the separate limit on sighash
bytes and the improvements from libsecp256k1 I think 40k sigops should
be fine, but I'm happy to be corrected.
For a regular transaction, of say 380 bytes with 2 sigops and hashing
about 800 bytes, that uses up about 291 units of the limit, meaning
that if a block was full of transactions of that form, the limit would
be 6872 tx or 2.6MB per block (along with 13.7k sigops and ~5.5MB hashed
for signatures).  Those weightings could probably be improved by doing
some detailed analysis and measurements, but I think they're pretty
reasonable for round figures.
The main advantage is that it would prevent blocks being cheaply filled
up due to hitting one of the secondary limits but only paying for the
contribution to the primary limit (presumably block size), which avoids
denial of service spam attacks.
I think having the limit take UTXO increase (or decrease) into effect
would be helpful too; but I don't have a specific suggestion. If it's
just a matter of making the limit stronger (eg adding "0.25*max(0,change
in UTXO bytes)" to the formula on the left, but not changing the limit on
the right), that would be a soft-forking change that could be introduced
later, and maybe that's fine.
If there was time to actually iterate on this proposal, rather than an
apparent aim to get it out the door in the next month or two, I think it
would be good to also design it so that the parameters of the weighted
sum could be adjusted by a soft-fork in future rather than requiring a
hard fork every time a limit's reached, or a weighting can be relaxed.
But I don't think that's feasible to design within a few weeks, so I
think it's off the table given the activation goal.

@_date: 2016-02-08 01:19:27
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
The stated reasoning for 75% versus 95% is "because it gives "veto power"
to a single big solo miner or mining pool". But if a 20% miner wants to
"veto" the upgrade, with a 75% threshold, they could instead simply use
their hashpower to vote for an upgrade, but then not mine anything on
the new chain. At that point there'd be as little as 55% mining the new
2MB chain with 45% of hashpower remaining on the old chain. That'd be 18
minute blocks versus 22 minute blocks, which doesn't seem like much of
a difference in practice, and at that point hashpower could plausibly
end up switching almost entirely back to the original consensus rules
prior to the grace period ending.
With a non-consensus fork, I think you need to expect people involved to
potentially act in ways that aren't very gentlemanly, and guard against
it if you want the fork to be anything other than a huge mess.

@_date: 2016-02-08 01:25:40
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Making a 2MB blocksize hardfork safer 
Hello world,
The core roadmap calls for having patches at the ready for
implementing hardforking blocksize increases [0]. However, at least
to my understanding, is that the deployment of segregated witness has
a significant impact on what a hardforking blocksize increase should
look like -- with segwit, the increase in the blocksize may have to
be traded off against decreasing the witness discount; without segwit,
alternative changes might need to be made to provide some of the other
benefits of segwit without segwit (in particular, additional limits to
prevent hashing massive amounts of data when checking sigs or to reduce
worst-case UTXO growth).
I don't personally have any real concerns that segregated witness will be
too complicated to implement and release by April, and given how quickly
CLTV rolled out, my guess is it will be usable prior to the block reward
halving. I'm also not terribly worried about fees rising significantly,
or that there will be a "fee event" [1] or "market disruption" -- fees
don't seem to be rising even with the spam attacks we've seen, and all
the problems with transactions not confirming that I've been able to see
so far seem to be due either to people trying to do free transactions,
fees not being calculated based on transaction size, or not checking
for dust outputs, all of which are things that can be dealt with by
individual wallets. [2]
But those are guesses and opinions, and I think it makes sense to have
a backup plan if everything goes horribly wrong -- someone discovers
a problem with segwit that requires major rearchitecturing to fix and
won't happen until 2017, eg.
To me, Gavin's BIP [3] and the Bitcoin Classic approach don't seem like
a good backup plan; but I don't see why they couldn't be *made* into a
good plan. In particular, if segwit turns out too hard to actually deploy
safely, I think Gavin's patchset -- an increase to ~2MB, coupled with
accurate counting and limiting of sighash bytes, and pretty much nothing
else -- is about the right set of *technical* things to do as a backup plan.
So the following are my suggestions for making Gavin's BIP workable
procedurally/politically as a backup plan. But that said, I don't know
if this is even remotely acceptable politically; I'm just following
bitcoin as a hobby and I don't have any backchannel contacts in mining
or bitcoin startups or anything.
1. Level of supermajority
First, it was reported that the Chinese miners came up with a 2MB
blocksize plan in late January [4], with the following summarised plan:
]  If:
]    1: Blocks are full
]    2: Core proposal is <2MB
]    3: Classic proposal have not gained consensus
]  Then:
]    Under the 90% hash power condition, switch from a 1MB limit to a
]    2MB limit to deal with the block size problem.
The summary also expresses concerns about segwit deployment; that it
makes significant changes, and that any issues with reliability may have
major impact. Those seem like valid concerns to me; though if they are
not addressed directly, then I expect miners will simply not enable the
segwit soft-fork until they are.
I think the only change to make this match Gavin's code for Bitcoin
Classic then is to require 90% hashpower support rather than 75%. I think
that can be easily done by a soft-forking change where miners reject any
block with a Classic vote (ie a version of 0x10000000) if the block height
is cleanly divisible by 6 [5]. As this is a soft-forking change, and one
that's only relevant until either Classic activates or the 2MB hardfork
attempt is permanently aborted on 2018-01-01, it seems like it could
easily be deployed prior to either segwit or Classic voting beginning.
2. Activation Time
The activation time for Gavin's BIP is very short -- 1000 blocks for
voting could be as short as 6 days, followed by 28 days grace period.
I haven't seen any indication that there is an immediate crisis, or
that there will be one in the next few months; and the fact that the
BIP does not expire for two years seems to indicate it's not a short
term issue. Allowing three to six months before attempting to activate
the hardfork seems like it would still provide plenty of opportunity to
address the issue quickly, and would also mean there was time to see if
the segwit rollout worked as planned.
That also could be enforced by a soft-fork: eg having a rule that until
the median time past is 2015-05-27, any block voting for the 2MB hardfork
will be rejected, would ensure the hard fork was not activated until
1st of July. A slightly more complicated rule, eg only rejecting the
blocks if the last three decimal digits of its height was 500 or greater,
would allow support to be measured in the leadup to possible activation,
without any risk of activation happening early.
3. Upgrade encouragement
I think there's three ways the 2MB hardfork could go: (a) not ever being
activated at all, similar to XT; (b) being activated with effective
consensus, where everyone switches to the hard-fork, whether happily
or not; or (c) being activated, but with the old chain being actively
mined and used on an ongoing, long-term basis.
If the 2MB blocksize hardfork is deployed as a fallback after segwit
deployment has failed, or determined to be much more complicated than
currently believed, then it seems like (c) would be a pretty undesirable
The only way I can see of avoiding/discouraging (c) is to have the new
hardfork be merge-minable with the existing chain, and having every
block in the new chain also commit to a merged-mined empty block on the
old chain, so that as long as the new chain has more hashpower than the
old chain, the longest valid old chain will have no outgoing payments
after the hardfork activates. (That requirement could probably be safely
dropped after some number of blocks, perhaps 25000 or 6 months?)
Alternatively, if the old blockchain has 10% or less hashpower remaining
(due to the 90% activation above), then the new chain has 9x the
hashpower. Perhaps a rule such that every 8th block in the hard-forked
chain must include an OP_RETURN in the coinbase that provides a valid,
empty block for the old chain. With a 90%/10% split, this would ensure
that the empty chain had more work than any other attempt at extending
it. However at the next difficulty change for the old chain (decreasing
by a factor of 4, presumably), I think they'd have to be mined every
second block rather than every 8th, and by the second difficulty change,
would need to be mined every block; otherwise I think 10% of hashpower
could catch up in chain-work. (Again, the requirement could probably be
dropped entirely after 6 months, or similar)
I believe this latter approach could be implemented as a soft-fork on
top of Gavin's BIP / Bitcoin Classic, provided activation was at 90% [7].
In this scenario, it would be possible for miners to simply sell empty
blocks on the old chain once they find them, so finding an empty block
for the old chain could plausibly be independent of finding the new
block for the new chain.
I think those three changes, which all should be implementable as
soft-forks compatible with Gavin's current code (the first two only
relevant prior to activation; the last only relevant after activation),
would mitigate what I see as the biggest risks of classic:
 - low-consensus/controversial activation
 - short preparation time, and resulting uncertainty and pressure
 - non-trivial chance of old chain remaining active after activation
 - miners' and core's plans being ignored [8]
And I think that would make this BIP (for me) a workable backup plan in
the event segwit doesn't work as planned. And for a multi-billion dollar
service, backup plans seem like a worthwhile thing to have, even if it's
highly unlikely it will actually get used.
However, these are all ideas where the benefits are basically "political"
rather than "technical", and I have no idea if the above *actually* makes
sense... And I guess trying to establish that is probably off-topic for
bitcoin-dev anyway? Anyway, as a consequence I've no idea if a write up
as a BIP and/or patches to implement any/all of the above as soft-forks
for classic/core that could be activated would be interesting for anyone,
and beyond posting about the ideas here, no idea how to find out. It
seemed like an interesting thought experiment to me, anyway. Apologies
in advance if it turns out I'm alone in that :)
[0] "Finally--at some point the capacity increases from the above may not
    be enough.  Delivery on [various improvements], and other advances
    in technology will reduce the risk and therefore controversy around
    moderate block size increase proposals (such as 2/4/8 rescaled to
    respect segwit's increase). Bitcoin will be able to move forward
    with these increases when improvements and understanding render
    their risks widely acceptable relative to the risks of not deploying
    them. In Bitcoin Core we should keep patches ready to implement them
    as the need and the will arises, ..."
        via [1] [2] I do think that, without segwit or a blocksize increase, there will be
    a discontinuity for venture funded bitcoin companies, because the
    transactions per second metric will become capped by the end of
    2016. I've argued that at:
        but I have not seen anyone from the a VC-backed bitcoin company
    actually confirm that's a concern, so perhaps it isn't something
    worth worrying about even there.
[3]     [4] [5] In that case, if 90% of miners by hashpower actually support the BIP,
    that would imply that 1/6th of blocks artificially don't vote for
    it, but 90% of the remaining 5/6th of blocks do, and 90% of 5/6th
    gives the 75% activation threshold specified in Gavin's BIP.
[6] [7] With activation at 75%, you'd need to dedicate 1/3rd of hashpower
    to mining empty old blocks to stay in the lead, which would then mean
    the hashpower for the new proof-of-work would only be half what it
    had previously been, and you'd end up with blocks taking 20 minutes
    on the new chain, and at least every second block including an empty
    block on the old chain. You could probably fix this by having the
    difficulty artificially halve when the hardfork activates though.
[8] Miners agree to 90% majority, code comes out with 75% majority. In
    December, core announces plans to deploy segwit with 1.6x capacity
    increase by April; Classic appears in January planning to do a hard
    fork with 2x capacity increase in/around March.

@_date: 2016-02-08 12:44:32
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Hardfork bit BIP 
This isn't true for soft-forks that only forbid transactions that would
already be rejected for forwarding and mining due to being non-standard.
I agree on that point; but ensuring soft-forks only affect non-standard
transactions already addresses that concern in every way I've been able
to discover.

@_date: 2016-02-09 19:00:02
@_author: Anthony Towns 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Is this intended to be activated soon (this year?) or a while away
(2017, 2018?)?
This would mean the limits go from:
   pre-segwit  segwit pkh  segwit 2/2 msig  worst case
   1MB         -           -                1MB
   1MB         1.7MB       2MB              4MB
   1.5MB       2.1MB       2.2MB            3MB
That seems like a fairly small gain (20% for pubkeyhash, which would
last for about 3 months if you're growth rate means doubling every 9
months), so this probably makes the most sense as a "quick cleanup"
change, that also safely demonstrates how easy/difficult doing a hard
fork is in practice?
On the other hand, if segwit wallet deployment takes longer than
hoped, the 50% increase for pre-segwit transactions might be a useful
Doing a "2x" hardfork with the same reduction to a 50% segwit discount
would (I think) look like:
   pre-segwit  segwit pkh  segwit 2/2 msig  worst case
   1MB         -           -                1MB
   1MB         1.7MB       2MB              4MB
   2MB         2.8MB       2.9MB            4MB
which seems somewhat more appealing, without making the worst-case any
worse; but I guess there's concern about the relay networking scaling
above around 2MB per block, at least prior to IBLT/weak-blocks/whatever?
This could potentially make old, signed, but time-locked transactions
invalid. Is that a good idea?
I think turning MAX_BLOCK_SIGOPS and MAX_BLOCK_SIZE into a combined
limit would be a good addition, ie:
   MAX_BLOCK_SIZE       1500000
   MAX_BLOCK_DATA_SIZE  3000000
   MAX_BLOCK_SIGOPS     50000
   MAX_COST             3000000
   SIGOP_COST           (MAX_COST / MAX_BLOCK_SIGOPS)
   BLOCK_COST           (MAX_COST / MAX_BLOCK_SIZE)
   DATA_COST            (MAX_COST / MAX_BLOCK_DATA_SIZE)
  if (utxo_data * BLOCK_COST + bytes * DATA_COST + sigops * SIGOP_COST
       > MAX_COST)
  {
      block_is_invalid();
  }
Though I think you'd need to bump up the worst-case limits somewhat to
make that work cleanly.
Could you just use leading non-zero bytes of the prevhash as additional
So to work out the actual prev hash, set leading bytes to zero until
you hit a zero. Conversely, to add nonce info to a hash, if there are
N leading zero bytes, fill up the first N-1 (or less) of them with
non-zero values.
That would give a little more than 255**(N-1) possible values
((255**N-1)/254) to be exact). That would actually scale automatically
with difficulty, and seems easy enough to make use of in an ASIC?

@_date: 2016-02-10 15:16:56
@_author: Anthony Towns 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
I was thinking ASICs could be passed a mask of which bytes they could
use for nonce; in which case the variable-ness can just be handled prior
to passing the work to the ASIC.
But on second thoughts, the block already specifies the target difficulty,
so maybe that could be used to indicate which bytes of the previous hash
must be zero? You have to be a bit careful to deal with the possibility
that you just did a maximum difficulty increase compared to the previous
block (in which case there may be fewer bits in the previous hash that
are zero), but that's just a factor of 4, so:
     RETARGET_THRESHOLD ((1ul<<24) / 4)
    y = 32 - bits[0];
    if (bits[1]*65536 + bits[2]*256 + bits[3] >= RETARGET_THRESHOLD)
        y -= 1;
    memset(prevhash, 0x00, y); // clear "first" y bytes of prevhash
should work correctly/safely, and give you 8 bytes of additional nonce
to play with at current difficulty (or 3 bytes at minimum difficulty),
and scale as difficulty increases. No need to worry about avoiding zeroes
that way either.
As far as midstate optimisations go, rearranging the block to be:
 version ; time ; bits ; merkleroot ; prevblock ; nonce
would mean that the last 12 bytes of prevblock and the 4 bytes of nonce
would be available for manipulation [0] if the first round of sha256
was pre-calculated prior to being sent to ASICs (and also that version
and time wouldn't be available). Worth considering?
I don't see how you'd make either of these changes compatible
with Luke-Jr's soft-hardfork approach [1] to ensuring non-upgraded
clients/nodes can't be tricked into following a shorter chain, though.
I think the approach I suggested in my mail avoid Gavin's proposed hard
fork might work though [2].
Combining these with making merge-mining easier [1] and Luke-Jr/Peter
Todd's ideas [3] about splitting the proof of work between something
visible to miners, and something only visible to pool operators to avoid
the block withholding attack on pooled mining would probably make sense
though, to reduce the number of hard forks visible to lightweight clients?
[0] Giving a total of 128 bits, or 96 bits with difficulty such that
    only the last 8 bytes of prevblock are available.
[1] [2] [3]     In particular, the paragraph beginning "Alternatively, if the old
    blockchain has 10% of less hashpower ..."

@_date: 2016-02-26 13:20:56
@_author: Anthony Towns 
@_subject: [bitcoin-dev] SIGHASH_NOINPUT in Segregated Witness 
+1 to both
I think the idea is that you have three transactions:
 anchor:
   input: whatever
   output:
     - single output, spendable by 2-of-2 multisig
     - [possibly others as well, whatever]
 commitment:
   input: anchor
   outputs:
     1. payment to A
     2. payment to B
     3. HTLC to A on R1, timeout T1
     4. HTLC to A on R2, timeout T2
     5. HTLC to B on R3, timeout T3
     ...
 penalty:
   inputs:
     all the outputs from the commitment tx
   outputs:
     1. 99% as payment to me
     2.  1% as outsourcing fee
As long as the key I use for spending each of commitment transactions
outputs is "single use" -- ie, I don't use it for other channels or
anywhere else on the blockchain, then as long as the signature commits
to the outputs it's safe afaics.
(You still have to send a lot of data to the place you're outsourcing
chain-monitoring to; all the R1,R2,R3 and T1,T2,T3 values are needed in
order to reconstruct the redeem scripts)
If the fee for commitment transactions changes regularly (eg, a new
commitment transaction is generated every few seconds/minutes, and the fee
is chosen based on whatever estimatefee returns), I think this would cause
problems -- you couldn't use a single signature to cover every revoked
commitment, you'd need one for each different fee level that you'd used
for the lifetime of the channel. Actually, the size of the commitment
transaction will differ anyway depending on how many HTLCs are open,
so even if estimatefee didn't change, the fee would still differ. So I
think commiting to a fee isn't workable for the lightning use case...
+1, though I'm not sure it's so much "vulnerable" to replay as it is
"explicitly designed" to be replayable...

@_date: 2016-01-09 01:33:29
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
Using Ethan Heilman's procedure, the attacker can create two scripts:
  2 V __A1__ 2 CHECKMULTISIG
  2 V __A2__ 2 CHECKMULTISIG
and find values A1 and A2 which hash the scripts to the same result
with under 3*2**80 work. I think you can do that by setting the next
private key as the result of RIPEMD(SHA256(script with pubkey)), so you
could still spend either. But it doesn't change the script, so it's not
*that* helpful -- you've just got two different keys you can use.
Ah, but you can make the form of the script be a function of your key, so:
  if privkey % 2 == 0:
    script = "2 V %s 2 CHECKMULTISIG" % (pubkey)
  else:
    script = "%s CHECKSIG" % (pubkey)
  hash = ripemd160(sha256(script))
  nextprivkey = hash
Then you have a 50% chance of your cycle giving you a matching hash for
one script with A1 and the other script with A2, and you can find the
cycle with under 3*2**80 work. Doing five attempts should give you ~96%
chance of hitting a usable pair, and should take under 15*2**80 work ~=
2**84 work, with trivial memory use.
Trying that in python with a vastly weakened hash function (namely,
the first five bytes of ripemd160(sha256()), with 40 bits of security
and 3*2**20 work) works as expected -- I got a "useful" collision on my
second try in about 7 seconds, seeding with "grumpycat3" ("grumpycat2"
didn't work) with the result being:
 hexlify(ripemd160(sha256("foo%sbar"%unhexlify("86f9fbac1a")))[:5])
 'ae94d9f908'
 hexlify(ripemd160(sha256("baz%squux"%unhexlify("104fc5093f")))[:5])
 'ae94d9f908'

@_date: 2016-01-18 22:02:51
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
1.7MB effective block size is a better estimate than 1.6MB for p2pkh
  with segwit. 2MB for 2/2 multisig still seems accurate.
  Additional post-segwit soft forked script improvements can improve
  the effective block size for p2pkh txns from 1.7MB to 1.9MB, and for
  2/2 multisig from 2MB to 2.5MB/3MB.
  (To the best of my knowledge, anyway; if I've made a mistake in my
  maths or assumptions, corrections appreciated)
I think these numbers are slightly mistaken -- I was only aware of version
1 segwit scripts at the time, and assumed 256 bit hashes would be used
for all segwit transaction, however version 0 segwit txns would be more
efficient for p2pkh, with the same security as bitcoin currently has
(which seems fine).
Also, segwit will make two additional soft-fork improvements possible that
would have a positive effect on transactions per block without requiring
more data per block: ecdsa public key recovery (more space efficient for
*both* multisig and p2pkh) and schnorr signatures (more space efficient
multisig) which might also improve things. I don't know how soon they're
planned to be worked on post segwit's roll out; basic Schnorr signatures
are in the Elements sidechain, but I don't think key recovery has been
implemented anywhere? (Actually, I guess they could both be done already
via softforking OP_NOP opcodes, though segwit makes them slightly
Anyhoo here's some revised figures, working explained in the footnotes.
If I've made mistakes, corrections appreciated, of course.
  now: 10+146i+34o [0]
  segwit: 10+41i+36o + 0.25*105*i [1]
  ecdsa recovery: 10+41i+33o + 0.25*71*i [2]
  80-bit schnorr: 10+41i+33o + 0.25*71*i (same as ecdsa recovery imo [3])
  128-bit schnorr: 10+41i+43o + 0.25*106*i [4]
(128-bit schnorr provides a not very useful increase in security here)
2-of-2 multisig:
  now: 10+254i+32o [5]
  segwit: 10+43i+43o + 0.25*213*i [6]
  ecdsa recovery: 10+43i+43o + 0.25+187*i [7]
  80-bit schnorr: 10+41i+33o + 0.25*71*i (same as p2pkh)
  128-bit schnorr: 10+41i+43o + 0.25*106*i (same as p2pkh)
(segwit, ecdsa recovery and 128-bit schnorr all provide a beneficial
security increase here, as per the "Time to worry about 80-bit collision
attacks" thread; 80-bit schnorr provides the same security as current
p2sh multisig)
Using the same assumptions in the previous mail, ie that over the long
term number inputs is about the same as number of outputs, these
simplify to:
        p2pkh           2-of-2 msig
now     10+180i         10+286i
segwit  10+104i         10+140i
recov   10+92i          10+133i
sch80   10+92i          10+92i
sch128  10+111i         10+111i
Translating "now" to 100%, the scaling factors work out to be:
i=1, i->inf
        p2pkh           2-of-2 msig
now     100%            100%
segwit  166%-173%       197%-204%
recov   186%-195%       207%-215%
sch80   186%-195%       290%-310%
sch128  157%-162%       244%-257%
So 170% for p2pkh (rather my original estimate of 160%) and 200% for
multisig (same as my original estimate), which can rise via further
soft-forks up to 190% for p2pkh and 250% or 300% for 2-of-2 multisig
(depending on whether you want additional security for 2/2 multisig
beyond what's currently available).
(I'm assuming people are mostly interested in the number of transactions
per block (or tx/second or tx/day); if miners are worried about the
actual data per block (which effects orphan rates) implied by the above,
but don't want to work it out themselves, I could do the maths for that
too pretty easily. Let me know)
If a 2MB hard fork is done first, then the 1/4 discount for segwit could
mean up to 8MB of total data per block -- from what I understand this
is currently infeasible; so I presume that segwit on top of a hardfork
and prior to IBLT/weak blocks would need to have a smaller discount or
no discount applied so as to ensure total data per block remains at 4MB
or less. With no discount for witness data (ie, no "accounting tricks")
those figures look like:
        p2pkh           2-of-2 msig
now     100%            100%
segwit  99%             95%
recov   122%-124%       104%
sch80   122%-124%       191%-198%
sch128  94%-95%         148%-150%
That is, without discounting, segwit comes at a slight cost in
transactions per block, and additional soft forks will only result in
25% gain for p2pkh (via key recovery) and 50%-100% for 2-of-2 multisig
(through the use of schnorr sigs and key recovery, and depending on
whether you want 128 bits of security rather than 80 bits).
(So without the discounting factor, with a 2MB block size, 2MB per block
with segwit and key recovery gives you 25% more p2pkh transactions than
just 2MB per block now; while segwit and schnorr signatures gives you
50%-100% more 2/2 multisig transactions in the same 2MB. Likewise with
1MB or 4MB and no discounting. Discounting has the indirect benefit of
providing a monetary incentive to limit UTXO sizes however)
(2 of 3 multisig for escrow payments would probably be interesting to
work out too; I think ecdsa key recovery combined with 1/4 discounting
would provide a substantial improvement there. I don't think Schnorr
helps at all for that case, unfortunately; and it's probably too small
scale for merkle-ised abstract syntax trees to do any good either)
A caveat: I'm only counting the script data from witnesses here; but it's
possible that additional metadata (such as a length for each witness
signature, or the value of the input, or even some/all of the merkle
hashes) should also be accounted for. I don't think any of them need to
be accounted for segwit as proposed, but I'm not sure. And it might well
be different for a hardforked segwit; there I have no idea at all. I
don't think a byte or two for length would make much difference, at least.
[0] 10 bytes for version (4), input count (1), output count (1) and
    locktime (4); 146 bytes per input consisting of tx hash (32), txout
    index (4), script length (1), scriptsig (signature and pubkey =
    105), CHECKSIG = 25), and sequence number (4); 34 bytes per output
    consisting of value (8), script length (1) and scriptpubkey (DUP
    HASH160 PUSH20 EQVERIFY CHECKSIG = 25).
[1] Same as now, except two extra bytes per output script (segwit push and
    segwit version byte), and moving the 105 bytes of signature script
    directly into the segregated witness
[2] Allowing ECDSA recovery requires an additional soft-fork post segwit
    to change the CHECKSIG operation; this requires bumping the
    segwit script version to 2 or higher and possibly using a different
    opcode, but allows the scriptsig to just be the 70 byte signature,
    without also including the 33 byte pubkey. The 33 byte pubkey is
    automatically calculated from the signature, and verified against
    the hash provided in the scriptpubkey to maintain security, with a
    scriptpubkey like: [PUSH (20 byte pubkey hash) CHECKSIG_RECOVER] (22
    bytes versus 25 bytes), and a scriptsig like [PUSH (70 byte sig)]
    (71 bytes versus 105 bytes).
[3] libsecp256k1 has a function to recover a pubkey from a schnorr
    signature, so I'm assuming that means pubkey recovery with schnorr
    is possible :) -- I haven't actually verified the maths
    [4] The witness scriptpubkey is limited to 32 bytes (plus push op and
    version byte for a total of 34 bytes, so 128 bit security requires
    version 1 segwit, and p2sh-style constuction. Hence: 10 bytes
    (version, input and output counts and locktime); 41 base bytes per
    input (tx hash, tx index, script length, and sequence number); 106
    witness bytes per input (sig (70 bytes) plus witness script (PUSH
    schnorr merged pubkey (32 bytes) plus CHECKSCHNORR), plus PUSH ops);
    and 43 bytes per output (value, script length, and 34 bytes for the
    v1-style witness script).
[5] Per input is (32 bytes tx hash, 4 bytes tx index, 4 bytes nsequence,
    1 byte scriptsig length, 143 bytes for actual signature (2x70
    for the sigs, 3 bytes for OP_0 and two OP_PUSH), and 70 bytes for
    the redeemscript (2 pub pub 2 OP_CHECKMULTISIG)) for 254 bytes;
    per output is (8 bytes value, 1 byte length, 23 bytes for HASH160
    [20 byte hash] OP_EQUAL) for 32 bytes.
[6] Per input is (34 bytes tx hash, 4 bytes tx index, 4 bytes nsequence,
    1 byte scriptsig length) for 43 bytes in the base block and (143
    bytes for the actual signature, plus 70 bytes for the redeemscript)
    for 213 bytes of witness data; per output is (8 bytes value, 1 byte
    length, and 34 bytes for version 1 segwit scriptpubkey) for 43
    bytes.
[7] Same as [6], but with key recovery on a MULTISIG op, rather than 33
    bytes per pubkey, this could be reduced to a 20 byte pubkey hash
    per pubkey, for a saving of 26 bytes of witness data.

@_date: 2016-01-22 19:46:18
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Latest segwit code just has version 0 witness format, and treats a 32
byte push as the sha256 of a script, and a 20 byte push as the hash of
the pub key. Also, the witness scriptPubKey format uses "OP_0 [hash]" to
push the version and hash to the script separately, rather than "[0x00
script]" or "[0x01 hash]" (this changes allows segwit transactions to
be encoded backwards compatibly as a p2sh payment).
So this change means segwit p2pkh needs 31 bytes per output not 36 bytes (value,
length stay the same, scriptpubkey becomes "OP_0 PUSH20" for 22 bytes
instead of 25+2 bytes). This gives another couple of percent gain, so:
    segwit: 10+41i+31o + 0.25*105*i [1]
Setting i=o makes:
segwit    10+99i          10+140i
and therefore,
segwit    174%-181%       197%-204%
Constantly creeping up! Pretty nice.
Also, p2pkh with segwit-via-p2sh is probably interesting, those numbers
work out as:
segwit:   10+41i+31o + 0.25*105*i (for comparison)
segp2sh:  10+60i+32o + 0.25*105*i [0]
  ->      10+119i
  ->      147%-151%
So that still looks like a reasonable improvement even if (eg) in the
short term merchants are the only ones that upgrade, and customers just
use non-segwit-aware wallets with a p2sh address that's only redeemable
by a segwit-aware wallet.
[0] 10 bytes standard. For each input, tx hash (32) plus index (4),
    script length (1) and scriptsig which is a push of the standard segwit
    pubscript (22+1) totaling to 60, and witness data is the same as for
    normal segwit (105). Each output is standard p2sh, which is value
    (8), length (1) and script (23) for a total of 32.

@_date: 2016-01-31 01:32:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Segwit Upgrade Procedures & Block Extension Data 
If I'm following the code right, the segwit branch has a fHaveWitness
flag for each connection, which is set when a HAVEWITNESS message comes
from the peer, and HAVEWITNESS is sent as part of handshaking. BIP144
suggests maybe this should be changed to a service bit though:
If you've got a limit of 8 outgoing connections and >4 of them don't
support witnesses, would it be enough to just drop a non-witness
connection and try a new one? Or is anything less than 8 of 8 outgoing
witness supporting connections likely to be bad for the network?
Commitments to a merkle sum tree of transaction bytes, sigops, sighash
bytes, fees, priority, etc for small fraud proofs also fit here,
don't they?
This isn't necessarily true -- you could just make the coinbase witness
nonce become HASH(newinformation | newnonce), and put newnonce back into
the coinbase as an additional OP_RETURN, so that it can be verified.
If you want to have layered soft-forks adding new commitments, I think
you have to keep adding 32 byte hashed nonces; dropping them would be
a hard fork as far as I can see. So if there might eventually be three
or four of them, putting them in the witness block rather than the base
block seems sensible to me.
If the linked list approach in BIP141 is used, then I think the logic
could be something like this:
 * suppose there have been a few soft-forks and the coinbase witness
   now handles three commitments: segwit, prevblockproof, fraudproofs
 * then the coinbase witness stack should have at least 3 32-byte
   values pushed on it, which should evaluate to:
     s[1] = HASH( fraudproof-info ; s[2] )
     s[0] = HASH( prevblockproof-info ; s[1] )
   and the coinbase should include
     OP_RETURN SHA256d( segwit-info ; s[0] )
 * old segwit code works fine (assuming the stack.size() != 1 check is
   changed to == 0), just treating s[0] as a meaningless nonce
 * old prevblockproof supporting nodes check s[0], treating s[1] as a
   meaningless nonce, but still validating the prevblock proofs
 * nodes running the latest code validate all three checks, and will
   continue to work if new checks are soft-forked in later
But I think this seems just as good, and a fair bit simpler. ie with the
same three commitments, the coinbase witness contains:
     s[2] = nonce
     s[1] = fraudproof-info-hash
     s[0] = prevblockproof-info-hash
and coinbase includes:
     OP_RETURN SHA256d( HASH(segwit-info) ; s[0] ; s[1] ; s[2] ; ... )
I'm not sure if the use of a nonce has any value if done this way.
With this apporach, the commitments could be ordered arbitrarily --
you only have to check that "fraudproof-info-hash" is somewhere on the
stack of the coinbase witness, not that it's s[1] in particular.
I think it makes sense to plan on cleaning these up with infrequent
hard forks, eg one that combines the transactions hashMerkleRoot
and the witnessMerkleRoot and the fraudProofMerkleSumRoot and the
prevBLockProofHash into a single entry in the block header, but being
able to add features with soft-forks in the meantime seems like a win.
Including it in the coinbase witness already includes it in the blocksize
limit, albeit discounted, no? If someone wants to propose a soft-fork
adding 500kB of extra consensus-critical data to each block for some
reason, sticking it in the coinbase witness seems about the least
offensive way to do it?
If you look at the witness as a proof of "this is why it's okay to
accept this transaction" in general, and the coinbase witness is "this
is why it's okay to accept this block", then including prevblock proofs
and fraud proof info in the coinbase witness seems pretty logical... Or
at least, it does to me.
This topic seems to be being discussed in a PR on the segwit branch:
  where there's a proposal to have the coinbase witness include the merkle
path to the segwit data, so, if I understand it right, the coinbase
commitment might be:
  OP_RETURN Hash( s[0] || Hash( Hash( s[2] || segwit-data ) || s[1] ) )
if the path to the segwit data happened to be right-left-right. That
would still make it hard to work out the path to some proof that happened
to be in position right-right-left, though, so it seems inferior to the
approaches described above to me...

@_date: 2016-01-31 01:48:57
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Segwit Upgrade Procedures & Block Extension Data 
Oh, there's a PR to change this to a NODE_WITNESS service bit:

@_date: 2017-07-12 09:28:39
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
Timelines have good and bad points (in this context, I'd generally call
projections good, deadlines bad :); people have interpreted the lack of
any clear timeline for a hardmark on the 2015 roadmap as no plan for a
hard fork at all; meanwhile the overly optimistic timeline for segwit
being "ready" in April or July last year has been interpreted as "ready
for use" and treated as a failure, when it didn't work out that way.
I think it would be helpful for the development community to have some
way of talking about timelines, for instance to be able to say "the
*minimum* timeline for a reasonable hard fork is 6 months for proposal
review, speculative analysis and initial coding, 3 months for concrete
proposal review and thorough testing, 3-6 months for consensus to develop
on whether to lock the proposed changes in as the new consensus, and
a further 6-24 months for wide scale deployment to occur before any
behavioural change to take actual effect".
Those numbers give a lead time of 18 to 38 months of engagement with the
developer community before it takes effect, as compared to the six month
timeline of the New York agreement. 18 months implies that a block size
increase would be *available* today if people wanting larger blocks had
engaged with the development community from January 2016 in the same
way that segwit was developed, rather than working in their own sandboxes.
That could have happened: the proposals in   from Dec 2015 could have been engaged with, and, optimistically, we could
have a non-controversial deployment of SpoonNet already if they had been.
It might be a good idea to actually engage with investors and businesses
on this: the point of the timelines above isn't to slow things down for
its own sake, it's that you need to take time in order to think through
the potential consequences of changes, and avoid unintended bad outcomes.
That seems like something that investors and businesses can understand,
and endorse up front -- and they could meaningfully do so simply by saying
"any hard-fork that does not go through each of the stages for at least
the minimum time will be treated as an altcoin rather than an upgrade
of bitcoin". But the process has to be "here is what it takes from a
technical POV to avoid fucking up bitcoin; does your company endorse
being responsible with other people's money despite the costs of doing
so?" If you're in a move-fast-and-break-things mode, the answer might
legitimately be "no", of course.
I'd suggest dividing the activities into phases more clearly; maybe:
 - Already available to users:
     * version bits
     * compact block relay
     * FIBRE
     * CSV
     * better fee estimation
 - Awaiting consensus:
     * segregated witness
 - Active development / concrete specifications:
     * lightning network
     * light client mode for bitcoin core (PR
 - Draft proposals at experimental stage:
     * transaction compression? (or is this the already deployed stuff?)
     * schnorr sig aggregation
     * drivechain
     * spoonnet
     * mimblewimble
     * block size increases
 - Ideas that aren't even experiments yet
     * asicboost prevention
As above, it seems to me that 18 months of engagement is likely a bare
minimum amount of time for a robustly implemented hard fork (6 months is
almost exactly segwit2x's total timeline, from proposal in late May as
the New York Agreement to the new rules being available in mid-November,
and it doesn't look at all robust to me).  Possibly if the existing features of spoonnet are already adequate,
you could cut that down by a few months. But realistically, that says
to me early 2019 at the absolute earliest, and if engagement with the
development process doesn't start tomorrow, later than that.
FWIW, here's a longer form draft of what I think hard fork guidelines
maybe could look like:
  It's obviously blatantly contradictory with support of the NYA/segwit2x,
but at this point I think that's true of any process that's not just
a rephrasing of "move fast and break things".
Publishing something like this as an informational BIP every year or
two seems like a good idea to me.
Instead of a "roadmap" (with the implication that there's a schedule
people might rely on and developers have to meet), maybe just have it as
a list of the current high impact scaling features being worked on --
where the purpose of publishing the list is to let people understand
how far various ideas have progressed currently, and focus attention on:
  - wider adoption of already deployed features, by users, exchanges,
    wallets, etc; eg segwit doesn't scale anything if no one uses it
  - achieving activation of implemented features
  - encouraging R&D on approaches that are currently still experimental
    in order to make them actually usable
In that case, there's no actual need for guessing at future dates;
just the current status is sufficient.
Documenting current roadblocks might also be valuable (eg, lightning and
signature aggregation and drivechains etc are kind-of stalled waiting
on segwit's activation, I think; for a brief point, segwit was stalled
waiting on compact blocks, etc). Might not be worthwhile updating the doc
regularly to keep track of what's a roadblock though.
(I think you could usefully generalise beyond scaling to just "high
impact features" really)

@_date: 2017-07-13 11:48:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] how to disable segwit in my build? 
I think that terminology isn't quite precise. I think your options are:
 - if you're a miner or run a mining pool, you can *signal* (or not
   signal) support for segwit activation; you do this by controlling
   the block version
 - if you're running a node, you can choose to *enforce* (or not
   enforce) the additional consensus rules associated with segwit
I think it's the latter you're talking about. "Activation" is different:
it's the collective action of the bitcoin ecosystem to start enforcing
the segwit consensus rules after a sufficient majority of miners are
signalling support. That's not something you as an individual can control.
If you want to disable enforcement of segwit rules, even if a majority of
mining power signal activation, you can change the code and recompile to
do so, for example by changing the nTimeout setting for DEPLOYMENT_SEGWIT
from 1510704000 (Nov 15 2017) to 1493596800 (May 1 2017, already expired).
This is probably a bad idea, in that it will cause you to risk accepting
blocks that nobody else in the network will accept, opening you up
to higher risk of double spends, and may cause you to be unable to
peer with segwit enabled nodes after segwit is activated if your node
is rejecting blocks with witness data because you think segwit is not
enabled while they think it is enabled. To avoid that you would also need
to stop setting the NODE_WITNESS p2p bit, which you might be able to do
by setting the nTimeout above to 0 instead of just a date in the past? I
believe a timeout of 0 gets treated as automatically FAILED. There might
be other complexities too though.
The unwanted side-effects are precisely the reasons not to do it. If you
don't understand what they are, you won't be able to avoid them. :)

@_date: 2017-06-07 09:20:15
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable 
CoinJoin works as a method of both improving fungibility and mixing with
coinbase transactions.
You probably don't need to do anything clever to split a coin though:
if you send a transaction with a standard fee it will get confirmed
in a normal time on the higher hashrate chain, but won't confirm as
quickly on the lower hashrate chain (precisely because transactions are
valid on both chains, but blocks are found more slowly with the lower
hashrate). When it's confirmed on one chain, but not on the other, you
can then "double-spend" on the lower hashrate chain with a higher fee,
to end up with different coins on both chains.
(also, no double-n in untenable)

@_date: 2017-06-28 17:01:33
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Replay protection via CHECKSIG 
I thought of a possibly interesting way to prevent transaction replay in
the event of a chain split, that seems better to the other approaches
I've seen. Basically, update OP_CHECKSIG (and MULTISIG and the VERIFY
variants, presumably via segwit versioning or using a NOP opcode) so that
signatures can optionally specify an additional integer block-height. If
this is provided, the message hash is combined with the block hash at
the given height, before the signature is created/verified, and therefore
the signature becomes invalid if used on a chain that does not have that
particular block in its history [0].
It adds four bytes to a signature that uses the feature [1], along with
a block hash lookup, and some extra sha ops when verifying the signature,
but it otherwise seems pretty lightweight, and scales to an arbitrary
number of forks including a pretty fair range of hard forks, as far
as I can see, without requiring any coordination between any of the
chains. So I think it's superior to what Johnson Lau proposed in January
[2] or BIP 115 from last year [3].
Thoughts? Has this been proposed before and found wanting already?
[0] For consistency, you could use the genesis block hash if the signature
    doesn't specify a block height, which would lock a given signature to
    "bitcoin" or "testnet" or "litecoin", which might be beneficial.
[1] Conceivably a little less if you allow "-5" to mean "5 blocks ago"
    and miners replace a four byte absolute reference ("473000") with a
    one or two byte relative reference ("-206") when grabbing transactions
    from the mempool to put in the block template.
[2] [3]

@_date: 2017-05-20 15:05:43
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
Another approach to ensuring the timeout might be to simply use input height. ie:
  * if there is a BIP-9 soft-fork using bit N currently STARTED or
    LOCKED_IN phase. since the soft-fork is started, set the height of
    the first block after starttime as "S".
  * then a transaction is invalid in a block if:
     * the soft-fork has not timed out or activated
     * the block does not signal bit N
     * the transaction nversion does signal bit N (by whatever formula)
     * at least one input to the transaction has a height >= S
That's compatible with bit reuse: if a transaction designed to encourage
soft-fork foo with bit 1 does not get mined by the time foo finishes (by
timeout or success), then when soft-fork bar reaches STARTED phase while
reusing bit 1, the old transaction can be mined by either signalling
or non-signalling miners -- because all of the transaction inputs are
prior to bar's block S, the invalidation rule doesn't apply for bar, and
because foo has timed out or activated, it doesn't apply for foo either.
It means you can't directly use a bunch of old coins on their own to
incentivise miner signalling -- you need to include a coin from after
starttime. That doesn't seem terribly onerous though; and should be
easily solvable by just providing a coinjoin API anyway.  I think it's
compatible with using bitcoin days destroyed as a weighting measure too,
since only one of the coins needs to be relatively recent.
The above is a "fail-open" timeout rather than "fail-closed" -- if you
signal for foo, but your transaction doesn't get mined because too few
miners are signalling foo, and then foo fails to activate and times out,
your transaction can then be mined by the miners that didn't signal. If
this isn't what you want, double-spending should be fine: provide a double
spend at market rates that doesn't require signalling directly to miners
and their choice becomes "mine this thing now and get the fee directly"
versus "hope no one else mines it now, and that I get the chance to mine
the original higher fee transaction after activation, before anyone else
does", so at least the economically-rational choice should be to mine
the lower-fee double spend immediately. So it should be reasonable to
offer a higher fee for signalling, without risking that non-signalling
miners will be able to claim that high fee eventually.
I'm not sure the incentives about tying user-signalling for a soft-fork
to miner signalling for a soft-fork are entirely sound; but if they are
then just using nversion seems a lot more user-friendly than requiring
script changes to me. In particular, it doesn't require any setup or
teardown costs -- you don't have to get an input with a particular
script encoded that you can then spend to signal, and you don't have to
remember variations on output address when you want to spend a transaction
that was signalling; likewise changes to wallets are pretty simple and
don't have "you lost all your money" results if there's a bug. Well,
the above timeout procedure requires getting a recent coin as "setup",
but that's pretty trivial, at least.

@_date: 2017-05-27 16:37:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Emergency Deployment of SegWit as a partial 
A couple of bits of math that might be of interest:
 * if 67% of the hash rate is running ASICBoost, and ASICBoost gives a
   20% performance improvement as stated on asicboost.com and in
   Greg's BIP proposal, then blocking ASICBoost would change the
   balance of miners from 67%/33% to 62.8%/37.2%; resulting in a 6.3%
   loss for income for ASICBoost miners (not 20%), and a 12.7% gain for
   non-ASICBoost miners.  In this case, total apparent hashrate reduces
   to 88.8% of what it originally was when ASICBoost is blocked (though
   the actual security either stays the same or increases, depending on
   your attack model) [0]
 * if ASICBoost use is lower than that, say 33% (eg made up of
   AntPool 18%, BTC.top 10%, ViaBTC 5%), then the shift is from 33%/67%
   to 29.1%/70.9%, and results in a 13% loss for ASICBoost miners,
   versus a 6% gain for non-ASICBoost miners. In these cases, a price
   rise in the region of 7% to 15% due to blocking ASICBoost would be
   enough to make everyone better off [1].
 * AIUI there are three feasible ways of doing ASICBoost: overt via
   the version field, semi-covert via mining an empty block and grinding
   the coinbase extra nonce, and fully covert by reordering the block
   transaction merkle tree. If the fully covert method is made infeasible
   via a secondary merkle commitment in the coinbase a la segwit, and for
   whatever reason overt ASICBoost isn't used, then empty block mining is
   still plausible, though presumably becomes unprofitable when the extra
   20% of block subsidy is less than the fees for a block.  That's adds
   up to fees per block totalling greater than 2.5BTC, and 2.5BTC/1MB is
   250 satoshis per byte, which actually seems to be where fees are these
   days, so unless they're getting more than the claimed 20% benefit,
   people mining empty blocks are already losing money compared to just
   mining normally... (Of course, 250 satoshis per byte is already fairly
   painful, and only gets more so as the price rises)
Personally, I expect any technical attempt to block ASICBoost use to fail
or result in a chain split -- 67% of miners losing 6% of income is on
the order of $5M a month at current prices. Having an approach that is as
simple as possible (ie, independent from segwit, carefully targetted, and
impossible to oppose for any reason other than wanting to use ASICBoost)
seems optimal to me, both in that it has the highest chance to succeed,
and provides the most conclusive information if/when it fails.
[0] Assuming ASICBoost miners have hardware capable of doing A hashes with
    ASICBoost turned off, or A*B (B=1.2) with ASICBoost turned on, and
    the remainder of miners have a total hashrate of R. Then overall
    hashrate is currently H=A*B+R, and ASICBoost hashrate is a = A*B/(A*B+R),
    with a = 67% if the quoted claim is on the money. Rearranging:
           a = A*B/(A*B+R)
           a*(A*B+R) = A*B
           a*A*B + a*R = A*B
           a*R = (1-a)*A*B
           R = (1/a-1)*A*B
    So a' = A/(A+R), the ASICBoost miner's hashrate if they're forced to
    turn ASICBoost off, is:
           a' = A/(A+R)
           a' = A/(A+(1/a-1)*A*B)
              = 1/(1+(1/a-1)*B)
    But if a=0.67 and B=1.2, then a' = 0.628.
    The ratio of what they are getting to what they would getting is
    just a/a',
           a/a' = a*(1+(1/a-1)*B)
                = (a+(1-a)*B)
    and their loss is a/a'-1, which is:
         a/a'-1 = (a+(1-a)*B) - 1
                = (a+(1-a)*B) - (a+1-a)
                = (1-a)*(B-1)
    which is only 20% (B-1) when a is almost zero. When a increases (ie,
    there is a higher percentage of ASICBoost miners, as sure seems to
    be the case) the potential loss from disabling ASICBoost dwindles
    to nothing (because 1-a goes to zero and B-1 is just a constant).
    Note that this is the case even with mining centralisation -- if you
    have 99% of the hashrate with ASICBoost, you'll still have 98.8% of
    the hashrate without it, making a 0.2% loss (though of course your
    competitors with 1% hashrate will go to 1.2%, making a 20% gain).
    The reason is you're competing with all the ASICBoost miners,
    *including your own*, for the next block, and the size of the reward
    you'll get for winning doesn't change.
    Total apparent hashrate is A+R versus A*B+R, so
        (A+R)/(A*B+R) = 1/(A/(A+R)) * (A*B/(A*B+R))/B
                      = 1/a' * a/B
                      = a/a' / B
                      = (a+(1-a)*B) / B
                      = a/B + (1-a)
    (yeah, so that formula's kind of obvious...)
[1] Except maybe the patent holders (err, applicants). Though per the
    recent open letter it doesn't seem like anyone's actually paying for
    the patents in the first place. If miners were, then coordinated
    disarmament might already be profitable; if you're paying say 10%
    of your mining income in licensing fees or similar, that might seem
    sensible in order to make 20% more profit; but if blocking everyone
    from using ASICBoost would reduce your licensing fees by 10% of your
    income, but only reduce your income by 6.3%, then that adds up to
    a 3.7% gain and a bunch less hassle.
    I think if the ASICBoost patent holders were able to charge perfectly
    optimally, they'd charge royalty fees of about 8.3% of miner's
    income (so ASICBoost miners would make 10% net, rather than 20%),
    and allow no more than 50% of miners to use it (so the effective
    ASICBoost hashrate would be about 55%). That way the decision to
    block ASICBoost would be:
        X * 1.2 * (1-0.083) / (0.5 * 1.2 + 0.5)  -- ASICBoost allowed
      = X * 1.1004 / 1.1
      > X
    vs
        X / (0.5 + 0.5) -- ASICBoost banned
      = X
    and ASICBoost wouldn't be disabled, but the patent holders would
    still be receiving 4.15% (50%*8.3%) of all mining income. If more
    than 50% of hashpower was boosted, the formula would change to, eg,
        X * 1.2 * (1-0.083) / (0.51 * 1.2 + 0.49)
      = X * 1.1004 / 1.102
      < X
    and similarly if the fee was slightly increased, and in that case all
    miners would benefit from disabling ASICBoost. Around these figures
    ASICBoost miners would only gain/lose very slightly from ASICBoost
    getting blocked; the big losers would be the patent holders, who'd
    go from raking in 4.15% of all mining income to nothing, and the
    big winners would be the non-ASICBoost miners, who'd gain that 4.15%
    of income. The possibility of transfer payments from non-ASICBoost
    miners to ASICBoost miners to block ASICBoost might change that
    equation, probably towards lower fees and higher hashrate.
    For comparison, if 67% of hashrate is using ASICBoost, they can't
    charge them all more than 5.5% of their mining income, or miners
    would prefer to block ASICBoost, and that would only give the patent
    holders 3.7% of all mining income, much less.
    If patent holders can convince miners not to communicate with each
    other so that they think that a smaller amount of hashpower is using
    ASICBoost than actually is, that might also allow collecting more
    royalties without risking collective action to block ASICBoost.
    Of course, this is assuming they can charge all miners optimally
    and no one infringes patents, and that if you're prevented from
    using ASICBoost you don't have to keep paying royalties anyway,
    and so on. Just completely realistic, plausible assumptions like that.

@_date: 2017-05-29 21:19:14
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Emergency Deployment of SegWit as a partial 
(That seems like the cue to move any further responses to bitcoin-discuss)
If there were no patent, and it were easy enough to implement it, then
everyone would use it. So blocking ASICBoost would decrease everyone's
hashrate by the same amount, and you'd just have a single retarget period
with everyone earning a little less, and then everyone would be back to
making the same profit.
But even without a patent, entry costs might be high (redesigning an
ASIC, making software that shuffles transactions so you can use the
ASIC's features) and how that works out seems hard to analyse...
Not really; for the formulation to apply you'd have to have some way
to block ASIC use via consensus rules, in a way that doesn't just block
ASICs completely, but just removes their advantage, ie makes them perform
comparably to GPUs/FPGAs or whatever everyone else is using.
Reportedly, ASICBoost is an option you can turn on or off on some mining
hardware, so this seems valid (I'm assuming not using the option either
increases your electricity use by ~20% due to activating extra circuitry,
or decreases your hashrate by ~20% and maybe also decreases your
electricity use by less than that by not activating some circuitry); but
"being an ASIC" isn't something you can turn off and on in that manner.
Maybe? It depends on whether the optimisation's use (or lack thereof)
can be detected (enforced) via consensus rules. If you've got a patent
on a 10nm process, and you build a bitcoin ASIC with it, there's no way
to stop you via consensus rules that I can think of.
I don't think that scenario's any different from charging miners income
tax, is it? If you don't pay the licensing fee / income tax, you get put
out of business; if you do, you have less profit. There's no way to block
either via consensus mechanisms, at least in general...
I think it's the case that any optional technology with license fees can't
be made available to all miners on equal terms, though, provided there is
any way for it to be blocked via consensus mechanisms. If it were, the
choice would be:
 my percentage of the hashrate is h (0(r-X)*h provided X>0, so all miners are better off if the
 technology is not allowed (because they all suffer equally in loss of
 hashrate, which is cancelled out in a retarget period; and they all
 benefit equally by not having to pay licensing fees).
Sadly, the solution to this argument is to use discriminatory terms,
either not offering the technology to everyone, or offering varying fees
for miners with different hashrates. Unless somehow it works to make it
more expensive for higher hashrate miners, this makes decentralisation

@_date: 2017-11-06 03:50:33
@_author: Anthony Towns 
@_subject: [bitcoin-dev] "Changes without unanimous consent" talk at Scaling 
Paper (and slides) for my talk in the Consensus stream of Scaling Bitcoin
this morning are at:
   Some analysis for split-related consensus changes, and (code-less)
proposals for generic replay protection (a la BIP 115) and providing a
better level of price discovery for proposals that could cause splits.

@_date: 2017-09-11 12:15:07
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
That seems like it just says bitcoin core has two classes of users:
people who use it directly following mainnet or testnet, and people who
make derived works based on it to run altcoins.
Having a "responsible disclosure" timeline something like:
 * day -N: vulnerability reported privately
 * day -N+1: details shared amongst private trusted bitcoin core group
 * day 0: patch/workaround/mitigation determined, CVE reserved
 * day 1: basic information shared with small group of trusted users
      (eg, altcoin maintainers, exchanges, maybe wallet devs)
 * day ~7: patches can be included in git repo
      (without references to vulnerability)
 * day 90: release candidate with fix available
 * day 120: official release including fix
 * day 134: CVE published with details and acknowledgements
could make sense. 90 days / 3 months is hopefully a fair strict upper
bound for how long it should take to get a fix into a rc; but that's still
a lot longer than many responsible disclosure timeframes, like CERT's at
45 days, but also shorter than some bitcoin core minor update cycles...
Obviously, those timelines could be varied down if something is more
urgent (or just easy).
As it is, not publishing vulnerability info just seems like it gives
everyone a false sense of security, and encourages ignoring good security
practices, either not upgrading bitcoind nodes, or not ensuring altcoin
implementations keep up to date...
I suppose both "trusted bitcoin core group" and "small group of trusted
users" isn't 100% cypherpunk, but it sure seems better than not both not
disclosing vulnerability details, and not disclosing vulnerabilities
at all... (And maybe it could be made more cypherpunk by, say, having
the disclosures to trusted groups have the description/patches get
automatically fuzzed to perhaps allow identification of leakers?)

@_date: 2017-09-12 13:37:03
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
If you can't pick even a small group that's trustworthy (top five by
market cap as a start [0]? or just major bitcoin wallets / exchanges /
alt node implementations?), then it still seems better to (eventually)
disclose publically than keep it unrevealed and let it be a potential
advantage for attackers against people who haven't upgraded for other
I find it hard to imagine bitcoin's still obscure enough that people
aren't tracking git commit logs to use them as inspiration for attacks
on bitcoin users and businesses; at best I would have thought it'd
only be a few months of development time between a fix being proposed
as a PR or committed to master and black hats having the ability to
exploit it in users who are running older nodes. (Or for that matter,
being able to be exploited by otherwise legitimate bitcoin businesses
with an agenda to push, a strong financial motive behind that agenda,
and a legal team that says they'll get away with it)
Isn't that just an argument for putting more effort into backporting
fixes/workarounds? (I don't see how you do that without essentially
publically disclosing which patches have a security impact -- "oh,
gosh, this patch gets a backport, I wonder if maybe it has security
(In so far as bitcoin is a consensus system, there can sometimes be a
positive network effect, where having other people upgrade can help your
security, even if you don't upgrade; "herd immunity" if you will. That
way a new release going out to other people helps keep you safe, even
while you continue to maintain the same definition of money by not
upgrading at all)
If altcoin maintainers are inconvenienced by tracking bitcoin-core
updates, that would be an argument for them to contribute back to their
upstream to make their own job easier; either helping with backports,
or perhaps contributing to patches like PR might help.
All of those things seem like they'd help not just altcoins but bitcoin
investors/traders too, so it's not even a trade-off between classes of
bitcoin core users.  And if in the end various altcoins aren't able to
keep up with security fixes, that's probably valuable information to
provide to the market...
[0] Roughly: BCash, Litecoin, Dash, BitConnect, ZCash, Dogecoin?
    I've no idea which of those might have trustworthy devs to work with,
    but surely at least a couple do?

@_date: 2017-09-12 14:47:58
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
That makes sense.
For comparison, Monero defines a response process that has three levels
and varies the response for each:
]     a. HIGH: impacts network as a whole, has potential to break entire
]        network, results in the loss of monero, or is on a scale of great
]        catastrophe
]     b. MEDIUM: impacts individual nodes, wallets, or must be carefully
]        exploited
]     c. LOW: is not easily exploitable
 -- Among other things, HIGH gets treated as an emergency, MEDIUM get fixed
in a point release; LOW get deferred to the next regular release eg.
Additionally, independently of the severity, Monero's doc says they'll
either get their act together with a fix and report within 90 days,
or otherwise the researcher that found the vulnerability has the right
to publically disclose the issue themselves...
I wouldn't say that's a perfect fit for bitcoin core (at a minimum, given
the size of the ecosystem and how much care needs to go into releases,
I think 90 days is probably too short), but it seems better than current
For comparison, if you're an altcoin developer or just bitcoin core user,
and are trying to work out whether the software you're using is secure;
if you do a quick google and end up at:
  you might conclude that as long as you're running version 0.11 or later,
you're fine. That doesn't seem like an accurate conclusion for people
to draw; but if you're not tracking every commit/PR, how do you do any
better than that?
Maybe transitioning from keeping things private indefinitely to having
a public disclosure policy is tricky. Maybe it might work to build up to it,
something like:
  * We'll start releasing info about security vulnerabilities fixed in
    0.12.0 and earlier releases as of 2018-01-01
  * Then we'll continue with 0.13.0 and earlier as of 2018-03-01
  * Likewise for 0.14.0 as of 2018-05-01
  * Thereafter we'll adopt a regular policy at That or something like it at least gives people relying on older,
potentially vulnerable versions a realistic chance to privately prepare
and deploy any upgrades or fixes they've missed out on until now.

@_date: 2017-09-14 15:27:40
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
Collecting various commentary from here and reddit, I think current de
facto policy is something like:
 * Vulnerabilities should be reported via security at bitcoincore.org [0]
 * A critical issue (that can be exploited immediately or is already
   being exploited causing large harm) will be dealt with by:
     * a released patch ASAP
     * wide notification of the need to upgrade (or to disable affected
       systems)
     * minimal disclosure of the actual problem, to delay attacks
   [1] [2]
 * A non-critical vulnerability (because it is difficult or expensive to
   exploit) will be dealt with by:
     * patch and review undertaken in the ordinary flow of development
     * backport of a fix or workaround from master to the current
       released version [2]
 * Devs will attempt to ensure that publication of the fix does not
   reveal the nature of the vulnerability by providing the proposed fix
   to experienced devs who have not been informed of the vulnerability,
   telling them that it fixes a vulnerability, and asking them to identify
   the vulnerability. [2]
 * Devs may recommend other bitcoin implementations adopt vulnerability
   fixes prior to the fix being released and widely deployed, if they
   can do so without revealing the vulnerability; eg, if the fix has
   significant performance benefits that would justify its inclusion. [3]
 * Prior to a vulnerability becoming public, devs will generally recommend
   to friendly altcoin devs that they should catch up with fixes. But this
   is only after the fixes are widely deployed in the bitcoin network. [4]
 * Devs will generally not notify altcoin developers who have behaved
   in a hostile manner (eg, using vulnerabilities to attack others, or
   who violate embargoes). [5]
 * Bitcoin devs won't disclose vulnerability details until >80% of bitcoin
   nodes have deployed the fixes. Vulnerability discovers are encouraged
   and requested to follow the same policy. [1] [6]
Those seem like pretty good policies to me, for what it's worth.
I haven't seen anything that indicates bitcoin devs will *ever* encourage
public disclosure of vulnerabilities (as opposed to tolerating other
people publishing them [6]). So I'm guessing current de facto policy is
more along the lines of:
 * Where possible, Bitcoin devs will never disclose vulnerabilities
   publically while affected code may still be in use (including by
   altcoins).
rather than something like:
 * Bitcoin devs will disclose vulnerabilities publically after 99% of the
   bitcoin network has upgraded [7], and fixes have been released for
   at least 12 months.
Instinctively, I'd say documenting this policy (or whatever it actually
is) would be good, and having all vulnerabilities get publically released
eventually would also be good; that's certainly the more "open source"
approach. But arguing the other side:
 - documenting security policy gives attackers a better handle on where
   to find weak points; this may be more harm than there is benefit to
   improving legitimate users' understanding of and confidence in the
   development process
 - the main benefit of public vulnerability disclosure is a better
   working relationship with security researchers and perhaps better
   understanding of what sort of bugs happen in practice in general;
   but if most of your security research is effectively in house [6],
   maybe those benefits aren't as great as the harm done by revealing
   even old vulnerabilities to attackers
If the first of those arguments holds, well, hopefully this message has
egregious errors that no one will correct, or it will quickly get lost
in this list's archives...
[0]     referenced from .github/ISSUE_TEMPLATE.md in git
[1] [2] [3] [4] [5] [6]  [7] Per     it seems like 1.7% of the network is running known-vulnerable versions
    0.8 and 0.9; but only 0.37% are running 0.10 or 0.11, so that might argue
    revealing any vulnerabilities fixed since 0.12.0 would be fine...
    (bitnodes.21.co doesn't seem to break down anything earlier than 0.12)

@_date: 2017-09-29 14:45:56
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
I think CPFP allows this to break: a miner getting paid out of band
would just make the block look like:
    (1) 100kB of 5s/byte transactions
    (2) 850kB of 35s/byte transactions
    (3) 50kB of 95s/byte transactions, miner paying themselves
As long as every transaction in (1) has an output spent in (3), that seems
like it would be perfectly legitimate for CPFP. I think it would be cheaper overall than the fee refund transactions
as well. People making arrangements with miners directly would have to
pay for block space to cover:
   out:
     30-40B dest address
     30-40B change address
     10B    cpfp link
   in:
     36B    cpfp txid
then to actual spend their change:
   in:
     36B+70B txid+idx + witness for change
for a total of 142-162B plus 70B witness, as well as some sort of out of
band payment to the miner (paying fees directly to miners via a lightning
channel, comes to mind). If I understand your suggestion correctly, it would look like:
   coinbase:
     30-40B fee overflow payment back to transactor
   out:
     30-40B dest address
     30-40B change address
     30-40B fee-overflow output marker
and to spend their change:
   in:
     36B+70B txid+idx + witness for change
     36B+70B txid+idx + witness for fee overflow
for a total of 192-232B plus 140B witness; so that's 40%-50% more block
weight used. The fee overflow would probably be pretty small amounts,
as well, so kind of annoying to actually collect.
If you end up with two change addresses per tx generally, that also seems
like it might it annoyingly easy to link your transactions together
(unless fees end up getting coinjoined or run through satoshidice or
something). If you end up sending lots of fee overflows to a single
address, that links your txes too of course.
A miner might be willing to do that in order to charge a two-part tariff:
ie, a very high "subscription" fee that's paid once a year or similar,
along with very low per-tx fees. The only reason I can think of why
someone would buy a subscription is if the miner's effectively a monopoly
and submitting transactions via the p2p network isn't reliable enough;
the whole point of a two-part tariff is to be as expensive as each user
can bear, so it won't ever be any cheaper.
FWIW, I think reliability-based-price-discrimination might allow higher
mining revenue via having txes with differing fee rates in the same
block: eg if people are happy to pay 100s/byte for confirmation within
30 minutes, and likewise willing to pay 10s/byte for confirmation within
3 hours, and there aren't enough transactions of either type to hit the
block size limit, then a monopoly miner / mining cartel would do better
by accepting 100s/byte txes at any time, while accepting 10s/byte txes
in any given block, but only with about a 1-in-7 chance for any given tx.
Looking at estimatefee.com, there's currently apparently ~200kB of 82s/B
or more transactions, while to fill a 1MB block you'd have to go all the
way down to 2.1s/B -- so if you have to charge all txes at the marginal
fee rate, that's 200kB at 82s/B for 0.16 BTC rather than 1MB at 2.1s/B
for 0.021 BTC.
I think ideally, it would probably be better to let the block weight
limit adjust to deal with high-frequency components to changes in demand,
and have fee rates adjust more slowly to address the long-term trends
in changes to demand: if fee rates only adjust slowly, then they're
(by definition) easily predictable and you don't *have* to have much
concern about getting them wrong. (You'd still need to add the correct
fee at the time you want to publish a pre-signed transaction that was
very old, but CPFP isn't too bad at that).

@_date: 2018-04-03 13:57:23
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Signature bundles 
So suppose you have two bundles you want to combine together, and they
didn't pay enough fees, so you have an extra input so you can bump up the
fees. Your tx looks like:
bundle 1:
  input 1:   500 bits, signed by key A [pre]
  input 2:   500 bits, signed by key B [pre]
  input 3:   500 bits, signed by key C
  output 1: 1450 bits
bundle 2:
  input 3:   600 bits, signed by key D [pre]
  output 2:  200 bits
  output 3:  380 bits
  input 4:  2000 bits, signed by key E
  output 4:  864 bits
Keys A, B and D have pre-signed their respective bundle, but you have
control over keys C and E at the time you're constructing the transaction.
So the things you'd have to do when signing would be:
  A,B,C decide on an order for the inputs and outputs (ideally just sort them)
  Because A turns out to be first, A signs with BUNDLESTART[3,1] INBUNDLE
  Because B is second, B just signs with INBUNDLE
  D just signs with BUNDLESTART[3,1] INBUNDLE
  And finally they get collected and C and E signs the entire transaction
  with SIGHASH_ALL
All bundles have to appear together, before any extra inputs; though they
can be reordered arbitrarily prior to adding the SIGHASH_ALL sigs.
If you've got one bundle that overpays fees and another that underpays,
you can safely combine the two only if you can put a SIGHASH_ALL sig in
the one that overpays (otherwise miners could just make their own tx of
just the overpaying bundle).
This could replace SINGLE|ANYONECANPAY at a cost of an extra couple of
witness bytes.
I think BUNDLESTART is arguably redundant -- you could just infer
BUNDLESTART if you see an INBUNDLE flag when you're not already in
a bundle. Probably better to have the flag to make parsing easier,
so just have the rule be BUNDLESTART is set for precisely the first
INBUNDLE signature since the last bundle finished.
Should be straightforward to establish BIP-69-style ordering rules too:
within a bundle, order inputs lexically, then order outputs lexically;
order bundles lexically by the first input; order remaining inputs
lexically, then order remaining outputs lexically.
I think the only reason to do bundling like this (rather than just post
separate transactions) is to deal with fees? It doesn't seem like you gain
any privacy -- the inputs/outputs in each bundle are tightly related, and
you're only saving about 10 bytes due to sharing a transaction structure.
Anyway, seems like it makes sense. That doesn't quite work with the HTLC-Success/HTLC-Timeout transactions
though, does it? They spend outputs from the commitment transaction
and need to be pre-signed by your channel partner in order to ensure
the output address is correct -- but if the commitment transaction gets
bundled, its txid will change, so it can't be pre-signed.
FWIW, a dumb idea I had for this problem was to add a zero-value
anyone-can-spend output to commitment transactions, that can then be
used with CPFP to bump the fees. Not very nice for UTXO bloat if fee
bumping isn't needed though, and I presume it would fail to pass the
dust threshold...
I wonder if it would be plausible to have after-the-fact fee-bumping
via special sighash flags at the block level anyway though. Concretely:
say you have two transactions, X and Y, that don't pay enough in fees,
you then provide a third transaction whose witness is [txid-for-X,
txid-for-Y, signature committing to (txid-for-X, txid-for-Y)], and can
only be included in a block if X and Y are also in the same block. You
could make that fairly concise if you allowed miners to replace txid-for-X
with X's offset within the block (or the delta between X's txnum and the
third transaction's txnum), though coding that probably isn't terribly

@_date: 2018-08-06 18:39:25
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Isn't the verification equation "R + s(-G) + eP = 0" equally good, then,
since -G is a constant? (ie, at worst it's a matter of optimising the
verifier for -G as well as G)
If not, what's the actual performance impact of having to negate "s"
as part of batch verifying ~10000 signatures? It seems like it should
be trivially small to me? (scalar_negate benchmarks at 0.00359us, while
ecdsa_verify benchmarks at 66us, which I believe then reduces by a factor
of ~3 for batches of 10k schnorr sigs?)
FWIW, I'm a fan of the formulation "s = r + H(R,P,m)p" mostly because
it seems like the simplest possible way of describing the setup, and I'm
all for optimising for people being able to understand what's going on.

@_date: 2018-12-13 10:05:53
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
This seems strange to me -- why wouldn't you just assume every signature
is 65 witness bytes, and just be grateful for the prioritisation benefit
if someone chooses a shorter signature? Your error margin is just 0.25
vbytes per signature.
An alternative generalisation: is there a proof that all valid witnesses
will have a weight within some small range?
(DEPTH 2 NUMNOTEQUAL seems like it would have been more obvious...)

@_date: 2018-12-13 10:24:38
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
The way I'm thinking about it is there's four amounts of knowledge you
could have about the input you're spending at the time you generate a
 ALL: you know everything about every input for this tx
 SINGLE: you know everything about the input you're signing for, but
   not necessarily the others
 SCRIPTPUBKEY: you know the exact scriptPubKey you're trying to satisfy, but
   don't know the txid
 SCRIPTMASK: you don't know the txid, don't know the scriptPubKey, don't
   know the other taproot branches, and maybe don't even know the masked
   out terms in the script -- but you do know the structure of the
   script, and the non-masked terms
There's no value to masking in any but the final case -- the txid and
scriptPubKey commit to the full scriptcode already, so also signing the
scriptcode is just belt-and-suspenders protection.
(It might be that the "SCRIPTPUBKEY" option isn't very useful in
practice; maybe you'll always either know the txid, or need to mask
In a taproot world, your scriptPubKey is a point P=Q+H(Q,S)*G, where S
is a merkle root of possibly many scripts, and is spendable either by:
  sig(P)
  Q, path(S,script), script, witness(script)
SCRIPTMASK lets you prepare a signature for one particular script in
advance, even before you've decided what the other scripts are (and even
what the base point Q is), let alone built an actual transaction.
At least, that's my current understanding; and I think it makes sense...

@_date: 2018-12-14 10:47:29
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Sorry, I elided some of my reasoning. Suppose witness data wasn't
malleable; in that case any valid witness for a particular script would
have the exact same weight, and it would be good enough to just sign
the script, because that also commits to the witness weight. (And if
you're doing SIGHASH_ALL, you're committing to the exact transaction
weight too)
I think the benefit of signing the weight is mostly that it also commits
to the feerate and hence transaction priority: you know how much you're
paying in fees when you sign, but the reason you're paying any fees is
to get a particular priority for your transaction, so if that can change
from under you because the tx weight changes, you're being ripped off
(either because you get less priority than you were paying for, or
because you get more than you wanted and would have paid less if you'd
But if, just from looking at the script, you can be sure the witness
weight will be between "w" and "w + 0.8%" and your fee is "f", you
know your feerate (and hence priority) is between "f/w - 0.8%" and
"f/w". If the "0.8%" is small enough, that's just a rounding error and
you probably have more uncertainty in your feerate estimations anyway. So
I think at that point it's reasonable to target the lower bound feerate
("f/w - 0.8%"), because your only potential loss is that you get a higher
feerate and would have saved "0.8%" on f if you'd been able to be 100%
sure of that.
The cases where the tx is malleable by someone else, and you know what
the weight should be in advance, and you can't take the final tx once it
hits your mempool and fix the weight to what it should be and
rebroadcast, seem limited to me?
Being able to commit to a minimum feerate seems like it would be more
generally useful: it would apply for ANYONECANPAY crowd-funding type
txes as well; "here's my input, and I'm paying 3 sat/vb feerate, but only
if everyone else does too!". You could do that, I think, with a rule
along the lines of:
  (a) take the actual tx feerate, f*4000/w
  (b) round it down to the nearest exponent of 1.05 sat/kvbyte,
      so 1.3 sat/vbyte becomes 1240.62 (1.05**146 < 1.05**147=1302)
  (c) if the signature doesn't have an extra byte, then it should
      commit to that exponent (146)
  (d) if the signature does have an extra byte, b, then b<=146 and
      the signature should commit to 146-(1+b)
That way if you sign something that says "minimum fee rate of 0.001 sat
per vbyte", you commit to an exponent of 0, and someone else can raise the
feerate anywhere up to 265.7 sat/vb just by tweaking your signature to
indicate how much they've raised the feerate. Likewise you could commit
to some other exponent, and anyone else could adjust your signature to
remain valid for a tx with a feerate of up to 265,742 times greater than
what you expected, but never more than 5% less than what you expected.
This seems too complicated to do any time soon; and maybe more
complicated than will ever be worthwhile, though.

@_date: 2018-12-14 19:30:02
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
If it's not safer in practice, we've spent a little extra complexity
committing to a subset of the script in each signature to no gain. If
it is safer in practice, we've prevented people from losing funds. I'm
all for less complexity, but not for that tradeoff.
Also, saying "I can't see how to break this, so it's probably good
enough, even if other people have a bad feeling about it" is a crypto
anti-pattern, isn't it?
I don't see how you could feasibly commit to more information than script
masking does for use cases where you want to be able to spend different
scripts with the same signature [0]. If that's possible, I'd probably
be for it.
At the same time, script masking does seem feasible, both for
lightning/eltoo, and even for possibly complex variations of scripts. So
committing to less doesn't seem wise.
For example, script masking seems general enough to prevent footguns
even if (for some reason) key and value reuse across eltoo channels
were a requirement, rather than prohibited: you'd make the script be
" MASK  CLTV 2DROP  CHECKSIG", and your
signature will only apply to that channel, even if another channel has
the same capacity and uses the same keys, a and b.
For my money, "NOINPUT" commits to dangerously little context, and
doesn't really feel safe to include as a primitive -- as evidenced by
the suggestion to add "_UNSAFE" or similar to its name. Personally, I'm
willing to accept a bit of risk, so that feeling doesn't make me strongly
against the idea; but it also makes it hard for me to want to support
adding it. To me, committing to a masked script is a huge improvement.
Heck, if it also makes it easier to do something safer, that's also
probably a win...
[0] You could, perhaps, commit to knowing the private keys for all the
    *outputs* you're spending to, as well as the inputs, which comes
    close to saying "I know this is a scary NOINPUT transaction, but
    we're paying to ourselves, so it will all be okay".

@_date: 2018-12-14 20:48:39
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
Hi *,
(All the following is heavily informed by talking with other smart people,
and while probably all the clever ideas are theirs, any nonsense and
mistakes are certainly my own. I guess I'll pretend there were Chatham
House rules or something to avoid any blame/responsibility accidently
landing on their shoulders? Anyway, I hope discussing this in public
turns out more useful and productive than disastrous and bikesheddy :)
Rusty wrote "Without a concrete taproot proposal it's hard to make
assertions". I'm not going to offer a completely concrete proposal,
but fwiw, here's my thoughts on what should be included in the segwit v1
proposal, which I think might be concrete enough for discussion purposes:
 - introduce 33-byte v1 witness addresses should encode a secp256k1 ECC
   point (P), spendable either by:
    - a direct schnorr signature (s,R) on that point
      (s*G = R + H(R,P,txdigest)*P), with the 1-byte sighash per the
      other thread indicating exactly what goes into the tx digest, etc
    - a script (s), the witness data for the script (wit(s)),
      with a taproot/merkle path to the script (P,path(S,s)),
      satisfying the taproot condition (Q = P + H(P,S)*G)
 - the taproot scripts should get a version, and since you have to
   provide P anyway in order to spend by a script, you've got 7-bits spare
   in the byte that encodes the evenness/oddness of P, so that gives you
   v1.0 to v1.127 for free. So if we define script version 0 initially,
   and just automatically accept any script with a later version, we
   can soft-fork arbitrary script upgrades without bumping the segwit
   (major) version.
 - we should replace the ECDSA CHECKSIG/CHECKMULTISIG ops with new
   Schnorr ops. A name that's been suggested for the new ops is "CHECKDLS"
   for discrete-log-signature; I'm using that.
   Rather than CHECKMULTISIG, a simple, more general approach seems to
   be "CHECKDLSADD" which takes a signature, a number, and a pubkey,
   and increments the number if the signature is valid, and leaves it
   untouched if not. So "2 of 3 multisig" becomes "0  CHECKDLSADD
    CHECKDLSADD  CHECKDLSADD 2 EQ", eg. That means replacing the
   current four CHECK(multi)SIG(verify) opcodes, with three opcodes:
   CHECKDLS, CHECKDLSVERIFY and CHECKDLSADD.
   To make batch verifiability of signatures work, the only acceptable
   invalid signature for CHECKDLS or CHECKDLSADD needs to be an empty
   vector; anything else should fail the script/transaction/block.
 - adding OP_MASK to support script masking via sighash per the other
   thread; note this only matters for the new CHECKDLS opcodes, since for
   direct signatures on the scriptPubKey, there is no script to mask.
   This means it's completely changeable with new script versions,
   if desired.
 - making (almost) all the currently invalid opcodes upgradeable
   with what I'm calling "OP_SUCCESS" semantics [0], so that we have more
   flexibility than OP_NOP gives us. An approach for those semantics
   that seems fairly easy to analyse is to treat script processing as
   going in phases:
      1. tokenise; check push sizes and overall script size
      2. if any OP_SUCCESS appeared; succeed
      3. if banned opcodes appeared; fail (OP_VERIF, OP_VERNOTIF?)
      4. otherwise, run the script; fail if there's an error
      5. if there's exactly one, non-zero item on the stack; succeed
      6. otherwise; fail
   (Obviously an implementation can do these in parallel if that's more
   efficient)
   That way any of the "OP_SUCCESS" opcodes can be replaced by any
   normal opcode (eg addition, a different sort of signature checking,
   push tx or blockchain data to the stack) in a soft-fork; and you
   can easily be sure that the new functionality is a soft-fork (as
   long as you're not trying to change how pushes work)
   [1]
   This even means you could use an OP_SUCCESS opcode to signal an
   upgrade of other opcodes, eg an OP_ARITH64BIT that upgrades OP_ADD
   etc to support arithmetic on 64 bit inputs.
 - and that's it.
I think this is a fairly modest collection of changes:
 signature/address stuff:
   - schnorr
   - new sighash (including "noinput")
   - taproot
   - merkelized-scripts
   - avoid weird CHECKMULTISIG behaviour
 upgradeability:
   - script minor versions
   - OP_SUCCESS
I think there's a good reason to bundle all those together: the signature
stuff go together with a new address version, and the upgradeability
stuff helps reduce the need to do more new address versions.
Well, it's modest at least compared to what's conceivable: there are a
*lot* of other neat ideas that could theoretically be done in the same
soft-fork, but IMHO are better left for later, eg:
 - graftroot, g'root, cross-input signature aggregation
 - non-interactive half-signature aggregation
 - re-enabling opcodes (CAT, MUL, XOR, etc)
 - check-sig-of-msg-on-stack, push txdata, other covenant-y things
 - different cryptosystems (eg, 384 bit curves for better protection
   against future quantum computing advances; conceivably pairing curves?)
 - "EVAL" and similar language features
 - [etc]
As far as how those things could get done in future, this collection of
features leaves four ways to make further improvements:
 - new segwit version (v2-v16)
   (needed for graftroot, signature aggregation, different signature
    systems)
 - different length segwit v1 pubkey
   (could be used to provide a hash instead of the actual taproot point,
    or use a larger ECC curve)
 - new segwit v1 script version (v1.0-v1.127)
   (needed for big redesigns/simplifications of script)
 - additional opcodes (OP_SUCCESS replacement)
   (can be used to re-enable disabled opcodes like MUL/CAT/XOR/etc;
    can be used to add more complicated things like CHECKSTACKDLS,
    or PUSHTXDATA; can be used to try out different signature
    schemes)
I think its worth noting that OP_SUCCESS upgrades could be
developed/deployed in parallel, since you just need to choose an opcode to
take over and (presumably) a versionbit to signal when the new behaviour
gets activated. The other methods require agreeing on everything that's
going to go in the new version, which needs a bit more coordination.
[2] Anyway, to get back to the intro sentence, and to give an example of how I
think v1 addresses will work, here's my take on Eltoo in a taproot world:
  Funding tx:
    inputs:     outputs:
      ...
      i. pay to Q = P+H(P,S)G
         P = muSig(A,B)
         S = "MASK <500M> CLTV  CHECKDLSVERIFY"
      ...
  Update tx n:
    nlocktime = 500M + n
    inputs:
      1. Funding tx, or Update tx m, m CHECKDLSVERIFY")
    outputs:
      1. pay to Qn = P+H(P,Sn)G
         Sn = "MASK <500M+n+1> CLTV  CHECKDLSVERIFY
  Settlement tx n:
    inputs:
      1. Update tx n (unknown txid);            nseq = csv delay
           (note: Qn != Qm unless n=m, because Sn != Sm)
    outputs:
      1: pay A's balance to A
      2: pay B's balance to B
      3..n: HTLC paying to B: see below
  Cooperative close:
    inputs:
      Funding tx, sig(Q, sighash=in_all+out_all)
    outputs:
      1..n: as agreed
(I'm assuming you create "Update tx 0" and "Settlement tx 0" to pay
yourself back if setup fails, prior to publishing the funding tx. The
eltoo paper has a "trigger" phase for that purpose instead, aiui. Also,
these two txs don't actually need to use NOINPUT, because they directly
spend from the funding tx)
As far as the HTLC outputs go... For SHA256 preimages, you prepare
a taproot address Q=P+H(P,SH), where SH is the merkle root for the
tree of two scripts, " CLTV  CHECKDLSVERIFY" and "HASH160 EQUAL  CHECKDLSVERIFY". For secp256k1 preimages, your address is
P'=muSig(A,B,n*G) for some value n that just ensures you have different
keys for each htlc, and you prepare two pre-signed transactions, spending
the settlement output (whose txid is unknown), both signed with sighash
committing to the scriptPubKey and a single output. One pays A and has
a partial signature from B and nTimeLock set to the timeout; so A can
complete the signature and claim after the timeout; the other pays B
and has a conditional partial signature from A, which B can complete
upon finding out the preimage.
The settlement and pre-signed-HTLC-spend transactions all make use of
the NOINPUT-commit-to-scriptPubKey varaint in this arrangement; so it
does seem like it's probably useful in practice; scriptless scripts make
the direct-signature path pretty useful.
[0] aka OP_RETURNTRUE     aka OP_RETURNVALID [1] The "drawback" to this approach is that it means that you can't
    partially verify a script if you think you know what the first few
    opcodes mean; so if a script upgrade has happened but your node hasn't
    upgraded, even if you see a transaction in a block with what you
    think is " OP_CHECKDLS OP_SUCCESS", you don't check the signature.
[2] One thing that could be feasible would be to have some simple
    OP_SUCCESS upgrades (like enabling CAT/XOR/etc or adding
    CHECKSTACKDLS) specced, implemented, and tested, and have them
    activate at the same time as schnorr/taproot/etc, while keeping them
    as an independent feature at the BIP/concept/implementation levels.
    The idea there is that if it turns out they're not ready in time,
    schnorr/taproot/etc don't need to get delayed, and the others can
    just be enabled when they're ready later using a separate version bit.
    I'm not sure if there's anyone who's interested in shepherding/doing
    the spec/implementation for any of the more straight-forward features
    like that, though.

@_date: 2018-12-18 14:58:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
Well, if CHECKSIG* and CHECKMULTISIG* are all disabled in favour of
CHECKDLS, CHECKDLSVERIFY and CHECKDLSADD with both different names and
different opcodes, copying a script template opcode-for-opcode from v0
to v1 will always fail. (With taproot, this doesn't necessarily mean you
lose money, even if the script is impossible to ever satisfy, since you
may be able to recover via the direct signature path)
It's definitely bikeshedding so whatever; but to me, it seems like it'd
be easier for everyone to have it so that if you've got the same opcode
in v0 script and v1.0 script; they have precisely the same semantics.
(That said, constructions like " CLTV  CHECKSIGVERIFY" that avoid
the DROP and work when you're expected to leave a true value on the
stack won't work if you have to end up with an empty stack)

@_date: 2018-12-23 14:26:59
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
If you don't have conditionals, then I think committing to the (masked)
script gives you everything you could do with codeseparator.
If you don't commit to the (masked) script, don't have conditionals,
and don't have codeseparator, then I don't think you can make a signature
distinguish which alternative script it's intending to sign; but you can
just give each alternative script in the MAST a slight variation of the
key and that seems good enough.
OTOH, I think for (roughly) the example you gave:
  DEPTH 3 EQUAL
  IF  CHECKSIGVERIFY HASH160  EQUALVERIFY CODESEP
  ELSE  CLTV DROP
  ENDIF
   CHECKSIG
then compared to the taproot equivalent:
  P = muSig(Alice,Bob)
  S1 =  CHECKSIGVERIFY  CHECKSIGVERIFY HASH160  EQUAL
  S2 =  CHECKSIGVERIFY  CLTV
the IF+CODESEP approach is actually cheaper (lighter weight) if you're
mostly (>2/3rds of the time) taking the S1 branch. This is because the
"DEPTH 3 EQUAL IF/ELSE/ENDIF CODESEP  CLTV DROP" overhead is less
than the 32B overhead to choose a merkle branch).
(That said, I'm not sure what Alice's signature in the S1 branch actually
achieves in that script; and without that in S1, the taproot approach is
cheaper all the time. Scriptless scripts would be cheaper still)
Since it only affects the behaviour of the checkdls (checksig) operators,
even if it was disabled, it could be re-enabled fairly easily in a new
script subversion if needed (ie, it could be re-added when upgrading
witness version 1 from script version 0 to 1).

@_date: 2018-03-01 08:30:44
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Simple lock/unlock mechanism 
Making the graftroot key be a 2-of-2 muSig with an independent third party
that commits to only signing CLTV scripts could avoid this. Making it
3-of-3 or 5-of-5 could be even better if you can find multiple independent
services that will do it.

@_date: 2018-01-23 16:44:19
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Is this really intended as paying directly to a pubkey, instead of a
pubkey hash?
If so, isn't that a step backwards with regard to resistance to quantum
attacks against ECC?
Paying direct to pubkey doesn't seem quite enough to make pay-to-taproot
cheaper than p2wpkh: the extra 12 bytes in the scriptPubKey would need
you to reduce the witness by 48 bytes to maintain the weight, but I think
you'd only be saving 33 bytes by not having to reveal the pubkey, and
another 6-7 bytes by having a tighter signature encoding than DER. Still,
that's pretty close with a difference of only a couple of vbytes per
input by my count.
If it were "pay-to-taproot-hash", then presuming taproot hashes were 256
bit, then p2wpkh would be a full 12 vbytes cheaper due to the shorter
hash. That might make it hard to maximise the anonymity set. I suppose
a small penalty/discount could be added to align the economic incentives
I wonder how this interacts with segwit versioning. I think you'd want
to have taproot be versioned overall so that you could cope with moving
to a new signing method (different curve, or something non-ECC based)
eventually, and segwit versioning will handle that already; but maybe
it would also be a good idea to also have "S" include a version, that
could be bumped to add new features to script, but left hidden within
the hash so that the fact you're using new (or old) features is only
revealed when it has to be.
Those nits aside, this seems great.

@_date: 2018-01-24 08:22:29
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Hmm, at least people can choose not to reuse addresses currently --
if everyone were using taproot and that didn't involve hashing the key,
there would be no way for individuals to hedge against quantum attacks
in case they're ever feasible, at least that I can see (well, without
moving their funds out of bitcoin anyway)?
Even "X + H(X|script)g" with X being a random point would end up
attackable, since that would almost always end up corresponding with a
valid public key that a successful attack could then find a private key
(It seems like using the point at infinity wouldn't work because P = 0+H(0||S)g = H(0||S)g, so as soon as you tried to spend it via S,
someone watching the mempool would know H(0||S), which is the secret key
for P, and be able to spend it via the pubkey path -- no quantum crypto
needed. Or am I missing something?)
Also, if the people currently reusing addresses tend to cycle the funds
through fairly quickly anyway, they might be able to simply stop doing
that when quantum attacks start approaching feasibility. If funds are
being held in reused addresses over the long term, that would be more
of a problem though...
Yeah, that was one of the assumptions for
 iirc. (Also, pretty sure you mean "advantageous", but at least I learnt a new
word today)

@_date: 2018-07-13 11:51:57
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Generalised taproot 
I think it's possible to do recursive taproot in this manner in a
neat way, using Pedersen Commitments. (Background: A Pedersen commitment uses a second generator in the curve,
and rather than constructing a point from a single secret, like A=a*G,
it constructs a point from two secrets, like C=a*G+b*G2, and finding a
different c,d such that C=c*G+d*G2 gives you the discrete log of G2)
So combining this with the taproot structure gives an equation like:
  P = a*G + s*G2 + H(a*G+s*G2, Q)*G
If you take "a" to be a private key (so A=a*G is the corresponding
pubkey), "s" to be (the hash of) a set of additional conditions for
spending with the pubkey, and "Q" to be an alternative method of spending,
you get a recursive taproot construction.
To spend "P", you would either:
  - sign with P directly (only possible if s=0, indicating there are no
    additional conditions to satisfy when spending with this key)
  - reveal the extra conditions you have to satisfy (s), satisfy
    them, and provide a signature the key "P-s*G2"
  - reveal the points "a*G+s*G2" and "Q", and satisfy "Q"
If you structure the conditions as:
  (pubkey A) |
    (pubkey B & script x) |
      (pubkey C & script y) |
        (merkle tree of scripts, root=z)
Then you can construct a pubkey point as:
   D' = z
   C' = C + y*G2 + H(C+y*G2, D')*G
   B' = B + x*G2 + H(B+x*G2, C')*G
   A' = A + H(A, B')*G
and if you want to spend something with a scriptPubKey of A', you could
   (1) plain signature with privkey = a+H(A,B')
   (2) reveal [A, B'], reveal [x], provide [witness(x)],
       signature with privkey = b+H(B+x*G2,C')
   (3) reveal [A, B'], reveal [B+x*G2, C'], reveal [y], provide
       [witness(y)], signature with privkey = c+H(C+y*G2, D')
   (4) reveal [A, B'], reveal [B+x*G2, C'], reveal [C+y*G2],
       reveal [script], reveal merkle path from script to z,
       provide [witness(script)].
That way, you can keep two sets of things secret:
 - until you hit the merkle-tree of scripts, you don't reveal
   whether there are or aren't any lower layers
 - you don't reveal the conditions corresponding with any of the
   keys, other than the key you're spending with
This is as (space) efficient as basic taproot:
  taproot: P + H(P, [Q CHECKSIGVERIFY cond])   witness:
    (1) sig(P)
    (2) P [Q CHECKSIGVERIFY cond] sig(Q) witness(cond)
  g'root: P + H(P, Q + cond*G2)*G
  witness:
    (1) sig(P+H(..)*G)
    (2) P Q sig(Q) cond witness(cond)
It's potentially more efficient for cases where the taproot assumption
doesn't hold, and the common case is to spend with conditions:
  g'root: P + cond*G2 + H(P+cond*G2, Q)*G
  witness:
    (1) cond witness(cond) sig(P+H(..)*G)
    (2) [P+cond*G2] Q sig(Q)
  taproot: Q + H(Q, [P checksig cond])*G
    (1) Q [P CHECKSIG cond] [sig(P) witness(cond)]   (64 bytes overhead)
    (2) sig(Q+H(..)*G)                               (64 bytes saved)
It's also potentially more efficient than using a merkle tree with taproot
when there are three spending paths, and one merkle branch is more likely
than the other, eg, if the conditions are "sign with A", or "sign with
B and satisfy x", or (least likely) "sign with C and satisfy y":
Let s = [B CHECKSIGVERIFY x], t = [C CHECKSIGVERIFY y], r = H(H(s),H(t))
 taproot+MAST: A + H(A,r)*G
  (1t) sig(A+H(..)*G)
  (2t) A,s,H(t),sig(B),witness(x)
  (3t) A,t,H(s),sig(C),witness(y)
 g'root: A', where:
           C' = C + y*G2
           B' = B + x*G2 + H(B+x*G2,C')*G
           A' = A + H(A,B')*G
  (1g) sig(A+H(..)*G)
  (2g) A B' x sig(B'-x*G2) witness(x)
  (3g) A B' [B+x*G2] C' y sig(C) witness(y)
(1t) and (1g) are the same; (2t) is about 32B larger than (2g) because
s=[B x], and  (3t) is about 32B smaller than (3g) because the g'root
descent reveals two additional points.
(As far as deployment goes, I think it makes sense to get an initial
schnorr/taproot/mast deployment out first, and add graftroot/aggregation
later. My feeling is there's no great urgency for generalised taproot, so
it would make sense to keep doing schnorr/taproot/mast for now, take time
analysing generalised taproot, and if it seems sane and useful, aim to
enable it in a later phase, eg at the same time as graftroot/aggregation)
[0] My inital name for these was "MAST-ended sc'roots", since it
    combines "taproot" and "scripts" and something MAST-like but only
    at the very end, but I was warned that the Mimblewimble folks have
    vast teams monitoring for Harry Potter references and will DMCA me,
    which I assume stands for "Dementors, Ministry, Cruciatus and Avada
    kedavra"... So I'm abbreviating generalised taproot as "g'root"
    instead. After all, what's the worst the Marvel guys could do?

@_date: 2018-06-27 17:29:09
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
This seems persuasive to me. I think you could implement graftroot in
a way that makes this explicit:
 * A graftroot input has >=2 items on the witness stack, a signature,
   a script (S), and possibly witness elements for the script. The
   signature has a SIGHASH_GRAFTROOT bit set.
 * To validate the signature, a virtual transaction is constructed:
     nVersion = 1
     locktime = 0
     inputs = [(txhash, txoutidx, 0, "", 0xffffffff)]
     outputs = [(txvalue, len(S), S)]
     locktime = 0
   The signature is then checked against the virtual transaction.
 * If the signature is valid, the virtual transaction is discarded, and
   the script and witness elements are checked against the original tx.
I think this approach (or one like it) would make it clear that
graftroot is a simple optimisation, rather than changing the security
parameters. Some caveats:
 * You'd presumably want to disallow signatures with SIGHASH_GRAFTROOT
   from being used in signatures in scripts, so as not to end up having
   to support recursive graftroot.
 * Checking the script/witness against the original transaction instead
   of the virtual one cheats a bit, but something like it is necessary
   to ensure locktime/csv checks in the script S behave sanely. You
   could have the virtual transaction be treated as being confirmed in
   the same block as the original transaction instead though, I think.
 * You would need to use SIGHASH_NOINPUT (or similar) in conjuction
   to allow graftroot delegation prior to constructing the tx (otherwise
   the signature would be committing to txhash/txoutidx). BIP118 would
   still commit to txvalue, but would otherwise work fine, I think.

@_date: 2018-03-14 12:12:11
@_author: Anthony Towns 
@_subject: [bitcoin-dev] {sign|verify}message replacement 
Wouldn't it be sufficient for old nodes to check for standardness of the spending script and report non-standard scripts as either invalid outright, or at least highly questionable? That should prevent confusion as long as soft forks are only making nonstandard behaviours invalid.

@_date: 2018-03-21 14:06:18
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation 
Hello world,
There was a lot of discussion on Schnorr sigs and key and signature
aggregation at the recent core-dev-tech meeting (one relevant conversation
is transcribed at [0]).
Quick summary, with more background detail in the corresponding footnotes:
signature aggregation is awesome [1], and the possibility of soft-forking
in new opcodes via OP_RETURN_VALID opcodes (instead of OP_NOP) is also
awesome [2].
Unfortunately doing both of these together may turn out to be awful.
RETURN_VALID and Signature Aggregation
There are two obvious solutions here:
 0a) Just be very careful to ensure any aggregated signatures that
     are conditional on an redefined RETURN_VALID opcode go into later
     buckets, but be careful about having separate sets of buckets every
     time a soft-fork introduces a new redefined opcode. Probably very
     complicated to implement correctly, and essentially doubles the
     number of buckets you have to potentially deal with every time you
     soft fork in a new opcode.
 0b) Alternatively, forget about the hope that RETURN_VALID
     opcodes could be converted to anything, and just reserve OP_NOP
     opcodes and convert them to CHECK_foo_VERIFY opcodes just as we
     have been doing, and when we can't do that bump the segwit witness
     version for a whole new version of script. Or in twitter speak:
     "non-verify upgrades should be done with new script versions" [3]
I think with a little care we can actually salvage RETURN_VALID though!
Solution 1
You don't actually have to write your scripts in ways that can cause
this problem, as long as you're careful. In particular, the problem only
occurs if you do aggregatable CHECKSIG operations after "RETURN_VALID"

@_date: 2018-03-21 21:21:19
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation 
Good evening Zeeman!
[pulled from the bottom of your mail]
Sorry, I probably didn't explain it well (or at all): during the script,
you're collecting public keys and messages (ie, BIP 143 style digests)
which then go into the signing/verification algorithm to produce/check
the signature.
You do need to gather signatures from each private key holder when
producing the aggregate signature, but that happens at the wallet/p2p
level, rather than the consensus level.
Checking the gathered public keys match the aggregated signature is
something that only happens for the entire transaction as a whole, so
you don't need an opcode for it in the scripts, since they're per-input.
Otherwise, I think that's pretty similar to what I was already saying;
   SIGHASH_ALL|BUCKET_1 pubkey OP_CHECKSIG
would be adding "pubkey" and a message hash calculated via the SIGHASH_ALL
hashing rules to the list of things that the signature for bucket 1 verifies.
FWIW, the Bellare-Neven verification algorithm looks something like:
   s*G = R + K   (s,R is the signature)
   K = sum( H(R, L, i, m) * X_i )   for i corresponding to each pubkey X_i
   L = the concatenation of all the pubkeys, X_0..X_n
   m = the concatenation of all the message hashes, m_0..m_n
So the way I look at it is each input puts a public key and a message hash
(X_i, m_i) into the bucket via a CHECKSIG operation (or similar), and once
you're done, you look into the bucket and there's just a single signature
(s,R) left to verify. You can't start verifying any of it until you've
looked through all the scripts because you need to know L and m before
you can do anything, and both of those require info from every part of
the aggregation.
[0] [1]
Pre-softfork nodes not doing any checking doesn't work with cross-input
signature aggregation as far as I can see. If it did, all you would have
to do to steal people's funds is mine a non-standard transaction:
  inputs:
   my-millions:
     pay-to-pubkey pubkey1
     witness=SIGHASH_ALL|BUCKET_1
   your-two-cents:
     pay-to-script-hash script=[1 OP_RETURN_TRUE pubkey2 CHECKSIG]
     witness=SIGHASH_ALL|BUCKET_1
   bucket1: 64-random-bytes
  output:
   all-the-money: you
Because there's no actual soft-fork at this point every node is an "old"
node, so they all see the OP_RETURN_TRUE and stop validating signatures,
accepting the transaction as valid, and giving you all my money, despite
you being unable to actually produce my signature.
Make sense?
[0] For completeness: constructing the signature for Bellare-Neven
    requires two communication phases amongst the signers, and looks
    roughly like:
     1. each party generates a random variable r_i, and sharing the
        corresponding curve point R_i=r_i*G and their sighash choice
        (ie, m_i) with the other signers.
     2. this allows each party to calculate R=sum(R_i) and m,
        and hence H(R,L,i,m), at which point each party calculates a
        partial signature using their respective private key, x_i:
          s_i = r_i + H(R,L,i,m)*x_i
        all these s_i values are then communicated to each signer.
     3. these combine to give the final signature (s,R),
        with s=sum(s_i), allowing each signer to verify that the signing
        protocol completed successfully, and any signer can broadcast
        the transaction to the blockchain
[1] muSig differs in the details, but is basically the same.

@_date: 2018-03-27 16:34:33
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation 
What's the reason for those? I could see an argument for RETURN_VALID, I guess:
  confA IF condB IF condC IF [pathA] RETURN_VALID ENDIF ENDIF ENDIF [pathB]
is probably simpler and saves 3 bytes compared to:
  1 condA IF condB IF condC IF [pathA] NOT ENDIF ENDIF ENDIF IF [pathB] ENDIF
but that doesn't seem crazy compelling? I don't see a reason to just keep
one OP_NOP though.
I was figuring (c), fwiw, and assuming that opcodes will just be about
manipulating stack values and marking the script as invalid, rather than,
say, introducing new flow control ops.
You're probably right. That still doesn't let you implement intercal's
COMEFROM statement as a new opcode, of course. :)
I guess when passing the script you could perhaps check if each witness
item could have been replaced with OP_FALSE or OP_1 and still get the
same result, and consider the transaction non-standard if so?
Hmm? The opcode I suggested works just as easily with arbitrary formulas,
eg, "There must be at least 1 signer from pka{1,2,3}, and 3 signers all
up, except each of pkb{1,2,3,4,5,6} only counts for half":
  0 pkb6 pkb5 pkb4 pkb3 pkb2 pkb1 pka3 pka2 pka1 9 CHECK_AGGSIG_VERIFY
    (declare pubkeys)
  0b111 CHECK_AGG_SIGNERS VERIFY
    (one of pka{1,2,3} must sign)
  0b001 CHECK_AGG_SIGNERS
  0b010 CHECK_AGG_SIGNERS ADD
  0b100 CHECK_AGG_SIGNERS ADD
  DUP ADD
    (pka{1,2,3} count double)
  0b000001000 CHECK_AGG_SIGNERS ADD
  0b000010000 CHECK_AGG_SIGNERS ADD
  0b000100000 CHECK_AGG_SIGNERS ADD
  0b001000000 CHECK_AGG_SIGNERS ADD
  0b010000000 CHECK_AGG_SIGNERS ADD
  0b100000000 CHECK_AGG_SIGNERS ADD
    (pkb{1..6} count single)
  6 EQUAL
    (summing to a total of 3 doubled)
Not sure that saves it from being "hacky and weird" though...
(There are different ways you could do "CHECK_AGG_SIGNERS": for
instance, take a bitmask of keys and return the bitwise-and with the
keys that signed, or take a bitmask and just return the number of keys
matching that bitmask that signed, or take a pubkey index and return a
boolean whether that key signed)

@_date: 2018-03-30 16:14:18
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Optimized Header Sync 
In that case, shouldn't the checkpoints just be every 2016 blocks and
include the corresponding bits value for that set of blocks?
That way every node commits to (approximately) how much work their entire
chain has by sending something like 10kB of data (currently), and you
could verify the deltas in each node's chain's target by downloading the
2016 headers between those checkpoints (~80kB with the proposed compact
encoding?) and checking the timestamps and proof of work match both the
old target and the new target from adjacent checkpoints.
(That probably still works fine even if there's a hardfork that allows
difficulty to adjust more frequently: a bits value at block n*2016 will
still enforce *some* lower limit on how much work blocks n*2016+{1..2016}
will have to contribute; so will still allow you to estimate how much work
will have been done, it may just be less precise than the estimate you could
generate now)

@_date: 2018-05-09 00:40:21
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP sighash_noinput 
So first, I'm not sure if I'm actually criticising or playing devil's
advocate here, but either way I think criticism always helps produce
the best proposal, so....
The big concern I have with _NOINPUT is that it has a huge failure
case: if you use the same key for multiple inputs and sign one of them
with _NOINPUT, you've spent all of them. The current proposal kind-of
limits the potential damage by still committing to the prevout amount,
but it still seems a big risk for all the people that reuse addresses,
which seems to be just about everyone.
I wonder if it wouldn't be ... I'm not sure better is the right word,
but perhaps "more realistic" to have _NOINPUT be a flag to a signature
for a hypothetical "OP_CHECK_SIG_FOR_SINGLE_USE_KEY" opcode instead,
so that it's fundamentally not possible to trick someone who regularly
reuses keys to sign something for one input that accidently authorises
spends of other inputs as well.
Is there any reason why an OP_CHECKSIG_1USE (or OP_CHECKMULTISIG_1USE)
wouldn't be equally effective for the forseeable usecases? That would
ensure that a _NOINPUT signature is only ever valid for keys deliberately
intended to be single use, rather than potentially valid for every key.
It would be ~34 witness bytes worse than being able to spend a Schnorr
aggregate key directly, I guess; but that's not worse than the normal
taproot tradeoff: you spend the aggregate key directly in the normal,
cooperative case; and reserve the more expensive/NOINPUT case for the
unusual, uncooperative cases. I believe that works fine for eltoo: in
the cooperative case you just do a SIGHASH_ALL spend of the original
transaction, and _NOINPUT isn't needed.
Maybe a different opcode maybe makes sense at a "philosophical" level:
normal signatures are signing a spend of a particular "coin" (in the
UTXO sense), while _NOINPUT signatures are in some sense signing a spend
of an entire "wallet" (all the coins spendable by a particular key, or
more accurately for the current proposal, all the coins of a particular
value spendable by a particular key). Those are different intentions,
so maybe it's reasonable to encode them in different addresses, which
in turn could be done by having a new opcode for _NOINPUT.
A new opcode has the theoretical advantage that it could be deployed
into the existing segwit v0 address space, rather than waiting for segwit
v1. Not sure that's really meaningful, though.

@_date: 2018-05-10 22:10:27
@_author: Anthony Towns 
@_subject: [bitcoin-dev] MAST/Schnorr related soft-forks 
Hello world,
After the core dev meetup in March I wrote up some notes of where I
think things stand for signing stuff post-Schnorr. It was mostly for my
own benefit but maybe it's helpful for others too, so...
They're just notes, so may assume a fair bit of background to be able to
understand the meaning of the bullet points. In particular, note that I'm
using "schnorr" just to describe the signature algorithm, and the terms
"key aggregation" to describe turning an n-of-n key multisig setup into
a single key setup, and "signature aggregation" to describe combining
signatures from many inputs/transactions together: those are often all
just called "schnorr signatures" in various places.
Anyway! I think it's fair to split the ideas around up as follows:
1) Schnorr CHECKSIG
  Benefits:
    - opportunity to change signature encoding from DER to save a few
      bytes per signature, and have fixed size signatures making tx size
      calculations easier
    - enables n-of-n multisig key aggregation (a single pubkey and
      signature gives n-of-n security; setup non-interactively via muSig,
      or semi-interactively via proof of possession of private key;
      interactive signature protocol)
    - enables m-of-n multisig key aggregation with interactive setup and
      interactive signature protocol, and possibly substantial storage
      requirements for participating signers
    - enables scriptless scripts and discreet log contracts via
      key aggregation and interactive
    - enables payment decorrelation for lightning
    - enables batch validation of signatures, which substantially reduces
      computational cost of signature verification, provided a single
      "all sigs valid" or "some sig(s) invalid" output (rather than
      "sig number 5 is invalid") is sufficient
    - better than ecdsa due to reducing signature malleability
      (and possibly due to having a security proof that has had more
      review?)
   Approaches:
     - bump segwit version to replace P2WPKH
     - replace an existing OP_NOP with OP_CHECKSCHNORRVERIFY
     - hardfork to allowing existing addresses to be solved via Schnorr sig
       as alternative to ECDSA
2) Merkelized Abstract Syntax Trees
   Two main benefits for enabling MAST:
    - logarithmic scaling for scripts with many alternative paths
    - only reveals (approximate) number of alternative execution branches,
      not what they may have been
   Approaches:
    - replace an existing OP_NOP with OP_MERKLE_TREE_VERIFY, and treat an
      item remaining on the alt stack at the end of script exeution as a
      script and do tail-recursion into it (BIP 116, 117)
    - bump the segwit version and introduce a "pay-to-merkelized-script"
      address form (BIP 114)
3) Taproot
   Requirements:
    - only feasible if Schnorr is available (required in order to make the
      pubkey spend actually be a multisig spend)
    - andytoshi has written up a security proof at
         Benefits:
    - combines pay-to-pubkey and pay-to-script in a single address,
      improving privacy
    - allows choice of whether to use pubkey or script at spend time,
      allowing for more efficient spends (via pubkey) without reducing
      flexibility (via script)
   Approaches:
    - bump segwit version and introduce a "pay-to-taproot" address form
4) Graftroot
   Requirements:
    - only really feasible if Schnorr is implemented first, so that
      multiple signers can be required via a single pubkey/signature
    - people seem to want a security proof for this; not sure if that's
      hard or straightforward
   Benefits:
    - allows delegation of authorisation to spend an output already
      on the blockchain
    - constant scaling for scripts with many alternative paths
      (better than MAST's logarithmic scaling)
    - only reveals the possibility of alternative execution branches,       not what they may have been or if any actually existed
   Drawbacks:
    - requires signing keys to be online when constructing scripts (cannot
      do complicated pay to cold wallet without warming it up)
    - requires storing signatures for scripts (if you were able to
      reconstruct the sigs, you could just sign the tx directly and wouldn't
      use a script)
    - cannot prove that alternative methods of spending are not
      possible to anyone who doesn't exclusively hold (part of) the
      output address private key
    - adds an extra signature check on script spends
   Approaches:
    - bump segwit version and introduce a "pay-to-graftroot" address form
5) Interactive Signature Aggregation
   Requirements:
    - needs Schnorr
   Description:
    - allows signers to interactively collaborate when constructing a
      transaction to produce a single signature that covers multiple
      inputs and/or OP_CHECKSIG invocations that are resolved by Schnorr
      signatures
   Benefits:
    - reduces computational cost of additional signatures (i think?)
    - reduces witness storage needed for additional signatures to just the
      sighash flag byte (or bytes, if it's expanded)
    - transaction batching and coinjoins potentially become cheaper than
      independent transactions, indirectly improving on-chain privacy
   Drawbacks:
    - each soft-fork introduces a checkpoint, such that signatures that
      are not validated by versions prior to the soft-fork cannot be
      aggregated with signatures that are validated by versions prior to
      the soft-fork (see [0] for discussion about avoiding that drawback)
   Approaches:
    - crypto logic can be implemented either by Bellare-Neven or MuSig
    - needs a new p2wpkh output format, so likely warrants a segwit
      version bump
    - may warrant allowing multiple aggregation buckets
    - may warrant peer-to-peer changes and a new per-tx witness
6) Non-interactive half-signature aggregation within transaction
   Requirements:
     - needs Schnorr
     - needs a security proof before deployment
   Benefits:
     - can halve the size of non-aggregatable signatures in a transaction
     - in particular implies the size overhead of a graftroot script
       is just 32B, the same as a taproot script
   Drawbacks:
     - cannot be used with scriptless-script signatures
   Approaches:
     - ideally best combined with interactive aggregate signatures, as it
       has similar implementation requirements
7) New SIGHASH modes
   These will also need a new segwit version (for p2pk/p2pkh) and probably
   need to be considered at the same time.
8) p2pk versus p2pkh
   Whether to stick with a pubkeyhash for the address or just have a pubkey
   needs to be decided for any new segwit version.
9) Other new opcodes
   Should additional opcodes in new segwit versions be reserved as OP_NOP or
   as OP_RETURN_VALID, or something else?
   Should any meaningful new opcodes be supported or re-enabled?
10) Hard-fork automatic upgrade of p2pkh to be spendable via segwit
   Making existing p2pk or p2pkh outputs spendable via Schnorr with
   interactive signature aggregation would likely be a big win for people
   with old UTXOs, without any decrease in security, especially if done
   a significant time after those features were supported for new outputs.
11) Should addresses be hashes or scripts?
   maaku's arguments for general opcodes for MAST make me wonder a bit
   if the "p2pkh" approach isn't better than the "p2wpkh" approach; ie
   should we have script opcodes as the top level way to write addresses,
   rather than picking the "best" form of address everyone should use,
   and having people have to opt-out of that. probably already too late
   to actually have that debate though.
Anyway, I think what that adds up to is:
 - Everything other than MAST and maybe some misc new CHECKVERIFY opcodes
   really needs to be done via new segwit versions
 - We can evaluate MAST in segwit v0 independently -- use the existing
   BIPs to deploy MAST for v0; and re-evaluate entirely for v1 and later
   segwit versions.
 - There is no point deploying any of this for non-segwit scripts
 - Having the taproot script be a MAST root probably makes sense. If so,
   a separate OP_MERKLE_MEMBERSHIP_CHECK opcode still probably makes
   sense at some point.
So I think that adds up to:
 a) soft-fork for MAST in segwit v0 anytime if there's community/economic
    support for it?
 b) soft-fork for OP_CHECK_SCHNORR_SIG_VERIFY in segwit v0 anytime
 c) soft-fork for segwit v1 providing Schnorr p2pk(h) addresses and
    taproot+mast addresses in not too much time
 d) soft-fork for segwit v2 introducing further upgrades, particularly
    graftroot
 e) soft-fork for segwit v2 to support interactive signature aggregation
 f) soft-fork for segwit v3 including non-interactive sig aggregation
The rationale there is:
  (a) and (b) are self-contained and we could do them now. My feeling is
  better to skip them and go straight to (c)
  (c) is the collection of stuff that would be a huge win, and seems
  "easily" technically feasible. signature aggregation seems too
  complicated to fit in here, and getting the other stuff done while we
  finish thinking about sigagg seems completely worthwhile.
  (d) is a followon for (c), in case signature aggregation takes a
  *really* long while. It could conceivably be done as a different
  variation of segwit v1, really. It might turn out that there's no
  urgency for graftroot and it should be delayed until non-interactive
  sig aggregation is implementable.
  (e) and (f) are separated just because I worry that non-interactive
  sig aggregation might not turn out to be possible; doing them as a
  single upgrade would be preferrable.
[0]

@_date: 2018-05-14 19:23:29
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev]  BIP sighash_noinput 
So, I don't find that very compelling: "there's already a way to lose
your money, so it's fine to add other ways to lose your money". And
again, I think NOINPUT is worse here, because a SIGHASH_NONE signature
only lets others take the coin you're trying to spend, messing up when
using NOINPUT can cause you to lose other coins as well (with caveats).
I could see either dropping support for SIGHASH_NONE for segwit
v1 addresses, or possibly limiting SIGHASH_NONE in a similar way to
limiting SIGHASH_NOINPUT. Has anyone dug through the blockchain to see
if SIGHASH_NONE is actually used/useful?
Taproot and graftroot aren't "less script" at all -- if anything they're
the opposite in that suddenly every address can have a script path.
I think NOINPUT has pretty much the same tradeoffs as taproot/graftroot
scripts: in the normal case for both you just use a SIGHASH_ALL
signature to spend your funds; in the abnormal case for NOINPUT, you use
a SIGHASH_NOINPUT (multi)sig for unilateral eltoo closes or watchtower
penalties, in the abnormal case for taproot/graftroot you use a script.
That's backwards. If you introduce a new opcode, you can use the existing
segwit version, rather than needing segwit v1. You certainly don't need
v1 segwit for regular coins and v2 segwit for NOINPUT coins, if that's
where you were going?
For segwit v0, that would mean your addresses for a key "X", might be:
   [pubkey]  X        - not usable with NOINPUT
   [script]  2 X Y 2 CHECKMULTISIG
    - not usable with NOINPUT
   [script]  2 X Y 2 CHECKMULTISIG_1USE_VERIFY
    - usable with NOINPUT (or SIGHASH_ALL)
CHECKMULTISIG_1USE_VERIFY being soft-forked in by replacing an OP_NOP,
of course. Any output spendable via a NOINPUT signature would then have
had to have been deliberately created as being spendable by NOINPUT.
For a new segwit version with taproot that likewise includes an opcode,
that might be:
   [taproot]  X
    - not usable with NOINPUT
   [taproot]  X or: X CHECKSIG_1USE
    - usable with NOINPUT
If you had two UTXOs (with the same value), then if you construct
a taproot witness script for the latter address it will look like:
    X [X CHECKSIG_1USE] [sig_X_NOINPUT]
and that signature can't be used for addresses that were just intending
to pay to X, because the NOINPUT sig/sighash simply isn't supported
without a taproot path that includes the CHECKSIG_1USE opcode.
In essence, with the above construction there's two sorts of addresses
you generate from a public key X: addresses where you spend each coin
individually, and different addresses where you spend the wallet of
coins with that public key (and value) at once; and that remains the
same even if you use a single key for both.
I think it's slightly more reasonable to worry about signing with NOINPUT
compared to signing with SIGHASH_NONE: you could pretty reasonably setup
your (light) bitcoin wallet to not be able to sign (or verify) with
SIGHASH_NONE ever; but if you want to use lightning v2, it seems pretty
likely your wallet will be signing things with SIGHASH_NOINPUT. From
there, it's a matter of having a bug or a mistake cause you to
cross-contaminate keys into your lightning subsystem, and not be
sufficiently protected by other measures (eg, muSig versus checkmultisig).
(For me the Debian ssh key generation bug from a decade ago is sufficient
evidence that people you'd think are smart and competent do make really
stupid mistakes in real life; so defense in depth here makes sense even
though you'd have to do really stupid things to get a benefit from it)
The other benefit of a separate opcode is support can be soft-forked in
independently of a new segwit version (either earlier or later).
I don't think the code has to be much more complicated with a separate
opcode; passing an extra flag to TransactionSignatureChecker::CheckSig()
is probably close to enough. Some sort of flag remains needed anyway
since v0 and pre-segwit signatures won't support NOINPUT.

@_date: 2018-11-21 06:29:04
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Current flags are {ALL, NONE, SINGLE} and ANYONECANPAY, and the BIP143
tx digest consists of the hash of:
  1 nVersion
  4 outpoint
  5 input scriptCode
  6 input's outpoint value
  7 input's nSeq
  9 nLocktime
 10 sighash
  2 hashPrevOuts (commits to 4,5,6; unless ANYONECANPAY)
  3 hashSequence (commits to 7; only if ALL and not ANYONECANPAY)
  8 hashOutputs
       - NONE: 0
       - SINGLE: {value,scriptPubKey} for corresponding output
       - otherwise: {value,scriptPubKey} for all outputs
The fee is committed to by hashPrevOuts and hashOutputs, which means
NOFEE is only potentially useful if ANYONECANPAY or NONE or SINGLE is set.
For NOINPUT, (2),(3),(4) are cleared, and SCRIPTMASK (which munges (5))
is only useful given NOINPUT, since (4) indirectly commits to (5). Given this implementation, NOINPUT effectively implies ANYONECANPAY,
I think. (I think that is also true of BIP 118's NOINPUT spec)
Does it make sense to treat this as two classes of options, affecting
the input and output side:
  output: (pick one, using bits 0,1)
    * NONE -- don't care where the money goes
    * SINGLE -- want this output
    * ALL -- want exactly this set of outputs
  input: (pick one, using bits 4,5)
    * PARTIALSCRIPT -- spending from some tx with roughly this script (and
                       maybe others; SCRIPTMASK|NOINPUT|ANYONECANPAY)
    * KNOWNSCRIPT -- spending from some tx with exactly this script (and
                     maybe others; NOINPUT|ANYONECANPAY)
    * KNOWNTX -- spending from this tx (and maybe others; ANYONECANPAY)
    * ALL_INPUTS -- spending from exactly these txes
  combo: (flag, bit 6)
    * NOFEE -- don't commit to the fee
I think NONE without NOFEE doesn't make much sense, and
NOFEE|ALL|ALL_INPUTS would also be pretty weird. Might make sense to
warn/error on signing when asking for those combinations, and maybe even
to fail on validating them.
(Does it make sense to keep SIGHASH_NONE? I guess SIGHASH_NONE|ALL_INPUTS
could be useful if you just use sigs on one of the other inputs to commit
to a useful output)
FWIW, OP_MASK seems a bit complicated to me. How would you mask a script
that looks like:
   OP_MASK IF  ENDIF  ...
   IF OP_MASK ENDIF  ...
I guess if you make the rule be "for every OP_MASK in scriptCode the
*immediately* subsequent opcode/push is removed (if present)" it would
be fine though -- that would make OP_MASK in both the above not have
any effect. (Maybe a more explicit name like "MASK_PUSH_FOR_SIGHASH"
or something might be good?)
I don't have a reason why, but committing to the scriptCode feels to me
like it reduces the "hackiness" of NOINPUT a lot.

@_date: 2018-11-23 15:03:30
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Would it be sufficient to sign the position within the script of the
last OP_CODESEPARATOR? That is, if your script is:
   DUP DUP CHECKSIG CODESEP CHECKSIG CODESEP CHECKSIG
with the idea being that it can be spent by providing any pub key and
three different signatures by that key, with the first sig committing
to a "codesep position" of 0, the second a "codesep position" of 4,
and the third a "codesep position" of 6? In each case, the signature
also commits to the full (possibly masked) script as well.
I think that covers all the behaviour you can currently achieve with
CODESEP (which is pretty limited since every sig effectively commits
to the full redeem script, and you can't commit to subsets of the
signature/witness), and it keeps the things you can do with the various
features a bit orthogonal:
 NOINPUT -- lets the sig apply to different transactions
 OP_MASK -- lets the different txes have variations in the script the
            sig applies to
 CODESEP -- lets you require different sigs for different parts of a
            single script
 MAST[0] -- provides alternative scripts, doesn't affect sigs
 IF/etc  -- provides control flow within a script, doesn't affect sigs
[0] (I think I'm going to claim "MAST" stands for "merkelized alternative
     script tree" these days, since they're not "abstract syntax trees")

@_date: 2018-11-23 16:04:04
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Commiting to just the sequence numbers seems really weird to me; it
only really prevents you from adding inputs, since you could still
replace any input that was meant to be there by almost any arbitrary
other transaction...
I could see this *maybe* making sense if you at least committed to the
values of each input's outpoint; since that would be an actual constraint?
I don't think you can commit to anything else about the other inputs:
   -- txids of the other transactions wouldn't work if you had other
      NOINPUT txes, and would introduce O(N^2) validation cost if someone
      signed every input with NOINPUT but committed to the txids of
      every other input
   -- scriptPubKeys wouldn't really work for eltoo-like constructions
      that want to vary the scripts but apply the same sig, but might
      work sometimes?
   -- witness scripts for the other inputs could be unknown at your
      signing time, or arbitrarily large and thus a pain to have to send
      to a hardware wallet
Just treating NOINPUT as a subset of ANYONECANPAY seems simpler to
me though...
I think OP_MASK is okay as far as layering goes, if you just think of it
as a (set of) multibyte "OP_MASKED_PUSH" opcode(s). So when you
pseudocode a script like:
     OP_CSV OP_DROP  OP_CHECKSIG
and then decide  needs to be masked, you rewrite it as:
    [n] OP_CSV OP_DROP  OP_CHECKSIG
indicating n is masked, and don't worry about the exact bytes that will
encode the push, anymore than you currently worry about whether it's
OP_0, OP_1..16, <1..75>+1..75-bytes, PUSHDATA[1,2,3]+n+n-bytes.
As long as OP_MASK only applies to a PUSH and it's an error for OP_MASK
not to be immediately followed by that PUSH, I think that all works
out fine.

@_date: 2018-11-28 20:49:46
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Multi party Schnorr Rust implementation 
I think you can work around this to some extent by "batching" signing
For interactive multisignatures (threshold or not), the protocol is:
   produce secret nonce r, calculate public nonce R=r*G
   everyone shares H(R)
   everyone shares R, checks received values match received hashes
   everyone calculates s=r+H(R',P',m)*p, shares s
For deterministic nonces, you generate r=H(p,m) based on the message
being signed and your private key, so can only start this process when
you start signing, and the sharing rounds mean interactivity.
But you don't strictly need deterministic nonces, you just have to never
use the same nonce with a different message. If you arrange to do that
by keeping some state instead, you can calculate nonces in advance:
phase 1:
    produce secret nonces r1..r1024, calculate R1..R1024
    share H(R1)..H(R1024)
phase 2:
    store other parties hashes, eg as H1..H1024
    share R1..R1024
phase 3:
    check received nonces match, ie H(R1)=H1, etc
phase 4:
    request to sign msg m, with nonce n
    if nonce n has already been used, abort
    mark nonce n as having being used
    lookup other signer's nonces n and sum them to get R'
    calculate s = rn + H(R',P',m)*p
    share s
That way you could do phases 1-3 once, and then do 1024 signatures during
the month on whatever your current timetable is.
You could also combine these phases, so when you get a signing request you:
   * receive msg to sign m, n=4; everyone else's R4, H(R5)
   * check  H(R4) = previously received "H(R4)"
   * calculate R4' by summing up your and everyone's R4s
   * bump state to n=5
   * do the signature...
   * send sig=(s,R4), R5, H(R6)
which would let you have an untrusted app that does the coordination and
shares the nonces and nonce-hashes, and getting all the needed air-gapped
communication in a single round. (This is effectively doing phase 3 and
4 for the current signature, phase 2 for the next signature, and phase
1 for the signature after that all in one round of communication)
That seems almost as good as true non-interactivity to me, if your signing
hardware is capable of securely storing (and updating) a few kB of state
(which is probably not quite as easy as it sounds).

@_date: 2019-12-03 18:35:38
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
Hmm, I take the opposite lesson from your scenario -- it's only fine for
people to bring their own 2-of-3 or 5-of-6 or whatever and replace a
simple key if you've got something like miniscript where you understand
the script completely enough that you can be sure those changes are
fine. For contrast, with ECDSA and pre-miniscript, the above scenario might
have gone like someone proposing to change:
  7 A B C1 C2 C3 C4 C5 C6 C7 C8 C9 11 CHECKMULTISIG
for something like
  7
  SWAP IF TOALT 2 A1 A2 A3 3 CHECKMULTISIGVERIFY FROMALT 1SUB ENDIF
  SWAP IF TOALT 5 B1 B2 B3 B4 B5 B6 6 CHECKMULTISIGVERIFY FROMALT 1SUB ENDIF
  C1 C2 C3 C4 C5 C6 C7 C8 C9 11 CHECKMULTISIG
but I think you'd want to be pretty sure you can decode those added
policies rather than just accepting it because your "C4" key is still
there. (In particular, any script fragment that uses an opcode that used
to be OP_SUCCESS could have arbitrary effects on the script)
(I'm trying to avoid using MAST in the context of taproot, despite the
backronym, so please excuse the rephrasing--)
I think if you're going to start using a taproot address with multiple
tapscripts, either as a participant in a multiparty smart contract,
or just to have different ways of spending your funds, then you do have
to analyse all the branches to make sure there's no hidden "all the
money goes to the Lizard People" script.
Once you've done that, you can then simplify things -- maybe some
scripts are only useful for other participants in the contract, or maybe
you've got a few different hardware wallets and one only needs to know
about one branch, while the other only needs to know about some other
branch, but you still need to have done the analysis in the first place.
Of course, probably most of the time that "analysis" is just making sure
the scripts match some well known, hardcoded template, as filled out
with various (tweaked) keys that you've checked elsewhere, but that
still ensures you know all the scripts do what you need them too.
Err? The current behaviour of CODESEP with taproot was first discussed in
[1], which summarised it as "CODESEP -- lets you require different sigs
for different parts of a single script" which seems to me like just a
different way of saying the same thing.
[1] I don't think tapscript's CODESEP or the current CODESEP can be used
for anything other than preventing a signature from being reused for a
different CHECKSIG operation on the same pubkey within the same script.
I think techniques like miniscript and having fixed templates specified
in BIPs and BOLTs and the like are better approaches -- both let you
easily allow a limited set of changes that can be safely made to a policy
(maybe just substituting keys, hashes and times, maybe allowing more
general changes).
Sounds like an economics argument :)
Right -- so if you're worried about this sort of attack, you need to
analyse your script to at least be sure that it's not one of these cases
that aren't covered. And if you've got to analyse the script anyway
(which I think you do no matter what), then there's no benefit -- you're
either doing something simple and you're using templates or miniscript
to make the analysis easy; or you're doing something novel and complex,
and you can probably cope with using CODESEP.
(Ultimately I think there's only really two cases where you're
contributing a signature for a tx: either you're a party to the contract,
and you should have fully analysed all the possible ways the utxo could
be spent to make sure the smart contract stuff is correctly implemented
and you can't be cheated; or you're acting as an oracle or similar and
don't really care how the contract goes because you're not a party to
it, in which case people reusing your signature as much as they like is
fine. Hardware wallets don't need to analyse scripts they sign for, eg,
but that's only because for those cases where their owners have done
that first)
I agree the burden's pretty minor; but I think having a single value
for the tx digest for each input for SIGHASH_ALL is kind-of nice for
validation; and I think having to pass through a CHECKSIG position
everytime you do a signature is likely to be annoying for implementors
for pretty much zero actual benefit.
I'm making two points with that example: (1) it's a case where if
you don't analyse the scripts somehow, you can still be vulnerable to
the attack with your change -- so your change doesn't let you avoid
knowing what scripts do; but also (2) that CODESEP is a marginally more
efficient/general fix the problem. Maybe (1) isn't too important,
because even if it weren't true, I still think you need to know what all
the scripts do, but I think (2)'s still reelevant.
As you aluded to in the previous mail; I think the problem's currently
extremely rare and trivially avoidable because we don't really have any
way of manipulating pubkeys -- there's no CAT, EC_ADD/EC_MUL/EC_TWEAK
or MERKLEPATHVERIFY opcode (or actual Merkle Abstract Syntax Trees or
OP_EXEC etc) to make it a dynamic concern rather than a static one.
If CODESEP had never existed, I think my first response would be to say
"well, just make sure you don't reuse pubkeys, and because each
bip-schnorr sig commits to the pubkey, problem solved."
There's only two use cases I'm aware of, one is the ridiculous
reveal-a-secret-key-by-forced-nonce-reuse script that's never actually
been implemented [2] and ntumblebit's escrow script [3]. The first of
those requires pubkey recovery so doesn't work with bip-schnorr anyway;
and it's not clear to me whether the second is really reason enough to
justify a dedicated opcode/sighash/etc.
[2] [3] An option would be to remove CODESEP and treat it as OP_SUCCESS -- that
way it could be introduced later with pretty much the exact semantics
that are currently proposed; or with some more useful semantics. That
way we could bring in whatever functionality was actually needed at the
same time as introducing CAT/EC_MUL/etc.
But my default position is to think that the way things currently work is
mostly fine, and we should default ot just keeping the same functionality

@_date: 2019-12-06 14:51:53
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
Thanks for the careful write up! That matches what I was thinking.
On IRC, gmaxwell suggests "OP_BREADCRUMB" as a name for (something like)
this functionality.
(I think it's a barely plausible stretch to use the name "CODESEPARATOR"
for marking a position in the script -- that separates what was before
and after, at least; anything more general seems like it warrants a
better name though)
FWIW, I think it's too soon to propose this because (a) it's not clear
there's a practical need for it, (b) it's not clear the functionality is
quite right (opcode vs more automatic sighash flag?), and (c) as you say,
it's not clear it's powerful enough.
Last bit first, it seems pretty clear to me that this is too novel an
idea to propose it immediately -- we should explore the problem space
more first to see what's the best way of doing it before coding it into
consensus. And (guessing) I think the tapscript upgrade methods should
be fine for handling this later.
I think the annex is also not general enough for what you're thinking
here, in that it wouldn't allow for one signature to constrain the witness
data more than some other signature -- so you'd need to determine all
the constraints for all signatures to finish filling out the annex,
and could only then start signing.
I think you could conceivably do any/all of:
 * commit to a hash of all the witness data that hasn't been popped off
   the stack ("suffix" commitment -- the data will be used by later script
   opcodes)
 * commit to a hash of all the witness data that has been popped off the
   stack ("prefix" commitment -- this is the data that's been used by
   earlier script opcodes)
 * commit to the hash of the current stack
That would be expensive, but still doable as O(1) per opcode / stack
element. I think any other masking would mean you'd have potentially
O(size of witness data) or O(size of stack) runtime per signature which
I think would be unacceptable...
I guess a general implementation to at least think about the possibilities
might be an "OP_DATACOMMIT" opcode that pops an element from the stack,
does hash_"DataCommit"(element), and then any later signatures commit
to that value (maybe with OP_0 OP_DATACOMMIT allowing you to get back to
the default state). You'd either need to write your script carefully to
commit to witness data you're using elsewhere, or have some other new
opcodes to do that more conveniently...
CODESEP at position "x" in the script is equivalent to " DATACOMMIT"
here, I think. "BREADCRUMB .. BREADCRUMB" could be something like:
   OP_0 TOALT [at start of script]
   ..
   FROMALT x CAT SHA256 DUP TOALT DATACOMMIT      ..
   FROMALT y CAT SHA256 DUP TOALT DATACOMMIT   if the altstack was otherwise unused, I guess; so the accumulator
behaviour probably warrants something better.
It also more or less gives you CHECKSIGFROMSTACK behaviour by doing
"SWAP OP_DATACOMMIT OP_CHECKSIG" and a SIGHASH_NONE|ANYPREVOUTANYSCRIPT
But that seems like a plausible generalisation to think about?

@_date: 2019-02-10 14:46:30
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
The way I look at this is:
 * you can have a "channel factory" of 3 or more members (A,B,C,...)
 * it's protected by an n-of-n multisig output
 * it contains some combination of:
    - spends directly to members
    - lightning channels between pairs of members
    - channel factories between subgroups of members
 * when initially setup, the factory just has direct spends to each
   member matching the amount they contributed to the factory
 * whether you create a lightning channel or a sub-factory is the same
   decision as whether you create a lightning channel or a factory
   on-chain, so there's no combinatorial explosion.
You can close any channel factory by publishing it (and any higher level
channel factories it was a subgroup of) to the blockchain (at which point
the lower level channel factories and lightning channels remain open),
or you can update a channel factory off-chain by having everyone agree
to a new state -- which is only possible if everyone is online, of course.
Updates to transactions in a lightning channel in a factory, or updates
to a subfactory, don't generally involve updating the containing factory
at all, I think.
I don't think there's much use to having sub-factories -- maybe if you
have a subgroup that's much more active and wants to change channel
balances between each other more frequently than the least active member
of the main factory is online?
As far as NOINPUT goes; this impacts channel factories because cheating
could be by any member of the group, so you can't easily penalise the
cheater. So an eltoo-esque setup where you publish a commitment to the
state that's spendable only by any later state, and is then redeemed
after a timelock seems workable. In that case closing a factory when
you can't get all group members to cooperatively close looks like:
   funding tx: n-of-n multisig
   state commitment: n-of-n multisig
      spends funding tx or earlier state commitment
      spendable by later state commitment or settlement
   settlement: n-of-n multisig
      relative timelock
      spends state commitment
      spends to members, channels or sub-factories
The settlement tx has to spend with a NOINPUT sig, because the state
commitment could have had to spend different things. If it's a
sub-factory, the funding tx will have been in a factory, so the state
commitment would also have had to be a NOINPUT spend. So tagging
NOINPUT-spendable outputs would mean:
 - tagging state commitment outputs (which will be spent shortly with
   NOINPUT by the settlement tx, so no real loss here)
 - tagging settlement tx outputs if they're lightning channels or
   sub-factories (which is something of a privacy loss, I think, since
   they could continue off-chain for an indefinite period before being
   spent)
I think Johnson's suggested elsewhere that if you spend an input with a
NOINPUT signature, you should make all the outputs be tagged NOINPUT (as
a "best practice rule", rather than consensus-enforced or standardness).
That would avoid the privacy loss here, I think, but might be confusing.
If you wanted to close your factory and send your funds to an external
third-party (a cold-wallet, custodial wallet, or just paying someone
for something), you'd presumably do that via a cooperative close of the
factory, which doesn't require the state/settlement pair or NOINPUT
spends, so the NOINPUT-in means NOINPUT-tagged-outputs doesn't cause
a problem for that use case.
FWIW, I think an interesting way to improve this model might be to *add*
centralisation and trust; so that instead of having the factory have
an n-of-n multisig, have it be protected by k-of-n plus a trusted third
party. If you have the trusted third party check that the only balances
that change in the factory are from the "k" signers, that allows (n-k)
members to be offline at any time, but the remaining members to rebalance
their channels happily. (Theoretically you could do this trustlessly
with covenants, but the spending proofs on chain would be much larger)
Of course, this allows k-signers plus the trusted party to steal funds.
It might be possible for the trusted party to store audit logs of the
partial signatures from each of the k-signers for each transaction to
provide accountability -- where the lack of such logs implies the
trusted third party was cheating.

@_date: 2019-01-31 16:04:05
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Seems like PSBT would be a sufficient protocol:
 0) lightning node generates a PSBT for a new channel,
    with no inputs and a single output of the 2-of-2 address
 1) wallet funds the PSBT but doesn't sign it, adding a change address
    if necessary, and could combine with other tx's bustapay style
 2) lightning determines txid from PSBT, and creates update/settlement
    tx's for funding tx so funds can be recovered
 3) wallet signs and publishes the PSBT
 4) lightning sees tx on chain and channel is open
That's a bit more convoluted than "(0) lightning generates an address and
value, and creates NOINPUT update/settlement tx's for that address/value;
(1) wallet funds address to exactly that value; (2) lightning monitors
blockchain for payment to that address" of course.
But it avoids letting users get into the habit of passing NOINPUT
addresses around, or the risk of a user typo'ing the value and losing
money immediately, and it has the benefit that the wallet can tweak the
value if (eg) that avoids a change address or enhances privacy (iirc,
c-lightning tweaks payment values for that reason). If the channel's
closed cooperatively, it also avoids ever needing to publish a NOINPUT
sig (or NOINPUT tagged output).
Does that seem a fair trade off?

@_date: 2019-06-05 19:30:39
@_author: Anthony Towns 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
I think you could generalise that slightly and make it fit in
with the existing opcode naming by calling it something like
"OP_CHECKTXDIGESTVERIFY" and pull a 33-byte value from the stack,
consisting of a sha256 hash and a sighash-byte, and adding a new sighash
value corresponding to the set of info you want to include in the hash,
which I think sounds a bit like "SIGHASH_EXACTLY_ONE_INPUT | SIGHASH_ALL"
FWIW, I'm not really seeing any reason to complicate the spec to ensure
the digest is precommitted as part of the opcode.

@_date: 2019-06-21 08:05:52
@_author: Anthony Towns 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Also, I think you can simulate OP_SECURETHEBAG with an ANYPREVOUT
(NOINPUT) sighash (Johnson Lau's mentioned this before, but not sure if
it's been spelled out anywhere); ie instead of constructing
  X = Hash_BagHash( version, locktime, [outputs], [sequences], num_in )
and having the script be " OP_SECURETHEBAG" you calculate an
ANYPREVOUT sighash for SIGHASH_ANYPREVOUTANYSCRIPT | SIGHASH_ALL:
  Y = Hash_TapSighash( 0, 0xc1, version, locktime, [outputs], 0,
                       amount, sequence)
and calculate a signature sig = Schnorr(P,m) for some pubkey P, and
make your script be "  CHECKSIG".
That loses the ability to commit to the number of inputs or restrict
the nsequence of other inputs, and requires a bigger script (sig and P
are ~96 bytes instead of X's 32 bytes), but is otherwise pretty much the
same as far as I can tell. Both scripts are automatically satisfied when
revealed (with the correct set of outputs), and don't need any additional
witness data.
If you wanted to construct "X" via script instead of hardcoding a value
because it got you generalised covenants or whatever; I think you could
get the same effect with CAT,LEFT, and RIGHT: you'd construct Y in much
the same way you construct X, but you'd then need to turn that into a
signature. You could do so by using pubkey P=G and nonce R=G, which
means you need to calculate s=1+hash(G,G,Y)*1 -- calculating the hash
part is easy, multiplying it by 1 is easy, and to add 1 you can probably
do something along the lines of:
    OP_DUP 4 OP_RIGHT 1 OP_ADD OP_SWAP 28 OP_LEFT OP_SWAP OP_CAT
(ie, take the last 4 bytes, increment it using 4-byte arithmetic,
then cat the first 28 bytes and the result. There's overflow issues,
but I think they can be worked around either by allowing you to choose
different locktimes, or by more complicated script)

@_date: 2019-06-28 19:49:37
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Taproot proposal 
FWIW, I think this would be essentially free, at least for the current
sighash modes, as (I think) all the non-ANYONECANPAY modes have at least
4 bytes of sha256 padding at present.
In addition to (or, perhaps, as a special case of) the reasons Russell
gives, I think this change would also better support proof-of-reserves
via taproot signatures (cf [0] or BIP 127), as it would allow the proof
tx to include more than 65k utxos with each utxo being signed with a
signature that commits to all inputs including the invalid placeholder.
[0] If you didn't have this, but wanted to do proof-of-reserves over >65k
taproot UTXOs, you could use ANYONECANPAY signatures, and use the output
amounts to ensure the signatures can't be abused, something like:
   inputs:
     0: spend from txid 0000..0000 vout 0, no witness data
     1: utxo1, signed with ANYONECANPAY|ALL
     2: utxo2, signed with ANYONECANPAY|ALL
     3: utxo3, signed with ANYONECANPAY|ALL
     [etc]
   outputs:
     0: sum(utxo1..utxoN), pay to self
     1: 2099999997690001-sum(utxo1..utxo3), payable to whatever
The total output value is therefore one satoshi more bitcoin than there
could ever have been, so none of the utxoK signatures can be reused on the
blockchain (unless there's severe inflation due to bugs or hardforks),
but the values (and sums) all remain less than 21M BTC so it also won't
fail the current "amount too big" sanity checks.
That seems a bit more fragile/complicated than using SIGHASH_ALL for
everything, though it means your cold wallet doesn't have to serialize
your >65k transactions to verify it's signing what it thinks it is.
Hmm? If I'm following what you mean, that's not the P2P rules, it's the
Unserialize code, in particular:
  compat/assumptions.h:52:static_assert(sizeof(int) == 4, "32-bit int assumed");
  serialize.h:289:uint64_t ReadCompactSize(Stream& is)
  serialize.h-679-template
  serialize.h-680-void Unserialize_impl(Stream& is, prevector& v, const V&)
  serialize.h-681-{
  serialize.h-682-    v.clear();
  serialize.h:683:    unsigned int nSize = ReadCompactSize(is);
  (and other Unserialize_impl implementations)
However, ReadCompactSize throws "size too large" if the return value is
greater than MAX_SIZE == 0x02000000 =~ 33.5M, which prior to the implicit
cast to 32 bits in Unserialize_impl. And it looks like that check's been
there since Satoshi...
So as far as I can see, that encoding's just unsupported/invalid, rather
than equivalent/non-canonical?

@_date: 2019-03-13 11:41:43
@_author: Anthony Towns 
@_subject: [bitcoin-dev] More thoughts on NOINPUT safety 
Hi all,
The following has some more thoughts on trying to make a NOINPUT
implementation as safe as possible for the Bitcoin ecosystem.
One interesting property of NOINPUT usage like in eltoo is that it
actually reintroduces the possibility of third-party malleability to
transactions -- ie, you publish transactions to the blockchain (tx A,
which is spent by tx B, which is spent by tx C), and someone can come
along and change A or B so that C is no longer valid). The way this works
is due to eltoo's use of NOINPUT to "skip intermediate states". If you
publish to the blockchain:
      funding tx -> state 3 -> state 4[NOINPUT] -> state 5[NOINPUT] -> finish
then in the event of a reorg, state 4 could be dropped, state 5's
inputs adjusted to refer to state 3 instead (the sig remains valid
due to NOINPUT, so this can be done by anyone not just holders of some
private key), and finish would no longer be a valid tx (because the new
"state 5" tx has different inputs so a different txid, and finish uses
SIGHASH_ALL for the signature so committed to state 5's original txid).
There is a safety measure here though: if the "finish" transaction is
itself a NOINPUT tx, and has a a CSV delay (this is the case in eltoo;
the CSV delay is there to give time for a hypothetical state 6 to be
published), then the only way to have a problem is for some SIGHASH_ALL tx
that spends finish, and a reorg deeper than the CSV delay (so that state
4 can be dropped, state 5 and finish can be altered). Since the CSV delay
is chosen by the participants, the above is still a possible scenario
in eltoo, though, and it means there's some risk for someone accepting
bitcoins that result from a non-cooperative close of an eltoo channel.
Beyond that, I think NOINPUT has two fundamental ways to cause problems
for the people doing NOINPUT sigs:
 1) your signature gets applied to a unexpectedly different
    script, perhaps making it look like you've being dealing
    with some blacklisted entity. OP_MASK and similar solves
    this.
 2) your signature is applied to some transaction and works
    perfectly; but then someone else sends money to the same address
    and reuses your prior signature to forward it on to the same
    destination, without your consent
I still like OP_MASK as a solution to (1), but I can't convince myself that
the problem it solves is particularly realistic; it doesn't apply to
address blacklists, because for OP_MASK to make the signature invalid
the address has to be different, and you could just short circuit the
whole thing by sending money from a blacklisted address to the target's
personal address directly. Further, if the sig's been seen on chain
before, that's probably good evidence that someone's messing with you;
and if it hasn't been seen on chain before, how is anyone going to tell
it's your sig to blame you for it?
I still wonder if there isn't a real problem hiding somewhere here,
but if so, I'm not seeing it.
For the second case, that seems a little more concerning. The nightmare
scenario is maybe something like:
 * naive users do silly things with NOINPUT signatures, and end up
   losing funds due to replays like the above
 * initial source of funds was some major exchange, who decide it's
   cheaper to refund the lost funds than deal with the customer complaints
 * the lost funds end up costing enough that major exchanges just outright
   ban sending funds to any address capable of NOINPUT, which also bans
   all taproot/schnorr addresses
That's not super likely to happen by chance: NOINPUT sigs will commit
to the value being spent, so to lose money, you (Alice) have to have
done a NOINPUT sig spending a coin sent to your address X, to someone
(Bob) and then have to have a coin with the exact same value sent from
someone else again (Carol) to your address X (or if you did a script
path NOINPUT spend, to some related address Y with a script that uses the same
key). But because it involves losing money to others, bad actors might
trick people into having it happen more often than chance (or well
written software) would normally allow.
That "nightmare" could be stopped at either the first step or the
last step:
 * if we "tag" addresses that can be spent via NOINPUT then having an
   exchange ban those addresses doesn't also impact regular
   taproot/schnorr addresses, though it does mean you can tell when
   someone is using a protocol like eltoo that might need to make use
   of NOINPUT signatures.  This way exchanges and wallets could simply
   not provide NOINPUT capable addresses in the first place normally,
   and issue very large warnings when asked to send money to one. That's
   not a problem for eltoo, because all the NOINPUT-capable address eltoo
   needs are internal parts of the protocol, and are spent automatically.
 * or we could make it so NOINPUT signatures aren't replayable on
   different transactions, at least by third parties. one way of doing
   this might be to require NOINPUT signatures always be accompanied by a
   non-NOINPUT signature (presumably for a different public key, or there
   would be no point). This would prevent NOINPUT key-path spends, you'd
   always have to use the taproot script-path for a NOINPUT signature so
   that you could specify both public keys, and would also increase the
   witness size due to needing two signatures and specifying an additional
   public key -- this would increase the cost in fees by about 25% compared
   to a plain key-path spend.
Conversely, this "nightmare" scenario *can't* be stopped if we allow
key-path spending of (untagged) taproot addresses with NOINPUT signatures:
exchanges could not distinguish such addresses from regular addresses, and
the only way to prevent the signature from applying to two tx's with the
same value and address would be for the sig to commit to info from the tx.
It seems like there's one big choice then:
 - just ignore this concern
 - drop NOINPUT from normal taproot key path spending
If we drop NOINPUT from taproot key path spending, then we can do NOINPUT
as a logically separate upgrade to taproot, rather than it needing to
be done at the same time.  There's two ways we could do proceed:
 - introduce a new NOINPUT-capable scriptPubKey format (ie, "tag"
   NOINPUT spendable addresses); either a different length segwit v1
   output, or a different segwit version entirely. Using version "16" in
   this scenario might be appealing: we could reserve all v16 addresses
   for "not intended to be used by humans directly" and update BIP 173
   to say these aren't even something you should use bech32 to represent.
 - alternatively, we could require every script to have a valid signature
   that commits to the input. In that case, you could do eltoo with a
   script like either:
         CHECKSIGVERIFY  CHECKSIG
     or  CHECKSIGVERIFY  CHECKSIG
   where A is Alice's key and B is Bob's key, P is muSig(A,B) and Q is
   a key they both know the private key for. In the first case, Alice
   would give Bob a NOINPUT sig for the tx, and when Bob wanted to publish
   Bob would just do a SIGHASH_ALL sig with his own key. In the second,
   Alice and Bob would share partial NOINPUT sigs of the tx with P, and
   finish that when they wanted to publish.
   This is a bit more costly than a key path spend: you have to reveal
   the taproot point to do a script (+33B) and you have two signatures
   instead of one (+65B) and you have to reveal two keys as well
   (+66B), plus some script overhead. If we did the  variant,
   we could provide a "PUSH_TAPROOT_KEY" opcode that would just push
   the taproot key to stack, saving 33B from pushing P as a literal,
   but you can't do much better than that. All in all, it'd be about 25%
   overhead in order to prevent cheating. [0]
I think that output tagging doesn't provide a workable defense against the
third party malleability via a deeper-than-the-CSV-delay reorg mentioned
earlier; but requiring a non-NOINPUT sig does: you'd have to replace
the non-NOINPUT sig to make state 5 spend state 3 instead of state 4,
and only the holders of the appropriate private key can do that.
In any event, if we get some experience with NOINPUT in practice, we can
reconsider whether NOINPUT key path spends are a good idea when we do
the next segwit version -- both cross-input signature aggregation and
graftroot will need an upgrade anyway.
(Also, note that, at least for eltoo, all of the above only applies to
non-cooperative closes: the funding tx's txid is known from the start,
so you can always arrange to spend it via SIGHASH_ALL, so it doesn't
need to be tagged, and a cooperative/mutual close of the channel will
still just be a simple key path spend)
Anyway, presented for your consideration.
FWIW, I don't have a strong opinion here yet, but:
 - I'm still inclined to err on the side of putting more safety
   measures in for NOINPUT, rather than fewer
 - the "must have a sig that commits to the input tx" seems like it
   should be pretty safe, not too expensive, and keeps taproot's privacy
   benefits in the cases where you end up needing to use NOINPUT

@_date: 2019-03-13 13:23:46
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Signet 
Maybe make the signature be an optional addition to the header, so
that you can have a "light node" that doesn't download/verify sigs
and a full node that does? (So signatures just sign the traditional
80-byte header, and aren't included in the block's tx merkle root, and
the prevHash reflects the hash of the previous block's 80-byte header,
without the signature)
I think you could do that by adding a p2p service bit to say "send me
signatures if you have them / I can send you signatures", which changes
the p2p encoding of the header from (ver, prev, mrkl, time, bits, nonce)
to (ver, prev, mrkl, time, 0, nonce, bits, sig), and change header
processing to ignore headers from nodes that don't have the service
bit set?
If you did this, it might be a good idea to enforce including the previous
block's header signature in the current block's coinbase.

@_date: 2019-03-13 21:10:50
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
With taproot, you could always do the 2-of-2 spend without revealing a
script at all, let alone that it was meant to be NOINPUT capable. The
setup I'm thinking of in this scenario is something like:
  0) my key is A, your key is B, we want to setup an eltoo channel
  1) post a funding tx to the blockchain, spending money to an address
     P = muSig(A,B)
  2) we cycle through a bunch of states from 0..N, with "0" being the
     refund state we establish before publishing the funding tx to
     the blockchain. each state essentially has two corresponding tx's,
     and update tx and a settlement tx.
  3) the update tx for state k spends to an output Qk which is a
     taproot address Qk = P + H(P,Sk)*G where Sk is the eltoo ratchet
     condition:
        Sk = (5e8+k+1) CLTV A CHECKDLS_NOINPUT B CHECKDLS_NOINPUT_VERIFY
     we establish two partial signatures for update state k, one which
     is a partial signature spending the funding tx with key P and
     SIGHASH_ALL, the other is a NOINPUT signature via A (for you) and
     via B (for me) with locktime set to (k+5e8), so that we can spend
     any earlier state's update tx's, but not itself or any later
     state's update tx's.
  4) for each state we have also have a settlement transaction,
     Sk, which spends update tx k, to outputs corresponding to the state
     of the channel, after a relative timelock delay.
     we have two partial signatures for this transaction too, one with
     SIGHASH_ALL assuming that we directly spent the funding tx with
     update state k (so the input txid is known), via the key path with
     key Qk; the other SIGHASH_NOINPUT via the Sk path. both partially
     signed tx's have nSequence set to the required relative timelock
     delay.
  5) if you're using scriptless scripts to do HTLCs, you'll need to
     allow for NOINPUT sigs when claiming funds as well (and update
     the partial signatures for the non-NOINPUT cases if you want to
     maximise privacy), which is a bit fiddly
  6) when closing the channel the process is then:
       - if you're in contact with the other party, negotiate a new
         key path spend of the funding tx, publish it, and you're done.
       - otherwise, if the funding tx hasn't been spent, post the latest
         update tx you know about, using the "spend the funding tx via
       - otherwise, trace the children of the funding tx, so you can see
         the most recent published state:
       - once the CSV delay for the latest update tx has expired, post
       - once the settlement tx is posted, claim your funds
So the cases look like:
   mutual close:
     funding tx -> claimed funds
     -- only see one key via muSig, single signature, SIGHASH_ALL
     -- if there are active HTLCs when closing the channel, and they
        timeout, then the claiming tx will likely be one-in, one-out,
   unilateral close, no cheating:      funding tx -> update N -> settlement N -> claimed funds
     -- update N is probably SINGLE|ANYONECANPAY, so chain analysis
        of accompanying inputs might reveal who closed the channel
     -- settlement N has relative timelock
     -- claimed funds may have timelocks if they claim active HTLCs via
        the refund path
     -- no NOINPUT signatures needed, and all signatures use the key path
        so don't reveal any scripts
   unilateral close, attempted cheating:
     funding tx -> update K -> update N -> settlement N -> claimed funds
     -- update K, update N are probably SINGLE|ANYONECANPAY, so chain
        analysis might reveal the identity of both sides of the channel      -- update N and settlement N both use NOINPUT signatures and
        reveal CLTV script that looks like eltoo
     -- update N has timelock set
     -- settlement N has a relative timelock
     -- claimed funds may have timelocks if they claim active HTLCs via
        the refund path
     Notes:
      * cheating isn't 100% accurate: could be due to someone having to
        restore from an old backup
      * you could end up with:
          funding tx -> update K -> update W -> update N
With the above setup, you don't discover that NOINPUT was possible unless it
is actually needed because someone cheated.
As long as you're using muSig key path spending for a cooperative close,
you're not even revealing the output is 2-of-2, let alone a weird
2-of-2 variant.
With taproot, the goal is it shouldn't look different from an ordinary
"pay to public key" spend, and I think that's pretty achievable.
Well, presumaby lightning will continue to support private channels that
don't get published, and the concern's definitely valid for them!
I think this is possible too, but I think the scheme I describe above
is superior: iit means calculating a few more signatures each update,
but keeps more information off chain, which is better for privacy, and
probably cheaper (unless you have very high-frequency channel updates?).

@_date: 2019-03-14 17:24:56
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
I'm thinking of tagged outputs as "taproot plus" (ie, plus noinput),
so if you used a tagged output, you could do everything normal taproot
address could, but also do noinput sigs for them.
So you might have:
   funding tx -> cooperative claim
   funding tx -> update 3 [TAGGED] -> settlement 3 -> claim
   funding tx -> update 3 [TAGGED] ->                  update 4 [TAGGED,NOINPUT] -> In the cooperative case, no output tagging needed.
For the unilateral case, you need to tag all the update tx's, because
they *could* be spend by a later update with a NOINPUT sig, and if
that actually happens, then the settlement tx also needs to use a
NOINPUT sig, and if you're using scriptless scripts to resolve HTLCs,
claiming/refunding the HTLCs needs a partially-pre-signed tx which also
needs to be a NOINPUT sig, meaning the settlement tx also needs to be
tagged in that case.
You'd only need the script path for the last case where there actually
are multiple updates, but because you have to have a tagged output in the
second case anyway, maybe you've already lost privacy and always using
NOINPUT and the script path for update and settlement tx's would be fine.
Yeah, that's my thinking.
Yeah, exactly.
Trying to maximise privacy there has the disadvantage that you have to
do a new signature for every in-flight HTLC every time you update the
state, which could be a lot of signatures for very active channels.

@_date: 2019-03-21 19:06:14
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
I meant "either of the two scripts is okay".
I think so. From Alice/Bob's point-of-view, the NOINPUT sig ensures they
control their money; and from the network's point-of-view (or at least
that part of the network that thinks NOINPUT is unsafe) the Q private
key being shared makes the tx no worse than a 1-of-n multisig setup,
which has to be dealt with anyway.
I think we could potentially make that shorter still:
   IF OP_CODESEPARATOR  OP_CHECKLOCKTIMEVERIFY OP_DROP ENDIF
    OP_CHECKDLSVERIFY  OP_CHECKDLS
Signing with NOINPUT,NOSCRIPT and codeseparatorpos=1 enforces CLTV
and allows binding to any prior update tx -- so works for an update tx
spending previous update txs; while signing with codeseparatorpos=-1
and NOINPUT but committing to the script code and nSequence (for the
CSV delay) allows binding to only that update tx -- so works for the
settlement tx. That's two pubkeys, two sigs, and the taproot point

@_date: 2019-03-21 21:55:22
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Yes, if you're not committing to the script code you need the separate
keys as otherwise any settlement transaction could be used with any
update transaction. If you are committing to the script code, though, then each settlement
sig is already only usable with the corresponding update tx, so you
don't need to roll the keys. But you do need to make it so that the
update sig requires the CLTV; one way to do that is using codeseparator
to distinguish between the two cases.
If codeseparator is too scary, you could probably also just always
require the locktime (ie for settlmenet txs as well as update txs), ie:
  OP_CHECKLOCKTIMEVERIFY OP_DROP
   OP_CHECKDLSVERIFY  OP_CHECKDLS
and have update txs set their timelock; and settlement txs set a absolute
timelock, relative timelock via sequence, and commit to the script code.
(Note that both those approaches (with and without codesep) assume there's
some flag that allows you to commit to the scriptcode even though you're
not committing to your input tx (and possibly not committing to the
scriptpubkey). BIP118 doesn't have that flexibility, so the A_s_i and
B_s_i key rolling is necessary)

@_date: 2019-03-22 12:58:46
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
You can enforce the relative timelock in the settlement branch simply
by refusing to sign a settlement tx that doesn't have the timelock set;
the OP_CSV is redundant.
settlement-1 was signed by you, and when you signed it you ensured that
nsequence was set as per BIP-68, and NOINPUT sigs commit to nsequence,
so if anyone changed that after the fact the sig isn't valid. Because
BIP-68 is enforced by consensus, update-1 isn't immediately spendable
by settlement-1.

@_date: 2019-05-08 14:49:28
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Taproot proposal 
Aside: if you want to commit to something extra *with* the witness
program, you could use either an unsolvable tapleaf branch (eg, one
that uses OP_RETURN and also pushes the data you want to commit to),
or a p2c construction like:
  Taproot: Q = P + H_TapTweak(P || S)*G
  P2C: P = R + H_"myp2c"(R || Contract)*G
If you don't have any scripts for S, you could set S=["OP_RETURN"]
to ensure there are no scripts. Having either the TapTweak formula or
the H_myp2c hash should be enough to ensure that your contract can't
get maliciously reinterpreted as a valid tapscript, having both is just
belts and suspenders.
But to address your question: if you want to commit to something extra
at spending/signing time rather than when creating the address, then
that's what the annex should be useful for. eg as a simple example,
your annex might be:
    0x50 [flag]
    0x25 [entry size]
      0x77 [annex entry id]
      0x0008c618 [block height == 575000]
      0x00000000000000000007df59824a0c86d1cc21b90eb25259dd2dba5170cea5f5
         [block hash for block at 575000]
which would allow you to commit to a particular block hash, and there
could be a soft fork that added the restriction that such a commitment
invalidates the transaction if the block at the given height doesn't
match the provided hash.
You still need to the soft-fork to do the enforcing, but once you have
that, *every* existing taproot address automatically gets "upgraded"
to allow you to make the commitment, including via key path spends,
which seems pretty nice.
(That construction (ie size,id,data) lets you have multiple entries in
the annex reasonably efficiently)

@_date: 2019-05-11 06:38:04
@_author: Anthony Towns 
@_subject: [bitcoin-dev] SIGHASH_ANYPREVOUT proposal 
Hi everybody!
Here is a followup BIP draft that enables SIGHASH_ANYPREVOUT and
SIGHASH_ANYPREVOUTANYSCRIPT on top of taproot/tapscript. (This is NOINPUT,
despite the name change)
I don't think we are (or should be) as confident that ANYPREVOUT is
ready for implementation and deployment as we are that taproot is.
In particular, we were still coming up with surprising ways that these
style of signatures could maybe cause problems over the past few months,
despite "NOINPUT" having been around for years, and having been thinking
seriously about it for most of the last year. In comparison we've had
a roughed out security proof for taproot [0] for over a year now.
So far, the best approach (in my opinion) that we've come up with to
limit the possible negative impacts of these types of signatures is to
require an additional regular signature to accompany every ANYPREVOUT
signature. As such, it's included in the BIP draft.
In theory this ensures that no ANYPREVOUT tx can cause any more problems
than some existing tx could; but in practice this assumes that the private
key for that signature is maintained in a similar way to the private keys
currently securing transactions are. After passing this around privately,
I'm not convinced the theory will survive meeting adversarial reality,
in which case I don't think this draft will be suitable for adoption.
But maybe I'm too pessimistic, or maybe we can come up with either
a proof that ANYPREVOUT is already safe without any other measures,
or maybe we can come up with some better measures to ensure it's safe.
So in any case I'm still hopeful that publishing the best we've got is
helpful, even if that still isn't good enough.
The BIP draft can be found here:
 A sample implementation based on the taproot branch is here:
 Some interesting features:
 * This demonstrates how to upgrade tapscript's existing CHECKSIG,
   CHECKSIGADD and CHECKSIGVERIFY opcodes for new SIGHASH methods or
   potentially a new signature scheme, a new elliptic curve or other
   public key scheme
 * This demonstrates a cheap way of using the taproot internal key
   as the public key for CHECKSIG operations in script
 * There are two variants, ANYPREVOUT and ANYPREVOUTANYSCRIPT, which
   seems helpful for eltoo
 * The BIP attempts to describe the security implications of ANYPREVOUT-style
   signatures
[0]

@_date: 2019-05-23 06:11:31
@_author: Anthony Towns 
@_subject: [bitcoin-dev] SIGHASH_ANYPREVOUT proposal 
Yeah, that's where I'm at: if we think something is UNSAFE enough to
need a warning, then I think it's too unsafe to include in the consensus
layer. I would like to find a way of making ANYPREVOUT safe enough that
it doesn't need scary warnings; a week or two ago, chaperone sigs were
my best idea for that.
Eltoo (and perhaps lightning more generally) seem like the most obvious
use case for ANYPREVOUT, so if it isn't going to opt-in (or is going
to opt-out in any way it can, as you suggest) then they're not a good
I'm not going to argue about any of that here, though I do reserve the
right to do so later. :)
So here's something I almost think is an argument that ANYPREVOUT is safe
(without chaperone sigs or output tagging, etc).
 I'm assuming funds are "safe" in Bitcoin if they're (a) held in
a cryptographically secured UTXO, (b) under "enough" proof of work
that a reorg is "sufficiently" unlikely. If you don't have both those
assumptions, your money is not safe in obvious ways; if you do have them
both, your money is safe.
 My theory is that as long as you, personally, only use ANYPREVOUT
to sign transactions that are paying the money back to yourself, your
funds will remain safe.
If you follow this rule, then other people replaying your signature is
not a problem -- the funds will just move from one of your addresses, to
a different address.
If other people *fail* to follow this rule, you might receive funds
directly authorised by an ANYPREVOUT signature. But those funds are only
secure if they're "sufficiently" buried in confirmations anyway, and
once they are, they won't disappear. You might be able to reuse that
signature against some different UTXO, but that's only to your benefit:
they lose funds after violating the rule, but you gain funds.
Eltoo naturally meets this requirement, as long as you consider "paying
to yourself" to cover both "paying to same multisig address" (for update
transactions) and "splitting funds between members of a group who owned
the funds". If you consider the "split" to be "you get 50% of our funds,
you get 20%, you get 30%", even if the signature gets replayed later
against a different utxo, the percentage split remains true it just
unexpectedly applies to more funds.
 Making ANYPREVOUT only available via script is aligned with this;
if you'repaying to yourself you probably want complicated rules that
you have to encode in script, and there's a mild economic incentive to
try to avoid that because the key path is cheaper.
 I think this covers the major security property for bitcoin (your
funds are yours to decide what to do with), but maybe there are other
ways in which ANYPREVOUT is scary that could be formalised and addressed?
 It's probably not compatible with luke's "malleability proof" wallet
idea [0]. Malleability is only a concern for funds that aren't already
"sufficiently" buried, and if you're only spending it to yourself that
doesn't help with burying, and if you're spending it to someone else
immediately after, doesn't help with making that transaction less
malleable. But if the line of argument above's correct, that just
recognises that a wallet like that risks losing funds if someone else
reuses its addresses; it doesn't add any systemic risk. And "wallet X
isn't safe" is a risk category we already deal with.
[0]

@_date: 2019-05-23 06:49:11
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
I think (especially with cross-input sig aggregation) it makes it easier
to do a coinjoin during a high fee period -- if you're willing to wait
'til fees are lower to claim your funds you can still do that, despite
participating now.
Otherwise, I don't think it makes coordination that much easier. If the coinjoin groups stays around in a Layer 2-ish protocol, and
coordinates to cut-through transactions, that could be a scaling and
privacy benefit, but comes with much harder coordination problems. ie:
   A,B,C,D do a coinjoin with outputs of 1 BTC each
   tx on chain looks like:
     in: 1 A
         1 B
         1 C
         1 D
     out: 4 to muSig(A,B,C,D) or COHV(1 A, 1 B, 1 C, 1 D)
but then A wants to spend 0.2 BTC to E, and B wants to spend 0.1 BTC to
F, so they agree to update the state and publish:
     in: (above, signed by A+B+C+D)
     out:          0.1 F
and they continue the protocol.
(I don't think this in any way replaces ANYPREVOUT or similar)
I think lightning is improved by this in that it makes it cheaper to
create lightning channels during a high fee period. If you're creating
1000 channels you can do that via a single output with this opcode, and
then wait until either there's a low fee period to publish the funding
tx cheaply; or until the channel fails and you need to extract the funds
which always has the risk of happening during a high fee period.
You might be able to slightly simplify eltoo (or conceivably some parts of
current lightning); if your eltoo update tx has as it's output [musig(A,B)
or (n+1 cltv checksig) or (d CSV COHV(balances))] then your settlement
transaction only needs to reveal the 40B script, rather than needing a
65B ANYPREVOUT signature.

@_date: 2019-05-27 17:21:28
@_author: Anthony Towns 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Counterpoint: haven't the flexibly designed parts of script mostly been
a failure -- requiring opcodes to be disabled due to DoS vectors or
consensus bugs, and mostly not being useful in practice where they're
still enabled in BTC or on other chains where they have been re-enabled
(eg, Liquid and BCH)?
FWIW, I'd like to see CAT enabled, though I'm less convinced about a
CHECKSIG that takes the message from the stack. I think CAT's plausibly
useful in practice, but a sig against data from the stack seems more
useful in theory than in practice. Has it actually seen use on BCH or
Liquid, eg?  (Also, I think BCH's name for that opcode makes more sense
than Elements' -- all the CHECKSIG opcodes pull a sig from the stack,
after all)
I think simulating an ANYPREVOUT sig with a data signature means checking:
    S1 P CHECKSIG -- to check S1 is a signature for the tx
    S1 H_TapSighash(XAB) P CHECKDATASIG
         -- to pull out the tx data "X", "A", "B")
    S2 H_TapSighash(XCB) Q CHECKDATASIG
         -- for the ANYPREVOUT sig, with A changed to C to
    X SIZE 42 EQUALVERIFY
    B SIZE 47 EQUALVERIFY
         -- to make sure only C is replaced from "XCB"
So to get all those conditions checked, I think you could do:
   P 2DUP TOALT TOALT CHECKSIGVERIFY
   SIZE 42 EQUALVERIFY
   "TapSighash" SHA256 DUP CAT SWAP CAT TOALT
   SIZE 47 EQUALVERIFY TUCK
   CAT FROMALT TUCK SWAP CAT SHA256 FROMALT SWAP FROMALT
   CHECKDATASIGVERIFY
   SWAP TOALT SWAP CAT FROMALT CAT SHA256 Q CHECKDATASIG
Where the stack elements are, from top to bottom:
   S1: (65B) signature by P of tx
   X:  (42B) start of TapSighash spec
   B:  (47B) end of TapSighash spec (amount, nSequence, tapleaf_hash,
             key_version, codesep_pos)
   A:  (73B) middle of TapSighash spec dropped for ANYPREVOUT (spend_type,
             scriptPubKey and outpoint)
   C:   (1B) alternate middle (different spend_type)
   S2: (64B) signature of "XCB" by key Q
So 298B for the witness data, and 119B or so for the script (if I've not
made mistakes), versus "P CHECKSIGVERIFY Q CHECKSIG" and S2 and S1 on
the stack, for 132B of witness data and 70B of script, or half that if
the chaperone requirement is removed.
I think you'd need to complicate it a bit further to do the
ANYPREVOUTANYSCRIPT variant, where you retain the commitment to
amount/nseq but drop the commitment to tapleaf_hash.
For practical purposes, this doesn't seem like a great level of
abstraction to me. It's certainly better at "permissionless innovation"
You could make these constructions a little bit simpler by having a
"CHECK_SIG_MSG_VERIFY" opcode that accepts [sig msg key], and does "sig
key CHECKSIGVERIFY" but also checks the the provided msg was what was
passed into bip-schnorr.

@_date: 2019-11-28 18:06:59
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
FWIW, there's discussion of this at
(You already know this, but:)
With taproot key path spending, the only other conditions that can be
placed on a transaction are nSequence, nLockTime, and the annex, all of
which are committed to via the signature; so I think this concern only
applies to taproot script path spending.
The proposed sighashes for taproot script path spending all commit to
the script being used, so you can't reuse the signature in a different
leaf of the merkle tree of scripts for the UTXO, only in a separate
execution path within the script you're already looking at.
First, it seems like a bad idea for Alice to have put funds behind a
script she doesn't understand in the first place. There's plenty of
scripts that are analysable, so just not using ones that are too hard to
analyse sure seems like an option.
Second, if there are many branches in the script, it's probably more
efficient to do them via different branches in the merkle tree, which
at least for this purpose would make them easier to analyse as well
(since you can analyse them independently).
Third, if you are doing something crazy complex where a particular key
could appear in different CHECKSIG operators and they should have
independent signatures, that seems like you're at the level of
complexity where learning about CODESEPARATOR is a reasonable thing to
I think CODESEPARATOR is a better solution to this problem anyway. In
particular, consider a "leaf path root OP_MERKLEPATHVERIFY" opcode,
and a script that says "anyone in group A can spend if the preimage for
X is revelaed, anyone in group B can spend unconditionally":
 IF HASH160 x EQUALVERIFY groupa ELSE groupb ENDIF
 MERKLEPATHVERIFY CHECKSIG
spendable by
 siga keya path preimagex 1
 sigb keyb path 0
With your proposed semantics, if my pubkey is in both groups, my signature
will sign for position 10, and still be valid on either path, even if
the signature commits to the CHECKSIG position.
I could fix my script either by having two CHECKSIG opcodes (one for
each branch) and also duplicating the MERKLEPATHVERIFY; or I could
add a CODESEPARATOR in either IF branch.
(Or I could just not reuse the exact same pubkey across groups; or I could
have two separate scripts: "HASH160 x EQUALVERIFY groupa MERKLEPATHVERIFY
CHECKSIG" and "groupb MERKLEPATHVERIFY CHECKSIG")
As it stands, ANYPREVOUTANYSCRIPT proposes to not sign the script code
(allowing the signature to be reused in different scripts) but does
continue signing the CODESEPARATOR position, allowing you to optionally
restrict how flexibly you can reuse signatures. That seems like a better
tradeoff than having ANYPREVOUTANYSCRIPT signatures commit to the CHECKSIG
position which would make it a fair bit harder to design scripts that
can share signatures, or not having any way to restrict which scripts
the signature could apply to other than changing the pubkey.
A hypothetical alternate "codeseparator" design: when script execution
starts, initialise an empty byte string "trace"; each time an opcode
is executed append "0xFF"; each time an opcode is skipped append
"0x00". When a CODESEPARATOR is seen, calculate sha256(trace) and store
it, everytime a CHECKSIG is executed, include the sha256(trace) from the
last CODESEPARATOR in the digest [0]. That should make each checksig
commit to the exact path the script took up to the last CODESEPARATOR
seen. I think it's probably more complex than is really useful though,
so I'm not proposing it seriously.
[0] If there's not been a CODESEPARATOR, then sha256(trace)=sha256("");
    if there's been one CODESEPARATOR and it was the first opcode seen,
    sha256(trace)=sha256("\xff").

@_date: 2019-10-02 00:45:48
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
I don't think there's any meaningful difference between making a new
opcode and making a new tapscript public key type; the difference is
just one of encoding:
   3301AC   [CHECKSIG of public key type 0x01]
   32B3     [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]
(How sighash flags are treated can be redefined by new public key types;
if that's not obvious already)

@_date: 2019-10-02 01:59:29
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
Hey Christian, thanks for the write up!
I think there's an important open question you missed from this list:
(1.5) do we really understand what the dangers of noinput/anyprevout-style
constructions actually are?
My impression on the first 3.5 q's is: (1) yes, (1.5) not really,
(2) weak opposition for requiring chaperone sigs, (3) mixed (weak)
support/opposition for output tagging.
My thinking at the moment (subject to change!) is:
 * anyprevout signatures make the address you're signing for less safe,
   which may cause you to lose funds when additional coins are sent to
   the same address; this can be avoided if handled with care (or if you
   don't care about losing funds in the event of address reuse)
 * being able to guarantee that an address can never be signed for with
   an anyprevout signature is therefore valuable; so having it be opt-in
   at the tapscript level, rather than a sighash flag available for
   key-path spends is valuable (I call this "opt-in", but it's hidden
   until use via taproot rather than "explicit" as output tagging
   would be)
 * receiving funds spent via an anyprevout signature does not involve any
   qualitatively new double-spending/malleability risks.
   (eltoo is unavoidably malleable if there are multiple update
   transactions (and chaperone signatures aren't used or are used with
   well known keys), but while it is better to avoid this where possible,
   it's something that's already easily dealt with simply by waiting
   for confirmations, and whether a transaction is malleable is always
   under the control of the sender not the receiver)
 * as such, output tagging is also unnecessary, and there is also no
   need for users to mark anyprevout spends as "tainted" in order to
   wait for more confirmations than normal before considering those funds
   "safe"
I think it might be good to have a public testnet (based on Richard Myers
et al's signet2 work?) where we have some fake exchanges/merchants/etc
and scheduled reorgs, and demo every weird noinput/anyprevout case anyone
can think of, and just work out if we need any extra code/tagging/whatever
to keep those fake exchanges/merchants from losing money (and write up
the weird cases we've found in a wiki or a paper so people can easily
tell if we missed something obvious).

@_date: 2019-10-03 11:47:58
@_author: Anthony Towns 
@_subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about 
I don't think you could reasonably do this for key path spends -- if
you included the sighash as part of the scriptpubkey explicitly, that
would lose some of the indistinguishability of taproot addresses, and be
more expensive than having the sighash be in witness data. So I think
that means sighashes would still be included in key path signatures,
which would make the behaviour a little confusingly different between
signing for key path and script path spends.
I don't think the problems with NONE and SINGLE are any worse than using
SIGHASH_ALL to pay to "1*G" -- someone may steal the money you send,
but that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if
you use it, someone may steal funds from other UTXOs too -- similar
to nonce-reuse. So I think having to commit to enabling NOINPUT for an
address may make sense; but I don't really see the need for doing the
same for other sighashes generally.
FWIW, one way of looking at a transaction spending UTXO "U" to address
"A" is something like:
 * "script" lets you enforce conditions on the transaction when you
   create "A" [0]
 * "sighash" lets you enforce conditions on the transaction when
   you sign the transaction
 * nlocktime, nsequence, taproot annex are ways you express conditions
   on the transaction
In that view, "sighash" is actually an *extremely* simple scripting
language itself (with a total of six possible scripts).
That doesn't seem like a bad design to me, fwiw.
[0] "graftroot" lets you update those conditions for address "A" after
    the fact

@_date: 2019-10-05 20:06:15
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
For me, the thing that distinguishes ANYPREVOUT/NOINPUT as warranting
an opt-in step is that it affects the security of potentially many
UTXOs at once; whereas all the other combinations (ALL,SINGLE,NONE
cross ALL,ANYONECANPAY) still commit to the specific UTXO being spent,
so at most you only risk somehow losing the funds from the specific UTXO
you're working with (apart from the SINGLE bug, which taproot doesn't
support anyway).
Having a meaningful prefix on the taproot scriptpubkey (ie paying to
"[SIGHASH_SINGLE][32B pubkey]") seems like it would make it a bit easier
to distinguish wallets, which taproot otherwise avoids -- "oh this address
is going to be a SIGHASH_SINGLE? probably some hacker, let's ban it".
Well, sure. I'm thinking of it more as a *necessary* step than a
*sufficient* one, though. If we can't demonstrate that we can deal with
the theoretical attacks people have dreamt up in a "laboratory" setting,
then it doesn't make much sense to deploy things in a real world setting,
does it?
I think if it turns out that we can handle every case we can think of
easily, that will be good evidence that output tagging and the like isn't
necessary; and conversely if it turns out we can't handle them easily,
it at least gives us a chance to see how output tagging (or chaperone
sigs, or whatever else) would actually work, and if they'd provide any
meaningful protection at all. At the moment the best we've got is ideas
and handwaving...

@_date: 2020-08-17 08:29:23
@_author: Anthony Towns 
@_subject: [bitcoin-dev] reviving op_difficulty 
An alternative approach for this might be to use taproot's annex concept.
The idea would be to add an annex restriction that's only valid if the
difficulty is a given value (or within a given range). Protocol design
could be:
  Alice, Bob, Carol, Dave want to make bets on difficulty futures
  They each deposit a,b,c,d into a UTXO of value a+b+c+d payable to
    a 4-4 of multisig of their keys (eg MuSig(A,B,c,D))
  They construct signed payouts spending that UTXO, all timelocked
    for the future date; one spending to Alice with the annex locking
    in the difficulty to Alice's predicted range, one spending to Bob
    with the annex locking in the difficulty to Bob's predicted range,
    etc
When the future date arrives, whoever was right can immediately
broadcast their payout transaction. (If they don't, then someone else
might be able to when the difficulty next retargets)
(Specifying an exact value for the difficulty rather than a range might
be better; it's smaller/simpler on the blockchain, and doesn't reveal
the ranges of your predictions giving traders slightly better privacy.
The cost to doing that is if Alice predicts difficulty could be any of 100
different values, she needs 100 different signatures for her pre-signed
payout, one for each possible difficulty value that would be encoded in
the annex)

@_date: 2020-08-21 12:36:47
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
Rather than allowing arbitrary messages, maybe it would make sense to
have a specific feature negotiation message, eg:
  VERSION ...
  FEATURE wtxidrelay
  FEATURE packagerelay
  VERACK
with the behaviour being that it's valid only between VERSION and VERACK,
and it takes a length-prefixed-string giving the feature name, optional
additional data, and if the feature name isn't recognised the message
is ignored.
If we were to support a "polite disconnect" feature like Jeremy suggested,
it might be easier to do that for a generic FEATURE message, than
reimplement it for the message proposed by each new feature.

@_date: 2020-02-10 10:20:11
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity 
I think the main cause of the perplexity is not seeing the benefit of
taproot. For me, the simplest benefit is that taproot lets everyone's wallet change
from "if you lose this key, your funds are gone" to "if you lose this key,
you'll have to recover 3 of your 5 backup keys that you sent to trusted
friends, and pay a little more, but you won't have lost your funds". That
won't cost you *anything* beyond upgrading your wallet sotware/hardware;
if you never lose your main key, it doesn't cost you any more, but if
you do, you now have a new recovery option (or many recovery options).
Note that doing graftroot isn't proposed as it requires non-interactive
half-signature aggregation to be comparably efficient, and the crypto
hasn't been worked out for that -- or at least, the maths hasn't been
properly written up for criticism. (If you don't care about efficiency,
you can do a poor man's graftroot with pre-signed transactions and CPFP)
More detailed responses below. Kinda long.
There's a bit more subtlety to the difference between a merkle branch
and a taproot alternative. In particular, imagine you've got three
alternatives, one of which has 60% odds of being taken, and the other
two have 20% odds each. You'd construct a merkle tree:
    /\
   a /\
    b  c
And would reveal:
  60%: a [
  20%: b [   20%: c [ So your overhead would be 32B 60% of the time and 64B 40% of the time,
or an expected overhead of 44.8 bytes.
With taproot, you construct a tree of much the same shape, but 60% of
the time you no longer have to reveal anything about the path not taken:
  60%: a-tweaked
  20%: b [a,   20%: c [a, So your overhead is 0B 60% of the time, and 65B 40% of the time, for an
expected overhead of 26B.
That math only works out as an improvement if your common case really
is (or can be made to be) a simple key path spend, though.
You can generalise taproot and combine it with a merkle tree arbitrarily,
with the end result being that using a merkle branch means you can
choose either the left or right sub-tree for a cost of 32B, while a
taproot branch lets you choose the left *leaf* for free, or a right
sub-tree for (essentially) 64B. So for equally likely branches you'd
want to use the merkle split, while if there's some particular outcome
that's overwhelmingly likely, with others just there for emergencies,
then a taproot-style alternative will be better. See:
for slightly more detailed background.
Ultimately, I think we can do this better, so that you could choose
whether to make the free "taproot" path be a key or a script, or to use
the taproot method to make other likely leaves cheaper than unlikely
ones, rather than just having that benefit available for the most likely
But I also think that's a lot of work, much of which will overlap with
the work to get cross-input signature aggregation working, so fwiw,
my view that the current taproot feature set is a good midway point to
draw a line, and get stuff out and released. This overall approach was
discussed quite a while ago:
You need to provide the internal public key, the actual script and the
path back; the root hash is easily calculated from the script and the
path, and then verified by ECC math against the scriptPubKey and the
internal public key.
In that example there is no taproot case -- you reveal the existance of
other paths no matter which leaf you make use of. In particular, the "pk
schnorr_checksig" alternative now has 96B of additional overhead (
 The benefit of taproot is that often you can preserve the anonymity set
even after you spend.
Yes, presuming single-pubkey-single-signature remains a common
authorisation pattern.
Taproot with a key is about as cheap as it gets -- you've got a 35 byte
scriptPubKey and 66 bytes of witness data.
It's then 33 bytes of witness data more expensive to use a script, which
presumably will make it more likely that people use the simple key path.
At the time you create a utxo, provided you don't reuse keys, all taproot
spends are indistinguishable. At the time you spend a taproot utxo,
you can distinguish:
 - spent via key path
 - spent via script path, internal key not known
 - spent via script path, internal key known NUMS point
but there's no fee rate advantage between reusing a NUMS point and
generating a fresh NUMS point (via NUMS + rand*G), so the third case is
Looking at blocks 616650 to 616700, I see outputs of:
     738  0.3% "pubkey"
    2091  0.8% "witness_v0_scripthash"
   42749 16.8% "witness_v0_keyhash"
  102962 40.4% "pubkeyhash"
  106441 41.7% "scripthash"
So for plain segwit, over 95% of outputs are plain key; and overall,
over 57.5% of outputs are plain key/signature -- that's not counting
however many p2sh-encoded p2wpkh there are, because they'll just look
like pubkeyhash until they're spent.
I don't think so; most of the risk for either of those is in getting
the details right.
That's pretty hard to evaluate if you can't review the crypto parts
yourself, but some resources are:
Most of the complicated crypto parts are at the application layer: muSig,
threshold signatures, adaptor signatures, scriptless scripts, etc.
That... sounds like it's asking for a group of other developers that
have looked at it close enough for you to trust?
That would decrease the anonymity set by a lot, make the code a bit
more complicated, and only end up saving 8 vbytes.
IMO, the driving force for bundling these changes together is the
advantages of taproot -- that is:
 - you can have either a simple public-key and signature to authorise
   a spend, or you can have a script, and decide which to use when
   you spend
 - using the key path comes at no cost compared to not using taproot
 - adding a script path comes at no cost if you don't end up using it
 - if you can interactively verify the script conditions off-chain,
   you can always use the key path
The latter of those means we want schnorr so that the key path can be
multisig, and using schnorr means that we can use scriptless scripts /
adaptor signatures for things like lightning making the key path more
You can't do taproot cheaply with segwit v0 -- you'd have to use p2wsh
and then reveal something like " OP_TAPROOT_VERIFY DROP DROP 1"
as the script, and then have either a signature or the script and its
witness data encoded as the arguments to that script, which is ugly,
but more importantly requires an extra 37 odd byte reveal of the point
every time.
So that leads to doing segwit v1 -- as otherwise you'd lose the
malleability protection segwit brought, or you'd have to reimplement
segwit to allow a top level "OP_TAPROOTVERIFY" to use witness data.
If you're doing segwit v1, you might as well make it so script is
more upgradable -- otherwise as soon as you want to upgrade script
further you'll end up having to jump to segwit v2. That brings in the
generalisation of "p2sh" that allows different scripts to satisfy a script
hash via a merkle path, the leaf version, OP_SUCCESS and the CHECKSIG*
changes, and that pretty much covers everything that's in bips 340-342.
It's not clear to me what "Merkle Branch Witnesses" are. Google comes up
    which don't go into specifics. There's different "MAST" proposals in
Bitcoin, such as bip 116+117 vs bip 114 -- bip 114 and taproot's bip 341
have a similar approach; bip 116 on the other hand gives a merkle verify
opcode, and 117 provides a tail-call semantic that combine allow a
script to produce MAST semantics; though in a more programmable way --
if you had a CAT opcode you could have two MASTs in a single script,
combine their result, and then execute it, for instance.
In order to do something like bip 341's merkle script paths, you'd need
a new segwit version, where the scriptpubkey would be the merkle root
of scripts. If not combined with Schnorr signatures, you'd need to
provide leaf versions or change the way CHECKSIG works from how it works
now in order to upgrade to Schnorr later.
But if we're designing soft-fork 1 in a particular way because we already
know we want to make particular changes from soft-fork 2, I don't think
it makes much sense to split them up.
Having done both of those, in order to do taproot, you'd need another
new segwit version, so that the scriptpubkey could be a taproot point,
but could otherwise reuse the script path.
Obviously I think taproot's desirable, and (roughly) ready to go now,
so I don't see any reason to split that up, particularly when doing so
would use up an additional segwit version.
Updating to schnorr signing makes it easier to validate the blockchain
(batch validation gives a modest speedup once there are many schnorr
signatures), and updating to the signature hashing algorithms described
in bip 341/342 has benefits for making hardware wallets more secure.
While it's obviously fine for people to not upgrade; upgrading sooner
rather than later does have systemic benefits.

@_date: 2020-01-12 00:42:07
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
FWIW etc, but my perspective on this is that the way we want consensus
changes in Bitcoin to work is:
 - decentralised: we want everyone to be able to participate, in
   designing/promoting/reviewing changes, without decision making
   power getting centralised amongst one group or another
 - technical: we want changes to be judged on their objective technical
   merits; politics and animal spirits and the like are fine, especially
   for working out what to prioritise, but they shouldn't be part of the
   final yes/no decision on consensus changes
 - improvements: changes might not make everyone better off, but we
   don't want changes to screw anyone over either -- pareto
   improvements in economics, "first, do no harm", etc. (if we get this
   right, there's no need to make compromises and bundle multiple
   flawed proposals so that everyone's an equal mix of happy and
   miserable)
In particular, we don't want to misalign skills and responsibilities: it's
fine for developers to judge if a proposal has bugs or technical problems,
but we don't want want developers to have to decide if a proposal is
"sufficiently popular" or "economically sound" and the like, for instance.
Likewise we don't want to have miners or pool operators have to take
responsibility for managing the whole economy, rather than just keeping
their systems running.
So the way I hope this will work out is:
 - investors, industry, people in general work out priorities for what's
   valuable to work on; this is an economic/policy/subjective question,
   that everyone can participate in, and everyone can act on --
   either directly if they're developers who can work on proposals and
   implementations directly, or indirectly by persuading or paying other
   people to work on whatever's important
 - developers work on proposals, designing and implementing them to make
   (some subset of) bitcoin users better off, and to not make anyone worse
   off.
 - if someone discovers a good technical reason why a proposal does make
   people worse off, we don't try to keep pushing the proposal over the
   top of objections, but go back to the drawing board and try to fix
   the problems
 - once we've done as much development as we can, including setting up
   experimental testnet/signet style deployments for testing, we setup a
   deployment. the idea at this point is to make sure the live network
   upgrade works, and to retain the ability to abort if last minute
   problems come up. no doubt some review and testing will be left until
   the last minute and only done here, but *ideally* the focus should be
   on catching errors *well before* this point.
 - as a result, the activation strategy mostly needs to be about ensuring
   that the Bitcoin network stays in consensus, rather than checking
   popularity or voting -- the yes/no decisions should have mostly been
   made earlier already. so we have two strategies for locking in the
   upgrade: either 95%+ of hashpower signals that they've upgraded to
   software that will enforce the changes forever more, _or_ after a
   year of trying to deploy, we fail to find any technical problems,
   and then allow an additional 2.5 years to ensure all node software is
   upgraded to enforce the new rules before locking them in.
The strategy behind the last point is that we need to establish that
there's consensus amongst all of Bitcoin before we commit to a flag day,
and if we've found some method to establish consensus on that, then we're
done -- we've already got consensus, we don't need to put a blockchain
protocol on top of that and signal that we've got consensus. (Activating
via hashpower still needs signalling, because we need to coordinate on
*when* sufficient hashpower has upgraded)
This approach is obviously compatible with BIP-148 or BIP-91 style
forced-signalling UASFs if some upgrade does need to be done urgently
despite miner opposition; the forced signalling just needs to occur during
the BIP-9 or BIP-8 phases, and no during the "quiet period". Probably the
first period of BIP-8 after the quiet period would make the most sense.
But without that, this approach seems very friendly for miners: even
if they don't upgrade, they won't mine invalid blocks (unless the rules
activate and someone else deliberately mines an invalid block and they
build on top of it), and if a change is incompatible with, say 10%
of hashpower, it won't be forced on them for 3.5 years, by which point
it's probably a good bet that everyone's upgrading to a new generation
of mining hardware anyway. But even that's a backstop, because if a
change *is* incompatible with existing mining hardware, that's an easily
describable technical problem that should mean we go back to the drawing
board and fix it, not deploy the change despite the problems. [0]
FWIW, that had been my (strong) preference too, but I think I'm now
convinced it's not needed/useful.
The 3.5 year window from BIP-9-starttime to BIP-8-flagday means you'd
have to be using *very* out of date software to need to autodetect
unknown upgrades. If an upgrade starts on 2021-01-01 say, it'd be
supported by 0.21.x, 0.22.0, and 0.23.0 (with bip8 as an opt-in) and
0.24.0, 0.25.0, 0.26.0, 0.27.0, and 0.28.0 (with bip8 as always on)
before flag day activation on 2024-06-01.
0.21.x to 0.23.x could warn if they see potential early BIP-8 activation
via versionbits, and also warn if the flag day date is seen saying "flag
day activation may have happened, please check external sources and
consider upgrading your node software".
So you'd need to be running 0.20.x, released 4 years prior to the
activation to be outdated enough to not get warnings, I think.
If you're knowingly doing a deliberate minority chain split, you'll
almost certainly change the PoW function, and trivially get a clean
split as a result of doing that.
But I think what we want is to move away from consensus decisions being a
"who has the most money/power/hashpower/nodes/reddit-accounts" contest
to being a question of "have we dealt with every reasonable technical
objection?" -- I think that's better for decentralisation in that anyone
can stop a bad proposal without having to be rich/powerful/persuasive,
and better for encouraging constructive contributions. The other side to this is that if it's just a matter of resolving
technical problems, then it's also straightforward for a small but skilled
group to get a consensus change through even if the vast majority doesn't
think it's a priority -- they just need to make a good proposal, make
sure it doesn't make people worse off, work through all the objections
people find, and be willing to wait for it to go through reviews and
upgrade steps which may take extra time if other people don't think it's
a high priority. But those are all just technical challenges, that can
be resolved with skill and patience, whoever you might be. So to me,
that's a win for decentralisation as well.
For me, the focus there is on Matt's first point: "avoid activating
[or merging, or even proposing] in the face of significant, reasonable,
and directed objection". If you want to stop a change you should have to
do nothing more than describe the problems with it; and if there aren't
problems with it, you shouldn't be trying to stop the change.
(A benefit to having the BIP-8 settings defined simultaneously with
the initial activation attempt is that it means that if the core
devs/maintainers go crazy with power and try to force/prevent the BIP-8
activation despite clear community consensus going the other way, then
it will be easy to for the client, and set the parameter correctly --
literally just a matter of changing a value in chainparams.cpp, unlike the
difficulties of changing the blocksize from 1MB to 2MB. Other variations
of this overall approach have the same benefit)
aj (very grateful to Greg and Matt for explaining a lot of thing
    about this approach and helping resolve my concerns with it)
[0] Trigger warning, PTSD over the 2015-2017 blocksize wars...
    The segwit timeline was something like this:
     2015-05 - blocksize debate begins on bitcoin-dev
     2015-08 - bitcoin xt with bip101 hardfork released
     2015-09 - scaling bitcoin phase 1
     2015-12 - segwit proposal at scaling bitcoin phase 2
     2016-01 - segwit testnet launched
     2016-02 - bitcoin classic with bip109 hardfork released
     2016-04 - first release (0.12.1) with a bip9 deployment (csv)
     2016-06 - segwit merged
     2016-07 - csv activated
     2016-10 - first release (0.13.1) with segwit activation params
     2016-11 - segwit activation starttime
     2017-02 - UASF first proposed
     2017-03 - antpool to swith to bitcoin unlimited
     2017-04 - covert ASICBoost vs segwit conflict described
     2017-05 - NY segwit2x agreement, btc1 with bip102 hardfork started
     2017-05 - BIP-91 proposed
     2017-06 - UAHF proposal from bitmain that became BCH
     2017-07 - BIP-91 lockin
     2017-08 - BIP-148 activation
     2017-08 - BCH chainsplit
     2017-08 - segwit lockin and activation
     2017-11 - 2x fork called off; btc1 nodes stall; 2x chain stillborn
     2018-02 - first release (0.16.0) with segwit wallet support
    (That's about 33 months in total, compared to the 24 months we've
    already spent since taproot was first described in Jan 2018, or the
    42 months before flag-day activation in Matt's proposal)
    I don't think that timeline is a good example of how things should
    work, and would call out a few mistakes in particular:
     * too much urgency to increase the blocksize resulting in rushed
       decision making, especially for the hardfork implementations, but
       also for segwit
     * alternative clients attempted to activate forks without
       resolving technical problems (eventually resulting in the btc1
       client stalling prior to the expected hard fork block, eg)
     * a lot of emphasis was on numbers (market share, hashpower, etc)
       rather than technical merits, resulting in a lot of false
       signalling an political meaneuvering
     * the incompatibility between ASICBoost and segwit wasn't noticed
       prior to activation, and wasn't fixed when it was noticed
       (certainly you can justify this as a tit-for-tat response to the
       other errors having been made in bad faith, or as not being a real
       problem because everyone claimed that they weren't doing covert
       ASICBoost, but considered on its own I think the incompatibility
       should have been resolved)
     * the UASF approach had significant potential technical problems
       (potential for long reorgs, p2p network splits) that weren't
       resolved by the time it became active. happily, this was mitigated
       by hashpower enforcement of BIP-148 rules via BIP-91. neither
       BIP-148 or BIP-91 gained enough consensus to be supported in
       bitcoin core though
    I don't personally think we need to fix every problem we had with
    segwit's process -- it eventually mostly worked out okay, after all --
    but I think Matt's approach has a good chance of fixing a lot of
    them, while still leaving us flexibility to deal with whatever new
    problems we come up with in their place.

@_date: 2020-01-14 13:20:26
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
The point of having hashpower upgraded is that it means that there's low
liklihood of long chains of blocks that are invalid per the new rules, so
that if you haven't upgraded your node but wait for a few confirmations,
you'll still (with very high liklihood) only see blocks valid per the
new rules.
If you have 80% of miners enforcing the rules, then if someone produces
a block that violates the new rules (but is valid for the old ones),
then you've got a 20% chance of one of the non-enforcing miners getting
the next block, and a 4% chance of non-enforcing miners getting both
the next blocks, giving 3 confirmations to invalid transactions. That
seems a bit high.
3 confirmations isn't unrealistic, eg Coinbase apparently recently
dropped its requirement to that apparently:
I could maybe see a 90% threshold though?
Is it? We went from 59% to 54% to 28% to 0% (!!) of blocks not signalling
for segwit during consecutive two-week blocks in the BIP-91/148
period; and from 100% of blocks not signalling for BIP-91 to 99.4%,
48%, 15%, and 11% during consecutive 2.3 day periods targeting an 80%
threshold. Certainly that was a particularly high-stakes period, but
they were both pretty short. For comparison, for CSV, we went from 100%
not signalling to 61%, to 54% to 3.4% in consecutive two-week periods.
I don't think you can really skip steps if you need a flag day:
 - the first 12 months is for *really seriously* making sure there's no
   problems with the proposed upgrade; you can't that because people
   might not look for problems until the code's out there and ready for
   actual use
 - the next 6 months is for updating the software to lock in the flag
   day; you can't skip that because it takes time to get new releases out
 - the next 24 months is to ensure everyone's upgraded their nodes so
   that they won't be at risk of thinking they've received bitcoins when
   those coins aren't in compliance with the new rules; and you can't
   skip that because if we don't have hashpower reliably enforcing the
   rules, *everybody* needs to upgrade, which can take a lot of time.
Times could be tweaked, but the "everyone has to upgrade their node
software" is almost the same constraint that hard forks have, so I think
you want to end up with a long overall lead time any which way. For
comparison, 0.12.1 came out about 45 months ago and 0.13.2 came out
about 36 months ago -- about 0.5% of nodes are running 0.12 or earler,
and about 4.9% of nodes are running 0.13 or earlier, at least per [0],
so the overall timeline of 42 months seems plausible to me...
[0] I think (especially if we attempt BIP-91/BIP-148-style compulsory
signalling again) it's worth also considering the failure case if miners
false-signal: that is they signal support of the new soft-fork rules,
but then don't actually enforce them. If you end up with, say, 15% of
hashpower not upgraded or signalling, 25% of hashpower not upgraded but
signalling so their blocks don't get orphaned, and only 65% of hashpower
upgraded, you have a 1% chance of 5 blocks built on top of a block
that's invalid according to the new rules, giving those transactions 6
confirmations as far as non-upgraded nodes are concerned.

@_date: 2020-01-16 13:46:17
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
One thing that I wonder is if your proposal (and BIP9) has enough of
time for this sort of observation?
If something looks good to devs and miners, but still has some
underlying problem, it seems like it would be pretty easy to for it
to activate quickly just because miners happen to upgrade quickly and
don't see a need to tweak the default signalling parameters. I think
the BIP 68/112/113 bundle starttime was met at block 409643 (May 1,
2016), so it entered STARTED at 411264 (May 11), was LOCKED_IN at 417312
(June 21), and active at 481824 (July 5). If we're worried people will
only seriously look at things once activation is possible, having just
a month or two to find new problems isn't very long.
One approach to improve that might be to have the first point release that
includes the soft-fork activation parameters _not_ update getblocktemplate
to signal the version bit by default, but only do that in a second point
release later. That way miners could manually enable signalling if there's
some reason to rush things (which still means there's pressure to actually
look at the changes), but by default there's a bit of extra time.
(This might just be a reason why people should look at proposals before
they're ready to activate, though; or why users of bitcoin should also
be miners)
ITYM version bits are set via mining software rather than the node
software the constructs blocks (when validation happens), so that
there's no strong link between signalling and having actually updated
your software to properly enforce the new rules? I think people have
suggested in the past moving signalling into the coinbase or similar
rather than the version field of the header to make that link a bit
tighter. Maybe this is worth doing at the same time? (For pools that
want to let their users choose whether to signal or not, that'd mean
offering two different templates for them to mine, I guess) That would
mean miners using the version field as extra nonce space wouldn't be
confused with upgrade signalling at least...
(I don't have an opinion on whether either of these is worth worrying

@_date: 2020-07-10 07:40:48
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Hello world,
After talking with Christina ages ago, we came to the conclusion that
it made more sense to update BIP 118 to the latest thinking than have
a new BIP number, so I've (finally) opened a (draft) PR to update BIP
118 with the ANYPREVOUT bip I've passed around to a few people,
Probably easiest to just read the new BIP text on github:
It doesn't come with tested code at this point, but I figure better to
have the text available for discussion than nothing.
Some significant changes since previous discussion include complete lack
of chaperone signatures or anything like it (if you want them, you can
always add them yourself, of course), and that ANYPREVOUTANYSCRIPT no
longer commits to the value (details/rationale in the text).

@_date: 2020-07-10 08:30:50
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Christian. Dr Christian Decker, PhD. Dr Bitcoin. cdecker. Snyke.
aj, hoping he typed one of those right, at least...

@_date: 2020-07-14 19:37:30
@_author: Anthony Towns 
@_subject: [bitcoin-dev] Thoughts on soft-fork activation 
I've been trying to figure out a good way to activate soft forks in
future. I'd like to post some thoughts on that. So:
I think there's two proposals that are roughly plausible. The first is
Luke's recent update to BIP 8:
    It has the advantage of being about as simple as possible, and (in my
opinion) is an incremental improvement on how segwit was activated. Its
main properties are:
   - signalling via a version bit
   - state tansitions based on height rather than median time
   - 1 year time frame
   - optional mandatory activation at the end of the year
   - mandatory signalling if mandatory activation occurs
   - if the soft fork activates on the most work chain, nodes don't
     risk falling out of consensus depending on whether they've opted in
     to mandatory activation or not
I think there's some fixable problems with that proposal as it stands
(mostly already mentioned in the comments in the recently merged PR,
 )
The approach I've been working on is based on the more complicated and
slower method described by Matt on this list back in January. I've got a
BIP drafted at:
    The main difference with the mechanism described in January is that the
threshold gradually decreases during the secondary period -- it starts at
95%, gradually decreases until 50%, then mandatorily activates. The idea
here is to provide at least some potential reward for miners signalling
in the secondary phase: if 8% of hashpower had refused to signal for
a soft-fork, then there would have been no chance of activating until
the very end of the period. This way, every additional percentage of
hashpower signalling brings the activation deadline forward.
The main differences between the two proposals is that the BIP 8 approach
has a relatively short time frame for users to upgrade if we want
mandatory activation without a supermajority of hashpower enforcing the
rules; while the "decreasing threshold" approach linked above provides
quite a long timeline.
In addition, there is always the potential to introduce a BIP 91/148
style soft-fork after the fact where either miners or users coordinate to
have mandatory signalling which then activates a pre-existing deployment
I think the design constraints we want are:
 * if everyone cooperates and no one objects, we activate pretty quickly
 * there's no obvious exploits, and we have plausible contingency plans
   in place to discourage people from try to use the attempt to deploy
   a new soft fork as a way of attacking bitcoin, either via social
   disruption or by preventing bitcoin from improving
 * we don't want to ship code that causes people to fall out of
   consensus in the (hopefully unlikely) event that things don't go
   smoothly [0]
In light of that, I think I'm leaning towards:
 * use BIP 8 with mandatory activation disabled in bitcoin core -- if
   you want to participate in enforcing mandatory activation, you'll
   need to recompile, or use a fork like bitcoin knots; however if
   mandatory activation occurs on the longest chain, you'll still follow
   that chain and enforce the rules.
 * be prepared to update the BIP 8 parameters to allow mandatory
   activation in bitcoin core if, after 9 months say, it's apparent that
   there aren't reasonable objections, there's strong support for
   activation, the vast majority of nodes have already updated to
   enforce the rules upon activation, and there's strong support for
   mandatory activation
 * change the dec-threshold proposal to be compatible with BIP 8, and
   keep it maintained so that it can be used if there seems to be
   widespread consensus for activation, but BIP 8 activation does
   not seem certain -- ie, as an extra contingency plan.
 * be prepared to support miners coordinating via BIP 91 either to
   bring activation forward in either BIP 8 or "decreasing threshold" or
   de-risk BIP 8 mandatory activation -- ie, an alternative contingency
   plan. This is more appropriate if we've found that users/miners have
   upgraded so that activation is safe; so it's a decision we can make
   later when we have more data, rather than having to make the decision
   early when we don't have enough information to judge whether it's
   safe or not.
 * (also, improve BIP 8 a bit more before deploying it -- I'm hoping for
   some modest changes, which is why "decreasing threshold" isn't already
   compatible with BIP 8)
 * (continue to ensure the underlying soft fork makes sense and is
   well implemented on its own merits)
 * (continue to talk to as many people as we can about the underlying
   changes and make sure people understand what's going on and that
   we've addressed any reasonable objections)
I'm hopeful activating taproot will go smoothly, but I'm not 100% sure
of it, and there are potentially many different ways in which things go
wrong; so starting with something simple and being ready to adapt if/when
we see things starting to go weird seems like a good approach to me.
[0] At least, that's how I'm phrasing some of the concerns that were
    expressed in, eg,

@_date: 2020-05-03 00:26:02
@_author: Anthony Towns 
@_subject: [bitcoin-dev] BIP-341: Committing to all scriptPubKeys in the 
I think Andrew's original suggestion achieves this:
presumably with sha_scriptPubKeys' inclusion being conditional on
hash_type not matching ANYONECANPAY.
We could possibly also make the "scriptPubKey" field dependent on
hash_type matching ANYONECANPAY, making this not cost any more
in serialised bytes per signature.
This would basically mean we're committing to each component of the
UTXOs being spent:
  without ANYONECANPAY:
    sha_prevouts commits to the txid hashes and vout indexes (COutPoint)
    sha_amounts commits to the nValues (Coin.CTxOut.nValue)
    sha_scriptpubkeys commits to the scriptPubKey (Coin.CTxOut.scriptPubKey)
  with ANYONECANPAY it's the same but just for this input's prevout:
    outpoint
    amount
    scriptPubKey
except that we'd arguably still be missing:
    is this a coinbase output? (Coin.fCoinBase)
    what was the height of the coin? (Coin.nHeight)
Maybe committing to the coinbase flag would have some use, but committing
to the height would make it hard to chain unconfirmed spends, so at
least that part doesn't seem worth adding.
If you didn't verify the output scriptPubKeys, you would *only* be able
to care about fees since you couldn't verify where any of the funds went?
And you'd only be able to say fees are "at least x", since they could be
more if one of the scriptPubKeys turned out to be OP_TRUE eg. That might
almost make sense for a transaction accelerator that's trying to increase
the fees; but only if you were doing it for someone else's transaction
(since otherwise you'd care about the output addresses) and only if you
were happy to not receive any change? Seems like a pretty weird use case?
There's some prior discussion on this topic at:
