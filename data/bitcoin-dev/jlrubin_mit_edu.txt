
@_date: 2014-07-16 13:56:54
@_author: Jeremy 
@_subject: [Bitcoin-development] Pay to MultiScript hash: 
Hey all,
I had an idea for a new transaction type. The base idea is that it is
matching on script hashes much like pay to script hash, but checks for one
of N scripts.
A motivating case is for "permission groups". Let's say I want to have a
single "root user" script, a 2 of 3 group, and a 2 of 2 group able to spend
a utxo. This would allow for any one of these permission groups to spend.
Right now, this could be expressed multiple ways (ie, using an op_dup if
then else chain) , but all would incur additional costs in terms of
complicated control flows. Instead, I would propose:
OP_HASH160 [20-byte-hash-value 1]...[20-byte-hash-value N] OP_N
could be spent with
...signatures... {serialized script}
?And the alternative formulation: (more complex!)?
?OP_HASH160 OP_DUP [20-byte-hash-value 1]?
? OP_IF OP_EQUAL?
? OP_VERIFY OP_ELSE   Of course, the permission group example is just one use case, there could
be other interesting combinations as well
There is an implication in terms of increased utxo pool bloat, but also an
implication in terms of increased txn complexity (each 20 byte hash allows
for a 500 byte script, only one of the 500 byte scripts has to be
permanently stored on blockchain).
Looking forward to your feedback -- the idea is a bit preliminary, but I
think it could be exciting.

@_date: 2014-07-17 01:59:25
@_author: Jeremy 
@_subject: [Bitcoin-development] Pay to MultiScript hash: 
Additional costs would be in terms of A) chance of user error/application
error -- proposed method is much simpler, as well as extra bytes for
control flow ( 4 per script if I am counting right).
The costs on a normal script do seem slightly more friendly, except this
method allows for hidden-till-spent permission groups, as well as as
smaller blockchain bloat overall (if scriptSig script has to store the
logic for all the potential permission group, it will be a larger script
versus only needing one permission group's script). An added benefit could
also be in blockchain analysis -- you can actively monitor the utxo pool
for your known associated scripts, whereas you couldn't for specialty
scripts assembled per group. Enables repeated spends with groups as a "cost
object" w/o having to recall all participants. ie, pay to the same perm
groups as the other employee did last time, but include me as a root this
Do you have a transcript of that chat by any chance? An interesting way to
do that would be to push the sigs onto the stack & have implicit orders,
then do expressions with their aliases, and then be able to assign
"spending groups".
push script0
push script1
push script2
push script3
mkgroup_2, 0,1      ; the id will be 4
mkgroup_3, 0,2,3   ; the id will be 5
mkUnionGroup_2, 4,5 ; the id will be 6
2_of_3_group 0, 1, 2
mkIntersectionGroup_2 5, 6
complement_last  ; complements last group, mutation
del_group 1          ; deletes the group  groups then reindex after
deletion (maybe the group was useful base class).
multisig check perm groups (checks if any groups on stack are valid from
or even something like adding a little SAT scripting language with an eval.
push script0
push script1
push script2
push script3
push

@_date: 2014-07-17 15:55:10
@_author: Jeremy 
@_subject: [Bitcoin-development] Pay to MultiScript hash: 
* the general cost of any network-wide change, versus P2SH which is
already analyzed by devs, rolled out and working
* the cost of updating everybody to relay this new transaction type,
whereas P2SH Just Works already
fair -- I think that there may be a big benefit realizable with this kind
of system.
* cost of increasing rate of UTXO growth versus P2SH
This operation is similar in cost to multisig? Although I suppose there is
the proposal to make all multisigs p2sh
* the cost of P2SH output is predictable, versus less predictable outputs
 * "default public", versus P2SH's "default private"

@_date: 2014-07-27 22:12:11
@_author: Jeremy 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
There is a potential network exploit going on. In the last three days, a
node (unnamed) came online and is now processing the most traffic out of
any tor node -- and it is mostly plaintext Bitcoin traffic.
Alex Stamos (cc'ed) and I have been discussing on twitter what this could
mean, wanted to raise it to the attention of this group for discussion.
What we know so far:
- Only port 8333 is open
- The node has been up for 3 days, and is doing a lot of bandwidth, mostly
plaintext Bitcoin traffic
- This is probably pretty expensive to run? Alex suggests that the most
expensive server at the company hosting is 299?/mo with 50TB of traffic

@_date: 2015-07-01 10:52:16
@_author: Jeremy 
@_subject: [bitcoin-dev] A possible solution for the block size limit: 
A simple hack to achieve this would be phase shifting the transaction fees
by one block. There may be other problems, but also potential
benefits, with that though.
This hack works because then a miner would orphan a block which didn't
properly reward them, which makes it very costly for even a miner to put in
a bunch of transactions for free. This phase can be adjusted to different
amounts, spread over multiple blocks, or even randomly selected at the time
of mining from a pool of un-fee claimed blocks (although this would require
some seeding to create a pool of any size greater than 1).
Again, this is probably a bad idea and I haven't thought it through
completely, but just tossing it out there.
Ps sorry if you're seeing this many times I think it bounced due to the
not-subscribed rule (sent from my other account)

@_date: 2015-11-05 10:32:39
@_author: Jeremy 
@_subject: [bitcoin-dev] Call for Proposals for Scaling Bitcoin Hong Kong 
The second Scaling Bitcoin Workshop will take place December 6th-7th at the
Cyberport in Hong Kong. We are accepting technical proposals for improving
Bitcoin performance including designs, experimental results, and
comparisons against other proposals. The goals are twofold: 1) to present
potential solutions to scalability challenges while identifying key areas
for further research and 2) provide a venue where researchers, developers,
and miners can communicate about Bitcoin development.
We are accepting two types of proposals: one in which accepted authors will
have an opportunity to give a 20-30 minute presentation at the workshop,
and another where accepted authors can run an hour-long interactive
Topics of interest include:
Improving Bitcoin throughput
Layer 2 ideas (i.e. payment channels, etc.)
Security and privacy
Incentives and fee structures
Testing, simulation, and modeling
Network resilience
Anti-spam measures
Block size proposals
Mining concerns
Community coordination
All as related to the scalability of Bitcoin.
Important Dates
November 9th - Last day for submission
November 16th - Last day for notification of acceptance and feedback
We are doing rolling acceptance, so submit your proposal as soon as you
can. Proposals may be submitted as a BIP or as a 1-2 page extended abstract
describing ideas, designs, and expected experimental results. Indicate in
the proposal whether you are interested in speaking, running an interactive
workshop, or both. If you are interested in running an interactive
workshop, please include an agenda.
Proposals should be submitted to proposals at scalingbitcoin.org by November
All talks will be livestreamed and published online, including slide decks.

@_date: 2015-11-05 20:56:49
@_author: Jeremy 
@_subject: [bitcoin-dev] summarising security assumptions (re cost metrics) 
I'd also like to see some more formal analysis of the notion that "$10 in
the hand of 10 people is more than $50 in the hand of two, or $100 in the
hand of one". I think this encapsulates the security assumption on why we
want decentralization at all.
This is a very critical property bitcoin exploits for being able to
transact large amounts, among other things. (Closely related is the notion
that defecting will destroy all the value...)
 On Thu, Nov 5, 2015 at 6:33 PM, Eric Voskuil via bitcoin-dev <

@_date: 2016-05-22 09:30:53
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP: OP_PRANDOM 
nack -- not secure.
OP_PRANDOM also adds extra validation overhead on a block potentially
composed of transactions all spending an OP_PRANDOM output from all
different blocks.
I do agree that random numbers are highly desirable though.
I think it would be much better for these use cases to add OP_XOR back and
then use something like Blum's fair coin-flipping over the phone. OP_XOR
may have other uses too.
I have a write-up from a while back which does Blum's without OP_XOR using
OP_SIZE for off-chain probabilistic payments if anyone is interested. No
fork needed, but of course it is more limited and broken in a number of
(sorry to those of you seeing this twice, my first email bounced the list)
 On Fri, May 20, 2016 at 2:32 PM, Eric Martindale via bitcoin-dev <

@_date: 2016-11-07 11:30:26
@_author: Jeremy 
@_subject: [bitcoin-dev] Implementing Covenants with 
I think
?the following implementation may be advantageous. It uses the same number
of opcodes, without OP_CAT.
Avoiding use of OP_CAT is still desirable as I think it will be difficult
to agree on semantics for OP_CAT (given necessary measures to prevent
memory abuse) than for OP_LEFT. Another option I would be in support of
would be to have signature flags apply to OP_CHECKSIGFROMSTACK and all
OP_CHECKSIG flags be ignored if they aren't meaningful...
*1. OP_CHECKSIGVERIFY3. OP_SHA256 OP_ROT OP_SIZE OP_SUB1
OP_SWAP OP_ROT OP_CHECKSIGFROMSTACK?VERIFY? (with same ?argument order?)?*
 On Fri, Nov 4, 2016 at 7:35 AM, Tim Ruffing via bitcoin-dev <

@_date: 2016-10-09 12:26:07
@_author: Jeremy 
@_subject: [bitcoin-dev] 1 Year bitcoin-dev Moderation Review 
Hi bitcoin-dev,
I'm well aware that discussion of moderation on bitcoin-dev is
discouraged*. However, I think that we should, as a year of moderation
approaches, discuss openly as a community what the impact of such policy
has been. Making such a post now is timely given that people will have the
opportunity to discuss in-person as well as online as Scaling Bitcoin is
currently underway. On the suggestion of others, I've also CC'd
bitcoin-discuss on this message.
Below, I'll share some of my own personal thoughts as a starter, but would
love to hear others feelings as well.
For me, the bitcoin-dev mailing list was a place where I started
frequenting to learn a lot about bitcoin and the development process and
interact with the community. Since moderation has begun, it seems that the
messages/day has dropped drastically. This may be a nice outcome overall
for our sanity, but I think that it has on the whole made the community
less accessible. I've heard from people (a > 1 number, myself included)
that they now self-censor because they think they will put a lot of work
into their email only for it to get moderated away as trolling/spam. Thus,
while we may not observe a high rate of moderated posts, it does mean the
"chilling effect" of moderation still manifests -- I think that people not
writing emails because they think it may be moderated reduces the rate of
people writing emails which is a generally valuable thing as it offers
people a vehicle through which they try to think through and communicate
their ideas in detail.
Overall, I think that at the time that moderation was added to the list, it
was probably the right thing to do. We're in a different place as a
community now, so I feel we should attempt to open up this valuable
communication channel once again. My sentiment is that we enacted
moderation to protect a resource that we all felt was valuable, but in the
process, the value of the list was damaged, but not irreparably so.
* From the email introducing the bitcoin-dev moderation policy, "Generally
discouraged: shower thoughts, wild speculation, jokes, +1s, non-technical
bitcoin issues, rehashing settled topics without new data, moderation
 concerns."

@_date: 2017-01-02 22:27:44
@_author: Jeremy 
@_subject: [bitcoin-dev] Script Abuse Potential? 
It is an unfortunate script, but can't actually
 that much
? it seems?
. The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.
? Thus, it would seem the worst you could do with this would be to
bytes  ~=~ 10 MB.
?Much more concerning would be the op_dup/op_cat style bug, which under a
similar script ?would certainly cause out of memory errors :)
 On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <

@_date: 2017-01-03 19:13:39
@_author: Jeremy 
@_subject: [bitcoin-dev] Script Abuse Potential? 
Sure, was just upper bounding it anyways. Even less of a problem!
RE: OP_CAT, not as OP_CAT was specified, which is why it was disabled. As
far as I know, the elements alpha proposal to reenable a limited op_cat to
520 bytes is somewhat controversial...

@_date: 2017-01-05 11:22:34
@_author: Jeremy 
@_subject: [bitcoin-dev] Script Abuse Potential? 
Appreciate the historical note, but as that op code was
simultaneously disabled in that patch I don't think we can look back to how
it was non-functionally changed (that number means nothing... maybe Satoshi
was trying it out with 520 bytes but then just decided to all-out disable
it and accidentally included that code change? Hard to say what the intent
That's one part of it that is worth hesitation and consideration. I'm not a
fan of the 520 byte limit as well. My gut feeling is that the "right"
answer is to compute the memory weight of the entire stack before/after
each operation and reasonably bound it.
Below text is from the chain core documentation:
Most instructions use only the data stack by removing some items and then
placing some items back on the stack. For these operations, we define the
standard memory cost applied as follows:
Instruction?s memory cost value is set to zero.
For each item removed from the data stack, instruction?s memory cost is
decreased by 8+L where L is the length of the item in bytes.
For each item added to the data stack the cost is increased by 8+L where L
is the length of the item in bytes.
Every instruction has a cost that affects VM run limit. Total instruction
cost consists of execution costand memory cost. Execution cost always
reduces remaining run limit, while memory usage cost can be refunded
(increasing the run limit) when previously used memory is released during
VM execution.
?Is there a reason to favor one approach over the other? I think one reason
to favor a direct limit on op_cat is it favors what?
? I'll dub "context free" analysis, where the performance doesn't depend on
what else is on the stack (perhaps by passing very large arguments to a
script you can cause bad behavior with a general memory limit?).? On the
other hand, the reason I prefer the general memory limit is it solves the
problem for all future memory-risky opcodes (or present day memory risks!).
Further, OP_CAT is also a bit leaky, in that you could be catting onto a
passed in large string.  The chief argument I'm aware of against a general
memory limit argument is that it is tricky to make a non-implementation
dependent memory limit (e.g., can't just call DynamicMemoryUsage on the
stack), but I don't think this is a strong argument for several
(semi-obvious? I can go into them if need be) reasons.

@_date: 2017-07-21 15:52:45
@_author: Jeremy 
@_subject: [bitcoin-dev] UTXO growth scaling solution proposal 
Hi Major,
I think that you'll enjoy Peter Todd's blogpost on TXO commitments[1]. It
has a better scalability improvement with fewer negative consequence.
[1]  On Fri, Jul 21, 2017 at 3:28 PM, Major Kusanagi via bitcoin-dev <

@_date: 2017-03-28 13:33:31
@_author: Jeremy 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
I think it's probably safer to have a fork-to-minumum (e.g. minimal
coinbase+header) after a certain date than to fork up at a certain date. At
least in that case, the default isn't breaking consensus, but you still get
the same pressure to fork to a permanent solution.
I don't endorse the above proposal, but remarked for the sake of guiding
the argument you are making.
 On Tue, Mar 28, 2017 at 1:31 PM, Wang Chun via bitcoin-dev <

@_date: 2018-02-08 23:29:58
@_author: Jeremy 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
This might be unpopular because of bad re-org behavior, but I believe the
utility of this construction can be improved if we introduce functionality
that makes a script invalid after a certain time (correct me if I'm wrong,
I believe all current timelocks are valid after a certain time and invalid
before, this is the inverse).
Then you can exclude old delegates by timing/block height arguments, or
even pre-sign delegates for different periods of time (e.g., if this
happens in the next 100 blocks require y, before the next 1000 blocks but
after the first 100 require z, etc).
 On Mon, Feb 5, 2018 at 11:58 AM, Gregory Maxwell via bitcoin-dev <

@_date: 2018-02-08 23:42:52
@_author: Jeremy 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
I'm also highly interested in the case where you sign a delegate
conditional on another delegate being signed, e.g. a bilateral agreement.
In order for this to work nicely you also need internally something like
segwit so that you can refer to one side's delegation by a signature-stable
I don't have a suggestion of a nice way to do this at this time, but will
stew on it.

@_date: 2019-12-10 16:37:59
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Three changes I would like to make to the OP_CTV draft. I think this should
put the draft in a very good place w.r.t. outstanding feedback.
The changes can all be considered/merged independently, though, they are
written below assuming all of them are reasonable.
1) *Make the hash commit to the INPUT_INDEX of the executing scriptPubKey.*
*Motivation:* As previously specified, a CTV template constructed
specifying a transaction with two or more inputs has a "half-spend" issue
whereby if the template script is paid to more than once (a form of
key-reuse), they can be joined in a single transaction leading to half of
the intended outputs being created.
Suppose I have a UTXO with a CTV requiring two inputs. The first is set to
be the CTV template, and the input has enough money to pay for all the
outputs. The second input is added to allow the adding of a fee-only utxo.
Now suppose someone creates an similar UTXO with this same CTV (even within
the same transaction).
TxA {vin: [/*elided...*/], vout: [TxOut{1 BTC,  CTV},
TxOut {1 BTC,  CTV}]}
*Intended Behavior:*
    TxB0 {vin: [Outpoint{TxA.hash(), 0}, /*arbitrary fee utxo*/], vout :
[TxOut {1 BTC, /* arbitrary scriptPubKey */}]
    TxB1 {vin: [Outpoint{TxA.hash(), 1}, /*arbitrary fee utxo*/], vout :
[TxOut {1 BTC, /* arbitrary scriptPubKey */}]
*Possible Unintended Behaviors:*
*    Half-Spend:*
        TxB {vin: [Outpoint{TxA.hash(), 1}, Outpoint{TxA.hash(), 0}], vout
: [TxOut {1 BTC, /* arbitrary scriptPubKey */}]
    *Order-malleation:*
        TxB0 {vin: [/*arbitrary fee utxo*/, Outpoint{TxA.hash(), 0}], vout
: [TxOut {1 BTC, /* arbitrary scriptPubKey */}]
        TxB1 {vin: [Outpoint{TxA.hash(), 1}, /*arbitrary fee utxo*/], vout
: [TxOut {1 BTC, /* arbitrary scriptPubKey */}]
With the new rule, the CTV commits to the index in the vin array that it
will appear. This prevents both the half-spend issue and the
order-malleation issue.
Thus, the only execution possible is:
*Intended Behavior:*
    TxB0 {vin: [Outpoint{TxA.hash(), 0}, /*arbitrary fee utxo*/], vout :
[TxOut {1 BTC, /* arbitrary scriptPubKey */}]
    TxB1 {vin: [Outpoint{TxA.hash(), 1}, /*arbitrary fee utxo*/], vout :
[TxOut {1 BTC, /* arbitrary scriptPubKey */}]
*Impact of Change:*
This behavior change is minor -- in most cases we are expecting templates
with a single input, so committing the input index has no effect.
Only when we do specify multiple inputs, committing the INPUT_INDEX has the
side effect of making reused-keys not susceptible to the "half-spend" issue.
This change doesn't limit the technical capabilities of OP_CTV by much
because cases where the half-spend construct is desired can be specified by
selecting the correct inputs for the constituent transactions for the
transaction-program. In the future, Taproot can make it easier to express
contracts where the input can appear at any index by committing to a tree
of positions.
This change also has the benefit of reducing the miner-caused TXID
malleability in certain applications (e.g., in a wallet vault you can
reduce malleability from your deposit flow, preventing an exponential
blow-up). However in such constructions the TXIDs are still malleable if
someone decides to pay you Bitcoin that wasn't previously yours through a
withdrawal path (a recoverable error, and on the bright side, someone paid
you Bitcoin to do it).
This change also has a minor impact on the cacheability of OP_CTV. In the
reference implementation we currently precompute and store single hash for
the StandardTemplateHash of the entire transaction. Making the hash vary
per-input means that we would need to precompute one hash per-input, which
is impractical. Given that we expect the 0-index to be the exceedingly
common case, and it's not horribly expensive if we aren't cached (a
constant sized SHA-256), the spec will be updated to precompute and cache
only the hash for the 0th index. (The hash is also changed slightly to make
it more efficient for un-cached values, as noted in change 3).
*2) Remove Constexpr restriction*
Currently it is checked that the template hash argument was not 'computed',
but came from a preceding push. Remove all this logic and accept any
I've had numerous conversations with Bitcoin developers (see above, see
 on Nov 28th 2019, in person at local meetups, and in
private chats with ecosystem developers) about the constexpr restriction in
OP_CTV. There have been a lot of folks asking to remove template constexpr
restriction, for a few reasons:
a) Parsing Simplification / no need for special-casing in optimizers like
b) The types of script it disables aren't dangerous
c) There are exciting things you can do were it not there and other
features were enabled (OP_CAT)
d) Without other features (like OP_CAT), there's not really too much you
can do
No one has expressed any strong justification to keep it.
The main motivation for the constexpr restriction was to keep the CTV
proposal very conservative in scope, increasing the likelihood that it is
an acceptable change. It was also designed to be able to be easily lifted
in a future soft-fork. There isn't a *specific* behavior the constexpr
restriction is attempting to prevent, it's just a belt-and-suspenders
measure to limit how creatively CTV could be used now or in the future.
Future OpCodes + OP_CTV may introduce a broader set of functionality than
possible if OP_CTV were to retain the constexpr rule. But I think given
that these future op-codes will likely be introduced intentionally to
introduce broader functionalities, we shouldn't limit the functionality of
OP_CTV today.
*Impact of Changes:*
The only mildly interesting thing that could be done with this change (with
no additional changes; that I could think of) would be to write a script
 SHA256 OP_CTV
which would be a "self-describing" covenant (for no space savings). This
could be useful in some protocols where "the public" should be able to
execute some step with only chain-data.
N.B. This cannot enable a case where the CTV is in the scriptSig like:
scriptPubKey:  CHECKSIG
scriptSig:  OP_SHA256 CTV because the serialization of the transaction contains a commitment to
non-null scriptSigs, a self-reference/hash cycle.
*3) Modify the template digest to be easier to cache and work with in
The current hash is:
    uint256 GetStandardTemplateHash(const CTransaction& tx) {
        auto h =  TaggedHash("StandardTemplateHash")
            << tx.nVersion << tx.nLockTime
            << GetOutputsSHA256(tx) << GetSequenceSHA256(tx)
            << uint64_t(tx.vin.size());
        for (const auto& in : tx.vin) h << in.scriptSig;
        return h.GetSHA256();
    }
I propose changing it to:
    uint256 GetStandardTemplateHash(const CTransaction& tx, uint32_t
input_index) {
        uint256 scriptSig_hash{};
        bool hash_scriptSigs = std::count(tx.vin.begin(),
tx.vin.begin(), CScript()) != tx.vin().size();
        if (hash_scriptSigs) {
            auto h =  CHashWriter()
            for (const auto& in : tx.vin) h << in.scriptSig;
            scriptSig_hash = h.GetSHA256();
        }
        auto h =  CHashWriter()
            << tx.nVersion
            << tx.nLockTime;
            if (hash_scriptSigs) h << scriptSig_hash;
            h << uint64_t(tx.vin.size())
            << GetSequenceSHA256(tx)
            << uint32_t(tx.vout.size())
            << GetOutputsSHA256(tx)
            << input_index;
        return h.GetSHA256();
    }
This changes a few things:
1) Getting rid of the TaggedHash use
2) Re-ordering to put input_index last
3) Re-ordering to put Outputs SHA256 second-to-last
4) Only computing scriptSig_hash if any scriptSig is non-null
5) Making scriptSig data hashed in it's own hash-buffer
6) explicitly committing to the vout.size()
7) Casting vout.size() but not vin.size() to uint32_t (vout is capped
by COutpoint indicies to 32 bits, vin is not)
The current digest algorithm is relatively arbitrarily ordered and set up.
Modifying it makes it easier to cache (given the input index change) and
makes it easier to construct templates in script (once OP_CAT, or
OP_SUBSTR, or OP_STREAMSHA256 are added to core).
*Impact of Changes:*
*1) Getting rid of the TaggedHash use*
Low-impact. TaggedHash didn't add any security to the template hashes,
but did make it harder to "confuse" a StandardTemplateHash for a hash
of another type.
However, the tagged hash makes it slightly more difficult/costly to
construct (with OP_CAT enabled) a template hash within script, so it
is removed.
*2) Re-ordering to put input_index last*
The input index should be put last because this makes it easy to cache
the intermediate hash state *just before* hashing the index, which
makes recomputing for different indexes cheaper.
It also allows (with OP_CAT or STREAMSHA256) to easily allow setting
the accepted indexes from script.
*3) Re-ordering to put Outputs SHA256 second-to-last*
In the future, with OP_CAT/SHA256STREAM or similar, changing the
outputs in the covenant is the most likely change. Placing it near the
end simplifies this operation.
*4) Only computing scriptSig_hash if any scriptSig is non-null*
There is no need to hash the scriptSig data at all if they are all
null. This is in most cases true, so we avoid extra hashing.
But the bigger win is for scripted construction of templates, which
can just completely ignore the scriptSig hashing if it is known to be
using all bare CTV/non-p2sh segwit inputs (which should be the common
*5) Making scriptSig data hashed in it's own hash-buffer, when hash is
This implies that there are two possible sizes for the hashed data,
+/- 1 hash (for scripSig_hash). This eliminates concerns that directly
hashing elements into the template hash buffer might expose some
length extension issue when constructing a template in script.
*6) explicitly committing to the vout.size()*
This makes it easier, when OP_CAT or similar is added, to write
restrictions that guarantee a limit on the number of inputs that may
be created.
*7) Casting vout.size() but not vin.size() to uint32_t (vout is capped
by COutpoint indicies to 32 bits, vin is not)*
This is just kind of annoying, but technically you can have more inputs in
a transaction than outputs because more than 32-bits of outputs breaks the
COutpoint class invariants.

@_date: 2019-12-13 15:06:59
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
I've prepared a draft of the changes noted above (some small additional
modifications on the StandardTemplateHash described in the BIP), but have
not yet updated the main branches for the BIP to leave time for any further
See below:
BIP: Thank you for your feedback,

@_date: 2019-12-19 12:08:03
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
I've updated the main branch (ctv) to match ctv-v2, and pushed branches
ctv-v1 which points at the prior versions.
Thanks to Dmitry Petukhov for helping me fix several typos and errors.
I also wanted to share some some "non-technical" tax analysis covering the
use of OP_CTV for batched payments. See here:
As an aside, the site  generally is a repository of
information & material on OP_CTV, it's design, applications, and analysis.
If you're interested in contributing any content please let me know!

@_date: 2019-05-31 22:35:45
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Hi All,
OP_CHECKOUTPUTSHASHVERIFY is retracted in favor of OP_SECURETHEBAG*.
OP_SECURETHEBAG does more or less the same thing, but fixes malleability
issues and lifts the single output restriction to a known number of inputs
OP_CHECKOUTPUTSVERIFY had some issues with malleability of version and
locktime. OP_SECURETHEBAG commits to both of these values.
OP_SECURETHEBAG also lifts the restriction that OP_CHECKOUTPUTSVERIFY had
to be spent as only a single input, and instead just commits to the number
of inputs. This allows for more flexibility, but keeps it easy to get the
same single output restriction.
Implementation: A particularly useful topic of discussion is how best to eliminate the
PUSHDATA and treat OP_SECURETHEBAG like a pushdata directly. I thought
about how the interpreter works and is implemented and couldn't come up
with something noninvasive.
Thank you for your review and discussion,
* Plus the name is better

@_date: 2019-06-02 14:32:20
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Hi Russell,
Thanks for the response. I double checked my work in drafting my response
and realized I didn't address all the malleability concerns, I believe I
have now (fingers crossed) addressed all points of malleability.
*The malleability concerns are as follows:*
A TXID is computed as:
def txid(self):
         r = b""
         r += struct.pack("

@_date: 2019-06-22 23:43:22
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
This is insufficient: sequences must be committed to because they affect
TXID. As with scriptsigs (witness data fine to ignore). NUM_IN too.
Any malleability makes this much less useful.
 On Fri, Jun 21, 2019 at 10:31 AM Anthony Towns via bitcoin-dev <

@_date: 2019-06-24 11:07:20
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Do you think the following hypothesis is more or less true:
H: There is no set of pure extensions* to script E such that enabling E and
OP_SECURETHEBAG as proposed enables recursive covenants, but E alone does
not enable recursive covenants?
* Of course there are things that specifically are specifically designed to
switch on if OP_SECURETHEBAG, so pure means normal things like OP_CAT that
are a function of the arguments on the stack or hashed txn data.
This is the main draw of the design I proposed, it should be highly
improbable or impossible to accidentally introduce more behavior than
intended with a new opcode.
I think that given that H is not true for the stack reading version of the
opcode, we should avoid doing it unless strongly motivated, so as to permit
more flexibility for which opcodes we can add in the future without
introducing recursion unless it is explicitly intended.
On Mon, Jun 24, 2019, 7:35 AM Russell O'Connor

@_date: 2019-06-24 15:47:44
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
I agree in principal, but I think that's just a bit of 'how things are'
versus how they should be.
I disagree that we get composability semantics because of OP_IF. E.g., the
script "OP_IF .... " and "OP_END" are two scripts that separately are
invalid as parsed, but together are valid. OP_IF already imposes some
lookahead functionality... but as I understand it, it may be feasible to
get rid of OP_IF for tapscripts anyways. Also in this bucket are P2SH and
segwit, which I think breaks this because the concat of two p2sh scripts or
segwit scripts is not the same as them severally.
I also think that the OP_SECURETHEBAG use of pushdata is a backwards
compatible hack: we can always later redefine the parser to parse
OP_SECURETHEBAG as the 34 byte opcode, recapturing the purity of the
semantics. We can also fix it to not use an extra byte in a future tapleaf
In any case, I don't disagree with figuring out what patching the parser to
handle multibyte opcodes would look like. If that sort of upgrade-path were
readily available when I wrote this, it's how I would have done it. There
are two approaches I looked at mostly:
1) Adding flags to GetOp to change how it parses
  a) Most of the same code paths used for new and old script
  b) Higher risk of breaking something in old script style/downstream
  c) Cleans up only one issue (multibyte opcodes) leaves other warts in
  d) less bikesheddable design (mostly same as old script)
  e) code not increased in size
2) Adding a completely new interpreter for Tapscript
  a) Fork the existing interpreter code
  b) For all places where scripts are run, switch based on if it is
tapscript or not
  c) Can clean up various semantics, can even do fancier things like
huffman encode opcodes to less than a byte
  d) Can clearly separate parsing the script from executing it
  e) Can improve versioning techniques
  f) Low risk of breaking something in old script style/downstream
  g) Increases amount of code substantially
  h) Bikesheddable design (everything is on the table).
  i) probably a better general mechanism for future changes to script
parsing, less consensus risk
  j) More compatible with templated script as well.
If not clear, I think that 2 is probably a better approach, but I'm worried
that 2.h means this would take a much longer time to implement.
2 can be segmented into two components:
1) the architecture of script parser versioning
2) the actual new script version
I think that component 1 can be relatively non controversial, thankfully,
using tapleaf versions (the architecture question is more around code
structure). A proof of concept of this would be to have a fork that uses
two independent, but identical, script parsers.
Part two of this plan would be to modify one of the versions substantially.
I'm not sure what exists on the laundry list, but I think it would be
possible to pick a few worthwhile cleanups. E.g.:
1) Multibyte opcodes
2) Templated scripts
3) Huffman Encoding opcodes
4) OP_IF handling (maybe just get rid of it in favor of conditional Verify
And make it clear that because we can add future script versions fairly
easily, this is a sufficient step.
Does that seem in line with your understanding of how this might be done?

@_date: 2019-05-20 13:58:03
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Hello bitcoin-devs,
Below is a link to a BIP Draft for a new opcode, OP_CHECKOUTPUTSHASHVERIFY.
This opcode enables an easy-to-use trustless congestion control techniques
via a rudimentary, limited form of covenant which does not bear the same
technical and social risks of prior covenant designs.
Congestion control allows Bitcoin users to confirm payments to many users
in a single transaction without creating the UTXO on-chain until a later
time. This therefore improves the throughput of confirmed payments, at the
expense of latency on spendability and increased average block space
utilization. The BIP covers this use case in detail, and a few other use
cases lightly.
The BIP draft is here:
The BIP proposes to deploy the change simultaneously with Taproot as an
OPSUCCESS, but it could be deployed separately if needed.
An initial reference implementation of the consensus changes and  tests
which demonstrate how to use it for basic congestion control is available
at   The
changes are about 74 lines of code on top of sipa's Taproot reference
Best regards,
Jeremy Rubin

@_date: 2019-05-21 18:47:11
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
I agree a little bit, but I think that logic is somewhat infectious. If
we're going to do covenants, we should also do it as a part of a more
comprehensive new scripting system that gives us other strong benefits for
our ability to template scripts. And so on. I'm excited to see what's
Given that this is very simple to implement and has obvious deployable big
wins with few controversial drawbacks, it makes more sense to streamline
adoption of something like this for now and work on a more comprehensive
solution without urgency.
The design is also explicitly versioned so short of an eventual full
redesign it should be easy enough to add more flexible features piecemeal
as they come up and their use cases are strongly justified as I have shown
here for certified post dated utxo creation.
Lastly I think that while these are classifiable as covenants in
implementation, they are closer in use to multisig pre-signed scripts,
without the requirement of interactive setup. We should think of these as
'certified checks' instead, which can also describe a pre-signed design
satisfactorily. With true covenants we don't want require the satisfying
conditions to be 'computationally enumerable' (e.g. we can't in
computational limits enumerate all public keys if the covenant expresses a
spend must be to a public key). And if the covenant is computationally
enumerable, then we should use this construct and put the spending paths
into a Huffman encoded taproot tree.
On Tue, May 21, 2019, 12:41 PM Matt Corallo

@_date: 2019-05-21 22:11:55
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Yes, in general, Bitcoin does not do anything to prevent users from
discarding their keys.
I don't think this will be fixed anytime soon.
There are some protocols where, though, knowing that a key was once known
to the recipients may make it legally valid to inflict a punitive measure
(e.g., via HTLC), whereas if the key was never known that might be a breach
of contract for the payment provider.

@_date: 2019-05-22 01:10:23
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
CoinJoin transaction is yours, and this does not really improve this
Coinjoin benefits a lot I think.
Coinjoin is improved because you can fit more users into the protocol and
create many more outputs at lower cost or include more participants.
Ideally a coinjoin creates a lot of outputs so that the ownership is
smeared more, but this has a cost at the time of the coinjoin.
Coinjoin is also improved because you don't reveal the outputs created by
the coinjoin until some time, perhaps very far in the future, when you need
the coin. In fact, you only need to reveal where you're moving the coins to
participants in your subtree because participants need only verify their
It also makes the protocol more stable with respect to input choice. This
is because, similar to how NOINPUT may work, OP_COSHV outputs are spendable
without knowing what the TXID will be. Therefore if someone changes their
input or non segwit spend script, it won't break the presigned txns. This
also means that all the inputs can be ANYONECANPAY, so there is no need to
reveal your inputs before anyone else.
This culminates in being able to open channels from a coinjoin safely, I
believe this is difficult/impossible to do currently.
and one input, ending up with *more* bytes onchain, and a UTXO that will be
removed later in (we hope) short time.
unnecessary intermediate transaction outputs, at times when congestion is a
This is a good idea because it improves QoS for most users.
For receiving money pending spendable but confirmed payment (i.e. certified
checks) is superior to having unconfirmed funds.
For sending money, being able to clear all liabilities in a single txn
decreases business exposure to fee variance and confirmation time variance.
E.g., if I'm doing payroll in Bitcoin I will pay big fines if I am a day
late. If I have 10,000 employees this might be painful if fees are
currently up.
It also helps to have a backlog of low priority txns to support the fee
Overall block bandwidth utilization is fairly spikey, so having long term
well known outputs that are not time sensitive can be used to better
utilize bandwidth.
The total extra bandwidth btw is really small given the expansion factor
optimizations available.
offchain update mechanism) on top of this opcode, so I cannot support
replacing `SIGHASH_NOINPUT` with this opcode.
first glance) to be useable as the "stepper" for an offchain update
mechanism, I cannot find a good way to short-circuit the transaction chain
without `SIGHASH_NOINPUT` anyway.
I'm not deeply familiar with DRO channels. This opcode isn't a replacement
for SIGHASH_NOINPUT -- SIGHASH_NOINPUT is mentioned merely to contrast
using SIGHASH_NOINPUT for the uses presented in this BIP.
Lastly there's no 'replacing'. Neither NOINPUT nor COSHV are accepted by
the community at large yet, and they do different things.
updates to the channel structure.
and a pre-signed offchain transaction (especially since the entities
interested in the factory are known and enumerable, and thus can be induced
to sign in order to enter the factory).
I'm not really an expert at Bitcoin Lightning, but this basic mechanism
should work.
Imagine the script at a leaf node:
Taproot([Alice, Bob], [OP_COSHV ]
where uncooperative script is:
Taproot([Alice, Bob], ["1 week" CHECKSEQUENCEVERIFY DROP OP_COSHV )
Cooperative closing skips the extra transactions. Updates are signed
against the uncooperative script with repudation. E.g.:
    HASH160  EQUAL
    IF
    ELSE
        "1 week" CHECKSEQUENCEVERIFY DROP
    ENDIF
    CHECKSIG
It can even be optimized by letting the uncooperative script branches in
the leaf be blaming Alice or Bob.
Does that not work?

@_date: 2019-05-24 13:36:03
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Hi Johnson,
Thanks for the review. I do agree that OP_COSHV (note the pluralization --
it would also be possible to do a OP_COHV   to do specific
I think the point of OP_COSHV is that something like ANYPREVOUT is much
more controversial. OP_COSHV is a subset by design. The IF on ANYPREVOUT is
substantial, discussion I've seen shows that the safety of ANYPREVOUT is
far from fully agreed. (I'll respond to your other email on the subject
too). OP_COSHV is also proposed specifically as a congestion control
mechanism, and so keeping it very easy to verify and minimal data
(optimizations allow reducing it to just OP_COSHV with no 32 byte argument)
suggest this approach is preferable.
In an earlier version, rather than have it be the first input restriction,
I had implemented it an an only one input restriction. This makes it easier
to work with SIGHASH_SINGLE. This works by having the PrecomputedData have
a atomic test_flag. However I felt that the statefulness between
verifications was not great and so I simplified it.
There actually is a reason to require minimal push -- maybe we can change
the rule to be non-minimal pushes are ignored, because we can later extend
it with a different rule. This seems a little error prone. There's also no
reason to not just treat OP_COSHV as a pushdata 32 itself, and drop the
extra byte if we don't care about versioning later.
Requiring a signature actually makes COSHV less useful. So I'm against that

@_date: 2019-05-24 13:51:21
@_author: Jeremy 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Hi Russell,
Thanks for this detailed comparison. The COSHV BIP does include a brief
comparison to OP_CHECKSIGFROMSTACKVERIFY and ANYPREVOUT, but this is more
I think that the power from CHECKSIGFROMSTACKVERIFY is awesome. It's
clearly one of the more flexible options available and would enable a
multitude of new use cases.
When I originally presented my work on congestion control at Jan 2017
BPASE, I also discussed it as an option for covenants. Unfortunately I
think it may be on the edge of too powerful -- there are a lot of use cases
and implications from having a potentially recursive covenant. If you see
my response to Matt in the OP_COSHV BIP thread I classify it as enabling a
non-computationally enumerable set of restrictions.
I think also from a developer point of view working with OP_COSHV is much
much simpler (maybe this can be abstracted) which will lead to increased
adoption. OP_COSHV also uses less per-block bandwidth which also makes it
preferable for a measure intended to decongest blocks. Do you know the
exact byte cost for OP_CHECKSIGFROMSTACK? OP_COSHV scripts, with templating
changes to taproot, can be a single byte. OP_COSHV also has less potential
to have a negative interaction with future opcodes we may want like
OP_PUBKEYTWEAK. While we're getting to an exact spec for the features we
want in Bitcoin scripting, it's hard to sign on to OP_CHECKSIGFROMSTACK
unless there's an exact specification which makes us confident we're
hitting all the points.
If the main complaint about OP_COSHV is that it peeks at surrounding data,
it's also possible to implement it more closely to a multi-byte pushdata
opcode or do the template optimization.
Lastly, as I have previously noted, OP_LEFT is probably safer to implement
than OP_CAT and should be more efficient for OP_CHECKSIGFROMSTACK scripts.

@_date: 2019-05-24 13:59:03
@_author: Jeremy 
@_subject: [bitcoin-dev] Safety of committing only to transaction outputs 
Hi Johnson,
As noted on the other thread, witness replay-ability can be helped by
salting the taproot key or the taproot leaf script at the last stage of a
congestion control tree.
I also think that chaperone signatures should be opt-in; there are cases
where we may not want them. OP_COSHV is compatible with an additional
checksig operation.
There are also other mechanisms that can improve the safety. Proposed below:
OP_CHECKINPUTSHASHVERIFY -- allow checking that the hash of the inputs is a
particular value. The top-level of a congestion control tree can check that
the inputs match the desired inputs for that spend, and default to
requiring N of N otherwise. This is replay proof! This is useful for other
applications too.
OP_CHECKFEEVERIFY -- allowing an explicit commitment to the exact amount of
fee limits replay to txns which were funded with the exact amount of the
prior. If there's a mismatch, an alternative branch can be used. This is a
generally useful mechanism, but means that transactions using it must have
all inputs/outputs set.
 On Fri, May 24, 2019 at 7:40 AM Johnson Lau via bitcoin-dev <

@_date: 2019-05-24 14:15:07
@_author: Jeremy 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
I think you're missing the general point, so I'm just going to respond to
one point to see if that helps your understanding of why OP_COSHV is better
than just pre-signed.
The reason why MuSig and other distributed signing solutions are not
acceptable for this case is they all require interaction for guarantee of
In contrast, I can use a OP_COSHV Taproot key to request a withdrawal from
an exchange which some time later pays out to a lot of people, rather than
having to withdraw multiple times and then pay. The exchange doesn't have
to know this is what I did. They also don't have to tell me the exact
inputs they'll spend to me or if I'm batched or not (batching largely
incompatible with pre-signing unless anyprevout)
The exchange can take my withdrawal request and aggregate it to other
payees into a tree as well, without requiring permission from the
They can also -- without my permission -- make the payment not directly
into me, but into a payment channel between me and the exchange, allowing
me to undo the withdrawal by routing money back to the exchange over
The exchange can take some inbound payments to their hot wallet and move
them into cold storage with pre-set spending paths. They don't need to use
ephemeral keys (how was that entropy created?) nor do they need to bring on
their cold storage keys to pre-sign the spending paths.
None of this really works well with just pre-signing because you need to
ask for permission first in order to do these operations, but with OP_COSHV
you can, just as the payer without talking to anyone else, or just as the
recipient commit your funds to a complex txn structure.
Lastly, think about this in terms of DoS. You have a set of N users who
request a payment. You build the tree, collect signatures, and then at the
LAST step of building the tree, one user drops out. You restart, excluding
that user. Then a different user drops. Meanwhile you've had to keep your
funds locked up to guarantee those inputs for the txn when it finalizes.
In contrast, once you receive the requests with OP_COSHV, there's nothing
else to do. You just issue the transaction and move on.
Does that make sense as to why a user would prefer this, even if there is
an emulation with pre-signed txns?

@_date: 2019-05-24 18:08:00
@_author: Jeremy 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
What do you think about having it be OP_CHECK_TXID_TEMPLATE_DATA where the
hash checked is the TXID of the transaction with the inputs set to 0000...
(maybe appended to the fee paid)?
This allows for a variable number of inputs to be allowed (e.g., one, two,
etc). This also fixes potential bugs around TXID malleability for lightning
like setups (Greg and I discussed in wizards about version malleability).
Allowing multiple inputs is great for structuring more complex contracts
with multiple nodes paying into the same covenantted transaction.
Also I personally prefer a RISC+CISC approach -- we should enable the
common paths easily as they are known (didn't you come up with jets?) and
improve security for API users, but also piecemeal enable features in
script to allow for experimentation or custom contracts.
 On Fri, May 24, 2019 at 4:15 PM Russell O'Connor

@_date: 2019-11-25 17:50:40
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Bitcoin Developers,
Pleased to announce refinements to the BIP draft for OP_CHECKTEMPLATEVERIFY
(replaces previous OP_SECURETHEBAG BIP). Primarily:
1) Changed the name to something more fitting and acceptable to the
2) Changed the opcode specification to use the argument off of the stack
with a primitive constexpr/literal tracker rather than script lookahead
3) Permits future soft-fork updates to loosen or remove "constexpr"
4) More detailed comparison to alternatives in the BIP, and why
OP_CHECKTEMPLATEVERIFY should be favored even if a future technique may
make it semi-redundant.
Please see:
BIP: Reference Implementation:
I believe this addresses all outstanding feedback on the design of this
opcode, unless there are any new concerns with these changes.
I'm also planning to host a review workshop in Q1 2020, most likely in San
Francisco. Please fill out the form here if you're interested in participating (even if you can't physically attend).
And as a "but wait, there's more":
1) RPC functions are under preliminary development, to aid in testing and
evaluation of OP_CHECKTEMPLATEVERIFY. The new command `sendmanycompacted`
shows one way to use OP_CHECKTEMPLATEVERIFY. See:
`sendmanycompacted` is still under early design. Standard practices for
using OP_CHECKTEMPLATEVERIFY & wallet behaviors may be codified into a
separate BIP. This work generalizes even if an alternative strategy is used
to achieve the scalability techniques of OP_CHECKTEMPLATEVERIFY.
2) Also under development are improvements to the mempool which will, in
conjunction with improvements like package relay, help make it safe to lift
some of the mempool's restrictions on longchains specifically for
OP_CHECKTEMPLATEVERIFY output trees. See:
This work offers an improvement irrespective of OP_CHECKTEMPLATEVERIFY's
Neither of these are blockers for proceeding with the BIP, as they are
ergonomics and usability improvements needed once/if the BIP is activated.
See prior mailing list discussions here:
Thanks to the many developers who have provided feedback on iterations of
this design.

@_date: 2019-11-29 04:59:42
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Thanks for the feedback Russell, now and early. It deeply informed the
version I'm proposing here.
I weighed carefully when selecting this design that I thought it would be
an acceptable tradeoff after our discussion, but I recognize this isn't
exactly what you had argued for.
First off, with respect to the 'global state' issue, I figured it was
reasonable with this choice of constexpr rule given that a reasonable tail
recursive parser might look something like:
parse (code : rest) stack alt_stack just_pushed =
    match code with
        OP_PUSH => parse rest (x:stack) alt_stack True
        OP_DUP => parse rest (x:stack) alt_stack False
        // ...
So we're only adding one parameter which is a bool, and we only need to
ever set it to an exact value based on the current code path, no
complicated rules. I'm sensitive to the complexity added when formally
modeling script, but I think because it is only ever a literal, you could
re-write it as co-recursive:
parse_non_constexpr (code : rest) stack alt_stack =
    match code with
        OP_PUSH => parse_constexpr rest (x:stack) alt_stack
        OP_DUP => parse_non_constexpr rest (x:stack) alt_stack
        // ...
parse_constexpr (code : rest) stack alt_stack  =
    match code with
        OP_CTV => ...
        _ => parese_non_constexpr (code : rest) stack alt_stack
If I recall, this should help a bit with the proof automatability as it's
easier in the case by case breakdown to see the unconditional code paths.
In terms of upgrade-ability, one of the other reasons I liked this design
is that if we do enable OP_CTV for non-constexpr arguments, the issue
basically goes away and the OP becomes "pure" without any state tracking.
(I think the switching on argument size is much less a concern because we
already use similar upgrade mechanisms elsewhere, and it doesn't add
parsing context).
It's also possible, as I think *should be done* for tooling to treat an
unbalanced OP_CTV as a parsing error. This will always produce
consensus-valid scripts! However by keeping the consensus rules more
relaxed we keep our upgrade-ability paths open for OP_CTV, which as I
understand from speaking with other users is quite desirable.
Best (and happy thanksgiving to those celebrating),
 On Thu, Nov 28, 2019 at 6:33 AM Russell O'Connor

@_date: 2019-10-03 16:22:47
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
I've updated the BIP to no longer be based on Taproot, and instead based on
a OP_NOP upgrade. The example implementation and tests have also been
The BIP defines OP_NOP4 with the same semantics as previously presented.
This enables OP_SECURETHEBAG for segwit and bare script, but not p2sh
(because of hash cycle, it's impossible to put the redeemscript on the
scriptSig without changing the bag hash). The implementation also makes a
bare OP_SECURETHEBAG script standard as that is a common use case.
To address Russel's feedback, once Tapscript is fully prepared (with more
thorough script parsing improvements), multibyte opcodes can be more
cleanly specified.
n.b. the prior BIP version remains at

@_date: 2019-10-03 22:02:14
@_author: Jeremy 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Awhile back, Ethan and I discussed having, rather than OP_CAT, an
OP_SHA256STREAM that uses the streaming properties of a SHA256 hash
function to allow concatenation of an unlimited amount of data, provided
the only use is to hash it.
You can then use it perhaps as follows:
OP_SHA256STREAM  (-1) -> [state]
OP_SHA256STREAM n [item] [state] -> [state]
OP_SHA256STREAM (-2) [state] -> [Hash]
<-1> OP_SHA256STREAM    <3> OP_SHA256STREAM <-2>
Or it coul

@_date: 2019-10-04 11:33:09
@_author: Jeremy 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Good point -- in our discussion, we called it OP_FFS -- Fold Functional
Stream, and it could be initialized with a different integer to select for
different functions. Therefore the stream processing opcodes would be
generic, but extensible.
 On Fri, Oct 4, 2019 at 12:00 AM ZmnSCPxj via Lightning-dev <

@_date: 2019-10-04 11:40:53
@_author: Jeremy 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Interesting point.
The script is under your control, so you should be able to ensure that you
are always using a correctly constructed midstate, e.g., something like:
scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>
 OP_EQUALVERIFY
would hash all the elements on the stack and compare to a known hash.
How is that sort of thing weak to midstateattacks?

@_date: 2019-10-27 12:13:09
@_author: Jeremy 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
The issues with mempool limits for OP_SECURETHEBAG are related, but have
distinct solutions.
There are two main categories of mempool issues at stake. One is relay
cost, the other is mempool walking.
In terms of relay cost, if an ancestor can be replaced, it will invalidate
all it's children, meaning that no one paid for that broadcasting. This can
be fixed by appropriately assessing Replace By Fee update fees to
encapsulate all descendants, but there are some tricky edge cases that make
this non-obvious to do.
The other issue is walking the mempool -- many of the algorithms we use in
the mempool can be N log N or N^2 in the number of descendants. (simple
example: an input chain of length N to a fan out of N outputs that are all
spent, is O(N^2) to look up ancestors per-child, unless we're caching).
The other sort of walking issue is where the indegree or outdegree for a
transaction is high. Then when we are computing descendants or ancestors we
will need to visit it multiple times. To avoid re-expanding a node, we
currently cache it with a set. This uses O(N) extra memory and makes O(N
Log N) (we use std::set not unordered_set) comparisons.
I just opened a PR which should help with some of the walking issues by
allowing us to cheaply cache which nodes we've visited on a run. It makes a
lot of previously O(N log N) stuff O(N) and doesn't allocate as much new
memory. See: Now, for OP_SECURETHEBAG we want a particular property that is very
different from with lightning htlcs (as is). We want that an unlimited
number of child OP_SECURETHEBAG txns may extend from a confirmed
OP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as
lightning (one dangling unconfirmed to permit channels).
OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a tree
where they are individualized leaf nodes with a preceding CSV. Then, the
above fix would ensure each HTLC always has time to close properly as they
would have individualized lockpoints. This is desirable for some additional
reasons and not for others, but it should "work".
 On Fri, Oct 25, 2019 at 10:31 AM Matt Corallo

@_date: 2020-04-22 18:18:05
@_author: Jeremy 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
Hi everyone,
Sorry to just be getting to a response here. Hadn't noticed it till now.
*(Plug: If anyone or their organizations would like to assist in funding
the work described below for a group of developers, I've been working to
put resources together for funding the above for a few months now, and I
think it would be high leverage towards seeing this through. There are a
lot of unsexy tasks to do  that aren't coming up with a solution
(e.g.,writing a myriad of Mempool stress test scenarios) that can be a well
defined full-time job for someone to do.)*
I've been working on exactly this problem in the mempool for months now.
I'm deeply familiar with the issues here and the types of pinning possible.
I think everyone can recognize that with my work on OP_CTV I want nothing
more than the mempool to be able to accept whatever long chains we can
throw at it, but I'm pretty well steeped at this point in the obstacles to
doing that.
I don't think that we should be entertaining further carve outs at the
moment, unless it is really trivial. Every new carve out rule added to the
way that the mempool operates is removing complexity invariants we aim to
preserve in the mempool in order to keep nodes operational. Many of these
invariants are well documented, some are not. I'm happy to go off list for
a more thorough discussion with anyone qualified to have it; this isn't the
best venue for that discussion.
resources towards finishing the mempool project I began. You can see the
outstanding work here: contributing review towards moving those PRs forward will greatly improve
our ability to consider a stopgap carve out measure.
The current focus of this work is primarily on:
1) Testing Construction to better test & catch regressions or
vulnerabilities introduced or extant in mempool
2) Refactoring algorithms in mempool to reduce constant factors &
3) Package Relay
None of these fix the exact problem at hand though, but here's part of how
they can help us:
If we finish up the algorithmic refactors I've been working on it seems
plausible to do a one-off increase of descendants limits to say, 100
descendants with no restriction. However, we could use the opportunity to
use the 75 descendant increase exclusively for a new carve out, and apply
some new stricter rules in that extra space. There are a few anti-pinning
countermeasures that you can apply in that space that you would not
generally want in the mempool. An example of one is that any new
transaction must pay more feerate and absolute fee than every child in that
space. Or that only the highest fee paying branch of the excess
transactions are mineable, no others. Another would be disabling RBF past
that watermark. In all likelihood, different subsystems interacting with
the mempool will require a different set of restrictions each with the
current architecture, I don't think there's a magic bullet.
Package relay is a promising approach for a future pinning solution as
there are opportunities to attach to packages compact proofs of improved
fee efficiency for pinned transactions. But the ground work for package
relay needs to come first. This is theoretically possible with our current
architecture of the mempool and can probably address much of the pinning
concerns by replacing pinning with more rational eviction policies.
Longer term I've been working on plans and designs to completely re-do the
mempool's architecture to make it behave for arbitrary cases. It's possible
to one day lift all preemptively enforced (e.g., before acceptance)
descendants limits, which can solve this problem for good. There is more
than one potentially good solution here, and a conjunction of them can be
used as they affect independent sub systems. But this work will probably
take years to complete to the point where restrictions can realistically be
If developers would like to coordinate resources around completing this
work and making more regular progress on it I'm happy to help point people
to specific tasks that need to be done in order to accelerate this and help
serialize the work so that we can not get into rebase hell.
Originally I had the plug at the top as a closing note, but I figured
people might miss it.

@_date: 2020-08-16 10:24:09
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
Concept ack!
It might be nice to include a few negotiation utility functions either in
this bip or at the same time in a separate bip. An example we might want to
include is a "polite disconnect", whereby a node can register that you
don't want to connect in the future due to incompatibility.
It also might be nice to standardize some naming convention or negotiation
message type so that we don't end up with different negotiation systems.
Then we can also limit the bip so that we're only defining negotiation
message types as ignorable v.s. some other message type (which can also be
ignored, but maybe we want to do something else in the future).
This also makes it easier for old (but newer than this bip) nodes to apply
some generic rules around reporting/rejecting/responding to unknown feature
negotiation v.s. an untagged message which might be a negotiation or
something else.

@_date: 2020-08-21 12:50:21
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
I have a proposal:
Protocol >= 70016 cease to send or process VERACK, and instead use
HANDSHAKEACK, which is completed after feature negotiation.
This should make everyone happy/unhappy, as in a new protocol number it's
fair game to change these semantics to be clear that we're acking more than
I don't care about when or where these messages are sequenced overall, it
seems to have minimal impact. If I had free choice, I slightly agree with
Eric that verack should come before feature negotiation, as we want to
divorce the idea that protocol number and feature support are tied.
But once this is done, we can supplant Verack with HANDSHAKENACK or
HANDSHAKEACK to signal success or failure to agree on a connection. A NACK
reason (version too high/low or an important feature missing) could be
optional. Implicit NACK would be disconnecting, but is discouraged because
a peer doesn't know if it should reconnect or the failure was intentional.
AJ: I think I generally do prefer to have a FEATURE wrapper as you
suggested, or a rule that all messages in this period are interpreted as
features (and may be redundant with p2p message types -- so you can
literally just use the p2p message name w/o any data).
I think we would want a semantic (which could be based just on message
names, but first-class support would be nice) for ACKing that a feature is
enabled. This is because a transcript of:
FEATURE A
FEATURE B
FEATURE A
It remains unclear if Node 1 ignored B because it's an unknown feature, or
because it is disabled. A transcript like:
FEATURE A
FEATURE B
FEATURE C
ACK A
FEATURE A
ACK A
NACK B
would make it clear that A and B are known, B is disabled, and C is
unknown. C has 0 support, B Node 0 should support inbound messages but
knows not to send to Node 1, and A has full bilateral support. Maybe
instead it could a message FEATURE SEND A and FEATURE RECV A, so we can
make the split explicit rather than inferred from ACK/NACK.
I'd also propose that we add a message which is SYNC, which indicates the
end of a list of FEATURES and a request to send ACKS or NACKS back (which
are followed by a SYNC). This allows multi-round negotiation where based on
the presence of other features, I may expand the set of features I am
offering. I think you could do without SYNC, but there are more edge cases
and the explicitness is nice given that this already introduces future
This multi-round makes it an actual negotiation rather than a pure
announcement system. I don't think it would be used much in the near term,
but it makes sense to define it correctly now. Build for the future and

@_date: 2020-08-21 14:08:33
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
Actually we already have service bits (which are sadly limited) which allow
negotiation of non bilateral feature support, so this would supercede that.
 On Fri, Aug 21, 2020 at 1:45 PM Matt Corallo

@_date: 2020-08-21 14:17:32
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
As for an example of where you'd want multi-round, you could imagine a
scenario where you have a feature A which gets bugfixed by the introduction
of feature B, and you don't want to expose that you support A unless you
first negotiate B. Or if you can negotiate B you should never expose A, but
for old nodes you'll still do it if B is unknown to them. An example of
this would be (were it not already out without a feature negotiation
existing) WTXID/TXID relay.
The SYNC primitve simply codifies what order messages should be in and when
you're done for a phase of negotiation offering something. It can be done
without, but then you have to be more careful to broadcast in the correct
order and it's not clear when/if you should wait for more time before

@_date: 2020-08-24 12:58:56
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
The benefit is not privacy oriented and I didn't intend to imply as such.
The benefit is that you may only wish to expose functionality to peers
which support some other set of features. For example, with wtxid relay, I
might want to expose some additional functionality after establishing my
peer supports it, that peers which do not have wtxid relay should not be
allowed to use. The benefit over just exposing all functions is then a node
might be programmed to support the new feature but not wtxid relay, which
can lead to some incompatibilities.
You cannot implement this logic as a purely post-hoc "advertise all and
then figure out what is allowed" because then you require strict
consistency between peers of that post-hoc feature availability implication

@_date: 2020-08-24 13:21:52
@_author: Jeremy 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
I interpreted* " This seems to imply a security benefit (I can?t discern
any other rationale for this complexity). It should be clear that this is
no more than trivially weak obfuscation and not worth complicating the
protocol to achieve.", *to be about obfuscation and therefore privacy.
The functionality that I'm mentioning might not be buggy, it might just not
support peers who don't support another feature. You can always disconnect
a peer who sends a message that you didn't handshake on (or maybe we should
elbow bump given the times).

@_date: 2020-08-25 08:24:46
@_author: Jeremy 
@_subject: [bitcoin-dev] New tipe of outputs that saves space and give 
You may wish to review bip-119 ChecktemplateVerify, as it is designed to
support something very similar to what you've described. You can see more
at On Tue, Aug 25, 2020, 6:48 AM Jule.Adka via bitcoin-dev <

@_date: 2020-02-03 00:20:52
@_author: Jeremy 
@_subject: [bitcoin-dev] CTV through SIGHASH flags 
I think these ideas shows healthy review of how OP_CTV is specified against
alternatives, but I think most of the ideas presented are ill advised.
 On Sat, Feb 1, 2020 at 2:15 PM Bob McElrath via bitcoin-dev <
I've previously brought this up in IRC
AFAIK, using an actual CheckSig SIGHASH Flag as is is a bad idea because
then you need to include an encoding valid signature and pubkey that map
onto the hash to check. This is not just extra 11 extra bytes of data (33
bytes PubKey + 9 bytes Signature + 2 push -32 bytes - 1 byte push), it's
also a very awkward API. I don't think you can soft-fork around these
encoding rules. But you're right that it's possible to add this as a
SIGHASH flag. I don't think doing CTV as a sighash flag is worth
considering further.
I get your point that CTV is kind of a signature hash, and that we might
want to not have a separate path. This ignores, however, that the current
SIGHASH code-path is kind of garbage and literally no one likes it and it
has been the source of nasty issues previously. Thus I posit that a
separate path creates less complexity, as we don't need to worry about
accidentally introducing a weird interaction with other sighash flags.
NOINPUT as specified here
(is this the latest?) isn't a great surrogate for CTV because CTV commits
to the input index which prevents half-spend. This also encumbers, as
proposed, an additional chaperone signature to fix it to a specific output.
This adds a lot of complexity and space to using CTV. Maybe NOINPUT could
make changes to special-case CTV, but then we're back to CTV again.
1. There is a semantic difference between the *commitment* being strictly
redundant, which has more to do with malleation, and being redundant from a
feature perspective. I could maybe do a better job here of expanding what
"easier" means here -- there are actually some scripts which are quite
difficult to write/impossible without this. I've described this a couple
places outside of the BIP, but essentially it allows you to pin the number
of inputs/outputs separately from the hashes themselves. So if you're
trying to build the template in script, you might want to allow the
Sequences to be set to any value, and pass them via a hash. But then from a
hash you can't check the validity of the length. An external length
commitment lets you do this, but without it you would have to pass in the
sequences directly.
2. The constexpr requirement was implemented in a soft-fork re-moveable
manner specifically so that if we wanted OP_CAT, we could add it without
also introducing constructing CTVs on the stack. Similarly, it would be
possible to specify with OP_CAT as a soft-fork removeable rule that if
OP_CAT executes before an OP_CTV, the script is invalid. The constexpr rule
was removed on the sentiment that if we introduce OP_CAT, we almost surely
intend to introduce it for OP_CTV (or related) use cases.
3. Committing to the input-index is not a *sender* policy choice. It's a
receiver policy choice. I tell a Payer my invoice/address, and they emit a
transaction matching it. From an address containing a CTV, I as the
receiver set the input_index. I don't see how this is related to the
4. You write as if OP_CTV + OP_CAT allows the input index to stripped
*unconditionally*. That's wrong. It's an opt in if you write a script
taking it as a parameter. You can't evade it in general use.
5. The "anti-footgun" measure is that it precludes reused-keys being spent
in the same transaction. Were you to opt out of the mechanism (via OP_CAT
input_index), then you opt out of the reuse protection. (This only matters
if there is more than one input to begin with).
6. Committing to it via a flag is strictly less flexible, because I can do
a lot more with OP_CAT than with a flag. For instance, I can do
   OP_WITHIN OP_VERIFY to ensure that it falls
within a certain range of inputs.
7. A flag is also an extra byte somewhere or uses a sighash bit.
8. Enabling a flag right away enables a big footgun right off the bat. I
think it's bad for use safety.
9. Rather than add flags, if you wanted to add this, I would suggest
reserving max input_index to specify a don't care value. Then you always
check a given CTV against the don't care value and the specified value.
Hashing the don't care value can be done in the PreComputedTxData. But I
don't think it's worth special casing or making available today because of
Sure -- happy to go down the renaming path again. Keep in mind that CTV
currently only applies rules when the argument is 32-bytes. Future
soft-forkers are welcome to define a rule for a 33byte 1st argument that
treats it as a pubkey and has CHECKSIG semantics, and looks for another
I think this "sender/redeemer" framework is a bit bunk. Ultimately all
redeemers are senders, and you aren't forcing a choice on someone. You
could be on to something though, but I think in general Bitcoin has gone
the way of opaque addresses so that people can't encumber arbitrary
policies on your coins. Maybe it swings the other way...
The sender commits to them, but legally, if you add a contract that I
didn't agree to as receipt (e.g., in segwit address -- which the script is
hashed) I won't even know I got paid. So the way Bitcoin works today, these
are receiver set policies.
One way to think of CTV is it's precise the opcode that lets you "wrap"
someone's known address in arbitrary new scripts. E.g., if you gave me an
address X, but I need to (for whatever reason) add an additional 1 month
So i just get the txn:
    sequence 1mo
    1 input
    1 output: pay X 1 coin
then take the STH(A), and create B
    ... inputs
    1 output pay `STH(A) CTV` 1 coin
I can also add other things, like secondary signers/alternatives
`IF {some checksig program} STH(A) CTV ELSE {multisig program} ENDIF
ANYPREVOUT/ANYSCRIPT are actually weirder than that, because once it has
been used that key is permanently "burned". Hence ANYPREVOUT has such
pubkeys be explicity tagged as ANYPREVOUT compatible. So a user kind of has
to pre-decide if they want to have ANYPREVOUT semantics or not.
And in this case, key-reuse is relatively unsafe (as you need to track what
else you've signed) so I think what you're suggesting is not robust.
These "MUST" conditions sound nice, but they don't actually help with
validity caching because we want to be able to compute this information
before we've fetched the outputs from the database so we can't know what to
cache yet. Contextless things are things you can precompute not knowing
input scripts.
Again, I don't think this sender/redeemer framework is super useful but I
admire the attempt.
I'm confused. Transactions don't have addresses. What are you talking about?
Input indexes accomplish two goals.
One, they eliminate third-party malleability of input order (which would
muck with sighash singles too).
Two, they prevent half-spend.
Signatures today commit to this in the signature hash (the field is nIn,
which is confusing because nIn might also look like vin.size()).
So the half spend problem doesn't exist today...
So OP_CAT already lets you do this kind of stuff with the SIGHASHes rather
than a new special-purpose verifier. Just pass the signature separately
from the flags, and cat them togehther for the checker but just look
flags for your new thing. Then check that the flags are exactly what you
wanted. If you don't want OP_CAT, you can also add OP_SUBSTRVERIFY wherein
you verify that a provided substr was inside another string. Then you pass
in the witness the full string you want, as well as sub-bits you want to
check properties on (like the flags).
It's not clear to me that we want this kind of stuff though. OP_CAT
requires very careful review because it has very surprising functional
consequences across the board.
ANYPREVOUT already precludes these by using a separate key type and
chaperone signatures.  I think a flag for MUST NOT ANYPREVOUT would maybe
help with making it safer. But this is a complete sidebar from CTV. This
exists already by generating a non-anyprevout capable key though...
I'll let the email above serve as the answer to your question.
I don't think there's anything gained by expressing CTV as a sighash type
today, especially since a future soft fork (when we've taking the time to
deeply rethink sighash flags, like bitmask sighash flags proposed for
elements) can make CTV (as specified today) a valid hash in this new
language and use the OP_NOP4 with a non 32-byte argument as the new
CheckSig operator anyways.
But now I'll pose a different question: why shouldn't we compute the
sighash entirely as a type of Bitcoin Script? SIGHASH_FLAGS are essentially
a tiny, crappy, language for putting together a message digest. You can
think of SIGHASH_FLAGS as being like optimized "jets" for known programs.
For custom programs, you can construct the digest pattern you want in
script. This is essentially what the bitmask sighash flags proposal is. I
think you're going to waste a lot of mental-cycles trying to cram in all
this logic into flags. As this stuff gets more complicated, you should just
write an actual language for dealing with sighashes and go from there.
Now why don't we want this sighash language? Quadratic hashing. If every
output commits to some different complex thing, we end up doing a lot of
rehashing. Flags are actually kind of bad because a few different flags can
trigger a lot of rehashing. But the way flags are *today* is relatively OK
because we can cache the important parts so validation is cheap.
The more complicated you plan gets, the less context free validation we can
CTV is fully compatible with context free validation optimizations,
trivially. It's not clear if your other stuff is, I suspect not.

@_date: 2020-02-14 11:16:26
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Hi Dmitry,
I don't think that this is fundamentally introducing new behavior, but
let's take a closer look.
We can talk about the issue you bring up purely in terms of a hypothetical
"OP_CHECKINPUTOUTPOINTVERIFY" and "OP_CHECKINPUTSCRIPTVERIFY" (CIOV, CISV)
with obvious implied by name semantics, as a separate construct from CTV
itself. Such opcodes would be strictly more powerful/flexible than what CTV
is enabling.
Using these opcodes I can make an output that can *only* be spent with
another output -- e.g.,
  OP_CISV OP_DROP  OP_CHECKSIGVERIFY
  OP_CIOV OP_DROP  OP_CHECKSIGVERIFY
Let's look at CISV first:
1) Assume that  is from the same owner as PK
2) Assume that  is from a different owner than PK
In case 1, the wallet can create or recreate the appropriate output as
needed if it gets spent/stuck
In case 2, the wallet can get "frozen" in a reorg until a signer on For CIOV:
1) Assume that  exists in the chain somewhere
2) Assume that  exists in the mempool somewhere
3) Assume that  does not exist (or, is provably non-creatable -- h =
txid(x) | x.IsValid() == false)
In case 2, this is just a fancy op-return.
Case 1 degrades into case 2 in the event of a reorg.
In Case 2, if the output  is spent in another transaction, our script
becomes provably unspendable (unless a second reorg).
Otherwise, it is possible to mine a block with our transaction.
Compare the above to normal transactions:
1) If a reorg occurs, and someone double-spends, your transaction gets
2) You can re-sign your UTXO onto a different transaction
However, if you have deleted your key (e.g. using a pre-signing HSM), or
your transaction was using a multi-sig with an uncooperating party, you
will have an output that may be effectively burned.
These issues are -- as with CTV -- not present in the single input use case.
Thus I argue that CTV -- whose semantics are less powerful/flexible than
CISV/CIOV -- aren't introducing something that's not already present when
doing protocols involving more than one input.
Further, on CTV "monotonic authorization":
Generally we want Bitcoin Scripts to have the property that once a
condition is reached, it is 'permanently' a true case. E.g., showing a hash
preimage to C x, H(x) == C. This can't change with the weather or anything
else. Even things like timelocks -- although not obvious at first glance --
have this property. They express logic that says "given the chain is at
this height, ...". This means that on any chain at such a height the txn is
valid. CISV/CIOV semantics also fall in line with this description. It
says, "given such an input U, ...". If that input is realizable one time,
it is provably realizable across reorgs. However, that doesn't mean someone
couldn't interrupt U from being created. But generally, with Reorg + Double
spend, or Reorg > 100 blocks (potentially destroying CB reward), all bets
are off as to the replay-ability of transactions.
I want to also point out that this "revocation" property -- to the extent
it is something new that can't already be emulated with pre-signeds or RBF

@_date: 2020-02-14 12:07:15
@_author: Jeremy 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity (reflowed) 
I think your point:
*When schnorr and taproot are done together, all of the following
transaction types can be part of the same set:     - single-sig spends
(similar to current use of P2PKH and P2WPKH)     - n-of-n spends with musig
or equivalent (similar to current use of       P2SH and P2WSH 2-of-2
multisig without special features as used by       Blockstream Green and LN
mutual closes)     - k-of-n (for low values of n) using the most common k
signers       (similar to BitGo-style 2-of-3 where the keys involved are
    alice_hot, alice_cold, and bob_hot and almost all transactions are
  expected to be signed by {alice_hot, bob_hot}; that common case       can
be the key-path spend and the alternatives {alice_hot,       alice_cold}
and {alice_cold, bob_hot} can be script-path spends)     - contract
protocols that can sometimes result in all parties       agreeing on an
outcome (similar to LN mutual closes, cross-chain       atomic swaps, and
same-chain coinswaps) *
Is the same if Schnorr + Merkle Branch without Taproot optimization, unless
I'm missing something in one of the cases? I guess there's a distinction on
"can" v.s. "are likely"?
That's a really interesting point about K-N systems making the most likely
K-K the taproot key. (For the uninitiated, MuSig can do N-of-N aggregation
non-interactively, but K-of-N requires interaction). I think this works
with small (N choose K), but as (N choose K) increases it seems the
probability of picking the correct one goes down?
I guess the critical question is if cases where there's not some timelock
will be mandatory across all signing paths.
 On Mon, Feb 10, 2020 at 9:16 AM Jonas Nick via bitcoin-dev <

@_date: 2020-02-14 13:21:15
@_author: Jeremy 
@_subject: [bitcoin-dev] Taproot public NUMS optimization (Re: Taproot 
I am working on CTV, which has cases where it's plausible you'd want a
taproot tree with a NUMS point.
The need for NUMS points is a little bit annoying. There are a few reasons
you would want to use them instead of multisig:
1) Cheaper to verify/create.
If I have a protocol with 1000 people in it, if I add a multisig N of N to
verify I need a key for all those people, and the probability of use seems
I then also need to prove to each person in the tree that their key is
present. My memory on MuSig is a bit rusty, but I think they key
aggregation requires sending all the public keys and re-computing. (Maybe
you can compress this to O(log n) using a Merkle tree for the tweak L?)
Further, these keys can't just be the addresses provided for those 1000
people, as if those addresses are themselves N of Ns or scripts it gets
complicated, fast (and potentially broken). Instead we should ask that each
participant give us a list of keys to include in the top-level. We'd also
want each participant to provide
two signatures with that key of some piece of non-txn data (so as to prove
it itself wasn't a NUMS point -- otherwise may as well skip this all and
just use a top-level nums point).
2) Auditable.
If I set up an inheritance scheme, like an annuity or something, and the
IRS wants me to pay taxes on what I've received, adverse inference will
tell them to assume that my parent gave me a secret get all the money path
and this is a tax dodge. With a NUMS point, heirs can prove there was no
top-level N of N.
3) I simply don't want to spend it without a script condition, e.g.,
Now, assuming you do want a NUMS, there is basically 4 ways to make one
(that I could think of):
1) Public NUMS -- this is a constant, HashToCurve("I am a NUMS Point").
Anyone scanning the chain can see spends are using this constant. Hopefully
everyone uses the same constant (or everyone uses 2,3,4) so that "what type
of NUMS you are using" isn't a new fingerprint.
2) Moslty Public NUMS -- I take the hash of some public data (like maybe
the txid) on some well defined protocol, and use that. Anyone scanning the
chain and doing an EC operation per-txid can see I'm using a constant --
maybe my HashToCurve takes 10 seconds (perhaps through a VDF to make it
extra annoying for anyone who hasn't been sent the shortcut), but in
practice it's no better than 1.
3) Interactive NUMS -- I swap H(Rx), H(Ry) with the other participant and
then NUMS with H(Rx || Ry). This is essentially equivalent to using a MuSig
key setup where one person's key is a NUMS. Now no one passively scanning
can see that it's NUMS, but I can prove to an auditor later.
4) 1/2 RTT Async-Interactive NUMS -- I take some public salt -- say the
txid T, and hash it with a piece of random data R and then HashToCurve(T ||
R)... I think this is secure? Not clear the txid adds any security. Now I
can prove to you that the hash was based on the txid, but I've blinded it
with R to stop passive observers. But I also need ot send you data out of
band for R (but I already had to do this for Taproot maybe?)
The downsides with 3/4 is that if you lose your setup, you lose your
ability to spend/prove it's private (maybe can generate R from a seed?). So
better hold on to those tightly! Or use a public NUMS.
Only 3,4 provide any "real" privacy benefit and at a small hit to
likelihood of losing funds (more non-deterministic data to store). I guess
the question becomes how likely are we to have support for generating a
bunch of NUMS points?
Comparing with this proposal which removes the NUMS requirement:
1) NUMS/Taproot anonymity set *until* spend, MAST set after spend
2) No complexity around NUMS generation/storage
3) If people don't have ecosystem-wide consistent NUMS practices, leads to
additional privacy leak v.s. bare MAST which would be equivalent to case 1
(Public NUMS)
4) Slightly less chain overhead (32 bytes/8 vbytes).
5) Slightly faster chain validation (EC Point tweak is what like 10,000 -
100,000 times slower than a hash?)
Matt raises a interesting point in the other thread, which is that if we
put the option for a more private NUMS thing, someone will eventually write
software for it. But that seems to be irrespective of if we make no-NUMS an
option for bare MAST spends.
Overall I think this is a reasonable proposal. It effectively only
introduces bare MAST to prevent the case where people are using a few
different Public NUMS leaking metadata by putting incentive to use the same
one -- none. Using a private NUMS is unaffected incentive wise as it's
essentially just paying a bit more to be in the larger anonymity set. I
think it makes some class of users better off, and no one else worse off,
so this change seems Pareto.
Thus I'm in favor of adding a rule like this.
I think reasonable alternative responses to accepting this proposed change
would be to:
1) Add a BIP for a standard Public NUMS Point exported through secp256k1 to
head off people defining their own point.
2) Add a discounting rule if the point P is the Public NUMS that discounts
the extra weight somehow.
3) Take a Bit out of the leaf version portion of C[0] to denote Public NUMS
and then elide having to include the point (as it's just standard). This
has the benefit of not needing as much code-change as The Group's proposed
change, but the downside of still requiring an extra EC Mul in validation.
Rejecting the proposal is also, IMO, reasonable. On my personal
preferences, I'd rather get something like Taproot and MAST available
sooner than later, even if there are small quirks on privacy and cost, and
ignore a small benefit rule change/exception that would hold it up by more
than a month or two. I don't see why a small tweak would add substantial
delay, but I think other BIP authors/reviewers would be able to better
 On Sun, Feb 9, 2020 at 12:25 PM Bryan Bishop via bitcoin-dev <

@_date: 2020-02-26 11:56:09
@_author: Jeremy 
@_subject: [bitcoin-dev] Removing Single Point of Failure with Seed Phrase 
As a replacement for paper, something like this makes sense v.s. what you
do with a ledger presently.
However, shamir's shares notoriously have the issue that the key does exist
plaintext on a device at some point.
Non-interactive multisig has the benefit of being able to sign transactions
without having keys in the same room/place/device ever.
 On Wed, Feb 26, 2020 at 9:14 AM Contact Team via bitcoin-dev <

@_date: 2020-01-04 17:58:00
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_CTV Workshop & CFP February 1st, 2020 
Dear Bitcoin Developers,
On February 1st, 2020 in San Francisco (location to be shared with
attendees only) I will be hosting a workshop to aid in reviewing and
advancing OP_CHECKTEMPLATEVERIFY.
The workshop will be from 10am-5pm. The basic schedule of events (subject
to change) is in the footer of this email.
If you would like to attend, please fill out the form
 . We should have capacity for everyone
who wants to come, but I'll need to know by January 15th if you plan to
attend. The primary audience for the event is Bitcoin developers, ecosystem
engineers (i.e., mining pools, wallets, exchanges, etc), and researchers.
If you have research or projects related to OP_CTV you would be interested
in presenting, please indicate in the application form with a brief summary
of your topic.
I may be able to sponsor travel for a few developers who would otherwise be
unable to attend. Please indicate on the form if you require such support.
If you're able to sponsor the event (for lunch/dinner, or for travel
subsidies), please reach out or indicate on the form.
If you cannot attend, I'll make a best effort to make all materials from
the event available online. The channel  is also available
for general discussion about OP_CTV.
Happy New Year!
10:00 AM - 10:30 AM: coffee & registration
BIP SESSION
10:30 AM - 11:00 AM: CTV BIP Design Walkthrough & Basic Motivation
11:00 AM - 11:30 AM: Small Group Discussion & BIP Reading
11:30 AM - 12:00 PM: BIP Q&A
12pm: Lunch
IMPLEMENTATION SESSION
1:00 PM - 1:30 PM: BIP Implementation Walkthrough
1:30 PM - 2:00 PM: Q&A + silent review implementation review time
DEPLOYMENT SESSION
2:00 PM - 2:15 PM: Deployment Plan Proposal
2:15 PM - 2:45 PM: Deployment Plan Discussion
2:45-3pm: BREAK
ECOSYSTEM SUPPORT SESSION
3:00 PM - 3:30 PM: Mempool Updates Presentation & Discussion
3:30 PM - 4:00 PM: Package Relay Informational Updates
DEMO SESSION & APPLICATION TALKS
4:00 PM - 4:10 PM: SENDMANYCOMPACTED Demo
4:10 PM - 4:20 PM: Vault Wallet Demo
4:20 PM: - 4:30 PM: TBA
4:30 PM - 4:40PM: TBA
4:40 PM - 4:50 PM: TBA
WRAP UP
4:50 PM - 5:00 PM
5:00 PM - 7:00 PM Dinner & Drinks

@_date: 2020-01-10 16:54:06
@_author: Jeremy 
@_subject: [bitcoin-dev] Modern Soft Fork Activation 
It's not at a "directly implementable policy state", but I think you might
be interested in checking out the spork protocol upgrade model I proposed a
while back. It has some interesting properties around the 5 properties you've mentioned.
1) Avoid activating in the face of significant, reasonable, and directed
objection. Period.
Up to miners to orphan spork-activating blocks.
2) Avoid activating within a timeframe which does not make high
node-level-adoption likely.
Mandatory minimum flag day for Spork initiation, statistically
improbable/impossible for even earlier adoption.
3) Don't (needlessly) lose hashpower to un-upgraded miners.
Difficulty adjustments make the missing spork'd block "go away" over time,
the additional difficulty of *not activating* a rejected spork fills in as
an additional PoW.
4) Use hashpower enforcement to de-risk the upgrade process, wherever
Miners choose to activate or build on activating blocks.
5) Follow the will of the community, irrespective of individuals or
unreasoned objection, but without ever overruling any reasonable
Honest signalling makes people be forced to "put their money where there
mouth is" on what the community wants.

@_date: 2020-01-13 17:05:21
@_author: Jeremy 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Yes, you should check out the material at the link above. Specifically non
interactive channels solve this problem of one sided opens, where the other
party is passive/offline.
On Mon, Jan 13, 2020, 12:42 PM Joachim Str?mbergson via bitcoin-dev <

@_date: 2020-01-17 19:58:27
@_author: Jeremy 
@_subject: [bitcoin-dev] OP_CTV Workshop & CFP February 1st, 2020 
It's not too late to sign up  to attend the workshop; but we are
approaching capacity!
Please fill out the form if you'd like to participate as soon as possible
so that we can plan accordingly.
Feel free to forward this posting to people who don't follow this list but
you think should attend.

@_date: 2020-01-26 09:23:57
@_author: Jeremy 
@_subject: [bitcoin-dev] op_checktemplateverify and number of inputs 
Hi Billy,
Restricting the number of inputs is necessary to preclude TXID
malleability. Committing to all of the information required necessitates
that the number of inputs be committed.
This allows us to build non-interactive layer 2 protocols which depend on
TXID non-malleability (most of them at writing).
You raise a good point that allowing *any number* of inputs is an
interesting case, which I had discussed offline with a few different
people. I think the conclusion was that that flexibility is better left
outside of the OP directly.
If you want an any number of inputs template, and we enable something like
OP_CAT (e.g., OP_CAT, OP_SHA256STREAM) then you can spend to something like:
 OP_SWAP OP_CAT OP_SWAP OP_CAT  OP_CAT OP_SHA256 OP_CTV
And then pass in the # of inputs and sequences hash as arguments to the
I can respond separately to your bitcointalk post as you ask a different
set of questions there.
 On Sun, Jan 26, 2020 at 8:59 AM Billy via bitcoin-dev <

@_date: 2020-06-07 15:45:16
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Hi Joachim,
Fantastic questions!
I think it makes sense to think about it in terms of today, and then in
terms of a long-dated future where wallets have much richer native
understandings of these things. This helps preserve the purity of the
arguments I'm making with respect to what it would look like today v.s.
what it could look like with strong integration.
1) I would expect that exchanges do this as a CTV txn that is one initial
confirmation to a single output, and then that output expands to either all
the payments in the batch, or to a histogram of single-layer CTVs based on
priority/amount being spent. E.g, either A -> B -> {C,D,E,F,G...} or
A->B->{C -> {D,E,F}, G -> {H, I J}, K -> ....}. I would further expect that
the entire tree would include fees such that it will get into at least the
bottom of the mempool. See  for
more info. If txns land in the mempool, then users learn about it (even
with an un-updated wallet) just like the learn of normal unconfirmed
transactions. Even this simple two-step transaction can deliver massive
batching savings. OpTech has some coverage of this simple
commit-now-distribute-later scheme here
I'd also expect that exchanges in particular already store their outbound
transactions in resilient storage (for audit and compliance as well as
liability protection), so they would likely be able to make this data
available to their customers on inquiry if discarded.
I'm all for redundancy, so exchanges can also e.g. send an email with a
backup file if they want to. But that's not necessary for it to work today,
you can just watch the mempool like wallets already do.
A slightly patched wallet can treat CTV outs as more confirmed (e.g., like
an own-change address) than a normal unconfirmed out.
2) I would expect that exchanges pay a reasonable amount of fees for the
transaction so it can expect to at least get to the bottom range of the
mempool for children, and top of the mempool for the parent. Your question
seems to be more about after this phase.
First I would note that it is truly O(log(N)), but E[O(1)], because it
amortizes. That is, to claim out all of the outputs is a total overhead of
O(N), not O(N log N). Fees in this model are paid by CPFP. Because CPFP is
currently *Child* pays for parent and not *Children* pay for parent, we
don't (unfortunately) have rational txn selection for this case. Any wallet
can construct this spend path by rebroadcasting (if evicted) the parents
and spending the txn. The exchange can also 'bound' themselves to seeing a
transaction to completion by including some change address at the leaf node
layer (not much overhead depending on radix).
Thus the payer of fees is the person who needs to spend.
3) Not exactly, the middle txns are immutable. but it may be possible to
construct a low-fee longchain which can cause transaction pinning. If you
do a shallow tree as described in (1), the current lightning carve should
help to prevent this.
1) Most likely the desirable radix for a tree is something like 4 or 5
which minimizes the amount of work on an individual basis (you can compute
this by figuring out how deep the tree would be and the per-tx overheads, 4
or 5 pop out as being minimal overhead and the benefit is recursive).
Mempool broadcast still should work, but it's possible that for privacy
reasons it's preferred to not broadcast through mempool. It's also possible
that all payouts are into non-interactive lightning channels with N-of-N
taproot at each layer, so you receive a proof through your lightning wallet
and can immediately route payments, and when you want to close
opportunistically cooperate to reduce chain overhead. You can think of CTV
as an anchor for bootstrapping these layer two protocols with an on-chain
bisection algorithm to discover online participants to re-negotiate with. A
privacy and scalability win!
I further expect business wallets (like exchanges) to be able to credit
deposits from CTV trees without requiring full expansion. This is also a
privacy win, and can decrease latency of moving large value funds (e.g.,
exceeding inter exchange channel balances) and crediting funds for trading.
2) I think we'll eventually converge on a non-destructive way of adding
fees. RBF is destructive in that you're replacing a TX. CPFP is destructive
in that you have a spend a coin to drive progress. Without a new opcode you
can emulate this with CTV by at nodes in the tree having a consumable
output that serves as a CPFP hook/a RBF hook. You can see some discussion
here (animated, so use pres mode)
This adds some extra chain weight, but is possible without further
extension. What I think we'll eventually land on is a way of doing a tx
that contributes fee to another tx chain as a passive observer to them.
While this breaks one abstraction around how dependencies between
transactions are processed, it also could help resolve some really
difficult challenges we face with application-DoS (pinning and other
attacks) in the mempool beyond CTV. I have a napkin design for how this
could work, but nothing quite ready to share yet.
3) Hopefully 2 solves pinning :)
 On Sun, Jun 7, 2020 at 9:51 AM Joachim Str?mbergson <

@_date: 2020-06-07 23:43:39
@_author: Jeremy 
@_subject: [bitcoin-dev] [was BIP OP_CHECKTEMPLATEVERIFY] Fee Bumping Operation 
Broke out to a separate thread.
At core, the reason why this method *might* work is that it's essentially
just CPFP but we can guarantee that the link we're examining is always
exactly one hop away, so we get rid of most of the CPFP graph traversal
Your description largely matches my thinking for how something like this
could work (pay for neighbor). The issue is that the extant CPFP logic is
somewhat brittle and doesn't work as expected (Child not Children, which is
problematic for multiple PFN's).
already confirmed, so the miners could have more fees than strictly
necessary. But this is the same as with CPFP.
This is problematic and can't be done as it requires a new index of all
past txns for consensus.
My thinking is that a Fee Bump transaction can name a list of TXIDs (Or one
TXID which implies all ancestors of) that it wishes to be included in a
block with. It must be included in that block. A Fee Bump transaction may
have no unconfirmed ancestors nor any children. Potentially, it also may
not be RBF'd. You treat the Fee Bump Transactions as the lowest descendant
of whatever it targets and then set it's feerate/total fee based on the
package that would have to co-confirm for it to be worth mining. This makes
it sort like normal transactions for inclusion. You can require some
minimums for mempool inclusion at all.
If it's target is confirmed or replaced, it should drop from the mempool.
Transactions in the mempool may set a flag that opts out of CPFP for
descendants/blocks any descendants. Channel protocols should set this bit
to prevent pinning, and then use the Fee Bump to add fees to whatever txns
need to go through. If done right you can also layer a coinswap protocol
with the fee-bumping txns change so that you are getting a privacy benefit
at the same time.
BTW the annex *could* be used for this purpose, but it would also be
acceptable to have it be in some kind of anyone can spend output. Then it
would just be a anyone-can-spend tx with OP_CHECK_TXID_IN_BLOCK (or
OP_CHECK_UTXO_SPENT_IN_BLOCK), and a miner could claim all such outputs at
the end of the block. This is worse in terms of on-chain overheads, but
nice in that it's the minimal semantic change & introduces some general
purpose functionality.
But my thoughts are still pretty loose at the moment around it. I suspect
that to make fee bumping work nicely would require removing CPFP entirely,
but I don't know that to be the case concretely.

@_date: 2020-06-11 10:21:08
@_author: Jeremy 
@_subject: [bitcoin-dev] CoinPool, 
Stellar work Antoine and Gleb! Really excited to see designs come out on
payment pools.
I've also been designing some payment pools (I have some not ready code I
can share with you guys off list), and I wanted to share what I learned
here in case it's useful.
In my design of payment pools, I don't think the following requirement: "A
CoinPool must satisfy the following *non-interactive any-order withdrawal*
property: at any point in time and any possible sequence of previous
CoinPool events, a participant should be able to move their funds from the
CoinPool to any address the participant wants without cooperation with
other CoinPool members." is desirable in O(1) space. I think it's much
better to set the requirement to O(log(n)), and this isn't just because of
wanting to use CTV, although it does help.
Let me describe a quick CTV based payment pool:
Build a payment pool for N users as N/2 channels between participants
created in a payment tree with a radix of R, where every node has a
multisig path for being used as a multi-party channel and the CTV branch
has a preset timeout. E.g., with radix 2:
                                      Channel(a,b,c,d,e,f,g,h)
                                     /                                   \
               Channel(a,b,c,d)
                    /
\                                                    /                 \
Channel(a,b)    Channel(c,d)                          Channel(e,f)
All of these channels can be constructed and set up non-interatively using
CTV, and updated interactively. By default payments can happen with minimal
coordination of parties by standard lightning channel updates at the leaf
nodes, and channels can be rebalanced at higher layers with more
Now let's compare the first-person exit non cooperative scenario across
Wait time: Log(N). At each branch, you must wait for a timeout, and you
have to go through log N to make sure there are no updated states. You can
trade off wait time/fees by picking different radixes.
TXN Size: Log(N) 1000 people with radix 4 --> 5 wait periods. 5*4 txn size.
Radix 20 --> 2 wait periods. 2*20 txn size.
Wait Time: O(1)
TXN Size: Depending on accumulator: O(1), O(log N), O(N) bits. Let's be
favorable to Accumulators and assumer O(1), but keep in mind constant may
be somewhat large/operations might be expensive in validation for updates.
This *seems* like a clear win for Accumulators. But not so fast. Let's look
at the case where *everyone* exits non cooperatively from a payment pool.
What is the total work and time?
CTV Pool:
Wait time: Log(N)
Txn Size: O(N) (no worse than 2x factor overhead with radix 2, higher
radixes dramatically less overhead)
Accumulator Pool:
Wait time: O(N)
Txn Size: O(N) (bear in mind *maybe* O(N^2) or O(N log N) if we use an
sub-optimal accumulator, or validation work may be expensive depending on
the new primitive)
So in this context, CTV Pool has a clear benefit. The last recipient can
always clear in Log(N) time whereas in the accumulator pool, the last
recipient has to wait much much longer. There's no asymptotic difference in
Tx Size, but I suspect that CTV is at least as good or cheaper since it's
just one tx hash and doesn't depend on implementation.
Another property that is nice about the CTV pool style is the bisecting
property. Every time you have to do an uncooperative withdrawal, you split
the group into R groups. If your group is not cooperating because one
person is permanently offline, then Accumulator pools *guarantee* you need
to go through a full on-chain redemption. Not so with a CTV-style pool, as
if you have a single failure among [1,2,3,4,5,6,7,8,9,10] channels (let's
say channel 8 fails), then with a radix 4 setup your next steps are:
[1,2,3,4] [5,6,7,X] [9,10]
[1,2,3,4] 5 6 7 X [9,10]
So you only need to do Log(N) chain work to exit the bad actor, but then it
amortizes! A future failure (let's say of 5) only causes 5 to have to close
their channel, and does not affect anyone else.
With an accumulator based pool, if you re-pool after one failure, a second
failure causes another O(N) work. So then total work in that case is
O(N^2). You can improve the design by making the evict in any order option
such that you can *kick out* a member in any order, that helps solve some
of this nastiness (rather than them opting to leave). But I'm unclear how
to make this safe w.r.t. updated states. You could also allow, perhaps, any
number of operators to simultaneously leave in a tx. Also not sure how to
do that.
With CTV Pools, you can make a payment if just your immediate conterparty
is online in your channel. Opportunistically, if people above you are
online, you can make channel updates higher up in the tree which have
better timeout properties. You can also create new channels, binding
yourself to different parties if there is a planned exit.
With Accumulator pools, you need all parties online to make payments.
Cooperation Case:
CTV Pools and Accumulator pools, in a cooperative case, both just act like
a N of N multisig.
Because Accumulator pools always require N signers, it's possible to build
a better privacy model where N parties are essentially managing a chaumian
ecash like server for updates, giving good privacy of who initiated
payments. You *could* do this with CTV pools as well, but I expect users to
prefer making updates at the 2 party channel layer for low latency, and to
get privacy benefits out of the routability of the channels and ability to
connect to the broader lightning network.
Technical Complexity:
Both protocols require new features in Bitcoin. CTV is relatively simple, I
would posit that accumulators + sighashnoinput are relatively not simple.
The actual protocol design for CTV pools is pretty simple and can be
compatible with LN, I already have a rudimentary implementation of the
required transactions (but not servers).
In both designs, the payment pool can be created non-interactively. This is
*super* important as it means third parties (e.g., an exchange) can
withdraw users funds *into* a payment pool.
Thanks for reading!

@_date: 2020-04-30 23:57:09
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP-341: Committing to all scriptPubKeys in the 
Hi Andrew,
If you use SIGHASH_ALL it shall sign the COutPoints of all inputs which
commit to the scriptPubKeys of the txn.
Thus the 341 hash doesn't need to sign any additional data.
As a metadata protocol you can provide all input transactions to check the
 On Thu, Apr 30, 2020 at 1:22 AM Andrew Kozlik via bitcoin-dev <

@_date: 2020-05-01 21:35:41
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP-341: Committing to all scriptPubKeys in the 
At the end of the day I don't really care that much I just prefer something
that doesn't throw taproot in for another review cycle.
A side effect of this proposal is it would seem to make it not possible to
produce a signature for a transaction without having access to the inputs.
This is limiting for a number of cases where you don't care about that
data. There are a litany of use cases where you don't want to have
SIGHASH_ALL behavior, and having to sign the scriptpubkeys breaks that. So
at the very least it should respect other flags.
I also don't really understand the exact attack. So you submit a
transaction to the wallet asking them to sign input 10. They sign. They've
committed to the signature being bound to the specific COutpoint and input
index, so I don't see how they wouldn't be required to sign a second
signature with the other output too? Is there an attack you can describe
end-to-end relying on this behavior?
If you look at the TXID hash the vouts are one of the last fields
serialized. this makes it possible (at least, I think) to do a midstate
proof so that all you are providing is the hash midstate, and the relevant
transaction output,  the siblings after, and the locktime. So you get to
skip all the input data, the witness data, and most of the output data.
This sort of data can easily go into the proprietary use (maybe becoming
well defined if there's a standardization push) area in PSBT, so that
hardware devices can get easy access to it. All they have to do to verify
is to finalize the hash against that buffer and match to the correct input.
As an alternative proposal, I think you can just make a separate BIP for
some new sigash flags that can be reviewed separately from taproot. There's
a lot of value in investing in figuring out more granular controls over
what the signature hash is you sign, which may have some exciting
contracting implications!
 On Fri, May 1, 2020 at 5:26 AM Greg Sanders via bitcoin-dev <

@_date: 2020-09-02 11:27:00
@_author: Jeremy 
@_subject: [bitcoin-dev] reviving op_difficulty 
Yep this is a good example construction. I'd also point out that modulo a
privacy improvement, you can also script it as something like:
IF   IF  CLTV B DROP CHECKSIG ELSE  CLTV DROP A CHECKSIG ENDIF ELSE
2 A B 2 CHECKMULTI ENDIF
This way you equivalently have cooperative closing / early closing
positions, but you make the redeem script non-interactive to setup which
enable someone to pay into one of these contracts without doing
pre-signeds. This is unfortunate for privacy as the script is then visible,
but in a taproot world it's fine.
Of course the non interactivity goes away if you want non-binary outcomes
(e.g., Alice gets 1.5 Coin and Bob gets .5 Coin in case A, Bob gets 1.5
Coin Alice gets .5 coin in Case B).
And it's also possible to mix relative and absolute time locks for some
added fun behavior (e.g., you win if > Time and > Blocks)
A while back I put together some python code which handles these embedded
in basic channels between two parties (no routing). This enables you to
high-frequency update and model a hashrate perpetual swap, assuming your
counterparty is online.
The general issue with this construction family is that the contracts are
metastable. E.g., if you're targeting a 100 block deficit , that means you
have 100 blocks of time to claim the funds before either party can win. So
there's some minimum times and hashrate moves to play with, and the less
"clearly correct" you were, the less clearly correct the execution will be.
This makes the channel version of the contract compelling as you can update
and revoke frequently on further out contracts.
 On Sat, Aug 22, 2020 at 9:47 AM David A. Harding via bitcoin-dev <

@_date: 2020-09-03 10:34:15
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
CTV does not enable this afaiu because it does not commit to the inputs
(otherwise there's a hash cycle for predicting the output's TXID.

@_date: 2020-09-03 10:47:35
@_author: Jeremy 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
It's also not something that's trivial to set up in any scheme because you
have to have an ordering around when you set up the tx intended to be the
inverse lock before you create the tx using it.

@_date: 2020-09-18 17:51:39
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive TXID 
Hi Bitcoin Devs,
I'd like to share with you a draft proposal for a mechanism to replace
CPFP and RBF for
increasing fees on transactions in the mempool that should be more
robust against attacks.
A reference implementation demonstrating these rules is available
for those who
prefer to not read specs.
Should the mailing list formatting be bungled, it is also available as
a gist [here](
Non-Destructive TXID Dependencies for Fee Sponsoring
This BIP proposes a general purpose mechanism for expressing
non-destructive (i.e., not requiring
the spending of a coin) dependencies on specific transactions being in
the same block that can be
used to sponsor fees of remote transactions.
The mempool has a variety of protections and guards in place to ensure
that miners are economic and
to protect the network from denial of service.
The rough surface of these policies has some unintended consequences
for second layer protocol
developers. Applications are either vulnerable to attacks (such as
transaction pinning) or must go
through great amounts of careful protocol engineering to guard against
known mempool attacks.
This is insufficient because if new attacks are found, there is
limited ability to deploy fixes for
them against deployed contract instances (such as open lightning
channels). What is required is a
fully abstracted primitive that requires no special structure from an
underlying transaction in
order to increase fees to confirm the transactions.
Consensus Specification
If a transaction's last output's scripPubKey is of the form OP_VER
followed by n*32 bytes, where
n>1, it is interpreted as a vector of TXIDs (Sponsor Vector). The
Sponsor Vector TXIDs  must also be
in the block the transaction is validated in, with no restriction on
order or on specifying a TXID
more than once. This can be accomplished simply with the following patch:
+    // Extract all required fee dependencies
+    std::unordered_set dependencies;
+    const bool dependencies_enabled = VersionBitsState(pindex->pprev,
versionbitscache) == ThresholdState::ACTIVE;
+    if (dependencies_enabled) {
+        for (const auto& tx : block.vtx) {
+            // dependency output is if the last output of a txn is
OP_VER followed by a sequence of 32*n
+            // bytes
+            // vout.back() must exist because it is checked in CheckBlock
+            const CScript& dependencies_script = tx->vout.back().scriptPubKey;
+            // empty scripts are valid, so be sure we have at least one byte
+            if (dependencies_script.size() && dependencies_script[0]
== OP_VER) {
+                const size_t size = dependencies_script.size() - 1;
+                if (size % 32 == 0 && size > 0) {
+                    for (auto start = dependencies_script.begin() +1,
stop = start + 32; start < dependencies_script.end(); start = stop,
stop += 32) {
+                        uint256 txid;
+                        std::copy(start, stop, txid.begin());
+                        dependencies.emplace(txid);
+                    }
+                }
+                // No rules applied otherwise, open for future upgrades
+            }
+        }
+        if (dependencies.size() > block.vtx.size()) {
+            return
+        }
+    }
     for (unsigned int i = 0; i < block.vtx.size(); i++)
     {
         const CTransaction &tx = *(block.vtx[i]);
+        if (!dependencies.empty()) {
+            dependencies.erase(tx.GetHash());
+        }
         nInputs += tx.vin.size();
 -2190,6 +2308,9  bool CChainState::ConnectBlock(const CBlock&
block, BlockValidationState& state,
         }
         UpdateCoins(tx, view, i == 0 ? undoDummy :
blockundo.vtxundo.back(), pindex->nHeight);
     }
+    if (!dependencies.empty()) {
+        return state.Invalid(BlockValidationResult::BLOCK_CONSENSUS,
+    }
 Design Motivation
The final output of a transaction is an unambiguous location to attach
metadata to a transaction
such that the data is available for transaction validation. This data
could be committed to anywhere,
with added implementation complexity, or in the case of Taproot
annexes, incompatibility with
non-Taproot addresses (although this is not a concern for sponsoring a
transaction that does not use
A bare scriptPubKey prefixed with OP_VER is defined to be invalid in
any context, and is trivially
provably unspendable and therefore pruneable.
If there is another convenient place to put the TXID vector, that's fine too.
As the output type is non-standard, unupgraded nodes will by default
not include Transactions
containing them in the mempool, limiting risk of an upgrade via this mechanism.
Policy Specification
The mechanism proposed above is a general specification for
inter-transaction dependencies.
In this BIP, we only care to ensure a subset of behavior sufficient to
replace CPFP and RBF for fee
Thus we restrict the mempool policy such that:
1. No Transaction with a Sponsor Vector may have any child spends; and
1. No Transaction with a Sponsor Vector may have any unconfirmed parents; and
1. The Sponsor Vector must have exactly 1 entry; and
1. The Sponsor Vector's entry must be present in the mempool; and
1. Every Transaction may have exactly 1 sponsor in the mempool; except
1. Transactions with a Sponsor Vector may not be sponsored.
The mempool treats ancestors and descendants limits as follows:
1. Sponsors are counted as children transactions for descendants; but
1. Sponsoring transactions are exempted from any limits saturated at
the time of submission.
This ensures that within a given package, every child transaction may
have a sponsor, but that the
mempool prefers to not accept new true children while there are
parents that can be cleared.
To prevent garbage sponsors, we also require that:
1. The Sponsor's feerate must be greater than the Sponsored's ancestor fee rate
We allow one Sponsor to replace another subject to normal replacement
policies, they are treated as
 Design Motivation
There are a few other ways to use OP_VER sponsors that are not
included. For instance, one could
make child chains that are only valid if their parent is in the same
block (this is incompatible
with CTV, exercise left to reader). These use cases are in a sense
incidental to the motivation
of this mechanism, and add a lot of implementation complexity.
What is wanted is a minimal mechanism that allows arbitrary
unconnected third parties to attach
fees to an arbitrary transaction. The set of rules given tightly
bounds how much extra work the
mempool might have to do to account for the new sponsors in the worst
case, while providing a "it
always works" API for end users that is not subject to traditional
issues around pinning.
Eventually, rational miners may wish to permit multiple sponsor
targets, or multiple sponsoring
transactions, but they are not required for the mechanism to work.
This is a benefit of the
minimality of the consensus rule, it is compatible with future policy
should it be implemented.
 Attack Analysis of new Policy
In the worst case the new policy can lead to a 1/2 reduction in the
number of children allowed
(e.g., if there are 13 children submitted, then 12 sponsors, the 25
child limit will saturate
before) and a 2x increase in the maximum children (e.g., if there are
25 children submitted, and
then each are sponsored). Importantly, even in the latter attack
scenario, the DoS surface is not
great because the sponsor transactions have no children nor parents.
 Package Relay/Orphan Pool
Future policy work might be able to insert sponsors into a special
sponsor pool with an eviction
policy that would enable sponsors to be queried and tracked for
transactions that have too low fee
to enter the mempool in the first place. This is treated as a separate
concern, as any strides on
package relay generally should be able to support sponsors trivially.
Reference Implementation
A reference implementation demonstrating these rules is available
This is a best
effort implementation, but has not been carefully audited for
correctness and likely diverges from
this document in ways that should either be reflected in this document
or amended in the code.

@_date: 2020-09-19 09:16:25
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Hi Cory!
Thanks for taking a look. CC nopara as I think your questions are the same.
I think there are a few reason we won't see functionally worse privacy:
1. RBF/CPFP may require the use of an external to the original transaction
to pay sufficient fee.
2. RBF/CPFP may leak which address was the change and which was the payment.
In addition, I think there is a benefit in that:
1. RBF/CPFP requires access to the keys in the same 'security zone' as the
payment you made (e.g., if it's a multi-sig to multi-sig requires m of N to
cpfp/or RBF, whereas sponsors could be anyone).
2. Sponsors can be a fully separate arbitrary wallet.
3. You can continually coinjoin the funds in your fee-paying wallet without
tainting your main funds.
4. You can keep those funds in a lightning channel and pay your fees via
loop outs.

@_date: 2020-09-19 09:30:56
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Hi David!
Thanks for taking a look, and great question.
It is indeed in the reference implementation. Please see
There is no requirement that there be any input in common, just that the
sponsor vectors are identical (keep in mind that we limit our sponsor
vector by policy to 1 element, because, as you rightfully point out,
multiple sponsors is more complex to implement).
Yup, I was aware of this limitation but I'm not sure how practical it is as
an attack because it's quite expensive for the attacker. But there are a
few simple policies that can eliminate it:
1) A Sponsoring TX never needs to be more than, say, 2 inputs and 2
outputs. Restricting this via policy would help, or more flexibly limiting
the total size of a sponsoring paying transaction to 1000 bytes.
2) Make A Sponsoring TX not need to pay more absolute fee, just needs to
increase the feerate (perhaps with a constant relay fee bump to prevent
I think 1) is simpler and should allow full use of the sponsor mechanism
while preventing this class of issue mostly.
What do you think?

@_date: 2020-09-19 12:46:25
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Yes I think you're a bit confused on where the actual sponsor vector is. If
you have a transaction chain A->B->C and a sponsor S_A, S_A commits to txid
A and A is unaware of S.
W.r.t your other points, I fully agree that the 1-to-N sponsored case is
very compelling. The consensus rules are clear that sponsor commitments are
non-rival, so there's no issue with allowing as many sponsors as possible
and including them in aggregate. E.g., if S_A and S'_A both sponsor A with
feerate(S*) > feerate(A), there's no reason not to include all of them in a
block. The only issue is denial of service in the mempool. In the future,
it would definitely be desirable to figure out rules that allow mempools to
track both multiple sponsors and multiple sponsor targets. But in the
interest of KISS, the current policy rules are designed to be minimally
invasive and maximally functional.
In terms of location for the sponsor vector, I'm relatively indifferent.
The annex is a possible location, but it's a bit odd as we really only need
to allow one such vector per tx, not one per input, and one per input would
enable some new use cases (maybe good, maybe bad). Further, being in the
witness space would mean that if two parties create a 2 input transaction
with a desired sponsor vector they would both need to specify it as you
can't sign another input's witness data. I wholeheartedly agree with the
sentiment though; there could be a more efficient place to put this data,
but nothing jumps out to me as both efficient and simple in implementation
(a new tx-level field sounds like a lot of complexity).
yes, this has been fixed in the gist (cred to Dmitry Petukhov for pointing
it out first), but is correct in the code. Thank you for your careful

@_date: 2020-09-21 09:27:09
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Responses Inline:
Would it make sense that, instead of sponsor vectors
*This seems like a fine suggestion and I think addresses Antoine's issue.*
*I think there are likely some cases where you do want TXID and not Output
(e.g., if you *
*are sponsoring a payment to your locktime'd cold storage wallet (no CPFP)
from an untrusted third party (no RBF), they can grift you into paying for
an unrelated payment). This isn't a concern when the root utxo is multisig
& you are a participant.*
*The serialization to support both, while slightly more complicated, can be
done in a manner that permits future extensibility as well if there are
other modes people require.*
*I think it's important to keep in mind this is not a rival to package
relay; I think you also want package relay in addition to this, as they
solve different but related problems.*
*Where you might be able to simplify package relay with sponsors is by
doing a sponsor-only package relay, which is always limited to 2
transactions, 1 sponsor, 1 sponsoree. This would not have some of the
challenges with arbitrary-package package-relay, and would (at least from a
ux perspective) allow users to successfully get parents with insufficient
fee into the mempool.*

@_date: 2020-09-23 15:10:22
@_author: Jeremy 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Hi Suhas,
Thanks for your thoughtful response!
Overall I'll boil down my thoughts to the following:
If we can eventually come up with something clever at the user+policy layer
to emulate a sponsor like mechanism, I would still greatly prefer to expose
that sort of functionality directly and in a fully-abstracted usable way
for the minimum amount of mempool attack risk in 2nd layer protocols, even
at the expense of some base layer complexity. It's better to pay a security
sensitive engineering cost once, than to have to pay it repeatedly and
perhaps insufficiently.
Specific responses inline below:
validated in, with no restriction on order or on specifying a TXID more
than once.
sponsor, the sponsor is no longer valid.  This breaks a design principle
that has been discussed many times over the years, which is that once a
valid transaction is created, it should not become invalid later on unless
the inputs are double-spent.  This principle has some logical consequences
that we've come to accept, such as transaction chains being valid across
small reorgs in the absence of malicious (double-spend) behavior.
*Certainly, this property is strictly broken by this proposal. It does not
break the weaker property that the transactions can be reorged onto another
chain, however (like OP_GETBLOCKHASH or similar would), which is important
to note. It's also important to note this property is not preserved against
reorgs longer than 100 blocks.*
high bar for doing away with it.  And it seems to me that this proposal
doesn't clear that bar -- the fee bumping improvement that this proposal
aims at is really coming from the policy change, rather than the consensus
*I think this is possibly correct.*
*IMO the ability to implement the policy changes is purely derived from the
consensus changes. The consensus changes add a way of third parties to a
transaction to specify economic interest in the resolution of a
transaction. This requires a consensus change to work generically and
without forethought.*
*It's possible that with specific planning or opt-in, you can make
something roughly equivalent. But such a design might also consume more
bandwidth on-chain as you would likely have to e.g. always include a CPFP
hook output.*
problems, we could instead just propose new policy rules for the existing
types of transaction chaining that we have, rather than couple them to a
new transaction type.
3rd parties to participate in fee bumping.  But that behavior strikes me as
also problematic, because it introduces the possibility of 3rd party
griefing, to the extent that sponsor transactions in any way limit chains
of transactions that would be otherwise permitted.  If Alice sends Bob some
coins, and Alice and Bob are both honest and cooperating, Mallory shouldn't
be able to interfere with their low-feerate transaction by (eg) pinning it
with a large transaction that "sponsors" it (ie a large transaction that is
just above the feerate of the parent, which prevents additional child
transactions and makes it more expensive to RBF).
*It's possible to modify my implementation of the policy such that there is
no ability to interfere with the otherwise permitted limits, it just
requires a little bit more work to always discount sponsors on the
descendant counting.*
*W.r.t. griefing, the proposed amendment to limit sponsors to 1000 bytes
minimizes this concern. Further, pinning in this context is mainly an issue
if Alice and Bob are intending to RBF a transaction, at a policy level we
could make Sponsoring require that the transaction be RBF opted-out (or
sponsor opted in). *
requiring that a sponsor transaction bring the effective feerate of its
package up to something which should be confirmed soon (rather than just
being a higher feerate than the tx it is sponsoring).  However, we could
also carve out a policy rule just like that today, without any consensus
changes needed, to help with pinning (which is probably a good idea!  I
think this would be useful work).  So I don't think that approaches in that
direction would be unique to this proposal.
*I agree this is useful work and something that Ranked indexes would help
with if I understand them correctly, and can be worked on independently of
Sponsors. Overall I am skeptical that we want to accept any child if it
puts something into an upper percentile as we still need to mind our DoS
budgets (which the sponsors implementation keeps a tight bound on). *
policies, they are treated as conflicts.
seems problematic; that means that if Alice is paying Bob in a transaction
that is also sponsoring some other transaction (perhaps from Alice to
someone else), then Mallory can cause the transaction going to Bob to
become invalid by RBF bumping it and sponsoring the parent transaction
herself?  Allowing 3rd parties to interfere with transactions between
others seems like a complex and undesirable design to introduce.
*If you'll note in the BIP draft text and implementation recursive
sponsoring is not permitted by policy for this reason. Sponsors may not
sponsor a transaction that is sponsoring another transaction, and a further
restriction that sponsors may not have any children.*
policy rules along with a consensus change to be worked out to get right; I
think we could achieve largely the same effect by improving the current
policy rules to make CPFP work better without a consensus change.  And
while what is unique about this proposal is that it allows for 3rd parties
to attach themselves to the transaction graph of other parties, I think
that is a complex interaction to introduce and has negative side effects as
*This is where I most significantly disagree. Some thoughts below on why a
consensus change is likely required for this and why the sponsors mechanism
may not be that invasive (if done right) on the way we currently understand
the transaction graph.*
*The main issue with CPFP like mechanisms is that they require an
omniscient like behavior around how to coordinate.*
*Given the complexity of CPFP, it's not possible to ever abstract its use
from the contract protocol you are implementing. It always requires deep
integration into any protocol as a mechanism, and many bespoke protocols
for game theoretically ensuring you can pay fees is much more brittle than
one higher-order composable mechanism (which is what sponsors aims to
achieve, but may fall short on in current incarnation).*
*Further, CPFP based protocols can be wasteful, requiring at least one CPFP
hook/anchor output per participant always. These CPFP hooks need a mempool
CPFP exemption (so that you don't get pinned by a sibling), which will have
to apply recursively in case your payment protocol is not at the base of a
transaction chain (as can happen within a multiparty channel factory). Thus
CPFP as a mechanism inherently suffers from chain bloat issues and it
composes poorly when used recursively.*
*I think it will be essentially impossible to generically handle CPFP based
on transaction graph anchors both from a protocol implementers side and
from a mempool policy side. And in the event that a new attack is
discovered, a mechanism that works with fewer assumptions about the setup
of your protocol should be more robust, or at least fixable. Whereas with a
pure CPFP transaction graph based design, once you are pinned, it may be
impossible to externally correct the incentives. *
*Third parties can already -- trustfully -- insert themselves into
another's transaction chain by bribing mining pools to escalate priority of
a transaction. These out-of-band fees are somewhat inevitable, so if your
protocol is not robust against 3rd party feerate boosting you may have
larger issues.*
*Because we already do not handle 100 block reorgs with the replayability
property, one fine change to the BIP would be to enforce a 100 block
maturing period on sponsor transactions outputs after confirmation. This
might make usage a little bit more unwieldy, but then it would not have the
issue on small reorg validity.* *I believe this could be safely done* *only
via policy and not consensus, as if someone wants to double spend a
transaction in a reorg they can, but it's fine either way.*
*It's not particularly important that a transaction be in the same block
once sponsored, it could also be in the last 100 blocks (the opposite of
proposed change 3). The main benefit to having same-block resolution is
that you never pay for something that would have gone in anyways, but this
mechanism could actually be generically useful if you are operating a
business and need to make reorg safe payments contingent on funds received
into cold-storage. This implies a new rolling index of txids, which has
some overhead, but combined with appropriate mempool policy (e.g., a 200
block txid index in consensus and only a 100 block index available by
mempool policy) would ensure that most reorgs could be handled cleanly.*
*The behavior of sponsors is already emulable in a transaction graph. A
more complicated construction is possible that's a more accurate emulation,
but a simple version is as follows:*
*if you were to require:*
*A) All non-sponsor transactions include a single CPFP anchor output/CPFP
hook with an OP_TRUE as the last output*
*B) Miners sweep all OP_TRUE outputs at the end of the block to an
*Then it would be possible for third parties to be a sponsor by spending
that OP_TRUE output.*
*With a couple modifications (happy to discuss off-list, they end up being
aggregately more complicated than the sponsors mechanism) this can also
permit multiple sponsors in the same block.*
*Because it's possible to convert sponsors mechanism into this transaction
graph (and back again), I don't see a sponsors mechanism as breaking any
strong inherent property of the transaction graph, it's merely an
optimization of a pattern that could be implemented without breaking the
*Therefore I am unconcerned with the impact that a sponsors-like mechanism
has on the properties of the transaction graph itself.*

@_date: 2020-09-25 09:33:14
@_author: Jeremy 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
If I understand correctly, this is purely a policy level decision to accept
first-seen or a secondary deterministic test, but the most-work chain is
still always better than a "more fit" but less work chain.
In any case, I'm skeptical of the properties of this change. First-seen has
a nice property that once you receive a block, you have a substantially
reduced incentive to try to orphan it because the rest of the network is
going to work on building on that block. With fitness, I have a 50% shot if
I mine a block of mine being accepted, so floating point would have the
effect of destabilizing consensus convergence at the tip.
I could see using a fitness rule like this be useful if you see both blocks
within some very small window, e.g., 10 seconds, as it could decrease
partition risk if it's likely the orphan was mined within close range of
the other.
