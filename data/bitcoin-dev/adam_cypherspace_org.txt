
@_date: 2013-12-12 21:51:06
@_author: Adam Back 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
I think the one thing that SSL does provide is some protection against ARP
or DNS poisoning to trick the user into downloading from a different site.
The PGP WoT surrounding bitcoin or OS related ISOs be weak - I am not sure
if I could even check it directly myself despite spending a few hours
tracking down keys and checking fingerprints of biz cards of core devs I met
in person, then that is a relevant point.

@_date: 2013-07-05 16:01:40
@_author: Adam Back 
@_subject: [Bitcoin-development] libzerocoin released, 
I presume everyone saw the announce from Matthew Green & Ian Miers at JHU on
the release of libzerocoin: So now that raises the question of how can people experiment with real money
with zerocoin.  I think its fair to summarize there is resistance to merging
into bitcoin as it slows validation, bloats the blockchain, and also a
policy aspect it also imports cryptographic privacy into bitcoin.
On the forum thread on zerocoin math etc I suggested maybe people interested
to explore bitcoin could create an all-zerocoin alt-coin that is either-or
mined and p2p exchangeable for bitcoin.
Do people think that should work?  It seems to me it should with minimal,
bitcoin changes.  I think the rule for either-or mining should be as simple
as skipping the value / double-spend validation of the blocks that are
zerocoin mining blocks.  Obviously zerocoin blocks can themselves end up on
forks, that get resolved, but that fork resolution can perhaps be shared? (Because the fork resolution is simply to accept the longest fork).

@_date: 2013-07-13 20:42:27
@_author: Adam Back 
@_subject: [Bitcoin-development] libzerocoin released, 
Without a bitcoin peg on the creation cost of zerocoins, it is hard for a
new alt-coin to have a stable value.  Bitcoin itself is volatile enough.
Generally the available compute for mining is what it is, adding more
alt-coins just dillutes the compute available for a given coin.  (Modulo
different mining functions like scrypt vs hashcash there is some
non-overlapping available compute because different hardware is more
efficient, or even cost-effective at all).
Merge mining is less desirable for the alt-coin - its mining is essentially
free, on top of bitcoin mining.  Cost free is maybe a weaker starting point
bootstrapping digital scarcity based market price.
I think that serves to explain why bitcoin sacrifice as a mining method is a
simple and stable cost starting point for an alt-coin.  Bitcoin sacrifice related applications do not require code changes to
bitcoin itself, which avoids the discussion about fairness of which alt-coin
is supported, and about sacrifice-based pegging being added or not.
I dont think it necessarily hurts investors in bitcoins as it just creates
some deflation in the supply of bitcoin.
You can sacrifice bitcoins as a way to mine zerocoins without having the
bitcoin network validate zerocoin.  For all bitcoin clients care the
sacrifice could be useless.
Bi-directional sacrifice is more tricky.  ie being allowed to re-create
previously destroyed bitcoins, based on the sacrifice of zerocoin.  That
would have other coin validation requirements.
But I am not sure 1:1 is necessarily far from the right price - the price is
arbitrary for a divisible token, so 1:1 is as good as any.  And the price
equality depends on the extra functionality or value from the
characteristics of the other coin.  The only thing I can see is zerocoin is
more cpu expensive to validate, the coins are bigger, but provide more
payment privacy (and so less taint).  Removing taint may mean that zercoins
should be worth more.  However if any tainted bitcoins can be converted to
zerocoin via sacrifice at 1:1, maybe the taint issue goes away - any coins
that are tainted to the point of value-loss will be converted to zerocoin,
and consequently the price to convert back should also be 1:1?
p2p transfer is a good idea.

@_date: 2013-07-15 02:14:37
@_author: Adam Back 
@_subject: [Bitcoin-development] libzerocoin released, 
I think bi-directional sacrifice is probably not needed to assure a close to
1:1 bi-directional peg.
(Bi-directional sacrifice meaning also to convert a zerocoin to a bitcoin
you sacrifice a zerocoin and bitcoin would be modified to accept a zerocoin
sacrifice as a way to replace a previously sacrificed bitcoin).
I say that because if users who want zerocoins can obtain them at 1:1
exchange via sacrifice (a mathematical peg), it is of no additional cost to
them to instead buy them from someone who previously obtained them via
sacrifice for bitcoin (rather than sacrificing a new bitcoin).  So
presumably for goodwill, or nominal fee (a small discount), people would buy
rather than sacrifice where there is availability.

@_date: 2013-06-02 23:45:54
@_author: Adam Back 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
So the idea is that people may want to use proof-of-work unrelated to
bitcoin, and abuse bitcoin to obtain that proof, in a way denominated in BTC
(and with a published USD exchange rate).  And the ways they can do that are
a) create unspendable addresses (which maybe you cant compact in the UTXO
set if the unspendable address choices are not standardized)
b) spend to anyone which I take it goes to a random person who happens to
see the address first and race the "spend to me" out on to the network, and
hope miners dnt replace it with "spend to miner", which is insecure
c) doesnt delay by 100 blocks just delay the "spend to me" race?  Also most
likely to be one by a big miner once they adapt and join the race.
d) some new standardized spend to fees (only miners can claim).
e) spend to charity/non-profit of choice could be useful also
f) I guess we see something related in zerocoin - locked but unlockable via
another type of transaction later.
g) why not instead make the beneficiary the address of the service the user
is consuming that is being DoS protected by the proof-of-sacrifice?  Seems
more useful than burning virtual money, then it helps the bitcoin network
AND it helps the service provide better service!
so if I understand what you proposed d) seems like a useful concept if that
is not currently possible.  eg alternatively could we not just propose a
standard recognized address that clearly no-one knows the EC discrete log

@_date: 2013-06-13 15:39:32
@_author: Adam Back 
@_subject: [Bitcoin-development] is there a way to do bitcoin-staging? 
I had one thought towards this which is a different kind of merged mining.
I think a "fair" merged mining aiming for price parity would be done by the
miner having to choose the altcoin or btc at mine time, and altcoin chain
considering btc mine unspendable and bitcoin considering ac unspendable.
In terms of validation which miners are currently doing to help SVP clients,
it implies verification of both chains.  Or more incrementally each mine
should indicate in its serialization which chain it has validated.  This wa
about a hypothethical pure zerocoin altcoin hence zc/zerocoin:
Maybe we can say that a mergemine does not count as a validation of the
network for the respective network unless there is serialization in the
coinbase indicating that the network is validated.  In that way you could
have zerocoin mined and zerocoin validated, zero mined and bitcoin validated
(strange but possible), zerocoin mined and both zero and bit coin validated,
and also the same for bitcoin mined and zerocoin validated (strange but
possible), bitcoin mined and bitcoin validated (normal bitcoin ignoring
zerocoin) and bitcoin mined and bitcoin and zerocoin validated.  Then the
validation events on zerocoin network might not be as frequent.  Maybe
miners will tend to validate both networks as then they can claim fees on
both networks, even if the protocol prevents direct merged mining on both
networks (one or the other mined, and whatever chains validated as indicated
by coinbase serialization).
(I described it in this thread
 which
is mostly about understanding zerocoin, but digressed at that point to a
hypothetical pure zerocoin alt-coin that retains a fair merged mine and
exchangeless tradeability with main bitcoin.)
I think another gap is the exchangeless tradeability.  Apparently the
contract based proposals have race conditions, and ransom issues (refuse to
complete agreed commitment phase without being part-paid again).  I didnt
follow that discussion yet but Greg Maxwell and Sergio Lerner were
discussing and that seemed to be their conclusion, and Sergio's proposed
solution relied on a non-standard and not-fully-worked-through assumption
for the alt-coin (probably non-SPV compatible I think).
ps I thought it was quite interesting that seemingly you could make a pure
zerocoin alt-coin, it turns out you could direct mine them, and do zc-zc
transactions.  They are fixed denomination however I think you could extend them with
homomorphic amounts.  I noticed Matthew Green mentioned this idea in his
presentation at microsoft research (saw in the video they have put online).  From my perspective (he didnt specify how other than as an attribute) its
something like a Brands credential where you can prove in ZK that two
attributes sum to a given value without revealing the attributes at all. The missing last part is you have to prove that the attributes are less than
some threshold to avoid people cheating and adding q to their balance. (Arithmetic in the exponents is modulo q in the subgroup used in zerocoin). There are several approaches to doing this some of them not that cheap (eg
involving k DSA-like signatures to prove vale v < 2^k).  The idea of proving
it is less than k where k is say 128 is that then to add q, you have to
spend 2^128 coins which you cant do.  You can either make the values
uncertain by having v eg have 44 bits of useful precision and a few binary
00s and then 80-bits of randomness, or you can use a second never disclosed
random attribute like in a Pederson commitment or Brands credential eg c=g^v h^r mod p where r is random and never disclosed, but the user proves
knowledge of discrete log representation of c in terms of powers of g and h.
The downside of k signatures is validation CPU cost, and worse transaction
There are several other approaches which seem to be able to prove v < 2^k
with less than k, eg even 1 DSA-like signature.  I need to gather that info
in one place and write something referencing the literature I found so far. A homomorphically verifiable coin balance transfer could be interesting
outside of zerocoin - eg for bitcoin, or an alt-coin.

@_date: 2013-06-14 22:50:31
@_author: Adam Back 
@_subject: [Bitcoin-development] is there a way to do bitcoin-staging? 
Agreed.  What I mean is a coinbase for parity-priced alt-coin would be
intentionally considered (and required by the alt-coin to be considered) an
invalid bitcoin address, and vice versa.  The difference is for this purpose
it is both valid alt-coin coinbase (as well as unspendable bitcoin

@_date: 2013-06-19 17:28:15
@_author: Adam Back 
@_subject: [Bitcoin-development] Optional "wallet-linkable" address format 
I think Timo's point is that while you cant do discrete log, you can do y-th
root.  So if P = xG is a parent public key (x private key, G base point),
then your proposed multiplier address is hash of Q=yP.  However its easy to
find another P such that Q=zP'.  ie just "divide by z" (EC multiply by z^-1
mod n, n the order of the curve).  So P'=z^-1.Q, which will work because
Q=zP', substituting P' you get Q=z.z^-1.Q, Q=Q.
Of course the attacker has just performed an unspenable DoS (maybe, or maybe
a useless collision) because he wont know the discrete log of Q, nor P, nor
P'.  So thats the question, does the protocol have any reliance on knowing
the discrete log - is it a problem if someone can find different multipliers
of different (unknown, uncomputable discrete log) parent keys.
If it was a concern I guess you could require a proof of knowledge of
discrete log.  ie as well as public key parent, multiplier the address must
include ECDSA sig or Schnorr proof of knowledge (which both demonstrate
knowledge of the discrete log of Q to base G.)
So his defense could probably be more simply viewed as hash rather than MAC
(same thing approximately) you provide the pre-image of the multiplier.  So
provide P (public parent), x' (mutiplier pre-image).  And compute Q=xP where
x=H(x',P).  You cant use just x=H(x') because I could choose random x',
compute x=H(x') compute x^-1 and multiply Q to find P'=x^-1.Q=H(x')^-1.Q as
before.  Because x includes P as well, I would have to simultaneously choose
a P' such that Q=H(x',P').P' which requires a birthday attack on the hash
(or MAC).

@_date: 2013-06-19 20:36:57
@_author: Adam Back 
@_subject: [Bitcoin-development] Optional "wallet-linkable" address format 
This maybe simpler and trivially compatible with existing type2 public keys
(ones that are multiples of a parent public key): send an ECDSA signature of
the multiplier, and as we know you can compute ("recover") the parent public
key from an the ECDSA signature made using it.

@_date: 2013-05-06 20:04:18
@_author: Adam Back 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
Bitcoin p2p seeding requirements hav some ToR similarities, and we went
through the same security considerations with Zero-Knowledge systems freedom
network.  Though bitcoins attacker profile and motivation is different - so
the defense maybe even more demanding.  At least you have no shortage of
nodes and perhaps merchant interest and general good-will to lean on.
At ZKS I proposed we should fix the exit node issue (exit sees where you go
often in the clear) with an apache mod so the freedom aip tunnel (ToR tunnel
equiv) could terminate right on the web site.  (ZKS freedom network is long
dead but some of the ideas I think made it into ToR, eg I hope my end2end
forward anonymity idea that is implemented in Zach Brown's cebolla.)
Anyway I'd have about DNS being of limited value: bitcoins primary
vulnerability IMO (so far) is network attacks to induce network splits,
local lower difficulty to a point that a local and artificially isolated
area of the network can be fooled into accepting an orphan branch as the
one-true block chain, maybe even from node first install time.
(btw I notice most of the binaries and tar balls are not signed, nor served
from SSL - for linux).
Therefore as it applies to discover, you want to be able to discover peers
through as many network routes, and even steganographic protocols as
possible.  eg if a popular web server (say apache, or an apache module) put
a steganographic peer discover relay from its own network area, even for a
small bitcoin fee, that would help a lot.  (Steganographic in the SSL sense
would just mean that the peer seed request to /btcseed.cgi would not be
distinguishable to someone highly sophisticated on the inside of the router
all the peers traffic is routed through.  Eg you could easily do this with a
special magic header that overwrites something else or deletes some
unnecessary header so that the request at least is a standard size, and pad
the response to the same size as the site index.html or whatever).  If the
user picks a few SSL sites and cross checks (more for high value) a subset
of peers available on all and uses them as his seed that seems like a better
In that way an attacker cant control the network without denying service to
popular SSL sites, which would be a warning sign to users, or having at his
disposal a SSL sub-CA cert (like happened with diginotar and gmail).  You
may be able to pin CAs for popular sites.  Obviously to the extent you're
using SSL you want to generally use EDH for forward-secrecy.  And not RC4 :)
Probably anysite that accepts bitcoin payment will be happy to run such a
With ToR, it has a similar bootstrap problem to bitcoin.  So while that may
help it is also passing the buck, not necessarily solving the problem.  And
as I said I think its possible bitcoin has a higher assurance need in that
the attackers motivated my $$ might put more effort in than the odd
dictatorship trying to pay lip service to preventing people reading pages on
a blacklist.
Given the vulnerability of DNS to poisoning I would not trust it too much. I know its just a bootstrap, but ideally you dont want to bootstrap from a
known publicly vulnerable protocol - it invites DNS poison net splits
against new users.
Also to the extent that users local clock is under his control (with
unuthentcated NTP?) he should also treat sudden dramatic changes in luck
(deviations from 10min interval) as suspicious.  Unfortunately at present because of the first past the post nature of the
bitcoin lottery, reduced variance hashcash cannot be used, so its hard to
infer too much even from quite significant luck changes.

@_date: 2013-05-06 20:32:22
@_author: Adam Back 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
btw with nodes for transport security you might use self-certifying keys. Referring to Zooko's triangle, then the key is the node identity.  Similar
to a bitcion address.  So then just another ECDSA key and use emphemeral
ECDH for transport authenticated with the nodes key.
Maybe there can be some value to reputation to a node - eg it can charge a
higher micropayment for its p2p network services, a node with a good
reptuation could charge a higher micropayment for relaying (though bitcoin
itself probably doesnt like micropayments as bloating the transaction log).
Another ZKS era idea I had was to have a gossip protocol for users to find
out what other people think about the trustworthiness and reliability of
nodes.  If that info is distributed via gossip over multiple channels and
network connections over time, and kept in something like a gnutella host
cache (just a cache of random info with some eg random replacement policy)
it becomes very hard for a dishonest node to censor evidence of its low
It is best as Gregory said to be able to directly prove, and punish by
block-chain validation, because that is more smart-contract like.  Bisbehave
and nodes wont connect to you or lose somehow.
But what exactly could you prove about a node?  You dont really know if a
node is an originator for a double spend, it could be relay.  And for
privacy and security you cant expect the node to use its coin address
private key.
Hmm: maybe one could use a Brands private credential with offline double
spend detection, with the reputation but not coin address of the node
disclosed, and the nodes coin address embedded in the proof.  Each node
could be is own CA, providing a ZKP.  If the node ever double spends a coin,
it loses its reputation as the coin address is revealed.
btw another old idea was to require proof of the existance of the private
key of a high value coin in the double-spend revealed information.  Then
basically to get a higher good-behaviour bond, the node ties up more coins,
and if a node cheats, the first person to discover this collects the
forfeited good behaviour bond.
ps I have an opensource openSSL based Brands (& Chaum) credential library at
 I didnt actually implement the ECDL
version, just the DL version, but that is not so hard, and its on my todo
list.  (There is also a strong RSA assumption version, also not

@_date: 2013-05-06 21:50:03
@_author: Adam Back 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
My comment was to say that a good behaviour bond for a relay node could be
put on an address that is defined as unspendable until such time as an
auditor can prove the node engaged in the undesired behaviour, at which
point the audit receives the payment as part of his proof.  Or until the
node ceases to operate.  Its a smart contract.
However I added to that, that it is still possible to do that while
preseving privacy, to point out that it is technically possible, for people
to be aware of in their mental toolbox, if it helps solve an otherwise
tricky problem.
So that would be a privacy preserving smart contract, the parties are
unknown, and unknowable (with unconditional security even), but still the
smart contract executes.  In some sense a privacy preserving smart-contract
is closer to the real point of Szabo's smart-contract idea because you cant
try to renege on the contract in a conventional court - because you cant
identify your counter-party.  Bitcoins privacy feature is fairly weak so
that is probably often not true.
Of course you'd probably need zerocoin to stand much chance of proving an
address private key of an unlinked coin was in the double-spend disclosed
attribute in the first place, and as we know zerocoin is not that efficient.
that could be easily done with the work of creating a vanity address.  eg
address containing many leading 0s.

@_date: 2013-05-07 00:51:46
@_author: Adam Back 
@_subject: [Bitcoin-development] limits of network hacking/netsplits (was: 
Well I take your point that you have to produce 2016 blocks, but at a lower
rate.  But that doesnt directly translate into my cost, I am thinking pure
network hacking.
Maybe I could hack a pool to co-opt it into my netsplit and do the work for
me, or segment enough of the network to have some miners in it, and they do
the work.
I am just thinking $500k/day worth of relatively perfect crime reward is a
lot of motivation for hacking networks.  Many routers home and even carrier
are vulnerable to people armed with cisco source code & 0-days.  The
netsplit doesnt have to be geographical, nor even topological, nor even
particularly long-lived.
If you control enough people's network routing at a low enough level, you
dont even have to stop transactions, nor do any mining work, just stop
blocks from the netsplit crossing over, and hold that position for say a day
(if your netsplit has 1/24 of network hash rate in it, so the split gets 6
confirmations to reassure the victims) and let the miners do the work.  Do
enough transactions to do a big cash out (spend differently on the two
netsplits).  Obviously a big and human inattentive pool, dark-miner etc is
the ideal target to put into the netsplit to increase the power while
controlling less nodes.
Malware could do the same thing for clients, dont forget most are running
windows.  Malware could also start a miner if none present.
Do you know if there is any downwards limit on difficulty?  I know it takes
going slow for a long and noticeable time, but I am just curious on the
theoretical limit.
I dont see the signatures.
I see no signatures for linux and none in the tarball.  There are some
public keys inside the tarball, thats it.  Also no SSL.  sourceforge support
SSL so you can download that.  But bitcoin.org doesnt even answer 443, and
the source forge link is HTTP.  But even if the sourceforge link was SSL one
should not serve an SSL download link from an HTTP page, any more than type
a password into an HTTPS form action on an HTTP page.  The attacker can just
redirect and the user doesnt know what is legitimate.
Consequently even if there is code signing on the windows exe, the user
doesnt know that, nor who they should be signed by, and as they are served
via HTTP, its bypassable.
I guess by far the easiest way to attack right now (at least linux users) is
just to change the binaries to create a user operated netsplit, or just have
all their wallets empty to you via a mix once the amount gets interesting.
(All attacks hypothetical of course - I'm actually a white-hat type of

@_date: 2013-05-07 13:07:40
@_author: Adam Back 
@_subject: [Bitcoin-development] limits of network hacking/netsplits (was: 
Well its a bit more hopeful than that :)
If they are PGP signatures, they can check the PGP WoT; its not that
hopeless some us eg have our keys in Ross Anderson's PGP Global Trust
Register, a PGP and CA key fingerprint book.  Probably most of the CA keys expired, but many of the PGP keys didnt.  So to
the extent that those people take PGP WoT seriously, and the main developers
names and email addresses are known and scattered around hundreds of web
mailing list archives etc there is some trust anchor.
And even without a PGP WoT connection, if the website had SSL enabled, they
can trust the binaries its sending to the extent that it is securely
maintained, and to the limit of the CA security weakest link (modulo sub-CA
malfeasance, and all the certification domain ownership laxness you or
someone mentioned in another mail).  That there are limitations in it doesnt
mean you should not avail of the (moderately crummy) state of the art!
And that is tied back to the domain itself hwich is very mnemonic and
referenced widely in print, tv, websites etc.
I guess its the least of the concerns but I believe Damgards is better. Another possibility is threshold DSA (which is built using Damgards Paillier
additively homomorphic cryptosystem extension) and discrete log schemes are
easier to setup with zero-trust.  Other simpler discrete log signatures ie
Schnorr are much easier to work with (threshold DSA is a mite complicated),
but NIST tweaked Schnorr to create DSA, and the rest is history.  The trust
n-1 of n is good enough for signatures because anyway that is above the
assurance of the signature.
Well before I tried the download I had downloaded and compiled a few
versions from git.  But to get a stable and experience the non-programmer
view I did first try "yum install bitcoin" and then "yum whatprovides bitcoin"
on fedora 18 with +rpmfusion and there appeared to be no package!
I didnt find the signature on the source either or I would've checked.
Other ways you could get usefully get assurance of the source is multiple
people signing the release, with an asserted meaning being - I checked the
patches that went into this and I see nothing malicious.  It might help if
one or more of the signer were pseudonymous even (eg Satoshi if willing)
because you cant coerce legally, nor physically a pseudonymous person
because you cant find them.  Its a lot of pressure on open secure coding
process when there is $1bil value protected by the integrity of the code. (It seems the most likely avenue to bypass that maybe simply the attacker to
just become a committer and slip the 0-day past the review process.  There
were in the past modest-impact and plausible looking mistakes in PGP
discovered after sometime.)

@_date: 2013-05-07 14:16:41
@_author: Adam Back 
@_subject: [Bitcoin-development] minor bitcoin-qt gripes moving BTC off 
Three minor security/other issues:
1. please a way to unlock the wallet without displaying wallet password in
   console screen (console unlock wallet, to import priv key); or 2. a button to import a private key (and option to transfer it to another
   key - if you are not the sole controller the private key)
3. a UX way to transfer BTC off a specific adress (eg choose from
   address), rather than having to spend the entire wallet onto a new
   address, just to get BTC off a specific address.  Doing it that way has
   problems: creates more network traffic/bigger packets, higher fees (if
   any transactions are young/low confirmation), and generally damages
   privacy as all your funds end up linked.
Stop reading here if thats clear.. below is how those scenarios happened
which I think are common enough.
So someone sent me a small BTC donation by emailing me the private key, as I
had no bitcoin address.  So naturally I need to move the funds off the
private key, or they could spend it under me.
I had another small amount on a self-controlled private key in the wallet
via a reddit tip (.05BTC tip payed to self-controlled key became .0498BTC).
So I went through the debug->console unlock wallet (password in cleartext on
screen? yuck), then importpriv key, process and that worked, though not
particularly intuitive - could do with an "import key" button?
Then I wanted to take the .01 BTC off the private key and the FAQs etc seem
to suggest that the only way to do it is to spend our entire balance (from
all keys) onto a new address.  Not exactly what I wanted, but I did it
anyway and the tx fee goes up from .0001 which I set it to to .0005 because
the transaction was young, to avoid network flooding.  Even though
transaction I actually wanted to move (on the non self-controlled key had a
big heap of confirmations and so could've been .0001 tx fee).
Would be kind of handy to be able to select the key to empty without having
to empty the entire wallet into a new key...  (Smaller transaction KB on the
network, less fees for the user, less confusing).
Maybe theres a way to do it, eg via the console again, but I didnt find it;
and it's surely common enough that it could do with being another button or
right click option.  eg I could've setup another wallet instance but thats
rather indirect.

@_date: 2013-05-07 18:06:17
@_author: Adam Back 
@_subject: [Bitcoin-development] minor bitcoin-qt gripes moving BTC off 
At ZKS other than freedom network (ToR precursor) we had psueudonyms
associated with cookie managers.  The idea was you create pseudonyms for
different purposes to segregate your online linkability.  Seems to me that people are always going to make mistakes with individual
keys, even if the feature were there and accidentally link all their coin
sources together.  I presume people saw the analysis of the slush related
25k BTC theft, even seemingly the thief made possible slips while trying
presumably not to:
Does the client have any privacy algorithm (to minimise coin source cross
linking) to reach a given payment?
eg consider say I use social media, with a screen name; I collect reddit
tips etc; I pay them out to others, or use them to buy virtual goods
associated with the same purpose.  It would be rather useful to help people achieve that, there is already the
ability to create addresses, label them.  But I think just for the GUI to
allow you to control which address the payment is from would be enough, it
doesnt seem like such a complicated concept.  And if people dont care, they
only need create one address.
Technically ZKS wasnt anonymous networking like ToR but pseudonymous
networking.  Multiple wallets for different unlinked purposes would be
somewhat analogous to ZKS freedom pseudonymous networking & cookie-jar. Because of the pseudonymity in ZKS misbehavers could be blocked by exit
nodes based on pseudonym.  Of course they can always create a new pseudonym
but then they lose their accumulated reputation.  You can even make people
pay for pseudonyms, as I recall users got 5 free pseudonyms but had to pay
for more.
(Though I have to admit the concensus after some years at ZKS was most end
users didnt understand what a pseudonym was!  They just wanted to be
"private" and have the computer magically solve it for them.)
If you want to simplify maybe you could consider normal (linked to AML
trading accounts, orders for physically delivered goods etc), and "private"
analogus to the private browsing mode in various browsers.  Maybe beyond 2
is an advanced feature but still available.

@_date: 2013-05-09 13:19:13
@_author: Adam Back 
@_subject: [Bitcoin-development] An initial replace-by-fee implementation 
In this thread discussing this idea
 it is suggested that the approach risks making 0-confirm double-spends
I dont see why this should be.  Cant part of the validation of accepting a
fee revision be that every aspct of the revision except the reward must be
unchanged, otherwise the revision is considered invalid and discarded?
(ie same payment amount, same input coins, same recipient and same change

@_date: 2013-05-09 14:07:23
@_author: Adam Back 
@_subject: [Bitcoin-development] An initial replace-by-fee implementation 
I have to say I do not think you want to have it be random as to who gets
paid (by having conflicting double spends floating around with different
payee & change addresses all from the same sending address.)  About current defacto no replacement: it is the best bitcoin currently has,
and it has value, with it you need to do a net-split to attack eg
1-confirmation, and this proposal weakens it.  Net-splits are possible but
not trivial.  This proposal moves them into spec - ie free.
About privacy: I think you are going to inherently disclose which is the
change address, because you will decrease the change when you increase the
fee.  There is no coin management in the client, and as far as I saw so far,
no privacy management of which coins to reduce coin cross linking.  Who's to
say the client has 100s of times as many coins as the payment.
If people dont want to reveal which is change and which payment, they need
to put a big enough fee up front based on a margin over prevailing fee
It would also be better to try to get the fee right first time than to create
more traffic revising it due to experience.  Though the ability to revise
the fee IFF the best effort fee doesnt work empirically after a couple of
blocks seems like a good feature.  (But not with revised recipient/change
Also if the bids are too flexibly different how do you stop both bids being
processed (eg one in a block, the next in the next block).

@_date: 2013-05-11 12:22:09
@_author: Adam Back 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
I didnt quite understand the writeup and the references were ambiguous.
But if you are talking about bitcoin/hashcash merged mining for email: it is
something I think should possible.  Of course for email the scale means
bitcoin style flood-fill and direct tiny payments are completely out of the
question, thats why hashcash itself has no communication overhead other than
a header in the mail - its only scalability limit is email itself.
Rivest's PayWord for people who dont know the reference in this context is
the observation that for a low value micro-payment, you dont mind if you
only receive a payment 1 time in k so long as the expected payment is n
after receiving n (eg satoshi sized) payments.  Eg like a penny tip jar so
long as your expected payment is correct long term (win as often as you
lose) you dont mind.  And a fair 100% payout lottery can be fun of itself.
So let say each email client sends in an email header the head of the
bitcoin hash chain, it has seen via other emails, which can be offline
verified back to the genesis hash.  Maybe some clients even have bitcoin
installed and ask the bitcoin client for the hash chain head.  The client
also generates an address on setup, and sends its bitcoin address in a
header.  If you send to a new address you dont know their address, so you
send to eg me (Adam;) as a default, or the bitcoin foundation, or an invalid
address to destroy the coin - the recipient assumes that is not the sender
as those address are in the client.  A sender can under-contribute but makes
no gain.  Under-contributing is fixable if desired (see under-contribute in
amortizable hashcash paper, but using PK decryption with recipients private
key x as its non-interactive b'=D(x,share).) Then clients merge mine involving the recipients bitcoin address (or one of
the default addresses).
Even if the merged stamp provdes to be an orphan, even a very old one, its
valid in a hashcash anti-spam sense, meeting the same purpose as destroyed
Maybe one can put the bitcoin hash in DNS with a 5min TTL and have mail
clients read that to reduce scope for stale mining.
In this way one can merge mine bitcoin & hashcash to the benefit of the
recipient (or some beneficiary trusted not to be paying the proceeds to the
spammer).  And in a way that scales to email scale, and does not involve
installing a bitcoin client in every client, nor mail server.

@_date: 2013-05-13 12:54:08
@_author: Adam Back 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
I think there are 3 choices:
1. merged-mine (almost zero incremental cost as the bitcoin mining return is
    still earned)
2. destroy bitcoin (hash of public key is all 00s so no computible private
    key)
3. anyone-can-spend (= first to spend gets coin?)
Surely in 3 if you mine the bitcoin its no particular assurance a you will
do your best to make sure that it is *you* tht spends it, so it devolves to
merged-mine.  (Eg delay revealing it for 10 seconds while you broadcast your
spend widely)
Peter talks about value, but the proof only proves cost equal to bitcoin. Those are not the same thing.  And they are so-far non-respendable.
I still dont understand what he was saying.  If you do please speakup.
I think potentially a publicly auditable pooled mining protocol would be a
place to start thinking about respendble micropayments.  I made a post
on the bitcointalk forum outlining how that could be done:
if you have a publicly auditable pool, where users can prove to each other
outside of the bitcoin transaction log that they contributed a number of
shares to a block, those could be traded somehow.  Possibly eg with the pool
keeping a double-spend db.  If the payments are low value, people maybe
happy trusting a pool.  If the pool cheats, everyone stops using the pool. You rely on the pool not to spend the backing bitcoin blocks.  But it
remains possible for the pool to cashout people who collected enough shares. Probably you could do that with blinding if desired.
You are right I was thinking of Rivest's peppercoin.
Well there doesnt need to be a one-true-blockchain DNS, though the power to
output a hash at any reasonable rate is a big proportion of the network
power.  And the outputs are instantly verifiable, so it forms a kind of
trapdoor hashchain (where the trap door is not a secret but havng a huge
amount of CPU power).  And there can and should be many blockchain via DNS.
For namesaces in general another approach other than DHT/flood is numerous
competing hierarchical, heavily cached, but publicly auditable.  Cheaters
are shunned.  Same effect, more scalable, most people are not cheating most
of the time.

@_date: 2013-05-13 23:12:44
@_author: Adam Back 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
Some musings about the differences between Peter's proof-of-sacrifice (you
did the work but elected to make the small reward chance unclaimable), vs
conventional actually doing the work but then destroying the bitcoin!
- proof-of-sacrifice seems similiar to hashcash except its difficulty is
   time stamped and measured against the bitcoins dynamic difficulty,
   (coordinated inflation control being something posited but never
   implemented in hashcash).  So thats kind of interesting, particularly if
   you can do it with very low bandwidth, and decentralized; unclear.
- with proof-of-sacrifice its more offline because you do not need an on
   block-chain double spend protection (via flood-fill, validation, and block
   chain mining) because it is simply "unspendable", though you could show
   the same proof to multiple people.  In any case the values are far too low
   to spam the block chain with.
- because proof-of-sacrifice is small you can afford to mine them on the
   spot and make them payable to the identity of the recipient, like cheques:
   they identify the recipient, so they are automaticaly non-respendable in
   the eyes of the recipient (he keeps his own double-spend db, and people
   wont accept cheques made payabe to other people).  This is how hashcash
   works for email.  Also a time-stamp ensures you dont even need a big
   double-spend db, as you can prune it if you reject expired cheqes.
- you could give a proof-of-sacrifice a private key, just like bitcoins;
   then they could be pre-mined and identity or other info could be signed
   later.  However then you have double spending issues again.  You can - I mentioned amortizable hashcash under-contribution feature you can make
   it so the recipient uncovers the actual value of the coin (if it is
   merge-mined).  (Put recipient public key in coinbase, hash for min share
   size eg 32-bits leading 0s call that "collision"; send to recipient, he
   decrypts the hash with private key, so the decryption is verifiable with
   public key.  Then the full value of the coin is    if that alternate validation was allowed in bitcoin.
- what about if a pool could lock the reward (rather than receive it or
   destroy it) eg some kind of merkle root instead of a public key hash in
   the reward recipient address field in the coinbase.  Then the miners who
   created that block have actual share proofs that are claims against
   something eventually redeemable.  Maybe if they collect enough
   share-proofs to reach a minimum bitcoin transaction size, they can redeem
   a big strip of shares for a few mBTC, but claims below that are rejected
   by the network due to tx fee.  (btw I think it seems possible to have a
   publicly auditable pool so it cant skim nor disclaim shares.)
There were some systems that charged hashcash for pseudonyms i2p names (i2p
is a ToR like system)...  see htp:// then there was of
course namecoin.  There was some remailer/email nymserver integration as
Yes it seems that having a proof-of-sacrifice that hardens the block chain
is the important part.
When you said destroy-via-miner-fee:
Is that directly possible?  Because the reward transaction has no source,
and no fee?  Or can you put a 25BTC fee in the reward transaction in the
If so that seems like the best option for proof-of-sacrifice rather than
proving destroying the possibility of reward.  But alternatively the bitcoin
foundation as recipient, or EFF etc.  25BTC is a big reward might have some
double roll-over lottery effects - everyone piles in for the occasional

@_date: 2013-05-14 11:25:07
@_author: Adam Back 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
Well if it is a later transaction, not an integral part of the reward
transaction (that is definitionally mined by being serialized into the
coinbase), the user may elect to withhold the promised transaction
give-to-miner, so thats not so good.
Or do you mean to say you could have (implicit reward 25BTC) and reward
transaction .001 BTC to self and 24.999 BTC with existing bitcoin format and
validation semantics?  That would be close enough to give-to-miner.  Also
the output sum > 0BTC limitation could be changed to >= maybe... (just one
well placed character :)

@_date: 2013-05-14 13:51:51
@_author: Adam Back 
@_subject: [Bitcoin-development] ecash and revocability 
So back in 1999, in an ecash thread on cypherpunks I claimed:
This was in the context of a discussion of digigold (e-gold stored the
physical gold, digigold offered "ecash" backed in that physical gold). Digigold ran on Systemics payment server/sox protocol.  Because of inferred
regulatory concerns and patent licensing issues digigold & systemics were
not using blind signatures.  However with systemics sox server, like
bitcoin, you could create multiple accounts on demand and shuffle payments
around for a degree of privacy.  The bitcoin analogy would be the
transaction log lived in the systemics server, so it had a central failure
point, but arguably more privacy as the log was not public.  Also systemics
SOX protocol (Ian Grigg & Gary Howland) had some aspect of bitcoins smart
contract concepts - ricardian contracts.  (Btw the anonymous reply itself was interesting -
 that could have been
Nakamoto, the only missing thing from the parts on the discussion room floor
to bitcoin is mathematical inflation control.)
The thread actually started here
 and then continues here
 because of a subject
line change and then and more subject line change confusion.
A related thread a few days later also covers Sander & Ta-Shma (which
zerocoin is based on):
there were many more threads about various ecash technologies.

@_date: 2013-05-14 16:09:02
@_author: Adam Back 
@_subject: [Bitcoin-development] bitcoin taint & unilateral revocability (Re: 
So I still think that is an important point.  "Ecash should not be
revocable".  I think bitcoin currently has a partial problem because of
Now blinding based unlinkability, in a distributed cryptographic payer/payee
anonymous system like Sander & Ta Shma [1] and zerocoin has so far been
based on ZKP of set membership.  Of course that is somewhat expensive,
though zerocoin improved the ZKP with an relatively efficient (but still
cut-and-choose) proof.
Bitcoins relative lack of privacy creates a problem with tainted coins
risking becoming unspendable, or spendable only with some users, or at a
discount.  So while the policy coded says all coins are equally acceptable,
the information exists so people can unilaterally reject them, depending on
what the taint is.  So far revocability hasnt reared it's head that I heard,
nor taint inspection too much?  However people have the choice and technical
means to check the taint and send the bitcoins back.
Another aspect is that bitcoin, like systemics sox/digigold, makes a
different privacy tradeoff.  Somewhat private, but not very much.
But it creates the question: could the taint issue be fixed efficiently (eg
even without blinding or ZKP of set membership?)
One related concept is commitments.  I think its relatively easy to commit
to a payment and lock a coin without identifying yourself, until the
commitment is released.  You might do the commitment, wait 6-blocks for
confirmation, then reveal the commitment.  Then that is like a self-issued
green coin with no need for trust, that can be immediately cleared.  The
recipient has to be committed to at the same time to prevent double
So just commit = H( input-pub ) H( transaction ) and put it in the block
chain.  Where transaction the is usual ( input signature, output-pub,
script).  (Fee for the commit would have to come from an unlinked coin or
the input-pub reveals the coin).  Wait 6 blocks, send/reveal the transaction
(free because fee was already paid).  Validators check input-pub hash
against committed coins by hash, check the transaction hash, and the usual
ransaction validations = sum inputs, otherwise reject.  The user better pay
change if any to a different public key, as the inputs public keys are one
use - are after the reveal they are DoS lockable by other people reposting
H( input-pub ).
The input-pub coin is locked as normal transactions have their public key hash
validate as not being locked.
[1] Sander & Ta Shma "Auditable, Anonymous Electronic Cash"

@_date: 2013-05-14 22:07:31
@_author: Adam Back 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
Yes but thats inferior in the sense that it is spamming the bitcoin payment
protocol slightly, to the small reward of miners, and involves actual money
and traceability to real-name (where did you get the coin from to spend). If alternatively you just proof you direct mined on a block with a coinbase
that immediately makes payment to future miners its better because: a) you
can do that with no new traffic for the bitcoin network (except when you
mine a whole block, you'll post it); and b) anyone with a reasonable
verification on the blockchain head (even if the spender has to give it to
them!) can verify it without any other network traffic; and c) if its
micro-mined on the spot it can be bound to the service whereas if you give
it to fees as an on network transaction you are limited to values above the
min tx fee.  So idealy I think you need to be able to simultanously mine and give reward
to future block miners.
What you could do with out that is d) mine for the reward of bitcoin
foundation/software author/or service provider.  In the last case (service
provider) its an extreme form of Rivests peppercoin probabilistic payment

@_date: 2013-05-14 22:12:32
@_author: Adam Back 
@_subject: [Bitcoin-development] Bitcoin2013 Speakers: Include your PGP 
Nice.  But we all kow about the security of DNS ;)

@_date: 2013-05-15 12:25:09
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
So in a previous mail I described a simple, extremely efficient and easy to
implement symmetric key commitment that is unlinkable until reveal time (at
bottom).  I think this can help improve the byzantine generals problem, that
bitcoin only defends to simple majority (with one vote per CPU power), and
so assumes most nodes by cpu power are honest.  With this simple protocol
change you dont need any honest nodes, just some honest clients to spend to,
to have your transaction accepted.  You can think of this in terms of a (somewhat distributed) server performing
validations, but in a way that it sufficiently blind to the details of the
validations that it can not selectively enforce a policy, it power is
limited to random DoS.
There are other situations where you can rely on a server for one property
but not another - eg a somewhat distributed encrypted backup (like Tahoe
LAFS) you rely on for availability, but not integrity nor confidentiality
(because you encrypt those, and some sharing scenarios still work.) So this
is in that class of protocols - zero-trust in server, but can extract
service and some guarantees from the (optionally distributed) server anyway.
(Bitcoin does not use known better than majority results for byzantine
generals based on fair coin toss, relying instead on simple majority and an
assumed largely unjammable network.  I notice Nick Szabo was complaining
about this on his blog and saying bitcoins majority is not even a standard
or proven byzantine voting protocol - something adhoc.  I think the bitcoin
unjammable network assumption is a false at the limit so that someone with
strong network hacking capabilities can create network splits long enough to
even overcome the network majority vote without having any compute power of
their own.  All they need is to have a split with enough power to plausibly
quickly get the victims their desired number of (split) confirmations.)
Anyway this should be a clear voting improvement, that is efficient.
Imagine a couple of big pools or ASIC miners started enforcing some
arbitrary coin policy, eg say coins must not have some taint according to
its list of black coins, or coins must be certified by some entity, be
traceable to some type of event etc.  Well call these miners/voters
"dishonest", in that they are not following the intended zero-policy
If the coins dont match their chosen policy, the dishonest miners will
refuse to include transactions in blocks they issue.  If they see a
transaction which does not match their policy in a block by someone else
they will ignore it and try to make it into an orphan.  As they have say 75%
of the network power they can do that successfully.  Even with current
validation protocols in the clients, so the "but clients wont accept the
change" argument does not apply - the existing clients will accept the
policy change, because they cant detect it, nor prove it, and dont have the
voting power to impose honest policies.
(For realism of this risk, note that according to Kaminsky there already
exist multiple entities with reserve ASIC power each exceeding current
network difficulty who are holding part of their power in reserve for profit
maximisation reasons.  This is a coming to fruition of the concentration of
power issue I was talking about in my first bitcoin forum post.  People who
have that kind of power in reserve have clearly invested millions of
dollars, which probably makes them more vulnerable to political influence.)
Alright so the solution.  Use the commitment protocol (below) which even
though it is symmetric key strongly hides the committed transaction public
key.  (Symmetric in the sense that the validation steps are all highly
efficient symmetric key based).  Now send the transaction (which includes
the public key) direct to the receiver, over a secure channel, or an assumed
non-eavesdropped direct channel, with no p2p flood of the transaction.  The
receiver can check the hash to the commitment, and decide how many
confirmtions he needs.  Once he has eg 6 confirmations he reveals the
commitment to the transaction (by publishing it).  The sender may also send
the reveal/transaction to the network directly himself, if the recipient is
offline.  However there is no advantage to publishing early so it seems
better to let the recipient do it when he is ready to incorporate the
payment into his wallet.  Now the powerful dishonest voters if they try to apply their policy when
they see the reveal triggers it, must redo the work of the 6-commitments
that they computed themselves.  This is like starting 6-steps behind in the
statstical gamblers ruin game that Nakamoto describes in the bitcoin paper. Consequently even with 75%, they will find it very hard to outcompete their
own prior work, to create a 6 chain long orphan while the 25% is moving
forward on the honest chain.  Each time they see transactions which violate
their policy, they have to restart their chain recalculation again from
scratch.  Often if simple lower powered intermittent recipient sends the
coin will be burried hundreds of blocks back.  In addition 6 chain long
branches are extremely unlikely with honest payers, so clients can (and
maybe already do?) act with suspicion of they see one.
Going further, I said for best security, the recipient should never even
reveal (to the network) until he is actually about to spend, but futher he
does not even have to reveal publicly ever, he can choose to reveal only to
the recipient with a direct connection (no p2p flood fill of transaction.)
And the direct spend argument composes, ie the 2nd recipient can not do the
same thing again.  (public key A sends to public key B sends to public key
C: B publishes COM( transaction B->C ), sends the reveal of COM( transaction
A->B ), and COM transaction B->C ) to C.  C waits 6 confirmations and is
convinced.  So its the approach is composable, and in fact the network
doesnt learn the size of the transaction even, though the spend grows each
time.  Eventually presumably someone will publish will the confirmations to
the network to trim the tansaction size, though it is not strictly
necessary, and the transaction flow is small and direct (no network scaling
issues), so that it wouldnt be a huge problem to have a 1MB payment
representing 1000s of hops of network blind transactions.  (For the
composable network blind respending the commitment has to commit publicly to
both the sender and next hop recipient keys, so the network can see how long
the chain is).
Probably you can cope with multiple inputs and outputs, and maybe given even
you can work with a 100% dishonest network mining network (all the dishonest
miner can do is selectively DoS transactions if they are all network blind
except the mining), maybe the mining can even be decoupled from the voting,
as you no longer demand much from the voting process.  That admits more
interesting things like pool free direct mining, low variance hashcash
coins, probably.  Many things to think through.
I suppose the commitment could be described as a blind symmetric commitment.

@_date: 2013-05-15 13:49:56
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
Protocol voting is a vote per user policy preference, not a CPU vote, which
is the point.  Current bitcoin protocol is vulnerable to hard to prove
arbitrary policies being imposable by a quorum of > 50% miners.  The blind
commitment proposal fixes that, so even an 99% quorum cant easily impose
policies, which leaves the weaker protocol vote attack as the remaining
avenue of attack.  That is a significant qualitative improvement.
The feasibility of protocol voting attacks is an open question, but you
might want to consider the seeming unstoppability of p2p protocols for a

@_date: 2013-05-15 15:00:14
@_author: Adam Back 
@_subject: [Bitcoin-development] double-spend deletes (or converts to fees) 
transactions easy)
About double-spends it might be better if the double-spend results in no-one
keeping the money ie it gets deleted by definition (except for the fees, or
the whole payment gets converted into a 0BTC output so 100% fees).  Ideally
you'd want a way for known double spends to be circulated at priority in the
p2p network, even routed directly to earlier recipients if known.  However
have to watch out for even the fees being double spent in other
transactions.  Maybe the fees could be demanded to be self-created (no
trusted green-coin issuer) 6-confirmation spend-to-miner green-coins.
(Note double spends are not-binary.  An attacker can spend a 25BTC coin via
50x 1BTC transactions.  Which 25 are valid?  Currently it is the first 25
from the network perspective of the miner that succeeds on the current
block.  (And that view overrides, even if other miners had differing views,
unless the block later becomes an orphan).  Its surely easy for a double
spender to accumulate fast connections to known powerful miners to get the
spends that benefit him to them first.)
In that way (with double-spend deletes) the would be double-spender can not
gain within the bitcoin protocol by double spending.  He can gain outside of
the protocol eg by persuading merchants to give him non-revocable resellable
non-physical goods (or physical but anonymous goods).  But that is harder
work, and people selling goods with no recourse will be defensive about
ps I still dont think replace-by-fee is a good idea, at least the way it was
implemented with changeable outputs, but I think that discussion seemed
closed, so I wont rehash it.

@_date: 2013-05-15 18:21:29
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
Bit-commitments are based on deterministic one-way functions eg like SHA1(
SHA256( public key ) ) Obviously it has to be a different one-way function
to the coin address calculation which is RIPEMD( SHA256( public key ) ) as
that is already public.  Alternatively it can be a different serialization
using the same hash eg RIPEMD( SHA256( 1 || public key ) ).
There is only one commitment possible per public key - so you can only
create one commitment that would validate to a receiver, or to the network. The network checks that there are no non-blind double spends of committed
coins which it can do as spends require disclosure of the public key, which
allows existing commitments to be verified, and it similarly qchecks that
there are no blind double-commitments.
Each committed coin would be:
one-spend-commit = Com( spender pub ), Com( transaction )
where Com is implemented as the above hash.  The network just places the
commitments in order as with conventional transactions.
The committed coins are not linkable to your non-blind coin because you did
not reveal your public key in the (largely passive) act of receiving to a
coin address.
The temporary unlinkability (until commitment reveal) is a necessary side
effect, not a cryptographic anonymity feature like zerocoin.  The
transactions are identical to bitcoins once revealed.  How long the
committed transaction chains can be between reveals is an implementation
choice could be 1 hop, or as long as you like.  (Actually it appears to be
up to the individual users how long the maximum chain they accept is - the
network itself, though ordering the committed spends (if there are multiple
spends on the same key) cant even tell how long the commitment payment
chains are).
Obviously the first coins in the network ordered committed coins on the same
key up to the coin value are spends as verified by the recipient, the rest
are double-spend and ignored.  If someone wants to waste fees by sending
more spends than there inputs thats up to them.
Probably the typical user doesnt care about long committed chains  other
than their wallet will bloat if the chains are too long, so probably they
would periodically compact it by revealing the long chains.  Committed coins
are probably a bit less SPV client friendly, though with correct formatting
in the merkle trees between blocks, probably a committed coin holder can
provide enough proof to an SPV client to verify even multi-spend committed
coins directly (without a network feed).
About privacy, up to the entire commitment chain can be opened at any time
(to other people or to the bitcoin network in general) with the cooperation
of any user on the chain (up to the point they saw it), so while the blind
commitment protocol is not vulnerable to a > 50% power quorum unilaterally
imposed policy (without even needing client updates), it is fully dependent
on the good will of the recipients for its temporary unlinkability.  Thats
the point: it puts policy control in the users hands not in the > 50% power
If you want cryptographic anonymity its better to look to zerocoin.  You may
have noticed zero coin talked about optional fraud tracing.  Its usually
trivial to add tracing to an otherwise privay preserving protocol.
The blind commitment if implemented as described (and its not obvious how to
get more privacy from it) offers somewhat like community policing.  Users on
the chain can still themselves do fraud tracing, or any policy they choose,
on any blind committed coins that they receive.  If they dont like the
colour of them they can refund them.  The point is to enforce that this is a
free uncoerced community choice, by individual end users, not a > 50% cpu
power quorum choice surreptitiously imposed.

@_date: 2013-05-16 01:40:30
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
btw I posted some of this thread on the dev forum:
A related idea is occuring to me that maybe these committed transactions
could actually as a side effect make bitcoin scale slightly better by
reducing the p2p flood filled transaction size.
As I said on the forum:
Surely its lower bandwidth for nodes to send only committed transactions to
the p2p network, and pass committed payment chains direct to recipients.
Say committed transaction size is c (20bytes+32bytes+16bytes +header ~ 72
bytes?) And full transaction smallest size is t (txin=20bytes, amount out
4bytes, sender pub key 32bytes, recip address 20bytes, change address
20bytes, formatting 5 bytes, ECDSA signature 64bytes, script 10 byte surely
~ 175bytes)?  Thats over twice the size.  Probably average more, and it is
sent to every node.  Its always going to be lower bandwidth to send
transactions to the recipients than to send to the network, even if you have
to increase the transaction size with each respend.  The alternative is for
the entire network to see the same transaction.
I think the commitment needs to bind the two parts together eg (blind-sender, auth-tag, tx-commit)
blind-sender = SHA1( SHA256( 1, pub ) )
auth = HMAC-SHA256-128( K, tx-commit )
tx-commit = SHA-256( tx )
Or some variantion, and you must not reuse the pub key, and must send change
if any to a different address, otherwise chain recipients or malicious
forwarders could lock your coin, by putting random junk onto the network
which would be unverifiable, and non-disclaimable - you cant prove you dont
know the preimage of some junk.  The MAC prevents it.  Maybe there's a more
compact way to do it even, but that works efficient demonstration of
security feasibility.
Other public key variants could be possible, P = xG is the ECDSA public key,
x the private key, G base point.  Sender could reveal P' = cP, c some fixed
constant (computing c from cP is ECDL problem considered oneway & hard), and
a signature by private key x' = cx over the tx-commit.  That is a publicly
auditable commitment also, but one tht can make an ECDSA signature over the
tx-commit hash, and can be revealed by revealing P later.  However that
imposes public key computation on the validation (which it already currently
does, but faster validation as above is nicer).  With that one you dont even
have to verify the transaction signature on reveal :)  You already did it,
just provide the tx that hashes to the signed hash, and P for the recipient
to verify the signature was made by cP.

@_date: 2013-05-16 13:32:22
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
I believe the coin size and verification cost is linear not quadratic, but
maybe it depends on the parameter you're measuing in.  The coin size is
linear with the number of committed (uncompacted) spends.  You can view
reveals as committed compaction.  For efficiency a recipient of a committed
coin may as well compact and spend in one transaction so no new messages are
Btw I believe if one were concerned about the committed coin size, I can see
a small tweak that would keep the size of the committed coins small eg
256-bit regardless of number of spends (no longer grows), and let the block
store the encrytped & MACed commitment.  Then compaction is no longer a
concern.  However I think that is SPV -> SPV client unfriendly.  (A full
client -> SPV client should still be workable as the full client could
alternatively send the client the MACed data and key, rather than have him
look at it from his block history.)  (Crypto sketch below).
However I am not sure multi-spend committed coin size is really a concern
because to the extent people hold long commitments without revealing to the
network for the long term, that is a bandwidth saving to the network.
Overall about privacy it would be typically temporary, though the peers have
the technical means to react and defend themselves by using longer committed
chains if dishonest mining is detected on a significant scale.
That was the seed idea.  The more aggressive "spend lots of times in
committed form" is just a technical threat that will keep dishonest mining
in check.  By definition the coin is already irrevocably spent before the
reveal (without the threat of having the dishonest miners endlessly redoing
their own deeply burried work).  The only person who could be punished by
policy by >50% dishonest miner (retroactively) is the recipient, not the
spender, and the punishment is very muted: all he can do is prevent coin
compaction.  If the committed coins are small, compact doesnt even hurt the
committed coin user, just network itself.  Therefore a dishonest miner is
wasting his time his dishonesty cant enforce his dishonest policy.
To store the commitments in the block chain replace:
(blind-sender, auth-tag, encrypted-tx-commit)
then a reveal is just to send the recipient the public key (32 bytes)
per hop, still linear but ~3x smaller.
I suggested fixed size committed coin spends, that also you can do but with
public key crypto needed probably, and so dropping to the verification
efficiency of standard transactions.  Sketch 2:
(blind-sender, auth-tag, encrypted-tx-commit)
(pub key P = xG, G = base point)
as K is random, knowledge of P if stored unencrypted does not allow
committed spend-to-junk.  To reveal to a recipient just send them P and K at
each hop.  (Same K each time, anyone on the committed coin spend chain can
already chose to reveal at any time so no loss of security.)
You dont need to verify a second signature inside the tx-commit because you
already signed the encrypted-tx which binds to it (encryption with out MAC
is malleable but you cant change it at all without invalidating the
encryption).  Just need to check the input tx in the tx-commit has P as its
recipient.  P does not even need to go into tx-commit as its already bound
by cP and signature security (cant create a signature with someone elses
key).  So I think the commited coins of this form are the same size and
verification cost for the network.  And small and fixed size to spend
offline.  (32+32=64 bytes fixed).
(*) You should not as a principle re-use keys across algorithms, I omitted a
second key for simplicity.  Really K1 = SHA256( 1||pub ), K2 = SHA256(
2||pub ) encrypted-tx-commit = AES( K1, tx-commit ), auth = HMAC( K2,
encrypted-tx-committ ).  Or more simply a combined authenticated mode like
CCM or GCM and a single key managed by the mode.

@_date: 2013-05-16 16:51:09
@_author: Adam Back 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
More somewhat improved crypto stuff...
Actually same K every time is not so hot, as then earlier in the committed
spend chain, can force a reveal for someone later.  A clearer requirement is
that each person should only be able to reveal committed coin chains up to
the point of their direct involvement.
So that is easily fixable, just include the K for the input committed coin
in the encrypted-tx-commit, as above but:
(different K for each spend).
And actually for symmetric encrypted variant the coin as specified was
already evaluatable with fixed size committed spend (of the last public key)
- I just didnt realize it in the previous mail: the input public key is
necessarily revealed when processing the decrypted tx-commit, allowing
identification and validation of the txin, and validation recursively back
to the first non-committed coin.  With symmetric verification, the
limitation is one-use coin committed addresses (and inability to remove
spend to committed junk with public validation, though there is the tx fee
as a discouragement, it does bloat a recipients verification and so maybe
frustates SPV->SPV consumption of committed coins).
(blind-sender, auth-tag, encrypted-tx-commit)
         blind-sender = SHA1( SHA256( 1, pub ) )
         auth = HMAC-SHA256-128( K, encrypted-tx-commit )
         encrypted-tx-commit = AES( K, tx-commit )
         K = SHA-256( pub )
ps and it would be better and clearer to read also in terms of purpose of
hashes, to use a KDF like IEEE P1363 KDF2, or PKCS PBKDF2 with 1
iteration, rather than adhoc hashes for key derivation.

@_date: 2013-05-19 15:23:59
@_author: Adam Back 
@_subject: [Bitcoin-development] is there a way to do bitcoin-staging? 
Is there a way to experiment with new features - eg committed coins - that
doesnt involve an altcoin in the conventional sense, and also doesnt impose
a big testing burden on bitcoin main which is a security and testing risk?
eg lets say some form of merged mine where an alt-coin lets call it
bitcoin-staging?  where the coins are the same coins as on bitcoin, the
mining power goes to bitcoin main, so some aspect of merged mining, but no
native mining.  and ability to use bitcoins by locking them on bitcoin to
move them to bitcoin-staging and vice versa (ie exchange them 1:1
cryptographically, no exchange).
Did anyone figure anything like that out?  Seems vaguely doable and
maybe productive.  The only people with coins at risk of defects in a new
feature, or insufficiently well tested novel feature are people with coins
on bitcoin-staging.
Yes I know about bitcoin-test this is not it.  I mean a real live system,
with live value, but that is intentionally wanting to avoid forking bitcoins
parameters, nor value, nor mindshare dillution.  In this way something
potentially interesting could move forward faster, and be les risky to the
main bitcoin network.  eg particularly defenses against It might also be a more real world test test (after bitcoin-test) because
some parameters are different on test, and some issues may not manifest
without more real activity.
Then also bitcoin could cherry pick interesting patches and merge them after
extensive real-world validation with real-money at stake (by early

@_date: 2013-05-31 20:14:23
@_author: Adam Back 
@_subject: [Bitcoin-development] Decentralizing mining 
I like this idea a lot.
To add: I think it is a bug and security risk if pooled-solo or (current
pooled miners) do not add randomness to their extraNonce2 (like 128-bits of
it).  For privacy and to avoid various hostile-pre-mining attacks it should
be done this way.  Lack of the self-chosen challenge field is the reason
Satoshi's first year mining is marked (plus forgetting to reset the
counter).  (Bitcoind I believe considered the direct miners key as defense
enough as a stand in for self-chosen challenge, which has a few problems).
The base counter I think is only 32-bits, the extranonce2 itself being
random can be incremented while still looking random.  But incrementing
extranonce directy while initializing it to 0 is not good (per previous
mining extranone marked coins bug - is that even fixed?)
(You dont want to  reveal the miners power in his pool shares, if the full
counter is revealed with no randomness it also reveals how many iterations
he can do since the block start).

@_date: 2013-11-04 12:10:38
@_author: Adam Back 
@_subject: [Bitcoin-development] Zeroconf-safe tx replacement 
Might leak less wiggle room and be simpler/more robut to validate that
*everything* has to be the same except for the amount going to one (presumed
change) address.  A privacy leak I know, but dont do that - ie send enough
change the first time.  And network analysis has shown change addresses
arent adding hardly any privacy.
We need more robust privacy fixes independently.  I do not support damaging
the 0-conf feature, so I think this later approach is a better track for
revising fees.

@_date: 2013-11-07 21:05:34
@_author: Adam Back 
@_subject: [Bitcoin-development] comments on selfish-mining model (Re: BIP 
(Talking about the paper, not the BIP).  With regard to racing the other
winner which catches up when private pool length=1:
i) the model does not appear to take into account that when another pool
goes on to mine a block, and the attacker publishes their selfishly-withheld
block, the selfish pool will not be able to change the existing winners
mind.  This is not insignificant as the pools have 30%, 20%, 15%, 7% etc. ii) The miners already have an incentive, as other big bitcoin processors,
to maintain fast, secure and redundant links to other significant miners. The attacker is giving up a large proportion of their winnings from the
times that they win at all.  Say the attacker IS the 30% pool, when he wins
and waits for someone else to win, > 80% of the network is pool mined, so
there is a good chance that the other winner individually represents a
non-negligible proportion of the network or a sufficiently well connected
portion of the network that the attacker will be unable to race them to
publication with a useful proportion of the network.
iii) Also broadcast is not instantaneous, lets say network propagation takes
10 seconds; a big proportion of the time, the actual mining times will be
more than 5 seconds apart so that by the time the selfish miner learns of
the block, much of the network will already have accepted it block as first.
iv) Even within the 10 seconds ambiguity period, the more powerful miner
will tend statistically to come first, and so reach a bigger portion of the
network, as well as having a stronger incentive to maintain links as in ii).
These four factors erode the achievable \gamma parameter.  I suspect it
unlikely \gamma>0.5 would be achievable, putting the profitable threshold
\alpha in 25% - 33%.  (And assuming whatever techniques to reduce latency
are used by the selfish pools can be used by other pools.)
Your main result that even with \gamma=0 (if you dont win any races) that
you still win once the selfish pool reaches 33% is an important new
indication, which needs further consideration.  (And you could expect to win
some percentage \gamma>0 even with the factors I mention, and full
implementation of the same latency reduction techniques in all moderate
sized miners, selfish and normal).
It is also not clear what will happen if multiple selfish miners compete
with each other.  A selfish miner cooperating as a peer to increase
percentage runs risk of mutual sabotage - he has to announce his private
block to his co-conspirator, and the co-conspirator may publish, or collude
with another non-selfish miner.  Your supposition is there is a profit motive to collude.  However there are
other profit motives in bitcoin that are not exercised - for example there
were for sometime 2 pools that had excess of 50% power, and yet this was not
abused for double spending.  Of course increasing profit by a new mining
strategy is not theft as double spending which has a clear loser.  Miners
even exercised restraint and volutarily avoided growing over 50%.
As others have I think said by now analysis is welcome.  It seems that Peter
Todd may have observed the same or something similar wrt miner incentives
some months ago, though it wasnt as widely read nor formally verified.  It might be useful to release the source for your simulator if that is open
to you.
In my opinion a constructive direction for reducing centralization risks is
to try to reduce the use of and motivation for pools.  Even at <51% per pool
there is (probabilistic) miner risk in double-spends.  And there is risk
that the large miners evolve to become a defacto policy enforcement point
for policies not aligned with user interests, or with fungibility of bitcoin
which itself presents another kind of risk (defacto reduced fungibility
should this arise would also be bad for bitcoin).
Also without even having mining power, there is scope to network hacking (eg
of routers in front of miners) to influence the mining profit, and even
double spend.  As I mentioned large miners have an incentive to maintain
secure redudant links (probably some links using Tor for blocks) as a
counter-measure.

@_date: 2013-11-15 10:59:59
@_author: Adam Back 
@_subject: [Bitcoin-development] moving the default display to mbtc 
While we're discussing the emotive (though actually of real relevance for
bitcoin user comprehension and sentiment) I couldnt resisnt to add some
trivia reference it is amusing that a currency rarely in history had to
deflate (remove 0s) rather than inflate (add 0s).  Viz this hyperinflated
fifty trillion zimbabwe dollar note I carry in my wallet for bitcoin
contrast/amusement purposes:
I like Alan's suggestion to show both to avoid denomination confusion.  That
is the one danger, and high risk given irrevocability.

@_date: 2013-11-21 22:11:57
@_author: Adam Back 
@_subject: [Bitcoin-development] bitcoin 1.x & 0.x in parallel (Re: is there a 
Yeah but that sounds pretty much like test-net and starts a new digital
scarcity on an alpha-qa level network, with an implied promise that maybe if
you're lucky your coins might survive the alpha testing and have some value.
I'm not talking about some slightly stabler version of test-net.
Probably bitcoin staging is the wrong name.  I mean like development of
bitcoin 1.x in parallel with bitcoin 0.x which includes like test net for
both, and strong (though maybe not quite as high) assurance of qa and care
as bitcoin 0.x.  Just as a way to get features like Mark Freidenbach's
freimarket script extensions, and some of the disabled scripts validated on
1.x testnet and then after rigorous testing deployed onto 1.x  Because they
are new features even with good testing that introduces non-zero risk, hence
the 1 way peg idea.  Welcome to suggest better names for the idea...
Of course maybe the other issue is insufficient people with the skills and
motivation to support two parallel efforts.
It gives somewhere to code and test and then deploy clearly useful things
but that dont warrant a hard fork.

@_date: 2013-10-01 16:26:03
@_author: Adam Back 
@_subject: [Bitcoin-development] homomorphic coin value (validatable but 
Thanks for providing the impetus to write down the current state, the
efficient version of which I only figured out a few days ago :)
I have been researching this for a few months on and off, because it seems
like an interesting construct in its own right, a different aspect of
payment privacy (eg for auditable but commercial sensistive information) but
also that other than its direct use it may enable some features that we have
not thought of yet.
I moved it to bitcointalk:
Its efficient finally (after many dead ends): approximately 2x cost of
current in terms of coin size and coin verification cost, however it also
gives some perf advantages back in a different way - necessary changes to
schnorr (EC version of Schnorr based proofs) allow n of n multiparty sigs,
or k of n multiparty sigs for the verification cost and signature size of
one pair of ECS signatures, for n > 2 its a space and efficiency improvement
over current bitcoin.

@_date: 2013-10-01 21:11:43
@_author: Adam Back 
@_subject: [Bitcoin-development] homomorphic coin value (validatable but 
Err actually not (efficient) I made a mistake that came out when I started
writing it up about how the t parameter in the proof relates to bitcoin
precision and coin representation (I thought t=2, but t=51).  Damn!  Back to
the not so efficient version (which is more zerocoin-esque in size/cost), or
the more experimental Schoenmaker non-standard p, q non EC one, or other
creative ideas to change the coin representation to simplify the proof (of
which this was a failed attempt).  See the bitcointalk thread for details.

@_date: 2013-10-07 21:01:03
@_author: Adam Back 
@_subject: [Bitcoin-development] homomorphic coin value (validatable but 
An update on the homomorphic coins, some more math validation & a test
implementation needs to be done, but a surprisingly good outcome so far of
predicted 2.5kB homomorphic valued coin.  Only coin splitting has to incur
the 2.5kB range proof.  Coin adding, full spending and mining is "free",
because adding existing range proofed and validated coins cant overflow by
definition (21 mil coin cap).  You can also (obviously I guess) add a
homomorphicaly encrypted "0" value to a few other peoples coin balance to
get a kind of taint mitigation.

@_date: 2013-10-10 16:21:54
@_author: Adam Back 
@_subject: [Bitcoin-development] malleability work-around vs fix (Re: 0.8.5 
Determinstic ECDSA signature aka k=H(d,m) insead of k=random, with signature
(r,s) calculated r=[kG].x, s=k^-1(H(m)+rd) with public key Q=dG and
verificaton relation [H(m)s^-1G+rs^-1Q].x=?r is cool and should be done. Otherwise RNG issues like EC_DRBG or even leaked partial bits like the RNG
bias in the original DSA spec that Bleichenbacher pointed out and then they
But k=random and k=H(d,m) create compatible signatures - or were you eaning
to cross check the two implementations with fuzz tester on lots of messages?
btw about malleability:
other than the ASN.1 related parsing ambiguity, if any (openSSL asn.1
parsing code is evil and shold not be used), the (r,s) vs (r,-s) ambiguity
can be plugged as discussed (eg define -s as invalid).  But that is ECDSA
specific, and signature malleability and its impact is a generic problem. Its probably a non-requirement of a signature scheme in terms of the
analysis effort put in by cryptanalysts that the signature itself be
non-malleable, eg there are some encryption schemes which are publicly
reblindable, like Elgamal.  By plugging the (r,s), (r,-s) specific case as a
DSA specific work-around there may be other malleability even in DSA, unless
someone has a clear proof that there is not.
And we may want to add ECS (schnorr) because it's simpler and allows more
flexibility and efficiency (eg native n of n multisig at the storage cost of
1 signature vs n with ECDSA, and k of n threshold signature at the cost of 1
sig (but some threshold secret share setup up front).  The relying party
doesnt need to know how many multi-sigs there are there is a single public
So I was thinking a more generic / robust way to fix this would be to change
the txid from H(sig,inputs,outputs,script) to H(pubkey,inputs,outputs,script)
or something like that in effect so that the malleability of the signature
mechanism doesnt affect the security of conditional payments.

@_date: 2013-10-10 17:06:03
@_author: Adam Back 
@_subject: [Bitcoin-development] malleability work-around vs fix (Re: 
btw if I got that right, it means you dont even have to fix the asn.1 level
ambiguity (though its a good idea to remove openSSL asn.1 parsing code) to
have conditional payments using not yet broadcast txid outputs as inputs to
work with high assurance.  (And even in the event that a new crypto level
malleability is discovered in ECDSA it remains secure.)

@_date: 2013-10-14 20:08:07
@_author: Adam Back 
@_subject: [Bitcoin-development] is there a way to do bitcoin-staging? 
Coming back to the staging idea, maybe this is a realistic model that could
work.  The objective being to provide a way for bitcoin to move to a live
beta and stable being worked on in parallel like fedora vs RHEL or odd/even
linux kernel versions.
Development runs in parallel on bitcoin 1.x beta (betacoin) and bitcoin 0.x
stable and leap-frogs as beta becomes stable after testing.
Its a live beta, meaning real value, real contracts.  But we dont want it to
be an alt-coin with a floating value exactly, we want it to be bitcoin, but
the bleeding edge bitcoin so we want to respect the 21 million coin limit,
and allow coins to move between bitcoin and betacoin with some necessary
security related restrictions.
There is no mining reward on the betacoin network (can be merge mined for
security), and the way you opt to move a bitcoin into the betacoin network
is to mark it as transferred in some UTXO recognized way.  It cant be
reanimated, its dead.  (eg spend to a specific recognized invalid address on
the bitcoin network).  In this way its not really a destruction, but a move,
moving the coin from bitcoin to betacoin network.
This respects the 21 million coin cap, and avoids betacoin bugs flowing back
and affecting bitcoin security or value-store properties.  Users may buy or
swap betacoin for bitcoin to facilitate moving money back from betacoin to
bitcoin.  However that is market priced so the bitcoin network is security
insulated from beta.  A significant security bug in beta would cause a
market freeze, until it is rectified.
The cost of a betacoin is capped at one BTC because no one will pay more
than one bitcoin for a betacoin because they could alternatively move their
own coin.  The reverse is market priced.
Once bitcoin beta stabalizes, eg say year or two type of time-frame, a
decision is reached to promote 1.0 beta to 2.0 stable, the remaining
bitcoins can be moved, and the old network switched off, with mining past a
flag day moving to the betacoin.
During the beta period betacoin is NOT an alpha, people can rely on it and
use it in anger for real value transactions.  eg if it enables more script
features, or coin coloring, scalabity tweaks etc people can use it. Probably for large value store they are always going to prefer
bitcoin-stable, but applications that need the coloring features, or
advanced scripting etc can go ahead and beta.
Bitcoin-stable may pull validated changes and merge them, as a way to pull
in any features needed in the shorter term and benefit from the betacoin
validation.  (Testing isnt as much validation as real-money at stake
survivability). The arguments are I think that: - it allows faster development allowing bitcoin to progress features faster,
- it avoids mindshare dilution if alternatively an alt-coin with a hit
   missing feature takes off;
- it concentrates such useful-feature alt activities into one OPEN source
   and OPEN control foundation mediated area (rather than suspected land
   grabs on colored fees or such like bitcoin respun as a business model
   things),
- maybe gets the developers that would've been working on their pet
   alt-coin, or their startup alt-coin to work together putting more
   developers, testers and resources onto something with open control (open
   source does not necessarily mean that much) and bitcoin mindshare
   branding, its STILL bitcoin, its just the beta network.
- it respects the 21 million limit, starting new mining races probably
   dillutes the artificial scarcity semantic
- while insulating bitcoin from betacoin security defects (I dont mean
   betacoin as a testnet, it should have prudent rigorous testing like
   bitcoin, just the very act of adding a feature creates risk that bitcoin
   stable can be hesitant to take).
Probably the main issue as always is more (trustable) very high caliber
testers and developers.  Maybe if the alt-coin minded startups and
developers donate their time to bitcoin-beta (or bitcoin-stable) for the
bits they are missing, we'll get more hands to work on something of reusable
value to humanity, in parallel with their startup's objectives and as a way
for them to get their needed features, while giving back to the bitcoin
community, and helping bitcoin progress faster.
Maybe bitcoin foundation could ask for BTC donations to hire more developers
and testers full time.  $1.5b of stored value should be interested to safe
guard their value store, and develop the transaction features.

@_date: 2013-10-15 15:34:46
@_author: Adam Back 
@_subject: [Bitcoin-development] two comments on brain-wallet security (and 
So I had a go at deciphering BIP 038 in summary what I think its doing is
(ommitting lot and sequence and deterinistic IVs for simplicity):
x1 = Scrypt( salt=random, pass )
P = x1*G
send (salt, P) to coin manufacturer -> verify code c:
(by recreating P, then k from Q & P, decrypt c to get B, check Q = x1*B)
x1 = Scrypt( salt, pass )
P = x1*G
k = Scrypt( salt2=H(Q)||salt, pass=P )
Which seems reasonable enough, however its unfortunate that you have to
repeat the Scrypt work at setup.
One thing that occurs to me eg as mentioned by Rivest et al in their
time-lock puzzle paper is that it is easy to create work, if you are ok with
parallelizable symmetric constructions (like scrypt(i) or PBKDF2(i) with i
iterations) without *doing* the work during setup.
It seems to me therefore that the above protocol could avoid the javascript
overhead issue that forces users to choose a weak iteration level if they
want to create the wallet in that way.
eg create a 32-bit random salt, replace scrypt(i=16384, salt, pass) with
scrypt(i=1,salt, pass) to be brute forced based on deleted salt.  Immediate
2^32 = 4billion iteration salt without any significant setup cost.  (Or if
you want to limit the parallelism say scrypt(i=65536, salt, pass) with a
deleted 16-bit salt.  That should be parallelizable up to 65536 GPU cores
(32x 7970 chips).
Symmetric time-lock puzzles can achieve decrypt asymmetry without repeating
the work at setup...
(Rivest et al goes on to avoid using that symmetric construct with an RSA
related mechanism, because they are trying to lock information for an
approximate future date, rather than protected by a specific amount of
grinding work.)
I proposed a different blind (securely server-offloadable) deterministic
proof of work relating to (asymmetric RSA-style) time-lock puzzles.  The
difference from time-lock is it is made blind (so the work can be securely
offloaded without the server learning your password or resulting key) and
can be easily made parallelizable also which is desirable for server
I think that could take brain-wallets to a new level of security, if you
protect the amount by an amount of computation proportional to the value, eg
0.1% or 0.01% redemption cost paid to blind proof of work miners.

@_date: 2013-10-28 13:14:33
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
I think its a mistake relying directly on X509, its subject to corrpution
attacks, involves ASN.1 and enough openSSL X.500 encoding abiguity (or other
code base) to be a security nightmare.
Why not make the payment messages signed by bitcoin keys.  If someone wants
to associate with X.509 they can put a bitcoin address on their SSL site.
If someone can get into your site deeply enough to modify your SSL served
code and you're trying to run ecommerce you have other problems.
Never the less if they care they can clear sign the bitcoin addr with xmlsig
and their X.509 private key, and/or with PGP and a WoT.
I think its smarter to pollute bitcoin main with X509.  Make a little helper
util if there are not enough xmlsig tools that you cant pick one up
opensource for multiple languages.
This then avoids the binding to Tor - you can publish a bitcoin address
authenticated anyway you like.  Eg tahoe-LAFS auth/integrity, i2p, tor, pgp
- you name it.
Maybe I voice this opinion a bit late in the cycle, but I think it would do
bitcoin a favor otherwise its a camels nose in the tent into the TLAs that
control their own X.509 CAs, or issue NSL letters for CA private keys, or
forged certs.  It's all happening and thanks to Snowden we now have even
more evidence...

@_date: 2013-09-29 11:37:08
@_author: Adam Back 
@_subject: [Bitcoin-development] smart contracts -- possible use case? yes 
There are some policy decision points in the protocol (and code) that may
become centralized risks or choke points that undermine the p2p nature.  So
the extent that those can be argued to have in principle have a technical
fix, it could be quite interesting to research the necessary technology
(advanced crypto, byzantine networking argument etc) that could address
them.  eg payee/payer blacklisting by a large group of miners and "committed
transaction" proposal to address it.
However even for that type of thing I think bitcoin-dev would probably
rather focus eg on something that has reached the stage of having a BIP.  Eg
it might be better to discuss early stage or speculative ideas on
bitcointalk.org technical thread.
taxation in particular there are examples where even the political sphere
accepts significantly anonymous taxation.  eg for europeans with certain
types of investment in a swiss bank, the swiss bank sends however many
million as a single payment across all users per european country to their
passport home country (minus 25% cut for the swiss government).  Perhaps
such things could be possible for bitcoin.  Again I think bitcoin talk would
be a good place for such a discussion if that was the OP question

@_date: 2014-04-16 13:06:27
@_author: Adam Back 
@_subject: [Bitcoin-development] mid-term bitcoin security (Re: Warning 
Big picture/mid-term I think air-gaps and zero-trust ecosystem components
are the only solution.  (zero-trust meaning like real-time auditability, or
type 2/type 3 exchanges based on atomic-swap, trustless escrow etc).
Need a mass-production and air-drop of trezors :)
There is one more problem address-substitution via untrusted network/user
and weak site with 1mil lines of swiss-cheese security app-store.  So some
kind of address authentication TOFU.  Aside from X509 bloatware which could
be extended from payment protocol to do that, I'd argue for a native simple
TOFU format like Alan Reiner's multiplier * base approach (where base is the
TOFU handle).  And/or something like the IBE address proposal (which gives a
bandwidth efficiently SPV queryable way to check if funds received).  Worst
case if weil-pairing gets broken it auto-devolves to the current status
Btw not to reignite the stealth vs reusable address bike shedding, but
contrarily I was thinking it maybe actually better to try to rebrand address
as "invoice number".  People understand double paying an invoice is not a
good idea.  And if they receive the same invoice twice they'll query it.

@_date: 2014-04-16 21:43:10
@_author: Adam Back 
@_subject: [Bitcoin-development] Warning message when running wallet in 
Not to get snarky or OS elitist but as I understand it windows security,
even during its support period has been measured in low digit number of days
in the year when is NOT an outstanding known remote root compromise or
combination of remote user compromise + priviledge escalation.  Add in
phishing, watering holes, malware and the average windows computer is
probably compromised a dozen times over.  Apparently for sometime it was not
easily possible to secure it install boot - install OS, connect to network
to download security updates, IP range scanned and compromised faster than
you can patch it.

@_date: 2014-04-28 14:41:02
@_author: Adam Back 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Nicely put.
I agree the idea of a populist vote to redistribute or remove mining reward
is an inelegant thing which would probably devolve into politics.
I think the reason that it would likely work out badly is that its not
provable, and so no consensus rule can be constructed requiring proof, so
then it risks devolving to a political decision.  Step 1: Finney attackers for hire anonymize their blocks (publish via Tor,
use a different reward address for each block, and each pool miner). Demanding identification of blocks is generally undesirable for the
objective of avoiding centralization and policy abuse.  Dont even think
about demanding identity, there is no identity in a distributed system.
Step 2: people send tracer payments through Finney attackers, and use that
evidence to decide to vote away their reward.  (However the proof is
non-transitive so people can vote anyway they like for any reason).
Step 3: Finney attackers vote down other pools to make the point.

@_date: 2014-12-21 10:01:37
@_author: Adam Back 
@_subject: [Bitcoin-development] The relationship between 
Well while you cant prevent it you could render it insecure enabling
miners to take funds.
That could work via a one-show signature; normal ECDSA being address
a=H(Q), public key Q=dG, R=kG, r=R.x, s=(H(m)+rd)/k, signature (r,s),
verify: a=?H(Q) and sR=?H(m)G+rQ one-show being: a=H(Q,R), verify
being: a=?H(Q,R) and sR=?H(m)G+rQ.  Now that is unsafe to double-spend
by design as only that specific R is usable and as we know reusing R
with different messages leaks the private key because: s=(H(m)+rd)/k
and s'=(H(m')+rd)/k implies sk=H(m)+rd and s'k=H(m')+rd so
k=(H(m)-H(m'))/(s'-s), and d=(sk-H(m))/r.

@_date: 2014-12-21 18:10:47
@_author: Adam Back 
@_subject: [Bitcoin-development] one-show signatures (Re: The relationship 
Yes you could for example define a new rule that two signatures
(double-spend) authorises something - eg miners to take funds. (And
this would work with existing ECDSA addresses & unrestricted R-value
I wasnt really making a point other than an aside that it maybe is
sort-of possible to do with math what you said was not possible where
you said "This [preventing signing more than one message] is
impossible to implement with math alone".
On 21 December 2014 at 15:29, Peter Todd

@_date: 2014-12-22 20:05:40
@_author: Adam Back 
@_subject: [Bitcoin-development] The relationship between 
(Again nothing new to say here, just putting my notes in this
discussion, where I started with an earlier discussion that Peter
wrote up with a subject of "disentangling" blockchain design).
In the discussion last year that started the analysis of
"disentangling" blockchain design I had broken out the candidate layer
properties that one could use as building blocks to construct a
decentralised PoW-chain assured immutable history based ecash system
- time-stamping (really just time-ordered as network time is weak)
- namespace (first come first served name value pairs)
I thought it was interesting to look at potential minimum enabling
functionality in order to explore whether the consensus critical code
could be simplified for security, and also to understand the tradeoffs
towards seeing if there were any improvements that could be found.
(And it seems its pretty hard to find any improvements was my
Time-stamping (or time-ordering) at a requirements level does not have
to imply that there is a uniqueness guarantee, or even that the nodes
see what they are time-stamping (it could be committed with a random
nonce) and indeed hiding the committed data from the service and
public view is a common property of time-stamping.  Time-ordering just
creates an immutable (and not strongly deduplicated) stream of data
items that came from various users and had a time-ordering placed on
Minimally the person who submitted the data item would need to know
the merkle path to it, and that might be achieved by publishing the
merkle tree, where some or all of the leaves are hidden commitments.
For bitcoin composability purposes you might require that there be no
hidden commitments, and then other miners and full nodes could
download all the merkle trees for each PoW-interval and ignore
Namespace service adds the uniqueness and first-come first-served
property up-front (as its more efficient for people catching up to not
have to download and discard duplicates/double-spends), and this more
strict rule requires miners to know about (and presumably index) all
previous information to avoid violating this rule. I assume name
attributes to hold information like a public key to approve changes in
ownership, an IP address, an email address etc.  For efficient proof
reasons there is still a merkle tree per PoW time-interval binding
names into a hash-chain.
For bitcoin re-described using a namespace the unique coins are the
names, and values and ownership public key etc are attributes of the
name; names (coins) are only added (via mining) or after deletion
(spend/transfer) of previous names.  Transfers are approved via
digital signature.
The additional property bitcoin requires is that the values add up.
I presume the phrase proof of publication means to draw out separately
that the full-node version of bitcoin requires a rule that miners
should not build on top of blocks unless they have copies of all data
committed to.  Otherwise a malicious party can hide ownership
transfers that can be revealed later, so that no one is assured of
ownership: any possibility for a gap in the ownership chain calls into
question ownership.  So from that perspective a miner consensus rule
that it should not build on top of blocks that it hasnt seen a full
gap-free history for makes the PoW chain a kind of proof that the
miner population at one time saw all data hashed into it.
I think you need one more thing which is that the miners (and other
full nodes who have copies of the data) are willing to share that
historic data with you.  There is some meta-incentive for bitcoin
holders to help others catchup and be assured of the history and
information has to be broadcast as there are many miners and
I presume anti-relay term is meant at system level, rather than node
level, though technically bitcoin nodes in the current protocol
version dont relay double-spent transactions.  Particularly that
miners wont bless double-spent transactions (and will do PoW only over
non double-spent transfers).
While there does seem to be some confusion from some people perhaps
not realising that it is essential that there are no gaps in the
ownership chain, I am not sure there are necessarily any practical
implication of philosophical differences between proof of publication
& anti-relay (or namespace for that matter).  It is also important
that there is no way to attack the insertion logic so that eg someone
cant get a hash into an internal nor leaf node of the merkle tree
without the miners first seeing that data.
Presumably as they are all describing ways to think about bitcoin and
assuming no one is confused about how bitcoin works, the distinction
just comes down to what features are assumed to be naturally included
in the layer definition, and what features have to be added.  For
example I think its relatively normally assumed that people can look
up names.
I suppose it might be possible to put a self-authenticating access
handle for the data item into the data set which points into a
redundant immutable data store.  In effect that is what the bitcoin
nodes do provide (with full redundancy).  But, more efficiently though
perhaps with less redundancy and assurance, one could put the data
into tahoe-lafs which implements immutability, append-only and
self-authenticating urls and such properties.  From that perspective
it does make sense to say there is a layer that provides assurance of
availability of history; the PoW-chain and merkle-tree in the header
assures already immutability.  The remaining thing that has to be
assured is availability.

@_date: 2014-02-02 16:26:24
@_author: Adam Back 
@_subject: [Bitcoin-development] (space) efficient reusable addr via weil 
I think you Peter & Jeremy figured it out - dont disagree with the
explanation details.
And it seems better explained between the two posts than I did.  I think
Peter is right that you do not need an explicit prefix, the prefix after
decryption can be a chosen number of leading 0s and this gives a tunable
amount of false positives.  You already have privacy becaue the query is
only revealed to the semi-trusted full node, and its query scope is limited
to one or a chosen batch of blocks.  But you can if desired add additional
ambiguity as Peter described by reducing the number of bits of 0 in the
decrypted block.  There is no need for matching a specific prefix as its
already a recipient specific calculation.  (It means the actual encrypted
value where it is chosen would have to mimic false positives: random with
n-bits of trailing 0s and expected probability distribution).
It should be compatible for combining with sharding or public prefixes, as
Peter mentioned but for short public prefixes those still has some (reduced)
blockchain ledger logged possibility to reduce anonymity set when combined
with flow analysis.
Maybe you could vary a public prefix per block.  Eg the public prefix for a
given user is the LSBits of the per block IBE derived pubic key for a given
user.  I am not sure if that helps or hinders.  Maybe it hurts anonymity set
because the analyst (Eve) is given multiple chances over time to exclude an
analysed flow candidate.
It would desirable to find a non-IBE way to do this.  (And more
computationally efficient / precomputable / indexable)
Or you could use different address types depending on the circumstance:
one-use, stealth, or IBE.  Kind of difficult to automate that (to know what
the user is planning to do with it) and avoid user confusion.  Clearly users
are quite confused and the convenient and understandable thing is to have a
(safely) reusable address.

@_date: 2014-01-03 14:09:11
@_author: Adam Back 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
You know if you want to make some form of investment, you might like make an
attempt to look them up on the internet, check the phone number in a phone
book or directory enquiries, look for references and reviews?
So it is with the hash of the binary you are about to trust with your
investment funds.  I dont think its such a difficult question.  Ask your
more technical friends to confirm this hash is correct.
Its interesting that hashes are more trustworthy than signatures, since all
the NSLs and backdoors, its hard to trust a signature.
I have the same problem with linux distros that want to install hundreds of
components downloaded over the internet, based on signatures.  I would far
rather a merkle hash of the distribution at that point in time, which
authenticates directly any of the optional downloadable components.
(Or better yet a distro that like comes on a CD and doesnt download
anything...  Amazing how most CD and even DVD iso images immediately
download stupid things like fonts???  What were they thinking?  I downloaded
fedora > 4GB of stuff and they need to download a font just to get past step
2 of the installer?  Thats a sensless, retrograde, selective backdoor

@_date: 2014-01-03 21:23:20
@_author: Adam Back 
@_subject: [Bitcoin-development] An idea for alternative payment scheme 
Seems like you (Nadav) are the third person to reinvent this idea so far :)
I wrote up some of the post-Bytecoin variants here:
The general limitation so far is its not SPV compatible, so the recipient
has to test each payment to see if its one he can compute the private key
for.  Or the sender has to send the recipient out of band the derivation
However at present most of the bitcoin infrastructure is using the bitcoin
broadcast channel as the communication channel, which also supports payer
and payee not being simultaneously online.  You have to be careful also not
to lose the key.  You dont want a subsequent payer data loss event to lose
money for the recipient.  You want the message to be sent atomically.
It does seem like a very attractive proposition to be able to fix the
address reuse issue.  Admonishment to not reuse addresses doesnt seem to
have been successful so far, and there are multiple widely used wallets that
reuse addresses (probably in part because they didnt implement HD wallets
and so are scared of losing addresses due to backup failure risks of non HD
wallets and fresh addresses).

@_date: 2014-01-14 12:41:34
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment protocol and reliable Payment 
He's probably thinking of fair advertising rules.  There are regulations
motivated by consumer protection/advertising standards (prevents merchant
listing attractive prices in media, and then when consumer goes to pay the
merchant says "oh actually that doesnt include X and Y, and the minimum
price is 10% more" after the user has already partly committed to the
purchase.  Ryanair, an airline near and dear to Europeans ;) is infamous for
aggressive use of such tactics.  Or worse systematic abuse of "sorry that
was a pricing mistake".
In trading situations its even more important, you're facing a dynamic
price, and revocable bids after acceptance but before payment allow system
gaming.  There were court cases about such things and trading systems gamed. So I think this is the use case to consider.  Payment request is an offer,
payment message is an acceptance, transaction broadcast is settlment.  After
acceptance the asker must not be allowed to retract ther ask.
Going back to Pieter's comment it seems there are two approaches: i) send
payment message to merchant, merchant broadcasts tx to network to claim
funds; ii) user broadcasts tx, and sends payment message to merchant.
In case i) the user is relying on the merchant in terms of retraction, for
many use-cases that doesnt matter, or consumer law says they can do that in
some places.  Though transferable proof the merchant is systematically
retracting advertised offers could be indirectly useful as it maybe evidence
of unfair trading, which can result in censure for the merchant!
In case ii) I think Andreas has a point.  Maybe the way to do that is to
also bind the transaction to the payment message.  Eg include the hash of
the payment message in the tx (circular ref may have to use multisig
approach?), or as Timo Hanke's paper where the offer/acceptance contact hash
is bound to the address (ie the address paid is Q'=H(Q+H(contract)G).

@_date: 2014-01-14 14:18:38
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment protocol and reliable Payment 
Maybe even pay to (address derived from) contract hash has a hole: it
assumes the merchant cashes the funds (or cashes then reimburses via the
refund address).  There is another rational (though abusive) move for the
merchant: let the buyers funds sit in limbo.  Then the buyer has the onus to
go into disupte, maybe the seller is anonymous, in another country, or the
cost of dispute resolution more than the value lost, and anyway its not very
smart-contract like.
It might be better if the buyer has time-stamped evidence of having sent the
funds to the merchant, and evidence of non-collection of funds by the
merchant (by omission from the block chain).

@_date: 2014-01-14 22:51:06
@_author: Adam Back 
@_subject: [Bitcoin-development] Stealth Addresses 
I saw in the math version you had said Q'=Q+H(S) and I presumed it was a
typo, but your code says the same thing.  I presume you meant Q'=Q+H(S)*G
and therefore that Util.SingleSHA256() multiplies by G internally?

@_date: 2014-01-16 00:09:01
@_author: Adam Back 
@_subject: [Bitcoin-development] unlinakble static address? & spv-privacy (Re: 
So I like static address name too.  In the write up for my variant I called
it something less sexy than stealth "unlinkable public address":
(there are 3 variants that are approximately the same thing).
Maybe we could call it an unlinkable static address.  Otherwise static
addresses are maybe too synonymous with reused addresses a bad practice we
have been complaining about users and wallet authors incorrectly doing.
But to explain, in Peter Todds (and Amir Taaki also?) variant stealth refers
to an actual useful security criteria.  Stealth objective actually means
"looks like a normal bitcoin payment to the outside observer".  You
generally want that to be the case for fungibility reasons.  Its like
browser cookie state, the more things that are unusual about your
transaction, the more your transactions identify you in the public
block-chain.  Statistics are cumulative so it matters.
And is an actual element of stealthiness (hence the name) in this variant
that Peter Todd proposed, at least as an objective, though I think the
stealthiness somewhat fails because the P parameter is extra and not present
in a normal transaction.
Unfortunately so far removing P and using an input in its stead breaks
CoinJoin which is also necessary for fungibility.  Maybe there is another
way to make an extended CoinJoin that can mix inputs and unlinkable static
I was meaning to comment on the SPV privacy properties.
For full-node use these unlinkable static addresses have quite nice
properties.  It also nicely solves the problem of having to educate users
and wallet authors to not reuse addresses.  But for SPV nodes they have no
direct-way to find the payments.  So then in Peter Todd's variant (maybe it
was suggested by Greg Maxwell?) there is a second address so that the SPV
client can delegate detection to a full node without giving it the private
key allowing it to spend!  (This is something analogous to bloom filtering). But I think its moderately expensive for the full node because it has to do
a DH calculation per transaction and its not precomputable so there is IO
per query.  (In the P version in fact only payments which are thereby
reconizable as unlinkable static need to be processed).
Then an artificial prefix is proposed to constrain the query to a subset,
however that leaks to everyone so in some wayts its a worse privacy leak
than bloom filtering.  It can be used to rule out recipients and could be
quite a powerful extra lever for statistical analysis.  (And also there is
proposed a version of the prefix computed via brute-force to make it
somewhat stealthy still).
So I also am quite enthusiastic about the possiblity to fix this address
reuse problem, but there remain a few open problems in my view, for SPV
uses.  Not nay-saying, I spent quite a bit of time trying to solve this for
my variant, its a tricky problem, or basically we wouldnt have one-use
addresses and bloom filtering.
But maybe its intereting enough already for full-node uses.  Many processors
and businesses are full nodes.  Many power users run full-nodes  The data
isnt lost, you just need to scan a full-node.
It could help the related problem of paying the wrong person.  Ie deposit
address given by merchant.  If the deposit address is static, and the used
address user derived from it, then that itself is an assurance to the user
that they are paying an address at least owned by the service.  (As opposed
to someone who hacked the web site or MITM the link).  Of course for users
probably the main likelihood is they have malware on their machine, but that
is what offline wallets are for.  A smartphone is maybe a little less
hackable and could be trained to store the static address and warn if its
not the same as the last time they used the site.  (TOFU for bitcoin
addresses, or at least be able to call someone you know who also uses the
service and compare static addresses).
Maybe in the payment address case the service should choose the derivation
factor and communicate it and the client with the static address, as
suggeste by Alan Reiner because then it can also serve the function of
allowing the service to tie the payment to the users account.
People also mention payment protocol for certifying addresses however I
think it is useful to have address level TOFU / static to principal
verification because it is simpler for harware wallets, maps to account
number concept users understand, and doesnt rely on the CA infrastructure. Also the typical payment protcol is talking about a message constructed by a
web app.  Thats millions of lines of web server, script language, db code
etc in play on an online server.  The static address private key would be
airgapped from that mess.  Mike Hearn proposed if I understand that you could something analogous and
upload in batches signed payment protocol sub-messages from a different CA
certificate key.  But I think the above is simpler, and its useful to have
something that works at the low level.  What we have now is like SSH without
the knownhosts cache.  Lets add it.  It can then play with the payment
protocol at the address level.

@_date: 2014-01-16 12:12:20
@_author: Adam Back 
@_subject: [Bitcoin-development] reusable address privacy problems & fuzzy 
Yeah I called my variant "(unlinkable) public" but I also think I prefer
Jeremy's "reusable address" which has the added bonus of being yet another
implied admonishment not to reuse the non-reusable ones :)
Anyway my primary concern so far is that the reusable addresses/(unlinkable)
public addresses are actually worse for privacy than SPV bloom mechanism by
any reasonable definition.  So I think we have some work to do yet, on a
tough problem which may not have an efficient index precomputable solution
(or a solution period.)  I would also have been promoting this as an
alternative solution to bloom privcy mechanism and address-reuse, if I
could've found a mechansim for the unlinkable public proposal...
Whats different so far I think is that Peter just went with it anyway
despite that problem, where as I put it in the pile of interesting but not
quite workable for privacy reasons ideas.  (Bearing in mind that my bloom
bait concept is the same as the prefix concept so I had functional
equivalence).  The additional feature of Peter's variant is to stealthify
the payment, which I do think is a useful additioanl consideration, however
as I said I think its fair to say it so far largely fails to do that,
because the exposed P parameter.  (And using the input instead of the P
parameter breaks CoinJoin, which is also thereby damaging to privacy).
So also about Greg Maxwell's improved prefix/bloom bait (lets call it fuzzy
bloom bait), while I agree that H(nonce)[rand(32)] ^ prefix is an
interesting incremental improvement, over raw bloom bait/prefix (with an
example 8-bit prefix, and [] being byte index, ^=xor), it is
index-precomputable, but it still publicly allows statistical elimination
which is still nearly as dangerous in lieu of the remarkable success people
have had doing statistical network flow analysis.  ie with probability
(255/256)^32=88% it eliminates you as a payee of any given reusable payment. (And that effect remains with any parameter set and conflicts with bandwidth
efficiency for the requestor - ie lower elimination probability seems
unavoidably to imply higher false positive match, right down to the point of
downloading the entire set, giving with 0 probability).
Thinking-hats time people.
(As I said I still like reusable-addr for full-node recipient scenarios.)

@_date: 2014-01-16 12:42:42
@_author: Adam Back 
@_subject: [Bitcoin-development] unlinakble static address? & spv-privacy 
Well only a linear increase, which is not any kind of realistic security
defense for even an academic researcher analysing flows.  More concern is
that it could be expensive enough discourage adoption by full-nodes as an
open/free service to support SPV clients in finding their reusable address
payments.  Its possibly an I/O DoS multiplier: send requests to the full
nodes at a moderate network rate and and watch as its disk thrashes.
Its not a decision with user localised effect.  If most users use it with
parameters giving high elimination probability, that affects everyone else's
privacy also.  Also statistical effects are accumulative as more plausibly
related addresses are eliminated at each potentially linked transaction.  I
think once the network flow analysis guys are done with incorporating it,
and if reusable addresses saw significant use, my prediction is the result
will be pretty close to privacy game over and it will undo most if not all
of the hard-won privacy benefit of CoinJoin.  (Recalling CoinJoin is only
adding a bit or two of entropy per join, this elimination effect could
easily undo more than that).
The point of the stealth security objective is to avoid creating a new and
smaller anonymity set.  If all reusable addresses are easily recognizable as
reusable, thats far more revealing and useful to the network flow analysis.

@_date: 2014-01-16 13:31:05
@_author: Adam Back 
@_subject: [Bitcoin-development] TOFU verifiable HD publicly derived addresses 
Put this into a separate thread about Alan Reiner's user validatable HD
address idea.
Yes it is.  This part is a separate topic, about simple TOFU cross-check &
verifiability of deposit addresses by users.
(However as we've seen in practice users treat as static and expect service
deposit addresses to be reusable.  So it could be useful to combine.)
My view is that certification (X509 or raw ECDSA message signature) is
functionally inferior (and more complex) than communicating scalar, base
address because certification requires an online private key to
authenticate.  Scalar, base does not require a private key.  In fact you
could use HD address public derivation as the mechanism to derive the scalar,
where the deposit address recipient does not know the scalar.  So I am a fan
of Alan Reiner's simple authenticatable derived address proposal.
Now of course you can take the private key offline and use eg private
derivation and upload batches, but that is complexity and work; so again its
an approach with arguably inferior characteristics.
Also to date payment message is application level, and while you could add
another level of signed message with a different offline X509 key, which
Mike Hearn did suggest as a future possibility, and upload signed addresses
in batches, its clunky by comparison, involves many more lines of code, adds
a security dependency on CAs, and I think you could somewhat argue is a
protocol layering violation.
Anyway if people want to do that, there is no loss in X509 signing a TOFU
validatable address form.  ie go ahead and combine them.  TOFU
validatability of the low level address format is useful, you can cross
One could also consider augmenting the derivation with Timo Hanke's bind to
contract hash, though there is risk that both parties lose the contract and
if it has too much entropy that could lose coins.

@_date: 2014-01-21 00:14:41
@_author: Adam Back 
@_subject: [Bitcoin-development] BIP0039: Final call 
Because the mnemonic is an encoding of a 128-bit random number using its
hash as a private key (or derived part of one) is not a problem, its just an
alternate alphabet encoding of the random private key.
Not being able to generically understand the checksum.  Seems tricky to
solve other than say brute force eg H(mnemonic||1) mod 2^k == 0 where k is
the amount of check digit redundancy.  But that might be expensive for a
trezor if k is very big at all.  And then key = H(mnemonic).

@_date: 2014-01-24 16:42:35
@_author: Adam Back 
@_subject: [Bitcoin-development] Bait for reusable addresses 
I think prefix has analysis side effects.  There are (at least) 4 things
that link payments: the graph of payment flows, timing, precise amounts, IP
addresses, but with prefix a 5th: the prefix allows public elmination of
candidates connections, I think that may make network flow analysis even
more effective than it has been.
So SPV can be tuned as Mike just said, and as Greg pointed out somewhere
bloom is more private than prefix because its a wallet to node connection,
not a node broadcast, and Mike mentioned embedded Tor in another post to
boost node-capture issues with hostile network.
So reusable addresses are cool for full node recipients (0-bit prefix) or
trusted server offload (your own desktop, VPS, or trusted service provider
node, and solve real problems for the use case of static and donation
addresses particularly with this second delegatable key for no-funds at risk
search (which is even good as Jeremey said for your own node, in a offline
wallet use case).
Now while it would be clearly a very nice win if reusable addresses could be
made SPV-like in network characteristics and privacy, but we dont have a
plausible mechanism yet IMO.  Close as we got was Greg's enhancement of
my/your "bloom bait"/"prefix" concept to make multiple candidate baits to
provide some ambiguity (still allows elimination, just slightly less of it).
If we can find some efficient crypto to solve that last one, we could even
adopt them generally if it was efficient enough without needing interactive
one-use address release.
Maybe we should ask some math/theoretical crypto people if there is anything
like public key watermarking or something that could solve this problem
For the related but different case of transaction level authenticity I like
Alan's server derived but communicated scalar & base to allow the client to
do at least TOFU.
Payment protocol may add another level of identity framework on top of TOFU
addresses (at a lower level than the payment messages defined now), and
without then needing a batch upload of offline signed secondary address
sigature that Mike described a while back, at least in person, maybe online
somewhere (an add on with similar purpose and effect to Alan's TOFU, but
then with revocation, identity and certification for merchants).
I have not talked about payment protocols main app level function I think we
all understand and agree on the purpose and use of the server and optional
client certs in that.  People may wish to add other cert types later (eg
PGP, SSH etc) but this version covers the common merchant tech, and allows
client-side certs to be experimented with for identity also (eg imagine as a
way to enrol with regulated entities like exchanges.)
Tell me if I am misunderstanding anything :)

@_date: 2014-01-25 17:19:01
@_author: Adam Back 
@_subject: [Bitcoin-development] (space) efficient reusable addr via weil 
I think I figured out a proof of existance for a space efficient way to do
better than bloom filters/prefix/bloom-bait.  (Must have been dreaming on it
as I woke up with the idea following Peter's suggestion to try prove instead
if its possible or not:).
I wrote up the details here:
In summary with a use of novel application of IBE (*) based on weil-pairing
so the recipient can send a delegation private key that is specific to the
block being queried.  It means the node that services the query has no
ability to correlate with queries in other blocks from the some user.  The
sender derives a pub=IBE-extract(master-pub, id=previous block hash).  The
above link has more explanation, links and costs/risks.
I think it maybe within possibility to do further than this because it is
not technically necessary to delegate decryption, only to delegate
filtering, which can be a simpler requirement so there remains perhaps
(speculatively) a possibility to do it without introducing weil pairing
hardness problem or using eg I mentioned public key steganography or
something like that if there is anything similarly efficient but with more
widely used hardness assumptions.
(*) analogous to the way IBE is used as a building block for Non-Interactive
Forward Secrecy (NIFS)

@_date: 2014-06-06 10:48:52
@_author: Adam Back 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
Advertising NODE BLOOM as a service sounds good.
But the critique of bloom filters, I am not so sure prefix filters are
better.  Prefix filters offer questionable privacy tradeoffs in my opinion. Same problem as with stealth address proposed use of prefixes.
All for scalability, efficiency and decentralization but not ideally at the
expense of nuking privacy.  The effects on privacy are cumulative, and
affect everyone not just the user.  Same pattern of local decision, global
effect as with reused addresses.

@_date: 2014-06-06 12:45:43
@_author: Adam Back 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
As I recall prefix brute forcing was a bit twiddle saving, ie searching for
EDH key that has the users public prefix.  That does not improve privacy
over an explicit prefix, it saves a byte or so at the expense of average 128
EDH exchanges to send and provides also some probably relatively ineffective
plausible deniability by enabling the transaction to be indistinguishable
from some other (not very common) types of transaction.
either its full node only where prefixes are not used, which is less
scalable than bloom; or prefixes are used explicitly or implicitly
(brute-force) and either way privacy is weakened by the extra correlation
hook provided by elimination from the network graph of payments with
mismatched prefixes.
We need to consider the two types of privacy involved.  Privacy from the
full node an SPV client is relying on to find its payments, vs privacy from
analysis of the public transaction graph.  The latter is more damaging. Better to design for privacy against future analysis of public info, than
privacy by argument to select non-hostile nodes.  Tor has changed recently
to account for the fact that chosing fresh random nodes is actually worse. ie you have a probability of identity/address identification per route/node,
and repeatedly selecting routes/nodes just cumulatively increases the chance
you'll be identified.  Better to pick a random node, identify it and stick
to it and hope you chose well.
Maybe other simpler, but yes there was the proof of concept that the math
exists in the form of the weil pairing.
But what problem are we trying to solve here?  Is it an immediate problem? Maybe better to figure out a more privacy compatible solution which will
take longer, than let coding drive protocol.

@_date: 2014-03-08 09:41:01
@_author: Adam Back 
@_subject: [Bitcoin-development] Is this a safe thing to be doing with ECC 
Also the other limitation for ECDSA is that there is no known protocol to
create a signture with a+b (where keys P=aG, Q=bG, R=P+Q=(a+b)G). without
either a sending its private key to b or viceversa (or both to a third
With Schnorr sigs you can do it, but the k^-1 term in ECDSA makes a (secure)
direct multiparty signature quite difficult.
ps probably only 1 party needs to hash their key
P=aG                  H(P) ->

@_date: 2014-03-16 15:58:19
@_author: Adam Back 
@_subject: [Bitcoin-development] 2-way pegging (Re: is there a way to do 
So an update on 1-way pegging (aka bitcoin staging, explained in quoted text
at bottom): it turns out secure 2-way pegging is also possible (with some
bitcoin change to help support it).  The interesting thing is this allows
interoperability in terms of being able to move bitcoin into and out of a
side chain.  The side chains may have some different parameters, or
experimental things people might want to come up with (subject to some
minimum compatibility at the level of being able to produce an SPV proof of
a given form).
At the time of the 1-way peg discussion I considered 2-way peg as desirable
and it seemed plausible with bitcoin changes, but the motivation for 1-way
peg was to make it less risky to make changes on bitcoin, so that seemed
like a catch-22 loop.  Also in the 2-way peg thought experiment I had not
realized how simple it was to still impose a security firewall in the 2-way
peg also.
So Greg Maxwell proposed in Dec last year a practically compact way to do
2-way pegging using SPV proofs.  And also provided a simple argument of how
this can provide a security firewall.  (Security firewall means the impact
of security bugs on the side-chain is limited to the people with coins in
it; bitcoin holders who did not use it are unaffected). [1]
How it works:
1. to maintain the 21m coins promise, you start a side-chain with no
in-chain mining subsidy, all bitcoin creation happens on bitcoin chain (as
with 1-way peg).  Reach a reasonable hash rate.  (Other semantics than 1:1
peg should be possible, but this is the base case).
2. you move coins to the side-chain by spending them to a fancy script,
which suspends them, and allows them to be reanimated by the production of
an SPV proof of burn on the side-chain.
3. the side-chain has no mining reward, but it allows you to mint coins at
no mining cost by providing an SPV proof that the coin has been suspended as
in 2 on bitcoin.  The SPV proof must be buried significantly before being
used to reduce risk of reorganization.  The side-chain is an SPV client to
the bitcoin network, and so maintains a view of the bitcoin hash chain (but
not the block data).
4. the bitcoin chain is firewalled from security bugs on the side chain,
because bitcoin imposes the rule that no more coins can be reanimated than
are currently suspend (with respect to a given chain).
5. to simplify what they hypothetical bitcoin change would need to consider
and understand, after a coin is reanimated there is a maturity period
imposed (say same as fresh mined coins).  During the maturity period the
reanimation script allows a fraud proof to spend the coins back.  A fraud
bounty fee (equal to the reanimate fee) can be offered by the mover to
incentivize side-chain full nodes to watch reanimations and search for fraud
6. a fraud proof is an SPV proof with a longer chain showing that the proof
of burn was orphaned.
There are a few options to compress the SPV proof, via Fiat-Shamir transform
to provide a compact proof of amount work contained in a merkle tree of
proofs of work (as proposed by Fabien Coelho link on
 with params like 90% of work is proven.  But
better is something Greg proposed based on skip-lists organized in a tree,
where 'lucky' proofs of work are used to skip back further.  (Recalling that
if you search for a 64-bit leading-0 proof-of-work, half the time you get a
65-bit, quarter 66-bit etc.)  With this mechanism you can accurately
prove the amount of proof of work in a compressed tree (rather than ~90%).
Apart from pegging from bitcoin to a side-chain, if a private chain is made
with same rules to the side-chain it becomes possible with some
modifications to the above algorithm to peg the side-chain to a private
chain.  Private chain meaning a chain with the same format but signature of
single server in place of hashing, and timestamping of the block signatures
in the mined side chain.  And then reactive security on top of that by full
nodes/auditors trying to find fraud proofs (rewrites of history relative to
side-chain mined time-stamp or approved double-spends).  The reaction is to
publish a fraud proof and move coins back to the side chain, and then
regroup on a new server.  (Open transactions has this audit + reactive model
but as far as I know does it via escrow, eg the voting pools for k of n
escrow of the assets on the private server.) I also proposed the same
reactive audit model but for auditable namespaces [4].
Private chains add some possiblity for higher scaling, while retaining
bitcoin security properties.  (You need to add the ability for a user to
unilaterally move his coins to the side-chain they came from in event the
chain server refuses to process transactions involving them.  This appears
to be possible if you have compatible formats on the private chain and
This pegging discussion involved a number of  Greg Maxwell,
Matt Corallo, Pieter Wuille, Jorge Timon, Mark Freidenbach, Luke Dashjr. The 2-way peg seems to have first been described by Greg.  Greg thought of
2-way pegging in the context of ZK-SNARKS and the coinwitness thread [2]. (As a ZK-SNARK could compactly prove full validation of a side chain rules).
There was also something seemingly similar sounding but not described in
detail by Alex Mizrahi in the context of color coins in this post [3].
[1] [2] [3] [4]

@_date: 2014-03-20 13:12:21
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
Whats a sensible limit on practical/convenient QR code size?
How much of the payment protocol message size comes from use of x509?
(Just exploring what the options are).

@_date: 2014-03-21 11:59:06
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
Maybe its time to explore raw ECDSA signed message based certs.
btw I dont think its quite 4kB.  eg bitpay's looks to be about 1.5kB in der
format.  And they contain a 2048-bit RSA server key, and 2048-bit RSA
signatures (256byte each right there = 512bytes).  And even 2048 is weaker
than 256-bit ECDSA.

@_date: 2014-03-21 13:25:42
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
According to Bernstein it's patent FUD (expired, ancient and solid prior

@_date: 2014-05-18 16:38:53
@_author: Adam Back 
@_subject: [Bitcoin-development] Bitcoin Protocol Specification 
Suggestion: maybe you want to write and post here a paragraph summarizing
the topic of your paper so people can know if they feel qualified and if
they need to review it from their interests.

@_date: 2014-05-19 16:47:09
@_author: Adam Back 
@_subject: [Bitcoin-development] patents... 
someone recently wrote (not pointing fingers, nor demanding a spirited
defense from that person, its a generic comment):
btw about patents, I wonder if people who feel the need to do that, would
you consider putting those patents into like a linux foundation defensive
I imagine a number of other bitcoin companies have patented things, but if
you think ahead a little bit, or look at prior ecash history, patents held
by individuals or companies can be outright dangerous.  We saw this in the past eg the digicash patents after the company went
bankrupt were sold by the investor to some random large company that parked
it in its huge pile of patents, didnt use it, and prevented anyone else from
using it - stalling Chaum dependent payment innovation for perhaps 5 years
until the thing expired, and a Chaum patent expiry party was held.
Just some food for thought.
hmm Yes and this topic now is more than a bit non dev related.  Sorry about
that.  There seems to be no convenient mailing list format for non-dev stuff
or I would Cc and set Reply-To for example?  (Web forums somewhat suck IMO).

@_date: 2014-05-20 19:48:38
@_author: Adam Back 
@_subject: [Bitcoin-development] good bitcoin summary paper in more detail 
Actually I read the paper now as it was linked somewhere else also, and its
quite good.  So now I can summarize it:
Its a writeup of bitcoin in 29 pages, which covers things in the original
bitcoin paper but with more detail of formats, scripts with some examples,
formats etc.  Quite nice paper, concise presentation of many bitcoin details
that are otherwise hard to put together, requiring examining source or
asking people knowledgeable at algorithm/code level.

@_date: 2014-11-17 11:20:56
@_author: Adam Back 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
It seems to me that people maybe arriving at the idea that they should
put transaction data in the blockchain for three related reasons: a)
its there and its convenient; and b) they are thinking about permanent
storage and being able to recover from backup using a master seed to a
bip32 address-set and want that logic to extend to the extra features;
c) they are thinking out of band, but they think they are forced to
send the data there in order to achieve atomicity.
I think the data that is sent on the blockchain is design-compressed
minimal necessary to achieve transaction integrity, and its important
for scalability that we keep it that way.  About the rationales for
using that scarce scalability impacting channel:
a) convenience: is not a great reason to my mind. there are lots of
channels: email, web forms, point2point various transports NFC, TCP,
HTTP for payment protocol or extensions or new protocols.  I think
there could be a need for a reliable privacy preserving store and
forward decentralised infrastructure to act as a channel for such
purposes.  Until then email could be pretty convenient, if you dont
get the message due to spam filter etc ask them to resend.  Or a web
storage locker related to the app.
b) backup: the blockchain is not an efficient reliable generic backup
mechanism because its broadcast.  there are cheaper and relatively
simple ways to get end2end secure backup, the main challenge of which
is having secure keys and not forgetting them.  bitcoin already has
that covered as its a central requirement of blockchain security.  If
you want to archive your payment protocol receipts store them on some
cloud storage service or disk encrypted with related keys.  for
example tahoe-lafs is optimised for the decentralised long-term
storage kind of use.
c) atomicity. as an example application requiring atomicity that may
use op_return stealth addresses where if the stealth auxiliary message
was sent out of band, then if message is lost, and the sender didnt
keep it or cant be relied on to care, then the money could be
permanently lost to both parties.
It occurred to me recently the kind of use requiring atomicity as
stealth address in c) can be achieved by sending both the extra
message (the stealth packet) AND the signed bitcoin transaction over
the reliable store & forward (eg email for now).  Then the recipient
can do the calculations involving the auxiliary message and payment
message, and relay the message to the blockchain IFF they receive the
message (and chose to accept it).  If they dont receive the message
they can ask for it to be resent.  And if the payment is unclaimed the
sender still owns it and can double-spend to avoid risk of later
spending in their replacement message, or double-spend to self if the
recipient declines the payment.  This has privacy, efficiency and SPV
advantages over sending to the blockchain.
I think we could make a case that as a design principle auxiliary data
could do with a bitcoin-related but separate reliable store and
forward channel, as email has been sufficiently spammed to end up with
loss of reliability.  So I think a payment message transport would be
good here: invoices & receipts, and other things necessary for
applications, transaction disputes, records for normal p2p trades and
business functions reliable store and forward substrate with
decentralisation & privacy. For email the existing mechanism with
closest semantics, add-on privacy features exist: mixmaster,
nymservers, webmail + encryption, webmail over Tor etc for privacy
related uses.  Slow transports can offer better security than
interactive transports.

@_date: 2014-10-09 07:14:31
@_author: Adam Back 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
I think you can do everything with the existing script level nlocktime
in some kind of turing completeness sense (maybe); but there is a
complexity cost that often you have to resort to extra dependent
transaction(s) (and work-around malleability until that is fully
fixed) just to get the effect.
When I tried building things that need nlocktime I found it quite
inconvenient that it was wasnt a function rather than a script
property, so I like this proposal.

@_date: 2014-10-15 16:54:57
@_author: Adam Back 
@_subject: [Bitcoin-development] BIP process 
please not google groups *, I'd vote for sourceforge or other simple
open list software over google groups.
* Google lists are somehow a little proprietary or gmail lockin
focused eg it makes things extra hard to subscribe with a non-google
address if google has any hint that your address is associated with a
gmail account.  Quite frustrating.

@_date: 2014-10-22 14:54:35
@_author: Adam Back 
@_subject: [Bitcoin-development] side-chains & 2-way pegging (Re: is there 
For those following this thread, we have now written a paper
describing the side-chains, 2-way pegs and compact SPV proofs.
(With additional authors Andrew Poelstra & Andrew Miller).

@_date: 2014-10-25 13:27:30
@_author: Adam Back 
@_subject: [Bitcoin-development] death by halving 
Some thoughts about Alex's analysis:
- bitcoin price may increase (though doubling immediately might be
unlikely) after the halving (because the new coins are in short
supply). Apparently there is some evidence of a feedback loop between
number of freshly mined coins sold to cover electrical costs ongoing
(which depends on halving also), in that there are claims that the btc
price experiences some downwards pressure when margins are slim as
miners sell almost all of them when the electrical cost takes most of
the profit, and otherwise tend more to hold coins longer term.
- that people who cant make money mining with 1/2 reward will resort
to attacking the network rather than living with it for 2weeks until
difficulty adjustment).  actually it will be longer than two weeks if
its going to result in a difficulty fall.
- that the miners wont act in their own meta-interest to aim for the
plausible new hashrate supported by the lower reward.  mining
equipment investment horizon being 3-6mo+ so it can easily make
economic sense to subsidise it for a bit to smooth the transition.
- fees might go up to unjam the network also, so the people
benefitting from the transactions utility also help cover the
transition costs.  or maybe someone makes an assurance contract to pay
the short fall and phase it out over a few months to smooth the shift.
- there is a wide range of electrical efficiency, and some are much
worse than others so there maybe a convenient equilibrium where there
are enough left who can still profit.
- alternatively you might say why not 1/100th reward reduction per 2
week period rather than 1/2 every 4 years, a difficulty retarget could
be a convenient point to do that.

@_date: 2015-08-03 08:34:02
@_author: Adam Back 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
If block-sizes are increased in a way detrimental to the Chinese miners, it
is not the Chinese miners that lose, it is all of the non-Chinese miners -
this is because the Chinese miners have the slight majority of the
hashrate.  The relatively low external bandwidth connecting China to the
net is actually the problem of the non-Chinese miners problem.  Non Chinese
miners will experience higher orphan rate once Chinese miners cease to
build on top of blocks that are too large to sync in a timely fashion into
On 2 August 2015 at 23:02, Jim Phillips via bitcoin-dev <

@_date: 2015-08-03 09:46:51
@_author: Adam Back 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
There are two problems with this statement.  Firstly we can not be doing
protocol design via political lobbying - this is not a scientific approach
and for sure leads to disaster; it is not the colour of the paint on the box,
it is the security of the system.
Secondly as I understand it the Chinese miners did this under duress
of threats of worse as a compromise beyond what they felt they could safely
cope with.
That is good news.

@_date: 2015-08-03 09:53:39
@_author: Adam Back 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
Again this should not be a political or business compromise model - we
must focus on scientific evaluation, technical requirements and
But specifically as you asked a group of Chinese miners said they
would not run it:
Imagine if we had a nuclear reactor design criteria - we would not be
asking around with companies what parameter would they compromise on.
We'd be looking to scientific analysis of what is safe, based on
empirical and theoretical work on safety.  If we're risking $4b of
other peoples money (and a little bit of mine) I would strongly want a
scientific approach.
A closer analogy would be the NIST SHA3 design process.  With crypto
building blocks it is a security / speed tradeoff, a little analogous
to the security / throughput trade off in Bitcoin.
They do not ask companies or governments which algorithm they like or
what parameter they'd compromise on.  They have a design competition
and analyse the algorithms and parameters for security margin and
speed optimisation in hardware and software.  Much effort is put in
and it is very rigorous because a lot is at stake if they get it

@_date: 2015-08-05 11:57:54
@_author: Adam Back 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
On 5 August 2015 at 11:18, Hector Chu via bitcoin-dev
This kind of thing has been discussed here, even recently.  It is not
without problems.
You may find the flexcap idea summarised in outline by Greg Maxwell
and Mark Friedenbach a month or so back interesting in showing that
one can achieve such effects without handing over a free vote to
miners and hence avoid many (though probably not all) of the
side-effects inherent in giving miners control.
About side-effects, I think we can make argument that there are limits
because other than in an extremis sense, miners are not necessarily in
alignment with security, nor maximising user utility and value
For example switching cost economics are common in networks (cell
phone service pricing), maybe Bitcoin would have a really high
switching cost if miners would cartelise.
Also miners are in a complex game competing with each other, and this
degree of control risks selfish mining issues or other cartel attacks
or bandwidth/verification/latency related attacks being made worse.
eg see the recent paper by Aviv Zohar.
Generally speaking economically dependent full nodes are holding
miners honest by design.  Changing that dynamic by shifting influence
can have security and control impacting side-effects, and needs to be
thought about carefully.
About security to try to make those comparisons a bit more formal I
posted this taxonomy of types of security that proposals could be
compared against, in order of security:
1. consensus rule (your block is invalid if you attack)
2. economic incentive alignment (you tend to lose money if you attack)
3. overt (attack possible but detectable, hence probably less likely
to happen for reputation or market signal reasons even if possible
4. meta incentive (assume people would not attack if they have an
investment or interest in seeing Bitcoin continue to succeed)

@_date: 2015-08-05 13:07:35
@_author: Adam Back 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
This prediction market in block-size seems like something extremely
complex to operate and keep secure in a decentralised fashion.  There
are several experimental projects right now trying to figure out how
to do this securely, using blockchain ideas, but it is early days for
those projects.
We also have no particular reason to suppose other than
meta-incentive, that it should result in a secure parameter set.
I suspect that, while it is interesting in the abstract, it risks
converting a complex security problem into an even more complex one,
rather than constituting an incremental security improvement which is
more the context of day to day discussions here.

@_date: 2015-08-07 23:53:43
@_author: Adam Back 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
On 7 August 2015 at 22:35, Thomas Zander via bitcoin-dev
It's not as simple as trusting miners, Bitcoin security needs some
reasonable portion of economic interest to be validating their receipt
of coins against a full node they run.
I do it myself because I dont want to lose money, as do many power
users.  Most bitcoin ecosystem companies do it.  You dont have to run
it all the time, just sync it when you want to check your own coin
receipt with higher assurance.
Even if you are willing to trust others, trusting miners or random
full nodes would be unsafe if not for the reasonable portion of
economic interest validating their own received coins.  That holds
miners honest, otherwise they could more easily present fake
information to SPV users.
Bitcoin's very reason for existence is to avoid that need.  For people
fully happy to trust others with their money, Bitcoin may not be as
interesting to them.
What Pieter said is an accurate summary and non-controversial.

@_date: 2015-08-08 00:06:28
@_author: Adam Back 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Please try to focus on constructive technical comments.
On 7 August 2015 at 23:12, Thomas Zander via bitcoin-dev
But > 99% of Bitcoin transactions are already off-chain.  There are
multiple competing companies offering consumer & retail service with
off-chain settlement.
I wasnt clear but it seemed in your previous mail that you seemed to
say you dont mind trusting other people with your money, and so
presumably you are OK using these services, and so have no problem?
Who said no to anything?  The systems of off-chain transfer already
exist and are by comparison to Bitcoins protocol simple and rapid to
adapt and scale.
Indications are that we can even do off-chain at scale with Bitcoin
similar trust-minimisation with lightning, and duplex payment
channels; and people are working on that right now.
I think it would be interesting and useful for someone, with an
interest in low trust, high scale transactions, to work on and propose
an interoperability standard and API for such off-chain services to be
accessed by wallets, and perhaps periodic on-chain inter-service

@_date: 2015-08-08 09:39:52
@_author: Adam Back 
@_subject: [bitcoin-dev] trust 
If you are saying that some people are happy trusting other people,
and so would be perfectly fine with off-chain use of Bitcoin, then we
agree and I already said that off-chain use case would be a
constructive thing for someone to improve scale and interoperability
of in the post you are replying to.  However that use case is not a
strong argument for weakening Bitcoin's security to get to more scale
for that use case.
In a world where we could have scale and decentralisation, then of
course it would be nice to provide people with that outlook more
security than they seem to want.  And sometimes people dont understand
why security is useful until it goes wrong, so it would be a useful
thing to do.  (Like insurance, your money being seized by paypal out
of the blue etc).  And indeed providing security at scale maybe
possible with lightning like protocols that people are working on.

@_date: 2015-08-08 12:54:36
@_author: Adam Back 
@_subject: [bitcoin-dev] trust 
That's basically the definition of off-chain.  When we say MtGox or
coinbase etc are off-chain transactions, that is because a middle man
has the private keys to the coins and gave you an IOU of some kind.

@_date: 2015-08-10 22:43:22
@_author: Adam Back 
@_subject: [bitcoin-dev] What Lightning Is 
In terms of usage I think you'd more imagine a wallet that basically
parks Bitcoins onto channels at all times, so long as they are
routable there is no loss, and the scalability achieved thereby is
strongly advantageous, and there is even the potential for users to
earn fees by having their wallets participate in channel rebalancing
(where hubs pay users to rebalance channels - end up with the same net
position but move funds from one user-owned channel to another.)
Exchange deposit, withdrawal, payments, even in-exchange trades can
usefully happen in lightning for faster, cheaper more scalable

@_date: 2015-08-11 21:12:43
@_author: Adam Back 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I think everyone is expending huge effort on design, analysis and
implementation of the lowest cost technology for Bitcoin.
Changing parameters doesnt create progress on scalability fundamentals -
there really is an inherent cost and security / throughput tradeoff to
blockchains.  Security is quite central to this discussion.  It is
unrealistic in my opinion to suppose that everything can fit directly
on-chain in the fullest Bitcoin adoption across cash-payments, internet of
things, QoS, micropayments, share-trading, derivates etc.  Hence the
interest in protocols like lightning (encourage you and others to read the
paper, blog posts and implementation progress on the lightning-dev mailing
Mid-term different tradeoffs can happen that are all connected to and
building on Bitcoin.  But whatever technologies win out for scale, they all
depend on Bitcoin security - anything built on Bitcoin requires a secure
base.  So I think it is logical that we strive to maintain and improve
Bitcoin security.  Long-term tradeoffs that significantly weaken security
for throughput or other considerations should be built on top of Bitcoin,
and avoiding creating a one-size fits all unfortunate compromise that
weakens Bitcoin to the lowest common denominator of centralisation,
insecurity and throughput tradeoffs.  This pattern (secure base, other
protocols built on top) is already the status quo - probably > 99% of
Bitcoin transactions are off-chain already (in exchanges, web wallets
etc).  And there are various things that can and are being done to improve
the security of those solutions, with provable reserves, periodic on-chain
settlement, netting, lightning like protocols and other things probably
still to be invented.
Some of the longer term things we probably dont know yet, but the future is
NOT bleak.  Lots of scope for technology improvement.
On 11 August 2015 at 20:26, Michael Naber via bitcoin-dev <

@_date: 2015-08-11 22:23:18
@_author: Adam Back 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I dont think Bitcoin being cheaper is the main characteristic of
Bitcoin.  I think the interesting thing is trustlessness - being able
to transact without relying on third parties.
On 11 August 2015 at 22:18, Michael Naber via bitcoin-dev

@_date: 2015-08-11 22:34:46
@_author: Adam Back 
@_subject: [bitcoin-dev] Fees and the block-finding process 
So if they dont care about decentralisation, they'll be happy using
cheaper off-chain systems, right?

@_date: 2015-08-14 16:03:49
@_author: Adam Back 
@_subject: [bitcoin-dev] Adjusted difficulty depending on relative 
There is a proposal that relates to this, see the flexcap proposal by
Greg Maxwell & Mark Friedenbach, it was discussed on the list back in
and On 14 August 2015 at 15:48, Jakob R?nnb?ck

@_date: 2015-08-16 18:01:56
@_author: Adam Back 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Hi Tamas
Do you find BIP 101, BIP 102, BIP 103 and the flexcap proposal
deserving of equal consideration?  Just curious because of your post.
Will you be interested to participate in the BIP review process and
perhaps attend the workshop on Bitcoin scaling announced here
On 16 August 2015 at 17:07, Tamas Blummer via bitcoin-dev

@_date: 2015-08-17 15:36:48
@_author: Adam Back 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
Thank you Eric for saying what needs to be said.
Starting a fork war is just not constructive and there are multiple
proposals being evaluated here.
I think that one thing that is not being so much focussed on is
Bitcoin-XT is both a hard-fork and a soft-fork.  It's a hard-fork on
Bitcoin full-nodes, but it is also a soft-fork attack on Bitcoin core
SPV nodes that did not opt-in.  It exposes those SPV nodes to loss in
the likely event that Bitcoin-XT results in a network-split.
The recent proposal here to run noXT (patch to falsely claim to mine
on XT while actually rejecting it's blocks) could add enough
uncertainty about the activation that Bitcoin-XT would probably have
to be aborted.
On 17 August 2015 at 15:03, Eric Lombrozo via bitcoin-dev

@_date: 2015-08-19 09:53:13
@_author: Adam Back 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
It seems to be a recurring meme that BIP 101 is somehow "a solution
put forward" where BIP 100, 102, 103, flexcap, extension blocks etc
etc are not.
That is not at ALL the case, and is insulting (present company excluded).
It is just that no one else is reckless enough to bypass the review
process and risk a controversial hard fork deployment war.  Myself and
many other people warned Gavin a network fork "war" would start (ie
someone would think of some way to sabotage or attack the deployment
of Bitcoin-XT via protocol, code, policy, consensus soft-fork etc.  He
ignored the warnings.  Many also warned that 75% was an optimally BAD
trigger ratio (and that in a hard fork it is not a miner vote really
as in soft-forks).  Gavin & Mike ignored that warning to.  I know they
heard those warnings because I told them 1:1 in person or via email
and had on going conversations.  Others did too.
People can not blame bitcoin core or me, that this then predictably
happened exactly as we said it would - it was completely obvious and
In fact noBitcoinXT is even more dangerous and therefore amplified in
effect in creating mutual assured destruction kind of risk profile
than the loose spectrum of technical counters imagined.  I did not
personally put much effort into thinking about counters because I
though it counter productive and hoped that Gavin & Mike would have
the maturity to not start down such a path.
Again any of the other proposals can easily be implemented.  They
*could* also spin up a web page and put up binaries, however no one
else was crazy enough to try to start a deployment in that way.
It is also puzzling timing - with all these BIPs and ongoing
discussion and workshops coming imminently to then release ahead of
that process where as far as I know Gavin said he was equally happy
with BIP 100 or other proposal which ever is best, and on basically
the eve of workshops planned to progress this collaboratively.
Bitcoin-XT is also under tested, people are finding privacy bugs and
other issues.  (Not even mentioning the above 75% optimally bad
parameter, and the damage to community reputation and collaborative
environment that this all causes.)
Very disappointing Gavin and Mike.
I find it quite notable that Gavin and Mike have been radio silent on
the bitcoin-dev list and yet we see a stream of media articles, blog
posts, pod casts, and from what I can tell ongoing backroom lobbying
of companies to run bitcoin-XT without trying AT ALL to offer a
neutral or balanced or multi proposal information package so that
companies technical people can make a balanced informed decision.
That is what the workshops are trying to provide.
Gavin, Mike - anything to say here?
On 18 August 2015 at 19:59, Angel Leon via bitcoin-dev

@_date: 2015-08-19 15:45:48
@_author: Adam Back 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Wouldnt the experience for SPV nodes be chaotic?  If the full nodes
are 50:50 XT and bitcoin core, then SPV clients would connect at
random and because XT and core will diverge immediately after
On 19 August 2015 at 15:28, Jorge Tim?n

@_date: 2015-08-23 09:05:51
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP 10X: Replace Transaction Fees with Data, 
Some comments:
"(i) remove any possibility of free transactions unless
associated with basic transaction data;"
I believe it is not possible to prevent free transactions for the
reason that people can pay out of band (via existing banking transfers
to miners) or make payments to addresses belonging to miners (that are
contingent on the requested user transaction being processed via input
dependency) .
I am not sure I fully understand the way you see monetisation working,
and you do indicate this is quite far future what-if stage idea, and
you do identify a conflict with fungibility - but I think this is
probably quite badly in conflict with fungibility to the point of
conflicting with many planned Bitcoin improvements?  And mid term
technical directions.
I would say the long term idealised requirements are that the
transaction itself would have cryptographic fungibility, and policy
relating to identity for authorisation, approval in regulated
transactions would take place at the payment protocol layer.  The
payment protocol is already seeing some use.
Lightning protocol sees more of the data going point to point and so
not broadcast nor visible for big data analytic monetisation.
On 22 August 2015 at 23:51, Jorge Tim?n

@_date: 2015-12-14 20:32:01
@_author: Adam Back 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
I think people have a variety of sequences in mind, eg some would
prefer to deploy versionBits first, so that subsequent features can be
deployed in parallel (currently soft-fork deployment is necessarily
Others think do seg-witness first because scale is more important.
Some want to do seg-witness as a hard-fork (personally I think that
would take a bit longer to deploy & activate - the advantage of
soft-fork is that it's lower risk and we have more experience of it).
I've seen a few people want to do BIP 102 first (straight move to 2MB
and only that) and then do seg-witness and other scaling work later.
That's possible also and before Luke observed that you could do a
seg-witness based block-size increase via soft-fork, people had been
working following the summary from the montreal workshop discussion
posted on this list about a loose plan of action, people had been
working on something like BIP 102 to 2-4-8 kind of space, plus
validation cost accounting.
So I think personally soft-fork seg-witness first, but hey I'm not
writing code at present and I'm very happy for wiser and more code and
deployment detail aware minds to figure out the best deployment
strategy, I wouldnt mind any of the above, just think seg-witness
soft-fork is the safest and fastest.  The complexity risk - well on
the plus side it is implemented and it reduces deployment risk, and
it's anyway needed work to have a robust malleability fix which is
needed for a whole range of bitcoin smart-contract, and scaling
features, including for example greenAddress like faster transactions
as used by BitPay?, BitGo and GreenAddress as well as lightning
related proposals and basically any smart-contract use that involves
multiple clauses and dependent transactions.  Also re complexity risk
Greg has highlighted that the complexity and overhead difference is
really minor.  About knock on code changes needed, a bunch of the next
steps for Bitcoin are going to need code changes, I think our scope to
improve and scale Bitcoin are going to be severely hampered if we
restricted ourselves with the pre-condition that we cant make protocol
improvements.  I think people in core would be happy to, and have done
this kind of thing multiple times in the past, to help people for free
on volunteer time integrate and fix up related code in various
languages and FOSS and commercial software that is affected.
As to time-line Luke I saw guestimated by march 2016 on reddit.
Others maybe be more or less conservative.  Probably a BIP and testing
are the main thing, and that can be helped by more eyes.  The one
advantage of BIP 102 like proposal is simplicity if one had to do a
more emergency hard-fork.  Maybe companies and power users, miners and
pool operators could help define some requirements and metrics about
an industry wide service level they are receiving relative to fees.
The other thing which is not protocol related, is that companies can
help themselves and help Bitcoin developers help them, by working to
improve decentralisation with better configurations, more use of
self-hosted and secured full nodes, and decentralisation of policy
control over hashrate.  That might even include buying a nominal (to a
reasonably funded startup) amount of mining equipment.  Or for power
users to do more of that.  Some developers are doing mining.
Blockstream and some employees have a little bit of hashrate.  If we
could define some metrics and best practices and measure the
improvements, that would maybe reduce miners concerns about
centralisation risk and allow a bigger block faster, alongside the
IBLT & weak block network protocol improvements.
On 14 December 2015 at 19:44, Jonathan Toomim via bitcoin-dev

@_date: 2015-12-14 20:44:57
@_author: Adam Back 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
I think someone, maybe Pieter, commented on this relay issue that it
would be likely very transitory, as a lot of stuff would be fairly
quickly upgraded in practice from previous deployment experience, and
I think anyway there is a huge excess connectivity and capacity in the
p2p network vs having a connected network of various versions, and
supporting SPV client load (SPV load is quite low relative to
capacity, even one respectable node can support a large number of SPV
(Ie so two classes of network node and connectivity wouldnt be a
problem in practice even if it did persist; also the higher capacity
better run nodes are more likely to upgrade due to having more clued
in power user, miner, pool or company operators).
Maybe someone more detailed knowledge could clarify further.
On 14 December 2015 at 19:21, Jonathan Toomim via bitcoin-dev

@_date: 2015-12-17 04:48:29
@_author: Adam Back 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
There are a range of opinions about input assumptions by different
people.  In each case, short of misunderstanding, if we have the same
input assumptions we're going to reach the same conclusions.  This is
the way of the world in a meritocracy.  The interesting point is to
compare the input assumptions and try to figure out which are more
realistic, pragmatic and achieve the best outcome.
It might be instructive to re-read Greg's roadmap and others to
re-read Jeff's original post (I will).
There is a proposed roadmap and soft-fork block-size increase and code
that Pieter is working on.  There has been rationale described for
this approach, and it achieves many useful things both short, mid and
long term for scale and other issues.
There seem to be a range of opinions on the fee market, and one
question is when do we deem it safe to aim to be prepared to support a
fee market.
How elastic is block-size demand?  (I think there is evidence of some
elasticity which indicates a partly working fee market already).  What
I mean by elasticity of block-size demand is there are off-chain
transactions and people make an economic choice of whether to go on
chain or not, and the vast majority of transactions, all told, are
off-chain.  Clearly it is ideal if they all go on chain, scale
If we look at the roadmap at high-level:
1) bump (seg-wit or ...)
2) network improvements (IBLT/weak-block/other)
3) longer term dynamic block-size (flexcap)
4) write-cache (lightning)
It would probably be good to see some work on preparing for fee
markets.  That has happened somewhat recently in response to the
stress tests.  We do have an observed problem that if there is no
incentive to prepare, the improvements dont happen, and so we can
never be ready for a fee market.  That's kind of how we got here,
people were talking about fee-estimation and dynamic fees several
years ago before the block-size went from 250kB to 750kB, and then
lost interest as there was another 500kB to play with.  There could be
a best practice doc written asking people to prepare.  That might
Presumably it's good if we do see the fee market more, for it to come
in gradually.  Flexcap probably helps there because the block-size
itself becomes elastic to demand (pay for bigger blocks).
If we want to avoid a fee market for the immediate term, are we more
worried about period 1, or period 2 or 3.  Probably 2 is more of a
worry as we're scaling in 1 where in period 2 we're preparing for
scaling and more time has passed for demand to grow.  That might for
example argue for seg-wit because it brings us closer to 4) and if we
spread things out we might delay the possibility to do lightning as
there is only so many cycles for forks (hard or soft) in testing,
deployment planning etc so it can be good to have a holistic view.
Also the question of time-frame that is safe for soft-forks or
hard-forks is another input where views seem to vary.  I think some
people are more optimistic about being able to avoid people losing
money in fast hard-forks.  One lesson on users, is users find failure
modes that testing cant, or do things you would expect them not to do.
Also we're calling hard-forks things that are really soft-forks to SPV
clients, and hard-forks only to full-nodes.  If we wanted to make a
real economic choice, we could artificially make an SPV hard-fork,
however that would make upgrade harder.
As I said in an earlier email I think everyone is empathetic to user
requirements, including economic desires - but Bitcoin has inherent
constraints that are complex to improve.  Each proposal is trying to
best meet those holistic user requirements.  There are no free lunches
and we dont want to economically hurt anyone in total or as a group or
type of use.  Not all requirements can be met, they are in a trade
off, so that calls for balance, planning and transparency.
This is also a market, we can discuss protocol tradeoffs without being
melodramatic - would be kind of undesirable if a dramatic or emotive
way to express something as easily or more clearly expressed in
technical constructive words is moving the price around.
On 17 December 2015 at 03:58, Jeff Garzik via bitcoin-dev

@_date: 2015-12-17 22:31:07
@_author: Adam Back 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
While it is interesting to contemplate moving to a world with
hard-fork only upgrades (deprecate soft-forks), now is possibly not
the time to consider that.  Someone can take that topic and make a
what-if sketch for how it could work and put it on the wishlist wiki
if its not already there.
We want to be pragmatic and constructive to reach consensus and that
takes not mixing in what-ifs or orthogonal long standing problems into
the mix, as needing to be fixed now.
On 17 December 2015 at 19:52, Jeff Garzik via bitcoin-dev

@_date: 2015-12-30 23:05:57
@_author: Adam Back 
@_subject: [bitcoin-dev] fork types (Re: An implementation of BIP102 as a 
No, segregated witness
 is a
soft-fork maybe loosely similar to P2SH - particularly it is backwards
and forwards compatible by design.
These firm forks have the advantage over hard forks that there is no
left-over weak chain that is at risk of losing money (because it
becomes a consensus rule that old transactions are blocked).
There is also another type of fork a firm hard fork that can do the
same but for format changes that are not possible with a soft-fork.
Extension blocks show a more general backwards and forwards compatible
soft-fork is also possible.
Segregated witness is simpler.
On 30 December 2015 at 13:57, Marcel Jamin via bitcoin-dev

@_date: 2015-02-14 11:04:49
@_author: Adam Back 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Strongly with Peter on this.  That its highly complex to maintain strict
consensus between bitcoin versions, does not justify consensus rewrite
experiments; it tells you that the risk is exponentially worse and people
should use and rally around libconsensus.
I would advise any bitcoin ecosystem part, wallet, user to not use software
with consensus protocol rw-writes nor variants, you WILL lose money.
You could view bitcoin as a digital signature algorithm speculatively
tinkering with the algo is highly prone to binary failure mode and
unbounded funds loss.
Want to be clear this is not a political nor emotive issue. It is a
critical technical requirement for security if users of software people
Please promote this meme.

@_date: 2015-02-20 12:44:24
@_author: Adam Back 
@_subject: [Bitcoin-development] bloom filtering, privacy 
I saw there was some discussion on this topic on the bitcoinj list.
(I dont think I can post there without subscribing probably.)
Someone had posted about the lack of privacy provision from the
current implementation parameters and real-world factors similar to
described in this academic paper
Mike had posted a detailed response on the topic on why its complex
and becomes bandwidth inefficient to improve it usefully.
The basic summary of which I think is that its not even intended to
provide any practical privacy protection, its just about compacting
the query for a set of addresses.
So I was wondering what about changing to committing a bloom filter of
the addresses in the block.  Its seems surprising no one thought of it
that way before (as it seems obvious when you hear it) but that seems
to address the privacy issues as the user can fetch the block bloom
filters and then scan it in complete privacy.  (Someone appeared on
bitcoin wizards IRC a while back and made this observation.)
Am I missing anything?

@_date: 2015-02-20 17:35:47
@_author: Adam Back 
@_subject: [Bitcoin-development] bloom filtering, privacy 
> every block). So you end up downloading every block?
I mean because the user is scanning he can binary search which set of
addresses from his wallet are possibly in the block and then request
the specific addresses and some will be false positives and some real,
but with the bloom commitment (and UTXO trie organised commitment) he
can verify that the positive hits are correct via the merkle path, and
that the false positives are not being wrongly withheld by obtaining
merkle path proof that they are not in the trie.

@_date: 2015-02-20 17:59:03
@_author: Adam Back 
@_subject: [Bitcoin-development] bloom filtering, privacy 
The idea is not mine, some random guy appeared in  one
day and said something about it, and lots of people reacted, wow why
didnt we think about that before.
It goes something like each block contains a commitment to a bloom
filter that has all of the addresses in the block stored in it.
Now the user downloads the headers and bloom data for all blocks.  The
know the bloom data is correct in an SPV sense because of the
commitment.  They can scan it offline and locally by searching for
addresses from their wallet in it.  Not sure off hand what is the most
efficient strategy, probably its pretty fast locally anyway.
Now they know (modulo false positives) which addresses of theirs maybe
in the block.
So now they ask a full node for merkle paths + transactions for the
addresses from the UTXO set from the block(s) that it was found in.
Separately UTXO commitments could optionally be combined to improve
security in two ways:
- the normal SPV increase that you can also see that the transaction
is actually in the last blocks UTXO set.
- to avoid withholding by the full node, if the UTXO commitment is a
trie (sorted) they can expect a merkle path to lexically adjacent
nodes either side of where the claimed missing address would be as a
proof that there really are no transactions for that address in the
block.  (Distinguishing false positive from node withholding)

@_date: 2015-02-21 05:12:54
@_author: Adam Back 
@_subject: [Bitcoin-development] bloom filtering, privacy 
Seems like Greg & I may be saying different things.  Maybe I am
misunderstanding something at the wire level or in size/scalability
but what I was trying to say is I think simpler.
By UTXO commitment I mean a merkle tree of unspent addresses is
committed in each block.  Also a bloom filter containing addresses in
the block is committed.
Now the user downloads the bloom filter for each block, and searches
it locally.  They see which addresses of theirs maybe in the block
(with some false positives).
Then they make fresh random connections to different nodes and request
download of the respective individual transactions from the full node.
The node can respond either a) here is the transaction and here is its
merkle path in the merkle tree (this is the way SPV works today); or
b) there is no such transaction, this is a false positive, and here is
a pair of merkle trie paths in the UTXO commitment (a trie) that
proves the full node is not withholding and its true that no such
transaction is in the block.
Additionally with UTXO commitments in case a) the user can keep up to
date with the chain tip and request from the full node a merkle path
in the UTXO commitment to show that the coin is still unspent.
(Otherwise you get long range attacks where someone can grind away
until they belatedly find a PoW on an old low hashrate block with UTXO
and fool an SPV node they know the address for into accepting a spend
of something long spent).
About privacy the node can make different random connections to
different nodes to fetch addresses.  Nothing is leaked by downloading
the bloom filter.  Scanning happens locally.  The full node cant
correlate the addresses as belonging to the same person by correlating
the download requests for them, because they are made via different
nodes.  Its not a surprise nor privacy revealing that someone would
want to check receipt of the funds.  The limit is the interactive
nature of ToR which isnt very secure against pervasive monitoring.
But for basic full-node passive attack resistant privacy pretty good.
Contrast with increasing the false positive on bloom queries: here the
full node can test correlation (modulo the false positive ratio) and
combine that with network flow analysis to narrow down who the user
might be.  Plus query size and privacy are in conflict.  Plus the
query size has to be continually retuned to even create a reliable
false positive rate relative to the current UTXO set.  Is that is even
happening now (other than parameter sets)?
About the bitmap:
how does the SPV client know what the bits in this map mean to scan?
I presume these would be one bit per address and one would need to
know the full UTXO set in order to know whats in there.  I am not sure
an SPV node would want the hit of keeping up to date with the full
UTXO set?
s/address/scriptpubkey for accuracy)

@_date: 2015-02-21 13:23:24
@_author: Adam Back 
@_subject: [Bitcoin-development] Request for a new BIP number (and 
Whats the objective?  Is it to require accidental disclosure of two
private keys to compute the master private key?

@_date: 2015-02-21 14:30:15
@_author: Adam Back 
@_subject: [Bitcoin-development] bloom filtering, privacy 
If you want to be constructive and index transactions that are not
p2sh but non-simple and contain checksig so the address is visible,
you could do that with a block bloom filter also.
I wasnt sure if the comments about the need to batch requests was
about downloading headers & filters, or about transactions, there is
no harm downloading headers & bloom filters without Tor - there is no
identity nor addresses revealed by doing so.  So over Tor you would
just be fetching transactions that match the address.
For downloading transactions unless you frequently receive
transactions you wont be fetching every block.  Or are you assuming
bloom filters dialled up to the point of huge false positives?  You
said otherwise.
Mid-term I'd say you want some basic request tunneling as part of
bitcoin, that maybe isnt Tor, to avoid sharing their fate if Tor
controversies are a risk to Tor service.  Some of the bitcoin-Tor
specific weak points could maybe then be addressed.
Relatedly I think bitcoin could do with a store-and-forward message
bus with privacy and strong reliability via redundancy (but less
redundancy maybe than consensus all-nodes must receiving and agree and
store forever).  That  provides an efficient store-and-forward SPV
receivable stealth-address solution that doesnt suck: send the
recipient their payment, if they like it they broadcast it themselves.
As a bonus store-and-forward message mixes are better able to provide
meaningful network privacy than interactive privacy networks.  You
could spend over the same channel
You seem to be saying at one point that Tor is useless against
pervasive eavesdropper threat model (which I am not sure I agree with,
minimally it makes them work for the info and adds uncertainty; and
not been paying super close attention but I think some of the Snowden
releases suggest Tor is a net win) and secondly that other types of
attackers are disinterested (how do we know that?) or maybe that you
dont care about privacy vs them (maybe some users do!)
It would certainly be nice to get real privacy from a wider range of
attackers but nothing (current situation) is clearly worse; using
block bloom filters we'd make the pervasive case harder work, and the
nosy full node learn nothing.

@_date: 2015-02-22 08:02:03
@_author: Adam Back 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
I agree with Mike & Jeff.  Blowing up 0-confirm transactions is vandalism.
bitcoin transactions are all probabilistic.  there is a small chance
1-confirm transactions can be reversed, and a different but also
usable chance that 0-confirm transactions can be reversed.  I know
0-confirm is implemented in policy and not consensus, but it provides
fast transactions and a lot of the current ecosystem is using it for
low value transactions.  why would anyone want to vandalise that.
to echo Mike bitcoin itself kind of depends on some honest majority,
we can otherwise get to situations soon enough where its more
profitable to double-spend than mine honestly as subsidy drops and
transaction values increase even without 0-confirm transactions.
subsidy doesnt last forever (though it lasts a really long time) and
even right now if you involve values that dwarf subsidy you could make
a "criminally rational" behaviour that was more profitable.  we even
saw 0-confirm odds-attacks against satoshi dice clones.  but if we
assume the "criminal rational" model, its a is a race to the bottom
logic, and bitcoin is already broken if we have someone who wants to
go for it with high values.  that'd be scorched earth also.
(I read the rest of the arguments, i understood them, I disagree, no
need to repeat in reply.)
So how about instead, to be constructive, whether you agree with the
anti-arson view or not, lets talk about solutions.  Here's one idea:
We introduce a new signature type that marks itself as can be spent by
miners if a double-spend is seen (before 1-confirm.)  We'd define a
double-spend as a spend that excludes outputs to avoid affecting valid
double-spend scenarios.  And we add behaviour to relay those
double-spends (at priority).  We may even want the double-spend to be
serialisation incomplete but verifiable to deter back-channel payments
to pretend not to receive one, in collusion with the double-spending
Now the risk to the sender is if they accidentally double-spend.  How
could they do that?  By having a hardware or software crash where they
sent a tx but crashed before writing a record of having sent it.  The
correct thing to do would be to transactionally write the transaction
before sending it.  Still you can get a fail if the hardware
irrecoverably fails, and you have to resume from backup.  Or if you
run multiple non-synced wallets on the same coins.
Typically if you recover from backup the 1-confirmation window will
have passed so the risk is limited.
The feature is opt-in so you dont have to put high value coins at risk
of failure.
(Its related to the idea of a one-use signature, where two signatures
reveals a simultaneous equation that can recover the private key;
except here the miner is allowed to take the coins without needing the
private key).
Its soft-forkable because its a new transaction type.
ps I agree with Greg also that longer-term more scalable solutions are
interesting, but I'd like to see the core network work as a stepping
stone.  As Justus observed: the scalable solutions so far have had
non-ideal ethos tradeoffs so are not drop-in upgrades to on-chain

@_date: 2015-02-22 14:11:31
@_author: Adam Back 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
My actual point outside of the emotive stuff (and I should've stayed
away from that too) is how about we explore ways to improve practical
security of fast confirmation transactions, and if we find something
better, then we can help people migrate to that before deprecating the
current weaker 0-conf transactions.
If I understand this is also your own motivation.
Feel free to comment on or improve the proposal or find other approaches.

@_date: 2015-01-23 16:12:28
@_author: Adam Back 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
its an always offline node, so it knows nothing really other than a
BIP 32 hierarchy of keys & a signature request.
So the signature request has to drag with it information to validate
what the value is, in order to be sure not to sign away 99% to fees.
Signing the transaction value and having the network validate that the
value in the sig matches full nodes view of the tx value avoids that
issue.  Simple, elegant, but... we have no live beta mechanism, and
hence risk & testing makes that tricky.  Plus the full network upgrade
issue if its not backwards compatible.

@_date: 2015-01-23 16:17:25
@_author: Adam Back 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
Issues like that particular one (simple elegant fix, strong utility
justification) plus previously more privacy stuff (like committed tx,
homomorphic encrypted values) was what got me wondering about a way to
do a live beta (one-way peg) and then to get excited about the 2wp &
Greg's mechanism for that.
I think it would be hypothetically possible to make a "special"
singleton sidechain which is merge mined, and has a consensus rule to
require some proportion of reward be sent to it via coinbase tx (a
mechanism to address incentive incompatibility) and a general timeline
eg 12mo to next version +/- etc. might be an interesting thing to
explore as a place to store live versions of "hard fork wishlist"
items where people who need them early can help validate them.
I am not sure that helps the full network upgrade issue though.

@_date: 2015-07-24 07:09:13
@_author: Adam Back 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
(Claim of large bitcoin ecosystem companies without full nodes) this
says to me rather we have a need for education: I run a full node
myself (intermittently), just for my puny collection of bitcoins.  If
I ran a business with custody of client funds I'd wake up in a cold
sweat at night about the security and integrity of the companies full
nodes, and reconciliation of client funds against them.
However I'm not sure the claim is accurate ($30m funding and no full
node) but to take the hypothetical that this pattern exists, security
people and architects at such companies must insist on the company
running their own full node to depend on and cross check from
otherwise they would be needlessly putting their client's funds at
The crypto currency security standards document probably covers
requirement for fullnode somewhere
 - we need some kind of basic
minimum bar standard for companies to aim for and this seems like a
reasonable start!
Reducing custody in my opinion should also be an aim eg 2 of 2
multisig + timelock seems like a more prudent approach, transaction
throughput permitting.  Right now exchange volume wouldnt fit on
chain, once bitcoin scaling has improved, perhaps it can.  I am
optimistic that within a year Bitcoin scaling and decentralisation
will look much better with current active work on decentralisation,
layer 2 scaling solutions.  As part of that I could see a modest
blocksize increase to smooth out the transition to layer 2.
In terms of a constructive discussion, I think it's interesting to
talk about the root cause and solutions: decentralisation (more
economically dependent full nodes, lower miner policy centralisation),
more layer 2 work.  People interested in scaling, if they havent,
should go read the lightning paper, look at the github and participate
in protocol or code work.  I think realistically we can have this
running inside of a year.  That significantly changes the dynamic.
Similarly a significant part of mining centralisation is artificial
and work is underway that will improve that.
What I mean about decentralisation is if decentralisation simple
metrics were in a healthy place, it would be a simple conversation to
make use of bandwidth improvements (in the range of 15%/year per Cisco
numbers) to get more throughput.  I do think flexcap is interesting as
a way to add one more control variable such that we can have
economically validated scaling.  Pushing fees to zero and increasing
centralisation to levels that weaken security with economically weak
payments is probably not desirable.  Without flexcap it seems the next
best thing we can do is rely on miners to balance user utility against
mining revenue, and it seems plausible that they would in extremis but
to my mind there are factors suggesting this could be problematic
incrementally: miners have not often been responsive to editing
defaults, or reacting to security or soft-fork upgrades; miners may
have some conflict of interest of users, eg they could use switching
cost economics to optimise for miner profit at the expense of user
utility, or attack each other in selfish-miner or other variants as
miners are also pitted against each other while being held honest by
economically dependent full nodes.

@_date: 2015-07-29 06:46:46
@_author: Adam Back 
@_subject: [bitcoin-dev] Disclosure: consensus bug indirectly solved by 
I believe the idea is to replace openSSL with
 that Pieter and Greg spent quite
some time rigorously testing and have at this point better confidence
in than *SSL libraries.
I think the lessons learned from it as concluded by Pieter and Greg
are that openSSL and derivatives are not focussed on consensus
consistency, such that even if actively maintained and security
reviewed, their own bug fixes can break bitcoin.
On 29 July 2015 at 06:41, Mike Hearn via bitcoin-dev

@_date: 2015-07-29 20:49:08
@_author: Adam Back 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
I dont think people consider other blockchains as a competitive
threat.  A PoW-blockchain is a largely singleton data structure for
security reasons (single highest hashrate), it is hard for an
alternative chain to bootstrap or provide meaningful security.
Secondly the world largely lacks expertise to maintain a blockchain to
bitcoin's security level, perhaps you can see a hint of this in the
recently disclosed security vulnerability by Pieter Wuille and Gregory
Maxwell.  Calls to this as an argument are not resonating and probably
not helping your argument.  Bitcoin has security properties, and a
competing system cant achieve better properties by bypassing security,
any blockchain faces the same fundamental security / decentralisation
Secondly Bitcoin can obviously compete with itself with different
parameters and defacto *does* today.  I think it is a safe estimate
that > 99% of Bitcoin transactions right now are happening in Bitcoin
related systems with various degrees of audit, reconciliation,
provable reserves etc.  I think we can expect this to continue and
become more secure via more reconciliation, and longer term via
lightning or Bitcoin sidechains with different parameters.  It is a
different story to have a single central system (Bitcoin with
parameters changed to the point of centralisation failure) vs having
multiple choices, because some transactions can more easily use
relatively centralised systems (eg micropayments), and more
interestingly the combination of a secure and decentralised layer 1
plus choices of less decentralised layer 2 options, can be interesting
because the layer 2 is provided cover from attack.  There is less to
be gained by attacking relatively centralised layer 2 because any
payments at risk of policy abuse (which is typically a small subset)
can easily switch to layer 1.  That in itself makes layer 2
transactions also less susceptible to policy abuse.  Further lightning
it appears from work so far should add significant scale while
retaining trustlessness and a good degree of decentralisation.
Finally you seem to be focusing on "artificial" limits where that is
not the issue under consideration.  The limits are technical and
relating to decentralisation and security.  I wont go over them again
as this topic has been covered many times in recent months.  Any chain
that tried to go to extreme parameters (very low block intervals, or
very large blocksizes) would have the same decentralisation problems
as Bitcoin would if it did the same thing.  There are a number of alt
coins that have failed as a result of poor parameter choices, there
are inherent security limits.
ps Etiquette note for yourself and others: please dont be repetitive
or attempt to be forceful.  Many people have spent many years
understanding this very complex system, from my own experience it is
rare indeed to think of an entirely new concept or analysis, that
hasnt' been long considered and put to bed 3 or 4 years ago.
Thoughtful polite and constructive comments are welcome but I
recommend to not start from an assumption that you have a clear and
better insight than the entire technical community, because I have to
say from my own experience that is very rarely the case.  It can be
useful to test theories on  IRC channel to find out what has
been already concluded, find the references and avoid having to have
that hashed out on this list which is trying to be focussed on
technical solutions.
On 29 July 2015 at 16:10, Raystonn . via bitcoin-dev

@_date: 2015-07-29 21:00:42
@_author: Adam Back 
@_subject: [bitcoin-dev] 
On 29 July 2015 at 20:41, Ryan Butler via bitcoin-dev
The assumption is that wont work because any miner can break ranks and
do so profitably, so to expect otherwise is to expect oligopoly
behaviour which is the sort of antithesis of a decentralised mining
system.  It's in fact a similar argument as to why decentralisation of
mining provides policy neutrality: some miner somewhere with some
hashrate will process your transaction even if some other miners are
by policy deciding not to mine it.  It is also similar reason why free
transactions are processed today - policies vary and this is good for
ensuring many types of transaction get processed.

@_date: 2015-07-29 21:05:35
@_author: Adam Back 
@_subject: [bitcoin-dev] 
btw the fact that mining is (or can be) anonymous also makes oligopoly
or cartel behaviour likely unstable.  Miners can break ranks and
process transactions others wish to block, or with lower fees than a
cartel would like to charge, without detection.
Anonymous mining is a feature and helps ensure policy neutrality.
This is all overlaid by the 51% attack - if a coherent cartel arose
that could maintain 51% and had enough mutual self-interest to make
that stable, they could attack miners bypassing their cartel policies,
by orphaning their blocks.  This is partly why mining decentralisation
is important.  Also that is an overt act which is very detectable and
could lead to technical counter-measures by the users, who are in
ultimately in control of the protocol.  So there is some game theory
suggesting it would be inadvisable for miners to be overt in cartel
attacks.  Non overt attacks cant prevent anonymous under cutting of
cartel desired fee minimums.

@_date: 2015-07-30 18:48:18
@_author: Adam Back 
@_subject: [bitcoin-dev] Block size following technological growth 
That's what is nice about proposals, they are constructive and help
move the conversation forward!
On 30 July 2015 at 18:20, Gavin Andresen via bitcoin-dev
But, if we agree that 17%/year is consistent with network
improvements, by arguing this is too conservative, does that not mean
you are actually going beyond suggesting throughput increases to
benefit from bandwidth improvements, and explicitly arguing to
borrowing from Bitcoin's already very weak decentralisation to create
more throughput?  (Or arguing to subsidise transaction fees if
borrowing so deeply that excess capacity pushes beyond demand).
I think the logical implication of this would be that we should be
first focussing on improving decentralisation, to make security room
to reclaim extra throughput.
(To be clear there are concrete things that can be done and actually
are being done to improve decentralisation via ecosystem education and
mining protocol improvements, but it's safer to wait a few months and
see if those things take effect well).
Secondly in this assumption are you considering that lightning will
likely be online for many years by 2021 and the situation will be
hugely different?
I think an incremental and conservative approach is safer.  People can
probably get a lightning prototype running about as fast as a
hard-fork could be safely rolled out.
I do think it is normal to be conservative with security and with $4b
of other peoples money.  It's no longer an experimental system to
consider fail fast experiments on.
What criteria should we be using in your opinion to balance?  I think
throughput increases trading off decentralisation would be more
reasonable if decentralisation wasnt in very bad shape.

@_date: 2015-07-31 12:16:15
@_author: Adam Back 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
I think trust the data-center logic obviously fails, and I was talking
about this scenario in the post you are replying to.  You are trusting the
data-center operator period.  If one could trust data-centers to run
verified code, to not get hacked, filter traffic, respond to court orders
without notifying you etc that would be great but that's unfortunately not
what happens.
Data-center operators are bound to follow laws, including NSLs and gag
orders.  They also get hacked, employ humans who can be corrupt,
blackmailed, and themselves centralisation points for policy attack.
Snowden related disclosures and keeping aware of security show this is very
This isn't much about bitcoin even, its just security reality for hosting
anything intended to be secure via decentralisation, or just hosting in
general while at risk of political or policy attack.
On Jul 31, 2015 10:39, "jl2012 via bitcoin-dev" <

@_date: 2015-07-31 15:17:58
@_author: Adam Back 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
That's all well and fine.  But the pattern of your argument I would
say is "arguing security down" ie saying something is not secure
anyway, nothing is secure, everything could be hacked, so lets forget
that and give up, so that what is left is basically no
decentralisation security.
It is not paranoid to take decentralisation security seriously, it is
necessary because it is critical to Bitcoin.  Security in depth
meaning take what security you can get from available defences.

@_date: 2015-06-01 14:00:49
@_author: Adam Back 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Agree with everything you said.  Spot on observations on all counts.
Thank you for speaking up.

@_date: 2015-06-01 16:40:17
@_author: Adam Back 
@_subject: [Bitcoin-development] soft-fork block size increase (extension 
Hi Gavin
Sorry for slow response & broken threading - mailbox filled up & only
saw your response on archive.
I do earnestly think opt-in block-size increases are politically
cleaner (gives different people different sized blocks by their own
volition without forcing a compromise) and less risky than hard forks.
Particularly if a hard-fork were really provoked without clear and
wide consensus - dragons lay there.
I don't think thats any particular concern, extension block payments
are forwards and backwards compatible.  Businesses who are keen to
have more transactions, would make it their problem to implement in
their wallet, or ask the wallet vendor/maintainer they're working with
to do it.  Nothing breaks if they dont use it.  The people that have
the need for it will work on it.  Market at work.  If it turns out
they dont really have a need for it, just projected huge numbers for
their business plan that say dont materialise, well no foul.
I am not a UX guy, but for example it might be appropriate for tipping
services or other micropayments to use an extension block.  Or small
retail payments.  They can choose what address they use.  Merchants,
integrators etc can do likewise.
It gives plenty enough scope that people can work with useful
trade-offs while others work on lightning.
Because the more complex one is safer, more flexible, more future
proof and better for decentralisation (and so as a bonus and might
actually get done without more months of argument as its less
contentious because it gives users choice to opt-in).  Bitcoin itself
is complex, a central ledger is simpler but as we know uninteresting
which is to say this is a security tradeoff.
Obviously I do appreciate KISS as a design principle, and utility of
incremental improvements, but this is a security trade-off we're
discussing here.  I am proposing a way to not weaken security, while
getting what you think is important - access to more TPS with a higher
centralisation tradeoff (for those who opt-in to it, rather than for
everyone whether that tradeoff is strongly against their interests or
The decentralisation metrics are getting worse, not better, see Greg
Maxwell's comments
This would not by those metrics be a good moment in history to make
the situation worse.
Not at all, thats the point.  Bitcoin has a one-size fits all
blocksize.  People can pool mine the 8MB extension block, while solo
or GBT mining the 1MB block.  Thats more centralising than staying at
1MB (because to get the fees from the extension block some people
without that kind of bandwidth are pool mining 8/9th of the lower
security/decentralisation transactions.  But its less centralising
than a fixed blocksize of 9MB (1+8 for apples/apples) because
realistically if those transactions are not spam, they would've
happened offchain, and offchain until we get lightning like systems
up, means central systems which are worse than the slight
centralisation of 8MB blocks being single servers and prone to custody
& security failure.  I think you made that point yourself in a recent
post also.
Sound good? ;)  Seriously I think its the least bad idea I've heard on
this topic.
As an aside, a risk with using companies as a sounding board, is that
you can get a misleading sense of consensus.  Did they understand the
tradeoff between security (decentralisation) and blocksize.  Did they
care?  Do they represent users interests?  Would they have "voted"
instead for extension blocks if it was presented in similar terms?  (I
have to imagine they might have preferred extension blocks given the
better story if you gloss over complexity and tradeoffs).

@_date: 2015-06-01 18:21:15
@_author: Adam Back 
@_subject: [Bitcoin-development] soft-fork block size increase (extension 
To be clear in case you are missing part of the mechanism.: it is
forward and backwards compatible meaning a 1MB address can receive
payments from an 8MB address (at reduced security if it has software
that doesnt understand it) and a 1MB address can pay an 8MB address by
paying to an OP_TRUE that has meaning to the extension block nodes.
A 1MB client wont even understand the difference between a 1MB and 8MB
out payment.  An 8MB client will understand and pay 1MB addresses in a
different way (moving the coin back to the 1MB chain).
So its opt-in and incrementally deployable.  Exchanges could encourage
their users to use wallets that support 8MB blocks, eg by charging a
fee for 1MB transactions.  If 1MB blocks experience significant fee
pressure, this will be persuasive.  Or they could chose not to and eat
the cost.  This is all normal market adoption of a new cheaper
technical option (in this case with a tradeoff of reduced
security/more centralisation for those opting in to it).
Extension blocks & lightning are unrelated things.
While I understand the need for being practical, there is IMO, amongst
engineering maxims something as far as being too pragmatic,
dangerously pragmatic even.  We cant do stuff in bitcoin that has bad
carry costs, nor throw out the baby with the bathwater.
The situation is just that we are facing a security vs volume tradeoff
and different people will have different requirements and comfort
zones.  If I am not misremembering, I think you've sided typically
with the huge block, big data center only end of the spectrum.  What I
am proposing empowers you to do experiments in that direction without
getting into a requirements conflict with people who value more
strongly the bitcoin properties arising from it being robustly
I am not sure personally where the blocksize discussion comes out - if
it stays as is for a year, in a wait and see, reduce spam, see
fee-pressure take effect as it has before, work on improving improve
decentralisation metrics, relay latency, and do a blocksize increment
to kick the can if-and-when it becomes necessary and in the mean-time
try to do something more long-term ambitious about scale rather than
volume.  Bitcoin without scale improvements probably wont get the
volume people would like.  So scale is more important than volume; and
security (decentralisation) is important too.  To the extreme analogy
we could fix scale tomorrow by throwing up a single high perf
database, but then we'd break the security properties arising from
decentralisation.  We should improve both within an approximately safe
envelope IMO.

@_date: 2015-06-01 19:44:22
@_author: Adam Back 
@_subject: [Bitcoin-development] Fwd: Block Size Increase Requirements 
So lets rephrase that and say instead more correctly it is the job of
miners (collectively) to be well connected globally - and indeed there
are incentivised to be or they tend to receive blocks at higher
latency and so are at increased risk of orphans.  And miner groups
with good block latency in-group and high hashrate are definitionally
the well connected, so the cost of getting good connectivity to high
hashrate groups is naturally borne by people outside of those groups.
Or thats the incentive anyway.

@_date: 2015-06-02 14:11:23
@_author: Adam Back 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced 
That would also introduce the anomaly of a script that was once valid
becoming later invalid, when nothing varies other than time.  That is
not super compatible with the current model of reprocessing
transactions in later blocks if the block they were first in gets
(Not a huge flexibility loss as you can implement "not after" by
making it the previous holders responsibility to spend a "not before"
back to themselves.)

@_date: 2015-06-14 23:23:55
@_author: Adam Back 
@_subject: [Bitcoin-development] comments on BIP 100 
I made these comments elsewhere, but I think really we should be
having these kind of conversations here rather than scattered around.
These are about Jeff Garzik's outline draft BIP 100 I guess this is
the latest draft:  (One good thing about getting off SF would be
finally JGarzik's emails actually not getting blocked!).
may have changed since the original [1]
Over the original proposal:
1. there should be a hard cap, not indefinitely growing.
2. there should be  a growth limiter (no more than X%/year)
3. I think the miners should not be given a vote that has no costs to
cast, because their interests are not necessarily aligned with users
or businesses.
I think Greg Maxwell's difficulty adjust [2] is better here for that
reason.  It puts quadratic cost via higher difficulty for miners to
vote to increase block-size, which miners can profitably do if there
are transactions with fees available to justify it. There is also the
growth limiter as part of Greg's proposal. [3]
I think bitcoin will have to involve layering models that uplift
security to higher layers, but preserve security assurances, and
smart-contracts even, with protocols that improve the algorithmic
complexity beyond O(n^2) in users, like lightning, and there are
multiple other candidates with useful tradeoffs for various use-cases.
One thing that is concerning is that few in industry seem inclined to
take any development initiatives or even integrate a library.  I
suppose eventually that problem would self-correct as new startups
would make a more scalable wallet and services that are layer2 aware
and eat the lunch of the laggards.  But it will be helpful if we
expose companies to the back-pressure actually implied by an O(n^2)
scaling protocol and don't just immediately increase the block-size to
levels that are dangerous for decentralisation security, as an
interventionist subsidy to save them having to do basic integration
work.  Otherwise I think whichever any kind of kick the can some 2-5
years down the road we consider, we risk the whole saga repeating in a
few years, when no algorithmic progress has been made and even more
protocol inertia has set in.
[1] original proposal comments on reddit
[2] flexcap propoal by Greg Maxwell see post by Mark Freidenbach
 at lists.sourceforge.net/msg07599.html
[3] growth limited proposal for flexcap by Greg Maxwell
 at lists.sourceforge.net/msg07620.html

@_date: 2015-06-14 23:59:24
@_author: Adam Back 
@_subject: [Bitcoin-development] Proposal: Move Bitcoin Dev List to a 
It might be as well to keep the archive but disable new posts as
otherwise we create bit-rot for people who linked to posts on
The list is also archived on mail-archive though.
 at lists.sourceforge.net/

@_date: 2015-06-15 01:58:10
@_author: Adam Back 
@_subject: [Bitcoin-development] comments on BIP 100 
Hi Mike
No slight intended obviously to people who do write actual client code.
That was probably insufficiently specific, let me rephrase: I am
referring to the trend that much of the industry is built on web2.0
technology using bitcoin via a library in a web scripting language,
often with consensus bugs, and even outsourcing and not even running
their own full node, so that the service itself offered to their users
isn't even SPV secure to the operator.  As well as being heavily based
on a third-party custody model that is the root cause of the repeated
wallet breaches.  Some of these companies have a noted tendency not to
upgrade or fix code.
So I mean this not to call out specific companies, but in the sense
that if we're technologists we should be trying to move the technology
forward, not just changing parameters which run into an O(n^2) scaling
wall or break decentralisation security.  And we shouldnt take the
above state of affairs as an immutable reality.  It can not persist
for bitcoin to reach maturity on scale nor security.
As I said I dont think we can expect Bitcoin to scale with no further
algorithmic improvements.  Algorithmic improvements take code.  There
is reasonable scope to build in an incrementally deployable way,
there's plenty of time for people to code, test and opt-in to things,
the sky is not falling.  Companies do care about scaling, and can
invest in the integration and coding implied to improve their products
scalability, they have an economic incentive to do it and there is no
scalable and safe way todo it without this work.
I am referring to global bandwidth O(n^2) with n=users, or O(n) per
user bandwidth cost to the system, while O(nm) is accurate nodes is an
internal system concept.  Anyway suffice to say the network does not
scale O(1) in per user cost.
A hard-fork takes a long period of time to deploy due to the
non-upgrade risk, people are working on things in the mean-time.
There can be a case for some increase to create some breathing room to
work on scaling and decentralising tech, I just mean to say that if we
do it in isolation, we're not focussing on the big picture.

@_date: 2015-06-15 02:04:44
@_author: Adam Back 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
I understand why you would be keen to scale bitcoin, everyone here is.
But as you seem to be saying that you will do a unilateral hard-fork,
and fork the code-base simultaneously, probably a number of people
have questions, so I'll start with some:
( I noticed some of your initial thoughts are online here
 or the full podcast
 )
- Are you releasing a BIP for that proposal for review?
- If the reviewers all say NACK will you take on board their suggestions?
- On the idea of a non-consensus hard-fork at all, I think we can
assume you will get a row of NACKs.  Can you explain your rationale
for going ahead anyway?  The risks are well understood and enormous.
- How do you propose to deal with the extra risks that come from
non-consensus hard-forks?  Hard-forks themselves are quite risky, but
non-consensus ones are extremely dangerous for consensus.
- If you're going it alone as it were, are you proposing that you will
personally maintain bitcoin-XT?  Or do you have a plan to later hand
over maintenance to the bitcoin developers?
- Do you have contingency plans for what to do if the non-consensus
hard-fork goes wrong and $3B is lost as a result?
As you can probably tell I think a unilateral fork without wide-scale
consensus from the technical and business communities is a deeply
inadvisable.  While apparently some companies have expressed interest
in increased scale, I can only assume they do no yet understand the
risks.  I suggest before they would actually go ahead that they seek
independent advice.
Of the overall process, I think you can agree we should not be making
technical decisions with this level of complexity and consensus risk
with financial implications of this magnitude under duress of haste?
This seems otherwise a little like the moral hazard of the 2008
financial collapse that Satoshi put the quote in the genesis block
I think its best that we progress as Jeff Garzik has done to have
engineering discussions centre around BIPs, running code for review,
simulation and careful analysis.
I understand this has been going on for a long time, and some people
are frustrated with the rate of progress, but making hasty,
contentious or unilateral actions in this space is courting disaster.
Please use your considerable skills to, along with the rest of the
community, work on this problem collaboratively.
I can sincerely assure you everyone does want to scale bitcoin and
shares your long term objective on that.

@_date: 2015-06-15 20:03:25
@_author: Adam Back 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
Hi Mike
Well thank you for replying openly on this topic, its helpful.
I apologise in advance if this gets quite to the point and at times
blunt, but transparency is important, and we owe it to the users who
see Bitcoin as the start of a new future and the$3b of invested funds
and $600m of VC funds invested in companies, we owe it to them that we
be open and transparent here.
I would really prefer on a personal nor professional basis to be
having this conversation period, never mind in public, but Mike - your
and Gavin's decision to promote a unilateral hard-fork and code fork
are extremely high risk for bitcoin and so there remains little
choice.  So I apologise again that we have to have this kind of
conversation on a technical discussion list.  This whole thing is
hugely stressful and worrying for developers, companies and investors.
I strongly urge that we return to the existing collaborative
constructive review process that has been used for the last 4 years
which is a consensus by design to prevent one rogue person from
inserting a backdoor, or lobbying for a favoured change on behalf of a
special interest group, or working for bad actor (without accusing you
of any of those - I understand you personally just want to scale
bitcoin, but are inclined to knock heads and try to force an issue you
see, rather than work collaboratively).
For you (and everyone)
- Should there be a summit of some kind, that is open attendance, and
video recorded so that people who are unable to attend can participate
too, so that people can present the technical proposals and risks in
an unbiased way?
(It is not theoretical question, I may have a sponsor and host - not
Blockstream, an independent, its a question for everyone, developers,
users, CTOs, CEOs.)
So here I come back to more frank questions:
The rest of the developers are wise to realise that they do not want
exclusive control, to avoid governance centralising into the hands of
one person, and this is why they have shared it with a consensus
process over the last 4 years.  No offence but I dont think you
personally are thinking far enough ahead to think you want personal
control of this industry.  Maybe some factions dont trust your
motives, or they dont mind, but feel more assured if a dozen other
people are closely reviewing and have collective review authority.
- Do you understand that attempting to break this process by
unilateral hard-fork is extremely weakening of Bitcoin's change
governance model?
- Do you understand that change governance is important, and that it
is important that there be multiple reviewers and sign-off to avoid
someone being blackmailed or influenced by an external party - which
could potentially result in massive theft of funds if something were
- Secondarily do you understand that even if you succeed in a
unilateral fork (and the level of lost coins and market cap and damage
to confidence is recoverable), that it sets a precedent that others
may try to follow in the future to introduce coercive features that
break the assurances of bitcoin, like fungibility reducing features
say (topically I hear you once proposed on a private forum the concept
of red-lists, other such proposals have been made and quickly
abandoned), or ultimately if there is a political process to obtain
unpopular changes by unilateral threat, the sky is the limit - rewrite
the social contract at that point without consensus, but by
calculation that people will value Bitcoin enough that they will
follow a lead to avoid risk to the system?
As you probably know some extremely subtle bugs in Bitcoin have at
times slipped past even the most rigorous testings, often with
innocuous but unexpected behaviours, but some security issues  Some
extremely intricate and time-sensitive security defect and incident
response happens from time to time which is not necessarily publicly
disclosed until after the issue has been rolled out and fixed, which
can take some time due to the nature of protocol upgrades,
work-arounds, software upgrade via contacting key miners etc.  We
could take an example of the openSSL bug.
- How do you plan to deal with security & incident response for the
duration you describe where you will have control while you are
deploying the unilateral hard-fork and being in sole maintainership
- Are you a member of the bitcoin security reporting list?
As you know the people who have written 95% of the code (and reviewed,
and tested, and formally proved segments etc) are strenuously advising
not to push any consensus code into public use without listening to
and addressing review questions which span beyond rigorous code &
automated guided fuzz testers, simulation and sometimes formal proofs,
but also economics, game-theory and critically very subtle
determinism/consensus safety that they have collectively 4-5 years
experience of each.
- Will you pause your release plans if all of the other developers
insist that the code or algorithm is defective?
- Please don't take this the wrong way, and I know your bitcoinj work
was a significant engineering project which required porting bitcoin
logic.  But If the answer to the above question is no, as you seemed
to indicate in your response, as you not have not written much bitcoin
core code yourself (I think 3 PRs in total), do you find yourself more
qualified than the combination of peer review of the group of people
who have written 95% of it, and maintained it and refactored most of
it over the last 4-5 years?
I presume from your security background you are quite familiar with
the need for review of crypto protocol changes & rigorous code review.
That is even more the case with Bitcoin given the consensus
That you are frustrated, is not a sufficient answer as to why you are
proposing to go ahead with a universally acknowledged extreme network
divergence danger unilateral hard-fork, lacking wide-spread consensus.
People are quite concerned about this.  Patience, caution and prudence
is necessary in a software system with such high assurance
So I ask again:
- On the idea of a non-consensus hard-fork at all, I think we can
assume you will get a row of NACKs.  Can you explain your rationale
for going ahead anyway?  The risks are well understood and enormous.
Note the key point is that you are working on a unilateral hard-fork,
where there is a clear 4 year established process for proposing
improvements and an extremely well thought out and important change
management governance process.  While there has been much discussion,
you nor Gavin, have not actually posted a BIP for review.  Nor
actually was much of the discussion even conducted in the open: it was
only when Matt felt the need to clear the air and steer this
conversation into the open that discussion arose here.  During that
period of private discussion you and Gavin were largely unknown to
most of us lobbying companies with your representation of a method
that concerns everyone of the Bitcoin users.  Now that the technical
community aware aware they are strenuously discouraging you on the
basis of risks.
- Do you agree that bitcoin technical discussions should happen in the open?
- As this is a FOSS project, do you agree that companies should also
be open, about their requirements and trade-offs they would prefer?
- Can you disclose the list of companies you have lobbied in private
whether they have spoken publicly or not, and whether they have
indicated approval or not?
- Did you share a specific plan, like a BIP or white paper with these
companies, and if so can we see it?
- If you didnt submit a plan, could you summarise what you asked them
and what you proposed, and if you discussed also the risks?  (If you
asked them if they would like Bitcoin to scale, I expect almost
everyone does, including every member of the technical community, so
that for example would not fairly indicate approval for a unilateral
I and others will be happy to talk with the CTO and CEOs of companies
you have lobbied in private, for balance to assure ourselves and the
rest of the community that their support was given - and with full
understanding of the risks of doing it unilaterally, without peer
review, benefit of maintenance and security inidence management, and
what exactly they are being quoting as having signed up for.
(This maybe more efficiently and openly achieved by the open process,
on a mailing list, maybe a different one even special purpose to this
topic, with additional option of the open public meeting I proposed at
the top).
- Do you agree that it would be appropriate, that companies be aware
of both the scaling opportunities (of course, great everyone wants
scalability) as well as the technical limits and risks with various
approaches?  And that these be presented by parties from a range of
views to ensure balance?
- Do you consider your expression of issues to hold true to the ideal
of representing balanced nuanced view of all sides of a technical
debate, even when under pressure or feeling impatient about the
You may want to review the opening few minutes of your epicenter 82
bitcoin for example where you claimed and I quote "[the rest of the
technical community] dont want capacity to ever increase and want it
to stay where it is and when it fills up people move to other
- Do you think that is an accurate depiction of the complex trade-offs
we have been discussing on this list?
(For the record I am not aware of a single person who has said they do
not agree with scaling Bitcoin.  Changing a constant is not the
hard-part.  The hard part is validating a plan and the other factors
that go into it.  It's not a free choice it is a security/scalability
tradeoff.  No one will thank us if we "scale" bitcoin but break it in
hard to recover ways at the same time.)
- Were you similarly balanced in your explanations when talking to
companies in private discussions?
- Do you understand that if we do not work from balanced technical
discussion, that we may end up with some biased criteria?
Neither you nor Gavin have any particular authority here to speak on
behalf of Bitcoin (eg you acknowledge in your podcast that Wladimir is
dev lead, and you and Gavin are both well aware of the 4 year
established change management consensus decision making model where
all of the technical reviewers have to come to agreement before
changes go in for security reasons explained above).  I know Gavin has
a "Chief Scientist" title from the Bitcoin Foundation, but sadly that
organisation is not held in as much regard as it once was, due to
various irregularities and controversies, and as I understand it no
longer employs any developers, due to lack of funds.  Gavin is now
employed by MIT's DCI project as a researcher in some capacity.  As
you know Wladimir is doing the development lead role now, and it seems
part of your personal frustration you said was because he did not
agree with your views.  Neither you nor Gavin have been particularly
involved in bitcoin lately, even Gavin, for 1.5 years or so.
- Do you agree that if you presume to speak where you do not have
authority you may confuse companies?
But I think this is a false dichotomy.  As I said in previous mail I
understand people are frustrated that it has taken so long, but it is
not the case that no progress has been made on scalability.
I itemised a long list of scalability work which you acknowledged as
impressive work (CPU, memory, network bandwidth/latency) and RBF, CPFP
fee work, fee-estimation, and so on, which you acknowledged and are
aware of.
There are multiple proposals and BIPs under consideration on the list right now.
- what is the reason that you (or Gavin) would not post your BIP along
side the others to see if it would win based on technical merit?
- why would you feel uniquely qualified to override the expert opinion
of the rest of the technical community if your proposal were not
considered to have most technical merit? (Given that this is not a
simple market competition thing where multiple hard-forks can be
considered - it is a one only decision, and if it is done in a
divisive unilateral way there are extreme risks of the ledger
Network Divergence Risk
But this is not a soft-fork, it is a hard-fork.  Miner voting is only
peripherally related.  Even if in the extremis 75% of miners tried a
unilateral hard-fork but 100% of the users stayed on the maintained
original code, no change would occur other than those miners losing
reward (mining fork-coins with no resale value) and the difficulty
would adjust.  The miners who made an error in choice would lose money
and go out of business or rejoin the chain.
However if something in that direction happens with actual users and
companies on both sides of it users will lose money, the ledger will
diverge as soon as a single double-spend happens, and never share a
block again, companies will go instantly insolvent, and chaos will
break out.  This is the dangerous scenario we are concerned about.
So the same question again:
- How do you propose to deal with the extra risks that come from
non-consensus hard-forks?  Hard-forks themselves are quite risky, but
non-consensus ones are extremely dangerous for consensus.
Being sensitive to alarming the market
It is something akin to Greece or Portugal or Italy exiting the euro
currency in a disorderly way.  Economists and central bank policy
makers are extremely worried about such an eventuality and talk about
related factors in careful, measured terms, watch Mario Draghi when he
Imagine that bitcoin is 10x or 100x bigger.  Bitcoin cant have people
taking unilateral actions such as you have been proposing.  It is not
following the consensus governance process, and not good policy and it
is probably affecting bitcoin confidence and price at this moment.
This is not a soft-fork, and the community will not want to take the
risks once they understand them, and they have months in which to
understand them and at this point you've motivated and wasted 100s of
developer man hours such that we will feel impelled to make sure that
no one opts into a unilateral hard-fork without understanding the
risks.  It would be negligent to allow people to do that.  Before this
gets very far FAQs will be on bitcoin.org etc explaining this risk I
would imagine.  Its just starting not finished.
What makes you think the rest of the community may not instead prefer
Jeff Garzik's BIP after revisions that he is making now with review
comments from others?
Or another proposal.  Taken together with a deployment plan that sees
work on decentralisation tying into that plan.
- If you persisted anyway, what makes you think bitcoin could not make
code changes defensively relating to your unilateral fork?
(I am sure creative minds can find some ways to harden bitcoin against
a unilateral fork, with a soft-fork or non-consensus update can be
deployed much faster than a hard-fork).
I tried to warn Gavin privately that I thought he was under-estimating
the risk of failure to his fork proposal due to it being unilateral.
Ie as you both seem sincere in your wish to have your proposal
succeed, then obviously the best way to do that is to release a BIP in
the open collaborative process and submit it to review like everyone
else.  Doing it unilaterally only increases its chance of failure.
The only sensible thing to do here is submit a BIP and stop the
unilateral fork threat.
Scalability Plans
Yes people have proposed other plans.  Bryan Bishop posted a list of them.
Jeff Garzik has a proposal, BIP-100 which seems already better than
Gavin's having benefit of peer review which he has been incorporating.
I proposed several soft-fork models which can be deployed safely and
immediately, which do not have ledger risk.
I have another proposal relating to simplified soft-fork one-way pegs
which I'll write up in a bit.
I think there are still issues in Jeff's proposal but he is very open
and collaborating and there maybe related but different proposals
It does not seem to me that you understand the issue.  Of course they
want to increase the scalability of bitcoin.  So does everyone else on
this mailing list.
That they would support that is obvious.  If you presented your
unilateral action plan without explaining the risks too.
I think I covered this further above.  If you would like to share the
company list, or we can invite them to the proposed public physical
meeting, I think it would be useful for them to have a balanced view
of the ledger divergence risks, and alternative in-consensus proposals
underway, as well as the governance risks, maintenance risks, security
incident risks.
Note that other people talk to companies too, as part of their day to
day jobs, or from contacts from being in the industry.  You have no
special authority or unique ability to talk with business people.  Its
just that the technical community did not know you were busy doing
I can not believe that any company that would listen to their CTO, CSO
or failing that board would be ok with the risks implied by what you
are proposing on full examination.
I know you want scale bitcoin, as I said everyone here does. I think
what you're experiencing is that you've had more luck explaining your
pragmatic unilateral plan to non-technical people without peer review,
and so not experienced the kind of huge pushback you are getting from
the technical community.  The whole of bitcoin is immensely
complicated such that it takes an uber-geek CS genius years to
catchup, this is not a slight of any of the business people who are
working hard to deploy Bitcoin into the world, its just complicated
and therefore not easy to understand the game-theory, security,
governance and distributed system thinking.  I have a comp sci PhD in
distributed systems, implemented p2p network systems and have 2
decades of applied crypto experience with a major interest in
electronic cash crypto protocols, and it took me a several years to
catchup and even I have a few hazy spots on low-level details, and I
addictively into read everything I could find.  Realistically all of
us are still learning, as bitcoin combines so many fields that it
opens new possibilities.
What I am expecting that yourself and Gavin are thinking is that
you'll knock heads and force the issue and get to consensus.
However I think you have seriously misjudged the risks and have not
adequately explained them to companies you are talking with.  Indeed
you do not fully seem to acknowledge the risks, nor to have a well
thought out plan here of how you would actually manage it, nor the
moral hazards of having a lone developer in hugely divisive
circumstances in sole control of bitcoins running code.  Those are
exactly the reasons for the code change governance process!
Even though you are trying to help, the full result is you are not
helping achieve anything by changing a constant and starting a
unilateral hard-fork (not to trivialise the work of making a patch to
do that).
The work to even make the constant change be feasible was a result of
1000s of hours of work by others in the development community, that is
emphatically and unilaterally telling you that hard-forks are hugely
You are trying to break the code change governance security procedure
that were put in place for good reason for the security of $3b of
other peoples money, even if you have a pragmatic intent to help, this
is flat out unacceptable.
There are also security implications to what you are proposing, which
I have heard you attempting to trivialise, that are core to Bitcoins
security and core functionality.
I think this is a significant mischaracterisation, and I think almost
everybody is on board with a combination plan:
1. work to improve decentralisation (specific technical work already
underway, and education)
2. create a plan to increase block-size in a slow fashion to not cause
system shocks (eg like Jeff is proposing or some better variant)
3. work on actual algorithmic scaling
In this way we can have throughput needed for scalability and security
work to continue.
As I said you can not scale a O(n^2) broadcast network by changing
constants, you need algorithmic improvements.
People are working on them already.  All of those 3 things are being
actively worked on RIGHT NOW, and in the case of algorithmic scaling
and improve decentralisation have been worked on for months.
You may have done one useful thing which is to remind people that
blocks are only 3x-4x below capacity such that we should look at it.
But we can not work under duress of haste, nor unilateral ultimatums,
this is the realm of human action that leads to moral hazard, and
ironically reminds us of why Satoshi put the quote in the genesis
Bitcoin is too complex a system with too much at stake to be making
political hasty decisions, it would be negligent to act in such a way.
Again please consider that you did your job, caused people to pay
attention, but return to the process, submit a BIP, retract the
unilateral hard-fork which is so dangerous and lets have things be
calm, civil and collaborative in the technical zone of Bitcoin and not
further alarm companies and investors.

@_date: 2015-06-15 20:14:56
@_author: Adam Back 
@_subject: [Bitcoin-development] comments on BIP 100 
I think he's more talking about like extension-blocks, however they
are actually soft-forkable even (and keep the 21m coins obviously)
See  See and Tier Nolan tech detail
 at lists.sourceforge.net/msg07927.html
Discussion / claimed properties on

@_date: 2015-06-19 11:22:35
@_author: Dr Adam Back 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
Developers
Nicely put Eric.  Relatedly my initial experience with Bitcoin in
trying to improve bitcoin in fungibility, privacy & decentralisation,
I found some interesting things, like Confidential Transactions (that
Greg Maxwell has now optimised via a new generalisation of the
hash-ring signature construct he invented and with Pieter made part of
the alpha side-chain release) and a few other things.
As I went then to discuss and learn: a) what are the characteristics
needed for inclusion (clearly things need to fit in with how things
work, not demand massive rewrites to accommodate and to not conflict
with existing important design considerations), so that I could make
proposals in a practically deployable way, and then b) the
practicality of getting a proposed change that say people found
clearly useful.  Then I bumped into the realisation that this is
actually really high risk to change, and consensus critical coding
security is very complex and there are some billion $ resting on
getting this rigidly correct under live conditions, so that deployment
must be cautious and incremental and rigorously tested.
So then I focussed instead on question of whether we could improve
bitcoins development model: how could we allow bitcoin to more rapidly
and agilely test beta features or try novel things to see how they
would work (as someone might do in a feature branch of a normal FOSS
project, to code and test a proposal for later addition), but with
criteria we want real bticoins so there is economic incentive as that
is actually part of the bitcoin protocol so you've not validated
something unless you're run it in a real network with money.  I was
hypothesising therefore we need a way to run bitcoin beta network.
There's a thread about this here stretching back to may 2013.
Or similarly to run in parallel kind of subnets with different
trade-offs or features that are not easy to merge or high risk to
apply all at once to bitcoin with the inflight billions in capital and
transactions on it.
Anyway I thought that was a productive line of thinking, and generally
people seemed to agree and problem statement of 2wp: then 1wp
mechanism was proposed and then Greg extracted a concept from his
SNARK witness idea (which encapsulates a snark variant of a 2wp) but
now without snarks, then 2wp a conservative crypto 2wp proposal was
made.  This was dec 2013 I think on wizards channel.  The sidechain
alpha release now makes this a (alpha quality and so testnet coin, and
without DMMS peg) reality.  I could imagine others who have a desire
to try things could elect to do so and copy that patch-set and make
more side-chains.
This is inherently non-coercive because you largely do not directly
change bitcoin by doing this, people elect to use which ever chain
suits them best given their usecase.  If the sidechain is really early
stage it should have test-net coins in it not bitcoins in it, but
still its caveat emptor kind of beta chain, with good testing but
non-trivial to soft-fork on bitcoin but managable refactor a sidechain
to integrate something novel or try some existing feature (like the
segregated witness which robustly addresses malleability for example)
So I dont want to say side-chains are some magical solution to
everything, but its a direction that others may like to consider for
how to test or even run alternative trade-offs bitcoin side-chains in
parallel.  For example it could hypothetically allow 10MB blocks on
one chain and 100kB blocks on the main chain.  People say complexity,
scary.  Sure I am talking longer term, but we have to also make
concrete forward progress to the future or we'll be stuck here talking
about perilously large constant changes in 5 years time!
This approach also avoids the one-size fits all problem.
Extension-blocks are an in-chain sub-net type of thing that has a
security boost by being soft-fork enforced (relative to side-chains
which are looser coupled and so more flexible relative to the simplest
form of extension-blocks)

@_date: 2015-06-19 12:45:57
@_author: Dr Adam Back 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
Developers
A lot of people think a layer2 is needed, that has a higher
(algorithmic) scale in use of layer1 block-space but preserves
functionality and uplifts security from layer1.  An example would be
lightning or similar.
But there are many things that could be done.  Pure offchain is a weak
form of layer2.  Its running today and maybe its handling 90-99% range
of all transactions right now (mostly in exchanges for example).  This
layer can be incrementally hardened.  It can also have standardised
APIs across vendors of custodians, and opt-in support of those APIs in
wallets.  This would provide a convenience choice.  Greenaddress also
for low-mid assurances solves the unconfirmed transactions. It's
probably not reasonable to expect bitcoin directly solve fast
unconfirmed transactions.   Probably intermediate configurations in
complexity somewhere between greenaddress (2 of 2 + timelocked 1 sig)
and lightning may exist also.  The internet doesnt stop at layer1.
(Which would then leave people who are uninterested in changing client
software to handle layer2, as "layer1 will always be enough die-hards"
(in the refusing the future and facing the O(n^2) scaling wall or
centralisation death with perplexing optimism :)  Ok, not so
constructive but maybe a gentle reminder that it is not constructive
in the reverse direction either to throw around often false
characterisations.  We're here now to improve bitcoin so lets do that.
What I said here seemed like it maybe subject to misinterpretation so
to clarify:
I should clarify that I meant there I was assuming we do one increase
within the next 12 months frame that gives buffer for 5 years r&d to
improve things and build layer2.
But if we do no R&D on layer2, and insist that clients can never
change to become layer2 aware, and layer2 is too hard etc then our
risk would be we'd be back in the discussion of kicking the can afresh
again in some years with some even more centralising size change.
Sure we should make the transition and introduction to layer2 and an
intermediate crunch smoother, but "20MB now or else" isn't really
helping.  It did help get the conversation revived, but at this point
its a hindrance.  Seriously a big hindrance.  No offence but please
find a way to gracefully stop and rejoin the constructive process.
You can disagree on factors and points and be collaborative others
disagree frequently and have done productive work cordially for years
under the BIP process.
About scaling again:
Here is what I said before in my TL;DR post about my thoughts on how
we would start on throughput short-term to have space to do layer2
Btw I wonder if Gavin or Mike would be willing to answer another
question I forgot from my TL;DR post which was:
- Did you accept payment from companies to lobby for 20MB blocks?  Do
you consider that something appropriate to publicly disclose if so?
Do you consider that user rights should come above or below company
interests in Bitcoin?
FWIW on pondering that last question "should user rights come above or
below company interests" I think my view of the guiding principle is
starkly clear to me: that user rights should be the primary thing to
optimise for.  Businesses are providing service to users, their
interests are secondary in so far as if they are enabled to provide
better service thats good.
Bitcoin is a user p2p currency, with a social contract and a strong
user ethos.  Importing and forcing company interests would likely be
the start of a slippery slope towards an end to Bitcoin.   If we allow
business rights to be paramount it seems likely that we will end back
at the status quo as bitcoin payment processors grow, conglomerate and
become paypal/bank like or actual banks and then their interests and
exposures are the same as the banks and they'll want to import their
business models into Bitcoin and erode the user ethos features that
are actually what gives Bitcoin any meaning and value in the majority.
That wont be good for the companies either, but they may not see that
until they've killed it, many companies operate on a1 or 2 year
time-horizon.  They may say screw layer2, I have a runway and I need
micropayments to the wazoo and I dont have the dev resources for that.
Thats a conflict and the resolution isn't to override bitcoin's
meaning, but rather that they should do it at layer2 (eg changeTip
does this.. simple trustme layer2 which is OK given the amounts).  The
world needs a neutral social contract enforcing layer1.  Layer1 must
be neutral and free from policy and dispute resolution otherwise
dispute resolution costs are imported and you lose viral open
innovation growth vector the internet benefitted from.  Jurisdiction
and regulation related things belong at the interfaces and at the
payment protocol layer in my view.  (If thats not obvious to some
lurkers I elaborate on that argument  amongst other things here:
 )
ps the O(n^2) misunderstanding of varying assumptions was explored at
length on reddit
if people are interested in that topic.  I do not think O( t*n ) is a
useful metric because its predictive but only of the obvious and
internal, the useful predictive thing is resources vs users (for
nodes/users or whole-system).

@_date: 2015-06-23 21:42:37
@_author: Adam Back 
@_subject: [bitcoin-dev] game-theory, 
We shouldnt lose track of the aspect that miner's interests are not
directly aligned with user interests.  Users want security,
decentralisation properties and reasonably cheap fees, miners profit
from fees.  Particular miners may profit from centralisation.  Miners
are in competition with each other in a complex game.
We could do with some analysis on Jeff's BIP and Greg Maxwell's
flexcap (or Meni Rosenfeld's somewhat similar pay for size variant) --
and Gavin's proposal of what miner game-theory is anticipated and how
the proposals hold up under those attacks.
In Gavin's proposal why would we assume that 8MB wont be used?  Or the
huge 8GB later. It is free even for a miner to create blocks of any
size up to the cap (zero fees or fees paid to himself).
The stale rate of propagation delay maybe hidden by the relay-network
or by collusion, or advantage of a miner already knowing its own
Will a group of network topology close miners try to create big blocks
that disadvantage other miners?
Or will miners keep blocks small to extract switching-cost fees from
users.  (Regardless of cap).
Jeff's proposal has a cost free miner vote for cap increase with a
25%-ile and 90% threshold.  But in the second attack (keeping blocks
small) it alternatively becomes easy for an advantaged 10% to force
the block to stay small, in order to extract switching cost fees from
users.  Maybe users really love the decentralised features of bitcoin
and are willing to pay a lot!  Of course overlaid as Jeff observes by
meta-incentive that miners need to sell mined bitcoin to pay
electricity bills, and want Bitcoin to be in demand and therefore
indirectly to satisfy user demand.  But that may still result in a
fair bit of switching cost.  Switching cost economics is common in
many networks.
I submit that in terms of robustness of mechanism assuring security,
it be ordered something like:
1. consensus rule
2. aligned economic interest
3. attack requires miner collusion
4. meta-incentive
Then we could evaluate proposals for how robustly they can enforce
user interests vs miner game-theory attack or collusion scenarios.

@_date: 2015-06-25 16:07:59
@_author: Adam Back 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
Note Jeff's proposal and Greg Maxwell's flexcap proposals also have a
growth limiter as a hard-cap and a mechanism for influencing a dynamic
cap within that envelope.
The hard-cap serves the purpose of a safety limit in case our
understanding about the economics, incentives or game-theory is wrong
worst case.
More comments to follow.

@_date: 2015-06-27 17:33:11
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Everyone here is excited about the potential of Bitcoin and would
aspirationally like it to reach its full potential as fast as
possible.  But the block-size is not a free variable, half those
parameters you listed are in conflict with each other.  We're trying
to improve both decentralisation and throughput short-term while
people work on algorithmic improvements mid-term.  If you are
interested you can take a look through the proposals:
Note that probably 99% of Bitcoin transactions already happen
off-chain in exchanges, tipping services, hosted wallets etc.  Maybe
you're already using them, assuming you are a bitcoin user.
They constitute an early stage layer 2, some of them even have on
chain netting and scale faster than the block-chain.
You can also read about layer 2, the lightning network paper and the
duplex micropayment channel paper:
and read the development list and look at the code:

@_date: 2015-06-28 12:07:40
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
The total system cost is more relevant, or total cost per user.  I think you
are stuck on the O( t * m ) t = tx, m = nodes thinking.  Total cost per user
is increasing.  That better scaling algorithms need to be found.  That's why
people are working on lightning-like systems.
People have been explaining quadratic system level increase, which is
not exponential,
wrong assumption.
No people are not assuming decentralisation would decrease.  They are assuming
the number of economically dependent full nodes would increase, that's where the
O( n^2 ) comes from!  If we assume say c= 0.1% of users will run full nodes,
and users make some small-world assumed number of transactions that doesnt
increase greatly as more users are added to the network, then O( t * m
) => O( n^2 ).
Seeing decentralisation failing isn't a useful direction as Bitcoin depends on
decentralisation for most of it's useful security properties.  People running
around saying great lets centralise Bitcoin and scale it, are not working on
Bitcoin.  They may more usefully go work on competing systems without
proof of work as that's where this line of reasoning ends up.  There
are companies working on such things.  Some of them support Bitcoin IOUs.
Some of them have job openings.
We can improve decentralisation, and use bandwidth and relay improvements
to get some increase in throughput.  But starting a direction of simplistic
thinking about an ever increasing block-size mode of thinking is destructive
and not Bitcoin.  If you want to do that, you need to do it in an offchain
system.  You cant build on sand so your offchain system wont be useful
if Bitcoin doesnt have reasonable decentralisation to retain useful meaning.
Hence lightning.  There are existing layer 2 things that have on-chain netting.
Go work on one of those.  But people need to understand the constraints
and stop arguing to break Bitcoin to "scale".  It's too simplistic.
Even Gavin's proposal is not trying to do that, hence reference to
Nielsen's law.
His parameters are too high for too long for basic safety or prudence, but the
general idea to reclaim some throughput from network advances, is reasonable.
Also decentralisation is key, and that is something we can improve with pooling
protocols to phase out the artificial centralisation.  We can also
educate people
to use fullnode they economically depend on to keep the full to SPV ratio
reasonable which is also needed for security.

@_date: 2015-06-28 14:37:57
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Sure we can work incrementally and carefully, and this is exactly what
Bitcoin has been doing, and *must* do for safety and security for the
last 5 years!
That doesnt mean that useful serious improvements have not been made.
I think you misunderstand how lightning works.  Every lightning
transaction *is* a valid bitcoin transaction that could be posted to
the Bitcoin network to reclaim funds if a hub went permanently
offline.  It is just that while the hubs involved remain in service,
there is no need to do so.  This is why it has been described as a
(write coalescing) write cache layer for Bitcoin.>
I believe people expect lightning to be peer 2 peer like bitcoin.

@_date: 2015-06-28 19:51:00
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Recipients do benefit from keeping connections to hubs because if a
hub goes away or a user abandons a hub that tends to generate new
on-chain traffic for balance reclaim, and new channel establishment,
as we understand the limits so far.
I believe Mark is talking about the one hop (direct) connections
benefits from being long-lived; the payment destination is not
restricted in the same way.  It's more like having a static IP address
with your ISP, that doesnt stop you reaching anywhere on the internet.
Say the Lightning Network has an average fan out of 10, now subject to
capital and rebalancing flows in the network you can pay anyone of a
billion people in 9 hops.  Maybe the fanout is lumpy, with some bigger
hubs - that just serves to reduce the number of hops.  Maybe there are
some capitalisation limits, that is dealt with by negative fees and
recirculation (more on that below) or failing that recapitalisation
on-chain. Some people assume that the hub will run out of
capitalisation on a given channel, however if people and hubs retain
redundant channels they can be paid to rebalance channels, and even
users can be paid by other users if there is a net flow from some
users, to a given business eg starbucks, where the users just buy new
BTC for USD and spend and dont earn BTC.  Rebalancing would work
because the exchange where they buy new BTC would be incentivised to
pay starbucks (or whoever has excess coins on a channel) to send the
coins back to the users topping up by paying them negative fees,
because the fees to do that should be less than using on-chain
Actually I think it may well be able to do that very well.  We dont
know for sure how it will work until we see the balance and
effectiveness of the network algorithms against usage (eg simulating
from Bitcoin's historic usage say), but there's good reason to see
that BTC can recirculate and rebalance due to the reversible
non-expiring channels and capitalisation requirements can be lower
than simple expectation due higher velocity and redistribution of fees
to anyone with excess liquidity and connectivity heading in the right

@_date: 2015-06-28 20:58:55
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
This is probably going to sound impolite, but I think it's pertinent.
Gavin, on dwelling on the the fact that you appear to not understand
the basics of the lightning network, I am a little alarmed about this,
given your recent proposals to unilaterally push the network into
quite dangerous areas of game theory, to lobby companies etc.
People are super polite and respectful around here, but this is not
looking good, if you don't mind me saying so.  You can't make balanced
or informed trade-offs on block-size schedules stretching into the
future, if you don't understand work that is underway, and has been
for months.  Lightning is a major candidate approach the rest of the
technical community sees for Bitcoin to scale.
Lightning allows Bitcoin to scale even without a block-size increase,
and therefore considerably impacts any calculation of how much
block-size is required.  In this light you appear to have been
attempting to push through a change without even understanding the
alternatives or greater ecosystem.

@_date: 2015-06-28 23:00:28
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP Process and Votes 
I think we need a second mailing list: bitcoin-process for people to
learn about bitcoin process.
And someone to write a FAQ on it's sign up page so people interested
could at least discuss from a starting point of understanding how and
why it works the way it does!

@_date: 2015-06-29 00:07:11
@_author: Adam Back 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Its a source routed network, not a broadcast network.  Fees are
charged on channels so
DoS is just a way to pay people a multiple of bandwidth cost.
in terms of trustlessness Andrew Lapp explained it pretty well:
I gave it a go a couple of posts up.  I didnt realise people here
proposing mega-blocks were not paying attention to the whole lightning
concept and detail.
People said lots of things about how it's better to work on lightning,
to scale algorithmically, rather than increasing block-size to
dangerously centralising proportions.
Did you think we were Gish Galloping you?  We were completely serious.
The paper is on though it is not so clearly explained there, however Joseph is working
on improving the paper as I understand it.
Rusty wrote a high-level blog explainer: though I don't recall that he got into recirculation, negative fees
etc.  A good question
for the lightning-dev mailing list maybe.
There are a couple of recorded presentation videos / podcasts from Joseph Poon.
sf bitcoin dev presentation:
epicenter bitcoin:
There's a related paper from Christian Decker "Duplex Micropayment Channels"
We don't need to convince people, we just have to code it and
demonstrate it, which people are working on.
But Lightning does need a decentralised and secure Bitcoin network for
anchor and reclaim transactions, so take it easy with the mega-blocks
in the mean-time.
maybe you want to check in on
and help code it.
I expect we can get something running inside a year.  Which kind of
obviates the burning "need" for a schedule into the far future rising
to 8GB with unrealistic bandwidth growth assumptions that will surely
cause centralisation problems.
For block-size I think it would be better to have a 2-4 year or one
off size bump with policy limits and then re-evaluate after we've seen
what lightning can do.
I have been saying the same thing ad-nauseam for weeks.
Not nearly as big as if you tried to put the transactions it would
enable on the chain, that's for sure!  We dont know what that limit is
but people have been imagining 1,000 or 10,000 transactions per anchor
transaction.  If micro-payments get popular many more.
Basically users would park Bitcoins a on a hub channel instead of the
blockchain.  The channel can stay up indefinitely, and the user has
assurances analogous to greenaddress time-lock mechanism
Flexcap maybe a better solution because that allows bursting
block-size when economically rational.
Note that the time-locks with lightning are assumed to be relative
CTLV eg using the mechanism as Mark Friedenbach described in a post
here, and as implemented in the elements sidechain, so there is not a
huge rush to reclaim funds.  They can be spread out in time.
If you want to scale Bitcoin - like really scale it - work on
lightning.  Lightning + a decentralised and secure Bitcoin, scales
further and is more trustless than Bitcoin forced into centralisation
via premature mega-blocks.
To my mind a shorter, more conservative block-size increase to give a
few years room is enough for now.  We'll be in a better position to
know what the right next step is after lightning is running.
Something to mention is you can elide transactions before reclaiming.
So long as the balancing transaction is correct, someone online can
swap it for you with an equal balance one with less hops of
intermediate payment flows.
It's pretty interesting what you can do already.  I'm fairly confident
we're not finished algorithmically optimising it either.  It's
surprising how much new territory there is just sitting there

@_date: 2015-06-30 15:12:52
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
It is correct to view first-seen miner and relay policy as
honour-based, though it is the current default.
I think it would be better to deploy full-RBF in an opt-in way and
leave the current default miner & relay policies as is.  So for
example a new signature flag or transaction type that is marked as
opting-in waiving the first-seen policy.
In this way we can get a smoother transition for people away from the
first-seen assumption, towards greenaddress (trust based) and
lightning / payment channel solutions.  Mid-term the channel and hub
model can provide fast secure 0-confirm transactions, which are
generically not known to be directly and robustly securable via miner
As we've seen abruptly stopping the first-seen miner & relay policy is
risky and unpopular and causes people to seek contracts with miners to
retain first-seen.  This is itself a bad trend for fungibility for
obvious reasons.
We shouldn't prejudge people's and segment's of business weak-reliance
on first-seen.  Some types of payments are generally high trust (known
relationships) or physical stores or very low marginal cost (coffee
marginal cost <10c, sale price > $2 or lower ebook, audio stream etc
as embodied by fremium model).  It is not ours to prejudge and kill
their business.  It our job to help improve network security however,
so that they do not have to eat a fraud cost.  And that is do able
with trust via greenaddress, or without trust (other than
time-preference) via the hub & channel model.
Security maybe incrementally improvable via non-spendable relaying of
proof of double-spent status, and services that try to measure % of
miners by hashrate with assumption of first-seen that have have seen a
given transaction first, though I am not sure such approaches are
robust enough to invest time in vs greenaddress or hub & channel.
Any thoughts on the simplest way to support an opt-in version of full-RBF?

@_date: 2015-06-30 21:54:47
@_author: Adam Back 
@_subject: [bitcoin-dev] block-size tradeoffs & hypothetical alternatives (Re: 
done to increase block size to a static 8MB, and help do it)
Not that I'm arguing against scaling within tech limits - I agree we
can and should - but note block-size is not a free variable.  The
system is a balance of factors, interests and incentives.
As Greg said here
there are multiple things we should usefully do with increased
a) improve decentralisation and hence security/policy
neutrality/fungibility (which is quite weak right now by a number of
b) improve privacy (privacy features tend to consume bandwidth, eg see
the Confidential Transactions feature) or more incremental features.
c) increase throughput
I think some of the within tech limits bandwidth should be
pre-allocated to decentralisation improvements given a) above.
And I think that we should also see work to improve decentralisation
with better pooling protocols that people are working on, to remove
some of the artificial centralisation in the system.
Secondly on the interests and incentives - miners also play an
important part of the ecosystem and have gone through some lean times,
they may not be overjoyed to hear a plan to just whack the block-size
up to 8MB.  While it's true (within some limits) that miners could
collectively keep blocks smaller, there is the ongoing reality that
someone else can take break ranks and take any fee however de minimis
fee if there is a huge excess of space relative to current demand and
drive fees to zero for a few years.  A major thing even preserving
fees is wallet defaults, which could be overridden(plus protocol
velocity/fee limits).
I think solutions that see growth scale more smoothly - like Jeff
Garzik's and Greg Maxwell's and Gavin Andresen's (though Gavin's
starts with a step) are far less likely to create perverse unforeseen
side-effects.  Well we can foresee this particular effect, but the
market and game theory can surprise you so I think you generally want
the game-theory & market effects to operate within some more smoothly
changing caps, with some user or miner mutual control of the cap.
So to be concrete here's some hypotheticals (unvalidated numbers):
a) X MB cap with miner policy limits (simple, lasts a while)
b) starting at 1MB and growing to 2*X MB cap with 10%/year growth
limiter + policy limits
c) starting at 1MB and growing to 3*X MB cap with 15%/year growth
limiter + Jeff Garzik's miner vote.
d) starting at 1MB and growing to 4*X MB cap with 20%/year growth
limiter + Greg Maxwell's flexcap
I think it would be good to see some tests of achievable network
bandwidth on a range of networks, but as an illustration say X is 2MB.
Rationale being the weaker the signalling mechanism between users and
user demanded size (in most models communicated via miners), the more
risk something will go in an unforeseen direction and hence the lower
the cap and more conservative the growth curve.
15% growth limiter is not Nielsen's law by intent.  Akamai have data
on what they serve, and it's more like 15% per annum, but very
variable by country
CISCO expect home DSL to double in 5 years
), which is about the same number.
(Thanks to Rusty for data sources for 15% number).
This also supports the claim I have made a few times here, that it is
not realistic to support massive growth without algorithmic
improvement from Lightning like or extension-block like opt-in
systems.  People who are proposing that we ramp blocksizes to create
big headroom are I think from what has been said over time, often
without advertising it clearly, actually assuming and being ok with
the idea that full nodes move into data-centers period and small
business/power user validation becomes a thing of the distant past.
Further the aggressive auto-growth risks seeing that trend continuing
into higher tier data-centers with negative implications for
decentralisation.  The odd proponent seems OK with even that too.
Decentralisation is key to Bitcoin's security model, and it's
differentiating properties.  I think those aggressive growth numbers
stray into the zone of losing efficiency.  By which I mean in
scalability or privacy systems if you make a trade-off too far, it
becomes time to re-asses what you're doing.  For example at that level
of centralisation, alternative designs are more network efficient,
while achieving the same effective (weak) decentralisation.  In
Bitcoin I see this as a strong argument not to push things to that
extreme, the core functionality must remain for Lightning and other
scaling approaches to remain secure by using the Bitcoin as a secure
anchor.  If we heavily centralise and weaken the security of the main
Bitcoin chain, there remains nothing secure to build on.
Therefore I think it's more appropriate for high scale to rely on
lightning, or a semi-centralised trade-offs being in the side-chain
model or similar, where the higher risk of centralisation is opt-in
and not exposed back (due to the security firewall) to the Bitcoin
network itself.
People who would like to try the higher tier data-center and
throughput by high bandwidth use route should in my opinion run that
experiment as a layer 2 side-chain or analogous.  There are a few ways
to do that.  And it would be appropriate to my mind that we discuss
them here also.
An experiment like that could run in parallel with lightning, maybe it
could be done faster, or offer different trade-offs, so could be an
interesting and useful thing to see work on.
A secondary function can be a market signalling - market evidence
throughput can increase, and there is a technical process that is
effectively working on it.  While people may not all understand the
trade-offs and decentralisation work that should happen in parallel,
nor the Lightning protocol's expected properties - they can appreciate
perceived progress and an evidently functioning process.  Kind of a
weak rationale, from a purely technical perspective, but it may some
value, and is certainly less risky than a unilateral fork.
As I recall Gavin has said things about this area before also
(demonstrate throughput progress to the market).
Another factor that people have said, which I think I agree with
fairly much is that if we can chose something conservative, that there
is wide-spread support for, it can be safer to do it with moderate
lead time.  Then if there is an implied 3-6mo lead time we are maybe
projecting ahead a bit further on block-size utilisation.  Of course
the risk is we overshoot demand but there probably should be some
balance between that risk and the risk of doing a more rushed change
that requires system wide upgrade of all non-SPV software, where
stragglers risk losing money.
As well as scaling block-size within tech limits, we should include a
commitment to improve decentralisation, and I think any proposal
should be reasonably well analysed in terms of bandwidth assumptions
and game-theory.  eg In IETF documents they have a security
considerations section, and sometimes a privacy section.  In BIPs
maybe we need a security, privacy and decentralisation/fungibility
NB some new list participants may not be aware that miners are
imposing local policy limits eg at 750kB and that a 250kB policy
existed in the past and those limits saw utilisation and were
unilaterally increased unevenly.  I'm not sure if anyone has a clear
picture of what limits are imposed by hash-rate even today.  That's
why Pieter posed the question - are we already at the policy limit -
maybe the blocks we're seeing are closely tracking policy limits, if
someone mapped that and asked miners by hash-rate etc.

@_date: 2015-05-07 19:16:12
@_author: Adam Back 
@_subject: [Bitcoin-development] Mechanics of a hard fork 
Well this is all very extreme circumstances, and you'd have to assume no
rational player with an interest in bitcoin would go there, but to play
your analysis forward: users are also not powerless at the extreme: they
could change the hash function rendering current deployed ASICs useless in
reaction for example, and reset difficulty at the same time, or freeze
transactions until some minimum hashrate is reached.  People would figure
out what is the least bad way forward.

@_date: 2015-05-12 16:48:06
@_author: Adam Back 
@_subject: [Bitcoin-development] Long-term mining incentives 
I think its fair to say no one knows how to make a consensus that
works in a decentralised fashion that doesnt weaken the bitcoin
security model without proof-of-work for now.
I am presuming Gavin is just saying in the context of not pre-judging
the future that maybe in the far future another innovation might be
found (or alternatively maybe its not mathematically possible).
Towards that it would be useful to try further to prove this one way
or another (prove that proof of stake cant work if that is generically
mathematically provable).

@_date: 2015-05-26 19:47:39
@_author: Adam Back 
@_subject: [Bitcoin-development] Cost savings by using replace-by-fee, 
The general idea for replace by fee is that it would be restricted so
as to make it safe, eg all the original addresses should receive no
less bitcoin (more addresses can be added).
The scorched earth game theory stuff (allowing removing recipients) is
kind of orthogonal.

@_date: 2015-05-26 23:06:42
@_author: Adam Back 
@_subject: [Bitcoin-development] Cost savings by using replace-by-fee, 
Well so for example it could have an additional input (to increase the
BTC paid into the transaction) and pay more to an existing change
address and higher fee, or add an additional change address, and leave
a larger fee, or if you had a right-sized coin add an additional input
that all goes to fees.
(As well as optionally tacking on additional pending payments to other
addresses funded from the higher input).

@_date: 2015-05-26 23:18:42
@_author: Adam Back 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
I think the point is the replacement tx spends the same inputs (plus
additional inputs), so if the original tx is accepted, and your
replacement rejected, thats good news - you dont have to pay the
higher fee, the extra input remains unspent (and can be used later for
other purpose) and the extra change address is unused.
(If you had bundled extra transactions into the replacement, spending
from the additional inputs, then you'll need to resubmit those as a
separate transaction).
(You have to keep in mind re-orgs so for example the original tx could
be put into a block, and then that block could get reorged by another
block that grows into a longer chain with the replacement tx in it (or
vice versa)).

@_date: 2015-05-28 09:11:21
@_author: Adam Back 
@_subject: [Bitcoin-development] Version bits proposal 
Or as far as that goes, permuting (the non-dependent) transactions in
the block by permuting the internal merkle tree nodes at increasing
depths.  (Dependent because transactions that depend on each other
have to come in-order; but one could eg put the n-1 of each n sequence
of in-order transactions in the left-half and unordered in the right
That makes the tree manipulations maximum depth independent, and even
transaction independent possibly - just need to know enough depth in
the tree of hashes that are permutation safe.

@_date: 2015-05-30 01:00:28
@_author: Adam Back 
@_subject: [Bitcoin-development] soft-fork block size increase (extension 
I discussed the extension block idea on wizards a while back and it is
a way to soft-fork an opt-in block-size increase.  Like everything
here there are pros and cons.
The security is better than Raylstonn inferred from Tier's explanation
I think..  It works as Tier described - there is an extension block
(say 10MB) and the existing 1MB block.  The extension block is
committed to in the 1MB chain.  Users can transfer bitcoin into the
extension block, and they can transfer them out.
The interesting thing is this makes block sizes changes opt-in and
gives users choice.  Choice is good.  Bitcoin has a one-size-fits-all
blocksize at present hence the block size debate.  If a bigger
block-size were an opt-in choice, and some people wanted 10MB or even
100MB blocks for low value transactions I expect it would be far
easier a discussion - people who think 100MB blocks are dangerously
centralising, would not opt to use them (or would put only small
values they can afford to lose in them).  There are some security
implications though, so this also is nuanced, and more on that in a
Fee pressure still exists for blocks of difference size as the
security assurances are not the same.  It is plausible that some
people would pay more for transactions in the 1MB block.
Now there are three choices of validation level, rather than the
normal 2-levels of SPV or full-node, with extension blocks we get a
choice: A) a user could run a full node for both 1MB and 10MB blocks,
and get full security for both 1MB and 10MB block transactions (but at
higher bandwidth cost), or B) a user could run a full node on the 1MB
block, but operate as an SPV node for the 10MB block, or C) run in SPV
mode for both 1MB and 10MB blocks.
Similarly for mining - miners could validate 1MB and 10MB transactions
(solo mine or GBT-style), or they could self-validate 1MB transactions
and pool mine 10MB transactions (have a pool validate those).
1MB full node users who do not upgrade to software that understands
extension blocks, could run in SPV mode with respect to 10MB blocks.
Here lies the risk - this imposes a security downgrade on the 1MB
non-upgraded users, and also on users who upgrade but dont have the
bandwidth to validate 10MB blocks.
We could defend non-upgrade users by making receiving funds that came
via the extension block opt-in also, eg an optional to use new address
version and construct the extension block so that payments out of it
can only go to new version addresses.
We could harden 1MB block SPV security (when receiving weaker
extension block transactions) in a number of ways: UTXO commitments,
fraud proofs (and fraud bounties) for moving from the extension block
to the 1MB block.  We could optionally require coins moving via the
extension block to the 1MB block to be matured (eg 100 blocks delay)
Anyway something else to evaluate.  Not as simple to code as a
hard-fork, but way safer transition than a hard-fork, and opt-in - if
you prefer the higher decentralisation of 1MB blocks, keep using them;
if you prefer 10MB blocks you can opt-in to them.

@_date: 2015-11-05 23:29:46
@_author: Adam Back 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
About the conflicting spends by the private key holder (self signature
malleability) that is in principle kind of fixable.
You make a new pub key type which is r,Q (where r is the DSA signature
component but chosen at key gen time, Q=xG is the pub key, r is point
compressed R = (r,f(r)) = kG ), r is the pre-computable part of an
ECDSA signature (unrelated to the message which can be decided later).
You make a new address type which is a = H(r,Q).
Then you make a new signature type which requires that the r from
sig=(r,s) matches the r committed to in the address.
As the ECDSA signature is s=(H(m)+r*x)/k mod n, if they sign two
different messages with the same r value they reveal the private key
via simultaneous equation, as s=(H(m)+r*x)/k and s'=(H(m')+r*x)/k and
solving k=(H(m)-H(m'))/(s-s') and x=(sk-H(m))/r allowing anyone who
sees both double spends to spend as they can replace the signature
with their own one.  That converts double signatures into miner can
It doesnt necessarily enforce no pubkey reuse (Q), as a=H(r,Q) and
a'=H(r',Q) are different addresses, though it does enforce no
extended-address reuse (H=(r,Q)).
Binary failure address reuse could be an issue.  Puts pressure on
transactional storage on wallets.
On 5 November 2015 at 20:36, Luke Dashjr via bitcoin-dev

@_date: 2015-11-06 00:03:32
@_author: Adam Back 
@_subject: [bitcoin-dev] summarising security assumptions (re cost metrics) 
Some thoughts, hope this is not off-topic.
Maybe we should summarise the security assumptions and design
requirements.  It is often easier to have clear design discussions by
first articulating assumptions and requirements.
Validators: Economically dependent full nodes are an important part of
Bitcoin's security model because they assure Bitcoin security by
enforcing consensus rules.  While full nodes do not have orphan
risk, we also dont want maliciously crafted blocks with pathological
validation cost to erode security by knocking reasonable spec full
nodes off the network on CPU (or bandwidth grounds).
Miners: Miners are in a commodity economics competitive environment
where various types of attacks and collusion, even with small
advantage, may see actual use due to the advantage being significant
relative to the at times low profit margin
It is quite important for bitcoin decentralisation security that small
miners not be significantly disadvantaged vs large miners.  Similarly
it is important that there not be significant collusion advantages
that create policy centralisation as a side-effect (for example what
happened with "SPV mining" or validationless mining during BIP66
deployment).  Examples of attacks include selfish-mining and
amplifying that kind of attack via artificially large or
pathologically expensive to validate blocks.  Or elevating orphan risk
for others (a miner or collusion of miners is not at orphan risk for a
block they created).
Validators vs Miner decentralisation balance:
There is a tradeoff where we can tolerate weak miner decentralisation
if we can rely on good validator decentralisation or vice versa.  But
both being weak is risky.  Currently given mining centralisation
itself is weak, that makes validator decentralisation a critical
remaining defence - ie security depends more on validator
decentralisation than it would if mining decentralisation was in a
better shape.
We should consider the pathological case not average or default behaviour
because we can not assume people will follow the defaults, only the
consensus-enforced rules.
We should not discount attacks that have not seen exploitation to
date.  We have maybe benefitted from universal good-will (everybody
thinks Bitcoin is cool, particularly people with skills to find and
exploit attacks).
We can consider a hierarchy of defences most secure to least:
1. consensus rule enforced (attacker loses block reward)
2. economic alignment (attacker loses money)
3. overt (profitable, but overt attacks are less likely to be exploited)
4. meta-incentive (relying on meta-incentive to not damage the ecosystem only)
Best practices:
We might want to list some best practices that are important for the
health and security of the Bitcoin network.
Rule of thumb KISS stuff:
We should aim to keep things simple in general and to avoid creating
complex optimisation problems for transaction processors, wallets,
We may want to consider an incremental approach (shorter-time frame or
less technically ambitious) in the interests of simplifying and
getting something easier to arrive at consensus, and thus faster to
We should not let the perfect be the enemy of the good.  But we should
not store new problems for the future, costs are stacked in favour of
getting it right vs A/B testing on the live network.
Not everything maybe fixable in one go for complexity reasons or for
the reason that there is no clear solution for some issues.  We should
work incrementally.

@_date: 2015-11-06 15:08:10
@_author: Adam Back 
@_subject: [bitcoin-dev] summarising security assumptions (re cost metrics) 
You're right that it is better that there be more APIs than fewer,
that is less of a single point of failure.  It also depends what you
mean by APIs: using an API to have a second cross-check of information
is quite different to building a wallet or business that only
interfaces with the blockchain via a 3rd party API.  There are
different APIs also: some are additive eg they add a second signature
via multisig, but even those models while better can still be a mixed
story for security, if the user is not also checking their own
full-node or checking SPV to make the first signature.
Power users and businesses using APIs instead of running a full-node,
or instead of doing SPV checks, should be clear about the API and what
security they are delegating to a third party, and whether they have a
reason to trust the governance and security competence of the third
party: in the simplest case it can reduce their and their users
security below SPV.
The bigger point however, which Erik explained, was: widespread use of
APIs as a sole means of interfacing with the blockchain also
contributes to reducing network security for everyone, because it
erodes the consensus rule validation security described under
"Validators" in the OP.

@_date: 2015-11-14 10:31:18
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP - Block size doubles at each reward halving 
There is a difference between miners signalling intent (as they have
been for various BIPs, which is mostly informational only - they are
mostly not running the code, and in some cases it is not implemented,
so they cant be) there is a difference between that and a 95% miner
majority consensus rule.  Former can be useful information as you
said, latter implies as Luke described something that is not really
accurate, it is not strictly only a miner upgrade needed for basic
safety as with soft-forks.  If you look at BIP 103 for example it is
flag day based, and I think this is a more accurate approach.  Also
with miner votes they can be misleading - vote for one thing, but run
something else; what they are running is not generally
detectable/enforceable - see for example what happened with the BIP66
accidental fork due to "SPV mining" (ie validationless mining).
A hard-fork is for everyone to upgrade and talk with each other to see
that the vast majority is on the same plan which includes users,
ecosystem companies & miners.
On 14 November 2015 at 01:02, digitsu412 via bitcoin-dev

@_date: 2015-10-07 12:42:08
@_author: Adam Back 
@_subject: [bitcoin-dev] on rough consensus 
Thank you for posting that, most informative, and suggest people
arguing here lately to read it carefully.
May I suggest that people who wish to debate what rough consensus
means, to take it to this reddit thread
Thanks again for posting, helpful context/reminder for all.
On 7 October 2015 at 07:07, Ryan Grant via bitcoin-dev

@_date: 2015-10-08 01:07:48
@_author: Adam Back 
@_subject: [bitcoin-dev] soft-fork security (Re: Let's deploy BIP65 
On 7 October 2015 at 18:26, Jonathan Toomim (Toomim Bros) via
[ie wait for a non-upgraded miner to win a block]
I dont think that is something strong and new to focus on or worry
about, because in Bitcoin's game theory there are lets say 3 types of
miners we're in aggregate trying to get security from:
a) honest (following protocol) bolstered by financial incentive to
remain honest of subsidy & fees
b) agnostic / lazy (just run software, upgrade when they lose money
and/or get shouted at)
c) dishonest
Bitcoin remains secure with various combinations of percentages.  For
sure you wont have a good time if you assume < 1% are dishonest.
Therefore this attack can already happen, and in fact has.  Users of
bitcoin must behave accordingly with confirmations.
Bitcoin direct is not super secure for unconfirmed (so-called
0-confirm) transactions, or even for 1-confirm transactions.  See also
Finney attack.
That does not prevent people using unconfirmed transactions with risk
scoring, or in high trust settings, or high margin businesses selling
digital artefacts or physical with nominal incremental cost.
But it does mean that one has to keep that in mind.  And it also
motivates lightning network or payment channels (lightning with one
intermediate node vs a network of nodes) - they can provide basically
instant 0-confirm securely, and that seems like the future.
In my opinion anyone relying on unconfirmed transactions needs to
monitor for problems, and have some plan B or workaround if the fraud
rates shoot up (if someone tries to attack it in an organised way),
and also a plan C mid-term plan to do something more robust.  Some
people are less charitable and want to kill unconfirmed transactions
immediately.  The message is the same ultimately.

@_date: 2015-10-13 22:52:39
@_author: Adam Back 
@_subject: [bitcoin-dev] Liquid 
Benjamin you may want to read this reddit thread on liquid:
and if you feel your questions are not answered, post them there?
(This list is about bitcoin development so it's not really on topic for here).
On 13 October 2015 at 21:37, Daniel Newton via bitcoin-dev

@_date: 2015-10-15 18:37:13
@_author: Adam Back 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Does that pre-judge that block interval would never change from
10mins?  Eg say with IBLT or fountain codes etc and security arguments
for the current limitations of them are found, such that orphan rates
can remain low in a decentralised way with 1min blocks, then the
locktime granularity would be coarse relative to the block interval
(with 512s locktime granularity.
On 15 October 2015 at 18:27, Btc Drak via bitcoin-dev

@_date: 2015-09-01 01:42:51
@_author: Adam Back 
@_subject: [bitcoin-dev] Let's kill Bitcoin Core and allow the green 
Peter this seems to be a not well thought through course of action,
fun though it maybe informally or philosophically or to tweak various
peoples sensibilities.
Bitcoin is a consensus system that does not work if there are
incompatible versions of consensus code competing on the network.
This is why work is underway on libconsensus so we can see diversity
of implementation without the risk of incompatibility arising by
software defect.  It has proven quite hard to match independent
consensus implementations bit for bit against an adaptive adversary
looking for inconsistencies in interpretation.
In terms of protocol updates it is more constructive therefore that
people with a technical interest analyse and validate others proposals
via testing, or make their own proposals so that we can arrive at a
well validated upgrade mechanism that everyone upgrades to in a
coordinated fashion.
Previous intentional upgrades to bitcoin have been
backwards-compatible (via soft-fork which can be secured reasonably
using a miner vote trigger and temporary SPV security for those who
not yet upgraded) but the current topic of a throughput increase is
non-backwards-compatible (via a hard-fork), and the way to minimise
risk of such an upgrade is for everyone to try to upgrade well in
advance of an agreed upgrade schedule, and to be all upgrading to the
*same* consensus rule change.
Encouraging nodes or miners to "vote" by running a range of different
consensus rules isnt really constructive I feel - it alarms people who
understand the risks, sets things on a path that have to be fixed
while in flight by obvious implication, and isnt collaborative - so it
risks being a politicising suggestion on what should be a purely
technical topic of choosing the best approach, where it is best to
strive to keep things non-emotive and professional and technically
focussed.  Such calls are offending the technical sensibilities of
people who understand the risks.
Anyway lets try to keep things constructive and focus on analysing proposals.
On 31 August 2015 at 19:16, Peter R via bitcoin-dev

@_date: 2015-09-08 14:13:16
@_author: Adam Back 
@_subject: [bitcoin-dev] Dynamic limit to the block size - BIP draft 
The maximum block-size is one that can be filled at zero-cost by
miners, and so allows some kinds of amplification of selfish-mining
related attacks.
On 8 September 2015 at 13:28, Ivan Brightly via bitcoin-dev

@_date: 2015-09-08 15:18:03
@_author: Adam Back 
@_subject: [bitcoin-dev] Dynamic limit to the block size - BIP draft 
You seem to be analysing a different attack - I mean that if someone
has enough hashrate to do a selfish mining attack, then setting up a
system that has no means to reduce block-size risks that at a point
where there is excess block-size they can use that free transaction
space to amplify selfish mining instead of collecting transaction

@_date: 2015-09-10 18:48:42
@_author: Adam Back 
@_subject: [bitcoin-dev] Bitcoin threat modelling thread 
Came across this
 useful
thread discussing Bitcoin threat modelling may reach wider audience on this
Text from Mike Hearn:
 think the next stage is to build a threat model for Bitcoin.
This mail starts with background. If you already know what a threat model
is you can skip to the last section where I propose a first draft, as the
starting point for discussion.
An intro to threat modelling
In security engineering, a threat model
 is a document that informally
Which adversaries (enemies) do you care about?What can they do?Why do they
want to attack you?As a result: what threats do they pose?How do you
prioritise these threats?
Establishing a threat model is an important part of any security
engineering project. In the early days of secure computing, threat
modelling hadn't been invented and as a result projects frequently hit the
following problem:
Every threat looked equally serious, so it became impossible to prioritise
Almost anything could become a threat, if you squinted right
So usability, performance, code maintainability etc were sacrificed over
and over to try and defend against absurd or very unlikely threats just
because someone identified one, in an endless race
The resulting product sucked and nobody used it, thus protecting people
from no threats
PGP is a good example of this problem in action.
Making good threat models isn't easy (see The Economist article,New Threat
Model Army
It can be controversial, because a threat model involves accepting that you
can't win all the time - there will exist adversaries that you
realistically cannot beat. Writing them down and not worrying about them
anymore liberates you to focus on other threats you might do a better job
at, or to work on usability, or features, or other things that users might
actually care about more.
You can make your threat model too weak, so it doesn't encompass real
threats your users are likely to encounter. But a much more common problem
is making the model too strong: including *too many* different kinds of
threats. Strangely, this can make your product *less* secure rather than
One reason is that with too many threats in your model, you can lose your
ability to prioritise: every threat seems equally important even if perhaps
really they aren't, and then you can waste time solving "threats" that are
absurd or incredibly unlikely.
Even worse, once people add things in to a threat model they hate taking
them out, because it'd imply that previous efforts were wasted.
The Tor threat model
A good example of this is Tor. As you my know I have kind of a love/hate
relationship with Tor. It's a useful thing, but I often feel they could do
things differently.
The Tor
* have
a threat model
, and
it is a very strong one. Tor tries to protect you against adversaries that
care about very small leaks of application level data, like a browser
reporting your screen size, because it sees its mission as making all
traffic look identical, rather than just hiding your IP address. As a
consequence of this threat model Tor is meant to be used with apps that are
specifically "Torified", like their Tor Browser which is based on Firefox.
If a user takes the obvious approach of just downloading and running the
Tor Browser Bundle, their iTunes traffic won't be anonymised. The rationale
is it's useless to route traffic of random apps via Tor because even if
that hides the IP address, the apps might leak private data anyway as they
weren't designed for it.
This threat model has a couple of consequences:
It's extremely easy to think you're hiding your IP address when in fact you
aren't, due to using or accidentally running non-Torified apps.
The Tor Browser is based on Firefox. When Chrome came along it had a
clearly superior security architecture, because it was sandboxed, but the
Tor project had made a big investment in customising Firefox to anonymise
things like screen sizes. They didn't want to redo all that work.
The end result of this is that Tor's adversaries discovered they could just
break Tor completely by hacking the web browser, as Firefox is the least
secure browser and yet it's the one the Tor project recommends. The Snowden
files contain a bunch of references to this.
Interestingly, the Tor threat model explicitly *excludes* the NSA because
it can observe the whole network (it is the so-called "global passive
adversary"). Tor does this because they want to support low latency web
browsing, and nobody knows how to do that fast enough when your adversary
can watch the traffic between all Tor nodes. So they just exclude such
enemies from their threat model and that is why Tor is possible.
But even more interestingly, it turned out that their threat model
assumptions weren't quite correct. The NSA/GCHQ should, in theory, be able
to totally deanonymise Tor. But in practice they can't. When the time
finally came the 5 Eyes agencies attacked Tor by hacking the web browser,
not by exploiting their global observation abilities.
Tor has competitors - the commercial VPN providers. They have a rather
different threat model, where they explicitly don't care about application
level attacks like web sites looking at your screen size. They *only* care
about hiding your IP address. As a result their products work for every
app, and users can easily use Chrome or any other secure web browser.
Additionally they only add one hop of latency because the VPN provider does
not include itself in the threat model.
This solves for a different set of adversaries, but for many users it's
actually a more appropriate set and as a result VPNs are vastly more
popular than Tor is.
So to recap, we should build a threat model for Bitcoin because:
We have limited manpower and therefore must prioritise, sometimes brutally
Without a model anything can be a threat, so changes that are obvious or
look like technical no-brainers can get shot down due to the risk of absurd
or ridiculous attacks. This happens in Bitcoin Core *a lot*.
It will bring more formality and rigour to our thinking about security.
Proposed model
This is *a suggestion only*. I expect vigorous debate and for some people
to want a different (probably stronger) model. Models are just documents so
they can always be tweaked later, there's no need for v1 to be perfect.
OK. Adversaries I think we should care about in version 1, in priority
Rational individuals and small groups, motivated by profit.
The "global passive adversary" as defined by the
, motivated by a desire to map Bitcoin
transactions to people in bulk.
And that's it.
*The GPA*
The "global passive adversary" can mean intelligence agencies *but only
sometimes*. Specifically, it assumes they only watch and they don't
actively interfere. This assumption is of course not entirely valid - IAs
do sometimes engage in active attacks. My suggested threat model doesn't
include that activity because (1) it's hard to do anything about it and (2)
they much prefer to stay stealthy anyway.
Of course, in the Bitcoin system, there may be other GPAs. Anyone who
watches the block chain can potentially be such an adversary. Note the
careful wording: you have to be doing deanonymization *in bulk* to be an
in-scope adversary. This is to avoid including block explorers that have
notes features, people who build lists of well known addresses etc. We
can't stop people doing that: it's up to Bitcoin users to avoid telling the
world which transactions are theirs. It also excludes exchanges that are
trying to monitor transactions going in and out of their platform for
compliance purposes: they are not attempting to do this for the entire
system, therefore, they are not adversaries in this threat model.
*Individuals and small groups*
They are assumed to have hacking skills that are considered good by the
standards of ordinary hackers - they are not script kiddies. However they
are also not state-level hackers: they do not have an endless bag of zero
days that can exploit any imaginable device.
These attackers are motivated by profit. An attack that yields only
worthless pieces of data is not interesting to these adversaries: they want
to monetise. Attacks that involve some incredibly convoluted process to
turn data into money is also uninteresting: we assume a level of
rationality that means they'll ignore attacks with very poor effort/reward
Here are some examples of attackers that would be in-scope for this threat
? A hacker who is attempting to steal the contents of your Bitcoin wallet
? A mugger at a conference who is trying to identify rich targets to beat up
? A business owner who is attempting to discover the revenue of his
? A government attempting to build a map of every Bitcoin transaction to
? Someone attempting to profit off a quick market panic by short selling
BTC and then DoSing the network
.... and would not be in scope .....
? An actor who learns IP addresses of people using Bitcoin (reason: not
profitable, mere fact of use is not enough to build a GPA map)
? A short seller who needs to successfully root a specific, well run server
to cause problems (reason: without zero days it's hard to attack a fully
patched and locked down machine)
? A bitcoin exchange that demands proof of where your money came from
(reason: not global adversary)
? A government who wants to shut down Bitcoin globally (reason: active
state adversary, can't realistically stop this as they can always mine a
bogus chain)
? A government who wants to shut down Bitcoin in their own territory
(reason: active state adversary, can just find and arrest anyone
advertising BTC acceptance)
? A developer who wants to turn the block chain into a file sharing network
(reason: not rational, the resulting product would be terrible)
? A random individual learning the balance of wallets on random IP
addresses (reason: can't monetise with any reasonable effort)
.... and could be argued either way .....
? A developer who wants to use the block chain for timestamping lots of
data (can be seen as "motivated by profit", OTOH, actual threat is pretty
? A miner who constantly tries to mine zero sized blocks or constantly
double spends against high profile merchants (can be seen as "motivated by
profit" but also not rational behaviour as it'd tank the price of BTC)
Obviously this stuff is subjective. We can argue about what "rational"
means for miners, for instance.
The goal of the model is not to be 100% accurate or a perfect prediction of
the future. It's just there to help people prioritise development efforts.
Should I work on *this* new feature or addressing *that* threat? A threat
model can help you decide whether it's worth it. People can still choose to
work on threats that are outside of this model if they want to, and we can
also choose to ignore threats that might be inside it, if the cost/benefit
ratio is really bad.
The exclusion of many types of government adversary might be controversial.
It's for practical reasons: governments have lots of very effective ways to
interfere with Bitcoin that we can't do anything about, like bank
blockades, and so far most of them seem to be taking a wait-and-see stance

@_date: 2015-09-11 12:47:22
@_author: Adam Back 
@_subject: [bitcoin-dev] Yet another blocklimit proposal / compromise 
Bitcoin security depends on the enforcement of consensus rules which
is done by economically dependent full nodes.  This is distinct from
miners fullnodes, and balances miners interests, otherwise SPV nodes
and decentralisation of policy would tend degrade, I think.  Therefore
it is important that it be reasonably convenient to run full nodes for
decentralisation security.
Also you may want to read this summary of Bitcoin decentralisation by Mark:
I think you maybe misunderstanding what the Chinese miners said also,
about 8MB, that was a cap on the maximum they felt they could handle
with current network infrastructure.
I had proposed 2-4-8MB growing over a 4 year time frame with 2MB once
the hard-fork is upgraded by everyone in the network.  (I dont
consider miner triggers, as with soft-fork upgrades, to be an
appropriate roll out mechanism because it is more important that
economically dependent full nodes upgrade, though it can be useful to
know that miners also have upgraded to a reasonable extent to avoid a
temporary hashrate drop off affecting security).
On 9 September 2015 at 15:00, Marcel Jamin via bitcoin-dev

@_date: 2015-09-28 07:00:26
@_author: Adam Back 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I wonder what Gavin's views are, he's usually constructive, and see if
he'll include it in XT - I think he may have said he was supportive.
The rationale for soft vs hard-forks is well known, so I wont go over them.
On 28 September 2015 at 06:48, Mike Hearn via bitcoin-dev

@_date: 2015-09-30 02:19:56
@_author: Adam Back 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I think from discussion with Gavin sometime during the montreal
scaling bitcoin workshop, XT maybe willing to make things easy and
adapt what it's doing.  For example in relation to versionBits Gavin
said he'd be willing to update XT with an updated/improved
versionBits, for example.
It seems more sensible to do what is simple and clean and have both
core do that, and XT follow if there is no particular philosophy
debate on a given technical topic.  This seems a quite constructive
On 30 September 2015 at 00:05, Rusty Russell via bitcoin-dev

@_date: 2015-09-30 13:14:08
@_author: Adam Back 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I was talking about the versionBits from Rusty's email (pasted below) and
simplifying that by XT adopting the patch as Gavin had seemed agreeable to.

@_date: 2015-09-30 14:15:03
@_author: Adam Back 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
BIP101 is a hybrid: in some ways it is a hard-fork and in other ways
it is a soft-fork.  It is a hard-fork to full-nodes, but also a
soft-fork to SPV clients, as by definition the SPV miners are having
changes made whether they approve or not as they are not even aware of
the change.
I think people are saying CLTV is long discussed and does have consensus.
Let's not conflate CLTV with a discussion about future possible
deployment methods.  Forks are an interesting but different topic.
Soft-forks have a lot of mileage on them at this point, hard-forks do
not, and are anyway inherently higher riskier, even ignoring our lack
of practical experience with planned hard-forks.
With a soft-fork, while it's clear there is a temporary security model
reduction for SPV nodes (and non-upgraded full nodes) in the period
before they upgrade, this is preferable to the risks of a system-wide
coordinated hard-fork upgrade.  There is some limit if the complexity
of soft-forking a feature is quite complicated (eg one could argue
that with soft-fork extension-blocks vs hard-fork method of increasing
block-size for example).  So the balance, which I think is easily met
with CLTV, is that soft-fork is simple-enough technically and the
feature is entirely non-controversial and additive functionality
improvement without downside or reason for dissent.
To my view this is an answer to your question "what is the specific
benefit to end users of doing [soft-forks]" -- it is a lower risk, and
therefore faster way to deploy non-controversial (additive) changes.
Given the CLTV is useful for improving lightning efficiency this is
good for improving Bitcoin's scalability.

@_date: 2016-12-11 10:21:01
@_author: Adam Back 
@_subject: [bitcoin-dev] Managing block size the same way we do difficulty 
Well I think empirical game-theory observed on the network involves more
types of strategy than honest vs dishonest.  At least 4, maybe 5 types of
strategy and I would argue lumping the strategies together results in
incorrect game theory conclusions and predictions.
A) altruistic players (protocol following by principle to be good network
citizens, will forgo incremental profits to aid network health) eg aim to
decentralize hashrate, will mine stuck transactions for free, run pools
with zero fee, put more effort into custom spam filtering, tend to be power
users, or long term invested etc.
B) honest players (protocol following but non-altruistic or just
lazy/asleep run default software, but still leaving some dishonest profit
untaken). Eg reject spy mining, but no charitable actions, will not
retaliate in kind to semi-honest zero sum attacks that reduce their profits.
C) semi-honest (will violate protocol if their attack can be plausibly
deniable or argued to be not hugely damaging to network security). Eg spy
mining, centralised pools increasing other miners orphan rates.
D) rational players (will violate the protocol for profit: will not overtly
steal from users via double spends, but anything short particularly
disadvantaging other miners even if it results in centralisation is treated
as fair game) eg selfish mining. Would increase block size by filling with
pay to self transactions, if it increased orphans for others.
E) dishonest players (aka hyper-rational: will actually steal from users
probabilistically if possible, not as worried about detection). Eg double
spend and probabilistic double spends (against onchain gambling games).
Would DDoS competing pools.
In part the strategies depend on investment horizon, it is long term
rational for altruistic behavior to forgo incremental short term profit to
improve user experience.  Hyper-rational to buy votes in a "ends justify
means" mentality though fortunately most network players are not dishonest.
So called meta-incentive (unwillingness to risk hurting bitcoin due to
intended long term ho dling coins or ASICs) can also explain bias towards
honest or altruistic strategies.
Renting too much hashrate is risky as it can avoid the meta-incentive and
increase rational or dishonest strategies.
In particular re differentiating from 51% attack so long as > 50% are
semi-honest, honest or altruistic it won't happen.  It would seem actually
that > 66-75% are because we have not seen selfish mining on the network.
Though I think conveniently slow block publication by some players in the
60% spy mining semi-honest cartel was seen for a while, the claim has been
it was short-lived and due to technical issue.
It would be interesting to try to categorise and estimate the network %
engaging in each strategy.  I think the information is mostly known.
On Dec 11, 2016 03:22, "Daniele Pinna via bitcoin-dev" <

@_date: 2016-02-06 18:01:49
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
Hi Gavin
It would probably be a good idea to have a security considerations
section, also, is there a list of which exchange, library, wallet,
pool, stats server, hardware etc you have tested this change against?
Do you have a rollback plan in the event the hard-fork triggers via
false voting as seemed to be prevalent during XT?  (Or rollback just
as contingency if something unforseen goes wrong).
How do you plan to monitor and manage security through the hard-fork?
On 6 February 2016 at 16:37, Gavin Andresen via bitcoin-dev

@_date: 2016-01-07 20:19:42
@_author: Adam Back 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
You could say 256 bit ECDSA is overkill lets go to 160 equivalently.
Saves even more bytes.
The problem with arguing down is where to stop.
As Matt said these things dont degrade gracefully so a best practice
is to aim for a bit of extra margin.
256-bit is quite common at this point since AES, SHA256 etc even in
things with much less at stake than Bitcoin.
You could send the compressed (unhashed) pubkey then there's no hash
(and omit it from the sig).  Greg had mentioned that in the past.
I think it might be possible to do both (reclaim the hash bits in the
serialisation of the pub key).
On 7 January 2016 at 20:02, Gavin Andresen via bitcoin-dev

@_date: 2016-01-08 16:26:50
@_author: Adam Back 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
Tricky choice. On the one hand I had spotted this too before and maybe
one or two more exceptions to bitcoin's 128-bit security target and
been vaguely tut-tutting about them in the background.  It's kind of a
violation of crypto rule of thumb that you want to balance things and
not have odd weak points as Watson was implying, it puts you closer to
the margin if there is a slip or other problem so you have an
imbalanced crypto format.
On the other hand it's not currently a problem as such and it's less
change and slightly more compact.
RIPEMD probably is less well reviewed than SHA2.  However SHA1 has
problems, and SHA2 is a bigger SHA1 basically so, hence the NIST
motivation for SHA3 designed to fix the design flaw in SHA1 (and SHA2
in principle).
So then if we agree with this rule of thumb (and not doing so would
fly against best practices which we probably shouldnt look to do in
such a security focussed domain) then what this discussion is more
about is when is a good time to write down tech debt.
I think that comes to segregated-witness itself which writes down a
tidily organised by lines of code robust fix to a bunch of long
standing problems.
Doing a 2MB hard-fork in comparison fixes nothing really.  Leaving
known issues to bake in for another N years eventually builds up on
you (not even in security just in software engineering) as another
rule of thumb.  I mean if we dont fix it now that we are making a
change that connects, when will we?
In software projects I ran we always disguised the cost of tech-debt
as non-negotiable baked into our estimates without a line item to
escape the PHB syndrome of haggling for features instead of tech debt
(which is _never_ a good idea:)
Pragmatism vs refactoring as you go.
But for scale I think segregated-witness does offer the intriguing
next step of being able to do 2 of 2, 3 of 3 and N of N which give
size of one sig multisig (indistinguishable even for privacy) as well
as K of N key tree sigs, which are also significantly more compact.
There was also the other thing I mentioned further up the thread that
if we want to take an approach of living with little bit of bloat from
getting back to a universal 128-bit target, there are still some
fixable bloat things going on:
a) sending pubKey in the signature vs recovery (modulo interference
with Schnorr batch verify compatibility*);
b) using the PubKey instead of PKH in the ScriptPubKey, though that
loses the nice property of of not having the key to do DL attacks on
until the signed transaction is broadcast;
c) I think there might be a way to combine hash & PubKey to keep the
delayed PubKey publication property and yet still save the bloat of
having both.
* I did suggest to Pieter that you could let the miner decide to forgo
Schnorr batch verifiability to get compaction from recovery - the pub
key could be optionally elided from the scriptSig serialisation by the
The other thing we could consider is variable sized hashes (& a few
pubkey size choices) that is software complexity however.  We might be
better of focussing on the bigger picture like IBLT/weak-blocks and
bigger wins like MAST, multiSig Schnorr & key tree sigs.
Didnt get time to muse on c) but a nice crypto question for someone :)
Another thing to note is combining has been known to be fragile to bad
interactions or unexpected behaviours.  This paper talks about things
tradeoffs and weaknesses in hash combiners.
Weak concept NACK I think for losing a cleanup opportunity to store it
up for the future when there is a reasonable opportunity to fix it?
On 8 January 2016 at 15:34, Watson Ladd via bitcoin-dev

@_date: 2017-01-04 11:00:55
@_author: Adam Back 
@_subject: [bitcoin-dev] Committed bloom filters for improved wallet 
I think this discussion started from the block bloom filter where
there is a bloom filter commitment in the block which can be
downloaded and is much smaller than the block.  An SPV node based on
that model would download headers and bloom filters, verify the bloom
filter is committed to, and test locally if any addresses managed by
the wallet are in the filter (or false positives for being in it), and
then download blocks with hits.  Apparently there are maybe 50% more
compact alternatives to bloom filters but people have been using bloom
filter as a short-hand for that.  The block bloom filter does seem to
have higher overhead than the query model, but it offers much better
privacy.  I think there was previous discussion about maybe doing
something with portions of blocks so you can know which half or
quarter of the block etc.
On 4 January 2017 at 10:13, Jorge Tim?n via bitcoin-dev

@_date: 2017-01-10 10:04:27
@_author: Adam Back 
@_subject: [bitcoin-dev] BIP - Block75 - Historical and future projections 
See discussion on bitcoin-discuss on this topic last few days.  People
may want to subscribe to that for more free wheeling discussion.
On 10 January 2017 at 04:14, Ryan J Martin via bitcoin-dev

@_date: 2019-01-22 21:22:36
@_author: Dr Adam Back 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Brands credentials use this single show, and multiple show
credentials. It's based on the representation problem which is the
generalisation to multiple bases where Schnorr is one base, Pedersen
Commitments are two bases, Representation problem is n>2 bases.
The method used would work for Schnorr or DSA and there was some 2013
era  discussion on this topic, where if you spend
twice miners can take your money, as a strong way to "discourage"
address reuse.  One side effect though is you force ACID log oriented
storage on the wallet, and many wallets are low power devices or even
a few in VMs that could be snapshotted or rolled back. Similar risk
model to the lightning penalty for accidentally doing a hostile close
in the current model (where ELTOO has non-penalty based close).
You would have to be careful to not use related nonces (k=nonce
committed to by R=kG), as Schnorr and DSA are highly vulnerable to
that, like simultaneous equation two samples solvable.
What the Brands n-show credential looks like is a precommitment like
single show the address becomes A=H(R,Q) where Q is the public key,
and n-show becomes A=H(R1,...,Rn,Q).
Signing becomes providing i,Ri,Q in the Script to satisfy a
ScriptPubKey that includes the three. You would need to in practice
store the Ri values in a merkle tree probably so that you don't need
to provide n inputs, but log(n) or some other structuring.
Anyway main point being the fragility to related nonces, and cost of
ACID log structured storage levels of reliability in wallets.
On Tue, 22 Jan 2019 at 15:14, ZmnSCPxj via bitcoin-dev
