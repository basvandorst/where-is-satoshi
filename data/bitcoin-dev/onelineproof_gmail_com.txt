
@_date: 2015-08-16 10:13:51
@_author: Andrew 
@_subject: [bitcoin-dev] Humans constantly arguing about bsize proves that 
Not necessarily some_averaging_function. Could also be something that
depends on how much work has been put in, i.e. make the miners do more
computational work if they want to add bigger blocks into the chain, and
the chain doesn't have to be the original chain, it could be a sidechain or
block extension, so as to not force people to upgrade.

@_date: 2015-12-09 23:35:07
@_author: Andrew 
@_subject: [bitcoin-dev] Scaling by Partitioning 
Hi Akiva
I sketched out a similar proposal here:
It's good to see people talking about this :). I'm not quite convinced with
segregated witness, as it might mess up some things, but will take a closer
On Dec 9, 2015 7:32 AM, "Loi Luu via bitcoin-dev" <

@_date: 2015-12-19 17:46:02
@_author: Andrew 
@_subject: [bitcoin-dev] On the security of softforks 
(even with old software), he should see that Mallory's signature is not on
a transaction with his address.
Do you mean Mallory creates a regular transaction as well as an
Anyone-can-spend segwit transaction that results in double spending in the
same block?
Sorry not sure what I'm missing...

@_date: 2015-07-17 16:11:17
@_author: Andrew 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
What are you trying to do? Break the ice with a hard fork so that later it
becomes easier to do so, with people more complacent towards it? There are
many solutions to the scaling problem that do not require a hard fork and
are quite simple to implement actually, and don't come with the
complications involved with a hard fork. I'm not a reputable developer on
this list, so my opinion probably doesn't matter much, but I watched and
analyzed this situation closely and I don't like this idea.
On Fri, Jul 17, 2015 at 3:55 PM, Jeff Garzik via bitcoin-dev <

@_date: 2015-06-13 17:55:39
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
First of all, I added more info to bitcointalk.org:
On Sat, Jun 13, 2015 at 2:39 PM, Pieter Wuille Thanks Pieter for your reply. The chain the transaction goes to does not
have to be based on the address (there can be a way for the protocol to
choose), but ya, the address scheme can be a good default. As I said, there
will be an incentive for empty chains to fill up since they will require
less fees (so the scaling benefit isn't dependent on a uniform distribution
of addresses).
The rule I mentioned is that at most 2 different chains can be involved in
one transaction. From a chain to itself is easy. From a parent or
grandparent chain to its child or grandchild chain, is also easy since the
child/grandchild always trusts its parent/grandparent. From a
child/grandchild to parent/grandparent, is also easy (no delay) since the
parent/grandparent will commit to its children (which recursively commit to
their children). As mentioned I am just doing a form of block extensions as
Adam Back described; the chains are not independent. From one chain to
another chain at the same level (sibling chains), the transaction is
recorded on both sibling chains (yes there is some duplication but this is
limited by requiring at most 2 sibling chains participating in a
transaction). They both have to be consistent and this will be ensured by
the miners of their parent chain (those miners will commit to their blocks).
So no, I don't see how it's slower, except that there needs to be some
delay for communication between children/grandchildren and
parents/grandparents, of time O(log n) where n is the number of levels.
Even a small number of levels corresponds to a large transaction volume: n
= 5 corresponds to the equivalent of 625 MB blocks.
Security-wise, it is true that the top level chain will likely have higher
security (more hash power), but at least you can fine tune the fees you pay
according to what level of security is acceptable to you, and as Bitcoin
grows, level 2,3,4 chains can be regarded as almost as secure as the level
1 chain, since there will still be a lot of hash power on them. And anyone
can run a full node on their chains of interest, so there is no SPV level
security here, it is full level security.
Transactions are not significantly different. Miners just have to deal with
child chains, but if there is a scaling benefit, we should not be scared of
complexity. It is probably the simplest way I can think of scaling.
The recipient will validate their own chain fully and will just need the
headers of the relevant parent chains to see whether an output from the
other chain involved in a transaction is really valid. They can also get
the headers of the sibling chain involved in the transaction if they want
to validate the work of the miners on these parent chains. They don't need
the full blocks of the parent and sibling chains involved since not all the
transactions in those blocks may be of interest to them, they just need
proof that any output used in their blocks of interest are valid, so that's
why the header-only SPV proof is sufficient. But yeah, typically a user
will have the full blocks of the parent and grandparent chains of the
chains they are interested in tracking, but it is not always necessary.
Also, in the bitcointalk forum I explain in more detail the mining
procedure and how to limit the extra traffic that may be caused on the
network in case this does get added as a soft fork and then later a new
better scaling method is invented that supercedes this.
But basically, for the mining, I think it should be merge-mined between
parent and direct children only. If all the chains are merge mining the
same root chain, then it would be bad for decentralization, right? But with
only direct parent and children merge mining, you can have smaller miners
on the lower (grand children) level chains and since they will need to
solve another hash problem than their grand parent chains, the grandparents
cannot solve blocks in the grand children chains (only the direct children).
So I still didn't hear a good argument against my proposal. I know Adam
Back's form of extension blocks is problematic because it still has the big
blocks, just at another level of chain, but just by partitioning his one 10
MB chain into 10 pieces, you get my idea, which I think solves the
scalability problem as well.

@_date: 2015-06-15 17:05:05
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Hi All,
I talked with Pieter off-list. And I guess the main opposition is that
coins that are coming from chains that you are not directly validating are
not fully validated by you in the sense that you only get an SPV type proof
to prove that miners have accepted those coins. Yes, it's true, but once
blocks have been mined, there is nothing you can really do about it.
Splitting up the transactions into multiple chains doesn't stop someone
from validating all chains, which results in the same validation workload
as a full node with one chain and big blocks that store the same number of
transactions per second. So there is no disadvantage from using this method
compared with having big blocks, and there are clear advantages. The only
excuse is laziness to create a proper system.
Martin: I'm not sure if random independent chains would be so useful since
there are delays with cross chain transfers and you would not be sure if
those chains will be maintained in the future. My idea is more the idea of
extension blocks, i.e. synchronised chains.
Also, some people think that CPU speed and memory size are the only
limitations to running a full node, and they think that it is ok to just
run a heavily pruned node. Pruned nodes (nodes that have less than 10 years
of transactions on their hard drives) are bad for the network. Reasons why
you would want the long term history of transactions on your hard drive:
1) Your computer could have been compromised when you did the initial
validation, so you may want to validate and see all the old transactions
2) You don't have to spend much time to download transactions that you want
to analyze but have already pruned.
3) Risk of denial of service attack from the "archival" nodes.
4) There is less of an inequality between the big data centers and regular
people. We can analyze the history of the transactions that are relevant to
us just as effectively as the data centers. With the pruning model, it will
be more like NSA-style nodes watching our transaction history, while
regular people can only see "snapshots". Remember how the Bitcoin community
was analysing the old Mt Gox transactions using the blockchain? This kind
of stuff will no longer be possible if most people can only run pruned
5) The data is more distributed thus more easy for others to download
(think torrent downloads vs downloading from a central server).
6) Again being distributed, more eyes will be looking at the long term
data, thus people can more easily investigate scandals and things like that.
7) Without the full history of blocks people cannot really give a proof to
others that what they noticed with their pruned nodes is actually what
happened (if they spot something interesting).
8) The time for a new user to start fresh and sync a full node with a long
term history of transactions is much more accessible (17 days for 100 years
of transactions with 1 MB blocks on high-end computers). Same with the time
needed to perform any kind of analysis on the old transactions. And
remember, any new transactions likely depend on old transactions, so yes
they are very relevant.
This is not paranoia. These are real security risks. So don't tell me that
you are really running a full node with the same level of security when you
are pruning it. Also, don't tell me that the security of running a full
node remains the same when centralization is increased (like with bigger
block sizes). Centralisation is a security risk.
Some people think that decentralisation means you have to run a possibly
noisy desktop in a possible space restricted home, which can be annoying.
No, you don't have to. You can run a full node (or an almost full node on
the chains you are interested in) in a shack in the middle of nowhere and
you can monitor it remotely with cameras or whatever. The point is that it
is easy for a regular person to run one and they can do so without causing
attention and without anyone's permission. That is decentralisation. Even
10 MB blocks are too much to enable this definition of decentralisation
(according to my calculations).
If there are people who choose to run Gavin/Mike's hard fork of Bitcoin
because they are uniformed or mentally challenged or have bad intentions,
then there is not much I can do (I try to inform but I don't have such a
high popularity level to be effective there), but I will surely not accept
any bitcoin that is only valid on blocks with size greater than 1 million
bytes. Such coins will have 0 or even negative value to me, and I expect
others to do the same.
In the meantime, I will start the development process of my proposed
scaling methods using bitcoin-core and possibly the sidechains code from
Blockstream as a base. I don't have much free time, so progress will likely
be slow, but if I believe in something, I will keep working on it. I'm
still seeking more criticism of my proposal, because you know, I don't want
to waste my time if there's something fundamentally wrong with it.
On Sun, Jun 14, 2015 at 6:55 AM, Martin Schwarz

@_date: 2015-06-15 18:00:17
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Pieter: I kind of see your point (but I think you're missing some key
points). You mean just download all the headers and then just verify the
transactions you filter out by using their corresponding merkle trees,
right? But still, I don't think that would scale as well as with the tree
structure I propose. Because, firstly, you don't really need the headers of
the sibling chains. You just need the headers of the parent chains since
the parent verifies all the siblings. All you really need in a typical
(non-mining) situation is the headers or full blocks in one path going down
the tree starting from the root chain. So that means O(log n) needs to be
stored (headers or blocks) (n the number of transaction on the network).
With big blocks, you still need O(n) headers. I know headers are small, but
still they take up space and verification time. Also, since you are storing
the full blocks on the chains you want, you are validating the headers of
those blocks and you are sure that you are seeing all transactions on those
blocks. And if certain addresses must stay on those blocks, you will know
that you are catching all of the transactions corresponding to those
blocks. If you just filter out based on addresses or other criteria, you
can be denied some of those transactions by full nodes, and you may not
know about it. Say for example, your government representative publishes on
of his public addresses that is used for paying for expenses. Then with my
system, you can be sure to catch every transaction being spent from that
address (or UTXO or whatever you want to call it). If you just filter on
any transaction that includes that address, you may not catch all of those
transactions. Same with incoming funds.
There are also advantages for mining decentralization as I have explained
in my previous posts. So still not sure you are right here...

@_date: 2015-06-16 15:23:33
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Let me ask a simpler question. How do you prove the state of the UTXO
database corresponding to your wallet? With my subchain method, all the
addresses in a wallet can be constrained to a path of subchains, so the
proof is O(log n). Yes, I know some people will say that it is not really a
proof because I didn't verify the transactions involving sibling chains
outside my path of chains. But the protocol is "parent chain always decides
in case of conflict". And the parent chains will have an incentive to be
careful with what child blocks they commit to as they will be merge mining
the (direct) child chains. Yes, the parents can make a mistake with some
really deep children chain transactions, but the deeper you go, the less
value the transactions, and the less important. Also, the children of the
parents are parents themselves so they will have incentive to be careful
with what child chains they commit to. So recursively, the system takes
care of itself.
I challenge anyone to come up with a <= O(log n) proof that each address
(output) they have in their wallet really has the balance they think it
has. If someone can do this, then maybe I will drop this idea. Actually,
rusty asked this on  last night and no one was able to
answer it.

@_date: 2015-06-16 18:43:25
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Well doesn't my proposal solve the miner decentralization problem. Only the
direct parent and children chains are merge mined. To be more clear, let
the top chain to have level 1. Each chain that is a child of a chain of
level n has level n+1. For any chain C, a block is accepted if the hash of
its header has an appropriate number of trailing zeros (as usual). It can
also be accepted with special transactions as I will explain. Let C be a
chain of level n. Let C0,C1,....,C9 be its children (each of level n+1).
For any i in {0,1,...,9}, any solution to the mining problem of C can be
inserted as a special transaction inside Ci and this enables the block to
be accepted in Ci (so C and C0,C1,...,C9 are merge mined. But, for any i in
{0,1,...,9} and any j in {0,1,...,9}, any solution to the mining problem of
C cannot be inserted as a special transaction inside of child Cij of Ci. So
that means all of the chains are not merge mined, only localised parts,
By the way, we can eventually get rid of the block size 1 MB limit by
requiring more than just the header to be hashed, but that can be done in
the future as soft fork with sidechains, and is a side topic.

@_date: 2015-06-16 19:04:44
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Actually, I have to think about this merge-mining thing a bit more. I'm
starting to think it's better to do without merge-mining at all. As I
explained in the forum post, the parent will put the hashes of its children
headers as transactions inside its blocks. Thus parents will have an
incentive to validate the children not by merge mining, but by collecting
fees from the children for putting those transactions inside (fees that can
be spent at the children chains). So, ya no merge mining needed for my
proposal. But I will think about it a bit more :)

@_date: 2015-05-07 11:15:57
@_author: Andrew 
@_subject: [Bitcoin-development] Block Size Increase 
I'm mainly just an observer on this. I mostly agree with Pieter. Also, I
think the main reason why people like Gavin and Mike Hearn are trying to
rush this through is because they have some kind of "apps" that depend on
zero conf instant transactions, so this would of course require more
traffic on the blockchain. I think people like Gavin or Mike should state
clearly what kind of (rigorous) system for instant transactions is
satisfactory for use in their applications. Be it lightning or something
similar, what is good enough? And no zero conf is not a real secure system.
Then once we know what is good enough for them (and everyone else), we can
implement it as a soft fork into the protocol, and it's a win win situation
for both sides (we can also benefit from all the new users people like Mike
are trying bring in).

@_date: 2015-05-08 17:17:49
@_author: Andrew 
@_subject: [Bitcoin-development] Block Size Increase 
These are good points and got me thinking (but I think you're wrong). If we
really want each of the 10 billion people soon using bitcoin once per
month, that will require 500MB blocks. That's about 2 TB per month. And if
you relay it to 4 peers, it's 10 TB per month. Which I suppose is doable
for a home desktop, so you can just run a pruned full node with all
transactions from the past month. But how do you sync all those
transactions if you've never done this before or it's been a while since
you did? I think it currently takes at least 3 hours to fully sync 30 GB of
transactions. So 2 TB will take 8 days, then you take a bit more time to
sync the days that passed while you were syncing. So that's doable, but at
a certain point, like 10 TB per month (still only 5 transactions per month
per person), you will need 41 days to sync that month, so you will never
catch up. So I think in order to keep the very important property of anyone
being able to start clean and verify the thing, then we need to think of
bitcoin as a system that does transactions for a large number of users at
once in one transaction, and not a system where each person will make a
~monthly transaction on. We need to therefore rely on sidechains,
treechains, lightning channels, etc...
I'm not a bitcoin wizard and this is just my second post on this mailing
list, so I may be missing something. So please someone, correct me if I'm

@_date: 2015-05-09 12:02:48
@_author: Andrew 
@_subject: [Bitcoin-development] Block Size Increase 
The nice thing about 1 MB is that you can store ALL bitcoin transactions
relevant to your lifetime (~100 years) on one 5 TB hard drive
(1*6*24*365*100=5256000). Any regular person can run a full node and store
this 5 TB hard drive easily at their home. With 10 MB blocks you need a 50
TB drive just for your bitcoin transactions! This is not doable for most
regular people due to space and monetary constraints. Being able to review
all transactions relevant to your lifetime is one of the key important
properties of Bitcoin. How else can people audit the financial transactions
of companies and governments that are using the Bitcoin blockchain? How
else can we achieve this level of transparency that is essential to keeping
corrupt governments/companies in check? How else can we keep track of our
own personal transactions without relying on others to keep track of them
for us? As time passes, storage technology may increase, but so may human
life expectancy. So yes, in this sense, 1 MB just may be the magic number.
Assuming that we have a perfectly functional off-chain transaction system,
what do we actually gain by going from 1 MB to 1000 MB (my approximate
limit for regular users having enough processing power)? If there is no
clear and substantial gain, then it is foolish to venture into this
territory, i.e. KEEP IT AT 1 MB! For example Angel said he wants to see
computers transacting with computers at super speeds. Why do you need to do
this on the main chain? You will lose all the transparency of the current
system, an essential feature.

@_date: 2015-05-09 18:33:32
@_author: Andrew 
@_subject: [Bitcoin-development] Block Size Increase 
On Sat, May 9, 2015 at 12:53 PM, Justus Ranvier I would expect at least 10 billion people (directly or indirectly) to be
using it at once for at least 100 years. But I think it's pointless to
guess how many will use it, but rather make the system ready for 10 billion
people. The point is that for small transactions, they will be done
off-chain. The actual Bitcoin blockchain will only show very large
transactions (such as a military purchasing a new space shuttle) or
aggregate transactions (i.e. a transaction consisting of multiple smaller
transactions done off-chain). There can also be multiple layers of chains
creating a tree-like structure. Each chain above will validate the
aggregate transactions of the chain below. You can think of the Bitcoin
blockchain as the "hypervisor" that manages all the other chains. While
your coffee purchase 4 days ago may not be directly visible within the
Bitcoin blockchain (the main chain), you can trace it down the sequence of
chains until you find it. Same with that fancy dinner your government MP
paid for using public funds. You don't have to store a copy of all
transactions that occurred for each chain in existence, but rather just the
transactions for the chains that you use or are relevant to you.
As you see, this kind of system is totally transparent to all users and
totally flexible (you can choose your sub chains). The flexibility also
allows you to have arbitrarily fast transactions (choose a chain or
lightning channel attached to that chain that supports it), and you can
enjoy a wide variety of features from other chains, like using one chain
that is known to have good anonymity properties.

@_date: 2015-05-20 02:55:49
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
I briefly mentioned something about this on the bitcoin-dev IRC room. In
general, it seems experts (like sipa i.e. Pieter) are against using
sidechains as a way of scaling. As I only have a high level understanding
of the Bitcoin protocol, I cannot be sure if what I want to do is actually
defined as a side chain, but let me just propose it, and please let me know
whether it can work, and if not why not (I'm not scared of digging into
more technical resources in order to fully understand). I do have a good
academic/practical background for Bitcoin, and I'm ready to contribute code
if needed (one of my contributions includes a paper wallet creator written
in C).
The main problem I see with increasing the block size limit is that it
increases the amount of storage required to fully validate all transactions
for say 100 years (a person's life). With 1 MB blocks, you can store all
lifetime transactions on a 5 TB drive, which basically any regular user can
do. With 10 MB blocks, you need a 50 TB drive, not accessible for regular
users. Yes, it's possible that in the future hard drive technology will get
cheaper and smaller, but this still didn't happen, so we can't just say "it
should be doable at the rate of Moore's law etc...", we need to know that
it is accessible for everyone, now. Also, don't forget that human life
expectancy can increase with time as well. I know, it sounds silly to use a
human lifetime as a measurement of how far back each user should be able to
store transactions for, but what is a better measurement? This is a
technology made for people i.e. humans, right, and the important part is
that it is for regular people and not just well privileged people. You can
search my last four emails for some more calculations.
What sipa told me on the IRC channel is that Bitcoin Core does not care
about old transactions. It only looks at the current blocks. Yes, that
makes sense, but how do you know that your machine wasn't compromised when
validating the previous blocks? And what if you want to check some old
transactions (assuming you didn't index everything)? What if some of your
old transaction data was lost or corrupted? I think it is clear that it is
useful to be able to validate all blocks (since 100 years) rather than just
a pruned part. It empowers people to have as much information about Bitcoin
transactions as do large data centers; transactions that may include
government or corporate corruption. This is the key to how Bitcoin enables
transparency for those who should be transparent (individual users with
private addresses can still remain anonymous). Also, 5 TB takes about 20
days to sync starting fresh, on a regular computer, so it allows easy entry
into the system.
So assuming we agree that people should be able to store ~ a lifetime of
transactions, then we need 1 MB blocks. But of course, this leads to huge
transaction costs, and small purchases will be out of limits. So to fix
this, I propose adding a 10 1 MB chains below the main chain (sorry on the
IRC room I said 10 10 MB chains by mistake), so effectively, you have a new
10 MB chain that is partitioned into 10 parts. You can also add a third
level: 100 1 MB chains, and keep going like that. The idea is that when you
make a large transaction, you put it through the top chain; when you make a
medium sized transaction, you put it through one of the middle chains,
which will be verified (mined) by the middle chain, and the top chain will
verify the aggregate transactions of the middle chain. If you have a small
sized transaction, you put it through one of the bottom chains, the bottom
chain verifies it, the middle chain verifies the aggregate transactions of
the bottom chain, and the top chain verifies the aggregate transactions of
the middle chain. By aggregate transaction, I mean the net result of
multiple transactions, and I suppose it can be 20 transactions belonging
only to one "sibling" chain for level 2, or 200 transactions for level 3,
Now, how does the system decide to which of the 10 chains the middle sized
transaction goes to? I propose just taking some simple function of the
input addresses mod 10, so that you can just keep randomly generating a
wallet until you get one with only addresses that map to only one of the 10
chains (even distribution), so that someone can choose one of the 10
chains, and store only the transactions that belong to that chain. They
should also choose a chain from level 3, etc... So in effect, they will be
storing a chain with block size O(n) where n is the number of levels. They
may store multiple sibling chains at one level, if they want to track of
other people's transactions, such as those of their government MP, or
perhaps, they want to have a separate identity that would be more anonymous
with a separate series of sibling chains. This will increase the storage
size, but the increase will be proportional to the number of things you
want to keep track of (at least this kind of system gives you the ability
to fine tune your storage needs to the level of "things" you want to keep
track of). Also, note that there may likely be duplication of transactions,
since transactions can include addresses that are associated with different
silbling chains, but this effect shouldn't make a big difference, and again
will depend on the complexity of the transactions you want to keep track of.
So how can this work? I propose that we keep the current chain as the top
chain, and then create 10 level 2 chains that also store Bitcoin and the
Bitcoin can be transferred between chains (I think this is the idea of
sidechains). How can we incentivize people to keep mining on the level 1
chain? Perhaps force it into the (soft fork) protocol that anyone mining on
level 2, has to also mine on level 1, and in general, anyone mining on
level n+1 has to also mine on levels n,n-1,...,1. Also, level 1 will have
the best decentralization, so there should be enough people paying fees to
get transactions there for their large transactions that require a high
level of security and trust. Even if people stop using level 1, any bitcoin
you own in the level 1 chain, can be transferred to level 2, and still you
have 1 MB blocks due to the partitioning scheme. How to prevent
transactions from clustering on one or a  few sibling chains in a
particular level? Well the more empty chains should have lower fees, so
that should incentivize people... Note: This system also allows for the
fine tuning of the transaction size to security ratio. Yes the lower chains
will be less secure, but a lower sized transaction does not need as much
For instant transactions, there can also be Lightning channels linked to
whatever level of chain you want.
OK so it seems to me that this can work and would only require a soft fork.
But, as I said, I only have a high level understanding of the Bitcoin
protocol, so it feels kind of too good to be true, and I am ready for heavy
criticism. I am only writing this because I care about Bitcoin and I want
it to remain decentralized and become the best that it can be. I think it
is important to do the right thing rather than (as most people naturally
do) the most convenient thing.

@_date: 2015-05-28 02:16:36
@_author: Andrew 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
Hi All
I discussed this idea with some other core developers (on IRC) and they
generally seem to agree that it can be done.
It may be equivalent to an idea called "blockchain extensions" but when I
looked it up on bitcointalk.org I didn't see exactly the same proposal I am
One person suggested I should replace the address to chain function with a
protocol addition that allows one to specify the target chain. Yes, this
can also be done without changing the key properties.
One person said that the main problem is that I am not saying anything
specific, and I should address the sidechain problems written about in the
sidechains paper. Well, actually, there is one quite specific thing I am
saying, in case you didn't notice: With this system, the network can
achieve effectively 5^{n-1} MB blocks with each participant only storing n
MB blocks. So for example, you can have effectively a block size of 625 MB
(= 5^4) with each participant only storing 3 MB blocks; or 3.125 GB blocks
with each participant only storing 4 MB blocks. For these calculations, I
am assuming that only two separate sibling chains are involved in a
transaction, so there is a duplication effect that divides in two the
effective size of a given level of blocks (that's why it's 5 instead of 10
as would be without duplication). If you want to involve multiple sibling
chains in one transaction, you can effectively achieve this by performing
multiple transactions involving 2 of the multiple chains. Yes, the fees
would be higher since you have more transactions to make, but it is
reasonable to expect more fees for more complicated transactions, and I
don't think it will result in people clustering on one chain (people who do
these kinds of transactions would probably track multiple chain paths). As
for the problems with sidechains, I think they would be eliminated due to
the child-parent dependence I specified. I also propose the following
additional rule: In case of conflict between parent and child chains (due
to reorganizations), the child chain must choose the consensus of the
parent chain. Also, for transferring from child to parent, the miners on
the parent have the final say, but to make it more clear, they can use the
relative difference of difficulty between their chain and the child chain
to decide how many blocks deep a transaction in the child chain has to be
to be accepted in the parent chain.
Gavin was the only one who disapproved of this, but I am not sure if he
actually read the whole thing that I wrote. He said something along the
line of "the outputs will span the subchains" and when I asked for an
explanation he just said that I need to learn more about things. I stated
to him my willingness to learn, but have yet to get a response from him.
Mike: You should also keep in mind the big picture when it comes to
decentralization. If the hard drives (or tapes) can only be produced by a
small number of large companies like Western Digital or Seagate, then you
can't really count those for a decentralized system. A truly decentralized
system would have the devices needed to participate in (and verify) the
system be easily created by a regular user of the system without relying on
a central power. So for example, the hard drives needed to store the
bitcoin transaction records should be able to be produced at a regular
person's home on a 3D printer starting from just the raw materials. I don't
know how close we are to this ideal, but just pointing out that it needs to
be considered. This is also a reason why I like that Bitcoin uses the
simple SHA sum for mining instead of a more complicated function such as
scrypt. It makes it easier for small scale entities to understand and to
produce the ASIC miners. Also, in addition to the centralization of storage
device manufacturing, one should also consider what would happen if
everyone wanted to have a 5 TB drive at home. What would happen to the
price of hard drives? Keep in mind also that the human population is likely
increasing, so there are less real resources per person... Yes maybe in the
future we can solve these problems, but we still haven't, so let's not
assume they are solved. Also, you mentioned sharing the costs of a hard
drive with other people. Do you mean trusting that others did not
compromise the hard drives? If you want you can do so, but not every
participant should be forced to trust others, a point I think I made
already. And finally, this is all a discussion on the costs of running a
Bitcoin node. Bitcoin is not all that people will use hard drives and
computers for; we need to leave room for other things.
So Mike, I have a question for you. Are you supporting a block size
increase partly due to philosophical reasons (i.e. you believe that regular
people shouldn't have such strong freedom as I want) or do you just not
care so much about the long term future and you just want to get your
Bitcoin related projects up and running with minimal complications? Or is
it a combination of both things? You should disclose this to the people
following your words because they trust you as an experienced professional
with a good reputation, and it would be dishonest to not disclose this to
them. (same goes for Gavin)
Overall, I think this system is the only system that I heard of that can
scale decentralization without a block size increase. Lightning by itself,
for example, requires a block size increase that depends on how many such
Lightning contracts are being made, so relies on people changing the
protocol, which is obviously less secure and robust than a fixed protocol.
But I am not ruling out any other possibilities, so other things should
also be considered. But eventually, we may have to decide how to scale
without knowing for sure whether the chosen scaling method is the ultimate
scaling method. And I think this is a good candidate for that, and also,
can be reversed later on without changing the original protocol before the
softfork. Actually, we can just make nodes advertise whether they support
the soft fork or not, and if a better scaling protocol comes along, those
nodes can switch to advertise the better one. So it is quite a harmless
soft fork to make, in my opinion.

@_date: 2015-05-30 03:27:36
@_author: Andrew 
@_subject: [Bitcoin-development] soft-fork block size increase (extension 
Hello Adam
First of all, thank you for inventing hashcash, which is basically what
bitcoin is!
Some people have said that my proposal, subject line "Scaling Bitcoin with
Subchains" is essentially the idea of blockchain extensions. Though, I
think there is quite a difference between what I propose and what you
propose. You want to add one optional 10 MB blockchain that synchronizes
with the 1 MB blockchain, while I want to add ten 1 MB
 blockchains that each synchronize with the 1 MB chain (and you can
continue like that). I think, as long as we want to keep using blockchains
for our cryptocurrency, we will need a tree structure of blockchains in
order to scale for an arbitrary number of transactions. With just one 10 MB
blockchain, someone who wants to do the lower valued transactions will need
to validate all 10 MB, while with ten 1 MB chains, they can choose just the
chain or chains that are of interest to them. With a tree structure you get
O(a^(n-1)) MB of transactions in the network while each participant only
has to validate O(n) MB of transactions (a is just the number of children
chains per parent divided by 2, so 5 in the case of 10 children as I
described). With just one child chain, you don't get this scaling, and it
is pretty much equivalent to increasing the blocksize, though with a
soft-fork instead of a hard-fork.
I think the actual way that the blockchains interact can be still worked
out (Recently I was thinking of maybe creating a contract system or even a
decentralized market between chains). But still, everyone should agree that
you need this kind of tree structure. Even if you want to only run a pruned
node, the CPU usage and memory scales just as bad. The tree structure also
has good privacy and miner decentralization properties, as I can write
about later.
But another thing that I recommend is an "exit plan". What if we go with
some kind of soft fork and then in the future some better idea comes along?
Then we should have a way to reverse the soft fork. If people already have
coins tied up in sidechains, it can be problematic. So perhaps, in case
people want to later ditch the soft fork, nodes in the parent chain can
allow only old transactions inside the child chains to be accepted back up,
while new transactions are not recognized anymore. That way you can limit
the amount of useless transaction traffic that results in case we want
something else.

@_date: 2016-12-22 18:29:07
@_author: Andrew 
@_subject: [bitcoin-dev] Multisig with hashes instead of pubkeys 
Is there a worked out scriptPubKey for doing multisig with just hashes
of the participants? I think it is doable and it is more secure to a
compromised ECDSA. I'm thinking something like this for the
 2 OP_SWAP OP_SWAP OP_SWAP OP_DUP OP_HASH160 OP_EQUALVERIFY OP_DUP OP_HASH160  OP_EQUALVERIFY OP_DUP
OP_HASH160  OP_EQUALVERIFY 3 OP_CHECKMULTISIG
and  for the scriptSig
Can anyone confirm or send me a link to the worked out script?

@_date: 2016-10-27 15:38:17
@_author: Andrew 
@_subject: [bitcoin-dev] The Soft Fork Deception 
I have been reading recently through the history of soft forks provided by
Bitcoin Core:
It has led me to think that there is a deceiving notion that soft forks do
not force Bitcoin users to upgrade software. Yes, it's true that the past
soft forks still allow old nodes to accept blocks under the tighter rules
as valid, but what about miners who are still using old software? What
about users who want to make a transaction using the old rules? Those
people are no longer able to do those things. And if they want to do those
things, a hard fork will result.
Remember what happened when BIP 66 was activated? Luckily, it was short
lived, but this is just the beginning. If you keep tightening the rules,
you are building up more and more pressure for a split in the network to
occur. You can call this split a "hard fork" or just a "fork", but it is
dangerous either way, and it leads to basically the creation of two coins
when before we just had one, people instantly lose value, and the trust in
Bitcoin's store of value dies.
Obviously every one can debate about what should be the definition of a
soft fork, but whatever that is, I think it is unacceptable how sloppily
the past soft forks have been deployed. I can think of many ways in which
we could have these new features that the soft forks provided, but without
forcing the new rules, and simply making them features that can be used on
an individual miner or transaction signer basis. Is there a document from
Bitcoin Core that outlines the philosophy of soft forks and why it is
acceptable to force the tightening of rules and cause such risks? And
please give me another reason other than "it removes a few if statements
from the code".
Now that Segregated witness is scheduled to be deployed on November 15, we
should take a look at this "soft fork" as well. I like the idea of
Segregated Witness, but from conversations on Reddit and IRC, I see people
saying that this soft fork will be like the others: requiring a hard fork
in order to revert it. Is this true? I am getting conflicting messages by
reading the BIP. It says that if all transactions are non-segwit, then a
node will validate the block as before. But if we pass the threshhold
(usually 95 % for 1000 blocks) will miners mining non-segwit blocks be
ignored? This is not good... I really think we should make it optional.
Miners will have an incentive to mine segwit blocks, since it allows for
more transactions per block, so why force them? What if we want to slightly
modify the Segwit protocol in the future? What if we want to replace segwit
with something much different? We will be forced to do a hard fork in order
to do that.
Now, we can't go back in time and fix the deployment of the soft forks, but
I do propose one clean way to fix things: Remove all the previously "soft
forked" rules for non segwit transactions, and require them only for segwit
transactions. But make segwit optional! In addition to what I talked about
above, this may also relieve some tensions of people who are not
comfortable with segwit and are thinking of joining a hard fork like the
Bitcoin Unlimited project.
Unless people can give me a good explanation as to why we are deploying
soft forks in such forceful manner, or Bitcoin Core accepts my proposal,
then I will have no choice but to create a new client (I'm thinking to call
it Bitcoin Authentic), that will be just as Bitcoin Core but will always
follow the chain with the most work regardless of whether soft fork rules
are respected, and I would put at least CHECKLOCKTIMEVERIFY as mandatory
within segwit transactions.

@_date: 2017-01-28 18:34:58
@_author: Andrew 
@_subject: [bitcoin-dev] Transaction Replacement by Fee 
Hi, recently been trying to get RBF working on a multisig input. I set
the nSequence to 0, but script didn't verify (used python-bitcoinlib).
Should it work for this type of transaction? I am using the
SignatureHash(...) method of signing, not rpc.signrawtransaction(...).
On Thu, Jan 12, 2017 at 7:58 PM, Peter Todd via bitcoin-dev

@_date: 2018-09-01 00:11:19
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
As I understand, selfish mining is an attack where miners collude to
mine at a lower hashrate then with all miners working independently.
What are the current strategies used to prevent this and what are the
future plans?
One idea I have is to let the block reward get "modulated" according
to peak hashrate. Say p is the peak hashrate for 365 periods (1 year)
consisting of 144 blocks, h is the hashrate of the last 144 block (1
day) period, and r is the base subsidy (12.5 BTC currently). You can
then make the max block reward 0.5 r (1 + h/p). So if hashrate is at
peak you get the full reward. Otherwise you get less, down to a min of
0.5 r.
If miners were to collude to mine at a lower than peak hashrate, then
they may be able to do it profitably for 144 blocks, but after that,
the reward would get modulated and it wouldn't be so much in their
interest to continue mining at the lower hashrate.
What flaws are there with this? I know it could be controversial due
to easier mining present for early miners, so maybe it would have to
be done in combination with a new more dynamic difficulty adjustment
algorithm. But I don't see how hashrate can continue rising
indefinitely, so a solution should be made for selfish mining.
Also when subsidies stop and a fee market is needed, I guess a portion
of the fees can be withheld for later if hashrate is not at peak.

@_date: 2018-09-13 23:19:37
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
I discussed this more at bitcointalk:
The attacks I'm interested in preventing are not only selfish mining
and collusion, but also more subtle attacks like block withholding,
and in general anything that aims to drive out the competition in
order to increase hashrate fraction. I also scrapped the idea of
changing the block subsidies, and I am only focuses on fees.
You can read more about the motivation and details in the bitcointalk
thread, but my proposal in short would be to add the concept of
"reserve fees". When a user makes a transaction, for each txout
script, they can add parameters that specify the fraction of the total
fee that is held in "reserve" and the time it is held in "reserve"
(can set a limit of 2016 blocks). This "reserve" part of the fee will
be paid to miners if the hashrate rises. So if hashrate is currently h
and peak hashrate (from past year) is p, then for each period (1 day),
a new hashrate is calculated h1, and if h1 > h, then the fraction
(h1-h)/p from the reserve fees created in the past 2016 blocks will be
released to miners for that period (spread out over the 144 blocks in
that period). And this will keep happening as long as hashrate keeps
rising, until the "contract" expires, and the leftover part can be
used by the owner of the unspent output, but it can only be used for
paying fees, not as inputs for future transactions (to save on block
This should incentivize miners to not drive out the competition, since
if they do, there will be less of these reserve fees given to miners.
Yes in the end the miners will get all the fees, but with rising
hashrate they get an unconditional subsidy that does not require
transactions, thus more space for transactions with fees.
I can make a formal BIP and pull request, but I need to know if there
is interest in this. Now fees don't play such a large part of the
block reward, but they will get more important, and this change
wouldn't force anything (would be voluntary by each user), just miners
have to agree to it with a soft fork (so they don't spend from the
anyone-can-spend outputs used for reserve fees). Resource requirements
for validation are quite small I believe.

@_date: 2018-09-14 17:30:02
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
(reposting to whole list instead of just him)  Agent:
Interesting proposal though it introduces some elements
of proof of stake so it would be more controversial in my view. Also,
something needs to be explained about how this would not create an
attack where difficulty is frequently dropping by 25%, and suddenly we
find ourselves with a very low difficulty and PoW attacks can easily
happen. I need to analyse your proposal more, but I prefer to discuss
it on your blog instead of here just to limit the side topics and
focus only on my proposal.
No one has yet given me a good reason for why not to support my proposal...

@_date: 2018-09-15 16:01:19
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
Agent: No problem. I did ask in the first post what the current
plans are for selfish miner prevention. So if anyone has any other
relevant ideas (not just for selfish mining but for making mining more
decentralized and competetive), then please post it, but I just prefer
to focus on my proposal more than others.
 I think you are concerned that this will incentivize more
global resource consumption and will be detrimental to our
environment? Personally, I believe centralization of energy does more
harm to the environment rather than total energy consumption. If
Bitcoin helps "power" to become more decentralized, then I wouldn't be
surprised if total (global) energy consumption actually decreases. The
debt based economy is forcing us to continuously grow and use up more
resources, and collectivism is turning individuals into
super-organisms capable of doing much more harm to the environment
than can be done by one or a just a few individuals working
independently. In my proposal, I actually allow for changing
environmental conditions by measuring only the peak hashrate of the
past year, and not the full history of blocks.

@_date: 2018-09-17 13:18:53
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
In general, a target hashrate can be set by users (can be less or more
than the peak hashrate). As long as hashrate is rising and still
didn't reach the target, miners will incrementally get the reserve
fees. Once the "contract" times out, the remaining part can be used as
fees by the users who created the reserve fee "contract". So if
hashrate remains the same or falls, then users get the reserve fees
I agree that we can't stop people from being greedy. If they are not
Bitcoin mining, they will try to put their energy to earn in some
other way...The hashrate is related the demand for Bitcoin (price) and
the amount of fees/subsidies the miners will get paid. For every level
of mining rewards (based on demand) there exists a limit on the
hashrate. Once hashrate gets large enough, no new miners (additional
hashrate) will want to join since their share of the hashrate is too
small to make a profit.
Also with merge mining and proof of space we can be quite efficient in
the future. But of course I sympathize with the "don't be greedy"
philosophy, and it can be good to educate people to use less resources
than they need, just I think it's a bit outside of the scope of what
the Bitcoin software protocol does.

@_date: 2018-09-18 20:26:16
@_author: Andrew 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
@ Eric: Yes I forgot to mention that cost (in addition to price) also
determines the profitability of mining and thus the total hashpower. I
disagree with your assessment of merge mining as really what matters
is opportunity cost of honestly mining vs attacking, and one reason we
are currently safe from other networks attacking is that SHA256 is
ASIC friendly and currently the main network utilising this (the
ASICs) is Bitcoin mining. It would be hard for people computing prime
numbers to quickly switch to Bitcoin mining, since they would need the
ASICs. But if you really want to discuss this then I suggest opening a
new thread here or bitcointalk since this is off-topic from my thread.
Also there is a discussion about merge mining here:
On Mon, Sep 17, 2018 at 2:09 PM, Zawy via bitcoin-dev
 Are you talking about my proposal to modulate the subsidies?
Because if you read my second post you see that I scrapped that part
as it can be disruptive, and I am only proposing to let users have
more control over how their fees are spent. A certain portion of fees
is put in reserve and gets allocated to miners based on hashrate
conditions, and once the "contract" expires, the remaining part goes
back to the user for future fee payments. I understand it is unclear
whether this will cause a significant benefit (I can work on
simulations if I have time), but what could possibly go wrong with
giving users more choice over how their fees are spent?
Also if you see my post, I am not just trying to prevent Selfish
Mining (33%) or 51% attacks, but in general any types of attacks that
try to drive away mining competition (like block withholding attacks,
networking attacks, etc).
Someone on bitcointalk was also worried about a positive feedback
loop, and I think my answer remains valid:
"First, I think a price drop will be slightly offset by the lower rate
of coins being mined. Also, confirmation times will get longer: Both
the time to get a block will increase and the number of confirmations
needed to have enough work done on the chain so that your transaction
is considered safe. The fees would likely rise and this would increase
rewards to miners, especially in a fee-market dominated future." Merge
mining can also help to smooth hashrate so it doesn't depend so much
on price, but even without merge mining it is not so clear that a
there would be a destructive feedback loop and that's where
simulations / math equations would help.
Your idea of increasing difficulty, I haven't thought about much, but
I don't think it's the same effect. When you increase the difficulty,
the reward per block remains the same, only reward per real time
falls, but it could also have the negative effect of incentivizing
selfish mining or timestamp attacks to reverse the increased
difficulty. Though actually timestamp attacks would sort of be
dis-incentivized if underestimates of hashrate led to lower rewards.
Obviously I will not work on a pull request if there is no strong
interest for this. I think it is a harmless addition, so if I have
time I can work on simulations to try to prove that there is a
significant benefit with doing this.
