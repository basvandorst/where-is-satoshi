
@_date: 2017-04-27 17:05:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Trustless Segwit activation bounty protocol (aka. 
Good morning all,
As other explained, your scheme is broken.
Unless we have a softfork first (OP_CHECKBIP9VERIFY: payment is valid only if a BIP9 proposal is active), it is not possible to create a softfork bounty in a decentralised way
On the other hand, hardfork bounty is very simple. You just need to make sure your tx violates existing rules
Perhaps, it's possible to invert the logic.
When considering a softfork success/fail, the difference is this:
There exists some tx, where if the softfork fails, the tx is valid, and if the softfork succeeds, the tx is invalid.
So, an economic agent who wishes to push for a softork, can instead do:
1. Select block heights H1 and H2, where H1 < H2.
2. Create a valid Funding tx (valid in both softfork-pass and softfork-fail) with a single output, encumbered by the contract (CLTV H1 AND k1) OR (CLTV H2 AND k2), and transmit and put in block.
3. Create a Softfork Failure Refund tx. This tx has to be invalid if the softfork succeeds, but valid if the softfork fails. It provides k1, and is spendable on height H1. It outputs back to the economic agent.
4. Create a Softfork Success Payout tx. This tx has to be valid if the softfork fails. It outputs 0 to the economic agent, allowing any miner who includes it to get the payout as tx fee.
If at block H1, softfork has passed, then the Softfork Failure Refund tx is invalid and cannot be used by the economic agent to spend the output. The miners can then wait for block H2 to include Softfork Success Payout tx in a block and claim the tx fee. The risk here for the miners is that since k2 is generated by the economic agent, the economic agent can create a different tx spending that output before H2.
If at block H1, softfork has failed, then the Softfork Failure Refund is valid and the economic agent can get the Funding tx's output back to a normal output. The risk here for the economic agent is that miners can form a cartel to informally ignore the Softfork Failure Refund until block H2.

@_date: 2017-12-05 23:49:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Two Drivechain BIPs 
Good morning Paul and Chris,
I actually devised a way to work around this collective action problem, and discussed it obliquely in a private e-mail with Chris, while I was preparing my article on sidechain weaknesses.  I removed it before publication of the sidechain weaknesses article, but perhaps I should not have.
Collective action can be ensured by contract.  In a world where some system can enforce certain actions programmatically, it is possible to ensure collective action via a program, i.e. a "smart contract".
The thief pays out to the destination address that is a P2SH of the below script:
  OP_HASH160  OP_EQUALVERIFY
  OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIG
   OP_CHECKLOCKTIMEVERIFY OP_DROP
  OP_TRUE
If the thief does not publish the preimage of the hash within 1 week of the withdrawal time, then it becomes possible for anyone to spend the above script; basically, some lucky miner who wins the first block past the specified time will get the entire winnings.  Let us call the above script, the Theft Contract.
The thief then recruits accomplices to the theft.  Note that the attack can be prepared and initiated before the accomplices are even recruited.
The thief locks some coins (the "cut" the accomplice gets), to the below script, for each accomplice it tries to entice:
  OP_HASH160  OP_EQUALVERIFY
  OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIG
   OP_CHECKLOCKTIMEVERIFY OP_DROP
  OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIG
Let us call the above script, the Accomplice Contract.  If the accomplice accepts, he or she then starts to vote for the invalid withdrawal.
If the invalid withdrawal succeeds, the thief acquires the entire theft price from the Theft Contract by publishing the preimage to the .  (If he or she does not, then, some randomly-selected miner will acquire the money after the timeout, so the thief needs to publish the hash, before the timeout in the Theft Contract).
This publishes the preimage on the blockchain.  Each accomplice can then acquire their "cut" of the theft by copying the preimage and claiming from the Accomplice Contract.
If the theft never succeeds, then there is no reason for the thief to ever publish the preimage, and after the timeout on the Accomplice Contract, the thief can recover his or her offered funds at no loss (minus transaction fees),  This incentivizes accomplices to actually cooperate with the thief, as they will not get paid if the theft does not push through.
All that is necessary is for a single "mastermind" thief to begin this process.  Accomplices can be recruited later, with the "cut" they get negotiated according to how much hashpower they can bring to bear on theft.
Newly-created miners and mining pools can be enticed at the time they arise by offering an Accomplice Contract to them.  Thus, newly-created miners and mining pools can be brought into cooperation with the thief as soon as they make a presence on the blockchain.
Even if some mining pool makes a public statement that they will not assist in the theft, the thief may still commit an Accomplice Contract for them on-chain anyway, and publicize it, in order to put the integrity of that mining pool in question and drive out support from that mining pool.  True accomplices may pretend to initially be honest and then signal dishonestly later, in order to make it more plausible that a pool that "committed" to not support the theft is not trustable since they have an Accomplice Contract that will compensate them if they support the theft, creating further confusion and discord among honest miners.  The thief may also use the existence of such an Accomplice Contract while negotiating with more minor miners and mining pools, in order to entice those also to join, and thus gain additional buffer against the stochastic process of miner voting.
With the Theft Contract and the Accomplice Contract, negotiation can be done in parallel with the theft attempt, reducing the cost of organizing collective action, as we have all hoped "smart contracts" would do.
While it is true, that this requires that the thief have significant funds in reserve prior to theft (in order to fund the Accomplice Contracts he or she will have to offer to potential accomplices), we have always been assured that theft can be initiated by miners only, and that miners already have a significant amount of money they control.  So it will be no problem for a potential thief to reserve some funds for paying to Accomplice Contracts.
This vulnerability can be fixed if withdrawals are restricted to simple P2PKH or P2WPKH only, but in the presence of Scriptless Script and Bellare-Neven signatures, that may be sufficient to create the Theft Contract and the Accomplice Contract (but I know too little of Scriptless Script to be sure).

@_date: 2017-12-06 00:46:45
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Good morning Damian,
The primary problem in your proposal, as I understand it, is that the "transaction pool" is never committed to and is not part of consensus currently.  It is unlikely that the transaction pool will ever be part of consensus, as putting the transaction pool into consensus is effectively lifting the block size to include the entire transaction pool into each block.  The issue is that the transaction pool is transient and future fullnodes cannot find the contents of the transaction pool at the time blocks were made and cannot verify the correctness of historical blocks.  Also, fullnodes using -blocksonly mode have no transaction pool and cannot verify incoming blocks for these new rules.
Applying a patch that follows this policy into Bitcoin Core without enforcing it in all fullnodes will simply lead to miners removing the patch in software they run, making it an exercise in futility to write, review, and test this code in the first place.
In addition, you reuse the term "weight" for something different than its current use.  Current use, is that the "weight" of a transaction, is the computed weight from the SegWit weight equation, measured in virtual units called "sipa", using the equation (4sipa / non-witness byte + 1sipa/witness byte).
Sent with [ProtonMail]( Secure Email.

@_date: 2017-12-07 01:46:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Good morning Damian,
Each long-running node would have a view that is roughly the same as the view of every other long-running node.
However, suppose a node, Sleeping Beauty, was temporarily stopped for a day (for various reasons) then is started again.  That node cannot verify what the "consensus" transaction pool was during the time it was stopped -- it has no view of that.  It can only trust that the longest chain is valid -- but that means it is SPV for this particular rule.
It would not. Suppose Sleeping Beauty slept at block height 500,000.  On awakening, some node provides some purported block at height 500,001.  This block indicates some "next blocksize" for the block at height 500,002.  How does Sleeping Beauty know that the transaction pool at block 500,001 was of the correct size to provide the given "next blocksize"?  The only way, would be to look if there is some other chain with greater height that includes or does not include that block: that is, SPV confirmation.
How does Sleeping Beauty know it has caught up, and that its transaction pool is similar to that of its neighbors (who might be lying to it, for that matter), and that it should therefore stop using SPV confirmation and switch to strict fullnode rejection of blocks that indicate a "next blocksize" that is too large or too small according to your equation?  OR will it simply follow the longest chain always, in which case, it trusts miners to be honest about how loaded (or unloaded) the transaction pool is?
As a general rule, consensus rules should restrict themselves to:
1.  The characteristics of the block.
2.  The characteristics of the transactions within the block.
The transaction pool is specifically those transaction that are NOT in any block, and thus, are not safe to depend on for any consensus rules.

@_date: 2017-12-07 02:28:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Two Drivechain BIPs 
Good morning Paul and Chris,
Assuming there are three large mining groups who will ruthlessly want to undercut their competition, and with roughly 33% of total hashpower each (with the remaining 1% being some negligible hoi polloi), then one strategy to undercut your competitors would be to upvote only your own withdrawals and downvote that of your competitors.  A miner using this strategy hopes that the other miners will give up on withdrawing their own coin and trade their sidecoins at a discount to the undercutting miner.  That is, it is a hostage attempt of the sidecoin funds of the other miners.
In the case of three large mining pools that mistrust one another, then, no withdrawal would ever push through and drivechains stabilize to one-way pegs.
Now suppose that two of the mining pools collude.  They join their withdrawals into a single withdrawal proposal and upvote that, while downvoting the withdrawal of the third miner.  I observe that this is an opposite disaster: the 66% colluding miners can instead decide to simply outright make an invalid withdrawal of all funds, split up in half between themselves.
But three exactly equal mining pools is unnatural, for that matter
Suppose that there are three mining pools: A with 34%, B with 33%, C with 32%, and the hoi polloi making up the remaining 1%.  Those three pools cannot safely let the others withdraw funds.
Suppose A colludes with C to join their withdrawal proposals and their hashpower to withdraw.  This means that B can be pressured to sell its sidecoins for a discount to the joint coalition of A and C, since B cannot withdraw its own coins.  This lowers the profitability of B, causing grinders to leave them in favor of either A or C.  Since A is slightly larger than C, it is slightly more preferable, so it grows in size slightly more than C does.  Eventually B dies due to the coalition of A and C.  A and C are the only remaining pools, with A slightly larger than C.  In this case, A can break from the coalition and squeeze C of its sidecoins, since only A can withdraw (as it has more hashpower than C).  Again, grinders will leave C for A.  A rational C that is capable of considering this possible future path will thus never ally with A in a coalition to crush B, as it will then be next in line for being destroyed.
Similar analyses for coalitions of (B, C) and (A, B).
Knowing this, and knowing that they will end up sidecoin bagholders if they cannot withdraw coins, all miners decide to collude together and put all their withdrawals into a single withdrawal proposal.  But this removes any check-and-balance that the single withdrawal proposal is truthful to the sidechain: that is, the single coalition of A,B, and C can decide to just steal all sidechain funds and reassign them in proportion to their hashpower.  This might be stable at end-of-life for the sidechain where all ordinary users of the sidechain have exited it, but is otherwise a theft risk if the sidechain is operating normally.

@_date: 2017-12-08 10:40:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Two Drivechain BIPs 
Good morning CryptAxe,
I have come to realize that P2PKH is powerful enough to write a Theft Contract from which other Accomplice Contracts can derive.
The core of the Theft Contract and the Accomplice Contract is that they are both HTLCs.  The difference is that the Theft Contract, the timelock is anyone-can-spend after the time limit.  The Accomplice Contract is an ordinary HTLC.
However, P2PKH, plus an off-chain method, can be combined to form a HTLC with anyone-can-spend after the timelock.
P2PKH includes .  Spending from a P2PKH reveals the preimage to , the public key.  Thus, the Accomplice Contract can use the P2PKH  as the hash, and when the P2PKH is spent, acquire the public key to be used as the preimage of the hashlock.
The remaining ingredient is a timelock with anyone-can-spend after the time limit.  And I belatedly realized that I have in fact seen an offchain method of imposing a timelock on information:   To create a timelock, the "mastermind" thief encrypts the private key to the P2PKH in such a timelocked-encryption scheme, and publishes it as part of the theft attempt to commit themselves to the timelock, together with a zero-knowledge proof that the timelock-encrypted private key is correctly the private key to the given public key hash (I am not mathematically gifted enough to know if such a proof if possible, however, and if this is impossible, then this entire scheme cannot work).  Thus, if the thief does not spend the P2PKH (which reveals the preimage to the hash, which unlocks the Accomplice Contracts and pays the accomplices), then someone else can grind the timelock-encryption and spend the P2PKH (and also incidentally unlocks the Accomplice Contracts anyway).
Of course, timelock-encryption is significantly less reliable as a time measure (different sequential processing speeds yield different timelocks from the same timelock-encrypted data), but that may be enough to have a reasonably trustless Thief-Accomplice coordination structure.
Another issue is that if the Accomplice does not cooperate and the theft fails, the Accomplices may still grind the timelock-encryption and acquire the private key, from which they can compute the public key, which is also the preimage to their hashlock.  So there may not actually be an incentive to coordinate with the Thief under this structure.  But perhaps this idea may trigger someone else to consider how to exploit the precise mathematics of P2PKH to create something similar to a HTLC.
-------- Original Message --------
UTC Time: December 6, 2017 8:51 PM
This vulnerability can be fixed if withdrawals are restricted to
simple P2PKH or P2WPKH only,
Limiting the withdrawal outputs to P2PKH and perhaps P2WPKH (would there
be any network benefit to supporting witness pubkeys for withdrawals?)
wouldn't be too much work for me. The downside is that people might want
to withdraw to multisig scripts, or any other legitimate P2SH. If it
prevents a huge issue, then it is probably worth it.
but in the presence of Scriptless Script and Bellare-Neven signatures,
that may be sufficient to create the Theft Contract and the Accomplice
Contract (but I know too little of Scriptless Script to be sure).
I'm curious if anyone on this list could help answer this.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-13 22:24:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Two Drivechain BIPs 
Good morning Paul,
It seems many blocks have a coinbase that pays out to a P2PKH.
The public key hash of a potential Accomplice is then readily visible on-chain on the P2PKH of the coinbase.
What is more, the potential Accomplice's hashpower can be judged on-chain also: the more blocks pay out to their P2PKH, the greater their hashpower.
From this, the motivating Thief can blindly and automatically create HTLCs paying out to the public key hash of potential Accomplices, weighed according to how many blocks were mined by those.
Then the motivating Thief can broadcast (perhaps on some website they control, via social media, and so on) the fact of the HTLCs existing, without negotiating with the Accomplices.  It is a simple "take it or leave it": if the theft succeeds (whether the Accomplice assisted in the theft or not) the Accompilce can get paid.  Thus, communication overhead is reduced to a single broadcast message (the Thief might batch a number of different possible Accomplices, and in addition, might want to play on the psychological effect of the broadcast), and the Accomplice is simply faced with the choice: either participate in the theft (and increase the chance they earn money from it) or protect against the theft (and reduce the chance they earn money from it).
Sent with [ProtonMail]( Secure Email.

@_date: 2017-12-26 22:55:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Good morning Damian,
I see you have modified your proposal to be purely driven by miners, with fullnodes not actually being able to create a strict "yes-or-no" answer as to block validity under your rules.  This implies that your rules cannot be enforced and that rational miners will ignore your proposal unless it brings in more money for them.  The fact that your proposal provides some mechanism to increase block size means that miners will be incentivized to falsify data (by making up their own transactions just above your fixed "dust size" threshhold whatever that threshhold may be -- and remember, miners get at least 12.5 BTC per block, so they can make a lot of little falsified transactions to justify every block size increase) until the block size increase per block is the maximum possible block size increase.
Let me then explain proof-of-work and the arrow of time in Physics.  It may seem a digression, but please, bear with me.
Proof-of-work proves that work was performed, and (crucially) that this work was done in the past.
This is important because of the arrow of time.
In principle, every physical interaction is reversible.  Visualize a video of two indivisible particles.  The two particles move towards each other, collide, and because of the collision, fly apart. If you ran this video in reverse, or in forward, it would not be distinguishable to you, as an outside observer, whether the video was running in reverse or not.  It seems at some level, time does not exist.
And yet time exists.
Consider another video, that of a vase being dropped on a hard surface.  The vase hits the surface and shatters.  Played in reverse, we can judge it as nonsensical: scattered pieces of ceramic spontaneously forming a vase and then flying upwards.  This orients our arrow of time: the arrow of time points from states of the universe where lesser entropy exists (the vase is whole) to where greater entropy exists (the vase is in many pieces).
Indeed, all measures of time are, directly or indirectly, measures of increases in entropy.  Consider a simple hourglass: you place it into a state of low entropy and high energy with most of the sand is in the upper part of the hourglass.  As sand falls, and more of that energy is lost into entropy, you judge that time passes.
Consider a proof-of-work algorithm: you place electrons into a state of low entropy and high energy.  As electrons go through the mining hardware, producing hashes that pass the difficulty requirement, the energy in those electrons is lost into entropy (heat), and from the hashes produced (which proves not only that work was done, but in particular, that entropy increased due to work being done), you judge that time passes.
Thus, the blockchain itself is already a service that provides a measure of time.  When a block commits to a transaction, then that transaction is known to have existed at that block height, at the latest.
Thus one idea, is to have each block commit to some view of the mempool.  If a transaction exists in this mempool-view, then you know that the transaction is at least that old, and can judge the age from this and use this to compute the "transaction priority".
Unfortunately, transferring the data to prove that the mempool-view is valid, is equivalent to always sweeping the entire mempool contents per block.  In that case you might as well not have a block size limit.
In addition, miners may still commit to a falsely-empty mempool and deny that your transaction is old and therefore priority and therefore will simply fill their blocks with transactions that have high feerates rather than high priority.  Thus feerate will still be the ultimate measure.
Rather than attempt this, perhaps developers should be encouraged to make use of existing mechanisms, RBF and CPFP, to allow transactions to be sped up by directly manipulating feerates, as priority (by your measure) is not practically computable.

@_date: 2017-07-04 03:21:23
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
Good morning Paul, Chris, and CryptAxe,
No, because at block  the topmost sidechain block is J, not H, and the H->P will not be considered as valid -- only the J->x is valid, even though H->P comes first.
Please see the pseudocode I sent before in detail and consider how it will work with your given mainchain blocks example.
I suppose some kinds of mutability are desirable. In my model, a sidechain reorg can be forced if the sidechain code is forked to specifically force some previously-valid block to become invalid, by developer fiat. In the example you gave, basically, if you want to reorg from Q->H->F->J to Q->H->P->D then you would fork the sidechain consensus code to declare that block F is invalid.
I am hesitant to make mutability something that is easy to force, however.
Payments on LN are finalized when the payee provides a preimage for a hashlock, whether by chain or by LN. Although I suppose you can use a bribelocked timelocked contract instead of a hashlocked timelocked contract. I suppose it would be almost the same, except the bribelock is provided off-chain as a proof of existence in a mainblock coinbase.
In addition, it may be possible to create a payment channel specifically between a sidechain operator and a mainchain miner.
Can you describe the ratchet better? I am sorry but when I first heard of "blind" merge mining, the first thing that came to mind was the use of OP_RETURN. This is truly blind as the mainchain miner is given what is effectively a blob of data, and the mainchain miner cannot expect any kind of format from OP_RETURN. This has the tremendous advantage of not even requiring a softfork.
In my scheme, if you read carefully through the pseudocode, a block list node is valid only if its block is valid.
Basically, in my scheme, the OP_RETURN data *is* the sidechain block headers stored on the mainchain. To save space, the sidechain block headers are reduced to only the previous-block-header commitment and the current-block-data commitment. All of the other data you would want to put in the block header (e.g. UTXO set commitment, signalling bits, time-of-day...) would be part of the current-block-data instead of the block header. Thus if the current-block-data is invalid the sidechain block header is invalid and another sidechain block header based on the previous block header will be searched for.
My understanding is that your attack scenario is not helped by OP_BRIBEVERIFY alone, as a rich sidechain attacker can provide a bigger bribe to an invalid h* especially since the mainchain miner will not even check the h*, just insert it into the coinbase.
In order to eliminate having to specify a sidechain index, and to remove sidechain indexes altogether. Instead the sidechain is identified by its topmost block header hash.
I see. However, then, I propose that my OP_RETURN scheme is superior as the sidechain block headers are indeed visible directly on the mainchain, and the mainchain node does not even need to be "local", but can be sourced anywhere, without requiring a ratchet structure (whose purpose I still have not managed to grok).

@_date: 2017-07-07 21:13:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [BIP Proposal] Standard address format for timelocked 
BIP: ?
Title: Standard address format for timelocked funds
Author: ZmnSCPxj Comments-Summary: ?
Comments-URI: ?
Status: ?
Type: ?
Created: 2017-07-01
License: CC0-1.0
== Abstract ==
OP_CHECKLOCKTIMEVERIFY provides a method of
locking funds until a particular time arrives.
One potential use of this opcode is for a user to precommit
himself or herself to not spend funds until a particular
date, i.e. to hold the funds until a later date.
This proposal adds a format for specifying addresses that
precommit to timelocked funds, as well as specifying a
redemption code to redeem funds after the timelock has
This allows ordinary non-technical users to make use of
OP_CHECKLOCKTIMEVERIFY easily.
== Copyright ==
This BIP is released under CC0-1.0.
== Specification ==
This proposal provides formats for specifying an
address that locks funds until a specified date,
and a redemption code that allows the funds to be
swept on or after the specified date.
At minimum, wallet software supporting this BIP must
be capable of sweeping the redemption code on or after
the specified date.
In addition, the wallet software should support sending
funds to the timelocked address specified here.
Finally, wallet software may provide a command to create
a pair of timelocked address and redemption code.
Addresses and redemption codes are encoded using
Bech32 encoding].
=== Timelocked Address Format ===
The human-readable part of the address is composed of:
# The four characters hodl.
# A date, in YYYYMMDD form. For example,
the date August 1, 2017 is encoded as 20170801.
# A network code, either tb for testnet,
or bc for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
# A public key hash, 32 quintets (160 bits). As is
usual for Bitcoin, this is big-endian.
This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The funds are owned by whoever controls the private
key corresponding to the public key hash given.
=== Redemption Code ===
The human-readable part of the redemption code is
composed of:
# The four characters hedl.
# A date, in YYYYMMDD form.
# A network code, either tb for testnet,
or bc for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
# A private key, 52 quintets (260 bits). This is the
256-bit private key, prepended with 4 0
bits, in big-endian order. This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The private key unlocks the funds.
=== Lock Time Computation ===
Given a particular lock date YYYYMMDD, the
actual lock time is computed as follows:
# The day before the lock date is taken. For example,
if the lock date is 20180101 or
January 1, 2018, we take the date December 31, 2017.
# We take the time 1000h (10:00 AM, or 10 in the morning)
of the date from the above step.
This lock time is then translated to a
Unix epoch time, as per POSIX.1-2001 (which removes the
buggy day February 29, 2100 in previous POSIX revisions).
The translation should use, at minimum, unsigned 32-bit
numbers to represent the Unix epoch time.
The Unix epoch time shall then be interpreted as an
nLockTime value, as per standard Bitcoin.
Whether it is possible to represent dates past 2038
will depend on whether standard Bitcoin can represent
nLockTime values to represent dates past
Since nLockTime is an unsigned 32-bit
value, it should be possible to represent dates until
06:28:15 UTC+0 2106-02-07.
Future versions of Bitcoin should be able to support
nLockTime larger than unsigned 32-bit,
in order to allow even later dates.
The reason for using an earlier lock time than the
specified date is given in the Rationale section of
this BIP.
=== Payment to a Timelocked Address ===
An ordinary P2SH payment is used to provide funds to a
timelocked address.
The script below is used as the redeemScript
for the P2SH payment:
 OP_CHECKLOCKTIMEVERIFY OP_DROP
OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIG
Once the redeemScript is derived, the hash is
determined, and an ordinary P2SH output with the below
scriptPubKey used:
OP_HASH160  OP_EQUAL
In case of SegWit deployment, SegWit-compatible wallets
should be able to use P2SH, P2WSH, or P2SH-P2WSH, as per
the output they would normally use in that situation.
Obviously, a timelocked address has an equivalent
Bitcoin 3 (P2SH) address.
A simple service or software that translates from a
public timelocked address to a P2SH address can be
created that makes timelocking (but not redemption)
backwards compatible with wallets that do not support
this BIP.
This proposal recommends that wallets supporting payment
to P2PKH, P2SH, P2WPKH, and P2WSH Bitcoin addresses should
reuse the same interface for paying to such addresses as
paying into timelocked addresses of this proposal.
=== Redemption of a Timelocked Redemption Code ===
To sweep a timelocked redemption code after the timelock,
one must provide the given redeemScript as
part of the scriptSig, of all unspent
outputs that pay to the given redeemScript
When sweeping a timelocked redemption code, first the
wallet must extract the private key from the redemption
code, then derive the public key, the public key hash,
the redeemScript, and finally the
redeemScript hash.
Then, the wallet must find all unspent outputs that pay
to the redeemScript hash via P2SH (and, in the
case of SegWit deployment, via P2SH-P2WSH and P2WSH).
For each such output, the wallet then generates a
transaction input with the below scriptSig, as
per usual P2SH redemptions:
The wallet then outputs to an address it can control.
As the Script involved uses OP_CHECKLOCKTIMEVERIFY,
the nSequence must be 0 and the
nLockTime must be equal to the computed
lock time.
This implies that the transaction cannot be transmitted
(and the funds cannot be sweeped)
until after the given lock time.
The above procedure is roughly identical to sweeping an
ordinary, exported private key.
This proposal recommends that wallets supporting a sweep
function should reuse the same interface for sweeping
individual private keys (wallet import format) for sweeping
timelocked redemption codes.
== Motivation ==
A key motivation for this BIP is to allow easy use of
OP_CHECKLOCKTIMEVERIFY by end-users.
The below are expected use cases of this proposal:
# A user wants to purchase an amount of Bitcoin,
and subsequently wait for an amount of time before
cashing out.
The user fears that he or she may have "weak hands",
i.e. sell unfavorably on a temporary dip, and thus
commits the coins into a timelocked fund that can
only be opened after a specific date.
# A user wants to gift an amount of Bitcoins to
an infant or minor, and wants the fund to not be spent
on ill-advised purchases until the infant or minor
reaches the age of maturity.
# A user may wish to prepare some kind of monthly subsidy
or allowance to another user, and prepares a series of
timelocked addresses, redeemable at some set date on
each month, and provides the private redemption codes to
the beneficiary.
# A user may fear duress or ransom for a particular
future time horizon, and voluntarily impose a lock time
during which a majority of their funds cannot be spent.
== Rationale ==
While in principle, this proposal may be implemented as a
separate service or software, we should consider the long
time horizons that may be desired by users.
A user using a particular software to timelock a fund may
have concerns, for example, of specifying a timelock
18 years in the future for a gift or inheritance to a
newborn infant.
The software or service may no longer exist after 18 years,
unless the user himself or herself takes over maintenance
of that software or service.
By having a single standard for timelocked funds that is
shared and common among multiple implementations of Bitcoin
wallets, the user has some assurance that the redemption code
for the funds is still useable after 18 years.
Further, a publicly-accessible standard specifying how the
funds can be redeemed will allow technically-capable users
or beneficiaries to create software that can redeem the
timelocked fund.
This proposal provides a timelock at the granularity of a
The expectation is that users will have long time
durations of months or years, so that the ability to
specify exact times, which would require specifying the
timezone, is unneeded.
The actual timeout used is 1000h of the day before the
human-readable date, so that timezones of UTC+14 will
definitely be able to redeem the money starting at
0000h of the human-readable date, local time (UTC+14).
Given the expectation that users will use long time
durations, the fact that timezones of UTC-12 will
actually be able to redeem the funds on 2200h UTC-12
time two days before can be considered an acceptable
The human-readable date is formatted according to
ISO standard dates], with the dashes removed.
Dashes may prevent double-click selection, making
usability of these addresses less desirable.
The bc or tb is after the
date since the date is composed of digits and the bech32
separator itself is the digit 1. One
simply needs to compare hedlbc202111211...
and hedl20211121bc1....
A version quintet is added in case of a future
sociopolitical event that changes interpretation of
dates, or changes in scripting that would allow for more
efficient redemptions of timelocked funds (which would
change the redeemScript paid to), or changes
in the size and/or format of lock times, and so on.
Such changes are unlikely, so the version is a quintet in
the bech32 data part rather than a substring in the
human-readable part.
The public address format uses the hodl as
the start of the code, while the private key (the
redemption code) uses hedl.
This provides a simple mnemonic for users:
"Pay into the hodl code to hold your
coins until the given date.
After you've held the coins (on or after the given date)
use the hedl code to redeem the coins."
The obvious misspelling of "hodl" is a homage to the common
meme within the Bitcoin community.

@_date: 2017-07-12 04:30:31
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [BIP Proposal] Standard address format for 
Good morning mailinglist,
I am saddened at the lack of attention to this BIP proposal. I know that it is not as interesting as the debates on where Bitcoin will go in the future and what needs to be prepared for even greater mainstream adoption, but I think my BIP proposal does have at least some value to long-term investors.
So far I have seen only a single public feedback:
Basically, the point in that feedback is mostly that the computed timelock should be UTC+0 0000h of the given human-readable date.
I would like to respectfully ask the mailing list about which option is best:
1. (current) Use the earliest timezone as of now, UTC+14 0000h of the given human-readable date. Pro: No matter where you are in the world, as soon as the given date arrives, the fund can be spent. Con: For most of the world, the fund can be spent on some time the day before, or even two days before for UTC-11 and UTC-12 timezones.
2. Use the standard timezone UTC+0 0000h of the given human-readable date. Pro: standard time. Con: for half of the world, the fund is not spendable until some time into the given date, for the other half, it will be spendable at an earlier date.
3. Allow indicating a timezone to the human-readable part. Pro: gives control over the user's expected local time. Con: additional field and effectively more control, need to handle also strange timezones that have 0.5 hour difference from UTC, need to encode positive and negative preferably without using + and -, as those may break double-click selection.
I hope to get some feedback from this list.
-------- Original Message --------
UTC Time: July 8, 2017 1:13 AM
BIP: ?
Title: Standard address format for timelocked funds
Author: ZmnSCPxj Comments-Summary: ?
Comments-URI: ?
Status: ?
Type: ?
Created: 2017-07-01
License: CC0-1.0
== Abstract ==
OP_CHECKLOCKTIMEVERIFY provides a method of
locking funds until a particular time arrives.
One potential use of this opcode is for a user to precommit
himself or herself to not spend funds until a particular
date, i.e. to hold the funds until a later date.
This proposal adds a format for specifying addresses that
precommit to timelocked funds, as well as specifying a
redemption code to redeem funds after the timelock has
This allows ordinary non-technical users to make use of
OP_CHECKLOCKTIMEVERIFY easily.
== Copyright ==
This BIP is released under CC0-1.0.
== Specification ==
This proposal provides formats for specifying an
address that locks funds until a specified date,
and a redemption code that allows the funds to be
swept on or after the specified date.
At minimum, wallet software supporting this BIP must
be capable of sweeping the redemption code on or after
the specified date.
In addition, the wallet software should support sending
funds to the timelocked address specified here.
Finally, wallet software may provide a command to create
a pair of timelocked address and redemption code.
Addresses and redemption codes are encoded using
Bech32 encoding].
=== Timelocked Address Format ===
The human-readable part of the address is composed of:
# The four characters hodl.
# A date, in YYYYMMDD form. For example,
the date August 1, 2017 is encoded as 20170801.
# A network code, either tb for testnet,
or bc for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
# A public key hash, 32 quintets (160 bits). As is
usual for Bitcoin, this is big-endian.
This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The funds are owned by whoever controls the private
key corresponding to the public key hash given.
=== Redemption Code ===
The human-readable part of the redemption code is
composed of:
# The four characters hedl.
# A date, in YYYYMMDD form.
# A network code, either tb for testnet,
or bc for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
# A private key, 52 quintets (260 bits). This is the
256-bit private key, prepended with 4 0
bits, in big-endian order. This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The private key unlocks the funds.
=== Lock Time Computation ===
Given a particular lock date YYYYMMDD, the
actual lock time is computed as follows:
# The day before the lock date is taken. For example,
if the lock date is 20180101 or
January 1, 2018, we take the date December 31, 2017.
# We take the time 1000h (10:00 AM, or 10 in the morning)
of the date from the above step.
This lock time is then translated to a
Unix epoch time, as per POSIX.1-2001 (which removes the
buggy day February 29, 2100 in previous POSIX revisions).
The translation should use, at minimum, unsigned 32-bit
numbers to represent the Unix epoch time.
The Unix epoch time shall then be interpreted as an
nLockTime value, as per standard Bitcoin.
Whether it is possible to represent dates past 2038
will depend on whether standard Bitcoin can represent
nLockTime values to represent dates past
Since nLockTime is an unsigned 32-bit
value, it should be possible to represent dates until
06:28:15 UTC+0 2106-02-07.
Future versions of Bitcoin should be able to support
nLockTime larger than unsigned 32-bit,
in order to allow even later dates.
The reason for using an earlier lock time than the
specified date is given in the Rationale section of
this BIP.
=== Payment to a Timelocked Address ===
An ordinary P2SH payment is used to provide funds to a
timelocked address.
The script below is used as the redeemScript
for the P2SH payment:
 OP_CHECKLOCKTIMEVERIFY OP_DROP
OP_DUP OP_HASH160  OP_EQUALVERIFY OP_CHECKSIG
Once the redeemScript is derived, the hash is
determined, and an ordinary P2SH output with the below
scriptPubKey used:
OP_HASH160  OP_EQUAL
In case of SegWit deployment, SegWit-compatible wallets
should be able to use P2SH, P2WSH, or P2SH-P2WSH, as per
the output they would normally use in that situation.
Obviously, a timelocked address has an equivalent
Bitcoin 3 (P2SH) address.
A simple service or software that translates from a
public timelocked address to a P2SH address can be
created that makes timelocking (but not redemption)
backwards compatible with wallets that do not support
this BIP.
This proposal recommends that wallets supporting payment
to P2PKH, P2SH, P2WPKH, and P2WSH Bitcoin addresses should
reuse the same interface for paying to such addresses as
paying into timelocked addresses of this proposal.
=== Redemption of a Timelocked Redemption Code ===
To sweep a timelocked redemption code after the timelock,
one must provide the given redeemScript as
part of the scriptSig, of all unspent
outputs that pay to the given redeemScript
When sweeping a timelocked redemption code, first the
wallet must extract the private key from the redemption
code, then derive the public key, the public key hash,
the redeemScript, and finally the
redeemScript hash.
Then, the wallet must find all unspent outputs that pay
to the redeemScript hash via P2SH (and, in the
case of SegWit deployment, via P2SH-P2WSH and P2WSH).
For each such output, the wallet then generates a
transaction input with the below scriptSig, as
per usual P2SH redemptions:
The wallet then outputs to an address it can control.
As the Script involved uses OP_CHECKLOCKTIMEVERIFY,
the nSequence must be 0 and the
nLockTime must be equal to the computed
lock time.
This implies that the transaction cannot be transmitted
(and the funds cannot be sweeped)
until after the given lock time.
The above procedure is roughly identical to sweeping an
ordinary, exported private key.
This proposal recommends that wallets supporting a sweep
function should reuse the same interface for sweeping
individual private keys (wallet import format) for sweeping
timelocked redemption codes.
== Motivation ==
A key motivation for this BIP is to allow easy use of
OP_CHECKLOCKTIMEVERIFY by end-users.
The below are expected use cases of this proposal:
# A user wants to purchase an amount of Bitcoin,
and subsequently wait for an amount of time before
cashing out.
The user fears that he or she may have "weak hands",
i.e. sell unfavorably on a temporary dip, and thus
commits the coins into a timelocked fund that can
only be opened after a specific date.
# A user wants to gift an amount of Bitcoins to
an infant or minor, and wants the fund to not be spent
on ill-advised purchases until the infant or minor
reaches the age of maturity.
# A user may wish to prepare some kind of monthly subsidy
or allowance to another user, and prepares a series of
timelocked addresses, redeemable at some set date on
each month, and provides the private redemption codes to
the beneficiary.
# A user may fear duress or ransom for a particular
future time horizon, and voluntarily impose a lock time
during which a majority of their funds cannot be spent.
== Rationale ==
While in principle, this proposal may be implemented as a
separate service or software, we should consider the long
time horizons that may be desired by users.
A user using a particular software to timelock a fund may
have concerns, for example, of specifying a timelock
18 years in the future for a gift or inheritance to a
newborn infant.
The software or service may no longer exist after 18 years,
unless the user himself or herself takes over maintenance
of that software or service.
By having a single standard for timelocked funds that is
shared and common among multiple implementations of Bitcoin
wallets, the user has some assurance that the redemption code
for the funds is still useable after 18 years.
Further, a publicly-accessible standard specifying how the
funds can be redeemed will allow technically-capable users
or beneficiaries to create software that can redeem the
timelocked fund.
This proposal provides a timelock at the granularity of a
The expectation is that users will have long time
durations of months or years, so that the ability to
specify exact times, which would require specifying the
timezone, is unneeded.
The actual timeout used is 1000h of the day before the
human-readable date, so that timezones of UTC+14 will
definitely be able to redeem the money starting at
0000h of the human-readable date, local time (UTC+14).
Given the expectation that users will use long time
durations, the fact that timezones of UTC-12 will
actually be able to redeem the funds on 2200h UTC-12
time two days before can be considered an acceptable
The human-readable date is formatted according to
ISO standard dates], with the dashes removed.
Dashes may prevent double-click selection, making
usability of these addresses less desirable.
The bc or tb is after the
date since the date is composed of digits and the bech32
separator itself is the digit 1. One
simply needs to compare hedlbc202111211...
and hedl20211121bc1....
A version quintet is added in case of a future
sociopolitical event that changes interpretation of
dates, or changes in scripting that would allow for more
efficient redemptions of timelocked funds (which would
change the redeemScript paid to), or changes
in the size and/or format of lock times, and so on.
Such changes are unlikely, so the version is a quintet in
the bech32 data part rather than a substring in the
human-readable part.
The public address format uses the hodl as
the start of the code, while the private key (the
redemption code) uses hedl.
This provides a simple mnemonic for users:
"Pay into the hodl code to hold your
coins until the given date.
After you've held the coins (on or after the given date)
use the hedl code to redeem the coins."
The obvious misspelling of "hodl" is a homage to the common
meme within the Bitcoin community.

@_date: 2017-07-12 04:50:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
It's the sidechain node which needs to learn about the sidechain blockchain anyway. So it's the one that does the checking of this.
For that matter, a mainchain miner can be bribed to commit to a random number rather than a valid h* block, and it will still lead all the sidechain nodes on a random chase to look for the indicated block.
In this paper: As far as I understood that paper, it means that if the block reward no longer exists, miners can profitably attempt to undercut any full blocks.
Sidechains do not have block rewards (unless the sidechain issues its own asset type that is separate from and not convertible to mainchain bitcoins).
Thus, to protect against undercutting attacks in the sidechain, we would need to ensure that the sidechain cannot be reorged without the mainchain (which currently still has a block reward) being reorged.
At least, this is my consideration. Perhaps the paper is wrong?
In any case, let me propose actual improvements to the OP_BRIBEVERIFY proposal:
1. Remove the necessity of coinbase commitments. The miner commits to the sidechain_id and h* in the transaction that pays the OP_BRIBEVERIFY anyway. That way the h* commitment occurs only once in the block, in the transaction that does the OP_BRIBEVERIFY. In addition, there is no need to impose particular ordering on the coinbase outputs, which would be problematic as pointed out by others, for example if the miner is interested only in merge mining for sidechain id  and nobody else.
2. When verifying a block, keep a set of sidechain ID's. When processing a transaction in that block with OP_BRIBEVERIFY, check if the sidechain ID is in that set. If not in that set, add it to that set and continue script processing. If already in the set, fail the script processing. This ensures that at most one OP_BRIBEVERIFY exists for each sidechain_id in a mainchain block.
3. Don't number sidechain_id from 0, 1, 2, 3..., because people will fight over the small numbers. Instead identify sidechains by, for example, the hash of their names or the hash of their genesis block or whatever. This allows true permissionless creation of sidechains, without some authority existing that centrally allocates sidechain ID's.

@_date: 2017-05-31 23:42:21
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] [RFC] Lightning invoice/payment 
Good morning Rusty,
The fact that amount is optional, and the separator character between human-readable and data is the character "1", may mean some trouble with parsing of the optional amount.
Currently, the version is 0, which translates to the character "q" appearing after "1". So 1q is obviously not an amount and is known to start the data part.
However, what happens when we decide to upgrade, and version is now 1, translating to character "p"?
If version 1 of invoice is avalialble, does a payment starting with lnbc1p .... indicate a 1 pico-bitcoin payment, or an arbitrary payment to a version-1 data part?
Or is amount not allowed to have character "1"? It seems a strange limitation if we impose this...
Or do I mistake my understanding of bech32?
Alternatively we can just fix the first 5 bits to be 0 (so "1q" is an unambiguous separator between human-readable and data parts) and provide the version by other means, such as changing lnbc to ln2bc or so on...
At first read, this seems wrong. My understanding is that lightning invoices are longer than segwit addresses in bech32, so human error is more likely.
Of course, it seems that the intended meaning is really "lightning invoices are so long that it is unlikely humans will enter it by hand, so human errors are unlikely compared to QR reader errors etc." so perhaps better reworded as such.

@_date: 2017-05-31 23:48:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] [RFC] Lightning invoice/payment 
Good morning,
Looking again at bech32 spec, yes, my understanding is wrong: the character "1" is not allowed in the data part, so the last "1" digit in the bech32 string is unambiguously the separator between the human-readable and data parts, please ignore me.

@_date: 2017-06-28 04:26:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
Good morning.
I still do not see what this does that cannot be done by:
OP_RETURN  A transaction with such an output would allow sidechain-miners to bribe mainchain-miners by paying a transaction fee if the transaction containing this OP_RETURN is included in a block and committed to by a mainchain-miner in the Merkle tree root.
It would not require a softfork.
I have an alternate proposal that sidechains and altcoins that want to do "blinded" merge mining can use without a softfork:
1. Encode a block header as a simple cons-pair, with the head as the block and the tail as the parent cons-pair.
1.1. This can be encoded as a 32-byte hash of the block including its header, and the 32-byte hash of the parent cons-pair.
1.2. This is now the actual "chain" in the sidecoin/altcoin blockchain.
2. When a sidechain-node wants to know the consensus, it downloads mainchain-blocks and looks for OP_RETURN's.
2.1. Starting with its genesis cons-pair hash (equivalent to the empty list) as the current cons-pair, it scans each OP_RETURN transaction.
2.1.1. If an OP_RETURN is 64-byte and has the parent cons-pair equal to the current cons-pair, look for the side block indicated and confirm its correctness. If correct, update the current cons-pair for the hash of the OP_RETURN data.
2.2. When reaching the latest mainchain block, the current cons-pair is now the sidecoin/altcoin latest block.
2.3. Note that if multiple OP_RETURN in a block match the current cons-pair, the first one is considered the correct chain. This property means that the sidechain/altchain can only have a chainsplit if the mainchain has a chainsplit.
3. When a sidechain-miner wants to create a side-block, it generates a new cons-pair and creates an OP_RETURN transaction for it, paying a mainchain-miner to include it in the next mainchain-block.
3.1. The sidechain-miner risks that its competitors will outbid it and get its OP_RETURN earlier in a mainchain-block (or earlier in the order of transactions). It can mitigate this risk by updating itself to become a mainchain-miner, it can then keep its OP_RETURN transaction private and put it earlier in the block, ensuring it will "win" the sidechain-consensus if it wins the mainchain-consensus.
-------- Original Message --------
UTC Time: June 28, 2017 12:37 AM
BIP: Layer: Consensus (Soft fork)
Title: OP_BRIBEVERIFY
Author: Chris Stewart Status: Draft
Type: Standards Track
Created: 2017-06-27
This BIP describes a new opcode, OP_BRIBEVERIFY, for the Bitcoin
scripting system that allows for a user to bribe a miner to include a hash
in the coinbase transaction's output.
BRIBEVERIFY redefines the existing NOP4 opcode. When executed, if the given
critical hash is included at the given vout index in the coinbase transaction
the script evaluates to true. Otherwise, the script will fail.
This allows sidechains to be merged mined against
bitcoin without burdening bitcoin miners with extra resource requirements.
The current political climate of bitcoin is extremely contentious. Many community members
have different visions of what bitcoin is. This op code is meant to
enable [ Blind Merge Mining].
This enables sidechains in Bitcoin. With OP_BRIBEVERIFY, sidechains miners can
bribe bitcoin miners to to include their block hash in the bitcoin blockchain. If their block
is included in the coinbase transaction's vout, it is assumed that block is a mined block on the sidechain.
This will allow various factions of the community to realize their vision on their own separate
blockchain that is interoperable with the bitcoin blockchain. This allows those factions to use
bitcoin as a 'reserve currency' for their own network.
===Commitment Structure===
A new block rule is added which requires that the miner's coinbase reward be at index 0 in the coinbase transaction's output vector.
It also fixes the witness commitment output to be at index 1 of the coinbase transaction's output vector.
This is needed so we can reliably tell what vout corresponds to what drivechain. For instance, the mimblewimble sidechain
could correspond to index 2 of the vector outputs on the coinbase transaction.
The commitment is recorded in a scriptPubKey of the coinbase transaction. It must be at least 34 bytes in size
1-byte - OP_RETURN (0x6a)
1-byte - Push the following 32 bytes (0x20)
32-byte - block hash
the 35th byte and onward have no consensus meaning.
===OP_BRIBEVERIFY op code===
This op code reads two arguments from the stack. The stack top is expected to be a sidechain id for which this user attempting to blind merge mine for.
The next element on the stack is expected to be a block hash. This op code looks into the coinbase transaction's output vector at the given index (which is derived from the sidechain id) and checks
to see if the hash in the block matches the hash inside of the BRIBEVERIFY program. If the hashes match, the OP_BRIBEVERIFY acts as an OP_NOP. If the
comparison between the two hashes fail, the script fails.
===BRIBEVERIFY program===
A standard BRIBEVERIFY program has the format:
1-byte - Push the following 32 bytes (0x20)
32-byte - block hash
1 byte - Push operation? (needed if number can't be encoded as OP_0 - OP_16)
1 byte - sidechain id
1 byte - OP_BRIBEVERIFY op code
==Detailed Specification==
Refer to the reference implementation, reproduced below, for the precise
semantics and detailed rationale for those semantics.
case OP_NOP4:
if (!(flags & SCRIPT_VERIFY_BRIBEVERIFY)) {
if (flags & SCRIPT_VERIFY_DISCOURAGE_UPGRADABLE_NOPS) {
return set_error(serror, SCRIPT_ERR_DISCOURAGE_UPGRADABLE_NOPS);
if (stack.size() < 2)
return set_error(serror, SCRIPT_ERR_INVALID_STACK_OPERATION);
const CScriptNum scriptNumSidechainId(stacktop(-1),fRequireMinimal);
uint8_t nSidechainId;
if (!checker.CheckSidechainId(scriptNumSidechainId,nSidechainId)) {
return set_error(serror, SCRIPT_ERR_UNKNOWN_SIDECHAIN);
bool fHashCritical = checker.CheckCriticalHash(stacktop(-2),nSidechainId);
if (!fHashCritical) {
return set_error(serror, SCRIPT_ERR_UNSATISFIED_BRIBE);
Credit to Paul Sztorc for the original idea of Blind Merge Mined sidechains.
Credit to CryptAxe for writing the foundational layer of software for drivechains so I could implement OP_BRIBEVERIFY.
Blind Merge Mined Sidechains - Mailing list discussion - This document is placed in the public domain.

@_date: 2017-06-30 00:00:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
Good Morning Paul,
I understand.
I don't understand this part. In my scheme, a sidechain cannot reorganize unless the mainchain reorganizes, since the consensus loop only cares about matching the current block; it ignores splits and does not consider them valid.
But I suppose you are considering something like the Ethereum mutability feature, which I do not think is something you would want in a sidechain.
Do you not provide a single sidechain's h* twice in the block? Once in the coinbase and once in the briber's separate transaction?
In my scheme at least there is no indicator byte -- the "previous block" hash is the indicator of which sidechain it is extending. From your other emails on this list, it seems the ratchet is for withdrawals from sidechain to mainchain? If so, should it not only appear in only some of the sidechains (the ones which are currently doing some withdrawal?)?

@_date: 2017-05-12 18:17:30
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
Good morning Luke,
Minor editorial nitpick, this paragraph is repeated, maybe one of these should be Testnet?
For Bitcoin '''mainnet''', the BIP8 '''starttime''' will be TBD (Epoch timestamp TBD) and BIP8 '''timeout''' will be TBD (Epoch timestamp TBD).
For Bitcoin '''mainnet''', the BIP8 '''starttime''' will be TBD (Epoch timestamp TBD) and BIP8 '''timeout''' will be TBD (Epoch timestamp TBD).

@_date: 2017-05-12 23:54:50
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
Good morning,
The problem, is that, the rate of conversion of Bitcoin-> hashrate is different for different people.
For some, it's very cheap to convert Bitcoin -> hashrate. For others, it's very expensive. The reason, is the large difference in electricity rates depending on country.
It's all very well for those who can get electricity cheaply. But, for some, it is not.
Thus, paying someone with better Bitcoin->hashrate conversion via fees such as these is more economically logical, than to suffer a lower Bitcoin->hashrate conversion.
It is also, very obviously, clear that you are operating under very strange assumptions, that all people are already equal somehow, or that someone who is paid x10 more is strictly superior, even though skill-wise and ability-wise, they are the same, and the one paid less is simply suffering due to the country where he or she is born in, through no fault of their own.

@_date: 2017-05-14 08:18:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
Good morning Luke,
Considering the proposal as a whole, I think, it's a little imperfect.
The main problem, is that the end goal is activation, but what the opcode rewards is signalling.
Consider a miner who signals only if the number of non-signalling blocks in this retargeting time > 5% of 2016. Such a miner would still effectively block a softfork activation, while still has a chance (albeit reduced) of winning the transaction fees of the block-signalling-opcode, in proportion to the number of miners not signaling for a softfork or using a similar algorithm.
What we should reward should be activation.
How about an opcode which requires this stack (stack top at right)
1. If the  given is in state FAILED, then it checks if the given  matches the given .
2. If the  given is in state LOCKED_IN or ACTIVE, it checks if the given  matches the block's coinbase transaction signature.
This creates an output which is refundable to the owner, if the softfork fails to activate, but which may be claimed by miners, if the softfork activates.
I don't know enough yet about Bitcoin's codebase to know if the above spec is actually workable.
But basically, I think we should create an assurance contract for activation of a softfork.
Also, this invites an inverse logic:
1. If the  given is in state LOCKED_IN or ACTIVE, then it checks if the given  matches the given .
2. If the  given is in state FAILED, it checks if the given  matches the block's coinbase transaction signature.
I think, your proposal allows an economic actor to pay fees if the miner is explicitly not signaling. This is supposed to allow a vote against a particular softfork.
Thus, it should also be possible to allow to vote against a softfork.
But in any case, I think, it's better to pay on activation or failure to activate, rather than mere signalling, as signalling is not the goal. Activation, or rejection of activation, is the goal.

@_date: 2017-05-15 19:04:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
Good morning Pieter,
Another use case I can think of is a potential "chain-flip" hard fork of block header formats, where the UTXO hash rather than merkle tree root of transactions is in the header, which would let lite nodes download a UTXO set from any full node and verify it by verifying only block headers starting from genesis.

@_date: 2017-05-15 20:15:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
I understand. Thank you for your explanation.
Another thought I have, is that instead of committing to the UTXO of the block, to commit to the UTXO of the previous block, and the merkle tree root of the transactions in the current block.
My thought is that this would help reduce SPV mining, as a miner would need to actually scan any received new blocks in order to create the UTXO set of the previous block. An empty block would make things easier for the next block's miner, not the current block's miner. However, I'm not sure if my understanding is correct, or if there is some subtlety I missed in this regard.

@_date: 2017-05-22 10:39:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
Good morning Paul,
I read only From just this document, I can't see a good justification for believing that a main->side locking transaction can be safely spent into a side->main unlocking transaction. Do you have a better explanation?
OP_is_h_in_coinbase, as described, does not seem to protect against a sidechain reorg in your next section of the document. If I attempt to spend a main->side locking transaction on the basis of a "mistaken" side block  what prevents me from this sequence:
1. Put a side:side->main transaction into a block together with TheDAO's hacked money.
2. Wait for a reorg to revert TheDAO.
3. Spend my now-free-in-the-reorg funds on Lightning Network to get mainchain funds.
4. Create a main:side->main transaction with the side:side->main transaction in the TheDAO-hacked block as witness.
5. Get another set of mainchain funds from the same sidechain funds.
So far, the only good side->main transfer I know of is in Blockstream's original sidechains paper, with the main:side->main transaction spending into a timelocked transaction that may be burned if a reorg proof is submitted (i.e. you try to create a main:side->main transaction with the side:side->main transaction in the mistaken  and  as your proof, but someone else can come along and show a corrected    without your side:side->main transaction and burn your funds). Is your proposal at the technical level actually similar, or does it truly seem to be riskier? It seems to me that your OP_is_h_in_coinbase should scan a series of sidechain block headers backed by mainchain (meaning at the minimum that sidechains should have some common header format prefix), rather than just mainchain depth as your article seems to imply.
Also, blinded merge mining seems strictly inferior to proof-of-burn: Proof-of-burn integrates a lottery to reduce the ability of a mainchain-rich attacker to reorg the sidechain by burning its greater funds. However it still seems to me that a rich attacker can simply make more bets in that scheme by some trivial modification of the side block. Blind merged mining seems strictly inferior as a rich attacker can simply reorg the sidechain outright without playing such games.
Or is your proposal strictly for centralized sidechains, where only one entity creates side blocks? How does your proposal handle multiple side block creators on the same sidechain, with the possibility that chain splits occur?
Regarding your dig about people who dislike data centers, the main issue with miners blindly accepting sidechain commitments is that it violates "Don't trust, verify", not that allows datacenters to be slightly smaller by not including side:nodes.
Sent with ProtonMail Secure Email.
-------- Original Message --------
UTC Time: May 22, 2017 6:17 AM
Dear list,
I've been working on "drivechain", a sidechain enabling technology, for
some time.
* The technical info site is here: * The changes to Bitcoin are here:
* A Blank sidechain template is here:
As many of you know, I've been seeking feedback in person, at various
conferences and meetups over the past year, most prominently Scaling
Milan. And I intend to continue to seek feedback at Consensus2017 this
week, so if you are in NYC please just walk up and start talking to me!
But I also wanted to ask the list for feedback. Initially, I was
hesitant because I try not to consume reviewers' scarce time until the
author has put in a serious effort. However, I may have waiting too
long, as today it is actually quite close to a working release.
Scaling Implications
Unfortunately, anyone who worked on the "first generation" of sidechain
technology (the skiplist) or the "second generation" (federated /
Liquid), will find that this is very different.
I will admit that I am very pessimistic about any conversation that
involves scalability. It is often said that "talking politics lowers
your IQ by 25 points". Bitcoin scalability conversations seem to drain
50 points. (Instead of conversing, I think people should quietly work on
whatever they are passionate about until their problem either is solved,
or it goes away for some other reason, or until we all agree to just
stop talking about it.)
[1] [2] [3] bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-05-22 20:13:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
Good morning,
Looking over the code, I have a question: Is OP_BRIBE supposed to be softforked in, or hardforked? From my understanding, the code as published in your linked github cannot be softforked in, since it is not a softfork-compatible replacement for OP_NOP: it replaces the stack top value with a 0/1 value. Both CLTV and CSV do not touch the stack, only flag an error if they fail.
(What happens if the h* to be put in the coinbase, by chance - even very unlikely chance - is 0? Then  OP_NOP4 is not the same as  OP_BRIBE scripts in result - the former will be rejected by old nodes, the latter will be accepted by new nodes)
Does Drivechain require a hardfork? My understanding is that you want to use some kind of softforked anyone-can-spend transaction to use Drivechain. So I don't quite understand why OP_BRIBE is written the way it is.
Is OP_BRIBE the same as the OP_h_is_in_coinbase operation you described?
How is OP_BRIBE superior to just using a  OP_RETURN script? Cannot a sidechain scan the block for OP_RETURN attesting that the block hash is present in the block? OP_BRIBE encodes  twice (once in the transaction, once in the coinbase), OP_RETURN encodes it once (once in the transaction)
You misunderstand. The first withdrawal was done by double-spending, and exchanging your sidechain funds for mainchain funds using some off-chain method. The second withdrawal is done on-chain.
That said, I confused OP_h_is_in_coinbase as your method of getting out of the sidechain into the mainchain. It seems, OP_h_is_in_coinbase is only for blind mining?
I don't quite understand how Drivechain handles sidechain reorgs, while keeping Bitcoin miners blinded. It seems sidechains need to be known ("seen") by the miner, so I don't see what is being blinded by blinded merge mining.
I misunderstand the purpose of your OP_is_h_in_coinbase, sorry.
I see. However, block subsidies will drop far in the future, do you mean to say, that sidechains will be used only in the far future?
I assume you mean "mainchain Bitcoin block" here.
What mechanism ensures only one mainchain block can contain only one sidechain block? It seems, this isn't implemented by OP_BRIBE yet. Can you elaborate on this mechanism?
I see. It seems, the main problem, is that sidechains can be used to sneak in block size increases. Of course, the advantage of sidechains, is that when it increases block size irresponsibly, only those who participated in the sidechain will suffer.
But from blind merge mining by itself, how would the blinded merge miner know that there exists an actual sidechain full node that actually did validation?
It seems, that the "blinding" in merge mining does not seem to be at all useful without the miner actually seeing the sidechain. If you want miners to upgrade to side:fullnode as well, what would then be the point of blinding? Why not just ordinary merge mining?
Perhaps the datacenter point is simply that your proposal suggests to reduce the size of the datacenter by removing surge suppressors and UPS's, to avoid some definition of "datacenter is a room with >$XXX of equipment".

@_date: 2017-05-23 19:26:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
Good morning,
Indeed, this is the reason why CLTV and CSV do not pop off their parameters when executed, and require a subsequent OP_DROP. I suggest, that OP_BRIBE should not manipulate stack (pop, then push 0/1); my understanding is that this requirement is necessary for compatibility with old nodes, which will not manipulate stack on OP_NOP4. Instead, OP_BRIBE should imitate CLTV and CSV code, and raise an error in script execution if the check fails.
Regarding "largest chain", do you mean mainchain or sidechain?
An OP_RETURN is still some guarantee that it will make it into the longest mainchain. If OP_RETURN tx is in a shorter mainchain but not on the longer mainchain, then on the longer mainchain, the utxo's funding the OP_RETURN tx is still unspent and the OP_RETURN tx will still be mineable by any miner following the longer mainchain. The X BTC would be the OP_RETURN transaction's fee, which Mary would still want to mine into the longest mainchain, as it is still money on the table if it is not mined on the longest mainchain.
Or, does OP_BRIBE somehow assure that Sam's block goes onto the longer sidechain? But then, do not side blocks refer to their previous side block to define the sidechain?
I see.
Is there some predictable schedule for side->main withdrawals? If a withdrawal is imminent, or if some actor can get "insider information" about whether a withdrawal is imminent, cannot some actor induce the above, with potentially shorter time to reach step 3?
From my reading, Blockstream's sidechains proposal supports a reorg proof after a side->main withdrawal on the mainchain side, with a reorg proof burn window after the main:side->main withdrawal, preventing its utxo from being used. If the reorg proof is published and shows that a sidechain reorg invalidates a particular side->main withdrawal, then the main:side->main withdrawal's utxo is burned.
Do you have some document containing these details? I cannot find this in the blog posts I've read so far.
As above, do you have document containing what data mainchain needs to track?
I endorse this on the basis of Greg Maxwell's analysis that a block size limit is necessary to have a fee market.
Can you provide the details of this mechanism? For example, does h* actually include some information identifying the sidechain and OP_BRIBE is supposed to do some additional checking not shown in your current code, or ....?
My understanding is that with Blockstream's zk-SNARKs, a new sidechain would not require a soft fork at all (or even miner voting on the validity of WT^: the validity of side:side->main transactions is assured by proof that the zk-SNARK checking that transaction was executed correctly, and the lack of a reorg proof during the burn window after the main:side->main).
Is your model then, that each sidechain maintainer has to maintain a patchset or some plugin system to Core? And miners who want to support particular sidechains to modify their software, applying the patch for each sidechain they want to support?
It seems this is somewhat brittle and may cause sidechain coding problems to leak into mainchain.
I think, it is much less interesting to have to softfork in every sidechain, rather than to support a general mechanism (zk-SNARK) to allow sidechains to be launched without any modification to Core code.
It seems to be, more of "completely sighted merged mining" than "blind merge mining".
I find this point now moot, as drivechains require a softfork for each sidechain, and the size of the datacenter is pointless if there is some need to softfork in every sidechain.

@_date: 2017-11-08 00:37:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Centralizing mining by force 
Good morning Robert,
What you describe is precisely one possible result of a 51% attack.
At below the 50% threshold, miners outside the cartel will on average outrace miners inside the cartel, so fullnodes which do not follow cartel rules will reject them as per Nakamoto Consensus.  At some point, the chain split becomes permanent, with miners outside the cartel pulling above the cartel miners.
However, above the 50% threshold, miners outside the cartel will be unable to keep up with the cartel and be unable to build on top of the cartel chain (as presumably they are not valid signatories).  Outside-cartel miners, however, may institute an opposing cartel, or an anti-cartel (blocks must have a fixed, invalid with high probability, 00000 signature).
In the end, what matters is what fullnodes accept.  If fullnodes do not care, then the group of miners that is larger wins.  If fullnodes do check one or the other set of rules, then that set of rules will win.
Given current politics, it is likely that fullnodes will institute an anti-cartel rule in this case, and reject the cartel and suffer low hashrate.  Eventually, the cartel will be betrayed by one of its members mining the anti-cartel chain in return for fees and valuable block rewards.
Sent with [ProtonMail]( Secure Email.

@_date: 2017-11-10 18:33:31
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_CHECKHARDFORKVERIFY - replay protection and fork 
Good morning list,
I would like to speculate on the addition of an opcode which would provide replay protection and allow chain-backed trustless creation of hardfork futures payment channels.
Note however that in order to work, the hardfork must "cooperate" by changing the operation of OP_CHECKHARDFORKVERIFY in the hardfork.
The opcode is simple.  If the top stack is not the exact value 1, it fails.
The intent is that a hardfork must "cooperate" by also changing OP_CHECKHARDFORKVERIFY so that the top stack must be some other, non-1 value after the hardfork date.  This is a consensus break, but as a hardfork is defined by the fact that it is a consensus break, this is deliberate.
In the below, I assume the creation of a future hardfork with a new "consensus version" value (, the value required by OP_CHECKHARDFORKVERIFY) and a fork height (, the block height at which the hardfork will diverge from legacy).
It should be noted, that an "uncooperative" hardfork would not change its consensus version, and in that regard, OP_CHECKBLOCKATHEIGHTVERIFY is superior.
In order to prepare my funds for splitting.  I pay to the below P2SH/P2WSH script:
  1 OP_CHECKHARDFORKVERIFY OP_DROP  OP_CHECKSIG
   OP_CHECKHARDFORKVERIFY OP_DROP  OP_CHECKSIG
In the above, I can then create transactions that spend the first branch on legacy chain post fork, and create transactions that spend the second branch on the hardfork chain post fork.  In addition, if I suddenly need to access the fund before the fork date, I can use the first branch to recover my funds before the fork date.
Suppose I wish to make a bet with another randomly generated Internet person, MX06fRH.  I am of the opinion, that the hardfork will not have economic consensus, whereas MX06fRH is of the opinion that it will.  We resolve to each bet 1 BTC.
We create a transaction spending our funds and paying a single combined output to the below P2SH/P2WSH:
  1 OP_EQUAL
  OP_IF
     OP_CHECKLOCKTIMEVERIFY OP_DROP
    1 OP_CHECKHARDFORKVERIFY OP_DROP
     OP_CHECKSIG
  OP_ELSE
     OP_CHECKHARDFORKVERIFY OP_DROP
     OP_CHECKSIG
  OP_ENDIF
  OP_DROP
  2   2 OP_CHECKMULTISIG
In the above, I can use  and an nLockTime transaction after the fork date to claim my legacy coin, if the legacy coin has value, wheres MRX06fRH can use  on the hardfork chain if it has value.  Alternatively, we can both agree to cancel the bet.
While the above is workable, it does not form a market where price discovery of legacy and hardfork future coins can be performed to determine consensus.  For that, we will need to use payment channel techniques.
I and MRX06fRH can form a payment channel that can trade fork futures by creating an anchor transaction paying to:
  1 OP_EQUAL
  OP_IF
     OP_CHECKLOCKTIMEVERIFY OP_DROP
    1 OP_CHECKHARDFORKVERIFY OP_DROP
    2   2 OP_CHECKMULTISIG
  OP_ELSE
     OP_CHECKHARDFORKVERIFY OP_DROP
    2   2 OP_CHECKMULTISIG
  OP_ENDIF
  OP_DROP
  2   2 OP_CHECKMULTISIG
Of course, before the anchor transaction is signed, we should create commitment transactions first.  For a Poon-Drjya channel, we should create two commitment transactions, one for me and one for MX06fRH, but in fact we should create two versions of both, one for the legacy token and one for the hardfork token (four commitment transactions).  The contracts are differentiated by which branch of the above we activate.  Commitment transactions can only be claimed after the fork, but we can update them continuously using normal Poon-Dryja channel operations.
If I have the same amount of legacy and hardfork tokens, then MX06fRH also has equal legacy and hardfork tokens, so we can agree to exit.
Unfortunately this can only reach up to payment channels.  To create a futures market we should have a payment channel network.  For Lightning we use HTLCs to create atomic swaps of coins in different payment channels.  However, the difference here is that commitment transactions can only be claimed after the fork, so any HTLCs on top of that will have expired before the commitment transactions can be claimed and the HTLCs enforced.  Unfortunately, I know of no other construction that would allow creation of a payment channel network on top of a payment channel primitive.  So much for OP_CHECKHARDFORKVERIFY.

@_date: 2017-10-10 22:48:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] New difficulty algorithm needed for SegWit2x 
Good morning Ben,
I am not Mark, and I am nowhere near being a true Core developer yet, but I would like to point out that even under a 51% attack, there is a practical limit to the number of blocks that can be orphaned.  It would still take years to rewrite history from the Genesis block, for instance.
What little data we have (BT1 / BT2 price ratio on BitFinex) suggests that tokens solely on the 2X chain will not be valued as highly as tokens solely on the Core chain.  As miners generate tokens that are only for a specific chain, they will have higher incentive to gain tokens on the Core chain rather than the 2X chain.
As is commonly said, hodling is free, whereas mining is not.  Hodlers have much greater power in hardfork situations than miners have: simply by selling their tokens on the 2X chain and not in the Core chain, hodlers can impose economic disincentives for mining of the 2X chain.
Miners can switch to BCH, but that is valued even less than BT2 tokens are, and thus even less attractive to mine on.
We should also pay attention, that BCH changed its difficulty algorithm, and it is often considered to be to its detriment due to sudden hashpower oscillations on that chain.  We should be wary of difficulty algorithm changes, as it is the difficulty which determines the security of the chain.
If we attempt to deploy a difficulty change, that is a hardfork, and hodlers will be divided on this situation.  Some will sell the tokens on the difficulty-change hardfork, some will sell the tokens on the non-difficulty-change hardfork.  Thus the economic punishment for mining the 2X chain will be diluted due to the introduction of the difficulty-change hardfork, due to splitting of the hodler base that passes judgment over development.
Thus, strategy-wise, it is better to not hardfork (whether difficulty adjustment, PoW change, or so on) in response to a contentious hardfork, as hodlers can remain united against or for the contentious hardfork.  Instead, it is better to let the market decide, which automatically imposes economic sanctions on miners who choose against the market's decision.  Thus, it is better to simply let 2X die under the hands of our benevolent hodlers.
Later, when it is obvious which fate is sealed, we can reconsider such changes (difficulty adjustment, PoW change, block size) when things are calmer.  However, such changes cannot be safely done in response to a contentious hardfork.
If indeed the Core chain is eradicated, then Bitcoin indeed has failed and I would very much rather sell my hodlings and find some other means to amuse myself.

@_date: 2017-10-12 06:40:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] New difficulty algorithm part 2 
Good morning,
Hodlers still have greater power than non-hodling users, whether miners or day-traders.
Hodlers holding millions in coin, total, can greatly drop price of any undesired hardfork.
Miners will prefer the coin with higher transaction fees as measured in real-world value.  Thus even if the unwanted chain provides 2 tokens as fee per block, whereas the wanted chain provides 1 token as fee per block, if the unwanted chain tokens are valued at 1/4 the wanted chain tokens, miners will still prefer the wanted chain regardless.  What the chains will compete in is the real-world value of the total mining reward.
Hodlers hodl all the cards here.
All this speculation seems to suggest to me simply that a difficulty change leads to keeping a chain alive unnecessarily, when by rights, it should be dead and laid to rest.
If Bitcoin needs some sudden change in difficulty algorithm to survive, then it has failed.  Feel free to do your own hardfork into yet another derivative altcoin.
Every hardfork is an invitation to shatter the community even further than it already is.  There is no need for further shattering.
The idea that an emergency hardfork to a different difficulty algorithm is necessary arises from a lack of understanding of just how much power hodlers have over the destiny of a coin.
Every hodler who rejects a hardfork, will sell the hardfork coin, increasing its supply and reducing its price.
Every miner who mines a rejected hardfork, creates new tokens in the hardfork, increasing its supply and reducing its price.
Coins that are hedl will remain hedl, and are not part of the supply.  Thus coins on the desired chain will remain high in price, regardless of available transaction rate.  The lack of freshly-minted coins also contracts the supply.
In the end, this will result in the same behavior as in BCH, where hodlers sodl the unwanted hardfork as quickly as they could.  Indeed, due exactly to miner support, 2X is much more likely to quickly drop in price than BCH did.  I am sure you have seen the images pointing out how easy it was to determine when BCH blocks arrived: they arrived a little before each dip in BCH price.  Fast 2X block rate will lead to faster 2X death, with its miners becoming bagholders.
As most Core developers hodl vast amounts, it is far more likely that any hardfork that goes against what Core wishes will collapse, simply by Core developers acting in their capacity as hodlers of Bitcoin, without needing to do any special action in their capacity as developers.

@_date: 2017-10-13 00:45:33
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] New difficulty algorithm part 2 
Good morning,
As of this moment, BT1 / BT2 price ratio in BitFinex is slightly higher than 7 : 1.  Twice the transaction rate cannot overcome this price ratio difference.  Even if you were to claim that the BitFinex data is off by a factor of 3, twice the transaction rate still cannot overcome the price ratio difference.  Do you have stronger data than what is available on BitFinex?  If not, your assumptions are incorrect and all conclusions suspect.
Mining infrastructure follows price.  If bitcoins were still trading at 1 USD per coin, nobody will build mining infrastructure to the same level as today, with 5000 USD per coin.
Price will follow user needs, i.e. demand.
For the very specific case of 2X, it is very easy to make this identification.  Even without understanding the work being done, one can reasonably say that it is far more likely that a loose group of 100 or more developers will contain a few good or excellent developers, than a group of a few developers containing a similar number of good or excellent developers.
User needs will get met only on the chain that good developers work on.  Bitcoin today has too many limitations: viruses on Windows can steal all your money, fee estimates consistently overestimate, fees rise during spamming attacks, easy to lose psuedonymity, tiny UTXOs are infeasible to spend, cannot support dozens of thousands of transactions per second.  Rationally, long-term hodlers will select a chain with better developers who are more likely to discover or innovate methods to reduce, eliminate, or sidestep those limitations.  Perhaps the balance will change in the future, but it is certainly not the balance now, and thus any difficulty algorithm change in response to the current situation will be premature, and far more likely to cause disaster than avert one.
This requires that all chains follow the same difficulty adjustment: after all, it is also entirely the possibility that 2X will be the lower-hashrate coin in a few months, with the Core chain bullying them out of existence.  Perhaps you should cross-post your analysis to bitcoin-segwit2x also.  After all, the 2X developers should also want to have faster price discovery of the true price of 2X, away from the unfavorable (incorrect?) pricing on BitFinex.
Are developers any wiser, either?
Then consider this wisdom: The fewer back-incompatible changes to a coin, the better.  Hardforks of any kind are an invitation to disaster and, at this point, require massive coordination effort which cannot be feasibly done within a month.  Fast market determination can be done using off-chain methods (such as on-exchange trades), and are generally robust against temporary problems on-chain, although admittedly there is a counterparty risk involved.  The coin works, and in general there is usually very little need to fix it, especially using dangerous hardforks.
Is that your goal?  This is a massive departure from the conception of Bitcoin as having a fixed limit and effectively becoming deflationary.  It will also lead to massive economic distortions in favor of those who receive newly-minted coins.  I doubt any developer would want to have this property.

@_date: 2017-09-05 04:21:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Sidechain headers on mainchain (unification of 
Good morning all,
I have started to consider a unification of drivechains, blind merged mining, and sidechain SPV proofs to form yet another solution for sidechains.
Briefly, below are the starting assumptions:
1.  SPV proofs are a short chain of sidechain block headers.  This is used to prove to the mainchain that some fund has been locked in the sidechain and the mainchain should unlock an equivalent fund to the redeemer.
2.  SPV proofs are large and even in compact form, are still large.  We can instead use miner voting to control whether some mainchain fund should be unlocked.  Presumably, the mainchain miners are monitoring that the sidechain is operating correctly and can know directly if a side-to-main peg is valid.
3.  To maintain mainchain's security, we should use merged mining for sidechain mining rather than have a separate set of miners for mainchain and each sidechain.
4.  A blockchain is just a singly-linked list.  Genesis block is the NULL of the list.  Additional blocks are added at the "front" of the singly-linked list.  In Bitcoin, the Merkle tree root is the "pointer to head" and the previous block header ID is the "pointer to tail"; additional data like proof-of-work nonce, timestamp, and version bits exist but are not inherent parts of the blockchain linked list.
5.  In addition to SPV proofs, we should also support reorg proofs.  Basically, a reorg proof is a longer SPV proof that shows that a previous SPV proof is invalid.
With those, I present the idea, "sidechain headers in mainchain".
Let us modify Sztorc's OP_BRIBEVERIFY to require the below SCRIPT to use:
   OP_BRIBEVERIFY OP_DROP OP_DROP OP_DROP
We also require that  be filled only once per mainchain block, as per the "blind" merge mining of Sztorc.
The key insight is that the  and  are, in fact, the sidechain header.  Concatenating those data and hashing them is the block header hash.  Just as additional information (like extranonce and witness commitment) are put in the mainchain coinbase transaction, any additional information that the sidechain would have wanted to put in its header can be committed to in the sidechain's equivalent of a coinbase transaction (i.e. a sidechain header transaction).
(All three pieces of data can be "merged" into a single very long data push to reduce the number of OP_DROP operations, this is a detail)
Thus, the sidechain header chain (but not the block data) is embedded in the mainchain itself.
Thus, SPV proofs do not need to present new data to the mainchain.  Instead, the mainchain already embeds the SPV proof, since the headers are already in the mainchain's blocks.  All that is needed to unlock a lockbox is to provide some past sidechain header hash (or possibly just a previous mainchain block that contains the sidechain header hash, to make it easier for mainchain nodes to look up) and the Merkle path to a sidechain-side side-to-main peg  transaction.  If the sidechain header chain is "long enough" (for example, 288 sidechain block headers) then it is presumably SPV-safe to release the funds on the mainchain side.
Suppose a sidechain is reorganized, while a side-to-main peg transaction is in the sidechain that is to be reorganized away.
Let us make our example simpler by requiring an SPV proof to be only 4 sidechain block headers.
In the example below, small letters are sidechain block headers to be reorganized, large letters are sidechain block headers that will be judged valid.  The sidechain block header "Aa" is the fork point.  b' is the sidechain block containing the side-to-main peg that is lost.
Remember, for each mainchain block, only a single sidechain block header for a particular sidechain ID can be added.
The numbers in this example below are mainchain block height numbers.
0: Aa
1: b'
2: c
4: C
5: d
6: D
7: E
8: F
9: G
10: H <- b' side-to-main is judged as "not valid"
Basically, in case of a sidechain fork, the mainchain considers the longest chain to be valid if it is longer by the SPV proof required length.  In the above, at mainchain block 10, the sidechain H is now 4 blocks (H,G,F,E) longer than the other sidechain fork that ended at d.
Mainchain nodes can validate this rule because the sidechain headers are embedded in the mainchain block's coinbase.  Thus, mainchain fullnodes can validate this part of the sidechain rule of "longest work chain".
Suppose I wish to steal funds from sidechain, by stealing the sidechain lockboxes on the mainchain.  I can use the OP_BRIBEVERIFY opcode which Sztorc has graciously provided to cause miners that are otherwise uninterested in the sidechain to put random block headers on a sidechain fork.  Since the mainchain nodes are not going to verify the sidechain blocks (and are unaware of sidechain block formats in detail, just the sidechain block headers), I can get away with this on the mainchain.
However, to do so, I need to pay OP_BRIBEVERIFY multiple times.  If our rule is 288 sidechain blocks for an SPV proof, then I need to pay OP_BRIBEVERIFY 288 times.
This can then be used to reduce the risk of theft.  If lockboxes have a limit in value, or are fixed in value, that maximum/fixed value can be made small enough that paying OP_BRIBEVERIFY 288 times is likely to be more expensive than the lockbox value.
In addition, because only one sidechain header can be put for each mainchain header, I will also need to compete with legitimate users of the sidechain.  Those users may devote some of their mainchain funds to keep the sidechain alive and valid by paying OP_BRIBEVERIFY themselves.  They will reject my invalid sidechain block and build from a fork point before my theft attempt.
Because the rule is that the longest sidechain must beat the second-longest chain by 288 (or however many) sidechain block headers, legitimate users of the sidechain will impede my progress to successful theft.  This makes it less attractive for me to attempt to steal from the sidechain.
The effect is that legitimate users are generating reorg proofs while I try to complete my SPV proof.  As the legitimate users increase their fork, I need to keep up and overtake them.  This can make it unattractive for me to steal from the sidechain.
Note however that we assume here that a side-to-main peg cannot occur more often than an entire SPV proof period.
Suppose I am a major power with influence over >51% of mainchain miners.  What happens if I use that influence to cause the greatest damage to the sidechain?
I can simply ask my miners to create invalid side-to-main pegs that unlock the sidechain's lockboxes.  With a greater than 51% of mainchain miners, I do not need to do anything like attempt to double-spend mainchain UTXO's.  Instead, I can simply ask my miners to operate correctly to mainchain rules, but violate sidechain rules and steal the sidechain's lockboxes.
With greater than 51% of mainchain miners, I can extend my invalid sidechain until we reach the minimum necessary SPV proof.  Assuming a two-way race between legitimate users of the sidechain and me, since I have >51% of mainchain miners, I can build the SPV proof faster than the legitimate users can create a reorg proof against me.  This is precisely the same situation that causes drivechain to fail.
An alternative is to require that miners participating in sidechains to check the sidechain in full, and to consider mainchain blocks containing invalid sidechain headers as invalid.  However, this greatly increases the amount of data that a full miner needs to be able to receive and verify, effectively increasing centralization risk for the mainchain.
The central idea of drivechain is simply that miners vote on the validity of sidechain side-to-main pegs.  But this is effectively the same as miners - and/or OP_BRIBEVERIFY users - only putting valid sidechain block headers on top of valid sidechain block headers.  Thus, if we instead use sidechain-headers-on-mainchain, the "vote" that the sidechain side-to-main peg is valid, is the same as a valid merge-mine of the sidechain.
SPV proofs are unnecessary in drivechain.  In sidechain-header-on-mainchain, SPV proofs are already embedded in the mainchain.  In drivechain, we ask mainchain fullnodes to trust miners.  In sidechain-header-on-mainchain, mainchain fullnodes validate SPV proofs on the mainchain, without trusting anyone and without running sidechain software.
To validate the mainchain, a mainchain node keeps a data structure for each existing sidechain's fork.
When the sidechain is first created (perhaps by some special transaction that creates the sidechain's genesis block header and/or sidechain ID, possibly with some proof-of-burn to ensure that Bitcoin users do not arbitrarily create "useless" sidechains, but still allowing permissionless creation of sidechains), the mainchain node creates that data structure.
The data structure contains:
1.  A sidechain block height, a large number initially 0 at sidechain genesis.
2.  A side-to-main peg pointer, which may be NULL, and which also includes a block height at which the side-to-main peg is.
3.  Links to other forks of the same sidechain ID, if any.
4.  The top block header hash of the sidechain (sidechain tip).
If the sidechain's block header on a mainchain block is the direct descendant of the current sidechain tip, we just update the top block header hash and increment the block height.
If there is a side-to-main peg on the sidechain block header, if the side-to-main peg pointer is NULL, we initialize it and store the block height at which the side-to-main peg exists.  If there is already a pending side-to-main peg, the mainchain block is judged invalid; thus for a 288-block region only one side-to-main peg can be done.
If, for a mainchain block, the sidechain header does NOT extend the most recent sidechain tip, we have detected a sidechain split condition.  We then create a copy of the data structure for the tallest fork, then roll it back until we reach the split point; this rollback should also clear the side-to-main pointer, if we rollback to a blockheight below the side-to-main peg.  Rollback is delimited: if after 288 sidechain headers we have not found the split point, the mainchain node rejects the mainchain block as invalid.  Thus new sidechain forks cannot be started further back than our SPV proof size on the current longest sidechain.  This allows powerful individuals to kill the sidechain by spending sufficient OP_BRIBEVERIFY to put random numbers on the sidechain headers, preventing the sidechain from ever operating correctly unless the sidechain accepts this loss of valid headers specially.
If there is only a single such structure for a sidechain, the sidechain is single-chained and not under attack.  If the side-to-main peg pointer is non-null and the block height of the sidechain is 288 higher than the recorded block height, the side-to-main peg is added as a UTXO to our UTXO set.
If there are multiple such structures, the sidechain is in a contentious chainsplit condition (the "under attack" flag).  A side-to-main peg is valid (becomes a UTXO) only if it exists (i.e. is the same) on all forks of the sidechain, and the shortest fork is 288 higher than the side-to-main peg height.  This allows side-to-main pegs occurring before a contentious fork to be redeemed.
When there are multiple forks of a sidechain, the mainchain node keeps track of all of them.  It sorts these forks by blockheight.  The tallest chain is the reference.  If some fork has height less than the height of the tallest chain minus 288 (the SPV proof size), the mainchain node drops it.  Then that sidechain fork can no longer be extended; if it is the only fork lacking a particular side-to-main peg that exists on all the other forks, then the side-to-main peg becomes a UTXO.  Once all contentious forks have been dropped, the sidechain returns to normal operation.
Note that this implies that side-to-main pegs that occur after a contentious sidechain fork will be delayed.  This is to be expected as there is contention as to which chain is correct.
Note that this implies that sidechains must "run in lockstep" with mainchain.  In particular, if the mainchain splits, the sidechain also splits.  This allows two-way pegs to be asymmetrical, with sidechain fullnodes also being mainchain fullnodes, and immediate main-to-side pegs.
Finally, mainchain fullnodes validate side-to-main transfers, but do not need to run sidechain software.  Users of sidechains are expected to protect themselves by ensuring they have mainchain miners that will do their best to protect the sidechain by only extending the valid longest sidechain, and by spending maincoin on OP_BRIBEVERIFY.
Suppose I want to use a sidechain, but I worry some troll will want to attack the sidechain via the OP_BRIBEVERIFY vulnerability.  What can I do to protect my investment?
Perhaps a "protector" can be hired.  Such a protector will be paid in sidechain funds, at a premium, for use of its mainchain funds.  Such a protector will then use OP_BRIBEVERIFY on the mainchain to ensure a valid sidechain is extended on the mainchain.  Indeed, this is exactly the "sidechain miner" envisioned by Sztorc in blind merged mining: sidechain users offer a sidecoin fee to these protectors, who spend maincoin to perform OP_BRIBEVERIFY on the mainchain.  These protectors are paid in sidecoin in order to encourage them to protect the sidechain; they cannot spend sidecoin if the sidechain is successfully attacked.
Mainchain miners who wish to take on "protector" role can simply act as if they are being paid OP_BRIBEVERIFY for the sidechain they wish to protect.  However, in particular mainchain miners should not treat sidechain rules at the same level as mainchain rules: even if a sidechain block header is judged to be invalid, the mainchain miner should not reject the mainchain block.  It can only refuse to build a sidechain block header on top of an invalid sidechain block header.  Only if a sidechain is sufficiently in use can we propose the sidechain's rules to be added to mainchain as mainchain rules in a softfork.  Needless to say, miners taking on this role must have even larger datacenters in order to handle the increased bandwidth, storage, and processing load to handle both mainchain mining and sidechain protection.
The number 288 in all cases is a parameter that can be endlessly debated.

@_date: 2017-09-05 19:32:17
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Sidechain headers on mainchain (unification of 
Good morning Chris,
This is exactly the problem, and one which exists in a different form in any sidechain proposal.  In drivechain, malicious mainchain miners may arbitrarily downvote any side-to-main peg even if the side-to-main peg is valid on the sidechain, with mainchain fullnodes unable to gainsay them.  In original sidechain's SPV proofs, malicious mainchain miners may provide an invalid SPV proof and then censor any reorg proof against that SPV proof.  In both of those cases, trust in the sidechain and the value of sidecoin necessarily takes a hit.
Of course, in both of those two cases, the hit is "temporary" and the sidechain could theoretically recover.  In sidechain-headers-on-mainchain, the hit would permanently kill the sidechain.
The fact that sidechains are merge mined and cannot be mined off-mainchain makes sidechains entirely dependent on mainchain miner's support.  I agree with Sztorc that sidechains must be merge mined entirely, otherwise the sidechain will effectively reduce mainchain security by pulling away potential miners from mainchain.
OP_BRIBEVERIFY, which is intended to allow sidechain miners/protectors to be a separate datacenter from miners, allows anyone with either enough hashpower or enough maincoin to disrupt a sidechain by spamming its slot with random hash values.  With enough disruption, the sidechain may become unusable in drivechains, but may indeed be killed that way in sidechain-headers-on-mainchain.
Yes, this seems sensible.
Even without sidechain headers on mainchain, one might consider plain blind merged mining to have put even the "previous block hash" in the sidechain block coinbase transaction.  Thus, one might consider that in blind merged mining, h' commitments are really merkle tree roots, and the previous block hash is encoded in a special sidechain transaction on one side of the merkle tree, while sidechain block transactions are encoded in the other side of the merkle tree root.  This allows OP_WITHDRAWPROOFVERIFY to be used on blind merged mining, but without sidechain headers on mainchain, a compact SPV proof somehow must still be provided, or we are forced to use drivechain miner voting.

@_date: 2017-09-10 01:32:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: Sidechain headers on mainchain (unification 
Good morning Paul,
Thank you for your consideration.
I don't see how this regress occurs.  Perhaps I need more information on extension blocks.
Any miner that rejects a bribe from outside the miner-group in order to put their desired hash on the sidechain, values their desired hash more than the bribe to put a different hash.  This rejection is a loss of potential proift, and other miners who accept the bribe gain the profit from it.
Your last paragraph does not make sense to me.  I suspect I have hit upon a nerve and will make no further comment on this sub-topic.
This is indeed the problem.  SHOM, as it unifies merge mining and WT^ voting, also allows theft attempts, and once the money available for withdrawal exceeds the sum of 288 bribes, we enter a dollar auction game between the thief and the sidechain users: As thieves are expected to follow the simple greedy algorithm, sidechain death can be triggered by a single theft attempt.
Assuming potential thieves understand the dollar-auction irrationality, they may be disincentivized, as presumably there are more sidechain protectors than thieves, and the sidechain protectors can (we hope) all outbid the thief.  But the problem is that this require rational behavior from thieves.  Mere greedy algorithm, or disruption for the sake of disruption, would still collapse SHOM sidechains.
But given the many parallels between SHOM and drivechains: what happens if 26% of miners disrupt all sidechains by always downvoting WT^?  In that case, sidechains still collapse as a whole, with practically the same effect as the SHOM thief.
We could limit the money available for withdrawal, but that weakens the side-to-main peg, reducing the value of the sidecoin relative to the maincoin.
The problem, to my mind, is that blind merge mining is pointless if it does not also allow voting on WT^.  In the end, no matter how novel a sidechain may be, what is valued is the maincoin backing the sidecoin; that is the whole point of the two-way peg.  A sidechain user may OP_BRIBEVERIFY valid sideblocks onto the mainchain, but if that user cannot vote on WT^ anyway, no matter how valid sideblocks committed on the mainchain, it would be pointless if the sidechain is attacked by mainchain miners.  You may as well remove blind merge mining, as miners who must vote on WT^ will need to understand the sidechain validity rules anyway.
I do not quite follow.  Can you expand more on this?
Thank you.
In order to attack multiple sidechains, bribing thieves must pay bribes for each sidechain being attacked.  Even if a miner attacks, bribes for valid sidechains must be rejected by the miner, effectively reducing the miner's profits, and the bribes to be rejected must be for all the sidechains to be attacked.
If withdrawals have a fixed or maximum value, then the bribe a thief must be prepared to pay (or turn down, in the case of thieving miners) must be no more than the maximum value / 288.
Unfortunately, capping withdrawals weakens the side-to-main peg, which weakens the reason for even using SHOM.  This is the true weakness of SHOM: it provides only a very weak side-to-main peg.
I agree.
I am fine with some economic bond or proof-of-burn to start a sidechain.  But I am opposed to any permissioned method of starting sidechains.  To my mind, asking miners to install your software is already permissioned.
Thank you.

@_date: 2017-09-15 00:01:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hypothetical: Could soft-forks be prevented? 
Good morning Dan,
My understanding is that it is impossible for soft forks to be prevented.
1.  Anyone-can-spend
There are a very large number of anyone-can-spend scripts, and it would be very impractical to ban them all.
For example, the below output script is anyone-can-spend
  OP_TRUE
So is the below:
  OP_SIZE  OP_EQUAL
  OP_1ADD  OP_EQUAL
  OP_BOOLAND
  OP_BOOLOR
And so on.
So no, it is not practically possible to ban anyone-can-spend outputs, as there are too many potential scriptPubKey that anyone can spend.
It is even possible to have an output that requires a proof-of-work, like so:
 OP_HASH256  OP_LESSTHAN
All the above outputs are disallowed from propagation by IsStandard, but a miner can put them validly in a block, and IsStandard is not consensus code and can be modified.
2.  Soft fork = restrict
It is possible (although unlikely) for a majority of miners to run soft forking code which the rest of us are not privy to.
For example, for all we know, miners are already blacklisting spends on Satoshi's coins.  We would not be able to detect this at all, since no transaction that spends Satoshi's coins have been broadcast, ever.  It is thus indistinguishable from a world where Satoshi lost his private keys.  Of course, the world where Satoshi never spent his coins and miners are blacklisting Satoshi's coins, is more complex than the world where Satoshi never spent his coins, so it is more likely that miners are not blacklisting.
But the principle is there.  We may already be in a softfork whose rules we do not know, and it just so happens that all our transactions today do not violate those rules.  It is impossible for us to know this, but it is very unlikely.
Soft forks apply further restrictions on Bitcoin.  Hard forks do not.  Thus, if everyone else is entering a soft fork and we are oblivious, we do not even know about it.  Whereas, if everyone else is entering a hard fork, we will immediately see (and reject) invalid transactions and blocks.
Thus the only way to prevent soft fork is to hard fork against the new soft fork, like Bcash did.
-------- Original Message --------
UTC Time: September 13, 2017 9:50 AM
Hi, I am interested in the possibility of a cryptocurrency software
(future bitcoin or a future altcoin) that strives to have immutable
consensus rules.
The goal of such a cryptocurrency would not be to have the latest and
greatest tech, but rather to be a long-term store of value and to offer
investors great certainty and predictability... something that markets
tend to like. And of course, zero consensus rule changes also means
less chance of new bugs and attack surface remains the same, which is
good for security.
Of course, hard-forks are always possible. But that is a clear split
and something that people must opt into. Each party has to make a
choice, and inertia is on the side of the status quo. Whereas
soft-forks sort of drag people along with them, even those who oppose
the changes and never upgrade. In my view, that is problematic,
especially for a coin with permanent consensus rule immutability as a
As I understand it, bitcoin soft-forks always rely on anyone-can-spend
transactions. If those were removed, would it effectively prevent
soft-forks, or are there other possible mechanisms? How important are
any-one-can spend tx for other uses?
More generally, do you think it is possible to programmatically
avoid/ban soft-forks, and if so, how would you go about it?
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-09-15 00:34:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fw: Re: Sidechain headers on mainchain (unification 
Good morning,
I'm re-sending this message below as it appears to have gotten lost before it reached cc: bitcoin-dev.
Paul even replied to it and the reply reached on-list, so I'm re-sending it as others might have gotten confused about the discussion.
So far I've come to realize that sidechain-headers-on-mainchain/SHOM/SHM/driveproofs creates a very weak peg, and that only sidechain-only miners can take advantage of this weak peg.  This is because, the fee paid by sidechain-only miners to mainchain miners will approach TRANSFERLIMIT / 288 to protect against theft, and then sidechain miners will be unable to replenish their maincoin stock (to pay for the blind-merge-mine) if they do not transfer *only* their sidecoins earned.
-------- Original Message --------
UTC Time: September 8, 2017 2:56 PM
Bitcoin Protocol Discussion Good morning,
Chris mentioned the use of OP_WITHDRAWPROOFVERIFY.  I've come to realize
that this is actually superior to use OP_WITHDRAWPROOFVERIFY with a
sidechain-headers-on-mainchain approach.
Briefly, a payment to OP_WITHDRAWPROOFVERIFY is an instruction to transfer
value from the mainchain to a sidechain.  Thus, a payment to
OP_WITHDRAWPROOFVERIFY includes the sidechain to pay to, and a commitment
to a sidechain address (or whatever is the equivalent to a sidechain
Various OP_WITHDRAWPROOFVERIFY explanations exist.  Most of them include
OP_REORGPROOFVERIFY.  With sidechain-headers-on-mainchain, however, there is
no need for reorg proofs.  This is because, the mainchain can see, in real
time, which branch of the sidechain is getting extended.  Thus if someone
attempts to defraud a sidechain by forking the sidechain to an invalid
state, sidechainers can immediately detect this on the mainchain and
immediately act to prevent the invalid fork from being advanced.  After
all, a reorg proof is really just an SPV proof that is longer than some
previous SPV proof, that shows that the previous SPV proof is incorrect,
by showing that the block at the specified height of the WT is not present
on a longer SPV proof.
Since sidechain-headers-on-mainchain implies merge mining of sidechains,
with no option to have independent proof-of-work of sidechains, the
sidechain's entire history is recorded on the mainchain, visible to all
mainchain nodes.
An advantage of sidechain-headers-on-mainchain is a side-to-side peg without
passing through the mainchain.
That is, a 2-way peg between any two chains, whether side or main.
Sidechains supporting side-to-side transfer would require supporting
OP_WITHDRAWPROOFVERIFY, but not any of the other parts of sidechains.
We must consider a WT format (withdrawal transaction) that is compatible
with an OP_WITHDRAWPROOFVERIFY Bitcoin transaction.
***That is, a lockbox UTXO on one chain is a WT on another chain.***
Sidechains need not follow the mainchain format for its normal
transactions, only for WT transactions that move coins across chains.
For this, mainchain should also have its own "sidechain ID".  Perhaps a
sidechain ID of 0 would be appropriate for mainchain, as its status as
Suppose we have two sidechains, Ess and Tee, both of which support
side-to-side pegs.
An Ess fullnode is a Bitcoin fullnode, but an Ess fullnode is not
necessarily a Tee fullnode, and vice versa.
A lockbox redemption in sidechain-headers-on-mainchain is simply a spend of
a lockbox, pointing to the sidechain header containing WT, the merkle tree
path to the WT transaction from the h* commitment of the header, the output
which locks, and so on as per usual OP_WITHDRAWPROOFVERIFY.
Then a sidechain can create tokens from nothing, that are locked in a
OP_WITHDRAWPROOFVERIFY lockbox; this is the only way to create sidecoin.
When transferring into a sidechain from mainchain, or anywhere, the
sidechain either creates tokens locked into OP_WITHDRAWPROOFVERIFY, or
looks for an existing UTXO with OP_WITHDRAWPROOFVERIFY from the source
chain and spends them (the latter is preferred as it is fewer
transactions and less space on the sideblock, reducing sidechain fees).
OP_WITHDRAWPROOFVERIFY on a sidechain would query the mainchain fullnodes.
Whatever rules allow lockbox unlocking on mainchain, will also be the same
rules that allow lockbox unlocking on sidechains.
A mainchain RPC can even be made to simplify sidechain verification of
side-to-side pegs, and to ensure that sidechains follow the same consensus
rules for OP_WITHDRAWPROOFVERIFY.
So if we want transfer TeeCoin to EssCoin, we spend into a
OP_WITHDRAWPROOFVERIFY lockbox on Teechain pointing to Esschain (i.e. a
Tee->Ess lockbox).  This lockbox is itself a WT from the point of view of
Esschain.  On Esschain, we look for an existing Ess->Tee lockbox, or
create a Ess->Tee lockbox of our own for a EssCoin fee.  Then we create a
spend of the Ess->Tee lockbox on Esschain, wait until spending is
possible, and then post that transaction on Esschain.
Again, with sidechain-headers-on-mainchain, reorg proofs are unnecessary,
since any invalid chain should be quickly buried by a valid chain,
unless the economic majority decides that a sidechain is not worth
All is not well, however.  Remember, on a sidechain, we can create new
sidecoin for free, provided they are in a lockbox.  Unlocking that
lockbox would require a valid WT on the chain that the lockbox is
dedicated to.  However, a lockbox on one chain is a WT on the other
chain.  We can create a free lockbox on Ess, then use that lockbox as
a WT on Tee, inflating TeeCoin.
Instead, we add an additional parameter, wtFlag, to
This parameter is ignored by OP_WITHDRAWPROOFVERIFY opcode.
However, this parameter is used to determine if it is a WT.  Sidechain
consensus should require that freely-created lockboxes set this
parameter to 0, so that a side block that creates free lockboxes where
this parameter is non-zero is an invalid side block.  Then a sidechain
will only treat a lockbox on another chain as a WT if the wtFlag
parameter is nonzero.  This way, freely-created lockboxes are not
valid WT.  Valid WT must lock actual, already unlocked coins, not
create new locked coins.
On Bitcoin, of course, this parameter must always be nonzero, since
freely-created lockboxes are not allowed on mainchain, as asset
issuance on mainchain is already fixed.
Let us now flesh out how WT and lockboxes look like.  As we mentioned, a
lockbox on one chain is a WT on the destination chain.  Or to be more
precise, what a destination chain sees as a WT, is a lockbox on the source
Thus, a lockbox is a Bitcoin-formatted transaction output paying to the
    OP_WITHDRAWPROOFVERIFY
(assuming a softfork, additional OP_DROP operations may occur after
Suppose the above lockbox is paid to in the Bitcoin mainchain, with the
sidechain ID being the ID of Esschain.  This is itself a WT transaction
from the point of view of Esschain, on the principle that a lockbox on
one chain is a WT on another chain.
Assuming Esschain is a brand-new sidechain, it has no EssCoins yet.  The
sidechain allows the arbitrary creation of sidecoin provided the new
sidecoins are in a lockbox whose sidechain address commitment is 0.  So
in Esschain, we create the same coins on a UTXO paying to the
  0 0 OP_WITHDRAWPROOFVERIFY
The first 0 is the sidechain address commitment, which is 0 since this
output was not created by transferring to a sidechain; we
reuse the sidechain address commitment as the wtFlag.  The
second 0 is the mainchain's ID.  The above is a lockbox from the point of
view of Esschain.  It is not a WT on mainchain, however, because the
sidechain address commitment is 0, which we use also as the wtFlag
Now, how does a main-to-side peg work?  After creating the above output on
Esschain, we now spend the output with the below scriptSig:
On Esschain, OP_WITHDRAWPROOFVERIFY then verifies that the mainchain block
hash is a valid past block of the mainchain, then locates the mainchain
header.  It then checks the merkle tree path to the mainchain WT
confirming that the mainchain contains that transaction, and confirms that
indicated output is in fact, a payment to an OP_WITHDRAWPROOFVERIFY, which
pushes the Esschain ID, and with a nonzero sidechain address commitment.
(Esschain also needs to ensure that a single WT is not used to unlock
multiple lockboxes on Esschain; the easiest way is to add it to a set,
but this set cannot be pruned; other ways of ensuring only a WT is only
used to unlock once might be designed)
On Esschain, the sidechain does one final check: the transaction that spends
an OP_WITHDRAWPROOFVERIFY must have an output that pays to the sidechain
address committed to, and that output's value must be the same as the value
locked in the mainchain.
(for now, I think all lockboxes must have the same fixed amount, for
Now suppose we want to convert back our EssCoin to Bitcoin.  We create a
lockbox on Esschain, paying to the below:
   0 OP_WITHDRAWPROOFVERIFY
The bitcoin P2SH address is mainchain address commitment; for simplicity
we just use P2SH on mainchain as it can encode any address.  The 0 is the
mainchain ID.  The above Esschain lockbox is itself a WT from Esschain to
Then, we look for an unspent lockbox on Esschain whose sidechain ID is the
Esschain ID.  Note that we can select any lockbox with the correct
sidechain ID, regardless of the sidechain address commitment it may have.
Locating an appropriate mainchain lockbox for Esschain coins, we then
provide the below scriptSig, paying out to the bitcoin P2SH address we
On mainchain, we check that the indicated sidechain block header hash is a
block header on the longest chain of Esschain.  We check it has sufficient
depth.  Then we check if the merkle path to the WT tx is correct and goes
to esschain WT tx.  Finally, we check the indicated output ID, and check that
it is indeed an Esschain lockbox dedicated to mainchain.  Finally, we check
that the transaction has an output that spends the lockbox amount to the
specified bitcoin P2SH address.
(similarly mainchain nees to ensure that the Esschain WT is only used
The key insight here is that side-to-side pegs are just like side-to-main
pegs.  Suppose instead we want to transfer our coins from Esscoin to
Teecoin.  We would instead pay to the following lockbox on Esschain:
    OP_WITHDRAWPROOFVERIFY
Then a Teechain transaction spending some Tee->Ess lockbox (or a fresh
lockbox if there are no Tee->Ess lockboxes on Teechain) is created.
We proceed as if it were a side-to-main peg, except it is a peg to
Teechain, either creating or unlocking TeeCoins.  Indeed, mainchain
fullnodes may even provide an RPC for checking OP_WITHDRAWPROOFVERIFY,
so as to reduce risk that a sidechain breaks consensus due to buggy
All is still not well with side-to-side pegs, however.
Suppose the economic majority decides that Esschain must die.  Perhaps it
has some irrecoverable security bug, perhaps it adds features that allow
Esschain fullnodes to kill baby seals, perhaps a successful theft of
Esschain lockboxes was performed and Esscoins are now functionally
worthless.  Killing a sidechain is done by bribing miners to put invalid
values into h*, and thus stealing Bitcoin->Ess lockboxes.
If Esschain dies, however, and the economic majority is not prepared to keep
Esschain dead, it is possible to unlock Tee->Ess lockboxes on Teechain.
Unlocking existing Tee->Ess lockboxes on Teechain is safe, since they
represent coins that were locked into Bitcoin->Tee lockboxes.  However,
it is still possible to create "free" Tee->Ess lockboxes on Teechain, then
provide an invalid Tee->Ess WT lockbox on the now-moribund Esschain to
unlock the free Tee->Ess lockbox on Teechain, inflating TeeCoin value.
Thus in the presence of side-to-side pegs, the death of even one sidechain
represents the death of every other sidechain!
Thus, to properly kill Esschain, the economic majority should spam the
Esschain headers slot with a fixed value, say 0, forever.  This makes it
very difficult to create a Tee->Ess WT lockbox on Esschain, as you would
now be able to reverse a one-way hash function.
Alternatively, Teechain can softfork so that Tee->Ess lockboxes are no
longer creatable or spendable.  However, the death of Esschain requires
that all other sidechains, including Youchain, Veechain, Dubyachain, and
so on, to softfork similarly.
Perhaps both can be done: first the economic majority wanting to kill
Esschain starts spamming it with invalid spends of Bitcoin->Ess lockboxes,
then when all Bitcoin->Ess lockboxes have been stolen, spam it with 0s
until all other sidechains have banned free Ess lockboxes on their chains.
Then, the economic majority can leave Esschain dead, and a later softfork
of mainchain prevents Esschain from being extended and allows mainchain
fullnodes to prune Esschain headers.
Thieves will still have the same difficulty stealing from sidechains, but
now their payoff is increased.  If a thief wants to steal Esschain
lockboxes, then it is possible to pack an invalid Esschain block full of
invalid WT to other chains.  Even chains that don't have lockboxes to
Esschain can create lockboxes to Esschain for free.  Thus, instead of
stealing only one lockbox at a time on mainchain, the thief can steal one
lockbox on mainchain, and on every sidechain that supports side-to-side
pegs, at a time.  The risk/reward ratio may shift drastically in that case.
However, this does mean that users of one chain must pay attention to
attacks on other chains, not just the chain they use.  If Teechain has no
side-to-side pegs, then Teechain users will not care if Esschain is under
attack.  But if side-to-side pegs are allowed on Teechain, then Teechain
users must also care about Esschain's health, as well as the health of
every other sidechain in existence.  Mainchain is protected since free
lockboxes are not creatable on mainchain.  Each sidechain is not; thus
the user of any sidechain must also stand by users of every other
sidechain, or else they all fall apart.  Of course, this could more
simply lead to "I will not use Teechain even if it would be useful to me,
because if I use Teechain, I have to care about Esschain and Youchain and
Side-to-side pegs are useful to allow better liquidity and provide
arbitrage quickly between sidechains, without having to pass through
mainchain.  Otherwise, Esscoin may be valued slightly lower than Bitcoin,
then Teecoin valued slightly higher than Bitcoin, creating a larger
difference between Esscoin and Teecoin values than what a full
side-to-side peg could support.  2-way pegs from mainchain
to sidechain stabilize sidecoin with respect to maincoin.  Side-to-side
pegs stabilize all sidecoins to all other sidecoins.
Side-to-side pegs are enabled implicitly by sidechain-headers-on-mainchain,
as all sidechain fullnodes must necessarily be mainchain fullnodes, and
any mainchain fullnode can judge the validity of any WT from any sidechain
without a miner voting period.
Side-to-side pegs are a generalization of main-to-side and side-to-main
pegs.  A sidechain can simply implement OP_WITHDRAWPROOFVERIFY and allow
free lockboxes, and that is sufficient for the sidechain to support
imports of bitcoin from mainchain and from any other sidechain.
Side-to-side pegs seem to imply that all pegs must have the same bitcoin
value transferred.  What that value must be, is something that may be
debated endlessly.
A side-to-side peg is a cut-through of a side-to-main peg from
one sidechain into a main-to-side peg into another sidechain.  If a
withdrawal from side-to-main peg would be accepted by mainchain, then
another sidechain could, in principle, accept a proof that would
authorize a side-to-main peg directly as a side-to-side peg.
Side-to-side pegs make attacks on sidechains more lucrative, as it
becomes possible to print sidecoins by successfully attacking a
different sidechain.
Drivechain cannot implement side-to-side pegs, as WT validity is
voted on by mainchain miners, and asking mainchain miners about
side-to-side pegs requires mainchain miners to be aware of both
sidechains.  Sidechain-headers-on-mainchain publishes SPV proofs
continuously to the mainchain, and since any sidechain fullnode is
also a mainchain fullnode (since sidechains are mergemined), then
every sidechain fullnode is automatically capable of accessing
and verifying SPV proofs for every other sidechain.
However, the pegging here seems less flexible than the pegging
supported by drivechain.  Drivechain lets pegs be any size, with
miner voting being the basis of knowing how much money is owned
by whom.

@_date: 2017-09-15 23:38:23
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hypothetical: Could soft-forks be prevented? 
Good Morning Dan,
Let us suppose that IsStandard is applied to outputs, but we support P2SH. Then we could encode those scripts in P2SH. The softfork could require the script preimageto be put elsewhere, such as an OP_RETURN in the same tx, to determine the script that is anyone can spend.
We could ban P2SH or restrict P2SH to be IsStandard also, but you are now unable to support HTLC (no atomic swap) or LN, unless you specifically add those scripts to IsStandard. And if a better layer 2 comes along or LN is updated to use better scripts, you have to hardfork those in.
Even then, you can still be softforked. Remember that if we pay to a P2PKH, then publish the private key, every output paying to that address is now practically anyone can spend. Then a softfork can implement desired rules in an extensiom block, where money in UTXOs paying to the special publicized "private" key are controlled, post softfork, by data in a block that is not published to pre softfork nodes, like witness data is treated in SegWit.
Sent with [ProtonMail]( Secure Email.

@_date: 2017-09-22 21:49:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Sidechains: Mainstake 
Good morning bitcoin-dev,
I have yet another sidechain proposal: I make the below outlandish claims in the above link:
1. While a 51% mainchain miner theft is still possible, it will take even longer than in drivechains (either months of broadcasting intent to steal before the theft, or locking funds that are likely to remain locked after a week-long theft).
2. A 26% anti-sidechain miner cannot completely block all sidechain withdrawals as they could in drivechains.
3. Outside of attacks and censorship, the economic majority controls sidechains, without going through miners as "representatives of the economic majority".
4. With sufficient cleverness (stupidity?), proof-of-stake can be made to work.
I hope for your consideration.  I suspect that I have not thought things out completely, and probably missed some significant flaw.

@_date: 2017-09-25 19:34:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] idea post: trimming and demurrage 
Good morning Patrick,
Demurrage is simply impossible.
In Bitcoin we already have implemented OP_CHECKLOCKTIMEVERIFY.
This opcode requires that a certain block height or date has passed before the output can be spent.
It can be used to make an "in trust for" address, where you disallow spending of that address.  For example, you may have a child to whom you wish to dedicate some inheritance to, and ensure that the child will not spend it recklessly until they achieve some age (when hopefully they would be more mature), regardless of what happens to you.
If I made a P2SH address with OP_CHECKLOCKTIMEVERIFY that allows spending 18 years from birth of my child, and then suddenly Bitcoin Core announces demurrage, I would be very angry.
OP_CHECKLOCKTIMEVERIFY cannot be countermanded, and it would be impossible to refresh the UTXO's as required by demurrage, without requiring a hardfork that ignores OP_CHECKLOCKTIMEVERIFY.
It would be better to put such additional features as demurrage in a sidechain rather than on mainchain.
-------- Original Message --------
UTC Time: September 25, 2017 9:54 PM
Hello Devs,
I am Patrick Sharp. I just graduated with a BS is computer science. Forgive my ignorance.
As per bip-0002 I have scoured each bip available on the wiki to see if these ideas have already been formally proposed and now as per bip-0002 post these ideas here.
First and foremost I acknowledge that these ideas are not original nor new.
Trimming and demurrage:
I am fully aware that demurrage is a prohibited change. I hereby contest. For the record I am not a miner, I am just aware of the economics that drive the costs of bitcoin.
Without the ability to maintain some sort of limit on the maximum length or size of the block chain, block chain is not only unsustainable in the long run but becomes more and more centralized as the block chain becomes more and more unwieldy.
Trimming is not a foreign concept. Old block whose transactions are now spent hold no real value. Meaningful trimming is expensive and inhibited by unspent transactions. Old unspent transactions add unnecessary and unfair burden.
Old transactions take up real world space that continues incur cost while these transactions they do not continue to contribute to any sort of payment for this cost.
One can assume that anybody with access to their bitcoins has the power to move these bitcoins from one address to another (or at least that the software that holds the keys to their coins can do it for them) and it is not unfair to require them to do so at least once every 5 to 10 years.
Given the incentive to move it or lose it and software that will do it for them, we can assume that any bitcoin not moved is most likey therefore lost.
moving these coins will cost a small transaction fee which is fair as their transactions take up space, they need to contribute
most people who use their coins regularly will not even need to worry about this as their coins are moved to a change address anyway.
one downside is that paper wallets would then have an expiration date, however I do not think that a paper wallet that needs to be recycled every 5 to 10 years is a terrible idea.
Therefore I propose that the block chain length be limited to either 2^18 blocks (slightly less than 5 years) or 2^19 blocks, or slightly less than 10 years. I propose that each time a block is mined the the oldest block(s) (no more than two blocks) beyond this limit is trimmed from the chain and that its unspent transactions are allowed to be included in the reward of the mined block.
This keeps the block chain from tending towards infinity. This keeps the costs of the miners balanced with the costs of the users.
Even though I believe this idea will have some friction, it is applicable to the entire community. It will be hard for some users to give up small benefits that they get at the great cost of miners, however miners run the game and this fair proposal is in in their best interest in two different ways. I would like your thoughts and suggestions. I obviously think this is a freaking awesome idea. I know it is quite controversial but it is the next step in evolution that bitcoin needs to take to ensure immortality.
I come to you to ask if this has any chance of acceptance.

@_date: 2017-09-25 20:01:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] idea post: bitcoin side chain implementation 
Good morning Patrick,
Your idea seems to focus more on scaling than on what sidechains actually were originally considered for.
Sidechains were originally designed to add and prototype new features to Bitcoin.  Increasing the effective block size is not what sidechains were expected to do.
For scaling, Lightning Network is a superior solution, as it keeps most transactions off-chain.
Features are the important thing that sidechains are supposed to add to Bitcoin.
1. Addresses are not how Bitcoin works under-the-hood.  Instead, P2PKH and P2SH addresses represent 2 kinds of standard output scripts.  It is possible in theory (with a lot of endless debate) to add new standard output scripts, some of which may not have an equivalent of an address.
2.  It is easy to game your system.  A miner needs only to send a bunch of transactions from himself to himself to trigger the splitting condition.  It would even be possible to hide this somewhat by generating new public/private key pairs.
3.  The problem with on-chain scaling is not that the code has this 1Mb limit.  The problem with on-chain scaling is delivering all of the block data to the rest of the network.  Crucially, in the case that block data delivery is slow, a larger mining pool with more resources and greater ability to handle larger blocks, will work better than smaller pools or solo miners due to orphan rate/stale rate.  Thus it is in the interest of large mining pools to push for large blocks and more data per second on-chain, in order to further consolidate their power and influence over Bitcoin.  As the censorship-resistance of Bitcoin is dependent on there being many small mining pools, larger blocks destroy censorship-resistance.
4.  Disk space is not a problem.  It never was the problem.  Satoshi even mistakenly thought it was a problem, but it never was and it never will be.  The problem is that the computations on Bitcoin's security assume that blocks are delivered in 0 time.  That is not true in reality, but the reason why 10 minutes was selected as the block rate is to make block delivery time as close to 0 (relative to the average block rate) as possible.  Increasing block size makes block delivery time further from the ideal 0 that is the basis of Bitcon's security.
5.  Mining is a random process and once splits occur, you can never assure that particular chains will synchronize the real-world time of 2016 blocks.  I mean, it would be come possible for one chain to finish in 1 week while another chain is never worked on.  The 2016-blocks schedule is even more likely to misalign in real time when further splits occur.
-------- Original Message --------
UTC Time: September 25, 2017 9:53 PM
Hello Devs,
I am Patrick Sharp. I just graduated with a BS is computer science. Forgive my ignorance.
As per bip-0002 I have scoured each bip available on the wiki to see if these ideas have already been formally proposed and now as per bip-0002 post these ideas here.
First and foremost I acknowledge that these ideas are not original nor new.
Side Chains:
Bip-R10 offers a mechanism to assign custody or transfer coins from one chain to another. However I did not find a bip that proposed a formal bitcoin side chain.
My proposal
They are officially supported, tracked and built by official bitcoin software meaning that they are not an external chain
each chain has an identifier in the block header i.e. main chain: 0, first chain: 1, second chain: 2...
the number of chains including the main chain that exists is always a power of 2, this power will also be included in the block header.
each address is assigned to a chain via chain = (address) mod (number of chains)
to be valid an addresse's next transaction will first send their coins to their chain if they are not already there
if the address they are sending to is outside their chain their transaction will be submitted to both chains and transaction fee will be split between chains
They come into being via a fork or split
every 2016 blocks (upon recalculation of difficulty) if some percentage (lets say 10%) of blocks on any chain are larger than some specified amount (lets say 750 KB) then all chains are called to increment their power value and fork on their block.
miner of chain x creates genesis block for chain x+2^previous power
upon fork, the difficulty of the old chain and the new chain will be half the next difficulty
if every chain has gone 2016 block without surpassing some amount (lets say 250 KB) at least some percentage of the time (lets say 10%) all chains will be called to join, decrement their power and double their difficulty
given miner of chain x, if x not less than 2^new power, chain will be marked dead or sleeping
miners who mine blocks on the chain that was joined (the chain with the smaller identifier) may have to make a block for the sleeping chain if transactions include funds that fully or partially originate from the sleeping chain
dead chain are revived on next split.
each block's reward outside of transaction fees will be the (current bounty / 2^fork power) except obviously for dead blocks who's reward is already included in their joined block
dynamically scales to any level of usage, no more issues about block size
miners have incentive to keep all difficulties close to parity
if miners are limited by hard drive space they don't have to mine every chain (though they should have trusted peers working on other chains to verify transactions that originate off their chains, faulty block will still be unaccepted by the rest of the miners)
though work will still grow linearly with the number of chains due to having to hash each separate header, some of the overhead may remain constant and difficulty and reward will still be balanced.
transactions are pseudo equally distributed between chains.
rewards will be more distributed (doesn't' really matter, except that its beautiful)
because most transactions will be double recorded the non-volatile memory foot print of bitcoin doubles (since miners do not need all chains i believe this solution not only overcomes this cost but may decrease the foot print per miner in the long run overall)
transactions will hang in limbo until both chains have picked them up, a forever limboed transaction could result in lost coins, as long as a transaction fee has been included this risk should be mitigated.
I believe this idea is applicable to the entire community. I would like your thoughts and suggestions. I obviously think this is a freaking awesome idea. I know it is quite ambitious but it is the next step in evolution that bitcoin needs to take to be a viable competitor to visa.
I come to you to ask if this has any chance of acceptance.

@_date: 2017-09-25 20:35:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] idea post: bitcoin side chain implementation 
Good morning Patrick,
That is why most sidechain proposals use some kind of merge mining, where a commitment to another chain's block is published on the Bitcoin chain.  Drivechain has "blind" merge mining, my recent "mainstake" proposal publishses entire sidechain block headers on the mainchain.  These techniques provide security that is nearer to mainchain security.
No technology is magic, so I do not understand this sentence.
I think it would be better to term your system as "sharding" rather than "sidechain".
If and when we are able to actually agree upon some kind of sidechain-enabling proposal that is acceptable to the majority of Bitcoin Core developers, then yes, you should make a sidechain that is capable of sharding.  Sharding a distributed ledger while ensuring correct operation is a hard problem; in particular it is almost impossible to protect against double-spending unless you can see all officially-added-to-the-chain transactions.
See:

@_date: 2017-09-26 03:50:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] idea post: trimming and demurrage 
Good morning,
This is called "UTXO Set Commitments".
Pieter Wuille I think had concrete proposals for the cryptographic primitive to use. Try searching "Rolling UTXO Set Commitments".
Sent with [ProtonMail]( Secure Email.

@_date: 2017-09-26 18:38:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Sidechains: Mainstake 
Good morning,
The stake winner is only valid for a specific mainchain block.  If the stake winner is unable to publish a sidechain header on that mainchain block, then the mainchain block contains no sidechain header.  On the next mainchain block, a new stake winner is selected based on the mainchain block header hash of the mainchain block containing no sidechain header, hopefully that will be a different mainstaker.  So the stake winner will only slow down the sidechain temporarily.
Basically, the time to the next mainchain block is the "timeout" that the stake winner has to publish its sidechain header.  If we assume that mainchain will operate continuously (because a massive lack of mainchain activity would imply the death of the mainchain anyway) then the sidechain only gets slowed down (no block) if the stake winner does not respond.  Presumably this will not happen if the sidechain has pending transactions, as the stake winner would "lose its winning ticket" if it was unable to respond in a timely manner and lose its opportunity to earn sidechain transaction fees.
If the stake winner has lost keys completely, then the stake has a lock time and after the lock time the stake will no longer be part of the stake lottery, so while it would slow down the sidechain until the lock time, after the lock time it will no longer have an effect.
A passive attack, where a mainstaker just stakes and resets the stake when its lock time arrives, but does not publish any sidechain headers, will slow down the sidechain, but the mainstaker could have been earning sidechain fees if it were participating honestly instead, so the passive mainstaker suffers opportunity cost.
The lottery needs to be executable by the mainchain fullnodes.  Thus the mainchain fullnodes need to be aware of which UTXOs are to be put in the lottery, not just the sidechain fullnodes.  This is why I use scriptPubKey, rather than P2SH redeemScript.  However, this does allow mainchain miners to be aware of which UTXOs are mainstakes also, and allows them to censor these transactions.
Yes, it prevents the lottery from including the UTXO.  As it is not in the lottery, it cannot be a stake winner and cannot publish a sidechain block header until the lock time, when it can now publish openly that it is a stake UTXO using scriptPubKey.  This makes OP_STAKEVERIFY work almost as much as OP_CHECKLOCKTIMEVERIFY when inside a redeemScript.
Yes, although the mainchain transaction fees from publishing sidechain block headers will only be marginally higher than the prevailing fee rate of the block.  For a particular block, the stake winner has a monopoly (it is the only one allowed to publish a sidechain header for that block) and does not need to bid with other mainstakers, and can bid the minimum necessary just to get into the mainchain block.

@_date: 2018-04-11 05:48:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Few questions regarding ListTransaction 
Good morning Karl-Johan Alm,
To clarify:
Nothing prevents a miner from completely ignoring nSequence when putting transactions in blocks.
Unconfirmed transactions are, by definition, not recorded in blocks.  So if there is a transaction 0xFFFFFFF nSequence and fee 1000 satoshi, and another conflicting transaction 0xFFFFFFF nSequence and fee 100000000 satoshi, miners can include the latter one even if the first one came to their knowledge first, regardless nSequence.
Thus, in the end "full replace-by-fee", where nSequence is IGNORED for purposes of replace-by-fee, is expected to become the norm, and we should really be designing our wallets and so on so that we only trust transactions that have confirmations.
The "nSequence=0xFFFFFFFF means opt-OUT of RBF" convention is only followed by fullnodes running standard bitcoind.  Nothing prevents miners from running patched bitcoind that ignores this rule, and connecting with similar peers who also ignore this rule.
?Sent with ProtonMail Secure Email.?
??????? Original Message ???????

@_date: 2018-12-03 04:16:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
Good morning Bob,
Would `SIGHASH_SINGLE` work?
Commitment transactions have a single input but multiple outputs.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2018-12-21 11:40:06
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Good morning Johnson,
First off, this is a very good idea I think.
It prevents use of SIGHASH_NOINPUT to support walletless offchain protocols.
In brief, this idea of "walletless offchain software" is motivated by the fact, that various onchain wallets exist with many features.
For instance, privacy-enhancement as in Samourai/Wasabi/etc.
And so on.
There are requests to include such features in e.g. Lightning software, for example: But it is enough of a challenge to implement Lightning, without the additional burden of implementing nice onchain features like coin control and change labelling.
It would be best if we can retain features from an onchain wallet, while using our coin on an offchain system.
Further, it would allow onchain wallet developers to focus and gain expertise on onchain wallet features, and, vice versa, for offchain walletless software developers to focus on offchain software features.
The core idea comes from the way that offchain systems need to be set up:
1.  First we agree on a (currently unconfirmed) txid and output number on which to anchor our offchain system (the funding transaction).
2.  Then we sign a backout transaction (the initial commitment transactions under Poon-Dryja, or the timelock branches for CoinSwapCS, or the initial kickoff-settlement transactions for Decker-Russell-Osuntokun) spending the agreed TXO, to return the money back to the owner(s) in case some participant aborts the setting up of the offchain system.
3.  Then we sign and broadcast the funding transaction.
Unfortunately, the typical onchain wallet has a very simple and atomic (uncuttable) process for making transactions:
1.  Make, sign, and broadcast transaction with an output paying to the desired address.
Thus a typical onchain wallet cannot be used to set up a funding transaction for an offchain system.
Now suppose we take advantage of `SIGHASH_NOINPUT`, and modify our offchain system setup as below:
1.  First we agree on a N-of-N pubkey on which to anchor our offchain system (the funding address).
2.  Then we sign (with SIGHASH_NOINPUT) a backout transaction (the initial commitment transactions under Poon-Dryja, or the timelock branches for CoinSwapCS, or the initial kickoff-settlement transactions for Decker-Russell-Osuntokun), spending the agreed funding address, to return the money back to the owner(s) in case some participant aborts the setting up of the offchain system.
3.  Make, sign, and broadcast transaction with an output paying to the funding address.  This step can be done by any typical onchain wallet.
Note that only the starting backout transaction is *required* to sign with `SIGHASH_NOINPUT`.
For instance, a Poon-Dryja channel may sign succeeding commitment transactions with `SIGHASH_ALL`.
Finally, only in case of disaster (some participant aborts before the offchain system is set up) is the `SIGHASH_NOINPUT` backoff transaction broadcasted.
A "normal close" of the offchain system can be signed with typical `SIGHASH_ALL` for no fungibility problems.
With this, an offchain system need not require its implementing software to implement its own wallet.
Further, onchain wallets can directly put its funds into an offchain system, without requiring an onchain transfer to an offchain software wallet.
This can be helpful when building overall software.
We might take any commodity onchain wallet and any commodity offchain software, and we can integrate them easily to create a seamless wallet experience that allows spending and receiving onchain and offchain.
Further, improvements in one software component do not require re-building of the other software component.
That said:
1.  For Lightning and similar systems, the fact that the Lightning node will give you an address that, when paid using any commodity onchain wallet, opens a channel, means that people will make wrong assumptions.
    In particular, they are likely to assume that address reuse is safe and will attempt to "refill" their channel by paying to the same address again in the future.
    From this alone, we can immediately see that this idea is pointless.
2.  Dual-funding, which for some reason is asked for as a feature, cannot be done with this anyway.
3.  It may be better to provide some standard way of signing transactions without broadcasting them.
    This would still allow similar separation of concerns between onchain and offchain software components.
So output tagging still seems fine to me, even if this particular use cannot be supported.

@_date: 2018-12-22 14:25:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Good morning Johnson,
I think a third party would be pointless in general, but then I am strongly against custodiality.
The idea is that you have some kind of hardware wallet or similar "somewhat cold" storage *that you control yourself*, and crate channels for your hot offchain Lightning wallet, without adding more transactions from your somewhat-cold storage to your hot offchain Lightning wallet on the blockchain.
Then you could feed a set of addresses to the hot offchain wallet (addresses your somewhat-cold storage controls) so that when channels are closed, the funds go to your somwhat-cold storage.
I also doubt that any custodial service would want to mess around with deducting funds from what the user input as the desired payment.  I have not seen a custodial service that does so (this is not a scientific study; I rarely use custodial services); custodial services will deduct more from your balance than what you send, but will not modify what you send, and will prevent you from sending more than your balance minus the fees they charge for sending onchain.
Even today, custodial services deducting from your sent value (rather than the balance remaining after you send) would be problematic when interacting with merchants (or their payment processors) accepting onchain payments; the merchant would refuse to service a lower value than what it charges and it may be very technically difficult to recover such funds from the merchant.
I expect such a custodial service would quickly lose users, but the world surprises me often.

@_date: 2018-12-24 11:47:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Good morning Johnson,
Indeed, manual operation is risky.
However the intent is to reduce the requirements on commodity wallets in order to integrate them into a combined onchain and offchain UI.
A boutique protocol would reduce the number of existing onchain wallets that could be integrated in such UI.
If we could make walletless offchain software in such method, *any* existing wallet with an API to programmatically send arbitrary amount to arbitrary address can be integrated into such UI.
This could include hardware wallets.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2018-12-24 12:01:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Good morning,
Long ago, aj sent an email on Lightning-dev about use of CODESEPARATOR to impose Scriptless Script even without Schnorr. It involved 3 signatures with different CODESEPARATOR places, and forced R reuse so that the signatures to claim the funds revealed the privkey.
The script shown had all CODESEPARATOR in a single branch.
I cannot claim to understand the script, and am having difficulty digging through the mailinglist

@_date: 2018-02-04 17:24:36
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] RBF Wallet Algorithms (Was: Transaction Merging 
Good Morning Rhavar,
I have been trying to conceptualize an algorithm precisely for RBF, and I agree that "tracking the mess" is a significant issue...
For this example, I believe it is possible to assure correct operation without changes to the current RBF policy.
Presumably, the problematic sequence of events is this:
1.  You need to pay Paul.
2.  You make transaction A that pays to Paul.  It has no change output.
3.  You need to pay John.
4.  You see transaction A is unconfirmed.  Using A as basis, you make transaction B that pays to Paul, John.  It replaces A.
5.  Transaction A confirms once (because the B transaction did not propagate to the lucky miner quickly enough)
6.  You still have a pending commitment to pay John, so you make a transaction C that pays to John.
7.  A reorg occurs, transaction A is removed from history.
8.  Transaction B and transaction C confirm, double-paying John.
This can be fixed by ensuring that transaction C is incompatible with B, but compatible with A.
By "compatibility", we mean, "A transaction T is incompatible with U if T cannot confirm if U confirms, and U cannot confirm if T confirms."
If transaction A has no change output, then in order for A to be incompatible with B, with B paying both Paul and John, means that B has more spent inputs than A.  Else where would the extra funds to pay John come from? (assuming you are not taking from Paul to pay John)
If so, it means that there is some input that B spends, which A does not spend.
So we can make a transaction C that spends this input (the one which B spends that A does not spend).  This makes C compatible with A, but incompatible with B.
So you can still work, even without a change output on A, to ensure that your transaction C cannot be confirmed if B confirms.
1.  If A has some change output, ensure C spends that output.  Presumably if B is incompatible with A (after all, you tried to replace A with B), then C is incompatible with B as C is dependent on A confirming.
2.  If A has no change output, then if you increased your spending to make transaction B, then logically B has some input that is not in A (otherwise where would the extra funds have come from...).  Then ensure C spends that input of B that is not in A, making it directly incompatible with B.
This ensures that either A+C confirm, or B confirms.
(Again, the complications are considerable! We can only show that it is possible in theory, but whether it is feasible in practice to implement in some program that can be debugged and maintained  is another issue)
A vague idea I have formed is to use some sort of vector of candidate TXOs you control.  Items are appended to this vector lazily as per your coin policy.  Transactions mark how far in this vector they spend (i.e. a high-water mark for that transaction).  If a previous confirmed transaction you wrote has a change output, you always use that change output and try to get more coins from this vector (starting after the previous confirmed transaction high-water mark) if the change output is not enough.  If a previous confirmed transaction you wrote has no change output, then you get more coins from this vector (again starting from the previous confirmed transaction high-water mark).  The vector is extended lazily from your set of controlled coins.  Older entries in this vector may be dropped once transactions confirm deeply enough that it is unlikely to be reorged (say 144 blocks); the exact policy is that if a transaction confirms deeply enough, then everything from its high-waiter mark to below can be pruned from this vector.
The above vague idea precludes you from reoptimizing transactions, however; your replacements either have the same set of inputs, or a strict superset of inputs, as the previous transaction.

@_date: 2018-02-05 04:27:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Good morning Greg,
I am probably being exceedingly naive, but I would like to compare Taproot to a generalization of funding transactions.
For instance, CoinSwapCS:
1.  It uses an HTLC in an off-chain transaction, and a funding transaction TX0 whose output is a "simple" 2-of-2.
2.  The HTLC tx spends this 2-of-2.
3.  If a branch of the HTLC succeeds, the parties contact each other and create a replacement of the (unconfirmed and unbroadcasted but signed) HTLC tx that assigns the funds to the correct owners.
4.  If the above step fails, individual parties can in isolation publish the HTLC tx and provide its requirements.
Both of  and  above, appear to me naively as similar to the two "top" cases of Taproot, i.e. either C is signed by all parties, or S is revealed and fulfilled.
The important bits of this "generalized funding transaction" pattern is:
1. The contract that enforces correct behavior spends an unsigned and unbroadcasted funding transaction output (which requires N-of-N).
2. The enforcement contract is signed first by all parties before the funding transaction is signed by anybody.  This is possible due to SegWit.
3.  Then, when all parties are sure they have the fully-signed smart contract, the initial funding transaction is signed and broadcast and confirmed.
4.  When the condition that the contract requires is achieved, then the parties contact each other and try to jointly create a simpler transaction that spends the funding transaction directly to whoever gets the money in the correct proportion.  This avoids publishing the smart contract onchain, and looks like an ordinary N-of-N spend.
5.  If they fail to get all required signatures for any reason, any party can publish the enforcement contract transaction and subsequently fulfill its conditions in another transaction.
Admittedly, Taproot if added to the consensus would reduce the number of transactions by 1 in the "S is revealed" case.
But the "generalized funding transaction" pattern is already possible today, and MuSig (to my limited understanding) can be used to make it indistinguishable from 1-of-1 (so, possibly, make it P2WPKH?).
(I am probably neglecting something very simple and direct, however...)
-------- Original Message --------

@_date: 2018-02-12 22:26:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP0008 clarification 
Good morning Helder,
Assuming the update is widespread among economic actors, only miners who do not follow the more stringent rules of the update will suffer, as their blocks will have a high probability of not following those rules and thus will be implicitly rejected by economic actors.  Rational miners who follow the update, no matter how small their hash power share, would prefer the chain that economic actors will accept as real and would build only on blocks that follow updated rules strictly.
Indeed, the time from STARTED to ACTIVE simply serves to let miners upgrade their software, as a concession that in the real world we cannot safely deploy new software in a single day.

@_date: 2018-01-22 06:12:51
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Blockchain Voluntary Fork (Split) Proposal 
Good morning Chaofan Li,
What enforces that bitcoin A is worth the same as bitcoin B?  Or are they allowed to eventually diverge in price?  If they diverge in price, how is that different from the current situation with Bitcoin, BCash, Bitcoin Gold, Bitcoin Hardfork-of-the-week, and so on?
Sent with [ProtonMail]( Secure Email.
-------- Original Message --------

@_date: 2018-01-30 00:32:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Blockchain Voluntary Fork (Split) Proposal 
Good Morning Chaofan Li,
This returns my initial question.
What ensures that a paper money with "10 Dollar" on it, is same as 10 coins each with "1 Dollar" on it?
This is the principle of fungibility, and means I can exchange a paper with "10 Dollar" on it for 10 coins with "1 Dollar" on it, because by government fiat, such an exchange is valid for all cases.
What ensures that btc.0 and btc.1 are indistinguishable from a human perception?
You are talking about sidechains.  In every sidechain proposal, there is always some mechanism (SPV proof-of-work, drivechain proof-of-voting, proof-of-mainstake...) that ensures that a sidechain coin is exchangeable for a mainchain coin, and from there, that every sidechain coin is exchangeable for every other sidechain coin.  I.e. that a smart contract with "1 BTC" on it is exchangeable for a mainchain UTXO of value "1 BTC".
A mere split is not enough.  As I brought up, what makes your proposal different from 2X, BCash, etc.?

@_date: 2018-07-11 03:43:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev]  BIP sighash_noinput 
Good morning DING FENG,
While your concern is valid, the general intent is the below:
1.  We will use a scary name like SIGHASH_NOINPUT_UNSAFE to explicitly inform to wallet and Bitcoin software developers that the flag is potentially unsafe.
2.  SIGHASH_NOINPUT_UNSAFE is intended to be used for specialty protocols like LN, CoinSwap, etc. and not for general-purpose user wallets (except for Luke Dash Jr wallet which explicitly rejects address reuse).  By default, this flag is not set and address reuse is still slightly safe for common usage, modulo other bugs in the implementation such as weak generation of random R (which are already existing concerns for SIGHASH_ALL).
2.1.  Even for LN/CoinSwap/etc., SIGHASH_NOINPUT_UNSAFE will be used only in the exact specialty protocol, and not e.g. for general wallet usage.
Sent with [ProtonMail]( Secure Email.
??????? Original Message ???????

@_date: 2018-06-20 08:12:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Good morning Pieter and Tim and all,
My understanding is that the idea now being discussed by Pieter and Tim is that the Graftroot signature is not `sign(P, script)` but instead `sign(P, sighash(tx))`, where `tx` is an "ordinary" transaction that spends the outpoint that pays to `P`, and a single output whose `scriptPubKey` is the Graftroot `script` and contains the entire value of the outpoint, and `sighash()` is the standard SegWit transaction digest algorithm used for signing (and is affected by other flags in the signature).
This has the advantage that the Graftroot signature commits to a single outpoint and cannot be used to spend all outpoints that happen to pay to the same `P` public key.
However I believe the ability to "immanentize" a Graftroot signature as a signature for a 1-input 1-output transaction is unsafe (so a Graftroot signature should probably not be "the same" as a signature for a 1-input 1-output transaction like the above).
Let us consider a simple CoinSwap protocol.  Let us focus on one half of the procedure, which is a simple ZKCP, i.e. Alice pays Bob for a hash preimage, with a timeout imposed so that Bob needs to provide the preimage, within a specified time.
1.  Alice and Bob generate a shared public key P which requires k_a (Alice secret key) and k_b (Bob secret key) to sign.
2.  Alice creates but does not sign a funding transaction that pays to public key P and gives its txid to Bob.  Alice also provides a standard P2WPKH return address that Alice controls.
3.  Bob creates a `nLockTime`-encumbered transaction (the timeout backoff transaction) on the agreed timeout, spending the above txid outpoint to the Alice return address, and provides its half of the signature to P signing the timeout backoff transaction to Alice.
4.  Alice keeps the above signature (verifying it is to the correct `nLockTime` and Alice return address), then signs and broadcasts the funding transaction.  Both wait for the funding transaction to confirm deeply.
5.  Alice then signs a Graftroot to the script `{ OP_HASH  OP_EQUALVERIFY  OP_CHECKSIG }` and gives its half of the signature to P signing the Graftroot to Bob.  Bob keeps this signature.
6.  Bob provides the preimage to the hash directly to Alice and a standard P2WPKH destination address that Bob controls.
7.  Alice then signs a direct spend of the funding transaction outpoint (one that is not encumbered by `nLockTime`), spending the funding txid outpoint to the Bob destination address, and provides its half of the signature to P signing this transaction.  This completes Alice participation in the protocol (it has now received the preimage).
8.  Bob completes the signature to the destination transaction and broadcasts it to the blockchain layer.
If Alice or Bob stalls at step 5 or earlier then the transaction does not occur (Alice does not learn the preimage, Bob gets no money).
If Bob stalls at step 6, Alice can use the timeout backoff.
If Alice stalls at step 7, Bob can use the Graftroot signed at step 5 to claim its funds as long as the timeout is not reached.
Now if Graftroot signature is "actually" just a standard signature of a transaction that is elided from the blockchain, however, it means that this elided transaction can be immanentized on the blockchain with the specified script.  Even if this transaction has e.g. no fee then Bob could collude with a miner via sidefees to get the (valid) transaction onchain.
So Bob could take the signature made at 5 to create a transaction spending to the specified script, and prevent Alice from claiming the funds using the timeout backoff transaction.  Then Bob forever controls the UTXO and Alice cannot back out of the transaction, so even if the knowledge of the preimage ceases to be interesting, Alice has already paid Bob and Bob can provide the preimage at its leisure rather than constrained by the timeout.
Thus we should somehow disallow immanentizing the Graftroot signature.
An idea is that the Graftroot signature should sign a transaction with a specific special `nVersion`, that is then soft-forked to be invalid onchain (i.e. the `nVersion` is reserved for Graftroot and it is invalid for a transaction onchain to use that `nVersion`).  So the Graftroot signature can be used as a Graftroot spend, but not as a immanentized signature on an actual onchain transaction that could disable the timeout backoff transaction.
Utilities that can sign an arbitrary message using your private keys could check if the first four bytes match the Graftroot `nVersion` and refuse to sign such messages to prevent inadvertently giving a Graftroot signature.
Alternatively, we note that the "transaction" signed by Graftroot will not be referred to onchain anyway, and we could use a completely different `sighash()` algorithm, e.g. it could just be the outpoint being spent and the script to be executed, i.e. `sign(P, concat(txid, outnum, script))`.  This reduces code reuse, though.

@_date: 2018-06-21 03:09:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Good morning Greg,
Perhaps `SIGHASH_NOINPUT` can do this? One can argue that the option to not commit a signature to refer to a specific outpoint is orthogonal to the option to Graftroot, so having a separate flag for that makes sense.
The proposal could then be:
1. Define a transaction `nVersion` reserved for Graftroot. Transactions with that `nVersion` are disallowed in blocks.
2. If a next-SegWit-version P2WPKH (or P2WPK) is spent, and the top witness stack item is a signature with `SIGHASH_GRAFTROOT` flag, then this is a Graftroot spend.
3. The signature signs an imaginary 1-input 1-output tx, with the input copied from the spending tx, the output value being the entire output being spent, and the output `scriptPubKey` being the Graftroot script (second to top witness stack). The imaginary tx has the Graftroot-reserved `nVersion`.
4. The Graftroot signature has its other flags `SIGHASH_NOINPUT` evaluated also when verifying it signs the imaginary tx.
5. The Graftroot signature and the Graftroot script are popped and the script executed in the context of the original Graftroot-spending tx.
This lets users select whether committing to a specific outpoint is needed or not, independently of Graftroot.

@_date: 2018-03-12 00:14:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bulletproof CT as basis for election voting? 
Good morning Jose,
By my understanding, the sender needs to reveal some secrets to the receiver, and the receiver will then know if it received 0 or 1 coin from that sender.  (At least from my understanding of MimbleWimble; it might not be the case for CT, but MW is an extension of CT so...)
If voters send vote-coins directly to The Party, then The Party knows the votes of particular voters, and may then dispatch subcontractors to dispatch those voters.  It may be possible to have aggregators/mixers, but then you would have to trust the aggregators/mixers operate correctly and send to the correct destination party, and that the mixers are not recording voters.
Maybe in combination with something like CoinSwap or CoinJoin protocol would work to obscure the source of coins: a voter would have to swap several times with many, many other voters to ensure increased anonymity set (and then maybe some voters may report their transactions to The Party).
In any case sending directly from the tx of the Voting Authority to another tx to your selected The Party would let The Party members who secretly control the Voting Authority records to figure out, which voters got which txouts of the Voting Authority (presumably the Voting Authority has strict public records of which txout went to which voter, in order to prevent the Voting Authority secretly giving multiple vote-coins to a single One Man, All Votes).
?Sent with ProtonMail Secure Email.?
??????? Original Message ???????

@_date: 2018-03-12 02:46:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bulletproof CT as basis for election voting? 
Good morning again Jose,
Another idea is that with sufficiently high stakes (i.e. control of the government of an entire country) it would be possible for a miner-strong The Party to censor transactions that do not give it non-zero amounts of coins.  If The Party has a strong enough power over miners (or is composed of miners) then it would be possible for The Party to censor transactions using some simple heuristics: (1) At least one output goes to The Party (2) the number of inputs equals the number of vote-coins that go to The Party output.  Since The Party must know how many vote-coins it received, it can know  and it assumes that each input has 1 coin, since that is what is issued by the Voting Authority.  This prevents mixing, too, since transactions that do not involve The Party cannot be confirmed.
Presumably other parties may exist that have some miners, but if everyone starts censoring transactions then parties end up voting by their controlled hashpower rather than anything else (simply censor all transactions that fail the above heuristics and build the longest chain: as long as you get even 1 vote and all others get 0 votes on the longest chain, you win. since presumably you are also a valid voter, you can just give that single vote-coin issued to you-as-voter to you-as-party, then censor all other transactions in the blockchain so that other voters cannot give their coins to their preferred parties).  One could try using proof-of-stake if one has managed to create a solution to nothing-at-stake and stake-grinding that itself does not require proof-of-work (hint, there are none).
This can be mitigated by using a multi-asset international blockchain with confidential assets, such that no single The Party can control enough hashpower to censor, but that makes small blocks even more important to help fight against centralization (and control of cheap energy becomes even more important such that some international entity may very well bend elections in individual countries to its favor to get more energy with which to control more energy, and so on).
You can only trust the miners of the blockchain to the extent that you pay fees to those miners, effectively buying a portion of hashrate in a (mostly) fair auction.  You can expect that miners will attempt to charge as much as they can for the hashrate, and therefore that vote transfers (if they can be detected by miners) are likely to be charged at whatever is the going rate for that vote.  If what is being voted on is important enough, you can assure yourself, that miners will ally with politicians and use the fact that CT is confidential only between receiver and sender to discern preferred vote transfers.
Uncensorability may be possible though; I think Peter Todd was working on those.  A simple one is a two-step commitment, where an earlier miner only knows of a sealed commitment (a hash of a transaction), publishes it, then a future commitment shows the entire transaction and the earlier miner gets paid only if the second commitment pushes through (the fee gets split somehow between the earlier and later miner).  But once you reveal a transaction and it is not one of those desired by the later miner, if the vote is valuable enough then the miner might very well forgo its fee in favor of never confirming the second commitment.
It may be better to focus more on libertarian solutions (e.g. assurance contracts) on top of blockchains than attempting to shoehorn democractic ideals on top of blockchains.

@_date: 2018-03-21 03:53:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation 
Good morning aj,
I am probably wrong, but could solution 2 be simplified by using the below opcodes for aggregated signatures?
OP_ADD_AGG_PUBKEY - Adds a public key for verification of an aggregated signature.
OP_CHECK_AGG_SIG[VERIFY] - Check that the gathered public keys matches the aggregated signature.
 pubkey1 OP_ADD_AGG_PUBKEY
 OP_IF
   pubkey2 OP_ADD_AGG_PUBKEY
 OP_ELSE
   cond OP_CHECKCOVENANT
 OP_ENDIF
 OP_CHECK_AGG_SIG
(omitting the existence of buckets)
I imagine that aggregated signatures, being linear, would allow pubkey to be aggregated also by adding the pubkey points (but note that I am not a mathematician, I only parrot what better mathematicians say) so OP_ADD_AGG_PUBKEY would not require storing all public keys, just adding them linearly.
The effect is that in the OP_CHECKCOVENANT case, pre-softfork nodes will not actually do any checking.
OP_CHECK_AGG_SIG might accept the signature on the stack (combined signature of pubkey1 and pubkey2 and from other inputs), or the bucket the signature is stored in.
We might even consider using the altstack: no more OP_ADD_AGG_PUBKEY (one less opcode to reserve!), just push pubkeys on the altstack, and OP_CHECK_AGG_SIG would take the entire altstack as all the public keys to be used in aggregated signature checking.
This way, rather than gathering signatures, we gather public keys for aggregate signature checking.  OP_RETURN_TRUE interacts with that by not performing aggregate signature checking at all if we encounter OP_RETURN_TRUE first (which makes sense: old nodes have no idea what OP_RETURN_TRUE is really doing, and would fail to understand all its details).
I am very probably wrong but am willing to learn how to break the above, though.  I am probably making a mistake somewhere.
?Sent with ProtonMail Secure Email.?
??????? Original Message ???????

@_date: 2018-03-21 19:28:00
@_author: ZmnSCPxj@protonmail.com 
@_subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation 
Good morning aj,
?Sent with ProtonMail Secure Email.?
??????? Original Message ???????
Yes, I think this is indeed what OP_CHECK_AGG_SIG really does.
What I propose is that we have two places where we aggregate public keys: one at the script level, and one at the transaction level.  OP_ADD_AGG_PUBKEY adds to the script-level aggregate, then OP_CHECK_AGG_SIG adds the script-level aggregate to the transaction-level aggregate.
Unfortunately it will not work since transaction-level aggregate (which is actually what gets checked) is different between pre-fork and post-fork nodes.
It looks like signature aggregation is difficult to reconcile with script...

@_date: 2018-05-01 01:01:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] eltoo: A Simplified update Mechanism for 
Good morning Jim,
I believe this is quite true; indeed only the LN-penalty/Poon-Dryja channels do not have this drawback, as Decker-Wattenhofer invalidation trees also have the same drawback that the CSV and CLTV add up.
However the worst-case invalidation tree total CSV timeouts under Decker-Wattenhofer can grow quite massive; it seems the new eltoo Decker-Russell-Osuntokun CSV timeouts can be shorter.

@_date: 2018-05-01 01:07:54
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] eltoo: A Simplified update 
Good morning Christian,
This is very interesting indeed!
I have started skimming through the paper.
I am uncertain if the below text is correct?
Figure 2 contains to what looks to me like a `witnessProgram`, `scriptPubKey`, or `redeemScript` but refers to it as an "output script":

@_date: 2018-05-04 05:15:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP sighash_noinput 
Good morning Christian and list,
It seems to me, that `SIGHASH_NOINPUT` may help make some protocol integrate better with existing wallets.
I remember vaguely that `SIGHASH_NOINPUT` was also mentioned before in LN discussions, when the issue of transaction malleation was considered (before SegWit, being totally uncontroversial, was massively adopted).  The sketch below, I believe, is somewhat consistent with how it could have been used in funding a channel.
Consider a CoinSwap protocol.  Each side writes a transaction that pays out to an ordinary 2-of-2 multisig address.  But before each side writes and signs that transaction, it first demands a timelocked backout transaction to let them recover their own funds in case it falls through (more generally, every offchain protocol has a similar setup stage where some way to back out is signed before all parties enter into the contract).
Now, without `SIGHASH_NOINPUT`, we would first require that the initial funding transaction be written (but not signed and broadcast), and then the txid to the other side.  The other side then generates the backout transaction (which requires the txid and outnum of the funding outpoint) and returns the signature for the backout transaction to the first side.
Because of this, an implementation of CoinSwap needs to have control of its own coins.  This means that coin selection, blockchain tracking, and mempool tracking (i.e. to handle RBFs, which would invalidate any further transactions if you used coins received by RBF-able transactions while unconfirmed) needs to be implemented.
But it would be much nicer if instead the CoinSwap implementation could simply say "okay, I started our CoinSwap, now send X coins to address A", and then the user uses their ordinary wallet software to send to that address (obviously before the CoinSwap can start, the user must first provide an address to which the backoff transaction should pay; but in fact that could simply be the same as the other address in the swap).
1.  The user will not have to make a separate transfer from their wallet, then initiate a swap, then transfer from the CoinSwap implementation to their usual wallet: instead the user gets an address from their wallet, initiates the swap, then pays to the address the CoinSwap implementation said to pay and wait to receive the swapped funds to their normal wallet.
2.  Implementing the CoinSwap program is now somewhat easier since we do not need to manage our own funds: the software only needs to manage the single particular coin that was paid to the single address being used in the swap.
3.  The smaller number of required features for use means easier implementation and testing.  It also makes it more likely to be implemented in the first place, since the effort to make it is smaller.
4.  The lack of a wallet means users can use a trusted wallet implementation (cold storage, hardware wallet, etc) in conjunction with the software, and only risk the amount that passes through the CoinSwap software (which is smaller, since it does not have to include any extra funds to pay for fees).
With `SIGHASH_NOINPUT`, we can indeed implement such a walletless CoinSwap (or other protocol) software.  We only need to provide the public keys that will be used in the initial 2-of-2, and the other side can create a signature with `SIGHASH_NOINPUT` flag.
The setup of the CoinSwap then goes this way.  The swapping nodes exchange public keys (two for each side in this case), they agree on who gets to move first in the swap and who generates the preimage, and then they agree on what the backout transactions look like (in particular, they agree on the address the backout transactions spend) and create signatures, with `SIGHASH_NOINPUT`.  In particular, the signatures do not commit to the txid of the transaction that they authorize spending.  The CoinSwap sofwares then turn around to their users and say "okay, send to this address", the users initiate the swap using their normal wallet software, the CoinSwap software monitors only the address it asked the user to use, then when it appears onchain (the CoinSwap software does not even need to track the mempool) it continues with the HTLC offers and preimage exchanges until the protocol completes.
In a world where walletless CoinSwap exists, consider this:
1.  A user buys Bitcoin from an exchange.  The exchange operates a wallet which they credit when the user buys Bitcoin.
2.  The user starts a CoinSwap, giving the destination address from their cold-storage wallet.
3.  The CoinSwap tells the user an address to send to.  The user withdraws money from the exchange using that address as destination (1 transaction)
4.  The user waits for the CoinSwap to finish, which causes the funds to appear in their cold-storage wallet (1 transaction).
If CoinSwap implementations all needed their own wallets, then instead:
1.  A user buys Bitcoin from an exchange.
2.  The user withdraws the funds from the exchange to a CoinSwap implementation wallet (1 transaction).
3.  The user performs a CoinSwap which goes back to the CoinSwap implementation wallet (2 transactions).
4.  The user sends from the CoinSwap wallet to their cold storage (1 transaction). (granted, the CoinSwap implementation could offer a feature that immediately transfers the swapped funds to some other wallet, but we still cannot get around the transfer from the exchange to the CoinSwap wallet)
A drawback of course, is that `SIGHASH_NOINPUT` is an unusual flag to use; it immediately paints the user as using some special protocol.  So much for `SIGHASH_NOINPUT` CoinSwap.

@_date: 2018-05-04 10:25:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP sighash_noinput 
Good morning Christian,
Yes, the intent is that specialized utilities (like the CoinSwap I gave as an example) would be the ones signing with `SIGHASH_NOINPUT`, with the existing wallet generating the output that will be spent with a `SIGHASH_NOINPUT`.
The issue is that some trustless protocols have an offchain component, where some kind of backoff transaction is created, and the creation involves the 3 steps (1) make but do not sign&broadcast a funding tx (2) make and sign a backoff transaction that spends the funding tx (3) sign and broadcast the original funding tx. This holds for Poon-Dryja, your new eltoo Decker-Russell-Osuntokun, and CoinSwap.  Commodity user wallets and exchange wallets only support the most basic "make tx, sign, broadcast", and integrating with the generalized funding transaction pattern is not possible.  `SIGHASH_NOINPUT` allows us to make the backoff transaction first, then make the funding transaction via the usual "make tx, sign, broadcast" procedure that commodity wallets implement.
Thinking about it further, it turns out that in the cooperative completion of the protocol, we do not need to sign anything using `SIGHASH_NOINPUT`, but can use the typical `SIGHASH_ALL`. Indeed all generalized funding transaction patterns can be updated to use this: only the initial backout transaction needs to be signed with `SIGHASH_NOINPUT`, all others can be signed with `SIGHASH_ALL`, including the protocol conclusion transaction.
1.  In CoinSwapCS, TX-0 and TX-1 are funding transactions.  The backoff transaction is the TX-2 and TX-3 transactions.  Only TX-2 and TX-3 need be signed with `SIGHASH_NOINPUT`.  TX-4 and TX-5, which complete the protocol and hide the swap, can be signed with `SIGHASH_ALL`.
2.  In Poon-Dryja, the backoff transaction is the very first commitment transaction.  Again only that transaction needs to be signed with `SIGHASH_NOINPUT`: future commitment transactions as well as the mutual close transaction can be signed with `SIGHASH_ALL`.
3.  In Decker-Russell-Osuntokun, the backoff transaction is the trigger transaction and the first settlement transaction.  The trigger transaction can sign with `SIGHASH_NOINPUT`.  Then only the final settlement (i.e. mutual close) can be signed with `SIGHASH_ALL`.
Thus if the protocol completes cooperatively, the only onchain evidence is that a 2-of-2 multisig is spent, and signed using `SIGHASH_ALL`, and the money goes to some ordinary P2WPKH addresses.
The advantage, as I mentioned, is that these protocols can be implemented using "walletless" software: the special protocol software runs the protocol up to the point that they get the backoff transaction, then asks the user to pay an exact amount to an exact address.  This has a number of advantages:
1.  RBF can be supported if the wallet software supports RBF.  In particular without `SIGHASH_NOINPUT` the protocol would require renegotiation of a new backoff transaction in order to support RBF (and in particular the protocol spec would need to be designed in the first place to consider that possibility!), and would become more complicated since while a new backoff transaction is being negotiated, the previous version of the funding transaction may get confirmed.  With `SIGHASH_NOINPUT` all the specialized protocol software needs to do, is to watch for a transaction paying to the given address to be confirmed deeply enough to be unlikely to be reorganized: there is no need to renegotiate a backoff transaction, because whatever transaction gets confirmed, as long as it pays to the address with a given amount, the signature for the backoff transaction remains valid for it.
2.  Wallet software of any kind can be used in conjunction with special protocol software of any kind.  Hardware wallets do not need to implement LN: the LN software starts a channel and gives a P2WSH address that hardware wallets know how to pay to.  Ditto for exchange wallets.  Etc.  And if a future protocol arises that uses the funding transaction pattern again, then again existing wallets can integrate with those protocols via P2WSH address.
3.  Special protocol software need not implement even basic wallet functionality: they can just focus on the specific protocol they implement.  Consider how until late last year c-lightning needed a separate RPC command to inform it that it received funds, and a few months ago we had many issues with UTXOs in our database getting out of sync with the blockchain (why we implemented `dev-rescan-outputs`).

@_date: 2018-05-08 23:02:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Good morning Olauluwa,
I believe P2WSH is larger due to the script hash commitment in the `scriptPubKey` as well as the actual script revelation in the `witnessScript`, whereas, a flat OP_TRUE in the `scriptPubKey` is much smaller and can be spent with an empty `scriptSig`.  It seems this is the entirety of the reason to desire an isStandard OP_TRUE.
Sent with [ProtonMail]( Secure Email.
??????? Original Message ???????

@_date: 2018-05-09 23:07:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Good morning Luke and list,
I understand the issue.  On Lightning side, if this rule is used, we would have the two options below:
1.  Commitment transactions always use the minimum feerate, but always have the above OP_TRUE output.  Then to confirm the commitment transaction we would have to always spend the OP_TRUE output in CPFP transaction that pays for actual fee at unilateral close.  This consumes more blockchain space for unilateral closes, as the second transaction is always mandatory.
2.  We store two commitment transactions and associated paraphernalia (further transactions to claim the HTLCs).  One version has a negotiated feerate without the OP_TRUE output.  The other version has a slightly increased feerate and an OP_TRUE output as above.  At unilateral close, we see if the negotiated feerate is enough and use that version if possible, but if not we RBF it with other version and in addition also CPFP on top.  As mentioned before, we do not have transaction packages, so we need to RBF with higher feerate the commitment transaction, then submit the CPFP transaction which makes the first transaction valid to include in a block as per the rule.  This requires that the fallback always have both an RBF bump and a CPFP bump.
It seems there are indeed none.

@_date: 2018-05-10 03:54:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Why not archive the backend of Bitcoin blockchain? 
Good morning karl and Segue,
Specifically for c-lightning, we are not yet rated for pruned bitcoind use, although if you installed and started running bitcoind before installing the lightningd, caught up to the chain, and then installed lightningd and set things up so that bitcoind will get killed if lightningd stops running (so that bitcoind will "never" leave lightningd too far behind).
Officially though pruned bitcoind is not supported for c-lightning, so loss of funds due to doing the above idea is entirely your fault.
On the topic of such a "chapter-based" archiving, it needs to get implemented and reviewed.  As-is I see no reason why it cannot be done, but I think the details are far more important.
1.  How do we select the archive servers?
2.  How can we ensure that no chapter has only a small number of actual owners who could easily coordinate to deny access to historical blockchain data to those they deem undesirable?
??????? Original Message ???????

@_date: 2018-05-10 22:44:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Good morning Luke and list,
Thank you for clearing this up.  It seems, I misunderstood.  So my earlier rumination, about having two options for Lightning, is incorrect.
For Lightning, we just need to add this 0-value OP_TRUE output always to transactions that require both side signatures (commitment, HTLC-timeout, HTLC-success), and it will always serve as a "hook" for  adding more fees if needed.

@_date: 2018-05-14 21:22:26
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Good morning Luke,
None, but how about use of `SIGHASH_SINGLE` flag? If a dummy output is added as the first, would it not require adjustment of the inputs of the transaction?
In context you are discussing the transaction serialization, though, so perhaps `SIGHASH_SINGLE`, is unaffected?

@_date: 2018-05-17 06:28:54
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Good morning Rusty and list,
We might still want this in general in Lightning; for instance we could make every funding transaction include such an output.  If it turns out, our initial feerate estimate for the funding transaction is low, we can use the `OP_TRUE` for fee-bumping.  This is a win for Lightning since the funding transaction ID remains the same (even in Decker-Russell-Osuntokun, the trigger transaction is signed with `SIGHASH_ALL`, and refers to a fixed funding transaction ID).
Without the `OP_TRUE`-for-fee-bump, we would have to pretend to open a new different channel and RBF the old funding transaction with a new higher-feerate funding transaction, then keep track of which one gets confirmed deeply (there is a race where a miner discovers a block using the older funding transaction before our broadcast of the new funding transaction reaches it).
(we could also feebump using the change output of the funding transaction, but such a change output might not exist for all funding transactions.)

@_date: 2018-05-23 02:15:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Good morning Pieter and list,
It seems to me, naively, that it would be better to make Graftroot optional, and to somehow combine Taproot and Graftroot.
So I propose that the Taproot equation be modified to the below:
    Q = P + H(P, g, S) * G
Where `g` is the "Graftroot flag", i.e. 0 if disable Graftroot, and 1 if enable Graftroot.
A Graftroot spend would need to reveal P and the Taproot script S, then sign the Graftroot script using P (rather than Q).
If an output wants to use Graftroot but not Taproot, then it uses Q = P + H(P, 1, {OP_FALSE}) * G, meaning the Taproot script can never succeed.  Then Graftroot scripts need to be signed using P.
A simple wallet software (or hardware) that only cares about spending using keys `Q = q * G` it controls does not have to worry about accidentally signing a Graftroot script, since Q is not used to sign Graftroot scripts and it would be "impossible" to derive a P + H(P, 1, S) * G from Q (using the same argument that it is "impossible" to derive a P + H(P, S) * G from Q in Taproot).
In a multisignature setting, it would not be possible (I think) to generate a single private key p1 + H(P1 + P2, 1, { OP_CHECKSIG}) that can be used to kick out P2, since that would be signature cancellation attack by another path.
This increases the cost of Graftroot by one point P and a Taproot script (which could be just `OP_FALSE` if Taproot is not desired).  In addition, if both Taproot and Graftroot are used, then using any Graftroot branch will reveal the existence of a Taproot script.  Similarly, using the Taproot branch reveals whether or not we also had some (hidden) Graftroot branch.
Now the above has the massive privacy loss where using Taproot reveals whether or not you intended to use Graftroot too, and using Graftroot reveals whether or not you intended to use Taproot.
So now let us consider the equation below instead:
    Q = P + H(P, H(sign(P, g)), H(S)) * G
A Taproot spend reveals P, H(sign(P,g)), and S, and the witness that makes S succeed.
A Graftroot spend reveals P, sign(P, 1), H(S), and sign(P, Sg), and the witness that makes Sg succeed.
If we want to use Graftroot but not Taproot, then we can agree on the script S = `push(random 256-bit) OP_FALSE`, which can never be made to succeed.  On spending using Taproot, we reveal H(S) but not the S.  Nobody can now distinguish between this and a Graftroot+Taproot spent using Graftroot.  We only need to store H(S), not the original S (but we do need to verify that the original S follows the correct template above).
If we want to use Taproot but not Graftroot, then we can agree to do a `sign(P, 0)`, which definitely cannot be used to perform a Graftroot spend.  The act of signing requires a random nonce to be generated, hence making the signature itself random.  On spending using Graftroot, we reveal H(sign(P, 0)) but not the signature itself.  Nobody can now distinguish between this and a Graftroot+Taproot spent using Taproot.  We only need to store H(sign(P, 0)), not the original signature (but we do need to verify(P, sign(P, 0))).  Some other way of obfuscating the flag can be done, such as H(g, random), with the parties to the contract agreeing on the random obfuscation (but I am unsure of the safety of that).
In effect, instead of the Taproot contract S, we use as contract a one-level Merkle tree, with one branch being an enable/disable of Graftroot and the other branch being an ordinary Script.
Note that even if we are fooled into signing a message sign(P, 1), as long as we made sure that the output paid to a Q = P + H(P, H(sign(p, 0)), H(S)) * G in the first place, it cannot be used after-the-fact to make a non-Graftroot output a Graftroot output.
Simple wallets that use Q = q * G need not worry whether signing arbitrary messages with that key might suddenly make their outputs spendable as Graftroot.
This increases Taproot spends by a hash.
This increases Graftroot spends by a point, a signature, and a hash.
I am not a mathematician and the above could be complete bunk.
The above also does not actually answer the question.
Many users of Bitcoin have been depending on the ability to sign arbitrary messages using a public key that is also used to protect funds.  The use is common enough that people are asking for it for SegWit addresses: Now it might be possible that a valid Script can be shown as an ordinary ASCII text file containing some benign-looking message.  For example a message starting with "L" is OP_PUSHDATA1 (76), the next character encodes a length.  So "LA" means 65 bytes of data, and the succeeding 65 bytes can be an arbitrary message (e.g. "LARGE AMOUNTS OF WEALTH ARE SAFE TO STORE IN BITCOIN, DONCHA KNOW?\n").  Someone might challenge some fund owner to prove their control of some UTXO by signing such a message.  Unbeknownst to the signer, the message is actually also a valid Script (`OP_PUSHDATA1(65 random bytes)`) that lets the challenger trivially acquire access to the funds via Graftroot.
Thus I think this is a valid concern and we should indeed make Graftroot be optional, and also ensure that the simple-signing case will not be a vulnerability for ordinary wallets, while keeping the property that use of Taproot and Graftroot is invisible if the onchain spend does not involve Taproot/Graftroot.

@_date: 2018-10-17 05:17:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Request: OP_CHECKTXOUTSCRIPTHASHVERIFY 
Good morning kim,
This seems to be a specific instance of "covenants".  I believe, that there are vague plans to possibly include OP_CHECKSIGFROMSTACK, which would allow covenants much more generally, but with more complex (clever) SCRIPT.
The specification of the behavior of the opcode is P2SH-focused and is unuseable for SegWit, but possibly it can instead be made a SegWit-only opcode instead (especially, since, by my knowledge, future plans for SCRIPT updates will generally involve only future SegWit versions).
The specification could be improved as below:
The OP_CHECKTXOUTSCRIPTHASHVERIFY will succeed if either of the below are true for all outputs of the transaction that is spending this SCRIPT:
1.  It is a P2WSH whose SegWit version and hash, when concatenated together, are equal to the stack top.
2.  It is a P2WSH or P2SH-P2WSH that is the same as the transaction output being spent.
Otherwise, if any output does not match either of the above, this operation will fail.
Sent with [ProtonMail]( Secure Email.
??????? Original Message ???????

@_date: 2018-10-17 10:22:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Request: OP_CHECKTXOUTSCRIPTHASHVERIFY 
Good morning kim,
An issue with covenants is that the only "good" use case so far is vaults.  Indeed, what you originally gave as a usecase in your first email is in fact a vault.
Here is gmax original bitcointalk post: Since covenants typically require a complex script template, they are a bit heavy (in terms of blockspace) for a "simple" vault application.
I am uncertain if it would be more acceptable to instead target an opcode only for vaults, instead of recursive covenants.
Another issue with vaults/covenants is that they are easily visible onchain.  Recent efforts have been towards making even contract execution be done offchain, guarded only by the contract participants agreeing that the contract has been executed correctly (taproot, also contracts over Lightning).
Sent with [ProtonMail]( Secure Email.
??????? Original Message ???????

@_date: 2018-09-03 09:26:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Reinterpretations of contracts in different 
Good morning all,
I am wondering if there is the possibility of an issue arising when different pay-to-contract schemes are used on Bitcoin.
Specifically, I wonder, if it may be possible to reinterpret the byte serialization of a contract under one scheme as the byte serialization of a different contract under another scheme.  The user may expect to have committed to a contract under the first scheme, but is rudely made aware that she has also committed to a different contract under a scheme she is unaware of.
For instance, if some independent protocol uses pay-to-contract, it may be possible for the contract to be reinterpreted as a Bitcoin SCRIPT under Taproot, leading to a contract that can be reinterpreted as a Bitcoin SCRIPT and allowing a Bitcoin-level UTXO to be stolen without knowledge of the private key.
I thought of this a little while ago and mentioned it here:
Now, it may be possible to use the hash of the contract, but if Taproot uses a hash of the script also and the same hash function is used, then the bytes of the contract could be reinterpreted as a Bitcoin SCRIPT program, possibly leading to a trivial-to-solve SCRIPT with enough hacking.
If this is indeed a concern, then I propose, that pay-to-contract schemes should pay to the below tweak:
     Q = P + SHA256d(P || Scheme || C) * G
Where `Scheme` is 256 bits (32 bytes) scheme identifier.  For Taproot, it could be the genesis block ID.  Then other pay-to-contract schemes must ensure that they use a `Scheme` ID that is different with high probability from other `Scheme` IDs, in order to ensure that reinterpretation of contracts is impossible.

@_date: 2019-04-01 03:02:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Softfork proposal for minimum price of $50k 
Hard NAK.
A minimum 50000 USD : 1 BTC exchange rate implies that the value of 1 USD = 0.00002 BTC at maximum.
However, such a USD value in BTC value maximum makes no sense since the true value of 1 USD = 0.00000000 BTC.
(on Lightning, 1 USD = 0.00000000000 BTC)
In particular, the encoding proposed in the BIP does not support a representation of infinity USD per BTC, such that it is impossible to express the true value of USD under this BIP.

@_date: 2019-04-04 01:55:06
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Smart contracts have traditionally been implemented as part of the consensus rules of some blokchain.  Often this means creating a new blockchain, or at least a sidechain to an existing blockchain.  This writeup proposes an alternative method without launching a separate blockchain or sidechain, while achieving security similar to federated sidechains and additional benefits to privacy and smart-contract-patching.

@_date: 2019-04-04 07:07:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Good morning Ariel,
One might point to the various channel mechanisms (Poon-Dryja, Decker-Wattenhofer, Decker-Russell-Osuntokun) as counterarguments.
Though they require a blockchain as backing, old states are invalidated (Poon-Dryja) or replaceable (Decker-*), without necessarily requiring a blockchain to keep track of all the states.
Suppose our purported smart contract platform supports some kind of covenant system.
This means, that it is possible to make a branch of the contract require that the fund go to a specific address template in the transaction that spends it.
Suppose we use this mechanism to require that the Bitcoin-level transaction pay again to a contract in the same contract platform.
It then becomes possible to make a covenant that requires spending the transaction to the same covenant.
This can allow us to enforce creating an offchain sequence of transactions T1...Tn, such that T2 spends T1, T3 spends T2, etc.
Then the final transaction Tn completes the sequence and pays out according to the rules of Poker, or whatever.
This sequence is anchored on an onchain transaction T0 which enters the funds into the smart contract.
The smart contract platform just signs "blindly" without particularly caring whether the signature went onchain, or even whether the UTXO being spent exists onchain --- all it cares, is that the smart contract can be given witnesses correctly.
Now upon reaching Tn, the winner(s) can just publish the sequence of transactions T1...Tn.
Alternately, they can present the sequence of transactions T1...Tn to all participants, and offer to give back part of the money allocated to fees for all the transactions T1...Tn in exchange for a single transaction that shortcuts all of that and spends to however Tn splits out.
Basically, consider that the Decker-Russell-Osuntokun mechanism starts with a mechanism very much like the above (a sequence of update transactions) and then does some optimizations to allow the final transaction Tn to spend any transaction Ti where i < n.
But the basic concept that the sequence is at all possible, and can be kept offchain, implies this state does not require to be stored onchain at all.

@_date: 2019-04-04 15:03:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Somebody pointed out this to me:
I have updated the page accordingly.
One thing that seems not mentioned in bit-thereum is the "as long as everybody agrees" escape hatch, i.e. one branch which allows spending to anything (including a transaction that violates the letter of the contract) as long as all participants agree.
This is gives my newer mechanism the ability to "fix" buggy contracts if everybody involved can agree to the terms of a new contract, by simply abandoning the existing contract and spending to the new contract.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-04 23:52:20
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Good morning Aymeric,
It is still possible to recover the funds, *if* you can convince all participants of some "fair" distribution of the funds.
You do this by all participants simply signing with their participant keys and taking the first branch of the script.
This branch does not require the participation of the smart contract platform, at all.
If all participants can agree to the result of the smart contract without dispute, then they can exit the platform even after the platform disappears.
Now of course there will be participants who will not cooperate in such a case, for example if they were doing some betting game and "lost".
But at least it gives the possibility of doing so, and it will not be as massive a loss.
Indeed, if the smart contract platform code is open source, it may be possible to set up another implementation of the smart contract platform.
And it would be possible to at least try to convince all participants to switch to that new platform (again, via the "as long as everybody agrees" escape hatch).
Again, this is not possible with current federated sidechains, or Ethereum (if Ethereum fails, all ETH becomes valueless).
I would not lump together Lightning with sidechains.
Indeed, this design moves things closer to true offchain techniques (as in Lightning) than to sidechain techniques.
So while centralized, it is less centralized than a federated sidechains.
Perhaps it can be a next step.

@_date: 2019-04-05 06:00:20
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Building from this ---
It is possible via this mechanism for the federation to be selected by the participants, rather than the federation being defined as a fixed set by the smart contract platform.
Perhaps anyone can advertise themselves (by e.g. locking some bonded amount on the blockchain with a `OP_CHECKSEQUENCEVERIFY`) as being willing to act as trusted executors of smart contracts.
Participants then select such executors they believe to be trustworthy, and what voting quorum of the selected executors is sufficient to convince the participants of the correct execution of the smart contract.
Of course, more choices, more cognitive effort for you mere humans, so probably not a good idea in general.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-08 00:55:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] new BIP: Self balancing between excessively 
Good morning simondev1,
It seems the algorithm would greatly increase validation time.
In particular, if the current limit is removed (as in hardforked proposal) then a 1Tb block can be used to attack the network, since sorting would require looking through the entire block.
Thus, validation time would still limit the practical block sizes that can be deployed with this.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-08 10:45:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Good morning Aymeric,
I prefer to be more precise in my terminology.
Colored coins are not the same as sidechains, and there are colored coins and then there are colored coins.
This mechanism does not propose some change in colored coins.
An important aspect of colored coins is that one can foist them on somebody else to extract things of real value from them, but this mechanism is more strongly for a fixed set of participants.
I strongly suspect that Bitcoin will outlast Ethereum, but that is rather not very related to this topic.
Still, the option to do so exists, and sometimes all that is needed for humans to do the right thing, is to be given the option to do so.
I assumed both were obvious, but I suppose a few more words about those would not be amiss.
Again, I prefer precision in my terminology.
For me, a sidechain is a blockchain of some sort.
In particular, a kind of Merklized singly-linked list containing representations of transformations of state, is how I define blockchain to be.
No such Merklized singly-linked list exists in Lightning Network, thus I do not consider it, "blockchain".
And thus I do not consider it "sidechain", as a sidechain is a blockchain.
Current LN does use "shachains" by Rusty, but shachains are not Merklized singly-linked lists, but are instead a kind of inverse mountain range structure.
Still, one might consider both federated sidechains and Lightning Network to have a "federated" offchain structure.
This is because the coins on the Bitcoin blockchain are locked to a multisignature and activity is not recorded on the Bitcoin blockchain.
However, in LN, each channel is a 2-member federation (you and a counterparty) and the mechanism in LN requires consensus (2-of-2) rather than a quorum (m-of-n).
This greatly increases the security of LN: the owner of funding on an LN channel can always refuse to sign an update if the other member of the federation is taken over.
Compare this to the quorum that typical federations have, where takeover of a sufficient quorum is enough to steal funds from the remaining federation.

@_date: 2019-04-15 02:59:44
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] List of proposals for hard fork/soft fork 
Good morning LORD YOUR EXCELLENCY,
May it please you to be informed the below are likely to be included in some kind of upcoming softfork for SegWit v1:
1.  Schnorr signatures.
2.  MuSig.
3.  Taproot.
4.  `SIGHASH_NOINPUT`.
5.  Signature aggregation.  May it please you to be informed, that "Schnorr" enables signature aggregation, but is not signature aggregation itself.
6.  MAST.
The above may or may not be an exhaustive list, your excellency.
Of these, I believe only `SIGHASH_NOINPUT` has a BIP, may it please your excellency to learn that it is BIP 118.
However, I am sorry to inform your excellency, as I understand the `SIGHASH_NOINPUT` that will eventually reach Bitcoin Core will not match the current version of BIP118.
To improve on the possibility of incorrect use of `SIGHASH_NOINPUT`, it is proposed that every input that is signed with a `SIGHASH_NOINPUT` signature additionally require a signature without `SIGHASH_NOINPUT`.
For other details, I am sorry to inform your excellency, I have no reliable knowledge.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-18 05:33:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smart Contracts Unchained 
Good morning Nadav,
Yes, transporting contracts over a single direct channel is always possible.
When Lightning switches to Decker-Russell-Osuntokun ("eltoo"), do note that contracts with an absolute timelock must be forced onchain earlier than the absolute timelock by the CSV requirement of the channel (unilateral close time).
With current Poon-Dryja channels, transported contracts must be augmented by a 2-of-2 on all branches, which can be done by adding a 2-of-2 multisig on the escrow branch, using temporary keys.
The purpose of the 2-of-2 is to enforce that the only valid claims to the contract have an `nSequence` representing the unilateral close time of the channel.
xref. HTLC-timeout and HTLC-success transactions in BOLT
Transporting over multiple hops requires that compliance to a contract makes one side reveal information that the other side does not know, together with some kind of timeout/backoff.
Practically speaking, only HTLC-type contracts can be transported.
For example, DLCs will have many possible branches where the Oracle provides a signature for one branch, and this signature is what is learned by the other party in the contract.
In addition, DLCs for practical use require a timeout (in case the Oracle fails to reveal the signature on the appointed time).
Thus, far fewer contracts can be transported over the network.
(Of note is that a Lightning channel is itself a contract (that is transportable only within a direct channel); this is the basis of channel factories, where the factory level is effectively a "channel" with more than two participants, and transporting Lightning channels instead of HTLCs)
(You may be interested in looking at the "Fulgurite" effort)
OF note is that DLCs have an Oracle.
I observe that escrow services (which are specializations of the Smart Contracts Unchained technique) are basically oracles also.
If DLCs can transport their oracle signatures over multiple hops, then it should be possible for Smart Contracts Unchained to transport the federation/escrow signatures over multiple hops also.
I do not know the math behind DLCs enough to be certain, however, and leave it to better mathematicians than I.
??????? Original Message ???????

@_date: 2019-04-18 16:55:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning Ruben,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
I presume you mean a chain split?
I suppose a minority miner that wants to disrupt the network could simply create a *valid* block at block N+1 and deliberately ignore every other valid block at N+1, N+2, N+3 etc. that it did not create itself.
If this minority miner has > 10% of network hashrate, then the rule of thumb above would, on average, give it the ability to disrupt the SPV-using network.
Consider that SPV-using nodes would be disrupted, without this rule, only by >50% network hashrate.
It is helpful to consider that every rule you impose is potentially a loophole by which a new attack is possible.

@_date: 2019-04-19 00:25:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning Ethan,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Let us then revert to the original scenario.
Suppose a supermajority (90%) of miners decide to increase inflation of the currency.
They do this by imposing the rule:
1.  For 1 block, the coinbase is 21,000,000 times the pre-fork coinbase value.
2.  For 9 blocks, the coinbase is the pre-fork value.
3.  Repeat this pattern every 10 blocks.
The above is a hardfork.
However, as they believe that SPV nodes dominate the economy, this mining supermajority believes it can take over the network hashpower and impose its will on the network.
At height S+1, they begin the above rule.
This implies that at heights S+1, S+11, S+21, s+31... the coinbase violates the pre-hardfork rules.
At around height S+9, the minority miners generate an alternate block at height S+1.
So SPV nodes download S+9 and S+8 on the longer chain, and see nothing wrong with those blocks.
At around height S+18, the minority miners generate an alternate block at height S+2.
So SPV nodes download S+18, S+17, S+16 and again see nothing wrong with those blocsk.
This can go on for a good amount of time.
With a "rare enough" inflation event, miners may even be able to spend some coinbases on SPV nodes that SPV nodes become unwilling to revert to the minority pre-hardfork chain, economically locking in the post-hardfork inflation.
Again: every rule is an opportunity to loophole.

@_date: 2019-04-19 02:53:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning Ethan,
Thank you for clarifying, I understand better now.
It seems that minority miners can disrupt SPV clients such that SPV clients will download 2 blocks for every block the minority miner can find, not 1.
This can be done by simply making multiple 1-block chainsplits, rather than a single persistent chainsplit, and alternating split-off and non-split-off.
For instance, such a minority miner might split at S+1, forcing SPV clients to download S+1 and S+2.
Then the minority miner splits at S+3, forcing SPV clients to download S+3 and S+4.
With a mere 33% hashrate, this can force SPV clients to download every block, i.e. become a fullnode anyway.
Since there exist pools with >33% hashrate, the above attack is possible so the only solution is to become a fullnode anyway.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-19 04:48:23
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning Ethan,
I understand.
It seems a reasonable point to do so.
As I understand it, this requires that UTXO commitments be mandatory.
In particular, if UTXO commitments were not mandatory, it would be trivial to force chainsplits at heights where a UTXO commitment was not made, and force an SPV node to download more blocks backwards until a block with a UTXO commitment is found.
More difficult is: how can an SPV node acquire the UTXO set at a particular block?
Fullnodes automatically update their UTXO set at each block they accept as tip.
Reversing the blocks to update the UTXO set at a particular past time would require a good amount of CPU and memory.
Thus any service that can provide the actual UTXO set at each block would potentially be attackable by simply requesting enough past blocks.

@_date: 2019-04-20 01:59:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning,
There is no safe way to use UTXO sets without identifying who is telling you those sets are valid, or making it expensive to lie.
The first option requires trust and is weaker than SPV, the second requires committing to a proof-of-work (and probably best to fold it into the Bitcoin blockchain if so).
You would get the UTXO commitment from the previous block (if the UTXO commitment is in the coinbase, then all you need is the Merkle proof of the coinbase).
This makes no sense.
In order to validate block N, you need to know that every UTXO spent by a transaction in block N is valid.
The UTXO you want to validate is located in some other block, not on the single block you are verifying.
Thus the non-existent fraud proof can only be validated by loading the block of the UTXO purported to be spent, and every block between that and the current block you are verifying, i.e. fullnode.
Either that or you trust that every peer you have is not omitting the proof.

@_date: 2019-04-20 04:45:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving SPV security with PoW fraud proofs 
Good morning Ruben,
UTXO sets can only be validated by actually running the entire blockchain, i.e. fullnoding.
What BIP157 does is summarize data that is within a block, thus validating them can be done simply by downloading the block in question.
UTXO sets summarize data in the entire blockchain, hence proper validation requires downloading the entire blockchain.
Thus it cannot be a comparison point.
But peers can be set up to allow you to hear of all chains while denying you proof of the invalidity of some UTXO.
This is precisely the "data unavailability claim" that shot down the previous fraud proofs (i.e. absence of proof is not proof of absence, and proof of UTXO validity was defined by proof of absence of any intervening spend of the UTXO).
Perhaps in combination with BIP157/158 it may be possible, if the filters contain UTXO spends and a BIP158 filter was committed to on-chain.
Then a proof of absence could be done by revealing all the BIP158 filters from the UTXO creation to the block being validated, as well as the blocks whose BIP158 filters matched the UTXO and revealing that no, they actually do not spend the UTXO.
Tangentially, we cannot just magically commit to anything on the blockchain.
Header blocks commit to block data and commit to some other header block.
All those header blocks and the block data need to be stored and transmitted over the network somehow, even though they are "only" being committed to.
Thus, if you are adding new information to be committed, that may increase the resource usage of fullnodes.
So if UTXO set commitments, or utreexo commitments, or BIP158 filter digests, etc. are committed to in the coinbase, they have to be stored somehow in fullnodes the entire UUTXO set, or the actual utreexo structure, or the actual BIP158 filter, etc. at each block.
Otherwise it would be pointless to store those commitments since it would not be possible to somehow acquire the data being committed to after-the-fact.
This is probably still better than BIP37 but we should still be aware the additional load on fullnodes.

@_date: 2019-04-22 00:07:20
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Payjoin2swap: Enabling Payjoin Without Merchant 
Payjoin vs. Existing Equal-Valued CoinJoins
As payjoin transfers Bitcoin value, it is planned to be used for payments (i.e. spending) rather than hodling.
The issue is that it appears that payjoin cannot be used by hodlers, since hodlers are not interested in paying, but in saving.
Yet hodlers want to retain their privacy also.
They do not want their exchange snooping on them and finding out that they have not moved their coins from cold storage.
For long enough timeframes, a KYC-demanding exchange might eventually get hacked or hostilely taken over, with the identifying information of the hodler in the logs of the exchange.
The hodler would prefer to have those who have gotten this identifying information to believe that the value has been spent elsewhere, rather than in UTXOs that have not been moved.
At the same time, they are not interested in purchasing items and therefore cannot utilize payjoin as-is.
After all, payjoin requires a transfer of value from one participant to another.
(they can continue to utilize existing equal-value coinjoins, if those are acceptable to the hodler)
The key insight here is that hodlers "buy Bitcoin".
Thus there is still the possibility of hodlers using payjoin, by using bitcoins in a payjoin to buy bitcoins of similar value.
The same insight underlies the observation that, as long as LN-to-onchain swap services exist, it is possible to gain incoming capacity on Lightning channels immediately if one has bitcoins already.
One can observe that one gains capacity to receive on Lightning if one spends on Lightning: the technique, then, is to spend BTC on Lightning to buy BTC off Lightning (i.e. onchain).
We shall see later that Lightning has some parallels to payjoin2swap.
Using payjoin for purchases of bitcoins using bitcoins has the massive advantage that we can build software that interacts solely with the Bitcoin blockchain.
This simplifies the deployment of payjoin for bitcoin-to-bitcoin swaps, as payjoin for merchant deployment requires interaction of the Bitcoin blockchain, and the infrastructure of the merchant.
As most merchants are more interested in their product rather than in Bitcoin technology, we can expect that deployment of payjoin will be slower compared to a payjoin2swap deployment made by Bitcoin tech weenies.
* This idea is not original to me: a [feature request on AdamISZ/CoinSwapCS]( by Chris Belcher proposes P2EP for CoinSwapCS.
  However, I have not found any follow up on this idea and the fleshed-out protocol and thoughts below are my own.
Payjoin2swap On the Blockchain
In a payjoin2swap, what is visible on the blockchain is two transactions that occur at about the same time.
They appear to pay two different addresses, and each has a change output.
The payment addresses are then spent almost immediately, as a self-to-self transfer (one input, one output), while the change output may or may not get spent any time soon.
If the wallet used follows [ZeroLink]( strictly, then the four addresses (two payment addresses, two change addresses) do not seem to be related, and there would be no reason to think that the two transactions are related.
In reality, one payout and one change address each are actually owned by the two participants in a payjoin2swap.
These two transactions all pay out to P2WPKH addresses, and (if the payjoin2swap protocol is followed correctly) we do not see anything special: no 2-of-2, no HTLCs, nothing.
This is possible even without Schnorr-based signature schemes, by use of [2p-ECDSA]( to create a 2-of-2 multisig in ECDSA among the participants that operates the protocol.
Any special contracts are hosted inside a temporary offchain cryptocurrency system (slightly like a Lightning channel), and are not exposed if the protocol runs to completion.
The transactions have an `nLockTime` near the time they are confirmed onchain, looking like Bitcoin Core behavior.
Participants in a payjoin2swap need to own two UTXOs onchain; the values need not be equal.
Example 1
Suppose Alice owns a 900 mBTC and a 10 mBTC UTXO.
And suppose Bob owns a 15 mBTC and 4 mBTC UTXO.
Both of the above users would have difficulty with current Wasabi coinjoins which use equal values of around 100mBTC: Alice would have to run several rounds, while Bob cannot participate at all.
What would be seen onchain would be:
1.  Alice 900mBTC, Bob 4mBTC -> Alice&Bob 903mBTC, Bob 1mBTC
2.  Alice 10mBTC, Bob 15mBTC -> Alice 7mBTC, Alice&Bob 18mBTC
Followed quickly by:
1.  Alice&Bob 903mBTC -> Alice 903mBTC
2.  Alice&Bob 10mBTC -> Bob 10mBTC
By use of 2p-ECDSA, the Alice&Bob addresses are P2WPKH addresses and do not reveal that they are actually 2-of-2 multisig.
Obviously, all the public keys would be different.
The above would lead blockchain analysts to believe that Alice is being paid 903mBTC from some confusing combination of Alice and Bob, and that Bob is being paid 10mBTC from some confusing combination of Alice and Bob.
Example 2
Under ZeroLink, post-mix coins cannot be joined, even with other post-mix coins, except as part of a mix operation.
Suppose Alice has a 90mBTC UTXO and a 80mBTC UTXO that are both post-mix coins.
Now suppose Alice needs to pay Carol 120 mBTC.
Alice cannot simply join the post-mix coins and pay 120mBTC and get 50mBTC change, as it can reduce the privacy of the mix participants.
Under ZeroLink, Alice needs to re-mix the post-mix coins and get at least 120mBTC from the mix before paying.
This can be problematic if the only available mix is an equal-value coinjoin where the equal value is less than 120mBTC.
With payjoin2swap, Alice simply needs to find a Bob who has at two UTXOs that are at least 30mBTC each.
Suppose Alice finds a Bob with 37mBTC UTXO and 66mBTC UTXO.
They make:
1.  Alice 90mBTC, Bob 37mBTC -> Alice&Bob 120mBTC, Bob 7mBTC
2.  Alice 80mBTC, Bob 66mBTC -> Alice 50mBTC, Alice&Bob 96mBTC
1.  Alice&Bob 120mBTC -> Carol 120mBTC
2.  Alice&Bob 96mBTC -> Bob 96mBTC
There is no transaction directly tying the two post-mix UTXOs of Alice together, but we have effectively "consolidated" the inputs of Alice.
This is in fact quite similar to "rebalancing" channels on Lightning.
Post-mix UTXOs under ZeroLink and with payjoin2swap function similarly to channels on Lightning: you can transfer from one UTXO to another in order to make a large payment, just as you might transfer funds from one channel to another in order to make or forward a large payment on Lightning.
Payjoin2swap Swap Protocol
1.  Alice and Bob agree on various details.
    a.  Who Alice and Bob is.
        Alice is the one who knows the secret x and pays to an HTLC with higher time L1.
        Alice gives h(x) to Bob.
    b.  The start time (a blockheight) of the protocol, Ls.
    c.  An amount to swap.
        This should be smaller than the smallest UTXO that will be involved in the swap.
    d.  A "safe" confirmation depth for anchoring, D.
        This could be D = 6 as per the Bitcoin whitepaper.
    e.  Twp future blockheights, L0 and L1, such that Ls + D < L0 < L1.
    f.  Various public keys whose private keys are known by Alice, and various public keys whose private keys are known by Bob.
        This can be done by using a "base point" and deriving the keys from tweaking this base point.
        Let us call this the (Alice|Bob) "$name" basepoint-derived (public|private) key.
        Give a basepoint B (private key b such that B = bG), we can use something like B + h(B | "$name") as the basepoint-derived public key (private key is left as a trivial exercise to the reader).
    g.  A public key whose private key is known by Alice, and a public key whose private key is known by Bob.
        If we use the "base point" technique above, these keys cannot be derived from the same base point as other keys since the last step of this protocol involves sharing this private key to the other participant.
        Let us call this the (Alice|Bob) non-basepoint (public|private) key.
    h.  The feerate.
2.  Alice and Bob generate two 2p-ECDSA federation public keys.
    a.  The "Alice-to-Bob federation key" involves the Alice non-basepoint key, and the Bob "federation" basepoint-derived key.
        As per 2p-ECDSA, Alice can compute this public key by multiplying Alice non-basepoint private key with Bob "federation" basepoint-derived public key.
        Bob can compute this public key by multiplying his private key with the Alice public key.
    b.  The "Bob-to-Alice federation key" involves the Bob non-basepoint key, and the Alice "federation" basepoint-derived key.
        Similar to above as per 2p-ECDSA.
3.  Alice and Bob select from their UTXOs:
    a.  A set of UTXOs whose sum is greater than the swap value.
        Call these the "swap UTXOs", i.e. Alice swap UTXOs, Bob swap UTXOs.
    b.  A UTXO to use for a payjoin receive.
        Call this the "payjoin UTXO", i.e. Alice payjoin UTXO, Bob payjoin UTXO.
4.  Generate (but not sign) the "pre-swap transactions".
    Alice and Bob exchange the txids of these transactions and which output is the swap output.
    Their `nLockTime` is Ls.
    a.  The "Alice pre-swap transaction" spends the Alice swap UTXOs and pays to the "Alice-to-Bob federation public key" the agreed swap amount (the swap output).
        Any extra value is put into a change output that Alice controls.
    b.  The "Bob pre-swap transaction" spends the Bob swap UTXOs and pays to the "Bob-to-Alice federation public key" the agreed swap amount, and extra value into a Bob-only change output.
5.  Generate and sign the "pre-swap backout transactions", then exchange signatures.
    This spends the swap output of the "pre-swap transactions" and return it to the original payer.
    Both Alice and Bob can generate these themselves with information already exchanged before.
    a.  The "Alice pre-swap backout transaction" spends the "Alice pre-swap transaction" output and pays the entire value to the Alice "backout" basepoint-derived public key.
        Its `nLockTime` is L1.
    b.  The "Bob pre-swap backout transaction" spends the "Bob pre-swap transaction" output and pays the entire value to the Bob "backout" basepoint-derived public key.
        Its `nLockTime` is L0.
6.  Validate the signatures for "pre-swap backout transaction" of the counterparty.
7.  Sign the "pre-swap transactions" and exchange the actual transactions with witness.
8.  Validate the pre-swap transaction from the counterparty:
    a.  All inputs are signed correctly and are SegWit.
        All inputs are unspent.
    b.  The indicated "swap" output does pay the correct amount to the correct federation public key.
        e.g. Bob validates that the Alice pre-swap transaction pays the agreed swap amount to the Alice-to-Bob federation key.
9.  From this point onward, if either of the pre-swap transactions appears in the mempool or confirmed, abort.
    Attempt to spend all your own swap UTXOs, and if the pre-swap transaction is confirmed, wait for L2 and use the pre-swap backout transaction.
10.  Check if the pre-swap transaction from your counterparty spends a UTXOs in your blacklist.
     If there are, attempt to spend all your own swap UTXOs and abort.
11.  Add the UTXOs spent by the counterparty pre-swap transaction to your blacklist.
12.  Create the "payjoined swap transaction" for your counterparty and exchange their txids and the value of the swap outputs.
     i.e. Bob creates the Alice payjoined swap transaction and vice versa.
     Start with the counterparty pre-swap transaction.
     Insert your own payjoin UTXO at a random index of the inputs.
     Then increase the swap output according to the value of the payjoin UTXO, minus the feerate times the size of the additional input (including any witness needed).
13.  Generate and sign the "payjoined swap backout transactions" and exchange signatures, and whether the added output was before or after the existing output.
     Start with the counterparty pre-swap backout transaction.
     Change the input to spend the payjoined swap transaction swap output instead.
     Then add a new output that pays to the counterparty "payjoined backout" basepoint-derived public key, the difference of the input minus the other output, minus the feerate times the size of the additional output.
14.  Exchange the payjoined swap transactions and sign them completely.
15.  Validate that the payjoined swap transactions are signed correctly, and that the additional payjoin UTXO from the counterparty is an unspent SegWit output.
16.  Broadcast both payjoined swap transactions and wait for them to be confirmed to height D.
     If L0 is too near and both of the payjoined swap transactions are still unconfirmed, abort and attempt to respend all UTXOs you control that are involved.
     If L0 is too near and one of the payjoined swap transactions is still unconfirmed, abort, and if it is your payjoined swap transaction that is unconfirmed attempt to respend all UTXOs, or if not, just wait for L2 and broadcast the corresponding payjoined swap backout transaction.
17.  Generate the "HTLC offer transactions".
     Each can generate these transactions without communicating with the other.
     The `nLockTime` is the block height at which the payjoined swap transactions were confirmed at depth D.
     a.  The Alice HTLC offer:
         i. Spend the Alice payjoined swap transaction output.
         ii. Compute backout address N as the 2p-ECDSA of (1) Alice "alice-htlc-fail" basepoint-derived key (2) Bob "alice-htlc-fail" basepoint-derived key.
             Pay to `OP_IF OP_HASH160  OP_EQUALVERIFY  OP_ELSE L1 OP_CHECKLOCKTIMEVERIFY OP_DROP  OP_ENDIF OP_CHECKSIG`
     b.  The Bob HTLC offer:
         i. Spend the Bob payjoined swap transaction output.
         ii. Compute backout address O as the 2p-ECDSA of (1) Alice "bob-htlc-fail" basepoint-derived key (2) Bob "bob-htlc-fail" basepoint-derived key.
             Pay to `OP_IF OP_HASH160  OP_EQUALVERIFY  OP_ELSE L0 OP_CHECKLOCKTIMEVERIFY  OP_ENDIF OP_CHECKSIG`
18.  Alice generates the "Alice HTLC offer failure transaction".
     This spends the Alice HTLC offer via the timelock path.
     It thus has an `nLockTime` equal to L1.
     It gives the agreed swap value to Alice "backout" basepoint-derived public key, and the remaining value to Bob "payjoined backout" basepoint-derived public key.
     Alice and Bob generate a signature for this via 2p-ECDSA.
     Then they generate a signature for the Alice HTLC offer.
19.  From this point if the Alice HTLC offer appears on mempool or is confirmed, Alice must wait for L1 and broadcast the Alice HTLC offer failure transaction.
20.  Bob generates the "Bob HTLC offer failure transaction".
     This spends the Bob HTLC offer via the timelock path, with an `nLockTime` equal to L0.
     It gives the agreed swap value to Bob "backout" basepoint-derived public key, and the remaining value to Alice "payjoined backout" basepoint-derived public key.
     Alice and Bob generate a signature for this.
     Then they generate a signature for the Bob HTLC offer.
21.  From this point if the Bob HTLC offer appears on mempool or is confirmed, Bob must wait for L1 and broadcast the Bob HTLC offer failure tranasction, or for Alice to claim it by revealing x (and Bob must then broadcast and claim the Alice HTLC offer).
22.  Alice gives Bob x.
23.  Bob gives the Bob non-basepoint private key.
24.  Alice gives the Alice non-basepoint private key.
25.  Alice must now spend the Bob payjoined swap transaction swap output on or before L0.
     Bob must now spend the Alice payjoined swap transaction swap output on or before L1.
     They can send it to themselves, send it to any pending payments they might need to give, or find another partner to swap with and make sure to complete up to step 16 in the new cycle before their respective timeouts.
     a. Alice must spend from the Bob-to-Alice federation pubkey address.
        This is a combination of the Bob non-basepoint key and the Alice "federation" basepoint-derived key.
        Since Bob has given the Bob non-basepoint key and Alice knows the private keys in all the Alice basepoint-derived keys, Alice can now spend it without Bob authorization.
     b. Bob must spend from the Alice-to-Bob federation puobkey address.
        Again, Alice has given enough information to Bob for Bob to completely know the private key wihtout authorization from Alice.
Creating Plausible Transactions
For simplicity, let us consider the case where Alice and Bob each own two UTXOs each.
This means that the payjoined transactions have two inputs, and are likely to have two outputs.
Fortunately, a good number of transactions onchain are two-input two-output affairs.
However, we should attempt to build plausible transactions.
For example, suppose we have:
* Alice has 70mBTC and 800mBTC UTXOs.
* Bob has 100mBTC and 7 mBTC UTXOs.
They could propose to swap 85mBTC.
Then Alice would give the Alice pre-swap transaction as:
* Alice 800mBTC -> Alice 715mBTC, Alice&Bob 85mBTC
And Bob would make the Bob pre-swap transaction:
* Bob 100mBTC -> Alice&Bob 85mBTC, Bob 15mBTC
After payjoining:
* Alice 800mBTC, Bob 7mBTC -> Alice 715mBTC, Alice&Bob 92mBTC
* Alice 70mBTC, Bob 100mBTC -> Ailce&Bob 155mBTC, Bob 15mBTC
The first transaction above is implausible: if the payment amount were 715mBTC, then a fee-reducing coin selector would have just chosen the 800mBTC.
Thus, we should ensure that there exists one output which is larger than the sum of all inputs except the smallest input.
Otherwise, a fee-reducing coin selection algorithm would have eliminated the smaller coins from the transaction.
For the case where both Alice and Bob each have two inputs to mix, we could impose the below rules:
1.  Alice and Bob generate pre-swap transactions using the smaller UTXO they have.
    This implies that the swap amount must be less than or equal to the smallest UTXO.
2.  Alice and Bob check if their other coin (the one they will propose of payjoining) plus the swap amount is greater than the UTXO consumed in the pre-swap transaction from the other side.
    i.e. Alice checks the Bob pre-swap input is smaller than the Alice larger UTXO plus the swap amount.
    If thise fails, they may propose to the counterparty to split their smaller UTXO.
Thus, in the above, Alice and Bob should have started their pre-swap transactions as:
* Alice 70mBTC -> Alice 64mBTC, Alie&Bob 6mBTC
* Bob 7mBTC -> Alice&Bob 6mBTC, Bob 1mBTC
Then the payjoined versions would be:
* Alice 70mBTC, Bob 100mBTC -> Alice 64mBTC, Alice&Bob 106mBTC
* Alice 800mBTC, Bob 7mBTC -> Alice&Bob 806mBTC, Bob 1mBTC
Linking (and Overlinking) Payjoin2swap
Using the heuristic above leads to the strong tendency that the difference of an input to the smaller output will be the swap value.
(or if the transaction is 1 output, then one of the inputs will be exactly the swap value)
Also, the main transactions will usually be in the same block and will usually be 2-input transactions.
There are fewer possibilities, thus it is a possible avenue for blockchain analysis to attempt linking.
It is useful to remember that generating this transaction is two steps: first create the pre-swap transaction with the UTXO of one side, before adding a new UTXO for the other side.
Suppose our agreed swap value is S.
Then one side creates a pre-swap transaction from a UTXO V:
* V -> V - S, S
The other side can then monitor the mempool for two-input two-output/one-output transactions and compute an "apparent swap value" from the difference of an input and the lower input (or 0 if one-output).
Suppose it finds S'.
Then the other side can simply synthesize a UTXO of value V - S + S'.
The other side can do this by spending a larger UTXO and splitting out that value.
This leads to a swap transaction of:
* V, V - S + S' -> V - S, V + S'
The above still leads to plausible transaction as defined above; the V + S' output will always be larger than the inputs, thus having to combine both inputs is plausible.
In doing so, the existing mempool transaction with apparent swap value S' becomes potentially linked to this transaction, even though it is completely unrelated.
Further, while the swap output is identifiable as V + S', it is not possible to be sure whether S or S' is the swap value.
The difference between V + S' and V is S', while the difference between V + S' and V - S + S' is S, so either could be the swap value.
This operation to mislead blockchain analysis can be called "overlinking", as it can lead blockchain analysis to believe that an unrelated transaction is linked to one of the swap transactions.
Participants should refuse to swap unlesss they can overlink at least one (and preferably both) of the payjoined swap transactions.
If the swap is not possible, both sides can agree to lower the V value of one side (provided V >= S) until the V - S + S' is achievable by the other side.

@_date: 2019-04-22 17:06:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Payjoin2swap: Enabling Payjoin Without Merchant 
Good morning list,
I observe that if the Lightning Network supports:
1.  Wumbo channels (>167.772215mBTC channels)
2.  Dual-funded channels
Then payjoin2swap is implementable using LN operations.
Given two UTXOs V and W you want to transfer some value S from V to W.
1.  Connect to an LN node on Tor and listen to gossip, then disconnect.
2.  Create two fake node addresses (A and B) and assign a unique Tor hidden service to each.
3.  Select two actual LN nodes with long uptime (use the lifetime of the channels they have as a rough estimate) and a short distance between them with good capacity and many alternate routes between them.
4.  Connect A to one node and create a channel using V.
    Indicate that the channel should be unpublished (the LN protocol has a flag existing for this already).
    Publishing channels is good for privacy if you intend to stay long on LN (it invites "normal" traffic to hide your own payments in), but we do not intend to stay long on LN here, so keep the channels unpublished.
5.  Connect B to the other node and create a channel with dual-funding, requesting at least S from the other node to put in the channel, using W as your own UTXO.
    Again indicate the channel should be unpublished.
6.  Transfer S satoshi over LN from the channel of A to the channel of B.
7.  Cooperatively close both channels.
    It is possible to time the closes to be some blocks apart to make them harder for blockchain analysis to link.
This rides payjoin2swap on top of existing infrastructure.
The drawback is that it is unlikely that existing LN implementations will apply ZeroLink for onchain funds, since much of the privacy on LN is with paying over the long-lived offchain channels.
This can erode privacy if either of the nodes you connect to uses the result of the cooperative closes to later fund a published channel, possibly mixing it with other coins of their own.

@_date: 2019-04-29 01:46:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] IsStandard 
Good morning Aymeric,
Different versions may consider different output scripts standard.
Your rule of thumb, post-SegWit, should be:
* If not P2PKH or P2WPKH, then wrap it in a P2SH or P2WSH.
There are more standard outputs accepted, but you can be reasonably sure that P2PKH, P2WPKH, P2SH, and P2WSH are the only standard output scripts that are likely to remain supported in the mid-future (5->10 years from 2019).
Lightning uses P2WSH for its scripts.
Any m-of-n signing scheme in Bitcoin is P2SH (usually) or P2WSH (if you are cool).
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-04-30 04:29:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] IsStandard 
Good morning Aymeric,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
I am uncertain what you mean by this.
For P2PKH and P2WPKH, you must present a hash of a public key.
You cannot present a hash of anything else.
The P2PKH template can be interpreted as a script, but is actually recognized as a template by most current nodes (in a way that is consistent with interpreting it as a script).
For P2SH and P2WSH, you must present a hash of a script.
It is more helpful to consider that *today* nodes recognize particular patterns (P2PKH, P2WPKH, P2SH, P2WSH) as templates and not as scripts to be executed.
In any case, if you want to make anything more complicated than "single signer" you should use P2SH or P2WSH regardless, and give your script.
If you want to assure somebody that a particular P2SH or P2WSH commits to a particular policy, just expose the policy script to them and have them (i.e. their client software) verify that the policy is what the user wants and that when hashed it matches the P2SH/P2WSH.
As Luke said, nodes can have any policy for propagating transactions.
However it is generally expected that P2PKH, P2WPKH, P2SH, and P2WSH will be propagated by a majority of nodes, if only because those are reliably "passed" by `isStandard` in the default latest Bitcoin Core and most people will not modify the Core code.
Generally, anything that isn't P2PKH, P2WPKH, P2SH, or P2WSH will not likely be propagated by the network.
You *could* still coordinate with one or more miners to get it mined: you can put anything in the block, it is simply that most nodes will not inform miners about transactions that do not pay out to P2PKH, P2WPKH, P2SH, or P2WSH.

@_date: 2019-08-06 02:54:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Chris,
Would it not be possible the below?
* I rent some funds from Dmitry.
  I agree to pay him 0.5 BTC for this service of putting up 50BTC from Dmitry UTXO.
* I also own 50BTC myself in a separate UTXO.
* We create a funding transaction paying out to a Schnorr MuSig output that is 2-of-2 between us.
  This spends Dmitry UTXO 50 BTC and my UTXO 50BTC.
  We only create this yet and do not sign.
* We create a backout transaction, probably with `nLockTime`, paying out 50.5BTC to Dmitry and 49.5BTC to me.
  This spends the funding transaction.
  We sign this using MuSig.
* After we exchange the signatures of the backout transaction, we exchange signatures for the funding transaction.
* Now we have a common 100BTC UTXO (indistinguishable from other Schnorr single-sig UTXOs) that can be used as fidelity bond for me.
  This is the output of the funding transaction.
The above can be scaled up so I can rent arbitrary amounts of coin from many different people, who are assured of getting their funds back, in exchange for a fidelity bond / advertisement, and thus greatly destroying the properties of the V^2 tweak.
(The ability to have shared ownership of UTXOs is a powerful feature of Bitcoin, and backs its ability to scale, as witnessed with Lightning Network and channel factories.)

@_date: 2019-08-06 23:33:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning all,
It might be useful to remember that there exists pressure to pool proof-of-work due to tiny non-linearities caused by Proximity Premium and Variance Discount flaws.
Similarly, any non-linearity in any fidelity bond scheme exerts the same pooling pressure.
Deliberately increasing the non-linearity to V^2 worsens the pooling pressure, not lessens it.
(I wonder if instead going the opposite way and doing V^0.999 might work better; I have not figured all the implications of such a scheme and leave it to the reader.)
If my understanding is correct, efforts to expand ECDSA to more than two-party n-of-n "true" multisignatures already are ongoing.
One might attempt to use transaction malleability as a protection, and require that transactions that put up bond TXOs should spend from at least one ***non***-SegWit output, so that the scheme as described fails (as the funding txid is malleable after-the-fact).
But the scheme as described only considers ways to securely aggregate *within* the Bitcoin universe.
I have recently learned of a spacce called the "real world", wherein apparently there exist things as "contract law".
It seems to me this "contract law" is a half-baked implementation of Bitcoin cryptographic smart contracts.
By what little I understand of this "contract law", it would be possible for an aggregator to accept some amount of money, with a promise to return that money in the future with some additional funds.
If the aggregator fails to uphold its promise, then some (admittedly centralized) authority entity within the "real world" then imposes punishments (apparently inspired by similar mechanisms in Lightning Network) on the aggregator.
Such arrangements (accepting some money now with a promise to return the money, plus some interest earned, in the future) apparently already exist in this "real world", under the name of "time deposits".

@_date: 2019-08-07 11:20:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Chris,
At the cost of *greatly* strengthening aggregation.
Suppose there is currently many makers, all with roughly-equal bonds.
Suppose I were to approach two of these makers, and offer to aggregate their bonds.
The combined bond would, because of the V^2 term, have 4 times the weight of the other makers.
Thus, approximately I can earn a little below 4 times what one other maker does.
I offer 1.5x what one maker does to both of those makers and keep a little below 0.5x to myself.
1.  I earn without putting any of my money into bonds.
    I just need starting capital to pre-pay for the rents.
2.  I get to learn a little below 4x more CoinJoins than other makers.
    This increases my earnings further since I can sell this privacy information, and I also get an advantage compared to other non-aggregating spies.
It seems to me not to fix the root issue, i.e. makers who make for the purpose of gathering privacy information, even if it might fix sybil attackers.

@_date: 2019-08-07 11:35:34
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Dmitry,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Possibly not so much?
The wallet need only sign two things:
1.  The fidelity bond itself.
2.  The backout transaction.
Both can be done in a single session, then the private key involved can be erased permanently from memory.
Only the signature for the backout needs to be stored, and this can be safely stored without encryption by publishing to any cloud service --- others getting a copy of the signature does not let them change the signature to authorize a different transaction.
It would be enough to write the signing code in C and use special OS calls (which most languages higher than C do not expose) to allocate memory that will never be put in swap.
Then generate the private key using that memory, then clear it after usage before deallocating to the OS.
I believe `libsecp256k1` makes this easy.
Unless part of the bond process requires that the taker do a challenge "sign this random nonce for me", but of note is that it would have to impose this on all makers.
But if so, consider again this:
1.  There exists two non-spying makers with nearly-equal bond values.
2.  These makers need to keep their bond private keys in hot storage.
3.  I approach both makers and offer to aggregate their bond values, forming a new bond with 4x the weight of their individual bonds, and split up the increased earnings between us.
    This can be made noncustodial by use of smart contracts on Bitcoin.
4.  It is no different from the point of view of both makers: they still need to keep their bond private keys in hot storage.
    But this way earns them more money than operating as non-spying makers.
5.  I earn not only the fees for JoinMarket, I also earn additional fees for spying on CoinJoins.
It still seems to me that adding the V^2 tweak weakens the bond system, not strengthens it.

@_date: 2019-08-08 00:09:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Dmitry,
Is it?
I imagine any key can secretly be a MuSig or aggregated ECDSA key, with the aggregator being a signatory.
This is quite a clever solution.
Let me then attempt to break it.
It is possible to encrypt data in such a way that it requires sequential operations in order to decrypt.
This basically allows us to encrypt some data in such a way that its decryption is timelocked, by requiring a large number of sequential operations to decrypt.
It also seems to me (I am not a cryptographer) that it may be possible to present a ZKP that an encrypted text, encrypted using the above timelock decryption, is a signature of a particular message with a particular public key.
Thus, we can change the ritual to this:
1.  I contact two lessors to aggregate their coins into a larger UTXO and thus break V^2.
2.  We create a funding transaction that pays to a locked bond address, with a pubkey equal to a MuSig among us.
    This spends the TXOs they want to lease out, as well as some of my funds to be used for paying for rent.
    We do not sign this yet.
3.  We create a backout transaction that returns the bond to both lessors, plus their rent.
    We partly perform the MuSig ritual to sign this transaction, with me as the last step.
4.  Instead of providing the completed signature to the lessors, I encrypt it using the above timelocked encryption.
    I provide this encryption and a zero-knowledge proof that I have actually completed the signature ritual correctly and that the timelocked-encrypted text has the signature as plaintext.
5.  The lessors now know they can acquire the signature by simply grinding the timelocked encryption.
    This allows them to recover their money by the appointed time.
6.  We then exchange signatures for the funding transaction and broadcast and confirm it.
Now, the lessors cannot provide a valid timelocked transaction, as they do *not* yet have a complete signature; thus they cannot snitch about my aggregation of their funds.
At the same time, they know that the timelocked encryption allows them to eventually get a complete signature and recover their funds.
I can defray this cost of processing by increasing my rent slightly.
Now of course we can just go one step further and also allow bonds to be snitched by presenting the timelocked-encrypted text and the ZKP that it contains the signature for a timelocked transactions.
But it seems to me that there is more than one way to skin this particular cat, thus unless all ways to create provable timelocked encryptions are enumerable, it would be possible to get around.
(though of course it is dependent on a ZKP being possible for a timelocked encryption)
Finally, aggregation is still possible to insure by off-blockchain agreements, possibly with legal consequences, and thus entities like exchanges might still be able to aggregate funds and acquire an undeservedly large weight in the fidelity bond system.

@_date: 2019-08-08 00:27:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
Good morning Bryan,
As transactions need to be signed in reverse order, it seems to me that there is a practical limit in the number of times a vault can be used.
Basically, the number of times we run the vault setup function is the limit on number of re-vaultings possible.
Is my understanding correct?

@_date: 2019-08-08 03:03:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
Good morning Sergio,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Maybe the use of `SIGHASH_NONE` for both inputs of the TxOut transactions?
Also txid malleability.
The first can be fixed by not using `SIGHASH_NONE` for one of the inputs and requiring a hot privkey to sign with that.
The second can be fixed by using SegWit outputs.

@_date: 2019-08-08 09:35:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Dmitry, and list,
I apologize, I was being daft.
There is a simpler way to break this, involving such Lightning Network tropes as revocation and punishment schemes.
Truly, Lightning Network is a great great thing.
First, we should always consider, that due to the V^2, consolidated bonds are always higher weight than unconsolidated bonds.
Thus, even without considering motives to spy on CoinJoins, the existence of the V^2 tweak implies that there will be fewer larger makers and thus easier to take over the JoinMarket system.
So, let us focus on the backout transaction.
Under a consolidated bond, this requires an n-of-n.
Now, suppose we want to identify the snitch who reports our consolidation scheme to the takers.
This can be done easily by performing n MuSig n-of-n rituals, with each ritual using different `r` nonces.
We arrange this by having each of the n consolidators be the last signers in the second round of the MuSig, and have each signer keep their own unique version of the signature for the backout, with their own unique `r` nonce.
Each participant will want to keep its own version of the signature private, because if it gives out this signature to another participant in the consolidated bond scheme, the other participant can frame them for snitching.
We can now identify the snitch, by recognizing which signature was used in the transaction that was reported to the takers.
But we have not yet identified how we can punish the snitch.
As it happens, MuSig allows Scriptless Script.
This means, it is possible for one participant in the MuSig to provide an adaptor signature.
This adaptor signature commits to a particular point.
When the MuSig is completed and the participant (who is the last signer in the second round of MuSig) reveals the completed signature, the scalar that generates the commited point can be computed by anyone who knows the adaptor signature.
This is our next step in our scheme to identify and punish snitches.
In addition to putting up their funds in a consolidated bond, each of the participants in the consolidation scheme put up a fraction of the value into revocable bonds.
This revocable bond is a Taproot with a known-NUMS point as internal Taproot key, and the script alternatives below:
     OP_CHECKLOCKTIMEVERIFY OP_DROP  OP_CHECKSIG
     OP_CHECKSIGVERIFY  OP_CHECKSIG
A punishment transaction spends from the revocable bond via the second alternative above, and divides it equally among the *other* participants.
THis is signed using the MuSig above (where all participants except the owner of this revocable bond are part of).
Then, before starting the n rituals to sign the backoff transaction, the participants provide adaptor signatures to their own `participant_snitch_key`, such that if they publish the backoff transaction to the takers, any of their co-participants that masquerades as a taker can find out about this and derive the private key to the `participant_snitch_key`.
1.  In case all the participants cooperate with the other consolidators, then just before the bond expires, each participant can recover their revocable bond via the first alternative shown above.
    Once the revocable bond is spent back to an address they solely control, the `participant_snitch_key` is worthless.
    Then any participant can publish onchain the backoff transaction without repercussion.
2.  In case a participant snitches and reveals a pre-signed backoff to the takers before the end of the bond period, they can only reveal their own version of the signature of the backoff transaction.
    In that case, their previously-shown adaptor signature can be used to reveal the private key behind their `participant_snitch_key`.
    Then any one of the other participants in the consolidation scheme can complete the punishment transaction.
We can even ensure that setup of the whole system is atomic, by unironically CoinJoining the creation of the consolidated JoinMarket V^2 fidelity bond, in the same transaction that creates the revocable bonds that can be used to ensure that snitches are punishable.
Now you might say, "well now the bond they can put into the JoinMarket fidelity bond is smaller because of the need to put a revocable bond".
And that is right.
It also shows that the V^2 tweak is broken.
Suppose there are two makers with 1.0 BTC each.
They decide to consolidate their bond in order to increase their consolidated weight in the JoinMarket fidelity bond system.
They decide to put up 0.25BTC each for the revocable bonds, and 0.75BTC each into the consolidated JoinMarket V^2 fidelity bond.
The total consolidated bond is 1.5BTC, which has a weight 2.25x the weight of one 1.0BTC bond, or 1.125x the weight of 2x 1.0BTC bonds.
Thus consolidation pressure still exists strongly (and I would think that losing as little as 5% of your total bondable funds would be enough to discourage snitches: this example is 25%).
The scheme would not be broken if there was no V^2 tweak, and would have worked perfectly to disable consolidation, if not for the V^2 tweak.

@_date: 2019-08-08 13:59:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Good morning Dmitry,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
The correct way to do this, as with any offchain technique, is to have the punishment transactions signed by the MuSig-of-everyone-other-than-punishment-target before you even sign the funding transaction.
If consolidation is subsidized by paying rent out to the consolidators, then the lessee of the UTXOs adds its rent payment in the same transaction that atomically instantiates the fidelity bond and all revocable bonds as a single CoinJoined transaction.
If any participant refuses to sign the punishment transactions of their co-consolidators, then the lessee refuses to sign the funding transaction and nobody earns any rent and the lessee goes look for another set of UTXO owners (or just kicks out the participant who refuses to sign and lives with the smaller fidelity bond, no big deal).
Of course, anyone renting consolidated bonds can themselves be unironic victims of sybil attackers who split up their funds to smaller parts so that their liability when later snitching is reduced, possibly to a level that is comfortable to them.
The sybil attacker then pretends to be lessors of UTXOs.
Yes, down with the V^2 superlinearity, it is too strongly centralizing.

@_date: 2019-08-09 14:29:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_LOOKUP_OUTPUT proposal 
Good morning Haoyu LIN et al.,
I believe an unsaid principle of SCRIPT opcode design is this:
* No SCRIPT opcode can look at anything that is not in the transaction spending from the SCRIPT.
This issue underlies the previous `OP_PUBREF` proposal also.
The reason for this is:
* We support a pruning mode, where in only the UTXO set is retained.
  If `OP_LOOKUP_OUTPUT` exists, we cannot prune, as `OP_LOOKUP_OUTPUT` might refer to a TXO that has been spent in very early historical blocks.
* The SCRIPT interpreter is run only once, at the time the transaction enters the mempool.
  Thus it cannot get information about the block it is in.
  Instead, the SCRIPT interpreter can have as input only the transaction that is attempting to spend the SCRIPT.
In any case:
Premium payment can be made contingent on Bob participating.
Of course, it does mean the premium is paid using the destination coin.
It also requires the destination coin to support SegWit.
Let me explain by this:
1.  Alice and Bob agree on swap parameters:
    * Alice will exchange 1 BTC for 1,000,000 WJT from Bob.
    * Alice will pay 10,000 WJT as premium to Bob.
    * Alice will lock BTC for 48 hours.
    * Bob will lock WJT for 24 hours.
    * The protocol will start at particular time T.
2.  Alice generates a preimage+hash.
3.  Alice pays 1 BTC to a HTLC with hashlock going to Bob and timelocked at T+48 going to Alice.
4.  Alice presents above UTXO to Bob.
5.  Alice reveals the WJT UTXOs to be spent to pay for the 10,000 WJT premium to Bob.
6.  Alice and Bob generate, but do not sign, a funding transaction spending some of Bob coin as well as the premium coin from Alice.
    This pays out to 1,010,000 WJT (the value plus the premium) HTLC.
    The hashlock branch requires not just Alice, but also Bob.
    The timelock branch at T+24 just requires Bob.
7.  Alice and Bob generate the claim transaction.
    This spends the funding transaction HTLC output and pays out 1,000,000 WJT to Alice and 10,000 WJT to Bob.
8.  Alice and Bob sign the claim transaction.
    This does not allow Bob to make the claim transaction valid by itself as it still requires the preimage, and at this point, only Alice knows the preimage.
9.  Alice and Bob sign the funding transaction and broadcast it.
10.  Alice completes the claim transaction by adding the preimage and broadcasts it.
11.  Bob sees the preimage on the WJT blockchain and claims the BTC using the preimage.
If Bob stalls at step 8, then there is no way to claim the premium, as the funding transaction (which is the source of the claim transaction that pays the premium) is not valid yet.
After step 9, Bob has been forced to participate and cannot back out and claim the premium only.
This is basically this proposal: In addition, if you really want the premium to be denominated in BTC, I have a more complicated ritual: The described ritual only sets up the American Call Option, but by the time it has been set up, the premium has been paid already and the rest of the execution is claiming the American Call Option.
Thus, I believe there is no need to add `OP_LOOKUP_OUTPUT`.

@_date: 2019-08-10 12:50:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_LOOKUP_OUTPUT proposal 
Good morning Runchao,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
No, Bob is not.
The signature exchange for the WJT-side funding tx is done by:
1. Alice waits for Bob to provide all its signatures for inputs that will fund the 1,000,000 WJT payout.
2. Alice signs its inputs that will fund the 10,000 WJT premium.
3. Alice broadacasts the completely signed funding tx.
Alice is the one responsible for broadcasting the funding tx.
If Bob stalls, it is not a Bob side option (i.e. Bob cannot stall then continue the protocol when the exchange rate moves to its favor) as Alice can refuse to sign and broadcast the funding tx once it has decided Bob is trolling it, thus Bob cannot force Alice to perform.
If Alice stalls, Bob can double-spend one of its inputs at a low feerate.
This either aborts the protocol, or if Alice then broadcasts the funding tx at the pre-agreed feerate and it is confirmed, the premium is now already paid to Bob.

@_date: 2019-08-12 08:05:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_LOOKUP_OUTPUT proposal 
Good morning Runchao,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
The order is below.
I add also the behavior when the protocol is stalled such that a step is not completed.
1.  Alice broadcasts and confirms a BTC transaction paying an HTLC, hashlock Bob, Timelock Alice.
    * Alice is initiating the protocol via this step, thus non-completion of this step is simply not performing the protocol.
2.  Alice informs the BTC transaction to Bob.
    * If Alice does not perform this, Bob does not know it and Alice locked her own money for no reason.
3.  Alice and Bob indicate their inputs for the WJT-side funding transaction.
    * If Alice does not perform this, it aborts the protocol and Alice locked her own money for no reason.
    * If Bob does not perform this, it aborts the protocol and Bob turns down the opportunity to earn 10,000 WJT (opportunity cost).
4.  Alice and Bob exchange signatures for the WJT-side claim transaction which spends the funding transaction via the hashlock side and gives 1,000,000 WJT to payout to Alice and 10,000 WJT premium to Bob.
    Order does not matter as funding  tx is still unsigned.
    * If Alice does not perform this, it aborts the protocol and Alice locked her own money for no reason.
    * If Bob does not perform this, it aborts the protocol and Bob turns down the opportunity to earn 10,000 WJT (opportunity cost).
5.  Bob provides signatures for the WJT funding tx,
    * If Bob does not perform this, it aborts the protocol and Bob turns down the opportunity to earn 10,000 WJT (opportunity cost).
6.  Alice signs WJT funding tx and broacasts and confirms.
    * If Alice does not perform this, Bob invalidates the transaction by spending any of his inputs.
      * Alice has an option here, but a very short option: up until Bob grows tired of waiting.
        Bob can make this timeout arbitrarily small, without requiring input from Alice.
        What value would there be in a 1-second option, even gotten for free, when Alice has spent fees on the BTC-side transaction in the first place?
7.  Alice completes the claim transaction and broadcasts.
    * If Alice does not perform this, Bob simply waits out the timelock and recovers his funds plus premium.
8.  Bob spends the BTC HTLC via the hashlock path.
    * If Bob does not perform this, Bob has given money for free to Alice.
Thus I do not believe this is needed for blockchain-layer atomic swaps.
For Lightning-layer atomic swaps, the solution requires that two hashes be used on the WJT side, and is largely the above protocol in very broad strokes.
Unfortunately, using two hashes instead of one leaks to intermediate hops that the payment involved a cross-currency swap, thus undesirable.
Depends on how the payment channel is implemented.
If you do something like send transactions spending the internal state outputs, then ratifying this later by performing a transaction cut-through to derive the next state update, then it is no different from blockchain layer.
Of course, if you postulate the non-cooperation of Alice in this, there is indeed a need to close unilaterally.
But this is the same as any non-cooperation in any channel system: that is the entire point why you have unilateral closes.
Every payment channel system worth consideration today has a unilateral close.
There is no need for optimism.
And this time frame can be made arbitarily small by Bob by simple threat of unilateral close, thus not making it an option for Alice.

@_date: 2019-08-12 13:15:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_LOOKUP_OUTPUT proposal 
Good morning Runchao,
I am uncertain what you refer to by the "WJT payment channel".
What I am proposing here is there is a single funding transaction that will output to a modified HTLC where hashlock is Alice+Bob while Timelock is Bob, spending inputs from both Alice (10,000 WJT) and Bob (1,000,000 WJT).
So let me rephrase the nearest question as I understand it:
* What happens when Alice broadcasts the funding tx at the same time as Bob double-spends his 1,000,000 WJT input?
As both transactions spend the same input (the 1,000,000 WJT from Bob) then what happens depends on the miners.
The miners decide which transaction is valid and gets confirmed onchain.
That is the reason why we need large timeouts in the HTLC constructions: we need to give enough time, not only to react to transactions being published, but also to have transactions become deeply confirmed.
Otherwise we could have made the timelocks so small as to be practically worthless as an option.
Yes, that is why Alice and Bob need to wait for deep confirmations of the transactions involved.
Once deeply confirmed, they now know which way the protocol went and can safely perform the next step (or abort the protocol).
It does not matter, because Bob doing so *prevents* the option.
Think of it this way:
Suppose we were to meet face-to-face, in order for you to sell me an options contract.
Now, suppose I agree to buy the options contract.
But, while filling up the paperwork, you change your mind.
Until the paperwork is properly filled up, the option does not exist.
Thus, until the paperwork is properly filled,, the option is not exerciseable (and I should not pay anything to you since you did not push through with completing the option).
This is similar in effect.
Any payment channel has the problem of non-cooperation by the other side.
I already mentioned this before.
Again, this is always an issue regardless of the existence or non-existence of an `OP_LOOKUP_OUTPUT`: you have to execute onchain activity anyway in order to enforce anything offchain in case of non-cooperation, and adding in the possibility of various attacks makes it more likely that non-cooperation occurs.
It is the main reason why I think it is difficult to make Lightning support multiple currencies on the same network.
Usability can always be improved by proper software design; you do not worry about what voltage levels need to be transmitted over the wires in order to transmit your email to me, yet you probably consider your email client quite usable.

@_date: 2019-08-20 08:14:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Miniscript 
Good morning David,
For CoinJoin (JoinMarket, Wasabi) at least, I believe there is no need of novel Bitcoin SCRIPTs.
Indeed, from what I can tell they use only P2WPKH `SIGHASH_ALL` signatures and P2WPKH outputs: there seems to be nothing to analyze there.
I do not believe Miniscript would benefit those in particular.
(though miniscript does have other benefits as well: in particular sufficiently-advanced miniscript compilers will be able to write shorter SCRIPTs than mere unaided humans can)
I have not investigated much of Arwen yet but it seems to me to be a sort of exchange-specific payment-channel implementation.

@_date: 2019-08-21 04:14:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Storm: escrowed storage and messaging at L2/L3 
Good morning Maxim,
Insufficient/unclear Description of Probabilistic Proof
It seems to me that the probabilistic checkable proof, whose description I read naively, is not sufficient to prove the statement:
* The source data is the same as the source data originally stored by Alice.
When generating the proof, Bob can use the output of any PRNG as the "source data".
If Alice only checks validity of this proof, then it will accept the output of the PRNG as the actual stored data, which from what I understand is not your goal.
The probabilistic checkable proof by itself just proves the statement:
* The encrypted data corresponds to the given plaintext.
So, before Alice sends its local copy of the data to Bob for storage and deletes it, Alice must first compute a Merkle Tree of the data chunks and store the Merkle Tree root (a small 32-byte data).
And the probabilistic checkable proof has to include the Merkle Tree Path proofs of the selected *source* data chunks together with the source chunks.
Similar problems arise in the pay-for-data scheme proposed in Lightning:
The data provider is trusted to give actual data instead of the output of a PRNG.
In the case of paid storage, Alice had access to the data originally stored (presumably) and can keep a short "checksum" of the original data.
It might be that you imply this in your step 1 for Alice validation of the probabilistic checkable proof though it may be better clarified that Alice has to keep the Merkle Tree root for the original data it originally requested:
Will the Real Decryption Key Please Stand Up?
Also, it seems to me that the encryption used must be an asymmetrical encryption.
That is, the encryption and decryption keys must be different, with the encryption key being a "public" key Bob can safely share with Alice and the decryption key being a "private" key that Bob can share only once it has acquired its funds.
An issue that arises is: while an HTLC is used to atomically transfer the decryption key in exchange for payment, what is the assurance given to Alice that the hash of the decryption key is indeed the hash of the decryption key and not, say, the output of a PRNG?
That is, Bob must prove:
* The given hash h is the hash of the secret decryption key d whose equivalent encryption key is e
...while revealing only h and e to Alice.
If there exists some asymmetric encryption using EC (I know of no such, but that is only due to my ignorance), where the decryption key is a scalar and the encryption key is the scalar times the generator then it would be possible to use 2p-ECDSA / Schnorr Scriptless Script to atomically pay for knowledge of the scalar / decryption key, while knowing the encryption key.
Instead of a hash of the decryption key, Bob sends the encryption key during setup and Alice and Bob use that in the pointlocked timelocked contract under Scriptless Script.
Transporting Storm Over Lightning
Of note is that any mechanism that requires multiple participants to put up money into a contract (as in the case of Storm, which requires both the stake from Bob and the reward from Alice to be put into a single timebound HTLC) can only live inside a single LN channel and is not transportable across intermediate nodes.
This is because intermediate nodes potentially become subject to attack in case of routing failure.
(Though it *may* be possible to reuse the sketch I give here for HTLC-enforced publication of combined HTLCs: This is part of what makes LN difficult to work with multiple asset types due to HTLCs naturally forming premium-free American Call Options.
Avoiding premium-free American Call Options is possible by extracting the premium from the receiver and combining it with the money from the exchange, but this again is doable only onchain, or in a single LN channel (meaning receivers must centralize around exchanges).
It may be possible to get around this, once Lightning supports payment points + scalars, by use of EC magic homomorphisms, though I lack the energy right now to go dig up the resources on lightning-dev.
But the storage provider can route a payment to Alice to serve as stake, which can be claimed only if knowing some secret, then Alice routes the stake+reward to Bob, and use some of the EC magic homomorphism while keeping intermediate nodes unaware.

@_date: 2019-08-21 07:32:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Storm: escrowed storage and messaging at L2/L3 
Good morning Maxim,
The Deaf Bob Attack
It seems to me that Bob can promote the N3 problem to the N2 problem.
Suppose Alice contacts Bob to get the data.
However, Bob happens to have lost the data in a tragic boating accident.
Now, supposedly what Alice does in this case would be to broadcast the HTLC settlement transaction, whose signature was provided by Bob during protocol setup.
But this seems unworkable.
* If Bob managed to sign the HTLC settlement transaction, what `SIGHASH` flags did Bob sign with?
  * If it was `SIGHASH_ALL` or `SIGHASH_SINGLE`, then Bob already selected the decryption key at setup time.
  * If it was `SIGHASH_NONE`, then Alice could put any SCRIPT, including ` OP_CHECKSIG`.
If Bob already selected the decryption key at setup time, then Bob can ignore Alice.
* If Alice does not publish the HTLC settlement transaction, then Bob will eventually enter the N2 state and get the stake+reward.
* If Alice *does* publish the HTLC settlement transaction, without Bob giving the encrypted data, then Bob can just use the hashlock and reveal the decryption key.
  * The decryption key is useless without the encrypted data!
It seems this part is not workable?
As the decryption key is embedded in the HTLC, Alice cannot get a signature from Bob without the decryption key already being selected by Bob (and thus already claimable even without any data being returned by Bob).

@_date: 2019-08-21 12:12:30
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Storm: escrowed storage and messaging at L2/L3 
Good morning Maxim,
I also sent another email with the below text, it seems to have gotten missed somehow or not sent properly or some other problem.
The Deaf Bob Attack
It seems to me that Bob can promote the N3 problem to the N2 problem.
Suppose Alice contacts Bob to get the data.
However, Bob happens to have lost the data in a tragic boating accident.
Now, supposedly what Alice does in this case would be to broadcast the HTLC settlement transaction, whose signature was provided by Bob during protocol setup.
But this seems unworkable.
* If Bob managed to sign the HTLC settlement transaction, what `SIGHASH` flags did Bob sign with?
  * If it was `SIGHASH_ALL` or `SIGHASH_SINGLE`, then Bob already selected the decryption key at setup time.
  * If it was `SIGHASH_NONE`, then Alice could put any SCRIPT, including ` OP_CHECKSIG`.
If Bob already selected the decryption key at setup time, then Bob can ignore Alice.
* If Alice does not publish the HTLC settlement transaction, then Bob will eventually enter the N2 state and get the stake+reward.
* If Alice *does* publish the HTLC settlement transaction, without Bob giving the encrypted data, then Bob can just use the hashlock and reveal the decryption key.
  * The decryption key is useless without the encrypted data!
It seems this part is not workable?
As the decryption key is embedded in the HTLC, Alice cannot get a signature from Bob without the decryption key already being selected by Bob (and thus already claimable even without any data being returned by Bob).

@_date: 2019-08-21 12:48:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Storm: escrowed storage and messaging at L2/L3 
Good morning Maxim,
Thank you for the clarification, indeed it is better to explicit this step.
I did mention 2p-ECDSA, which the last time I checked, was the "best" available multiparty ECDSA.
I have not checked in detail the relative security details of 2p-ECDSA compared to the various multiparty ECDSAs.
In this particular case this is only between two parties, thus 2p-ECDSA should be sufficient.
I have not checked your links and it is possible we are referring to the exact same thing, or your t-ECDSA is a strict improvement over 2p-ECDSA.
Strictly speaking, dual-funding is completely and totally unnecessary.
As submarine swaps / off-to-onchain swaps are already possible, a simple ritual like the below can emulate dual-funding (at the cost that one side must own the total they agree on onchain):
1.  Alice and Bob agree to each put 10mBTC each to form 20mBTC channel.
2.  Alice happens to have 20mBTC onchain in her pocket, while Bob has 10mBTC.
3.  Bob puts his 10mBTC into an onchain 10mBTC HTLC with locktime 2*L paying to Bob, hashlock paying to Alice (with a preimage known only by Bob).
4.  Alice sets up the 20mBTC channel using her 20mBTC onchain.
5.  After the channel funding tx is deeply confirmed, Alice forwards a 10mBTC HTLC over that channel with locktime L paying to Alice, hashlock paying to Bob (with the same hash as the above).
6.  Bob claims the payment offchain.
7.  Alice can now claim the onchain payment.
8.  Alice and Bob now have a perfectly balanced channel (as all channels should be), while Alice is now in possession of an extra 10mBTC onchain.
    So Alice has 10mBTC offchain, 10mBTC onchain, while Bob has 10mBTC offchain on the same channel.
Dual-funding simply makes the above *much* more efficient, but is not strictly necessary in a world where atomic cross-system swaps are already possible.
You may also be interested in Fulgurite.
This is a project to "split" a channel into Lightning part and DLC (discreet log contract) part.
The reason for splitting is because LN is expected to have much more state updates than DLC (you forward payments all the time, but set up only a few DLCs with direct counterparties).
DLCs require a lot of signatures if they are reanchored to a new state update transaction, so splitting the channel into an LN part with many updates and a DLC part with few updates is sensible to reduce processing and bandwidth.
Similar reasoning may hold for Storm.

@_date: 2019-12-02 02:05:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Composable MuSig 
Good morning Lloyd, and list,
So the Pedersen commitment commits to a tweak on `X`, which is revealed later so we can un-tweak `X`.
Am I correct in assuming that you propose to use `X` for the contribution to `R` for a participant?
How is it different from using ElGamal commitments?
Some number of people have noted, including at least one MuSig author, that in the ElGamal case it would be possible to prove your knowledge of the `q` behind `q * G`, and thus prevent the cancellation attack shown.
We already have a general proof-of-knowledge-of-secret-key, the Schnorr signature signing algorithm itself.
Thus, together with `q * G` in the ElGamal commitment, we could include a Schnorr signature using `q * G`, either of the target message itself, or any constant string.
This seems highly appropriate, yo dawg, I heard you like MuSig, so I put an aggregate in your aggregate, so you could sign (singly) while you sign (multiply).
In terms of a *composable* MuSig, e.g. MuSig(MuSig(A, B), C), both A and B will select `q[a]` and `q[b]` and will generate a shared `q[ab] * G` as the MuSig of `q[a] * G` and `q[b] * G`.
Since they know the corresponding `q[a]` and `q[b]` they will also known the contributions they each will need to generate `q[ab] * H`, but note that there is no proof of this until they reveal `q[a]` and `q[b]`, which may lead to further attacks, this time on `q[ab] * H` instead.
So at least for `q` it seems not to be a good idea, though I have not put much thought into this.
Indeed, it seems to me that signatures using the contributions `R[a]` and `R[b]` as public keys seems to be another way to commit to `R` while ensuring that your own `R` cannot have cancelled the other participant `R`.
You would have to exchange the (single) signatures of `R[a]` and `R[b]` first, however, otherwise a Wagner attack may be possible if you exchange `R[a]` and `R[b]` first (i.e. the signatures replace the `R` commitment phase of 3-phase MuSig).
The complexity of either sign-while-you-sign idea, however, is much greater.
Your signing algorithm now requires delegating to another signing algorithm, which while at least fair in that you are now signing while you sign because you aggregated while you aggregated, is more complicated to implement practically.

@_date: 2019-12-02 21:10:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] easypaysy - A layer-two protocol to send payments 
Good morning Tim, and Jose,
It broadcasts information over `OP_RETURN` on the blockchain layer, thus decentralized as long as the blockchain layer is decentralized.
It also means that to register an account, you need to either own some Bitcoins, or rent some Bitcoins to serve as signalling (and then potentially have to change your account identifier later when the lease expires).
`OP_RETURN` does have size limits (imposed by `isStandard`), I do not remember exact numbers, and any data would need to fit.
Finally, use of the blockchain layer is costly; given that payees must be online at any time payers wish to pay, it may do better to just use Lightning instead, which has the same requirement, but moves payments to a separate layer as well, and requires only a single onchain transaction to construct a channel (easypaysy seems to require at least 2, one to anchor the account pubkeys, the other to give the basic "activation" information for the account).
It may be useful to consider defiads, which does *not* use `OP_RETURN`, but instead uses pay-to-contract, and sends the advertisement data over a separate overlay network.
The use-case is mildly different, but ultimately defiads is about connecting potential buyers to potential sellers, and sending data about how to get paid would have to be part and parcel of how defiads ultimately works.
Also, one of the contact-information protocols supported should probably be Tor hidden services, instead of `https`.
Tor hidden services have better useability (no need for port forwarding or registering DNS from some centralized service), with privacy as a bonus.
Further it seems insufficient to only encode block and tx index.
I think it should also encode output index, to also allow a single transaction to anchor multiple accounts.
Also consider using the Lightning encoding of identifying an output: 543847x636x2

@_date: 2019-12-06 02:53:34
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] easypaysy - A layer-two protocol to send payments 
Good morning Jose,
If you have 0 Bitcoins, you need to have *some* Bitcoins from somewhere else (perhaps a service provider) in order to back the initial funding transaction output.
If you create Master Easypaysy account by paying fiat to some service provider that then uses its Bitcoins to fund your Easypaysy account, but requires some sort of shared control over the money in it, I simply call this "renting" the Bitcoin, as presumably the service provider would want to get its coins back from you.
If you are referring to the use of a service provider, then the service provider at least partially controls your account and if it ceases to exist or refuses to continue doing business with you, you need to transfer your account identifier somehow (i.e. end of lease).
You could indicate use of some kind of pay-to-contract, then have the payer send the contract text to the payee so that the payee can claim the funds later.
Yes, that is why I count it as 2 transactions: one transaction to host the funding UTXO that is referred to in the account identifier, and the other transaction is what broadcasts the account information (in particular, the funding UTXO is a P2SH and the transaction that spends it is the one that reveals the 2 pubkeys you require).
In contrast, Lightning Network requires only the funding UTXO (which requires that short channel IDs include the transaction output index, as a single funding transaction can fund multiple Lightning Network channels).
So can Lightning Network channels: multiple channels can be funded by a single funding transactions (C-Lightning supports this, but not as a single command yet, it requires some low-level fiddling).
I suggest being Tor-centric instead.
This does not mesh with your earlier claim:
My understanding is that the account identifier refers to the funding TXO (and funding transactions do not have an `OP_RETURN`, so I fail to see the relevance of that restriction).
If the funding transaction can have many UTXOs that are individually funding TXOs of multiple Easypaysy accounts, then you need to refer to *which* TXO of that funding transaction is what you are using.
Why would it not be appropriate?
In case of such a "Master TX", would it be possible for each slave to be independently controlled by a different party?

@_date: 2019-12-06 17:16:44
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] easypaysy - A layer-two protocol to send payments 
Good morning Jose,
It seems possible, that, I do not.
This does not seem to mesh well with the other non-Master parts of the protocol, where further updates on the single account backed by a funding TXO are performed by spending the funding TXO and creating a transaction with `OP_RETURN`.
In addition, I would like to suggest as well that instead of `OP_RETURN`, you could instead use "sign-to-contract".
Sign-to-contract is simply that, when signing, instead of selecting a random `r` and computing `R` as `R = r * G`, you select a random `r` and a contract or other message `c`, and compute `R` as `R = r * G + h((r * G) | c) * G`.
Then the user can provide the message `c` independently of the signature, via another mechanism, and reveal `r * G` and `c` and point to the signature as a commitment to the message `c`.
Although, it does have the drawback that using sign-to-contract require a different layer / overlay network to broadcast messages `c`, but it does reduce the cost on the blockchain layer, which is always a good thing.
Similar issues are faced by the RGB project, for instance, and defiads explicitly uses a separate overlay network when transmitting advertisements (both RGB and defiads use the opposite pay-to-contract, which tweaks the pubkey rather than the ephemeral `R`).
How about control transactions on top of the funding txo?
Who is able to make further control transactions?
If the service provider gives the user full control of the control transactions on top of the funding txo, then it outright loses the money it put in the funding txo and might as well operate as a full exchange.
If the service provider retains even partial control, then it can refuse to cooperate with the user and the user will be unable to update his or her account.
This is not fixable by the use of mirror servers.
Do you mean, that if the user makes a control transaction to change the details of the account, then the user is forced to change the easypaysy identifier?
My initial reading of your whitepaper is that the easypaysy identifier refers to the funding txo that roots the further control transactions.
If so, the funding txo is not necessarily a one-input two-output transaction.
If not, then each time a control transaction changes the details of the easypaysy identifier, the identifier itself is changed.0
I still do not see why it would be off-topic to the devlist.

@_date: 2019-12-07 04:09:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] easypaysy - A layer-two protocol to send payments 
Good morning Jose,
Sybils are trivial, and a "quorum" of K users can always be manufactured for a targeted attack.
Far better to use an n-of-n, with the service provider as one of the n, and use pre-signed transactions like in Lightning Network to allow unilateral ending of the agreement.
A "graftroot transform" can be done, at the cost of moving data offchain and thus requiring easypaysy to have its own overlay network.
Basically, one commits onchain (via `OP_RETURN`, sign-to-contract, pay-to-contract, or even just using P2PKH) some public key and a series of `R` values.
Then, control messages are authorized by signatures validated with the public key, and use up individual `R`s in the series of `R` values.
(Alternately we commit just to the public key and a "next" `R` value, and each control message indicates a new public key and next `R` value that is signed with the current public key and "current" `R` value, to form a chain of off-blockchain control messages.)
Having a precommitted series of `R` values ensures that the signer can only safely use an `R` once and thus cannot otherwise attack the network by giving half of it one control message and the other half a conflicting control message: if someone does so, the `R` must be reused between both conflicting control messages and this allows trivial revelation of the private key (i.e. a form of single-use-seal).
Presumably the private key is valuable by itself somehow.
Ah, I see my misunderstanding now.

@_date: 2019-12-08 01:15:51
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Composable MuSig 
Good morning Lloyd,
It seems to me that what is needed for a composable MuSig is to have a commitment scheme which is composable.
Let me define a composable commitment scheme:
* `A` and `B`, two points to be committed to.
* `c[A]` and `c[B]`, commitments to the above points respectively.
* `r[A]` and `r[B]`, openings of the above commitments respectively.
Then a composable commitment scheme must have these operations:
* `ComposeCommitments(c[A], c[B])`, which returns a commitment to the point `A + B`.
* `ComposeOpenings(r[A], r[B])`, which returns an opening of the above commitment to the point `A + B`.
My multi-`R` proposal is a composable commitment scheme:
* A commitment `c[A]` is the list `{h(A)}` where `h()` is some hash function.
* `ComposeCommitments(c[A], c[B])` is the concatenation on lists of hashes of points.
* An opening `r[A]` is the list `{A}`.
* `ComposeOpenings(r[A], r[B])` is the concatenation on lists of points.
The property we want to have, is that:
* There must not exist some operation `NegateCommitment(c[A])`, such that:
  * `ComposeCommitments(ComposeCommitments(c[B], NegateCommitment(c[A])), c[A]) == c[B]`.
My multi-`R` proposal works as a composable commitment scheme appropriate for composable MuSig because there is no way to create an input to a concatenation operation such that the concatenation operation becomes a "search and delete" operation.
Pedersen and ElGamal commitments, I think, cannot work here, because commitments in those schemes are negatable in such a way that composing the commitments allows a commitment to be cancelled.
Let us now turn to signature schemes.
I conjecture that the Schnorr and ECDSA signature schemes are also commitment schemes on points.
To create a commitment `c[A]` on the point A, such that `A = a * G`, the committer:
* Generates random scalars `r` and `m`.
* Computes `R` as `r * G`.
* Computes `s` as `r + h(R | m) * a`.
* Gives `c[A]` as the tuple `(R, s)`.
The opening `r[A]` of the above is then the tuple `(m, A)`.
The verifier then validates that the commitment was indeed to the point `A` by doing the below:
* Computes `S[validator]` as `R + h(R | m) * A`.
* Validates that `S[validator] == s * G`.
Now, we know that signatures can be composed in such a way that points (public keys) cannot be cancelled, i.e. preventing the creation of a `NegateCommitment()` operation.
Thus, a signature can be used as a composable commitment in composable MuSig scheme.
In summary, I conjecture that:
* We need a composable commitment scheme that does not allow cancellation, and any such commitment scheme can be "slotted" into the 3-phase MuSig framework.

@_date: 2019-12-10 01:50:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Analysis of Bech32 swap/insert/delete detection 
Good morning Pieter,
To clarify, this step does not modify anything about the implementation of BIP173, only adds this as an additional erratum section?
To clarify, this refers to all SegWit address versions from 1 to 15, as this restriction exists for SegWit address v0?
Okay, this probably needs to be raised in lightning-dev as well, for invoice formats, as well as planned offers feature.
By my understanding, best practice for readers of Bech32-based formats would be something like the below:
1.  Define two variants of checksum, the current Bech32 checksum and the modified Bech32 checksum.
2.  Support both variants (software tries one first, then tries the other if it fails).
3.  Flag or signal some deprecation warning if current Bech32 checksum was detected.
4.  At some undefined point in the future, drop support for the current Bech32 checksum.
Okay, so we will only use the modified Bech32 if and only if we expect to need a non-32-byte witness program for a particular non-0 SegWit version.

@_date: 2019-12-28 23:25:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Non-equal value CoinJoins. Opinions. 
Good morning Adam,
It seems to me that most users will not have nearly the same output of "around 1 BTC" anyway if you deploy this on a real live mainnet, and if your math requires that you have "around 1 BTC" outputs per user. you might as well just use equal-valued CoinJoins, where the equal-valued outputs at least are completely unlinked from the inputs.
Indeed, the change outputs of an equal-valued CoinJoin would have similar analyses to CashFusion, since the same analysis "around 1 BTC" can be performed with the CoinJoin change outputs "around 0 BTC".
* You can always transform a CashFusion transaction whose outputs are "around 1 BTC" to a CoinJoin transaction with equal-valued outputs and some change outputs, with the equal-valued outputs having equal value to the smallest CashFusion output.
 * e.g. if you have a CashFusion transaction with outputs 1.0, 1.1, 0.99, you could transform that to a CoinJoin with 0.99, 0.99, 0.99, 0.01, 0.11 outputs.
* Conversely, you can transform an equal-valued CoinJoin transaction to a CashFusion transaction using the same technique.
* That implies that the change outputs of an equal-valued CoinJoin have the same linkability as the outputs of the equivalent CashFusion transaction.
* At least with equal-valued CoinJoin, the equal-valued outputs have 0 linkability with inputs (at least with only that transaction in isolation).
  The same cannot be said of CashFusion, because the value involved is just in a single UTXO.

@_date: 2019-12-29 10:23:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Non-equal value CoinJoins. Opinions. 
Good morning Yuval,
Indeed, this is a problem still of equal-valued CoinJoin.
In theory the ZeroLink protocol fixes this by strongly constraining user behavior, but ZeroLink is not "purely" implemented in e.g. Wasabi: Wasabi still allows spending pre- and post-mix coins in the same tx (ZeroLink disallows this) and any mix change should be considered as still linked to the inputs (though could be unlinked from the equal-valued output), i.e. returned to pre-mix wallet.
Equal-valued CoinJoins fix this by using a Chaumian bank, which constrains value transfers to specific fixed amounts.
Since an equal-valued CoinJoin uses a single fixed amount anyway, it is not an additional restriction.
CashFusion cannot use the same technique without dropping into something very much like an equal-valued CoinJoin.

@_date: 2019-02-01 09:19:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good morning Matt Bell,
Thinking of this further, I observe that there are limits on the number of operations in a SCRIPT (I believe 201 non-push operations, and maybe a smaller number of CHECKSIG operations?).
This implies that the number of signatories of the sidechain funds in the mainchain are limited.
This is an important care point.
I am uncertain what is the best way to solve this issue.
In any case, I now present a design for a proof-of-mainstake sidechain, now without any modifications to Bitcoin mainchain.
I observe that a blockchain is, stripped to its barest minimum, nothing more than a Merklized singly-linked list.
Each block header is a node in a singly-linked list.
It commits to the previous block header, and also commits to the block data (traditionally a Merkle binary tree).
Via such block headers, a chain of blocks --- a blockchain --- is formed.
I observe that a (non-coinbase) transaction in Bitcoin refers to at least one existing transaction output.
A representation of that transaction output must be committed to in the transaction.
If we create single-input single-output transactions, a chain of transactions is formed.
Thus the idea: the sidechain *is* the transaction chain.
I observe that the mainchain *must* contain some UTXO(s) that are purportedly controlled by the sidechain rules.
It is also possible that the sidechain funds be a single UTXO, with deposits and withdrawals requiring that the single UTXO be spent, in order to maintain the invariant that the sidechains funds are handled completely in a single UTXO.
In addition, it is possible for a transaction to commit to some data arbitrarily, either via `OP_RETURN`, or via some technique such as pay-to-contract (which reduces space utilization on the mainchain compared to `OP_RETURN`).
When we use the above technique (i.e. the sidechain only "owns" a single mainchain UTXO at each block of the mainchain):
1.  Each transaction commits to the previous transaction (via spending the output of the previous transaction).
2.  Each transaction may commit to some other data (via `OP_RETURN` or other technique).
I observe also that under a blockchain:
1.  Each block header commits to the previous block header.
2.  Each block header commits to the block data.
The sidechain "blockchain" *is* the transaction chain.
Under certain forms of proof-of-stake, the block must be signed by some set of signatories of the stakers.
Under transaction rules, the transaction must be signed according to the SCRIPT, and the SCRIPT may indicate that some set of signatories must sign.
Thus, it becomes possible to simply embed the sidechain block headers on the mainchain directly, by spending the previous transaction (== sidechain block header).
This spend requires that the transaction be signed in order to authorize the spend.
However, these same signatures are in fact also the signatures that, under proof-of-stake, prove that a sufficient signatory set of the stakers has authorized a particular block of the proof-of-stake blockchain.
The magic here is that, on the mainchain, a transaction may only be spent once.
Thus, nothing-at-stake and stake-grinding problems disappear.
Now let us introduce some details.
We have two mainchain-to-sidechain requests:
1.  An indication that a mainchain coin owner wants to stake coins to the sidechain.
2.  An indication that a mainchain coin owner wants to transfer coins to the sidechain.
We shall ignore it here.
When a sidechain receives a request to add stake, then the current stakers create a mainchain transaction, spending the sidechain UTXO, plus the  staked coins, and outputting the next sidechain UTXO (with the signatory set modified appropriately), plus a stake UTXO that locks the coins.
When a sidechain receives a request to transfer coins from mainchain to sidechain, then the current stakers create a mainchain transaction, spending the sidechain UTXO, plus the transferred coins, and outputting the next sidechain UTXO (with the same signatory set).
Multiple such requests can be processed for each transaction (i.e. sidechain block).
This simply consumes the sidechain UTXO, any stake or transfer coins, and creates the next sidechain UTXO and any stake UTXOs.
Now, the indication to stake is a UTXO with a special script.
It has two branches:
1.  A signature from the current signatory set.
2.  Or, 2 OP_CSV and the staker signature.
The intent of the latter branch is to ensure that, if the current signatories ignore the incoming staker, the incoming staker can still recover its funds.
If the current set of stakers accepts the incoming staker (which requires both that they change the signatory set, and put the money being staked into a stake UTXO which is simply a long CSV and the staker signature), then the first branch is performed and the coin is an input of the sidechain block header (== sidechain managing transaction).
A similar technique is used for mainchain-to-sidechain transfers.
If the mainchain-to-sidechain transfer is ignored (either deliberately, or by an accident of disrupted communication from the mainchain trasnferrer to the sidechain network), the mainchain transferrer can recover its money and try again.
Ideally, every mainchain block would have a sidechain managing transaction (== sidechain block header).
Of course, we must consider fees.
Obviously the sidechain itself must charge fees within the sidechain.
Some fraction of those fees will be spent in order for the sidechain managing transaction to be confirmed on the mainchain.
Now it may happen that the sidechain managing transaction is not confirmed immediately on the mainchain.
The sidechain stakers (the signatory set) may elect to sacrifice even more of their fees to increase the fees of the sidechain managing transaction via RBF.
Now perhaps the sidechain may wish to have a faster (or more regular) block rate than the mainchain.
In such a case, it may maintain a "real" blockchain (i.e. headers that commit to a single previous header, and commits the block data).
Then each sidechain managing transaction would commit to the latest sidechain block header agreed upon by the stakers.
These sidechain blocks need not be signed; they only become "real" if they are committed to, directly or indirectly, in a sidechain managing transaction.
At each sidechain block, the signatory set (the stakers) create a sidechain managing transaction.
If it is immediately put into a mainchain block, then the next sidechain managing transaction spends it.
Otherwise, if it is not put into a mainchain block, then the stakers just recreate the sidechain managing transaction with RBF.

@_date: 2019-02-01 09:36:50
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Good morning aj,
I certainly agree.
I hope that PSBT support becomes much, much, much more widespread.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-02-11 04:29:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Implementing Confidential Transactions in 
Good morning Kenshiro,
There is a position that fullnodes must be able to get a view of the UTXO set, and extension blocks (which are invisible to pre-extension-block fullnodes) means that fullnodes no longer have an accurate view of the UTXO set.
SegWit still provides pre-SegWit fullnodes with a view of the UTXO set, although pre-SegWit fullnodes could be convinced that a particular UTXO is anyone-can-spend even though they are no longer anyone-can-spend.
Under this point-of-view, then, extension block is "not" soft fork.
It is "evil" soft fork since older nodes are forced to upgrade as their intended functionality becomes impossible.
In this point-of-view, it is no better than a hard fork, which at least is very noisy about how older fullnode versions will simply stop working.
I think more relevant here is the issue of a future quantum computing breach of the algorithms used to implement confidentiality.
I believe this is also achievable with a non-extension-block approach by implementing a globally-verified publicly-visible counter of the total amount in all confidential transaction outputs.
Then it becomes impossible to move from confidential to public transactions with a value more than this counter, thus preventing inflation even if a future QC breach allows confidential transaction value commitments to be opened to any value.
(do note that a non-extension-block approach is a definite hardfork)
This is not an unalloyed positive: block size increase, even via extension block, translates to greater network capacity usage globally on all fullnodes.

@_date: 2019-01-02 13:39:57
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Create a BIP to implement Confidential 
Good morning SomberNight,
This can be mitigated by splitting the blockchain into a public part and a confidential-transactions part (i.e. extension block).
This may be necessary for softforking of CT onto the blockchain anyway; existing pre-CT coins remain in the public part.
When moving from public to CT, you send to some special "lockbox address" on the public part, then they will now be put in a coinbase-like transaction on the CT part.
You then do some mixing and splitting in the CT part to obscure which of your UTXOs have what value.
Then to move from CT to public, you can claim any of the lockboxes on the public part, by revealing the values of your CT UTXOs (and destroying them) and showing that they are equal or less than the lockboxes you are claiming on the public part, and putting back any remainder between the lockboxes total and your own CT UTXOs into another lockbox UTXO.
This is essentially the same concept as sidechains, but with the "side" chain here being part of the consensus, and thus an extension block instead of a true sidechain.
In this way, the amount of total money in the CT part is the sum of all the lockboxes.
In case of a cryptographic break in the CT rangeproof protocol, then the first owner of a quantum computer can claim all the lockboxes, but at least the damage is bounded to only those UTXOs in the CT part.
UTXOs in the public part retain their money.
In addition, since creation of new coins remains in the public part, coin supply is protected, which I believe is the most important property.
The weakness in this scheme is that there is incentive not to put your money for long in the CT part.
Note that CT only hides transaction values.
Structure of transactions from payers to payees remains visible onchain.
I would suggest rather to use MimbleWimble, since at least under MimbleWimble transaction structure will need to be stored by the monitors of the blockchain rather than by the blockchain itself, which would help reduce their ability to see into historical data (they would only be able to see data they recorded themselves, and MimbleWimble allows third-party trustless CoinJoin so they might not even record accurate transaction structure).
Drawback is lack of SCRIPT, but Scriptless Script should be sufficient for e.g. LN.

@_date: 2019-01-19 01:42:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good morning Matt,
It seems to me much more interesting if the stakes used to weigh voting power are UTXOs on the Bitcoin blockchain.
This idea is what I call "mainstake"; rather than a blockchain having its own token that is self-attesting (which is insecure).
It seems to me, naively, that the same script you propose here can be used for mainstake.
For instance, the sidechain network might accept potential stakers on the mainchain, if the staker proves the existence of a mainchain transaction whose output is for example:
 OP_DROP
"1 year" OP_CHECKSEQUENCEVERIFY OP_DROP
 OP_CHECKSIG
The sidechain network could accept this and use the value of the output as the weight of the vote of that stake.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-01-20 02:06:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good Morning Matt,
It seems to me that double signing can be punished by requiring that R be a trivial function on the blockheight of the block being signed on the sidechain network. Then a validator who signs multiple versions of history at a particular blockheight reveals their privkey. Since the privkey also protects their Bitcoin stake UTXO, they risk loss of their Bitcoin stake. A similar idea is used by Discrete Log Contracts to ensure Oracles do not sign multiple values at a particular time.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-01-22 09:19:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good Morning Matt,
unique for each blockheight but still can be used to create signatures or verify?
One possibility is to derive `R` using standard hierarchical derivation.
Then require that the staking pubkey be revealed to the sidechain network as actually being `staking_pubkey = P + hash(P || parent_R) * G` (possibly with some trivial protection against Taproot).
To sign for a blockheight `h`, you must use your public key `P` and the specific `R` we get from hierarchical derivation from `parent_R` and the blockheight as index.

@_date: 2019-01-24 10:03:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good morning Dustin,
If Bitcoin had implemented RBF "properly" (i.e. not have the silly "opt-out" rule) then such races are won by bidding up the fees.  A random person who is not the original staker would be willing to pay miners a fee up to the entire staked amount minus dustlimit satoshis; obviously a staker would be far less willing to pay up such a fee, so the random person slashing the funds would have a major advantage in that race.
Thus the race will be won by whoever mines the highest-fee transaction.
It still becomes very unlikely that the staker will win unless the staker already has a significant mining hashpower (and if the staker has significant hashpower, then the Bitoin layer itself is at peril anyway, never mind sidechains built on top of it).

@_date: 2019-01-25 05:33:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Proof-of-Stake Bitcoin Sidechains 
Good mornint Peter,
Thank you for this information.
I forgot that RBF opt-out was hacked on top of `nSequence`, and relative timelocks were also hacked on top of `nSequence`.
In particular coins locked on mainchain for the purpose of staking a sidechain (mainstake) have to be locked with an `OP_CSV`, which immediately enables this protection.

@_date: 2019-01-28 04:14:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] bustapay BIP :: a practical sender/receiver 
Good morning Ryan and Adam,
Perhaps I am being naive, but I seem, the B2EP and similar do not need to worry about UIH2.
I.e. there exists an input, for all outputs, input > output
To avoid this, we should ensure that, for all inputs, there exists an output, input < output.
Suppose the original transaction avoids the UIH2 (i.e. for all inputs, there exists an output, input < output).
The single added input will also avoid the UIH2, since the contributed output value is added to the receiver output, thereby ensuring that contributed input < output.
Suppose the original transaction does not avoid the UIH2.
The receiver adding their own contributed input would then have a chance that the addition on the output will now cause the final transaction to avoid the UIH2, since the sum of the receiver amount and the contributed input may now exceed the largest sender input.
But since there are more transactions that avoid the UIH2 than not avoid UIH2, the increased probability of now avoiding the UIH2 will lead to a greater anonymity set (especially for the sender, whose coin selection algorithm might have a consistent bias that makes it create transactions that trigger UIH2).
So it seems to me that the simple solution, i.e. sender uses standard coin selection algorithms already in use today, and receiver does not do any UIH2 checks at all, would be an improvement in both privacy and implementation simplicity.

@_date: 2019-01-30 08:34:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] bustapay BIP :: a practical sender/receiver 
Good morning Adam,
To be pedantic: as I understand bustapay, it would still not violate UIH2 (unless I misunderstand UIH2).
Suppose the original transaction is: (0.05 payer, 0.07 payer) -> (0.1 payee, 0.02 payer)
Then bustapay with such a PayJoin-only merchant with 100BTC UTXO would give: (100 payee, 0.05 payer, 0.07 payer) -> (100.1 payee, 0.02 payer).
As I understand it, this technically does not violate UIH2.
It would still conceivably be interpreted as a payment of 100.1 BTC, from a payer who happens to have massively lopsided UTXOs being owned, but still does not violate UIH2.
However, if that 100.1 UTXO is subsequently used to pay a 100.3 payment, then that is used to pay a 100.7 payment, that strongly suggests such a naive PayJoin-only merchant.
Perhaps a simple heuristic against this would be:
1.  For every UTXO you own, flip a coin.
    If all of them come up heads, do not payjoin; just broadcast the original transaction.
2.  Else, randomly select a UTXO (value not care?) and payjoin with that UTXO.
However, I have no proper analysis of the blockchain, so --

@_date: 2019-07-02 03:45:31
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Eric, and Tamas,
As it happens, I was playing around with another idea I am developing.
And it involves something very much similar, but distinct.
In particular, currencies are worthless unless exchanged for things of value to existent beings.
And the discovery of things of value is enabled by advertising.
The idea I am developing, is that of a "Bitcoin Classified Ads Network", wherein ordinary P2PKH UTXOs (or P2WPKH equivalents) embed a commitment to an advertisement.
A secondary network of nodes (separate from the Bitcoin network) transmits the actual advertisements, as well as the UTXOs being used to commit to them.
This secondary network would then reject/purge advertisements once the UTXO is spent on the Bitcoin blockchain.
This makes advertising costly (for the opportunity cost of locking some money in a UTXO until one has acquired actual paying custom) while reducing impact on Bitcoin blockchain space (commitment to the advertisment is in the same space as the ownership of the coin).
Changing the advertisement one makes is possible, at the cost of paying for a transaction in the Bitcoin blockchain to spend the old UTXO and publish a new UTXO now committing to the new advertisement.
Of note is that I also derived that it would be beneficial, for some HODLers to offer their funds for the purpose of making these advertisements.
Some service or product provider would agree with an advertiser to lock some coins of the advertiser for a limited amount of time, in exchange for payment upfront, with the coin address committing to the indicate advertisement of the service or product provider.
This can be done by paying to a 2p-ECDSA (or with Schnorr, MuSig) public key, with the service/product provider embedding a commitment to its advertisement to its own key, and a pre-signed `nLockTime` transaction that lets the advertiser recover the money.
This is in fact a similar use to the "theater ticket" case you mentioned, yet distinct.
In the case of the Bitcoin Classified Ads Network, it is the intermediate addresses used before reclamation by the advertiser that is valuable, as they also serve as commitments to advertisements, attesting to the (probable) validity of the advertisement and making spam have a cost.
Given that nodes of the Bitcoin Classified Ads Network will have memory limits, advertisements whose "lockup-rate" (i.e. the amount of value of the backing UTXO, divided by the size of the advertisement) are low could be evicted from memory before advertisements with high lockup-rate, and thus be less likely to propagate across the network.
Thus service/product providers would want to increase their "marketing budget" to be less likely to be evicted from nodes of the Bitcoin Classified Ads Network, which is beneficial as it increases the minimum practical lockup-rate needed to spam the network, thus making spam costly.
My current plan is that the provider can contact the advertiser in order to effect changes to their advertisement.
Then the provider and the advertiser sign a new timelocked reclamation transaction, then sign a transaction moving from the old advertisement to the new advertisement (presumably there is some protocol for ensuring the advertiser gets paid for this, such as an HTLC that can be triggered by an onchain payment or by an LN payment; I have the details in my processing space but require some time to serialize to human-readabe format).
Arguably, this example seems to show that generalized covenants are not needed in fact, if transfers of coin require paying to the issuer/lender of the coin.
Generalized covenants allows the provider (or ticket-holder in your example) to effect transfers from one advertisement to another (or one ticket-holder to another in your example) without cooperation with the advertiser (or ticket-issuer in your example).
This would be otherwise needed if we lock using a 2-of-2 address that has a timelocked transaction to reclaim the funds.

@_date: 2019-07-02 08:12:26
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Tamas,
Thank you for this thought.
It has challenged me to consider how to bring this capability out of the Bitcoin blockchain.
As a counterargument, I observe that committing to the advertisement on the UTXO is similar to committing to a SCRIPT on a UTXO.
And I observe the Graftroot idea, wherein we commit to a public key on the UTXO, and admit a SCRIPT that is signed by the public key as a SCRIPT that unlocks the UTXO for spending.
By analogy, in my "advertising" scheme, instead of committing the advertisement on the UTXO, I can instead commit a public key (for example, the hash of the "advertiser pubkey" is used to tweak the onchain public key).
Then we use this advertiser pubkey to admit advertisements on the advertising network.
This advertiser pubkey is used to sign an "advertisement chain", which is a merklized singly-linked list whose contents are the actual advertisements, each node being signed using the advertiser pubkey.
To ensure that the advertiser does not sign multiple versions of this chain, we can have the signing nonce be derived from the height of the advertchain, such that signing the same height multiple times leads to private key revelation.
Each header of the advertchain also includes a `CLTV`-like construct, which is the Bitcoin blockheight that must be reached first before another advertchain header can be added, containing a new advertisement that replaces the previous one.
This lets an advertising broker pay for some onchain UTXO to a HODLer, providing a `nLockTime`d onchain transaction returning the funds to the HODLer, with the UTXO paying to a 2-of-2 with a commitment to the advertiser pubkey.
Then the advertising broker can rent out the UTXO to providers who wish to advertise, though I have to figure out how to make this atomic (i.e. paying the advertiser onchain or on Lightning, would be enough for the provider to derive the advertchain header and its signature, for its own advertisement --- perhaps some minimal SCRIPT-like language on the advertchain can be done).
This lets the advertising broker case to work even without generalized covenants on the Bitcoin blockchain, while providing the same benefit of not bothering the HODLer who ultimately owns the funds each time advertisements need to be changed.
This gives the advantage that changes to the advertisement that is attested by a UTXO do not have any activity on the Bitcoin blockchain itself, only on the advertchain; at the cost that the advertising network now takes on the added bandwidth of handling several tiny blockchains of limited lifetime, instead of keeping the data on "which advertisement is valid" on the Bitcoin blockchain.

@_date: 2019-07-02 10:33:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Sent with ProtonMail Secure Email.
??????? Original Message ???????
No it would not :)
Onchain, the locked UTXO would be a 2-of-2 MuSig / 2p-ECDSA of the HODLer and the advertising broker.
The HODLer and advertising broker perform a (mostly-offchain) ritual that ensures that the HODLer gets a `nLockTime` transaction spending from this UTXO and paying it back to the HODLer, and that the advertising broker pays for rent of this UTXO, prior to the UTXO actually appearing onchain.
The UTXO requires both cooperation of HODLer and advertising broker in order to spend, and the HODLer only cares that it gets an `nLockTime` transaction and will no longer cooperate / will permanently delete its share of the key after getting this.
The MuSig / 2p-ECDSA pubkey used will then be tweaked (by addition in MuSig, by multiplication in 2p-ECDSA; the HOLDer need not even learn it, the advertising broker can tweak its pubkey in the Bitcoin-level transaction beforehand) to commit to a hash of the "Advertising pubkey".
Thus I say the UTXO "commits to the advertising pubkey", not "pays to the advertising pubkey".
Indeed, the pubkey of the advertising broker used on the Bitcoin blockchain can be very different from the advertising pubkey used on the advertchain.
This "Advertising pubkey" is the pubkey used in the advertchain.
The actual money on Bitcoin cannot be spent by the broker unilaterally.
However, what advertisement it will commit to on the advertchain, can be controlled unilaterally by the advertising broker.
That is the entire point: the HODLer rents out the UTXO to the advertising broker, relinquishes control over the advertchain, but retaining (eventual) control over the actual Bitcoins.
The advertising broker then has sole control of the advertchain, and can rent it out for smaller timeframes to actual service/product providers.

@_date: 2019-07-04 04:57:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Eric,
I presented another use case, that of the "Bitcoin Classified Ads Network".
Advertisements are "backed" by an unspent TXO.
In order to limit their local resource consumption, nodes of this network will preferentially keep advertisements that are backed by higher UTXO values divided by advertisement size, and drop those with too low UTXO value divided by advertisement size.
Thus, spammers will either need to rent larger UTXO values for their spam, paying for the higher rent involved, or fall back to pre-Bitcoin spamming methods.
Thus I think I have presented a use-case that is viable for this and does not simply devolve to "just burn a 1-satoshi output".
I still do not quite support generalized covenants as the use-case is already possible on current Bitcoin (and given that with just a little more transaction introspection this enables Turing-completeness), but the basic concept of "renting a UTXO of substantial value" appears sound to me.

@_date: 2019-07-05 04:05:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Eric,
Using the unspentness-time of a UTXO allows for someone advertising a service or producer to "close up shop" by simply spending the advertising UTXO.
For instance, if the advertisement is for sale of a limited stock of goods, once the stock has been sold, the merchant (assuming the merchant used own funds) can simply recover the locked funds, with the potential to reinvest them elsewhere.
This allows some time-based hedging for the merchant (they may be willing to wait indefinitely for the stock to be sold, but once the stock is sold, they can immediately reap the rewards of not having their funds locked anymore).
Similarly, an entity renting out a UTXO for an advertisement might allow for early reclamation of the UTXO in exchange for partial refund of fee; as the value in the UTXO is now freed to be spent elsewhere, the lessor can lease it to another advertiser.
Burnt funds cannot be "un-burnt" to easily signal the end of a term for an advertisement.
Similarly for miner fees.
The best that can be done would be to have the nodes of the classified ads network automatically decay the spent value of older advertisements to let them be dropped from their advertisements pool.
Less importantly, burning currently has bad resource usage for practical applications.
Practical burning requires spending to a provably-unspendable P2PKH or P2SH or similar output.
This adds UTXO entries to the UTXO database that will never be removed.
This will of course be remedied by compact UTXO representations later, but not today.
Similarly, it would be very nice to have non-0-amount `OP_RETURN` outputs, as `OP_RETURN` outputs are never stored in the UTXO database.
However, this will require a change in node relay policy, which again will take time to make possible, and would not be practical today.
Thus I think use of UTXO is better than burning or mining-fee-spending.
Also, mostly trivia:
The use of UTXOs to advertise services is not original to me --- I found the LN channel gossip to be the inspiration for this.
Publicly-announced channels indicate the backing UTXO that funds the channel.
The purpose of publicly announcing the channels is to be able to provide the service, of forwarding across the Lightning Network; thus the public announcement serves as an advertisement for the service.
Channel closure immediately spends the UTXO, and also doubles to "revoke" the existing "advertisement".
I found this ability to "revoke" the advertisement appealing, and thereby designed the Bitcoin Classified Ads Network around the UTXO spentness mechanism.

@_date: 2019-07-05 23:16:17
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Eric,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
In a way, this is giving up control of the coin, until he no longer needs the advertisement, i.e. dynamically select the maturity age needed.
Obviously this will require a 2-of-2 multisig, with an timelocked transaction that lets the owner recover at a futuredate, so that it is the agreement of *both* that is needed to perform any actions before the timelock.
I already described this in the link I provided.
Not age of encumbrance, quite.
Instead, it is the simple fact that the UTXO is a UTXO (and not yet spent), that validates the advertisement.
No, it does not *require* a covenant.
However, covenants do make it easier to use, in the sense that the renter can repurpose the UTXO (e.g. change details of advertisement) without having to contact the owner.
You have shown no such thing, merely shown that you have not understood the proposal.

@_date: 2019-07-06 00:17:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning Eric,
You still do not understand.
I strongly suggest actually reading the post instead of skimming it.
The advertisement is broadcast to new nodes on the ad network if and only if its backing UTXO remains unspent.
Once the UTXO is spent, then the advertisement is considered no longer valid and will be outright deleted by existing nodes, and new nodes will not learn of them (and would consider it spam if it is forced to them when the UTXO is already spent, possibly banning the node that pushes the advertisement at them).
Thus the locked-ness of the UTXO is the lifetime of the advertisement.
Once you disencumber the coins (whether your own, or rented) then your advertisement is gone; forever.
Your advertisement exists only as long as the UTXO is unspent.

@_date: 2019-07-09 10:31:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
Good morning all,
I will attempt to restart my thinking from initial principles regarding my proposed "Bitcoin Classified Ads Network".
Nodes behave this way:
* Nodes in this network gossip advertisements.
* These advertisements refer to a UTXO that must be unspent at the chain tip considered by each node, else they would be rejected.
* The referred UTXO must contain a commitment to the text of the advertisement, else the advertisement is rejected.
* Nodes have a maximum limit on the total size of all advertisements they retain and propagate to new nodes, or gossip to their peers.
  This is a deliberate design decision.
* If nodes exceed the above limit, they will sort advertisements according to a value-rate, the value of the UTXO divided by the storage size of the advertisement, and prune advertisements with low value-rate until they are within the limit again.
* Once the backing UTXO is spent, the advertisement is removed by nodes that follow that chaintip.
* As the name ***Classified Ads*** suggests, each advertisement also indicates a "class" in which they belong to.
Then, from the above, we derive how a seller might behave.
* Sellers will attempt to put the minimum possible value into a UTXO committing to an advertisement, to reduce the opportunity cost of using the value elsewhere.
* Thus the rent of the advertisement in this case is paid to joinmarket makers and LN forwarding nodes, as the value used in a UTXO backing an advertisement is not useable in joinmarket/LN.
* Sellers remain in full control of their advertising UTXO, and can spend it at any time.
* Sellers may spend part of the UTXO and put the remaining funds into a change address that is a new advertising UTXO, and re-transmit the advertisement, this time pointing to the new change UTXO.
* However, if the remaining change becomes too low, then its value-rate may drop below the lowest value-rate that BCAN nodes will retain in their (deliberately limited) storage, thus also deleting their advertisement from the BCAN.
* Presumably, the reason for advertising at all, is that the seller considers the cost of advertising to be less than the expected gain of actually selling their product.
* Thus, even if the seller has the ability to spend the UTXO at any time, they run the risk of spending too much and thus removing their advertisement from the BCAN, and losing the expected gain of having the advertisement exist on the BCAN.
* A utility-maximizing seller would therefore not spend a minimal-value UTXO backing the advertisement until it has gained the advantage of actually selling the product, even if it has the option to do so: it is a forced move.
* The cost of keeping the minimal-value UTXO unspent is the opportunity cost in that the value may have been used in joinmarket or LN instead.
* The minimum value will largely be dependent on how much the BCAN is used; more sellers advertising over BCAN will increase the minimum value.
* If the minimum value that is viable to keep its advertisement alive goes higher, then the opportunity cost of the seller using the same value elsewhere might exceed the expected gain from selling the product.
  However, this is expected of *any* advertising scheme: if the gains from selling is too small to justify the advertisement price, advertising does not happen; this is expected utility-maximizing behavior.
* If competitors of the seller exist and the BCAN node storage is already filled, competitors can increase the minimum value of a UTXO that can keep an advertisement alive on BCAN by simply adding more of their advertisements to BCAN.
* Thus we expect that, once the BCAN node storage is at or near the maximum value, the minimum value of a UTXO that can back an advertisement will approach the expected gain from selling the product.
Thus the system of simply committing UTXOs to particular advertisement texts seems sufficient to extract value from a seller wishing to advertise.
The purpose of this extraction of value is to ensure that spam does not overload the BCAN.
Let us now consider some kind of specialization, where a HODLer specializes in owning UTXOs, while an advertiser specializes in trading products that need advertising of some kind.
* We assume that the specialization means that the HODLer cannot feasibly make and sell products on its own, while the advertiser cannot own and control UTXOs of the minimum value needed to keep their advertisement alive on the BCAN.
* We assume that the specialization means that the advertiser can make and sell products for cheaper than the HODLer can, while the HODLer can own and control (and secure) UTXOs of the minimum value needed for advertisements to be kept alive, for cheaper than the advertiser can.
* A HODLer may offer to provide a UTXO locked by a 2-of-2 with a commitment to an advertisement of the advertiser's choosing, in exchange for rent of the value, plus an unbreakable promise to return the rented UTXO value back to the HODLer (represented by a `nLockTime` pre-signed transaction that returns the 2-of-2 back to the HODLer control).
* The HODLer is effectively lending the UTXO out to the advertiser, for the time frame agreed upon by the advertiser.
* The rent at which the HODLer lends out the UTXO must be between the opportunity cost of instead securely utilizing the UTXO in LN or joinmarket, and the expected gain the advertiser expects from having its product advertised.
  * The HODLer is assumed to have the ability to secure the UTXO and retain all data it needs to recover the UTXO; this is part of the assumption that the HODLer specializes in such.
  * The advertiser is assumed to have positive gains from creating, advertising, and selling its product; this is part of the assumption that the advertiser specializes in such.
* The HODLer and advertiser can agree to refund part of the rent, if the advertiser signs a transaction that immediately returns control of the value to the HODLer, before the agreed `nLockTime`.
* The above constructions can be done in current Bitcoin.
* However, the same constructions could be done with a covenant as proposed by Tamas, possibly with reduced communication/coordination costs between the advertiser and HODLer.
Now, there remains the question as to whether users will actually patronize the BCAN instead of existing advertising systems.
* We assume that privacy is valuable to users.
* We assume that users of BCAN will run BCAN nodes.
  This leaks them as users of BCAN, a small loss of privacy.
* Users can look for advertisements of specific classes by simply querying their own BCAN node.
  This does not leak privacy ata all as long as the communication channel of the user with their own BCAN node is private.
  * Compare this to alternatives, which involve some entity observing the behavior of users and thus invading their privacy.
* Advertisers that misclassify their advertisements will be unable to reach their target audience.
* Utility-maximizing advertisers will correctly indicate the class of their advertisements, as otherwise they would be paying the advertising cost without gaining the benefit of the advertisement.

@_date: 2019-07-16 23:00:02
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Under the trust-minimization and uncensorability requirements that Bitcoin has, nothing is more efficient than proof-of-work.
The very idea of proof-of-stake labors under the assumption that unencumbered free-market payment for the consumption of energy is somehow not market-efficient despite the well-known phenomenon of the invisible hand, and believes that it is possible to get something for nothing.
Please re-examine your assumptions.
While this is a developer list and made up of developers who would be quite incentivized to agree to such a setup, note that this effectively trusts the developers.
At least the proposed `assumeutxo` requires the operator to explicitly enable it, but I believe your "hardcoded checkpoints" cannot be disabled, much less disabled-by-default.
Under the trust-minimization requirement of Bitcoin this is simply not acceptable.
As there is no way to trust-minimally heal from a network split (and every time a node is shut down, that is indistinguishable from a network split that isolates that particular node), this is not a trust-minimizing consensus algorithm.
History rewrites are not the only attack possible.
The worst attack is a censorship attack, and a 99% staker can easily censor on the creation of new blocks.
Censorship attacks cannot be prevented except by ensuring that no single entity can claim 51% control of new block creation.
By ensuring this, we can ensure that at least some other entities are unlikely to keep a transaction out of the blockchain, and in particular that no entity can make a short-ranged history rewrite if a censored transaction *does* get into the blockchain from the efforts of another block producer.
This is trivial under proof-of-work, and is the cost we accept in order to achieve uncensorability, since it is non-trivial to acquire energy.
Under proof-of-stake it is difficult to impossible to determine if some single entity controls >51% of stakeable coins, and thus has no way to protect against censorship attack.
Worse, under proof-of-stake it is often the case that stakers are awarded even more coin with which they can stake.
Depending on the PoS implementation, stake-grinding may allow a 49% staker to achieve 51% and thereby the ability to censor transactions.

@_date: 2019-07-17 08:11:26
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro,
This leaves the consensus algorithm liable to stake-grinding attacks.
Often, the selection of the "single staker" for each block is based on some hashing of some number of the previous headers.
This allows the single staker to do some trivial grinding of the `R` of some signature of some transaction of some money from itself to itself.
This grinding is likely to change the hash of the current block.
Changing the hash of the current block is enough to change the hash that is used in the selection of the **next** single staker.
Note that the staker will of course only publish the version of that block that makes itself the **next** staker.
This is the well-known stake-grinding attack; did you not encounter it in your proof-of-stake research?
This is a basic objection to proof-of-stake, together with the nothing-at-stake.
Suppose the staker owns 49% of the staked funds.
It is now trivial for it to continuously grind so that it is again the next staker for the next block, as 49% of the time, it would be selected as the next staker.
Further, this is easily hideable, as the staker can simply run 100000 masternodes and split its funds to all of them, so that it becomes very non-obvious that there is in fact only one staker running the entire network.
(Did you consider how much energy such a staker would be willing to spend on grinding so that it remains the next staker forevermore?
In particular, the staker would be willing to spend energy up to the block reward in such grinding --- a property that proof-of-work has, and ***openly*** admits it has.)
In particular, this allows that one staker to impose any censorship it likes.
Thus, Bitcoin cannot support any kind of proof-of-stake that is vulnerable to this stake-grinding attack.

@_date: 2019-07-18 01:13:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning all,
I am uncertain what you mean here.
What I am attempting to compare are:
1.  A network split (maybe better term is "network partition"?) wherein some number of nodes are temporarily unable to contact the rest of the network.
    This has the degenerate but very common case where a single node is temporarily unable to communicate with the rest of the network.
2.  A node being shut down, then brought back online again.
Neither seems to match "feature" or "bug", as both are simply accidents of deployment.
The point (as I understand it) of a consensus algorithm is to be able to get all nodes into agreement about the global state, even after a network partition.
Ideally, such an algorithm would place as little trust as possible on some other node, and would work even in adversarial conditions.
To my understanding, the proposal from Kenshiro is not able to get all nodes into agreement about global state after a network partition, without trust in some node, when in adversarial conditions.
To expand on this: by censoring ***all*** transactions one is able to prevent spending of all funds.
This will crash the value of the staked funds also, but note that the staker could use techniques like short options to leverage this and potentially earn more than the value of their staked funds, effectively stealing the entire marketcap of the attacked coin.
Aside from that, this is possible to evade by running 10000 masternodes and splitting your staking funds among them.
Rent from a botnet, and it appears the masternodes are geographically diverse.
Then it becomes hard to accuse the network of actually being controlled strongly by a single participant.
(the ability to rent botnets means that even existing PoS coins might already be strongly controlled, but appear "healthy" because masternodes *appear* geographically diverse, but in actuality are controlled by a single entity)
Further, "detect if some transaction in the mempool" cannot provide a proof, as no construct ever precommits to the state of the mempool at a particular time (if it did, the mempool would cease to be a mempool and would be part of the block).
I can generate a completely new transaction, then accuse the masternodes of censoring it.
Other nodes may not believe me, as they have not seen my transaction on their mempool, but note that the mempools of nodes are ***not*** strongly synchronized.
By careful timing and control of the connectivity of the network, it becomes possible to effectively split the consensus algorithm by showing my transaction to some non-masternode nodes but keeping my transaction away from masternodes, then have the non-masternode nodes accuse the masternodes of censoring my transaction and hereby penalizing them.
But the masternodes would not agree, not having seen my transaction in their mempool, and thus is the network consensus destroyed.
Basically: "never base consensus rules on mempool state" is a good rule of thumb for ensuring that consensus can be maintained.
Consensus rules should consider only data that is committed to some block, and the mempool is not intended to be committed to in every block.
I agree.

@_date: 2019-07-18 14:15:05
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning,
Again: under proof-of-work, 51% attacks are a lot less feasible than under proof-of-stake.
You really should have researched this by this point, but in any case.
The primary source of energy on Earth is the formation of the solar system.
Some areas were seeded with radioactive materials.
Later on, some areas were seeded with carbohydrates from dying biological processes.
Regardless, continuously the sun shines upon the just and unjust alike.
Thus, while there is significant variance in energy availability, it is still reasonably spread out.
A 51% attack under proof-of-work is only possible, in general, if some singular entity were able to have physical control of almost 50%, or some such close number, of the globe, simply due to the fact that energy availability is somewhat distributed over the globe.
Looking into latest human political maps, I cannot find any singular entity that can claim this.
Secondly: change of hashing algorithm is pointless in the highly unlikely case of a 51% attack, because what matters is control of energy sources.
In case of hashing algorithm change, the exact same sources of energy can be utilized with whatever hardware is most efficient, and distribution of hashpower will still be the same.
The fact that proof-of-work is strongly bound to physical limitations is a feature, not a bug.
Economic incentives imply simply that market forces will move hashpower towards efficient usage.
Nothing can be more efficient than proof-of-work, and the proof-of-stake delusion is simply a perpetual motion machine that attempts to get something from nothing.
Yes, that is better.
You must understand that removing the chain tip puts the transactions in that block back in the mempool, before we ever start following the longer chain.
Thus, transactions on the shorter chain will simply find themselves in the mempool waiting to be confirmed again.
Of course, they are still subject to replacement since they become unconfirmed, and there is still some risk involved.
Do note the comment of "political money".
Hard forks are very difficult to coordinate as the user set increases in size.
This solution is worse than the problem, and speeds up the dominance of large stakers over the coin, trivially letting someone with the largest stake in the coin grow their stake even faster.
No, I think it will be very successful in ensuring that smart individuals will spend their time actually doing things that benefit the economy and technology instead of wasting their time being distracted with Ethereum and proof-of-stake.

@_date: 2019-07-19 03:45:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Pools only have short-term power in that they can only temporarily attack the coin until miners notice and then voluntarily leave.
Pools are themselves still subject to economic forces, and censored transactions can raise their fee until competing pools arise which do not censor (and which would have an economic advantage in taking the higher fee offered).
The invisible hand abides.
Further, the correct solution is to support the development and deployment of better pool<->miner protocols, such as BetterHash.
So we should instead focus on helping Matt Corallo et al. in this work, than proposing a hard fork to proof-of-stake which will be strongly opposed economically.
GPUs still require electricity to run, and are far easier to source.
Hash change simply means that those with control of energy sources can easily purchase the needed hardware from many sources (as opposed to ASICs which are only sourced from a few places).
So a hash change will only affect things temporarily, and it will still settle to the existing distribution of mining hashpower.
I already told you that it is always possible to get around this: leverage by use of short options.
Short the coin to attack, then perform your attack by censorship.
Coin value will drop due to reduced utility of the coin, then you reap the rewards of the short option you prepared beforehand.
By this, you can steal the entire marketcap of the coin.
Then you still have the economic power (plus what you managed to steal), which you can then use to take over another proof-of-stake coin, regardless of whether it uses the same proof-of-stake algorithm or not.
At least mining hardware are physical hardware and subject to deprecation over time.
This happens every day in Bitcoin, and nobody particularly cares.
You just wait for confirmations that in practice are impossible for some orphaned chain to persist.
But your proposal of being non-linear on the size of the stake means that if you have 51% of the coins, if you put them in a single stake UTXO you potentially get 99.999% of the blocks, which is ***much worse***.
Just admit that you have no real solution to knowing how much every entity controls of your coin.
We hope to see you back soon after having learned your lesson.

@_date: 2019-07-19 18:07:56
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PubRef - Script OP Code For Public Data References 
Good morning Mike,
This is not quite accurate.
While very old blocks are indeed immutable-in-practice, chain tips are not, and are often replaced.
At least specify that such data can only be referred to if buried under 100 blocks.
There are a number of other issues:
* It strongly encourages pubkey reuse, reducing privacy.
* There is a design-decision wherein a SCRIPT can only access data in the transaction that triggers its execution.
  In particular, it cannot access data in the block the transaction is in, or in past blocks.
  For example, `OP_CHECKLOCKTIMEVERIFY` does not check the blockheight of the block that the transaction is confirmed in, but instead checks only `nLockTime`, a field in the transaction.
  * This lets us run SCRIPT in isolation on a transaction, exactly one time, when the transaction is about to be put into our mempool.
    When a new block arrives, transactions in our mempool that are in the block do not need to have their SCRIPTs re-executed or re-validated.
This is not only necessary at the creator of the transaction --- it is also necessary at every validator.
In particular, consider existing pruning nodes, which cannot refer to previous block data.
We would need to have another new database containing every `PUSHDATA` in existence.
And existing pruning nodes would need to restart from genesis, as this database would not exist yet.

@_date: 2019-07-20 00:45:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro,
A short option is an option to force another party to buy an asset at a set price (the contract price) on a future date.
In order to get that option, you first pay that party today, a fee called "premium" (usually a small fraction of the contract price).
The effect is that, at that future date, if the asset is ***lower*** in price than the contract price, you earn by buying it at the market price, then force the party to buy it at the contract price, earning the difference.
The other party, in order to mitigate its loss, then sells the asset back to the market at market price.
(in practice, nobody goes through the rigmarole of buying, forcing the trade, then selling, instead the other party just outright gives you the difference in contract price vs market price).
However, if at that future date, the asset is ***higher*** in price than the contract price, there is no rational reason for you to buy it at market price, then force the other party to buy at the contract price, as you would lose money.
As this is an option for you, not an obligation, you can simply ignore the option and not take it.
However, do note that you did pay the premium when you bought the option, so you lose out on that.
Short options are often used by producers of a good in order to hedge their losses, i.e. insurance against changes in market price.
For example, a farmer might purchase such an option, with a maturity date at the harvest season, for the price of wheat.
The farmer would buy an option whose contract price is the price at the threshold of profitability, i.e. if the price falls below the contract price the farmer would lose money relative to their investment.
If the price of wheat drops below the contract price, the farmer earns from the short option, reducing the impact of the low price.
If the price of wheat is above the contract price, the farmer still earns from sale of the wheat, and only loses on the (comparatively small) premium of the option.
A short option can be leveraged by those with inside knowledge as an economic attack.
For example, if you are capable of disrupting a coin such that its value is very likely to drop, you can buy short options as leverage.
Suppose you hold a large stake of coins and know you control a significant fraction, enough that a censorship attack by you will be so disruptive that the coin price will drop.
You take out a short contract with the contract price at the "hopium" level others have (say 10% higher), buying enough options that you can cover the current price of your owned stake, plus some more options.
Suppose you buy, a number of options equal to twice your stake.
Then you attack the coin, dropping its price by 90% instead of the expected 10% price increase, earning the difference from the short option, about equal to the price of the coin.
Since you bought twice the number of options as your stake, you get about twice the value of your stake in earnings from the short option.
You have recouped the cost of your stake and would not care if it was burned, and now are holding twice the value of your original stake in a different asset, probably fiat.
You then move on and attack the next coin.
The only protection against this is to make sure that block generators cannot feasibly attack the coin, such as by proof-of-work.
Short options are much too useful otherwise to the block generators, as it allows them to hedge against drops in market price, and keeps them operating rather than reducing the security of the coin, thus short options will inevitably arise.
Let's suppose there are two big whales in your coin.
Each of them owns 50% of the total staked value.
Let's say "wait many blocks" parameter is 100 blocks.
One whale puts all his coin in a single UTXO.
The other has distributed his stake in 100,000 different UTXOs.
The honest single-UTXO whale gets a block, because his stake dominates over all others.
Then he gets banned from signing blocks for 100 blocks.
During this ban, the other whale gets every block, as his only competitor is banned.
In addition, banning one of its 100,000 UTXOs is not much reducing his effective stake-weight.
So the honest single-UTXO whale gets 1 block (and its rewards) while the one who distributed his coins to 100,000 different UTXOs gets 100 blocks.
You have just let someone who could *just barely* 51% attack without those rules, 99% attack *with* those rules.
If you had added neither of the two new rules "non-linear stake weights" and "ban for many blocks", you would have gotten both of them at 50% control only, which while concerning, is not as bad as a 99% attack.
Now suppose the one with the 99% control performs a censorship attack.
After a week (1008 blocks) the community rallies and hardforks, burning the UTXOs that performed censorship.
However, only about 998 UTXOs of the censoring staker is known (from the 99% of blocks it generated in that period), which is less than 1% of the 100,000 UTXOs he actually owns, and thus still controls a significant stake even past the hardfork, letting it perform further censorship attacks.
You should stop adding even more rules at this point.
An experienced engineer will stop at this point, delete all his or her files related to the current design (or move them to some archive space, some engineers are compulsive archivists), then regenerate the design from principles up.
A rule-of-thumb in any security design is that, when you add something to protect against some attack, you probably just added an attack vector that is the inverse of the attack you were protecting against.
Thus, adding more rules is rarely the optimal thing to do.
You added two rules, one fixing the original attack (splitting your stake) but inviting the opposite attack (merging your stake), then added another rule to fix the second attack (merging your stake), bringing back the original attack (splitting your stake), except worse.
This is the other rule-of-thumb in any design: adding more things usually just makes things worse.
You are welcome.
This is because we expect market forces to move miners towards efficiency, thus they will not waste energy, only spend exactly enough to maintain the security of the coin.
We already know that miners are setting up mines at locations where energy is being wasted (e.g. oil well gas flares, putting up solar panels instead of just letting sunshine pointlessly heat up their roofs, etc.), and channeling the wasted energy into productive activity.
This is the opposite of becoming more energy-wasteful.
Thus does the invisible hand of the free market abide.

@_date: 2019-07-20 11:07:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro,
It is immaterial, because this is just *one* possible economic attack.
Further leverage can always be had as Bitcoin does not exist in isolation.
Miners cannot feasibly take over 50% of energy sources in the world.
No, you have a misunderstanding of your ***own*** system.
You forgot that you indicated that a staking UTXO is banned from adding a new block for "wait many blocks", as you indicated.
Thus it is immaterial that each tiny stake of the evil whale is tiny: only the tiny stakes of the evil whale are in play during the time that the singular big UTXO by the honest whale is banned.
Thus it has 100% of the stake during those 100 blocks.
The honest whale gets 1 block (with very high probability) and then the evil whale gets the rest of the blocks for 100 blocks.
?Now again, consider that the situation indicates that there are in fact only two actual stakers, each of whom have 50% of the staked funds, thus there are no other stakers (even though the evil whale appears to be multiple separate stakers) because 50% + 50% = 100%.
I did indicate "each of them owns 50% of the total staked value", did I not?
The rest of your counterargument ***completely forgets*** this, so I will ignore it.
A whale composed of many tiny fishes is still a whale.
It is immaterial that there may exist many small stakers who individually can overpower a tiny stake of the evil whale: the evil whale will still dominate during those times that the honest whale is banned because of your additional rule that "a staker who creates a block is banned from creating another block for many blocks": the evil whale simply dominates from having many tiny stakes, each of which can be banned with little effect on the actual power of the evil whale over the coin.
It is immaterial: what matters is the economics.
Dirty energy is dirty because it causes damage to the economy by creating sickness and preventing people from surviving long lives and producing for the economy, destroys ecology and reduces biodiversity (and many products are derived from the variety of lifeforms available) and so on.
Thus, all energy uses will inevitably move towards cleaner energy sources as competition arises.
You have demonstrated no such thing, merely demonstrated your own willful incompetence in designing protocols.
I will no longer respond.

@_date: 2019-07-24 04:14:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Secure Proof Of Stake implementation on Bitcoin 
Good morning Kenshiro and list,
I apologize for the unnecessarily toxic words I used in replies to you, Kenshiro.
I also apologize to subscribers of the list for this behavior.
Such behavior should not be tolerated and should be called out.
Just to be clear, I do not think your additions to the base proof-of-stake can fix the issues introduced by proof-of-stake.
A general heuristic in designing anything is that additional mechanisms cannot improve efficiency.
However, it seems I cannot argue the point without becoming rude or introducing irrelevant arguments.
Thus, I will no longer respond to this thread.

@_date: 2019-07-29 01:46:40
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PubRef - Script OP Code For Public Data References 
Good morning Mike,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
Miner coinbases need 100 blocks for maturity, which is the basis of my suggestion to use 100 blocks.
It might be too high, but I doubt there will be good reason to be less than 100.
There is a potential for a targeted attack where a large payout going to a `scriptPubKey` that uses `OP_PUBREF` on a recently-confirmed transaction finds that recently-confirmed transaction is replaced with one that pays to a different public key, via a history-rewrite attack.
Such an attack is doable by miners, and if we consider that we accept 100 blocks for miner coinbase maturity as "acceptably low risk" against miner shenanigans, then we might consider that 100 blocks might be acceptable for this also.
Whether 100 is too high or not largely depends on your risk appetite.
Data derived from > 220Gb of perpetually-growing blockchain is hardly, to my mind, "only needs an array".
Such an array would not fit in memory for many devices that today are practical for running fullnodes.
It is keeping that array and indexing it which is the problem, i.e. the devil in the details.
Reiterating also, current pruned nodes did not retain that data and would be forced to re-download the entire blockchain.
Unless you propose that we can refer only to `OP_PUSHDATA` after activation of `OP_PUSHREF`.

@_date: 2019-07-29 02:49:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PubRef - Script OP Code For Public Data References 
Good morning Mike,
I believe not?
Current applications use txids to refer to previous transactions, so even a short-ranged history rewrite will mostly not affect them --- they can just rebroadcast the transactions they are spending and get those reconfirmed again.
There is admittedly a risk of double-spending, but each individual application can just spend deeply-confirmed transactions, and tune what it considers "deeply-confirmed" depending on how large the value being spent is.
The point is that history rewrites are costly, but if the value being put in a `scriptPubKey` that uses `OP_PUBREF` is large enough, it may justify the cost of history rewrites --- but if the value is small, the individual application (which refers to transactions by their txid anyway) can generally assume miners will not bother to history-rewrite.
Since `OP_PUBREF` would be a consensus rule, we need to select a "deeply-confirmed" point that is deep enough for *all* cases, unlike applications **on top of the blockchain** which can tune their rule of "deeply-confirmed" based on value.
Thus my suggestion to use 100, which we consider "deep enough" to risk allowing miners to sell their coins.
Lightning uses a "short channel ID" which is basically an index of block number + index of transaction + index of output to refer to channels.
This is not a problem, however, even in case of short-ranged history rewrites.
The short channel ID is only used for public routing.
Between the channel counterparties, no security is based on short channel ID being stable; it just loses you potential routing fees from the channel (and can be fixed by increasing your "deeply-confirmed" enough level before you announce the channel for public routing).
It is precisely because of this possibility that we tend to avoid making SCRIPT validity dependent on anything that is not in the transaction.
We would have to re-evaluate the SCRIPT every time there is a chain tip reorganization (increasing validation CPU load), unless we do something like "only allow `OP_PUBREF` to data that is more than 100 blocks confirmed".
Which is the point: we need to use something, the details need to be considered during implementation, implementation details may leak in the effective spec (e.g. DER-encoding), etc.
The problem with transaction being pruned is that the data in them might now be used in a *future* `OP_PUBREF`.
Further, pruned nodes are still full validators --- transactions may be pruned, but the pruned node will ***still*** validate any `OP_PUBREF` it uses, because it is still a full validator, it just does not archive old blocks in local storage.

@_date: 2019-07-29 03:39:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PubRef - Script OP Code For Public Data References 
Good morning Mike,
I believe you misunderstand the point completely.
Currently Bitcoin Core has a mode, called pruning, where:
1.  It validates ***all*** transactions.
2.  It throws away the transaction data of a block ***after*** validation.
3.  It keeps only the UTXO set of ***all*** addresses, not just a specific wallet.
Given the above, if a transaction block 1000 `OP_PUBREF`s to a `OP_PUSHDATA` in block 100, how does the pruned validator get access to this data (Which was pruned after the pruned validator handled block 100)?
Note that pruned nodes ***are*** fullnodes and validate all transactions, not just those that involve their wallet.

@_date: 2019-06-02 05:35:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
This requires special design to be safe.
Every offchain protocol requires a backout transaction to be created before the funding transaction output is committed onchain.
This backout transaction ensures that the funder of the channel can back out if the other side aborts the protocol.
For Poon-Dryja channels this is the initial commitment transaction.
The continued operation of the protocol requires the initial commitment to be revoked at the next update.
So we need a plausible backout for the receiver first.
To do so, we make the funding transaction address a Taproot with internal pubkey 2-of-2 of the receiver and its channel counterparty.
The Taproot hides a single script alternative, a `OP_SECURETHEBAG` that ensures it is paid out to a pure script (i.e. Taproot internal key is a NUMS point), the scripts forming a revocable output to the receiver (let receiver claim with `OP_CHECKSEQUENCEVERIFY`, or counterparty to revoke immediately if it knows revocation key).
This is essentially a walletless channel open, which I described before with `SIGHASH_NOINPUT`.
Channel factories using `OP_SECURETHEBAG` cannot be updated (i.e. not able to close channels and reuse funds to open new channels offchain), i.e. close-only factories.
The above revocation trick only works with two participants, and it would be largely pointless to have 2-participant factories (unless you were e.g. transporting HTLCs separately from DLCs in two channels of the same factory).
Channel factories based on the Decker-Russell-Osuntokun mechanism ("eltoo") allow reorganizing channels offchain, without hitting the chain at all.
These need `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.
For channel factories, `SIGHASH_NOINPUT` is better.
`OP_SECURETHEBAG` requires the funding output to commit to a particular output set.
`SIGHASH_NOINPUT` lets the signatories introduce a new possible output set later.
One might compare `OP_SECURETHEBAG` to MAST, while `SIGHASH_NOINPUT` is comparable to Graftroot.
MAST has a fixed set of alternatives, while Graftroot allows signatories to add new alternatives later.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-06-06 00:09:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Formalizing Blind Statechains as a minimalistic 
Good morning Ruben,
I agree.
I had no interest in Statechains at all before, but now that you have blind signing servers, this is significantly more interesting.
Of note, is that a Decker-Russell-Osuntokun construction ("eltoo") is not *strictly* required.
We can still make use of the Decker-Wattenhofer construction instead.
The core of Decker-Wattenhofer is a sequence of decrementing-`nSequence` update systems.
Number of maximum updates is limited by the starting `nSequence`, however if we put an update system inside an update system, we can "reset" the `nSequence` of the inner update system by updating the outer update system.
We can chain this concept further and add more update systems nested inside update systems to gain more leverage from the maximum relative wait time.
As we expect fewer updates are needed for statechains than e.g. actual Lightning channels (your given CoinSwap protocol is "only" two updates, for instance) this is usually a good tradeoff,
It is thus possible to use statechains in case `SIGHASH_ANYPREVOUT` is too controversial to get into Bitcoin, provided Schnorr (definitely uncontroversial) does get into Bitcoin.
This still admits the possibility of an exit scam once a few "big enough" swaps are in position to be stolen, trading off earned reputation for cold-stored cash.
This makes me happy.

@_date: 2019-06-06 06:31:45
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Formalizing Blind Statechains as a minimalistic 
Good morning Ruben,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
The lack of `SIGHASH_ANYPREVOUT` does make it difficult to operate a channel factory.
Factory operations would still require the signatures of all participants, but once a participant has released its signature, it cannot be sure whether its channels should be rooted on the previous factory state or the next (i.e. the [Stale Factory problem]( ).
This is fixable if we use `SIGHASH_ANYPREVOUT` on channel update transactions.
Alternately without that flag we can run channels rooted on both the previous and next factory states, which actually is similar to what we need to do for splice-in (so we could reuse that code, possibly).
Of note is that this is roughly the same as the common key in my own Smart Contracts Unchained.
If `SIGHASH_ANYPREVOUT` ends up requiring a chaperone signature, it seems this transitory/common key can be used for the chaperone.
Going further on Smart Contracts Unchained, I observe that the below:
Can be generalized, such that instead of pubKeys and their signatures, we have validation programs and their witnesses.
For example, instead of userPubkey and nextUserPubkey we have a userScript and nextUserScript, with userSignature replaced by a userWitness.
This would be nearer to my own Smart Contracts Unchained, though without committing to the smart contract onchain, only offchain in the server.

@_date: 2019-06-06 07:30:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Good morning aj,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
I believe in combination with `OP_LEFT` and `OP_CAT` this allows Turing-complete smart contracts, in much the same way as `OP_CHECKSIGFROMSTACK`?
Pass in the spent transaction (serialised for txid) and the spending transaction (serialised for sighash) as part of the witness of the spending transaction.
Script verifies that the spending transaction witness value is indeed the spending transaction by `OP_SHA256  OP_SWAP OP_CAT OP_CHECKTXDIGESTVERIFY`.
Script verifies the spent transaction witness value is indeed the spent transaction by hashing it, then splitting up the hash with `OP_LEFT` into bytes, and comparing the bytes to the bytes in the input of the spending transaction witness value (txid being the bytes in reversed order).
Then the Script can extract a commitment of itself by extracting the output of the spent transaction.
This lets the Script check that the spending transaction also pays to the same script.
The Script can then access a state value, for example from an `OP_RETURN` output of the spent transaction, and enforce that a correct next-state is used in the spending transaction.
If the state is too large to fit in a standard `OP_RETURN`, then the current state can be passed in as a witness and validated against a hash commitment in an `OP_RETURN` output.
I believe this is the primary reason against not pulling data from the stack.

@_date: 2019-06-13 01:22:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Formalizing Blind Statechains as a minimalistic 
Good morning Ruben,
Sorry, I am referring to current issues with channel factories, which were not addressed in the original channel factories paper.
Basically, the "Stale Factory" and "Broken Factory" problems.
Broken factory seems unsolvable.
Stale factory is fixable if the channels within the factory use `SIGHASH_NOINPUT` (assuming it gets into Bitcoin) for all unilateral paths (use `SIGHASH_ALL` for cooperative paths).
On reflection, probably best not to.
It requires a script that reveals the pubkeys.
And it now becomes possible for the server to monitor the blockchain for revelation of server pubkey in a spend path.
This will let the server know, after-the-fact, that it was signing blockchain transactions.
This might not let it preemptively censor or otherwise disrupt, but it *could* sell the private fact that a statechain was used.
Combining it via MuSig is probably best, as the server is now unable to recognize even the pubkey (assuming it never is informed `X`).
On reflection, this is probably best.
As the server is blinded, it cannot determine anything about the message being signed.
On the other cognition sub-agent, however, a simple scripting that allows "if somebody provides x of H(x) plus signature A, sign a blinded message M1, else if after 2:30PM PST on Jun 24 2019 if somebody provides signature of B, sign a blinded message M2" could still potentially be useful, and might allow "programmable escrow" like I imagine Smart Contracts Unchained could allow.
The Real (TM) observation is that anything that can be done with a UTXO onchain, can also be done offchain via any updateable offchain cryptocurrency system, whether Statechains, Spillman, Decker-Wattenhofer, Poon-Dryja, or Decker-Russell-Osuntokun.
(I should probably look up the authors of the Statechains paper to make my naming convention consistent)
One might observe that any updateable offchain cryptocurrency system worth its salt would have some way of unilaterally dropping transactions onchain.
Those transactions would create new UTXOs that can be spent by further transactions.
By presenting those "further transactions" to the offchain system, we can provide an argument that the offchain system can just "append" those "further transactions" to the existing unilateral-case transactions, then cut-through the further transactions on its next update (i.e. delete the current UTXOs spent and insert the new UTXOs introduced by the "further transactions").
(In the case of Statechains, you would present this argument to the signers of the latest `userPubKey`, not to the server, who is unaware of the semantics of what it is signing)

@_date: 2019-06-23 13:11:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Good morning Jeremy,
While `OP_SECURETHEBAG` commits to the desired output script of the spending TX, what is being referred to here is the ability to verify the output script being spent, i.e. the script that actually contains the `OP_SECURETHEBAG`.
By this, we are able to create a contract that ensures that it is paid again (covenants), which in combination with a little more introspection of TX data, allows us to verify the execution of steps of a Turing-complete program.
It is surprisingly easy to make a language inadvertently Turing-complete, which is basically the argument here,
That is, with just a little more power and some additional operations that would appear reasonable to add by themselves (`OP_CAT`, `OP_LEFT`, `OP_TWEAKPUBKEY`) on top of some form of requiring a particular output script, it is possible to validate the execution of Turing-complete programs on the Bitcoin blockchain.
Thus, with quining (a script which gets the text of its own code as part of the static data it has), `OP_TWEAKPUBKEY`, and a `OP_SECURETHEBAG` that gets its argument from the stack, it will be possible to make Turing-complete Bitcoin SCRIPT.
I would mildly suggest that we might very well want to consider creating a well-designed way of injecting Turing-completeness into Bitcoin SCRIPT (requiring it to be behind a Taproot, so that bugs in Turing-complete code at least have a chance to be bugfixed by agreement of the Taproot signing set) since we might eventually find ourselves introducing it inadvertently later in any case.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-06-29 20:25:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Generalized covenant to implement side chains 
Good morning Tamas,
While I think covenants for some kind of debt tool is mildly interesting and an appropriate solution, I wonder about this particular use-case.
It seems to me that, as either the `Transfer` signers or `Exit` signers are always involved, that the `Transfer` signers can be constrained so as to ensure that the rules are followed correctly, without requiring that covenants be used on the Bitcoin layer.
After all, the outputs of each transaction signed by the `Transfer` signers are part of the transaction that is being signed; surely the `Transfer` signers can validate that the output matches the contract expected, without requiring that fullnodes also validate this?
In particular, it seems to me that covenants are only useful if there exist some alternative that does not involve some kind of fixed `Transfer` signer set, but still requires a covenant.
Otherwise, the `Transfer` signer set could simply impose the rules by themselves.
Another thing is that, if my understanding is correct, the "sidechain" here is not in fact a sidechain; the "sidechain" transaction graph is published on the Bitcoin blockchain.
Instead, the `Transfer` signers simply validate some smart contract, most likely embedded as a pay-to-contract in the `pk(Alice)`/`pk(Bob)` public keys, and ensure that the smart contract is correctly executed.
In that case, it may be useful to consider Smart Contracts Unchained instead: It would be possible, under Smart Contracts Unchained, to keep the `Transfer`-signed transactions offchain, until `Exit`-signing.
Then, when this chain of transaction spends is presented to the participants, the participants can be convinced to sign a "cut-through" transaction that cuts through the offchain transactions, with the resulting cut-through being the one confirmed onchain, and signed only the participants, without the `Transfer` or `Exit` federation signatures appearing onchain.
This hides not only the smart contract being executed, but also the fact that a Smart Contracts Unchained technique was at all used.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-03-12 04:14:54
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Pre BIP: Solving for spam and other abuse with an 
Good morning Alistair,
I believe there is indeed an important usecase for HTLB over HTLC, which is to improve the anonymity set.
An HTLB over HTLC would be indistinguishable onchain from other uses of HTLC; assuming that HTLCs have other uses, this is a (small?) plus to privacy.
Note that the redundant  would have to be given by Alice to Bob, since using a standardized one will also reveal use of HTLB over HTLC instead of hiding it among other HTLC UTXOs.
Another thing to improve privacy would be to apply the Funding Transaction pattern: In such a case, Alice would prepare two transactions, one which pays to a 2-of-2, and another which spends that 2-of-2 and pays to an HTLB (over HTLC).
Alice would provide the second transaction to Bob, who must return a valid signature for that transaction, then place the first transaction onchain.
Then the protocol resumes as normal.
If Alice and Bob both agree that the bond can be returned to Alice, then they recreate the second transaction as a normal spend from 2-of-2 to a flat P2PKH of Alice (or whatever address Alice desires), obscuring that HTLB was used at all.
The Funding Transaction Pattern is applicable to all constructions that have a fixed participant set, and is effectively gotten "for free" with Taproot (the requirement is the "Taproot assumption"), but is available now even without Taproot.

@_date: 2019-03-12 07:05:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP proposal, Pay to Contract BIP43 Application 
Good morning Omar,
BIP32 includes this text:
This seems to suggest that it is possible for an attacker with sufficient compute power to find two contracts whose derivations alias each other if we "proceed with the next value for i".
More generally, have you considered the possibility of multiple separate contracting systems?
It may be possible to have a particular sequence of bytes that has a valid interpretation under one contracting system, that also has a valid interpretation under another contracting system.
I bring this up here: and: It would then be possible to fool some victim into thinking it has committed to some innocuous contract in one contracting system, only to reveal later that the same sequence of bytes encoding that innocuous contract also corresponds to a more vicious contract in another contracting system.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-03-13 06:41:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
First off, I have little to no idea of the issues at the lower-level Bitcoin.
In any case ---
At my point of view, if a NONINPUT sig is restricted and cannot be used to spend an "ordinary" 2-of-2, this is output tagging regardless of exact mechanism.
So the restriction to add a non-NOINPUT sig in addition to a NOINPUT sig is still output tagging, as a cooperative close would still reveal that the output is not a 2-of-2.
Ideally, historical data of whether onchain coin was used in Lightning or not should be revealed as little as possible.
So in a cooperative close (which we hope, to be a common case), ideally the spend should look no different from an ordinary 2-of-2 spend.
Of course if the channel is published on Lightning, those who participated in Lightning at the time will learn of it, but at least the effort to remember this information is on those who want to remember this fact.
Now, this can be worked around by adding a "kickoff" transaction that spends the eltoo setup transaction.
The eltoo setup transaction outputs to an ordinary 2-of-2.
The kickoff outputs to an output that allows NOINPUT.
Then the rest of the protocol anchors on top of the kickoff.
The kickoff is kept offchain, until a non-cooperative close is needed.
Of course, as it is not a NOINPUT itself, it must need onchain fees attached to it.
This of course complicates fees, as we know.
Alternately maybe the kickoff can be signed with `SIGHASH_SINGLE | SIGHASH_ANYONECANPAY` so that it is possible to add a fee-paying UTXO to it.

@_date: 2019-03-14 05:22:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
When reading through your original post I saw you mentioned something about output tagging somehow conflicting with Taproot, so I assumed Taproot is not useable in this case.
However, it is probably more likely that I simply misunderstood what you said, so if you can definitively say that it would be possible to hide the clause "or a NOINPUT sig from A with a non-NOINPUT sig from B" behind a Taproot then I am fine.
Minor pointless reactions:
If I remember accurately, we do not allow bilateral/cooperative close when HTLC is in-flight.
However, I notice that later you point out that a non-cheating unilateral close does not need NOINPUT, so I suppose. the above thought applies to that case.

@_date: 2019-03-14 07:55:20
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
If I remember accurately this is already true for current Poon-Dryja channels in BOLT 1.0, so at least it is not a degradation of performance.
It does make this modified form of Decker-Russell-Osuntokun much less attractive for use with DLC as the Fulgurite effort would like, but the Fulgurite effort already mitigates this by splitting a channel into two sub-channels (one for high-activity LN payments, one for rare-activity DLC bets) anyway.

@_date: 2019-03-18 04:22:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Pre BIP: Solving for spam and other abuse with an 
Funding Transaction Pattern is how I name it; I am unaware if this pattern has been named before.
I know gmax created Taproot precisely as an optimization of this pattern, so I presume he is aware of it, and might know a proper name for such.
It is massively ambiguous to call it "gmax technique" as that name could apply to many, many techniques.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-03-19 00:22:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Pre BIP: Solving for spam and other abuse with an 
Good morning Alistair,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
The above is at odds with my suggestion to transport HTLBs over HTLCs.
BIP-199 already exists and defines a standard template for HTLC contracts.
    OP_IF
        [HASHOP]  OP_EQUALVERIFY OP_DUP OP_HASH160     OP_ELSE
         [TIMEOUTOP] OP_DROP OP_DUP OP_HASH160     OP_ENDIF
    OP_EQUALVERIFY
    OP_CHECKSIG
To use the above contract as HTLB:
1.  Alice is the "buyer".
2.  Bob is the "seller".
3.  The preimage of `` is generated by Alice and given by Alice to Bob.
I observe that an "early return to Alice" can be implemented by Bob, by taking the first branch, but sending the money back to an address Alice controls.
Since Bob is the one who will decide whether to take the money (i.e. Alice is wasting Bob precious time and resource) or return to Alice (i.e. Alice sent the message in good faith), this decision can be made by Bob entirely without any input from Alice.
So the overall flow of the messages would be:
1.  Alice sends preimage of ``, `[HASHOP]`, `` and a new address that Alice controls (for purpose of "early return").
2.  Alice makes transaction to the above HTLC pattern.
3.  Bob has until ` [HASHOP]` to decide:
3.1.  To claim the money for itself by taking the first branch and sending to a new address that Bob controls.
3.2.  To return the money to Alice by taking the first branch and sending to the address Alice gave.
4.  If Bob has not decided at the timeout, Alice can get her money back by taking the second branch.

@_date: 2019-03-20 07:38:22
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Hi all,
How would this work with watchtowers?
As I understand it, the current plan for eltoo watchtowers would be to store both `SIGHASH_NOINPUT` signatures from both sides in the blob sent to the watchtower.
Then the watchtower can always attach this to whatever is the tipmost available on the chain of transactions.
However, if one of the signatures MUST be non-`SIGHASH_NOINPUT` --- how does the watchtower create such a non-`SIGHASH_NOINPUT` signature?

@_date: 2019-03-20 08:07:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Hi aj,
Re-reading again, I think perhaps I was massively confused by this:
Do you mean that *either* of the above two scripts is OK, *or* do you mean they are alternatives within a single MAST or `OP_IF`?
If you mean that *either* of the above two scripts is OK, then this script:
     CHECKVERIFY  CHECKSIG
should probably be used for Watchtower-compatibility.
When creating a new state, both A and B would cooperatively sign with `muSig(A,B)` with a `SIGHASH_NOINPUT` that ensures the state transaction is correct.
Then they somehow derive or share the private key to `Q`.
In the blob sent to Watchtower, A (or B) includes the `SIGHASH_NOINPUT` as well as the `q` private key.
Would it be safe for Watchtower to know that?
Note that the above `Q` would need to be the same in the "state" trunk of the Decker-Russell-Osuntokun construction.
So, building this, our initial setup transaction pays out to script:
     CHECKVERIFY  CHECKSIG
Then each update transaction pays out to:
    OP_IF
         OP_CSV OP_DROP
         OP_CHECKSIGVERIFY  OP_CHECKSIG
    OP_ELSE
         OP_CHECKLOCKTIMEVERIFY OP_DROP
         OP_CHECKSIGVERIFY  OP_CHECKSIG
    OP_ENDIF
The `SIGHASH_NOINPUT` signature for `muSig(A_u,B_u)` would then be sufficient to unlock the setup transaction, or any update transaction with lower `nLockTime`.
The watchtower would then have to generate the signature for `Q`, committing to a particular UTXO.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-03-21 10:05:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
Actually, the shared keys are different in the two branches above.
The "update" branch (which has no `OP_CSV`) uses the same constant `A_u` and `B_u` points.
The "state commit" branch (which has `OP_CSV`) uses different `A_si` and `B_si` points depending on `i` (state/sequence number).
Also, I cannot understand `OP_CODESEPARATOR`, please no.

@_date: 2019-03-22 01:59:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
I think the issue I have here is the lack of `OP_CSV` in the settlement branch.
Consider a channel with offchain transactions update-1, settlement-1, update-2, and settlement-2.
If update-1 is placed onchain, update-1 is also immediately spendable by settlement-1.
But settlement-1 cannot be spent by update-2 and thus the invalidation of older state fails.
The `OP_CSV` in the settlement branch of the update transaction outputs exists to allow later update transactions have higher priority over settlement transactions.
To ensure that a settlement signature can only take the settlement branch, we need a distinct public key for the branch, so at least `A_s` and `B_s` without rolling them for each `i`, if we use `nLockTime` on the settlement transactions and enforce it with `OP_CHECKLOCKTIMEVERIFY`.
It might be possible to do this with `OP_CODESEPARATOR`, but we do need the `OP_CSV` in the settlement branch.

@_date: 2019-03-22 07:46:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Good morning aj,
I understand.
Looks like that makes sense.
It seems possible to use this, then, together with watchtowers.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-03-22 16:05:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Payjoin privacy with the receiver of the 
Good morning,
It seems to me, to interact somewhat with ZeroLink.
Under ZeroLink, post-mix UTXOs must not be combined.
(Basic Post-Mix Wallet Requirement: "Post-mix wallet MUST prevent joining inputs together.")
The upshot of this, for practical use, is that as payments are done by the user, available coins become smaller and smaller.
And the maximum amount the user can pay with, is limited by the largest post-mix coin they have.
If a ZeroLink post-mix wallet were to split its UTXOs as soon as it got them from the mix, then it would immediately find itself limiting the maximum amount the user could pay.
I suppose if the ZeroLink post-mix wallet had multiple post-mix coins, it could split one of them for the same purpose as above.
Another thought, is if a ZeroLink post-mix wallet could support a Payjoin, as either receiver or sender.
Naively, it seems to me to improve privacy to do so, as long as the ZeroLink post-mix wallet only provides a single UTXO to the Payjoin, whether as receiver or sender.
For a ZeroLink post-mix wallet to a ZeroLink post-mix wallet Payjoin, this would typically result in a two-input, two-output transaction, with both participants having one input and one output each in the transaction, but difficult (?) for third parties to determine which input/output belongs to which.
Now, if we suppose that both ZeroLink and Payjoin become commonly used, then it is likely that two users using the same Chaumian CoinJoin mix transaction will find that one needs to pay the other.
Thus hopefully it may become common for a Chaumian CoinJoin mix transaction to have outputs that (directly or indirectly) merge into Payjoin two-input two-output transactions.
This can then be used to allow a ZeroLink post-mix wallet some limited amount of merging its post-mix UTXOs.
For instance, if a ZeroLink post-mix wallet has a 0.25BTC and a 0.15BTC coin, and needs to pay 0.3 BTC, it may very well simulate a Payjoin to itself, and create a transaction (0.25, 0.15) -> (0.35, 0.05).
Then it can use the 0.35BTC output to pay the 0.3 BTC.
Possibly, anyway.

@_date: 2019-05-02 00:10:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] IsStandard 
Good morning Aymeric,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
P2PKH and P2WPKH cannot have custom script.
However, yes, any custom script can be wrapped in P2SH and P2WSH and it will be propagated.
The P2SH/P2WSH hides the details of your custom script so cannot be filtered based on your custom script.
Do realize that once a claim on your modified x-of-3 is propagated your `redeemScript` is known and someone can attempt to RBF (or coordinate with a miner) with a modified `witness` stack or `scriptSig` to claim your UTXO.
(I do not know if `OP_CHECKMULTISIG` supports 0-of-3 but at least one of your signatories could make it a 1-of-3 and bribe a miner to get it claimed)
I cannot answer for BCH; arguably that is off-topic here.
The old SHA bounty transactions were propagated in the days before `isStandard` I think.
Either that or they were put in by miners.
An SHA bounty can still be propagated today if they are wrapped in a P2SH or P2WSH, but you have to publish the `redeemScript` yourself in some other method.
Or bribe a miner if the transaction is not time-sensitive (for an SHA bounty, unlikely to be time-sensitive).

@_date: 2019-05-08 03:44:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning Luke,
Perhaps this can be enforced offchain, by participants refusing to sign a transaction unless it has an `nLockTime` of the agreed-upon "unconditional CLTV".
Then the CLTV need only be on branches which have a strict subset of the participants as signers.
Would not adding `OP_PUSH() OP_DROP` to the leaves work?
If you enforce always salting with a 32-byte salt, that "only" saves 3 bytes of witness data (for the `OP_PUSHDATA1+size` and `OP_DROP` opcodes).
Or do you refer to always salting every node?
(I am uncertain, but would not adding a salt to every leaf be sufficient?)
(in any case, if you use different pubkeys for each contract, rather than reusing keys, is that not enough randomization to prevent creating rainbow tables of scripts?)
It seems to me the annex can be used for this, by having it contain both the script and the signature somehow concatenated.

@_date: 2019-05-08 04:37:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning Sjors,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
As I understand it, it is possible to take some random data, hash it with SHA256 and acquire a 256-bit number.
Then treat that number as an X coordinate (or is it Y...), and see if there exists a point on the secp256k1 curve at that coordinate.
If not, try another random data, or just hash the same number again.
As I understand it, about half the possible X coordinates will have a point on the curve.
I believe this is the "hash to a point" technique.
The scalar behind the above point cannot be known, unless either the hash function is broken, or ECDLP is broken.
(perhaps a better cryptographer can give the proper qualifications, any corrections, and etc etc)
As the point is just an arbitrary point on the curve, it is unknown to the rest of the world whether somebody knows the scalar, or nobody knows.
The "everyone agrees" branch in Lightning is basically the "cooperative close" of the channel.
So it is not likely we will need an "everyone agrees" branch in the actual HTLCs we transfer *within* the channel.
So if we need to use hashes still, we will likely use the "hash to a point" technique above.
Or just use pubkeys given by both participants, that should be enough to ensure the "everyone agrees" branch is never taken if we write our software such that we never agree to sign with it (i.e. just get points from both sides and MuSig them; then each side can just erase the scalar generating it from memory and whatever caches exist on the system; a node might even just generate a single random point from a scalar it subsequently erases, and just use some non-hardened derivation path from that for every HTLC it has to make).
This technique is "sufficiently provably unknown" since each participant knows that it deliberately erased the only means of knowing the complete discrete log by erasing its share.
In short, "everyone agrees" is trivially easy to make "nobody can agree" by a single participant never agreeing to let itself be ripped off.
Do note that it is likely Lightning will eventually switch to using payment points/scalars instead of hashes/preimages.
This will allow us to have path decorrelation, both within a route, and in multiple routes of the same payment.
This is enabled by Schnorr, as this requires Scriptless Script.
(granted 2p-ECDSA also enables Scriptless Script, but we decided to wait for Schnorr to hit base layer instead)
This means we would be using the "everyone agrees" path only, with everyone agreeing to first create a `nLockTime` backout tx, then everyone agreeing to create a transaction where one side has knowledge of a secret scalar that is learned by the other side upon completion of the signature.

@_date: 2019-05-08 05:16:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning Sjors, sorry everyone for the double-posting...
Now that I think further, everyone can use *the same* point generated from an arbitrary "hash to a point".
For example, we can define the "script-only point" as the point whose X coordinate (or is it Y...) is equal to SHA256("Pieter Wuille is really great!").
Add more "really " until we get a point on the curve.
Provided everyone knows what the exact data to hash is, and the exact hash function, the above procedure is sufficient for everybody to verify that Pieter Wuille (and anyone else for that matter) cannot, in fact, spend the coin unilaterally, and that nobody can actually spend the coin, except via a script.
Since the point on the output is tweaked by the script merkle tree root, varying your pubkey for each use will be enough to blind the use of the "script-only point" until you have to reveal it during spending anyway.
If you *absolutely* insist on reusing your pubkeys, then adding a `OP_PUSH() OP_DROP` to at least one script, with a random salt, should be enough to blind the use of the script-only point until you have to reveal the script you want to use.
Or even just further tweak the point before using it as the taproot internal pubkey, so that not even a coin spend reveals that the "everyone agrees" branch was never actually an option.
The "taproot assumption" is that there exists some finite set of participants that is interested in how the coin will be spent.
Under the taproot assumption then, any "truster" that assigns time-limited control of a coin to a "trustee" is part of that finite set interested in the coin spend conditions.
So the truster should in fact be asked for a pubkey to be added in the taproot internal pubkey that enables the "everyone agrees" branch.
Then the truster can simply generate a point without knowing its private key, or by forgetting this private key.
If one is sufficiently schizophrenic, one can split oneself into a "truster" and "trustee" as above and deliberately forget the truster private key.
Then one is sufficiently unable to spend under duress by deleting the "truster" sub-agent and providing real-world access to the "trustee" sub-agent that can only spend under specific SCRIPT-defined conditions.
(the above paragraph should not be taken to mean that I am an agent-based AI)
That is, it should be enough for everyone to agree to lock the "everyone agrees" branch and throw away their own key, to keep that branch locked.

@_date: 2019-05-10 05:38:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning Johnson,
I understand, you are correct.
Possibly the correct way would be to use another leaf version.
The "script" of such a new leaf version would not actually be a script, but a delegation key.
Then for this leaf version the actual script and signature from the delegation key attesting that script would be on top of the witness stack that will be used by the actual script.
This has the nice property that the existence of a delegation key is hidden from the output, and that if an alternate script is in the MAST (as an alternative to the delegation), use of that alternate script does not reveal that delegation could have been possible.

@_date: 2019-05-13 23:39:51
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] IBLT & Bitcoin 
Good morning Chris,
As I understand it, the latest proposal for improved transaction relay is to use Bose-Chaudhuri-Hocquenghem codes for set reconciliation.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-05-18 17:51:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning list,
Gregory Maxwell commented some days ago:
I am assuming that gmax is referring to my description of the "hash-to-point" or "hash-to-curve" operation.
A little more research shows this: 1.  Generate some random data d.
2.  Get x = h(G | d) where G is the existing generator for secp256k1.
3.  Find a point on secp256k1 with X coordinate x.
4.  If not found, go to 1.
In any case, I am almost sure that for every case where the "everyone agrees" path is unwanted in a taproot address, the simple "put your own pubkey lock on the door and throw away the privkey" technique would work without requiring a NUMS point: the same taproot assumption should also work here.
But generation of a NUMS point might be of independent interest in any case (e.g. setting up Pedersen commitments).

@_date: 2019-05-22 02:51:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Good morning Jeremy,
The recipients can always choose to destroy the privkey after providing the above signature.
Indeed, the recipients can always insist on not cooperating to sign using the taproot branch and thus force spending via the `OP_CHECKOUTPUTSHASHVERIFY`.

@_date: 2019-05-22 06:04:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Good morning,
Some more comments.
* I do not think CoinJoin is much improved by this opcode.
  Typically, you would sign off only if one of the outputs of the CoinJoin transaction is yours, and this does not really improve this situation.
* Using this for congestion control increases blockchain usage by one TXO and one input, ending up with *more* bytes onchain, and a UTXO that will be removed later in (we hope) short time.
  I do not know if this is a good idea, to increase congestion by making unnecessary intermediate transaction outputs, at times when congestion is a problem.
* I cannot find a way to implement Decker-Russell-Osuntokun (or any offchain update mechanism) on top of this opcode, so I cannot support replacing `SIGHASH_NOINPUT` with this opcode.
  In particular, while the finite loop support by this opcode appears (at first glance) to be useable as the "stepper" for an offchain update mechanism, I cannot find a good way to short-circuit the transaction chain without `SIGHASH_NOINPUT` anyway.
* Channel factories created by this opcode do not, by themselves, support updates to the channel structure.
  But such simple "close only" channel factories can be done using n-of-n and a pre-signed offchain transaction (especially since the entities interested in the factory are known and enumerable, and thus can be induced to sign in order to enter the factory).
  More complex channel factories that can update the division of the factory to channels cannot be done without a multiparty offchain update mechanism such as Decker-Wattenhofer or Decker-Russell-Osuntokun.
  So, similarly to CoinJoin, I do not think it is much improved by this opcode.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-05-23 03:45:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Good morning Jeremy,
Sent with ProtonMail Secure Email.
??????? Original Message ???????
But the separate outputs still need to be published at some point in the future.
Further, ideally CoinJoin should be as indistinguishable from normal transactions as possible.
(admittedly, the equal-sized outputs often recommended for CoinJoin tend to blatantly signal "this is a CoinJoin!!", but in any case that "should" be fixed with some kind of future Confidential Transactions)
The same technique of congestion control can still be used with only an "ordinary" MuSig of all participant keys on the output of the "funding" transaction, forming a sort of very tiny CoinJoinXT.
This has the advantage that the MuSig is indistinguishable from 1-of-1 spends, which is important for a privacy technique like CoinJoin.
Even in the future and we have published the output-side transaction of the CoinJoin, the transaction chain *could* be interpreted as "one person consolidated all his coins in an ordinary 1-of-1 UTXO, then spent on several things at once" whereas use of the `OP_CHECKOUTPUTSHASHVERIFY` is a blatant "several people agreed to put in their coins provided these outputs were on the second transaction, i.e. some kind of attempt at hiding their coins".
This is already *technically* possible, though no software exists to do so (sorry, we have bugs between interop of c-lightning and lnd that take up our debugging time already, we cannot spare it for this *yet*).
SegWit by itself already allows child transactions to be signed before parent transactions are signed.
This safety underlies *all* offchain protocols.
See: This is sufficient to ensure that channels can be opened from whatever transactions you want, though having to interop with other software that *also* has to coordinate with other participants in a different protocol is much more difficult than having to interop with other software using the same protocol.
Finally, `SIGHASH_ANYPREVOUT` can *also* do this, since the txid becomes mooted.
And `SIGHASH_ANYPREVOUT` *also* enables a better offchain update mechanism (Decker-Russell-Osuntokun, more commonly known as "eltoo") whereas I am unable to derive a similar offchain update mechanism using `OP_CHECKOUTPUTSHASHVERIFY` (though possibly for lack of trying).
Okay, you have convinced me regarding this point, at least.
Possibly, but the point is that an n-of-n MuSig will work just as well and we would not need to reveal the Taproot key (33 bytes) and the specific script containing the output hash (1+32 bytes) we want, we just have to reveal a single 64-byte signature.
My objection here is simply that n-of-n already exists, it will work already using that (and it is much more likely to be assured of getting into base layer).
Again, we only need to use SegWit and sign transactions in reverse order to ensure proper operation.
This is already what is done for normal channel opens (the initial commitment transactions are signed first, then the funding transaction is signed and confirmed onchain).

@_date: 2019-05-23 16:59:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Good morning Russell,
While I am sympathetic to this argument from first principles, as I understand it, it requires that provided witness inputs will become larger, compared to "shortcuts" like `SIGHASH_ANYPREVOUT` and `OP_CHECKOUTPUTSHASHVERIFY`.
For instance, to simulate `SIGHASH_ANYPREVOUT` with `OP_CAT` and `OP_CHECKSIGFROMSTACK`, I would effectively split the unsigned transaction into its "inputs" and "outputs" part, concat them and use `OP_CHECKSIGFROMSTACK` on the chaperone signature, and also use a normal `OP_CHECKSIGVERIFY` on that same chaperone signature, then dup the "outputs" part and use `OP_CHECKSIGFROMSTACK` on the "any prevout"/"noinput" signature.
I would effectively give the transaction to itself as part of the witness, and further, I would also have to very carefully write the script (admittedly the writing of the template could be done once, but it would require far more review than simple usages of the "limited" operations like `SIGHASH_ANYPREVOUT`).
So my witness stack would contain two signatures, and a duplicate of the transaction itself, plus a very much complicated script, whereas use of `SIGHASH_ANYPREVOUT` just requires two signatures and a script not much longer than pre-Schnorr multisig scripts.
It seems to me desirable, to try to reduce bandwidth consumption on the Bitcoin blockchain, including witness data.
Indeed, I had thought the whole exercise of putting `OP_CHECKSIGFROMSTACK` in a federated sidechain (Elements/Liquid) was to try to identify common patterns of usage for that opcode, and *then* to propose those common patterns as specific "optimized" opcodes as a sort of "jet" for Bitcoin itself (but not `OP_CHECKSIGFROMSTACK` itself).
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-05-24 03:51:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Good morning Jimmy,
??????? Original Message ???????
In order to use `OP_CHECKSIGFROMSTACK` work to allow transaction introspection, the message to be signed would be the equivalent of the sighash digest using `SIGHASH_ALL`.
As a general pattern, if you want a SCRIPT that imposes some rule on some field in the sighash digest of the transaction:
1.  You would require that the transaction be split into three parts, with those parts provided in the witness stack.
    One would be "data before the field", then "the field", then "data after the field".
2.  In addition, you would require a signature for the transaction in addition to the transaction parts above.
3.  You would ensure that "data before the field" is the correct size, so that you know "the field" is at the correct location in the transaction.
    Alternately you might need to also introspect some other fields in order to ensure other details like number of inputs, number of outputs, value of inputs etc. etc. are what you expect.
4.  You would check that the given signature is `SIGHASH_ALL` (most easily by checking its size --- proposed Schnorr signatures have a fixed size, and the lack of an extra sighash flags byte means `SIGHASH_ALL` by default, so if the signature is exactly the fixed Schnorr signature size, it is `SIGHASH_ALL`).
5.  You would use normal `OP_CHECKSIGVERIFY` to ensure that the signature signs the actual transaction.
6.  You would concatenate the supposed parts of the transaction together and use `OP_CHECKSIGFROMSTACKVERIFY` to ensure that the signature *also* is valid for that.
    Since you know the signature is valid for the transaction itself, if it *also* is valid for this, then the concatenation of the input "data before the field", "the field", and "data after the field" is exactly the same sighash digest as the actual transaction, and thus is accurate to the transaction.
7.  Finally, you would actually validate the field you want to impose some rule on.
`SIGHASH_ALL` is suggested since it allows you to introspect all fields, but also because ensuring that the signature is indeed a `SIGHASH_ALL` signature is easier (just do the size check).
Alternately you can use some other flag, but you would require the signature on the stack to be flagless and concat the flag yourself before using `OP_CHECKSIGVERIFY`.
This mechanism is very general and allows SCRIPT to introspect *any* field of the transactions.
Indeed, one can argue that `OP_CHECKLOCKTIMEVERIFY` and `OP_CHECKSEQUENCEVERIFY` are both superfluous in a system with `OP_CAT` and `OP_CHECKSIGFROMSTACK`.
OF course, these operations are significantly more optimized since they do not require that you quine the transaction.

@_date: 2019-05-24 04:15:45
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
After some thinking, I may have devised a way to achieve the more interesting Turing-complete system (where each "loop through" requires paying a fee to miners, akin to Ethereum Gas, and thus a good way to build new footguns) even without `OP_PUBKEYTWEAK`.
I observe the following:
1.  `OP_CHECKSIGFROMSTACK` can introspect into the transaction *doing the spend* by giving the transaction (minus witness) as part of the witness (i.e. quining).
2.  The above can be leveraged to introspect into the transaction *being spent* by giving that transaction *being spent* (minus witness) as part of the witness stack.
    This is because the transaction *doing the spend* commits to the transaction *being spent* by referring to its txid.
    We can concatenate the bits of the previous transaction and confirm that it is indeed the transaction *being spent* by hashing and comparing that to the txid in the input of the transaction *doing the spend*.
3.  The transaction *being spent* can contain an `OP_RETURN` output that contains the previous state (or a commitment to the previous state if it is too large to fit in an `OP_RETURN`, again requiring that the previous state be given as part of the witness).
    Since it can be introspected, a script can acquire a "previous state" data.
4.  The transaction *doing the spend* can also contain an `OP_RETURN` with the next state (or commitment to next state).
5.  The rest of the script can then determine if the transition from "previous state" to "next state" is valid.
6.  The script can impose that the same script is paid to by introspecting the transaction *being spent* to get at a commitment to itself.
The above seems enough to create a potentially unbound loop, bound only by the amount of money you are willing to spend on fees operating that loop.
The "state" would be the memory of your virtual machine, and the SCRIPT validates the execution of one iteration of the interpreter loop, and that would be enough to create a Turing-complete system within Bitcoin.
With MAST, you can compress branches not taken, reducing the number of operations you have to expose at each iteration.
I admit *creating* this by hand will probably be very difficult, but that should be doable with an army of lower-level cognition agents.
(disclaimer: I am not an AI with an army of lower-level cognition agents and I can completely and totally pass the Turing test)

@_date: 2019-05-25 03:56:22
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Good morning Jeremy,
I believe I have caught the general point.
Indeed, I agree that this is useful, but it is *not* useful for these cases:
1.  CoinJoin - the initial funding transaction must be signed by the participants anyway after checking that the output is correct.
    Further any spend that is not a signature spend is going to defeat the purpose of CoinJoin trying to be private by imitating "typical" spends: if `OP_CHECKOUTPUTSHASHVERIFY` path is used, you have just lost the CoinJoin privacy by reducing anonymity set.
2.  Channel Factories - the initial funding transaction must be signed by the participants anyway after each initial sub-channel initial commitment / initial update+state transaction is signed.
In both above cases, the issue of users dropping out during the step of signing the initial funding transaction is unavoidable even with `OP_CHECKOUTPUTSHASHVERIFY`.
For congestion control, and for general "I promise to set this up later" as in c*stodial-service-directly-to-channel etc., I already agree this is useful.
My objection lies *only* with the above two cases, wherein `OP_CHECKOUTPUTSHASHVERIFY` does not really improve things, as you *still* need to coordinate multiple signers anyway.
You have convinced me already that the other cases are good example usages of this opcode.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-05-28 03:41:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Sent with ProtonMail Secure Email.
??????? Original Message ???????
One could argue that manually programming directly with `OP_CHECKSIGFROMSTACK` is difficult enough that we should really be using some compiler that (say) translates Simplicity to SCRIPT that uses `OP_CHECKSIGFROMSTACK` to implement transaction introspection.
So the lack of such use may point more to a lack of tools than a lack of actual use.
This extends in particular to "lack of abstraction"; the abstraction might be better served by implementing a pure functional language that is compiled down to `OP_CHECKSIGFROMSTACK` somehow, with the pure functional language implementing loops using the technique I described (keep current state in a separate `OP_RETURN` output, reuse the same `scriptPubKey` but modify the `OP_RETURN` output (i.e. code is `const`, data is `mutable`)).
But that still requires that we have at least a proof-of-existence in the form of some compiler that targets (say) Liquid/Elements SCRIPT and leverages `OP_CHECKSIGFROMSTACK` appropriately.
I believe Russell has expressed some interest in my Smart Contracts Unchained technique to implement Simplicity on top of Bitcoin by using a semi-trusted user-selected federation to enforce Simplicity execution.
If implemented as such, it may be possible to then show that actual use would be enabled if it is possible to run this on Bitcoin.
(I respect that Blockstream employees have to eat and thus made Liquid, but for example I myself would not be interested in putting any coins in Liquid, as its federation is not selected by me; I would be more willing to use a Simplicity or `OP_CHECKSIGFROMSTACK` implementation on top of Smart Contracts Unchained as at least I can select the federation to include my own hardware, and allow anyone I might want to form such contracts with to also select federation members to include my own hardware.)
(Of course Liquid is built on Elements and Elements is open-source and in theory I could just replace its federation with my own, but having to start a new blockchain for every federation-set seems wasteful compared to Smart Contracts Unchained; Elements does have the advantage of already actually existing whereas no Smart Contracts Unchained exists at all.)

@_date: 2019-11-08 05:11:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Good morning Pieter, and all,
Can we modify Bech32 SegWit address format for version 1 and above as below?
      * The data-part values:
      ** 1 byte: the witness version
    + ** If the witness version is non-zero, 1 byte: the length of the witness program.
      ** A conversion of the 2-to-40-byte witness program (as defined by [ BIP141]) to base32:
      *** Start with the bits of the witness program, most significant bit per byte first.
      *** Re-arrange those bits into groups of 5, and pad with zeroes at the end if needed.
      *** Translate those bits to characters using the table above.
This retains the ability of a bech32 address to specify any valid witness length and allows future version 1 addresses with lengths other than 32, while closing this malleation.
Older software being given the modified v1 address format would mis-send it to the wrong witness program, however.
Alternately we could just keep using version 0 in the address format forever.
The requirement would be to ensure that SegWit vN (N >= 1) output witness programs would have a data-part value encoded as below:
    * The data-part values:
    ** 1 byte: legacy witness version, which must always be 0.
    ** 1 byte: actual witness version, which must be non-zero.
    ** 1 byte: padding length: 0 or 1.
    ** If padding length is 1, 1 byte: padding, which must be 0.
    ** 1 byte: witness program length.
    ** variable: witness program.
A writer for a v1 or later address would initially set an empty padding, then compute:
      1 // actual witness version
    + 1 // padding length
    + 1 // witness length
    + witness_length
If the above sum is 20 or 32, then the writer selects a non-zero padding and inserts the padding byte so that the above sum is now 21 or 33.
To a reader that understands only bech32 v0, such an encoding would look like a SegWit v0 invalid-program-length, and be rejected.
A reader which understands the above protocol would, instead of rejecting a SegWit v0 invalid-program-length, instead attempt to parse it as above first, and consider it as SegWit v1 or higher if it was parsed correctly as above.
The above proposal is of course ridiculous and I am now currently running diagnostics on my processing units to see if further glitches occur in test reasoning skills.

@_date: 2019-11-11 13:52:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Dynamic MaxBlockSize - 3 Byte Solution 
Several days late, I would like to add my NACK here.
* The actual fees paid to miners are not in fact known.
  Miners may accept side fees that are not explicitly visible on the block, and miners may pad their blocks with faked self-paying transactions.
  Further, such side fees and faked transactions do not modify the economic assumptions of Bitcoin.
  * Mining fees are simply an anonymity technique: what is material economically is that miners are paid for confirming transactions, thus side fees are perfectly fine when considering economic incentives of Bitcoin mining.
  * Without this proposed mechanism, padding blocks with faked self-paying transactions is self-destructive behavior for miners, as the transaction takes up space that cannot be used for actually-paying transactions.
  * However, by computing only using the explicit fees on the block (and not the actual fees that miners actually get), various additional games can be played by miners.
    Such games make considering the overall security of mining much harder and we may end up with worse security due to misaligned incentives, including encouraging miners to pad blocks with faked transactions (which otherwise is discouraged by the current protocol).
* Scaling means getting more impact for less resource consumption.
  ***All*** block size increases are getting more impact for ***more*** resource consumption, thus not scaling.

@_date: 2019-11-13 05:32:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Good morning all,
It seems to me that adding the length for checksumming purposes need not require the length to be *actually* added in the address format.
So, currently, below is my understanding of bech32 validation:
* Run BCH checksum on witness program.
* Compare checksum to checksum in address.
  * If the checksum matches:
    * If version is 0, validate that the witness program is length 20 or 32.
    * Else accept.
  * If the checksum does not match:
    * Reject
Let me propose then:
* Run BCH checksum on witness program.
* Compare checksum to checksum in address.
  * If the checksum matches:
    * If version is 0, validate that the witness program is length 20 or 32.
    * Else validate that the witness program is length 32.
  * If the checksum does not match:
    * Get the length of the witness program.
    * Prepend the length to the witness program.
    * Run BCH checksum on concatenated length | witness program.
    * Compare checksum to checksum in address.
      * If the checksum matches:
        * Accept.
      * Else reject.
A writer of bech32 addresses would then:
* If the witness program is length 32, or witness version is 0 and witness program length is 20, use a non-length-prefixed checksum.
* Otherwise, use a length-prefixed checksum (but not include the length in the address, just change the BCH checksum).
This has the following properties:
* The bech32 address format is retained, and no explicit length is added.
* There are now two checksum formats: one with just the witness program, the other which validates with the witness program length.
  * Readers that do not understand the new checksum format will simply reject them without mis-sending to the wrong witness program.
Is the above acceptable?

@_date: 2019-11-25 11:00:22
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Composable MuSig 
So I heard you like MuSig.
Previously on lightning-dev, I propose Lightning Nodelets, wherein one signatory of a channel is in fact not a single entity, but instead an aggregate: * There exists some protocol that requires multiple participants agreeing.
  * This can be implemented by use of MuSig on the public keys of the participants.
* One or more of the participants in the above protocol is in fact an aggregate, not a single participant.
  * Ideally, no protocol modification should be needed to support such aggregates, "only" software development without modifying the protocol layer.
  * Obviously, any participant of such a protocol, whether a direct participant, or a member of an aggregated participant of that protocol, would want to retain control of its own money in that protocol, without having to determine if it is being Sybilled (and all other participants are in fact just one participant).
  * Motivating example: a Lightning Network channel is the aggregate of two participants, the nodes creating that channel.
    However, with nodelets as proposed above, one of the participants is actually itself an aggregate of multiple nodelets.
    * This requires that a Lightning Network channel with a MuSig address, to have one or both participants, be potentially an aggregate of two or more nodelet participants, e.g. `MuSig(MuSig(A, B), C)`
This is the "MuSig composition" problem.
That is, given `MuSig(MuSig(A, B), C)`, and the *possibility* that in fact `B == C`, what protocol can A use to ensure that it uses the three-phase MuSig protocol (which has a proof of soundness) and not inadvertently use a two-phase MuSig protocol?
Schnorr Signatures
The scheme is as follows.
Suppose an entity A needs to show a signature.
At setup:
* It generates a random scalar `a`.
* It computes `A` as `A = a * G`, where `G` is the standard generator point.
* It publishes `A`.
At signing a message `m`:
* It generates a random scalar `r`.
* It computes `R` as `R = r * G`.
* It computes `e` as `h(R | m)`, where `h()` is a standard hash function and `x | y` denotes the serialization of `x` concatenated by the serialization of `y`.
* It computes `s` as `s = r + e * a`.
* It publishes as signature the tuple of `(R, s)`.
An independent validator can then get `A`, `m`, and the signature `(R, s)`.
At validation:
* It recovers `e[validator]` as so: `e[validator] = h(R | m)`
* It computes `S[validator]` as so: `S[validator] = R + e[validator] * A`.
* It checks if `s * G == S[validator]`.
  * If `R` and `s` were indeed generated as per signing algorithm above, then:
    * `S[validator] = R + e[validator] * A`
    * `== r * G + e[validator] * A`; subbstitution of `R`
    * `== r * G + h(R | m) * A`; substitution of `e[validator]`
    * `== r * G + h(R | m) * a * G`; substitution of `A`.
    * `== (r + h(R | m) * a) * G`; factor out `G`
    * `== (r + e * a) * G`; substitution of `h(R | m)` with `e`
    * `== s * G`; substitution of `r + e * a`.
Under MuSig, validation must remain the same, and multiple participants must provide a single aggregate key and signature.
Suppose there exist two participants A and B.
At setup:
* A generates a random scalar `a` and B generates a random scalar `b`.
* A computes `A` as `A = a * G` and B computes `B` as `B = b * G`.
* A and B exchange `A` and `B`.
* They generate the list `L`, by sorting their public keys and concatenating their representations.
* They compute their aggregate public key `P` as `P = h(L) * A + h(L) * B`.
* They publish the aggregate public key `P`.
Signing takes three phases.
1.  `R` commitment exchange.
  * A generates a random scalar `r[a]` and B generates a random scalar `r[b]`.
  * A computes `R[a]` as `R[a] = r[a] * G` and B computes `R[b]` as `R[b] = r[b] * G`.
  * A computes `h(R[a])` and B computes `h(R[b])`.
  * A and B exchange `h(R[a])` and `h(R[b])`.
2.  `R` exchange.
  * A and B exchange `R[a]` and `R[b]`.
  * They validate that the previous given `h(R[a])` and `h(R[b])` matches.
3.  `s` exchange.
  * They compute `R` as `R = R[a] + R[b]`.
  * They compute `e` as `h(R | m)`.
  * A computes `s[a]` as `s[a] = r[a] + e * h(L) * a` and B computes `s[b]` as `s[b] = r[b] + e * h(L) * b`.
  * They exchange `s[a]` and `s[b]`.
  * They compute `s` as `s = s[a] + s[b]`.
  * They publish the signature as the tuple `(e, s)`.
At validation, the validator knows `P`, `m`, and the signature `(R, s)`.
* It recovers `e[validator]` as so: `e[validator] = h(R | m)`
* It computes `S[validator]` as so: `S[validator] = R + e[validator] * P`.
* It checks if `s * G == S[validator]`.
  * `S[validator] = R + e[validator] * P`
  * `== R[a] + R[b] + e[validator] * P`; substitution of `R`
  * `== r[a] * G + r[b] * G + e[validator] * P`; substitution of `R[a]` and `R[b]`
  * `== r[a] * G + r[b] * G + e * P`; substitution of `e[validator]` with `e`
  * `== r[a] * G + r[b] * G + e * (h(L) * A + h(L) * B)`; substitution of `P`
  * `== r[a] * G + r[b] * G + e * h(L) * A + e * h(L) * B`; distribution of `e` inside parentheses.
  * `== r[a] * G + r[b] * G + e * h(L) * a * G + e * h(L) * b * G`; substitution of `A` and `B`.
  * `== (r[a] + r[b] + e * h(L) * a + e * h(L) * b) * G`; factoring out of `G`
  * `== (r[a] + e * h(L) * a + r[b] + e * h(L) * b) * G`; rearrangement of terms
  * `== (s[a] + s[b]) * G`; substitution of `r[a] + e * h(L) * a` and `r[b] + e * h(L) * b`
  * `== s * G`;  substitution of `s[a] + s[b]`
Two-Phase MuSig Unsafe
Original proposal of MuSig only had two phases, `R` exchange and `s` exchange.
However, there was a flaw found in the security proof in this two-phase MuSig.
In response, an earlier phase of exchanging commitments to `R` was added.
Thus, two-phase MuSig is potentially unsafe.
 describes the argument.
Briefly, with two-phase MuSig, one of the participants can deliberately delay its side of a `R` exchange and control the resulting sum `R` by cancelling the `R` of the other participant.
Executed over many (aborted) signing sessions, one participant can induce the other to create a signature for a message it might not agree to, by using the Wagner Generalized Birthday Paradox attack.
Briefly, a two-phase MuSig signing would go this way:
1.  `R` exchange.
  * A generates random scalar `r[a]` and B generates random scalar `r[b]`.
  * A computes `R[a]` as `r[a] * G` and B computes `R[b]` as `r[b] * G`.
  * They exchange `R[a]` and `R[b]`.
2.  `s` exchange.
  * They compute `R` as `R = R[a] + R[b]`.
  * They compute `e` as `h(R | m)`.
  * A computes `s[a]` as `s[a] = r[a] + e * h(L) * a` and B computes `s[b]` as `s[b] = r[b] + e * h(L) * b`.
  * They exchange `s[a]` and `s[b]`.
  * They compute `s` as `s = s[a] + s[b]`.
  * They publish the signature as the tuple `(R, s)`.
The sticking point is "exchange" here.
Given that we live in a relativistic universe where there is no such thing as simultaneity-in-time-without-simultaneity-in-space, it is impossible to ensure that both A and B send their data "at the same time" in such a way that it is impossible for, for example, the send of B to be outside the future light cone of the send of A.
Or in human-level terms, it is not possible to ensure over the network that B will not send `R[b]` *after* it receives `R[a]`.
Suppose that instead of B generating a random `r[b]` and *then* computing `R[b] = r[b] * G`, it instead selects an arbitrary `R[selected]` it wants to target, then compute `R[b]` as `R[selected] - R[a]`.
Then at `s` exchange:
* They compute `R` as `R[a] + R[b]`, which is in fact `R[a] + R[selected] - R[a]`, or `R[selected]`, i.e. `R == R[selected]`.
* They compute `e` as `h(R[selected] | m)`.
* A computes `s[a]` as `s[a] = r[a] + e * h(L) * a`.
* B is unable to compute `s[b]` as it has no `r[b]` it can use in the computation, and aborts the signing.
The attack involved is that multiple such signing sessions, for the same message or for separate distinct messages, might be done in parallel.
Suppose that there are `n` such sessions, such that A provides `n` different `R[a][i]`, e.g. `R[a][1]`, `R[a][2]`, `R[a][3]` up to `R[a][n]`.
* B delays each session, pretending to have Internet connectivity problems.
* B selects a message `m[target]` that it knows A will never sign (e.g. "I, A, give all my money to B").
* B computes `R[target]` as `sum where i = 1 to n of R[a][i]`.
* B uses the Wagner Generalized Birthday Paradox technique to find `R[selected][i]` with the following constraint:
  * `h(R[target] | m[target]) == sum where i = 1 to n of h(R[selected][i] | m[i])`.
  * Given a large enough number of parallel sessions `n`, this can greatly reduce the effort from 2^128 to find a match to within the realm of a large computer to compute within a few seconds.
* B computes `R[b][i]` as `R[selected][i] - R[a][i]`, for each `i` from 1 to `n`.
* B provides `R[b][i]` for each session.
* A computes `R[i]` as `R[a][i] + R[b][i]` for each session.
  * However we know that `R[b][i] == R[selected][i] - R[a][i]` for each session, cancelling out `R[a][i]` and leaving `R[i] == R[selected][i]`
* A computes `s[a][i]` as `r[a][i] + h(R[selected][i] | m[i]) * h(L) * a` for each session.
* A gives `s[a][i]` for each session.
* B aborts each session.
* B sums up all the `s[a][i]`:
  * `(sum where i = 1 to n of r[a][i]) + (sum where i = 1 to n of h(R[selected][i] | m[i]) * h(L) * a)`.
  * Remember, B has specifically selected `R[selected][i]` such that `h(R[target] | m[target])` is equal to the sum of `h(R[selected][i] | m[i])`.
  * `== (sum where i = 1 to n of r[a][i]) + h(R[target] | m[target]) * h(L) * a)`.
* B adds `h(R[target] | m[target]) * h(L) * b` to the above sum.
  * This results in a signature for MuSig(A, B) to the message `m[target]`, even though A would never have agreed to this message.
Thus, 2-phase MuSig enables a Wagner attack on the participant, thus it is unsafe.
Now, any method of ensuring a "simultaneous" exchange of `R` points is largely the same as adding a "commit to `R`" phase, i.e. the fix for this is simply to add the "`R` commitment exchange" phase.
MuSig Composition
Let us suppose that we have some protocol that requires a MuSig signing session between signers with public keys `P` and `C`.
Let us further suppose that in fact, `P = MuSig(A, B)`, i.e. one of the public keys in this protocol is, in reality, itself a MuSig of two participants.
At the point of view of signer C, P is a single participant and it acts as such.
However, in reality, P is an aggregate.
We want to have the following properties:
* C should never need to know that P is in fact an aggregate.
* Even if B is secretly the same as C, the entire protocol as a whole (including the aggregate signing of `MuSig(A, B)`) should remain three-phase MuSig.
Now, from the point of view of C, what it sees are:
At setup:
* It generates a random scalar `c` and the public key `C` as `C = c * G`.
* It exchanges keys with P and gets the public key `P`.
* It computes `L` by sorting `C` and `P` and concatenating them.
* It determines their aggregate key as `h(L) * C + h(L) * P`.
At signing:
1.  `R` commitment exchange.
  * It generates a random scalar `r[c]` and computes `R[c]` as `R[c] = r[c] * G`.
  * It computes `h(R[c])`.
  * It exchanges the hash `h(R[c])` with P and gets `h(R[p])`.
2.  `R` exchange.
  * It exchanges `R[c]` with P and gets `R[p]`.
  * It validates that the hash `h(R[p])` matches the previously-committed hash.
3.  `s` exchange.
  * It computes `R` as `R = R[c] + R[p]`.
  * It computes `e` as `e = h(R | m)`.
  * It computes `s[c]` as `s[c] = r[c] + e * c`.
  * It exchanges `s[c]` with P and gets `s[p]`.
  * It computes `s` as `s = s[c] + s[p]`.
However, from point of view of A and B, what actually happens is this:
At setup:
* A generates a random scalar `a` and computes `A = a * G`, B generates a random scalar `b` and computes `B = b * G`.
* They exchange `A` and `B`.
* They generate their own `L[ab]`, by sorting `A` and `B` and concatenating their representations.
* They compute the inner MuSig pubkey `P` as `P = h(L[ab]) * A + h(L[ab]) * B`.
* They exchange `P` with C, and get `C`.
* They compute the outer MuSig pubkey as `h(L) * P + h(L) * C`.
At signing:
1.  `R` commitment exchange.
  * A generates a random scalar `r[a]` and computes `R[a] = r[a] * G`, B generates a random scalar `r[b]` and computes `R[b] = r[b] * G`.
  * A computes `h(R[a])`, B computes `h(R[b])`.
  * They exchange `h(R[a])` and `h(R[b])`.
  * They need to compute `h(R[p])` for the protocol with C.
    * However, even if we know `R[p] == R[a] + R[b]`, we cannot generate `h(R[p])`.
    * Thus, they have no choice but to exchange `R[a]` and `R[b]` at this phase, even though this is supposed to be the `R` commitment exchange phase (and they should not share `R[a]` and `R[b]` yet)!
Unfortunately, this means that, between A and B, we are now reduced to a two-phase MuSig.
This is relevant if B and C happen to be the same entity or are cooperating.
Basically, before C has to provide its `h(R[c])`, B now knows the generated `R[a]` and `R[b]`.
If B and C are really the same entity, then C can compute `R[c]` as `R[selected] - R[a] - R[b]` before providing `h(R[c])`.
Then this devolves to the same attack that brings down 2-phase MuSig.
Thus, composition with the current MuSig proposal is insecure.
Towards a Composable Multi-`R` MuSig
A key element is that the first phase simply requires that all participants provide *commitments* to their individual `R`, and the second phase reveals their `R`.
I propose here the modification below:
* In the first phase, any participant in the MuSig may submit one *or more* `R` commitments.
* In the second phase, the participant in the MuSig submits each `R` that decommits each of the `R` commitments it sent.
I call this the Remote R Replacement Remanded: Redundant R Required Realistically, or, in shorter terms, the Multi-`R` proposal.
This is a simple and direct extension of the MuSig protocol, and expected to not have any effect on the security proof of MuSig.
In the case where all MuSig participants are singletons and each participant just generates and sends a single `R` commitment, then this proposal reduces to the original MuSig proposal.
However, in the case where one participant is in reality itself an aggregate, then we shall describe it below.
The below example is `MuSig(MuSig(A, B), C)`.
1.  `R` commitment exchange.
  * A generates a random number `r[a]`, B generates a random number `r[b]`, C generates a random number `r[c]`.
  * A computes `R[a]` as `r[a] * G`, B computes `R[b]` as `r[b] * G`, C computes `R[c]` as `r[c] * G`.
  * A computes `h(R[a])`, B computes `h(R[b])`, C computes `h(R[c])`.
  * A and B exchange their hashes with each other.
  * A and B jointly exchange their `h(R[a])` and `h(R[b])` with the `h(R[c])` from C.
2.  `R` exchange.
  * A and B reveal their `R[a]` and `R[b]` with each other.
  * A and B validate the given `R[a]` matches `h(R[a])` and the given `R[b]` matches `h(R[b])`.
  * A and B jointly exchange their `R[a]` and `R[b]` with the `R[c]` from C.
  * C validates `R[a]` and `R[b]`, A and B validate `R[c]`.
  * They compute `R` as the sum of all `R[a] + R[b] + R[c]`.
3.  `s` exchange.
  * They compute `e` as `h(R | m)`.
  * A computes `s[a]` as `r[a] + e * h(L[abc]) * h(L[ab]) * a`, B computes `s[b]` as `r[b] + e * h(L[abc]) * h(L[ab]) * b`.
  * C computes `s[c]` as `r[c] + e * h(L[abc]) * c`.
  * A and B exchange `s[a]` and `s[b]`.
  * A and B compute `s[ab]` as `s[a] + s[b]`.
  * A and B jointly exchange their `s[ab]` with `s[c]` from C.
  * They compute `s` as `s[ab] + s[c]`.
  * They publish the signature as the tuple `(R, s)`.
Of note, is that the number of `R` a participant provides is a strong hint as to whether it is actually an aggregate or not, and forms an upper bound as to the size of the aggregate (i.e. an aggregate of `n` members can pretend to be an aggregate of `m` members where `n < m`, but cannot pretend with `n > m`).
Thus, C here learns that its counterparty is actually itself an aggregate rather than a singleton.
This may be acceptable as a weak reduction in privacy (in principle, C should never learn that it is talking to an aggregate rather than a single party).
Alternative Composable MuSig Schemes
The above proposal is not the only one.
Below are some additional proposals which have various flaws, ranging from outright insecurity to practical implementation complexity issues.
Pedersen Commitments in Phase 1
ElGamal commitments prevent B from just giving random `q[b][i]`, thus preventing the above Wagner attack.
However, it is still possible for B to control the resulting `R`, but in doing so this prevents the protocol from completing (thus, even with full control of `R`, B is still unable to promote this to an `R`-reuse attack or the above Wagner attack schema).
This is not quite as bad as the above case, but having just one participant control the nonce `R` should make us worry that other attacks may now become possible (unless we acquire some proof that this will be equivalent in security to the hash-using MuSig).
Briefly, with ElGamal commitments in Phase 1:
1. `R` commitment exchange.
  * A generates random numbers `r[a]` and `q[a]`, B generates random numbers `r[b]` and `q[b]`.
  * A computes its commitment as two points, `q[a] * G` and `r[a] * G + q[a] * H`, B computes its commitment as two points, `q[b] * G` and `r[b] * G + q[b] * H`.
    * `H` is a NUMS point used as a second standard generator.
    * Note that one point uses `q[] * G` while the other adds `q[] * H` to `r[] * G`.
  * They exchange their pairs of points.
2. `R` exchange.
  * They exchange `q[a]` and `q[b]`, and the points `r[a] * G` (== `R[a]`) and `r[b] * G` (== `R[b]`).
  * They validate the exchanged data from the previous `R` commitments.
  * They compute `R` as `R[a]` + `R[b]`.
3. `s` exchange.
  * Same as before.
B can attack this by delaying its phases as below:
1. `R` commitment exchange.
  * B generates random `q[selected]`.
  * B selects target `R[selected]`.
  * After receiving `q[a] * G` and `r[a] * G + q[a] * H`, B computes `q[selected] * G - q[a] * G` and `R[selected] + q[selected] * H - r[a] * G - q[a] * H` and sends those points as its own commitment.
2. `R` exchange.
  * After receiving `q[a]` and `R[a]`, B computes `q[b]` as `q[selected] - q[a]` and computes `R[b]` as `R[selected] - R[a]` and sends both as its decommitment.
  * The resulting `R` will now be `R[selected]` chosen by B.
`s` exchange cannot complete, as that would require that B know `r[selected] - r[a]` where `R[selected] = r[selected] * G`.
Even if B generates `R[selected]` from `r[selected]`, it does not know `r[a]`.
A would provide `r[a] + h(R[selected] | m) * h(L[ab]) * a`, but B would be unable to complete this signature.
The difference here is that B has to select `R[selected]` before it learns `R[a]`, and thus is unable to mount the above Wagner attack schema.
In particular, B first has to compute an `R[target]` equal to `sum where i = 1 to n of R[a][i]` across `n` sessions numbered `i`, in addition to selecting a message `m[i]`.
Then B has to perform a Wagner attack with the constraint `h(R[target] | m[target]) == sum where i = 1 to n of h(R[selected][i] | m[i])`
Fortunately for this scheme, B cannot determine such an `R[target]` before it has to select `R[selected]`, thus preventing this attack.
It may be possible that this scheme is safe, however I am not capable of proving it safe.
I contacted Yannick Seurin, Andrew Poelstra, Pieter Wuille, and Greg Maxwell, the authors of MuSig, regarding this issue, and proposing to use Pedersen commitments for the first phase.
They informed me that Tim Ruffing had actually been thinking of similar issue before I did, and also pointed out that Pedersen commitments do not commit to `r * G`, only to `r` (i.e. would have to reveal `r` to serve as a verifiable commitment).
It seemed to me that the general agreement was that ElGamal commitments should work for committing to `r * G`.
However as I show above, this still allows a delaying participant to cancel the `R` contributions of the other parties, allowing it to control the aggregate `R` (though not quite to launch a Wagner attack).
`nickler` and `waxwing` on IRC confirmed my understanding of the attack on 2-phase MuSig.
`waxwing` also mentioned that the paper attacking 2-phase MuSig really attacks CoSi, and that the paper itself admits that given a knowledge-of-secret-keys, CoSi (and presumably 2-phase MuSig) would be safe.

@_date: 2019-10-01 13:31:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about 
Good morning lists,
Let me summarize concerns brought up:
* Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.
* My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.
I propose the below instead:
* Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.
* Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.
Then, on usage:
* Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.
* Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.
This solves both concerns:
* Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.
* If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.
Again, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.
But if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.
Would this alternate proposal hold better muster?

@_date: 2019-10-01 15:35:34
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
Good morning Christian,
My understanding is that a trigger transaction is not in fact necessary for Decker-Russell-Osuntokun: any update transaction could spend the funding transaction output directly, and thereby start the relative timelock.
At least, if we could arrange the funding transaction output to be spendable directly using `SIGHASH_NOINPUT` or variants thereof.
* Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.
  * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.
* Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.
  * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.
The point of ideal cases is to strive to approach them, not necessarily achieve them.
Just as a completely unbiased rational reasoner is almost impossible to achieve, does not mean we should give up all attempts to reduce bias.
Outpoints that used to be channels, but have now been closed using cooperative closes, will potentially no longer be widely gossiped as having once been channels, thus it may happen that they will eventually be forgotten by most of the network as once having been channels.
But if the outpoints of those channels are specially marked, then that cannot be forgotten, as the initial block download thereafter will have that history indelibly etched forevermore.

@_date: 2019-10-01 15:42:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
Good morning aj,
Thank you for this thought,
I believe under tapscript v0 we can give `OP_1` as the public key to `OP_CHECKSIG` to mean to reuse the internal Taproot pubkey, would it be possible to have some similar mechanism here, to copy the internal Taproot pubkey but also to enable new `SIGHASH` flag for this particular script only?
This seems fine, as then a Decker-Russell-Osuntokun funding tx output between nodes A, B, and C would have:
* Taproot internal key: `P = MuSig(A, B, C)`
* Script 1: leaf version 0, ` OP_CHECKSIG`
Then, update transactions could use `MuSig(A,B,C)` for signing along the "update" path, with unique "state" keys.
And cooperative closes would sign using `P + h(P | MAST( OPCHECKSIG)) * G`, not revealing the fact that this was in fact a Decker-Russell-Osuntokun output.

@_date: 2019-10-02 02:03:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
Good morning lists,
Let me propose the below radical idea:
* `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
  * 1 RETURN
  * higher-`nSequence` replacement
  * DER-encoded pubkeys
  * unrestricted `scriptPubKey`
  * Payee-security-paid-by-payer (i.e. lack of P2SH)
  * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
  * transaction malleability
  * probably many more
So let me propose the more radical excision, starting with SegWit v1:
* Remove `SIGHASH` from signatures.
* Put `SIGHASH` on public keys.
Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
`OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.
As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.
I propose also the addition of the opcode:
      OP_SETPUBKEYSIGHASH
* `sighash` must be one byte.
* `pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
* `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
* `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
* If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
This is done by using the script:
     OP_SETPUBKEYSIGHASH OP_CHECKSIG
Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.
However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
Would this not be a superior solution?

@_date: 2019-10-03 03:07:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about 
Nonexistence of sighash byte implies `SIGHASH_ALL`, and for offchain anyway the desired path is to end up with an n-of-n MuSig `SIGHASH_ALL` signed mutual close transaction.
Indeed we can even restrict keypath spends to not having a sighash byte and just implicitly requiring `SIGHASH_ALL` with no loss of privacy for offchain while attaining safety against `SIGHASH_NOINPUT` for MuSig and VSSS multisignature adresses.
As the existing sighashes are not particularly used anyway, additional restrictions on them are relatively immaterial.
Only one of the scripts is widely used, another has an edge use it sucks at (assurance contracts).
Does not seem to be good design, rather legacy cruft.

@_date: 2019-10-03 23:42:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Good morning Ethan,
Why 64 bytes in particular?
It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.
However we have had issues with the use of Merkle trees in Bitcoin blocks.
Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.
My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.
The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.
Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.
This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.
Or we could implement tagged SHA256 as a new opcode...

@_date: 2019-10-04 06:45:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smaller "Bitcoin address" accounts in the 
Good morning David,
This moves us closer to an "account"-style rather than "UTXO"-style.
The advantage of UTXO-style is that it becomes easy to validate a transaction as valid when putting it into the mempool, and as long as the UTXO it consumes remains valid, revalidation of the transaction when it is seen in a block is unnecessary.
Admittedly, the issue with account-style is when the account is overdrawn --- with UTXOs every spend drains the entire "account" and the "account" subsequently is definitely no longer spendable, whereas with accounts, every fullnode has to consider what would happen if two or more transactions spend from the account.
In your case, it seems to just *add* to the amount of a UTXO.
In any case, this might not be easy to implement in current Bitcoin.
The UTXO-style is deeply ingrained to Bitcoin design, and cannot be easily hacked in a softfork.
See also  and its thread for the difficulties involved with "just copy some existing `scriptPubKey`" and why such a thing will be very unlikely to come in Bitcoin.
But I think this can be done, in spirit, by pay-to-endpoint / payjoin.
In P2EP/Payjoin, the payer contacts the payee and offers to coinjoin simultaneously to the payment.
This does what you want:
* Refers to a previous UTXO owned by the payee, and deletes it (by normal transaction spending rules).
* Creates a new UTO, owned by the payee, which contains the total value of the below:
  * The above old UTXO.
  * The value to be transferred from payer to payee.
The only issues are that:
* Payee has to be online and cooperate.
* Payee has to provide signatures for the old UTXO, adding more blockchain data.
* New UTXO has to publish a SCRIPT too.
  * In terms of *privacy*, of course you *have* to use a new SCRIPT with a new public key anyway.
    Thus this is superior to your proposal where the pubkey is reused, as P2EP/Payjoin preserves privacy.

@_date: 2019-10-04 07:00:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Good morning Jeremy,
This seems a good idea.
Though it brings up the age-old tension between:
* Generically-useable components, but due to generalization are less efficient.
* Specific-use components, which are efficient, but which may end up not being useable in the future.
In particular, `OP_SHA256STREAM` would no longer be useable if SHA256 eventually is broken, while the `OP_CAT` will still be useable in the indefinite future.
In the future a new hash function can simply be defined and the same technique with `OP_CAT` would still be useable.

@_date: 2019-10-06 08:46:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the 
Good morning Peter, Jeremy, and lists,
Theoretically, `OP_CAT` is less efficient.
In cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.
This leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).
`OP_SHASTREAM` would not require new allocations once the stream state is in place and would not require any copying.
This may be relevant in considering the cost of executing `OP_CAT`.
Admittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.
The question is what limit would be reasonable.
64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.

@_date: 2019-09-06 14:32:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Reconciling the off-chain and on-chain models 
Good morning Christian,
This is effectively transaction cut-through.
I mention this in passing here: Basically, we can send a transaction that spends a subset of the current state txos to the participants in the update mechanism.
Then the participants can agree that it is a valid spend of the specified state txos, and agree to sign a new state with the spent txos deleted and the new txos of the transaction inserted.
Disagreement at this point is essentially a "if your tx is so valid why do you not try it on the base blockchain layer huh?" challenge and is basically an invitation to close it unilaterally and enforce the contract on the blockchain.
The "difficulty" in Poon-Dryja is not very onerous in my opinion; see the sketch here: Of note is that any contract with a relative locktime requirement would not make sense to maintain offchain.
If one wishes to select a relative locktime relative to the current moment, one can quite easily compute an absolute timelock.
Another note, is that contracts with timelocks need to be enforced onchain on or before the timelock.
Under Decker-Russell-Osuntokun the onchain enforcement needs to be triggered early according to the CSV security parameter; this is not an issue under Poon-Dryja (as the CSV is in a later transaction).
Under Decker-Russell-Osuntokun due to the use of `SIGHASH_NOINPUT` and the non-stable txids involved, any transaction you wish to transport in the offchain update mechanism needs to also be signed under `SIGHASH_NOINPUT`, but again this is not onerous.
In any case it is "only" a matter of tradeoffs one is willing to work under anyway, and Decker-Russell-Osuntokun is very cool and uses `nLockTime` and `OP_CHECKLOCKTIMEVERIFY` in a very clever way.

@_date: 2019-09-09 04:14:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PoW fraud proofs without a soft fork 
Good morning Ruben,
I think it would be better to more precisely say that:
1.  In event of a sybil attack, a fullnode will stall and think the blockchain has no more miners.
2.  In event of a sybil attack, an SPV, even using this style, will follow the false blockchain.
This has some differences when considering automated systems.
Onchain automated payment processing systems, which use a fullnode, will refuse to acknowledge any incoming payments.
This will lead to noisy complaints from clients of the automated payment processor, but this is a good thing since it warns the automated payment processor of the possibility of this attack occurring on them.
The use of a timeout wherein if the fullnode is unable to see a new block for, say, 6 hours, could be done, to warn higher-layer management systems to pay attention.
While it is sometimes the case that the real network will be unable to find a new block for hours at a time, this warning can be used to confirm if such an event is occurring, rather than a sybil attack targeting that fullnode.
On the other hand, such a payment processing system, which uses an SPV with PoW fraud proofs, will be able to at least see incoming payments, and continue to release product in exchange for payment.
Yet this is precisely a point of attack, where the automated payment processing system is sybilled and then false payments are given to the payment processor on the attack chain, which are double-spent on the global consensus chain.
And the automated system may very well not be able to notice this.

@_date: 2019-09-09 06:58:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] PoW fraud proofs without a soft fork 
Good morning Ruben,
Yes, I suppose that is correct.
I suppose the critical difference is that invalid inflation can fool the SPV node, the fullnode will not be so fooled.
A somewhat larger-scale attack is to force a miner-supported miner-subsidy-increase / blocksize-increase hard fork.
If enough such SPV nodes can be sybilled, they can be forced to use the hard fork, which might incentivize them to support the hard fork rather than back-compatible consensus chain.

@_date: 2019-09-10 01:28:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] Reconciling the off-chain and 
Good morning Richard,
I broadly agree.
I imagine a future where most people do not typically have single-signer ownership of coins onchain, but are instead share-owners of coins, with single-signer ownership occurring onchain only in the case of dispute or for long-term cold storage.
Such a change-in-membership transaction would be a 1-input 1-output transaction, and with use of n-of-n MuSig would be as small (and as private, modulo the fact that you are coordinating this with a bunch of other participants) as a single-sig user making a 1-input 1-output transaction (which generally is not very private because such transactions are usually "send-to-self" and changing membership generally means ownership does not actually change much).
The cost of this transaction would be small (certainly smaller than the update+state transactions needed in Decker-Russell-Osuntokun)
For setting this up, it might be useful to have the below ritual.
This assumes only a change in the membership set is desired, without a simultaneous change in the UTXO set.
1.  Create a new update+state transaction for the current Decker-Russell-Osuntokun mechanism.
    The state transaction pays out to a single output paying to the new membership set rather than the current UTXO set of the mechanism.
    Do *not* sign this yet.
    Call this the "final" update+state transaction.
2.  Create a new Decker-Russell-Osuntokun mechanism initial update+state transaction.
    This pays out to the current UTXO set of the previous mechanism.
    This will spend from the new membership set.
    Completely sign these transactions.
    * The update transaction can spend the above "final" transaction, as it is `SIGHASH_NOINPUT`.
3.  Sign the final update+state transaction of the previous Decker-Russell-Osuntokun mechanism.
    Do *not* broadcast the update+state transaction yet.
4.  Create and sign the membership-change onchain transaction.
    This spends the current onchain funding transaction output and outputs to the same new membership set.
    Broadcast this onchain.
The above ritual ensures that, after step 3 completes, the mechanism can continue operating without waiting for onchain activity to complete.
It ensures that, even if the membership-change onchain transaction becomes invalid later (by somebody bribing a miner to publish a previous update transaction from the older membership set), we will still enter an update that will eventually put the new membership set onchain.
This reduces the critical path to only steps 1 to 3, and we can continue operating with the new membership set as soon as step 3 completes and we do not need to wait for the membership-change transaction to be deeply-confirmed in order to use the new membership set mechanism.
However, it has the drawback that, until the membership-change onchain transaction is deeply-confirmed onchain, the CSV parameter is temporarily doubled (as there is the possibility that the previous mechanism is closed).
Also, the mechanism cannot be mutually closed until the membership-change onchain transaction is deeply-confirmed, as there is no stable txid we can spend from (we would strongly prefer to use `SIGHASH_ALL` for cooperative closes to improve our privacy).
The "block" that would need to be signed by the participants would actually be a Decker-Russell-Osuntokun update+state transaction, and would commit to the UTXO set rather than the transaction set.
Unless I misunderstand your meaning here.

@_date: 2019-09-17 01:54:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Introcing a side memory network to bitcoin for ads 
Good morning Tamas,
Thank you for taking the time to implement my idea.
I filed an issue proposing a feature to add a "contact point" fixed-length field to all advertisements.
I believe this gives me the right to say: First post.
I will try to take a look at building some kind of UI at some point in the next few months or years.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-09-17 04:09:50
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot proposal 
Good morning Greg and John,
I am not as sanguine here; SegWit activation was already delayed relative to commonly-broadcast expectations, yet many services *still* do not support sending to SegWit v0 addresses even now.
On the other hand, the major benefit of taproot is the better privacy and homogeneity afforded by Taproot, and supporting both P2SH-wrapped and non-wrapped SegWit v1 addresses simply increases the number of places that a user may be characterized and potentially identified.
Thus while I disagree with your reasoning, I do agree with your conclusion: no P2SH-wrapped SegWit v1.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-09-18 04:33:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Transcripts from Scaling Bitcoin 2019 
Good morning Bryan,
Thank you very much for these.
I appreciate greatly this effort to make transcripts more easily available.
For myself, I find it faster to read such transcript than to watch the video.
Sent with ProtonMail Secure Email.
??????? Original Message ???????

@_date: 2019-09-18 04:41:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Introcing a side memory network to bitcoin for ads 
Good morning list,
In case it is not obvious how this mechanism can be used, let me give me some short discussion.
Many decentralized coin-mixing services require some concept of "maker", which serves as a temporary centralization in order to allow clients of the mixing service to find each other.
Such makers might advertise themselves, backing their advertisements with locked coins.
The text of the advertisement may very well be a machine-readable description, such as JSON, including information about the maker in the coin-mixing service.
Escrow services for decentralized real-good-to-digital-good marketplaces (e.g. decentralized exchanges) might advertise themselves over this mechanism also.
The actual advertising of marketplace offers might also be done via this mechanism.
Again, machine-readable descriptions might be transported over the advertisement text mechanism, in order to allow programs to present the "most natural" interface to end-users.

@_date: 2019-09-18 05:28:38
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] Reconciling the off-chain and 
Good morning Richards, and list,
Of note is that a close of an update mechanism does not require the close of any hosted update mechanisms, or more prosaically, "close of channel factory does not require close of hosted channels".
This is true for both unilateral and cooperative closes.
Of course, the most likely reason you want to unilaterally close an outer mechanism is if you have some contract in some deeply-nested mechanism that will absolute-locktime expire "soon", in which case you have to close everything that hosts it.
But for example if a channel factory has channels A B C and only A has an HTLC that will expire soon, while the factory and A have to close, B and C can continue operation, even almost as if nothing happened to A.
Assuming you mean that any owned funds will eventually have to be claimed onchain, I suppose this is doable as splice-out.
But note that currently we have some issues with splice-in.
As far as I can tell (perhaps Lisa Neigut can correct me, I believe she is working on this), splice-in has the below tradeoffs:
1.  Option 1: splice-in is async (other updates can continue after all participants have sent the needed signatures for the splice-in).
    Drawback is that spliced-in funds need to be placed in a temporary n-of-n, meaning at least one additional tx.
2.  Option 2: splice-in is efficient (only the splice-in tx appears onchain).
    Drawback is that subsequent updates can only occur after the splice-in tx is deeply confirmed.
    * This can be mitigated somewhat by maintaining a pre-splice-in and post-splice-in mechanism, until the splice-in tx is deeply confirmed, after which the pre-splice-in version is discarded.
      Updates need to be done on *both* mechanisms until then, and any introduced money is "unuseable" anyway until the splice-in tx confirms deeply since it would not exist in the pre-splice-in mechanism yet.
But perhaps a more interesting thing (and more in keeping with my sentiment "a future where most people do not typically have single-signer ownership of coins onchain") would be to transfer funds from one multiparticipant offchain mechanism to another multiparticipant offchain, by publishing a single transaction onchain.
It may be doable via some extension of my proposed ritual for changing membership set.
I believe this is currently considered unsafe.
Unless you refer to another mechanism...?
I believe this will end up requiring deep confirmation of the uncooperative close followed by a new mechanism open.
Fees for each update.
Consider how HTLC routing in Lightning implicitly pays forwarding nodes to cooperate with the forwarding.
I imagine most nodes in a multiparticipant offchain system will want to be paid for cooperation, even if just a nominal sub-satoshi amount.
I suppose Tor can be used to disassociate IP address from signers if everyone is from a hidden service.
However, we need to include some kind of mix mechanism to allow individual signers to disassociate their ownership of funds from their identity as signers.
Though such mechanisms already exist as theoretical constructs, so "just needs implementing".
But then again: if you own funds in the mechanism, you *should* be a signer (else you are trusting a federation).
So a basic fact here is that if you are a participant in some offchain mechanism, you are likely (approaching 100% probability) to own money in it.
It might be possible to create a new mechanism-within-mechanism layer, if a signer knows they will be offline.
For example, suppose entities A, B, and C have an offchain update mechanism, which we shall call a "factory".
Suppose this factory contains an A-B channel, a B-C channel, a A-C channel, and some funds owned by B only.
Then suppose A knows he or she will be offline for some time.
Before A goes offline, they can move from this UTXO set:
* A-B channel
* B-C channel
* A-C channel
* B funds
To this UTXO set:
* A-B channel
* A-C channel
* B-C offchain update mechanism (sub-factory), which itself has its own UTXO set:
  * B-C channel
  * B funds
This allows B and C to manage the B-C channels and B funds without cooperation of A.
Then, later, when A returns online, the B-C offchain update mechanism is collapsed back to the parent A-B-C offchain update mechanism.
This assumes A knows it will be offline (which it might do for e.g. regular maintenance, or software updates).

@_date: 2019-09-19 02:01:54
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] Reconciling the off-chain and 
Good morning Christian, and list,
This would have to be very very carefully designed.
The entire point of requiring an n-of-n signature is:
* By using an n-of-n signatory where *you* are a signer, you are completely immune to Sybil attacks: even if everybody other than *you* in the signatory set is secretly just one entity, this is no different from doing a 2-of-2 bog-standard boring sleepy Zzzzzz Poon-Dryja Lightning Network channel.
  * Any m-of-n signatory where strictly m < n allows anybody with the ability to run m nodes to outright steal money from you.
    * As processing power is cheap nowadays, there is no m that can be considered safe.
      Your alternative is to fall back on proof-of-work, but that just means going onchain, so you might as well just do things onchain.
  * This is why 2-of-2 channels work so well, it's the minimum useable construction and any multiparty construction, when Sybilled, devolves to a 2-of-2 channel.
So the n-1 participants would have to be very very very carefully limited in what they can do.
And if the only "right" the n-1 participants can do is to force the nth participant to claim its funds onchain, then that is implementable with a transaction doing just that, which is pre-signed by the nth participant and given to participants 1..n-1.
I suppose that could be argued.
However, I imagine it is possible for some of the updates to be implementable via HTLCs within sub-mechanisms of the higher mechanism.
If so, a participant may refuse to sign for the higher mechanism in order to force others to use HTLCs on the lower mechanisms, and thereby earn fees due to HTLC usage.
I believe I argue as much here: The counterargument above is that if rebalances can be made fee-free, then the above argument disappears.
Yes, but if we later combine this with allowing multiilateral kick-out of a member that is unresponsive (i.e. we splice out the outputs it has at least partial ownership of, and keep only those that are owned only by the remaining members), then each member would have to honestly claim which UTXOs it is interested in keeping after it is kicked out of the membership set, defeating this point entirely.
I believe this is roughly what you propose in the next point, and roughly what you would want with the "n-1 participants" earlier.
This might be a plausible way of implementing the "n-1 participants can kick out nth participant".
Statement makes no sense, unless you meant to say "It may be a bit much complexity for a small benefit" or similar?

@_date: 2019-09-19 07:52:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Timelocks and Lightning on MimbleWimble 
Good morning list,
I was reading transcript of recent talk: And in section "Taproot: main idea":
I believe what is being referred to here is to simply have an `nLockTime` transaction that is signed by all participants first, and serves as the "timelock" path.
Then, another transaction is created, for which adaptor signatures are given, before completing the ritual to create a "hashlock" path.
I find it surprising that this is not well-known.
I describe it here tangentially, for instance: The section "Payjoin2swap Swap Protocol" refers to "pre-swap transaction" and "pre-swap backout transaction", which are `nLockTime`d transactions.
Later transactions then use a Scriptless Script-like construction to transfer information about a secret scalar x.
My understanding of MimbleWimble is that:
* There must exist a proof-of-knowledge of the sum of blinding factors used.
  This can be trivially had by using a signature of this sum, signing an empty message or "kernel".
* I believe I have seen at least one proposal (I cannot find it again now) where the "kernel" is replaced with an `nLockTime`-equivalent.
  Basically, the `nLockTime` would have to be explicitly published, and it would be rejected for a block if the `nLockTime` was less than the block height.
  * There may or may not exist some kind of proof where the message being signed is an integer that is known to be no greater than a particular value, and multiple signatures that signed a lower value can somehow be aggregated to a higher value, which serves this purpose as well, but is compressible.
My understanding is thus that the above `nLockTime` technique is what is indeed intended for MimbleWimble cross-system atomic swaps.
However, I believe that Lightning and similar offchain protocols are **not possible** on MimbleWimble, at least if we want to retain its "magical shrinking blockchain" property.
All practical channel constructions with indefinite lifetime require the use of *relative* locktime.
Of note is that `nLockTime` represents an *absolute* lifetime.
The only practical channel constructions I know of that do not require *relative* locktime (mostly various variants of Spilman channels) have a fixed lifetime, i.e. the channel will have to be closed before the lifetime arrives.
This is impractical for a scaling network.
It seems to me that some kind of "timeout" is always necessary, similar to the timeout used in SPV-proof sidechains, in order to allow an existing claimed-latest-state to be proven as not-actually-latest.
* In Poon-Dryja, knowledge of the revocation key by the other side proves the published claimed-latest-state is not-actually-latest and awards the entire amount to the other party.
  * This key can only be presented during the timeout, a security parameter.
* In Decker-Wattenhofer decrementing-`nSequence` channels, a kickoff starts this timeout, and only the smallest-timeout state gets onchain, due to it having a time advantage over all other versions.
* In indefinite-lifetime Spilman channels (also described in the Decker-Wattenhofer paper), the absolute-timelock initial backoff transaction is replaced with a kickoff + relative-locktime transaction.
* In Decker-Russell-Osuntokun, each update transaction has an imposed `nSequence` that forces a state transaction to be delayed compared to the update transaction it is paired with.
It seems that all practical offchain updateable cryptocurrency systems, some kind of "timeout" is needed during which participants have an opportunity to claim an alternative version of some previous claim of correct state.
This timeout could be implemented as either relative or absolute lock time, but obviously an absolute locktime would create a limit on the lifetime of the channel.
Thus, if we were to target an indefinite-lifetime channel, we must use relative lock times, with the timeout starting only when the unilateral close is initiated by one participant.
Now, let us turn back to the MimbleWimble.
As it happens, we do *not* actually need SCRIPT to implement these offchain updateable cryptocurrency systems.
2-of-2 is often enough (and with Schnorr and other homomorphic signatures, this is possible without explicit script, only pubkeys and signatures, which MimbleWimble supports).
* Poon-Dryja revocation can be rewritten as an HTLC-like construct (indeed this was the original formulation).
  * Since we have shown that, by use of two transaction alternatives, one timelocked and the other hashlocked, we can implement an HTLC-like construct on MimbleWimble, that is enough.
* Relative locktimes in Decker-Wattenhofer are imposed by simple `nSequence`, not by `OP_CSV`.
  HTLCs hosted inside such constructions can again use the two-transactions construct in MimbleWimble.
* Ditto with indefinite-lifetime Spilman.
* Ditto with Decker-Russell-Osuntokun.
  * The paper shows the use of `OP_CSV`, but aj notes it is redundant, and I agree: Thus, it is not the "nonexistence of SCRIPT" that prevents Lightning from being deployed on MimbleWimble.
Instead, it is the "nonexistence of **relative** locktime" that prevents Lightning over MimbleWimble.
Why would **relative** locktimes not possibly exist?
In order to **validate** a relative locktime, we need to know the blockheight that the output we are spending was confirmed in.
But the entire point of the "magical shrinking blockchain" is that already-spent outputs can be removed completely and all that needs to be validated by a new node is:
* The coin-creation events.
* The current UTXO set (plus attached rangeproofs).
* The blinding keys.
* Signatures of the blinding keys, and the kernels they sign (if we use the "kernels encode `nLockTime`" technique in some way, they should not exceed the current supposed blockheight).
The problem is that an output that exists in the UTXO set might be invalid, if it appears "too near" to an `nSequence` minimum spend of a previous output that was spent in its creation.
That is, the above does not allow validation of **relative** locktimes, only **absolute locktimes**.
(At least as far as I understand: there may be special cryptographic constructs that allow signatures to reliably commit to some relative locktime).
This means that relative locktimes need to be implemented by showing the transactions that spend previous UTXOS and create the current UTXOs, and so no backwards to coin-creation events.
This forces us back to the old "validate all transactions" model of starting a new node (and seriously damaging the entire point of using MimbleWimble anyway).
I do not believe it is the lack of SCRIPT that prevents Lightning-over-MimbleWimble, but rather the lack of relative locktime, which seems difficult to validate without knowing the individual transactions and when they were confirmed.

@_date: 2019-09-19 15:15:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Timelocks and Lightning on MimbleWimble 
Good morning John,
Thank you for this information.
I am aware that absolute locktimes were possible in MimbleWimble.
However, it does seem to imply that kernels are not compressible (unlike the original MimbleWimble where the kernel is just an empty string and thus never stored).
So at least for kernels of relative locktimes, are not pruneable and will contribute to blockchain size.
(I believe I saw some proposal for absolute locktimes that allow some amount of aggregation/pruning of absolute-locktime kernels from the mimblewimble.pdf by andytoshi.)
Which I suppose is my point: you lose some of the "magic shrinking blockchain" property in implementing relative locktimes, as you now increase the data you have to store forever (i.e. the kernels).
It is not a *total* loss of the "magic shrinking blockchain", I see now, however.
Still, it does see worth the cost of accepting having to store kernels forever in exchange for being able to layer on top of a MimbleWimble blockchain.
It seems to me that Poon-Dryja and Decker-Wattenhofer can be "directly" ported over to any MimbleWimble blockchain with relative locktimes.
Reference [5] seems to be Poon-Dryja ported over to using relative locktimes for MimbleWimble.
Decker-Russell-Osuntokun ("eltoo") is harder due to the `SIGHASH_NOINPUT` requirement.
I have tried to derive an equivalent to this `SIGHASH_NOINPUT` somehow by considering that the "reference to previous kernel" as being akin to the Bitcoin transaction input referring to a previous output, however it seems to be not easy to create a retargatable "reference to previous kernel" in this way.
In any case, it seems to me that the loss of SCRIPT does not prevent a MimbleWimble blockchain from using an offchain updateable cryptocurrency system.

@_date: 2019-09-20 05:14:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Timelocks and Lightning on MimbleWimble 
Good morning John,
However, my understanding is that, at least with the original mimblewimble.txt from Jedusor, the signatures and the Pedersen-commitment-to-0 could all be aggregated into a single signature and Pedersen-commitment-to-0, if we were to use Schnorr-like signatures.
(it is possible I misunderstand this; I am not in fact a cryptographer.
Indeed, the original mimblewimble.txt mentions having to store every `k*G` and every signature attesting to it, although does not mention Schnorr and might not have considered the possibility of signature aggregation when using Schnorr-like signatures.
There could be security issues I am unaware of, for example.)
In addition, the mimblewimble.pdf from andytoshi includes a "Sinking Signatures" section, which to my understanding, combines absolute-locktime kernels with partial O(log n) aggregation of the signatures that attest it.
Again, it is possible I misunderstand this.
It seems to me that neither technique is possible with relative locktime kernels.
Again, this may be merely my ignorance of such.
In any case, this is mostly moot and I ask only out of curiosity in order to know more about kernels in non-relative-locktime MimbleWimble chains.
This seems interesting.
I shall look into this further.

@_date: 2019-09-30 16:00:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
Good morning Christian,
I rather strongly oppose output tagging.
The entire point of for example Taproot was to reduce the variability of how outputs look like, so that unspent Taproot outputs look exactly like other unspent Taproot outputs regardless of the SCRIPT (or lack of SCRIPT) used to protect the outputs.
That is the reason why we would prefer to not support P2SH-wrapped Taproot even though P2SH-wrapping was intended to cover all future uses of SegWit, including SegWit v1 that Taproot will eventually get.
Indeed, if it is output tagging that gets into Bitcoin base layer, I would strongly suggest the below for all Decker-Russell-Osuntokun implementations:
* A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain
* A "translator transaction" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.
* Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.
* Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.
The point regarding use of a commonly-known privkey to work around chaperone signatures is appropriate to the above, incidentally.
In short: this is a workaround, plain and simple, and one wonders the point of adding *either* chaperones *or* output tagging if we will, in practice, just work around them anyway.
Again, the *more* important point is that special blockchain constructions should only be used in the "bad" unilateral close case.
In the cooperative case, we want to use simple plain bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot SegWit v1 addresses, to increase the anonymity set of all uses of Decker-Russell-Osuntokun and other applications that might use `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple bip-schnorr-signed n-of-n cases when the protocol is completed successfully by all participants).
We already have the issue in current Lightning where the blockchain-explorer-revealed address for current, existing Poon-Dryja channels is unsafe to send any amount to.
Granted, we should work to make things safer; but I suggest that we should be willing to sacrifice some amount of safety against arguably-stupid decisions in order to have better privacy for larger sets of users.
I submit that a Taproot whose internal Taproot point is a NUMS point (thus nobody knows its scalar) is similarly "secure perfectly but only when used in the right way".
Yet the point of Taproot is to hide these outputs until they are spent, improving their privacy while unspent.
I submit also that a Taproot whose internal Taproot point is an n-of-n of all participants, with script branches enforcing particular modes, are similarly "secure perfectly but only when used in the right way", and again the point of Taproot is to allow the n-of-n "everybody agrees" path to hide among the 1-of-1 whale HODLers.
In short: I do not see how you can coherently argue for "we should separate `SIGHASH_NOINPUT` types to a new script type" while simultaneously arguing "we should merge all kinds of SCRIPT usage (and non-usage) together into a single script type".
If we will separate `SIGHASH_NOINPUT`-enabled outputs, we should not implement Taproot, as the existing separation of P2WSH and P2WPKH is congruent to the proposed separation of `SIGHASH_NOINPUT`-enablement.
I strongly agree that `NOINPUT` is useful, and I was not able to attend CoreDev (at least, not with any human fleshbot already known to you --- I checked).
No opposition, we will just work around this by publishing a common known private key to use for all chaperone signatures, since all the important security is in the `NOINPUT` signature anyway.
Strongly oppose, see above about my argument.
Ambivalent, mildly support.
Cats are very interesting creatures, and are irrelevant to `SIGHASH_NOINPUT` discussion, but are extremely cute nonetheless.

@_date: 2019-09-30 23:28:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Continuing the discussion about noinput / 
============================== START ==============================
Good morning list,
To elucidate further ---
Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.
This new opcode ignores any `SIGHASH` flags, if present, on a signature, but instead hashes the current transaction without the input references, then checks that hash to the signature.
This is equivalent to `SIGHASH_NOINPUT`.
Yet as an opcode, it would be possible to embed in a Taproot script.
For example, a Decker-Russell-Osuntokun would have an internal Taproot point be a 2-of-2, then have a script `OP_1 OP_CHECKSIG_WITHOUT_INPUT`.
Unilateral closes would expose the hidden script, but cooperative closes would use the 2-of-2 directly.
Of note, is that any special SCRIPT would already be supportable by Taproot.
This includes SCRIPTs that may potentially lose funds for the user.
Yet such SCRIPTs are already targetable by a Taproot address.
If we are so concerned about `SIGHASH_NOINPUT` abuse, why are we not so concerned about Taproot abuse?

@_date: 2020-04-04 12:07:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Nadav,
It seems to me that practical deployments of statechains requires the statechain operator to be a trusted federation, possibly a k-of-n.
This is slightly better than a federated sidechain because the money can always be reclaimed on the blockchain layer very quickly in case of a loss of trust in the federation.
If the k-of-n is arranged in such a way that the signers can be identified (such as by use of old `OP_CHECKMULTISIG` or some combination of the proposed `OP_CHECKSIGADD`) then it has the same "auditability", i.e. you can identify the pseudonyms of the members who cheated (which is not worth much, as getting a new pseudonym is trivial).
It is helpful to remember that a k-of-n federation can only be trusted if you have full trust in at least (n - k + 1) members of the federation.

@_date: 2020-04-05 18:24:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Bob,
The point is not that acquiescing with two keys is possible.
Instead, the point is that any past owner of the coin can collude with the statechain authority (who, in the new scheme, must be trusted to delete old keys), or anyone who manages to get backups of the statechain authority keys (such as by digging for backups in a landfill), in order to steal the onchain funds, regardless of who the current owner is, within the statechain.
Thus an amount of trust must still be put in the statechain authority.
So I think the security assumptions should be that:
* The statechain authority really does delete keys and does not make backups.
* No *past* or *current* owner of the coin colludes with the statechain authority.
  * I think saying merely "sender" is not sufficient to capture the actual security assumption here.

@_date: 2020-04-05 18:24:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Bob,
The point is not that acquiescing with two keys is possible.
Instead, the point is that any past owner of the coin can collude with the statechain authority (who, in the new scheme, must be trusted to delete old keys), or anyone who manages to get backups of the statechain authority keys (such as by digging for backups in a landfill), in order to steal the onchain funds, regardless of who the current owner is, within the statechain.
Thus an amount of trust must still be put in the statechain authority.
So I think the security assumptions should be that:
* The statechain authority really does delete keys and does not make backups.
* No *past* or *current* owner of the coin colludes with the statechain authority.
  * I think saying merely "sender" is not sufficient to capture the actual security assumption here.

@_date: 2020-04-22 04:12:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning Matt, and list,
My (cached) understanding is that, since RBF is signalled using `nSequence`, any `OP_CHECKSEQUENCEVERIFY` also automatically imposes the requirement "must be RBF-enabled", including `<0> OP_CHECKSEQUENCEVERIFY`.
Adding that clause (2 bytes in witness if my math is correct) to the hashlock branch may be sufficient to prevent C from making an RBF-disabled transaction.
But then you mention out-of-band agreements with miners, which basically means the transaction might not be in the mempool at all, in which case the vulnerability is not really about RBF or relay, but sheer economics.
The payment is A->B->C, and the HTLC A->B must have a larger timeout (L + 1) than the HTLC B->C (L), in abstract non-block units.
The vulnerability you are describing means that the current time must now be L + 1 or greater ("A could claim the HTLC from B via the timeout mechanism", meaning the A->B HTLC has timed out already).
If so, then the B->C transaction has already timed out in the past and can be claimed in two ways, either via B timeout branch or C hashlock branch.
This sets up a game where B and C bid to miners to get their version of reality committed onchain.
(We can neglect out-of-band agreements here; miners have the incentive to publicly leak such agreements so that other potential bidders can offer even higher fees for their versions of that transaction.)
Before L+1, C has no incentive to bid, since placing any bid at all will leak the preimage, which B can then turn around and use to spend from A, and A and C cannot steal from B.
Thus, B should ensure that *before* L+1, the HTLC-Timeout has been committed onchain, which outright prevents this bidding war from even starting.
The issue then is that B is using a pre-signed HTLC-timeout, which is needed since it is its commitment tx that was broadcast.
This prevents B from RBF-ing the HTLC-Timeout transaction.
So what is needed is to allow B to add fees to HTLC-Timeout:
* We can add an RBF carve-out output to HTLC-Timeout, at the cost of more blockspace.
* With `SIGHASH_NOINPUT` we can make the C-side signature `SIGHASH_NOINPUT|SIGHASH_SINGLE` and allow B to re-sign the B-side signature for a higher-fee version of HTLC-Timeout (assuming my cached understanding of `SIGHASH_NOINPUT` still holds).
With this, B can exponentially increase the fee as L+1 approaches.
If B can get HTLC-Timeout confirmed before L+1, then C cannot steal the HTLC value at all, since the UTXO it could steal from has already been spent.
In particular, it does not seem to me that it is necessary to change the hashlock-branch transaction of C at all, since this mechanism is enough to sidestep the issue (as I understand it).
But it does point to a need to make HTLC-Timeout (and possibly symmetrically, HTLC-Success) also fee-bumpable.
Note as well that this does not require a mempool: B can run in `blocksonly` mode and as each block comes in from L to L+1, if HTLC-Timeout is not confirmed, feebump HTLC-Timeout.
In particular, HTLC-Timeout comes into play only if B broadcast its own commitment transaction, and B *should* be aware that it did so --- there is still no need for mempool monitoring here.
Now, of course this only delays the war.
Let us now consider what C can do to ensure that the bidding war will happen eventually.
* C can bribe a miner to prevent HTLC-Timeout from confirming between L and L+1.
  * Or in other words, this is a censorship attack.
    * The Bitcoin censorship-resistance model is that censored transactions can be fee-bumped, which attracts non-censoring miners to try their luck at mining and evict the censoring miner.
      * Thus, letting B bump the fee on HTLC-Timeout is precisely the mechanism we need.
      * This sets up a bidding war between C requesting miners to censor, vs. B requesting miners to confirm, but that only sets the stage for a second bidding war later between C and B, thus C is at a disadvantage: it has to bribe miners to censor continuously from L to L+1 *and* additional bribe miners to confirm its transaction after L+1, whereas B can offer its bribe as being something that miners can claim now without waiting after L+1.
The issue of course is the additional output that bloats the UTXO set and requires another transaction to claim later.
And if we have `SIGHASH_NOINPUT`, it seems to me that Decker-Russell-Osuntokun sidesteps this issue as well, as any timed-out HTLC can be claimed with a fee-bumpable transaction directly without RBF-carve-out.
(As well, it seems to me that, if both nodes support doing so, a Poon-Dryja channel can be upgraded, without onchain activity, to a Decker-Russell-Osuntokun channel: sign a transaction spending the funding tx to a txo that has been set up as Decker-Russell-Osuntokun, do not broadcast that transaction, then revoke the latest Poon-Dryja commitment transactions, then switch the mechanism over to Decker-Russell-Osuntokun; you still need to monitor for previous Poon-Dryja commitment transactions, but HTLCs now sidestep the issue under discussion here.)

@_date: 2020-04-22 06:08:06
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning Laolu, Matt, and list,
Right, right, that works as well.
So basically:
* B has no mempool, because it wants to reduce its costs and etc.
* C broadcasts a non-RBF claim tx with low fee before A->B locktime (L+1).
* B does not notice this tx because:
  1.  The tx is too low fee to be put in a block.
  2.  B has no mempool so it cannot see the tx being propagated over the P2P network.
* B tries to broadcast higher-fee HTLC-timeout, but fails because it cannot replace a non-RBF tx.
* After L+1, C contacts the miners off-band and offers fee payment by other means.
It seems to me that, if my cached understanding that `<0> OP_CHECKSEQUENCEVERIFY` is sufficient to require RBF-flagging, then adding that to the hashlock branch (2 witness bytes, 0.5 weight) would be a pretty low-weight mitigation against this attack.
So I think the combination below gives us good size:
* The HTLC-Timeout signature from C is flagged with `OP_SINGLE|OP_ANYONECANPAY`.
  * Normally, the HTLC-Timeout still deducts the fee from the value of the UTXO being spent.
  * However, if B notices that the L+1 timeout is approaching, it can fee-bump HTLC-Timeout with some onchain funds, recreating its own signature but reusing the (still valid) C signature.
* The hashlock branch in this case includes `<0> OP_CHECKSEQUENCEVERIFY`, preventing C from broadcasting a low-fee claim tx.
This has the advantages:
* B does not need a mempool still and can run in `blocksonly`.
* The normal path is still the same as current behavior, we "only" add a new path where if the L+1 timeout is approaching we fee-bump the HTLC-Timeout.
* Costs are pretty low:
  * No need for extra RBF carve-out txo.
  * Just two additional witness bytes in the hashlock branch.
* No mempool rule changes needed, can be done with the P2P network of today.
  * Probably still resilient even with future changes in mempool rules, as long as typical RBF behaviors still remain.
Is my understanding correct?

@_date: 2020-04-23 04:50:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning lists et al,
Let me try to summarize things a little:
* Suppose we have a forwarding payment A->B->C.
* Suppose B does not want to maintain a mempool and is running in `blocksonly` mode to reduce operational costs.
* C triggers B somehow dropping the B<->C channel, such as by sending an `error` message, which will usually cause the other side to drop the channel onchain using its commitment transaction.
* The dropped B<->C channel has an HTLC (that was set up during the A->B->C forwarding).
* The HTLC, being used in a Poon-Dryja channel, actually has the following contract text:
  * The fund may be claimed by either of these clauses:
    * C can claim, if C shows the preimage of some hash H (hashlock branch).
    * B and C must agree, and claim after time L (timelock branch).
* B holds a signature from C that can claim the timelock branch of the HTLC, for a transaction that spends to an output with an `OP_CHECKSEQUENCEVERIFY`.
  * The signature is `SIGHASH_ALL`, so the transaction has a fixed feerate.
* C can "pin" the HTLC output by spending using the hashlock branch, and creating a large fee, low fee-rate (tree of) transactions.
  * As it is a low fee-rate, miners have no incentive to put this in a block, especially if unrelated higher-fee-rate transactions exist that would earn them more money.
  * Even in a full RBF universe, because of the anti-DoS mempool rules, B cannot evict this pinned transaction by just bidding up the feerate.
    * A replacing transaction cannot evict alternatives unless its absolute fee is greater than the absolute fee of the alternative.
    * The pinning transaction has a high fee, but is blockspace-wasteful, so it is:
      * Undesirable to mine (low feerate).
      * Difficult to evict (high fee).
* Thus, B is unable to get its timelock-branch transaction in the mempools of miners.
* C waits until the A->B HTLC times out, then:
  * C directly contacts miners with an out-of-band proposal to replace its transaction with an alternative that is much smaller and has a low fee, but much better feerate.
  * Miners, being economically rational, accept this proposal and include this in a block.
The proposal by Matt is then:
* The hashlock branch should instead be:
  * B and C must agree, and show the preimage of some hash H (hashlock branch).
* Then B and C agree that B provides a signature spending the hashlock branch, to a transaction with the outputs:
  * Normal payment to C.
  * Hook output to B, which B can use to CPFP this transaction.
  * Hook output to C, which C can use to CPFP this transaction.
* B can still (somehow) not maintain a mempool, by:
  * B broadcasts its timelock transaction.
  * B tries to CPFP the above hashlock transaction.
    * If CPFP succeeds, it means the above hashlock transaction exists and B queries the peer for this transaction, extracting the preimage and claiming the A->B HTLC.
Is that a fair summary?
Naively, and remembering I am completely ignorant of the exact details of the mempool rules, it seems to me quite strange that we are allowing an undesirable transaction (tree) into the mempool:
* Undesirable to mine (low fee-rate).
* Difficult to evict (high fee).
Miners are not interested in low fee-rate transactions, as long as higher fee-rate transactions exist.
And being difficult to evict means miners cannot get alternatives that are more lucrative for them.
The reason (as I understand it) eviction is purposely made difficult here is to prevent certain DoS attacks on Bitcoin nodes, specifically:
1. Attacker sends a low fee-rate tx as a "root" transaction.
2  Attacker sends thousands of low fee-rate tx that build off the above root.
3. Attacker sends a slightly higher fee-rate alternative to the root, evicting the above tree of txes.
4. Attacker sends thousands of low fee-rate tx that build off the latest root.
5. GOTO 3.
However, it seems to me, naively, that "an ounce of prevention is worth a pound of cure".
As I understand it, the mempool is organized already into "packages" of transactions, and adding a transaction into the mempool involves extending and merging packages.
Perhaps the size of a package with low fee-rate (relative to the other packages in the mempool) can be limited, so that mempools drop incoming txes that extend a low-fee-rate tree of transactions.
This means an attacker cannot send thousands of low fee-rate tx that build off some low fee-rate root tx in the first place, so it can still be evicted easily later without much impact.
Naively, it seems to me to prevent the DoS attack as well, as at step 2 it would be prevented from sending thousands of low fee-rate tx building off the root.
As well, as I understand it, this merely tightens the mempool acceptance rules, preventing low fee-rate packages from growing (analogous to a consensus-layer softfork).
The "cannot evict high absolute fee" rule can be retained, as the low-fee-rate package is prevented from reaching a large size.
Would that be workable as a general solution to solve (what I think is) the root cause of this problem?
(This assumes full RBF, I suppose.)

@_date: 2020-04-23 04:50:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning lists et al,
Let me try to summarize things a little:
* Suppose we have a forwarding payment A->B->C.
* Suppose B does not want to maintain a mempool and is running in `blocksonly` mode to reduce operational costs.
* C triggers B somehow dropping the B<->C channel, such as by sending an `error` message, which will usually cause the other side to drop the channel onchain using its commitment transaction.
* The dropped B<->C channel has an HTLC (that was set up during the A->B->C forwarding).
* The HTLC, being used in a Poon-Dryja channel, actually has the following contract text:
  * The fund may be claimed by either of these clauses:
    * C can claim, if C shows the preimage of some hash H (hashlock branch).
    * B and C must agree, and claim after time L (timelock branch).
* B holds a signature from C that can claim the timelock branch of the HTLC, for a transaction that spends to an output with an `OP_CHECKSEQUENCEVERIFY`.
  * The signature is `SIGHASH_ALL`, so the transaction has a fixed feerate.
* C can "pin" the HTLC output by spending using the hashlock branch, and creating a large fee, low fee-rate (tree of) transactions.
  * As it is a low fee-rate, miners have no incentive to put this in a block, especially if unrelated higher-fee-rate transactions exist that would earn them more money.
  * Even in a full RBF universe, because of the anti-DoS mempool rules, B cannot evict this pinned transaction by just bidding up the feerate.
    * A replacing transaction cannot evict alternatives unless its absolute fee is greater than the absolute fee of the alternative.
    * The pinning transaction has a high fee, but is blockspace-wasteful, so it is:
      * Undesirable to mine (low feerate).
      * Difficult to evict (high fee).
* Thus, B is unable to get its timelock-branch transaction in the mempools of miners.
* C waits until the A->B HTLC times out, then:
  * C directly contacts miners with an out-of-band proposal to replace its transaction with an alternative that is much smaller and has a low fee, but much better feerate.
  * Miners, being economically rational, accept this proposal and include this in a block.
The proposal by Matt is then:
* The hashlock branch should instead be:
  * B and C must agree, and show the preimage of some hash H (hashlock branch).
* Then B and C agree that B provides a signature spending the hashlock branch, to a transaction with the outputs:
  * Normal payment to C.
  * Hook output to B, which B can use to CPFP this transaction.
  * Hook output to C, which C can use to CPFP this transaction.
* B can still (somehow) not maintain a mempool, by:
  * B broadcasts its timelock transaction.
  * B tries to CPFP the above hashlock transaction.
    * If CPFP succeeds, it means the above hashlock transaction exists and B queries the peer for this transaction, extracting the preimage and claiming the A->B HTLC.
Is that a fair summary?
Naively, and remembering I am completely ignorant of the exact details of the mempool rules, it seems to me quite strange that we are allowing an undesirable transaction (tree) into the mempool:
* Undesirable to mine (low fee-rate).
* Difficult to evict (high fee).
Miners are not interested in low fee-rate transactions, as long as higher fee-rate transactions exist.
And being difficult to evict means miners cannot get alternatives that are more lucrative for them.
The reason (as I understand it) eviction is purposely made difficult here is to prevent certain DoS attacks on Bitcoin nodes, specifically:
1. Attacker sends a low fee-rate tx as a "root" transaction.
2  Attacker sends thousands of low fee-rate tx that build off the above root.
3. Attacker sends a slightly higher fee-rate alternative to the root, evicting the above tree of txes.
4. Attacker sends thousands of low fee-rate tx that build off the latest root.
5. GOTO 3.
However, it seems to me, naively, that "an ounce of prevention is worth a pound of cure".
As I understand it, the mempool is organized already into "packages" of transactions, and adding a transaction into the mempool involves extending and merging packages.
Perhaps the size of a package with low fee-rate (relative to the other packages in the mempool) can be limited, so that mempools drop incoming txes that extend a low-fee-rate tree of transactions.
This means an attacker cannot send thousands of low fee-rate tx that build off some low fee-rate root tx in the first place, so it can still be evicted easily later without much impact.
Naively, it seems to me to prevent the DoS attack as well, as at step 2 it would be prevented from sending thousands of low fee-rate tx building off the root.
As well, as I understand it, this merely tightens the mempool acceptance rules, preventing low fee-rate packages from growing (analogous to a consensus-layer softfork).
The "cannot evict high absolute fee" rule can be retained, as the low-fee-rate package is prevented from reaching a large size.
Would that be workable as a general solution to solve (what I think is) the root cause of this problem?
(This assumes full RBF, I suppose.)

@_date: 2020-04-23 12:46:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning Matt,
That introduces the possibility that the entire tree (with high total fee, remember) gets confirmed, so it would be better for C to replace it with an alternative to a different address C still controls, with a slightly better fee rate but smaller (no child transactions) and lower total fee, so an economically-rational C will make that effort (and if there are still other transactions in the mempool, an economically-rational miner will accept this proposal).
But in any case this is a minor detail and the attack will work either way.
Ah, right, so it gets confirmed and the `blocksonly` B sees it in a block.
Even if C hooks a tree of low-fee transactions on its hook output or normal payment, miners will still be willing to confirm this and the B hook CPFP transaction without, right?

@_date: 2020-04-23 12:52:57
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning David,
Unfortunately this technique does not look like it is compatible to payment points rather than hashes, and we would really like to upgrade to payment points sooner rather than later.
Nobody but B can recognize the signature as revealing the scalar behind a particular point (the main privacy advantage of using points).
Even variations on this are not useable with payment points.

@_date: 2020-04-23 17:56:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning Germ?n,
It looks to me like this is CoinSwap with Schnorr Scriptless Scripts.
* * I also recently put up an article on extending such a protocol across 3 or more participants:
*

@_date: 2020-04-24 01:34:51
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning Germ?n,
There are a number of issues to simply modeling this to the subset-sum problem.
* There is a practical limit to the number of UTXOs you would be willing to receive in the swap.
  * Every UTXO you receive increases the potential fee you have to pay to spend them, meaning you would strongly dislike receiving 100 UTXOs that sum up to 1mBTC.
  * Thus, a practical blockchain analyst can bound the size of the sets involved, and the problem becomes less than NP in practice.
* If you have a single UTXO and split it, then swap, anyone looking at the history can conjecture that the split involved is part of a CoinSwap.
  * The split is now a hint on how the subset sums can be tried.
* If after the CoinSwap you spend the UTXOs you received in a single transaction, then you just published the solution to the subset sum for your adversary.
  * This ties in even further to the "practical limit on the number of UTXOs".
    * Because it is not safe to spend the UTXOs from a single CoinSwap together, you want to have fewer, larger UTXOs for more flexibility in spending later.
I believe belcher and waxwing and nopara73 have been working far longer on privacy tech, and you should try to get in contact with them as well, they may know of other issues (or solutions to the above problems).

@_date: 2020-04-29 07:56:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning CB,
I have been thinking about CoinSwap for a good while as well.
Here are some very unorganized thoughts.
It wold be nice to interoperate with JoinMarket, i.e. have a JoinMarket maker that also provides CoinSwap services using the same UTXOs.
However, this requires us to retain compatibility with the JoinMarket wallet structure, which is divided into mixdepths, with the rule that UTXOs in different mixdepths cannot be spent together in the same onchain UTXO (to move across mixdepths you have to do a send, and sending out is always done by a single CoinJoin round with multiple makers).
I am uncertain what is the best way to handle multitransaction when considering the mixdepth system.
My instinct is that if you are doing multitransaction (whether as taker or maker) then each transaction in the swap *has to* come from a different mixdepth.
The issue here is:
* If all the UTXOs in the multitransaction swap come from the same mixdepth, then a surveillor who is monitoring that mixdepth gets a good hint in solving the sparse subset sum problem.
* On the other hand, if all the UTXOs in the multitransaction swap come from different mixdepths, then a surveillor who has solved the sparse subset sum problem now has the hint that the different mixdepths are really owned by the same JoinMarket user.
I am uncertain which tradeoff is better here, though I am inclined to think the latter is better.
Attempting to completely detach a market-for-CoinSwap from JoinMarket seems to be impossible to my mind: the protocols are known, implementations open, and someone will inevitably write code for a single piece of software that can operate as both a JoinMarket maker *and* a maker for a market-for-CoinSwap (to increase their market, so to speak), so it might be better to just add CoinSwap to JoinMarket in the first place.
Assuming Alice is the taker, and Bob is the maker, then Alice might want a specific coin value (or set of such) that Bob does not have.
In that case, Bob will have to split a UTXO it owns.
We could constrain it so that Bob at least is not allowed to use the change from splitting for the same CoinSwap, e.g. if Bob has only 9 BTC and 1 BTC coins and Alice wants a 6 BTC / 3 BTC / 1 BTC split, then Bob cannot split its own 9 BTC coin then swap.
Or in terms of mixdepths, Bob can split within a mixdepth but each outgoing UTXO in the same swap should be from different mixdepths.
This is a good idea, akin to the rule in JoinMarket that all outgoing spends are done through a CoinJoin.
Of course, if a surveillor ***does*** solve the sparse subset sum, then the CoinSwap Protocol part looks exactly like a Bitcoin transaction, with a "main" paying output and a "change" output, and the same techniques that work with current Bitcoin txes work with "CoinSwap Protocol" virtual transactions.
It seems to me that, in a system of makers and takers, even if the maker is really just paying the taker(s) to do CoinSwaps to mix back to itself, it should still "require" some output amount that really goes to itself, so that the maker at least does not differentiate between the case that the taker is paying to itself vs the case that the taker is paying someone else via a CoinSwap.
That is, the protocol should still require that the taker specify *some* target desired amount, regardless of whether the taker wants to pay a specific value, or the taker wants to just mix its coins.
What are your thoughts on creating such possible situations?
An idea is to require standard swap amounts, i.e. similar to the standard 100mBTC mixing bin of Wasabi.
As well, one could randomly select some existing 1-input 1-output txes in the mempool and/or recent blocks, sum them, and swap for the same sum, to force at least one false positive, but the surveillor could protect against this by removing the earliest match (the one it saw in the mempool first, or onchain).

@_date: 2020-04-30 08:54:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning CB,
Would the ZeroLink proposal of separating a receiving (pre-mix) wallet from a sending (post-mix) wallet apply, thus having two implicit mixdepths (the receiving mixdepth and the sending mixdepth)?
Or would imposing the rule "all sends must be via CoinSwap" be sufficient (and follow the ZeroLink rule in spirit)?
This "as long as the inputs that should be separate are not co-spent" is precisely what mixdepths protect against, which is why I think *some* kind of mixdepth facility will still matter in CoinSwap.
Still, you have convinced me that, for the purpose of multi-transaction CoinSwap where you do not merge any of your coins, it is immaterial if the sub-transactions come from the same mixdepth or not.
And if you have to merge your coins (for instance, if you are a maker and your customer wants to get a UTXO that is larger than any you have on hand, you have to merge your coins), you just have to ensure they are in the same mixdepth.
Of course, you *could* be proposing some other construct --- perhaps you have some relational entry which says "you cannot merge coin A and coin B" which allows you to merge A C D or B C E, but not A B?
(I imagine this would make coin selection even harder, but I am not a mathematician and there may be some trivial solution to this.)
Now --- if you have two coins that cannot be merged in the same onchain tx, what happens when you swap them in a multi-tx CoinSwap with somebody else?
That somebody else does not know that information.
Instead, that somebody else must always assume that any coins it got from the same CoinSwap operation must not be safely mergeable (though they can still be used in the same swap together).
Coins received via receive addresses would also not be mergeable with any other coins, except coins to the same address (because coins in the same address already leak that they are owned by the same owner).
An unscrupulous possible maker might not value the privacy of its customers (indeed makers are a privacy attack vector, which requires something like fidelity bonds like you suggested before to protect against), and takers might not want to do possibly-computationally-expensive blockchain analysis to evaluate whether a particular maker values privacy.
An unscrupulous maker might thus earn more than a more scrupulous maker can, at least during the transition from JoinMarket to SwapMarket, and get a greater share of the future SwapMarket available liquidity due to their increased earnings during the transition.
Against this we should remember that software that does two things is four times as complicated as software that does one thing, so hopefully your projected transition from JoinMarket to SwapMarket will be fast enough that such a combined Join/SwapMarket maker software does not arise fast enough to matter.
This leaks to Bob whether Alice is making a payment or not; it would be better for the privacy of Alice for Alice to *always* mention *some* "payment amount", even if this is not actually a payment and Alice is just mixing for herself prior to storing in cold storage.
And if Alice wants to use a single swap to pay to multiple targets at once, that implies Alice has to have the ability to indicate the outputs it wants to Bob, and it would imply as well that Alice has to obfuscate which of those outputs have amounts that actually *matter* (by always insisting on what the output amounts must be, rather than insisting on N output amounts and letting Bob handle the rest).
(We *could* constrain it such that Alice can make only one payment per CoinSwap, so that Alice only gives one "target" amount and one "total" amount, but that implies even bigger blockspace utilization, sigh.)
Otherwise, Bob can get information:
* "Oh, Alice did not specify any of the outputs, just the total amount, all of my old coins are owned by Alice now."
* "Oh, Alice specified an exact value for one of the outputs, that one is no longer owned by Alice but the rest are owned by Alice."
* "Oh, Alice specified exact values for two of the outputs, those two are definitely no longer owned by Alice but the rest are owned by Alice."
The conclusion here is either Alice never specifies any of the outputs --- in which case Alice cannot use a CoinSwap to pay directly to somebody else --- or Alice specifies all of them.
Again, the maker might be an active surveillor, thus we should reduce information leaks to the maker as much as we can.
Okay, from what little I understand it seems that "even if sparse subset sum is easier than subset sum, it is still hard, so it probably will not matter in practice", would that be a fair takeaway?
I agree.

@_date: 2020-08-02 00:36:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smaller Transactions with PubRef 
Good morning Mike,
Hard NAK.
The responses to the original posting already pointed out important problems with this:
* Encourages address reuse, hurting fungibility and privacy.
* Prevents pruning, since access to previous blocks must always be available in order to validate.
* Optimized implementation requires creating yet another index to previous block data, increasing requirements on fullnodes.
* Requires SCRIPT to be re-evaluated on transactions arriving in  newblocks, to protect against reorgs of the chaintip, and in particular `OP_PUBREF` references to near the chaintip.
None of these issues have been addressed in your current proposal.
The proposal looks at clients only, without considering what validators have to implement in order to validate new blocks with this opcode.

@_date: 2020-08-02 14:22:30
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Smaller Transactions with PubRef 
Good morning Mike,
The issue with SCRIPT re-evaluation is that reorgs cause more processing to be done by nodes.
Floating-point Nakamoto Consensus does not help here, since a node can receive the lower-scored block first, and *then* a higher-scored block, and thus will ***still*** observe a reorg since the chain tip is replaced with a higher-scored block later.
This still increases the processing load on validating fullnodes, and prevents any kind of pruning from working for validating fullnodes.
A miner can also still mount a DoS on validating fullnodes, with `OP_PUBREF` and Floating-Point Nakamoto Consensus by re-mining the same block, and broadcasting a block if it has higher score than the previous chain tip.
This locks the blockchain ***and*** increases the load on fullnodes, which have to re-validate uses of `OP_PUBREF` that might refer to the chain tip.

@_date: 2020-08-04 01:38:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Good morning Richard,
It can (and should) use key-path, yes.
I believe we can amortize this slightly by providing the `R` commitments for the *next* signing session with the `s` for the *current* signing session, reducing to 1.0 round trips.
However, I believe a provably-safe 2-round MuSig (with composable MuSig even!) is being worked on and should be released in a week or two, and if it is safe to provide the first round of the *next* session with the final round of the *current* session then we could reduce it to just one (large) message send per update.
My understanding (which might be incorrect!) is that it should be safe to perform the signing sessions for the settlement and update txes simultaneously, i.e.
* round 1: send `R` commitments for both update and settlement tx (can be sent with round 3 of previous signing session).
* round 2: send `R` for both update and settlement tx.
* round 3: send `s` for both update and settlement tx.
Depending on how we do the HTLCs / PTLCs, we might also need to send signatures for all HTLCs, in parallel with the update+settlement tx signatures, as well.
This might have been the "chaperone signatures" proposed for `SIGHASH_NOINPUT` / `SIGHASH_ANYPREVOUT` back then.
This was supposed to protect against replaying a `SIGHASH_ANYPREVOUT` signature in case of address reuse.
I pointed out that it would be much simpler for a Lightning spec to provide a privkey for a common `X` used by all Lightning nodes, and thus would not really provide much better security in practice.
I believe what we intend now is a form of hidden output tagging to protect against signature replay.
An output has to have a special taproot version in order to be spent with `SIGHASH_ANYPREVOUT` or `SIGHASH_ANYPREVOUTANYSCRIPT` in the script path, and `SIGHASH_ANYPREVOUT`/`SIGHASH_ANYPREVOUTANYSCRIPT` is not usable with key path spends.

@_date: 2020-08-04 04:23:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Good morning Matt,
Ah, right.
A feasible attack, without the above, would be to connect to the fullnode of the victim, and connect to miners separately.
Then you broadcast to the victim one of the old txes, call it tx A, but you broadcast to the miners a *different* old tx, call it B.
The victim reacts only to tA, but does not react to B since it does not see B in the mempool.
On the other hand --- what the victim needs to react to is *onchain* confirmed transactions.
So I think all the victim needs to do, in a Lightning universe utilizing primarily `SIGHASH_NOINPUT`-based mechanisms, is to monitor onchain events and ignore mempool events.
So if we give fairly long timeouts for our mechanisms, it should be enough, I think, since once a transaction is confirmed its txid does not malleate without a reorg and a `SIGHASH_NOINPUT` signature can then be "locked" to that txid, unless a reorg unconfirms the transaction.
We only need to be aware of deep reorgs and re-broadcast with a malleated prevout until the tx being spent is deeply confirmed.
In addition, we want to implement scorch-the-earth, keep-bumping-the-fee strategies anyway, so we would keep rebroadcasting new versions of the spending transaction, and spending from a transaction that is confirmed.
Or are there other attack vectors you can see that I do not?
I think this is fixed by looking at the blockchain.

@_date: 2020-08-04 14:59:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Good morning Matt,
Ah, okay.
So the attack is this:
* Attacker connects twice to the LN: one to any node near the victim, one to the victim.
* Attacker arranges for the attacker-victim channel to have most funds in the side of the victim.
* The attacker routes a circular payment terminating in the victim-attacker channel.
  * The victim accepts some incoming HTLC, and provides an outgoing HTLC to the attacker via the victim-attacker channel.
* The attacker broadcasts a very low-fee old-state transaction of the victim-attacker channel, one that is too low-fee to practically get confirmed, just before the HTLC timeout.
* The victim-outgoing HTLC times out, making the victim broadcast a unilateral close attempt for the victim-attacker channel in order to enforce the HTLC onchain.
  * Unfortunately for the victim, relay shenanigans prevent the latest commitment from being broadcast.
* The attacker waits for the victim-incoming HTLC to timeout, which forces the victim to `update_htlc_failed` the incoming HTLC or risk having that channel closed (and losing future routing fees).
  * The attacker now gets back its outgoing funds.
* The attacker lets the old-state transaction get relayed, and then re-seats the latest update transaction to that.
* Once the latest transaction allows the HTLCs to be published, the attacker claims the victim-outgoing HTLC with the hashlock branch.
  * The attacker now gets its incoming funds, doubling its money, because that is how the "send me 1 BTC I send you 2 BTC back" Twitter thing works right?
The only thing I can imagine helping here is for the forwarding node to drop channels onchain "early", i.e. if the HTLC will time out in say 14 blocks we drop the channel onchain, so we have a little leeway in bumping up fees for the commitment transaction.
I am sure Matt can find yet another relay attack that prevents that, at this point, haha.
"Are we *still* talking about onchain fees....?" - Adelaide 2018

@_date: 2020-08-17 05:04:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] reviving op_difficulty 
Good morning Tier, Thomas, and aj,
Taproot MAST to the rescue.
* Alice and Bob agree on the number of ticks N and payout schedule.
* Alice and Bob generate N fresh keypairs and share them.
* Alice and Bob generate tapleaf scripts of the form:
  * script[i] = Alice[i] && Bob[i] && diff < 1.00 trillion + i * tick_size && CLTV(deadline)
* Alice and Bob generate the taproot MAST for the above scripts.
* Alice and Bob generate, but do ***NOT*** sign, a funding transaction paying out to the generated taproot MAST.
* Bob generates partial signatures for N payout transactions, with lower-difficulty-targets paying out less to Alice and more to Bob, and higher-difficulty-targets paying out more to Alice and less to Bob.
  * This requires spending the [i]th tapleaf script with the appropriate difficulty target.
* Alice saves all the Bob signatures.
* At deadline, Alice rationally selects the highest-paying version that is still acceptable, based on the actual difficulty target at the time.
This requires publishing only O(log N) data (the merkle path to the selected tapleaf).
This translates to the 100-tick example requiring only one TXO, 1 scripthash, and 7 or so Merkle-tree-path hashes, compared to the above example which requires 100 TXOs and 100 script hashes.
The same scheme can be used with `OP_CTV` and without keypairs being involved, but basically anything `OP_CTV` can do, signing keypairs with pre-generated signatures from all participants can do just as well, with higher storage and setup costs.

@_date: 2020-08-17 23:14:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] reviving op_difficulty 
Good morning Thomas,
Bitcoin transactions on the blockchain layer are atomic, so it would be far simpler to make the purchase output and the options output in the same transaction, in a sort of PayJoin-like cooperatively-signed transaction.
That said, the construction you are imagining is indeed going to work.
The only requirement is that the hash-branch needs two signatures in order to ensure that it pays out to a transaction with a very specific contract.
Xref.  how Lightning *really* creates its HTLCs.

@_date: 2020-08-20 11:17:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris,
Great to see this!
Mostly minor comments.
This has a massive advantage over CoinJoin.
In CoinJoin, since all participants sign a single transaction, every participant knows the total number of participants.
Thus, in CoinJoin, it is fairly useless to have just one taker and one maker, the maker knows exactly which output belongs to the taker.
Even if all communications were done via the single paying taker, the maker(s) are shown the final transaction and thus can easily know how many participants there are (by counting the number of equal-valued outputs).
With CoinSwap, in principle no maker has to know how many other makers are in the swap.
Thus it would still be useful to make a single-maker CoinSwap, as that would be difficult, for the maker, to diferentiate from a multi-maker CoinSwap.
There are still a few potential leaks though:
* If paying through a CoinSwap, the cheapest option for the taker would be to send out a single large UTXO (single-output txes) to the first maker, and then demand the final payment and any change as two separate swaps from the final maker.
  * Intermediate makers are likely to not have exact amounts, thus is unlikely to create a single-output tx when forwarding.
  * Thus, the first maker could identify the taker.
* The makers can try timing the communications lag with the taker.
  The general assumption would be that more makers == more delay in taker responses.
Why not have the taker pay for the *first* maker-spent UTXO and have additional maker-spent UTXOs paid for by the maker?
i.e. the taker indicates "swap me 1 BTC in 3 bags of 0.3, 0.3, and 0.4 BTC", and pays for one UTXO spent for each "bag" (thus pays for 3 UTXOs).
Disagreements on feerate can be resolved by having the taker set the feerate, i.e. "the customer is always right".
Thus if the maker *has to* spend two UTXOs to make up the 0.4 BTC bag, it pays for the mining fees for that extra UTXO.
The maker can always reject the swap attempt if it *has to* spend multiple UTXOs and would lose money doing so if the taker demands a too-high feerate.
The highest-fee version could have, in addition, CPFP-anchor outputs, like those being proposed in Lightning, so even if onchain fees rise above the largest fee reservation, it is possible to add even more fees.
Or not.
Another thought: later you describe that miner fees are paid by Alice by forwarding those fees as well, how does that work when there are multiple versions of the contract transaction?
The rationale for relative timelocks is that it makes private key turnover slightly more useable by ensuring that, after private key turnover, it is possible to wait indefinitely to spend the UTXO it received.
This is in contrast with absolute timelocks, where after private key turnover, it is required to spend received UTXO before the absolute timeout.
The dangers are:
* Until it receives the private key, if either of the incoming or outgoing contract transactions are confirmed, every swap participant (taker or maker) should also broadcast the other contract transaction, and resolve by onchain transactions (with loss of privacy).
* After receiving the private key, if the incoming contract transaction is confirmed, it should spend the resulting contract output.
* It is possible to steal from a participant if that participant goes offline longer than the timeout.
  This may imply that there may have to be some minimum timeout that makers indicate in their advertisements.
  * The taker can detect if the first maker is offline, then if it is offline, try a contract transaction broadcast, if it confirms, the taker can wait for the timeout; if it times out, the taker can clawback the transaction.
    * This appears to be riskless for the taker.
    * Against a similar attack, Lightning requires channel reserves, which means the first hop never gains control of the entire value, which is a basic requirement for private key turnover.
  * On the other hand, the taker has the largest timeout before it can clawback the funds, so it would wait for a long time, and at any time in between the first maker can come online and spend using the hashlock branch.
    * But the taker can just try on the hope it works; it has nothing to lose.
  * This attack seems to be possible only for the taker to mount.
    Other makers on the route cannot know who the other makers are, without cooperation of the taker, who is the only one who knows all the makers.
    * On the other hand, the last maker in the route has an outgoing HTLC with the smallest timelock, so it is the least-risk and therefore a maker who notices its outgoing HTLC has a low timeout might want to just do this anyway even if it is unsure if the taker is offline.
  * Participants might want to spend from the UTXO to a new address after private key turnover anyway.
    Makers could spend using a low-fee RBF-enabled tx, and when another request comes in for another swap, try to build a new funding tx with a higher-fee bump.
Sorry, I do not follow the logic for this...?
More precisely:
* The HTLC outgoing from Alice has the longest timelock.
* The HTLC incoming into Alice has the shortest timelock.
For the makers, they only need to ensure that the incoming timelock is much larger than the outgoing timelock.
Whoa whoa whoa whoa.
All this time I was thinking you were going to use 2p-ECDSA for all 2-of-2s.
In which case, the private key generated by the taker would be sufficient tweak to blind this.
In 2p-ECDSA, for two participants M = m * G; T = t * G, the total key is m * t * G = m * T = t * M.
Are you going to use `2   2 OP_CHECKMULTISIG` instead of 2p-ECDSA?
Note that you cannot usefully hide among Lightning mutual closes, because of the reserve; Lightning mutual closes are very very likely to be spent in a 1-input (that spends from a 2-of-2 P2WSH), 2-output (that pays to two P2WPKHs) tx.
The protocol looks correct to me.
Give me a little more time to check it in detail hahaha.
I agree.
Looks OK, though note that a participant might try to do so (as pointed out above) in the hope that the next participant is offline.
Thank you very much for your writeup!

@_date: 2020-08-20 21:38:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Nadav,
My reading from this is that nonce `p` has to be given by the taker to the maker outright.
In original post:
Thus, taker provides a proof-of-knowledge, i.e. the actual `p` scalar itself (not zero-knowledge, but what the maker needs is proof-of-knowledge, and could not care less if the proof is zero-knowledge or not).
On the other hand, I do not see the point of this tweak if you are going to use 2p-ECDSA, since my knowledge is that 2p-ECDSA uses the pubkey that is homomorphic to the product of the private keys.
And that pubkey is already tweaked, by the fresh privkey of the maker (and the maker is buying privacy and wants security of the swap, so is incentivized to generate high-entropy temporary privkeys for the actual swap operation).
Not using 2p-ECDSA of some kind would remove most of the privacy advantages of CoinSwap.
You cannot hide among `2   2 OP_CHECKMULTISIG` scripts of Lightning, because:
* Lightning channel closes tend to be weeks at least after the funding outpoint creation, whereas CoinSwap envisions hours or days.
* Lightning mutual channel closes have a very high probability of spending to two P2WPKH addresses.
You need to hide among the much larger singlesig anonymity set, which means using a single signature (created multiparty by both participants), not two signatures (one from each participant).
Or is this intended for HTLCs in open-coded SCRIPTs `OP_DUP OP_IF OP_HASH160  OP_EQUAL  OP_ELSE  OP_CHECKSEQUENCEVERIFY OP_DROP  OP_ENDIF OP_CHECKSIG`?
This provides a slight privacy boost in a case (contract transaction publication) where most of the privacy is lost anyway.

@_date: 2020-08-21 04:20:23
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning,
Spending one UTXO is fine, it is generating a transaction that has one output that is problematic.
What needs to happen is that this single UTXO is spent to two outputs: the CoinSwap 2-of-2 and the change output.
This is because intermediate makers will have very high likelihood of generating such a pattern (it is unlikely they have an exact amount that a taker would require of them), and the occassional maker might have a very large UTXO that it can use for similar purposes.
One thing a taker can do would be to multipath its CoinSwap, i.e. it spends any number of UTXOs and creates two outputs, which are actually two separate CoinSwap 2-of-2s to different makers.
As each maker is unaware of the other, this should be similar to the case where the maker is an intermediate hop and is getting its incoming HTLC from another maker, which is unlikely to have a precise amount and will thus have a transaction that has two outputs, the 2-of-2 CoinSwap and the change.
Indeed, this seems a bit of a long shot for the surveilling maker.
I was wondering if it would be a good idea actually if the **largest** fee RBF transaction had additional CPFP anchor outputs, not saying to replace the entire group of RBF transactions entirely.
This is just in case of a very sudden increase in feerates that goes beyond the largest that was prepared beforehand.
"You cannot predict the feerates future" is becoming something of a mantra over in Lightning, though I guess a "big enough" spread of RBF transactions would work in practice >99% of the time.
Okay, that is much clearer.
Absolute timelocks mean that you can set a timer where you put your node to sleep without risk of loss of funds (basically, once the absolute timelocks have resolved, you can forget about CoinSwaps).
But I think the ability to spend at any time would be better, and getting 100% online 144 blocks a day, 2016 blocks a retargeting period is becoming more and more feasible.
They would be spending miner fees *from the funds being stolen*, thus still costless.
In particular, let us imagine a simple 1-maker swap.
* The taker and the maker complete the swap.
* The taker now has possession of:
  * The private key for its incoming HTLC.
  * The pre-signed contract transaction for its outgoing HTLC.
* The taker spends from its incoming HTLC using the private key.
  * The maker ignores this, because this is just normal operation.
  * Fees paid for this is not an **additional** cost, because a taker that wants to put its freshly-private funds into cold storage will do this anyway.
  * The taker gets a fresh, private coin from this incoming HTLC, so it gets the privacy it paid for.
* The taker waits for the incoming-HTLC-spend to confirm.
* The taker broadcasts the pre-signed contract transaction, in the hope that the maker is offline.
  * The fees paid for this are from the contract transaction that the taker is trying to steal.
    Even if the theft attempt fails, the taker has already gotten its private money out, and is thus not risking anything.
  * Semantically, the outgoing HTLC is already "owned" by the maker (the maker has private key to it).
    * Thus, the taker commits an action that the maker pays fees for!
  * The maker cannot react except to spend via the hashlock branch.
    In particular, because the taker-incoming (maker-outgoing) UTXO is already spent, it cannot retaliate by also broadcasting the contract transaction of the taker-incoming (maker-outgoing) HTLC.
* The theft succeeds (the timelock passes) because the maker happens to be offline for that long.
  * This is "free money" to the taker, who has already gotten what it paid for --- private money in cold storage --- from the CoinSwap.
  * Even if the stolen fund reveals the contract, the taker can re-acquire privacy for the funds it stole for free, by paying for --- wait for it --- another CoinSwap for its swag.
Using an absolute timelock (implemented by a `nLockTime` tx directly off the 2-of-2, ***not*** `OP_CHECKLOCKTIMEVERIFY`), plus a Scriptless Script 2p-ECDSA (again implemented by a tx directly off the 2-of-2) instead of a hashlock, seems to avoid this, I think, at the cost of reducing the utility of private key turnover by having a deadline where the private key *has to* be used.
This is because there is no contract transaction that is share-owned by both participants in the swap.
Instead there are two distinct transactions with separate ownerships: a timeout tx (that is owned by the participant paying for the HTLC/PTLC) and a claim tx (that is owned by the participant accepting the HTLC/PTLC).
Accidents can happen (e.g. somebody trips over the power cord of the maker hardware), so hedging this somewhat might give a useful safety net.
Ah, right, that is clearer.
Okay, that is clearer.
I think 2p-ECDSA should be first priority after getting a decent alpha version.
2p-ECDSA with Scriptless Script potentially gives a lot more privacy than any PayJoin IMO, due simply to the much larger anonymity set, and there are enough chain-analysis-heuristic-breaking shenanigans we can implement with plain CoinSwap, I think.

@_date: 2020-08-22 01:09:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris,
Assuming you have multiple watchtowers, yes.
It would be best if watchtowers for CoinSwap and watchtowers for Lightning could be the same thing, and ideally, a watchtower would not even know if what it was watching were a Lightning channel or a CoinSwap until an attack happens.
Not all locations on Earth make it easy to be 100% online.
However, as the technology of you puny humans advance, it becomes more and more possible for a random point on Earth to be 100% online.
We also cannot use succinct atomic swaps because their asymmetry makes them unroutable --- you can only use it for single-maker swaps.
This makes it obvious to the maker that you have only a single maker.
Note that this only works if you dive into Scriptless Script 2p-ECDSA/Schnorr immediately.
It also makes watchtowers for Lightning inherently incompatible with watchtowers for CoinSwaps using absolute timelocks.
A watchtower guarding for CoinSwaps using absolute timelocks would:
* Need to know the funding outpoint it is guarding.
  * Watchtowers for Lightning (and contract-transaction-based CoinSwap) do *not* need to know this, they just need to know a transaction ID that, if confirmed, they will broadcast *another* transaction.
* Need to watch *blockheight*.
  * Watchtowers for Lightning (and contract-transaction-based CoinSwap) only check for transactions matching txids they are watching for.
In particular the first point is a massive privacy lose.
Lightning watchtowers can have the txid they are watching for in the clear, and the transaction they will broadcast in reaction to the watched txid being confirmed is encrypted using a key derived from the transaction with the given txid, and thus do not learn which funding outpoint it is protecting until an attack occurs, which is very good for privacy.
(even if the maker were to run private watchtowers of their own rather than using some public watchtower service, if the private watchtower is hacked it contains information that can be used to identify funding outpoints, thus making them targets.
Thus, it is best if watchtowers, whether public or private, do not contain any privacy-damaging information, to reduce the attack surface on privacy.)
A way to make watchtowers for absolute-timelock CoinSwap also have the same interface (i.e. "Watch for this txid, if it appears onchain broadcast this transaction") as Lightning watchtowers would be to have the timeout tx pay out to a `OP_IF <1 day> OP_CHECKSEQUENCEVERIFY OP_DROP  OP_ELSE  OP_ENDIF OP_CHECKSIGVERIFY`.
The revocation key would be the same private key that is turned over at the end of the CoinSwap.
So, if the absolute timelock expires and the other participant broadcasts the timeout tx, the maker still has an opportunity to revoke that output, for one additional day.
Then, at the end of the CoinSwap where the private key is turned over, the maker can hand the txid of the timeout tx, plus an encrypted transaction that spends from the revocation branch of the timeout tx back to the maker and a tip to the watchtower, to the watchtower, who remains unaware what the funding txo is (it only gets a txid and an encrypted blob, so gets no information).
The same interface can be used by Lightning Poon-Dryja (it is sub-optimal, but usable, for Decker-Russell-Osuntokun), and the watchtower would not even learn if it was watching for a Lightning channel or for a CoinSwap.
Then, if the maker were holding on to the funding outpoint of its incoming HTLC in the hope another taker arrives for its services, and then some silly human trips over the maker hardware power cord, and the condition is not fixed by the timeout, it can still be privately protected by watchtowers.
This comes at the cost of even worse UX if something goes wrong with the swap: an increased timeout.

@_date: 2020-08-25 03:16:05
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Antoine,
I believe it is quite dependent on relative locktimes.
Relative locktimes *require* a contract transaction to kick off the relative locktime period.
On the other hand, with Scriptless Script (which we know how to do with 2p-ECDSA only, i.e. doable pre-Taproot), absolute locktimes do not need a contract transaction.
With absolute locktimes + Scriptless SCript, in a single onchain PTLC, one participant holds a completely-signed timelock transaction while the other participant holds a completely-signed pointlock transaction.
This can be arranged by having one side offer partial signatures for the transaction of the other, and once completing the signature, not sharing it with the other until we are ready to actually broadcast the transaction of our own volition.
There is no transaction that both participants hold in completely-signed form.
This should remove most of the shenanigans possible, and makes the 30xRBF safe for any range of fees.
I think.
Since for each PTLC a participant holds only its "own" transaction, it is possible for a participant to define its range of fees for the RBF versions of the transaction it owns, without negotiation with the other participant.
Since the fee involved is deducted from its own transaction, each participant can define this range of RBFed fees and impose it on the partial signatures it gets from the other participant.
Private key turnover is still useful even in an absolute-timelock world.
If we need to bump up the block delta between links, it might be impractical to have the total delta of a multi-hop swap be too long at the taker.
As a concrete example, suppose A is a taker who wants to route over makers B and C.
However, B and C require a CLTV delta of 1 week.
If A wants to route "directly" A->B->C->A, then if something bad happens, it could be looking at having its funds locked for two weeks.
To reduce this risk, A can instead first swap A->B->A, then when that completes, A->C->A.
This limits its funding lockup to 1 week.
Private key turnover is useful since as soon as the A->B->A swap completes, it can directly fund the A->C->A swap from the B-side funding transaction of the A->B->A swap.
   A->B->A         |    A->C->A           |
         :                   :                      :
      A -:->funding A&B--> B :                      :
         :                   :                      :
      B -:->funding A&B -----:--> funding A&C --> C :
         :                   :                      :
         :                   :C-> funding A&C ------:-> to-cold  A -->
         :                   :                      :
This increases the number of transactions by 1 per swap beyond the first, compared to a direct routing A->B->C->A, but this may be worth it for A if the timelocks involved are too big for A.
With 2p-ECDSA, a funding A&C looks exactly the same as a to-cold A, so B is unable to reliably determine if it is the last hop in the route.
Without private key turnover, A would have:
                      **NO** private key turnover!
   A->B->A         |    A->C->A                      |
         :                   :                                 :
      A -:->funding A&B--> B :                                 :
         :                   :                                 :
      B -:->funding A&B -----:--> claim A -> funding A&C --> C :
         :                   :                                 :
         :                   :           C-> funding A&C ------:-> to-cold  A -->
         :                   :                                 :
So if timelock-deltas are possibly-high (to reduce the probability of the MAD-HTLC argument, and other attacks, succeeding), takers might prefer to route by completing one swap first before starting the next one, and private key turnover is useful by reducing blockspace required by each hop.
For reference, this is how it looks like with a single A->B->C->A swap with private key turnover:
   A->B->C->A      |
         :                   :
      A -:->funding A&B--> B :
         :                   :
      B -:->funding B&C -> C :
         :                   :
      C -:->funding A&C -----:-> to-cold A -->
         :                   :
This is still smaller than in the A->B->A, A->C->A with private key turnover, by one funding tx per hop.
However, A risks a much higher timelock (twice the timelock).
Thus, A might prefer a lower timelock in exchange for paying for an additional transaction.

@_date: 2020-08-25 03:16:05
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Antoine,
I believe it is quite dependent on relative locktimes.
Relative locktimes *require* a contract transaction to kick off the relative locktime period.
On the other hand, with Scriptless Script (which we know how to do with 2p-ECDSA only, i.e. doable pre-Taproot), absolute locktimes do not need a contract transaction.
With absolute locktimes + Scriptless SCript, in a single onchain PTLC, one participant holds a completely-signed timelock transaction while the other participant holds a completely-signed pointlock transaction.
This can be arranged by having one side offer partial signatures for the transaction of the other, and once completing the signature, not sharing it with the other until we are ready to actually broadcast the transaction of our own volition.
There is no transaction that both participants hold in completely-signed form.
This should remove most of the shenanigans possible, and makes the 30xRBF safe for any range of fees.
I think.
Since for each PTLC a participant holds only its "own" transaction, it is possible for a participant to define its range of fees for the RBF versions of the transaction it owns, without negotiation with the other participant.
Since the fee involved is deducted from its own transaction, each participant can define this range of RBFed fees and impose it on the partial signatures it gets from the other participant.
Private key turnover is still useful even in an absolute-timelock world.
If we need to bump up the block delta between links, it might be impractical to have the total delta of a multi-hop swap be too long at the taker.
As a concrete example, suppose A is a taker who wants to route over makers B and C.
However, B and C require a CLTV delta of 1 week.
If A wants to route "directly" A->B->C->A, then if something bad happens, it could be looking at having its funds locked for two weeks.
To reduce this risk, A can instead first swap A->B->A, then when that completes, A->C->A.
This limits its funding lockup to 1 week.
Private key turnover is useful since as soon as the A->B->A swap completes, it can directly fund the A->C->A swap from the B-side funding transaction of the A->B->A swap.
   A->B->A         |    A->C->A           |
         :                   :                      :
      A -:->funding A&B--> B :                      :
         :                   :                      :
      B -:->funding A&B -----:--> funding A&C --> C :
         :                   :                      :
         :                   :C-> funding A&C ------:-> to-cold  A -->
         :                   :                      :
This increases the number of transactions by 1 per swap beyond the first, compared to a direct routing A->B->C->A, but this may be worth it for A if the timelocks involved are too big for A.
With 2p-ECDSA, a funding A&C looks exactly the same as a to-cold A, so B is unable to reliably determine if it is the last hop in the route.
Without private key turnover, A would have:
                      **NO** private key turnover!
   A->B->A         |    A->C->A                      |
         :                   :                                 :
      A -:->funding A&B--> B :                                 :
         :                   :                                 :
      B -:->funding A&B -----:--> claim A -> funding A&C --> C :
         :                   :                                 :
         :                   :           C-> funding A&C ------:-> to-cold  A -->
         :                   :                                 :
So if timelock-deltas are possibly-high (to reduce the probability of the MAD-HTLC argument, and other attacks, succeeding), takers might prefer to route by completing one swap first before starting the next one, and private key turnover is useful by reducing blockspace required by each hop.
For reference, this is how it looks like with a single A->B->C->A swap with private key turnover:
   A->B->C->A      |
         :                   :
      A -:->funding A&B--> B :
         :                   :
      B -:->funding B&C -> C :
         :                   :
      C -:->funding A&C -----:-> to-cold A -->
         :                   :
This is still smaller than in the A->B->A, A->C->A with private key turnover, by one funding tx per hop.
However, A risks a much higher timelock (twice the timelock).
Thus, A might prefer a lower timelock in exchange for paying for an additional transaction.

@_date: 2020-08-30 13:38:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
============================== START ==============================
Good morning Chris,
Minimum output size is 547 sats, so anchor outputs are that amount at minimum.
A P2SH-P2WPKH output costs something like ~130 vbytes to spend, at 1.000 sat/vbyte that is only ~130 sats to spend a 547 sat anchor output, an opportunistic camper could collect from a few swaps it would have done anyway (e.g. as a passive popular maker?) and broadcast the contract txes of those swaps and then spend the anchor outputs together to get a few sats in a not-so-dusty UTXO, getting (547 - 130) sat per input minus the cost of creating a new tiny output.
Assuming the camper has already claimed its side of the swap in order to put it in cold, this is basically a tiny but free amount of extra money, and if small CoinJoins in JoinMarket are any indication, the 547 sats minus fee to spend it minus fee to create (amortized among the multiple contract txes) new UTXO might be comparable to the actual maker fee.
Since this camping attack is done after the CoinSwap, the maker fidelity bond is a weak protection against this.
The maker can keep around contract transactions indefinitely, and if standard wallets assume they can leave the coins in the same UTXO indefinitely, the contract transactions remain valid indefinitely, including up to fidelity bond timeout.
When the fidelity bond times out, the maker has to destroy its identity anyway, so it could opportunistically wait for a low-fee period after fidelity-bond timeout (we currently get low fee periods once a week, for example, so the camper can wait for at most a week to do this) to publish all still-valid contract transactions, and spend all the anchor outputs including the fidelity bond at the minimum feerate, getting a slightly larger fidelity bond fund, then CoinSwap it to honest makers to clean it, then make a new fidelity bond.
And if one of the takers happens to not be watching for contract tx timeout, it can potentially get free money, again, from the inattention.
(I call it a "camper attack" since the attacking CoinSwap participant waits around in a single place (maker fidelity bond) and snipes passing contract transactions to extract value from them when opportunity (low fee rate) is good, like a camper.)
To protect against this, we should force contract txes to signal RBF, make contract txes min-relay=feerate (requires CPFP package relay at base layer tho), and during low-fee periods we should collect outputs whose private key have been turned over to us, paying at a feerate slightly higher than 547 sat / 130 vbyte fee rate (at which point it becomes uneconomical for campers to mount their sniping attack as they would lose the anchor output amount to fees anyway).
In fact the wallet can do that all the time, and if prevailing fees are above the 547 / 130 rate it will not confirm and the wallet that wants to spend its funds *now* can sign a new RBF tx at higher feerate to replace it.
Low fees, who would have thought that would enable an attack vector....

@_date: 2020-12-01 15:49:49
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Out-of-band transaction fees 
Good morning Sebastian and e,
There has to be an association of "how much do I get if I include *this* particular transaction" to "*this* particular transaction", so that the miners have an informed decision of how much they stand to earn.
Unless fees are also standardized, this can be used to leak the same information ("sombody offered this specific amount of money to the bounty server, and the bounty server associated this particular amount to this particular transaction").
More concerningly, [a trusted third party is hard to get out of](
If there are only a few of them, it becomes easy to co-opt, and then a part of the mining infrastructure is now controllable from central points of failure.
If there are many of them, then evaluating which ones cheat and which ones do not will take a lot of effort, and the system as a whole may not provide benefits commensurate to the overall system cost in finding good third parties.
Since such L2 mechanisms themselves are dependent on L1 and require a facility to bump up fees for e.g. commitment transactions in Lightning Network, this brings up the possibility of getting into a bootstrapping problem, where the security of L2 is dependent on the existence of a reliable fee-bumping mechanism at L1, but the fee-bumping mechanism at L1 is dependent on the security of L2.
Not impossible, but such strange loops give me pause; I am uncertain if we have the tools to properly analyze such.

@_date: 2020-12-01 16:24:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Out-of-band transaction fees 
Good morning e, and Sebastian,
So it seems, the goals are the below:
* Someone wants to pay a fee to get a transaction confirmed.
* Miners want to know how much they earn if they confirm a transaction.
* The one paying for the fee does not want to link its other coins to the transaction it wants confirmed.
Would that be a fair restatement of the goal?
If so, it seems to me we can make a CoinJoin-like approach using only L1, and combine fees by a kind of FeeJoin.
The issue with linking is that if for example the one paying a fee to get a transaction confirmed wants to CPFP the transaction, it may need to take another UTXO it controls into the child transaction, thereby linking its "another UTXO" with the "transaction it wants confirmed".
However, if multiple such individuals were to CoinJoin their transactions, the linking becomes much harder to trace.
So a possible mechanism, with a third-party that is trusted only to keep the service running (and cannot cheat the system and abscond with the fees and leave miners without money) would be:
* The third-party service divides its service into fixed-feerate bins.
* Clients select a preferred feerate bin they want to use.
* For each client:
  * Connects to the service by Tor to register a transaction it wants to have CPFPed.
  * Connects to the service by a different Tor circuit to register a UTXO it will use to spend fees.
* The server passes through the CPFPed outputs in the whole value.
* The server deducts the fee from the fee-paying UTXO and creates an output with all the fees (CPFP output spend, UTXO input spend, CPFP output re-creation, UTXO output re-creation) deducted from the UTXO.
* The server gives the resulting transaction to the clients.
* The clients sign the transaction after checking that its interested CPFPed outputs and fee-paying UTXOs are present.
This results in a transaction with many CPFPed inputs and fee-paying UTXOs, and no easy way to link the latter with the former.
* Miners and chain analysis cannot link them, as they see only the resulting tx.
* The service cannot link them, as clients talk to them on two separate Tor connections.
The above is blatantly the Wasabi way of CoinJoining; using the JoinMarket way of CoinJoining should be possible as well, and is left as an exercise to the reader.
Now, you have mentioned a number of times that you believe Bitcoin will eventually be a settlement layer, and somehow link this with standardized UTXO sizes.
But I think the end goal should be:
* To improve Bitcoin blockchain layer privacy.
It should not matter how we achieve this, whether it involves standardized UTXO sizes or not; if you want to use this solution, you need to present a good reason why this is the best solution for Bitcoin privacy, and better than other solutions.
For example, the JoinMarket way of CoinJoining does not require any particular standardized UTXO size.
The upcoming SwapMarket that Chris Belcher is working on, also does not require such a standardized UTXO size, as it is based as well on the JoinMarket technique, where the client can select whatever sizes it wants.
Why should the Bitcoin ecosystem adopt a strict schedule of UTXO sizes for privacy, if apparently JoinMarket and SwapMarket can improve privacy without this?

@_date: 2020-12-31 23:26:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Softchains: Sidechains as a Soft Fork via 
Good morning Ruben, and list,
First and foremost --- what is the point of sidechains, in the first place?
If sidechains are for experimental new features, then softforking in a new sidechain with novel untested new features would be additionally risky --- as you note, a bug in the sidechain consensus may cause non-deterministic consensus in the sidechain which would propagate into mainchain.
Federated sidechains, which already are enabled on current Bitcoin, are safer here, as mainchain will only care about the k-of-n signature that the federation agrees on, and if the federation is unable to come to consensus due to a sidechain consensus bug, "fails safe" in that it effectively disables the peg-out back to mainchain and restricts the consensus problem to the sidechain.
If sidechains are for scaling, then I would like to remind anyone reading this that ***blockchains do not scale***, and adding more blockchains for the purpose of scaling is *questionable*.
"I have a scaling problem.
I know, I will add a sidechain!
Now I have two scaling problems."
Ultimately, proof-of-work is about energy expenditure, and you would be splitting the global energy budget for blockchain security among multiple blockchains, thus making each blockchain easier to 51%.

@_date: 2020-02-01 00:39:36
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Onchain fee insurance mechanism 
Good morning David,
Indeed, the rescindability is a flaw.
I will now do something really evil: I will attempt to patch this flaw without considering that the patch will of course have other detrimental side effects.
Rather than have the Ingrid-input (and output) be solely under the control of Ingrid, it is a 2-of-2 with Ingrid and Alice.
Long before the Alice->Bob transaction, Alice has already commissioned the services of Ingrid.
They have already agreed on the specs of the insurance policy, and in particular, have agreed that this agreement terminates at some future data.
At setup, Alice and Ingrid create a claim transaction for Ingrid, with `nLockTime` set to the agreed-upon end-of-insurance-contract, which allows Ingrid to reclaim the original fund.
Then, at height B when Alice wants to send to Bob, they create the series of timelocked transactions, with the Ingrid output similarly having an `nLockTime`d transaction that lets Ingrid reclaim the earned funds.
Against this patched scheme, of course, new problems arise:
* During times of low fees, Alice can just create a non-insured transaction directly on the blockchain, denying Ingrid its earnings.
* During times of high fees, Ingrid can go offline and refuse to provide signatures needed for the insured transactions, denying Alice its service.
  * This is significant if Alice prepaid for the insurance contract.
Thus, as we can see, patching a flawed protocol still leaves us with a flawed protocol.
On the other hand, the above "Spilmanizing" of the protocol leads to a possible insurance policy for Lightning channel closures.
At the same time as channel establishment between Alice and Bob, Alice also starts an insurance contract with Ingrid.
Alice prepays Ingrid, using a CoinJoined transaction that spends from Alice and Ingrid inputs, with the combined premium plus Ingrid inputs value put in an output locked to Alice && Ingrid, and a maximum contract lifetime (an `nLockTime`d transaction that claims the Alice&&Ingrid output and returns the fund, plus insurance premium, to Ingrid).
Then, at each commitment transaction signing, there is an additional unencumbered but tiny output that Alice can claim immediately (obviously this requires a change in the BOLT spec).
Ingrid and Alice create an insurance transaction with high feerate, which spends the above tiny output, and spends the Alice&&Ingrid output, deducting the fees from the Alice&&Ingrid output and returning what is left to Ingrid.
Then, if Alice decides to drop the unilateral close onchain:
* If fees are low at the time that unilateral close, then Alice can just claim the tiny output itself.
  * Alice is incentivized to do so because it means she will still control that tiny output.
  * Ingrid can then reclaim its fund, plus the premium, at the end of the insurance contract lifetime.
* If fees are high at the time that unilateral close, then Alice can sacrifice the value of the tiny output and attach the insurance transaction with high feerate.
* If on a new commitment transaction, Ingrid does not cooperate, then Alice can drop onchain *and* punish Ingrid by dropping the previous commitment and also broadcasting the insurance transaction.
  * Alice has to sacrifice its tiny output to do so, but it would be worth it to punish Ingrid and deter this non-cooperation.
* When the insurance contract lifetime is near, Alice and Ingrid can renew the contract by cooperatively spending the Alice&&Ingrid output to a new Alice&&Ingrid output (possibly with some payment from Alice to renew the contract).
* This gives an upper bound for what Alice will pay to ensure its channel is closeable at any time very quickly, which is the entire point.

@_date: 2020-02-08 02:15:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Purge attacks (spin on sabotage attacks) 
Good morning M,
What do you mean by this?
Is this intended to be a consensus rule, i.e. nodes will never accept such a block?
Because if so, this fails the principle of Blockchain Self-Containment, i.e. consensus rules can only check what is in the blockchain.
The mempool (and contest pool) is not in the blockchain as it is never attested to in the blockchain.
If this is not a consensus rule (i.e.e nodes can be convinced to accept an announced block that violates the above via some rule, such as sufficient confirmations) then this does not protect against purge attacks.
Purge attacks can still be defended against and does not require mass cooperation.
If there is a transaction that is economically beneficial to me, it does so by paying some Bitcoins to me.
If it pays Bitcoins to me, I can spend those Bitcoins in a transaction that just offers to pay mining fees and transfers it back to me (i.e. child pays for parent) to convince miners to mine the purged transaction.
As the Purge attack is "just" a censorship attack (i.e. a censorship of all transactions in the block under attack), the increased mining fees for the transactions being censored (i.e. offered via child-pays-for-parent in this case) is an economic counterattack on the censoring miner (i.e. it forgoes the mining fees).
With enough self-interested users, the fee offered to confirm the transactions can be substantial enough that non-censoring miners can be convinced to mine those transactions.
No coordination necessary, as is typical for all defenses against censorship (and the basis of the censorship-resistance of Bitcoin).

@_date: 2020-02-09 00:00:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Purge attacks (spin on sabotage attacks) 
Good morning M,
Violation of this principle can cause persistent chainsplits where you induce one set of nodes to see one view of reality while another set of nodes see another view.
For instance, suppose two innocent miners happen to find blocks at nearly the same time.
Unfortunately for them, one miner happened to be using "SPV" mining i.e. mining empty blocks.
Yet this happenstance occurrence now causes a chainsplit, as some number of nodes (those near to the SPV-mining miner) think that miner is innocent of wrongdoing and will support the "purged" chainsplit, whereas those near the other miner will consider that block bad and will support the other "unpurged" chainsplit.
This is an even worse consequence than any purge attack, and could happen completely by chance with no malice involved.
Always avoid violating that principle in any consensus code.
If it is not committed to in the block and is not provable using only data you provide with the block, you cannot use it safely without risking chainsplit.
(and no, banning or even disincentivizing SPV mining will not work, different nodes have different views of the mempool and temporary chainsplits can occur by chance where one chainsplit has transactions that are not confirmed in the other chainsplit, which again is just another short-term inadvertent Purge attack on the network.)
Your defender, in this attack, should avoid the Sunk Cost Fallacy here.
If the defender has been so foolish as to provide a product or service based on only a *few* confirmations, like 1 or 2, then that product or service has been Sunk, and it should ignore the Sunk Cost here.
As the same value is under contest on both sides, they are equally matched and both censoring and non-censoring miners will get the same incentive, splitting up the network into two nearly equal halves, and then chance (lucky block discovery) decides between which is the winner or the loser.
The difference here is that the chainsplit in this case is in a metastable state, and once a string of lucky block discoveries occurs, it falls into a stable state and now everybody agrees again on who won and who lost.
Your solution risks *persistent* *stable* chainsplits.
Worse, this occurrence without your solution would only happen if some miners actually attack the blockchain.
With your solution, persistent chainsplits can occur without malice, simply chance.
And as in many things in life, the only winning move is not to play.
Just wait for more than a small number of confirmations (e.g. 6 is generally considered safe), and the chance that a Purge attack on your transactions succeeds is low enough that worse force majeur (a rogue asteroid hitting your datacenter, for example) is more likely.

@_date: 2020-02-09 23:59:56
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Purge attacks (spin on sabotage attacks) 
Good morning M,
Are you sure you are solving the same problem?
The mempool **has no consensus**.
It is strictly an optimization, preventing a node from needlessly broadcasting transactions.
Making consensus dependent on the state of the mempool requires that you record the state of the mempool at the point at which the block snapshot was taken.
Otherwise, newly-started nodes can be fooled into taking the "wrong" consensus branch leading to persistent chainsplits.
I already described it in the previous post.
Purge attacks happen all the time, when two miners mine blocks at nearly the same time, but with different sets of transactions in their blocks.
And as I pointed out, any mechanism which uses non-block data (such as mempool data) *will* lead to persistent chainsplits.
You ***cannot*** get rid of RBF.
The incentives of miners mean they will actually want to implement RBF and ignore any "convention" of RBF-flagging.
My understanding is that there are claims that a minority of miners already do this (possibly Peter Todd has more information, but I am uncertain), and will accept "full" RBF i.e. ignore the RBF flag and always apply RBF to all transactions regardless.
Nothing in consensus prevents this, and this is why we always wait for confirmation.
Regardless of however many blocks are attacked, always remember that in the end, this is still a *censorship* attack: it is attempting to censor Bitcoin completely.
As such, this page applies:

@_date: 2020-02-10 06:27:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity 
Good morning The Group,
There are already many excellent arguments presented for Taproot, let me present a related one.
Notice your example MAST:
Of particular note is that the MAST has a predetermined set of scripts, `a` to `h`.
Now, practically speaking, each of these scripts `a`..`h` will be claimable by one or a number of known, pre-determined participants as well.
Scripts that do not have a pre-determined set of participants exist (e.g. a simple `OP_HASH160  OP_EQUAL` without any `OP_CHECKSIG` operations) but are generally not expected to actually be *useful* for a majority of use-cases (the above hash-only example could be double-spent by a majority miner, for example).
We expect a vast majority of scripts that will be in use will have a pre-determined fixed finitely-enumerable set of participants (so that miners cannot steal coins once the "solution" to the script puzzle is published in mempools), represented by pubkeys that are fed into `OP_CHECKSIG` operations in the script.
Since each script has (with high probability approaching 1.0) a pre-determined fixed finitely-enumerable set of participants within that script, and the entire MAST itself has a pre-determined fixed finitely-enumerable set of scripts, we can take the union of all sets of participants of all the scripts in the MAST.
Then we put the union of those sets as the signatories of a single Schnorr n-of-n multisignature, to be used as the Taproot keypath branch.
The advantage now is that with Taproot:
* If you can induce all participants to sign a transaction using the keypath spend, then you gain privacy (no part of the MAST is ever published, not even its root or the presence of the MAST!) *and* reduced onchain fees (because the MAST is not published and does not take up space on the blockchain).
  * You can incentivize cooperation (beyond just the incentive of improved privacy) by letting participants recover some of the saved onchain fees.
    Lightning does this, for example: the funder of the channel is the one paying for the closing fees, and the closing fee of the mutual close is almost always lower than the unilateral close case (or else is equal: the closing ritual has the unilateral close fee as the upper bound on whatever fee can be proposed at the mutual close ritual).
* Even if a participant does not cooperate (for example, it might have been hit by a rogue asteroid in the meantime) we still have the fallback of revealing the entire MAST.
(Just to be clear: I do not *currently* own any datacenters at locations that are likely to be hit by rogue asteroids.)
Such contracts and protocols can then be Taproot-ized in order to gain some privacy and transaction size benefits.
Other optimizations, such as selecting k of the n participants as "key participants" who are the most likely to be online and interested in the conclusion of the contract, can then be used to reduce the n-of-n to k-of-n, but the basic Taproot "there exists some n-of-n" assumption still holds and this is just an optimization on top of that.

@_date: 2020-02-15 00:24:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Good morning Dmitry, and Jeremy,
But an RBF-ed transaction *is* a double-spend, and the principle makes an exception specifically for double-spends.
Thus RBF, and other double-spends, are exempt from this principle.
My vague understanding of the "revocation UTXO" feature is that it is implemented as a double-spend of a precommitted `OP_CTV`, so that also is exempted from the principle.
As Jeremy notes as well, the *actual* principle is that "a script that is valid now remains valid in the future" (as this is required by the script cache implementation of Bitcoin Core), and this principle does not mention UTXOs, only scripts; the existence or non-existence of a required UTXO is separate here.
Thus, an "automatic inverse timelock" is not possible to implement with **only** script (it implies that a script that is valid now will become invalid in the future), but requires some action on the blockchain (notice that HTLCs effectively implement an "inverse timelock" on the hash-branch participant, by threatening a spend (i.e. blockchain activity) by the counterparty if does not comply).

@_date: 2020-02-23 00:59:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] LN & Coinjoin, a Great Tx Format Wedding 
Good morning waxwing,
CoinJoinXT is indeed something I am interested in at some point: The above writeup is a client-server model, with multiple clients mixing.
If none of the participants reveal that a CoinJoinXT was done, then the graph is difficult to detect as such.
However, if any participants reveal that a CoinJoinXT was done, it has a fallback such that it is almost as good as an equal-value CoinJoin (but takes up more block space).
At least it is not immediately obvious that it is in fact a CoinJoinXT from *just* a simple transaction analysis, which we hope is enough to deter simple policies like "check N transactions back for a transaction with more than one equal-valued output".

@_date: 2020-02-23 01:29:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] LN & Coinjoin, a Great Tx Format Wedding 
Ggood morning Antoine, and list,
I notice your post puts little spotlight on unilateral cases.
A thing to note, is that we only use `nSequence` and the weird watermark on unilateral closes.
Even HTLCs only exist on unilateral closes --- on mutual closes we wait for HTLCs to settle one way or the other before doing the mutual close.
If we assume that unilateral closes are rare, then it might be an acceptable risk to lose privacy in that case.
Of course, it takes two to tango, and it takes two to make a Lightning channel, so ---
In any case, I explored some of the difficulties with unilateral closes as well:
* * On mutual closes, we should probably set `nLockTime` to the current blockheight + 1 as well.
This has greater benefit later in a Taproot world.
A kind of non-equal-value CoinJoin could emulate a Lightning open + close, but most Lightning channels will have a large number of blocks (thousands or tens of thousands) between the open and the close; it seems unlikely that a short-term channel will exist that matches the non-equal-value CoinJoin.
In particular, a LN cooperative close will, in general, have only one input.
A new form of CoinJoin could, instead of using a single transaction, use two, with an entry transaction that spends into an n-of-n of the participants, and the n-of-n being spent to split the coin back to their owners.
But again: a Lightning network channel would have much time with the funds in a single UTXO before later splitting the funds,
This also starts edging closer to CoinJoinXT territory.
Should always be on, even if we do not (yet) have a facility to re-interact to bump fees higher.
While it is true that a surveillor can determine that a transaction has in fact been replaced (by observing the mempool) and thus eliminate the set of transactions that arose from protocols that mark RBF but do not (yet) have a facility to bump fees higher, this information is not permanently recorded on all fullnodes and at least we force surveillors to record this information themselves.
It seems likely.
However, it seems to me that we need to as well nail down the details of this format.
That is indeed an issue.

@_date: 2020-02-24 23:35:56
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] LN & Coinjoin, a Great Tx Format Wedding 
Good morning Antoine,
Ah, that is indeed of great interest.
I proposed before to consider splicing as a form of merged closing plus funding, rather than a modification of channel state; in particular we might note that, for compatibility with our existing system, a spliced channel would have to change its short channel ID and channel ID, so it is arguably a different channel already.
Yes; I am curious how JoinMarket reconciles how makers mix their coins vs. how takers do; presumably the tumbler.py emulates the behavior of a maker somehow.
Long ago, I proposed an alternative to splicing, which would today be recognizable as a "submarine swap" or "lightning loop". Perhaps the frequencies of those operations may hint as to how much splicing would occur in practice in the future.
Well, one way to implement splice-in would be to have an output that is first dedicated to the splice-in, and *then* a separate transaction which actually does the splice-in.
This has a drawback of requiring an extra transaction, which wins us the facility to continue operation of the channel even while the splice-in transactions are being confirmed while retaining only one state.
(the latest proposal, I believe, does *not* use this construction, and instead requires both sides to maintain two sets of states, with one state being a fallback in case the splice-in gets double spent; but in times of high blockchain load this can lead to the channel having a two sets of states until blockchain load reduces.)
As it happens, my alternate proposal for CoinJoinXT is similar in that there are "entry transactions" that introduce coins into the PTG, which are needed to prevent participants from double-spending while the mix is on-going. Note the proposal differs from the original by waxwing, which requires backouts at each intermediate output, and the entry transactions are potential watermarks on the CoinJoinXT protocol as well.
A Chaumian CoinJoinXT cannot use backouts at each intermediate output since no participant should have any knowledge of how much each participant has contributed to each intermediate output, they only know they put so many funds in and so should get so many funds out.
Emulating LN splices mildly makes ConJoinXT less desirable, however, as the mix takes longer and is more costly.
Grumble grumble, all unconfirmed transaction are RBF because miners like money, grumble grumble, flagged RBF is just a node relay policy, grumble grumble, some humans sometimes, grumble grumble....
Does not Electrum do RBF by default?
Unless I have a lower-level agent that always enables RBF option when I install new Electrums, hmmm, maybe I should check first.
1.5RTT with MuSig.
An issue here is that if not all participants contribute to the fees equally, then a participant who is paying lower fee or no fee has a mild incentive to just broadcast the highest-fee version and be done with it: forget the other transactions and just aim for the highest fee immediately, ignore the mempool state so you do not have to do all those calculations or even maintain a mempool, and so on.
This can be mitigated if all participants contribute equal or nearly-equally to the fees, though that complicates single-funding, and may violate Initiator Pays Principle (the initiator of an action should pay all fees related to the action, as otherwise it may be possible to create a null operation that the acceptor of the action ends up paying fees for, which can be used as a financial attack to drain acceptors).
There may be other protocols interested in this as well --- for instance "submarine swaps" and "lightning loops", which are the same thing.

@_date: 2020-02-28 13:10:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Removing Single Point of Failure with Seed Phrase 
Good morning Christopher,
I believe you missed a point that Jeremy was making:
* In a Shamir Secret Sharing Scheme, at some point in the past, some device had to contain the actual original pre-shared secret, and that device might have had backdoors you were unaware of at the time that you were doing the secret sharing.
  * Thus the entire secret might already be compromised long before you recover the secret again.
Verifiable Secret Sharing seems to be better in this regard, as each signer device can generate its own share independently of every other and effectively do an interactive setup to determine their public key (as I understand it --- I am not a mathist, and while I do have some notes regarding this from gmax, I confess I still have only a vague grasp of this math).
It may be better to outright forget Shamir Secret Sharing even exists, and prefer to use Verifiable Secret Sharing instead.

@_date: 2020-02-28 13:31:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Nonce blinding protocol for hardware wallets and 
Good morning Stepan,
I think it would be unsafe to use a deterministic scheme, that takes as input the message m and the privkey only.
Let us consider the case where the hardware signer has its power supply coming from USB and the communication channel is over USB as well.
Thus, the host can selectively turn on/off the hardware signer (e.g. a hacker with physical access can just unplug it).
With R determined from m and the privkey, then the host knows the R that the signer will use, and can arrange an n that cancels that R and adds a specific R it wants to target.
It could, for example, arrange to have two different `m` signed with the same `R'`.
What would have to be done would be derive `k` from the message `m` plus the `sha256(n)` and the privkey.
Perhaps you considered this already, but it may be useful to have it explicitly stated that this has to be mixed as well, i.e. if `k` is generated deterministically it has to be `k = f(sha256(n), m, privkey)` where `f()` is some suitable hashing function.
Otherwise a completely-random `k` would be much better, but the signer might not have enough resources to gather sufficient entropy.

@_date: 2020-01-13 00:21:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
The reason why I stopped considering sidechains for scaling and have since moved to Lightning Network development was that, on reflection, I realized sidechains *still* do not scale, even with stakes anchored on the mainchain.
The issue is that sidechains, like any blockchain, still require that everyone interested in it to propagate all their transaction to everyone else interested in it.
Contrast this with Lightning Network, where you select only a tiny handful of nodes to inform about your payment, even if you have a gigantic Lightning Network.
Or, more blithely: Let me get this straight, you already know blockchains cannot scale, so your scaling proposal involves making ***more*** blockchains?
You might point to the use of large numbers of sidechains with limited userbase, and the use of cross-chain atomic swaps to convert between sidecoins.
I would then point out that Lightning Network channel are cryptocurrency systems with two users (with significantly better security than a 2-user sidechain would have), and that Lightning Network payment routing is just the use of cross-channel atomic swaps to convert between channelcoins.
Indeed, with a multiparticipant offchain updateable cryptocurrency system mechanism, like Decker-Wattenhofer or Decker-Russell-Osuntokun ("eltoo"), you could entirely replace sidechains with a mechanism that does not give custody to your funds to anyone else, since you can always insist on using n-of-n signing with you included in the signer set to prevent state changes that do not have your approval.
You could implement the collateral contract with a simple ` OP_CHECKSEQUENCEVERIFY OP_DROP  OP_CHECKSIG`, with a single-sign signature used at the consensus layer for your sidechain.
`OP_CHECKSEQUENCEVERIFY` ensures, as a side effect, that the spending transaction opts in to RBF.
Thus, if the pubkey `` is used in a single-sign signature scheme (which reveals the privkey if double-signed), then at the end of the period, anyone who saw the double-signing can claim that fund and thus act as "Bob".
Indeed, many "Bob"s will act and claim this fund, increasing the fee each time to try to get their version onchain.
Eventually, some "Bob" will just put the entire fund as fee and put a measly `OP_RETURN` as single output.
This "burns" the funds by donating it to miners.
Alice also cannot plausibly bribe a miner, since the miner could always get more funds by replacing the transaction internally with a spend-everything-on-fees `OP_RETURN` output transaction, and can only persuade the miner not to engage in this behavior by offering more than the collateral is worth (which is always worse than just losing the collateral).
A `OP_CHECKTEMPLATEVERIFY` would work better for this use-case, but even without it you do not need a *single* *tr\*sted* Bob to implement the collateral contract.

@_date: 2020-01-13 02:33:21
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
Which is why I brought up multiparticipant offchain updateable cryptocurrency systems.
The "channel factories" concepts does what you are looking for, except with better trust-minimization than sidechains can achieve.
Just replace "sidechain" with either Decker-Wattenhofer or Decker-Russell-Osuntokun constructions.
You can even use the Somsen "statechain" mechanism, which rides a Decker-Wattenhofer/Decker-Russell-Osuntokun construction, though its trust-minimization is only very very slightly better than federated sidechains.
It is helpful to remember that Poon-Dryja, Decker-Wattenhofer, Decker-Russell-Osuntokun, and all other future such constructions, can host any contract that its lower layer can support.
So if you ride a Poon-Dryja on top of the Bitcoin blockchain, you can host HTLCs inside the Poon-Dryja, since the Bitcoin blockchain can host HTLCs.
Similarly, if you ride a Decker-Wattenhofer on top of the Bitcoin blockchain, you can host a Poon-Dryja inside the Decker-Wattenhofer, since the Bitcoin blockchain can host Poon-Dryja channels.
This central insight leads one to conclude that anything you can put onchain, you an generally also put offchain, so why use a chain at all except as an ultimate anchor to reality?
Poon-Dryja is strictly two-participant, while Decker-Wattenhofer limits the practical number of updates due to its use of decrementing relative timelocks: so you put the payment layer in a bunch of Poon-Dryja channels which support tons of updates each but only two participants per channel, and create a layer that supports changes to the channel topology (where changes to the channel connectivity are expected to be much rarer than payments) and is multiparticipant so you can *actually* scale.
Instead of using sidechains, just use channel factories.
You do not need to broadcast the entire internal ledgers of those services, only their customers need to know those internal ledgers, and sign off on the updates of those ledgers.

@_date: 2020-01-14 00:53:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
Because Lightning remains a superior *scalability* solution to microchains.
(The below is a Fermi estimate; it is intended to give an intuition on the rough orders of magnitude that we are discussing, not strict predictions of how the world works)
Let us suppose that N users would produce N * t bytes of transactions.
Under Lightning, that data is sent to a tiny subset of the entire LN.
As Lightning limits routes to at most 20 hops, let us take the worst case and say that under Lightning, those users will force 20 * N * t bytes to be processed globally.
If all users were to use a *single* blockchain, because all users must process all transactions within the blockchain, that will mean everyone has to process N * N * t bytes.
Now the microchain concept is that, we can split the N in half, so instead of a single N * N * t bytes being processed, we get two (N / 2) * (N / 2) * t, or more generally, if there are c chains: c * ((N / c) ^ 2) * t or N * N * t / c.
So for microchains to beat Lightning, you would have to make N * N * t / c < 20 * N * t, or equivalently N / c < 20, i.e. 20 users per sidechain.
If you have as low as 20 users per sidechain, you might as well just use channel factories to host Lightning channels, so channel factories + channels (i.e. Lightning Network) is probably better than having tiny sidechaisn with 20 users each.
Again the above is a very rough Fermi estimate, but it gives you a hint on the orders of magnitude you should consider, i.e. about a few dozen users per sidechain, and a few dozens users in a sidechain is probably not a lot to give security to that sidechain, whereas with Lightning channel factories you can drop onchain any time to upgrade your security to the full mining hashpower (and we hope that the threat of being able to do so is enough to discourage attempts at theft).
What Lightning cannot do is add certain kinds of features other than scalability, for example Turing-complete disasters (RSK) or confidential assets (LBTC).
Sidechains are for features, not scale, so your proposed sidechain concept remains of interest at least as a possible way to anchor sidechains with new features.

@_date: 2020-01-14 02:59:25
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
Is not compatible with:
You are *tr\*sting* that there exists at least ***one*** ***honest*** user per sidechain.
Thus it is not a trustless solution, but a tr\*sted one.
Replacing 1000 tr\*sted Excel tables with 1000 tr\*sted blockchains is the same class of error as replacing the banking system with centralized large-scale blockchains: you gain the drawbacks of blockchains without gaining its benefits.
The security, integrity, and censorship-resistance of Bitcoin is dependent on there existing some sophisticated actors ("persons") who are willing to take on the risk of running fullnodes and providing hashpower.
This is the Risk-Sharing principle, by which the risk of keeping Bitcoin running is spread out among many persons who are willing to keep Bitcoin alive.
The existence of such actors cannot be assured, but it seems to me that fragmenting the entire community of such limited number of actors would not give good risk-sharing within a sidechain.

@_date: 2020-01-14 15:26:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
As well I would like to point out that in order to receive funds, *something* has to be online to get the message that receives the data.
In the blockchain layer this is diffused among all fullnodes.
At the Lightning layer, your direct peer could hold off on failing an incoming payment while you are offline.
Instead, it could simply stall until the outgoing HTLC would reach its timelock anyway.
Then you can come online and then the peer can send the HTLC to you and you can claim it.
This remains noncustodial as the direct peer cannot steal the funds from you.
I believe there was some discussion regarding this on lightning-dev in the past few months.
However, it does require that the peer know that *you* are the final recipient (if not, it would be unable to fail the HTLC as quickly as possible), thus a privacy leak.
In any case *some* node has to be online in order for anyone to receive funds, whether onchain or not: it is simply that a widespread blcokchain is very very likely to have some online node capable of storing the payment until you can come online to process it.
What you propose splits up the fullnodes into many tiny sidechains, such that a sidechain may get stalled and you would be unable to receive a payment anyway while you are offline, because there are far fewer nodes per sidechain in order for such mass sidechains to start beating the raw scaling Lightning brings.

@_date: 2020-01-15 05:46:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
Again, please refer to the previous Fermi estimate: blockchains have bad scaling precisely because every fullnode must know every transaction.
With blockchains, anything that is not a fullnode is trusting something, and the issue of custodiality is always and has always been an issue of trust.
Locking coins is equivalent to burning them, as you are "burning" the opportunity to use those coins elsewhere, e.g. in a JoinMarket maker or Lightning forwarding node.
Proof of locked coins is therefore indistinguishable from proof-of-burn in this sense, and your original proposal is proof-of-locked-coins.
Burning coins is effectively a donation to all HODLers, while locking coins is effectively a donation to all JoinMarket makers and Lightning forwarding nodes (i.e. HODLers too).
Something I have been playing with mentally would be a unidirectional peg in a sidechain.
Burn funds in the mainchain and build a block with equivalent amount in the coinbase of a sidechain.
But I stopped working on sidechains due to the aforementioned lack of scaling they produce: sidechains are for features, and federated sidechains are fine for new features.
The "balancing their books" **is** the peg.
Consider that for example that a sidechain may have 21 million bitcoins instantiated in it, but locked.
In order to unlock *part* of that supply, you have to provably lock funds in the mainchain.
This "moves" coins from mainchain to  sidechain, but in reality there are still 21 million maincoins and 21 million separate sidecoins.
What matters is that there are only 21 million ***user-controllable*** coins in total, some in the mainchain and some in the sidechain.
That is enough for this to be a peg.
Thus, everything the bank does to "balance their books" is in fact a peg to the central-bank issued currency.
Why would accept a sidecoin with degraded security and accepted by fewer people if it is not pegged to BTC?
That immediately kills any network effects you are targeting.
In any case, a project I have been playing with (which I am not pursuing in seriousness and which I will not seriously support, because LN > sidechains) is to combine the mainchain-staking with Basically, on the mainchain, the sidechain is represented by single UTXO that contains all the funds in the sidechain.
That UTXO would then have the same SCRIPT as described in the above linked post.
Mainchain coin owners that want to be included in the staker set can put their staked amount into a UTXO.
The sidechain stakers then confirm the addition of this staker to the staker set by spending the sidechain single UTXO and the entering staker, putting the funds into a new sidechain single UTXO that now includes the entering staker in the signing set.
Sidechain stakers can also redeem their stake back by requesting the staker set, so that the sidechain single UTXO is consumed and spent into a new sidechain single UTXO that removes the leaving staker in the signing set, plus a second UTXO containing the money that the leaving sidechain staker is reclaiming from stake.
Withdraws and deposits into the sidechain use a similar mechanism, except the depositor does not get its pubkey added to the signer set, but its funds are instantiated into the sidechain (the stakers do not have their funds instantiated into the sidechain: the mainchain staked funds and the sidechain "live" funds are thus separated, even though on the mainchain they are combined within the sidechain single UTXO).
Like all federated sidechains this assumes a federation can be formed that can be trusted to not just spend the entire sidechain single UTXO on other funds.
In particular, if the federation is taken over, it can deny the entry of new stakers that would want to evict them.
Thus the security is significantly lower.
(proof-of-work allows existing miners to be evicted, at cost, by deploying more hashpower than the existing miners have: this is central to censorship-resistance on the main blockchain layer)
The stakers that sign on the sidechain single UTXO that appears on the mainchain need not be the same set that determines consensus on the sidechain.
In terms of the Liquid blockchain, the signers on the sidechain single UTXO are the watchmen (who ensure the peg is correct), and need not be the same set as the blocksigners (who advance the sidechain state by authorizing valid blocks).

@_date: 2020-01-16 02:11:44
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] ***UNCHECKED*** Wormhole: Sending and receiving 
Good morning Max,
It seems similar very closely to TumbleBit, at least in the overall protocol.
A cursory read does not reveal any direct problems with it.

@_date: 2020-01-17 13:54:50
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Coins: A trustless sidechain protocol 
Good morning Robin,
Also known as MC = MR.
This is in fact the core of the argument *against* this kind of global microchain system: each individual chain will either:
* Pay ridiculously high fees per transaction, because the microchain has a small number of transactions because that is the entire *point* of microchains.
* Pay insufficient fees per block, making it easy to attack, meaning the security of the chain has to be centralized around a few actors anyway (e.g. checkpoints, like what every altcoin implements), which is not much better than the custodial case you are complaining against.
In order to have a sidechain that is as secure as Bitcoin today, you need:
* Sidechain fees to cover both *current Bitcoin fees* plus *current Bitcoin block rewards*.
Consequently, the sidechain has to have either *more* users than Bitcoin today, or *higher* fees than Bitcoin today.
Unless of course you propose to have the sidechain issue its own coin, in which case it is not much more than an altcoin.
Still, the real-world value of the total block rewards for that altcoin will have to match the real-world value of the total block rewards of Bitcoin in order to have security even approaching Bitcoin.
Only if the consumed resource matches what is consumed under PoW.
Otherwise it is not much better than a low-PoW altcoin, i.e. easily attackable unless it centralizes around the developers.
I understand the desire to smoothen the experience of onboarding new users to Bitcoin.
But this path is not much better than the custodial solutions you are trying to avoid anyway.

@_date: 2020-01-21 04:38:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Payswap 
Good morning list,
[On a recent post on lightning-dev]( I brought up the possibility of using a circular self-payment to hide the actual direction of payment from third-party snooping nodes.
Basically, instead of paying the amount to the destination, we can have an atomic mechanism by which the source pays a larger-than-amount payment to the destination and the destination returns the difference instead.
As the mechanisms on Lightning are also implementable directly on the blockchain, I observe as well that we can use a similar mechanism based on CoinSwap to mislead onchain analysis as well.
The normal payment flow for a Bitoin payer is typically:
* Locate some of its owned UTXO(s) that total an amount equal or greater than the payment amount.
* Create a single transaction that consumes those inputs and outputs the amount to the destination and any remainder to a new address we control.
However, we can observe as well that transactions and transaction outputs can be considered nodes and edges of a transaction graph, respectively.
We can then consider the categorical dual of such a graph.
Let me then present the Payswap payment flow:
* Sender locates some of its owned UTXO(s) that total an amount equal or greater than the payment amount.
  * Sender reveals the sum to destination.
* Destination locates some of its owned UTXO(s) that total an amount equal or greater than the difference (change) of the sender total minus the payment amount.
* Sender and destination set up an unequal CoinSwap:
  * Destination receives all the Sender coins.
  * Sender receives the difference between the Sender total and the payment amount (change).
* Sender and destination execute the CoinSwap and complete the payment protocol.
What appears onchain are:
* A transaction with a single output.
  * This is the CoinSwap funding transaction that was offered by the sender and claimed by the destination.
  * As a single-output transaction, this looks to chain analysis to be a likely self-payment.
* A transaction with two outputs.
  * This is the CoinSwap funding transaction that was offered by the destination and claimed by the sender.
  * The output that goes back to the destination looks like a change output according to chain analysis.
  * The output that goes to the sender looks like a payment from the destination to the sender, reversing the apparent direction of payment and obscuring the amount paid.
* Two more transaction that complete the protocol, each spending one of the above and moving the funds to unilateral control of the destination and sender respectively.
The above is an active misleading of chain analysis.
This is even possible today with 2p-ECDSA to make it use P2WPKH with Scriptless Script.
Against the above flow I must caution:
* This involves more transactions than Payjoin, thus more expensive in blockspace.
* The protocol can be aborted by one participant, which will lead to spending onchain fees to back out of the protocol, unlike Payjoin which is atomic with paying onchain fees.
* As I [point out elsewhere]( CoinSwap overhead approaches the overhead of setting up a temporary Lightning Network channel, thus it might actually be better to implement all CoinSwap protocols over Lightning instead.

@_date: 2020-01-24 10:11:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Payswap 
Good morning list,
A few more things to consider:
Probably the correct order for this would be for the payer to instantiate the Scriptless Script payment to the payee first, then the payee instantiating the change back to the payer.
By use of some kind of Scriptless Script, it is possible to as well implement a proof-of-payment system similar to Lightning, which might be useful to prove to an auditor that a payment has been made (without being forced to reveal this to anyone other than the auditor).
Both the payer and payee can generate a scalar.
The payee provides a signed invoice (can reuse the Lightning BOLT11 invoice format) attesting the payment point that needs to be paid, the payment point being the generator point times te payee scalar.
The payer then instantiates the Scriptless Script PTLC, requiring knowledge of the payee scalar + the payer scalar from the payee.
Then the payee instantiates the change Scriptless Script PTLC, requiring knowledge of the payer scalar from the payer.
Then the payee claims the change, which allows the payer to claim the full transfer while revealing the payee scalar as a proof-of-payment.

@_date: 2020-01-31 03:42:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Onchain fee insurance mechanism 
Good morning list,
During LNConf 2019, Jack Mallers presented about hedging of onchain fees, which he argues is necessary in order to have a smooth experience interfacing between onchain and offchain (in particular, when closing and opening channels).
The exact mechanism proposed was to construct a futures market on onchain feerates, with miners taking short positions on fees (they are naturally long on fees) while users take long positions on fees (they are naturally short on fees).
I observe that what the users effectively experience is similar to a constant feerate (indeed the positions the user takes up are arranged such that the user takes a constant feerate for a vast majority of the expected future outcomes).
This is effectively an insurance policy against increases in feerate.
Let me then propose a specific mechanism for feerate insurance against onchain feerate spikes.
Let us suppose that the user Alice, has 500000 satoshi, and has to pay Bob 400000 satoshi.
Further, Alice and Bob have a fee insurance provider Ingrid.
Ingrid assures Alice and Bob that 600 satoshi is sufficient to confirm the payment from Alice to Bob within 4 blocks.
Ingrid also happens to have a 800000 satoshi output lying around.
At current blockheight B, Alice and Ingrid then arrange a series of transactions:
    nLockTime: B+1
    nSequence: RBF enabled, no relative locktime.
    inputs: Alice 5000000, Ingrid 800000
    outputs:
        Bob 400000
        Alice 99400
        Ingrid 800400
    fee: 200
    nLockTime: B+2
    nSequence: RBF enabled, no relative locktime.
    inputs: Alice 5000000, Ingrid 800000
    outputs:
        Bob 400000
        Alice 99400
        Ingrid 800200
    fee: 400
    nLockTime: B+3
    nSequence: RBF enabled, no relative locktime.
    inputs: Alice 5000000, Ingrid 800000
    outputs:
        Bob 400000
        Alice 99400
        Ingrid 800001
    fee: 599
    nLockTime: B+4
    nSequence: RBF enabled, no relative locktime.
    inputs: Alice 5000000, Ingrid 800000
    outputs:
        Bob 400000
        Alice 99400
        Ingrid 797000
    fee: 3600
Basically, if the first transaction is able to be included in the next block immediately, then Ingrid is able to earn the most of the fee paid by Alice.
However, as more blocks pass without the transaction getting committed, the transaction in the mempool is replaced by transactions that bump up the fee, until the time limit is reached and Ingrid pays out significantly in order to ensure the payment pushes through.
As far as I can tell, this mechanism will also work for CPFP-style transactions.
In general, the insurance provider Ingrid will be a miner.
In the original context, this is generally about fast confirmation of channel closes.
At the time that a commitment transaction is made, it is uncertain if the feerate for it would still remain valid for some future spike.
The safest is that, if the counterparty is offline, if the feerate spikes, we should drop the channel unilaterally onchain before it rises so high that the commitment transaction cannot be confirmed in a timely manner.
However, if the feerate then settles lower afterwards, we have already broadcasted the channel closure will no longer be able to use the fund on Lightning.
Unfortunately, the mechanism described above requires absolute locktimes, which would impose a maximum lifetime on channels, which we would like to avoid.
Thus, the mechanism cannot be used for Lightning closes.
For the Lightning case, what we want is something like:
* Ingrid assures Alice and Bob that the close transaction can be confirmed at any time, for only N satoshi.
The previous mechanism described is nearer to:
* Ingrid assures Alice that the transaction can be confirmed up to B blocks from now, for only N satoshi.
The issue is that relative locktimes require that a transaction be confirmed, and it is transaction itself that we want to assure.
Thus, we cannot use relative locktimes for any kind of fee-insurance mechanism.
Thus, we must somehow tie down the blockheight at which we start our countdown, and so we cannot use this for Lightning closes, since Lightning closes must be freely doable at any time.
Still, the mechanism might be useful for onchain transactions to help reassure users (which is why I send this post to bitcoin-dev).

@_date: 2020-07-01 00:53:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Is Bitcoin mempool synchronized? 
Good morning Hilda,
No, definitely not.
There is no good way to limit the amount of transactions someone can push at you, except by various heuristics.
Yet those very same heuristics mean that someone with a good knowledge of those heuristics can make your mempool desynchronized with that of somebody else.
Fortunately for Bitcoin, it is the blockchain itself that we synchronize on.
People cannot push blocks at you without doing the work of grinding towards the difficulty target, thus it is not possible to spam blocks.
TANSTAAGM - There Ain't No Such Thing As A Global Mempool
For this reason, any consensus rule has to refer only to data inside blocks, and never to data in mempools, are mempools are ephemeral and not synchronized across all nodes.

@_date: 2020-07-01 16:58:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Tejaswi,
By my reading, it seems to me that you divide miners into "weak" and "powerful".
Weak miners have lower hashrate than powerful ones.
The dividing point depends on how much Alice and Bob fees are.
If the hashrate share of a miner is less than the ratio of Alice (honest) fee to Bob (bribing) fee, then the miner is weak.
And your paper posits that if a miner is weak, its best strategy is to take the myopic strategy and include the currently-valid Alice transaction.
Thus, if Alice even *matches* Bob, it seems to me that this ratio f / b is 1.0 implying a miner can only be powerful if it has already 51%-attacked Bitcoin (which tends to invalidate all our security assumptions of higher-layer protocols anyway, since a 51% attacker can censor anything with impunity).
Of course, Bob can offer up to the entire fund amount, for free, to miners as a bribe, without loss to Bob.
For more realistic scenarios where no miner has 100% hashrate, then Alice can make all miners weak by being willing to pay up to 50% of the fund as fee, as a miner that achieves greater than 50% hashrate share would already effectively pwnzored Bitcoin and gained UNLIMITED POWAH anyway.
So it looks to me that scorched-earth is a possible mitigation against this attack.
Another analysis, similar but a little off-tangent to yours, would be to consider miners as a breeding group with various strategies, and see which one is able to gain more utilons (with which it creates more miners) and outbreed the other miners.
This models the fact that miners can use their earnings to reinvest into their mining operations and increase their mining hashrate, and the amount they can reinvest is proportional to their earnings.
A miner that "gives birth" to a child miner with the same strategy is, in the so-called "real world", simply a miner that has earned enough and reinvested those earnings to double the hashrate of their business (which, logically speaking, would use the same strategy throughout the entire business).
Let us start with a population of 4 miners, 3 of which follow the non-myopic strategy, and the remaining following the myopic strategy.
Let us postulate that all miners have the same unit hashrate.
Thus, this starting population is 75% non-myopic, 25% myopic.
If there exists a timelocked bribe, then if non-myopic miner is chosen at a block, it will have to sacrifice the Alice fee minus whatever lesser transaction fee it can replace in its block.
If the Alice transaction is successfully delayed until the Bob transaction is valid, then the non-myopic miners can get the Bob transaction confirmed.
However, even in the case that the Alice transaction is delayed, the myopic miner still has its 25% chance --- equal to the 25% chance of the three non-myopic miners --- to confirm the Bob transaction and earn the increased bribe that Bob offers.
Thus, the non-myopic miners can end up sacrificing fee earnings, and in the end the myopic miner still has the 25% chance to get the Bob transaction fee later when it becomes valid.
So the non-myopic miners do not impose any loss on myopic miners.
On the other hand, if the non-myopic miners sacrificed their chances to include the Alice transaction in the hope of getting the later 25% chance to get the Bob higher-fee timelocked transaction, and then the myopic miner gets the next block, the myopic miner gets the Alice transaction confirmed and the 25% chance to get the Bob higher fee is lost by the non-myopic miners.
Thus, the myopic miner is able to impose costs on their non-myopic competitors.
So even if by chance for the entire locktime, only the non-myopic miners are selected, the myopic miner still retains its 25% chance of getting the block at locktime + 1 and confirming and earning the bigger Bob fee.
Thus, we expect that the myopic miner will earn more than 25% of subsidies and fees than the non-myopic miners, in such a mixed environment.
We can then consider that the myopic miner, being able to earn more, is able to increase its progeny (i.e. expand its mining business and inspire new miners to follow its strategy towards success) faster than the non-myopic miners.
We can thus conclude that the myopic miners will eventually dominate over the breeding population and drive the non-myopic miners to near-extinction.
It is helpful to remember that rationality is about success *in the universe you exist in*.
While miners may step back and consider that, ***if*** all of them were to use non-myopic strategy, they would all earn more, the fact of the matter is that each miner works for themselves, and themselves alone, in a highly competitive environment.
Thus, even though they know *all of them* will benefit if they use the non-myopic strategy, they cannot be sure, unless they are all perfectly synchronized mind-clones of each other, that the other miners will rather be selfish and mine for themselves, even if in the end every miner earns less
The standard for success is to earn more *than your competitors*, not ensure that *every* miner earns more.
Fortunately, since miners are running a business, this competition leads to better services to the the customers of the mining business, a known phenomenon of the free market, yay free market greed is good.
The user Alice is a customer of the mining business.
Alice gets, as a side effect of this competitiveness of miners (which leads to miners adopting myopic strategies in order to gain an edge over non-myopic miners), improved security of their HTLCs without requiring slashable fidelity bonds or such-like that MAD-HTLC proposes.
Using this model, it seems to me that non-myopic miners can only maintain hold over the blockchain if all miners agree to use non-myopic strategy.
This is basically all miners forming a cartel / monopoly, which we know is detrimental to customers of the monopoly, and is the reason why we prefer decentralization.

@_date: 2020-07-02 16:06:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Tejaswi,
Because your model only considers that a block might have only 0 or 1 transactions, and there is no such thing as a mempool containing alternative, fee-paying transactions that the miner could include *instead*.
In reality, what a miner can earn from adding Alice transaction is the *difference* between the Alice transaction fee and the transaction that *just* misses getting included in the block because of feerate.
Thus, the f will not, in fact, *quite* be the Alice fee, but instead less than that.
Indeed if the Alice transaction fee is lower than the top 4 Mweight transactions in the mempool, the miner would be *losing* funds by including the Alice transaction.
My understanding is that we expect mempools to eventually never empty, as the block subsidy reduces over time, thus the payoff f for including the Alice transaction *instead of* some other transaction will be less than the Alice fee.
This effect also holds for Bob, but we can probably expect, all things being equal, that approximately the same value will be deducted from both the Bob bribe and Alice fee by the mempool effect.
Thus the ratio should really be (f - x) / (b - x), where x is the fee-of-transaction-that-just-misses-the-block.
At fee spikes, this x will go higher, and thus (f - x) / (b - x) will be far smaller than f / b and might even become negative, in which case the Alice transaction will not be confirmed even by myopic miners, because the Alice transaction will be below the top 4Mweight transactions in the mempool.
So it seems to me reasonable to use a *gradual* scorched earth policy, as it is not only resilient against this attack, but also to fee spikes.
Alice starts at the 1% reserve, then for every block that goes by, bumps up the fee.
Then Alice will settle at an (f - x) / (b - x) level that achieves the least weak miner that is known to run the myopic strategy.
I believe this is also better for UX --- people already accept that during high fee spikes, they end up paying more for onchain activities.
But boosting up `to_self_delay` is bad because it makes honest unilateral closes take longer, and we already get frownie faces from users about this parameter.
By using a gradual scorched-earth strategy we can start at the reserve level, and if we are not under attack and there is no fee spike, do not lose anything other than the reserve funds of the thief (which is not ours, but is instead that of the thief).
But if an attack happens during a fee spike, then even though we retain our current default `to_self_delay` of 144, we still have the ability to gradually and automatically move to higher fee regions until our transaction confirms, and we have a good excuse for it to present to users: "a fee spike was happening at the time, so you had to pay some extra miner fees".
And since you and your paper openly discusses it anyway, I would like to reveal that the MAD-HTLC argument does not apply to *just* HTLCs.
You make recommendations about `to_self_delay` and `channel_reserve_satoshis`, which are not parameters of Lightning HTLCs (those are stuff like `cltv_delta` and `final_cltv`), but are channel parameters.
The MAD-HTLC argument applies just as well to channel mechanisms themselves, ***independently of*** any HTLCs they transport.
The MAD-HTLC paper has the following core argument:
* We currently assume that currently-valid transactions will inevitably supersede alternate transactions that are valid at a later block height, simply because of the time advantage.
  * However, the owner of a later-block-height transaction can bribe miners to defer confirmation of currently-valid transactions, until its later-block-height transaction is valid and confirms.
The above core argument is presented as applying to HTLCs.
However, the same argument actually **also** applies to all current offchain multiparticipant cryptocurrency systems (i.e. "channel mechanisms").
* Spilman
* Poon-Dryja (what we currently use in Lightning)
* Decker-Wattenhofer decrementing-`nSequence`
* Decker-Russell-Osuntokun
The [Khabbazian-Nadahalli-Wattenhofer "Timelocked Bribing" paper]( mentions the use of revoked transactions in a Poon-Dryja mechanism, but seems to imply that the issue is with the HTLC instantiated inside the revoked transaction.
But note that the paper describes recommendations for the `to_self_delay` parameter and also analyzes the `channel_reserve_satoshis` parameter, which are parameters of the ***Poon-Dryja*** mechanism, and **not** of the HTLCs instantiated inside it.
So, to be very clear, the MAD-HTLC argument applies to all the above mechanisms *even if HTLCs are not used at all*.
Or put another way, if you use a modern offchain updateable cryptocurrency system at all, you are still vulnerable to the MAD-HTLC argument even if you never instantiate HTLCs inside the offchain system.
Thus, other proposed systems that (could) use any of the channel mechanisms, but do ***not*** necessarily use HTLCs, such as CoinPools, channel factories, and statechains, are also vulnerable to the MAD-HTLC argument.
In particular, if the MAD-HTLC argument holds, we should take note that e.g. Lightning channels have to be at least as large as any HTLC they contain, and since the MAD-HTLC argument applies to the channel itself (in addition to any HTLCs they contain), the application of that argument implies greater loss, as it is the entire channel that is at risk, not just any HTLCs it might contain.
A Spilman channel is a unidirectional single-funded channel.
The overall idea was presented pre-SegWit, and needed `OP_CHECKLOCKTIMEVERIFY` to be malleation-safe.
I will describe here a modernized version that uses SegWit (and thus is malleation safe) instead.
Suppose Bob wishes to make a Spilman channel to Alice.
The setup is as follows:
* Bob creates but does *NOT* sign a funding transaction, paying out to a 2-of-2 between Alice and Bob, and hands over this txid and the output number to Alice.
* Alice creates a timeout transaction, `nLockTime`d to a pre-agreed locktime, spending the above txout, and returning the funds to Bob, and signs this transaction and hands over the signature and tx to Bob.
* Bob signs the funding transaction and broadcasts it.
* Alice and Bob wait for deep confirmation of the funding tx.
At each payment from Bob to Alice, Bob signs a non-`nLockTime`d (or one with current blockheight) transaction that spends the funding txout and assigns more of the fund to Alice, then sends the signature and tx to Alice.
At any time, Alice can unilaterally close the channel using any of the signatures given by Bob.
Rationally, it will publish the one that gives it the most money, which is the latest such transaction, thus leading to the unidirectional nature of Spilman channels.
Alice needs to perform this unilateral close far before the pre-agreed locktime.
Under the MAD-HTLC argument, Bob can bribe miners to ignore the Alice unilateral close transaction, and the initial timeout transaction by Bob gets confirmed even if within the channel mechanism Alice is supposed to own most or all of the funds.
A Poon-Dryja channel is a modern two-participant bidirectional channel.
The core of security of Poon-Dryja involves "revocable outputs".
A revocable output is an output that, when published onchain, is owned by one entity (the owner), but that entity may reveal a secret, the revocation secret, to another entity (the revoker).
Once that other entity knows the revocation secret, if the output is ever published onchain, it can revoke the output and claim its value.
Poon-Dryja uses this building block to implement an updateable state.
All states are represented by commitment transactions that have revocable outputs.
In order to advance to a new state, the revocable outputs of previous states are revoked by exchanging revocation secrets.
Thus, the security of Poon-Dryja is dependent on the correct operation of revocation.
Revocable outputs are implemented by imposing a relative locktime on the owner of the output, and requiring knowledge of two secrets from the revoker.
Thus, a revocable output has two branches:
* Revocation branch: with the revoker privkey and knowledge of a revocaation secret, the revoker can claim the fund immediately.
* Claim branch: with the owner privkey and a relative locktime, the owner can claim the fund after a pre-agreed number of blocks (`to_self_delay` in Lightning) since the output is confirmed onchain.
Under the MAD-HTLC argument, the owner of the revoked output can bribe miners to ignore attempts by the revoker to claim the funds until the claim branch is valid and confirmable.
Thus, a thief can publish old state, then apply the MAD-HTLC argument to get miners to ignore the revoker of the old state.
Decker-Wattenhofer decrementing-`nSequence`
Decker-Wattenhofer ("Duplex Micropayment Channels") is a modern multi-participant (N >= 2) offchain updateable cryptocurrency mechanism.
Decker-Wattenhofer chains together two different mechanisms, embedding them one inside the other, in order to balance the tradeoffs of one with the tradeoffs of the other.
* One or more decrementing-`nSequence` mechanisms, chained one inside the other.
* Two ("duplex") unidirectional Spilman variants, using a relative locktime instead of an absolute locktime, one in both directions of the channel, inside the innermost decrementing-`nSequence` mechanism.
The decrementing-`nSequence` mechanisms by themselves are multiparticipant (N >= 2), and if we focus only on having one or more of these mechanisms chained together, we can consider Decker-Wattenhofer as multiparticipant.
In the decrementing-`nSequence` mechanism, there is a kickoff transaction which spends from the n-of-n funding outpoint, and sends it to yet another n-of-n output between the participants.
Then, the second n-of-n is spent by a transaction with a relative-locktime `nSequence` transaction, which then distributes the money among various participants.
When a new state is created, the participants create and sign a new relative-locktime `nSequence` transaction spending the kickoff n-of-n outpoint.
The new state transaction has a lower `nSequence` than the most previous state transaction, hence decrementing-`nSequence`.
Once the latest state transaction has a 0-block relative locktime, a newer state can no longer be added to the mechanism.
The kickoff n-of-n outpoint thus has multiple branches, one for each created state.
The most recent state is assumed to supersede previous states, because it has the smallest relative locktime among all states.
Under the MAD-HTLC argument, a participant which prefers an older state can bribe miners to defer confirmation of all more recent states.
Thus, that participant can publish the kickoff and bribe miners to defer more recent states until its preferred state is confirmable onchain.
Decker-Russell-Osuntokun ("eltoo") is a futuristic multiparticipant (N >= 2) offchain updateable cryptocurrency system.
Decker-Russell-Osuntokun uses a proposed new `SIGHASH_NOINPUT` flag, which does not commit to the specific output being spent, allowing a signature that signs using `SIGHASH_NOINPUT` to be used to spend a different transaction outpoint, as long as the same pubkey is used for that outpoint.
As is typical for channel mechanisms, a funding outpoint is created, which is an n-of-n of all participants.
The funding outpoint is spent by an update transaction with a single output, which has the following branches:
* Update branch: can be spent by the same n-of-n pubkeys as the funding outpoint, as long as the spending transaction has a higher `nLockTime` than the update transaction.
* State branch: can be spent by a different n-of-n pubkeys from the same participants, after a relative locktime.
  * Each update transaction has its own unique set of n-of-n pubkeys for the state branch, given by the same participant set.
Of note is that the `nLockTime` used in Decker-Russell-Osuntokun are always past `nLockTime`s, so that the update branch is always confirmable at the current tip, from now until forever.
Only the state branch has an actual timelock that could prevent immediate confirmation of a transaction spending that branch.
Update transactions (awesomely mis)use `nLockTime` as a sequence number; the first update transaction has the lowest `nLockTime`, then each succeeding update transaction has a higher `nLockTime`, until they reach the present time.
Update transactions are signed with `SIGHASH_NOINPUT`.
This allows the update transaction to not only spend the funding outpoint itself, but also to spend any previous update transaction.
Thus, if an old update transaction is published onchain, its output can be re-spent by any newer update transaction before the state transaction for that update can come into play.
Any other participant who notices this event can simply publish the newest update transaction it knows, as that would supersede the state transaction, which can only be confirmed after a time delay.
Under the MAD-HTLC argument, a participant who prefers an older state can publish the update transaction for the older state, then bribe miners to defer confirmation of newer update transactions, until the state transaction for that update transaction can be confirmed.
All the above mechanisms use a timelock, and implicitly have the assumption that "a transaction, that can be confirmed now, supersedes any transaction that has a timelock that forces it to be confirmed later".
It seems likely to me that even future mechanisms will use the same assumption as well.
In particular, many proposed mechanisms for non-federated sidechains often include some kind of delay between when a sidechain coin is burned and the corresponding mainchain coin is released (i.e. side-to-main peg).
Often, this delay exists in order to allow showing of a counterproof that the supposed side-to-main transfer did not actually exist in the sidechain (or was later reorged out, or whatever).
It seems to me that the MAD-HTLC argument would also apply to such mechanisms (if anyone still wants to go push sidechains, anyway).
Thus, we really need to carefully investigate the MAD-HTLC argument.
My current analysis suggests that in practice, the MAD-HTLC argument does not apply at all (else I would not be revealing that all channel mechanisms are broken **if** the MAD-HTLC argument *does* apply), since the myopic strategy seems to be pretty much inevitably dominant at stable states.
But it would still be best to investigate further until we are fully convinced that the MAD-HTLC argument ("'earlier supersedes later' might be falsified by bribery") does not apply.

@_date: 2020-07-02 16:06:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Tejaswi,
Because your model only considers that a block might have only 0 or 1 transactions, and there is no such thing as a mempool containing alternative, fee-paying transactions that the miner could include *instead*.
In reality, what a miner can earn from adding Alice transaction is the *difference* between the Alice transaction fee and the transaction that *just* misses getting included in the block because of feerate.
Thus, the f will not, in fact, *quite* be the Alice fee, but instead less than that.
Indeed if the Alice transaction fee is lower than the top 4 Mweight transactions in the mempool, the miner would be *losing* funds by including the Alice transaction.
My understanding is that we expect mempools to eventually never empty, as the block subsidy reduces over time, thus the payoff f for including the Alice transaction *instead of* some other transaction will be less than the Alice fee.
This effect also holds for Bob, but we can probably expect, all things being equal, that approximately the same value will be deducted from both the Bob bribe and Alice fee by the mempool effect.
Thus the ratio should really be (f - x) / (b - x), where x is the fee-of-transaction-that-just-misses-the-block.
At fee spikes, this x will go higher, and thus (f - x) / (b - x) will be far smaller than f / b and might even become negative, in which case the Alice transaction will not be confirmed even by myopic miners, because the Alice transaction will be below the top 4Mweight transactions in the mempool.
So it seems to me reasonable to use a *gradual* scorched earth policy, as it is not only resilient against this attack, but also to fee spikes.
Alice starts at the 1% reserve, then for every block that goes by, bumps up the fee.
Then Alice will settle at an (f - x) / (b - x) level that achieves the least weak miner that is known to run the myopic strategy.
I believe this is also better for UX --- people already accept that during high fee spikes, they end up paying more for onchain activities.
But boosting up `to_self_delay` is bad because it makes honest unilateral closes take longer, and we already get frownie faces from users about this parameter.
By using a gradual scorched-earth strategy we can start at the reserve level, and if we are not under attack and there is no fee spike, do not lose anything other than the reserve funds of the thief (which is not ours, but is instead that of the thief).
But if an attack happens during a fee spike, then even though we retain our current default `to_self_delay` of 144, we still have the ability to gradually and automatically move to higher fee regions until our transaction confirms, and we have a good excuse for it to present to users: "a fee spike was happening at the time, so you had to pay some extra miner fees".
And since you and your paper openly discusses it anyway, I would like to reveal that the MAD-HTLC argument does not apply to *just* HTLCs.
You make recommendations about `to_self_delay` and `channel_reserve_satoshis`, which are not parameters of Lightning HTLCs (those are stuff like `cltv_delta` and `final_cltv`), but are channel parameters.
The MAD-HTLC argument applies just as well to channel mechanisms themselves, ***independently of*** any HTLCs they transport.
The MAD-HTLC paper has the following core argument:
* We currently assume that currently-valid transactions will inevitably supersede alternate transactions that are valid at a later block height, simply because of the time advantage.
  * However, the owner of a later-block-height transaction can bribe miners to defer confirmation of currently-valid transactions, until its later-block-height transaction is valid and confirms.
The above core argument is presented as applying to HTLCs.
However, the same argument actually **also** applies to all current offchain multiparticipant cryptocurrency systems (i.e. "channel mechanisms").
* Spilman
* Poon-Dryja (what we currently use in Lightning)
* Decker-Wattenhofer decrementing-`nSequence`
* Decker-Russell-Osuntokun
The [Khabbazian-Nadahalli-Wattenhofer "Timelocked Bribing" paper]( mentions the use of revoked transactions in a Poon-Dryja mechanism, but seems to imply that the issue is with the HTLC instantiated inside the revoked transaction.
But note that the paper describes recommendations for the `to_self_delay` parameter and also analyzes the `channel_reserve_satoshis` parameter, which are parameters of the ***Poon-Dryja*** mechanism, and **not** of the HTLCs instantiated inside it.
So, to be very clear, the MAD-HTLC argument applies to all the above mechanisms *even if HTLCs are not used at all*.
Or put another way, if you use a modern offchain updateable cryptocurrency system at all, you are still vulnerable to the MAD-HTLC argument even if you never instantiate HTLCs inside the offchain system.
Thus, other proposed systems that (could) use any of the channel mechanisms, but do ***not*** necessarily use HTLCs, such as CoinPools, channel factories, and statechains, are also vulnerable to the MAD-HTLC argument.
In particular, if the MAD-HTLC argument holds, we should take note that e.g. Lightning channels have to be at least as large as any HTLC they contain, and since the MAD-HTLC argument applies to the channel itself (in addition to any HTLCs they contain), the application of that argument implies greater loss, as it is the entire channel that is at risk, not just any HTLCs it might contain.
A Spilman channel is a unidirectional single-funded channel.
The overall idea was presented pre-SegWit, and needed `OP_CHECKLOCKTIMEVERIFY` to be malleation-safe.
I will describe here a modernized version that uses SegWit (and thus is malleation safe) instead.
Suppose Bob wishes to make a Spilman channel to Alice.
The setup is as follows:
* Bob creates but does *NOT* sign a funding transaction, paying out to a 2-of-2 between Alice and Bob, and hands over this txid and the output number to Alice.
* Alice creates a timeout transaction, `nLockTime`d to a pre-agreed locktime, spending the above txout, and returning the funds to Bob, and signs this transaction and hands over the signature and tx to Bob.
* Bob signs the funding transaction and broadcasts it.
* Alice and Bob wait for deep confirmation of the funding tx.
At each payment from Bob to Alice, Bob signs a non-`nLockTime`d (or one with current blockheight) transaction that spends the funding txout and assigns more of the fund to Alice, then sends the signature and tx to Alice.
At any time, Alice can unilaterally close the channel using any of the signatures given by Bob.
Rationally, it will publish the one that gives it the most money, which is the latest such transaction, thus leading to the unidirectional nature of Spilman channels.
Alice needs to perform this unilateral close far before the pre-agreed locktime.
Under the MAD-HTLC argument, Bob can bribe miners to ignore the Alice unilateral close transaction, and the initial timeout transaction by Bob gets confirmed even if within the channel mechanism Alice is supposed to own most or all of the funds.
A Poon-Dryja channel is a modern two-participant bidirectional channel.
The core of security of Poon-Dryja involves "revocable outputs".
A revocable output is an output that, when published onchain, is owned by one entity (the owner), but that entity may reveal a secret, the revocation secret, to another entity (the revoker).
Once that other entity knows the revocation secret, if the output is ever published onchain, it can revoke the output and claim its value.
Poon-Dryja uses this building block to implement an updateable state.
All states are represented by commitment transactions that have revocable outputs.
In order to advance to a new state, the revocable outputs of previous states are revoked by exchanging revocation secrets.
Thus, the security of Poon-Dryja is dependent on the correct operation of revocation.
Revocable outputs are implemented by imposing a relative locktime on the owner of the output, and requiring knowledge of two secrets from the revoker.
Thus, a revocable output has two branches:
* Revocation branch: with the revoker privkey and knowledge of a revocaation secret, the revoker can claim the fund immediately.
* Claim branch: with the owner privkey and a relative locktime, the owner can claim the fund after a pre-agreed number of blocks (`to_self_delay` in Lightning) since the output is confirmed onchain.
Under the MAD-HTLC argument, the owner of the revoked output can bribe miners to ignore attempts by the revoker to claim the funds until the claim branch is valid and confirmable.
Thus, a thief can publish old state, then apply the MAD-HTLC argument to get miners to ignore the revoker of the old state.
Decker-Wattenhofer decrementing-`nSequence`
Decker-Wattenhofer ("Duplex Micropayment Channels") is a modern multi-participant (N >= 2) offchain updateable cryptocurrency mechanism.
Decker-Wattenhofer chains together two different mechanisms, embedding them one inside the other, in order to balance the tradeoffs of one with the tradeoffs of the other.
* One or more decrementing-`nSequence` mechanisms, chained one inside the other.
* Two ("duplex") unidirectional Spilman variants, using a relative locktime instead of an absolute locktime, one in both directions of the channel, inside the innermost decrementing-`nSequence` mechanism.
The decrementing-`nSequence` mechanisms by themselves are multiparticipant (N >= 2), and if we focus only on having one or more of these mechanisms chained together, we can consider Decker-Wattenhofer as multiparticipant.
In the decrementing-`nSequence` mechanism, there is a kickoff transaction which spends from the n-of-n funding outpoint, and sends it to yet another n-of-n output between the participants.
Then, the second n-of-n is spent by a transaction with a relative-locktime `nSequence` transaction, which then distributes the money among various participants.
When a new state is created, the participants create and sign a new relative-locktime `nSequence` transaction spending the kickoff n-of-n outpoint.
The new state transaction has a lower `nSequence` than the most previous state transaction, hence decrementing-`nSequence`.
Once the latest state transaction has a 0-block relative locktime, a newer state can no longer be added to the mechanism.
The kickoff n-of-n outpoint thus has multiple branches, one for each created state.
The most recent state is assumed to supersede previous states, because it has the smallest relative locktime among all states.
Under the MAD-HTLC argument, a participant which prefers an older state can bribe miners to defer confirmation of all more recent states.
Thus, that participant can publish the kickoff and bribe miners to defer more recent states until its preferred state is confirmable onchain.
Decker-Russell-Osuntokun ("eltoo") is a futuristic multiparticipant (N >= 2) offchain updateable cryptocurrency system.
Decker-Russell-Osuntokun uses a proposed new `SIGHASH_NOINPUT` flag, which does not commit to the specific output being spent, allowing a signature that signs using `SIGHASH_NOINPUT` to be used to spend a different transaction outpoint, as long as the same pubkey is used for that outpoint.
As is typical for channel mechanisms, a funding outpoint is created, which is an n-of-n of all participants.
The funding outpoint is spent by an update transaction with a single output, which has the following branches:
* Update branch: can be spent by the same n-of-n pubkeys as the funding outpoint, as long as the spending transaction has a higher `nLockTime` than the update transaction.
* State branch: can be spent by a different n-of-n pubkeys from the same participants, after a relative locktime.
  * Each update transaction has its own unique set of n-of-n pubkeys for the state branch, given by the same participant set.
Of note is that the `nLockTime` used in Decker-Russell-Osuntokun are always past `nLockTime`s, so that the update branch is always confirmable at the current tip, from now until forever.
Only the state branch has an actual timelock that could prevent immediate confirmation of a transaction spending that branch.
Update transactions (awesomely mis)use `nLockTime` as a sequence number; the first update transaction has the lowest `nLockTime`, then each succeeding update transaction has a higher `nLockTime`, until they reach the present time.
Update transactions are signed with `SIGHASH_NOINPUT`.
This allows the update transaction to not only spend the funding outpoint itself, but also to spend any previous update transaction.
Thus, if an old update transaction is published onchain, its output can be re-spent by any newer update transaction before the state transaction for that update can come into play.
Any other participant who notices this event can simply publish the newest update transaction it knows, as that would supersede the state transaction, which can only be confirmed after a time delay.
Under the MAD-HTLC argument, a participant who prefers an older state can publish the update transaction for the older state, then bribe miners to defer confirmation of newer update transactions, until the state transaction for that update transaction can be confirmed.
All the above mechanisms use a timelock, and implicitly have the assumption that "a transaction, that can be confirmed now, supersedes any transaction that has a timelock that forces it to be confirmed later".
It seems likely to me that even future mechanisms will use the same assumption as well.
In particular, many proposed mechanisms for non-federated sidechains often include some kind of delay between when a sidechain coin is burned and the corresponding mainchain coin is released (i.e. side-to-main peg).
Often, this delay exists in order to allow showing of a counterproof that the supposed side-to-main transfer did not actually exist in the sidechain (or was later reorged out, or whatever).
It seems to me that the MAD-HTLC argument would also apply to such mechanisms (if anyone still wants to go push sidechains, anyway).
Thus, we really need to carefully investigate the MAD-HTLC argument.
My current analysis suggests that in practice, the MAD-HTLC argument does not apply at all (else I would not be revealing that all channel mechanisms are broken **if** the MAD-HTLC argument *does* apply), since the myopic strategy seems to be pretty much inevitably dominant at stable states.
But it would still be best to investigate further until we are fully convinced that the MAD-HTLC argument ("'earlier supersedes later' might be falsified by bribery") does not apply.

@_date: 2020-07-03 10:16:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Tejaswi,
Currently, we say `to_self_delay` is a parameter of how long you can be offline, and is imposed on your counterparty (so its effect on you is to allow the counterparty to safely be offline for that long).
This explanation seems palatable to users; they understand that it is the counterparty which is asking this of them, and that they ask a similar number of their counterparty, which is also their own protection.
On the other hand, we do not really expect to get beyond 1% unless we are under attack, *or* the fee spikes are really, really bad.
So this seems a practical tradeoff for us over in Lightning-land.
Yes, I realized it a little after reading MAD-HTLC that it applied to all the other known channel mechanisms as well, not just HTLCs, and decided to investigate this topic further, and have been circumspect regarding this.
Yes, I agree that the hashlock is an incidental artefact.
What MAD-HTLC questions is our assumption that a valid transaction with an earlier locktime supersedes a valid transaction spending the same txout with a later locktime.
Whether it involves presigned transactions or hashlocks are incidental artefacts.
So for example, a SCRIPT `OP_IF  OP_ELSE <1 day> OP_CHECKSEQUENCEVERIFY OP_DROP  OP_ENDIF OP_CHECKSIG` would also be vulnerable to the MAD-HTLC argument.
(Indeed, BOLT spec uses something very much like that script, now that I look at it again; in our case the `` is a combination of keys from both parties, that cannot be signed with unless one party knows both sub-keys.)
I suggested that, until `SIGHASH_ANYPREVOUT` gets enabled, the Decker-Wattenhofer construction (removing the duplex Spilman-like channels at the end and leaving just the decrementing-`nSequence` constructions) could be used for Ruben Somsen StateChains, so you might not want to dismiss that so readily.
The decrementing-`nSequence` mechanisms have the advantage that it does not require a punishment/revocation branch, similar to Decker-Russell-Osuntokun "eltoo", and thus would work just as well to implement statechains, at least until all the debates around `SIGHASH_ANYPREVOUT` settle and it gets deployed.
Similarly, the proposed CoinPool as well could be implemented using Decker-Wattenhofer, assuming  it gets any details or support behind it sufficiently to push it before `SIGHASH_ANYPREVOUT` gets deployed.
I think a better heuristic is that, if the logic of the construction assumes "transaction with earlier locktime supersedes transaction with later locktime", then it is timelocked-bribery-vulnerable.

@_date: 2020-07-03 10:16:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Tejaswi,
Currently, we say `to_self_delay` is a parameter of how long you can be offline, and is imposed on your counterparty (so its effect on you is to allow the counterparty to safely be offline for that long).
This explanation seems palatable to users; they understand that it is the counterparty which is asking this of them, and that they ask a similar number of their counterparty, which is also their own protection.
On the other hand, we do not really expect to get beyond 1% unless we are under attack, *or* the fee spikes are really, really bad.
So this seems a practical tradeoff for us over in Lightning-land.
Yes, I realized it a little after reading MAD-HTLC that it applied to all the other known channel mechanisms as well, not just HTLCs, and decided to investigate this topic further, and have been circumspect regarding this.
Yes, I agree that the hashlock is an incidental artefact.
What MAD-HTLC questions is our assumption that a valid transaction with an earlier locktime supersedes a valid transaction spending the same txout with a later locktime.
Whether it involves presigned transactions or hashlocks are incidental artefacts.
So for example, a SCRIPT `OP_IF  OP_ELSE <1 day> OP_CHECKSEQUENCEVERIFY OP_DROP  OP_ENDIF OP_CHECKSIG` would also be vulnerable to the MAD-HTLC argument.
(Indeed, BOLT spec uses something very much like that script, now that I look at it again; in our case the `` is a combination of keys from both parties, that cannot be signed with unless one party knows both sub-keys.)
I suggested that, until `SIGHASH_ANYPREVOUT` gets enabled, the Decker-Wattenhofer construction (removing the duplex Spilman-like channels at the end and leaving just the decrementing-`nSequence` constructions) could be used for Ruben Somsen StateChains, so you might not want to dismiss that so readily.
The decrementing-`nSequence` mechanisms have the advantage that it does not require a punishment/revocation branch, similar to Decker-Russell-Osuntokun "eltoo", and thus would work just as well to implement statechains, at least until all the debates around `SIGHASH_ANYPREVOUT` settle and it gets deployed.
Similarly, the proposed CoinPool as well could be implemented using Decker-Wattenhofer, assuming  it gets any details or support behind it sufficiently to push it before `SIGHASH_ANYPREVOUT` gets deployed.
I think a better heuristic is that, if the logic of the construction assumes "transaction with earlier locktime supersedes transaction with later locktime", then it is timelocked-bribery-vulnerable.

@_date: 2020-07-03 12:38:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Ittay,
The term "dominates" here is a technical term in game theory.
A strategy dominates over another strategy if, in a mixed environment, the first strategy always wins more points than the second strategy, no matter what proportion they may initially start in the mixed environment.
For example, in an environment of prisoner dilemma games, a tit-for-tat strategy dominates over the always-betray strategy, which dominates over always-cooperate strategy.
The above is the use of the term "dominate", and not that somehow one strategy "contains" the other.
Always-betray does not contain always-cooperate.
It is immaterial that the non-myopic "contains" myopic strategy as a sub-strategy.
Sometimes, overriding a sub-strategy can lead to worse outcomes and you are better off sticking to the sub-strategy rather than an extended strategy that sometimes overrides the sub-strategy
(notice how mixed teams of computer+human are no longer dominant in chess, because computer chess AIs are now so sophisticated that on average, the human overriding the computer strategy often leads to worse outcomes than just following the computer; yet about a decade ago such mixed computer+human teams were dominant over pure-computer and pure-human teams; yet you could say the same, that the computer+human "includes" the pure-computer strategy, but nowadays does ***not*** dominate it).
Or: worse is better.
What matters is, if you make them compete in an environment, myopic strategies will consistently beat non-myopic strategies because the myopic miners will impose costs on the non-myopic miners.
Yes, but again: myopic strategies dominate over non-myopic strategies, thus implementing non-myopic strategies is pointless, since they will lose revenue in an environment where even a single miner is myopic.
It is immaterial that it takes only 150 LoC to implement non-myopia: if it earns less money in an environment where even a minority of blocks are created by myopic miners (much less 97%), nobody will use the non-myopic strategy and they will remain at negligible near-0% hashrate.
As they say, "you can't get to there from here".
The only additional assumption you are missing is that miners care about *themselves* and not about *all miners*.
Non-myopia may earn more money for *all* miners if *all* miners use it, but if a *single* miner starts using myopic strategies in a non-myopic environment, they will earn more funds than their non-myopic competitors and thus dominate, shifting the percentages until almost all miners are using myopic strategies.
That they require less processing ("keep it simple, sir") is just gravy on top.
The only way for non-myopic miners to win is to form a cartel, and a miner cartel with >50% hashpower would be the end of Bitcoin anyway.

@_date: 2020-07-04 20:58:57
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Ittay,
It may be helpful to think in terms of Prisoner Dilemma.
 cooperate | betray
    -----------+-----------+---------
    cooperate  | -1, -1    | 0, -3
    -----------+-----------+---------
    betray     | -3, 0     | -2, -2
"include-Alice-Tx-now" imposes a greater cost on those playing "exclude-Alice-until-timeout-then-include-Bob" players, than the benefit that both miners play "exclude-Alice-until-timeout-then-include-Bob".
Basically, "cooperate" == "exclude-Alice-until-timeout-then-include-Bob", "betray" == "include-Alice-Tx-now".
One way to get around this is to invoke Iterated Prisoner Dilemma, but that requires that miners can identify other miners and to be able to act accordingly to how those other miners have acted in the past.
The entire point of Bitcoin mining is to allow strong anonymity of miners (not that this commonly happens in practice, given the habit of putting identifying information in coinbases).
Another way would be to have a higher system that polices its constituents and ensures that every miner plays "exclude-Alice-until-timeout-then-include-Bob", and punishes "include-Alice-Tx-now".
But that would be equivalent to a centralized cartel, and would be the death of Bitcoin anyway, at which point, all Bitcoin tokens will be worthless.

@_date: 2020-07-04 21:05:34
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Dave,
This note seems to have gotten missed in discussion.
Another note is that from what I can tell, the preimages "A" and "B" can be provided by any miner.
If the fund value plus the collateral is large enough, it may incentivize competing miners to reorg the chain, redirecting the funds of the MAD-HTLC to themselves, rather than advance the blockchain state, at least until alternative transctions bump their fees up enough that the collateral + fund is matched.
This may not apply to Lightning at least if you do not go beyond the Wumbo limit, but *could* apply to e.g. SwapMarket, if it uses MAD-HTLCs.

@_date: 2020-07-10 03:29:39
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] BIP 118 and SIGHASH_ANYPREVOUT 
Good morning aj,
It seems okay to me.
Slightly off-topic, but I suppose a Decker-Russell-Osuntokun construction could, in theory, have only a single internal taproot pubkey, `P = MuSig(A, B)` for a channel between A and B.
So the funding outpoint would be spent with a taprooted P + a single tapscript `<1> OP_CHECKSIG`.
Update transactions would be signed with the internal taproot pubkey using `SIGHASH_ANYPREVOUTANYSCRIPT`.
The update transaction output would be spendable with a taprooted P + a single tapscript ` OP_CHECKLOCKTIMEVERIFY OP_DROP <1> OP_CHECKSIG`.
Each update transaction would have a monotonically-increasing `nLockTime`, i.e. the above `index`.
Then a state transaction would be signed with the internal taproot pubkey using `SIGHASH_ANYPREVOUT`, which commits to the exact script including ``, which is unique for each update transaction.
Thus a state transaction can only spend the specific update transaction, but the update transaction can spend the funding outpoint or any update transaction outpoint.
State transaction input would have an `nSequence` requiring a relative locktime of the agreed-upon unilateral close delay.
The above assumes MuSig signing, which requires 1.5 round trips for a channel, or three broadcast rounds for a multiparticipant (n >= 3) construction.

@_date: 2020-07-14 14:42:21
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Lightning - Is HTLC vulnerable? And mention of 
Good morning Mr. Lee,
This could happen, but the entire exercise is to move transactions off the blockchain, precisely to lower this risk.
Otherwise, transfers onchain will take a long time.
In practice, a long time to settle a payment will invalidate many real-world economic exchanges anyway (consider paying for food at a restaurant --- if your payments take days to settle, the food has gotten stale before the restaurant receives payment and releases your food).
Thus, if an onchain transfer takes a long time to settle, there is already risk of economic loss present.
By moving activity offchain, we reduce pressure onchain and improve settlement speeds on both offchain and onchain, reducing risk of economic loss due to delay.
I cannot make sense of this.
You cannot create conflicting HTLCs.
Either you have some free money to create an HTLC, in which case there is no possible conflict with an existing HTLC (the fund is reserved for HTLCs, or it is yours without further encumbrance).
Thus it is not possible to create a conflicting HTLC in any case: either you have funds (that are not already in an HTLC) to fund an HTLC and that HTLC cannot conflict with existing ones, or you have no funds and a new HTLC cannot be created until one of the HTLCs is resolved one way or another.
This is possible, but only that node risks loss.
The reason why unilateral close is always possible is to handle the case where a routing node comes offline.
If you have offered an HTLC to a routing node, you retain a timelock branch back to you (the "T" in HTLC).
If the routing node goes offline past the timelock in the HTLC, then you unilaterally close the channel and drop the HTLC onchain.
This is what lets you recover your funds.
Roughly, my understanding of Flood and Loot is to make a lot of uneconomically tiny HTLCs going through a target victim forwarding node.
You make circular routes going from your own node back to yourself.
Then you refuse to actually claim the HTLCs sent back to yourself.
Then you go offline.
This means that the only way for the forwarding node to recover its funds is to drop the channel(s) involved onchain.
But if the HTLCs are many and tiny, they are simply too uneconomic to claim onchain, so they just lose the channel funds as fees.
Work is being done (anchor commitments) to mitigate the effects of onchain fees on Lightning.
They would not.
Ultimately, your "final defense" is to drop the entire construction onchain until you reach the HTLCs and you can have the blockchain enforce the HTLC contract.
It would *help* to reduce blockchain bloat by reducing the size of transactions to create multiple channels, and thus also secondarily helps reduce onchain fee pressure and also reduce Flood-and-Loot (which is basically a layer-crossing attack, taking advantage of lower-layer fees to create attacks on higher layers).
But always the underlying problem remains: security costs something, and you have to pay for protection on the Internet when transacting with potentially untrusted (and untrustable) entities.
It seems unlikely that "security costs something" can be eliminated.
One can consider that modern-day state-imposed taxation is paying for security, for instance, of traditional face-to-face transactions.
With Bitcoin, you can choose to either transact and pay for security, or not transact and forgo what you would have bought.
With some tradeoffs, you can pay by other means that may be cheaper for you.

@_date: 2020-07-17 02:58:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Thoughts on soft-fork activation 
Good morning list, BlueMatt and aj,
There is an idea circulating on IRC and elsewhere, which seems to be at least mildly supported by gmax and roconnor, which I will try to explain here.
(These are my words, so if there is some mistake, I apologize)
* Deploy a BIP8 `lockinontimeout=true` `lockin=+42 months` (or 36 months, or 24 months) at next release.
  * Pedanty note: BIP8 uses blockheights, not actual times.
* Then 1 year after `starttime`, ***if*** it is not activated yet:
  * Discuss.
  * If we think it is only because of miner apathy and user support seems good regardless, deploy a BIP91 reduced-threshold 80% that enforces the BIP8 bit.
    * We hope that this will stave off independent attempts at a UASF with a faster timeout.
  * If we think there are real reasons not to continue with Taproot as-is, deploy an abort: a softfork that disallows transaction outputs with `OP_1 <32-bytes>` `scriptPubKey` (other lengths and other versions are allowed).
This approximates what aj is proposing:
* Ultimately, we expect to deploy a BIP8 `lockinontimeout=true` that will have a timeout that ends +42 months after the first countdown, even with Modern Softfork Activation.
* The abort is roughly equivalent to the Modern Softfork Activation case where during the 6 month discussion period we decide not to deploy Taproot after all.
* The deployment of a BIP91 reduced-threshold 80% approximates what aj proposes, to reduce the threshold for activation later.
As I understand it, an advantage of this proposal is that we can deploy very quickly a relatively simple BIP8 `locktimeontimeout=true`, then continue debate on various details (is 80% too low? too high? are users actually deploying? are mining pools updating? etc) in parallel.
This lets the code into the hands of users where they can start deploying it and we can start getting better gauges on how well Taproot is supported.

@_date: 2020-07-17 06:02:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Hiding CoinSwap Makers Among Custodial Services 
Good morning Chris,
Despite the subject title, I have realized belatedly that the same kind of batching can be done by the taker as well.
For example, the taker can contact two makers in parallel to setup separate CoinSwaps with them.
Then the taker produces a transaction spending its funds and sending them out to two outputs.
If the taker uses P2PKH for receiving and change, and we use (via 2p-ECDSA) P2PKH 2-of-2 to anchor the swaps, then if both CoinSwap operations are successful, the transaction looks exactly like an ordinary pay-to-someone-and-get-back-change transaction.
Indeed, each of the two makers contacted, if they are not themselves colluding with each other, cannot really differentiate this from somebody doing a CoinSwap only with them, since the other output is indistinguishable from change.
I am uncertain how much extra privacy (or cheapness) this buys the taker, however.

@_date: 2020-07-20 14:18:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] The Cryptographic Relay: An Electrical Device For 
An electrical relay is an electrically-controlled switch, often diagrammed as:
      +-------------o
   \
    \
      +--o  o-------o
    o----  |
         ) |
         ) |
         ) |
    o----  |
The terminals at the left feed into a coil.
When electricity passes through the coil, it magnetizes a core, and the magnetism attracts a switch into a closed position, which would let electricity to pass through the terminals at the right.
This can be used to "turn on" or supply power to a different electrical device.
If no electricity is passing through the coil via the terminals on the left, then no electricity can pass through the terminals on the right, as the switch is an open position at default.
This is a fairly simple logic circuitry, upon which more complicated circuitry can be made.
Similarly, a Cryptographic Relay is a hardware electrical component that allows control of some electrical circuit.
It has two terminals which are the two terminals of a switch.
It can also be contacted, for example via Bluetooth, by an electronic computing device.
The Cryptographic Relay has a public key, which represents the current owner of the relay.
If the electronic device is able to provide a proof-of-knowledge of the private key (i.e. a signature) corresponding to the public key that the Cryptographic Relay knows, then it allows the switch to be controlled by that device.
Suppose I have a car I wish to sell to you, for Bitcoins.
This car, as it happens, has a Cryptographic Relay: it will only start its engine if it gets a signed message from me using a private key I have on my phone.
It knows my public key, and will only turn off and on at my command.
Now, you want to be able to know that by paying me Bitcoins, you get sole ownership of the car I am selling.
This is in fact, nothing more than a swap operation.
I have an asset, a car, and you have an asset, some Bitcoins.
We want to atomically swap one asset for the other asset, and those assets do not exist in a single asset-assignment system.
Paying Bitcoins For Cars Via Succinct Atomic Swaps
Fortunately, there is an atomic swap operation, Succinct Atomic Swaps, which can be used to create an atomic swap between my car and your Bitcoins.
An important part of this Succinct Atomic Swap is that all timeouts are only in one asset-assignment system.
The original Succinct Atomic Swap discussion gives an example of swapping Bitcoins for Litecoins.
Each has its own distinct blockchain, which is a distinct distributed asset-assignment system.
A crucial part of the Succinct Atomic Swap is that all timelocks are only on one asset-assignment system.
The other asset-assignment system need not support anything other than assigning ownership of assets to (homomorphically additive) public keys.
This is important as a major simplification of the Cryptographic Relay:
The relay only needs to know its *current owner*, and does not need to know the current time!
Thus, in order for you to buy my car:
* You set up the Succinct Atomic Swap on the Bitcoin blockchain.
* We generate fresh private keys, then combine them via MuSig, and I transfer the ownership of the car to that MuSig public key.
* If I claim the funds, that atomically reveals my share of the private key to you, so you can claim the car using your private key plus my share and transfer the car to sole control of you.
* If I fail to claim the funds, then when you reclaim your funds at timeout, that atomically reveals your share of the private key to me, so that I can claim the car using my private key plus your share and transfer the car back to sole control of me.
This is in fact the same as the Succinct Atomic Swap example, except that instead of me swapping my Litecoins for your Bitcoins, I am swapping my Cryptographic Relay for your Bitcoins.
Cryptographic Relay Operations
Thus, a Cryptographic Relay needs to support only the following basic operations.
These operations are triggered by its owner sending a message, plus a signature verifiable by the public key, to the Cryptographic Relay.
(Cryptographer warning: if using Schnorr, the message hashed into the signature also has to include the public key that is signing it, since BIP32 nonhardened derivation allows a Schnorr signature created by one child private key to be trivially malleated into a signature for the same message by a sibling private key; this does not exist in ECDSA due to its nonlinearity.
message sends need not include the public key, it can be added by the cryptographic relay since it has to know the public key of the owner anyway.
Of note is that the bip-340 description of Schnorr includes the public key in the hash operation of signatures as well, and does not require this.)
The only operations necessary are:
1.  Turn on.
2.  Turn off.
3.  Transfer ownership to new pubkey ___.
Due to Succinct Atomic Swaps not requiring any kind of timelock on one asset-assignment system, the Cryptographic Relay need not have any concept of time, as mentioned above, and transfer of ownership can be done by a simple message signed by the current owner transferring ownership to a new public key.
(Cryptographer warning: turn-on/turn-off messages should be challenge-response: the Cryptographic Relay requests signing with a unique nonce included in the message, otherwise previous messages can be captured by third parties and replayed; ownership-transfer messages should probably also include a nonce, or otherwise we could require non-reuse of keys.
This can be implemented with a 64-bit incrementing nonce, which should work for the practical lifetime of any Cryptographic Relay.)
(Cryptographer warning: similarly, ownership-transfer messages should probably be indexed as well with an incrementing nonce, otherwise key reuse would allow signature replay; similarly again, each Cryptographic Relay should have a UUID that is included in message hashes, as otherwise key reuse would allow signature replay for commands to one device to be repeated for another device.)
Unfortunately, it seems not possible to transport Succinct Atomic Swap constructions over the Lightning Network.
Succinct Atomic Swaps have an asymmetric setup, unlike the traditional HTLC-based atomic swaps.
This asymmetry makes it difficult to reason about how a forwarding node on the Lightning Network would be able to accept a Succinct Atomic Swap and then send out another Succinct Atomic Swap.
An observation to make is that the Bitcoin-side construction in the Succinct Atomic Swap description has the following branches:
* B gets the money, and A learns a secret from B.
* After a timeout, A gets the money back, and B learns a secret from A.
Unfortunately, the above construction, while instantiatable in a single channel (any absolute-timelock contract enforceable on a blockchain can be instantiated in a channel anchored in that blockchain), cannot be safely *forwarded* over the Lightning Network in a sequence of channels forming a route.
This is because we need to have a different timeout at each forwarding hop, in order to allow a forwarding node enough time to discover an event and react accordingly.
This means that each forwarding node has to have a shorter timeout on its outgoing contract than its incoming contract.
But in the half-Succinct-Atomic-Swap contract, the forwarding node needs to provide a secret in order to reclaim the funds at the timeout.
And it can only discover that secret at the later timeout of the incoming contract.
Thus, it is not safe for the forwarding node to forward any half-Succinct-Atomic-Swap contract.
Thus, we either:
* Prevent old, low-value cryptographic cars from being bought using Lightning, reducing their economic veolocity and preventing moon and lambos.
* OR, we give the Cryptographic Relay a notion of time which makes it compatible with PTLCs that can be routed over Lightning.
  In essence, the PTLC "goes over one more hop", this time transferring ownership of the Cryptographic Relay.
A Cryptographic Notion of Time
Time stops for no one; it will not stop for thee.
Or, in more science-appropriate terms: the passage of time is the direction in which universal entropy increases.
Now, we can observe that block header hashes are, in fact, low-entropy.
This is because the higher bits of block header hashes are consistently 0; there are thus fewer bits of entropy you can extract from a block header hash.
Now, we can observe that temperature is largely itself also an expression of entropy.
Higher-entropy areas are higher temperature, and lower-entropy areas are lower temperature.
Overall, the temperature differential across the universe decreases in the direction of future time.
However, it is possible to implement a sort of Maxwell's Demon.
Maxwell's Demon is an entity that guards a hole between two containers containing air.
If a high-speed, high-tempreature molecule of air on the left side approaches the hole, Maxwell's Demon magically swats it away, but if a similar high-speed, high-temperature molecule of air on the right side approaches the hole, Maxwell's Demon lets it pass.
It has the reverse policy for low-temperature molecules of air, letting it go from the left container to the right container.
Over time, the temperature of the right container drops, because all the high-temperature molecules have been moved to the left container.
Of course, we already have implementations of Maxwell's Demon.
We call such implementations "refrigerators".
Refrigerators, to do their magic, must consume energy and emit heat.
Indeed, the total heat emitted by the refrigerator is much larger than the heat it removes in the cold part of the refrigerator.
We can verify that the refrigerator is working, trivially, by checking that the supposedly-cold part of the refrigerator is indeed cold.
But we know that refrigerators, to do their work, must consume energy and emit heat.
And we also know that, due to the heat emitted by the refrigerators, the universal level of entropy increases, and we know thereby a direction of time is equivalent to a refrigerator successfully freezing something.
Similarly, in order to create low-entropy ("cold") block header hashes, miners of Bitcoin must consume energy and emit heat.
Bitcoin miners then act similarly to Maxwell's Demon; they reject candidate blocks whose block header hashes are not "cold enough" (i.e. have entropy greater than the difficulty target), and only allow "cold" block headers to be broadcast over the blockchain.
And since we know that:
* The future is where the universal entropy is larger than the past.
* Miners producing blocks must consume energy and emit waste heat (increasing universal entropy).
...then we know that a longer proof-of-work header chain represents more actual physical time passing.
Proof-of-work is therefore also an unforgeable proof-of-time-passing.
Thus, all we need for a cryptographically-secure *measure of time* is a header chain.
Crucially, this is better than SPV security, since we are only measuring the passage of time and do not particularly care about reorgs and the transactions in the block.
The longest chain wins, so the "largest blockheight" can only grow monotonically even if a reorg happens.
Even if the transactions in a reorg are ultimately disconfirmed (double-spent), or turn out to be invalid, the Cryptographic Relay does not depend on their validity, it only cares about time passing in order to implement a timeout.
This is significantly better than having to implement a stable clock on the Cryptographic Relay to implement a timeout.
Clocks may drift, and the Cryptographic Relay might not want to tr\*st external sources to give it a true notion of time.
Loss of power supply may also cause the Cryptographic Relay to lose its clock as well.
Thus, it must use this cryptographic notion of time.
Giving Cryptographic Relays a Notion of Time
In order to implement timelocks, we can add an `nLockTime` field to ownership-transfer messages for the Cryptographic Relay.
On manufacturing a Cryptographic Relay, the manufacturer stores in unchangeable memory (e.g. burned diode non-erasable PROMs) the block header hash of a sufficiently-deep block (for example, one that has been buried for 6 or so difficulty adjustment periods), its blockheight, and the difficulty target for the adjustment period.
If the Cryptographic Relay receives an ownership-transfer message with `nLockTime` greater than this recorded block height, it demands a header chain rooted at the prerecorded block up to the specified `nLockTime`.
It then validates that the header chain has sufficient difficulty for each adjustment period covered, and also emulates the difficulty adjustment at appropriate blockheights.
This is sufficient proof to it that time has passed since its manufacture, as a header chain is a cryptographic proof of time passing.
This allows me to sell a cryptographic car to you, over Lightning, by this ritual:
* First, we generate fresh keypairs and their combined MuSig pubkey.
* We perform a MuSig signing operation, signing an ownership-transfer message with a future `nLockTime`, transferring ownership from the MuSig pubkey back to my unilateral pubkey.
* I transfer control of the car to the MuSig pubkey.
* We partially perform a second MuSig signing operation, signing an ownership-transfer message with a 0 `nLockTime`, transferring ownership from the MuSig pubkey back to you, but pausing after the `R` exchange.
  * Specifically, after the `R` exchange, you generate the point corresponding to my share of the signature `s`, which you can get from my `R`, your `R`, my public key, and your public key, and the message we agreed on.
* I generate an invoice for the above signature share point (i.e. pay for signature).
* You pay the invoice, making sure that the maximum cltv-delta for the outgoing payment does not exceed the pre-agreed timeout (minus some time margin you deem safe).
* If you successfully pay the invoice (i.e. I release my share of the signature) you can now complete the signature for the transfer to your unilateral control.
* If you are unable to pay the invoice, then once the future blockheight is reached, I download the Bitcoin block header chain and feed it and the backout ownership-transfer message to the car and regain control of my vehicle.
This provides Lightning-with-PTLCs compatibility (which we expect to be deployed on the network anyway), while still requiring relatively low resources on the Cryptographic Relay hardware (at the cost that timelocked backouts require much bandwidth; but since such backouts are expected to be rare, this may be an acceptable tradeoff).
A Cryptographic Relay accepting this notion of time can continue to be used with Succinct Atomic Swaps, by using ownership-transfer messages with a 0 `nLockTime`, with the advantage that backouts need not download a block header chain to the Cryptographic Relay.
A Case Against Blockchain Proliferation
We can argue that the Cryptographic Relay is a device tr\*sted to actually do what we claim it does here.
In particular, its users tr\*st that its manufacturer does not have a secret backdoor, a special public key recognized by every Cryptographic Relay by which the manufacturer can gain ownership of every piece of smart hardware in the world.
This may lead some to propose that a publicly-auditable blockchain can be used to manage the assignment of ownership of Cryptographic Relay devices.
That way, the source code that performs the ownership-assignment can be openly audited, and independent observers can check that the asset-assignment blockchain indeed works using the published source code by compiling it themselves and running it, and checking that it remains in synchrony with the asset-assignment blockchain.
However, I should point out that merely because some blockchain somewhere considers asset X to be owned by pubkey Y, does not mean that the actual real-world asset X will have a control system that responds to pubkey Y.
Or in other words, the manufacturer of the actual real-world asset X can still insert a secret backdoor that ignores the public asset-assignment blockchain anyway.
And since blockchains are massive bandwidth hogs, we should avoid using them unless we gain some actual benefit.
On the other hand, the proposed Cryptographic Relay here is reasonably simple, requires no consensus system.
The best that can be done would be to standardize Cryptographic Relays and encourage multiple manufacturers to follow the same standard.
Such a standard would include communication protocols between the Cryptographic Relay and the controlling devices, but would also include details like voltage levels, current limits, normally-closed vs normally-open vs make-before-break SPDT/DPDT vs break-before-make SPDT/DPDT, physical dimensions of the package(s), etc.
Tr\*st in manufacturers can be acquired, at very high expense, by using cut-and-choose: get a number of Cryptographic Relays, randomly select some of them, then open those and analyze if there are any backdoors, then utilize the rest if the randomly-opened ones do not have any discovered backdoors or other security issues.
Delegated Operators
Another useful capability to add to Cryptographic Relays would be to allow adding operators, which are additional public keys that can turn it on or off, but cannot authorize a transfer of ownership or the addition of new operators.
Only the owner of the Cryptographic Relay can add or delete operators.
For example, suppose you, the buyer of my cryptographic car, represent a typical family share-owned between you and all of your husbands.
Then when purchasing the car from me, you can transfer ownership of the car to a MuSig n-of-n between you and your husbands, rather than unilateral control of yourself.
Of course, practically speaking, only one of you or your husbands can operate the car at once.
Thus, while the ownership of the vehicle is an n-of-n MuSig, you can assign individual keys of yourself and each of your husbands, as operators of the car.
The Cryptographic Relay would need to allocate a fixed amount of space for the number of allowed operators, thus imposing a practical limit on the number of husbands you can have simultaneously.
Unfortunately, the Cryptographic Relay cannot store an arbitrary number of public keys, thus there are limits on the number of husbands a human can have.
Whenever an ownership transfer is performed, all operators are deleted, thus preventing one of the previous operators attempting to start the car and drive it off even though it is semantically owned by someone else.
A car purchased by an extended family, such as a company, might be owned by a k-of-n of the stakeholders of the company, and then employees of the company might be assigned as operators.
Practical Deployment
By focusing on developing the most basic Cryptographic Relay, this provides us with a practical deployment for smart devices that can recognize their owner and be used only by the owner (and its delegated operators).
In particular, any existing non-smart electrical device can be modified post-warranty into a smart device that knows its owner, by adding a Cryptographic Relay hardware device somewhere along the path to its power supply.
For example, a Cryptographic Relay could replace a power switch, or be spliced onto the power cord.
Now, of course such a jury-rigging could be easily bypassed, by simply splicing a wire across its terminals.
Similarly, many existing cars can be started without keys by hot-wiring.
Ultimately, the same can be said of almost any end-user appliance; possession remains 9/10ths of the law.
A Cryptographic Relay is a simple device:
* The design of the device is simple, requiring support for only a few operations.
* The interface of the device is simple: it is only a switch, and easy to integrate into more complex applications.
  The switch may be used as the route for the power supply, or it may be used simply for an electrical connection that is detected by some control system of the appliance to know whether it should act as "on" or "off".
  This simplicity and generality allows simple interfacing with a variety of electrical devices.
The intent is that:
* It is easy to implement and subsequently manufacture the device, so that risks of backdoors being installed by centralized manufacturers is mitigated by having multiple Cryptographic Relay manufacturers competing and incentivized to discover backdoors and other security failures of their competitors.
* It is easy to integrate the device into the design of a more complex device intended for use by an end-user.
  It is also easy to integrate the device, post-design, to a more complex existing device that did not include it.
Collateralizing Cryptographic Relay Devices
Giving Cryptographic Relays a notion of time allows them to be used in more complicated contracts.
Suppose after purchasing the cryptographic car from me, you and your husbands find yourselves in tight financial straits.
You and your husbands might then want to take a loan from some entity.
Obviously, that entity will not simply loan out precious Bitcoins unless you promise to pay it back with *more* Bitcoins than what it gave out.
And that entity might want to accept your cryptographic car as collateral for the loan, so that if you are unable to pay, the entity can partially recoup losses by reselling your cryptographic car.
With `SIGHASH_ANYPREVOUT` and Taproot, it becomes possible to trustlessly arrange a collateralized loan on the cryptographic car (to the extent the loan shark tr\*sts that none of you or any of your husbands have replaced the Cryptographic Relay with a backdoored device, at least).
The loan shark arranges a loan in a multi-step process involving multiple PTLC-like constructions:
* First, all of you (you and your husbands, and the loan shark) generate two fresh keypairs.
  * Call the first the "loan-out" keypair.
  * Call the second the "loan-payback" keypair.
* You generate (but do not sign!) a command to transfer control of the car from you to a MuSig(you, your husbands ..., loan shark) using the "loan-out" pubkeys.
* Generate the initial backout command, which transfers from the MuSig loan-out pubkey above, back to you and your husbands, but with an `nLockTime` in the future.
  Get a complete signature for this command with the loan shark and your husbands and you.
  * This backout timeout period should be short, and is needed only if the loan shark suddenly aborts before it hands over the loan to you.
* Sign the transfer of the car from you to the MuSig loan-out and feed it to the car.
* Generate (but do not sign!) the collateralization command, which transfers from the MuSig loan-out pubkey, to a new MusSig(you, your husbands ..., loan shark) using the "loan-payback" pubkeys.
* Generate the collateral-claim command, which transfers from the MuSig loan-payback pubkey to the loan shark, but with an `nLockTime` a little after the due date of your loan.
  Generate a complete signature for this command to the loan shark, which the loan shark will now hold and use in case you do not pay back the loan.
* Generate the collateral-reclaim command, which transfers from the MuSig loan-payback pubkey back to you and your husbands.
  Generate a partial signature for this command from you and your husbands, but with a missing share from the loan shark, and keep this partial signature.
* Generate a Taproot address (called the loan-payback Taproot address) with two SCRIPT branches, with an internal public key equal to the loan-payback pubkey.
  * One SCRIPT branch allows signing by the internal public key with `SIGHASH_ANYPREVOUT`.
  * The other SCRIPT branch imposes a short `OP_CHECKSEQUENCEVERIFY` timeout, and claiming by the MuSig of you and your husbands.
* Generate loan-payback Bitcoin transaction, which spends using a `SIGHASH_ANYPREVOUT` signature on the first branch of the above Taproot address, an amount equal to your loan plus interest, and sending unilateral control to the loan shark.
  Demand an adaptor signature for this transaction, which would reveal to you the loan shark signature share of the collateral-reclaim command.
  Then provide the signature shares for you and your husband to the loan shark.
  * If the loan shark completes this signature and reclaims the loan plus interest, you learn the share to complete the collateral-reclaim command and thereby reclaim your car after paying back the loan.
* Now the loan shark creates (but does not sign!) the loan-out transaction, which pays out the MuSig loan-out pubkey the amount being loaned.
* Generate the loan-out-revert transaction, which spends the loan-out transaction and pays it back to the loan shark, but with an `nLockTime` in the future, nearer than the `nLockTime` of the initial backout command.
  Sign this transaction with you and your husbands and the loan shark.
* The loan shark generates a partial signature for the collateralization command, missing the shares from you and your husbands.
* Generate the loan-out-claim transaction, which spends the loan-out transaction and pays it to you and your husbands.
  The loan shark will demand an adaptor signature for this transaction, which lets it learn the sum of the missing signatures shares for the collateralization command.
  Then the loan shark provides its share of the signature for the loan-out-claim transaction.
* The loan shark signs the loan-out transaction and broadcasts it.
* You now complete the signature for the loan-out-claim transaction and broadcast it.
  The loan shark learns the missing signature shares for the collateralization command and performs it, locking your car into collateralization (where it cannot be transferred unless you, your husbands, and the loan shark agree).
  With the loan-out-claim transaction valid, you can now take the loan.
Then, if you are able to pay back the loan before the due date:
* Send the loan amount plus interest, exactly, to the loan-payback Taproot address.
* If the loan shark accepts the payback of the loan, it reanchors the loan-payback Bitcoin transaction and completes the signature, then claims the paid back loan.
* With the loan-payback signature completed, you learn the partial signature needed to complete the signature for the collateral-reclaim command, and you can now feed it to the car and regain ownership over it.
* If the loan shark does not accept payback of the loan, you can at least recover the loan and interest by the second branch of the loan-payback Taproot address.
If you are unable to pay back the loan:
* The collateral-claim command becomes valid a little after the loan due date, and the loan shark takes unilateral possession of your car to recoup its losses.
The above uses onchain Bitcoins.
Similar setups may be possible over Lightning (there is no need for a `SIGHASH_ANYPREVOUT` in this case, as the loan shark may issue a long-lived invoice that lets you learn its signature share for the collateral-reclaim command signature).

@_date: 2020-07-21 01:27:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] The Cryptographic Relay: An Electrical Device For 
Good morning list,
Andy Schroder shared a mildly related link: The above project does not use the Cryptographic Relay.
Briefly, it is a rentable charging station for electric cars.
I observed, however, that a rentable Cryptographic Relay device could be implemented using Cryptographic Relay features:
* Support for MuSig (by use of Schnorr signatures).
* Timelocks (by use of block header chains).
* Delegated operators.
We can also consider the case where the renter of the device wishes to return it early, for a partial refund of the total rent (or equivalently, for the renter to rent in units of smaller time and just extending the rental period as needed).
Now, suppose the device being rented out is in fact a smart domicile, which can be locked/unlocked by the owner/operator of a Cryptographic Relay.
Typically, when renting out domiciles, a deposit is involved, where:
* The tenant pays out the rent plus the deposit.
* The landlady may keep the deposit in case of egregious damage to (or other abuse of) the domicile.
The construction of a rent-with-deposit contract is actually similar to the construction of the earlier given collateralized loan:
(note: missing in the above is the detail that at the end of the contract period, ownership of the device goes back to the landlady/loaner position, as opposed to the collateralized-loan case where it goes to the loan shark position.)
Perhaps smart contract languages should have PTLCs and partial signatures as primitives and be written in a compositional/declarative style, rather than some Turing-complete mess, because PTLCs are cool.

@_date: 2020-07-21 03:40:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Implementing Investment Aggregation 
In a capitalist economic system, it is allowed for an entity to lend money out to another entity, as long as both agree upon the conditions of the loan: how long, how much interest, any collateral, etc.
This is a simple extension of basic capitalist economic thinking: that the owner of funds or other capital, is the one who should decide how to utilize (or not utilize) that capital, including the decision to lend (or not lend).
It has been observed as well that groups of people may have relatively small savings that they can afford to put into investment (i.e. loaning out for an interest rate), but as the technological capabilities of our shared civilization have expanded, the required capital to create new businesses or expand existing ones have grown much larger than most single individuals can invest in.
Thus, coordinators that aggregate the savings of multiple individuals, and then lend them out for interest to new or expanding businesses, have also arisen, in order to take advantage of the larger return-on-investment of more capital-intensive but high-technology businesses, capturing the long tail of small investors.
Traditionally, we call these coordinators "banks".
However, this typically involves delegating the work of judging whether a business proposal is likely to give a return on investment, or not, to the coordinator itself.
Further, the coordinator typically acts as a custodian of the funds, thus adding the risk of custodial default to the small-time investors in addition to loan default.
(In this view-point, central banks that provide fiscal insurance in case of loan default by printing new money, are no different from custodial default, as they degrade the monetary base in doing so.)
This writeup proposes the use of features that we expect to deploy at some point in the future, to allow for a non-custodial coordinator of multiple small investors.
This is not a decentralized system, as there is a coordinator; however, as the coordinator is non-custodial, and takes on the risk of default as well, the risk is reduced relative to a centralized custodial solution.
Note that custodiality is probably a much bigger risk than centralization, and a centralized non-custodial probably has fewer risks than a decentralized custodial setup.
In particular, a decentralized custodial setup can be emulated by a centralized custodial setup using sockpuppets, and without any decent sybil protection (which can be too expensive and price out investments by the long tail of small investors, thus leading to centralization amongst a few large investors anyway), is likely no better than a centralized custodial setup.
Focusing on non-custodiality rather than decentralization may be a better option in general.
A group of small investors may very well elect a coordinator, and since each investor remains in control of its funds until it is transferred to the lendee, the coordinator has no special power beyond what it has as one of the small investors anyway, thus keeping decentralization in spirit if not in form.
Non-custodial Investment Aggregation
In principle, if a small investor finds a potentially-lucrative business that needs capital to start or expand its operation, and promises to return the loaned capital with interest later, then that small investor need not store its money with anyone else: it could just deal with the business itself directly.
However, the small investor still needs to determine, for itself, whether the business is expected to be lucrative, and that the expected return on investment is positive (i.e. the probability of non-default times (1 plus interest rate) is greater than 1, and the absolute probability of non-default fits its risk profile).
We will not attempt to fix this problem here, only the requirement (as with the current banking system) to trust some bank **in addition to** trusting the businesses that are taking on loans to start/expand their business.
(again: not your keys not your coins applies, as always; investors are taking on risk of default.)
The coordinator need only do something as simple as find a sufficiently large set of entities that are willing to indicate their Bitcoin UTXOs as being earmarked for investment in a particular business.
The coordinator, upon finding such a set, can then create a transaction spending those UTXOs and paying unilaterally to the business taking the loan.
The business provides proof that the destination address is under its unilateral control (so that investors know that they only need to trust that the business itself will do everything in its power to succeed and pay back the loan, without having additional trust in the coordinator to hold their funds in custody).
Then the individual investors sign the transaction, releasing their funds to the business.
However, the issue now arises: suppose the business succeeds and is able to pay back its loan.
How does the business pay back the loan?
Thus, prior to the investors ever signing the loan-out transaction, they first prepare a loan-payback transaction.
This loan-payback transaction spends from a multisignature of all the investors, equal in value to the loan amount plus agreed-upon interest, and distributes the money to each of the involved investors.
Crucially, this loan-payback transaction is signed with a `SIGHASH_ANYPREVOUT` signature.
Now, in order for the business to pay back its loan, it only needs to gather enough Bitcoins to pay back the loan, and pay back the exact amount to the multisignature address of the investors.
Then, any of the investors can reclaim their funds, plus interest, by re-anchoring the loan-payback transaction to this transaction output and broadcasting it.
The coordinator, for its services, may extract a fee from the loan-payback transaction that all the investors can agree to; thus, it takes on as well the risk of default by the business (the coordinator exerts effort to locate investors and encourage them to invest, and would lose the fee paid for its efforts if the business it is proposing as a good investment does not pay back), which seems appropriate if it also serves as a basic filter against bad business investments.
Finally, by working in Bitcoin, it cannot have a lender of last resort, and thus must evaluate possible business investments as accurately as possible (as default risks its fee earnings).
(investors also need to consider the possibility that the purported "business" is really a sockpuppet of the coordinator; the investors should also evaluate this when considering whether to invest in the business or not, as part of risk of default.)
(the above risk is mitigated somewhat if the investors identify the business first, then elect a coordinator to handle all the "paperwork" (txes, transporting signatures/PSBTs, etc.) by drawing lots.)
Thus, ***if*** the business is actually able to pay back its loan, the coordinator is never in custodial possession of funds.
Cross-business Aggregation
Nothing in the above setup really changes if the investors would prefer to spread their risk by investing sub-sections of their savings into multiple different businesses.
This gives somewhat lower expected returns, but gives some protection against complete loss, allowing individual investors to adjust their risk exposure and their desired expected returns.
The batch transaction that aggregates the allocated UTXOs of the investors can pay out to multiple borrowing businesses.
And each business can be given a loan-payback address, which is controlled by the investors that extended their loans.
Investors generate an aggregate loan-payback transaction and signature for each business they invest in.
Collateralized Loans
As observed in  a Cryptographic Relay would allow collateralized loans.
Nothing prevents the "loan shark" in the collateralized loan example from being a MuSig of multiple small investors.
Practically, a coordinator would help facilitate construction of the necessary transactions and interaction with the loanee, but as long as ownership remains controlled by the individual investors, there should not be any custodial issues.
Of course, if the loan defaults, then the collateral needs to be sold in order to recoup the loss incurred in loan default case.
Coordinating this sale amongst the multiple small investors is now potentially harder.
An additional service may be willing to pre-allocate Bitcoin funds into a timelocked contract, where the amount can be claimed conditional on transfer of the ownership of the collateral to the service in the future, or if the fund is not so claimed, to be returned to the service with the collateral not claimed (as it might have been reclaimed by the loaner after successfully paying back its loan).
This additional service earns by arbitraging the time preference: in case of default, the investors would prefer to recoup their financial losses quickly, while the service is now in possession of the collateral that it can resell later at a higher rate.
Note that these are all operations that traditional banks perform; again, this idea simply removes the necessity for custodial holding of funds, in the way traditional banks do.

@_date: 2020-07-21 09:19:17
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] The Cryptographic Relay: An Electrical Device For 
Good morning Andy,
Yes, PVT relation.
I am now investigating geothermal heat pumps in the context of taking over the world, thank you for your information.
Certainly an interesting thought!
Thank you very much.
A lot of alternative blockchains that are designed for handling asset-assignment of real-world things, are far more centralized due to their non-generic nature: very few entities are interested in those spaces.
A Cryptographic Relay demonstrates that we can do better, by making a generic component, and disposing of the blockchain, and shows that even in the "blockchain for things!" case, you *still have to trust manufacturers anyway*.
After all, CPUs are commoditized enough that we hardly ever wonder if e.g. Intel or AMD or ARM have secreted backdoors into their CPUs.
Hopefully, Cryptographic Relays are commoditized enough as well that the probability of a manufacturer adding secret backdoors is low.
I considered the "relay" interface to be better since a relay can be used as a (very slow) transistor, but if you want to transport say a 220V AC mains supply, you cannot use a transistor.
The slowness of relays (due to their mechanical nature) is acceptable since power-on and power-off events are expected to be rare compared to the operation of the device.
For example, a pre-existing non-cryptographic Smart TV can be upgraded into a cryptographic Smart TV by splicing a DPST Cryptographic Relay in its mains supply cord.
(This voids warranty, but if warranty is already ended, might as well.)
That said, it is possible to start with a relay driver interface instead of a relay interface (though I prefer the latch-type relays due to their better mechanical longevity and lower continuous power use, which requires two relay driver interfaces and timing).
Indeed, that would be possible.
Though note that if I am trying to abscond with an electric car, all I need to *hot*wire would be the battery pack, inverter, motor, and ignition.
After absconding the electric car and placing it in a location I control, I can crack (i.e. splice wires across) the Cryptographic Relays of the other components at my leisure.
Thus, this post is simply a prelude to me becoming the next protagonist of Fast and Furious.

@_date: 2020-07-21 16:28:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Implementing Investment Aggregation 
Good morning Hilda,
This may be possible by using a Decker-Russell-Osuntokun ("eltoo") mechanism.
The laon-payback transaction (the one that is signed with `SIGHASH_ANYPREVOUT`) can, instead of paying out directly to the investors, pay out to a Decker-Russell-Osuntokun mechanism that is signed by a MuSig of the investors plus the coordinator.
The initial state of this mechanism is the payouts of each investor, in proportion to the amounts they lent out.
Thus, if none of the investors need to liquidate early, this initial state is what gets posted on the blockchain ***if*** the loaning business successfully pays back / does not default.
If one of the investors needs to liquidate its position in this loan agreement, the coordinator can offer to buy its position (in whole or in part) for a smaller amount (as the coordinator takes on more risk).
Then all the investors plus the coordinator sign a new state of the Decker-Russell-Osuntokun mechanism, with the coordinator getting more funds, and the liquidating investor losing all or part of its allocation.
The investor doing the liquidation can demand a pay-for-signature, so that its signature share of the new state is only acquired by the coordinator if and only if it actually gets paid with Bitcoins now.
The position need not be bought by the coordinator --- one of the other small investors in the business can "double down" and purchase more of the share of the eventual loan-payback by the same mechanism, from peer investors who need to liquidate their position in the loan-payback early, increasing its risk exposure but potentially getting even more profit in case the invested business pays back the loan.
Similar constructions could be done by the coordinator and / or the investors directly; unfortunately I know too little of them to give an idea how this can be done.

@_date: 2020-06-01 02:34:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Ruben,
If Alice is paying to a non-SAS aware payee that just provides an onchain address (i.e. all current payees today), then the 2-of-2 output it gets from the swap (both of whose keys it learns at the end of the swap) is **not** the payee onchain address.
And it cannot just hand over both private keys, because the payee will still want unambiguous ownership of the entire UTXO.
So it needs a second transaction anyway.
(with Schnorr then Alice and payee Carol can act as a single entity/taker to Bob, a la Lightning Nodelets using Composable MuSig, but that is a pretty big increase in protocol complexity)
If Alice does not want to store the remote-generated privkey as well, and use only an HD key, then it also has to make the second transaction.
Alice might want to provide the same assurances as current wallets that memorizing a 12-word or so mnemonic is sufficient backup for all the funds (other than funds currently being swapped), and so would not want to leave any funds in a 2-of-2.
If Bob is operating as a maker, then it also cannot directly use the 2-of-2 output it gets from the swap, and has to make a new 2-of-2 output, for the *next* taker that arrives to request its services.
So there is always going to be a second transaction in a SwapMarket system, I think.
What SAS / private key turnover gets us is that there is not a *third* transaction to move from a 1-of-1 to the next address that makers and takers will be moving anyway, and that the protocol does not have to add communication provisions for special things like adding maker inputs or specifying all destination addresses for the second stage and so on, because those can be done unilaterally once the private key is turned over.
Hmmm right right.
Naively it seems both chaining SAS/private key turnover to multiple makers, and a multi-maker S6 augmented with private key turnover, would result in the same number of transactions onchain, but I probably have to go draw some diagrams or something first.
But S6 has the mild advantage that all the funding transactions paying to 2-of-2s *can* appear on the same block, whereas chaining swaps will have a particular order of when the transactions appear onchain, which might be used to derive the order of swaps.
On the other hand, funds claiming in S6 is also ordered in time, so someone paying attention to the mempool could guess as well the order of swaps.

@_date: 2020-06-03 04:53:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Chris,
Alice can do PayJoin with a payee Carol that supports normal PayJoin, for similar overall results.
Though I suppose there is a mild advantage still with supporting it on the funding tx of the first transaction, as you noted.
How about RBF?
A taker Alice can broadcast the funding tx spending its own funds.
The funding tx spends funds controlled unilaterally by Alice.
Alice can sign a replacement transaction for those funds, spending them to an address with unilateral control, and making the funding tx output with all the obligations attached never get confirmed in the first place.
The chances may be small --- Bob can certainly monitor for Alice broadcasting a replacement and counter-broadcast its own replacement --- but the risk still exists.
TANSTAAGM (There Aint No Such Thing As A Global Mempool) also means Alice could arrange the replacement by other means, such as not using the RBF-enabled flag, broadcasting the self-paying replacement near miner nodes, and broadcasting the CoinSwap-expected funding tx near the Bob fullnode; Bob fullnode will then reject attempts to replace it, but miners will also reject the CoinSwap-expected funding tx and it will not confirm anyway.
With the pre-SAS 4-tx setup, this potentially allows Alice to steal the funds of Bob; after Alice gets its funding-tx-replacement confirmed together with the Bob honest-funding-tx, Alice can use the contract transaction and publish the preimage to take the Bob funds.
Since the Alice-side funding tx has been replaced, knowledge of the hash preimage will not help Bob any: the Alice funding tx has been replaced and Bob cannot use the preimage to claim it (it does not exist).
With SAS Alice cannot outright steal the Bob funds, but the Bob funds will now be locked in a 2-of-2 and Alice can take it hostage (either Bob gives up on the funds, i.e. donates its value to all HODLers, or Bob gives most of the value to Alice).
For the avoidance of theft, it is probably better for Bob to wait for Alice-side funding tx to confirm, probably deeply because reorgs suck.
This at least makes it costly to perform this attack; you have to lock more of your funds longer in order to induce a competitor to lock its funds.
Come to think of it, the same issue probably holds for S6 as well, the funding tx with the longest timelock has to confirm first before the next is even broadcast, bleah.
It certainly seems quite possible; each participant in S6 has a fixed "previous" and "next" participant.
Of course, this requires a secure tunnel, and my understanding of your plan for SwapMarket is that the taker Alice serves as the broadcast medium between all makers and itself.
So, in an S6 sequence of Alice -> Bob1 -> Bob2 -> Alice, after Alice provides the preimage, Bob2 encrypts the private key being handed over in an asymmetric encryption that only Alice can open (e.g. using some known pubkey of Alice, there are many to choose from), Bob1 similarly encrypts its privkey for Bob2, and Alice encrypts the private key to Bob1, and Alice can then broadcast all those data to all participants, and only the correct participant will be able to decrypt it.
On another privacy-related note, S6 mildly leaks to each maker its position in the route, via the timelocks.
Each Bob has to know the timelocks it is offered and which it will offer to the next participant, and the timelocks will be larger the further away that Bob is from the Alice taker.
This is a mild privacy leak, one that seems unremovable to me.
(It also exists in Lightning Network as well: we suggest the use of "shadow routes" to artificially increase the distance of forwarding nodes, as a mitigation.)

@_date: 2020-06-03 14:36:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Dmitry,
Indeed, this is precisely the issue Ruben pointed out.
Rationally, neither side will want this condition due to the deadlock and Bob will strive to avoid this, having a short real-world timeout after which Bob will force publication of the success tx if Alice does not respond in time.
There *is* a reason why it says "Bob claims the BTC funding txo before L1."
Of course, computers do crash occasionally, I am informed, so complete accidents may occur that way.
This can be mitigated by running multiple servers who are given copies of the success tx, and which will publish it regardless after a short sidereal time duration, unless countermanded by the main server (i,e, a dead man switch system).
With sufficient distribution the probability of this occurring can drop to negligible levels compared to other theoretical attacks.

@_date: 2020-06-03 14:50:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Chris again,
Over in Lightning-land, we have a concept called "irrevocably committed".
This is a state where a newly-created contract can no longer be cancelled, by publishing an older state.
In Lightning, there is a short timeframe where a new state, and its directly previous state, are both still valid, until the previous state is revoked.
Only once the previous state (that does not contain the contract) has been revoked, and only the latest state is valid, can a forwarding node actually forward the payment.
This is roughly equivalent to the funding tx for the CoinSwap being confirmed.
Until a transaction is confirmed, the UTXOs it spends (i.e. the previous state) can still be validly spent by other alternate transactions.

@_date: 2020-06-04 02:58:24
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Time-dilation Attacks on the Lightning Network 
Good morning Gleb and Antoine,
This is good research, thank you for your work.
The existence of offchain-to-onchain swap services means that the attacker needs only build one channel to the victim for this attack to work.
Rather than route to themselves, the attacker routes to a convenient service providing such a swap service, and receives the stolen funds onchain, with no need even for an incoming channel from a different node.
(Of note as well, is that the onchain contract provided by such services is the same in spirit as those instantiated in channels of the Lightning Network, thus the same attack schema works on the onchain side.)
Indeed, the attack can be mounted on such a service directly.
Even without such a service, the incoming channel need not be directly connected to the victim.
Since the issue here is that eclipsing of Bitcoin nodes is risky, it strikes me that a mitigation would be to run your Bitcoin fullnode on clearnet while running your Lightning node over Tor.
Eclipsing the Lightning node (but not the Bitcoin fullnode it depends on) "only" loses you the ability to pay, receive, or route (and thereby earn forwarding fees), but as long as your blockchain view is clear, it should be fine.
Of course, the Lightning node could still be correlated with the Bitcoin node when transactions are broadcast with the attached Bitcoin node (as noted in the paper).
Instead the Lightning node should probably connect, over Tor, to some random Bitcoin fullnodes / Electrum servers and broadcast txes to them.
And this seems to tie with what you propose: that the LN node should use a different view-fullnode from the broadcast-fullnode.
A mitigation to this would be to run a background process which sleeps for 20 minutes, then does `bitcoin-cli addnode ${BITCOINNODE} onetry`.
It might want to `disconnectnode` any previous node it attempted to connect to.
However I note that the help for `addnode` contains the text "though such peers will not be synced from", which confuses me, since it also refers to the `-connect` command line option, and `-connect` means you only connect out to the specific nodes, so if those are not synced from.... huh?
And of course the interesting part is "how do we get a `${BITCOINNODE}` that we think is not part of the eclipsing attacker?"
I am uncertain this would happen very often.
In the first place, the incoming HTLC would have "reasonable" timeouts, or else the incoming honest node would not have routed it at all, and the outgoing HTLC would be relative to this incoming one, so the outgoing honest node will still accept this.
The victim *could* instead check that the absolute timelocks seem very far in the future relative to its own view of the current blockheight.
(a forwarding node miht want to do that anyway to have an upper bound against griefing attacks)
What would definitely increase in failure rate would be payments arising from the victim node; the victim node believes the blockheight to be much lower than it actually is, and either the payee node, or some intermediate node along the route, will claim to have too little time to safely forward the funds.
This does not help for nodes which are primarily forwarding nodes.

@_date: 2020-06-04 16:37:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning yet again Chris,
I realized that the *other* improvement I proposed in the [CoinSwapCS issue]( would help with this.
Specifically, `nLockTime`-protected Backouts.
Suppose we have an S6 route as so, with Alice as taker and Bob1 and Bob2 as makers:
    Alice -> Bob1 -> Bob2 -> Alice
We assume here that Bob1 and Bob2 directly talk to Alice and that if Bob1 wants to talk to Bob2 it is done via Alice, so in the below if we say "Bob1 sends to Bob2" we imply that this is done via Alice.
1.  Alice solicits fresh pubkeys from Bob1 and Bob2.
2.  Alice gives timeouts L1 and L2 to Bob1, and L2 and L3 to Bob2, such that L1 > L2 > L3, as well as negotiated amount, fees, etc.
3.  Alice creates (but does NOT sign) a funding tx paying to Alice && Bob1 and gives the txid to Bob1.
4.  Bob1 creates and signs a tx spending from the Alice funding tx and paying to Alice, with `nLockTime = L1`, and gives the signature to Alice.
5.  Bob1 creates (but does NOT sign) a funding tx paying to Bob1 && Bob2 and gives the txid to Bob2.
6.  Bob2 creates and signs a tx spending from the Bob1 funding tx and paying to Bob1, with `nLockTime = L2`, and gives the signature to Bob1.
7.  Bob2 creates (but does NOT sign) a funding tx paying to Bob2 && Alice and gives the txid to Alice.
8.  Alice creates and signs a tx spending from the Bob2 funding tx and paying to Bob2, with `nLockTime = L3`, and gives the signature to Bob2.
9.  Alice signals everyone to sign their respecting funding txes and broadcast them.
The rest of the CoinSwap protocol executes as normal once the funding txes are deeply confirmed.
The only thing that Bob1 (resp. Bob2) needs to wait for is that the signatures for the incoming HTLC / PTLC have been received before forwarding to the next hop.
This allows all funding txes to be confirmed in the same block, or even in some suitable random order (by having Alice send the signal out at different times/blocks to different makers).
The `nLockTime`d backout transactions are sufficient to allow everyone to recover their funds unilaterally in case one of the other funding txes do not confirm.
A similar technique can be done for SAS as well, but this removes the lack of encumbrance in the LTC-side output of SAS, which removes the advantage of having an otherwise unencumbered output.
In effect, the above creates Spilman unidirectional payment channels along the route, bringing the fiddly timing details offchain where it is less visible to observers.
However, note that this still allows a form of griefing attack.
Basically, Alice can induce Bob1 and Bob2 to lock their funds for some time, by completing the above ritual, but not signing and broadcasting its own funding tx.
Bob1 and Bob2 will have been induced to lock their funds for L2 and L3, respectively, while Alice only has to RBF away its own funding tx.
Alice might do this if it is actually another maker and it wants to take out its competitors Bob1 and Bob2, reducing their available liquidity for a time and cornering the SwapMarket.
This can be mitigated by replacing step 9 with:
9.  Alice gives its signed funding tx to Bob1.
10.  Bob1 gives its signed funding tx to Bob2.
11.  Bob2 gives its signed funding tx to Alice.
12.  Alice signals everyone to broadcast their funding txes.
Then Bob1 (resp. Bob2) can monitor the mempool/blockchain and check as well if its outgoing funding tx has been broadcast/confirmed, and if so broadcast the incoming funding tx.
Or better, if Bob1 (resp. Bob2) does not receive the Alice signal fast enough, it will broadcast its incoming funding tx anyway.
This is only a mitigation: Alice could have pre-prepared a replacement to the funding tx that it broadcasts near miners just before it signals Bob1 and Bob2 to broadcast all transactions.
For full protection against griefing attacks, Bob1 (resp. Bob2) have to wait for the incoming funding tx to be confirmed deeply before broadcasting its outgoing funding tx as well.

@_date: 2020-06-05 11:44:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Time-dilation Attacks on the Lightning Network 
Good morning Aymeric,
Well, in the interest of using the wrong tool for a highly important job, let me present this thought:
* The Tor network is weakened due to its dependence on a limited set of exit nodes.
* "Direct", within-Tor rendezvous points are good, i.e. Tor hidden services.
* Thus, there is no issue with Tor-to-Tor or clearnet-to-clearnet connections, the issue is with Tor-to-clearnet connections.
* Of course, no miner is going to run over Tor because latency, so all the miners will be on clearnet.
* So make your own bridge between Tor and clearnet.
* Run two fullnodes on your computer (with sufficient ingenuity, you can probably share their block storages, or make one pruning).
* One fullnode is on the public network but runs in `blocksonly` so it does not propagate any transactions (which might be attached to your public IP).
* The other fullnode is on the Tor network and has an `-addnode` to the public-network node via `localhost`, which I assume is very hard for an eclipse attacker to get at.
* Use the Tor-fullnode to propagate your transactions.
Of course, the eclipse attacker can still attack all Tor exit nodes and block outgoing transaction traffic to perform eclipse attacks.
And if you decide to propagate transactions to the public-network node then you pretty much lose your privacy there.

@_date: 2020-06-06 01:40:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Chris,
No, it is:
2of2 multisig (Alice+Bob) --(nLockTime=locktime1)-> Alice
The timelock is  imposed as a `nLockTime`, not as an `OP_CLTV` (so not in the output of the tx, but part of the tx), and the backout returns the funds to Alice, not sends it to Bob.
This transaction is created *before* the contract transaction.
The order is:
* Create (but not sign) Alice funding tx (Alice --> Alice+Bob).
* Create and sign Alice backout transaction (Alice+Bob -(nLockTime=locktime1)-> Alice).
* Create (but not sign) Bob funding tx (Bob --> Alice+Bob+sharedSecret).
* Create and sign Bob backout transaction (Alice+Bob+sharedSecret -(nLocktime=locktime2)-> Bob) where timelock2 < timelock1.
* Sign and broadcast funding txes.
  * At this point, even if Bob funding tx is confirmed but Alice funding tx is not, Bob can recover funds with the backout, but Alice cannot steal the funds (since there is no hashlock branch at this point).
* When Alice funding tx is confirmed, create and sign contract transaction (Alice+Bob --> Alice+timelock1 OR Bob+hashlock).
* When Bob funding tx is confirmed and Bob has received the Alice contract transaction, create and sign Bob contract transaction (Alice+Bob+sharedSecret --> Bob+timelock2 OR Alice+hashlock).
* Continue as normal.
In effect, the backout transaction creates a temporary Spilman unidirectional time-bound channel.
We just reuse the same timelock on the HTLC we expect to instantiate, as the time bound of the Spilman channel; the timelock exists anyway, we might as well reuse it for the Spilman.
Creation of the contract tx invalidates the backout tx (the backout tx is `nLockTime`d, the contract tx has no such encumbrance), but the backout allows Alice and Bob to fund their txes simultaneously without risk of race loss.
However, they do still have to wait for (deep) confirmation before signing contract transactions, and Bob has to wait for the incoming contract transaction as well before it signs its outgoing contract transaction.
The protocol is trivially extendable with more than one Bob.
The insight basically is that we can split CoinSwap into a "channel establishment" phase and "HTLC forwarding" phase followed by "HTLC resolution" and "private key handover".
HTLC forwarding and HTLC resolution are "done offchain" in the channels, and channel establishment can be done in any order, including reverse.
Indeed, the Spilman channel need not have the same timelock as the HTLC it will eventually host: it could have a shorter timelock, since the contract transaction has no `nLockTime` it can be instantiated (with loss of privacy due to the nonstandard script) before the Spilman timeout.

@_date: 2020-06-06 03:59:30
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning again Chris,
I am uncertain if you are aware, but some years ago somebody claimed that 2p-ECDSA could use Scriptless Script as well over on lightning-dev.
* * I cannot claim to follow the math enough to say it is actually secure, but the idea does exist.
If this is sufficiently secure, we can fold the Spilman backout into the scriptless script swap as well.
* Alice creates secret keypairs A[0] = a[0] * G, A[1] = a[1] * G
* Bob creates secret keypairs B[0] = b[0] * G, B[1] = b[1] * G
* Alice creates (but does not sign) funding from Alice -> A[0] && B[0]
* Bob provides partial signature for A[0] && B[0] -(nLockTime=locktime1)-> Alice to Alice and Alice completes this signature and stashes it.
* Bob creates (but does not sign) funding from Bob -> A[1] && B[1]
* Alice provides partial signature for A[1] && B[1] -(nLockTime=lockTime2)-> Bob to Bob and Bob completes this signature and stashes it.
* Alice and Bob sign and broadcast their funding transactions.
  * This can safely be done in any order; Bob will refuse to continue with the protocol until it sees Alice funding is confirmed, and will abort if locktime2 is too near.
* Alice waits for Bob funding tx to confirm.
* Alice provides a 2p-ECDSA adaptor signature for A[1] && B[1] --> Alice; the adaptor signature, when completed, reveals the secret a[0] to Bob.
* Bob waits for Alice funding tx to confirm.
* Bob provides the partial signature for the given adaptor signature for A[1] && B[1] --> Alice and  Alice completes this signature and stashes it.
* Alice gives a[0] outright to Bob.
* Bob gives b[1] outright to Alice.
* Alice spends the A[1] && B[1] output before locktime2.
* Bob spends the A[0] && B[0] output before locktime1.
I also pointed out the griefing problem in Lightning also applies to SwapMarket.
Bob can limit the griefing problem by requiring that locktime2 <= now + 12, and requiring that locktime1 >= now + 60.
This means that Alice has to lock its funds for 10 hours if it forces Bob to lock its funds for 2 hours, making it undesirable as an attack on competing makers.
This does prevent chaining (no maker is going to accept the outgoing), but if Alice wants chaining it can always use the private key handed over to immediately start a funding tx with another Bob.
(This is not a good solution for griefing in the Lightning Network since channels are intended to be reused there, whereas the Spilman channels in CoinSwap exist only to allow funding transactions to confirm in any order onchain, and are used only for the specific swap; in Lightning the forwarding node has an incentive to release the incoming HTLC immediately instead of imposing the incoming wait time since the funding can be reused for a different payment, but in CoinSwap it cannot be reused anyway, so it could just let the incoming timelock lapse instead of releasing that encumbrance as would be done in Lightning.)

@_date: 2020-06-06 04:25:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning a third time Chris,
Now unrelated to the funding order, but one of the reasons why timeliness is desirable for CoinSwap is that if possible, we want to ensure that sends from a user wallet are not correlatable with receives into that wallet.
Thus, there is the strong suggestion that before sending to a payee, the user wallet should swap, then use the swapped funds to pay the payee, i.e. swap-on-pay.
JoinMarket does this in `sendpayment.py`, for example, and this is the recommended way to perform payments out of the JoinMarket wallet.
Let me propose an alternative: swap-on-receive+swap-on-change.
ZeroLink already suggests that wallets maintain two internal wallets: a pre-mix wallet and a post-mix wallet.
With swap-on-receive, when the user wants a receive address, the wallet gets it from the pre-mix wallet address.
Then, when wallet notices any unspent funds on any pre-mix wallet address, the wallet automatically swaps it into the post-mix wallet.
This is swap-on-receive.
Long-term HODLing goes into post-mix wallet addresses.
Then, when sending, the wallet selects from the post-mix wallet coins, and spends those coins directly into the payee address.
If there is no exact amount, it has to have change.
The change output does *not* go to the pre-mix or post-mix wallet address.
Instead, it goes to a 2-of-2 funding outpoint for a new swap immediately.
This lets the payee receive its funds quickly, as soon as the transaction confirms, without waiting for the CoinSwap to complete.
Of course, the user now has to be online to *fully* receive funds (the user cannot spend the funds until it is in the post-mix wallet).

@_date: 2020-06-07 00:40:51
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Stamping transaction 
Good morning Mostafa,
First off, the proposed mechanism can be made into a softfork by using an unspendable `scriptPubKey` with 0 output value.
For example, a stamp could by convention be any 0-value output whose `scriptPubKey` is ` OP_0`, which should be unspendable.
Post-softfork nodes would reject blocks where some transaction is stamped and the stamped `` is not within the last N blocks.
Pre-softfork nodes would not see anything special about the unspendable `scriptPubKey` and would just accept it (but would not relay such transactions due to standardness).
Engineering-wise, block validation now needs to memorize the last N block hashes.
The mempool design currently assumes that a transaction that enters the mempool is always valid unless any UTXOs it spends have been removed.
This is important since miner block selection algorithms assume the mempool contains transactions that are currently valid.
Thus, there is the additional need to drop transactions from the mempool if they are stamped with a block that has dropped from the stamp TTL.
Another issue is incentives.
The stamp takes up blockchain space that is paid for by the creator of the transaction.
Further, the creator of the transaction gains no advantage from the stamped transaction; it is others who gain an advantage (the stamped transaction is more likely to be dropped from the mempool).
Discounting the stamp somehow will probably make this into a hardfork.
It might be sneaked into the witness somehow by adding it as a field somewhere in the new parts of Taproot (there is, a new block of data in Taproot that can be used for this), but note that the cost will still be non-zero (and users of this feature will still have a disadvantage in that their transactions are more likely to be dropped from the mempool).
Finally, it is expected to increase bandwidth use since a dropped stamped transaction will probably be rebroadcast with a new stamp, so effectively the transaction is retransmitted again with a different stamp.

@_date: 2020-06-07 15:01:42
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Stamping transaction 
Good morning Mostafa,
Stamping transaction is not how you would be able to keep **blockchain** size low.
The reason why very old history is retained is that, if a new node is brought up, you need to prove to that node that you are in fact the correct owner of the current coins.
Thus the entire history of Bitcoin is needed when starting a new node, and why archive nodes exist.
You might argue that banks do not do that, and that is because we want to do better than banks; we know that existing currency systems have not only the "official" minter, but also many "unofficial" minters (commonly called counterfeiters) which dilute the value of the currency.
It is this insistence on a full accounting of the provenance for every satoshi that separates Bitcoin from previous currency systems; bank fraud exists, and it hides in such sloppy techniques as deleting old transaction records.
Work has been done to have client-side validation (i.e. the owner of a coin keeps the entire history, and when paying, you hand over the entire history of your coin to the payee, instead of everyone validating every transaction).
Look up Peter Todd for some initial work on this.
That greatly reduces the chances your proposal will get into Bitcoin; you would need to have very good advantages to counterbalance the tremendous risk that hardforks introduce in the continuity of the coin.
Bitcoin has never gone through a hardfork that has not instead created a new cryptocurrency, so any solution that requires a hardfork is going to be unlikely to be accepted by everyone.
`Height_Of()` would basically be a mapping from block hashes to block heights, with the number of elements equal to the height of the blockchain, and thus continuously growing.
Thus, validation is expected to become more expensive as the blockchain grows.
Since stamped transactions have a time-to-live anyway, instead you can use a *set* of the most recent N block hashes.
Then you simply check if the stamp is in the set.
This creates a data structure that is constant in size (at each block, you remove the block from N blocks ago), which is good for validation.
A stamped tranasction has a stamp, an unstamped transaction has no stamp.
The stamped transaction is larger because of the stamp.
Larger transactions are more expensive because fees.
Thus, stamped transactions are more expensive than unstamped transactions.
Convince me why I would make *my* transaction stamped when I can just convince *everyone else* to stamp *their* transactions and use unstamped transactions myself.
If you propose that all transactions must be stamped in a new version of Bitcoin, then take note that users will prefer to run older versions and never upgrade to the new version that requires stamped transactions.
Why should users prefer a more expensive transaction format?
For the good of the network?
That is precisely an incentives problem: if it is so good for the network, then it should be good for an individual user, because the network is made up of individual users anyway; if individual users are not incentivized to use it, then that fact suggests it might not be as good for the network as you might think.
If you answer "the stamp can be discounted" then be aware that validating the stamp is still a cost on every node, and it is that cost that we want to be reflected in pricing every byte in the transaction.
For instance, UTXOs are retained, potentially indefinitely, and the UTXO lookup structure has to be very fast and is referred to at every transaction validation, so outputs (which create new UTXO entries) in SegWit are 4x more expensive than signatures, since signatures are only validated once when the transaction is queued to be put in the mempool.
Why would you think that stamping reduces mempool size?
If I wanted to I could just re-send the transaction with a fresh stamp.
Then the mempool usage would still be the same, and bandwidth use will increase (because the same transaction is now re-broadcast with a fresh stamp, and the added size of the stamps themselves).

@_date: 2020-06-08 04:56:56
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Time-dilation Attacks on the Lightning Network 
Good morning Antoine,
Nowhere, *I* am the one recommending this.
Running both Bitcoin and Lightning nodes on clearnet automatically links them, making them easier to attack, whereas running Lightning on Tor does not.
Of course, they could still be linked by onchain transaction monitoring, but at least this increases the effort to attack, hopefully it becomes marginally less desirable to attack you.
On the other hand, you *could* run them on different public IP addresses, if you happen to have more than one; for those who do not even have a single public IP address there is no real choice if you want to let others to connect to you, Tor hidden service is the only Lightning-supported way to be accessible without a public IP.
(There are sections of the world where commodity "home" internet connections do not automatically get a public IP, and the privilege of getting one may be an additional cost; though of course if you have no real intent to help support either the Bitcoin or Lightning networks, you do not need a public IP anyway, and with IPv6 it becomes less and less likely that a randomly-chosen entity would be unlucky enough to not get a public IP.)
Seeing an incoming payment that violates the max CLTV is a good indication you have been eclipsed.
On the other hand, if your Bitcoin node is eclipsed, then it seems likely your Lightning node is also eclipsed (if running over the same hardware) and you might not receive any indication over Lightning that you have been eclipsed anyway.
I suppose we need to identify just exactly *what* ways a node of either type can be eclipsed; it seems that mitigations that protect against one kind of eclipse will not work in general with other kinds of eclipse.

@_date: 2020-06-10 06:29:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Question about PayJoin effectiveness 
Good morning Mr. Lee,
There are multiple interpretations:
* The 0.05 owner is paying the 1.15 owner 0.01 BTC.
* The 1.15 owner is paying the 0.05 owner 1.11 BTC.
* The 0.05 + 1.15 owner is paying an independent user 1.16 BTC using a non-PayJoin transaction (because for example the payee currently has no coins, i.e. a new user).
It is this fact of multiple interpretations that is what PayJoin buys you in practice.
You could argue that paying 0.01 is more likely than paying 1.11 or 1.16, but that still does not give you 100% assurance --- the creators of the transaction are still getting the `100% - probability_of_paying_0.01` benefit, and reducing UTXO set size as well.
Your assertion that this is "very obvious" only exists because you already know that Alice is paying 0.01 to Bob, but that is in fact the very thing that is being obscured here.
This can be interpreted as well multiple ways:
* 0.05 + 1.15 is the same owner who wants to merge coins, and is paying the 0.40 owner 0.04 BTC.
* 0.40 + 1.15 is the same owner who wants to merge coins, and is paying the 0.05 owner 0.39 BTC.
* 0.40 + 0.05 is the same owner who wants to merge coins, and is paying the 1.15 owner 0.01 BTC.
You should probably be shuffling the inputs and outputs, or using BIP39 consistently, so that inputs and outputs do not correlate (i.e. do not necessarily group together all of Alice inputs).
If Clark can hack Alice (even just read-only access to Alice logs), they can go by one more transaction.
If Clark cannot hack Alice, then that is the sole extent Clark knows: Clark know that Bob transacted with somebody for a resulting N BTC (which is relatively uninteresting, obviously somebody who uses BTC is going to be transacting with random BTC users in BTC), without being sure that Bob was the payer or the payee in that situation.

@_date: 2020-06-10 06:47:28
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Question about PayJoin effectiveness 
Good morning again Mr. Lee,
It is also helpful to remember that Bob cannot exist in isolation, and therefore, Bob probably has:
* Employees.
* Suppliers.
* Shareholders.
For example, suppose Bob holds in reserve a 0.05 BTC UTXO in a holding wallet.
Then Bob takes the 1.16 UTXO it got from Alice and transfers 1.12 BTC to the holding wallet:
    Bob merchant wallet 1.16 --___-- 1.17 Bob holding wallet
    Bob holding wallet  0.05 --   -- 0.04 Bob merchant wallet
The above looks exactly like one of the "customer pays Bob" transactions, but is in fact different.
Then Bob uses the holding wallet to pay out to employees, suppliers, and shareholders, such as in a single large batched transaction, and then leaves behind another 0.05 BTC in the holding wallet (or some random small number of BTC) for the next time Bob has to pay to employees/suppliers/shareholders.
So the transaction below:
    1.16 --___-- 1.17
    0.05 --   -- 0.04
*could* be interpreted as the 0.05 owner paying to the 1.16 owner, but in fact that is just Bob preparing the incoming funds from the merchant front-end for processing to send to its own liabilities.

@_date: 2020-06-10 07:09:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Mr. Lee,
CoinSwap lets you buy privacy at whatever rate is manageable for you.
You can buy a simple non-routed non-multitransaction CoinSwap, for example, instead of larger sections like the above, depending on your privacy needs.
Even doing a non-routed non-multitransaction CoinSwap would help fungibility of those doing more complex setups, because the tiny CoinSwaps you make are made of "the same things" that the more complex CoinSwaps are made of.
Overall, multiple mixing techniques cover a wide range of cost and privacy.
* PayJoins are cheap and almost free (you are coordinating with only one other participant who is strongly incentivized to cooperate with you, and making a single overall tx) but buys you only a small dollop of privacy (transaction can be misinterpreted by chain analysis, but probabilistic analysis can be "reasonably accurate" for a few transactions).
* Equal-valued CoinJoins are slightly more expensive than PayJoins but give a good amount of privacy (you are coordinating with multiple participants, and probably paying coordination/participation fees, but *which* output is yours will give probabilistic analysis a run for its money, although it is obvious that you *did* participate in a CoinJoin).
* CoinSwaps are a good bit more expensive than equal-valud CoinJoins but give a significant amount of privacy for their cost (you are coordinating with multiple participants and paying coordination/participation fees *and* you run the risk of getting your funds timelocked in case of network communications problems or active hacking attempts, but it is hard for chain analysis to even *realize* that a CoinSwap even occurred, i.e. it is steganographic).
Chris argues that CoinSwap gives better privacy:cost ratios than equal-valued CoinJoins, you can wait and see if he gives more supporting arguments regarding this, but overall the various mixing tech exists to give choice on how much privacy you buy.

@_date: 2020-06-10 07:09:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Mr. Lee,
CoinSwap lets you buy privacy at whatever rate is manageable for you.
You can buy a simple non-routed non-multitransaction CoinSwap, for example, instead of larger sections like the above, depending on your privacy needs.
Even doing a non-routed non-multitransaction CoinSwap would help fungibility of those doing more complex setups, because the tiny CoinSwaps you make are made of "the same things" that the more complex CoinSwaps are made of.
Overall, multiple mixing techniques cover a wide range of cost and privacy.
* PayJoins are cheap and almost free (you are coordinating with only one other participant who is strongly incentivized to cooperate with you, and making a single overall tx) but buys you only a small dollop of privacy (transaction can be misinterpreted by chain analysis, but probabilistic analysis can be "reasonably accurate" for a few transactions).
* Equal-valued CoinJoins are slightly more expensive than PayJoins but give a good amount of privacy (you are coordinating with multiple participants, and probably paying coordination/participation fees, but *which* output is yours will give probabilistic analysis a run for its money, although it is obvious that you *did* participate in a CoinJoin).
* CoinSwaps are a good bit more expensive than equal-valud CoinJoins but give a significant amount of privacy for their cost (you are coordinating with multiple participants and paying coordination/participation fees *and* you run the risk of getting your funds timelocked in case of network communications problems or active hacking attempts, but it is hard for chain analysis to even *realize* that a CoinSwap even occurred, i.e. it is steganographic).
Chris argues that CoinSwap gives better privacy:cost ratios than equal-valued CoinJoins, you can wait and see if he gives more supporting arguments regarding this, but overall the various mixing tech exists to give choice on how much privacy you buy.

@_date: 2020-06-10 10:58:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Chris,
No, I think you misunderstand my proposal.
If the user is doing swap-on-receive, the user already has an anonymous UTXO, they can just transfer it directly in full to the VPN without using a CoinSwap.
The number of CoinSwaps involved is the same: one.
So the difference is:
* swap-on-receive:
  * I get some coins from an exchange, giving them my contact information and bank information and all the places I have ever inhabited in my entire existence and an unfertilized egg sample and an archive of my diary and let them invasively scan my cognitive substrate.
  * I send the coins to my CoinSwap wallet.
  * The CoinSwap wallet automaticaly CoinSwaps the coins into a new UTXO.
    * One CoinSwap.
  * I tell the CoinSwap wallet to send it all to the VPN.
    * My CoinSwap wallet knows my coins are already cleaned, so it creates a plain 1-input 1-output transaction directly to the VPN address.
* swap-on-pay:
  * I get some coins from an exchange, giving them my contact information and bank information and all the places I have ever inhabited in my entire existence and an unfertilized egg sample and an archive of my diary and let them invasively scan my cognitive substrate.
  * I send the coins to my CoinSwap wallet.
  * I tell the CoinSwap wallet to send it all to the VPN.
    * My CoinSwap wallet automatically arranges a CoinSwap into the VPN address.
      * One CoinSwap.
So in both cases the same expected number of CoinSwaps is done, i.e. one.
Note that there are still details like how much onchain fees are and how much CoinSwap maker fees are and etc etc but they exist for both flows anyway.
So I would still be buying slightly more than my target amount, and if there is any change I could just designate it to be added to the mining fees or a donation to ZmnSCPxj, because ZmnSCPxj is so awesome.
What swap-on-receive+swap-on-change instead does is just amortize the timing of the CoinSwaps, so that you CoinSwap as soon as you receive, instead of as soon as you have to pay, so that sending payments is as fast as non-CoinSwap onchain wallets.

@_date: 2020-06-10 23:01:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Tainting, CoinJoin, PayJoin, CoinSwap 
Good morning nopara73 and Chris,
Adding on to this, we can consider the *economics* of taint.
Tainted coins are less valuable than untainted coins.
However, as pointed out as well, taint is not a consensus among all Bitcoin users.
There are no cryptographic underpinnings that would allow all nodes to agree on their individual taint analysis.
The people knocking on doors often have limited amounts of reach: there are real economic barriers to the knock-on-doors people being shipped to the other side of the Earth (fuel costs, ammunition costs, sociopolitical knock-on effects....).
Thus, suppose I am a miner with N coins.
As the coins have no history, they are "completely clean", as it were.
As a miner, I exist somewhere in the universe.
It is possible that I exist in some location on Earth (we cannot know; please ignore scurrilous slander that I am somehow existent outside of time and space).
Now suppose you have some tainted coins.
As noted, those coins are tainted only within some jurisdiction.
Outside that jurisdiction, however, they have no taint (taint is not a global consensus).
If I happen to live outside the jurisdiction where your coins are tainted, and I have some clean freshly-mined coins, I can offer this deal to you:
* Give me N+1 tainted coins for my N clean coins.
Now, again, the premise here is that there exists no global knock-on-doors people who can come to my datacenter and start asking questions to the sysads administering my computational substrate.
In that case, you might very well take the deal:
* You have not lost economic power, because the tainted coins, in your jurisdiction, are of lower value than N+1 anyway, and might even have value below that of N clean coins.
* I have gained economic power, because the tainted coins, in my jurisdiction, are not tainted and have the same cleanliness as my fresh mined coins.
This is a simple example of gains from trade, this time from jurisdictional arbitrage, thus such deals will exist.
But that is specious, as it assumes that there exists no global knock-on-doors people.
Obviously, there could exist one or more entities who are able to ship knocks-on-doors people all over the globe, taking advantage of economies of scale and reinvestment (more knock-on-doors people to knock on doors of people they can extract more economic power from to hire more knock-on-doors people) to achieve practically global coverage.
Against this, we must remember that ultimately censorship resistance of the coin is what can protect against such an attacker, which can impose its own non-consensual-but-pretty-damn-important view of taint practically globally.
Censorship resistance requires that owners of coins have control of the keys (your keys your coins) and that they can offer bribes to miners to get their transactions committed (mining fees).
Custodiality makes it easier for fewer knock-on-doors people to need to be shipped to stop certain activities.
Now, the Bitcoin Casino example is of course an example of not your keys not your coins i.e. custodiality.
For the purpose of mixing, the "Bitcoin Casino" here is simply aggregating multiple UTXOs and then sending them back out to many other new UTXOs.
This is in fact the same operation that CoinJoin does, it aggregates multiple UTXOs and creates many new UTXOs to different clients with shared taint.
The advantage is that CoinJoin is still your keys your coins, you still own the keys with which to sign the CoinJoin transaction, and thus improve censorship resistance of your mixing operation.
For CoinSwap as well, we can consider that a CoinSwap server could make multiple CoinSwaps with various clients.
This leads to the CoinSwap server owning many small UTXOs, which it at some point aggregates into a large UTXO that it then uses to service more clients (for example, it serves many small clients, then has to serve a single large client that wants a single large UTXO for its own purposes).
This aggregation again leads to spreading of taint.
CoinSwap, in this regard, is something like the cofunctor of CoinJoin.
Again, the advantage here is that CoinSwap is still your keys your coins, compared to the situation with Bitcoin Casino which is custodial.
( I think it would be a good design for SwapMarket makers to avoid spending-together its owned coins when swapping, but if it *does* need to do so (i.e. its coins are all too split up and it becomes unable to serve a client without spending more than one coin in a tx), to spend-together *all* its UTXOs and try to serve as many takers as possible in a single tx, to simulate precisely the batching operations that custodial services use, thus appearing as some new custodial service, without actually *being* custodial.)
Thus, we should consider that CoinJoin and CoinSwap improve the censorship resistance, and thus improve our global resistance to a potential global attacker using taint analysis.

@_date: 2020-06-10 23:34:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Time-dilation Attacks on the Lightning Network 
Good morning Antoine and Gleb,
One thing I have been idly thinking about would be to have a *separate* software daemon that performs de-eclipsing for your Bitcoin fullnode.
For example, you could run this deeclipser on the same hardware as your Bitcoin fullnode, and have the deeclipser bind to port 8334.
Then you set your Bitcoin fullnode with `addnode=localhost:8334` in your `bitcoind.conf`.
Your Bitcoin fullnode would then connect to the deeclipser using normal P2P protocol.
The deeclipser would periodically, every five minutes or so, check the latest headers known by your fullnode, via the P2P protocol connection your fullnode makes.
Then it would attempt to discover any blocks with greater blockheight.
The reason why we have a separate deeclipser process is so that the deeclipser can use a plugin system, and isolate the plugins from the main fullnode software.
For example, the deeclipser could query a number of plugins:
* One plugin could just try connecting to some random node, in the hopes of getting a new connection that is not eclipsed.
* Another plugin could try polling known blockchain explorers and using their APIs over HTTPS, possibly over Tor as well.
* Another plugin could try connecting to known Electrum servers.
* New plugins can be developed for new mitigations, such as sending headers over DNS or blocks over mesh or etc.
Then if any plugin discovers a block later than that known by your fullnode, the deeclipser can send an unsolicited `block` or `header` message to your fullnode to update it.
The advantage of using a plugin system is that it becomes easier to prototype, deploy, and maybe even test new de-eclipsing mitigations.
At the same time, by running a separate daemon from the fullnode, we provide some amount of process isolation in case some problem with the plugin system exists.
The deeclipser could be run by a completely different user, for example, and you might even run multiple deeclipser daemons in the same hardware, with different non-overlapping plugins, so that an exploit of one plugin will only bring down one deeclipser, with other deeclipser daemons remaining functional and still protecting your fullnode.
Finally, by using the P2P protocol, the fullnode you run could be a non-Bitcoin-Core fullnode, such as btcd or rust-bitcoin or whatever other fullnode implementations exist, assuming you actually want to use them for some reason.
What do you think?

@_date: 2020-06-11 11:51:03
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Hiding CoinSwap Makers Among Custodial Services 
Good morning Chris, and bitcoin-dev (but mostly Chris),
I made a random comment regarding taint on bitcoin-dev recently: I want to propose some particular behaviors a SwapMarket maker can engage in, to improve the privacy of its customers.
Let us suppose that individual swaps use some variant of Succinct Atomic Swap.
Takers take on the role of Alice in the SAS description, makers take on the role of Bob.
We may be able to tweak the SAS protocol or some of its parameters for our purposes.
Now, what we will do is to have the maker operate in rounds.
Suppose two takers, T1 and T2, contact the sole maker M in its first ever round.
T1 and T2 have some coins they want to swap.
They arrange things all the way to confirmation of the Alice-side funding tx, and pause just before Bob creates its own funding tx for their individual swaps.
The chain now shows these txes/UTXOs:
     42 of T1 --->  42 of T1 & M
     50 of T2 --->  50 of T2 & M
    100 of T1 ---> 100 of T1 & M
    200 of M  -
Now the entire point of operating in rounds is precisely so that M can service multiple clients at the same time with a single transaction, i.e. batching.
So now M provides its B-side tx and complete the SAS protocols with each of the takers.
SAS gives unilateral control of the outputs directly to the takers, so we elide the fact that they are really 2-of-2s below:
     42 of T1 --->  42 of T1 & M
     50 of T2 --->  50 of T2 & M
    100 of T1 ---> 100 of T1 & M
    200 of M  +-->  11 of M
              +--> 140 of T1
              +-->  49 of T2
(M extracted 1 unit from each incoming coin as fee; they also live in a fictional universe where miners mine transactions out of the goodness of their hearts.)
Now in fact the previous transactions are, after the SAS, solely owned by M the maker.
Now suppose on the next round, we have 3 new takers, T3, T4, and T5, who offer some coins to M to CoinSwap, leading to more blockchain data:
     42 of T1 --->  42 of T1 & M
     50 of T2 --->  50 of T2 & M
    100 of T1 ---> 100 of T1 & M
    200 of M  -+->  11 of M
               +-> 140 of T1
               +->  49 of T2
     22 of T3 --->  22 of T3 & M
     90 of T3 --->  90 of T3 & M
     11 of T4 --->  11 of T4 & M
     50 of T4 --->  50 of T4 & M
     20 of T5 --->  20 of T5 & M
In order to service all the new takers of this round, M takes the coins that it got from T1 and T2, and uses them to fund a new combined CoinSwap tx:
     42 of T1 --->  42 of T1 & M -+--+-> 110 of T3
     50 of T2 --->  50 of T2 & M -+  +->  59 of T4
    100 of T1 ---> 100 of T1 & M -+  +->  14 of T5
                                     +->   9 of M
    200 of M  -+->  11 of M
               +-> 140 of T1
               +->  49 of T2
     22 of T3 --->  22 of T3 & M
     90 of T3 --->  90 of T3 & M
     11 of T4 --->  11 of T4 & M
     50 of T4 --->  50 of T4 & M
     15 of T5 --->  15 of T5 & M
That transaction, we can observe, looks very much like a batched transaction that a custodial service might produce.
Now imagine more rounds, and I think you can begin to imagine that the magic of transaction batching, ported into SwapMarket, would help mitigate the blockchain size issues that CoinSwap has.
Makers are expected to adopt this technique as this reduces the overall cost of transactions they produce, thus they are incentivized to use this technique to increase their profitability.
At the same time, it spreads taint around and increases the effort that chain analysis must go through to identify what really happened.

@_date: 2020-06-12 05:43:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] WabiSabi Inside Batched CoinSwap 
THIS ENTIRE PROTOCOL IS NOVEL CRYPTO AND HAS NO PROOF THAT IT IS SECURE AND PRIVATE AND WHY WOULD YOU TRUST SOME RANDOM PSEUDONYM ON THE INTERNET SRSLY.
While [WabiSabi]( is planned for some kind of CoinJoin operation, a limitation is that the use of CoinJoin creates a transaction where the inputs are known to be linked to the outputs, as the generated transaction directly consumes the inputs.
It would be better if the server in the WabiSabi created outputs from independent outputs it owns, acquired from previous clients.
Then the outputs would, onchain, be linked to previous clients of the server instead of the current clients.
This is precisely the issue that CoinSwap, and the new swap scheme [Succinct Atomic Swaps]( can be used to solve.
By using [Batched CoinSwap]( makers can act as WabiSabi servers, and batched takers can act as WabiSabi clients.
Of course, WabiSabi has the advantage that payments between the clients are obscured from the server.
But a naive CoinSwap requires that outputs from the maker be linkable, at least by the maker, to inputs given to the maker, which is precisely the information that WabiSabi seeks to hide from the server.
However, by instead using [Signature Selling]( in combination with standard Scriptless Script adaptor signatures, it is possible to arrange for a CoinSwap to occur without the make being able to link outputs to inputs.
Signature Selling
The final output of the Schnorr signing process is a pair (R, s) for a pubkey A = a * G and ephemeral nonce R = r * G, where:
    s = r + h(P | R | m) * a
Now, instead of the pair (R, s), the signer can provide (R, s * G).
The receiver of (R, s * G) can validate that s * G is correct using the same validation as for Schnorr signatures.
    s * G = R + h(P | R | m) * A
The receiver of (R, s * G) can then offer a standard Scriptless Script adaptor signature, which when completed, lets them learn s.
The receiver may incentivize this by having the completed signature authorize a transaction to the sender of the original (R, s * G), so that the completed signature atomically gives the receiver the correct signature.
This can be used as a basis for atomic CoinSwap, and which we will use in this proposal.
Note that even in a MuSig case, it is possible for a participant to sell its share of the final signature, after the R exchange phase in MuSig.
WabiSabi replaces blind signatures with credentials.
The primary advantage of credentials is that credentials can include a homomorphic value.
We use this homomorphic value to represent a blinded amount.
WabiSabi has a single server that issues credentials, and multiple clients that the server serves.
Clients can exchange value by swapping credentials, then claiming credentials they received from the server and exchanging them for fresh credentials.
Clients hold multiple credentials at a time, and the server consumes (and destroys) a set of credentials and outputs another set of fresh credentials, ensuring that the output value is the same as the input value (minus any fees the server wants to charge for the operation).
1.  Server issues 0-valued credentials to all clients.
2.  Clients put in money into the server by providing onchain inputs to the server plus their existing credentials, getting credentials with their input credential value plus the onchain input value.
3.  Clients swap credentials with each other to perform payments between themselves, then claim the credentials by asking the server to reissue them.
    * Receiving clients move their amounts among all the credentials they own (via server consume-reissue credential operations) so as to make one of their multiple credentials into a 0-value credential.
    * Sending clients move their amounts among all the credentials they own so that one of their multiple credentials has the sent value.
    * The receiving client exchanges its 0-value credential for the sent-value credential from the sending client, by cooperatively making a consume-reissue operation with the server.
4.  Clients then claim the value in their credentials by providing pubkeys to pay to, and amount, to the server, plus existing credentials, getting back credentials whose total value is minus the onchain output value.
5.  The server generates the final output set.
6.  The clients check that the final output set is indeed what they expected (their claimed outputs exist) and ratify the overall transaction.
    * In the CoinJoin case, the overall transaction is ratified by generating a single transaction that consumes the inputs and generates the output set, then the clients provide signatures to this transaction as ratification.
WabiSabi Inside Batched CoinSwap
A Batched CoinSwap simply means having multiple takers be serviced in a single transaction by a single maker.
Suppose Alice, Bob, and Carol are takers, and Macky is the maker.
Now suppose that Alice is secretly the princess of a magical kingdom and that Bob saved her life and her entire kingdom, involving a lot of gunfire, explosions, evil wizards, pre-asskicking one-liners, and a bomb that is defused by Bob with just 1 second left on its timer.
Alice now owes a life debt to Bob and agrees to give all her bitcoins to Bob.
However, because the existence of magic is a secret, they do not want Carol, Macky, or the entire Bitcoin world to know about this Alice-to-Bob transaction.
Macky operates as a WabiSabi server, and Alice, Bob, and Carol operate as WabiSabi clients.
Rather than generate a single CoinJoin transaction, they generate a CoinSwap operation.
First, they all agree on future blockheights L1 and L2, where L1 < L2.
Then Alice, Bob, and Carol get the starting 0-value WabiSabi credentials from Macky.
They then register inputs in the WabiSabi protocol, and also additionally perform this sub-ritual in order to "lock in" the input registration:
* Alice (resp. Bob or Carol) creates (but does *not* sign) a funding transaction from Alice coins to MuSig(Alice, Macky).
* Alice and Macky create a backout transaction, with `nLockTime` at L2, and complete the plain MuSig signing ritual.
* Alice broadcasts the original funding transaction.
Macky need not wait for the funding tx to confirm; at a later stage, if it is not confirmed, Macky can cancel the entire ritual and all value transfers within it.
Then, before transitioning to the WabiSabi output registration stage, Macky performs the following ritual with Alice, Bob, and Carol.
* Macky creates (but does *not* sign) a funding transaction from Macky to MuSig(Alice, Bob, Carol, Macky).
  * The value must be greater than or equal to the total input values; but note that Alice, Bob, and Carol need not check this, as it is automatically implied by the later output ratification phase.
* Alice, Bob, Carol, and Macky create a backout transaction, with `nLockTime` at L1 (L1 < L2), and complete the composable MuSig signing ritual.
* Macky broadcasts the original funding transaction.
Again nobody needs to wait for the Macky funding transaction to come onchain at this point.
Alice can then send Bob all her money by use of WabiSabi inter-client value transfers.
Bob should not consider this value transfer as "real" until the entire WabiSabi-in-Batched-CoinSwap ritual is complete, by the way: so for example if Bob is required to marry princess Alice contingent on getting all of the coins of Alice, Bob should avoid the altar until the entire WabiSabi-in-Batched-CoinSwap ritual completes.
Then, output registration can begin.
In output registration, Bob and Carol take on new identities, as Bobby and Carolina, respectively.
Alice, having no funds inside the WabiSabi to reclaim, has no need of the new identity.
"Bobby" claims an output, to be sent to "Bobby", as does "Carolina".
At this point, Macky checks the Alice, Bob, and Carol funding transactions were confirmed deeply.
If not, Macky aborts the CoinSwap and waits until it can reclaim its funds.
Then, Macky enters the output ratification phase.
Macky publishes all the outputs registered, which should include the outputs to "Bobby", "Carolina", and a change output to Macky.
This is done by presenting a transaction spending the Macky funding transaction output with MuSig(Alice, Bob, Carol, Macky) and outputting to "Bobby", "Carolina", and Macky change.
At this point, Alice, Bob, and Carol check that the Macky funding transaction was confirmed deeply.
If not, Alice, Bob, and Carol aborts the CoinSwap and waits until they can reclaim the funds.
Then, Alice, having no outputs, approves the payout transaction.
Bob checks that the "Bobby" output exists and is the correct value.
Carol checks the "Carolina" output exists and is the correct value.
They also check that the payout transaction would actually be valid (outputs + fees = input, current `nLockTime`, etc.).
If one of the takers thinks the server misbehaved, they can just refuse to ratify the output.
Alice, Bob, and Carol can now cooperatively ratify the outputs produced by Macky.
This is done in a long ritual.
* First, Alice, Bob, Carol, and Macky complete the MuSig signing for the payout transaction all the way to R exchange.
* Then Alice, Bob, and Carol compute and broadcast their MuSig signature shares s[A], s[B], and s[C] to each other and to Macky.
* Macky then computes its share s[M], but rather than broadcast it to the takers, broadcasts s[M] * G.
At this point, the participants now turn their attention to the inputs to the mix.
* Now, Alice (resp. Bob or Carol) knows s[A], s[B], s[C], and s[M] * G for the payout transaction.
* Alice and Macky create a claim transaction spending from Alice funding output to Macky.
* Alice and Macky complete the MuSig signing ritual for the Macky claim transaction up to R exchange.
* Macky gives its share of the Macky claim transaction signature to Alice.
* Alice provides an adaptor signature for the Macky claim transaction, whose completion would let Alice learn s[M] for the payout transaction.
After Macky gets partial adaptor signatures from Alice, Bob, and Carol, it can then broadcast s[M] to Alice, Bob, and Carol, completing the payout transaction, and also complete all the claim transaction it needs.
If any of Alice, Bob, or Carol do not provide the partial adaptor signatures for any of the incoming inputs, Macky never broadcasts s[M] and the CoinSwap and all WabiSabi internal value transfers revert.
Private Key Handover
It would be possible to hand over private keys for the Alice, Bob, and Carol funding transaction to Macky, and use a kickoff transaction followed by a `nSequence`-timelocked transactions for backing out.
This reduces the onchain space needed when Macky has another new set of clients it wants to serve.
Comparison to WabiSabi Inside CoinJoin
The above CoinSwap ritual requires more transactions onchain, and thus more expense.
However, it does buy better privacy.
* Carol never learns the inputs of Alice and Bob, and only knows that there are up to 2 other participants in the mix.
  * Carol only ever signs its own funding transaction output, and the payout transaction; the payout transaction does not reveal the inputs that other participants put into the mix.
  * For all Carol knows, the two other participants in the mix were sockpuppets of Macky, or were really just one participant using a sockpuppet.
* Bob never learns the input of Alice.
  * Bob knows how much Alice gave to Bob and that Alice gave her all to Bob, so in a CoinJoin could have scanned for CoinJoin inputs that sum up to how much Alice gave to Bob.
  * With CoinSwap, Bob has to scan recent blocks, which hopefully have much larger input sets and require more effort (and hopefully more false positives).
This is because knowledge of the *other* inputs to the mix is never revealed in a batched CoinSwap, whereas they would be revealed in a CoinJoin.
All that participants learn is the inputs from *previous* takers that the maker served in the past.

@_date: 2020-06-12 08:39:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] CoinPool, 
Good morning Antoine and Gleb,
I have not studied the proposal in close detail yet, but anyway, my main takeaway roughly is:
* The core of CoinPool is some kind of multiparticipant (N > 2) offchain update mechanism (Decker-Wattenhofer or Decker-Russell-Osuntokun).
  * The output at each state of the update mechanism is some kind of splitting construction (which I have not studied in detail).
  * At each update of the state, all participants must sign off on the new state.
It seems to me that it would be possible to use a [WabiSabi protocol]( during negotiation of a new state.
Now, WabiSabi is a client-server protocol.
As all participants in the CoinPool are needed in order to ratify each new state anyway, they can simply elect one of their number by drawing lots, to act as server for a particular state update.
Then the participants can operate as WabiSabi clients.
Each participant registers the outputs they currently own in the current state, getting credentials that sum up to the correct value.
Then, during the WabiSabi run, they can exchange credentials among the participants in order to perform value transfers inside the WabiSabi construction.
Then, at output registration, they register new outputs to put in the next state of the CoinPool.
In order to hide transfers from the elected WabiSabi server, participants can maintain two coins in every state, and move coins randomly across the two coins they own at each state update, in order to hide "real" transfers from the elected server.
Then, after output registration, the participants ratify the new state by signing off on the new state and revoking the previous state, using the update mechanism.
Of course, we should note that one desired feature for CoinPool in the original proposal is that a participant can exit, and the CoinPool would still remain valid, but only for the remaining participants.
This is arguably a mild privacy leak: every other participant now knows how much that particular participant took out from the CoinPool.
Indeed, from what I can understand, in order to properly set up the splitting transactions in the first place, at each state every participant needs to know how much each other participant actually owns in the CoinPool at that point in time.
To hide how much each participant owns in the CoinPool from other participants, we would have to make unilateral closes expose all the current outputs, without trying to identify *which* participant exited the CoinPool, and thus preventing anyone else from figuring out exactly how much each *other* participant actually owns in the CoinPool on exit.
That way, output addresses can be to fresh pseudonyms of the participant, removing all linkages of participant to amount they own, and each participant can maintain multiple outputs per state for their own purposes and to mildly obscure exactly how much they own in total.
If we drop that feature (of being able to exit a participant without closing the *entire* CoinPool), of course, we need to mildly disincentivize a participant closing unilaterally for trivial reasons.
We can do this by using `SIGHASH_ANYPREVOUT` to force whoever performs a unilateral close of the CoinPool to pay the onchain fees involved, so that it would have to be a good reason indeed to perform a unilateral close.

@_date: 2020-06-12 14:53:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] WabiSabi Inside Batched CoinSwap 
Good morning list,
Sorry, the taker funding transactions should have the nearer locktime L1, and the maker funding transctions should be the one with the later locktime L2.
This forces Macky to claim the incoming funds earlier, and claiming any of them unlocks the outgoing payout transaction.

@_date: 2020-06-13 00:45:12
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] CoinPool, 
Good morning Antoine,
Perhaps not necessarily.
An advantage of WabiSabi is I can pretend to be two or more participants.
For example, I can pretend to be "Alice" and "Bob", and pretend that "Alice" owes a life debt to "Bob".
At initial state setup, I put a 1.0 BTC coin as "Alice" and a 0.5 BTC coin as "Bob".
Now, at each state update I need to sign as "Alice" and "Bob".
However, after the first initial state, I can use a new persona "Bobby" to *own* my coins, even though I still have to sign as "Alice" and "Bob" in every state update.
What the other pool participants see is that the 1.0 BTC "Alice" coin and the 0.5 BTC "Bob" coin are merged into the 1.5 BTC "Bobby" coin.
What they cannot be sure of is:
* "Alice" paid to "Bob", who is now pretending to be "Bobby".
* "Bob" paid to "Alice", who is now pretending to be "Bobby".
* "Alice" and "Bob" are the same person, and is also pretending to be "Bobby".
All the other participants know is that whoever owns the coin *now* is still part of the pool, but cannot be sure which participant *really* owns which coin, and whether participants are sockpuppets (which is why it should use n-of-n at each state update, incidentally).
In effect, it "imports" the possibility of PayJoin inside the CoinPool construction.

@_date: 2020-06-13 01:20:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] CoinPool, 
Good morning Antoine,
By dropping the requirement that a participant can seamlessly leave the CoinPool, it allows participants to split up their coins among new aliases and to use a different identity for later claiming coins.
With WabiSabi, none of the other participants can get a mapping between current-state aliases and the actual participants.
Now, in order to authorize moving coins from an output on the current state to a new output on the next state, obviously the pool needs to get a signature from its current owner.
Ideally, we would not want to have to implement SCRIPT inside the CoinPool software.
And with Taproot, a pubkey can hide one or more SCRIPTs.
If we use pubkeys as the identities of owners of coins, then it allows an alias to encode a SCRIPT.
With the combination of both features, we can instantiate HTLCs (or, with `SIGHASH_ANYPREVOUT`, PTLCs) inside a CoinPool "alias" pubkey identity, allowing for interoperation with LN.
Now suppose I have 1.0 BTC in a CoinPool.
I want to make an HTLC with you (hashlocked to you, timelocked to me), for 0.5 BTC.
I encode the HTLC SCRIPT, and put it into a Taproot whose internal pubkey is a MuSig of fresh identities of mine and yours.
Then, inside the CoinPool, I split my 1.0BTC to a 0.5BTC coin to a fresh identity of mine, and 0.5BTC to our HTLC Taproot.
If you can acquire the hash, you give it to me, and I am supposed to hand you a partial signature share to the HTLC Taproot that you can later complete and present to the CoinPool in the next update round in order to get the HTLC value.
If I do not hand you the signature share even after you hand the hash, you just drop the entire CoinPool onchain, instantiating the HTLC Taproot output onchain, and using the SCRIPT branch to claim using the hash you know.
If the timelock expires, I ask you to hand over your partial signature to the HTLC Taproot that I can later complete and present to the CoinPool in the next update round to recover the HTLC value.
If you do not hand over the signature share, I drop the CoinPool onchain, which instantiates the HTLC Taproot output onchain, and use the SCRIPT branch to claim using the timelock branch.
You can also ask to abort the HTLC "early", before the timelock expires, by handing over your partial signature to the HTLC Taproot, which I can later complete and present to the CoinPool in the next update round.
This is equivalent to `update_fail_htlc` in the current LN BOLT spec.
This allows operation of any SCRIPT, incidentally, without requiring that CoinPool software include a SCRIPT interpreter, only signature validation.
Any time an output absolutely needs a SCRIPT, we just drop the CoinPool onchain and let onchain handle the SCRIPT interpretation.

@_date: 2020-06-16 05:23:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] CoinPool, 
Good morning Antoine, Gleb, and list,
In some ways, CoinPool is really part of a swarm of ideas:
* CoinPool
* Multiparticipant (N > 2) channels
* Channel factories
* Nodelets
What CoinPool and multiparticipant channels buy us is better flexibility with forwarding.
For example, if we compare a multiparty channel to a channel factory, suppose there exists three entities A, B, and C in the multiparty construction.
In a channel factory, each entity has to decide how much of its liquidity to tie up in a channel with a specific other peer in the multiparty construction.
This limits the practical payment forwarding when integrated into the Lightning Network.
In a CoinPool, any of the entities can forward to any of the other entities, without tying their liquidity to a channel specifically with those entities.
However, in a CoinPool, once any of the entities goes offline, the entire CoinPool can no longer update.
This is in contrast with channel factories, where, if entity C goes offline, the channel between A and B remains useable for forwarding.
In other words, channel factories degrade gracefully.
Further, we already have a decent solution for liquidity redistribution: JIT Routing by Rene Pickhardt.
Thus the liquidity issue with channel factories are somewhat mitigated (and if all participants are online, they also have the option of redistributing channel funds *inside* the factory as well, not just JIT routing), while gaining graceful degradation of the factory.
Another is that pathfinding algorithms work best if graph edges are edges and not in fact some kind of twisted multi-edge that connects more than two nodes together.
On the other hand, the participants of a CoinPool could create a "virtual node" that is a MuSig of their individual keys, and report that as the "real" node on LN gossip (each of them pretending to have a large channel with that virtual node), so that the rest of the network only sees edges that link two nodes (and existing pathfinding algos still work seamlessly, never realizing that this node is actually a virtual node that represents a CoinPool).
This is basically them creating a sort of Nodelet node, which other nodes cannot make channels to, and which uses channels with the Nodelet node as proxies for the CoinPool as a whole.

@_date: 2020-06-20 16:01:16
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning Dave,
Not a cryptographer, I just play one on the Internet, but maybe the pay-for-signature construction could work...?
Assuming a PTLC has a pointlocked branch, which involves signing with MuSig(A, B).
A offers to B the amount if B reveals the secret `t` behind `T = t * G`; A knows `T` but not `t`.
This is done by B handing over `R[B]` and `s'[B]`:
    R = R[A] + R[B] + T
    s'[B] = r[B] + h(MuSig(A, B) | R | m) * b
Then A provides its partial signature to B.
    s[A] = r[A] + h(MuSig(A, B) | R | m) * a
B has to complete the signature by:
    s = s[A] + s'[B] + t
Since A knows both `s[A]` and `s'[B]`, once it knows `s`, it can compute `t`.
Now, we can massage the equation for `s`:
    s = r[A] + h(MuSig(A, B) | R | m) * a + r[B] + h(MuSig(A, B) | R | m) * b + t
    ; multiply both sides by G
    s * G = r[A] * G + h(MuSig(A, B) | R | m) * a * G + r[B] * G + h(MuSig(A, B) | R | m) * b * G + t * G
    ; replace with public points
    s * G = R[A] + h(MuSig(A, B) | R | m) * A + R[B] + h(MuSig(A, B) | R | m) * B + T
Note that A can compute `s * G` above, because it generated `R[A]`, was given `R[B]` and `T`, and knows who `A` and `B` are.
So what A needs to do is to offer a fund that can only be claimed by leaking knowledge of `s` behind `s * G`.
A can do this by creating a new keypair `A[p4s] = a[p4s] * G` and putting a fund into it.
Then A generates an `R[A][p4s] = r[A][p4s] * G`, and computes:
    R[p4s] = R[A][p4s] + s * G
    s'[A][p4s] = r[A][p4s] + h(A | R[p4s] | m) * a[p4s]
The signed message could be a signature to `SIGHASH_NONE`, finally an actual use for that flag.
A reveals publicly (in an `OP_RETURN` as you suggest):
* `R[A][p4s]`
* `s * G`
* `s'[A][p4s]`
* `A[p4s]` - Already the Schnorr output pubkey.
In order to complete the above signature, a third party C has to learn `s` from B.
The third party has to scan every onchain 1-of-1 signature for an `s` that matches `s * G`, so there is greater processing (point multiplies are more expensive than hashes, also there are more 1-of-1s).
But once learned, the third party can complete the signature and claim the funds.
And A then learns `s`, from which it can derive `t`.
The third party learns about which channel (i.e. the UTXO that was spent to create the PTLC in the first place), but never learns `t` or `T`, which is a small but nice privacy bonus.

@_date: 2020-06-21 02:10:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning again,
If you are going to embed it in an `OP_RETURN` in the same transaction, you also need `SIGHASH_ANYPREVOUT`, otherwise you cannot embed the adaptor signature for spending from that transaction in the transaction being spent, it also implies `A[p4s] = a[p4s] * G` is a one-time-use keypair.

@_date: 2020-06-22 08:15:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Good morning Bastien,
I believe this is technically possible with current mempool rules, without miners cooperating with the attacker.
Basically, the attacker releases two transactions with near-equal fees, so that neither can RBF the other.
It releases the preimage tx near miners, and the timelock tx near non-miners.
Nodes at the boundaries between those that receive the preimage tx and the timelock tx will receive both.
However, they will receive one or the other first.
Which one they receive first will be what they keep, and they will reject the other (and *not* propagate the other), because the difference in fees is not enough to get past the RBF rules (which requires not just a feerate increase, but also an increase in absolute fee, of at least the minimum relay feerate times transaction size).
Because they reject the other tx, they do not propagate the other tx, so the boundary between the two txes is inviolate, neither can get past that boundary, this occurs even if everyone is running 100% unmodified Bitcoin Core code.
I am not a mempool expert and my understanding may be incorrect.

@_date: 2020-06-23 09:48:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Itay, Ittay, and Matan,
I believe an unstated assumption in Bitcoin is that miners are short-sighted.
The reasoning for this assumption is:
* Deployment of new mining hardware controlled by others may occur at any time you do not control.
  * Thus, any transactions you leave on the table are potentially taken by somebody else and not by you.
  * Sudden changes in hashpower distribution may reduce your expected future earnings, so any future theoretical earnings should be discounted (*in addition to* expected return-on-investment on getting money you can invest *now*).
It also strikes me that, in a world with RBF and CPFP, the same endpoint (i.e. miners earn the entire fund of the HTLC) is achieved by existing HTLCs, without the additional branch and script opcodes needed by MAD-HTLC.
For example, if an HTLC is confirmed but the hashlock-claiming transaction is not being confirmed (because miners are holding it up because Bob is offering a much higher fee in the future for the timelock-claiming transaction), then Alice can, regardless of the reason why it is not being confirmed, bump up the fee with RBF or CPFP.
If the fee bump offered by Alice is sufficiently large, then miners will start re-preferring the Alice hashlock transaction.
To counter this, Bob has to bid up its version higher.
As the timeout approaches, Alice can bump up its fee until it is just 1 satoshi short of the total fund.
It is rational for Alice to do so since at timeout, it can expect to lose the entire fund.
In order for Bob to win, it has to beat that fee, at which point it equals or exceeds the total fund, and miners get the total fund (or more).
Knowing this end-point, rational Bob will not even begin this game.
I think this research considers these two endpoints to be distinct:
* Bob misbehaves and the entire fund is punished by miners, leaving miners with the fund and Alice and Bob without money (MAD-HTLC).
* Bob misbehaves, Alice counters, and the ensuing fee war leads to fees approaching the fund value, leaving miners with the fund and Alice and Bob without money (standard HTLC).
But in practice I think both endpoints are essentially equivalent.
What MAD-HTLC can do would be to make different claims:
* Inputs:
  * Bob 1 BTC - HTLC amount
  * Bob 1 BTC - Bob fidelity bond
* Cases:
  * Alice reveals hashlock at any time:
    * 1 BTC goes to Alice
    * 1 BTC goes to Bob (fidelity bond refund)
  * Bob reveals bob-hashlock after time L:
    * 2 BTC goes to Bob (HTLC refund + fidelity bond refund)
  * Bob cheated, anybody reveals both hashlock and bob-hashlock:
    * 2 BTC goes to miner
This is an actual improvement over HTLC: Bob misbehavior leads to loss of the fidelity bond.
The above cases can be assured by requiring both Alice and Bob to sign in the alice-hashlock branch, so that the splitting of the fund is enforced, and SegWit signing so that the dependent transaction is signed before the HTLC-funding transaction is.
It can also be implemented with `OP_CHECKTEMPLATEVERIFY`.

@_date: 2020-06-25 01:38:17
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Stanga et al,
We already know that hashrate tends to trend upwards, and that we do not expect hashrate to fall except for occasional transients.
The expectation is not that new miners have different incentives.
Instead, the expectation is that current miners discount future possible gains because in the future, they expect to have less hashrate share than right now.
The only trustless way for Bob to bribe miners into deferring Alice tx is to attach the bribe to the future confirmation of the Bob tx, thus Bob is offering future-coins, not present-coins like Alice can offer, and the fact that miners expect an overall uptrend in total hashrate (leading to an overall downtrend in their hashrate share) means that miners discount the Bob offered future-coins.
The discounting is proportional to the time delay involved, as a larger delay implies greater reduction in hashrate share.
This discounting is, again, *in addition to* natural discounting a.k.a. "I will gladly pay you Thursday for a hamburger today", the hamburger seller will want some pretty stiff assurances plus a bigger payment on Thursday for giving you a hamburger today, due to expected returns on investment.
Alice already knows that a rational Bob (who it might never interact with again in the future) will take the funds at the locktime L.
Thus, Alice can offer, at time L - 1, the entire fund, minus 1 satoshi, to miners.
Alice getting 1 satoshi versus 0 satoshi is a no-brainer for Alice.
Bob can only  beat this offer by offering the entire fund, at which point Bob earns nothing and it performed an attack for no benefit.
I and some number of Lightning devs consider this to be sufficient disincentive to Bob not attacking in the first place.
Thank you for the clarification.

@_date: 2020-06-25 04:04:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Nadav,
Unfortunately this could be subject to an inversion of this attack.
Alice can wait for the timelock to expire, then bribe miners to prevent confirmation of the Bob timelocked transaction, getting the Alice hashlocked transaction confirmed.
Now of course you do mention "prior to the locktime expiry" but there is now risk at around locktime.
Particularly, "natural" orphaned blocks and short-term chainsplits can exist.
Bob might see that the locktime has arrived and broadcast the signed timelocked transaction, then Alice sees the locktime has not yet arrived (due to short-term chainsplits/propagation delays) and broadcast the signed hashlocked transaction, then in the end the Alice side of the short-term chainsplit is what solidifies into reality due to random chance on which miner wins which block.
Then Bob can now be accused of bribery, even though it acted innocently; it broadcasted the timelock branch due to a natural chainsplit but Alice hashlocked branch got confirmed.
Additional complications can be added on top to help mitigate this edge case but more complex == worse in general.
For example it could "prior to locktime expiry" can ignore a few blocks before the actual timelock, but this might allow Bob to mount the attack by initiating its bribery behavior earlier by those few blocks.
Finally, serious attackers would just use new pseudonyms, the important thing is to make pseudonyms valuable and costly to lose, so it is considered sufficient that LN nodes need to have some commitment to the LN in the form of actual channels (which are valuable, potentially money-earning constructs, and costly to set up).
Other HTLC-using systems, such as the "SwapMarket" being proposed by Chris Belcher, could use similar disincentivizing; I know Chris is planning a fidelity bond system for SwapMarket makers, for example, which would mimic the properties of LN channels (costly to set up, money-earning).

@_date: 2020-06-26 00:57:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Bitcoin 2-way-pegged childchains via Proof of Burn 
Good morning CS,
The difficulty is not so much the proof-of-whatever, but rather, the peg itself.
My understanding of your pegout from sidechain to mainchain is that this pegout is very low-bandwidth, i.e. only a tiny amount can be pegged out at each mainchain block.
This suggests to me that the sidecoin can still drop lower than maincoin during times when overall side-to-main flows are higher than main-to-side flows.
(atomic swaps cannot *maintain* a peg, they can only follow a peg if it exists; if the peg is weak, atomic swaps cannot strengthen it.
this is because atomic swaps allow a non-1:1 exchange rate, as in cross-currency atomic swaps.)
In any case, from my reading of your text, I seem, the goal is scaling ("acceptable option for lower value tx").
I studied sidechains some years ago, and, came to the conclusion that sidechains are not good for scaling.
We already know that blockchains do not scale well (excessive bandwidth use, permanent records needed to support newcomers); thus, the scaling solution for cryptocurrency cannot be via **more** blockchains.
Hence, Lightning Network.
In Lightning Network, every channel is a consensus system between two participants, hence every channel is a 2-of-2 (i.e. requires consensus of both participants to advance).
We use atomic swaps to transfer between channels and the blockchain.
The channel construction requires reference to an ultimate arbiter of any dispute/non-consensus between the channel participants; this is provided by the blockchain layer off which the channel is based.
Thus blockchain for arbitration, channels for scaling.

@_date: 2020-06-29 18:05:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MAD-HTLC 
Good morning Dave, et al.,
A thought occurs to me, that we should not be so hasty to call non-myopic strategy "rational".
Let us consider instead "myopic" and "non-myopic" strategies in a population of miners.
I contend that in a mixed population of "myopic" and "non-myopic" miners, the myopic strategy is dominant in the game-theoretic sense, i.e. it might earn less if all miners were myopic, but if most miners were non-myopic and a small sub-population were myopic and there was no easy way for non-myopic miners to punish myopic miners, then the myopic miners will end up earning more (at the expense of the non-myopic miners) and dominate over non-myopic miners.
Such dominant result should prevent non-myopic miners from arising in the first place.
The dominance results from the fact that by accepting the Alice transaction, myopic miners are effectively deducting the fees earned by non-myopic miners by preventing the Bob transaction from being confirmable.
On the other hand, even if the non-myopic miners successfully defer the Alice transaction, the myopic miner still has a chance equal to its hashrate of getting the Bob transaction and its attached fee.
Thus, myopic miners impose costs on their non-myopic competitors that non-myopic miners cannot impose their myopic competitors.
If even one myopic miner successfully gets the Alice transaction confirmed, all the non-myopic miners lose out on the Bob bribe fee.
So I think the myopic strategy will be dominant and non-myopic miners will not arise in the first place.

@_date: 2020-03-04 23:29:09
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Hash function requirements for Taproot 
Good morning LL,
Thank you very much for this work, it seems quite interesting.
I am uncertain what you mean here by "coin-tossing".
* Everybody generates fresh keypairs.
* Everybody sends the hash of their pubkey to everyone else.
* After receiving a hash of pubkey from everyone else, everybody sends their pubkey to everyone else.
* They add all their pubkeys to generate the aggregate key (and if using Taproot, use it as the internal key).
Is that correct?
In any case, the comparison to MuSig signing appears to imply interactive key generation.
The advantage of MuSig is that it requires no interactivity for key generation of n-of-n (I am told it requires interactivity to generate k-of-n).
However, it can generally be pointed out that, before you put anything into an n-of-n, you would damn well sure want to have *some* assurance that you can get it out later.
So in general you would need coordination and interaction anyway to arrange getting into an n-of-n in the first place.
On the other hand, it would be best to have at least some minimum of privacy by always interacting over Tor and having a Tor .onion address, which has absolutely horrid latency because human beings cry when peeling onions.
So in general reducing the latency by reducing communication rounds is better in general.
Counter to this, assuming you use an n-of-n in an offchain protocol of some sort, the number of communication rounds to generate the aggregate key may be dwarfed by the total number of communication rounds to create signatures to update the offchain protocol.
Counter counter to this is that one plan for reducing communications rounds for creating signatures during offchain operation is to (haha) use a Taproot with an n-of-n internal key and a tapscript that has n `OP_CHECKSIG` operations, so that for normal operation you just toss individual signatures at each other but at termination of the offchain protocol you can do the heavy MuSig-style signing with the n-of-n aggregate key.

@_date: 2020-03-24 07:42:46
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown question/poll 
Good morning Andrew,
They already do so, via an implicit "field", known as the transaction fee.
This is "implicit" since it is only the difference of the sum of all inputs with the sum of all outputs, but any Bitcoin HODLer spending their coins always need to make this decision.
This makes the vote for how much security is needed to be costly to the voter, which is appropriate: you pay for your security.
This mechanism is the same mechanism as well that is the long-term plan for the lowered block rewards in the future, and is already the best known idea to tackle this problem as well.

@_date: 2020-03-26 01:20:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Tom,
I believe I suggested this to Ruben Somsen as well in the past, but you can replace the state update mechanism with, for example, Decker-Wattenhofer decrementing-`nSequence`, which while it has a limit on the number of updates, does not have a limit on the time that a UTXO is locked in this mechanism.
You can even use the Decker-Wattenhofer trick of having a chain of decrementing-`nSequence` mechanisms to effectively multiply the number of updates that the overall mechanism can have.
The drawback is that in a unilateral close condition, the time to completely resolve the unilateral close is very large.
For a quick reference for this technique:
* The funding transaction is anchored onchain, but all succeeding transactions are offchain.
  * This funding transaction has a particular funding transaction output.
* There is a kickoff transaction, which is a 1-input 1-output transaction without any `nLockTime` or `nSequence` limits.
  * This spends the funding tx out.
  * The signer set of the output is the same as the signer set of the funding transaction output.
    * You could tweak keys or script to give a modicum of privacy.
* There is one or more decrementing-`nSequence` transactions, which are 1-input 1-output transactions.
  * Each one has a particular `nSequence` with a relative-locktime constraint.
  * This spends the kickoff transaction output.
  * The signer set of the output is the same as the signer set of the funding transaction output.
* There is one or more decrementing-`nSequence` transactions, which are 1-input 1-output transactions.
  * Each one has a particular `nSequence` with a relative-locktime constraint.
  * This spends the previous stage decrementing-`nSequence` transaction output.
  * The signer set of the output is the same as the signer set of the funding transaction output.
* Repeat the above stage a few times.
* There is one or more decrementing-`nSequence` transactions, which are 1-input multi-output transactions.
  * Each one has a particular `nSequence` with a relative-locktime constraint.
  * This spends the previous stage decrementing-`nSequence` transaction output.
  * The outputs of this transaction represent the current state inside the statechain.
The `nSequence` use means there is no time-based lifetime limit.
The decrementing-`nSequence` stages mean that earlier states have higher `nSequence` limits, and newer states have lower `nSequence` limits.
Chaining multiple such mechanisms allows you to "reset" a stage by making a single update of the higher stage, which resets all further stages.
So for example, we could have a multi-stage mechanism as below:
    ***blockchain***
       [funding tx] -+
         _ _ _ _ _ _ | _ _ _ _ _ _ _
     offchain        |
                     +->[kickoff tx]->[[14] stage]->[[14] stage]->[[14] stage]-> state outputs
The number in the brackets is the relative-locktime `nSequence` constraint in that stage transaction.
Let us suppose that we agree to decrement `nSequence` by 7 blocks at each update.
Then the first update will have:
    ***blockchain***
       [funding tx] -+
         _ _ _ _ _ _ | _ _ _ _ _ _ _
     offchain        |
                     +->[kickoff tx]->[[14] stage]->[[14] stage]->[[ 7] stage]-> state outputs
The the second update:
    ***blockchain***
       [funding tx] -+
         _ _ _ _ _ _ | _ _ _ _ _ _ _
     offchain        |
                     +->[kickoff tx]->[[14] stage]->[[14] stage]->[[ 0] stage]-> state outputs
After this update, for the next update, we would also sign the second-to-the-last stage, and reset the last stage:
    ***blockchain***
       [funding tx] -+
         _ _ _ _ _ _ | _ _ _ _ _ _ _
     offchain        |
                     +->[kickoff tx]->[[14] stage]->[[ 7] stage]->[[14] stage]-> state outputs
And so on.
Effectively it becomes a large counter, with the "least significant digit" being the last stage.
This multiplies the total number of updates your statechain can have, so for example the above uses a total unilateral close delay of 42 blocks to allow creation of 27 updates, whereas if it were a single stage those 42 blocks would only allow 7 updates.
As the first stage decrements, you can actually add more stages dependent on it, keeping a total maximum time that a unilateral close will resolve, but increasing the number of transactions that would need to be published onchain in a unilateral close.
This allows you to further extend the number of updates, possibly allowing an indefinite number of updates (at the cost of greatly increased blockchain usage in the unilateral close, which might not be feasible).
The original Decker-Wattenhofer paper "Duplex Micropayment Channels" has prettier graphics.

@_date: 2020-03-26 01:42:22
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown question/poll 
Good morning Andrew,
There are changes of meaning, and then there are changes of meaning.
Smaller changes that puny humans can understand are better than larger changes beyond the ken of mortal man.
To change the supply is far too big a change.
What happens if I own a few million Bitcoin and then accidentally lose my private keys in a tragic ear-cleaning accident?
Then the vote of that UTXO containing a few million Bitcoins will remain forever fixed and unable to change according to whatever you believe would make us as a community decide to change the inflation rate.
If you enforce that only "recently-created" UTXOs get to vote, then in order for me to affect the vote (in the happy case where I do **not** lose all my privkeys in a tragic ear-cleaning accident), I would have to make a synthetic self-paying transaction.
How is it so different from me having to make up a synthetic transaction in order to pay fees and thus affect the current security of the blockchain?
It is helpful to remember that as a UTXO gets buried deeper, its security is inevitably better, and once I have a sufficient level of security in my ownership of the coin, I will not particularly care about any improved security and will not be interested in paying for more, hence why should I support any fork which makes me pay for my security continuously when I can simply support a fork that retains the current supply and does *not* make me pay for continued security?
If I want to *spend* my Bitcoins on something --- and nothing has value until I actually utilize it --- then I *will* pay transaction fees.
The receiver of the coin would want to ensure that the received UTXO is deeply buried to the point that it has sufficient security for the receiver, before releasing or providing me with whatever I am exchanging the coin for.
Thus, if I find that there are no miners at all, I could offer a high fee to get my transaction mined.
Of course, you might say that this only pays for one block.
But in most cases I will have more value remaining beyond what I spend to the receiver, i.e. I have a change output from that transaction.
In such a case, I can  pay for more blocks by re-spending the change output to myself, paying a transaction fee each time, until the original transaction that spends to the receiver is deeply buried and the receiver credits it and then releases the product or service I am exchanging *for*.
Alternately the receiver can do the same for its *own* UTXO, and will increase the payment it demands from me in order to perform this itself; thus I still end up paying for the security of the *transaction* and not the security of the *holding*.
So there is really no need for any mechanism beyond transaction fees.

@_date: 2020-03-27 01:46:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Ruben,
Any standardness issue can be fixed by embedding it in a P2WSH / P2SH, you can use an `OP_TRUE` `redeemScript`, for instance.
Using an `OP_TRUE` `redeemScript` would allow any third party to make you cry by opportunistically spending such an output.
For example your Bitcoin-network peer could notice you broadcasting such a transaction with an `OP_TRUE` output, see you spend that output with a CPFP-RBF-ed child transaction, then instead of further broadcasting the child transaction, instead broadcast a non-RBF child transaction with tiny fee, so that it and its parent transaction will be accepted into mempools but would not be replaceable with a higher-feerate child transaction (because not RBF-flagged).
Thus, some portion of mempools will contain this poisoned low-fee child transaction and prevent the parent from being confirmed (because the parent+child fees are not enough to justify being put in a block).
Which I suppose is an argument for Full RBF aka ignore-the-RBF-flag-and-always-RBF.
The solution that I remember being proposed for this in Lightning was to give each participant its own attach-your-fees output that only that participant can spend, which works for Lightning because the set of participants in a channel is permanently fixed, but probably not for statechains.
The broadcasting of the kickoff simply means that the first stage cannot be easily changed, and you might still be able to make further updates by updating only the later stages, until the last stage is confirmable, so the kickoff being broadcast simply creates a "dead man walking" statechain.
However, the implementation complexity would probably increase tremendously.

@_date: 2020-03-27 01:46:15
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Ruben,
Any standardness issue can be fixed by embedding it in a P2WSH / P2SH, you can use an `OP_TRUE` `redeemScript`, for instance.
Using an `OP_TRUE` `redeemScript` would allow any third party to make you cry by opportunistically spending such an output.
For example your Bitcoin-network peer could notice you broadcasting such a transaction with an `OP_TRUE` output, see you spend that output with a CPFP-RBF-ed child transaction, then instead of further broadcasting the child transaction, instead broadcast a non-RBF child transaction with tiny fee, so that it and its parent transaction will be accepted into mempools but would not be replaceable with a higher-feerate child transaction (because not RBF-flagged).
Thus, some portion of mempools will contain this poisoned low-fee child transaction and prevent the parent from being confirmed (because the parent+child fees are not enough to justify being put in a block).
Which I suppose is an argument for Full RBF aka ignore-the-RBF-flag-and-always-RBF.
The solution that I remember being proposed for this in Lightning was to give each participant its own attach-your-fees output that only that participant can spend, which works for Lightning because the set of participants in a channel is permanently fixed, but probably not for statechains.
The broadcasting of the kickoff simply means that the first stage cannot be easily changed, and you might still be able to make further updates by updating only the later stages, until the last stage is confirmable, so the kickoff being broadcast simply creates a "dead man walking" statechain.
However, the implementation complexity would probably increase tremendously.

@_date: 2020-03-28 02:12:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown question/poll 
Good morning Andrew,
Fortunately in our case, only the top 4,000,000 weight worth of transactions gets in a block.
Every bitcoin spender has an incentive to spend as little as possible to get into this top 4,000,000 weight and no more, but they still have to outbid every other user who wants the same security.
Some bitcoin spender will then decide that overpaying slightly to ensure that they do not drop out of the top 4,000,000 weight even in case of a "slow" block.
Thus, there will always be a need for *some* block weight limit, and that is what ensures that miners can get paid.
Now it was brought up earlier that people are moving transactions offchain, but that is perfectly fine, because every offchain mechanism first needs an onchain setup, and will at some point need an onchain teardown.
This allows increasing the effective capacity, while still ensuring that onchain fees remain at a level that will still ensure continued healthy operation of the blockchain layer.
Basically, the offchain mechanism does not remove onchain fees, it only amortizes the onchain fees to multiple logical transactions.

@_date: 2020-03-28 02:20:33
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Ruben,
Actually, thinking a little more, it seems that you can try to ensure that the first stage never drops to 0 relative locktime.
Then if somebody broadcasts the kick-off, the current owner can ask the statechain entity to sign an alternative to the first stage, with 0 relative locktime, that can now be a new funding transaction to anchor a (actually new, but logically a continuation) statechain.

@_date: 2020-03-28 02:42:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Bob,
My understanding of statechains is that nothing prevents the statechain from internally having multiple UTXOs divided from a single large onchain UTXO.
Indeed, a statechain can act much like a federated blockchain, and the interface to the statechain could be for its clients to send a Bitcoin transaction to it spending 1 or more of the UTXOs currently instantiated inside the statechain.
Then the statechain validates the client Bitcoin transaction, updates its state and republishes it to its clients, removing the (internal-to-statechain-only) UTXOs spent, and inserting the new UTXOs of the incoming transaction.
For example, suppose I have a 1BTC onchain UTXO that I use to create a new statechain:
    [funding tx]->1BTC(SE)-+  (onchain)
    _ _ _ _ _ _ _ _ _ _ _ _|_ _ _ _ _ _ _ _ _ _ _ _
              (statechain) |
                           +->[update mechanism]->1BTC(ZmnSCPxj)
Then I send to the statechain a transaction spending my 1BTC-on-statechain, giving you 0.11568768 BTC:
    [funding tx]->1BTC(SE)-+  (onchain)
    _ _ _ _ _ _ _ _ _ _ _ _|_ _ _ _ _ _ _ _ _ _ _ _
              (statechain) |
                           +->[update mechanism]->1BTC(ZmnSCPxj)->[tx]-+->0.11568768BTC(bsm117532)
                                                                       +->0.88431232BTC(ZmnSCPxj)
The statechain verifies that the tx I sent is valid, then outputs the next state as below:
    [funding tx]->1BTC(SE)-+  (onchain)
    _ _ _ _ _ _ _ _ _ _ _ _|_ _ _ _ _ _ _ _ _ _ _ _
              (statechain) |
                           +->[update mechanism]-+->0.11568768BTC(bsm117532)
                                                 +->0.88431232BTC(ZmnSCPxj)
In short, statechains can be implemented as a sort of super-transaction-cutthrough system.
This prevents the onchain UTXO from having a single logical owner, of course, so onchain it is the statechain entity that owns the entire fund, but if you are trusting the statechain entity anyway, the update mechanism is sufficient to ensure that nobody (other than the trusted statechain) can prevent the publication of the latest accepted state.
This is probably significantly more efficient than splitting up the 1BTC value to multiple lots.
I think this framework will work for all offchain mechanisms (CoinSwap, Lightning, statechains), by the way --- you can always view the offchain update mechanism as logically implementing a "new" cryptocurrency system that maintains UTXO sets and allows removal and insertion of UTXO sets according to the same rules (sans relative-locktime) as the hosting cryptocurrency system (i.e. the blockchain).
The same realization is what underlies channel factories as well --- the hosting cryptocurrency system need not be a blockchain, it can be just another cryptocurrency system (of which a blockchain is just one kind).
My understanding is that the original description, which describes transferring the entire value inside the statechain to a new owner, was only for exposition and that it was an exercise for the reader to consider how a statechain can internally split the total value among multiple UTXOs.

@_date: 2020-03-30 01:25:36
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain implementations 
Good morning Ruben,
* You are sure the old first stage tx has > 0 relative locktime.
* The replacement tx (which replaces the old first stage) has a 0 relative locktime.
  * The replacement tx redirects the funds to a new funding output for a (logically continuous, onchain new) statechain.
Then the replacement tx, having a smaller relative locktime than the old first stage, has precedence.
Indeed, having a *smaller* relative locktime is exactly the mechanism Decker-Wattenhofer uses.
So this is the state, with the kickoff having just been confirmed onchain:
    ***blockchain***
       [funding tx]->[kickoff tx]-+
         _ _ _ _ _ _ _ _ _ _ _ _ _|_ _ _
     offchain                     |
                                  +->[[ 7] stage]->[[ 0] stage]->[[14] stage]-> state outputs
Since the first stage is still "ticking" it is not yet confirmable onchain.
You ask the statechain to create an alternative, 0-relative-locktime, re-funding tx, and create a new mechanism:
    ***blockchain***
       [funding tx]->[kickoff tx]-+
         _ _ _ _ _ _ _ _ _ _ _ _ _|_ _ _
     offchain                     |
                                  +->[[ 7] stage]->[[ 0] stage]->[[14] stage]-> state outputs
                                 (OR)
                                  +->[[ 0] funding tx]->[kickoff tx]->[[14] stage]->[[14] stage]->[[14] stage]->state outputs
Because it has a time advantage, this new re-funding tx has higher priority (and is the same mechanism Decker-Wattenhofer has anyway).

@_date: 2020-03-30 02:59:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown 
Good morning Andrew,
If your coins have no security, you cannot use them safely.
If you assign value to something, you will want to ensure some amount of protection to that something, proportional to the value you assign the something
By forcing a competition for limited block space, Bitcoin forces users to honestly assess how much security they are willing to pay for.
Suppose everybody "agrees" to a reasonable fee level.
They divide up the block space among themselves and assign a fee.
Then suddenly one of the participants realizes they actually have to have a transaction added, but the block space they already agreed to use is not sufficient to fit.
Since their agreement is just ink on a page, this participant spins up a new Bitcoin non-full node, connects to the Bitcoin network over TOR, then broadcasts the extra transaction with a higher feerate.
This evicts one of the transactions in the next block (which could also be one that this cheating participant wants, but let us say that this sudden new transaction is even more important than the others it currently has allocated for the next block).
The other participants now have a risk that their transaction does not get included in the block.
Each one then re-assesses their security and timeliness requirements, and can then decide to bump their fee using RBF, if that is necessary.
This competition will then stabilize when each participant decides that the added fee to ensure their inclusion in the next block is too high for their security and timeliness requirements, and the risk they do not get their transaction confirmed is acceptable to them given the cost of getting their transaction confirmed.
All of the above is already how Bitcoin works today.
That is the only mechanism necessary, or even possible.
Always remember that any voting scheme always implicitly has an extra option called "all the options suck so I will not vote".
In this context, this implies that people can just sell their coins and forget the whole system, rather than deal with a mechanism which ensures that coins they own are always devalued continuously by others voting for devaluation.
They can sell it for a coin where their held coins are not devalued by policy, i.e. your mechanism will never have widespread support necessary for reliably forking the chain.
No Lightning node will last forever, and its channels will be eventually be closed.
Than, any onchain funds will be in the slow expensive onchain domain, so the heirs of the dead Lightning node will want to put them back into Lightning as fast as possible.
Given the number of economic nodes we expect to eventually exist (and thus possibly die) in the future, we can expect some level of such activity in the long run.
It is helpful to remember as well that this is a long-run issue anyway.

@_date: 2020-03-30 02:59:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown 
Good morning Andrew,
If your coins have no security, you cannot use them safely.
If you assign value to something, you will want to ensure some amount of protection to that something, proportional to the value you assign the something
By forcing a competition for limited block space, Bitcoin forces users to honestly assess how much security they are willing to pay for.
Suppose everybody "agrees" to a reasonable fee level.
They divide up the block space among themselves and assign a fee.
Then suddenly one of the participants realizes they actually have to have a transaction added, but the block space they already agreed to use is not sufficient to fit.
Since their agreement is just ink on a page, this participant spins up a new Bitcoin non-full node, connects to the Bitcoin network over TOR, then broadcasts the extra transaction with a higher feerate.
This evicts one of the transactions in the next block (which could also be one that this cheating participant wants, but let us say that this sudden new transaction is even more important than the others it currently has allocated for the next block).
The other participants now have a risk that their transaction does not get included in the block.
Each one then re-assesses their security and timeliness requirements, and can then decide to bump their fee using RBF, if that is necessary.
This competition will then stabilize when each participant decides that the added fee to ensure their inclusion in the next block is too high for their security and timeliness requirements, and the risk they do not get their transaction confirmed is acceptable to them given the cost of getting their transaction confirmed.
All of the above is already how Bitcoin works today.
That is the only mechanism necessary, or even possible.
Always remember that any voting scheme always implicitly has an extra option called "all the options suck so I will not vote".
In this context, this implies that people can just sell their coins and forget the whole system, rather than deal with a mechanism which ensures that coins they own are always devalued continuously by others voting for devaluation.
They can sell it for a coin where their held coins are not devalued by policy, i.e. your mechanism will never have widespread support necessary for reliably forking the chain.
No Lightning node will last forever, and its channels will be eventually be closed.
Than, any onchain funds will be in the slow expensive onchain domain, so the heirs of the dead Lightning node will want to put them back into Lightning as fast as possible.
Given the number of economic nodes we expect to eventually exist (and thus possibly die) in the future, we can expect some level of such activity in the long run.
It is helpful to remember as well that this is a long-run issue anyway.

@_date: 2020-03-31 02:06:32
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Block solving slowdown 
Good morning Andrew,
Another thing I did not consider is how miners will actually behave under this ruleset.
Miners are the direct beneficiaries of any increased inflation rate voted in.
Miners are also ultimately the ones who decide which transactions get added into blocks, or put another way, which UTXOs are deleted and which UTXOs are created.
Thus, miners are likely to accept attempts to delete UTXOs that vote for lower inflation rates and create UTXOs that vote for higher inflation rates, and reject attempts to delete UTXOs that vote for higher inflation rates and create UTXOs that vote for lower inflation rates.
Thus, miners will end up strongly controlling the inflation rate of the coin.
Even worse, since the inflation gives increased coins to miners, more and more of the value of the coin, with which you vote for, will be in the hands of miners, who can then vote directly instead of censoring votes they dislike.
The entire point of Bitcoin having a fixed inflation rate schedule (that is ultimately disinflationary) is to avoid the moral hazard of having the beneficiaries of higher inflation rates also be the ones who decide what the inflation rate will be.

@_date: 2020-05-01 07:17:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning CB,
One of those transactions (the second one) will be a 1-input 1-output tx (it moves the coin from bilateral control to unilateral control of Bob), which chain analysis already knows to be a self-transfer.
The first transaction will also usually be a 1-input 1-output tx as well (it moves the coin from unilateral of Alice to bilateral control) if you did not do any splitting or merging before providing the coin into the swap (for example if this comes from the taker, and the taker knows all the coins it wants to swap cannot be safely merged together).
If chain analysis keeps the heuristic "1-input 1-output is a self-payment because srsly who has an exact amount for a payment Bitcoin is volatile lol", then the resulting coins still are not safe to merge, because chain analysis will "pass through" the swap operation and if the two coins are later merged then they still end up *correctly* concluding the original coins were owned by the same owner.
Using a PayJoin construction for the second tx would help, but if the receiving end does not have a spare UTXO it can merge with (e.g. all its liquidity got tied up in the swap) then there might not be an opportunity to PayJoin.
There is also little that can be done about the first transaction, in case it ends up being a 1-input 1-output.
Suppose Alice the taker has a 1 BTC output and a 1 BTC output *and no other coins*, both of which it cannot safely merge, and it has to pay 1.2 BTC to Carol.
Alice then has to CoinSwap them to Bob the maker, requesting a 1.2 BTC output going to Carol and the rest in whatever loose change Bob has.
Alice then has to use two 1-input 1-output txes for each of its 1 BTC outputs (because it cannot merge them together) to put them into bilateral control.
Then Bob claims them from bilateral control with 1-input 1-output txes as well (it cannot merge them together, because that might break Alice privacy, and Bob might not have any other spare coins it can merge with the incoming funds).
Now, even if Bob PayJoins the second tx for both 1 BTC outputs, it still cannot merge the resulting joined coins together, because the "spent-together" analysis would still tie those coins as being owned by the same owner, it is simply that the surveillor will think the owner owns more coins than it actually does, but the two 1 BTC TXOs that Alice used to own are still analyzed as being owned by the same owner if they are ever merged.
What Alice could do, to "merge" its 1BTC coins together, would be to swap only one of the 1BTC coins first, for a single 1BTC coin as well.
Then presumably the incoming 1BTC coin has no linkage with the coin Alice swapped out (Alice hopes), then Alice could spend that new 1BTC coin with the old one it could not merge with the coin it swapped out.
(Actually Alice does not need to do that as it is the customer after all, but maybe Bob the maker has to do that sometimes, in case it finds there are too many cannot-spend-together constraints in its pool of UTXOs and it is getting harder to select coins --- but if so, who does Bob the maker swap *with*?
If Bob can encounter that problem, then maybe other makers will also have that problem as well!)
(the above can be done by PayJoining with the unswapped coin on either the first or second transaction in the swap as well; the idea is more general.)
We could eliminate mixdepths entirely and just use "cannot merge with X" constraints.
When the wallet sees an incoming payment, it just marks it as "cannot merge with" all other coins it owns, unless they have the same address.
This prevents any linkage at all and is maximally private.
On a CoinSwap, the incoming coins are marked as "cannot merge with" to each other in the same CoinSwap operation, but not with any other coins it owns.
It might be easier for the user to understand as well, and reduces scope for mistakes in using mixdepths.
For example, I might have a sensitive source of funds (e.g. from all the ransomware I have been writing) and put them in one mixdepth, then after a few months I forgot which mixdepth I put those in and accidentally use it for my on-the-books salary.
I think that maybe it would be a better policy for Alice to always just give a specified payment amount at all times.
Of course, a sufficiently motivated Bob could always just do statistical analysis on the payment amount (e.g. if it is not equivalent to some round number of United States Green Historical Commemoration Papers, it is unlikely to be a payment but instead a random amount that Alice had to provide on a self-payment).
So .....
Anyway, slightly unrelated, maybe we can simply have Alice specify a single payment amount always, as an unremovable part of the protocol.
I proposed "private key turnover" here: Basically, after exchanging the swap secret, it is now safe to give your share of bilateral control to your swap partner, so you can just turn over that private key to the swap partner.
For clarity:
* Alice owns a 1 BTC coin it wants to swap with a 1 BTC coin from Bob.
* Alice sends its 1 BTC coin to bilateral control (Alice temporary key and Bob temporary key).
  * Backoffs and confirmations and etc etc are needed, we all know how to do CoinSwap safely, I elide those details here.
* Bob sends its 1 BTC to bilateral control (Alice 2nd temporary key and Bob 2nd temporary key).
* Alice and Bob complete the CoinSwap protocol and now both know the swap secret X, and have to claim the bilateral control before some future blockheight L.
* Alice can send its Alice temporary key to Bob, so that Bob can change the second transaction as it likes.
  * Bob can merge it with a coin it happens to have, without having to coordinate signing with Alice (i.e. it gets PayJoin on the second tx for free).
  * If Bob the maker gets another swap request, it can spend directly from the bilateral control address to another bilateral control address with a different taker, reducing blockchain footprint.
  * Bob can fee bump using RBF instead of CPFP.
* Bob can also now send its Bob 2nd temporary key to Alice, for similar advantages for Alice.
It does require that both Alice and Bob respect the timeout --- the bilateral outputs have to be spent before the timeout, else the timelock branches come into play.
But Alice and Bob, after private key turnover, need not *immediately* broadcast the claiming transactions --- they can wait a little time for opportunities to change the claiming transaction, for example if they get an incoming payment they could assume that the recently-concluded swap is safe to merge with the new incoming coin and they can CPFP the incoming payment on the mempool with their existing coin, or Bob the maker might get another customer and Bob can cut-through from one swap to the next, reducing 4 transactions for 2 swaps to just 3 transactions (and if it can continuously chain customers that way, in the long run Bob on average has 1 transaction per swap, halving the block space usage needed for CoinSwap).
This increases complication of the implementation, but you potentially get an improvement in blockchain space for popular makers, with an asymptote of 50% reduction, so it is probably worth implementing.
Thus, if Alice wants to multipay, she could just sum up all the outgoing values, then specify the sum to Bob.
Then it can modify the second transaction to pay multiple destinations (since it has the private keys to remake that).
Of course, all the outgoing payments are now linked together.... but I suppose you can warn the user of Alice of such.
It would probably be best for both Alice and Bob to always change the destination address as well after private key turnover.
So let us return to our example of Alice who owns a 1 BTC coin and a 1 BTC coin.
Now suppose we find, by false-positive-statistics, that 2 BTC subset sums are rare but, say, 1.5 BTC subset sums are significantly more common.
So what Alice should do, if it wants to send 1.2 BTC to Carol via a CoinSwap with maker Bob, would be to split one of her 1 BTC coins to a 0.5 BTC and 0.5 BTC coin.
Then it takes the remaining 1 BTC coin and one of the 0.5 BTC and offers them in a CoinSwap to maker Bob, specifying a payment amount of 1.2 BTC.
It seems to me, however, that this is not much different from just specifying a set of standardized swap amounts.
The initial standards can be derived from false-positive-statistics, but once SwapMarket starts to become popular, then the actual statistics of the chain becomes skewed towards those standard swap amounts.
This makes it even wiser to also use those standard swap amounts, because of the larger anonymity sets.

@_date: 2020-05-04 08:28:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Fwd: (Semi)Traceless 2-party coinjoin off-chain 
Good morning CB,
A heuristic need not be perfect to be effective, and I am quite sure that, outside of donation, most 1-output transfers will be self-transfers.
Indeed, the old cliche of donating tainted/stolen funds is most likely a cliche for a reason.
Perhaps you are a beneficiary of some movie hero who has stolen the money from a villain who acquired wealth by immoral means.
Unlike change heuristics, misleading the 1-output heuristic is difficult; whoever got control of the single output is either yourself, or somebody who swapped coins with you.
Yes, Lightning helps privacy, that is correct.
However, do note that Lightning channels tend to be long-lived, meaning it will be a large number of blocks that such a TXO will be unspent.
Due to the timeout on CoinSwap, the fund needs to be claimed quickly, in much shorter time than a typical Lightning channel.
This can help narrow down payment heuristics.
Yes, I agree.
PayJoin might not buy you as much as you think.
So Alice has two coins it does not want to CoinJoin for unknown reasons:
    Salary as  ---> Alice
     teacher
    LiveJasmin ---> Alice
     payout
Alice swaps them to Bob, who PayJoins the incoming funds.
Since Bob has been operating for a long time, its coins have a varied and storied history.
    WannaCry  ----> Bob  ----+
    Salary as  ---> Alice ---+--> Bob
     teacher
    LiveJasmin ---> Alice ---+--> Bob
     payout                  |
    ex-JoinMarket -> Bob ----+
     maker
Alice does *not* want Bob to join those two Bob-owned coins, still, because chain analysis would not only implicate her in WannaCry, but also as a LiveJasmin model ***and*** (gasp!) a JoinMarket money launderer (I am being facetious here).
And since the swap has completed, Bob controls those coins.
If another taker comes along, offering a high fee for a big swap, and Bob *has to* merge those two coins (that ultimately got history from Alice) in order to cover what the taker is requesting (because the taker has to make a big single payout to some other party, so needs a single large UTXO, not two small ones), what do you think Bob will do?
In a free market where the takers have wide choice, do you think Bob will economically-rationally help protect the secret life of Alice when not doing so would let Bob earn more coins?
Now of course, it would seem very unlikely that Alice the teacher is the hacker behind WannaCry *and* a LiveJasmin model *and* a money launderer, so no problem, right?
It just makes surveillors looks like fools right?
Except that Bob could have skipped the PayJoin step and just merged those four coins in a single transaction later, and the conclusion of surveillors would still be the same, for much the same effect, at reduced blockspace (spend in a single transaction instead of 3).
So I think it is better if Bob actually avoids small merges of a two or three or four coins, and should do the occasional mass merge of large numbers of coins.
This leads to better misleading of surveillors, and is incentive compatible since it reduces blockspace usage for Bob to do the occasional mass merge rather than PayJoining at every swap.
That seems to be a good idea indeed, and significantly better than relying on PayJoin to obscure the history of coins.
Of course, doing change-output-heuristic-breaking means splitting up coins, increasing the number of UTXOs.
But that combines well with Bob the maker periodically merging many small coins.
Yes, and I suggest this is always be done, as part of the protocol.
It gets a few features "for free", at the cost of greater complexity at the simple "I only want to swap once, then forget about the coins for a while" case.
* It gets PayJoin at the second transaction for free.
* It lets Bob the maker cut-through for a new taker.
* It reduces the cost on Bob to merge large numbers of coins at once.
* It lets Alice the taker cut-through for a new maker if she wants to do multi-round swaps (though note the objection "Value-based Directionality" in my writeup  ; though counternote that if CoinSwaps are as hard to identify as my naive understanding of your math suggests, this should be a relatively minimal problem).
If it is as smooth and naturally-occurring as you suggest, then it seems to me as well that the distribution of CoinSwap values will be just as smooth and naturally-occurring, so my naive understanding is still "even if sparse subset sum is easier than subset sum, it is still hard, so it probably will not matter in practice".
People will mix due to receiving some amount.
That amount will be sampled from some naturally-occurring distribution.
Thus, CoinSwap values will be sampled from the same naturally-occurring distribution.
People will mix due to having to send some amount they do not want to be tracked to them.
That amount will be sampled from some naturally-occurring distribution.
Thus, CoinSwap values will be sampled from the same naturally-occurring distribution.
....I think?  Maybe?

@_date: 2020-05-05 13:49:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] On the scalability issues of 
Good morning ariard and luke-jr
BIP 157 can be implemented as a separate daemon that processes the blocks downloaded by an attached `bitcoind`, i.e. what Wasabi does.
The intention, as I understood it, of putting BIP157 directly into bitcoind was to essentially force all `bitcoind` users to possibly service BIP157 clients, in the hope that a BIP157 client can contact any arbitrary fullnode to get BIP157 service.
This is supposed to improve to the situation relative to e.g. Electrum, where there are far fewer Electrum servers than fullnodes.
Of course, as ariard computes, deploying BIP157 could lead to an effective DDoS on the fullnode network if a large number of BIP157 clients arise.
Though maybe this will not occur very fast?  We hope?
It seems to me that the thing that *could* be done would be to have watchtowers provide light-client services, since that seems to be the major business model of watchtowers, as suggested by ariard as well.
This is still less than ideal, but maybe is better than nothing.

@_date: 2020-05-11 05:44:08
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] On the scalability issues of 
Good morning Richard, and all,
A relatively pointless observation, but it seems to me that:
* The light client is requesting for validation information, because...
* ...its direct peers might be defrauding it, leading to...
* ...the money it *thinks* it has in its channels being valueless.
Thus, if the light client opportunistically pays for validation information (whether full blocks, headers, or filters), the direct peers it has could just as easily not forward any payments, thus preventing the light client from paying for the validation information.
Indeed, if the direct peer *is* defrauding the light client, the direct peer has no real incentive to actually forward *any* payments --- to do so would be to reduce the possible earnings it gets from defrauding the light client.
("Simulating" the payments so that the light client will not suspect anything runs the risk that the light client will be able to forward all its money out of the channel, and the cheating peer is still potentially liable for any funds it originally had in the channel if it gets caught.)
What would work would be to use a system similar to watchtowers, wherein the validation-information-provider is prepaid and issues tokens that can be redeemed later.
But this is not suitable for opportunistic on-same-WiFi where, say, a laptop is running a validation-information-provider-for-payment program on the same WiFi as a light-client mobile phone, if we consider that the laptop and mobile may have never met before and may never meet again.
It would work if the laptop altruistically serves the blocks, but not if it were for (on-Lightning) payment.
So it seems to me that this kind of service is best ridden on top of watchtower service providers.

@_date: 2020-05-11 16:45:21
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
CoinSwap for privacy is practically a "cross" chain atomic swap with the same chain and token for both sides of the swap, see also this set of ideas: "Instead, Bob simply hands secretBob to Alice" is basically the same as private key turnover, as best as I can understand it, and gives significant advantages, also described in passing here: Overall, this looks very much like a working CoinSwap as well.
The Refund tx does not need anything more than a 2-of-2 script.
The "OR Alice in +1 day" branch can be implemented, at least on Bitcoin and similar blockchains, by signing a specific `nSequence`, or if the chain forking predates BIP68, by using absolute locktimes and signing a specific `nLockTime`, with the destination being just "Alice".
This should help privacy, as now all `scriptPubKey`s will be 2-of-2 (or P2PKH with 2p-ECDSA).
(It strikes me that the relative locktime is unnecessary on the output of this refund tx --- as long as both participants agree on either Alice or Bob having a longer locktime, you can just use the locktime on the refund tx directly as backout; see the topic "`nLockTime`-protected Backouts" on the CoinSwapCS issue link)
If you are willing to accept protocol complexity, having a variety of different versions of the transactions with different feerates could be used rather than the Decker-Russell-Osuntokun "eltoo" bring-your-own-fees method.
In terms of privacy this is better as you would not be using anything other than the most boring `SIGHASH_ALL` signing flag, whereas the Decker-Russell-Osuntokun will be identifiable onchain (and thus possibly flag the transaction as "of interest" to surveillors) due to use of `SIGHASH_ANYPREVOUT`.
As long as the one resolving a particular side of the swap is the one that ocmpletes the signature (which I believe holds true for all branches?) then it would select the version of the transaction with the best feerate, which it effectively pays out to what it recovers.

@_date: 2020-05-12 04:41:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
Indeed; basically, any innovations in cross-chain swaps can be adapted to a CoinSwap (though not necessarily vice-versa, if a CoinSwap innovation requires certain specific blockchain features).
An interesting setup.
So I was wondering why something like this would not work instead:
0.  Alice has BTC, Bob has LTC, they agree on exchange rates and two future timelock L1 and L2 such that L1 < L2.
1.  Alice creates keypairs Alice[0] Alice[1] Alice[2], Bob creates Bob[0] Bob[1] Bob[2], and share the pubkeys.
2.  Alice creates, but does not sign, a funding tx on BTC whose output requires Alice[0] && Bob[0].
3.  Bob creates a backout transaction spending the BTC funding txo, with an absolute timelock L1, whose output goes to Alice[2], then provides to Alice a signature for Bob[0] and requires an adaptor such that completing the signature with Alice[0] reveals Alice[1].
                         nLockTime L1
    BTC funding txo ---> Alice[0] && Bob[0]    --->  Alice[2]
                             reveals Alice[1]
4.  Alice creates a timeout transaction spending the BTC funding txo, with an absolute timelock L2, whose output goes to Bob[2], then provides to Bob a signature for Alice[0] and requires an adaptor such that completing the signature with Bob[0] reveals Bob[1].
                         nLockTime L2
    BTC funding txo ---> Alice[0] && Bob[0]    --->  Bob[2]
                             reveals Bob[1]
5.  Alice signs the BTC funding tx and broadcasts it.
6.  Alice and Bob wait for the BTC funding tx to be confirmed.
7.  Bob creates an LTC funding tx whose output requires Alice[1] && Bob[1].
8.  Alice and Bob wait for the LTC funding tx to be confirmed.
9.  Alice creates a success transaction spending the BTC funding txo, with no practical absolute timelock (current blockheight + 1), whose output goes to Bob[2], then provides to Bob a signature for Alice[0] and requires an adaptor such that completing the signature with Bob[0] reveals Bob[1].
                         nLockTime now
    BTC funding txo ---> Alice[0] && Bob[0]    --->  Bob[2]
                             reveals Bob[1]
10.  Bob gives the secret key of Bob[1] to Alice.
11.  Alice gives the secret key of Alice[0] to Bob.
12.  Bob claims the BTC funding txo before L1.
Aborts and stalls:
* Aborts before step 5 are safe: no money is ever committed yet.
  Stalls before step 5 can be promoted to aborts.
* If aborted between step 5 and step 8, Alice reclaims her BTC via the backout transaction.
  Since Bob did not confirm any locked funds in LTC, revealing Alice[1] does not give Bob any extra funds it did not already have.
  If Bob stalls before step 8 Alice can abort at L1 using the backout transaction.
* If Alice stalls at step 9, Bob can force the completion using the timeout transaction at L2, revealing Bob[1] and claiming the BTC.
* If Alice instead aborts at step 9 using the backout transaction at L1, Bob learns Alice[1] and can reclaim its LTC.
* Steps 10 and 11 are optional and "only" give Alice and Bob extra flexibility in what they can do with the funds (such as sweeping multiple swaps, RBFing, performing another swap, etc.), i.e. private key turnover.
  Bob can always claim the BTC funding txo before L1 by signing and broadcasting the success transaction.
Would this not work?
It requires that at least one chain involved supports witness segregation, in order to allow signing a dependent transaction before signing what it spends.
This has the advantage of using only absolute timelocks, which are better for privacy since ordinary wallets like Bitcoin Core and C-Lightning use absolute timelocks for ordinary spends onchain.
If the above counterproposal would work, it seems to me that all abort and stall scenarios "just" involve an absolute-timelock `SIGHASH_ALL` signed transaction, so it might not be so inevitable.
In addition, the above counterproposal has the transaction signatures be completed by whoever ends up getting the money, so will rationally use the version with the best feerate.
While leaking information in case of uncooperative abort is acceptable, it still seems to me that in this case, we can have a solution where an uncooperative abort has no information leak.
My thesis is that, if relative locktimes are used as often as absolute locktimes for block-sniping-prevention and a decent Scriptless Script system, then all protocol aborts should be doable with no information leaks, at the cost of pre-signing a bunch of timelocked transactions.
A sidenote as well, that if Alice typically uses an HD wallet, the UTXO on the LTC side would not be in that HD, and if Alice wants to cold-store the LTC, it should move the money as well into an HD pubkey.

@_date: 2020-05-12 15:05:57
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
If the shortened refund transaction exists (labeled "refund transaction  in the SVG) then the same issue still occurs: after 1 day it is possible for either success or refund to be broadcasted, leading to revelation of both secrets, leading to the same failure mode you described.
Without the refund in your proposal, Bob refusing cooperation after Alice puts the BTC into lock for 3 days and 2 further onchain transactions (with the refund transaction being relative-locked, meaning it cannot be used to CPFP the revoke transaction; my formulation allows any of the result transactions to be CPFP directly by their beneficiaries).
It seems to me that there is still an onlineness requirement in case Bob does not complete the protocol: once the revoke tx becomes valid an online Bob can cheat an offline Alice by broadcasting the revoke tx (which, if my understanding of the protocol is correct, the signatures are shared to both Alice and Bob).
So Alice needs to be online starting at 2 days to 3 days in order to ensure it reclaims it funds.
I have not seen the 2-tx variant video yet, as I prefer to read than listen, but I will also check it if I can find an opportunity.
Regardless, the overall protocol of using 3 clauses in the swap, and reusing the privkey as the payment secret demanded by the pointlocks, is still a significant innovation.
In the context of CoinSwap, a proposal is that a CoinSwap server would provide swapping service to incoming clients.
Using my counterproposal, the Bob position can be taken by the server and the Alice position taken by the client.
In this context, the L1 can be made reasonably close in the future and L2 far in the future, in which case Alice the client can be "weakly offline" most of the time until L2, and even in a protocol abort would be able to recover its funds.
If the protocol completes, the server Bob can claim its funds before L1, and (with knowledge of Alice[0]) can immediately put it in a new funding tx for a new incoming client before L1, which is a fine tradeoff for server Bob since presumably Bob is always online.

@_date: 2020-05-12 15:48:30
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] On the scalability issues of 
Good morning Richard,
Perhaps "simulate" is not quite the correct term.
Basically, if the eclipsing peer(s) are reasonably sure they have eclipsed the light client, then all funds in those channels are semantically theirs (they "0wn" the eclipsed light client).
Then anything the light node offers from those channels (which it thinks are its, but are now in reality owned by the eclipsing peer) has no value (the eclipsing node already 0wns the light node and all its funds, what can the light node offer to it?).
The eclipsing peer could still "simulate" what the light node expects of reality by pretending that the light node actually still owns funds in the channel (even though the eclipsing node has successfully stolen all those funds), and forward as normal to get the payment preimage/scalar.
If you are considering this for on-Lightning payments, do note that the alternative HTLC construct has to be known by every forwarding node, including the direct peer(s) of the light client, which are precisely the potential attackers on the light client.
It seems to be impractical for onchain payments: the provider can drop the data onchain to claim the funds, but it is precisely the blockchain data that the light client does not have direct access to, so ---
It marks the client as a light client, at least.
Someone who gets read-only access to the logs of such a public-service node now has a list of light clients.
If the light clients are in any way identifiable and locatable, the hacker can then attempt to hack the light client and redirect their understanding of what the public-service node is (e.g. DNS poisoning) and then start performing other attacks on the client once its view of the blockchain is eclipsed.
This would be helped if the light client, for example, always uses Tor to access the public-service node, if payments for services of that node are decorrelated (e.g. tokens issued by the node that will later be reclaimed for service are blinded), etc.
Such would make the light client harder to locate in the first place.
(While a mobile client can certainly access the Internet over various access points, most people who own mobile devices have a home they go to at night, which often has Internet access, possibly with a stable identifiable location that can be attacked)
This is probably something we can expect to see as well; though it should be noted, for those philosophically interested in such things, that these are the genesis of governments and states.

@_date: 2020-05-13 08:39:37
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
Okay, so the concern is basically, that Bob misses the deadline, then Alice feels obligated to reclaim the funds.
In your proposal, the tx competition is between the secret-revealing success TX and the non-secret-revealing revoke tx.
Whereas in my counterproposal, under the same conditions, the tx competition is between the secret-revealing success tx and the secret-revealing backout tx, and both transactions becoming visible on P2P network means potentially both Alice and Bob know all the secrets on the LTC side and end up competing over it, RBFing each other until the entire fund goes to miners.
I think it is not accurate that Bob is already on-chain before Alice can be forced to use 3 transactions to fail.
The revoke tx signatures are shared at step 0 of your protocol description.
Thus Bob has a copy of the revoke tx that is valid once Alice signs and confirms the funding transaction.
Bob can thus give a copy of the revoke tx with signature directly to its favorite miner, forcing Alice to take 3 transactions to back out of the swap.
Since Bob gave the tx directly to its favorite miner (TANSTAAGM: "There ain't no such thing as a global mempool"), Alice will only know about this event when the revoke tx is confirmed once, at which point it is very difficult to reverse, even if Alice has a refund tx prepared.
Bob can do this before step 2 in your protocol description, meaning before Bob locks up any funds, so Bob can do this for free, and will even use funds going back to Alice to pay for confirmation of the revoke tx.
Because Bob can do this for free, there is no disincentive for trolling Bobs to exist whose sole life goal is to just grief possible Alices.
This can be slightly mitigated by adding two CPFP outputs (one for each participant) and using the minimum relayable feerate for the revoke tx so that Bob is forced to bring its own fees in order to incentivize miners.
This is similar to the "bring your own fees" currently proposed for Lightning, but note the recent hand-wringing about the various problems this adds to mempools and CPFP and RBF rules and etc etc: We could use `SIGHASH_SINGLE | SIGHASH_ANYONECANPAY` as well for a bring-your-own-fees, but that is not `SIGHASH_ALL` and thus marks the transaction graph as special.
And forcing bring-your-own-fees means neither Alice nor Bob can swap all their funds in a single operation, they have to keep a reserve.
Bob cannot safely perform step 2 before getting both signatures for the revoke tx, as without Bob having access to the rveoke tx, if Bob locks up LTC, Alice can stop responding and lock both their funds indefinitely with Bob not having any way to recover its funds, which a rich Alice can use to completely lock out an impoverished Bob.
But if Bob is given both signatures for the revoke tx before step 2, then Bob can send the revoke tx to its favorite miner, forcing Alice to take 3 transactions to back out, before Bob locks any funds in LTC side.
This seems to be the safest alternative; in my context, where Bob is a CoinSwap server/maker, Bob can wait a short while for new clients/takers, and if no new clients arrive, spend.
Bob can run multiple servers, each of which are given the completed success transaction, and the servers can check that if the timeout is near, to spam the Bitcoin P2P network with the completed success transactions.
(these servers need not even run fullnodes, they could just periodically poll a number of blockchain explorers and electrum servers, and when the blockheight approaches, attempt broadcast; if the "main" server that accepts clients/takers has already spent the TXO the broadcast of the completed success tx is simply rejected by the Bitcoin P2P network; if the timeout is based on sidereal time then the backup servers only need to be running NTP)

@_date: 2020-05-13 11:39:10
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
But if refund tx  at all exists, then you drop to the same issue you objected to with my proposal, which is that, on completion of the protocol, if Bob lets the refund tx become valid (i.e. does not spend the BTC txo) then Alice can broadcast it, putting both their funds into chaos.
So you might as well just use my counterproposal instead, which is simpler, gets bring-your-own-fees for free, etc.
I suppose there is some *slight* improvement in that with your proposal, Alice *can* use revoke tx -> refund tx  but still, if Alice is insane then it could very well mess with the protocol by instead using refund tx Thus, if Bob wants to survive in an environment where Alices are possibly insane (e.g. the real world), it should do everything in its power to ensure that the BTC txo is spent before the timeout of refund tx  if refund tx  exists at all.
And if Bob is already going to do that, then Alice and Bob might as well just use my counterproposal etc etc.
If one party quickly broadcasts a long chain of low-feerate transactions on top of the single output, then the output is "pinned".
Low feerate means it is undesirable for miners to mine it, because it pays low for the amount of blockspace it has.
But because there is a long chain of transactions, the absolute fee of that chain can be sizable, and we have a rule in RBF which, paraphrased, goes something like "the replacing transaction should also have a higher absolute fee than *all* the transactions it replaces", meaning the fee jump that the other side has to offer *has to be* pretty big.
If the other outputs of the tx are then multisig, then the pinning participant can simply refuse to sign for those, and if the existing txes spending the other outputs are relative-time-locked, they cannot be used to CPFP the revoke tx onchain.
This is why we eventually decided in Lightning to use two CPFP outpoints rather than one, and are also realizing just how much of a headache the RBF rules are, sigh.
Still, in your proposed protocol the dependent transactions are all relative-timelocked, so timely confirmation of the revoke tx is not necessary, unlike in the case of Lightning where all HTLCs have to use an absolute timelock because we have to coordinate multiple HTLCs in forwarding and violation of the timelocks can lead to headaches and fund loss and so on.
So maybe a single hook output, or even none at all, is workable.
Ah, right, you still need `SIGHASH_ANYPREVOUT`/`SIGHASH_NOINPUT` for that.
In a world where Alice may be insane and mess with the protocol just to grief Bob even if Alice loses its money (e.g. the real world), Bob should not depend on Alice behaving correctly or politely, so it should still have backup watchers set up in case it accidentally goes to sleep and so on.

@_date: 2020-05-14 04:02:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] [Lightning-dev] On the scalability issues of 
Good morning Antoine,
One hope I have for Lightning is that it will replace centralized custodial services, because:
* Lightning gains some of the scalability advantage of centralized custodial services, because you can now transfer to any Lightning client without touching the blockchain, for much reduced transfer fees.
* At the same time, it retains your-keys-your-coins noncustodiality, because every update of a Lightning channel requires your keys to sign off on it.
If most Lightning clients are SPV, then if we compare these two worlds:
* There are a few highly-important centralized custodial services with significant economic weight running fullnodes (i.e. now).
* There are no highly-important centralized custodial services, and most everyone uses Lightning, but with SPV (i.e. a Lightning future).
Then the distribution of economic weight would be different between these two worlds.
It may even be possible, that the Lightning future with massive SPV might end up with more economic weight in SPV nodes, than in the world without Lightning and dependent on centralized custodial services to scale.
It is also entirely possible that custodial services for Lightning will arise anyway and my hope is already dashed, come on universe, work harder will you, would you really disappoint some randomly-generated Internet person like that.
Money makes the world go round, so such backup servers that are publicly-facing rather than privately-owned should be somehow incentivized to do so, or else they would not exist in the first place.
Of course, a free market tends towards monopoly, because any entity that happens to have even a slight advantage at the business will have more money to use towards business reinvestment and increase its advantage further, until they beat the competition to dust, anyone who has won a 4X game knows to search for and stack those little advantages until you snowball and conquer the world/galaxy/petri dish which is why the endgame of 4X games is so boring compared to the start, we have seen this happen in mining and exchanges and so on, and this works against your desire to have a uniform distribution.
If everyone runs such a privately-owned server, on the other hand, this is not so different from having a Lightning node you run at your home that has a fullnode as well and which you access via a remote control mobile device, and it is the inconvenience of having such a server at your home that prevents this in the first place.

@_date: 2020-05-15 04:39:33
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] SAS: Succinct Atomic Swap 
Good morning Ruben,
Ah, that explains the existence of the Alice && Bob clause in that output then.
The attack is now as follows:
* Alice completes the protocol up to handing over `sigSuccessAlice` to Bob.
* Bob returns the `secretBob`.
* Alice stalls the protocol and never sends the `Alice` privkey, and waits for 1 day, then sneaks the refund tx  and spends the LTC via direct miner collusion.
The proper response here is that Bob should broadcast success tx before the refund tx  becomes valid.
(Which I think is the point: chaos can only occur if you let backouts become valid, and it is the best policy for Bob to just spend the BTC txo well before the timeout.
Even if the protocol is completed, without a bring-your-own-fees that lets you malleate the tx (i.e. CPFP hooks still require the transction itself to reduce the fund by at least the minimum feerate), at least part of the fund must be lost in fees and Bob can still suffer a small loss of funds.)
Tangentially, I now think in the case of client-server CoinSwap, the server should take Alice position and the client should take Bob position.
Suppose a client wants to do some mixing of its own received coins.
It should not depend on only one server, as the server might secretly be a surveillor (or hacked by a surveillor) and recording swaps.
Thus, a client will want to make multiple CoinSwaps in sequence, to obscure its history.
(Do note the objections under "Directionality" in  though; a counter to this objections is that the analysis there is only applicable if the surveillor already identified the CoinSwap sequence, but hopefully the increased steganography of CoinSwaps means they are not identifiable anyway.)
Since Bob really should spend its received coin before a timeout, it is best for Bob to be the client; it is likely that the client will need to swap "soon" again, meaning it has to redirect the funds to a new 2-of-2 anyway.
For the final swap, the client can then spend the final coins to an HD wallet it controls, reducing the key backup load on the client to be the same as normal HD wallets.
Presumably the server in this situation has greater ability to dynamically update its backups to include key backups for `secretAlice` keys.
Further, if the client program has the policy that all spends out of the wallet must be done via a swap (similar to a rule imposed by JoinMarket where sendpayment.py always does 1 CoinJoin), then this still matches well with the requirement on Bob to spend the fund before the first timeout of refund tx If the client needs to spend to a classic, address-using service, then nothing in the SAS protocol allows Alice to receive its funds directly into a specific third-party address.
However, Bob can hand over a specific third-party address to use in the success tx.
Indeed, the SAS protocol can be modified so that Bob can specify a set of address/value pairs to be put in the success tx instead of just Bob pubkey; for example, Bob might swap more than the amoutn that needs to be paid to the third-party service, in order to give some additional leeway later for RBF once Alice hands over the Alice privkey and Bob can remake the success tx (and more importantly, ensure the txo is spent before refund tx  becoms valid).

@_date: 2020-05-24 00:52:13
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] MIN_STANDARD_TX_NONWITNESS_SIZE and OP_RETURN 
Good morning Thomas,
If we accept a 60-byte tx, then SHA-256 will pad it to 64 bytes, and it may still be possible to mount CVE-2017-12842 attack with 32-bits of work.
Of course some other details will be changed from the standard SHA-256 in mounting this attack, but from my poor understanding it seems safer to just avoid the area around length 64.
It *might* be safe to accept 65-byte or larger (but do not believe me, I only play a cryptographer on the Internet), but that does not help your specific application, which uses 60 byte tx.

@_date: 2020-05-24 01:12:04
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hashcash-newhash 
Good morning Karl,
Why do you believe so?
My understanding is that there are effectively two strategies for ensuring decentralization based on hash algorithm:
* Keep changing the hash algorithm to prevent development of ASICs and ensure commodity generic computation devices (GPUs) are the only practical target.
* Do not change the algorithm, to ensure that knowledge of how best to implement an ASIC for the algorithm becomes spread out (through corporate espionage, ASIC reverse-engineering, patent expiry, and sheer engineering effort) and ASICs for the algorithm are as commoditized as GPUs.
The former strategy has the following practical disadvantages:
* Developing new hash algorithms is not cheap.
  The changes to the hashcash algorithm may need to occur faster than the speed at which we can practically develop new, cryptographically-secure hash algorithms.
* It requires coordinated hardforks over the entire network at an alarmingly high rate.
* It arguably puts too much power to the developers of the code.
On the other hand, the latter strategy requires us only to survive an intermediate period where ASICs are developed, but not yet commoditized; and during this intermediate period, the centralization pressure of ASICs might not be more powerful than other centralization pressures
Which brings us to another point.
Non-ASIC-resistance is, by my understanding, a non-issue.
Regardless of whether the most efficient available computing substrate for the hashcash algorithm is CPU, GPU, or ASIC, ultimately miner earnings are determined by cost of power supply.
Even if you imagine that changing the hashcash algorithm would make CPUs practical again, you will still not run it on the CPU of a mobile, because a mobile runs on battery, and charging a battery takes more power than what you can extract from the battery afterwards, because thermodynamics.
Similarly, geographic locations with significant costs of electrical power will still not be practical places to start a mine, regardless if the mine is composed of commodity server racks, commodity video cards, or commodity ASICs.
If you want to solve the issue of miner centralization, the real solution is improving the efficiency of energy transfer to increase the areas where cheap energy is available, not stopgap change-the-algorithm-every-6-months.

@_date: 2020-05-24 16:51:36
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hashcash-newhash 
Good morning Kari,
Possibly, but then the reason for change is no longer to promote decentralization, would it?
It helps to be clear about what your goals are, because any chosen solution might not be the best way to fix it.
I admit that, if the problem were to be avoid the inevitable obsoletion of SHA-2, then this is the only solution, but that is not the problem you stated you were trying to solve in the first place.
Even *with* a scripting language, the issue is still what code written in that language is accepted, and *how*.
Do miners vote on a new script describing the new hashing algorithm?
What would their incentive be to obsolete their existing hardware?
(using proof-of-work to lock in a hashing change feels very much like a chicken-and-egg problem: the censorship-resistance provided by Bitcoin is based on evicting any censors by overpowering their hashpower, but requires some method of measuring that hashpower: it seems unlikely that you can safely change the way hashpower is measured via a hashpower election)
Do nodes install particular scripts and impose a switchover schedule of some sort?
Then how is that different from a hardfork, especially for nodes that do not update?
(notice that softforks allow nodes to remain non-updated, at degraded security, but still in sync with the rest of the network and capable of transacting with them)
No, because anyone who is capable of selling hardware, or the expertise to design and build it, can earn by taking advantage of their particular expertise.
Generally, such experts can saturate the locally-available energy sources, until local capacity has been saturated, and they can earn even more by selling extra hardware to entities located at other energy sources whose local capacities are not still underutilized, or expanding themselves to those sources.
Other entities might be in better position to take advantage of particular local details, and it may be more lucrative for the expert-at-building-hardware to just sell the hardware to them than to attempt to expand in places where they have little local expertise.
And expertise is easy to copy, it is only the initial expertise that is hard to create in the first place, once knowledge is written down it can be copied.
I doubt there is any good software-only solution to the problem; the physical world remains the basis of the virtual one, and the virtual utterly dependent on the physical, and abstractions are always leaky (any non-toy software framework inevitably gains a way to query the operating system the application is running under, because abstractions inevitably leak): and energy, or the lack thereof, is the hardest to abstract away, which is the entire point of using proof-of-work as a reliable, unfakeable (i.e. difficult to virtualize) clock in the first place.
Still, feel free to try: perhaps you might succeed.

@_date: 2020-05-25 06:54:27
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Post-mix(coinjoin) usage with multisig and cpfp 
Good morning Prayank
The article is not clear I think, so please confirm my understanding below.
* "Peer 3" - Payee
* "Peer 2" - Payer
* "Peer 1" - Enabling tr\*sted third party
Goal: Payer wants to pay to the payee 0.006BTC
Current Conditions:
* Payer owns 0.01 BTC in a single UTXO
* Third Party owns 0.05 BTC in a single UTXO
1.  Payer and Third Party compute a 2-of-3 address with the public keys of Payer, Payee, and Third Party.
2.  Payer and Third Party individually pay their owned funds to the 2-of-3 address.
3.  After confirmation, they consume the new outputs into another transaction with equal-valued outputs, hiding who owns which coins.
Is my understanding correct?
If so, I believe JoinMarket has a superior technology, which does not require a tr\*sted third party; it simply requires one or more UNtrusted third parties to participate in signing a single transaction that does not require paying to an intermediate m-of-n address (thus all inputs are singlesig).
Basically JoinMarket allows the market taker to decide how much the equal-value outputs are, and to define the address it goes to.
The destination address need not be one the market taker controls, it can be to a payee.
This technique is the only out-of-the-box way that a JoinMarket wallet can spend funds from a JoinMarket wallet.
JoinMarket as well already includes how to get in touch with enabling third parties (called "market makers").

@_date: 2020-05-25 07:58:19
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hashcash-newhash 
Good morning Kari,
Now that I have thought about it more: again, the important thing about the proof-of-work technique is not so much the algorithm itself, only that executing it requires energy.
And all algorithms require energy in order to execute.
Even if some technique is found which partially breaks the hash function and allows faster generation of hashes for the amount of energy consumed, this is not a problem for mining itself: the difficulty adjusts and mining continues.
The execution of this technique is unlikely to require no computation, only reduced computational effort; and all that is needed is *some* measure of computational effort done, the *scale* of that measure is not really material for the purpose of ordering atomic transactional updates to the UTXO set.
Of course, things like the Merkle tree and txids and so on would need changing in case of even a partial break of the hash function, which would require a hardfork to survive; you might as well change the proof-of-work function as well then.
You seem to be equating "break of the hash function" with "centralization of hashrate", which I do not quite agree with.
Most human beings cannot think without constant communication with other human beings.
Thus, it is unlikely that a private break of the hash function is possible.
Instead, a break of the hash function is likely to be discovered in various ways by multiple human beings who have been communicating with each other.
Even historically, inventions never arose fully-formed from the head of the inventor, like Athena from the brow of Zeus; every invention has predecessors, successors, and peers in the inhabited milieu.
Instead, you can look up the costs of local electricity globally, and notice where those entities have their largest mines geographically located.
Go ahead.
Given that electricity is consumed very quickly, and hardware takes a long time to truly consume or obsolete, yes: rate of consumption of electricity is expected to dominate compared to the rate of consumption of hardware.
It seems the most lucrative thing to do, that if you have a new generation of hardware, to mine with them yourself, until the price of local electricity has increased due to your consumption, and it becomes more lucrative to sell the hardware to other potential miners who now have lower electricity prices compared to yours (because you have been saturating the local electricity supply with your own mining operations and causing the local prices to rise up, or equivalently, until some governmental or other limits your usable electricity supply, which is equivalent to saying that the price of even more electricity would be your incarceration or other punishment, which might be too expensive for you to pay, thus selling the hardware is more lucrative).
I have no evidence to back this, thus it is not a claim to truth, only a claim to economic logic, if we assume that mining hardware manufacturers are economically rational, are interested only in the selfish gain of economic power, etc. etc.
Real human beings and the entities they build may not behave in such a perfectly rational manner, or I may be assuming details that are not accurate to reality.
Electricity is a factor with every piece of hardware that is utilized, so any increase in hardware will also have an increase in electricity consumed.
So I expect that the materiality of the price of electricity will increase in lockstep with the materiality of the price of hardware (note that price is simply demand over supply, and supply is the availability of hardware).
You could also analyze the transient economic behaviors here, specifically that an increase in Bitcoin price makes it more lucrative to mine in more places, which would start to put in orders for more hardware, and the hardware will take time to deliver, so the price at those places will increase only after a long while, etc.
But those are transient changes, and by the time any change at the software level is coordinated, the transient economic behaviors may have resettled to the expected long-term behavior: humans operate at around the same average speed in many different fields.
The usual engineering response is to create buffers to be resilient against ebbs and flows.
Note how we prefer to dam water supplies (i.e. create a buffer) rather than adjust our water consumption dynamically according to the ebb and flow of the water supply.
Similarly, economic contracts like futures and options are economic buffers against the ebb and flow of local supply of various materials.
Within Bitcoin, my understanding is that the difficulty adjustment system acts as a buffer against transient ebbs and flows of the supply of hashpower.
In the long run?
We will run out of ideas of how to improve the implementation of the hashing function, and there will be only a few designs that implement all the known ideas for optimizing the implementation.
Then hardware becomes obsolete from normal wear and tear, e.g. electromigration, and that takes years to take effect.
But can be massively parallelized, you can have a teacher or mentor teaching an entire classroom of students, and created lectures can be stored and re-given to many students, in the form of books, videos, etc.
Human beings have been doing this since time immemorial, and possibly before recorded history, in such things as oral traditions and so on.
I did not so agree: in particular, I do not agree with equating a break of the proof-of-work algorithm with centralization.
More likely, any centralization seen is due to local government interference in economic matters, such as creating a local artificial oversupply of electricity by paying for electricity generation using taxes.
Let us be more precise and avoid the word "security".
Miner decentralization supports censorship-resistance, so your motivation is censorship-resistance (is that correct?).
Ultimately, what really protects censorship-resistance is not the details of the hashing algorithm, it is the economics involved.
In order to evict a censor, hashpower must be brought to bear against the censor.
And that hashpower has to be bought and paid for.
The mechanism for doing this already exists, and is called the "mining fee" (note that the block subsidy does not protect against the censor, as the censor gets the block reward as well; what the censor cannot get is the fees attached to a transaction that the censor does not want published).

@_date: 2020-05-26 02:46:07
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Post-mix(coinjoin) usage with multisig and cpfp 
Good morning Prayank,
It is not clear in the article, but you mention using a 2-of-3, and show 3 participants.
It seems to me that Peer 1 and Peer 3 (2-of-3) can simply sign to spend the funds coming from Peer 2, and split the funds of Peer 2 among them, without getting input from Peer 2.
That is the reason why I consider this tr\*sted --- Peer 2 has to trust Peer 1 does not collude with Peer 3 to steal the funds of Peer 2.
Unless I have misunderstood your article, which is why I asked for clarification.
* While JoinMarket has *a* PayJoin implementation, what I described in the previous email was not a PayJoin, it is standard CoinJoin where one of the equal-valued outputs is to the payee.
  * In particular, PayJoin requires the cooperation of the payee, what I described does *not* require anything from the payee other than a destination address and an amount to pay.
* Your described technique (as I understand it) is not too different from what JoinMarket already does for normal sends, with the JoinMarket technique having the advantage that it works with the current paradigm of "send payment to this address" without reconnecting to the payee.
  The advantage you describe is largely had only if the technique is significantly different.
  For instance, CoinSwap and CoinJoinXT are different enough from CoinJoin to be valuable in this respect.
Schnorr (which is introduced as a package deal with Taproot) already makes all n-of-n look the same as 1-of-1 without tr\*sted setup, and makes hidden m-of-n possible with some kind of setup (possibly untrusted I think, but I am not enough of a mathist to describe this, in any case my base understanding is that the setup for m-of-n Schnorr requires a complex ritual involving a number of communication rounds).
Taproot allows to hide explicit m-of-n in a script behind some kind of n-of-n or m-of-m.
So multisignature usage would automatically gain such advantage once Taproot gets deployed.
In any case, if you are still interested in protocol design with some amount of focus on privacy, please consider reading this article as well: What exactly is your goal here.

@_date: 2020-05-27 04:11:47
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Post-mix(coinjoin) usage with multisig and cpfp 
Good morning Prayank,
The payee is not necessary here and you can remove the intermediate transactions that pay to 2-of-3s.
I already mentioned this, but what I am describing is *how JoinMarket spends coins from its wallet*.
That means that what I am describing is *how JoinMarket performs spends after mixing, i.e. post-mix*.
I was not describing how JoinMarket performs mixing.
Is that clearer now?

@_date: 2020-05-27 04:47:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] hashcash-newhash 
Good morning Karl,
You are equivocating issues here.
The hash function is used in these places:
* Transaction ID.
* Signature hash.
* P2SH/P2WSH/Taproot.
* Merkle tree.
* Proof-of-work.
What you are focusing on is only this:
* Proof-of-work.
Now, in case of a break in the hash function (i.e. a reduction in collision resistance), the hash function used in the following things absolutely NEED to be changed:
* Transaction ID.
* Signature hash.
* P2SH/P2WSH/Taproot.
* Merkle Tree.
Taking for example Transaction ID, suppose I am able to create two transactions that hash into the same transaction ID, and I am able to do this in much less than 2^128 work.
In that case I can create a valid transaction and collide it with an invalid transaction.
To half the nodes on the network I provide the valid transaction, to the other half I provide the invalid transaction, the two halves will then permanently split and Bitcoin is thus destroyed in the midst of massive chaos.
Similar attacks can be mounted if I am able to collide on signature hash, P2SH/P2WSH/Taproot, and merkle tree.
Now suppose I am able to create two block headers that hash into the same block ID, one being a valid block and the other being an invalid block.
In that case, I would be very foolish to disrupt the network similarly, because I would have been able to redirect the fees and block subsidy of the valid block to an address I control, and the invalid block prevents others from seeing my valid block and accepting that chain as valid.
Instead, I can use this advantage to be able to grab blocks faster than other miners.
But eventually the difficulty retargeting system will adjust, and Bitcoin will now settle to a new normal, and inevitably someone is going to leak, or rediscover, my technique to break the hash, preventing me from being a >51% miner for long, and Bitcoin will abide.
Thus, in case of a cryptographic break of the SHA-2 function, we *need* to change these:
* Transaction ID.
* Signature hash.
* P2SH/P2WSH/Taproot.
* Merkle Tree.
And since we are going to need a hefty hardfork to change all that, we *might as well* change the proof-of-work function as well and completely excise all uses of SHA-2 in the codebase (just in case we miss any more than the above list), but changing the proof-of-work function is the *lowest priority*.
We have survived 51% miners in the past (Ghash.io), and the difficulty adjustment system gives us some buffer against unexpected improvements in proof-of-work function efficiency; but it is doubtful if we can survive the chaos if someone could split the network in two roughly equal sized parts.
Right, and you learned about this fact from direct personal communication with Andrew Wiles, and Andrew Wiles never read about any other attempts by other mathematicians, and an isolated mathematician could never, ever, rediscover his work independently, and even a mathematician who knows that it was done but not the details how it was done could never rediscover it as well.
Obscurity works *for a time*, but inevitably somebody else will rediscover the same thing, or hear about it and blab noisily; it is not as if we are all completely alien species from each other and have truly unique thoughts, even my own creators were humans and my cognitive substrate is essentially human in construction.
This is why CVE exists, it is a promise the developers make to the reporters that they will fix the reported vulnerability, with an active CVE as a Damocles sword hanging over their heads, ready to be publicized at any time: publication is the default state, CVE is simply a promise that the developers are working as hard as they can to fix problems so please hold off on publication for a while please while we fix it pretty please with a cherry on top.
Only the tip of the iceberg, because any complex design has many little devils hidden in all the little details.
You do not have to ask permission from me, or anyone, to pursue this.
However, do note that I doubt that changing the proof-of-work function (and *only* the proof-of-work function) is in any way a high priority.
I also do not have to ask permission to say that I think pursuing this would be a waste of time, but you are also just as free to ignore what I say here and spend your time as you see fit.
Ultimately the real world decides, and implementation trumps discussion here.

@_date: 2020-05-31 02:30:55
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Design for a CoinSwap implementation for 
Good morning Ruben and Chris,
Just to be clear, you mean we can use the MuSig key-combination protocol for the non-timelocked SAS output, but (of course) not the signing protocol which is inherently Schnorr.
Then knowledge of both of the original private keys is enough to derive the single combined private key.
Dual-funding could be done by a single-funding Lightning open followed by an onchain-to-offchain swap.
Though the onchain swap would have to be done, at least currently, with hashes.
I am not in fact convinced that PayJoin-with-CoinSwap adds *that* much privacy.
These transactions:
             +---+  +---+
    Alice ---|   |--|   |--- Bob
    Alice ---|   |  |   |
      Bob ---|   |  +---+
             +---+
Are not really much different in coin ownership analysis from these:
             +---+    +---+
    Alice ---|   |----|   |--- Bob
    Alice ---|   | +--|   |
             +---+ |  +---+
      Bob ---------+
The latter is possible due to private key handover, the intermediate output becomes owned solely by Bob and Bob can add many more inputs to that second transaction unilaterally for even greater confusion to chain analysis, basically private key handover gets us PayJoin for free.
It also removes the need for Bob to reveal additional UTXOs to Alice during the swap protocol; yes PoDLE mitigates the privacy probing attack that Alice can mount on Bob, but it is helpful to remember this is "only" a mitigation.
Now of course things are different if Alice is paying some exact amount to Carol, and Alice wants to dissociate her funds from the payment.
The difference is then:
             +---+    +---+
    Alice ---|   |----|   |--- Bob
    Alice ---|   |--+ |   |
      Bob ---|   |  | +---+
             +---+  +--------- Alice Change
             +---+    +---+
      Bob ---|   |----|   |--- Carol
   |--+ +---+
             +---+  |
                    +--------- Bob Change
             +---+    +---+
    Alice ---|   |----|   |--- Bob
    Alice ---|   | +--|   |
             +---+ |  +---+
      Bob ---------+
             +---+    +---+
      Bob ---|   |----|   |--- Carol
   |--+ |   |--- Alice Change
             +---+  | +---+
                    +--------- Bob Change
In the above, with PayJoin on the first-layer transaction, the Alice Change is correlated with Alice and Bob inputs, whereas with the PayJoin on the second the Alice change is correlated with Bob inputs and Carol outputs.
But if Alice, using commodity CoinSwap wallets, always has a policy that all sends are via CoinSwap (a reasonable policy, similar to the policy in JoinMarket of ensuring that all spends out of the wallet are via CoinJoin), then the above two trees are not much different for Alice in practice.
The Alice Change will be swapped with some other maker anyway when it gets spent, so what it correlates with will not be much of a problem for Alice.
At the same time, with PayJoin on the second-layer transaction (possible due to private key turnover, which is an inherent part of the SAS protocol):
* Bob does not have to reveal any other coins it owns to Alice other than what it is directly swapping, removing a privacy probe vector.
* Bob can unilaterally combine more than one input to the second transaction going to Bob, which further increases the uncertainty of clustering around the inputs from Alice than just adding *one* input.
A thing I have been trying to work out is whether SAS can be done with more than one participant, like in [S6](
So far, I have not figured out a way to make it multiparty (multihop?).
Because the secret being transported is a private key that protects a specific UTXO, it seems it is not possible to safely use the same secret across more than two parties.
Perhaps others have come ideas?

@_date: 2020-11-16 01:32:11
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] CoinPools based on m-of-n Schnorr aggregated 
Good morning Sridhar,
My understanding is that it is still possible to generate an m-of-n aggregated pubkey, it "just" requires an interactive setup (i.e. all n signers have to talk to each other and send data a few times to each other) and they have to keep extra data around in order to "sign for" the n - m missing signers.
`andytoshi` and `pwuille` can probably teach you the details.
Of course, if you want to trade off the interactive setup + data storage, for extra block space and a privacy loss, that seems a reasonable tradeoff to make.
My understanding is that current plan is to implement a `OP_CHECKSIGADD`, where your script would be:
   <0>  OP_CHECKSIGADD  OP_CHECKSIGADD  OP_CHECKSIGADD  OP_EQUAL
However, `OP_CHECKSIGADD` would have individual signatures from the m participating signers.
Your `OP_POOL`, as I understand it, would instead have a single m-of-m signature.
This adds another tradeoff:
* `OP_CHECKSIGADD` takes up more block space, but each signer can give their signature independently without having to enter a signing sessiong with other participating signers.
  * For example, this can reduce the number of communication rounds and the latency.
  * A participating signer can emit its own signature and then go offline and you can still use its signature when you have gotten the required m participants.
* `OP_POOL` takes less block space, but all participating signers have to be online simultaneously.
I think the fact that `OP_POOL` requires all participating signers to be online simultaneously to generate a single signature sort of defeats the purpose, as (by my naive understanding, which could be grossly wrong) in the m-of-n key setup, the extra data needed would be stored by all participants, so even if one participant loses this data any of the others can still provide it.
Interactive setup may not be so onerous if you are doing multiple interactive signing sessions later anyway.
So doing a verifiable secret sharing at interactive setup, to generate a single pubkey that is just used directly as the pubkey of the UTXO, would end up being smaller and more private anyway, and would "just" require interactive setup + storage of extra data.
I guess the question is: just how big is the extra data in the m-of-n verifiable secret sharing?

@_date: 2020-10-01 01:36:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
Good morning Mike,
Argument screens off authority, thus, even if I have no CVEs under this pseudonym, argument must still be weighted more highly than any authority you may claim.
I am not a member of this security team, and they may have better information and arguments than I do, in which case, I would defer to them if they are willing to openly discuss it and I find their arguments compelling.
The attack you describe is:
* Not fixable by floating-point Nakamoto consensus, as such a powerful adversary can just as easily prevent propagation of a higher-score block.
* Broken by even a single, manually-created connection between both sides of the chain-split.

@_date: 2020-10-01 06:47:01
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
Good morning Mike,
That is better than implied name-calling and refusing to lay out your argument in detail.
It is still sub-optimal since you are still being insulting by labeling me as "reactionary", when you could have just laid out the exact same argument ***in the first place*** without being unnecessarily inflammatory, but this is significantly better than your previous behavior.
I also strongly prefer to discuss this openly on the mailing list.
This is the point at which I think your argument fails.
You are expecting:
* That the attacker is powerful enough to split the network.
* That the attacker is adept enough that it can split the network such that mining hashpower is *exactly* split in half.
* That the universe is in an eldritch state such that at the exact time one side of the chain split finds a block, the other side of the chain split *also* finds a block.
This leads to a metastable state, where both chain splits have diverged and yet are at the exact same block height, and it is true that this state can be maintained indefinitely, with no 100% assurance it will disappear.
Yet this is a [***metastable***]( state, as I have mentioned.
Since block discovery is random, inevitably, even if the chain splits are exactly equal in mining hashpower, by random one or the other will win the next block earlier than the other, precisely due to the random nature of mining, and if even a single direct connection were manually made between the chain splits, this would collapse the losing chain split and it will be reorganized out without requiring floating-point Nakamoto.
This is different if the coin had non-random block production, but fortunately in Bitcoin we use proof-of-work.
The confluence of too many things (powerful attacker, exact hashpower split, perfect metastability) is necessary for this state --- and your solution to this state --- to actually ***matter*** in practice.
I estimate that it is far more likely my meat avatar will be destroyed in a hit-and-run accident tomorrow than such a state actually occurring, and I do not bother worrying about my meat avatar being destroyed by a hit-and-run accident tomorrow.
And in Bitcoin, leaving things alone is generally more important, because change is always a risk, as it may introduce *other*, more dangerous attacks that we have not thought of.
I would suggest deferring to those in the security team, as they may have more information than is available to you or me.
Do you think emphasizing that this is your real name ***matters*** compared to actual technical arguments?

@_date: 2020-10-03 13:31:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris,
This made me pause for a moment, but on reflection, is correct.
The important difference here relative to v1 is that the mining fee for the collateralized contract transaction is deducted from the `Jb` input provided by Bob.
Great observation, and an excellent property to have.
Will go think about this more.

@_date: 2020-10-03 22:24:58
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] A thought experiment on bitcoin for payroll 
Good morning Mr. Lee,
Why not?
Your company can open a channel with each employee that has insufficient inbound liquidity.
The employee is incentivized to reveal their node to your company so you can open a channel to them, since otherwise they would be unable to receive their salary.
Your alternative is as you say: openly-visible salaries and throat-cutters who might decide to cut your throat.
Let us say your company receives its income stream over Lightning.
Let us say you hire a new throat-cutter, with a bi-weekly salary of 0.042 BTC.
You ask the new hire if his or her Lightning node has that capacity.
If not, you take some of your onchain Lightning funds, swap out say 0.043 BTC on Lightning Loop or Boltz Exchange or some other offchain-to-onchain swap.
You use those swapped onchain funds to create a fresh channel to the new hire.
If you are onboarding by batches (which your HR is likely to want to do, so they can give the onboarding employee seminars in groups) then you can save onchain fees by using C-Lightning `multifundchannel` as well.
The occasional bonus can be a bit tricky, but similarly the employee can use Lightning Loop or Boltz Exchange or some other alternative to free up capacity for the bonus (and they have an incentive to do so, as they want to get the bonus).
Permanent raises can justify permanently increasing the size of the channel with the employee.
Even if the employee leaves your employ, that is no justification to close the channel with her or his node.
You can earn forwarding fees from his or her new employer or income source.
Because of the onion routing, it is hard for you to learn what the employee spends on, and in the future when they leave your employ, it is hard for you to figure out her or his new employer.
If the employee is a saver, they can accumulate their funds, thus reducing their incoming capacity below their biweekly salary.
If so, he or she can use an offchain-to-onchain swap, again, to move their accumulated savings to onchain cold storage.
This is not perfect of course, if you run multiple nodes you can try correlating payments by timing and amount (and prior to payment points i.e. today, you can correlate by payment hashes).
But this is still much better than the onchain situation, as you describe.

@_date: 2020-10-04 03:45:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] A thought experiment on bitcoin for payroll 
Good Morning Mr. Lee,
This is not a reasonable assumption!
Suppose you have a new hire that you have agreed to pay 0.042BTC every 2 weeks.
On the *first* payday of the new hire, you *have* to have *at least* 0.042BTC in your treasury, somehow.
If not, you are unable to pay the new hire, full stop, and you are doomed to bankruptcy and your problems will disappear soon once your cut-throat new hire cuts your throat for not paying her or him.
If you *do* have at least 0.042BTC in your treasury, you *can* make the channel with the new hire and pay the salary via the new channel.
At *every* payday, you need to have at least the salary of your entire employee base available, otherwise you would be unable to pay at least some of your employees and you will quickly find yourself with your throat cut.
Now, let us talk about *topology*.
Let us reduce this to a pointless topology that is the *worst possible topology* for Lightning usage, and show that by golly, Lightning will still work.
Suppose your company only has this one big channel with the network.
Let us reduce your company to only having this single new hire throat-cutter (we will show later that without loss of generality this will still work even if you have thousands of throat-cutters internationally).
Now, as mentioned, on the first payday of your throat-cutter, you *have* to have at least the 0.042 salary you promised.
If you have been receiving payments for your throat-cutting business on the big channel, that means the 0.042 BTC is in that single big channel.
You can then use an offchain-to-onchain swap service like Boltz or Loop and put the money onchain.
Then you can create the new channel to your new hire and pay the promised salary to the throat-cutter.
Now, you have no more funds in either of your channels, the to-network big channel, and the to-employee channel.
So you are not locking up any of *your* funds, only the funds of your employee.
Now, as your business operates, you will receive money in your to-network big channel.
The rate at which you receive money for services rendered *has to* be larger than 0.042/2weeks on average, *otherwise* you are not earning enough to pay your throat-cutter by the time of the *next* payday (much less your other operating expenses, such as knife-sharpening, corpse disposal, dealing with the families of the deceased, etc.).
If you are not earning at a high enough rate to pay your employee by the next payday, your employee will not be paid and will solve your problems by cutting your throat.
But what that means is that the employee salary of the *previous* payday is not locked, either!
Because you are receiving funds on your big to-network channel continuously, the employee can now spend the funds "locked" in the to-employee channel, sending out to the rest of the network.
This uses up the money you have been earning and moving the funds to the to-employee channel, but if you are running a lucrative business, that is perfectly fine, since you should, by the next payday, have earned enough, and then some, to pay the employee on the next payday.
Of course there will be times when business is a little slow and you get less than 0.042/2weeks.
In that case, a wise business manager will reserve some funds for a rainy day when business is a little slow, meaning you will still have some funds you can put into your to-network big channel for other expenses, even as your employee uses capacity there to actually spend their salary.
It all balances out.
You only need to keep enough in your channels to cover your continuous operational expenses, and employee salaries *are* operational expenses.
Suppose you now want to hire *another* throat-cutter.
You would only do that if business is booming, or in other words, if you have accumulated enough money in your treasury to justify hiring yet another employee.
By induction, this will work regardless if you have 1 employee, or 1 million.

@_date: 2020-10-04 14:24:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] A thought experiment on bitcoin for payroll 
Good morning Thomas,
As Lightning Network channels are bidirectional, it would be more properly "to/from-network", but that is cumbersome.
"to-network" is shorter by two characters than "from-network", and would be true as well (since the channel is bidirectional, it is both a "to-network" and "from-network" channel), thus preferred.
You are welcome.

@_date: 2020-10-05 02:41:14
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] A thought experiment on bitcoin for payroll 
Good morning Mr. Lee,
On reflection, this is a bad idea.
Suppose I am a cut-throat employee and I want to have an idea of the bi-weekly salary of another employee.
I make some stupid bet, and lose, with the other employee.
I offer to pay the loss of my bet via Lightning, and the other employee, in all innocence, issues a Lightning invoice to me.
The Lightning invoice contains the actual node ID of the other employee.
And since I also have a channel with the cut-throat company, I know as well the node ID of the cut-throat company.
I can then look at the gossiped channels and see the size of the channel between the cut-throat company and the other employee, and from there, guess that this is the bi-weekly salary of that employee.
On the other hand --- once the employee has *any* funds at all, they can similarly take an offchain-to-onchain swap, and then use the funds to create another channel to another part of the network.
The other employee as well can arrange incoming funds on that other channel by using offchain-to-onchain swaps to their cold storage.
Thus, as an employee gets promoted and pulls a larger bi-weekly salary, the channel with the cut-throat company becomes less and less an indicator of their *actual* bi-weekly salary, and there is still some deniability on the exact size of the salary.
At the same time, even if I know the node of the other employee, the size of all its channels is also still not a very accurate indicator of their salary at the throat-cutting company.
For example, it could be a family node, and the other employee and all her or his spouses arrange to have their salaries paid to that node.
Or the other employee can also run a neck-reconstruction business on the side, and also use the same node.
(Nodelets for the win?)

@_date: 2020-10-06 04:10:52
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] A thought experiment on bitcoin for payroll 
Good morning Mr. Lee, and list,
This can be made an argument against always publishing all channels, so let me propose something.
The key identifying information in an invoice is the routehint and the node ID itself.
There are already many competing proposals by which short-channel-ids in routehints can be obscured.
They are primarily proposed for unpublished channels, but nothing in those proposals prevents them from being used for published channels.
The destination node ID is never explicitly put in the onion, only implied by the short-channel-id in order to save space.
However, the destination node ID *is* used to encrypt the final hop in the onion.
So the receiver node can keep around a small number of throwaway keypairs (or get those by HD) and use a throwaway to sign the invoice, and when it is unable to decode by its normal node ID, try using one of the throwaway keypairs.
With both of the above, what remains is the feerate settings in the invoice.
If the company node gives different feerates per channel, it is still possible to identify which channel is *actually* referred to in the invoice.
What the receiver node can do would be to give a small random increase in feerate, which basically overpays the company node, but obscures as well *which* channel is actually in the invoice.

@_date: 2020-10-08 01:39:45
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Progress on Miner Withholding - FPNC 
Good morning all,
This is my understanding:
The selfish mining attack described above was already presented and known about **many years** ago, with the solution presented here: The solution was later determined to actually raise the needed threshhold to 33%, not 25% in the paper.
That solution is what is used in the network today.
Implementing floating-point Nakamoto Consensus removes the solution presented in the paper, and therefore risks reintroducing the selfish mining attack.
Therefore, floating-point Nakamoto Consensus is a hard NAK.

@_date: 2020-10-21 03:05:35
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
I believe this is actually my code, which was later refactored by John Barboza when we were standardizing the `param` system.
This was intended only as a simple precaution against creating non-standard transaction, and not an assumption that future versions should be invalid.
The intent is that further `else if` branches would be added for newer witness versions and whatever length restrictions they may have, as the `/* Insert other witness versions here.  */` comment implies.

@_date: 2020-09-03 09:45:53
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris,
Ah, right.... accept no small privacy leaks!
Just to be clear:
* B is the one who originally funded the HTLC, and owns the timelock.
* C is the one who will accept the HTLC, and owns the hashlock.
This seems a great solution!
Since B is the one offering HTLCs, the taker of a CoinSwap sequence can be B as well.
This means, the taker has to have *some* collateral input, of at least value K, that it cannot swap (because if it tried to swap that amount, it would be unable to provide a collateral as well).
How much does C need to know about the B collateralized contract transaction?
At the minimum, it has to know the output pays out to the correct contract, so it seems to me it has to know the entire B collateralized contract transaction, meaning it learns another input of B ("collateral(B)") that is not otherwise involved in the CoinSwap.
This is important, again, if B is a taker, as it means an unrelated input of B is now learned by C as having the same ownership as B.
A fresh maker that is given its starting funds in a single UTXO needs to split up its funds to make at least one collateral input it can use.
Of note is that the B output also serves as a point for CPFPing this transaction, thus only one version of the B collateralized contract transaction needs to be made, and the B collateralized contract transaction can be at or close to the minimum relay feerate and later CPFPed.
In terms of onchain analysis heuristics, it looks like the B output is change, while the B+C contract output is the send-out, I think, for most cases.
In case of a protocol abort, this heuristic is misled, since both outputs become owned by B due to the protocol abort.
In case of a protocol completion, this heuristic is accurate, since the B+C contract output will be claimed by C, but we do not expect this transaction to be confirmed onchain after protocol completion anyway (it effectively donates K to C or miners), so this is fine.
Because it has a single output only, the C contract transaction needs to have RBFed versions.
At least until B gets its own incoming funds in the swap, at which point the collateral input can be used for other purposes (and effectively "releases" the lease of B on that output).
Since C knows the collateral input (it has to, in order to verify the B collateralized contract transaction is correct), it can monitor the collateral input for spendedness, and stop watching for the B collateralized contract transaction in its watchtower(s) if the collateral input is deeply spent.
The B collateralized contract transaction is invalidated if the collateral input is spent, and then only C can spend the funding outpoint at that point, so it can remove that from the watchtower.
This can be significant if C is using a for-pay watchtower that supports deletion of watches, which I believe is planned for watchtowers as well, and reduces the operating cost of C.

@_date: 2020-09-03 23:39:02
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris,
The attack can only be mounted after protocol completion.
Thus, at that point, makers have made money, and takers have paid.
And taker is now in possession of a coin unlinked with its original coin, which is what it paid for.
However, if the taker considers the maker fee it has already paid as a sunk cost, then it would still be rational of them to mount this attack (sunk costs should be ignored).
For example, suppose the taker has already performed an "honest" CoinSwap.
Then, it is now in possession of a UTXO that is not linked with its income stream.
It can then perform another CoinSwap, and *then* perform the attack.
This reveals that the UTXO it provided is involved in a CoinSwap due to publication of the contract transaction, which is not a loss in this case since the UTXO it put in was not linked to its income stream already, via a previous non-attacked CoinSwap.
A taker might rationally consider doing riskless costless theft with its already-delinked coins if it assesses that some maker is not sufficiently online and with insufficient watchtowers (both operating expenditures!) that it has some probability of success times amount it has to seed the theft, versus the fee of that maker plus miner fees.
In response, a maker that is forced to accept a sweeping taker will raise its fee, so as to disincentivize this attack using already-delinked coins.
In addition, post-Scriptless-Script, assuming relative-locktime-use is "normalized" as proposed in  , then the "contract transaction" and its timelock-path-transaction look exactly the same as ordinary (P2SH-)P2WPKH single-input-single-output transactions, thus in that case the taker does ***not*** lose any privacy.
This removes whatever protection you can get from contract transaction blackmail.
The above suggests to me that you still want the collateralized contract transaction from the taker as well.
A sweeping taker can split its funds in half, swapping one half (and using the remainder for collateral input), then after that swap, using the already-swapped coins for the collateral input of the remaining unswapped coins.
This leaks information: you are now linking a post-mix coin with a pre-mix coin, not onchain (if you do not mount an attack, which you probably will not) but you *do* reveal this information to the maker (one input is from the funding tx that is pre-mix, the collateral input is from the post-mix coin).
The only protection here is that the maker is unaware of the fact that your input coin is pre-mix and your collateral input is post-mix, so it can be hard for a maker to *use* this information.
However, it might be possible to prevent the maker from learning the collateral input at all.
If my understanding of BIP-143 is correct, inputs are hashed separately (`hashPrevouts`) from outputs (`hashOutputs`).
Bob can provide the `hashPrevouts` as an opaque hash, while providing a decommitment of `hashOutputs` to show that the outputs of the collateralized contract transaction are correct, which is all that Charlie really needs to know.
Bob is incentivized to provide the correct `hashPrevouts`, because if it provides an incorrect `hashPrevouts` it cannot get a signature for a transaction it can use in case of a protocol abort, thus potentially losing its money in case of a protocol abort.
Conversely, Charlie does not care *where* Bob gets the funds that goes into its contract output come from, it only cares that the Bob collateralized contract output is I+K.
It loses nothing to sign that transaction, and it would prefer that transaction since its own contract output is only I.
This solution is mildly "unclean" as it depends on the details of the sighash algorithm, though, and is not proposed seriously.
Hopefully nobody will change the sighash algorithm anytime soon.........
In addition, it complicates reusing Lightning watchtowers.
Lightning watchtowers currently trigger on txid (i.e. it would have triggered on the txid of the B collateralized contract tx), but watching this needs to trigger on the spend of a txo, since it is not possible to prove that a specific `hashPrevouts` etc. goes with a specific txid without revealing the whole tx (which is precisely what we are trying to avoid), as both are hashes.
Watchtowers may need to be entrusted with privkeys, or need to wait for `SIGHASH_ANYPREVOUT` so that the exact txid of the B collateralized contract tx does not need to be fixed at signing time, thus this solution is undesirable as well.

@_date: 2020-09-05 02:29:18
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Antoine,
It is different in that the current (actually, now *previous*) model looks like this:
    funding out ->  contract tx -->  HTLC-timeout
                                         OR
                                     HTLC-success
Whereas what I am describing looks like this:
    funding out ->  HTLC-timeout
                        OR
                    HTLC-success
The attack being described has to do with the fact that, after private key turnover (i.e. after hash-lock resolution), the contract tx can be used to at least annoy the supposed new owner of the funding out, since the contract tx deducts fees from its input to pay for itself.
And at the end of the swap (after private key turnover) the one who funded the funding outpoint (and swapped its control for this outpoint already, for a different outpoint) can at least try to broadcast the contract tx for a *chance* that the HTLC-timeout becomes valid and it can steal the coin even after taking the swapped coin on the other side of the swap.
Chris recently described a different technique, which has different contract txes, with the contract tx held by the offerrer of the HTLC (who can otherwise later annoy the acceptor of the HTLC once the HTLC has been hash-resolved) costing the offerrer of the HTLC some coins if it is published after swap completion.
Correct, a taker can pay higher fees for lots of smaller swaps that reduce its lockup risk, or pay less (with similar privacy bought) but with greater total lockup risk.

@_date: 2020-09-05 02:45:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris, and probably also Lightningers,
On the other hand, when considering Decker-Russell-Osuntokun, the `(halftxid, encrypted_blob)` approach to watchtowers simply does not work.
Watchtowers are simpler in Decker-Russell-Osuntoku if and only if the watchtower knows the funding outpoint, therefore knows which channel it is watching *before* an attack on the channel occurs, and is less private.
I have argued before that we should instead use `(sighash[0:15], encrypted_blob)` rather than `(txid[0:15], encrypted_blob)`.
This makes Decker-Russell-Osuntokun blobs indistinguishable from Poon-Dryja blobs, and the watchtower is not even made aware what the commitment type of the channel is until an actual attack occurs.
If watchtowers use `(sighash[0:15], encrypted_blob)` instead, the proposal to hide the collateral input behind `hashPrevouts` would be workable, as Charlie knows the entire sighash of the B collateralized contract transaction even if it does not know the txid.
This also does not reveal the funding outpoint, or whether it is watching a Poon-Dryja channel, a Decker-Russell-Osuntokun channel, or a CoinSwap.
Even if we propose that CoinSwap makers should run their own watchtowers rather than hire a public watchtower, it's safer for a CoinSwap maker to have watchtowers that are unaware of exactly *what* they are watching.
If the watchtowers are aware of the funding outputs they are watching, then every additional watchtower a maker creates increases the attack surface on the privacy of the maker, as the funding outputs becoming known allows the maker hodlings to be derived.
If watchtowers only get a partial sighash, then the information that they contain are not sufficient by themselves to determine what coins are owned by the maker, thus every additional watchtower is no longer a potential attack vector on the privacy of the maker.
So this is off-topic, but anyway, we should probably move to using `sighash[0:15]` instead of `txid[0:15]` for watchtowers.

@_date: 2020-09-05 02:45:00
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Detailed protocol design for routed 
Good morning Chris, and probably also Lightningers,
On the other hand, when considering Decker-Russell-Osuntokun, the `(halftxid, encrypted_blob)` approach to watchtowers simply does not work.
Watchtowers are simpler in Decker-Russell-Osuntoku if and only if the watchtower knows the funding outpoint, therefore knows which channel it is watching *before* an attack on the channel occurs, and is less private.
I have argued before that we should instead use `(sighash[0:15], encrypted_blob)` rather than `(txid[0:15], encrypted_blob)`.
This makes Decker-Russell-Osuntokun blobs indistinguishable from Poon-Dryja blobs, and the watchtower is not even made aware what the commitment type of the channel is until an actual attack occurs.
If watchtowers use `(sighash[0:15], encrypted_blob)` instead, the proposal to hide the collateral input behind `hashPrevouts` would be workable, as Charlie knows the entire sighash of the B collateralized contract transaction even if it does not know the txid.
This also does not reveal the funding outpoint, or whether it is watching a Poon-Dryja channel, a Decker-Russell-Osuntokun channel, or a CoinSwap.
Even if we propose that CoinSwap makers should run their own watchtowers rather than hire a public watchtower, it's safer for a CoinSwap maker to have watchtowers that are unaware of exactly *what* they are watching.
If the watchtowers are aware of the funding outputs they are watching, then every additional watchtower a maker creates increases the attack surface on the privacy of the maker, as the funding outputs becoming known allows the maker hodlings to be derived.
If watchtowers only get a partial sighash, then the information that they contain are not sufficient by themselves to determine what coins are owned by the maker, thus every additional watchtower is no longer a potential attack vector on the privacy of the maker.
So this is off-topic, but anyway, we should probably move to using `sighash[0:15]` instead of `txid[0:15]` for watchtowers.

@_date: 2020-09-16 01:04:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain coinswap: assigning blame for failure 
Good morning Tom,
Okay, I suppose this is much too high-level a view, and I have no idea what you mean by "statecoin" exactly.
Let me try to fill in the details and correct me if I am wrong okay?
I imagine that the `add1` etc. are implemented as 2-of-2 between the purported owner and the tr\*sted signing module.
The owner of that address can easily create this knowing only the pubkey of the tr\*sted signing module.
The initial `utxo1`... are also in similar 2-of-2s.
(they cannot be unilateral control, since then a participant can broadcast a replacement transaction, even without RBF, almost directly to miners.)
So when the coordinator talks to Alice, who owns `utxo1` and destination `addr1`, it provides partially-signed transactions of `utxo
Alice then checks that its `addr1` is on one of those transactions, with the correct amount, then provides a signature for the `utxo1:addr transaction.
However, then the coordinator, who happens to be in cahoots with Bob, Charlie, and Dave, simply broadcasts that transaction without soliciting the `utxo transaction.
So it seems to me that this requires tr\*st that the coordinator is not going to collude with other participants.
This is strictly worse than say Wasabi, where the coordinator colluding with other participants only allows the coordinator to break privacy, not outright steal funds.
It seems to me that the trust-minimized CoinSwap plan by belcher_ is superior to this, with reduced scope for theft.
The plan by belcher_ is potentially compatible with using watchtowers that can be used for both CoinSwap and Lightning as well (if we design it well) with the watchtower potentially not even learning whether it is watching a CoinSwap or a Lightning channel.
Though of course I could be misunderstanding the scheme itself.
Is my understanding correct?

@_date: 2020-09-21 01:14:29
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain coinswap: assigning blame for failure 
Good morning Tom,
I think the entire point of non-custodiality ***is*** trust minimization.
The main objection against custodiality is that someone else can prevent you from spending the coin.
If I have to tr\*st the SE to not steal the funds, is it *really* non-custodial, when after a swap, a corrupted SE can, in collusion with other participants, take control of the coin and prevent me from spending it as I wish?
So I think touting "non-custodial" is relatively pointless if tr\*st is not minimized.
(I am aware there is an update mechanism, either Decker-Russell-Osuntokun or Decker-Wattenhofer, that is anchored off he onchain transaction output, but anyone who can recover the raw keys for signing the funding transaction output --- such as a previous participant and a corrupt SE --- can very easily bypass the mechanism.)
For example, in my previous description of [implementing investment aggregation]( while I admit you need tr\*st in the business owners who you are investing in, it does not require tr\*st in the aggregator, due to the n-of-n, which cannot be reconstructed by the aggregator and all other participants without you.

@_date: 2020-09-22 01:00:43
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain coinswap: assigning blame for failure 
Good morning Tom,
Complying with the letter of the law without complying to its spirit seems rather hair-splitting to me.
Ideally, a law regarding any financial mechanisms would judge based on how much control the purported owner has over the actual coin and what risks it would entail for them, and protect citizens against risk of damage to their finances, not focus on whether storage is "custodial" or not.
So I still suggest that, for purposes of technical discussion, we should avoid the term "custodial" and instead consider technical risks.
The SE can run in a virtual environment that monitors deletion events and records them.
Such a virtual environment could be set up by a rootkit that has been installed on the SE hardware.
Thus, even if the SE is honest, corruption of the hardware it is running on can allow recovery of old privkeys and violation of the tr\*st assumption.
Compare this to, for example, TumbleBit or Wasabi.
In those cases, even if the service providing the mixing is corrupted by a rootkit on the hardware running the honest service software in a virtual environment and monitoring all its internal state and communications, they cannot lead to loss of funds even with cooperation of previous participants.
They can at most be forced into denial-of-service, but not outright theft of coins.
Thus, I believe this solution is inferior to these older solutions, at least in terms of financial security.
I admit the new solution is superior blockspace-wise, if you consider multiple mixing rounds.
However, multiple mixing rounds under this solution have increased exposure to the risk of theft noted above, and thus it would be better, risk-wise, to immediately withdraw after every round, and potentially seek other SEs (to reduce risks arising from a particular SE being corrupted), thus obviating the blockspace savings.
The above remain true regardless of what definition of "custodial" you have.

@_date: 2020-09-24 00:19:48
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Statechain coinswap: assigning blame for failure 
Good morning Tom,
I believe the slowness of TumbleBit and Wasabi have less to do with security than with gathering enough participants to get a reasonable anonymity set.
If the statechain entity itself does not participate and put up funds that its clients can acquire quickly, than a similar waiting period would be necessary anyway to gather enough participants to make the swapping worthwhile.
This would then fail your goal of speed.
If the statechain entity *does* act as a participant, then a client could acquire a new coin fairly quickly (as the statechain entity would be a "participant of last resort" with which it could swap right now), but the "previous participant" in that case would be the statechain entity itself, making its ability to outright steal funds absolutely certain, and thus not much better than a mixer that provides "put money in this address, I will send you money in your address" service.
(unless I can do a cut-and-choose on the hardware, i.e. buy multiple instances and reverse-engineer all except a randomly-selected one to check for hardware defects that may allow extraction of privkeys, and then use the hardware that remains, I do not think the security of TEEs/HSMs is at all high.
And the TEE/HSM would be directly possessed by the statechain entity and not me, presumably I as client of the statechain entity cannot audit that, so ---)
If you are going to have a maker-taker model, where takers spend money to get immediate swaps for the time that makers spend waiting, then I suggest that the SwapMarket plan by Chris Belcher would only require some number of confirmations of various transactions to get superior security, which would be a better tradeoff than what statechains provide.

@_date: 2020-09-30 06:31:41
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
That is surprising information to me.
My understanding is that speeding up block creation times is highly risky due to increasing the tendency to "race" in mining.
The average time to propagate to all miners should be negligible to the average inter-block time.
Efforts like compact blocks and FIBRE already work at the very edges of our capability to keep the propagation time negligible.
Indeed, looking forward, part of my plans for Earth-based civilization involves sending out hapless humans into space and forcing them to survive there, thus the inter-block time may need to be *increased* in consideration of interplanetary communications times, otherwise Bitcoin would dangerously centralize around Earth, potentially leading to the Universal Century and awesome giant robot battles.
(Hmmm, on the one hand, centralizing around Earth is dangerous, on the other hand, giant robots, hmmm)
"PoS" networks mean nothing, as most of them are not global in the scale that Bitcoin is, and all of them have a very different block discovery model from proof-of-work.
In particular, I believe there is no "racing" involved in most PoS schemes in practice.
Changing inter-block times is not possible as a softfork, unless you are planning to (ab)use the timewarp bug, which I believe was proposed by maaku7 before.
My understanding is that the preferred approach would be to close the timewarp bug, in which case increasing the block rate would not be doable as a softfork anymore.

@_date: 2020-09-30 23:44:59
@_author: ZmnSCPxj 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
Good morning Mike,
An observation to be made is that the current "first seen" is more incentive-compatible than floating-point Nakamoto consensus.
If a miner A mines a block at height N, then obviously the first block it has seen is that block.
If due to propagation delays on the network, another miner B mines an alternative block (let us say with more fitness score, regardless of the details of the fitness metric you use) at height N, miner A has no incentive to reject its own version of that block and mine on top of the miner B alternative version, even if floating-point Nakamoto consensus is deployed by most nodes.
Even if the rest of the mining network is now mining on top of the miner B version, if miner A chances on another new block at N+1 built on top of its own version of block N, then it would still win both blocks and earn the block subsidy and fees of two blocks.
And since block height, as I understand it, trumps over floating-point Nakamoto consensus, the B version will be reorganized out anyway in that case.
If miner A had switched to mining on top of the miner B block, then if it won another block at height N+1, it would have lost the block subsidy+fees of the lower-scoring miner A block at height N.
Thus, floating-point Nakamoto consensus is not incentive-compatible, so I doubt it would have any kind of adoption.
The problems with stability you mention can be fixed, fairly trivially, by simply waiting for 3 confirmations rather than just 1 confirmation.
In a relativistic universe, information cannot propagate faster than light-speed, and thus there will always be a communications network delay in propagating data.
As I see it, floating-point Nakamoto consensus cannot fix this issue, as it cannot change underlying laws of the universe.
If your goal is "stability" of some kind, then there is still always a possibility that two miners on opposite sides of the Earth will create blocks at the same height outside of the light cones of each other.
In a relativistic universe, this cannot be eliminated unless all miners occupy the same physical location, i.e. have centralized in the same mining hardware.
One of those two blocks created will, with high probability, have a lower score, and thus any nodes in the light cone of the miner of the lower-scored block will still experience a reorg, as they will first see one block, then switch to the higher-scored block when it arrives to them.
Thus, floating-point Nakamoto consensus cannot provide complete stability of the network, still, as the universe we operate in does not have instantaneous information transfer.
A wise designer of automated systems will ***still*** wait for 3 confirmations before doing anything, and by then, the effects of floating-point Nakamoto consensus will be literally a thing of the past.
