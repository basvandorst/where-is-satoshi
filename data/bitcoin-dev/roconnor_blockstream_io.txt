
@_date: 2016-08-16 18:23:00
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
On Tue, Aug 16, 2016 at 3:43 PM, Peter Todd via bitcoin-dev <
If one's goal is to mess with an transaction to prevent it from being
mined, it is more effective to just not relay the transaction rather than
to mess with the witness.  Given two transactions with the same txid and
different witness data, miners and good nodes ought to mine/relay the
version with the lower cost (smaller?) witness data.
Worries about "illegal data" appearing in the blockchain is not an issue
worth writing a soft-fork over.
There may be good reasons for this BIP, but I don't think the reasons give
above are good.

@_date: 2016-08-16 18:36:24
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
On Tue, Aug 16, 2016 at 6:30 PM, Pieter Wuille Can I already do something similar with replace by fee, or are there limits
on that?

@_date: 2016-08-16 18:52:38
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
I see.
But is it really necessary to soft fork over this issue?  Why not just make
it a relay rule?  Miners are already incentivized to modify transactions to
drop excess witness data and/or prioritize (versions of) transactions based
on their cost.  If a miner wants to mine a block with excess witness data,
it is mostly their own loss.
On Tue, Aug 16, 2016 at 6:39 PM, Pieter Wuille

@_date: 2016-08-16 20:27:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
I'm not really opposed to this BIP, but I am worried that fighting script
malleability is a battle that can never be won; even leaving one avenue of
malleability open is probably just as bad as having many avenues of
malleability, so it just doesn't seem worthwhile to me.

@_date: 2016-06-15 13:08:13
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] RFC for BIP: Derivation scheme for 
On Wed, Jun 15, 2016 at 7:00 AM, Pieter Wuille via bitcoin-dev <
Indeed, and you can go even further. When there are multiple "sending"
This isn't quite perfect because if there is only 1 P2PKH output and you
know the person is using the above algorithm then you know the P2PKH output
isn't the change.
I don't know what the perfect method is.  My guess is that it is to let p
be the probability that a P2PKH output is produced over the entire network
and to pick P2PKH for your change output with probability p (and similarly
for other output types).
On Wed, Jun 15, 2016 at 7:00 AM, Pieter Wuille via bitcoin-dev <

@_date: 2016-05-11 21:23:21
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Is the design and manufacturing processes for the most power efficient
ASICs otherwise patent unencumbered?  If not, why do we care so much about
this one patent over all the others that stand on the road between pen and
paper computation and thermodynamically ideal computation?
On Wed, May 11, 2016 at 8:02 PM, Gregory Maxwell via bitcoin-dev <

@_date: 2016-11-02 13:30:12
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Implementing Covenants with OP_CHECKSIGFROMSTACKVERIFY 
Hi all,
It is possible to implement covenants using two script extensions: OP_CAT
and OP_CHECKSIGFROMSTACKVERIFY.  Both of these op codes are already
available in the Elements Alpha sidechain, so it is possible to construct
covenants in Elements Alpha today.  I have detailed how the construction
works in a blog post at <
  As
an example, I've constructed scripts for the Moeser-Eyal-Sirer vault.
I'm interested in collecting and implementing other useful covenants, so if
people have ideas, please post them.
If there are any questions, I'd be happy to answer.

@_date: 2016-11-03 00:19:48
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Implementing Covenants with 
Right.  There are minor trade-offs to be made with regards to that design
point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this covenant
construction isn't sensitive to that choice and can be made to work with
either implementation of OP_CHECKSIGFROMSTACKVERIFY.

@_date: 2016-11-03 16:02:33
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Implementing Covenants with 
I admit I didn't study their poison transactions very carefully.  It seemed
specific to Bitcoin-NG.
I used to be hesitant to the idea of adding transaction introspection
operations, because the script design seemed to be deliberately avoiding
doing that.  One of the big takeaways from this work, for me at least, is
that since the transaction data is so easily recoverable anyways, adding
transaction introspection operations isn't really going to provide any more
power to script; it will just save everyone a bunch of work.  There are no
specific plans to put transaction introspection opcodes into Elements at
this moment, but I feel that the door for that possibility is wide open now.
Really minor nit: "Notice that we have appended 0x83 to the end of the
Probably should reed "Notice that we have appended 0x83000000 to the end of
the transaction data".  I'll make an update.

@_date: 2016-11-21 10:54:19
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Flexible Transactions. 
Hi Tom,
I'm a fan of simplicity too; Unfortunately, your proposal above to change
the semantics of OP_CHECKSIG is too naive.
The SIGHASH data used in both the original Bitcoin script and in Segwit
script contains data indicating which input is being signed.  In Bitcoin
script, the input is being signed is indicated by the input that has a
non-empty scriptSig field.  In the Segwit script, the outpoint
corresponding to the input being signed is explicitly included in the
signature data. By signing only the transaction id, your proposed signature
does not include the data that tells which input of the transaction is
being signed.  Thus if different inputs share the same public key due to
key reuse, then the signatures on those different inputs will be
identical.  Your Flexible Transactions proposal opens up a new line of
attack against Bitcoin that doesn't currently exist.
Consider the following simple example, suppose you and I are jointly
preparing a transaction to mix our coins, or perhaps we are jointly funding
some purchase.  We jointly prepare a transaction with one input from you
and another input from me.  We each sign the transaction and hand the
signature data over to each other so we can produce a completed
transaction.  But oh no! I lied to you. I didn't use my own input to the
transaction.  "My input" was actually the outpoint from one of *your*
transactions; one that has the same public key as the input you have
chosen.  Now I copy your signature you have provided in your input to cover
"my input", which is really your coins.  Surprise, it turns out you are
funding both inputs to our "jointly" funded purchase.  Other protocols are
likely similarly broken by your Flexible Transactions proposal.
I personally rate this flaw as about the same caliber as the transaction
malleability you are trying to fix.  Sure, with enough vigilance, perhaps
you can detect and avoid this trap.  However, it requires a bunch of
unexpected work.  You must always examine every other input to a
transaction you are about to sign to make sure that it isn't one of your
inputs, which means you probably need a copy of the UXTO set to lookup
outpoints, which is a huge burden, especially if you are a hardware
wallet.  If you are not vigilante, your funds may end up stolen. Surely it
is better not to open this line of attack.
For the most part, the SIGHASH works the way it does in Bitcoin for a
reason. You cannot simply throw away the parts you don't understand or
appreciate.  You should take the time to learn why things are the way they
are, and then, only once you are certain that some aspects are not, or no
longer, needed then can you propose removing them.

@_date: 2016-11-21 16:29:21
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Flexible Transactions. 
Oh, that is good news!  I look forward to seeing BIP 134 updated with your

@_date: 2016-10-02 14:17:12
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
If I understand this BIP correctly, the values pushed onto the stack by the
OP_COUNT_ACKS operation depends on the ack and nack counts relative to the
block that this happens to be included in.
This isn't going to be acceptable.  The validity of a transaction should
always be monotone in the sense that if a transaction was acceptable in a
given block, it must always be acceptable in any subsequent block, with the
only exception being if one of the transaction's inputs get double spent.
The added check lock time and check sequence number operations were
carefully constructed to ensure this property.
On Sun, Oct 2, 2016 at 11:49 AM, Sergio Demian Lerner via bitcoin-dev <

@_date: 2016-10-02 17:46:43
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
The values returned by OP_COUNT_ACKS vary in their exact value depending on
which block this transaction ends up in.  While the proposed use of this
operation is somewhat less objectionable (although still objectionable to
me), nothing stops users from using OP_EQUALVERIFY and and causing their
transaction fluctuate between acceptable and unacceptable, with no party
doing anything like a double spend.  This is a major problem with the

@_date: 2016-10-02 17:54:08
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
I haven't really been following the sidechain developements, but my
understanding was that redemption from a side chain would be two phase.
The person unpegging the funds provides a proof that they have locked the
funds on the side chain and are eligible to withdraw the funds, plus they
put up a bounty.  Then there is a time-locked period where others can
collect the bounty by providing a fraud proof, that the locked funds given
in the proof have actually been double spent.  This two phase system
doesn't violate this rule.

@_date: 2016-10-02 19:00:16
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Fwd: Re:  Drivechain proposal using OP_COUNT_ACKS 
I forget to send to bitcoin-dev.
innocent reorg, one that doesn't involve a double spend, the transaction
may never get back in unless it occurs at exactly  the same height, which
is not guaranteed.
therefore it
depending on which block this transaction ends up in.  While the proposed
use of this operation is somewhat less objectionable (although still
objectionable to me), nothing stops users from using OP_EQUALVERIFY and and
causing their transaction fluctuate between acceptable and unacceptable,
with no party doing anything like a double spend.  This is a major problem
with the proposal.
of P2WSH) an OP_COUNT_ACKS are not broadcast by the network. That means
that the network cannot be DoS attacked by flooding with a transaction that
will not verify due to being too late.
is invalidated after the liveness times expires.
fails to provide DoS protection for block validation since active polls can
accumulate forever.

@_date: 2016-10-02 19:26:18
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
If someone uses OP_EQUALVERIFY after OP_COUNT_ACKS then the transaction
probably won't be able to be included at a different height.
On Oct 2, 2016 19:16, "Sergio Demian Lerner"

@_date: 2016-09-05 10:55:10
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
For sake of example, suppose we have a marginal fee rate of 50 satoshis per
byte.  At that rate reducing the size of the witness data by 1 byte is
approximately equivalent from a miner and relayer's perspective as a
replace by fee that increases the fee by 50 satoshis.  In both cases miners
get an extra potential of 50 satoshis in revenue.
So in this sense replacing witness data with smaller witness data can pay
for its own relay cost as much as RBF can simply by requiring that the
smaller witness be smaller enough to cover the same RBF threshold.
On Tue, Aug 16, 2016 at 6:39 PM, Pieter Wuille

@_date: 2016-09-23 09:43:15
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
I believe Bitcoin currently enjoys the property that during an "innocent"
re-org, i.e. a reorg in which no affected transactions are being double
spent, all affected transactions can always eventually get replayed, so
long as the re-org depth is less than 100.
My concern with this proposed operation is that it would destroy this
On Fri, Sep 23, 2016 at 5:57 AM, Luke Dashjr via bitcoin-dev <

@_date: 2017-04-02 16:39:13
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP draft: Extended block header hardfork 
Someone told me a while back that it would be more natural if we move the
nHeight from the coinbase script to the coinbase locktime.  Have you
considered doing this?

@_date: 2017-04-06 09:55:31
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
Hi Jonathan,
The proposal raised here does not deny miners the ability to use ASICBOOST.
Miners can still use overt ASICBOOST by version bit fiddling and get the
same power savings.  In fact, overt ASICBOOST is much easier to implement
than covert ASICBOOST, so I don't really understand what the objection is.
On Apr 6, 2017 13:44, "Jonathan Toomim via bitcoin-dev" <
Ethically, this situation has some similarities to the DAO fork. We have an
entity who closely examined the code, found an unintended characteristic of
that code, and made use of that characteristic in order to gain tens of
millions of dollars. Now that developers are aware of it, they want to
modify the code in order to negate as much of the gains as possible.
There are differences, too, of course: the DAO attacker was explicitly
malicious and stole Ether from others, whereas Bitmain is just optimizing
their hardware better than anyone else and better than some of us think
they should be allowed to.
In both cases, developers are proposing that the developers and a majority
of users collude to reduce the wealth of a single entity by altering the
blockchain rules.
In the case of the DAO fork, users were stealing back stolen funds, but
that justification doesn't apply in this case. On the other hand, in this
case we're talking about causing someone a loss by reducing the value of
hardware investments rather than forcibly taking back their coins, which is
less direct and maybe more justifiable.
While I don't like patented mining algorithms, I also don't like the idea
of playing Calvin Ball on the blockchain. Rule changes should not be
employed as a means of disempowering and empoverishing particular entities
without very good reason. Whether patenting a mining optimization qualifies
as good reason is questionable.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-04-26 17:34:29
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Segwit v2 
On Wed, Apr 26, 2017 at 4:01 PM, Luke Dashjr via bitcoin-dev <
I'm not sure what you are referring to here.  The data in the scriptSigs
are never signed.  The scriptSigs are always stripped from the transaction
before the sigHash is made.

@_date: 2017-04-28 16:53:05
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Quadratic Hashing in BIP 134 
============================== START ==============================
I noticed that the the latest BIP 134
now supports SIGHASH_SINGLE and friends.  However, this support seems to
reintroduce some quadratic hashing behavior because it calls
per non-SIGHASH_ALL input
In particular, if each input in a transaction has one SIGHASH_SINGLE
CHECKSIG operation then the total amount of hashing done for the
transaction will be quadratic in the number of inputs.  While amount of
hashing is not as severe as with the SIGHASH_ALL case, the amount of
hashing done is still non-linear.

@_date: 2017-02-25 15:53:12
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <
Be aware that the issue is more problematic for more complex contracts.
For example, you are building a P2SH 2-of-2 multisig together with someone
else if you are not careful, party A can hand their key over to party B,
who can may try to generate a collision between their second key and
another 2-of-2 multisig where they control both keys. See

@_date: 2017-01-03 00:04:50
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Script Abuse Potential? 
OP_2DUP?  Why not OP_3DUP?
On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau via bitcoin-dev <

@_date: 2017-01-03 22:13:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Script Abuse Potential? 
For the record, the OP_CAT limit of 520 bytes was added by Satoshi
on the famous August 15, 2010 "misc" commit, at the same time that OP_CAT
was disabled.
The previous limit was 5000 bytes.
On Tue, Jan 3, 2017 at 7:13 PM, Jeremy via bitcoin-dev <

@_date: 2017-01-27 15:34:13
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
Other researchers have come to the conservative conclusion that we could
handle 4MB blocks today.
I believe this is a mischaracterization of the research conclusions.  The
actual conclusion was that the maximum value for the blocksize that the
network can safely handle (at that time) is some value that is
(conservatively) no more than 4MB.  This is because the research only
studies one aspect of the effect of blocksize on the network at a time and
the true safe value is the minimum of all aspects.  For example, the 4MB
doesn't cover the aspect of quadratic hashing for large transactions in
large blocks.

@_date: 2017-07-12 09:39:17
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
On Wed, Jul 12, 2017 at 4:50 AM, ZmnSCPxj via bitcoin-dev <
In any case, let me propose actual improvements to the OP_BRIBEVERIFY
At this point can we eliminate the need to use the scripting system at all
and just use a special, currently non-standard, OP_RETURN output to hold
the sidechain_id and h* instead?  We can soft fork in a rule that at most
one such output can appear in a block per sidechain_id.

@_date: 2017-06-01 11:10:56
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Yes, either way would be fine.
I was looking to add the property mostly because it was free to do with my
original design when the set of tags was small and could make some
applications more robust and/or easier to debug.

@_date: 2017-06-28 18:49:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
I haven't really been following the drivechain discussion; I have found the
documentation about how drivechains are supposed to work scattered and
difficult to follow. So, without advocating for or against this proposal,
I'd also suggest that adding an opcode is not the best way to implement
this bribe.
The problem I see is that to send a bribe one must first post a transaction
to a script that uses the OP_BRIBE code that fixes the critical hash (and
the sidechain id), and then a second transaction is needed to pay the bribe
to the miner.
I suggest instead to use a 0 output value with some currently non-standard
OP_RETURN output script that specifies the critical hash (and the sidechain
id), similar to ZmnSCPxj's idea.  The difference is that I we would
soft-fork a rule that says that such an output is only legal when a miner
places the same critical hash suitably in their coinbase output.
OP_RETURN outputs are prunable from the UTXO set.  The special bribe output
can be fixed to 0 value because the bribe will be paid using the
transaction's fees.  To perform a bribe, a user creates and signs a
transaction containing one (or more) of these special bribe outputs.  The
fee of this transaction constitutes the bribe, and any change the user has
can be sent back to themselves.  This way only a single transaction is
required to make a bribe.  I didn't really understand the bribe refund
mechanism, but I think the fact that the bribe can be done in a single
transaction this way alleviates any need for bribe refunds.
Hopefully I have understood the goal of this proposal.
On Wed, Jun 28, 2017 at 6:20 PM, Paul Sztorc via bitcoin-dev <

@_date: 2017-06-28 21:09:27
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
HI Chris,
My proposal isn't intended to assume that the bitcoin miner is also
following the sidechain.  In line with my understanding of your proposal,
I'm only proposing to bribe miners to put particular data into the coinbase
output regardless of any semantics that doing so may entail. By my proposed
soft-fork rules, only one of these Bribe TXOs can be included per coinbase
slot (except unless there are identical Bribe TXOs), so there is a
competition of which one of a set of conflicting Bribe TXOs will be
included in the next block.  (That said, the losing set Bribe TXOs can be
included in the later blocks; I don't know what this means semantically for
sidechains; however, the same thing occurs with your proposal as well.)

@_date: 2017-05-09 21:59:06
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Per-block non-interactive Schnorr signature 
I'm a bit amateur at this sort of thing, but let me try to argue that this
proposal is in fact horribly broken ;)
Suppose Alice has some UTXO with some money Bob wants to steal.  Grant me
that the public key P0 protecting Alice's UTXO is public (say because the
public key has been reused elsewhere).
Bob going to spend Alice's UTXO by generating random values s0, k0 and R0
:= k0*G and thus creating a random signature for it, [R0, s0].  Now clearly
this signature isn't going to be valid by itself because it is just random.
Bob's goal will be to make a transaction with other inputs such that, while
the individual signatures are not valid, the aggregated signature will be
To do this Bob generates a set of random public keys P1 ... P_n of the form
P_i := P0 + r_i*G, and a bunch of random k1 ... k_n with R1 := k1*G ... R_n
:= k_n*G, such that
    h(m1, R1, P1) + ... + h(m_n, R_n, P_n) = -h(m0, R0 P0) (modulo the
order of the elliptic curve)
I understand that this can be done efficiently with Wagner's Generalized
Birthday attack.
The RHS aggregated signature equation on the private side is
    k0 + k1 + ... k_n - h(m0, R0, P0)x0 - h(m1, R1, P1)(x0 + r1) - ... -
h(m_n, R_n, P_n)(x0 + r_n)
with x0 unknown to Bob.  Rearranging the terms we get
    k0 + k1 + ... k_n - [h(m0, R0, P0) + h(m1, R1, P1) + ... + h(m_n, R_n,
P_n)]*x0 - [h(m1, R1, P1)*r1 + ... + h(m_n, R_n, P_n)*r_n]
However [h(m0, R0, P0) + h(m1, R1, P1) + ... + h(m_n, R_n, P_n)] is 0 so
cancelling that we are left with
    k0 + k1 + ... k_n - [h(m1, R1, P1)*r1 + ... + h(m_n, R_n, P_n)*r_n]
which no longer depends on the unknown value x0, so that is good.  Bob
knows what this value is.
Bob creates a set UTXOs by spending to the set of public keys P1 .. P_n.
Bob don't know what the private keys are for these public keys, but that is
going to be okay.
Bob creates a final transaction that takes as input the UTXO of Alice's
funds he wants to steal, with public key P0, and also his newly created
UTXOs with public keys P1 ... P_n.
For the signature on Alice's input he uses [R0,s0].  For the rest of the
signature he picks s1 ... s_n such that
    s0 + s1 + ... + sn = k0 + k1 + ... k_n - [h(m1, R1, P1)*r1 + ... +
h(m_n, R_n, P_n)*r_n] (which is equal to k0 + k1 + ... k_n - h(m0, R0,
P0)x0 - h(m1, R1, P1)(x0 + r1) - ... - h(m_n, R_n, P_n)(x0 + r_n)).
and uses signatures [R1, s1] ... [R_n, s_n] on his other inputs.
Thus, while none of the individual signatures are valid, the aggregated
signature does validate.
One wrinkles in this argument is that Bob needs to pick m1 ... m_n before
he knows what the transaction will be.  I think this can be mitigated by
using some combination of SIGHASH_ANYONECANPAY, but I'm not sure if that
works.  Even if my argument doesn't actually work, I think it is close
enough to be pretty scary.
Thanks goes to Pieter Wuille for helping explain things to me; however any
errors above are my own.
On Sun, May 7, 2017 at 2:45 AM, adiabat via bitcoin-dev <

@_date: 2017-05-13 00:23:41
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
I recall chatting about this idea recently and my conclusion was the same
as Peter Todd's conclusion: this will just encourage miners to false signal
readiness with undermines both BIP 9 and BIP 8.
I felt that rather than using script system for this construction, it would
be better to use the transaction version number instead by soft-forking in
a rule that says when the most significant bits of a transaction version
are 001 then the transaction can only be included in blocks whose lower 29
version bits are set at the same position as the lower 29 version bits set
in the transaction version.
That is to say, if we have block version blkVersion and transaction version
txVersion, we soft fork in a rule that requires that
(txVersion & 0xe0000000 != 0x020000000) || ((blkVersion & 0xe0000000 =
0x020000000) && (blkVersion & txVersion = txVersion))
While I think that making use of the transaction version number is superior
to adding an opcode, because it doesn't interfere with caching of script
validity and because it doesn't use any more transaction space by making
use of the otherwise useless transaction version number, I still think it
is a bad proposal.
On Fri, May 12, 2017 at 3:22 PM, Luke Dashjr via bitcoin-dev <

@_date: 2017-05-13 13:11:27
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
To add a timeout a user can optionally bundle a pair of similar
transactions.  One with the transaction version bits set and a second with
a locktime set.  The effect is the same.
Also, doing it the way you describe would fail to enforce that BIP9 is
My formal condition does include a check for the block version (I've
corrected the constants below):
(txVersion & 0xe0000000 != 0x200000000) || (*(blkVersion & 0xe0000000 =
0x200000000)* && (blkVersion & txVersion = txVersion))
Nothing here prevents upgrading versionbits AFAICT.  Any txVersion that
does not begin with 0b001 is unconditionally acceptable and available for
further soft-forking.

@_date: 2017-05-22 03:05:49
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Introduction
This document aims to specify and justify a method for computing Merkle
roots for tree structures whose nodes are annotated with other data.  While
this proposal could be used to replace Bitcoin's Merkle root calculation,
it is primarily aimed at new applications such as MAST, (U)TXO commitments
or other Merklized structures.
 Background
Bitcoin uses a Merkle tree construction to build a commitment to a sequence
of transactions for a Bitcoin block.  The main operation for computing a
Merkle tree is one that takes the recursively computed Merkle roots of two
branches and combines them to compute the Merkle root of the tree with
those two branches.  In Bitcoin, a Merkle root is 256 bits and the
construction is
    MerkleRoot := SHA256(SHA256(LeftRoot ? RightRoot))
One problem with this construction is that it is unnecessarily expensive.
While the concatenation of the LeftRoot and the RightRoot fits in 512 bits,
the size of a SHA256 chunk, a second chunk is needed to fit SHA256's
padding.  This means that inner SHA256 call invokes the SHA256 compression
function twice, once for each chunk.  The outer SHA256 call invokes the
SHA256 compression function a third time.  Bitcoin's Merkle root procedure
calls the SHA256 compression function a total of three times per node.
The main purpose of composing two calls to the SHA256 hash function is to
protect against length extension attacks.  A length extension attack is
where an attacker has access to the hash of a message `HASH(msg)` and,
without knowing `msg`, is able to construct the hash of a new message
`HASH(msg ? attackerMsg)`.  This is attack is usually used against poorly
constructed MACs (Message Authentication Codes).  In the case of Bitcoin
there are no secret messages that are hashed, so the outer call to SHA256
is unnecessary.
As mentioned above, the inner call to SHA256 requires two chunks because of
SHA256's padding.  For the MerkleRoot construction added padding value is
because the input is of a fixed length.  SHA256's padding function is
1. to add bits to the message so that the resulting message size is a
multiple of the chunk size (which is 512-bits in the case of SHA256).
2. to satisfy the Merkle--Damg?rd property which says that if we can find a
hash collision in SHA256, which operates on variable length messages, then
we can find a hash collision in SHA256's compression function, which
operates on fixed length messages.
We could remove the second call to the SHA256 compression function and use
SHA256 without padding.  If we did, we would lose the Merkle--Damg?rd
property.  While this may be acceptable for some use cases, we are still
left with no room to add annotation data held at a node.  For Bitcoin's
Merkle tree this is not a problem, because its trees are not annotated.
However, for other applications, we would need to add another chunk to hold
the annotation, which would mean adding back the second call to the SHA256
compression function per node of the tree.
 A New Merkle Root Algorithm
Below is an algorithm for computing Merkle roots that directly uses the
SHA256 compression function.  This algorithm operates on binary trees and
supports per node annotations.  Furthermore this proposal satisfies the
Merkle--Damg?rd property and more.
 Remark 1
Any finitely branching tree can be represented by a binary tree, so this
construction also applies to arbitrary finitely branching trees.
The SHA256 compression function takes two inputs:
1. A 256-bit value for the previous chunk in a chain, or an initial vector
in the case of the first chunk.
2. A 512-bit chunk of data.
    sha256Compress : Word256 ? Word512 -> Word256
In total, the SHA256 compression function compresses 768-bits into
256-bits.  The Merkle roots of two branches occupy 512 bits, and this
leaves another 256-bits of space available for tags.
Let us define a recursive type for binary trees annotated with tags.
    type Tree tag where
      Leaf   : tag                       -> Tree tag
      Unary  : tag ? Tree tag            -> Tree tag
      Binary : tag ? Tree tag ? Tree tag -> Tree tag
We define a recursive algorithm for trees whose tags are 256-bit Words (or
whose tags can be represented by 256-bit words).
    merkleRoot : Tree Word256 -> Word256
    merkleRoot (Leaf (t))                :=
0b0^256 ? t)
    merkleRoot (Unary (t, child))        := sha256Compress(merkleRoot(child),
0b0^256 ? t)
    merkleRoot (Binary (t, left, right)) := sha256Compress(merkleRoot(left),
merkleRoot(right) ? t)
We need some initial value for the `Leaf` case.  This could be taken as the
initial vector for SHA256.  However, I suggest using the hash of an
application specific name.
    ApplicationName : BitString
We further require that the tags dictate the kind of node it is attached
to.  For example, if a given tag is used for Binary nodes, then it can only
appear in other Binary nodes.  One way this can be accomplished is by
requiring the first two bits of a tag follow a particular scheme (the
scheme below ensures that one of the first two bits is set, which will be
useful later when we define safe tags) such as
- 0b11 when the node is a Leaf node.
- 0b10 when the node is a Unary node.
- 0b01 when the node is a Binary node.
This condition suffices to provide the Merkle--Damg?rd property:
 Theorem 2
Suppose `t_1` and `t_2` are two different `Tree Word256` values such that
any tag occurring in the two trees only occurs in one kind of node.  If
`merkleRoot(t_1) = merkleRoot(t_2)` then we can effectively find a
collision in the SHA256 compression function.
We proceed by structural induction on `t_1`.  Assume `t_1` and `t_2` are
two different `Tree Word256` values such that `merkleRoot(t_1) =
merkleRoot(t_2)`.  Define a `tag` function to extract the tag name from the
root of a tree.
    tag : Tree Word256 -> Word256
    tag (Leaf (t))         := t
    tag (Unary (t, _))     := t
    tag (Binary (t, _, _)) := t
Case 1: Suppose `tag(t_1) =/= tag(t_2)`.
This means that the inputs to `sha256Compress` that produced
`merkleRoot(t_1)` and `merkleRoot(t_2)` are different and we have found a
collision in the SHA256 compression function.
Case 2: Suppose `tag(t_1) = tag(t_2)`.
Let `tg` be this tag.  By our requirement on tags, this means that `t_1`
and `t_2` are the same kind of node.  Suppose `t_1 = Binary (tg, t_(1l),
t_(1r))` and `t_2 = Binary (tg, t_(2l), t_(2r))`.  Since `t_1` and `t_2`
are different, then either `t_(1l) =/= t_(2l)` or `t_(1r) =/= t_(2r)`.
Without loss of generality, assume `t_(1l) =/= t_(2l)`.
Case 2a: Suppose `merkleRoot(t_(1l)) =/= merkleRoot(t_(2l))`.
This means that the inputs to `sha256Compress` that produced
`merkleRoot(t_1)` and `merkleRoot(t_2)` are different and we have found a
collision in the SHA256 compression function.
Case 2b: Suppose `merkleRoot(t_(1l)) = merkleRoot(t_(2l))`.
Then by induction we can find a collision in the SHA256 compression
The cases where `t_1` and `t_2` are both `Unary` or `Leaf` nodes are
handled in a similar way.  The reader can verify the algorithm implied by
the above proof provides an effective method of finding a collision in the
SHA256 compression function.  QED
 Remark 3
We have filled half of the chunk with `0b0^256` in for the `Unary` and
`Leaf `cases in the definition of `merkleRoot`.  The proof of Theorem 2
does not depend on this, and if we have extra ancillary data for these
types of nodes it can replace the `0b0^256` in this first half of the
chunk.  This is particularly useful for the `Leaf `case because `Leaf`
nodes often hold extra data.  We also remark that if this data is too large
to be represented by a `Word256`, it can instead be hashed with SHA256 and
the hash included instead.
Not all of the inputs to the SHA256 compression function are created
equal.  Only the second argument, the chunk data, is applied to the SHA256
expander.  `merkleRoot` is designed to ensure that the first argument of
the SHA256 compression function is only fed some output of the SHA256
compression function.  In fact, we can prove that the output of the
`merkleRoot` function is always the midstate of some SHA256 hash.  To see
this, let us explicitly separate the `sha256` function into the padding
step, `sha256Pad`, and the recursive hashing step, `unpaddedSha256`.
    sha256Pad : BitString -> List Word512
    sha256IV : Word256
    sha256Loop : Word256 ? List Word512 -> Word256
    sha256Loop (prev, Nil)                := prev
    sha256Loop (prev, Cons (chunk, rest)) := sha256Loop(sha256Compress(prev,
chunk), rest)
    unpaddedSha256 : List Word512 -> Word256
    unpaddedSha256 (chunks) := sha256Loop(sha256IV, chunks)
    sha256 : BitString -> Word256
    sha256 (s) := unpaddedSha256(sha256Pad(s))
Now we can state what we mean when we say that the output of the
`merkleRoot` function is always the midstate of some SHA256 hash.
 Theorem 4
For all `t : Tree Word256` there exists some `l : List Word512` such that
`merkleRoot(t) = unpaddedSha256(l)`.
We construct a function to transform a tree into the required list of
    merkleChain : Tree Word256 -> List Word512
    merkleChain (Leaf (t))                := sha256Pad(ApplicationName) +
[0b0^256 ? t]
    merkleChain (Unary (t, child))        := merkleChain(child) + [0b0^256
? t]
    merkleChain (Binary (t, left, right)) := merkleChain(left) +
[merkleRoot(right) ? t]
The reader can verify by induction that
    forall t : Tree Word256. merkleRoot(t) = unpaddedSha256(merkleChain(t)).
 Large Tag Space
If one's application cannot directly represent the space of tags with a
`Word256`, then one can hash the tags and still maintain the
Merkle--Damg?rd property given by Theorem 2, as long as we still have the
property that each tag uniquely determines the kind of node it applies to.
We first note that `Tree` is a functor.
    treeMap : (a -> b) -> Tree a -> Tree b
    treeMap f (Leaf (t))                := Leaf (f(t))
    treeMap f (Unary (t, child))        := Unary (f(t), child)
    treeMap f (Binary (t, left, right)) := Binary (f(t), left, right)
We can hash the tags of a tree.
    hash256Tags : Tree BitString -> Tree Word256
    hash256Tags (t) := treeMap sha256 t
Now we can compute the `merkleRoot` of the result of `hash256Tags`.
 Theorem 5
Suppose `t_1` and `t_2` are two different `Tree BitString` values such that
any tag occurring in the two trees only occurs in one kind of node.  If
`merkleRoot(hash256Tags(t_1)) = merkleRoot(hash256Tags(t_2))`, then we can
effectively find a collision in the SHA256 compression function.
Assume `t_1` and `t_2` are two different `Tree BitString` values such that
`merkleRoot(hash256Tags(t_1)) = merkleRoot(hash256Tags(t_2))`.
Case 1: Suppose `hash256Tags(t_1) = hash256Tags(t_2)`.
This means there must be a collision in the `sha256` function.  By the
Merkle--Damg?rd property of SHA256, this means we can effectively find a
collision in the SHA256 compression function.
Case 2: Suppose `hash256Tags(t_1) =/= hash256Tags(t_2)`.
If the SHA256 hashes of each tag in the two trees uniquely determines the
kinds of their nodes, then we can apply Theorem 2 to conclude that we can
effectively find a collision in the SHA256 compression function.  If the
SHA256 hashes of each tag in the two trees does not uniquely determine the
kinds of their nodes then there are two different tags among the two trees,
`tg_1` and `tg_2`, such that `sha256(tg_1) = sha256(tg_2)`.  Hence there is
a collision in the `sha256` function, and therefore we can effectively find
a collision in the SHA256 compression function.  QED
 Avoiding collisions between merkleRoot and sha256
For the moment, let us assume there is no effective way to find a collision
in the SHA256 compression function.  By the Merkle--Damg?rd property of
SHA256, we cannot effectively find a collision within the sha256 function.
We have shown, by Theorem 2, that merkleRoot has the same Merkle--Damg?rd
property and hence we cannot effectively find a collision within the
merkleRoot function.  However, we may have collisions between the `sha256`
and the `merkleRoot` functions.  That is, we may be able to effectively
find values `s : BitString` and `t : Tree Word256` such that `sha256(s) =
While this may not be a problem for most applications, it turns out that we
can place restrictions on the format of tags to ensure that there are never
collisions between `sha256` and `merkleRoot`.  Define a safe tag of type
`Word256` to be a a value such that one the first 192 bits is set and the
9th last bit is cleared.
 Theorem 6
Let `t : Tree Word256` be a tree in which every tag is a safe tag.  Suppose
we have some `s : BitString` such that `sha256(s) = merkleRoot(t)`.  Then
we can effectively find a collision in the SHA256 compression function.
The last chunk of `sha256Padding(s)` is a padding chunk as defined by the
SHA256 standard.  If we look at the second half of this last chunk, then
according to the padding rules, it can never be the case that one of the
first 192 bits is set and the 9^th last bit is cleared.  Because `t` only
contains safe tags, the inputs to their last compression function in the
computation of `sha256(s)` and `merkleRoot(t)` must be different.
Therefore if `sha256(s) = merkleRoot(t)` then we must have encountered a
collision in the final compression function of the two computations.  QED
 Remark 7
If we use safe tags we can consider a modified `merkleRoot'` function.
    merkleRoot' : Tree Word256 -> Word256
    merkleRoot' (Leaf (t))                :=
sha256("") ? t)
    merkleRoot' (Unary (t, child))        := sha256Compress(merkleRoot'(child),
sha256("") ? t)
    merkleRoot' (Binary (t, left, right)) := sha256Compress(merkleRoot'(left),
merkleRoot'(right) ? t)
If we use `merkleRoot'` we no longer require that tags uniquely define
their node types in order to ensure the Merkle--Damg?rd property.  Instead
we can rely on the safe tags to ensure that the use of `sha256(s)` will
never collide with `merkleRoot'(t)`, and therefore nodes of different types
will never collide with each other the `merkleRoot'` computation.  However,
I don't feel this is a particularly good approach, so I will not elaborate
on it further.
Unfortunately, there is no guarantee that the result of `hash256Tags` will
consist of only safe tags, so we cannot use it to avoid collisions between
`sha256` and `merkleRoot ? hash256Tags`.  To remedy this we define an
alternative to `hash256Tags`.
    hash224Tag : BitString -> Word256
    hash224Tag (s) := 0xFFFF ? sha224(s) ? 0x0000
    hash224Tags : Tree BitString -> Tree Word256
    hash224Tags (t) := treeMap hash224Tag t
We see that by appropriately padding the result of `sha224` we guarantee
that `hash224Tag(s)` is a safe tag.
 Theorem 8
Suppose `t_1` and `t_2` are two different `Tree BitString` values such that
any tag occurring in the two trees only occurs in one kind of node.  If
`merkleRoot(hash224Tags(t_1)) = merkleRoot(hash224Tags(t_2))`, then we can
effectively find either a collision in the SHA256 compression function, or
a collision in SHA224.
 Remark 9
We can safely replace the `0xFFFF` prefix in `hash224Tag` with one where
the first two bits determine the kind of node in accordance with our
previously defined scheme.
 Remark 10
Rather than using SHA224 we could use SHA256 and tweak it afterwards to set
the first bit and clear the 9th last bit. This comes at the cost of
effectively using a non-standard hash function.
 Remark 11
Most of this development not depend specifically on SHA256.  It all works
similarly for SHA512 and other hash functions that compress in a similar
manner.  SHA256 is used as the primary example because one can find
hardware support for it on commodity hardware (for example, see the Intel
SHA extensions).
 Conclusion
We have defined a merkleRoot function for computing Merkle roots of trees
that include annotations per node.  The resulting function uses one SHA256
compression function call per node and supports arbitrary 256-bit
annotations per node.  Arbitrary annotations can be supported at the cost
of more hashing.  We have also shown that by using safe tags we can
additionally get a property that the `merkleRoot` will not effectively
collide with `sha256` function.
 Further Reading
The article [Characterizing Padding Rules of MD Hash Functions Preserving
Collision Security]( by Mridul Nandi
provides a nice overview of various options for padding rules.

@_date: 2017-05-22 18:32:38
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
On Mon, May 22, 2017 at 03:05:49AM -0400, Russell O'Connor via bitcoin-dev
To be clear, what math operations do you mean by "?" and "?"?
By "?", I usually mean concatenation (though I also use it for function
composition in one instance).   By "?", I mean the Cartesian product.

@_date: 2017-05-27 18:07:07
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Cartesian product can mean a lot of things.
What specifically do you mean by "cartesian product" here?
Oops, I forgot to reply all.  Below is my reply.
Given two types A and B, then A ? B is the type of pairs of A and B in the
sense of type theory as used in Standard ML or Haskell or other typed
To follow up, by "sha256Compress : Word256 ? Word512 -> Word256" I mean
that sha256Compress is a function that takes two arguments, the first being
a 256 bit word and the second being a 512 bit word, and returns a 256 bit
word (or equivalently sha256Compress is a function that takes a pair as
input, the first component being a 256 bit word and the second component
being a 512 bit word, and returns a 256 bit word).
sha256Compress is meant to be the compression function defined by the
SHA256 standard, though nothing here depends on anything more that its type

@_date: 2017-05-29 10:55:37
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A Method for Computing Merkle Roots of Annotated 
Thanks for the review Peter.  This does seem like a serious issue that I
hadn't considered yet.  As far as I understand, we have no reason to think
that the SHA-256 compression function will be secure with chosen initial
Some of this proposal can be salvaged, I think, by putting the hash of the
tags into Sha256Compress's first argument:
    merkleRoot : Tree BitString -> Word256
    merkleRoot (Leaf (t))                := sha256Compress(sha256(t),
    merkleRoot (Unary (t, child))        := sha256Compress(sha256(t),
merkleRoot(child) ? 0b0^256)
    merkleRoot (Binary (t, left, right)) := sha256Compress(sha256(t),
merkleRoot(left) ? merkleRoot(right))
The Merkle--Damg?rd property will still hold under the same conditions
about tags determining the type of node (Leaf, Unary, or Binary) while
avoiding the need for SHA256's padding.  If you have a small number of
tags, then you can pre-compute their hashes so that you only end up with
one call to SHA256 compress per node. If you have tags with a large amount
of data, you were going to be hashing them anyways.
Unfortunately we lose the ability to cleverly avoid collisions between the
Sha256 and MerkleRoot functions by using safe tags.

@_date: 2017-11-02 21:10:42
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Simplicity proposal - Jets? 
Hi Jose,
Jets are briefly discussed in section 3.4 of
The idea is that we can recognize some set of popular Simplicity
expressions, and when the Simplicity interpreter encounters one of these
expressions it can skip over the Simplicity interpreter and instead
directly evaluate the function using specialized C or assembly code.
For example, when the Simplicity interpreter encounters the Simplicity
expression for ECDSA verification, it might directly call into libsecp
rather than continuing the ECDSA verification using interpreted Simplicity.
On Nov 2, 2017 18:35, "JOSE FEMENIAS CA?UELO via bitcoin-dev" <
I am trying to follow this Simplicity proposal and I am seeing all over
references to ?jets?, but I haven?t been able to find any good reference to
Can anyone give me a brief explanation and or a link pointing to this
On 31 Oct 2017, at 22:01, bitcoin-dev-request at lists.linuxfoundation.org
The plan is that discounted jets will be explicitly labeled as jets in the
commitment.  If you can provide a Merkle path from the root to a node that
is an explicit jet, but that jet isn't among the finite number of known
discounted jets,
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-10-01 15:05:38
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
Given the proposed fixed signature size, It seems better to me that we
create a SIGHASH_WITNESS_WEIGHT flag as opposed to SIGHASH_WITNESS_DEPTH.
Mark, you seem to be arguing that in general we still want weight
malleability even with witness depth fixed, but I don't understand in what
scenario we would want that.
It strikes me that is most scenarios all parties signing an input would do
so after an execution path through the script has been agreed upon by all
parties, in which case the witness weight can be fixed.
In rare cases where the smart contract requires that some parties sign in
advance of the decision about the execution path (for example, I'm thinking
about delegation here, but I want to keep my remarks general), we wouldn't
want to fix the witness depth either.
A SIGHASH_WITNESS_WEIGHT would prevent all possible malleability that would
modify the transaction's fee/weight priority (at least for that one input),
and greatly reduce the overall attack surface of witness malleability
On Sun, Oct 1, 2017 at 1:04 AM, Mark Friedenbach via bitcoin-dev <

@_date: 2017-10-01 15:41:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
On Sun, Oct 1, 2017 at 3:27 PM, Mark Friedenbach Creating a Bitcoin script that does not allow malleability is difficult and
requires wasting a lot of bytes to do so, typically when handling issues
around non-0-or-1 witness values being used with OP_IF, and dealing with
non-standard-zero values, etc.  Adding a witness weight flag cuts through
the worst of all this, and makes script design enormously simpler and makes
scripts smaller and cheaper.
I'll argue that I don't want my counter-party going off and using a very
deeply nested key in order to subvert the fee rate we've agreed upon after
I've signed my part of the input.  If we are doing multi-party signing of
inputs we need to communicate anyways to construct the transaction.  I see
no problem with requiring my counter-party to choose their keys before I
sign so that I know up front what our fee rate is going to be.  If they
lose their keys and need a backup, they should have to come back to me to
resign in order that we can negotiate a new fee rate for the transaction
and who is going to be covering how much of the fee and on which inputs.

@_date: 2017-10-02 13:15:38
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
(Subject was: [bitcoin-dev] Version 1 witness programs (first draft)), but
I'm moving part of that conversation to this thread.
I would very much like to retain the ability to do static analysis.  More
generally, the idea of interpreting arbitrary data as code, as done in
OP_EVAL and in TAILCALL, makes me quite anxious.  This at the root of many
security problems throughout the software industry, and I don't relish
giving more fuel to the underhanded Bitcoin Script contestants.
Actually, I have a half-baked idea I've been thinking about along these
The idea is to add a flag to each stack item in the Script interpreter to
mark whether the item in the stack is "executable" or "non-executable", not
so different from how computers mark pages to implement executable space
protection.  By default, all stack items are marked "non-executable".  We
then redefine OP_PUSHDATA4 as OP_PUSHCODE within ScriptSigs.  The
operational semantics of OP_PUSHCODE would remain the same as OP_PUSHDATA4
except it would set the pushed item's associated flag to "executable".  All
data pushed by OP_PUSHCODE would be subject to the sigops limits and any
other similar static analysis limits.
Segwit v0 doesn't use OP_PUSHDATA codes to create the input stack, so we
would have to mark executable input stack items using a new witness v1
format. But, IIUC, TAILCALL isn't going to be compatible with Segwit v0
During a TAILCALL, it is required that the top item on the stack have the
"executable" flag, otherwise TAILCALL is not used (and the script succeeds
or fails based on the top item's data value as usual).
All other operations can treat "executable" items as data, including the
merkle branch verification.  None of the Script operations can create
"executable" items; in particular, OP_PUSHDATA4 within the ScriptPubKey
also would not create "executable" items.  (We can talk about the behaviour
of OP_CAT when that time comes).
One last trick is that when "executable" values are duplicated, by OP_DUP,
OP_IFDUP, OP_PICK. then the newly created copy of the value on top of the
stack is marked "non-executable".
Because we make the "executable" flag non-copyable, we are now free to
allow unbounded uses of TAILCALL (i.e. TAILCALL can be used multiplie times
in a single input).  Why is this safe?  Because the number of "executable"
items decreases by at least one every time TAILCALL is invoked. the number
of OP_PUSHCODE occurrences in the witness puts an upper bound on the number
of invocations of TAILCALL allowed.  Using static analysis of the script
pubkey and the data within the OP_PUSHCODE data, we compute an upper bound
on the number of operations (of any type) that can occur during execution.
Unbounded TAILCALL should let us (in the presence of OP_CHECKSIGFROMSTACK)
have unbounded delegation.
Overall, I believe that OP_PUSHCODE
1. is fully backwards compatible.
2. maintains our ability to perform static analysis with TAILCALL.
3. never lets us interpret computed values as executable code.
4. extends TAILCALL to safely allow multiple TAILCALLs per script.

@_date: 2017-10-02 16:38:49
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
On Sun, Oct 1, 2017 at 4:39 PM, Mark Friedenbach Script validation isn't the correct place to do this.  The reason is that
script operations are not aware of whether the stack items they are
processing are witness malleable items or Script computed values.  Let me
take OP_IF as one example.  When OP_IF operates directly on witness data,
it is subject to witness malleability, and therefore one needs to add extra
code around that to prevent witness malleability.  On the other hand, when
OP_IF operates on computed data, it isn't subject to malleability and can
safely process non-zero-or-one values. If OP_IF were restricted to
requiring canonical inputs, then for the cases that OP_IF operates on
computed data, they will need to add extra code to canonicalize their
inputs.  I don't think there is a correct answer here.  That is because I
believe this isn't the correct place to aim to restrict witness
OTOH, signatures are a fine place to aim to restrict witness malleability.
In fact, if signatures could securely cover all witness data, I think
everyone here would jump at the opportunity to implement that.  However,
since that isn't known to be possible, we are left with doing the best we
can, which is to have signatures cover weight (or bytes).  This prevents
the worst effects of witness malleability and does so without burdening
Script development.  (This also requires signatures have a fixed size, so
it is understandable that signature-covers-weight wasn't included in Segwit
v0 scripts).
I would be fine your suggestion above, though I think Luke's suggestion of
having both SIGHASH_WITNESS_SIZE and SIGHASH_WITNESS_DEPTH flag is better
because it is simpler.
Those people worried about restarting interactive signing session in the
unlikely event of parties not knowing what keys they are planning to use
can use just the SIGHASH_WITNESS_DEPTH flag.  Those people worried about
counterparties fiddling with fee rates can use both flags.  The choice
doesn't even need to be made at script commitment time.

@_date: 2017-10-05 17:28:48
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
For the record, it's Johnson Lau's proposal where I read this idea.

@_date: 2017-10-30 11:22:20
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
I've been working on the design and implementation of an alternative to
Bitcoin Script, which I call Simplicity.  Today, I am presenting my design
at the PLAS 2017 Workshop  on Programming
Languages and Analysis for Security.  You find a copy of my Simplicity
paper at Simplicity is a low-level, typed, functional, native MAST language where
programs are built from basic combinators.  Like Bitcoin Script, Simplicity
is designed to operate at the consensus layer.  While one can write
Simplicity by hand, it is expected to be the target of one, or multiple,
front-end languages.
Simplicity comes with formal denotational semantics (i.e. semantics of what
programs compute) and formal operational semantics (i.e. semantics of how
programs compute). These are both formalized in the Coq proof assistant and
proven equivalent.
Formal denotational semantics are of limited value unless one can use them
in practice to reason about programs. I've used Simplicity's formal
semantics to prove correct an implementation of the SHA-256 compression
function written in Simplicity.  I have also implemented a variant of ECDSA
signature verification in Simplicity, and plan to formally validate its
correctness along with the associated elliptic curve operations.
Simplicity comes with easy to compute static analyses that can compute
bounds on the space and time resources needed for evaluation.  This is
important for both node operators, so that the costs are knows before
evaluation, and for designing Simplicity programs, so that smart-contract
participants can know the costs of their contract before committing to it.
As a native MAST language, unused branches of Simplicity programs are
pruned at redemption time.  This enhances privacy, reduces the block weight
used, and can reduce space and time resource costs needed for evaluation.
To make Simplicity practical, jets replace common Simplicity expressions
(identified by their MAST root) and directly implement them with C code.  I
anticipate developing a broad set of useful jets covering arithmetic
operations, elliptic curve operations, and cryptographic operations
including hashing and digital signature validation.
The paper I am presenting at PLAS describes only the foundation of the
Simplicity language.  The final design includes extensions not covered in
the paper, including
- full convent support, allowing access to all transaction data.
- support for signature aggregation.
- support for delegation.
Simplicity is still in a research and development phase.  I'm working to
produce a bare-bones SDK that will include
- the formal semantics and correctness proofs in Coq
- a Haskell implementation for constructing Simplicity programs
- and a C interpreter for Simplicity.
After an SDK is complete the next step will be making Simplicity available
in the Elements project  so that anyone can
start experimenting with Simplicity in sidechains. Only after extensive
vetting would it be suitable to consider Simplicity for inclusion in
Simplicity has a long ways to go still, and this work is not intended to
delay consideration of the various Merkelized Script proposals that are
currently ongoing.

@_date: 2017-10-31 16:38:16
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
(sorry, I forgot to reply-all earlier)
The very short answer to this question is that I plan on using Luke's
fail-success-on-unknown-operation in Simplicity.  This is something that
isn't detailed at all in the paper.
The plan is that discounted jets will be explicitly labeled as jets in the
commitment.  If you can provide a Merkle path from the root to a node that
is an explicit jet, but that jet isn't among the finite number of known
discounted jets, then the script is automatically successful (making it
anyone-can-spend).  When new jets are wanted they can be soft-forked into
the protocol (for example if we get a suitable quantum-resistant digital
signature scheme) and the list of known discounted jets grows.  Old nodes
get a merkle path to the new jet, which they view as an unknown jet, and
allow the transaction as a anyone-can-spend transaction.  New nodes see a
regular Simplicity redemption.  (I haven't worked out the details of how
the P2P protocol will negotiate with old nodes, but I don't forsee any
Note that this implies that you should never participate in any Simplicity
contract where you don't get access to the entire source code of all
branches to check that it doesn't have an unknown jet.
On Mon, Oct 30, 2017 at 5:42 PM, Matt Corallo

@_date: 2017-10-31 17:01:05
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
============================== START ==============================
That approach is worth considering.  However there is a wrinkle that
Simplicity's denotational semantics doesn't imply an order of operations.
For example, if one half of a pair contains a assertion failure
(fail-closed), and the other half contains a unknown jet (fail-open), then
does the program succeed or fail?
This could be solved by providing an order of operations; however I fear
that will complicate formal reasoning about Simplicity expressions.  Formal
reasoning is hard enough as is and I hesitate to complicate the semantics
in ways that make formal reasoning harder still.
Nit, but if you go down that specific path I would suggest making just
the jet itself fail-open. That way you are not so limited in requiring
validation of the full contract -- one party can verify simply that
whatever condition they care about holds on reaching that part of the
contract. E.g. maybe their signature is needed at the top level, and
then they don't care what further restrictions are placed.
On Tue, Oct 31, 2017 at 1:38 PM, Russell O'Connor via bitcoin-dev
and a
which I
code.  I

@_date: 2017-09-06 21:59:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Fast Merkle Trees 
The fast hash for internal nodes needs to use an IV that is not the
standard SHA-256 IV. Instead needs to use some other fixed value, which
should itself be the SHA-256 hash of some fixed string (e.g. the string
"BIP ???" or "Fash SHA-256").
As it stands, I believe someone can claim a leaf node as an internal node
by creating a proof that provides a phony right-hand branch claiming to
have hash 0x80000..0000100 (which is really the padding value for the
second half of a double SHA-256 hash).
(I was schooled by Peter Todd by a similar issue in the past.)
On Wed, Sep 6, 2017 at 8:38 PM, Mark Friedenbach via bitcoin-dev <

@_date: 2017-09-07 11:43:52
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Fast Merkle Trees 
In that case, you may as well remove all references to leaves and double
SHA-256 from your BIP since your design has no method for distinguishing
between internal nodes and leaves.
I think that if this design stands, it will play a role in some future
CVEs.  The BIP itself is too abstract about its data contents to
specifically say that it has a vulnerability; however, I believe it is
inviting vulnerabilities.
For example, I might agree with a counterparty to a design of some sort of
smart contract in the form of a MAST.  My counterparty has shown me all the
"leaves" of our MAST and I can verify its Merkle root computation.
After being deployed, I found out that one of the leaves wasn't really a
leaf but is instead a specially crafted "script" with a fake pubkey chosen
by my couterparty so that this leaf can also be interpreted as a fake
internal node (i.e. an internal node with a right branch of 0x8000...100).
Because the Fast Merkle Tree design doesn't distinguish between leaves and
internal nodes my counter party gets away with building an Inclusion Proof
through this "leaf" to reveal the evil code that they had designed into the
MAST at a deeper level.
Turns out my counterparty was grinding their evil code to produce an
internal node that can also be parsed as an innocent script.  They used
their "pubkey" to absorb excess random data from their grinding that they
cannot eliminate.
(The counterparty doesn't actually know the discrete log of this "pubkey",
they just claimed it was their pubkey and I believed them).
Having ambiguity about whether a node is a leaf or an internal node is a
security risk. Furthermore, changing the design so that internal node and
leaves are distinguishable still allows chained invocations.
Arbitrary data can be stored in Fast Merkle Tree leaves, including the
Merkle root of another Fast Merkle Tree.
Applications that are limited to proof with paths no longer than 32
branches can still circumvent this limit by staging these Fast Merkle Trees
in explicit layers (as opposed to the implicit layers with the current
By storing a inner Fast Merkle Tree root inside the (explicit) leaf of an
outer Fast Merkle Tree, the application can verify a Inclusion Proof of the
inner Fast Merkle Tree Root in the outer Fast Merkle Tree Root, and then
verify a second Inclusion Proof of the desired data in the inner Faster
Merkle Tree Root.  The application will need to tag their data to
distinguish between inner Fast Merkle Tree Roots and other application
data, but that is just part of the general expectation that applications
not store ambiguous data inside the leaves of Fast Merkle Trees.
On Wed, Sep 6, 2017 at 10:20 PM, Mark Friedenbach

@_date: 2017-09-07 11:51:14
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Fast Merkle Trees 
I 100% agree.
With SHA256 every final state is also a valid midstate.  Therefore, using a
custom IV of the SHA256 hash of some fixed string results in a hash of data
that is functionally equivalent to prefixing the data with the padded
version of the fixed string and using a regular SHA256 hash of the combined
data.  This is important and I should have explicitly pointed it out.

@_date: 2017-09-07 14:55:25
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Fast Merkle Trees 
or  fast-SHA256(fast-SHA256(double-SHA256(malign) || r1) || r0)
or  fast-SHA256(fast-SHA256(r1 || double-SHA256(malign)) || r0)
or ...
The particular scenario I'm imagining is a collision between
    double-SHA256(innocuous)
    fast-SHA256(fast-SHA256(fast-SHA256(double-SHA256(malign) || r2) || r1)
where innocuous is a Bitcoin Script that is between 32 and 55 bytes long.
Observe that when data is less than 55 bytes then double-SHA256(data) =
fast-SHA256(fast-SHA256(padding-SHA256(data)) || 0x8000...100) (which is
really the crux of the matter).
Therefore, to get our collision it suffices to find a collision between
    padding-SHA256(innocuous)
    fast-SHA256(double-SHA256(malign) || r2) || r1
r1 can freely be set to the second half of padding-SHA256(innocuous), so it
suffices to find a collision between
   fast-SHA256(double-SHA256(malign) || r2)
and the first half of padding-SHA256(innocuous) which is equal to the first
32 bytes of innocuous.
Imagine the first opcode of innocuous is the push of a value that the
attacker claims to be his 33-byte public key.
So long as the attacker doesn't need to prove that they know the discrete
log of this pubkey, they can grind r2 until the result of
fast-SHA256(double-SHA256(malign) || r2) contains the correct first couple
of bytes for the script header and the opcode for a 33-byte push.  I
believe that is only about 3 or 4 bytes of they need to grind out.

@_date: 2017-09-13 09:24:14
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] SigOps limit. 
If there were no sigops limits, I believe the worst case block could have
closer to 1,000,000 CHECKSIG operations.  Signature checks are cached so
while repeating the sequence "2DUP CHECKSIGVERIFY" does create a lot of
checksig operations, the cached values prevent a lot of work being done.
To defeat the cache one can repeat the sequence "2DUP CHECKSIG DROP
CODESEPARATOR", which will create unique signature validation requests
every 4 bytes.

@_date: 2018-08-04 08:22:28
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
I propose changing the verification equation from "Let *R = sG - eP*" to
    Let *R = sG + eP*
This allows faster verification by avoiding negating a point (or a
If, instead of directly following the literal verification specification,
one is instead reconstructing R from r by finding a y coordinate that is a
quadratic residue, under the existing scheme one would verify
*sG - eP = R*
which is effectively verifying
  0 = *sG - eP* - R  or 0 = R - *sG + eP*
Either way one needs to negate at least one point (or one coefficient)
because of the opposite signs between sG and eP.
Under my proposed revised verification scheme, one would instead verify
  0 = sG + eP + (-R).
While it seems that this requires negating R, it does not.  Rather (-R) can
be directly constructed from r by finding a y coordinate that is *not* a
quadratic residue, which is precisely the same amount of work that
construction R from r was.
In either verification procedure, changing the verification equation to my
proposal removes one negation operation from the cost of doing verification.

@_date: 2018-08-05 10:33:52
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Over chat it has been pointed out to me that computing the non-quadratic
residue is not the same cost as computing a quadratic residue.  As pointed
out in footnote 7 of the the proposed BIP, c^((p+1)/4) is always a
quadratic residue and must be negated to find the non-quadratic residue.
In light of this, I revise my proposed change to make the verification
R + sG + eP = 0.
(by 0 in the equation above I mean the identity element for the (+)
operation, which is the point at infinity.)
This equation is suitable for batch verification.  This equation is clearly
written as a linear combination that doesn't use negation.  In most
implementations, equality comparison tests are implemented by subtraction
and a comparison with zero. By writing the verification equation this way,
we clearly see that only the comparison with zero test is needed.
For single signature verification the check becomes, compute Q := sG + eP.
Verify that Q isn't the point at infinity and Q.x = r.  Verify that Q.y is
*not* a quadratic residue. (While I was incorrect earlier about the costs
of computing a non-residue, it is the case the *verifying* a value is a
quadratic residue is the same cost as verifying a value is a non-residue.)
Effectively in my first email I was suggesting that the 'e' value in a
signature be negated from the current BIP proposal.  In this revision I am
effectively suggesting that the 's' value in a signature should be the one
that is negated instead.
On Sat, Aug 4, 2018 at 8:22 AM, Russell O'Connor

@_date: 2018-08-06 10:00:59
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Yes you are right.
Thanks, I withdraw my proposal.

@_date: 2018-12-06 11:57:09
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
One more item to consider is "signature covers witness weight".
While signing the witness weight doesn't completely eliminate witness
malleability (of the kind that can cause grief for compact blocks), it does
eliminate the worst kind of witness malleability from the user's
perspective, the kind where malicious relay nodes increase the amount of
witness data and therefore reduce the overall fee-rate of the transaction.
Generally users should strive to construct their Bitcoin Scripts in such a
way that witness malleability isn't possible, but as you are probably
aware, this can be quite difficult to achieve as Scripts become more
complex and maybe isn't even possible for some complex Scripts.
Given the new fixed-sized signature of the Schnorr BIP, it becomes much
easier to compute the final witness weight prior to signing.  In complex
multi-party signing protocol, the final witness weight might not be known
at signing time for everyone involved, so the "signature covers witness
weight" ought to be optional.
On Tue, Nov 27, 2018 at 11:59 PM Pieter Wuille via bitcoin-dev <

@_date: 2018-12-11 10:36:59
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I don't believe that the default RBF policy works that way.  My
understanding is that current policy requires an absolute fee increase (by
an amount related to incrementalrelayfee).  There have been proposals to
change default RBF policy, however even my proposal <
still requires a minimal amount of absolute fee increase as a DoS defense.
(I'm reading your comment as attempting to rebroadcast the original
transaction with the same fee amount, with its relatively higher fee-rate).

@_date: 2018-12-11 17:50:24
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I tend to think in opposite terms. Is there a proof that any script can be
transformed into an equivalent one that avoids witness weight
malleability?   But I admit there is a trade off:  If we don't allow for
signature covers weight, and we do need it, it will be too late to add.  On
the other hand if we add signature covers weight, but it turns out that no
Script ever needs to use it, then we've added that software complexity for
no gain.  However, I think the software complexity is relatively low,
making it worthwhile.
Moreover, even if witness weight malleability is entirely avoidable, it
always seems to come at a cost.  Taking as an example libwally's proposed "
it begins with "OP_DEPTH OP_1SUB OP_1SUB" spending 3 vbytes to avoid any
possible witness malleability versus just taking a witness stack item to
determine the branch, costing 1 or 2 (unmalleated) vbytes.  Now to be fair,
under Taproot this particular script's witness malleability problem
probably goes away.  Nonetheless, I think it is fair to say that Bitcoin
Script was designed without any regard given to scriptSig/witness
malleability concerns and the result is that one is constantly fighting
against malleability issues.  Short of a wholesale replacement of Bitcoin
Script, I do think that having an option for signature covers weight is one
of the best ways to address the whole problem.
Regarding your point about 64/65-byte signatures; I speculate that in most
protocols, all parties that are able to consider signing the weight, know
what sighash flags the other parties are expected to be using.  However,
your point is well-taken, and if we choose to adopt the option of
signatures covering weight, we ought to make sure there exists a 65-byte
signature that performs the equivalent of a sigHashAll (of course, still
covering that particular sighash flag under the signature), to ensure that
anti-weight-malleability can be use even when the sighash flags that other
parties will use are unknown.  Even with the extra vbytes in the
signatures, there may be a net weight savings by avoiding the need for
anti-malleability Script code. (It might also be reasonable to have
participants create signatures for a small range of different weight
values? (Sorry in advance to PSBT)).

@_date: 2018-12-13 11:21:10
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
The issue is that the proposal is to sign the actual weight, rather than
sign an upper bound on the weight.
The problem with signing an upper bound, is that you need to specify that
upper bound someplace in the transaction, and we are out of sneaky places
to stash that data.
Signing the actual weight is easy because the total weight is implicit, but
now you need to know the total weight before signing.
DEPTH 1SUB is shorter than DEPTH 1 NUMNOTEQUAL.

@_date: 2018-12-13 11:34:17
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
In multi-party protocols, the last person to sign knows what the total
weight is going to be (now that we have fixed sized signatures) and at
least they have the ability to sign it.  They are probably motivated to
sign the weight as long as they are interested in the success of the
transaction.  I suppose there could be asynchronous protocols where there
isn't a last person to sign, but that seems a bit weird.  Greg, you are
probably more familiar with examples of multi-party protocols than I am.
OTOH maybe the last person to sign isn't interested in the success of the
transaction and wants to cause grief by bloating the transaction and
signing the bloated weight.  I guess in such protocols, you'll have to keep
the anti-malleablity Script Code.
I totally get the idea that signing weight has a lot of issues in many
scenarios.  But I still feel than on the whole it is better to make the
option available than to be forced to rely on anti-malleability Script Code
or non-consensus relay policy.

@_date: 2018-12-13 11:50:10
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I don't know if it such a clear cut case for miner's policy.  A miner is
passed a malleated tx.  They know that there is probably a non-malleated
variant floating around out there somewhere, and they would rather have
it.  But right now they don't, and they probably not going to try to
unmalleate it themselves.  So, why not stick it into their mempool?  If it
eventually makes it into one of their blocks, then it will because it has
the best fee rate available, and to reject it outright is harmful to their
bottom line.  If they find the non-malleated variant later, great, they can
replace it and gain a higher-fee rate tx.  Of course, such a policy opens
them up to a Denial of Service attack.
So what do they do?  Do they accept malleated tx's and implement an RBF
policy that requires sufficient fee rate increases?  Do they reject
malleated txs outright to avoid falling in this trap in the first place as
you suggest?  I don't know, but I don't think things are as clear cut as
you present.
That aside, your list of weight malleable opcodes is shorter than I
imagined and I'm grateful you've compiled it.  Perhaps the best solution is
to make MINIMAL_IF and minimal CScriptNum consensus enforced in the next
version of Script and all but eliminate weight malleability in practice?

@_date: 2018-12-15 18:38:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
Unless it is too much bikeshedding, I'd like to propose that to succeed the
stack must be exactly empty.  Script is more composable that way, removing
the need for special logic to handle top-level CHECKSIG, vs mid-level

@_date: 2018-12-17 22:18:40
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
Thanks.  I probably read that and internalized it and forgot you wrote it.
This one is almost a no-brainer I think.  Nearly every instance of OP_CSV
is followed by an OP_DROP and we'd save 1 WU per OP_CSV if we pop the stack

@_date: 2018-02-12 10:52:30
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
I think it is worth revisiting BIP 125's replace-by-fee policy for when to
replace transactions.
The current policy can be problematic. As noted earlier by Rhavar,
sometimes one's transaction becomes pinned making it infeasible to fee bump
with RBF.  This happens when one makes a normal payment to a large
commercial service, and, while the transaction remains unconfirmed, the
commercial service creates a low-fee-rate sweep of one's payment, among a
collection of others.  If one wants to RBF this original payment, for
example to get confirmation of the change output for use in further
transactions, the current BIP 125 rules require that you make a fee bump
that exceeds the combined total fees of the original transaction and the
low-fee-rate sweep of the commercial service.
The problem is that, while the fee rate of the sweep is low, the absolute
size of the fee can still be large, making it infeasible to RBF the
original transaction.  BIP 125 was defined back in 2015, when perhaps
rational miners did care about absolute fee amounts. However, today we are
in an era where rational miners care about fee-rates more than absolute
fees.  The fee-rate of the large sweep transaction is low enough that we do
not expect that miners will be mining it in the same block as the original
transaction.  Today, a rational miner will prefer a fee-bumped version of
original transaction without consideration of the low-fee sweep transaction
(or at least discounting the low-fee sweep in proportion to the miner's
hash-rate fraction).
Let me quote the five rules that define the current BIP 125 policy:
One or more transactions currently in the mempool (original transactions)
changing rules 3 and 4 to something like the following.
3'. The replacement transaction pays a fee rate of at least the effective
fee rate of any chain of transactions from the set of original transactions
that begins with the root of the original transaction set.
4'. The replacement transaction must also pay for replacing the original
transactions at or above the rate set by the node's minimum relay fee
setting. For example, if the minimum relay fee is 1 satoshi/byte and the
replacement transaction and the original transactions are 1000 bytes total,
then the replacement must pay a fee at least 1000 satoshis higher than the
fee of the root transaction of the original transactions.
Rule 3' is a fancy way of saying that the replacement transaction must have
a fee rate that is larger than the package fee rate of the root of the set
of transactions it replaces, where the package fee rate is the fee rate
implied by considering CPFP.
Rule 4' is an amended anti-spam rule that is intended to avoid DOS attacks
from churning the mempool. I don't know if it is really necessary to pay
for the size of the original transactions being evicted, but some people I
chatted with thought it could be important.
Other people on the mailing list have been thinking about RBF policy for
far longer than I have, so I wouldn't be surprised if my proposal above is
naive.  However, I think it can start a conversation about addressing the
problems with the current RBF policy.

@_date: 2018-02-12 18:19:40
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
The problem is that rule 3 of BIP 125 requires you pay a fee that is higher
than the the fee of T_a *plus* the fee of the sweep-transaction that the
Alice has added as a unconfirmed child transaction to T_a because
double-spending to pay Alice and Bob invalidates Alice's
sweep-transaction.  Alice's sweep-transaction is very large, and hence pays
a large absolute fee even though her fee-rate is very low.  We do not have
any control over its value, hence Alice has "pinned" our RBF transaction.
Yes, that is what I mean.  My proposal was off-the-mark.
Surely CPFP is already computing the package-fee rates of mempool
transactions.  That is the value we need to compute.

@_date: 2018-02-12 18:46:43
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Adding such a restriction was Rhavar's original suggestion in
but it seems the proposal wasn't well received because it kinda destroys

@_date: 2018-02-14 09:08:01
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Do you (or anyone else) know if the package fee rate is considered when
ejecting transactions from the bottom of the mempool when the mempool gets
too large?

@_date: 2018-02-27 11:25:59
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
When you say that you don't think it is possible to solve, do you mean that
there is a specific problem with this proposal of replacing transactions
when offered a new transaction whose fee rate exceeds the package fee rate
of the original transaction (and ensuring that the fee increase covers the
size of the transactions being ejected)?  Is your concern only about the
ability to computing and track the package fee rate for transactions within
the mempool or is there some other issue you foresee?

@_date: 2018-01-09 11:20:20
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
This entropy argument seems confused.  Ignoring constant factors, the
entropy of a checksum is the sum over all possible checksums, i, of
-n_i*log(n_i), where n_i is the number of times the ith checksum occurs
over the space of all possible data being checksummed.  In this application
the checksum is being applied to a fixed m-bit blob of uniformly random
The entropy is maximized when every possible checksum occurs equally as
frequently, that is we achieve maximum entropy when all the n_i values are
equal to each other.  Any error correcting code worth it's salt will try to
achieve this property because the designers want every checksum value to
have as much error correcting power as every other checksum value.  I'm
almost certain that the algebraic properties of your typical error
correcting codes allow you to prove that maximum entropy is perfectly
achieved whenever the data-blob size is at least as large as the checksum
Meanwhile the truncated value of a cryptographic hash function is expected
to be slightly under the maximum entropy value, under the assumption that
the hash function it behaves like a random function.
The main properties of a "strong cryptographic hash function" is that it is
infeasible to find collisions and preimages.  However these properties are
lost when you truncate the hash down to 16-bits.  At this point is it
entirely feasible to find collisions and preimages.
So using a truncated cryptographic hash function doesn't provide you with
more entropy (and, in fact, probably a sliver less entropy), and doesn't
provide you with any of the befits of strong cryptographic hash function.
Every checksum is error correcting.  Given an failed checksum, all you have
to do is search around the space of edits to find the smallest set edits
that yield a valid checksum.  With a 2^16 bit checksum one will expect to
find a nearby checksum within 2^16 trails, even when using a truncated hash
What an error-correcting codes gives you isn't the ability to correct
errors, which we have seen is something that all short checksums provide,
rather they provide *guarantees* about the ability to detect (and correct)
certain common classes of errors.  For example we can have an ECC that
guarantees to find the error where are word is accidentally written down
twice (see
The advice you have been given will only result in losing any guarantees
about detecting common classes or errors; it won't stop attackers from
recovering missing information, and it won't provide a cryptographically
strong function.

@_date: 2018-01-12 05:48:33
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP 117 Feedback 
Putting aside for the moment the concerns that Pieter and Rusty have raised
about BIP 117 (concerns which I agree with), is BIP 117 even a viable soft
fork to begin with?
When it comes to soft forks of Script, in the past there have been two
The first kind is soft-forking new script semantics into NOPn codes.  In
this case, everyone ought to know that these op codes are reserved for
future extensions and no one should be writing script that depends on NOPn
having NOP behavior (For users who want real nop behaviour, there does
exist a real NOP opcode).
The second kind of soft-forking new script semantics is the
reinterpretation of various wholesale scripts (historically via
templates).  Examples of this are Segwit and P2SH.  In the case of Segwit,
the scripts gaining new semantics were applied to a form of completely
unsecured "anyone-can-spend" programs.  Anyone who created such output
prior to the activation of Segwit would know that anyone could claim
ownership of those outputs, and therefore the possibility of losing the
ability to spend legacy forms of these segwit-style outputs is arguably not
harmful as no one in particular had ownership of such funds.  The story for
P2SH is somewhat similar: Prior to the activation of P2SH the creator of of
P2SH style outputs would know that anyone could claim ownership of that
style of output as soon as the hash preimage is published (in the mempool,
for example).
However, if I understand correctly, the situation for BIP 117 is entirely
different.  As far as I understand there is currently no restrictions about
terminating a v0 witness program with a non-empty alt-stack, and there are
no restrictions on leaving non-canonical boolean values on the main stack.
There could already be commitments to V0 witness programs that, when
executed in some reasonable context, leave a non-empty alt-stack and/or
leave a non-canonical true value on the main stack.  Unlike the P2SH or
Segwit soft-forks, these existing commitments could be real outputs that
currently confer non-trivial ownership over their associated funds.  If BIP
117 were activated, these commitments would be subject to a new set of
rules that didn't exist when the commitments were made.  In particular,
these funds could be rendered unspendable.  Because segwit commitments are
hashes of segwit programs, there is no way to even analyze the blockchain
to determine if these commitments currently exist (and even if we could it
probably woudln't be adequate protection).
Naturally we shouldn't be making new rules that could, in principle,
retroactively remove ownership of existing user's funds.

@_date: 2018-01-16 03:39:28
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP 117 Feedback 
By "standard transaction" here, Rusty means that (P2SH or Segwit) scripts
that use the alt-stack pass the standardness checks and will be relayed by
recent Bitcoin Core software.
Regarding lowS:  I think the more severe standardness change was the added
requirement that (some of the) pubkeys in a multisig must be parsable.  I
have talked with people who cannot retrieve their funds now, when before
they could.  However, like lowS, this was only a change to the standardness
rules and not a consensus change, so these funds are not necessarily
permanently lost.  They can be retrieved with miner help.
I don't see how BIP 117, which is a change in consensus rules that could
cause permanent loss of otherwise well-secured funds (in addition to the
other issues raised about BIP 117), is even comparable to the previous
changes in only the standardness rules.

@_date: 2018-01-17 10:28:15
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Hi Ond?ej,
1. There is no similarity between SSS and RSA or any other public-key or
symmetric crypto.  SSS is effectively a one-time pad and is
information-theoretically secure.
2. Even if there were a problem (which there cannot be, due to (1)), using
error correcting codes and truncated hash functions create identical
amounts of information theoretic redundancy.
Let me repeat that SSS is "information-theoretically secure"!  It isn't
only computationally infeasible to break SSS; it is impossible to break
SSS.  If you have all but one necessary share of SSS, there is no
information leaked about the the hidden data, because for every possible
message that could be encoded, there exists some final share that would
decode to that message.  Any of the possibilities for the missing final
share are equally as likely.
It is of no use to apply the precautionary principle against impossible
attacks, especially at the cost of losing the useful properties of a real
error correcting codes that would provide actual guarantees against likely
On Wed, Jan 17, 2018 at 6:39 AM, Ond?ej Vejpustek via bitcoin-dev <

@_date: 2018-01-22 14:21:14
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
On Thu, Jan 18, 2018 at 1:58 PM, Gregory Maxwell via bitcoin-dev <
At this point, is it better just to use GF(2^256+n)?  Is GF(2^256+n) going
to be that much slower than GF(2^8) that we care to make things this
complicated?  (I honestly don't know the answer.)

@_date: 2018-01-27 12:07:25
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
I've been thinking about your comment, and I think your concern can be
addressed.  Taproot would almost certainly be deployed in conjunction with
cross-input signature aggregation.  Because aggregation doesn't work with
ECDSA, only those signatures using Taproot and other Schnorr signatures
would be available for aggregation.  Just having the ability to support
cross-input signature aggregation may be motivation enough for ordinary
pub-key users to switch to Taproot.  However, there is more.
Cross-input signature aggregation probably requires a new field to be added
to the P2P transaction structure to hold the aggregated signature, since
there isn't really a good place to put it in the existing structure (there
are games you can play to make it fit, but I think it is worthwhile).  The
obvious way add block commitments to a new tx field is via the witness
reserved value mechanism present in BIP 141.  At this point I think there
will be some leeway to adjust the discount on the weight of this new
aggregated signature tx field so that even a single input taproot using the
aggregated signature system (here an aggregation of 1 signature) ends up no
more expensive than a single input segwit P2WPKH.

@_date: 2018-01-30 14:12:31
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Design approaches for Signature Aggregation 
That would be the expedient approach.
I want to preface what I'm about to write by first stating that I think the
cross-input signature aggregation is the most important forthcoming
development for Bitcoin and I would be very happy to have any solution for
it deployed in any workable form.  Also, it is difficult to discuss pros
and cons of various designs without concrete proposals, but perhaps we can
try to say some things about various design approaches while still saying
something useful.
I think there are some issues with the expedient proposal for signature
aggregation.  The problems begin with the arbitrary choice of which input
witness will be the canonical choice for holding the aggregated signature.
We want to strictly define which input is the canonical choice for holding
the aggregated signature because we wish to avoid introducing new witness
malleability vectors.  However, the definition of the canonical input is
somewhat complicated.  Because not all inputs are necessarily participating
the aggregation, the canonical choice of input necessarily depends on the
run-time behavior of all the other input Scripts in the transaction.  This
complicates the specification and makes the implementation somewhat
Furthermore designing the canonical choice of input for the aggregated
signature to support future extensions of new script versions or new
opcodes that may want to participate in signature aggregation (for example,
adding CHECKSIGFROMSTACK later) is going to be extraordinarily difficult, I
think.  I don't know how it could even be done.
On the other hand, the extended-transaction approach supports a clean model
of script semantics whereby the signature aggregation is supported via a
new writer (aka logging) side-effect for Script[1].  In this model, rather
than the semantics of Script returning only failure or success, Script
instead results in either failure or conditional success plus a log of
additional constraints that need to be satisfied for the transaction to be
valid.  In the case of signature aggregation, these constraints are of the
form "I require cryptographic evidence that there is a signature on message
M from public key P".  The aggregated signature in the extension of the
transaction provides a witness that demonstrates all the constraints
emitted by all the scripts are satisfied.
Even in the extended-transaction approach, supporting future extensions of
new script versions or new opcodes that may want to participate in
signature aggregation is going to be very difficult.  However, I do have
some half-baked ideas (that you will probably like even less) on how we
could support new script versions and new opcodes based on this idea of a
writer side-effect model of Script semantics.  I hope that designing
support for extendable signature aggregation isn't infeasible.
I think that the cleaner semantic model of the extended-transaction
approach is by itself enough reason to prefer it over the expedient
approach, but reasonable people can disagree about this.  However, there
are even larger issues lurking which appear when we start looking for
unintended semantic consequences of the expedient design.  This is a common
problem with expedient approaches.  It is hard enough to come up with a
design that enables a new feature, but it is even harder to come up with a
design that enables a new feature without enabling other, unintended
"features".  I worry that people do not pay enough attention to the later,
after achieving the former. This sort of thing happened with OP_EVAL in bip
12.  In that situation, the goal was to create a design that enabled pay to
script hash, and OP_EVAL does achieve that in a very straightforward way.
However, the unintended semantic consequences was that bip 12 also enable
unbounded recursion[2] and extended the class of functions definable by
script all the way to the entire class of all computable functions.
We can find unintended semantic consequences of the expedient approach to
signature aggregation by looking at the ways it fails to fit into the
writer side-effect model for signature aggregation.
A. Firstly, we notice that scripts can determine whether or not they are in
canonical position or not by checking the length of their signature data.
This is an effect that goes beyond the abilities of just allowing signature
aggregation.  We can build scripts that can only be redeemed when they are,
or aren't the ones holding the aggregated signature.
B. In the presence of sufficient computation power[3], I expect that
scripts can recover the public keys and signed message data of the
aggregated data, using the same methods used in Enchancing Bitcoin
Transactions with Covenants
. With this
ability, the script in canonical position can determine what messages are
being signed by other inputs, and which public keys they have chosen to
use.  Perhaps a script could enforce a whitelist or blacklist of
approved/disapproved public keys that it is willing or unwilling to be
aggregated with, etc.
C. Scripts can subvert the use the public keys being aggregated themselves
for the purpose of communicate arbitrary data to other script inputs.  With
aggregated CHECKSIGFROMSTACK, scripts can directly use signed messages for
this communication.
I'm not trying to say that the above are good or bad things, after all
signature aggregation is an interactive process so it is expected that
users could decide which keys they are willing to aggregate with.  What I'm
trying to say is that the expedient proposal has a host of unintended
semantic consequences and the above list is only the ones that I can think
of off the top of my head.  I do not even know the full extent of what we
will be enabling with this design but it seems to include adding a
subversive unidirectional cross-input communication channel for Script. Is
that really a feature we want to be bundling with a signature aggregation
I believe that the extended-transaction design is the conservative design.
I conjecture that one can build a reduction from scripts supporting
signature aggregation in the extended-transaction design to scripts that
don't support signature aggregation, while preserving the same security
properties. (The proposed reduction would "simply" replace every aggregated
signature call with a non-aggregated signature call.)  If this conjecture
holds, that means we can prove that the extended-transaction design is only
an optimization and doesn't have any further unintended semantic
consequences.  In particular, we see that the expedient approach doesn't
have such a reduction proof because scripts that are using the expedient
design for cross-input communication cannot be modeled by scripts that
don't have the signature aggregation ability.
I would be disappointed if we end up taking the expedient approach to
signature aggregation (but still very happy that we get signature
aggregation), and there are probably other designs for signature
aggregation beyond the two designs I'm discussing here.

@_date: 2018-01-30 18:25:55
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Design approaches for Signature Aggregation 
============================== START ==============================
On Tue, Jan 30, 2018 at 2:12 PM, Russell O'Connor For example, in private communication Pieter suggested putting the
aggregate signature data into the top of the first segwit v1+ input witness
(and pop it off before evaluation of the input script) whether or not that
input is participating in the aggregation or not.  This makes this
canonical choice of position independent of the runtime behaviour of other
scripts and also prevents the script from accessing the aggregate signature
data itself, while still fitting it into the existing witness data
structure. (It doesn't let us toy with the weights of aggregated signature,
but I hope people will still be motivated to use taproot solely over P2WPKH
based on having the option to perform aggregation.)
Being able to allow aggregation to be compatible with future script or
opcode upgrades is still very difficult to design.

@_date: 2018-07-06 17:05:03
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Some quick comments:
commonly done in RFCs, but I think it is fairly confusing to have `k`
defined in two different ways within a single specification.
Let's let k' = k when jacobi(y(R)) = 1 and let k' = n - k when jacobi(y(R))
= -1.  Note that this ensures that jacobi(y(k'G)) = 1.
Also you've sort of left it undefined what to do when k = 0.  According to
the current specification, you will produce an invalid signature.  The
expected result is that you should win a 1000 BTC prize.
One solution is to let k = *1 + int(hash(bytes(d) || m)) mod (n-1)*.
Alternatively you could let k' = 1 when k = 0.  Or you could just make a
note that signature generation fails with this message and private key pair
when this happens.
Let *e = int(hash(bytes(x(R)) || bytes(dG) || m)) mod n*.
P = dG should probably be noted somewhere in the text.  I.e. this signature
is generated for the public key P = dG.
If the inputs to hash were reordered as *hash(bytes(dG) || bytes(x(R)) ||
m)* then there is an opportunity for SHA256 expander to be partially
prefilled for a fixed public key.  This could provide a little benefit,
especially when multiple signatures for a single public key need to be
generated and/or verified.  If all things are otherwise equal, perhaps this
alternate order is better.
 The signature is *bytes(x(R)) || bytes(k + ex mod n)*.
You haven't defined `x`.  I'm guessing you mean `d` instead.
another condition here to ensure that `P` is not infinity.
On Fri, Jul 6, 2018 at 2:08 PM, Pieter Wuille via bitcoin-dev <

@_date: 2018-07-08 10:36:16
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Right.  I readily admit my proposal has extremely marginal efficiency
benefits. However, I didn't realize there is also an extremely marginal
security benefit to placing the nonce in front of everything.  Although
these things are so marginal that it is perhaps a waste of time to even be
considering them, I think I'd judge the extremely marginal security benefit
to exceed the value of the extremely marginal efficiency gain.  It's
probably best to leave the nonce at the beginning after all.
I did consider this, however the 31 bytes of zeros, plus the SHA256 padding
means we would need to compress *three* blocks in general instead of the
current proposal of just two blocks.  This burden seems to exceed the
benefit of maybe sometimes getting a slightly fast
two-blocks-with-lots-of-zeros when public keys are reused. I wouldn't
recommend it.
There is an alternative of just dropping the SHA-256 length padding.  This
would still be secure in this context because the data is of fixed size.
However, I doubt it is worth breaking the API of every SHA-256 library in
existence to enable that.

@_date: 2018-07-19 09:11:28
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Multiparty signatures 
When engaging in a multiparty signature, the attacker can more than one
variable to modify.  When you are party to a multi-party signature (for
example, in some sort of coin-join protocol) it could be that every other
participant in the multi-party signature is, in fact, the same single
attacker representing themselves as multiple participants.  This is how the
attacker gets their hands on multiple variables.

@_date: 2018-07-26 09:43:19
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
Hi Pieter,
or anything else that is relevant to the reader. This part MUST contain 1
to 83 US-ASCII characters, with each character having a value in the range
[33-126]. HRP validity may be further restricted by specific applications.
You should also add to this section that the HRP should be lowercase.
Since Bech32 forbids mixed-case and otherwise converts everything to
lowercase, it is good to warn upfront against using uppercase in the HRP.
I know the BIP is marked as final, but this wouldn't be a normative change.

@_date: 2018-07-26 10:31:30
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
I think I phrased this badly.
What I mean is that there should be a note that HRP should be specified in
lowercase, or at least mention that uppercase and lowercase HRPs are
considered equivalent and will be canonicalized to lowercase during
On Thu, Jul 26, 2018 at 9:43 AM, Russell O'Connor

@_date: 2018-06-01 11:03:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
Should we replace the Double SHA256 with a Single SHA256?  There is no
possible length extension attack here.  Or are we speculating that there is
a robustness of Double SHA256 in the presence of SHA256 breaking?
I suggest putting `sigversion` at the beginning instead of the end of the
format.  Because its value is constant, the beginning of the SHA-256
computation could be pre-computed in advance.  Furthermore, if we make the
`sigversion` exactly 64-bytes long then the entire first block of the
SHA-256 compression function could be pre-computed.
Can we add CHECKSIGFROMSTACK or do you think that would go into a separate

@_date: 2018-06-01 14:15:32
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
In theory, having a fixed 64 byte constant at the beginning results in zero
overhead for those 64 bytes.  An implementation would just start the usual
SHA-256 algorithm with a different pre-computed and fixed initial value
than SHA-256's standard initial value.  The SHA-256 padding counter would
also need to start at 64*8 bits rather than starting at 0 bits.  In
practice, assuming a OpenSSL-like implementation of SHA-256, it should be
easy to implement this optimization. One would replace SHA256_Init call
with a variant that initializes the SHA256_CTX to this pre-computed value
and sets SHA256_CTX's num counter to the appropriate value.  Non-optimized
implementations can still just add the 64 byte prefix and use any SHA-256
For CHECKSIGFROMSTACK (CSFS), I think the question is whether we want to
I prefer a different opcode for CHECKSIGFROMSTACK because I dislike opcodes
that pop a non-static number of elements off the stack.  Popping a dynamic
number of stack elements makes it more difficult to validate that a Script
pubkey doesn't allow any funny business.

@_date: 2018-06-13 10:58:33
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
In a 33 byte compressed public key, only 1 bit from the first byte conveys
information.  The other 7 bits can be discarded.  This will allow you to
reduce the bech32 encoded result by one character.

@_date: 2018-06-15 11:54:30
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
At the risk of making the proposal more complex, I wonder if it might be
better to support multiple checksum variants?  The trade-off between code
length and recovery seems to be largely determined by the user's medium of
storage, which is likely to vary from person to person.  I personally would
probably be interested in the 51 or even 102 character checksums variants.

@_date: 2018-03-08 10:39:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
Thanks, that makes sense.
I still think it is worthwhile pursuing this proposed change in RBF policy
as it would seem that the current policy is problematic in practice today
where participants are just performing normal transactions and are not
trying to attack each other.

@_date: 2018-03-08 15:07:43
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Revisiting BIP 125 RBF policy. 
I'm not suggesting removing the anti-DoS protections.  I'm suggesting that
replaced transaction require a fee increase of at least the min-fee-rate
times the size of all the transactions being ejected (in addition to the
other proposed requirements).
replacements in the event that someone does spend an unconfirmed output that
Just ask rhavar.  It happens regularly.
Not many wallets let you spend unconfirmed outputs that you didn't create.
The problem is with institutional wallets sweeping incoming payments.  It
seems that in practice they are happy to sweep unconfirmed outputs.
Setting all of the above aside for a moment.  We need to understand that
rational miners are going to prefer to transactions with higher package fee
rates regardless of whatever your personal preferred RBF policy is.  If we
do not bring the RBF policy to alignment with what is economically
rational, then miners are going to change their own policies anyways,
probably all in slightly different ways.  It behooves everyone to develop a
reasonable standard RBF policy, that is still robust against possible DoS
vectors, and aligns with miner incentives, so that all participants know
what behaviour they can reasonably expect.  It is simply a bonus that this
change in RBF policy also partially mitigates the problem of pinned

@_date: 2018-05-01 12:58:37
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP sighash_noinput 
At the risk of bikeshedding, shouldn't NOINPUT also zero out the
hashSequence so that its behaviour is consistent with ANYONECANPAY?
On Mon, Apr 30, 2018 at 12:29 PM, Christian Decker via bitcoin-dev <

@_date: 2018-05-10 10:23:09
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] MAST/Schnorr related soft-forks 
Thanks for writing this up Anthony.
Do you think that a CHECKSIGFROMSTACK proposal should be included within
this discussion of signature soft-forks, or do you see it as an unrelated
CHECKSIGFROMSTACK enables some forms of (more) efficent MPC (See
 enables poor-man's
covenants, and I believe the lightning folks are interested in it as well
for some constant space storage scheme.
On Thu, May 10, 2018 at 8:10 AM, Anthony Towns via bitcoin-dev <

@_date: 2018-05-21 10:20:39
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
In the thread "Revisting BIP 125 RBF policy" @
I propose replacing rule 3 with a rule that instead demands that the
replacement package fee rate exceeds the package fee rate of the original
transactions, and that there is an absolute fee bump of the particular
transaction being replaced that covers the min-fee rate times the size of
the mempool churn's data size.
Perhaps this would address your issue too Rusty.
On Sun, May 20, 2018 at 11:44 PM, Rusty Russell via bitcoin-dev <

@_date: 2018-11-21 12:07:30
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Hopefully my comment is on-topic for this thread:
Given that we want to move away from OP_CODESEPARATOR, because each call to
this operation effectively takes O(script-size) time, we need a replacement
for the functionality it currently provides.  While perhaps the original
motivation for OP_CODESEPARTOR is surrounded in mystery, it currently can
be used (or perhaps abused) for the task of creating signature that covers,
not only which input is being signed, but which specific branch within that
input Script code is being signed for.
For example, one can place an OP_CODESEPARATOR within each branch of an IF
block, or by placing an OP_CODESEPARATOR before each OP_CHECKSIG
operation.  By doing so, signatures created for one clause cannot be used
as signatures for another clause.  Since different clauses in Bitcoin
Script may be enforcing different conditions (such as different time-locks,
hash-locks, etc), it is useful to be able to sign in such a way that your
signature is only valid when the conditions for a particular branch are
satisfied.  In complex Scripts, it may not be practical or possible to use
different public keys for every different clause. (In practice, you will be
able to get away with fewer OP_CODESEPARATORS than one in every IF block).
One suggestion I heard (I think I heard it from Pieter) to achieve the
above is to add an internal counter that increments on every control flow
operator, OP_IF, OP_NOTIF, OP_ELSE, OP_ENDIF, and have the signature cover
the value of this counter.  Equivalently we divide every Bitcoin Script
program into blocks deliminated by these control flow operator and have the
signature cover the index of the block that the OP_CHECKSIG occurs within.
More specifically, we will want a SigHash flag to enables/disable the
signature covering this counter.
There are many different ways one might go about replacing the remaining
useful behaviour of OP_CODESEPARATOR than the one I gave above. I would be
happy with any solution.

@_date: 2018-11-22 11:23:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I see, so your suggestion is that a sequence of OP_IF ... OP_ENDIF can be
replaced by a Merklized Script tree of that depth in practice.
I'm concerned that at script creation time it takes exponential time to
complete a Merkle root of depth 'n'.  Can anyone provide benchmarks or
estimates of how long it takes to compute a Merkle root of a full tree of
various depths on typical consumer hardware?  I would guess things stop
becoming practical at a depth of 20-30.

@_date: 2018-11-22 17:10:11
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Even still, each call to OP_CODESEPARATOR / OP_CHECKSIG pair requires
recomputing a new  scriptCode from BIP 143, and hence computes a new
transaction digest.  I understood that this issue was the main motivation
for wanting to deprecate OP_CODESEPARATOR and remove it from later versions
of script.
However, given that we are looking at a combinatorial explosion in SIGHASH
flag combinations already, coupled with existing SigOp limitations, maybe
the cost of recomputing scriptCode with OP_CODESEPARATOR isn't such a big
And even if we choose remove the behavior of OP_CODESEPARATOR in new
versions of Script, it seems more than 30 layers of sequential OP_IFs can
be MASTified, so there is no need to use OP_CODESEPARATOR within that limit.
above is to add an internal counter that increments on every control flow

@_date: 2018-11-23 15:18:13
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Thanks for bringing this up.  I was thinking the same thing as well, that
yes that should be sufficient to cover the semantics of OP_CODESEPARATOR.
Though to be more precise you would sign the position of the last
_executed_ OP_CODESEPARATOR.
That said, while I agree the above is a superior realization of the
OP_CODESEPARATOR, given that we are probably going to support
OP_CODESEPARATOR inside legacy P2SH scripts indefinitely, it is probably
better to keep the existing akward implementation of OP_CODESEPARATOR in
future versions of Script.  (At least until we decide to stop mangling the
Script consensus code with more and more flag combinations and decide it is
better to cut and paste code for new versions of Script to help ensure we
don't make consensus changes to legacy behaviour).
Sorry for hijacking the thread about OP_MASK and friends.

@_date: 2018-11-30 12:38:04
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] CPFP Carve-Out for Fee-Prediction Issues in 
It seems to me that this two-output scheme does address the specific attack
without tweaking the RBF rules of BIP 125, since you are not doing an RBF
at all.
Suppose we have a 1k-vbyte unconfirmed transaction, TX0, with outputs Z, A,
and B, where A and B are small outputs controlled by the participants Alice
and Bob respectively, with a 1ksat fee, yielding a fee rate of 1sat/vbyte.
Someone, maybe Alice, attempts to pin the transaction, maliciously or not,
by attaching a 10k-vbyte transaction, TX1, to either output Z or output A,
with a fee of 21ksats.  This brings the fee rate for the TX0-TX1 package to
2sat/vbyte, being 11k-vbyte total size with 22ksats in total fees.
Now Bob wants to CPFP to increase the effective fee rate of TX0 to
3sats/vbyte using output B.  He attaches a 1k-vbyte transaction, TX2, to
output B with a fee of 5ksats.  This ought to create a new TX0-TX2 package
with a 3sat/vbyte fee rate, being 2k-vbyte total size with 6ksats in total
fees.  TX1 has now been excluded from the package containing TX0. But TX1
hasn't been replaced, so the RBF rules from BIP125 don't apply.  TX1 is
still a valid unconfirmed transaction operating at a fee rate of
That said, I'm not an expert on how packages and package fee rates are
calculated in Bitcoin Core, so I am speculating a bit.  And, because I'm
talking with Matt, it's more likely that I'm mistaken.  AFAIK, any rules
about CPFP's behaviour in Bitcoin Core is undocumented.

@_date: 2018-09-20 17:12:42
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
It would be helpful to add the intermediate 'e' values computed to the
first four test vectors.
On Fri, Jul 6, 2018 at 2:08 PM, Pieter Wuille via bitcoin-dev <

@_date: 2019-12-01 11:09:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
I think variants like signing the position of the enclosing
OP_IF/OP_NOTIF/OP_ELSE of the OP_IF/OP_NOTIF/OP_ELSE block that the
checksig is within, or signing the byte offset instead of the opcode number
offset are all fine.  In particular, signing the enclosing OP_IF... would
allow sharing of the hashed signed data in a normal multisig sequence of
operations.  Below I'll continue to refer to my proposal as signing the
CHECKSIG position, but please take it to mean any of these proposed,
semantically equivalent, realizations of this idea.
I also think that it is quite reasonable to have a sighash flag control
whether or not the signature covers the CHECKSIG position or not, with
SIGHASH_ALL including the CHECKSIG position.
I don't think this is true in general.  When constructing a script it seems
quite reasonable for one party to come to the table with their own custom
script that they want to use because they have some sort of 7-of-11 scheme
but in one of those cases is really a 2-of-3 and another is 5-of-6.  The
point is that you shouldn't need to decode their exact policy in order to
collaborate with them.  This notion is captured quite clearly in the MAST
aspect of taproot.  In many circumstances, it is sufficient for you to know
that there exists a branch that contains a particular script without need
to know what every branch contains.  Because we include the tapleaf in the
signature, we already prevent this signature copying attack against
attempts to transplant one's signature from one tapleaf to another.  My
proposal is to simply extend this same protection to branches within a
single tapscript.
Second, if there are many branches in the script, it's probably more
Of course this should be done when practical.  This point isn't under
So while I agree that learning about CODESEPARATOR is a reasonable thing to
do, given that I haven't heard the CODESEPARATOR being proposed as
protection against this sort of signature-copying attack before and given
the subtle nature of the issue, I'm not sure people will know to use it to
protect themselves.  We should aim for a Script design that makes the
cheaper default Script programming choices the safer one.
On the other hand, in a previous thread a while ago I was also arguing that
sophisticated people are plausibly using CODESEPARATOR today, hidden away
in unredeemed P2SH UTXOs.  So perhaps I'm right about at least one of these
two points. :)
I think CODESEPARATOR is a better solution to this problem anyway. In
I admit my proposal doesn't automatically prevent this signature-copying
attack against every Script template.  To be fully effective you need to be
aware of this signature-copying attack vector to ensure your scripts are
designed so that your CHECKSIG operations are protected by being within the
IF block that does the verification of the hash-preimage.  My thinking is
that my proposal is effective enough to save most people most of the time,
even if it doesn't save everyone all the time, all while having no
significant burden otherwise.  Therefore, I don't think your point that
there still exists a Script where a signature copying attack can be
performed is adequate by itself to dismiss my proposal.  However if you
believe that if we don't save everyone all the time then there is no point
in trying, or if you believe that signing the CHECKSIG position probably
will not protect most users most of the time, or if you believe the burden
on all the other cases is too great, then maybe it is better to rely on
people using CODESEPARATOR.
Given that MAST design of taproot greatly reduces this problem compared to
legacy script, I suppose you could argue that "the burden on all the other
cases is too great" simply because you believe the problematic situation is
now extremely rare.
I still think we ought to choose designs that are safer by default and
include as much user intention within the signed data as we can reasonably
get away, and use other sighash flags for those cases when we need to
exclude data from the signature.
In particular, imagine a world where CODESEPARATOR never existed.  We have
this signature copying attack to deal with, and we are designing a new
Segwit version in which we can now address the problem.  One proposal that
someone comes up with is to sign the CHECKSIG position (or sign the
enclosing OP_IF/OP_ELSE... position), maybe using a SIGHASH flag to
optionally disable it.  Someone else comes up with a proposal to add new
"CODESEPARATOR" opcode which requires adding a new piece of state to the
Script interpreter (the only non-stack based piece of state) to track the
last executed CODESEPARATOR position and include that in the signature.
Would you really prefer the CODESEPARATOR proposal?
Um, I believe that signing the CODESEPERATOR position without signing the
script code is nonsensical.  You are talking about signing a piece of data
without an interpretation of its meaning.
Recall that originally CODESEPARTOR would let you sign a suffix of the
Script program.  In the context of signing the whole script (which is
always signed indirectly as part of the txid in legacy signatures) signing
the offset into that scripts contains just as much information as signing a
script suffix, while being constant sized.  When you remove the Script from
the data being signed, signing an offset is no longer equivalent to signing
a Script suffix, and an offset into an unknown data structure is a
meaningless value by itself.  There is no way that you should be signing
CODESEPARATOR position without also covering the Script with the signature.

@_date: 2019-12-05 15:24:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
After chatting with andytoshi and others, and some more thinking I've been
convinced that my specific concern about other users masquerading other
people pubkeys as their own in complex scripts is actually a non-issue.
No matter what you write in Script (today), you are limited to expressing
some policy that is logically equivalent to a set of conditions and
signatures on pubkeys that can be expressed in disjunctive normal form.  We
can write such a policy as
(C[1] && PK[1,1] && ... && PK[1,m[1]]) || ... || (C[n] && PK[n,1] && ... &&
where C[i] is some conjunction of conditions such as timelock constraints,
or hash-lock constraints or any other kind of proof of publication, and
where PK[i,j] is a requirement of a signature against a specific public key.
disjunction into those that contain a pubkey that she considers (partially)
under her control and those clauses that she does not control (even though
as we shall see those other keys might actually be under Alice's control,
unbeknownst to her). To that end, let us consider a specific representative
    (C[1] && APK[1]) || (C[2] && APK[2] && BPK[2]) || (C[3] && BPK[3])
where Alice considers herself in control of APK[1] and APK[2], and where
she considers Bob in control of BPK[2] and BPK[3] and where C[1], C[2], and
C[3] are different conditions, let's say three different hash-locks.  We
will also say that Alice has ensured that her pubkeys in different clauses
are different (i.e. APK[1] != APK[2]), but she doesn't make any such
assumption for Bob's keys and neither will we.
When Alice funded this Script, or otherwise accepted it for consideration,
she agreed that she wouldn't control the redemption of the UTXO as long as
the preimage for C[3] is published.  In particular, Alice doesn't even need
to fully decode the Script semantics for that clause beyond determining
that it enforces the C[3] requirement that she cares about. Even if Bob was
masquerading Alice's pubkey as his own (as in BPK[3] = APK[1] or BPK[3] =
APK[2]), and he ends up copying her signature into that clause, Alice ends
up with C[3] published as she originally accepted as a possibility.  Bob
masquerading Alice's pubkey as his own only serves to hamper his own
ability to sign for his clauses (I mean, Bob might be trying to convince
some third party that Alice signed for something she didn't actually sign
for, but such misrepresentations of the meaning of digital signatures is
outside our scope and this just serves as a reminder not to be deceived by
Bob's tricks here).
And the same argument holds for BPK[2].  Even if BPK[2] = APK[1] and Bob
tries to copy Alice's signature into the C[2] condition, he still needs a
countersignature with her APK[2], so Alice remains in control of that
clause.  And if BPK[2] = APK[2] then Bob can only copy Alice's signature on
the C[2] condition, but in that case she has already authorised that
condition.  Again, Bob masquerading Alice's pubkey as his own only serves
to hamper his own ability to sign for his clauses.
So what makes our potential issue here safe, versus the dangers that would
happen in  where Bob
masqurades Alice's UTXO as his own?  The key problem in the UTXO case isn't
so much Bob masquerading Alice's pubkey as his own, as it is an issue with
Alice reusing her pubkeys and Bob taking advantage of that.  We do, in
fact, have exactly the same issue in Script.  If Alice were to reuse
pubkeys such that APK[1] = APK[2], then Bob could take her signature for
C[1] and transplant it to redeem under condition C[2].  We see that it is
imperative that Alice ensures that she doesn't reuse pubkeys that she
considers under her control for different conditions when she wants her
signature to distinguish between them.
For various reasons, some historical, it is much harder to avoid pubkey
reuse for different UTXOs than it is to avoid pubkey reuse within a single
script.  We often use Bitcoin addresses in non-interactive ways, such as
putting them on flyers or painting them on walls and such.  Without a
standard for tweaking such pubkeys in a per-transaction way, we end up with
a lot of pubkey reuse between various UTXOs.  However, within a Script,
avoiding pubkey reuse ought to be easier.  Alice must communicate different
pubkeys intended for different clauses, or if Bob is constructing a whole
complex script on Alice's behalf, he may need to add CODESEPARATORs if
tweaking Alice's pubkey isn't an option.
The conversion of a policy to disjunctive normal form can involve an
exponential blowup (see <
For instance, if Alice's policy (not in disjunctive normal form) is of the
    (C[1] || D[1]) && ... && (C[n] || D[n]) && APK
where C[i] and D[i] are all distinct hashlocks, we require O(2^n) clauses
to put it in disjunctive normal form.  If Alice wants to create signatures
that are restricted to a specific combination of C[i]'s and D[i]'s, she
needs to use an exponential number of pubkeys, which isn't tractable to do
in Script.  But neither my original proposal nor CODESEPARATOR helps in
this case either because CODESEPARATOR can mark only the last executed
position.  Taproot's MAST (Merklized Alternative Script Tree per aj's
suggestion), can maybe provide a tractable solution to this in cases where
it is applicable.  The MAST is always a disjunction and because the tapleaf
is signed, it is safe to reuse pubkeys between alternative branches.
This analysis suggests that we should amend CODESEPARATORs behaviour to
update an accumulator (presumably a running hash value), so that all
executed CODESEPARATOR positions end up covered by the signature.  That
would provide a solution to the above problem for those cases where
taproot's MAST cannot be used.  I'm not sure if it is better to propose
such an amendment to CODESEPARATOR's behaviour now, or to propose to
soft-fork in such optional behaviour at a later time.
However, what I said above was even too simplified.  In general, a policy
of the form.
    (Exists w[1]. C[1](w[1]) && PK[1,1](w[1]) && ... && PK[1,m[1]](w[1]) ||
... || (Exists w[n]. C[n](w[n]) && PK[n,1](w[n]) && ... && PK[n,m[n]](w[n]))
where each term could possibly be parameterized by some witness value
(though at the moment there isn't enough functionality in Script to
parameterize the pubkeys in any reasonably way and it maybe isn't even
possible to parameterise the conditions in any reasonable way).  In
general, you might want your signature to cover (some function of) this
witness value.  This suggests that we would actually want a CODESEPARATOR
variant that pushes a stack item into the accumulator that gets covered by
the signature rather than pushing the CODESEPARATOR position.  Though at
this point the name CODESEPARATOR is probably not suitable, even if it
subsumes the functionality.  Again, I'm not sure if it is better to propose
such a replacement for CODESEPARATOR's behaviour now, or to propose to
soft-fork in such optional behaviour at a later time.

@_date: 2019-06-02 10:32:33
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Can you elaborate a bit more on what the issues were?
I'm not a Core developer but from what I understand, I'd be inclined to to
treat OP_SECURETHEBAG as with an immediate 32-byte parameter by modifying
GetScriptOp to return the 32-byte parameter through pvchRet.
bool GetScriptOp(CScriptBase::const_iterator& pc,
CScriptBase::const_iterator end, opcodetype& opcodeRet,
std::vector* pvchRet)
    opcodeRet = OP_INVALIDOPCODE;
    if (pvchRet)
        pvchRet->clear();
    if (pc >= end)
        return false;
    // Read instruction
    if (end - pc < 1)
        return false;
    unsigned int opcode = *pc++;
    // Immediate operand
    if (opcode <= OP_PUSHDATA4)
    {
        // ...
    }
    if (opcode == OP_SECURETHEBAG) {
        if (end - pc < 0 || (unsigned int)(end - pc) < 32)
            return false;
        if (pvchRet)
            pvchRet->assign(pc, pc + 32);
        pc += 32;
    }
    opcodeRet = static_cast(opcode);
    return true;
and go from there.
Thank you for your review and discussion,

@_date: 2019-06-03 08:56:42
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
Hi Rusty,
On Sun, Jun 2, 2019 at 9:21 AM Rusty Russell via bitcoin-dev <
Is it not possible for Alice to grief Bob's node by alternating RBFing two
transactions, each one placing itself at the bottom of Bob's top 4,000,000
weight mempool which pushes the other one below the top 4,000,000 weight,
and then repeating with the other transaction?  It might be possible to
amend this proposal to partially mitigate this.

@_date: 2019-06-09 00:21:19
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
For example,  "If the original transaction was not in the first 4,000,000
weight units of the fee-ordered mempool and the replacement transaction is
in the first 2,000,000 weight units...." might adequately address the issue.
There are probably other ways as well.

@_date: 2019-06-18 16:57:34
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Just to be clear, while OP_CHECKTXDIGESTVERIFY would enable this style of
covenants if it pulled data from the stack, the OP_SECURETHEBAG probably
cannot create covenants even if it were to pull the data from the stack
unless some OP_TWEEKPUBKEY operation is added to Script because the
"commitment of the script itself" isn't part of the OP_SECURETHEBAG.
So with regards to OP_SECURETHEBAG, I am also "not really seeing any reason
to complicate the spec to ensure the digest is precommitted as part of the
On Thu, Jun 6, 2019 at 3:33 AM ZmnSCPxj via bitcoin-dev <

@_date: 2019-06-24 10:34:54
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
OP_SECURETHEBAG doesn't include the script being executed (i.e the
scriptPubKey specified in the output that is redeemed by this input) in its
hash like ordinary signatures do
Of course, this ScriptPubKey is indirectly committed to through the input's
prevoutpoint.  However Script isn't able to reconstruct this script being
executed from the prevoutpoint in tapscript without an implementation of
public key tweeking in Bitcoin Script.
On Sun, Jun 23, 2019 at 2:41 AM Jeremy Rubin

@_date: 2019-06-24 14:48:51
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
I suspect that your conjecture is true.  And given that it is plausible
that we would want to add an opcode to tweak public keys, it seems like a
reason design to avoid accidental covenants.
(That said, I strongly prefer that the SECURETHEBAG data be the 32-bytes
immediately following the opcode rather than a OP_PUSHDATA, and I'd be
willing to help code this up (see below)).
On Sat, Jun 1, 2019 at 12:47 PM Jeremy via bitcoin-dev <
We shouldn't be using the complexity of the changes to the Bitcoin Core a
measure of the complexity of a proposal.  That is looking the issue from
the wrong side.  If we measure the complexity of Script proposals by how
hard it is to change Bitcoin Core, what will happen is more and more of the
incidental details of Bitcoin Core's implementation will be pulled into the
semantics of Script (e.g. the fact that surrounding opcode values are
readily available in Bitcoin Core's particular implementation of its Script
interpreter).  Instead we should use the complexity of how hard it is to
reason about the new Script semantics.
The peeking semantics of OP_SECURETHEBAG is particularly awful because it
more-or-less breaks the fact that Bitcoin Script can be decomposed into
individual units of "opcodes" whose semantics and be individually
described, and it harms the composability of Bitcoin Script where you can
divide the script between any opcodes and the semantics of the
concatenation of those two scripts is simply the composition of the
semantics of the two halves.  (For those interested in formal semantics,
what we have here is a monoid homomorphism from list of opcodes (syntax) to
stack transformation functions (with side-effects) under (Kleisli)
composition (semantics).) Being able to decompose a Bitcoin Script this way
and reasoning about components is how one would reason about Bitcoin Script
in practice.  (Technically the structure is more involved than a list of
opcodes due to OP_IF, and instead you get a railroad diagram
Putting the 32 bytes of data required by OP_SECURETHEBAG immediately after
the opcode, like how OP_PUSHDATA* works, is a superior design choice.  It
lets us treat the opcodes and its immediate data as an atomic unit when
reasoning about Script and removes the need to define what happens when
OP_SECURETHEBAG is not followed by an OP_PUSDATA.

@_date: 2019-06-25 13:05:39
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY) 
Bitcoin Core is somewhat outside my core competence, but the various
OP_PUSHDATA are already multi-byte opcodes and GetOp already has a data
return parameter that is suitable for returning the payload of an immediate
32-byte data variant of OP_SECURETHEBAG.  All that I expect is needed is to
ensure that nowhere else is using a non-empty data-field as a proxy for a
non-empty push operation and fixing any such occurrences if they exist.
(AFAIKT there are only a handful of calls to GetOp).
It is probably worth updating the tapscript implementation to better
prepare it for new uses of OP_SUCCESSx.  Parsing should halt when an
OP_SUCCESSx is encountered, by having GetScriptOp advance the pc to end
after encountering such a code (decoding Script is no longer meaningful
after an OP_SUCCESS is encountered).  However, that means that GetScriptOp
needs to know what version of script it is expected to be parsing.  This
could be done by sending down some versioning flags, possibly by adding a
versioning field to CScript that can be initialized @
or some other mechanism (and at the same time perhaps having GetSigOpCount
return 0 for tapscript, since counting sigops is not really meaningful in
tapscript). There are probably other reasonable approaches too (e.g your
option 2 below).  I could write some code to illustrate what I'm thinking
if you feel that would be helpful and I do think such changes around
OP_SUCCESS should be implemented regardless of whether we move forward with
OP_SECURETHEBAG or not.
It is probably worth doing this properly the first time around if we are
going to do it at all.
P.S. OP_RESERVED1 has been renamed to OP_SUCCESS137 in bip-tapscript.

@_date: 2019-06-26 20:08:01
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Taproot proposal 
I have a comment about the 'input_index' of the transaction digest for
taproot signatures.  It is currently listed as 2 bytes.  I think it would
be better to expand that to 4 bytes.
The two byte limit is derived from the block size / weight limit, which
limits the maximum size of a transaction, which in turn, due to a minimum
size of an inputs, places a limit on the maximum number of inputs.
However, I think it is a mistake to mix limits from the block layer into
the transaction layer of the consensus rules.  For example, I believe that,
as it stands currently, if we wanted to hardfork an increase in the block
weight limit, doing so wouldn't have any impact on the transaction layer
and we could transparently manage larger transactions with the current
transaction format [2].  However if we start incorporating the block limits
into the transaction layer, then we run the risk of such a hard fork
needing to also make consensus changes in the transaction
format/interpretation if we wanted to handle larger transaction sizes,
which, while doable, wouldn't be so great.
The current transaction format limits the number of inputs (and the number
of outputs) to 2^32-1 or less [1].  So using 4 bytes for the 'input_index'
will suffice.
Given that adding 2 bytes to the signed transaction digest isn't a big
deal, it's probably better just to keep block limits and transaction limits
[1]The var-integer field for the number of inputs (and the number of
outputs) in a transaction looks like it should allow upto 2^64-1 inputs;
however this is an illusion.  The P2P rules dictate that these values are
immediately taken modulo 2^32 after decoding.  For example, if the number
of inputs is a var-integer encoding of 0x0100000001, it is actually just a
non-canonical way of encoding that there is 1 input.  Try this at home!
[2]If we were to hardfork an increase in the block weight limit, we would
probably want to still keep the limit on the size of transactions that
consume legacy UTXOs in order to avoid the quadratic computation problems
that plagues the legacy transaction digest.
On Mon, May 6, 2019 at 2:36 PM Pieter Wuille via bitcoin-dev <

@_date: 2019-06-28 07:16:46
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Taproot proposal 
Hmm? If I'm following what you mean, that's not the P2P rules, it's the
Thanks for this correction!  I totally missed that MAX_SIZE == 0x02000000.
I think I mistook it for SIZE_MAX when reviewing this, or just didn't
notice it at all.

@_date: 2019-03-07 10:03:17
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
OP_CODESEPARATOR is the only mechanism available that allows users to sign
which particular branch they are authorizing for within scripts that have
multiple possible conditions that reuse the same public key.  Because of
P2SH you cannot know that no one is currently using this feature.
Activating a soft-fork as describe above means these sorts of funds would
be permanently lost.  It is not acceptable to risk people's money like this.
I suggest an alternative whereby the execution of OP_CODESEPARATOR
increases the transactions weight suitably as to temper the vulnerability
caused by it.  Alternatively there could be some sort of limit (maybe 1) on
the maximum number of OP_CODESEPARATORs allowed to be executed per script,
but that would require an argument as to why exceeding that limit isn't

@_date: 2019-03-07 10:16:43
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Sighash Type Byte; 
The sighash type byte is a "great" place to store a few bits of ancillary
data when making signatures.  Okay it isn't great, but it is good enough
that some misguided users may have been using it and have unbroadcast
transactions in cold storage (think sweeps) for UTXOs whose private keys
may have been lost.  I don't think that one's hunch that there isn't much
risk in disabling these sighashes is good enough to put people funds at
risk, especially given the alternative proposal of caching the
just-before-the-last-byte sighash midstate that is available.

@_date: 2019-03-08 10:57:14
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Sighash Type Byte; 
I don't think "somewhat less candidates for soft-forking" is a fair
description.  These bits essentially unsuitable for soft-forking within
legacy Script.
I don't think "someone
I disagree. It is sufficient.
When was the last time Bitcoin soft-forked out working transactions that
sent funds from securely held UTXOs to securely held UTXOs (aside from
those secured by Scripts using the reserved OP_NOP1-OP_NOP10)?  AFAIK it
has never occurred since the time of Satoshi, even for the most
hypothetical of transactions.  It is my understanding is that Bitcoin Core
would never do such a thing unless the security of Bitcoin protocol itself
was under existential threat (see OP_CODESEPARATOR) and even then Bitcoin
Core would only soft-fork out the minimal amount necessary to safely
diffuse such a threat.
Since the above soft-fork isn't addressing addressing any such threat (that
I'm aware of), and could hypothetically destroy other people money, it
crosses a line I thought we were never supposed to cross.
Perhaps you don't see them in used in the blockchain because the users
trying to use them are caught up by the fact they they are not being
relayed by default (violating SCRIPT_VERIFY_STRICTENC) and are having
difficulty redeeming them.
You cannot first make transactions non-standard and then use the fact that
you don't see them being used to justify a soft-fork.
I know of users who have their funds tied up due to other changes in
Bitcoin Core's default relay policy.  I believe they waiting for their
funds to become valuable enough to go through the trouble of having them
directly mined.  Shall we now permanently destroy their funds too, before
they have a chance to get their transactions mined?

@_date: 2019-03-08 10:57:25
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
On Thu, Mar 7, 2019 at 2:50 PM Matt Corallo It's very easy to construct a practical script using OP_CODESEPARATOR.
IF <2>   <2> CHECKMULTISIGVERIFY ELSE CODESEPARATOR
 CHECKSIGVERFY ENDIF
Now when someone hands Alice, the CFO of XYZ corp., some transaction, she
has the option of either signing it unilaterally herself, or creating a
partial signature such that the transaction additionally needs Bob, the
CEOs signature as well, and Alice's choice is committed to the blockchain
for auditing purposes later.
Now, there are many things you might object about this scheme, but my point
is that (A) regardless of what you think about this scheme, it, or similar
schemes, may have been devised by users, and (B) users may have already
committed funds to such schemes, and due to P2SH you cannot know that this
is not the case.
Please don't strawman my position.  I am not suggesting we don't fix a
vulnerability in Bitcoin.  I am suggesting we find another way.  One that
limits the of risk destroying other people's money.
Here is a more concrete proposal:  No matter how bad OP_CODESEPARATOR is,
it cannot be worse than instead including another input that spends another
identically sized UTXO.  So how about we soft-fork in a rule that says that
an input's weight is increased by an amount equal to the number of
OP_CODESEPARATORs executed times the sum of weight of the UTXO being spent
and 40 bytes, the weight of a stripped input. The risk of destroying other
people's money is limited and AFAIU it would completely address the
vulnerabilities caused by OP_CODESEPARATOR.
Even soft forking a rule like, "it is illegal to execute an
OP_CODESEPARATOR after any CHECKSIG/CHECKMULTISIG operation", would be
vastly better than the current proposal, even though I would still object
to it.
I already know of people who's funds are tied up due to in other changes to
Bitcoin Core's default relay policy.  Non-standardness is not an excuse to
take other people's tied up funds and destroy them permanently.
There is some sort of crisis in the Bitcoin protocol stemming from the
possible excessive usage of OP_CODESEPARTOR otherwise we wouldn't even be
considering this soft fork.  Fine.  But presumably it is impossible for a
transaction to both be produced in good faith for legitimate use and at the
same time are expensive enough to be used as an attack vector, and
hopefully there is a wide gap between these two cases.  So let's draw a
line between the two cases to rule out attacks while allowing legitimate
uses by simply suitably pricing the OP_CODESEPARATOR opcode by weight.  At
worst case this moderately-large transaction is very expensive, reflecting
its true cost, or is was so expensive that it couldn't possibly have been
legitimate to begin with since the resources to validate it exceed the
amount that are reasonable to validate an entire block of regular

@_date: 2019-03-09 13:29:15
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Hi Sjors,
I did say per executed OP_CODESEPARATOR, but upon reflection, I agree that
we'd like to know the weight without execution.  I think counting the
number of occurrences of OP_CODESEPARATOR (perhaps at the same time we
count OP_CHECKSIG operations?) is a reasonable compromise, and increasing
the weight according to my proposed formula based on that count (ideally
we'd take OP_IF branches into account).
I wish this were the case too, but I don't think it is reasonable to assume
that (even maaku isn't subscribed
and I don't even think it is fair to assume such a someone necessarily even
speaks English.

@_date: 2019-03-09 13:29:24
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Hi Matt,
On Fri, Mar 8, 2019 at 1:35 PM Matt Corallo No one is required to test their Scripts on a public testnet; they can use
regtest. Because these transactions are non-standard on mainnet, it could
take years to arrange for these funds to be recovered by having their
transactions mined directly, or take years to become valuable enough to be
worth bothering having them directly mined.  As I have noted elsewhere, you
cannot first make transactions non-standard and then use the fact that you
don't see them being used on mainnet to justify a soft-fork.
My argument isn't weird; it is principled.  You are skeptical that any uses
of OP_CODESEPARATOR have P2SH commitments.  I am also skeptical, and so is
everyone reading this mailing list.  But none of us know this with
certainty, and it is /wrong/ for any of us to gamble with other people's
money that our assumptions are true.
Instead, it is this soft-fork proposal that is unprecedented. Let me
reiterate what I posted in another thread:
Bitcoin has *never* made a soft-fork, since the time of Satoishi, that
invalidated transactions that send secured inputs to secured outputs
(excluding uses of OP_NOP1-OP_NOP10).
The fact that Bitcoin has stuck to this principle gives me and everyone
else confidence in the protocol; that anyone can secure their funds by
whatever scheme they dream up, and deploy it without needing permission or
anyone else to vet their Scripts. So long as they are not impairing the
Bitcoin protocol itself, the most that Bitcoin Core will do is stop
relaying their transactions by default.
Undermining this principle means undermining what provides Bitcoin's value
in the first place.
The problem in this particular case is that there exist valid secure
transactions that make use OP_CODESEPARATOR such that these transactions
themselves impair the Bitcoin protocol (through excessive validation costs)
in a way that, AFAIU, is fundamental to the nature of such transactions (in
particular, it isn't just due to an implementation detail of Bitcoin
Core).  Thus to fix this vulnerability we must necessarily violate the
principle of not invalidating, secure transactions.  However, this fact
isn't license to freely invalidate any transactions we want.  We ought to
strive to minimize the scope of violation of this principle.  Alice and Bob
from XYZ. corp should be able to keep their benign transaction illustrated
above, and we only eliminate those transactions that actually impair the
Bitcoin protocol.
This is the perfect opportunity to show the world that Bitcoin Core simply
doesn't take chances when it comes to other people money.
There is no consensus rule about minimum fees, and CPFP could add the more
fees. But yes, I am saying that Alice and Bob could be building on their
transaction illustrated above, but not creating a many input tx that
wouldn't fit into a block with my proposed added weight, because if their
transaction won't fit into a block with the added weight then it was a
malicious transaction to begin with.
Do you not recognize the material difference between a soft-fork that
doubles the cost of a transaction like Alice and Bob's versus making their
transaction entirely illegal?
Further note that if you don't remove it getting the efficiency wins
How can this be "additional" complexity when this is how the protocol works
today?  All you have to do is not change the semantics of
OP_CODESEPARATOR.  It is literally no work.
Regarding the efficiency wins, let me repeat myself: The performance costs
of wiping the cached sighashs is not worse than what the performance costs
would be if the transaction had an additional input spending an equally
sized UTXO.
People have told me that they are hurt by some other non-standardness
changes and I understand that they have been sitting on those funds for
years.  Maybe they don't realize their is some place to complain or maybe
they think there must be a good reason why they are not allowed to do what
they were previously allowed to do.  Perhaps others don't want to risk
blowing their pseudonymity.  Perhaps they think that attempting to undo
some of these non-standardness changes is futile.  I can bring up the
specific cases I've encountered in a new thread if you think it is
Regarding OP_CODESEAPRATOR specifically, disabling the rely of such
transactions partially mitigates the vulnerability.  Once the vulnerability
is properly patched, for example by suitably increasing the weight of the
operation or opcode, we could drop the prohibition on relaying such
transactions.  Non-standardness is not necessarily a path to a new
consensus rule. We have several non-standardness rules in place that are
never intended to become new consensus rules.  Sometimes non-standardness
is a temporary mitigation.

@_date: 2019-03-10 11:22:44
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
I fear that we cannot simply wait 10 years to address the vulnerability
that OP_CODESEPARATOR has in its current form.
On Fri, Mar 8, 2019 at 7:32 PM LORD HIS EXCELLENCY JAMES HRMH <

@_date: 2019-03-11 13:49:33
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Hi Jacob,
It is so easy to say stuff like this when one's own money isn't what is at
While I encourage users who would be harmed to chime in if they can,
unfortunately, I think it is mostly wishful thinking on our part that they
necessarily would.  In fact, there is evidence that in practice people
To illustrate this, consider the example of the people affected by PR , which makes unparsable
public keys non-standard.  As far as I am aware none have commented on this
mailing list about it yet even though I happen to know such people do exist
because I've talked with them on Slack.  I believe the person I spoke with
to took over a year (and probably more than two years) to even notice that
the transactions they want to redeem with are no longer standard.  To be
fair, their money that is stuck due to PR  isn't lost yet, but I'm
skeptical they would think or know to speak up here even if their money was
on the chopping block.  The fact that they haven't been able to move their
money in the last *4 years* doesn't mean they wouldn't like it back one day.
While non-standardness is a helpful in dissuading users from committing new
funds to OP_CODESEPARATOR scripts, it doesn't do anything to help users
that may have been caught unaware by the non-standardness change.
Furthermore, because these transactions are non-standard, anyone caught off
guard by the change is going to have a very hard time redeeming their
funds, as we have already seen with PR  a non-standardness change
that is far older than the OP_CODESERPATOR change in PR

@_date: 2019-03-11 15:15:38
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Increasing the OP_CODESEPARATOR weight by 520 (p2sh redeemScript size
limit) + 40 (stripped txinput size) + 8 (stripped txoutput size) + a few
more (overhead for varints) = 572ish bytes should be enough to completely
eliminate any vulnerability caused by OP_CODESEPARATOR within P2SH
transactions without the need to remove it ever.  I think it is worth
attempting to be a bit more clever than such a blunt rule, but it would be
much better than eliminating OP_CODESEPARATOR within P2SH entirely.
Remember that the goal isn't to eliminate OP_CODESEPARATOR per se; the goal
is to eliminate the vulnerability associated with it.
On Mon, Mar 11, 2019 at 12:47 PM Dustin Dettmer via bitcoin-dev <

@_date: 2019-03-12 21:34:21
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Sighash Type Byte; 
Hi Matt,
(I moved your comment to this thread, where I think it is more relevant).
This is a fair point.  I concede that as far as Sighash Type Byte is
concerned, the type of change is fairly similar to BIP 68 (though I don't
think the argument applies to OP_CODESEPARATOR).
I might rephrase what you say as "invalidating otherwise-unusable bits of
the protocol".  I don't quite know the right phrasing that captures both
the insecure and redundant aspects of the protocol.  I'm willing to accept
that nSequence numbers (as they originally were), NOP1-NOP10 and these
extra sighash types can all be classified as redundant aspects of the
Bitcoin protocol.
I still think the alternative proposal of caching the sha256 midstate is
the better choice.  We should strive to avoid changing the consensus rules
when we have reasonable alternatives to achieve our goals. However, I now
see that this proposal isn't entirely unprecedented.
On Tue, Mar 12, 2019 at 5:08 PM Matt Corallo On Fri, Mar 8, 2019 at 10:57 AM Russell O'Connor

@_date: 2019-03-12 21:34:50
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
On Tue, Mar 12, 2019 at 6:39 PM Jacob Eliosoff The purpose of making OP_CODESEPARATOR non-standard was to partly mitigate
the risk of the vulnerability that OP_CODESEPARATOR induces while we
consider how to patch it.

@_date: 2019-03-12 21:38:44
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
Hi Matt,
On Mon, Mar 11, 2019 at 10:23 PM Matt Corallo I see.  I was under the mistaken impression the concerns about of
OP_CODESEPARATOR was simply due to the vulnerability it induces.
I'll say it now then: Simplifying the theoretical operation of Bitcoin is
not a sufficient reason to make changes to the consensus rules, and it is
most certainly not a sufficient reason to remove usable op codes.
Had I understood that this was your motivation I would have presented my
opinion earlier. I understand that the OP_CODESEPARATOR vulnerability is
quite serious and making it non-standard while we address the problem is a
good idea (hence the reason why I never objected before now).
What I don't understand is why you feel that avoiding flushing the sigcache
is so critical that you are willing to go through a risky consensus change
just to achieve it?  The sigcache is effectively flushed for each input of
a transaction anyways, so what's the big deal about flushing it during
Script execution as well?
Well you've spoken to me now, and I believe I have given you good reasons
to keep it.  We all used to think that OP_CODESEPARATOR was a useless
operation that no one in their right mind would ever use, but it turns out
that we were wrong.  Lesson learned.  We should be more humble about
considering these sorts of changes in the future because it seems we might
not understand Bitcoin as well as we think we do.  At the very least I was
caught by surprise by the utility of OP_CODESEPARATOR.
You misunderstand my point regarding invalid public keys.  My point is that
if no one has spoken up about the invalid public key issue on this mailing
list, something that we know really does affects people, why do you expect
that people would have spoken up about OP_CODESEPARATATOR affecting them?
Agreed, that's why we will want to not simply count the OP_CODESEPARATORS,
but rather count the maximum number of OP_CODESEPARATORS that can be
executed through the any of the various possible OP_IF branches.  Adding
this sort of control-flow analysis is a pretty simple. It just requires a
small stack of pairs of numbers and linear traversal through the Script.
This sort of OP_IF control flow analysis ought to have been done for
counting CHECKSIG operations, but unfortunately it is too late for that
now.  I could prototype the sort of analysis I have in mind if you think
that would be helpful.
In fact, it is really alternating uses of OP_CODESEPARATOR and CheckSig
operations that is problematic, so it is probably worth attempting to count
these pairs rather than just OP_CODESEPARATORS.

@_date: 2019-05-21 13:20:32
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Taproot proposal 
Regarding Tapscript, the specification calls for the final value of the
stack being a single non-false value:
The tapscript is executed according to the rules in the following section,
Perhaps it is worth taking this opportunity here to remove a minor wart of
the Script language and instead require the stack to be exactly empty upon
In addition to removing a potential malleability vector, I expect it would
simplify development of Bitcoin Script.  A rule requiring an empty stack
means that the conjunction (logical and) of two policies can be implemented
by the simple concatenation of Bitcoin Scripts.  This combined with the
taproot ability to form the disjunction (logical or) of policies by having
multiple Merkle branches, means that the translation of a policy written in
disjunctive normal form (the logical ors of logical ands of primitive
policies) can be straightforwardly translated to a taproot of tapscript.
That said, I think the developers of miniscript <
 are in a much better
position to comment on whether my above intuition is correct given that
they've had to implement a host of various calling conventions.  I
understand that at least some of this complexity is due to Bitcoin Script's
one element stack rule.
Scripts under the old one element rule can be translated to the new rule by
adding an OP_VERIFY operation to the end of the script; however it is
likely that this OP_VERIFY can be folded into the previous operation
yielding an OP_EQUALVERIFY or OP_CHECKSIGVERIFY in many cases.
Even if we choose not to implement the empty stack rule, we should at least
require that the last element be 0x01 to remove a potential malleability
vector and bring it in line with MINIMAL_IF semantics.
On Mon, May 6, 2019 at 2:36 PM Pieter Wuille via bitcoin-dev <

@_date: 2019-05-22 17:01:21
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Recently there have been some tapscript proposals, SIGHASH_ANYPREVOUT and
OP_CHECKOUTPUTHASHVERIFY, that aim to enable particular new features for
Bitcoin via new Script operations.  However, I think that these proposals
miss the mark when it comes to how they approach Bitcoin Script and
language features.
Bitcoin Script appears designed to be a flexible programmable system that
provides generic features to be composed to achieve various purposes.
Thus, when we design new language features for Script, we should be
striving, as much as possible, to similarly build general purpose tools
which can in turn be used for a variety of purposes.
I feel the SIGHASH_ANYPREVOUT and OP_CHECKOUTPUTHASHVERIFY proposals fail
to achieve these design goals.  They are both are designed with very narrow
applications in mind, while also going out of their way to extend the
semantic domain of the interpretation of Bitcoin operations in new ways
that complicate their specification.  In the case of SIGHASH_ANYPREVOUT,
the semantic domain is extended by adding new counters to track the use of
various v0 and v2 signature types.  In the case of
OP_CHECKOUTPUTHASHVERIFY, it employs a new context-sensitive operation that
peeks at the value of surrounding opcodes.
Instead, I propose that, for the time being, we simply implement OP_CAT and
OP_CHECKSIGFROMSTACKVERIFY.  OP_CAT pops two byte arrays off the stack and
pushes their concatenation back onto the stack.  OP_CHECKSIGFROMSTACKVERIFY
pops a signature, message, and pubkey off the stack and performs a
bip-schnorr verification on the SHA256 hash of the message.
In concert, these two operations enable:
* Oracle signature verification, including discrete log contracts.
* Amortized secure multiparty computations (see "Amortizing Secure
Computation with Penalties" by Kumaresan and Bentov).
* Transaction introspection including:
+ Simulated SIGHASH_ANYPREVOUT, which are necessarily chaperoned simply by
the nature of the construction.
+ Decide if a transaction has exactly one input or not. (etc.)
+ Weak covenants, which can verify output scripts to see if they are among
a set of predefined values or verify the output hash.
and presumably more applications as well.
For better or for worse, without an OP_PUBKEYTWEEK operation available, the
more interesting recursive-covenants remain largely out of reach, with the
exception of a recursive covenant that is only able to send back to its own
address, possibly abusing its own TXO value as a state variable.
All this is accomplished by two straightforward opcodes whose semantics are
pure computational operations on stack values.  The only semantic
side-effect is that OP_CHECKSIGFROMSTACKVERIFY would count towards the
existing 'sigops_passed' count.  Moreover, I feel that adding these
operations does not preclude adding more specialized opcodes in the future
as an optimization for whatever popular constructions come up, once we know
what those are.
I feel that this style of generic building blocks truly embodies what is
meant by "programmable money".

@_date: 2019-05-22 22:32:26
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Taproot proposal 
Thanks for the info.  I'm surprised to learn that 'T' would still exist
internally.  That does make my proposed ammendment a somewhat more marginal
than I expected.  I still think it would be an improvement, but I guess it
is acceptable the way it is if that is what other people prefer.
That is a very good argument.  If we were to go with an empty stack we'd
probably also want modify to have CSV and CLTV pop their inputs off the
stack.  But at that point perhaps we'd want to change their opcode values
to avoid confusion with old style script.  I guess I'm getting more
convinced to not touch this stuff just and just bear with the somewhat
unfortunate legacy behaviour.

@_date: 2019-05-23 18:00:59
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Hi Jimmy,
The message could really be anything.  For example, in discreet log
contracts, AFAIU, you might have a specific public key from a trusted third
party (the Oracle) that is signs the closing price of corn in BTC on
2019-05-23 with a particular nonce dedicated to that product-date pair, in
which case the message would be the price expressed in binary.  In the case
of amortized secure multiparty computations, the message is protocol
specific binary data that consists of a counter (or counters), concatenated
with shares of secret data that is used to construct the result of the
multiparty computation.  In the case of transaction reflection, the message
would be a duplicate copy of the tapscript signed transaction data (about
244 bytes of data plus a 64 byte prefix).
As you note, the message is likely to constructed from a value computed
from a mix of witness and committed data, though the message might be pure
witness data, as in the discreet log contract example.  In that the
discreet log contract example, you'd probably duplicate the integer value
and do further processing (e.g. compare it to some other committed value).

@_date: 2019-05-23 18:06:45
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Hello ZmnSCPxj,
I agree that adding OP_CHECKSIGFROMSTACK doesn't preclude adding shortcuts
such as `SIGHASH_ANYPREVOUT` and `OP_CHECKOUTPUTSHASHVERIFY`, and I agree
we ought to support such operations directly, especially if we see
widespread use of these constructions in practice.
I think it is desirable to add OP_CHECKSIGFROMSTACK for its direct purposes
of enabling oracle verification and discreet log contracts.  Moreover, it
would be better decide if we do or do not want to do this first, because
whether or not we chose to implement a general OP_CHECKSIGFROMSTACK will
influence the design of these other proposals.
For example, if we choose to deploy OP_CHECKSIGFROMSTACK, then the design
of OP_CHECKOUTPUTSHASHVERIFY ought to be simplified to OP_PUSHOUTPUTHASH
and OP_PUSHNUMINPUTS (etc.) because the proposal would no longer be
extending the expressiveness of Bitcoin Script.  And while
OP_CHECKSIGFROMSTACK doesn't directly address whether SIGHASH_ANYPREVOUT
should be with or without a chaperone (as the simulated version with
OP_CHECKSIGFROMSTACK is necessarily chaperoned), we might get an
opportunity to learn if users are willing to take advantage of the
chaperone, or whether they rather bypass it by using a short well-known
pubkey: (e.g.
and/or similar short signatures if we deploy OP_CHECKSIGFROMSTACK first.
Since most of the "scary" recursive convents are not available with
OP_CHECKSIGFROMSTACK within taproot (without further extensions), the
OP_CHECKSIGFROMSTACK proposal now has quite different consequences than

@_date: 2019-05-24 11:10:21
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Jonas informs me that I've misunderstood how discreet log contracts work.
The DLC signatures are not directly checked by Script and do not rely on
CHECKSIGFROMSTACK.  I apologize for my hasty literature review.

@_date: 2019-05-24 19:07:28
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
In order of escalating scope of amendments to OP_COSHV, I suggest
1) Peeking at surrounding data surrounding data should definitely be
replaced by a pushdata-like op-code that uses the subsequent 32-bytes
directly.  The OP_SUCCESSx upgrade path specifically allows for this, and
avoids complicating the semantics Bitcoin Script.
2) Furthermore, the number-of-input-verification and the
outputhash-verification operations ought to be split into different opcodes
as they are logically unrelated.
3) Better still, we should instead implement the transaction reflection
operations of OP_PUSHOUTPUTHASH and OP_NUMINPUTS that puts the outputhash
and number of inputs respectively onto the stack.  Recursive covenants
appear to be effectively impossible without either an OP_TWEEKPUBKEY or an
OP_PUSHSCRIPTPUBKEY so the effort your proposal goes through to guard
against placing an arbitrary outputhash onto the stack appears to be wasted
effort to me.
4) If we anticipate adding OP_CHECKSIGFROMSTACKVERIFY, then we should most
definitely prefer (3) instead of OP_COSHV, if we still feel the need to do
anything at all.  It is probably best to have both
OP_CHECKSIGFROMSTACKVERIFY and transaction reflection operations of
OP_PUSHOUTPUTHASH and OP_NUMINPUTS but I think I would be fine with just
OP_CHECKSIGFROMSTACKVERIFY as well.
On the other hand, if we are serious about preferring less per-block
bandwidth over reusable primitive opcodes for programming, then we should
instead abandon the RISC-style Bitcoin Script and instead add an
alternative CISC-style taproot leaf type that directly provides (a
conjunction of) the various popular common policies: channel opening,
channel factories, coinjoins, hashlocks, timelocks, congestion control
etc.  Segwit v0 already implements this CISC-style for the single most
popular policy: single signature verification.

@_date: 2019-05-25 08:52:44
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
Allowing multiple inputs is certainly better than the 1 restriction COSHV.
However, I agree on your preference for a RISC+CISC approach.  Which is why
instead of COSHV or CHECK_TXID_TEMPLACE_DATA we should do the more RISC-y
thing and begin adding transaction reflection primitives, starting with
OP_NUMINPUTS and OP_PUSHOUTPUTSHASH.  Nothing bad will happen by pushing
the OUTPUTSHASH onto the stack, and we won't even get recursive covenants
with just these transaction reflection primitives in tapscript.

@_date: 2019-05-29 02:49:29
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] An alternative: OP_CAT & OP_CHECKSIGFROMSTACK 
============================== START ==============================
You may have a point.  However, I'm still inclined to think that problem is
that you want some subset of concatenation, arithmetic, CHECKDATASIG,
transaction reflection and/or covenants in order to create particularly
useful programs.
A while ago, I was designing a moderately sophisticated Script for Elements
Alpha to see if I could implement a toy game, but ultimately I was thwarted
due to the fact that Elements Alpha didn't support multiplication.
I did briefly consider using repeated additions and nested if statements to
implement multiplication since I was expecting my numbers to be 11 or less,
but ultimately I decided to just continue my work on an alternative to
Script rather than trying to work around the missing multiplication.
I haven't checked your details but the above looks about correct to me.
So what I was thinking is that we could add CHECKDATASIG first, and then
people could get started on actually using ANYPREVOUT in practice and we
can take our time to debate the merits of the chaperone vs non-chaperone,
and possibly learn something about actual use before making a decision.
There is no doubt that using ANYPREVOUT directly uses less weight, but they
seem close enough to that it the simulation is usable, though perhaps far
enough apart that we would want to eventually add ANYPREVOUT.  However, do
keep in mind that our goal is not to minimize the weight of specific
redemption policies.  The weight of implementing any particular redemption
policy in Script is somewhat arbitrary to begin with anyways, being
dependent on the choices made for the Script language operations and its
encoding.  Again, if our goal were to minimize weight for specific
redemption policies we should abandon SCRIPT and directly use a language
similar to Miniscript, and/or just directly implement an enumeration of
However, my proposal CHECKSIGFROMSTACK (aka CHECKDATASIG) proposal was
based on my argument that CHECKDATASIG covenant abilities wouldn't be
controversial since it was limited to self-recursion and had less than
64-bits of state space.  But ZmnSCPxj has shown that my conclusions were
hasty and that self-recursion has access to arbitrarily large amounts of
state space.  In light of this, it would appear that self-recursive
covenants is nearly as powerful as arbitrary recursive covenants, and
therefore is nearly as controversial.
So, while I do think that we should add support for recursive covenants to
Bitcoin, we probably not ready to add it yet given the controversy around
the far more innocent ANYPREVOUT.  I do think it would be useful to add
support for CAT and CHECKDATASIG in order to implement MPC with penalties,
but perhaps we should support that via a HASH_tapdata digest function
rather than SHA256, in order to avoid any accidental covenants.  Of course
doing so would no longer count as "an alternative" proposal to ANYPREVOUT
or COSHV, and simply "an additional" proposal.
The whole point is to keep the functionality simple and let users program
what they want.  What we don't want to do is tailor an opcode for the
specific use case we have in mind, because that just comes at the expense
of all the use cases we don't have in mind.

@_date: 2019-11-08 08:03:52
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
I do like the idea of length prefixing the witness program.  I will note
that the 1 byte witness version is really more like a 1 character witness
version.  There are 17 different segwit versions and there are 32
characters in the bech32 alphabet.  That leaves 15 unused characters that
we can use for assigning new meanings too.
That said, it is probably most sensible to define a new
human-readable-prefix for length prefixed bitcoin witness programs.  "btc1"
On Fri, Nov 8, 2019 at 12:12 AM ZmnSCPxj via bitcoin-dev <

@_date: 2019-11-27 16:29:32
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] Signing CHECKSIG position in Tapscript 
Hi all,
I'd like to revisit an old topic from last year about the data signed in
tapscript signatures <
The current tapscript proposal requires a signature on the last executed
CODESEPRATOR position.  I'd like to propose an amendment whereby instead of
signing the last executed CODESEPRATOR position, we simply always sign the
position of the CHECKSIG (or other signing opcode) being executed. Then we
can deprecate CODESEPARTOR (either by making it OP_SUCCESS, or a nop, or
always fail when executed, or whatever).
The main motivation for this proposal is to increase robustness against
various signature-copying attacks in Scripts that have multiple spending
conditions.  Bitcoin is already robust against attacks where the attacker
attempts to peddle a victim's UTXO as their own and try to copy the
victim's signature from one transaction input to another input.  Because
Bitcoin signatures specify which input within a transaction is being signed
for, such attacks fail (see However, unless CODESEPARATOR is explicitly used, there is no protection
against these sorts of attacks when there are multiple participants that
have signing conditions within a single UTXO (or rather within a single
tapleaf in the tapscript case).  As it stands, Bitcoin's signed data only
covers which input is being signed, and not the specific conditions are
being signed for.  So for example, if Alice and Bob are engaged in some
kind of multi-party protocol, and Alice wants to pre-sign a transaction
redeeming a UTXO but subject to the condition that a certain hash-preimage
is revealed, she might verify the Script template shows that the code path
to her public key enforces that the hash pre-image is revealed (using a
toolkit like miniscript can aid in this), and she might make her signature
feeling secure that it, if her signature is used, the required preimage
must be revealed on the blockchain.  But perhaps Bob has masquated Alice's
pubkey as his own, and maybe he has inserted a copy of Alice's pubkey into
a different path of the Script template.  Now Alice's signature can be
copied and used in this alternate path, allowing the UTXO to be redeemed
under circumstances that Alice didn't believe she was authorizing.  In
general, to protect herself, Alice needs to inspect the Script to see if
her pubkey occurs in any other branch.  Given that her pubkey, in
principle, could be derived from a computation rather that pushed directly
into the stack, it is arguably infeasible for Alice to perform the required
check in general.
I believe that it would be safer, and less surprising to users, to always
sign the CHECKSIG position by default.  This will automatically enforce
conditions with the signature in most cases, rather than requiring users to
proactively try to reason if CODESEPARATOR is required for protection
within their protocol or not, and risk having them leave it out for cost
savings when it ends up being required for security after all.
I do not believe signing the CHECKSIG position is an undue burden on those
signers who have no conditions they require enforcement for.  As it stands,
the tapscript proposal already requires the tapleaf_hash value under the
signature; this CHECKSIG position value is simply more of the same kind of
data.  In simple Script templates (e.g. those with only one CHECKSIG
operation) the signed position will be a fixed known value.  Complex Script
templates are precisely the situations where you want to be careful about
enforcement of conditions with your signature.
As a side benefit, we get to eliminate CODESEPARATOR, removing a fairly
awkward opcode from this script version.

@_date: 2019-11-27 16:32:51
@_author: Russell O'Connor 
@_subject: [bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY 
Thanks for this work Jeremy.
I know we've discussed this before, but I'll restate my concerns with
adding a new "global" state variable to the Script interpreter for tracking
whether the previous opcode was a push-data operation or not.  While it
isn't so hard to implement this in Bitcoin Core's Script interpreter,
adding a new global state variable adds that much more complexity to anyone
trying to formally model Script semantics.  Perhaps one can argue that
there is already (non-stack) state in Script, e.g. to deal with
CODESEPARATOR, so why not add more?  But I'd argue that we should avoid
making bad problems worse.
If we instead make the CHECKTEMPLATEVERIFY operation fail if it isn't
preceded by (or alternatively followed by) an appropriate sized
(canonical?) PUSHDATA constant, even in an unexecuted IF branch, then we
can model the Script semantics by considering the
PUSHDATA-CHECKTEMPLATEVERIFY pair as a single operation.  This allows
implementations to consider improper use of CHECKTEMPLATEVERIFY as a
parsing error (just as today unbalanced IF-ENDIF pairs can be modeled as a
parsing error, even though that isn't how it is implemented in Bitcoin
I admit we would lose your soft-fork upgrade path to reading values off the
stack; however, in my opinion, this is a reasonable tradeoff.  When we are
ready to add programmable covenants to Script, we'll do so by adding CAT
and operations to push transaction data right onto the stack, rather than
posting a preimage to this template hash.
Pleased to announce refinements to the BIP draft for OP_CHECKTEMPLATEVERIFY
