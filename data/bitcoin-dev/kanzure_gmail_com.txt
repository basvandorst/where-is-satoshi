
@_date: 2014-08-19 10:44:55
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Reconsidering github 
Smaller first step would be to mirror the git repository on
bitcoin.org, which is necessary anyway before switching primaries.
- Bryan
1 512 203 0507

@_date: 2014-10-22 17:35:38
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] side-chains & 2-way pegging (Re: is there 
[Unsolicited administrivia follows.]
You have been posting this in a bunch of places for a while now, at
least three times today by my count on other mediums. I also observed
negative karma scores associated with these posts. Maybe you could
consider toning down the message frequency? I think by now everyone
knows you want them to use your site. I also think that in the limit
that it would be inappropriate for /everyone/ to post all possible
research sites, or even vaguely topical discussion sites, for every
paper posted. Personally, I would much rather have discussions happen
on the mailing list anyway, although if I had a different opinion I
certainly hope I would still send this message.
Thank you.
- Bryan
1 512 203 0507

@_date: 2014-09-25 21:32:44
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] BIP43 Purpose code for voting pool HD 
One possible reason is that non-subscribed users aren't able to access
the file through sourceforge. The attachment through their web
interface is giving back HTTP 500.
see - Bryan
1 512 203 0507

@_date: 2015-08-07 13:35:29
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Fri, Aug 7, 2015 at 1:17 PM, jl2012 via bitcoin-dev <
Some arguments have floated around that even in the absence of "causing an
increase in the number of full nodes", that a reduction of the max block
size might be beneficial for other reasons, such as bandwidth saturation
benefits. Also less time spent validating transactions because of the fewer
- Bryan
1 512 203 0507

@_date: 2015-08-11 13:51:00
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Tue, Aug 11, 2015 at 1:46 PM, Michael Naber via bitcoin-dev
You don't need consensus on the lightning network because you are
using bitcoin consensus anyway. Commitment transactions are deep
enough in the blockchain history that removing that transaction from
the history is impractical. The remaining guarantees are ensured by
the properties of the scripts in the transaction. You don't need to
see all the transactions, but you do need to look at the transactions
you are given and draw conclusions based on the details to see whether
their commitments are valid or the setup wasn't broken.
- Bryan
1 512 203 0507

@_date: 2015-08-15 15:47:05
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
On Sat, Aug 15, 2015 at 3:36 PM, Milly Bitcoin via bitcoin-dev <
You may be misremembering; nobody has ever disagreed that you can fork a
source code repository. Perhaps you are thinking instead about the concerns
regarding "asymmetric" rule incompatibilities?
- Bryan
1 512 203 0507

@_date: 2015-08-19 19:21:06
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Separated bitcoin-consensus mailing list (was Re: 
FWIW, and I mentioned this opinion in  on IRC, but I am
perfectly fine with receiving everything through a single mailing list. I
used to read the Wikipedia firehose of recent edits because I thought
that's how you were supposed to use the site. Edits per second eventually
reached beyond any reasonable estimate of human capacity and then I
realized what was going on. Any sort of "glorious future" for bitcoin with
hundreds of millions of users will also see this problem for future
developers, even if only 0.1% of that population are money-interested
programmers then that's 100,000 programmers to work with. I would never
want to turn off this raw feed. Having said that, I am somewhat surprise
that nobody has taken to weekly summaries of research and development
activity. Summarizing recent work is a valuable task that others can engage
in just by reading the mailing list and aggregating multiple thoughts
together, similar to release notes. I was also expecting to see something
like "individual developer's summaries of things they have found
interesting over the past 30-90 days or past year" digging up arcane
details from the mailing list archives, or more infrequent summaries of the
other smaller batched review emails. Digest mode mailing list consumption
is often recommended to those who are uninterested in dealing with low
signal-to-noise, but I suspect that summarizing activity would be more
valuable for this community, especially for the different cognitive niches
that have developed.
- Bryan
1 512 203 0507

@_date: 2015-08-30 13:56:34
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Short review of previously-proposed exotic SIGHASH 
Here is a short review of previously-proposed and exotic SIGHASH types.
Similarly, petertodd has asked for a SIGHASH_DONT_SIGN_TXID before to
make OP_CODESEPARATOR more useful.
06:41 < lorenzoasr> maybe a SIGHASH_DOUBLE that signs INPUT[i] and
OUTPUT[i] and OUTPUT[i+1] could be very helpful
Some sighash types briefly proposed by petertodd in SIGHASH_SUM (for merging multiple payments)
And finally one from wumpus (
- Bryan
1 512 203 0507

@_date: 2015-12-07 00:50:23
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Some transcripts from the Scaling Bitcoin workshops 
Hey while I was listening to the talks I also typed most of the words down.
Here are some talks from the Hong Kong workshop:
Also, here are some talks from the Montreal workshop:
These are not always exact transcripts because I am typing while I am
listening, thus there are mistakes including typos and listening errors, so
please keep this discrepancy in mind between what's said and what's typed.
- Bryan
1 512 203 0507

@_date: 2015-12-07 16:54:07
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
One of the interesting take-aways from the workshops for me has been
that there is a large discrepancy between what developers are doing
and what's more widely known. When I was doing initial research and
work for my keynote at the Montreal conference (
 -- an
attempt at being exhaustive, prior to seeing the workshop proposals ),
what I was most surprised by was the discrepancy between what we think
is being talked about versus what has been emphasized or socially
processed (lots of proposals appear in text, but review efforts are
sometimes "hidden" in corners of github pull request comments, for
example). As another example, the libsecp256k1 testing work reached a
level unseen except perhaps in the aerospace industry, but these sorts
of details are not apparent if you are reading bitcoin-dev archives.
It is very hard to listen to all ideas and find great ideas.
Sometimes, our time can be almost completely exhausted by evaluating
inefficient proposals, so it's not surprising that rough consensus
building could take time. I suspect we will see consensus moving in
positive directions around the proposals you have highlighted.
When Satoshi originally released the Bitcoin whitepaper, practically
everyone-- somehow with the exception of Hal Finney-- didn't look,
because the costs of evaluating cryptographic system proposals is so
high and everyone was jaded and burned out for the past umpteen
decades. (I have IRC logs from January 10th 2009 where I immediately
dismissed Bitcoin after I had seen its announcement on the
p2pfoundation mailing list, perhaps in retrospect I should not let
family tragedy so greatly impact my evaluation of proposals...). It's
hard to evaluate these proposals. Sometimes it may feel like random
proposals are review-resistant, or designed to burn our time up. But I
think this is more reflective of the simple fact that consensus takes
effort, and it's hard work, and this is to be expected in this sort of
system design.
Your email contains a good summary of recent scaling progress and of
efforts presented at the Hong Kong workshop. I like summaries. I have
previously recommended making more summaries and posting them to the
mailing list. In general, it would be good if developers were to write
summaries of recent work and efforts and post them to the bitcoin-dev
mailing list. BIP drafts are excellent. Long-term proposals are
excellent. Short-term coordination happens over IRC, and that makes
sense to me. But I would point out that many of the developments even
from, say, the Montreal workshop were notably absent from the mailing
list. Unless someone was paying close attention, they wouldn't have
noticed some of those efforts which, in some cases, haven't been
mentioned since. I suspect most of this is a matter of attention,
review and keeping track of loose ends, which can be admittedly
Short (or even long) summaries in emails are helpful because they
increase the ability of the community to coordinate and figure out
what's going on. Often I will write an email that summarizes some
content simply because I estimate that I am going to forget the
details in the near future, and if I am going to forget them then it
seems likely that others might.... This creates a broad base of
proposals and content to build from when we're doing development work
in the future, making for a much richer community as a consequence.
The contributions from the scalingbitcoin.org workshops are a welcome
addition, and the proposal outlined in the above email contains a good
summary of recent progress. We need more of this sort of synthesis,
we're richer for it. I am excitedly looking forward to the impending
onslaught of Bitcoin progress.
- Bryan
1 512 203 0507

@_date: 2015-12-08 10:45:32
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Scaling by Partitioning 
At first glance this proposal seems most similar to the sharding proposals:
 (committee approach)
Offloading work to the client for spends has in the past been a
well-received concept, such as the linearized coin history idea.
- Bryan
1 512 203 0507

@_date: 2015-12-09 22:31:42
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Scaling by Partitioning 
Mining equipment isn't for transaction verification. The mining
equipment is used to work on Proof-of-Work. Thanks.
Unfortunately Bitcoin does not work like those centralized systems; it
should not be surprising that a system focused so much on
decentralized and independent verification would have developers
working on so many non-bandwidth scaling solutions. These other
proposals seek to preserve existing properties of Bitcoin (such as
cheap verification, low-bandwidth) while also increasing the amount of
activity that can enjoy the decentralized fruits of Proof-of-Work
labor. But not helpful to assume this can only look like Visa or any
database on a cluster etc...
I don't suppose I could tempt you with probabilistically checkable
proofs, could I? These verify in milliseconds, grow sublinear in size
of the total data, but have no near-term proposal available yet.
Perhaps instead of failure-to-scale you mean "failure to apply
traditional scaling has already failed", which shouldn't be so
surprising given the different security model that Bitcoin operates
see the following recent text,
Bitcoin is P2P electronic cash that is valuable over legacy systems
because of the monetary autonomy it brings to its users through
decentralization. Bitcoin seeks to address the root problem with
conventional currency: all the trust that's required to make it work--

@_date: 2015-12-10 02:28:49
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
nhashtype proposal:
summary email about sighash type proposals (which IIRC you saw, so
leaving this link here is mainly for the benefit of others):
- Bryan
1 512 203 0507

@_date: 2015-12-19 09:48:09
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] On the security of softforks 
On Fri, Dec 18, 2015 at 6:18 AM, Peter Todd via bitcoin-dev <
Er, this sounds like something that should go into bip99. Right?
- Bryan
1 512 203 0507

@_date: 2015-12-20 12:17:30
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Increasing the blocksize as a (generalized) 
This seems conceptually similar to "extension blocks":
"Extended blocks" are also mentioned over here too:
- Bryan
1 512 203 0507

@_date: 2015-12-26 18:13:40
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
On Sat, Dec 26, 2015 at 5:15 PM, Justus Ranvier via bitcoin-dev <
I think you'll find that there hasn't been stalling regarding an
uncontroversial hard-fork deployment. You might be confusing an
uncontroversial hard-fork decision instead with how developers have brought
up many issues about various (hard-forking) block size proposals.... I
suspect this is what you're intending to mention instead, given your
mention of "capacity emergencies" and also the subject line.
The uncontroversial hard-fork proposals from 6 months ago were mostly along
the lines of jtimon's proposals, which were not about capacity. (Although,
I should say "almost entirely uncontroversial"-- obviously has been some
minor (and in my opinion, entirely solvable) disagreement regarding
prioritization of deploying a jtimon's uncontroversial hard-fork idea I
guess, seeing as how it has not yet happened.)
There wasn't 6 months of "stonewalling" or "denial" about an
uncontroversial hard-fork proposal. There has been extensive discussion
regarding the controversial (flawed?) properties of other (block size)
proposals. But that's something else. Much of this has been rehashed ad
nauseum on this mailing list already...  thankfully I think your future
emails could be improved and made more useful if you were to read the
mailing list archives, try to employ more careful reasoning, etc. Thanks.
("Capacity emergency" is too ambiguous in this context because of the
competing concerns and tradeoffs regarding transaction rate capacity
exhaustion vs. p2p low-bandwidth node bandwidth exhaustion.)
- Bryan
1 512 203 0507

@_date: 2015-12-30 17:10:05
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] fork types (Re: An implementation of BIP102 as a 
I was drafting an email for a new thread with some links about this topic,
instead I'll just send this as a reply now that we are writing down fork
auxiliary blocks and evil soft-forks or forced soft-forks:
soft-fork block size increase using extension blocks:
generalized soft-forks:
bip102 forced soft-fork:
extension blocks were also discussed in this interview:
.... also there was something about a "soft-hard fork".
some discussion from today re: origin of the term evil fork, evil
soft-fork, forced soft-fork:
some much older discussion about extension blocks and sidechains:
some discussion about "generalized soft-forks" and extension blocks and
evil soft-forks:
some discussion about evil forks and evil soft-forks and extension blocks:
segwit soft-fork makes use of a similar idea:
Note: I am taking the term "forced soft-fork" from petertodd; it's pretty
much the same thing as "evil fork" in every way but intent.
This is an x-post from
- Bryan
1 512 203 0507

@_date: 2015-02-14 13:29:16
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
Correct. However, those maintenance costs absolutely do justify working
towards formal proofs of correctness for the existing implementation. These
plans are no secret and are publicly discussed, but I think it would be
instrumental to outsiders if the correctness plans and ongoing progress
could be mentioned whenever a warning is made about unjustified and
dangerous Bitcoin consensus rewrite attempts.
- Bryan
1 512 203 0507

@_date: 2015-02-19 08:03:48
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] On Rewriting Bitcoin (was Re: 
On Wed, Feb 18, 2015 at 11:22 PM, Tamas Blummer First, I strongly disagree with voting here for reasons that I hope others
will elaborate on. Second, I think that squeezing all possible language
bindings into a project is also unproductive. What is it that the webkit
people did for this? I think they had gobject bindings, and then all of the
languages have their own gobject bridge to take advantage of that.
Naturally the downside here is that gobject means you have a gtk
dependency. A similar solution would be interesting and worth exploring,
though, especially if something similar without gtk exists.
- Bryan
1 512 203 0507

@_date: 2015-02-22 08:25:03
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] alternate proposal opt-in miner takes 
Scenario: Users are using some system in a way that the system was not
intended to be used. Let me also further constrain the scenario and
suggest that the function (pretend that means instantaneous confirmed
transactions) that the user wants is impossible. So in this scenario,
is it your job as some developer to change the system to do something
it wasn't designed to do? I mean, you certainly weren't the one
telling them they should accept zero confirmation transactions. Also,
I make no claims as to whether this scenario maps accurately to the
current topic.
- Bryan
1 512 203 0507

@_date: 2015-07-30 10:24:07
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
On Thu, Jul 30, 2015 at 9:52 AM, Thomas Zander via bitcoin-dev <
Ah, well that's simple. Because any decentralized system is going to have
high transaction costs and scarcity anyway. So far the only mechanism we
know for how to do this is something like bitcoin. As a centralized system,
bitcoin is already strongly outcompeted by many, many other designs, so
that shouldn't be very surprising I think.
- Bryan
1 512 203 0507

@_date: 2015-07-30 11:36:29
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Block size following technological growth 
That's an interesting claim; so suppose you're living in a future where
transactions are summarizing millions or billions of other daily
transactions, possibly with merkle hashes. You think that because a user
can't individually broadcast his own personal transaction, that the user
would not be interested in verifying the presence of a summarizing
transaction in the blockchain? I'm just curious if you could elaborate on
this effect. Why would I need to see my individual transactions on the
network, but not see aggregate transactions that include my own?
- Bryan
1 512 203 0507

@_date: 2015-07-31 09:52:56
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Block size following technological growth 
Well, even in a centralized scheme you can have more users, more nodes and
more miners. Just having more does not mean that the system isn't
centralized, for example we can point to many centralized services such as
PayPal that have trillions of users.
Nobody claimed that moving to smaller blocks would "solve everything wrong
with Bitcoin".
You cannot "destroy Bitcoin through centralization" by adjusting a single
Why not? That's exactly the sort of change that would be useful to do so,
in tandem with some forms of campaigning.
I am confused here; is that idea operating under an assumption (or rule)
like "we shouldn't count aggregate transactions as representing multiple
other transactions from other users" or something? I have seen this idea in
a few places, and it would be useful to get a fix on where it's coming
from. Does this belief extend to P2SH and multisig...?
- Bryan
1 512 203 0507

@_date: 2015-07-31 10:27:45
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
On Thu, Jul 30, 2015 at 10:55 AM, Gavin Andresen Specifically I was replying to the argument that went like "the bitcoin
system, in any of its futures with a bunch of non-zero transaction fees, is
going to be replaced by a decentralized system that can commit to
transactions that have lower or zero transaction fees, and which also
otherwise provides the same benefits as bitcoin". My reply was that
decentralized systems are going to have physical limitations that force
their solutions to look certain ways, which would do something like, for
example, explain why there were "$10 fees" in that original scenario in the
first place. Your reply does not seem to share this context?
Also, I don't mean to start a discussion about internet architecture, but
ISP peering agreements do not look particularly like a cryptographic,
decentralized system to me at all. I agree that the internet needs better
architecture. I would call the IETF about this but I think Greg would be
the one to answer or something :-). Would be sorta redundant, heh.
- Bryan
1 512 203 0507

@_date: 2015-06-12 14:13:35
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Various block size proposals 
Here are some proposals regarding the minimum block size questions, as well
as other related scalability issues.
Dynamic block size limit controller (maaku)
 at lists.sourceforge.net/msg07599.html
Re: dynamic block size limit controller (gmaxwell)
 at lists.sourceforge.net/msg07620.html
Various other gmaxwell-relayed ideas
Increasing the max block size using a soft-fork (Tier Nolan)
 at lists.sourceforge.net/msg07927.html
Elastic block cap with rollover penalties (Meni Rosenfield)
worked example
section 6.2.3 of rollover transaction fees Variable mining effort (gmaxwell)
BIP100 Soft-fork limit of 2 MB (jgarzik)
Transaction fee targeting
Difficulty target scaling
Annual 50% max block size increase
Various algorithmic adjustment proposals
Average over last 144 blocks
Extension blocks (Adam Back) (why would he burn this idea for something so
 at lists.sourceforge.net/msg07937.html
 at lists.sourceforge.net/msg08005.html
Voting by paying to an address (note: vote censorship makes this
impractical, among other reasons)
 at lists.sourceforge.net/msg02325.html
Vote by paying fees
 at lists.sourceforge.net/msg08164.html
 at lists.sourceforge.net/msg02323.html
Double the max block size at each block reward halving
Reducing the block rate instead of increasing the maximum block size
(Sergio Lerner)
 at lists.sourceforge.net/msg07663.html
Decrease block interval
Increase default soft block size limit in Bitcoin Core
Consider the size of the utxo set when determining max block size (note
that utxo depth cannot have consensus)
Reduce and decrease the max block size
Change the value of MAX_BLOCK_SIZE in Bitcoin Core
Problems with floating block size limits (petertodd)
Develop other ways to support high transaction volumes (gavinandresen)
Simplified payment verification
Lightning network
Payment channels
Tree chains
 at lists.sourceforge.net/msg04388.html
fedpeg + SPV
Known missing:
- old bitcoin-development proposals
- old bitcointalk proposals
- anything unique from IRC
On a related note, the other day I found that reading all of the -wizards
logs regarding sidechains only takes 2 hours. So... that's something. YMMV.
- Bryan
1 512 203 0507

@_date: 2015-06-15 16:56:05
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
I worry that if this is the level of care you take with reading and
(mis)interpreting Adam's messages, that you might not be taking extreme
care with evaluating consensus changes, even while tired or sleeping. I
encourage you to evaluate both messages and source code more carefully,
especially in the world of bitcoin. However, this goes for everyone and not
just you. Specifically, when Adam mentioned your conversations with
non-technical people, he did not mean "Mike has talked with people who have
possibly not made pull requests to Bitcoin Core, so therefore Mike is a
non-programmer". Communication is difficult and I can understand that, but
we really have to be more careful when evaluating each other's messages;
technical miscommunication can be catastrophic in this context. On the
topic of whether you are a programmer, I suspect that ever since you built
CIA.vc we have all known you're a programmer, Mike.
- Bryan
1 512 203 0507

@_date: 2015-06-18 09:33:24
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
Well hold on, his concerns are real and he seems perfectly calm to me and
others apparently.
Nobody is worried about Gavin or anyone else making commits to git
repositories. You'll notice that this wasn't even mentioned in the original
email you were replying to. Instead, the email was talking about commit
access, which is a property of GitHub's repositories.
So why did I say it? Because it's consistent with what I've always said:
(That's not a good reason.)
you cannot run a codebase like Wikipedia
Wikipedia is a top-down centralized authority-based hierarchy. That's
pretty close to what you're proposing. That's what everyone else is
disagreeing with. You seem to have switched your position mid-flight...?
This is not a radical position. That's how nearly all coding projects work.
There are a number of reasons why that perspective is broken; throughout
our conversations others have repeatedly reminded you (such as in -wizards)
that forking an open-source repository is not at all like a hard-fork of
the blockchain. Anyone can be glorious leader of any repository they want,
that's how open-source works. However, it's critical that bitcoin users are
never convinced to trust BDFLs or anything else that can be compromised.
Should all bitcoin users suddenly start using software with BDFLs, even
multiple implementations with separate BDFLs, then those users can be
trivially compromised through their trust in those individuals and projects.
The alternative is that every developer and every user is personally
responsible for self-validation of the rules, checking for correctness and
validity. Happy coincidence that this seems to match the strategy of
operating the bitcoin network itself, which is to run a node that sees
everything and validates all the transactions. Anyone is able to find an
error in logic or flaw in the system rules, and they should make it known
as widely as possible so that others may evaluate the evidence and consider
which solutions preserve the important properties of the system. This is
not a matter of majorities or minorities; these arguments should be true
for anyone independent of who or what they are, or what level of
unpopularity they may have.
Anything other than this is somewhat radical, and I am confused as to why
others have been talking about "developer consensus". I suspect that the
reason why they have been saying developer consensus is because they are
talking about the Bitcoin Core project on GitHub at the moment. But the
topic switched to contentious hard-forks already, which is not a topic of
repositories but a topic of the blockchain and network; and in the context
of contentious hard-forks it is clear why everyone individually must
evaluate the rules and decide whether they the software is correct, or
whether changes can trivially cause catastrophic broken hard-forks. These
lines of reasoning should be true for everyone, and not merely true for one
person but not another. Users, companies and developers must be aware of
this, even though it's different from their usual expectations of how
systems operate and are maintained. And it is important to be careful to
not misconstrue this to others because it is entirely possible to
unintentionally convince others that traditional and centralized models are
safely applicable here.
I realise some people think this anti-process leads to better decision
Well, if you're talking about the recent disputes regarding a certain patch
to hard-fork the blockchain, a decision to not include such a patch seems
like the very definition of a decision.
Gavin and I say - there is a process, and that process is a hard fork of
I doubt that other bitcoin software maintainers would agree with that sort
of toxic reasoning; contentious hard-forks are basically a weapon of war
that you can use against any other collaborator on any bitcoin project. Why
would anyone want to collaborate on such a hostile project? How would they
even trust each other?
- Bryan
1 512 203 0507

@_date: 2015-06-19 06:35:28
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
Developers
Someone might find it more convenient to consume that in the form of text
- Bryan
1 512 203 0507

@_date: 2015-06-24 19:11:53
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] BIP Process and Votes 
I think that statement is too weak. Users are all personally responsible
for evaluating all rules for themselves. Many have chosen and will continue
to choose to just keep an ear out for rule changes that they may be
interested in using. Ever user should be educated on this topic...
otherwise there are too many principal agent problems, even with the
ability to "fire" developers (a.k.a "use different software"). It's similar
to the reasons why it's important to see all the transactions on the
- Bryan
1 512 203 0507

@_date: 2015-03-12 14:08:43
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Electrum 2.0 has been tagged 
I haven't looked at the existing sweep implementations, but it would
be unfortunate if sweep functions were not available that create at
least the same number of keys, or possibly more, for the purposes of
sweeping. I suppose there are different levels of emergency, where
perhaps you want to sweep all at once in a single transaction and lose
out on (already nebulous) privacy benefits. I say nebulous because
broadcasting a bunch of transactions all at the same time during the
sweep can compromise privacy even when the transactions have no common
ancestor outputs.
- Bryan
1 512 203 0507

@_date: 2015-05-06 19:07:41
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Block Size Increase 
Well, there has been significant public discussion in on irc.freenode.net which is available in public logs, specifically
about why increasing the max block size is kicking the can down the
road while possibly compromising blockchain security. There were many
excellent objections that were raised that, sadly, I see are not
referenced at all in the recent media blitz. Frankly I can't help but
feel that if contributions, like those from  have
been ignored in lieu of technical analysis, and the absence of
discussion on this mailing list, that I feel perhaps there are other
subtle and extremely important technical details that are completely
absent from this--and other-- proposals. I have some rather general
thoughts to offer.
Secured decentralization is the most important and most interesting
property of bitcoin. Everything else is rather trivial and could be
achieved millions of times more efficiently with conventional
technology. Our technical work should be informed by the technical
nature of the system we have constructed.
I suspect that as bitcoin continues to grow in all dimensions and
metrics, that we will see an unending wave of those who are excited by
the idea of Something Different in the face of archaic, crumbling
software and procedures in the rest of the financial world. Money has
found its way into every aspect of human life. There's no doubt in my
mind that bitcoin will always see the most extreme campaigns and the
most extreme misunderstandings. Like moths to a flame or water in the
desert, almost everyone is excited by ANY status quo change
whatsoever. This is something that we have to be vigilante about,
because their excitement is motivation to do excellent work, not
simply any work. For some who are excited about general status quo
changes that bitcoin represents, they may not mind if bitcoin
decentralization disappears and is replaced with just a handful of
centralized nodes. Whereas for development purposes we must hold
ourselves to extremely high standards before proposing changes,
especially to the public, that have the potential to be unsafe and
economically unsafe. We have examples from NASA about how to engineer
extremely fault tolerant systems, and we have examples from Linux
about how to have high standards in open-source projects. Safety is
absolutely critical, even in the face of seemingly irrational
excuberance of others who want to scale to trillions of daily coffee
transactions individually stored forever in the blockchain.
When designing bitcoin or even some other system, an important design
target is what the system should be capable of. How many transactions
should the system perform? What is the correct number of transactions
for a healthy, modern civilization to perform every day? And how fast
should that (not) grow? Should we allow for 2 billion trillion coffee
transactions every day, or what about 100 trillion transactions per
second? I suspect that these sorts of questions are entirely
unanswerable and boring. So in the absence of technical targets to
reach during the design phase, I suspect that Jeff Garzik was right
when he pointed out a few months ago that bitcoin is good at
settlement and clearing. There are many potential technical solutions
for aggregating millions (trillions?) of transactions into tiny
bundles. As a small proof-of-concept, imagine two parties sending
transactions back and forth 100 million times. Instead of recording
every transaction, you could record the start state and the end state,
and end up with two transactions or less. That's a 100 million fold,
without modifying max block size and without potentially compromising
secured decentralization.
The MIT group should listen up and get to work figuring out how to
measure decentralization and its security :-). Maybe we should be
collectively pestering Andrew Miller to do this, too. No pressure,
dude. Getting this measurement right would be really beneficial
because we would have a more academic and technical understanding to
work with. I would also characterize this as high priority next to the
"formally verified correctness proofs for Script and
Also, I think that getting this out in the open on this mailing list
is an excellent step forward.
- Bryan
1 512 203 0507

@_date: 2015-05-07 09:18:17
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Block Size Increase 
Hmm, git repositories don't quite work like that. Instead, you should
imagine everyone having a local copy of the git repository. Each
developer synchronizes their git repository with other developers.
They merge changes from specific remote branches that they have
received. Each developer has their own branch and each developer is
the "single decision maker" for the artifact that they compile.
- Bryan
1 512 203 0507

@_date: 2015-05-08 11:55:42
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Block contents can be grinded much faster than hashgrinding and
mining. There is a significant run-away effect there, and it also
works in the gradual sense as a miner probabilistically mines large
blocks that get averaged into that 2016 median block size computation.
At least this proposal would be a slower way of pushing out miners and
network participants that can't handle 100 GB blocks immediately..  As
the size of the blocks are increased, low-end hardware participants
have to fall off the network because they no longer meet the minimum
performance requirements. Adjustment might become severely mismatched
with general economic trends in data storage device development or
availability or even current-market-saturation of said storage
devices. With the assistance of transaction stuffing or grinding, that
2016 block median metric can be gamed to increase faster than other
participants can keep up with or, perhaps worse, in a way that was
unintended by developers yet known to be a failure mode. These are
just some issues to keep and mind and consider.
- Bryan
1 512 203 0507

@_date: 2015-05-27 21:34:28
@_author: Bryan Bishop 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
I am a huge fan of do-it-yourself at-home ASIC manufacturing. The original
4004 and earlier devices are within the scope of what could be accomplished
in a home environment. The homecmos project is an interesting glimpse at
these possibilities. Relevant-scale mining will most likely never be an
option for home manufacturing, but bitcoin wallets and other devices can
definitely be etched by hand or using maskless projector lithography.
Here's what the homecmos group was up to:
LCD projection lithography:
DMD lithography:
There's actually a method of doing this with conventional camera roll film:
- Bryan
1 512 203 0507

@_date: 2015-11-08 11:19:15
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] summarising security assumptions (re cost metrics) 
Gavin, could you please provide some clarity around the definition and
meaning of "key-holder [decentralization]"? Is this about the absolute
number of key-holders? or rather about the number of transactions (per unit
time?) that key-holders make? Both/other?
Anyone can generate a private key, and anyone can sign a transaction
spending to a new commitment. Child-pays-for-parent could be used when
transaction fees are too high. Perhaps more interesting would be something
like lightning network payment channels, where only the commitment
transaction needs to be in the blockchain history; does that count as
key-holder decentralization at all?
Also, consider the following scenario. Suppose there's a bunch of
merge-mined sidechains that are mainnet BTC-pegged, and these sidechains
are accessible by the lightning network protocol (multi-chain payments).
Suppose also that on the different sidechains there are different
transaction fee trends because of various technical differences underlying
consensus or a different blockchain implementation (who knows). When
someone routes payments to one of those different sidechains, because UTXOs
could be cheaper over there due to different fee pressures, ... would that
count as key-holder decentralization? Some of this scenario is described
here, although not in more detail:
Previously there has been the suggestion to use BTC-pegged merge-mined
chains to handle excess transaction demand:
I notice that in the Poon file there is a concern regarding "only 10 key
holders", but how does that scenario really work? I think the actual
scenario they mean to describe is "there's always a transaction backlog
where the fees are so high that lower fee transactions can never get
confirmations". So, more specifically, the scenario would have to be
"lightning network exists and is working, and no lightning node can ever
route enough different payments to commit to the blockchain under any
circumstance". How would that be possible? Wouldn't most participants
prefer the relatively instantaneous transactions of lightning, even if they
can afford extremely high fees? Seems like the settlements have all
necessary reason to actually happen, don't know what your concern is,
please send help.
I don't mean to put words in anyone's mouth, everything above is mostly
asking for clarification around definitions. Some of these questions are
repeats from:
Thank you.
- Bryan
1 512 203 0507

@_date: 2015-11-24 15:51:12
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] OP_CHECKWILDCARDSIGVERIFY or "Wildcard Inputs" or 
Some (minor) discussion of this idea in -wizards earlier today starting
near near "09:50" (apologies for having no anchor links):
- Bryan
1 512 203 0507

@_date: 2015-10-14 10:37:14
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Lightning Network's effect on miner fees 
Additionally, lightning network hot wallets are not an ideal place to
store large quantities of BTC and users that don't expect to be
actively using LN should in general prefer confirmed UTXOs for
long-term cold storage. So far the guess that I have seen floating
around is that LN usage will at first be restricted to very tiny
amounts of BTC in tiny hot wallets, since nobody is particularly
interested in running large hot wallets.
- Bryan
1 512 203 0507

@_date: 2015-10-14 13:12:27
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
Taking reward compensation back by fraud proofs is not enough to fix
the problems associated with double spending (such as, everyone has to
wait for the "real" confirmations instead of the "possibly
double-spend" confirmations). Some of this was discussed in -wizards
For a system based entirely on fraud proofs and threat of fraud
proofs, see fidelity-bonded ledgers:
- Bryan
1 512 203 0507

@_date: 2015-09-01 08:50:17
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Open Block Chain Licence, BIP[xxxx] Draft 
Here is a previous discussion of this topic (2012):
- Bryan
1 512 203 0507

@_date: 2015-09-03 19:02:10
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Multi-chain payment channel nodes 
Here is a brief overview of a way to use something like the lightning
network, or another multi-hop payment channel network, to handle more
transactions per second.
These ideas were mentioned yesterday in  and my email
does not significantly elaborate on any of it (search for
Mailing list discussion of this can be found at [6] and on the forum at [7].
Payment channel network nodes could operate on multiple chains or
ledgers, especially if those ledgers are 2-way-peg compatible with
BTC. Payment network users may have a variety of different preferences
about security models or fees or any other number of system
properties, and this method can be more accomodating than only
offering mainnet UTXOs.
During the IRC monologue I was using the word "hub" and "cross-chain
hubs" to describe a payment channel network node. Rusty Russell later
brought to my attention a good argument from Joseph Poon to prefer to
call them nodes, namely that "hub" implies centralization which isn't
really necessary to implicate in these designs. So I'll stick with
Vague requirements and scenario speculation
- bip70-like payment-channel-compatible wallet-to-wallet communication
protocol; useful for sender and receiver to negotiate how payment
should be routed over the payment channel network.
- assume existence of other ledgers with working 2-way-peg.
- lightning network[1], amiko pay[2], or some other payment channel
network with multi-hop payment routing
- (at least some) payment channel nodes which access more than one
blockchain or ledger system
- can be more than two chains or ledgers that the node opens channels
on or operate on (octoledger nodes?)
- node can eventually setup 2-way-pegs through moxiebox code embedded
in some opcode for a specification of an alternative federated ledger
(just kidding, there be dragons here)
Implication: Chain or ledger UTXO ambivalence
The sender (receiver) doesn't need to be concerned with which chain
the receiver (sender) wishes to transact with, as long as both parties
have wallets that can communicate with each other for fee negotiation
while payment channel routing happens.
Implication: UTXO preferences informed by fee pressures
An earlier identified implication by others has been that transaction
fee trends may influence when payment channel users choose to settle
on the main chain, because fees may be too high to make the tradeoffs
worthwhile for the user.
Transaction fee pressure on the bitcoin mainnet chain may influence
receivers, otherwise busy negotiating their payment channel payments,
to negotiate receipt of their UTXOs on alternative chains which might
be charging lower fees. Users that wish to settle to a ledger for a
lower fee can do so without paying for main chain transaction
(Concerns about market exchange rate of BTC even with the presence of
2-way-pegs could be alleviated by multi-chain nodes that practice
arbitrage. However, perhaps the financial markets will not bother
assigning different values to BTC-compatible UTXOs on different
sidechains? High mainnet fees may be reflected in market price
differences, maybe....)
Minor lightning network protocol change
Add chain parameter to onion routing protocol message. Receipt of this
message was acknowledged by Rusty Russell yesterday. However, this
change alone may be insufficient to enable this described usage. Also
while I hope that onion routing continues to be the direction there's
no guarantee of course.
Other: Centralized ledgers
Centralized ledger operators, such as companies running spot
exchanges, could run payment channel nodes, allowing their users to
deposit and withdraw funds "immediately" subject to whether the
service provider is operating anything resembling a hot wallet. A
centralized ledger operator could be considered a valid multi-chain
destination in the above-mentioned imaginary lightning-compatible
payment protocol. Payment channel network programmers should not be
burdened with a thousand different standards for this ability, and
instead there should be an attempt at general compatibility, although
trustless implementations should be preferred if available.
Luke-Jr mentions that the same (currently imaginary) payment protocol
could also provide for user-to-user transfers on the same centralized
services, skipping the payment channels entirely.
Here are some things that I have been meaning to look at, but haven't looked at:
- can we do probabilistic payments[3][4] over payment channels? does
it require all payments to be probabilistic payments?
- is lightning network + multi-chain compatible with terminating on a
chain like zerocash? or inside coinjoin/coinshuffle schemes? mixing
- are payment channel networks compatible with confidential
transactions[5], and what about in the multi-chain regime?
- should work if 2-way-peg between multiple assets on same chain, like
in elements alpha?
[1] [2] [3] [4] [5] [6] [7] - Bryan
1 512 203 0507

@_date: 2015-09-03 19:43:15
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] [BIP/Draft] BIP Acceptance Process 
Some quick comments:
I have some objects that I am not ready to put into words, but I do
think there are easily some major objections to committee design. If I
vanish and never respond with my objections, perhaps there's an IETF
RFC about this already....
Something that may mitigate my possible objections would be some
mandatory requirement about ecosystem echo-chambers making many
attempts and efforts at steelman representations of alternative
viewpoints. Understanding objections at a fundamental level, enough to
make strong steelman statements, is very important to ensure that the
competing opinions are not censored from consideration. Pathological
integration and internalization of these steelman arguments can be
very useful, even if the process looks unusual.
Your process does not have to replace any particular BIP process
as-is, but rather could be an alternative that proceeds on its own
perhaps indefinitely without replacement. I don't think too many BIP
processes are necessarily incompatible except by namespace collision.
- Bryan
1 512 203 0507

@_date: 2015-09-23 11:07:08
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Weak block thoughts... 
Here are some other "weak blocks" and "near blocks" proposals or mentions:
more recently:
- Bryan
1 512 203 0507

@_date: 2016-08-02 02:20:08
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] A conversation with Dan Boneh (2016-08-01) 
Some Bitcoin developers and miners went to visit with Dan Boneh at Stanford
earlier today, and I thought I would share what we talked about.
Topics discussed include elliptic curve crypto, ECDSA, Schnorr signatures,
signature aggregation, BLS signature schemes, pairing crypto, group
signatures, block-level signature aggregation, transaction-level signature
aggregation, post-quantum crypto, quantum mining ASICs, provable solvency
schemes, scrypt password hashing, and balloon hashing.
(I would include the text directly but it's nearly 60 kilobytes in size and
past the point where I am presently comfortable with gunking up other
- Bryan
1 512 203 0507

@_date: 2016-08-03 13:41:27
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Mimblewimble: non-interactive coinjoin, 
Someone dropped this document in -wizards the other day:
Some commentary:
- Bryan
1 512 203 0507

@_date: 2016-08-17 13:36:38
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Hardware Wallet Standard 
On Tue, Aug 16, 2016 at 7:14 PM, Peter Todd via bitcoin-dev <
"Welcome to my threat model."
In multisig scenarios, there must be a different "trust root" for each key.
For example, storing two private keys next to each other on the same web
server is broken because if one key is compromised it is infinitely trivial
to compromise the second key. Using multiple web servers is also broken if
the two servers are controlled by the same AWS keys or same "help me get my
servers back" support email request to whatever single sign-on service is
used. In some cases, it can be better to write software such that
transaction data is served at a particular location, and another
security-critical step is responsible for downloading that data from the
first machine, rather than the first computer directly pushing (with
authentication credentials in place for the attacker to compromise) the
data to the second computer.
I recommend using hardware security modules (HSMs). It's important to have
a public, reviewed bitcoin standard for hardware wallets, especially HSMs.
I expect this is something that the entire industry has a tremendous
interest in following and contributing to, which could even lead to
additional resources contributed (or at the very least, more detailed
requirements) towards libconsensus work.
Instead of signing any bitcoin transaction that the hardware wallet is
given, the hardware should be responsible for running bitcoin validation
rules and business logic, which I recommend for everyone, not only
businesses. Without running business logic and bitcoin validation rules,
the actual bitcoin history on the blockchain could be a very different
reality from what the hardware thinks is happening. Using a different
out-of-band communication channel, the hardware could query for information
from another database in another trust root, which would be useful for
business logic to validate against.
As for a screen, I consider that somewhat limited because you only get text
output (and I don't know if I can reasonably suggest QR codes here). With a
screen, you are limited to text output, which can compromise privacy of the
device's operations and info about the wallet owner. An alternative would
be to have a dedicated port that is responsibly only for sending out data
encrypted to the key of the wallet owner, to report information such as
whatever the hardware's transaction planner has decided, or to report about
the state of the device, state of the bitcoin validation rules, or any
accounting details, etc. Additionally, even a signed transaction should be
encrypted to the key of the device owner because a signed transaction can
be harmless as long as the owner still has the ability to control whether
the signed transaction is broadcasted to the network. It's "separation of
concerns" for transaction signing and decrypting a signed transaction
should be unrelated and uncoupled.
Also I am eager to see what the community proposes regarding signed and
authenticated payment requests.
((insert here general promotional statement regarding the value of reusable
checklists used during every signing ritual ceremony))
- Bryan
1 512 203 0507

@_date: 2016-02-04 12:00:49
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Hardfork bit BIP 
Are there any plans written down anywhere about the "hastily imposed
checkpoint" scenario? As far as I know, we would have to check-point on
both blockchains because of the way that hard-forks work (creating two
separate chains and/or networks). Nothing about this should be an
"emergency", we have all the time in the world to prepare a safe and
responsible way to upgrade the network without unilaterally
declaring obsolescence.
- Bryan
1 512 203 0507

@_date: 2016-02-06 11:34:02
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Gavin: A Response to Your Forking BIP 
Thank you for the emails. Bitcoin Core has been working with the Bitcoin
ecosystem on developing and now testing a new capacity increasing feature
called segregated witness (segwit). Segregated witness is a voluntary,
mutually backwards-compatible capacity upgrade for the Bitcoin system.
Many, many hundreds of millions of dollars of Bitcoin value have flowed
through soft-forked upgrades to the Bitcoin system, representing upgrades
from across the entire ecosystem and the entire Bitcoin network, over
multiple years including BIP 12, BIP 16, BIP 17, BIP 30, BIP 34, BIP 42,
BIP 62, BIP 65, BIP 66, etc. So that?s the context from which I have been
approaching your hard-fork ideas for the past year.
Benefits of segregated witness
Ecosystem buy-in and support for segregated witness continues to grow:
There is also a segwit testnet which everyone is encouraged to investigate
and develop against-- companies love them some testing, after all:
A plan for Bitcoin Core capacity increases was put forward and can be found
With respect, the question should not be "is 28 days enough time for anyone
to roll out new binaries", it's instead a question of "how long does it
take someone to agree to upgrade to these new incompatible rules".
If Bitcoin users don't want to upgrade to incompatible rules right now, why
would they agree when 10% of the hashpower is setting some flag in a block?
Why would they change their minds at 20%? 90%? I am not saying here that
hard-forks should never be attempted, although we need as an ecosystem to
develop much more rigor and a more data-driven approach, and while that
might be hard to define exactly, as was once said by regulators, ?I know it
when I see it?. Companies in the financial sector give a year or more
before deprecating old APIs even after the new one has been up and running
concurrently and well proven, and would not shut off their old one in order
to get adoption of the new one.
Are we OK with some percent of the Bitcoin ecosystem not agreeing with the
existing rules? What would that mean? Are you willing to maintain two
separate networks, and if not, would you please document this in your BIP?
Deprecation timeline and emergency procedures?? Should we include
rationalizations for not using a new address prefix? In the event of a
partial hard-fork where two chains exist, wouldn't it make more sense to
have the new chain use a new address prefix? Using a new address prefix
could conceivably serve to minimize the impact of what almost looks like an
intentionally constructed y2k-bug type of event for the ecosystem.
I suspect that soft-fork upgrades have in the past tolerated _less_ rigor
around planning because voluntary soft-fork upgrading does not
intentionally break backwards-compatibility. Over time I expect that even
soft-fork upgrades will have much more planning, but again, it seems that
incompatible changes require much more rigor. If the sky is truly falling
according to your pronouncements, then there are millions if not billions
of dollars of value on the line which are being risked from lack of
engineering rigor without a well documented procedure, and suggesting that
we agree on that "next time" is not going to create the results that meet
your or anyone else?s desire. Much more, we need to signal to the broader
ecosystem and world that we are serious, mature and ready for business.
Regarding your request for definitions about soft-hard forks and
generalized soft-forks, you can find some definitions over here:
About hard-forks you may be interested in reading and internalizing,
This was an interesting exploration of soft-forks and hard-forks:
On the security of soft-forks
Are soft-forks misnamed?
- Bryan
1 512 203 0507

@_date: 2016-02-25 19:34:24
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] SIGHASH_NOINPUT in Segregated Witness 
Well if you are bothering to draft up a BIP about that SIGHASH flag,
then perhaps also consider some other SIGHASH flag types as well while
you are at it?
Various proposed sighash types:
"Build your own nHashType" proposal draft:
jl2012's reply:
petertodd's reply about OP_CODESEPARATOR linked back to this thread
regarding "Build your own nHashType":
((That particular thread had other replies which can be viewed here:
Also, there was a draft implementation of SIGHASH_NOINPUT:
FWIW there was some concern about replay using SIGHAHS_NOINPUT or something:
- Bryan
1 512 203 0507

@_date: 2016-01-02 10:54:43
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Segregated witnesses and validationless mining 
prior-block possession proofs, fraud proofs, non-fraud correctness
proofs, commitments and segwit:
- Bryan
1 512 203 0507

@_date: 2016-01-17 02:02:29
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Some more lightning-related git repos 
I saw these two repositories through the -wizards IRC channel earlier
today. I have not reviewed any of the source code for quality, security or
functionality, so I don't have word to offer regarding status of these.
Also other git repositories with related work:
- Bryan
1 512 203 0507

@_date: 2016-01-19 10:31:17
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Segregated Witness App Development 
Yeah, I agree that this is on topic for  I think that if
segwit discussion got to the point of clouding out all other development
discussions, then perhaps it would be time for a new channel. I would
definitely like to see segwit discussion not fragmented into a separate
- Bryan
1 512 203 0507

@_date: 2016-07-30 20:31:56
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Holdup on Block Alerts / Fraud Proofs ? 
There are some helpful discussions that happened over here:
- Bryan
1 512 203 0507

@_date: 2016-03-02 10:17:31
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
One reason "hard-fork to fix difficulty drop algorithm" could be
controversial is that the proposal involves a hard-fork (perhaps
necessarily so, at my first and second glance). There are a number of
concerns with hard-forks including security, deployment, participation,
readiness measurement, backwards incompatibility, etc. In fact, some
Bitcoin Core developers believe that hard-forks are not a good idea and
should not be used.
# Hard-forks
An interesting (unspoken?) idea I?ve heard from a few people has been ?we
should try to avoid all hard-forks because they are backwards
incompatible?, another thought has been "there should only be one more
hard-fork if any" and/or "there should only be one hard-fork every 30
years". I also recognize feedback from others who have mentioned "probably
unrealistic to expect that the consensus layer can be solidified this early
in Bitcoin's history". At the same time there are concerns about ?slippery
Also, if you are going to participate in a hard-fork then I think you
should make up some proposals for ensure minimal monetary loss on the old
(non-hard-forked) chain, especially since your proposed timeline is so
short seems reasonable to expect even more safety-related due diligence to
minimize money loss (such as using a new address prefix on the hard-forked
upgrade). Anyway, it should be clear that hard-forks are an unsettled issue
and are controversial in ways that I believe you are already aware about.
# Have miners gradually reduce their hashrate instead of using a step
function cliff
adam3us recently proposed that miners who are thinking of turning off
equipment should consider gradually ramping down their hashrate, as a show
of goodwill (and substantial loss to themselves, similar to how they would
incur losses from no longer mining after the halving). This is not
something the consensus algorithm can enforce at the moment, and this
suggestion does not help under adversarial conditions. Since this
suggestion does not require a hard-fork, perhaps some effort should be made
to query miners and figure out if they need assistance with implementing
this (if they happen to be interested).
# Contingency planning
Having said all of the negative things above about hard-forks, I will add
that I do actually like the idea of having backup plans available and
tested and gitian-built many weeks ahead of expected network event dates.
Unfortunately this might encourage partial consensus layer hard-forks in
times of extreme uncertainty such as "emergencies".... creating an even
further emergency.
# "Indefinite backlog growth"
You write "the backlog would grow indefinitely until the adjustment
occurs". This seems to be expected behavior regardless of difficulty
adjustment (in fact, a backlog could continue to grow even once difficulty
adjusts downward), and the consensus protocol does not commit to
information regarding that backlog anyway...
# Difficulty adjustment taking time is expected
This is an expected part of the protocol, it's been mentioned since
forever, it's well known and accounted for. Instead, we should be providing
advice to users about which alternative payment systems they should be
using if they expect instantaneous transaction confirmations. This has been
a long-standing issue, and rolling out a hard-fork is not going to fix
mistaken assumptions from users. They will still think that confirmations
were meant to be instantaneous regardless of how many hard-forks you choose
to deploy.
- Bryan
1 512 203 0507

@_date: 2016-05-09 09:04:20
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
On Mon, May 9, 2016 at 8:57 AM, Tom via bitcoin-dev <
IIRC you were previously informed by moderators (on the same reddit thread
to which you refer) that it would seem you had canceled your email from the
moderation queue, contrary to your retelling above. This is now reaching
far into off-topic and further posts on this subject should be sent to
bitcoin-discuss at lists.linuxfoundation.org or
bitcoin-dev-owners at lists.linuxfoundation.org instead of the bitcoin-dev
mailing list.
- Bryan
1 512 203 0507

@_date: 2016-05-27 16:24:36
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Zurich engineering meeting transcript and notes 
It has occurred to me that some folks may not have seen the link floating
around the other day on IRC.
Meeting notes summary:
Topics discussed and documented include mostly obscure details about
segwit, segwit code review, error correcting codes for future address
types, encryption for the p2p network protocol, compact block relay,
Schnorr signatures and signature aggregation, networking library, encrypted
transactions, UTXO commitments, MAST stuff, and many other topics. I think
this is highly useful reading material.
Any errors in transcription are very likely my own as it is difficult to
capture everything with high accuracy in real-time. Another thing to keep
in mind is that there are many different parallel conversations and I only
do linear serialization at best... and finally, I also want to mention that
this is the result of collaboration with many colleagues and this should
not be considered merely the work of just myself.
- Bryan
1 512 203 0507

@_date: 2016-10-15 13:34:13
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Some transcripts from Scaling Bitcoin 2016 
Here are some talks from Milan:
These are not always exact transcripts because I am typing while I am
listening, thus there are mistakes including typos and listening
errors, so please keep this discrepancy in mind between what's said
and what's typed.
- Bryan
1 512 203 0507

@_date: 2017-04-06 07:11:35
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
I think that it is an attack is a completely unambiguous technical
description of what it is. If a signature is supposed to resist forgery
against 2^128 operations, but you find a way to do it with 2^80 instead,
this is an attack. It is, perhaps, not a very concerning attack and you may
or may not change your signature scheme to avoid it or may just instead say
the scheme has 2^80 security. But there is no doubt that it would be called
an attack, especially if it was not described in the original proposal.
In Bitcoin's Proof of Work, you are attempting to prove a certain amount of
work has been done. This shortcut significantly reduces the amount of work.
It's an attack. Normally it wouldn't be a serious attack-- it would just
get appended to the defacto definition of what the Bitcoin Proof of work
is-- similar to the signature system just getting restarted as having 2^80
security-- but in it's covert form it cannot just be adopted because it
blocks many further improvements (not just segwit, but the vast majority of
other proposals), and additional the licensing restrictions inhibit
The proposal I posted does not prevent the technique, only the covert form:
That is, it doesn't even attempt to solve the patented tech eventually will
centralize the system problem. It is narrowly targeted at the interference
with upgrades.
Taking a step back-- even ignoring my geeking out about the technical
definition of 'attack' in crypographic contexts, we have a set of issues
here that left addressed will seriously harm the system going forward for
the the significant monetary benefit of an exploiting party. I think that
also satisfies a lay definition of the term: Something someone does, that
none one expected, that makes them money at everyone elses expense.
- Bryan
1 512 203 0507

@_date: 2017-08-17 06:31:30
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Lightning-dev] Lightning in the setting of 
lightning-dev at lists.linuxfoundation.org
Hi Martin,
this is the perfect venue to discuss this, welcome to the mailing list :-)
Like you I think that using the first forked block as the forkchain's
genesis block is the way to go, keeping the non-forked blockchain on the
original genesis hash, to avoid disruption. It may become more difficult in
the case one chain doesn't declare itself to be the forked chain.
Even more interesting are channels that are open during the fork. In these
cases we open a single channel, and will have to settle two. If no replay
protection was implemented on the fork, then we can use the last commitment
to close the channel (updates should be avoided since they now double any
intended effect), if replay protection was implemented then commitments
become invalid on the fork, and people will lose money.
Fun times ahead :-)
On Thu, Aug 17, 2017 at 10:53 AM Martin Schwarz Lightning-dev mailing list
Lightning-dev at lists.linuxfoundation.org

@_date: 2017-08-20 19:00:19
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] [BIP Proposal] Partially Signed Bitcoin 
Just a quick note but perhaps you and other readers would find this thread
(on hardware wallet BIP drafting) to be tangentially related and useful:
- Bryan
1 512 203 0507

@_date: 2017-12-31 17:46:57
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Single signature for all transactions in a block? 
Here are some resources to read regarding signature aggregation and
- Bryan
1 512 203 0507

@_date: 2017-07-11 18:36:12
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
I can't help but notice that I have read Greg's email before-- all the
way back in 2016. It would have been impossible for him to write a
reply to Paul's current email back then... but I also notice that Greg
did not indicate that he was copy-pasting until the very end (and even
then his aside at the end was sort of not the most clear it could have
been I think).
On Tue, Jul 11, 2017 at 5:17 PM, Paul Sztorc via bitcoin-dev
[ snip ]
I believe that's an artifact of a 2016 email. And the rest of it, for
that matter. See below.
The vast majority of Greg's email, including all the positiontext on
roadmaps was mostly text sent on 2016-11-04 to Adam Back, myself,
Wladimir, and others. Some of the other parts aren't, like the part
mentioning Blockstream.
Here is a commitment to a list of the recipients (for whatever good
such a commitment might do):
- Bryan
1 512 203 0507

@_date: 2017-07-11 21:48:38
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
On Tue, Jul 11, 2017 at 8:40 PM, Paul Sztorc via bitcoin-dev
To me it looked as if I was reading an email that was making a bunch
of points about how bitcoin development can't be coordinated and how
things would be broken if it were to be coordinated ("high authority
edicts"). There was a lot of harm caused by calling that 2015 email a
roadmap. Somehow-- and there's no way to figure out how this happened
I guess- people started putting timeline commitments to different
features. But there's really no way to guarantee any of those
timelines. And I think it's very quick to reach the point of unethical
to advocate a perspective that there's guarantee to things will happen
according to that timeline in the standard bitcoin development model.
I think there's already such a huge amount of public misunderstanding
around how bitcoin development works that giving guarantees even as a
community would further increase the misunderstandings.
I think generally communicating about research directions and projects
is useful and valuable, and I don't see any disagreement about that in
itself from anyone in this thread. I recommend an abundance of caution
with regards to whether to call these efforts roadmaps.
Well I mean he did look at some of the people putting the most effort
into bitcoin development. Why would he start at the other end of the
list as a rough check..?
Those suggestions were mixed with strong avocado that summaries are
good, coupled with recommendations that these aren't really roadmaps.
As to qualifying from where knowledge is sourced, yeah it seems like
talking with developers is a good idea, it seems everyone agrees with
that in this thread.
Well, to the extent that criticism is being misinterpreted as hostile,
I have seen people get upset from basic security review because "why
were't we more friendly and just say OK instead of pointing out the
- Bryan
1 512 203 0507

@_date: 2017-07-12 20:38:24
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Lightning-dev] Lightning Developers Biweekly 
Hi all!
        Every two weeks we've been running an informal Google Hangout
for implementers of the Lightning spec; as the spec is starting to
freeze, we've decided to formalize it a bit which means opening access
to a wider audience.
        The call is at 20:00 UTC every two weeks on Monday: next call is
on the 24th July.  We'll be using  on freenode's IRC
servers to communicate as well: if you're working on the Lightning
protocol and want to attend, please send me a ping and I'll add you to
the invite.
        I'll produce an agenda (basically a list of outstanding PRs on
github) and minutes each week: I'll post the latter to the ML here.
The last one can be found:
                The routine with the spec itself is that from now on all
non-spelling/typo changes will require a vote with no objections from
call participants, or any devs unable to make it can veto or defer by
emailing me in writing before the meeting.
Lightning-dev mailing list
Lightning-dev at lists.linuxfoundation.org

@_date: 2017-07-27 13:56:43
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Mimblewimble] proofs of position and UTXO set 
I have quite a few thoughts about proofs of position. I gave a talk about
it which hopefully gets the points across if you go through all the Q&A:
On Mon, Jul 24, 2017 at 12:12 PM, Ignotus Peverell <

@_date: 2017-03-20 17:36:04
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Mimblewimble] Lightning in Scriptless Scripts 
In my last post about scriptless scripts [2] I described a way to do
deniable atomic swaps by pre-sharing a difference of signatures. This
had the limitation that it required at least one party to be shared
between the signatures, allowed only pairwise linking, and required
both signatures to cover data that is known at the time of setup.
Linking a multi-hop Lightning channel with these constraints has proved
* * *
Recently I've found a different construction that behaves much more like
a hash preimage challenge, and this can actually be used for Lightning.
Further, it supports reblinding, so you can learn a preimage but hide
which one you're looking for. (Ethan, one might actually overlap with
TumbleBit, sorry :)).
It works like this. We'll treat x -> xG as a hash function, so x is the
preimage of xG. There are two separate but related things I can do: (a)
construct a signature which reveals the preimage; or (b) create a
"pre-signature" which can be turned into a signature with the help of
the preimage.
Here's how it works: suppose I send xG to Rusty and he wants to send
me coins conditional on my sending him x. Lets say I have key P1 and
nonce R1; he has key P2 and nonce R2. Together we're going to make a
multisignature with key P1 + P2 and Rusty is going to set things up
so that I can't complete the signature without telling him x.
Here we go.
  0. We agree somehow on R1, R2, P1, P2.
  1. We can both compute a challenge e = H(P1 + P2 || R1 + R2 || tx).
  2. I send s' = k1 - x - x1e, where R1 = k1G and P1 = x1G. Note he
     can verify I did so with the equation s'G = R1 - xG - eP1.
  3. He now sends me s2 = k2 - x2e, which is his half of the multisig.
  4. I complete the sig by adding s1 = k1 - x1e. The final sig is
     (s1 + s2, R1 + R2).
Now as soon as this signature gets out, I can compute x = s1 - s'.
* * *
Ok, pretty nifty. But now suppose Rusty wants to receive coins conditioned
on him revealing x, say, because he's a middle hop in a Lightning channel.
You might think he could act the same as I did in step (2), computing
s' = k1 - x - x1e, but actually he can't, because he doesn't know x himself!
All good. Instead he does the following.
To put names on things, let's say he's taking coins from Tadge. The
protocol is almost the same as above.
  0. They agree somehow on R1, R2, P1, P2. Tadge's key and nonce are
     R1 and P1, but there's a catch: P1 = x1G as before, but now
     R1 - xG = k1G. That is, his nonce is offset by k1G.
  1. They can both compute a challenge e = H(P1 + P2 || R1 + R2 || tx).
  2. Tadge sends the "presignature" s' = k1 - x1e. Rusty can verify this
     with the equation s'G = R1 - xG - eP1.
  3. Now whenever Rusty obtains x, he can compute s1 = s' - x, which is
     Tadge's half of the final signature.
  4. Rusty computes s2 himself and completes the signature.
* * *
Ok, even cooler. But the real Rusty complained about these stories, saying
that it's a privacy leak for him to use the same xG with me as he used with
Tadge. In a onion-routed Lightning channel, this xG-reuse would let all
any two participants in a path figure out that they were in one path, if
they were colluding, even if they weren't directly connected.
No worries, we can fix this very simply. Rusty chooses a reblinding factor
rG. I give him x, as before, but what Tadge demands from him is (x + r).
(I give xG to Rusty as a challenge; he forwards this as xG + rG to Tadge.)
Since Rusty knows r he's able to do the translation. The two challenges
appear uniformly independently random to any observers.
* * *
Let's put this together into my understanding of how Lightning is supposed
to work. Suppose Andrew is trying to send coins to Drew, through Bob and
Carol. He constructs a path
  A --> B --> C --> D
where each arrow is a Lightning channel. Only Andrew knows the complete
path, and is onion-encrypting his connections to each participant (who
know the next and previous participants, but that's it).
He obtains a challenge T = xG from D, and reblinding factors U and V
from B and C. Using the above tricks,
  1. A sends coins to B contingent on him learning the discrete logarithm
     of T + U + V.
  2. B sends coins to C contingent on him learning the discrete logarithm
     of T + V. (He knows the discrete log of U, so this is sufficient for
     him to meet Andrew's challenge.)
  3. C sends to D contingent on him learning the discrete log of T, which
     is D's original challenge. Again, because C knows the discrete log
     of V, this is sufficient for her to meet B's challenge.
The resulting path consists of transactions which are signed with single
uniformly random independent Schnorr signatures. Even though they're all
part of an atomic Lightning path.
* * *
Note that the s' values need to be re-communicated every time the
changes (as does the nonce). Because it depends on the other party's nonce,
this might require an additional round of interaction per channel update.
Note also that nothing I've said depends at all on what's being signed. This
means this works just as well for MimbleWimble as it would for
as it would for Monero (with a multisig ring-CT construction) as it would
for Ethereum+Schnorr. Further, it can link transactions across chains.
I'm very excited about this.
[1] Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   "A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom
Mailing list: Post to     : mimblewimble at lists.launchpad.net
Unsubscribe : More help   :

@_date: 2017-03-26 15:22:19
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
On Sun, Mar 26, 2017 at 2:05 PM, Peter R via bitcoin-dev <
False. With bip9-based soft-fork-based activation of segwit, miner blocks
will not be orphaned unless they are intentionally segwit-invalid (which
they currently are not). If you have told miners otherwise, let me know.
- Bryan
1 512 203 0507

@_date: 2017-03-26 15:44:12
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
On Sun, Mar 26, 2017 at 3:37 PM, Trevin Hofmann It's the other part of the statement- the "wakeup call to upgrade" from
producing invalid blocks? They aren't producing invalid blocks.
Additionally, if they want to be even more sure about this, they can run
the so-called "border nodes". No wakeup calls needed.... the point of a
soft-fork is to reduce incompatibility.
- Bryan
1 512 203 0507

@_date: 2017-11-16 11:14:47
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Protocol-Level Pruning 
It's not clear to me if you are have looked at the previous UTXO set
commitment proposals.
some utxo set commitment bookmarks (a little old)
TXO bitfields
delayed TXO commitments
TXO commitments do not need a soft-fork to be useful
rolling UTXO set hashes
lotta other resources available, including source code proposals..
- Bryan
1 512 203 0507

@_date: 2017-11-25 13:21:55
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Lightning-dev] General question on routing 
---------- Forwarded message ----------
(final try as the prior mail hit the size limit, sorry for the spam!)
Hi Pedro,
I came across this paper a few weeks ago, skimmed it lightly, and noted a
few interesting aspects I wanted to dig into later. Your email reminded me
to re-read the paper, so thanks for that! Before reading the paper, I
wasn't aware of the concept of coordinate embedding, nor how that could be
leveraged in order to provide sender+receiver privacy in a payment network
using a distance-vector-like routing system. Very cool technique!
After reading the paper again, my current conclusion is that while the
protocol presents some novel traits in the design a routing system for
payment channel based networks, it lends much better to a
closed-membership, credit network, such as Ripple (which is the focus of
the paper).
In Ripple, there are only a handful of gateways, and clients that seek to
interact with the network must chose their gateways *very* carefully,
otherwise consensus faults can occur, violating safety properties of the
network. It would appear that this gateway model nicely translates well to
the concept of landmarks that the protocol is strongly dependant on.
Ideally, each gateway would be a landmark, and as there are a very small
number of gateways within Ripple (as you must be admitted to be a verified
gateway in the network), then parameter L (the total number of landmarks)
is kept small which minimizes routing overhead, the average path-length,
When we compare Ripple to LN, we find that the two networks are nearly
polar opposites of each other. LN is an open-membership network that
requires zero initial configuration by central administrators(s). It more
closely resembles *debit* network (a series of tubes of money), as the
funds within channels must be pre-committed in order to establish a link
between two nodes, and cannot be increased without an additional on-chain
control transaction (to add or remove funds). Additionally, AFAIK (I'm no
expert on Ripple of course), there's no concept of fees within the
network. While within LN, the fee structure is a critical component of the
inventive for node operators to lift their coins onto this new layer to
provider payment routing services.  Finally, in LN we rely on time-locks
in order to ensure that all transactions are atomic which adds another set
of constraints. Ripple has no such constraint as transfers are based on
bi-lateral trust.
With that said, the primary difference between this protocol is that
currently we utilize a source-routed system which requires the sender to
know "most" of the path to the destination. I say "most" as currently,
it's possible for the receiver of a payment to use a poor man's rendezvous
system to provide the sender with a set of suffix paths form what one can
consider ad-hoc landmarks. The sender can then concatenate these with
their own paths, and construct the Sphinx routing package which encodes
the full route. This itself only gives sender privacy, and the receiver
doesn't know the identity of the sender, but the sender learns the
identity of the receiver.
We have plans to achieve proper sender/receiver privacy by extending our
Sphinx usage to leverage HORNET, such that the payment descriptor (payment
request containing details of the payment) also includes several paths
from rendezvous nodes (Rodrigo's) to the receiver. The rendezvous route
itself will be nested as a further Anonymous Header (AHDR) which includes
the information necessary to complete the onion circuit from Rodrigo to
the receiver. As onion routing is used, only Rodrigo can decrypt the
payload and finalize the route. With such a structure, the only nodes that
need to advertise their channels are nodes which seek to actively serve as
channel routers. All other nodes (phones, laptops, etc), don't need to
advertise their channels to the greater network, reducing the size of the
visible network, and also the storage and validation overhead. This serves
to extend the "scale ceiling" a bit.
My first question is: is it possible to adapt the protocol to allow each
intermediate node to communicate their time lock and fee references to the
sender? Currently, as the full path isn't known ahead of time, the sender
is unable to properly craft the timelocks to ensure safety+atomicity of
the payment. This would mean they don't know what the total timelock
should be on the first outgoing link. Additionally, as they don't know the
total path and the fee schedule of each intermediate node, then once
again, they don't know how much to send on the first out going link. It
would seem that one could extend the probing phase to allow backwards
communication by each intermediate node back to the sender, such that they
can properly craft a valid HTLC. This would increase the set up costs of
the protocol however, and may also increase routing failures as it's
possible incompatibilities arise at run-time between the preferences of
intermediate nodes. Additionally, routes may fail as an intermediate node
consumes too many funds as their fee, causing the funds to be insufficient
when it reaches the destination. One countermeasure would maybe: the
sender always sends waaay more than necessary, and gives the receiver a
one-time payment identifier, requiring that they route the remainder of
the funds *back* to them.
To solve this issue presently, we extend the header in Sphinx to include a
per-hop payload which allows the sender to precisely dictate the
structure of the route, allows the intermediate nodes to authenticate the
information given to it, and also allow the intermediate node to verify
that their policies have properly been respected. These payloads can also
be utilized by applications to communicate a small-ish amount of data to
construct higher-level protocols on top of the system. Examples include:
cross-chain swaps, chance payment games, higher-level B2B protocols,
flavors of ZKCP's, media streaming, internet access proxying, etc.
protocol (landmarks), becomes the weakest component. From my reading,
*all* nodes need to be ware of an *identical* set of landmarks (more or
less similar to the desired homogeneity of Gateways), otherwise the
coordinate embedding scheme breaks down. Currently, there's no requirement
that all nodes have a globally consistent view of the network. So then an
important questions arises: who choose the landmarks? A desirable property
of a routing system for LN (IMO) is that is has close to zero required
initial set up by a central administrator. With this protocol, it would
seem that all nodes much ship with a hard coded set of global landmarks
for the path finding to succeed.  This itself pins a hard coordination
requirement amongst implementers to have something like this deployed.
Even ignoring this requirement for a minute, I see several other
   * As *all* payments must flow through landmarks (since nodes break up
     their payment into L sub-flows), the landmarks must be very, very
     well capitalized. This would cause strong consolidation of the
     selection of landmarks, as they need extremely large channels in
     order to facilitate transfer within the network.
   * As landmarks must be globally known, this it seems this would
     introduce fragility in the network. If most of the landmarks go down
     (fails stop crashes) due to hardware issues, DoS, exploited bugs,
     etc, then the network's throughput instantly becomes crippled.
   * If all payment flow *must* go through landmarks, and the transfers
     within the network are relatively uni-directional (all payment going
     to Candy Crush Ultra: Lighting Strikes Twice), then their
     channels would become unbalanced very quickly.
The last point there invokes another component of the network: passive
channel rebalancing. With source routing, it's possible for nodes to
passive rebalance their channels, in order to keep the in equilibrium,
such that on average they'll be able to handle a payment flow coming from
any direction. This is possible as with source routing, it's easy for a
node to simply send a payment to himself incoming/outgoing from the pair
of channels they wish to adjust the available flow of. With
distance-vector-like protocols, this doesn't seem possible, as the node
doesn't have any control of the incoming channel that the payment will
arrive on.
Finally, the notion of value privacy within the scheme seems a bit weak.
payments to the world would achieve this trait. The base Bitcoin
blockchain doesn't mask the values of transfers (yet), but even if it did
unconditionally maintaining value privacy of channel doesn't seem
compatible with multi-hop payment networks (nodes can simply perform
probing/tagging attacks to ascertain a range of the size of a channel). A
possible mitigation would be for nodes to probabilistically drop incoming
payments, with all nodes sampling from the same distribution. However,
this would dramatically increase routing failures by senders, removing the
"low-latency" trait of payment networks that many find desirable.
Personally, I've very excited to see additional research on the front of
routing within the network! Excellent work by all authors.
In the end, I don't think it'll be a one-size fits all solution, as each
routing protocol delivers with it a set of tradeoffs that should be
weighed depending on target characteristics, and use-cases. There's no
strong requirement that the network as a whole uses a *single* routing
protocol. Instead several distinct protocols can be deployed based on
use-case requirements, as we only need to share a single end-to-end
construct: the HTLC. I could see a future in a few years where we have
several deployed protocols, similar to the wide array of existing routing
protocols deployed on the Internet. What we have currently gets us from
Zero to One. We'll definitely need to experiment with additional
approaches as the size of the network grows, and the true economic flow
patterns emerge after we all deploy to mainnet.

@_date: 2017-09-11 21:13:24
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
On Mon, Sep 11, 2017 at 9:03 PM, Mark Friedenbach via bitcoin-dev
original bip114:
revised bip114: from - Bryan
1 512 203 0507

@_date: 2017-09-12 00:18:14
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
On Mon, Sep 11, 2017 at 10:37 PM, Anthony Towns via bitcoin-dev
I have a reply to your point, but I want to clarify first that I am
not trying to provide any sort of criticism of your character, and to
any extent that my text is misinterpreted that way, that's entirely my
fault here. Anyway, here goes.
It's not enough to defend bitcoin and its users from active threats,
there is a more general responsibility to defend all kinds of users
and different software from many kinds of threats in whatever forms,
even if folks are using stupid and insecure software that you
personally don't maintain or contribute to or advocate for. Handling
knowledge of a vulnerability is a delicate matter and you might be
receiving knowledge with more serious direct or indirect impact than
originally described.
Besides the moral and ethical reasons to not unduly accelerate the
exploitation of a vulnerability, there is also a reputational
standpoint to consider, in that your position that your own (security)
work is credible is actually harmed by showing negative care for other
works by being first to publish either insecure software or knowledge
of a vulnerability. And sometimes the opposite is true: by not
disclosing knowledge of how a design is broken to someone inviting its
review, you're showing negative care in that way too, such as by
unintentionally encouraging the implementation of really bad ideas or
entirely novel misunderstandings of what you once thought were clear
concepts. So there is a difficult path to walk and especially in
security not all may be as it seems; caution is highly recommended.
Yes it would be good for "the market" to "get the signal" that
altcoins are insecure, and that some altcoin vendors are literally and
actively malicious entities, but I think everyone needs to take a step
back here and very carefully consider the color of their hats,
including those who advocate in the name of insecure downstream/forked
The downside of the approach I've advocated for is that it requires
knowledge, thinking and outsmarting the red teams; I am certainly
aware of the allure of the approaches that involve absolutist
statements like "anything weak [including bitcoin if it does have
weaknesses] deserves to die and be actively exploited" but it's not
something I am interested in espousing...nor do I think it would be
healthy for this community to internalize that perspective. Instead we
should continue to work on highly defensible software, and keep
vigilant in regards to security. In "the [civilized] garden" I would
expect there to be a general understanding that people collaborate and
work together to build highly defensible evolving systems even if
there exists knowledge of vulnerabilities. But we shouldn't be
surprised when we don't go out of our way to contribute to
alternative/parasitic systems... and we shouldn't be encouraging each
other to actively bring about the eschaton by way of mishandling
knowledge of vulnerabilities...
I know these issues are difficult to get a handle on. Hopefully I've
provided some useful perspective.
- Bryan
1 512 203 0507

@_date: 2017-09-27 14:01:40
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Revising BIP 2 to expand editorial authority 
Even minor revisions can not change the meaning of text. Changing a single
word can often have a strange impact on the meaning of the text. There
should be some amount of care exercised here. Maybe it would be okay as
long as edits are mentioned in the changelog at the bottom of each
document, or mention that the primary authors have not reviewed suggested
changes, or something as much; otherwise the reader might not be aware to
check revision history to see what's going on.
- Bryan
1 512 203 0507

@_date: 2018-02-08 11:49:23
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Lightning-dev] AMP: Atomic Multi-Path Payments 
Hi Y'all,
A common question I've seen concerning Lightning is: "I have five $2
channels, is it possible for me to *atomically* send $6 to fulfill a
payment?". The answer to this question is "yes", provided that the receiver
waits to pull all HTLC's until the sum matches their invoice. Typically, one
assumes that the receiver will supply a payment hash, and the sender will
re-use the payment hash for all streams. This has the downside of payment
hash re-use across *multiple* payments (which can already easily be
correlated), and also has a failure mode where if the sender fails to
actually satisfy all the payment flows, then the receiver can still just
pull the monies (and possibly not disperse a service, or w/e).
Conner Fromknecht and I have come up with a way to achieve this over
Lightning while (1) not re-using any payment hashes across all payment
flows, and (2) adding a *strong* guarantee that the receiver won't be paid
until *all* partial payment flows are extended. We call this scheme AMP
(Atomic Multi-path Payments). It can be experimented with on Lightning
*today* with the addition of a new feature bit to gate this new
feature. The beauty of the scheme is that it requires no fundamental changes
to the protocol as is now, as the negotiation is strictly *end-to-end*
between sender and receiver.
TL;DR: we repurpose some unused space in the onion per-hop payload of the
onion blob to signal our protocol (and deliver some protocol-specific data),
then use additive secret sharing to ensure that the receiver can't pull the
payment until they have enough shares to reconstruct the original pre-image.
Protocol Goals
1. Atomicity: The logical transaction should either succeed or fail in
entirety. Naturally, this implies that the receiver should not be unable to
settle *any* of the partial payments, until all of them have arrived.
2. Avoid Payment Hash Reuse: The payment preimages validated by the
consensus layer should be distinct for each partial payment.  Primarily,
this helps avoid correlation of the partial payments, and ensures that
malicious intermediaries straddling partial payments cannot steal funds.
3. Order Invariance: The protocol should be forgiving to the order in which
partial payments arrive at the destination, adding robustness in the face of
delays or routing failures.
4. Non-interactive Setup: It should be possible for the sender to perform an
AMP without directly coordinating with the receiving node. Predominantly,
this means that the *sender* is able to determine the number of partial
payments to use for a particular AMP, which makes sense since they will be
the one fronting the fees for the cost of this parameter. Plus, we can
always turn a non-interactive protocol into an interactive one for the
purposes of invoicing.
Protocol Benefits
Sending pay payments predominantly over an AMP-like protocol has several
clear benefits:
  - Eliminates the constraint that a single path from sender to receiver
    with sufficient directional capacity. This reduces the pressure to have
    larger channels in order to support larger payment flows. As a result,
    the payment graph be very diffused, without sacrificing payment
    utility
  - Reduces strain from larger payments on individual paths, and allows the
    liquidity imbalances to be more diffuse. We expect this to have a
    non-negligible impact on channel longevity. This is due to the fact that
    with usage of AMP, payment flows are typically *smaller* meaning that
    each payment will unbalance a channel to a lesser degree that
    with one giant flow.
  - Potential fee savings for larger payments, contingent on there being a
    super-linear component to routed fees. It's possible that with
    modifications to the fee schedule, it's actually *cheaper* to send
    payments over multiple flows rather than one giant flow.
  - Allows for logical payments larger than the current maximum value of an
    individual payment. Atm we have a (temporarily) limit on the max payment
    size. With AMP, this can be side stepped as each flow can be up the max
    size, with the sum of all flows exceeding the max.
  - Given sufficient path diversity, AMPs may improve the privacy of LN
    Intermediaries are now unaware to how much of the total payment they are
    forwarding, or even if they are forwarding a partial payment at all.
  - Using smaller payments increases the set of possible paths a partial
    payment could have taken, which reduces the effectiveness of static
    analysis techniques involving channel capacities and the plaintext
    values being forwarded.
Protocol Overview
This design can be seen as a generalization of the single, non-interactive
payment scheme, that uses decoding of extra onion blobs (EOBs?) to encode
extra data for the receiver. In that design, the extra data includes a
payment preimage that the receiver can use to settle back the payment. EOBs
and some method of parsing them are really the only requirement for this
protocol to work. Thus, only the sender and receiver need to implement this
feature in order for it to function, which can be announced using a feature
First, let's review the current format of the per-hop payload for each node
described in BOLT-0004.
?Realm (1 byte) ?Next Addr (8 bytes)?Amount (8 bytes)?Outgoing CLTV (4
bytes)?Unused (12 bytes)? HMAC (32 bytes) ?
                                              ???????????????????
                                              ?65 Bytes Per Hop ?
                                              ???????????????????
Currently, *each* node gets a 65-byte payload. We use this payload to give
each node instructions on *how* to forward a payment. We tell each node: the
realm (or chain to forward on), then next node to forward to, the amount to
forward (this is where fees are extracted by forwarding out less than in),
the outgoing CLTV (allows verification that the prior node didn't modify any
values), and finally an HMAC over the entire thing.
Two important points:
  1. We have 12 bytes for each hop that are currently unpurposed and can be
  used by application protocols to signal new interpretation of bytes and
  also deliver additional encrypted+authenticated data to *each* hop.
  2. The protocol currently has a hard limit of 20-hops. With this feature
  we ensure that the packet stays fixed sized during processing in order to
  avoid leaking positional information. Typically most payments won't use
  all 20 hops, as a result, we can use the remaining hops to stuff in *even
  more* data.
Protocol Description
The solution we propose is Atomic Multi-path Payments (AMPs). At a high
level, this leverages EOBs to deliver additive shares of a base preimage,
from which the payment preimages of partial payments can be derived. The
receiver can only construct this value after having received all of the
partial payments, satisfying the atomicity constraint.
The basic protocol:
Let H be a CRH function.
Let || denote concatenation.
Let ^ denote xor.
Sender Requirements
The parameters to the sending procedure are a random identifier ID, the
number of partial payments n, and the total payment value V. Assume the
sender has some way of dividing V such that V = v_1 + ? + v_n.
To begin, the sender builds the base preimage BP, from which n partial
preimages will be derived. Next, the sender samples n additive shares s_1,
?, s_n, and takes the sum to compute BP = s_1 ^ ? ^ s_n.
With the base preimage created, the sender now moves on to constructing the
n partial payments. For each i in [1,n], the sender deterministically
computes the partial preimage r_i = H(BP ||  i), by concatenating the
sequence number i to the base preimage and hashing the result. Afterwards,
it applies H to determine the payment hash to use in the i?th partial
payment as h_i = H(r_i). Note that that with this preimage derivation
scheme, once the payments are pulled each pre-image is distinct and
indistinguishable from any other.
With all of the pieces in place, the sender initiates the i?th payment by
constructing a route to the destination with value v_i and payment hash h_i.
The tuple (ID, n, s_i) is included in the EOB to be opened by the receiver.
In order to include the three tuple within the per-hop payload for the final
destination, we repurpose the _first_ byte of the un-used padding bytes in
the payload to signal version 0x01 of the AMP protocol (note this is a PoC
outline, we would need to standardize signalling of these 12 bytes to
support other protocols). Typically this byte isn't set, so the existence of
this means that we're (1) using AMP, and (2) the receiver should consume the
_next_ hop as well. So if the payment length is actually 5, the sender tacks
on an additional dummy 6th hop, encrypted with the _same_ shared secret for
that hop to deliver the e2e encrypted data.
Note, the sender can retry partial payments just as they would normal
payments, since they are order invariant, and would be indistinguishable
from regular payments to intermediaries in the network.
Receiver Requirements
Upon the arrival of each partial payment, the receiver will iteratively
reconstruct BP, and do some bookkeeping to figure out when to settle the
partial payments. During this reconstruction process, the receiver does not
need to be aware of the order in which the payments were sent, and in fact
nothing about the incoming partial payments reveals this information to the
receiver, though this can be learned after reconstructing BP.
Each EOB is decoded to retrieve (ID, n, s_i), where i is the unique but
unknown index of the incoming partial payment. The receiver has access to
persistent key-value store DB that maps ID to (n, c*, BP*), where c*
represents the number of partial payments received, BP* is the sum of the
received additive shares, and the superscript * denotes that the value is
being updated iteratively. c* and BP* both have initial values of 0.
In the basic protocol, the receiver cache?s the first n it sees, and
verifies that all incoming partial payments have the same n. The receiver
should reject all partial payments if any EOB deviates.  Next, the we update
our persistent store with DB[ID] = (n, c* + 1, BP* ^ s_i), advancing the
reconstruction by one step.
If c* + 1 < n, there are still more packets in flight, so we sit tight.
Otherwise, the receiver assumes all partial payments have arrived, and can
being settling them back. Using the base preimage BP = BP* ^ s_i from our
final iteration, the receiver can re-derive all n partial preimages and
payment hashes, using r_i = H(BP || i) and h_i = H(r_i) simply through
knowledge of n and BP.
Finally, the receiver settles back any outstanding payments that include
payment hash h_i using the partial preimage r_i. Each r_i will appear random
due to the nature of H, as will it?s corresponding h_i. Thus, each partial
payment should appear uncorrelated, and does not reveal that it is part of
an AMP nor the number of partial payments used.
Non-interactive to Interactive AMPs
Sender simply receives an ID and amount from the receiver in an invoice
before initiating the protocol. The receiver should only consider the
invoice settled if the total amount received in partial payments containing
ID matches or exceeds the amount specified in the invoice. With this
variant, the receiver is able to map all partial payments to a pre-generated
invoice statement.
Additive Shares vs Threshold-Shares
The biggest reason to use additive shares seems to be atomicity. Threshold
shares open the door to some partial payments being settled, even if others
are left in flight. Haven?t yet come up with a good reason for using
threshold schemes, but there seem to be plenty against it.
Reconstruction of additive shares can be done iteratively, and is win for
the storage and computation requirements on the receiving end. If the sender
decides to use fewer than n partial payments, the remaining shares could be
included in the EOB of the final partial payment to allow the sender to
reconstruct sooner. Sender could also optimistically do partial
reconstruction on this last aggregate value.
Adaptive AMPs
The sender may not always be aware of how many partial payments they wish to
send at the time of the first partial payment, at which point the simplified
protocol would require n to be chosen. To accommodate, the above scheme can
be adapted to handle a dynamically chosen n by iteratively constructing the
shared secrets as follows.
Starting with a base preimage BP, the key trick is that the sender remember
the difference between the base preimage and the sum of all partial
preimages used so far. The relation is described using the following
    X_0 = 0
    X_i = X_{i-1} ^ s_i
    X_n = BP ^ X_{n-1}
where if n=1, X_1 = BP, implying that this is in fact a generalization of
the single, non-interactive payment scheme mentioned above. For i=1, ...,
n-1, the sender sends s_i in the EOB, and  X_n for the n-th share.
Iteratively reconstructing s_1 ^ ?. ^ s_{n-1} ^ X_n = BP, allows the
receiver to compute all relevant r_i = H(BP || i) and h_i = H(r_i). Lastly,
the final number of partial payments n could be signaled in the final EOB,
which would also serve as a sentinel value for signaling completion. In
response to DOS vectors stemming from unknown values of n, implementations
could consider advertising a maximum value for n, or adopting some sort of
framing pattern for conveying that more partial payments are on the way.
We can further modify our usage of the per-hop payloads to send (H(BP),
s_i) to
consume most of the EOB sent from sender to receiver. In this scenario, we'd
repurpose the 11-bytes *after* our signalling byte in the unused byte
to store the payment ID (which should be unique for each payment). In the
of a non-interactive payment, this will be unused. While for interactive
payments, this will be the ID within the invoice. To deliver this slimmer
2-tuple, we'll use 32-bytes for the hash of the BP, and 32-bytes for the
partial pre-image share, leaving an un-used byte in the payload.
Cross-Chain AMPs
AMPs can be used to pay a receiver in multiple currencies atomically...which
is pretty cool :D
Open Research Questions
The above is a protocol sketch to achieve atomic multi-path payments over
Lightning. The details concerning onion blob usage serves as a template that
future protocols can draw upon in order to deliver additional data to *any*
hop in the route. However, there are still a few open questions before
something like this can be feasibly deployed.
1. How does the sender decide how many chunked payments to send, and the
size of each payment?
  - Upon a closer examination, this seems to overlap with the task of
    congestion control within TCP. The sender may be able to utilize
    inspired heuristics to gauge: (1) how large the initial payment should
    and (2) how many subsequent payments may be required. Note that if the
    first payment succeeds, then the exchange is over in a signal round.
2. How can AMP and HORNET be composed?
  - If we eventually integrate HORNET, then a distinct communications
    sessions can be established to allow the sender+receiver to exchange
    up-to-date partial payment information. This may allow the sender to
    accurately size each partial payment.
3. Can the sender's initial strategy be governed by an instance of the
Push-relabel max flow algo?
4. How does this mesh with the current max HTLC limit on a commitment?
   - ATM, we have a max limit on the number of active HTLC's on a particular
     commitment transaction. We do this, as otherwise it's possible that the
     transaction is too large, and exceeds standardness w.r.t transaction
     size. In a world where most payments use an AMP-like protocol, then
     overall ant any given instance there will be several pending HTLC's on
     commitments network wise.
     This may incentivize nodes to open more channels in order to support
     the increased commitment space utilization.
We've presented a design outline of how to integrate atomic multi-path
payments (AMP) into Lightning. The existence of such a construct allows a
sender to atomically split a payment flow amongst several individual payment
flows. As a result, larger channels aren't as important as it's possible to
utilize one total outbound payment bandwidth to send several channels.
Additionally, in order to support the increased load, internal routing nodes
are incensed have more active channels. The existence of AMP-like payments
may also increase the longevity of channels as there'll be smaller, more
numerous payment flows, making it unlikely that a single payment comes
across unbalances a channel entirely. We've also showed how one can utilize
the current onion packet format to deliver additional data from a sender to
receiver, that's still e2e authenticated.

@_date: 2018-02-20 16:42:19
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [Lightning-dev] Post-Schnorr lightning txes 
Hi *,
My understanding of lightning may be out of date, so please forgive
(or at least correct :) any errors on my behalf.
I was thinking about whether Greg Maxwell's graftroot might solve the
channel monitoring problem (spoiler: not really) and ended up with maybe
an interesting take on Schnorr. I don't think I've seen any specific
writeup of what that might look like, so hopefully at least some of this
is novel!
I'm assuming familiarity with current thinking on Schnorr sigs -- but all
you should need to know is the quick summary at footnote [0].
So I think there's four main scenarios for closing a lightning channel:
 - both parties are happy to close, do so cooperatively, and can
   sign a new unconditional transaction that they agree on. already fine.
   (should happen almost all of the time, call it 80%)
 - communications failure: one side has to close, but the other side
   is happy to cooperate as far as they're able but can only do so via
   the blockchain and maybe with some delay (maybe 15% of the time)
 - disappearance, uncooperative: one side effectively completely
   disappears so the other side has to fully close the channel on their
   own (5% of the time)
 - misbehaviour: one side tries publishing an old channel state due to
   error or maliciousness, and the other collects the entire balance as
   penalty (0% of the time)
With "graftroot" in mind, I was thinking that optimising for the last
case might be interesting -- despite expecting it to be vanishingly
rare. That would have to look something like:
   (0) funding tx
   (1) ...which is spent by a misbehaving commitment tx
   (2) ...which is spent by a penalty tx
You do need 3 txes for that case, but you really only need 1 output
for each: so (0) is 2-in-1-out, (1) is 1-in-1-out, (2) is 1-in-1-out;
which could all be relatively cheap. (And (2) could be batched with other
txes making it 1 input in a potentially large tx)
For concreteness, I'm going to treat A as the one doing the penalising,
and B (Bad?) as the one that's misbehaving.
If you treat each of those txes as a muSig Schnorr pay-to-pubkey, the
output addresses would be:
   (0) funding tx pays to [A,B]
   (1) commitment tx pays to [A(i),Revocation(B,i)]
   (2) pays to A
(where i is a commitment id / counter for the channel state)
If B misbehaves by posting the commitment tx after revealing the
revocation secret, A can calculate A(i) and Revocation(B,i) and claim
all the funds immediately.
As far as the other cases go:
  - In a cooperative close, you don't publish any commitment txes, you
    just spend the funding to each party's preferred destinations
    directly; so this is already great.
  - Otherwise, you need to be able to actually commit to how the funds
    get distributed.
But committing to distributing funds is easy: just jointly sign
a transaction with [A(i),Revocation(B,i)]. Since B is the one we're
worrying about misbehaving, it needs to hold a transaction with the
appropriate outputs that is:
  - timelocked to `to_self_delay` blocks/seconds in advance via nSequence
  - signed by A(i)
That ensures A has `to_self_delay` blocks/seconds to penalise misehaviour,
and that when closing properly, B can complete the signature using the
current revocation secret.
This means the "appropriate outputs" no longer need the OP_CSV step, which
should simplify the scripts a bit.
Having B have a distribution transaction isn't enough -- B could vanish
between publishing the commitment transaction and the distribution
transaction, leaving A without access to any funds. So A needs a
corresponding distribution transaction. But because that transaction can
only be published if B signs and publishes the corresponding commitment
transaction, the fact that it's published indicates both A and B are
happy with the channel close -- so this is a semi-cooperative close and
no delay is needed. So A should hold a partially signed transaction with
the same outputs:
  - without any timelock
  - signed by Revocation(B,i), waiting for signature by A(i)
Thus, if B does a non-cooperative close, either:
  - A proves misbehaviour and claims all the funds immediately
  - A agrees that the channel state is correct, signs and publishes
    the un-timelocked distribution transaction, then claims A's outputs;
    B can then immediately claim its outputs
  - A does nothing, and B waits for the `to_self_delay` period, signs
    and publishes its transaction, then claims B's outputs; A can eventually
    claim its own outputs
In that case all of the transactions except the in-flight HTLCs just look
like simple pay-to-pubkey transactions.
Further, other than the historical secrets no old information needs
to be retained: misbehaviour can be dealt with (and can only be dealt
with) by creating a new transaction signed by your own secrets and the
revocation information.
None of that actually relies on Schnorr-multisig, I think -- it could
be done today with normal 2-of-2 multisig as far as I can see.
I'm not 100% sure how this approach works compared to the current one
for the CSV/CLTV overlap problem. I think any case you could solve by
obtaining a HTLC-Timeout or HTLC-Success transaction currently, you could
solve in the above scenario by just updating the channel state to remove
the HTLC.
So I believe the above lets you completely forget info about old HTLCs,
while still enforcing correct behavior, and also makes enforcing correct
behaviour cheaper because it's just two extremely simple transactions
to post. If I haven't missed any corner cases, it also seems to simplify
the scripts a fair bit.
Does this make sense? It seems to to me...
So for completeness, it would make sense to do HTLCs via Schnorr --
at least to make them reveal elliptic curve private keys, and ideally
to make them mostly indistinguishable from regular transactions as a
"scriptless script" [1] or "discreet log contract" [2]. (I think, at
least for HTLCs, these end up being the same?)
The idea then is to have the HTLC payment hash be R=r*G, where r is the
secret/payment receipt.
Supposing your current commitment has n HTLCs in-flight, some paying A
if the HTLC succeeds and "r" is revealed, others paying B. We'll focus
on one paying A.
So you succeed by A completing a signature that reveals r to B,
and which simultaneously allows collection of the funds on chain. A
needs to be able to do this knowing nothing other than r (and their own
private keys). So agree to sign to muSig 2-of-2 multisig [A,B]. A and B
generate random values i and j respectively and reveal I=i*G and J=j*G,
and each calculates Q=I+J+R, and they generate partial signatures of a
transaction paying A:
    I, i + H(X,Q,m)*H(L,A)*a
    J, j + H(X,Q,m)*H(L,B)*b
where L = H(A,B) and X = H(L,A)*A + H(L,B)*B as usual. Once A knows r,
A can construct a full signature by adding R, r to the above values,
and B can then determine r by subtracting the above values from signature
A generated.
To ensure B gets paid if the HTLC timesout, they should also sign a
timelocked transaction paying B directly, that B can hold onto until
the channel state gets updated.
And once you're doing payment hashes via ECC, you can of course change
them at each hop to make it harder to correlate steps in a payment route.
I think that when combined with the above method of handling CSV delays
and revocation, this covers all the needed cases with a straightforward
pay-to-pubkey(hash) output, no script info needed at all. It does mean
each HTLC needs a signature every time the channel state updates (B needs
to sign a tx allowing A to claim the output once A knows the secret,
A needs to sign a tx allowing B to claim the output on timeout).
For channel monitoring this is pretty good, I think. You need to
keep track of the revocation info and your secret keys -- but that's
essentially a constant amount of data.
If you're happy to have the data grow by 64 bytes every time the channel
state updates, you can outsource channel monitoring: arrange a formula
for constructing a penalty tx based on the channel commitment tx --
eg, 95% of the balance goes to me, 4% goes to the monitor's address, 1%
goes to fees, there's a relative locktime of to_self_delay/3 to allow me
to directly claim 100% of the funds if I happen to be paying attention;
then do a partial signature with A(i), and then allow the monitoring
service to catch fraudulent transactions, work out the appropriate
revocation secret, and finish the signature.
If your channel updates 100 times a second for an entire year, that's
200GB of data, which seems pretty feasible. (You can't just regenerate
that data though, unless you keep each commitment tx) And it's pretty
easy to work out which bit of data you need to access: the funding
tx that's being spent tells you which channel, and the channel state
index is encoded in the locktime and sequence, so you should only need
small/logarithmic overhead even for frequently updated channels rather
than any serious indexes.
I don't think you can do better than that without serious changes to
bitcoin: if you let the monitoring agency sign on its own, you'd need some
sort of covenant opcode to ensure it sends any money to you; and with
segwit outputs, there's no way to provide a signature for a transaction
without committing to exactly which transaction you're signing.
I was hoping covenants and graftroot would be enough, but I don't
think they are. The idea would be that since the transaction spends to
A(i)+Rev(B,i), you'd sign an output script with A that uses covenant
opcodes to ensure the transaction only pays the appropriate monitoring
reward, and the monitor could then work out A(i)-A and Rev(B,i) and finish
the signature. But the signature by "A" would need to know A(i)+Rev(B,i)
when calculating the hash, and that's different for every commitment
transaction, so as far as I can see, it just doesn't work. You can't
drop the muSig-style construction because you need to be protect yourself
against potential malicious choice of the revocation secret [3].
 - Funding txes as 2-of-2 multisig is still great. Convert to
   Schnorr/muSig when available of course.
 - Generate 6+8*n transactions everytime the channel state is updated,
   (n = number of HTLCs in-flight)
   1. Channel state commitment tx, held by A, spends funding tx,
      payable to Schnorr muSig address [A(i),Rev(B,i)], signed by B
   2. Channel fund distribution tx, held by A (CSV), spends (1),
      signed by Rev(B,i)
   3. Channel fund distribution tx, held by B (no CSV), spends (1),
      signed by A(i)
   4. Channel state commitment tx, held by B, spends funding tx
      payable to Schnorr muSig address [B(i),Rev(A,i)], signed by A
   5. Channel fund distribution tx, held by B (CSV), spends (4),
      signed by Rev(A,i)
   6. Channel fund distribution tx, held by A (no CSV), spends (4),
      signed by B(i)
   The fund distribution txs all pay the same collection of addresses:
     - channel balance for A directly to A's preferred address
     - channel balance for B directly to B's preferred address
     - HTLC balance to muSig address for [A,B] for each in-flight HTLC
       paying A on success
     - HTLC balance to muSig address for [B,A] for each in-flight HTLC
       paying B on success
     - (probably makes sense to bump the HTLC addresses by some random
       value to make it harder for third parties to tell which addresses
       were balances versus HTLCs)
   Both (1) and (4) include obscured channel state ids as per current
   standard.
   For each HTLC that pays X on timeout and Y on success:
     a. Timeout tx, held by X, signed by Y, spends from (2)
     b. Timeout tx, held by X, signed by Y, spends from (3)
     c. Timeout tx, held by X, signed by Y, spends from (5)
     d. Timeout tx, held by X, signed by Y, spends from (6)
     e. Success tx, held by Y, signed by X, spends from (2)
     f. Success tx, held by Y, signed by X, spends from (3)
     g. Success tx, held by Y, signed by X, spends from (5)
     h. Success tx, held by Y, signed by X, spends from (6)
     (these should all be able to be SIGHASH_SINGLE, ANYONECANPAY
      to allow some level of aggregation)
 - Fund distribution tx outputs can all be pay2pubkey(hash): HTLCs work
   by pre-signed timelocked transactions and scriptless
   scripts/discreet-log contracts to reveal the secret; balances work
   directly; CSV and revocations are already handled by that point
 - You can discard all old transaction info and HTLC parameters once
   they're not relevant to the current channel state
 - Channel monitoring can be outsourced pretty efficiently -- as little as
   a signature per state could be made to works as far as I can see,
   which doesn't add up too fast.
 - There's still no plausible way of doing constant space outsourced
   channel monitoring without some sort of SIGHASH_NOINPUT, at least
   that I can see
aj, very sad that this didn't turn out to be a potential use case for
    graftroot :(
[0] In particular, I'm assuming that:
    - Schnorr sigs in bitcoin will look something like:
        R, r + H(X,R,m)*x
      (where m is the message being signed by private key x, r is a
      random per-sig nonce, R and X are public keys corresponding to r,x;
      H is the secure hash function)
    - muSig is a secure way for untrusting parties to construct an n-of-n
      combined signature; for public keys A and B, it produces a combined
      public key:
        X = H(L,A)*A + H(L,B)*B
      with L = H(A,B)
   See [1]     [2]     [3] Well, maybe you could request a zero-knowledge proof to ensure a new
    revocation hash conforms to the standard for generating revocation
    secrets without revealing the secret, and have the public key be
    a(i)*G + r(B,i)*G without using the muSig construct, but that would
    probably be obnoxious to have to generate every time you update
    the channel state.
[4] As an aside -- this could make it feasible and interesting to penalise
    disappearance as well as misbehaviour. If you add a transaction
    the B pre-signs, spending the commitment tx A holds, giving all the
    channel funds to A but only after a very large CSV timeout, perhaps
    `to_self_delay`*50, then the scenarios are:
    If A is present:
      - B publishes an old commitment: A immediately steals all the
        funds if active or outsourced misbehaviour monitoring. Whoops!
      - B publishes the current commitment: A publishes its distribution
        transaction and collects its funds immediately, allowing B to
        do likewise
    If A has disappeared:
      - B publises the current commitment and waits a modest amount
        of time, publishes its distribution transaction claiming its
        rightful funds, and allowing A to collect its funds if it ever
        does reappear and still knows its secrets
      - B publishes the current commitment, waits a fair while,
        A reappears and publishes its distribution transactions, both
        parties get their rightful funds
      - B publishes the current commitment, waits an extended period
        of time, and claims the entire channel's funds. If B is
        particularly reputable, and A can prove its identity (but not
        recover all its secrets) maybe B even refunds A some/all of its
        rightful balance
    Perhaps that provides too much of an incentive to try blocking
    someone from having access to the blockchain though.
Lightning-dev mailing list
Lightning-dev at lists.linuxfoundation.org

@_date: 2018-01-29 16:23:46
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] How accurate are the Bitcoin timestamps? 
A perspective on block timestamp and opentimestamps can be found here:
- Bryan
1 512 203 0507

@_date: 2018-07-02 18:03:13
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Alert key disclosure 
The bitcoin alert keys are disclosed in this email, followed by a
disclosure of various known vulnerabilities in what was once the alert
system. The bitcoin alert system has been completely retired. The
network is not at risk and this warning may be safely ignored if you
do not have an ancient node (running v0.12.x or older) using the
deprecated bitcoin alert system or its public keys.
mainnet public key:
mainnet private key:
testnet public key:
testnet private key:
These are openssl-serialized private keys.
In 2016, a plan was proposed[1] for the completion of the retirement
of the bitcoin alert system which included the idea of revealing the
alert system private keys. The proposal still contains good
information regarding the purpose and intention of alert system
retirement and motivation for the disclosure of the private keys.
Additionally, an overview of the alert system retirement and its
timeline is available on the web at [2]. This disclosure was recently
discussed in an IRC meeting logs at [3]. A media site also recently
discussed this topic[4].
One of the reasons for disclosure of the keys is to mitigate the
effects of unknown dissemination and proliferation of the keys. By
broadcasting the values to make them available to everyone, the value
of the keys is intended to be to be eliminated, since now everyone
could feasibly sign messages, the value of the signed messages becomes
[1] [2] [3] [4] # Vulnerabilities in the bitcoin alert system
The following text[5] discloses a number of known vulnerabilities in
the alert system. Writeup contributed by achow101.
[5] The Alert System previously utilized by Bitcoin has several issues
(some of which may be classified as vulnerabilities). These issues no
longer exist in Bitcoin as of network protocol version 700013 which
was released with Bitcoin Core 0.13.0. Many altcoins and Bitcoin
client implementations were notified of the Alert System's removal and
have since removed the alert system themselves or transitioned to
using an Alert system that does not share an Alert Key with Bitcoin.
All of the issues described below allow an attacker in possession of
the Alert Key to perform a Denial of Service attack on nodes that
still support the Alert system. These issues involve the exhaustion of
memory which causes node software to crash or be killed due to
excessive memory usage.
Many of these issues were not known until the Alert System was removed
as developers inspected the code for vulnerabilities prior to
releasing the Alert Key. Due to these issues, the publication of the
Alert Key was delayed and affected altcoins and software were
As of this writing, less than 4% of Bitcoin nodes are vulnerable.
Furthermore, the Bitcoin Core developers have created a "final alert"
which is a maximum ID number alert which overrides all previous alerts
and displays a fixed "URGENT: Alert key compromised, upgrade required"
message on all vulnerable software. The Bitcoin Core developers
believe that so few vulnerable nodes are present on the network, and
risks to those nodes so minor, that it is safe to publish the Alert
An Alert contains these fields:
    int32_t nVersion;
    int64_t nRelayUntil;      // when newer nodes stop relaying to newer nodes
    int64_t nExpiration;
    int32_t nID;
    int32_t nCancel;
    std::set setCancel;
    int32_t nMinVer;            // lowest version inclusive
    int32_t nMaxVer;            // highest version inclusive
    std::set setSubVer;  // empty matches all
    int32_t nPriority;
Alerts are also identified by their SHA256 hash. The above fields can
be freely modified to generate alerts with differing hashes.
# Infinitely sized map (CVE-2016-10724)
The Alert System was designed to support multiple Alerts
simultaneously. As such, Alerts were stored in memory in a map.
However, there is no limit on how large this map can be, thus an
attacker with the Alert Key can send a large number of Alerts to a
node. Eventually, the map containing all of the Alerts will be so
large that the node runs out of memory and crashes, thus causing a
Denial of Service attack.
The infinitely sized map is the basis for which the Alert system can
be used to cause Denial of Service attacks.
# Infinitely sized alerts
Although the infinitely sized map is what causes the crash itself, an
attacker can also send very large Alerts. Alerts themselves are not
limited in size explicitly, they are only limited by the maximum
network message size. This maximum network message size has varied
between versions. At times in the past, it has been 32 MB. For Bitcoin
Core 0.12.0 (the most recent version of Bitcoin Core with the alert
system enabled by default), the maximum message size is 2 MB.
Although large Alerts do not directly cause a Denial of Service by
themselves, combined with the infinitely sized map, large Alerts can
more quickly cause a node to run out of memory.
* The setCancel field has no length limit (besides the maximum message
size) and is a std::set of 32-bit integers. Given that it has no size
constraints, an attacker can use this field to create a very large
Alert by filling the set with many integers.
* The setSubVer field, like setCancel, has no length limit and is a
std::set. However instead of integers it has std::strings. These
strings do not have a length limit themselves and can thus be
arbitrarily long to produce an Alert that is arbitrarily large.
* Bitcoin Core versions prior to 0.10.0 did not have a limit on the
length of the strComment, strStatusBar, and strReserved fields. These
strings can have an arbitrary length.
# The final alert
To protect against attackers abusing the Alert key following its
publication, the Bitcoin Core developers constructed a "final alert".
This final alert is a maximum ID alert which overrides all previous
alerts. All Bitcoin Core versions since and including Bitcoin Core
0.14.0 contain the final alert and will send it to any node which is
vulnerable to issues including the following disclosures. However this
protection is not enough to protect those nodes as a few issues were
found with the final alert implementation itself.
Final alerts are those which meet the following conditions:
    nExpiration == maxInt &&
    nCancel == (maxInt-1) &&
    nMinVer == 0 &&
    nMaxVer == maxInt &&
    setSubVer.empty() &&
    nPriority == maxInt &&
    strStatusBar == "URGENT: Alert key compromised, upgrade required"
maxInt is the maximum signed integer as defined by
# Multiple final alerts
The definition for a final alert does not include a few fields.
Because alerts are identified by their hashes, changing the omitted
fields allows an Alert to be classified as a final alert but still be
an alert that is added to the infinitely sized map. The nCancel field
omits the maxInt ID number used by the final alert so all of the final
alerts share the same ID.
* Since setCancel is not required to be empty for an alert to be a
final alert, the setCancel field can contain different integers to
produce alerts that have different hashes and are thus different
alerts. Combined with the infinitely sized map and the infinitely
sized setCancel issues, many final alerts can be created which are
large, fill the map, and cause a node to run out of memory.
* The strComment field, while having a maximum length of 65536 bytes
(and no maximum length prior to Bitcoin Core version 0.10.0), is not
required to be a particular string in order for an alert to be a final
alert. Thus multiple final alerts can be crafted which have different
hashes by using different values for strComment
* The strReserved field, while having a maximum length of 256 bytes,
is not required to be a particular string in order for an alert to be
a final alert. Thus multiple final alerts can be crafted which have
different hashes by using different values for strReserved.
* The nVersion field is also not required to be a particular value.
Thus this can be used to construct final alerts with different hashes
by having different values for nVersion.
* nRelayUntil field is also not required to be a particular value.
Thus this can be used to construct final alerts with different hashes
by having different values for nRelayUntil.
# Final Alert Cancellation (CVE-2016-10725)
Although the final alert is supposed to be uncancellable, it
unfortunately is cancellable due to the order of actions when
processing an alert. Alerts are first processed by checking whether
they cancel any existing alert. Then they are checked whether any of
the remaining alerts cancels it. Because of this order, it is possible
to create an alert which cancels a final alert before the node checks
whether that alert is canceled by the final alert. Thus an attacker
can cancel a final alert with another alert allowing a node to be
vulnerable to all of the aforementioned attacks.
# Protecting against DoS attacks from the alert system
Fixing these issues is relatively easy. The first and most obvious
solution is to simply remove the Alert system entirely. As nodes
upgrade to versions without the Alert system, fewer nodes will be
vulnerable to attack should the Alert keys become public. This is the
option that Bitcoin has taken. However, because Bitcoin has retired
the Alert system entirely, the Alert key will also be published to
reduce the risk that the Alert Key is mistakenly depended upon in the
Should altcoins wish to continue using the Alert system but with a
different Alert Key, a few very simple fixes will safeguard nodes from
the aforementioned issues. Limiting the number of alerts, the size of
setCancel and setSubVer, and only allowing one final alert altogether
fix the above issues. This patch[6], on top of Bitcoin Core 0.11 (a
vulnerable version), fixes the aforementioned issues. Altcoins that
still use the Alert system are recommended to port this patch to their
software. Outdated node software is still vulnerable.
[6] This disclosure was authored primarily by Bryan Bishop (kanzure) and
Andrew Chow (achow101). Special thanks to reviewers. Also, an
interesting proposal was floated to not disclose the private keys in
WIF format-- one is that this is not how the original values were
received, and second (more importantly) to prevent users from
importing the key into their wallet and reusing it in their wallet key
- Bryan
1 512 203 0507

@_date: 2018-06-17 12:46:00
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Alert key retirement? 
Alert key has yet to be disclosed. The alert system itself has been retired
for quite a while now. More information about this can be found here:
Recently it was suggested to me that it would be helpful to wait for v0.13
end-of-life before revealing the alert keys.
I am seeking information regarding anyone that has requested final alert
messages for any particular projects that may have copied the bitcoin alert
Thank you.
- Bryan
1 512 203 0507

@_date: 2018-06-18 15:51:33
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Miner dilution attack on Bitcoin - is that 
Bram, actually I thought the previous discussions determined that less than
51% hashrate would be required for certain soft-hard-forks employing empty
I don't have a specific reference:
- Bryan
1 512 203 0507

@_date: 2018-09-22 12:54:12
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [bitcoin-core-dev] On the initial notice of 
bitcoin-core-dev at lists.linuxfoundation.org>
For some reason I don't understand, Andrea Suisani is stating on
twitter that the the report by awemany was a report of an inflation
bug, contrary to the timeline we published.   This is not the case:
the report specifically stated that inflation was not possible because
the node crashed. It also described a reproduction of the crash, but
not of inflation.
I generally understand how someone could be confused about what a
report they hadn't seen said, but I'm confused in this case because
Andrea Suisani was copied on the report to us. So I'm not sure what is
up with that, perhaps the message got lost in email.  If the reporter
knew the bug permitted inflation, they still specifically reported
otherwise to us.
Since people are also expressing doubt that awemany was actually the
author of the report, I'll include it here in its entity to aid
people's validation of the claim(s). There is a better test for the
crash issue include in master branch of the Bitcoin repository, the
reporter's reproduction instructions here are only included for
, Andrea Suisani , Gregory
Maxwell , "Wladimir J. van der Laan"
Dear Bitcoiners,
Please find attached an encrypted description of a crashing zero day
exploit for Bitcoin Core as well as Bitcoin ABC. This has not been
reproduced for Bitcoin Unlimited, though for advisory reasons, I am
sending it to one of their members that I could find a PGP key for as
Please forward this to any party who might have a valid interest,
including Bitcoin miners.
Thank you very much.
Problem description:
The following, miner-exploitable zero day has been found in Bitcoin ABC as
well as in Bitcoin Core:
Duplicate inputs are not checked in CheckBlock,
only when they are accepted into the mempool.
This creates a problem insofar as a transaction might bypass
the mempool when it is included in a block, for example if
it is transmitted as an extra transaction along with a compact
A later assertion assert(is_spent) in SpendCoins (in validation.cpp)
seems to prevent the worse outcome of monetary inflation by
the comparatively better result of crashing the node.
To reproduce (Description is for Bitcoin ABC, but applies similarly to
Bitcoin Core):
Create one instance of ABC bitcoind without the patch below
applied (A) and create on instance of ABC with the patch applied (B).
The patch removes sending of transactions and testing for double-spent
inputs for the attacker node.
Run both in regtest mode and point them to different data directories,
like so and connect them together:
A: ./bitcoind -regtest -rpcport=15000 -listen -debug -datadir=/tmp/abc.1
B: ./bitcoind -regtest -rpcport=15001 -connect=localhost -debug
Now on the prepared attacker node B, create a bunch of blocks and a
that double-spends its input, like  so for example:
in=: outaddr=99.9:
The double entry of the input here is not a typo. This is the desired
Sign the resulting transaction hex like so:
signrawtransaction For Core, this step needs to be adapted to signrawtransactionwithkey.
And send the result into the small regtest test netwrok:
sendrawtransaction Voila, your node A should have just aborted like this:
bitcoind: validation.cpp:1083: void SpendCoins(CCoinsViewCache&, const
CTransaction&, CTxUndo&, int): Assertion `is_spent' failed.
Aborted (core dumped)
If you like this work or want to pay out a bounty for finding a zero day,
please do so in BCH to this address. Thank you very much in advance.
The patch for ABC:
diff --git a/src/consensus/tx_verify.cpp b/src/consensus/tx_verify.cpp
index ee909deb9..ff7942361 100644
--- a/src/consensus/tx_verify.cpp
+++ b/src/consensus/tx_verify.cpp
 -229,7 +229,7  static bool CheckTransactionCommon(const CTransaction
     // Check for duplicate inputs - note that this check is slow so we
skip it
     // in CheckBlock
-    if (fCheckDuplicateInputs) {
+    if (0) {
         std::set vInOutPoints;
         for (const auto &txin : tx.vin) {
             if (!vInOutPoints.insert(txin.prevout).second) {
diff --git a/src/net_processing.cpp b/src/net_processing.cpp
index e4ecc793c..ee1cc3cda 100644
--- a/src/net_processing.cpp
+++ b/src/net_processing.cpp
 -1269,12 +1269,6  static void ProcessGetData(const Config
&config, CNode *pfrom,
                             // however we MUST always provide at least
what the
                             // remote peer needs.
                             typedef std::pair
-                            for (PairType &pair : merkleBlock.vMatchedTxn)
-                                connman->PushMessage(
-                                    pfrom,
-                                    msgMaker.Make(NetMsgType::TX,
-                                                  *block.vtx[pair.first]));
-                            }
                         }
                         // else
                         // no response
 -1321,25 +1315,6  static void ProcessGetData(const Config
&config, CNode *pfrom,
                 bool push = false;
                 auto mi = mapRelay.find(inv.hash);
                 int nSendFlags = 0;
-                if (mi != mapRelay.end()) {
-                    connman->PushMessage(
-                        pfrom,
-                        msgMaker.Make(nSendFlags, NetMsgType::TX,
-                    push = true;
-                } else if (pfrom->timeLastMempoolReq) {
-                    auto txinfo = mempool.info(inv.hash);
-                    // To protect privacy, do not answer getdata using the
-                    // mempool when that TX couldn't have been INVed
in reply to
-                    // a MEMPOOL request.
-                    if (txinfo.tx &&
-                        txinfo.nTime <= pfrom->timeLastMempoolReq) {
-                        connman->PushMessage(pfrom,
-                                             msgMaker.Make(nSendFlags,
-                                                           NetMsgType::TX,
-                                                           *txinfo.tx));
-                        push = true;
-                    }
-                }
                 if (!push) {
                     vNotFound.push_back(inv);
                 }
diff --git a/src/validation.cpp b/src/validation.cpp
index a31546432..a9edbb956 100644
--- a/src/validation.cpp
+++ b/src/validation.cpp
 -1080,7 +1080,7  void SpendCoins(CCoinsViewCache &view, const
CTransaction &tx, CTxUndo &txundo,
     for (const CTxIn &txin : tx.vin) {
         txundo.vprevout.emplace_back();
         bool is_spent = view.SpendCoin(txin.prevout,
-        assert(is_spent);
+        //assert(is_spent);
     }
 }
The same patch for Core:
diff --git a/src/consensus/tx_verify.cpp b/src/consensus/tx_verify.cpp
index 0628ec1d4..a06f77f8b 100644
--- a/src/consensus/tx_verify.cpp
+++ b/src/consensus/tx_verify.cpp
 -181,7 +181,7  bool CheckTransaction(const CTransaction& tx,
CValidationState &state, bool fChe
     }
     // Check for duplicate inputs - note that this check is slow so
we skip it in CheckBlock
-    if (fCheckDuplicateInputs) {
+    if (0) {
         std::set vInOutPoints;
         for (const auto& txin : tx.vin)
         {
diff --git a/src/net_processing.cpp b/src/net_processing.cpp
index b48a3bd22..9b7fb5839 100644
--- a/src/net_processing.cpp
+++ b/src/net_processing.cpp
 -1219,8 +1219,6  void static ProcessGetBlockData(CNode* pfrom,
const CChainParams& chainparams, c
                     // Thus, the protocol spec specified allows for
us to provide duplicate txn here,
                     // however we MUST always provide at least what
the remote peer needs
                     typedef std::pair PairType;
-                    for (PairType& pair : merkleBlock.vMatchedTxn)
-                        connman->PushMessage(pfrom,
msgMaker.Make(SERIALIZE_TRANSACTION_NO_WITNESS, NetMsgType::TX,
                 }
                 // else
                     // no response
 -1284,18 +1282,6  void static ProcessGetData(CNode* pfrom, const
CChainParams& chainparams, CConnm
             bool push = false;
             auto mi = mapRelay.find(inv.hash);
             int nSendFlags = (inv.type == MSG_TX ?
SERIALIZE_TRANSACTION_NO_WITNESS : 0);
-            if (mi != mapRelay.end()) {
-                connman->PushMessage(pfrom, msgMaker.Make(nSendFlags,
NetMsgType::TX, *mi->second));
-                push = true;
-            } else if (pfrom->timeLastMempoolReq) {
-                auto txinfo = mempool.info(inv.hash);
-                // To protect privacy, do not answer getdata using
the mempool when
-                // that TX couldn't have been INVed in reply to a
MEMPOOL request.
-                if (txinfo.tx && txinfo.nTime <=
pfrom->timeLastMempoolReq) {
-                    connman->PushMessage(pfrom,
msgMaker.Make(nSendFlags, NetMsgType::TX, *txinfo.tx));
-                    push = true;
-                }
-            }
             if (!push) {
                 vNotFound.push_back(inv);
             }
diff --git a/src/validation.cpp b/src/validation.cpp
index 947192be0..66536af24 100644
--- a/src/validation.cpp
+++ b/src/validation.cpp
 -1315,7 +1315,7  void UpdateCoins(const CTransaction& tx,
CCoinsViewCache& inputs, CTxUndo &txund
         for (const CTxIn &txin : tx.vin) {
             txundo.vprevout.emplace_back();
             bool is_spent = inputs.SpendCoin(txin.prevout,
-            assert(is_spent);
+            //assert(is_spent);
         }
     }
     // add outputs
bitcoin-core-dev mailing list
bitcoin-core-dev at lists.linuxfoundation.org

@_date: 2018-09-25 22:40:32
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] CVE-2018-17144 disclosure (inflation vulnerability) 
It has been informed to me that the writeup for the recent
vulnerability was not distributed to this mailing list. Please find
details at the following blog post:
I believe a release notice was posted but not information about the bug,
There was also further discussion here:
Also, around the time those emails were sent, there was a mailing list
moderator queue bug and nobody was able to approve emails including
myself. This bug was subsequently resolved by Linux Foundation.
The remainder of this email is copy-paste from the bitcoincore.org
page linked above.
=== Full disclosure ===
CVE-2018-17144, a fix for which was released on September 18th in
Bitcoin Core versions 0.16.3 and 0.17.0rc4, includes both a Denial of
Service component and a critical inflation vulnerability. It was
originally reported to several developers working on Bitcoin Core, as
well as projects supporting other cryptocurrencies, including ABC and
Unlimited on September 17th as a Denial of Service bug only, however
we quickly determined that the issue was also an inflation
vulnerability with the same root cause and fix.
In order to encourage rapid upgrades, the decision was made to
immediately patch and disclose the less serious Denial of Service
vulnerability, concurrently with reaching out to miners, businesses,
and other affected systems while delaying publication of the full
issue to give times for systems to upgrade. On September 20th a post
in a public forum reported the full impact and although it was quickly
retracted the claim was further circulated.
At this time we believe over half of the Bitcoin hashrate has upgraded
to patched nodes. We are unaware of any attempts to exploit this
However, it still remains critical that affected users upgrade and
apply the latest patches to ensure no possibility of large
reorganizations, mining of invalid blocks, or acceptance of invalid
transactions occurs.
=== Technical details ===
In Bitcoin Core 0.14, an optimization was added (Bitcoin Core PR
 which avoided a costly check during initial pre-relay block
validation that multiple inputs within a single transaction did not
spend the same input twice which was added in 2012 (PR  While
the UTXO-updating logic has sufficient knowledge to check that such a
condition is not violated in 0.14 it only did so in a sanity check
assertion and not with full error handling (it did, however, fully
handle this case twice in prior to 0.8).
Thus, in Bitcoin Core 0.14.X, any attempts to double-spend a
transaction output within a single transaction inside of a block will
result in an assertion failure and a crash, as was originally
In Bitcoin Core 0.15, as a part of a larger redesign to simplify
unspent transaction output tracking and correct a resource exhaustion
attack the assertion was changed subtly. Instead of asserting that the
output being marked spent was previously unspent, it only asserts that
it exists.
Thus, in Bitcoin Core 0.15.X, 0.16.0, 0.16.1, and 0.16.2, any attempts
to double-spend a transaction output within a single transaction
inside of a block where the output being spent was created in the same
block, the same assertion failure will occur (as exists in the test
case which was included in the 0.16.3 patch). However, if the output
being double-spent was created in a previous block, an entry will
still remain in the CCoin map with the DIRTY flag set and having been
marked as spent, resulting in no such assertion. This could allow a
miner to inflate the supply of Bitcoin as they would be then able to
claim the value being spent twice.
=== Timeline ===
Timeline for September 17, 2018: (all times UTC)
14:57 anonymous reporter reports crash bug to: Pieter Wuille, Greg
Maxwell, Wladimir Van Der Laan of Bitcoin Core, deadalnix of Bitcoin
ABC, and sickpig of Bitcoin Unlimited.
15:15 Greg Maxwell shares the original report with Cory Fields, Suhas
Daftuar, Alex Morcos and Matt Corallo
17:47 Matt Corallo identifies inflation bug
19:15 Matt Corallo first tries to reach slushpool CEO to have a line
of communication open to apply a patch quickly
19:29 Greg Maxwell timestamps the hash of a test-case which
demonstrates the inflation vulnerability
20:15 John Newbery and James O?Beirne are informed of the
vulnerability so they can assist in alerting companies to a pending
patch for a DoS vulnerability
20:30 Matt Corallo speaks with slushpool CTO and CEO and shares patch
with disclosure of the Denial of Service
20:48 slushpool confirmed upgraded
21:08 Alert was sent to Bitcoin ABC that a patch will be posted
publicly by 22:00
21:30 (approx) Responded to original reporter with an acknowledgment
21:57 Bitcoin Core PR 14247 published with patch and test
demonstrating the Denial of Service bug
21:58 Bitcoin ABC publishes their patch
22:07 Advisory email with link to Bitcoin Core PR and patch goes out
to Optech members, among others
23:21 Bitcoin Core version 0.17.0rc4 tagged
September 18, 2018:
00:24 Bitcoin Core version 0.16.3 tagged
20:44 Bitcoin Core release binaries and release announcements were available
21:47 Bitcointalk and reddit have public banners urging people to upgrade
September 19, 2018:
14:06 The mailing list distributes an additional message urging people
to upgrade by Pieter Wuille
September 20, 2018:
19:50 David Jaenson independently discovered the vulnerability, and it
was reported to the Bitcoin Core security contact email.
- Bryan
1 512 203 0507

@_date: 2019-08-02 06:43:27
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] [Meta] bitcoin-dev moderation 
It really shouldn't be 24 hours. Our strategy was to have a few
moderators in different timezones to cover sleep shifts or other
disruptions of service. Evidently this has not been adequate.
Makes sense. I'll go find a few people.
There is an active software vulnerability which requires moderation to
be enabled. This version of mailman is unmaintained, and Linux
Foundation is migrating away from or abandoning the email protocol so
they are less willing to do backend infrastructure work. This
manifests in other ways, like downtime, but also weird situations like
missing emails that never hit the moderation queue. I get pings from
different people about two times a year where they report an email
that they think I missed, but in fact it never hit the moderation
queue at all. Email clearly isn't the greatest protocol.
- Bryan
1 512 203 0507

@_date: 2019-08-07 08:48:06
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
I have a proposal for implementing bitcoin vaults in a way that does not
require any soft-forks or other software upgrades, although it could benefit
from SIGHASH_NOINPUT which I'll describe later.
I call them pre-signed vaults.
Vault definition
Here, a vault is defined as a transaction setup scheme that binds both the user
and the attacker to always using a public observation and delay period before a
weakly-secured hot key is allowed to arbitrarily spend coins. This is the same
definition previously used[1]. During the delay period, there is an opportunity
to initiate recovery/clawback which can either trigger deeper cold storage
parameters or at least reset the delay period to start over again for the same
One of the important components of this is the delete-the-key pre-signed
transaction concept, where only a single transaction is (pre)signed before
deleting the key. This is basically an emulation of a covenant and enforces a
certain outcome.
Background and motivation
I was looking at Eyal and Sirer's 2016 vaults paper [1], and I saw this
This was probably written before the introduction of OP_CHECKSEQUENCEVERIFY.
Still, a viable construction would have more steps than just using OP_CSV. They
were probably not thinking about what those steps might be, because in the
context of the paper they were proposing a bitcoin vault implemented using
recursive consensus-enforced covenants via a new opcode, which obviously cannot
be deployed without an upgrade fork. Covenants have been discussed for years,
but require new opcodes or other consensus-enforcement changes.
Relative locktimes are useful here because there is no knowledge as to when the
transactions might be broadcasted in the future. The delays need to be relative
to after the transaction is included in the blockchain, not to setup
initialization time.
Also, from [2]:
I haven't seen any previous proposal for how to implement recursive bitcoin
vaults without a fork and without a covenant. After asking around, I am pretty
sure this is somewhat novel. The closest I guess is [3].
Vaults are particularly interesting as a bitcoin cold storage security
mechanism because they enable a publicly observable delay period during which
time a user could be alerted by a watchtower that a thief might be in the
process of stealing their coins, and then the user may take some actions to
place the coins back into the vault before the relative timelock expires. There
seems to be no way to get this notification or observation period without a
vault construction. It might have been assumed it required a covenant.
Having a vault construction might go a long way to discourage would-be
attackers, on principle that the attacker might be incapable of recovering
their cost-of-attack because the recovery mechanism can lock up the coins
indefinitely. Griefing or denial-of-service would still be possible, of course,
but with multisig there might be some ways to put a halt to that as well. I am
working under the assumption that the attacker knows that the user is a vault
The idea is to have a sequence of pre-generated pre-signed transactions that
are generated in a certain way. The basic components are a vaulting transaction
that locks coins into a vault, a delayed-spend transaction which is the only
way to spend from a vault, and a re-vaulting transaction which can
recover/clawback coins from the delayed-spend transaction. The security of this
scheme is enforced by pre-signing transactions and deleting private keys, or
with the help of SIGHASH_NOINPUT then there's another scheme where private keys
are provably never known. This enforces that there's only a specific set of
possible outcomes at every step of the vault.
Some examples of what the set of broadcasted transactions might look like in
regular usage:
    coins -> VT -> DST -> exit via hot wallet key
    coins -> VT -> DST -> RVT
    coins -> VT -> DST -> RVT -> DST -> ...
    coins -> VT -> ... -> RVT998 -> nuclear abort
    VT = vault transaction
    DST = delayed-spend transaction
    RVT = re-vaulting transaction
The delayed-spending transaction would have a single output with a script like:
    30 days AND hot wallet key
 OR 10 days AND re-vaulting public key
 OR 1 day AND 4-of-7 multisig
 OR 0 days and super-secure nuclear abort ragequit key
Another diagram:
    VT_100 -> DST -> (optionally) RVT -> coins are now in VT_99
    VT_99 -> DST -> (optionally) RVT -> coins are now in VT_98
    ...
    VT_1 -> burn-all-coins nuclear abort ragequit (final)
Transactions and components:
* Commitment/funding vault setup transaction. Signed after setting up the
transaction tree, and it is broadcasted whenever funds are to be placed into
the vault.
* Delayed-spend transaction. Signed during the vault transaction tree setup,
and it is broadcasted when the user wants to withdraw coins from cold storage
or otherwise manipulate the coins. The output script template used by the
delayed-spend transaction was defined earlier.
* Hot wallet key: Somewhat insecure key. This can also be multisig using
multiple hot keys.
* Re-vaulting key: It is important to note that the private key either never
existed (SIGHASH_NOINPUT + P2WPK for the re-vaulting transaction) or the
private key was deleted after pre-signing the re-vaulting transaction.
* 4-of-7 multisig: This is a group of differently-motivated individuals who are
responsible for signing transactions. This multisig group is not necessry to
describe the technique, I just think it's a useful feature for a vault to
* Nuclear abort key: Also unnecessary. This is a key for which only a single
signed transaction will ever exist, and that single transaction will spend to a
proof-of-burn key like 0x00. This key must be extremely secure, and if there
is any doubt about the ability to keep such a key secured, then it is better to
not include this in the protocol. Alternatively, maybe include it as an option
50 layers down in the revaulting sequence.
* Nuclear-abort pre-signed transaction. This is signed during transaction tree
setup, before constructing the delayed-spend transaction. It is broadcasted
only if the user wants to provably relinquish coins forever without giving the
attacker any coins.
* Re-vaulting transaction. This is where the magic happens. The re-vaulting
transaction is signed during transaction tree setup, before constructing the
delayed-spend transaction for the parent vault. The re-vaulting transaction is
broadcasted when someone wants to prevent a coin withdrawal during the public
observation delay period. The re-vaulting transaction spends the delayed-spend
transaction outputs. It has a single output with a script created by running
the entire vault setup function again. Hence, when the re-vaulting transaction
is confirmed, all of the coins go back into a new identically-configured vault
instead of being relinquished through the delayed-spend transaction timeout for
hot wallet key signing.
* Special case: final transaction. This is the very first pre-signed
transaction during setup, and the transaction spends the coins using any
provable burn technique. This is broadcasted only at the end of the game, as an
ultimate abort and forfeiture of coins without giving in to an adversary. It's
similar to the nuclear-abort ragequit transaction but it sits at the same place
that a delayed-spend transaction would, at the very end of the rainbow or
yellow brick road.
Example log during vault setup
When running the recursive vault setup function, the created artifacts (in
order) will look like:
1) choose one of:
   (first iteration) pre-signed burn-all-coins nuclear abort ragequit (final)
   (all others) a new vault setup transaction spendable only by its
                delayed-spend transaction
2) pre-signed re-vaulting transaction sending to vault setup or final
transaction, with a unique private key
3) pre-signed delayed-spend transaction, with a unique private key
4) vault transaction spendable only by the delayed-spend public key
In pseudocode (where PTX is a pre-signed transaction function with
private key deletion):
    VT(counter, *args, **kwargs) =
        if counter == 0:
            DST = PTX("burn-all-coins")
        else:
            next_vault = VT(counter-1, *args, **kwargs)
            revaulting = PTX("only spendable by next_vault public key")
            DST = PTX("DST policy including revaulting and other conditions")
        vault = PTX("spendable only by this DST")
        return vault
Pre-signed transactions
What has been known for a while is that a covenant can be somewhat emulated
using a pre-signed transaction where the user then deletes the private key,
enforcing that the user's chosen policy must be enforced since there is only
one existing option and there will only ever be one option.
Such a scheme has been previously described for simple one-time and chained
vaults [3]. I have learned that the author has an implementation that is in
preparation, for a non-recursive version.
Note that a series of pre-signed transactions can be considered to be an
emulation of a covenant. Imagine a linear chain of pre-signed transactions
where each hop has a relative locktime before being able to broadcast the next
transaction. To recover the coins at the end of the rainbow, one would need to
broadcast each sequential transaction in order and wait for the relative
timelocks to expire each time. Here, covenants provide something like an undo
for bitcoin, but only between pre-determined addresses and scripts.
Fees for pre-signed transactions
There's a few different techniques to talk about:
1) SIGHASH_SINGLE|SIGHASH_ANYONECANPAY to let someone add inputs and outputs.
This can get pretty complex though.
2) Add a zero-value OP_TRUE output and let anyone spend the zero-value output
and attach a child-pays-for-parent (CPFP) transaction to pay for everything.
3) Pre-sign a variety of different possible fee rates. Unfortunately this
involves an explosive blow-up in the amount of transaction data to generate. It
might actually be a reasonable blow-up amount, only resulting in a few hundred
megabytes of additional data. But given the other options, this is unnecessary.
Delete the key (for pre-signed transactions)
The delete-the-key trick is simple. The idea is to pre-sign at least one
transaction and then delete the private key, thus locking in that course of
Unfortunately, delete-the-key doesn't really work for multisig scenarios
because nobody would trust that anyone else in the scheme has actually deleted
the secret. If they haven't deleted the secret, then they have full unilateral
control to sign anything in that branch of the transaction tree. The only time
that delete-the-key might be appropriate would be where the user who deletes
the key and controls the key during the setup process is also the sole
beneficiary of the entire setup with the multisig participants.
Alternative fee rates are easier to deal with using delete-the-key, compared to
a technique where the private key never existed which can only be used to sign
one fee rate per public key, requiring an entirely new vault subtree for each
alternative fee rate. With delete-the-key, the alternative fee rates are signed
with the private key before the private key is deleted.
Multisig gated by ECDSA pubkey recovery for provably-unknown keys
A group can participate in a multisig scheme with provably-unknown ECDSA keys.
Instead of deleting the key, the idea is to agree on a blockheight and then
select the blockhash (or some function of the chosen blockhash like
H(H(H(blockhash)))) as the signature. Next, the group agrees on a transaction
and they recover the public key from the signature using ECDSA pubkey recovery.
A pre-signed transaction is created, which will trigger the start of the public
observation period described earlier and also start the clock for the bip112
relative timelock on its output. In the output script, an OR branch
is added that enables the use of a re-vaulting key which could also be its own
separate multisig construction.
This is incompatible with P2WPKH because the P2WPKH spending scriptSig needs to
have the pubkey (to check the hash of the pubkey against the pubkeyhash in the
scriptPubKey), which in turn makes it incompatible with ECDSA pubkey recovery
which requires a hash of the message. However, with P2WPK and SIGHASH_NOINPUT
instead of P2WPKH it could conceivably work. SIGHASH_NOINPUT is required because
otherwise the input includes a txid which references the public key. With P2WPK,
the scriptSig only needs a signature and not a public key. Note that what would
be required is a version of SIGHASH_NOINPUT that does not commit to the public
key, and I think a few of the NOINPUT proposals are committing to the public
Alternatively, there may be some constructions using the 2-party ECDSA
techniques or m-n party ECDSA techniques.
Deploying exceedingly large scripts
A brief interlude to share a somewhat obvious construction. I haven't seen this
written down yet.
Suppose there is a bitcoin script that someone is interested in using, but it
far exceeds the size limits and sigop limits. To fix this, they would split up
the script into usable chunks, and then use the delete-the-key mechanism (or
the other one) to create an OR branch that is signable by a single key for
which only a single signature is known. That new pre-signed transaction would
spend to a script that has the output with the remainder of the script of
interest. Re-vaulting or clawback clauses can be added to that output as well,
but spending back to the original root script will only work by generating new
scripts and keys (since the final hash isn't known until the whole tree is
constructed, it's a dependency loop).
Recursively-enforced multi-party multisig bitcoin vaults
Ideally, to enforce a covenant with impossible fairy dust magic, we would ask
for a bitcoin transaction that could be self-referential because the
only-one-signature-ever trick requires that the signed message be known before
producing the signature, and the signature has to be known before the public
key can be known, and the public key would have to be included in the
self-referential message/transaction hash value. So, that's a dependency loop
and it doesn't work. It would be interesting to explore a variation of this
idea with masking, such that a value X can be replaced by a hash over the whole
script with the X value, even though the real script will have the hash.
Someone else can figure that one out for me :-).
Instead of the self-referential values attempting to reference the same
script that is in the process of being constructed, an alternative is to use
the same script template but populate it with different parameters. The script
template gets reused over and over again, all the way down the tree, until the
final transaction which could be >100 years into the future once done adding up
all the relative locktimes. In fact, to create and populate this terrifying
recursive script tree, the final transaction needs to be created first, and
then it is given as input to the script template function and that output is
then given to the script template function itself-- and so on. At each stage,
there are additional pre-signed transactions and values to remember.
This can be written as:
    final_transaction = TX(spend to 0x0000 to burn the coins)
    initial_transaction = F(F(...F(final_transaction))
    (This is missing parameters to indicate to the function what the spending
    keys requirements are to be.)
See earlier explanation for more details.
Each call to the template populating function produces values that each must be
preserved for a very long time. It is less safe to store all of the pre-signed
transactions together at the same time, but more convenient. With less
redundancy, there is an increased chance of losing data over time, which could
render the coins completely frozen. This doesn't particularly worry me because
forgetting a key has that property already, and this could be likened to
hundreds of megabytes of extra key data or something. Unlike the much smaller
covenant-based (opcode-based covenant) vault construction, the multiple layers
here can be separately stored and protected, which might be able to protect
against an adversary that has stolen some of the re-vaulting keys but not all
of them.
Optimizations can be made to store parameters for generating the remainder of
the tree, such as using deterministic key derivation, such that megabytes of
data wouldn't need to be long-term stored. Only the initial parameters would
need to be stored.
Financial privacy for custody
One of the concerns raised in [2] is that if all coins at an exchange are
stored together in the same vault, then attackers would be able to learn about
access control policies by observing scripts and keys. Some privacy can be
recovered by using segregated vaults, at the cost of additional setup
complexity and keeping more data in long-term storage.
However, note that I think vaults are also useful for personal cold storage
Fail-deadly mechanism
An early nuclear abort option can be added to these scripts. This idea was
explored in [2]. This would be a very cold very secret key that would abort the
re-vaulting procedure and send all coins to a (provably) nonsense key. This
allows a vault user to destroy the coins instead of continuously monitoring the
bitcoin blockchain for the rest of his life. The attacker can't recover their
cost of attack if they never get the coins, and this eliminates an entire class
of potential attackers who are directly interested only in financial gain. The
disadvantage is that if the attacker finds the secret key for the fail-deadly
mechanism and uses it, then all of the coins are gone forever.
Multisig variations
The re-vaulting key could be the same key at each layer, or only sometimes the
same key, or always a unique key stored separately in another secure location.
Additionally, these re-vaulting keys could be subjected to multisig schemes, as
well as Shamir secret sharing schemes or other secret sharing schemes.
The idea of adding the 4-of-7 multisig component is to avoid griefing
situations, at the cost of the additional security requirements for the 4-of-7
multisig group.
Key rotation for vaults
Keeping the same hot wallet key for 100 years is not advisable. Rotate the keys
by setting up a new vault construction and initiating a withdrawal transaction
from the old vault to the new vault.
Single-use seals
This proposal may have inadvertedly demonstrated a practical way to implement
Peter Todd's single-use seals concept [4]. I am hesitant to say so, though,
because I think he would ask for a more sophisticated way to verify seal
Paid defection
It might be advisable to add small rewards for evidence of defection amongst
multiparty multisig setups. Besides amounts spendable by individual keys from a
multisig setup, it may be possible to use a zero-knowledge contingent payment
for a zero-knowledge statement like: I have a signature s over some message m
which validates for pubkey pk where pk is a member of the multisig group. Then
the zkcp transaction would pay for knowledge of defectors. The zkcp procedure
would require interaction with the defector, while the direct pubkey method
would not. This is similar to companies paying employees to quit when they
value the payment over the value of continued employment.
Handling change
It is important to note that this vault setup is one-time and once-only. There
must only ever be one deposit into one vault. Also, spending some coins would
require sending the change amount back into a new vault.  Alternatively,
upfront work can be done to set a regular withdrawal stipend or assumption
about how many coins are left, such that the transaction tree can be
pre-generated for those possibilities, hence cutting down on future vault
reinitializations. It would also be possible to commit upfront to only ever
working in some minimum increment number of bitcoin or something.
It is very important to only fund the vault once, and only with the amount that
was configured when setting up the vault.
[1] [2] [3] [4] or or * Jeremy Rubin for pointing out something embarrassingly broken in an earlier
* Bob McElrath for telling me to use SIGHASH_NOINPUT which I proceeded to
promptly forget about.
* Andrew Poelstra for the OP_TRUE trick.
* Joe Rayhawk for paid defection.
* Tadge Dryja for pointing out a few differences between SIGHASH_NOINPUT
Thank you,
- Bryan

@_date: 2019-08-07 15:32:47
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
One of the biggest problems with the vault scheme (besides all of the
setup data that has to be stored for a long time) is an attacker that
silently steals the hot wallet private key and waits for the vault's
owner to make a delayed-spend transaction to initiate a withdrawal
from the vault. If the user was unaware of the theft of the key, then
the attacker could steal the funds after the delay period.
To mitigate this, it is important to choose a stipend or withdrawal
amount per withdrawal period like x% of the funds. This limits the
total stolen funds to x% because once the funds are stolen the user
would know their hot key is compromised, and the user would know to
instead use one of the other clawback paths during all of the future
withdrawal delay periods instead of letting the delay timeout all the
way to the (stolen) default/hot key.
The reason why a loss limiter is the way to go is because there's
currently no way (that I am aware of, without an upgrade) to force an
attacker to reveal his key on the blockchain while also forcing the
attacker to use a timelock before the key can spend the coins. I am
curious about what the smallest least invasive soft-fork would be for
enabling this kind of timelock. There are so many covenant proposals
at this point (CHECKSIGFROMSTACK, SECURETHEBAG, CHECKOUTPUTVERIFY,
....). Or there's crazy things like a fork that enables a transaction
mode where the (timelock...) script of the first output is
automatically prefixed to any of the other scripts on any of the other
outputs when an input tries to spend in the future. A thief could add
his key to a new output on the transaction and try to spend (just like
a user would with a fresh/rotated key), but the OP_CSV would be
automatically added to his script to implement the public observation
delay window.
Also, there was other previous work that I was only informed about
today after posting my proposal, so I should mention these as related
- Bryan

@_date: 2019-08-07 20:16:42
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
Replying to two emails below.
Yes, that is correct. When setting up the vault, plan it "all the way to
the end" like next 100+ years. With exponential backoff on the relative
timelock values, the total number of pre-signed transactions isn't really
that high. With a few thousand pre-signed transactions (more than enough),
you can have high resolution timelocks well into the future.
On Wed, Aug 7, 2019 at 4:19 PM Dustin Dettmer Honestly, no idea. The answer to that might depend on each individual vault
user. If the user doesn't want to deal with the expense of managing a bunch
of unique keys and other data, then it might make more sense to use the
same values and have a small blob that has to be stored for a long time,
rather than many different blobs stored in different places to deal with.
- Bryan

@_date: 2019-08-12 21:09:43
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
True, I did not intend for everyone to know the meaning of the observed
transaction. It turns out to not be too useful to the scheme anyway, unless
you're interested in protecting against an adversary dumb enough to tell
you he has stolen your key before spending your coins. To reiterate my
other follow-up email, the best you can do (... or the best I can do right
now) is limit losses to k% where k is selected by the user, e.g. 1 input
100 outputs each with succesively increasing timeouts allowing the rotten
non-rotated(pre-inserted) key to spend, and instant spending by a recovery
flow. Once the attacker steals any one of the k% outputs, you know to not
let the outputs timeout to that key in the future. Unfortunately, without
an opcode-style covenant, the only way to know if a stale hot key is stolen
is to observe an unexpected spend or, if you're lucky, observe an
unexpected signature otherwise unassociated with a transaction.
Obviously normally to provably destroy coins you'd spend to an OP_RETURN
Oh, right. Well, that works.
I was thinking about another construction where you pick a key as a group
(separate from the multisig setup) and sign with that. But in practice, as
you have pointed out, you would do the delete-the-key trick on the multisig
construction itself with each party contributing their own pubkey,
requiring 1/n honest deletes.
Well you need to pick an entropy source, and I wouldn't want to tell people
to just trust the first party to tell you a good sequence of bytes.
- Bryan
1 512 203 0507

@_date: 2019-06-05 09:24:29
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Fwd: [ots-dev] miniOTS: ots proofs that fit in a tweet 
Hello OTS people,
Following from my previous post about cleartext OTS proof sharing[1],
I've been working on a new OTS format called miniOTS, which is
minimal/compressed format that allows attestations to fit in a tweet[2]
and for other space constrained contexts.
Just stripping out additional attestations in the standard format only
gets it down to just above ~280 bytes when base58 encoded, which is too
much for a tweet, so I decided to roll a custom format that is a bit
more efficient with attestation and op tags.
The goal was to have small enoughs proofs that I could reply to a tweet
with the stamp of the previous message, instead of relying on external
sites such as Current format (210 bytes, 288 bytes base58-encoded):
00000000: 004f 7065 6e54 696d 6573 7461 6d70 7300  .OpenTimestamps.
00000010: 0050 726f 6f66 00bf 89e2 e884 e892 9401  .Proof..........
00000020: 08cb 2d4a f572 8d44 a5b0 7c7b f1ff 78a9  ..-J.r.D..|{..x.
00000030: 1818 7270 13f1 9bbd f4b0 344b 9e93 0c6b  ..rp......4K...k
00000040: 39f0 1020 34fe cad9 edef bab0 3420 e4ee  9.. 4.......4 ..
00000050: d3a7 c608 fff0 107c 31f7 da6c dbf2 3271  .......|1..l..2q
00000060: 904c c5dd f58d eb08 f120 e4f7 3eaf a747  .L....... ..>..G
00000070: 324a f096 1aa0 928d e1c1 91bf 3c38 237d  2J..........<8
00000080: d412 c1c0 e94c d4ae 3f76 08f1 045c 4cb3  .....L..?v...\L.
00000090: a4f0 08f7 834d 4b14 68fd 41ff 0083 dfe3  .....MK.h.A.....
000000a0: 0d2e f90c 8e2c 2b68 7474 7073 3a2f 2f62  .....,+
000000b0: 6f62 2e62 7463 2e63 616c 656e 6461 722e  ob.btc.calendar.
000000c0: 6f70 656e 7469 6d65 7374 616d 7073 2e6f  opentimestamps.o
000000d0: 7267                                     rg
miniOTS format (133 bytes, 183 bytes base58-encoded):
00000000: 6f74 7301 8a20 34fe cad9 edef bab0 3420  ots.. 4.......4
00000010: e4ee d3a7 c683 8a7c 31f7 da6c dbf2 3271  .......|1..l..2q
00000020: 904c c5dd f58d eb83 8be4 f73e afa7 4732  .L.........>..G2
00000030: 4af0 961a a092 8de1 c191 bf3c 3823 7dd4  J..........<8
00000040: 12c1 c0e9 4cd4 ae3f 7683 8b5c 4cb3 a48a  ....L..?v..\L...
00000050: f783 4d4b 1468 fd41 9a2b 6874 7470 733a  ..MK.h.A.+https:
00000060: 2f2f 626f 622e 6274 632e 6361 6c65 6e64  //bob.btc.calend
00000070: 6172 2e6f 7065 6e74 696d 6573 7461 6d70  ar.opentimestamp
00000080: 732e 6f72 67                             s.org
base58 before:
base58 after:
You can check it out here:
  git clone This is still a work in progress, I haven't built the miniOTS -> OTS
decoder yet.
./otsmini file.ots
[1] id:878t1fzn0v.fsf at jb55.com
[2]

@_date: 2019-06-07 10:02:13
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Transcripts from coredev.tech Amsterdam 2019 meeting 
The following are some notes from the coredev.tech Amsterdam 2019 meeting.
Any mistakes are my probably my own.
Here is a conversation about the code review process in Bitcoin Core:
Here is a conversation with some of the maintainers about what problems
they are seeing:
Wallet re-architecture discussion
Great consensus cleanup
SIGHASH_NOINPUT, OP_CHECKSIGFROMSTACK, OP_CHECKOUTPUTSHASHVERIFY,
Taproot discussion
Hardware wallets and HWI
bip151, p2p encryption and v2 message format
Signet for bitcoin test networks
Statechains overview
- Bryan

@_date: 2019-06-08 09:21:40
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] testnet4 
Be greeted Emil,
On Sat, Jun 8, 2019 at 9:21 AM Emil Engler via bitcoin-dev <
At the moment, I somewhat doubt this is likely to happen. Signet provides
an alternative for configuring multiple separate private and public testing
networks. If you would like to get involved, check out the recent
discussion on the topic recorded here:
- Bryan

@_date: 2019-06-09 16:14:39
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Transcripts from Breaking Bitcoin 2019 
Hi all,
The following are some notes I took during Breaking Bitcoin 2019, selected
for relevance. Any mistakes are most likely my own.
Carl Dong gave an excellent talk on guix as a replacement for the gitian
build system:
but really just watch his presentation:
Mempool analysis, client-side filtering, client updates
Some privacy and coinjoin talks:
Hardware wallets:
Bitcoin upgrades:
p2pool analysis:
Lightning network:
Of possible interest (general, not really development focused):
- Bryan

@_date: 2019-03-05 18:34:05
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Mailing list downtime, archive, and its future 
Hi all,
Just fyi, but this bitcoin-dev mailing list has been down for a few weeks.
It's currently hosted by Linux Foundation, and they are slowly deprecating
their support for email. We will have to find an alternative service
provider for the mailing list moving forward. I have received a variety of
recommendations for how to move forward.
My one reservation in this process is that I am concerned about the
subscriber list and I am not sure how we want to treat this. I view this as
a possible privacy issue. For example, if we were to migrate to a new
mailing list, it's important that list subscribers are not disclosed. At
the same time, some people rely on this mailing list for important
announcements maybe even security announcements in some situations. I'd
appreciate feedback on what people think about this particular issue. Since
the mailing list is not reliable anymore, please remember to cc the
feedback to me directly.
In the mean time, here is an archive of the mailing list content including
some old timestamps in the opentimestamps format for some of the older
- Bryan
1 512 203 0507

@_date: 2019-10-04 05:02:59
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] ChainWallet - A way to prevent loss of funds by 
Since the user can't prove that they are using this technique, or
petertodd's timelock encryption for that matter, an attacker has little
incentive to stop physically attacking until they have a spendable UTXO.
I believe you can get the same effect with on-chain timelocks, or
delete-the-bits plus a rangeproof and a zero-knowledge proof that the
rangeproof corresponds to some secret that can be used to derive the
expected public key. I think Jeremy Rubin had an idea for such a proof.
Also, adam3us has described a similar thought here:
- Bryan
On Fri, Oct 4, 2019, 4:43 AM Saulo Fonseca via bitcoin-dev <

@_date: 2019-09-15 08:49:30
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] [BIP-able idea] Regular testnet reset 
As a reminder, here is where you last brought up the idea, and the feedback:
Since then, here is some new material on signet:
- Bryan
1 512 203 0507

@_date: 2019-09-16 09:29:05
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Transcripts from Scaling Bitcoin 2019 
Here are some transcripts of talks from Scaling Bitcoin 2019 Tel Aviv. Any
errors are most likely my own.
Training material
Training materials for bitcoin developers:
Foundation topics:
Developing Bitcoin Core:
Lightning network:
Scaling Bitcoin conference
LN, payment networks and hubs:
Private information retrieval methods for lightweight clients:
More privacy:
- Bryan
1 512 203 0507

@_date: 2020-04-13 10:50:00
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] On-chain vaults prototype 
High-security protection against theft depends on multisig and timelocks,
but more tools are possible. Last year I discussed one method where
would-be attackers are discouraged by specially designed vault covenants
[1] allowing re-vaulting transactions, where a watchtower can override a
proposed delayed-spend transaction during a public observation delay
period. Splitting coins into multiple timelocked UTXOs can give a user time
to react to theft of a much smaller portion of the total amount.
If better and better cold storage designs can be shared openly, reviewed,
and used easily, this can increase security for all bitcoin users. When the
understanding among the general public includes "bitcoin is extremely
valuable" then it becomes more urgent that the understanding in the general
public also includes "bitcoin cold storage security is impenetrable".
Today I would like to announce the release of an open-source prototype for
on-chain bitcoin vaults using pre-signed transactions and secure key
deletion. I am hoping for feedback and discussion around these concepts. To
be very clear, this is a prototype and not fit for production use.
During the delay period, this design allows initiation of a recovery or
clawback which triggers funds being moved to deeper cold storage.
Reviewers: Generally interested in your feedback about the concept. My hope
is that the prototype and its source code helps answer some questions about
how this might work. I would suggest to also pay close attention to the
script templates for both outputs and witnesses.
Also included is an implementation of this same bitcoin vault using bip119
I have also been working with Spencer Hommel, Jacob Swambo, and Bob
McElrath on two related manuscripts, one addressing the topic of bitcoin
covenants and the other addressing the topic of vaults based on pre-signed
transactions. As part of that project, there is a separate vault
implementation that is already available on Fidelity's github account [2].
A more bare bones implementation of python vaults can be found at [3].
Also, Kevin Loaec has an unrelated implementation using pre-signed
Thank you,
- Bryan
[2] [3]

@_date: 2020-02-09 14:19:55
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity 
The following is a message forwarded from an anonymous email that, for
whatever reason, couldn't be relayed through the mailing list without my
This email is the first of a collection of sentiments from a group of
who in aggregate prefer to remain anonymous. These emails have been sent
under a
pseudonym so as to keep the focus of discussion on the merits of the
issues, rather than miring the discussion in personal politics. Our goal
to cause a schism, but rather to help figure out what the path forward is
Taproot. To that end, we:
1) Discuss the merits of Taproot's design versus simpler alternatives (see
thread subject, "Taproot (and Graftroot) Complexity").
2) Propose an alternative path to deploying the technologies described in
BIP-340, BIP-341, and BIP-342 (see thread subject, "An Alternative
Path for Taproot Technologies").
3) Suggest a modification to Taproot to reduce some of the overhead (see
subject, "Taproot Public NUMS Optimization").
Now that the BIP has moved to draft we felt that now was the time to
review to make sure it was an acceptable change for our activities. As a
we're excited about the totality of what Taproot has to offer. However,
our review, we're left perplexed about the development of Taproot (and
Graftroot, to a lesser extent).
We also want to convey that we have nothing but respect for the developers
community who have poured their heart and soul into preparing Taproot. Self
evidently, it is an impressive synthesis of ideas. We believe that the
form of respect to pay such a synthesis of ideas is a detailed and critical
review, as it's pertinent to closely consider changes to Bitcoin.
In essence, Taproot is fundamentally the same as doing
 and Schnorr
signatures separately.
The main reason for putting them together -- as mentioned in the BIP -- is a
gain in efficiency. But this efficiency pre-supposes a specific use case and
probability distribution of use cases.
Suppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something
like this:
      /\
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f g h
If we want this to be functionally equivalent to Taproot, we add a new path:
       /\
      /\ { schnorr_checksig}
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f g h
Now, to spend from this MBV you have to reveal 32 bytes on the stack for
the not
taken branch, and 35 bytes for the  schnorr_checksig (1 byte push, 33
PK, 1 byte checksig).
This is 67 bytes more than Taproot would require for the same spending
However, suppose we wanted to use one of the script paths instead. We still
to have one extra hash for the { schnorr_checksig} (depending on if we
the key in this position or not--see below). But now we can spend with just
logarithmic control program path.
However, if we do the same script via taproot, we now need to provide the
public key (33 bytes) as well as the root hash (32 bytes) and path and then
actual scripts. With the need for 2 push bytes, this ends up being back at
bytes extra.
Is Taproot just a probability assumption about the frequency and likelihood
the signature case over the script case? Is this a good assumption?  The BIP
only goes as far as to claim that the advantage is apparent if the outputs
*could be spent* as an N of N, but doesn't make representations about how
that N of N case would be in practice compared to the script paths. Perhaps
among use cases, more than half of the ones we expect people to be doing
could be
spent as an N of N. But how frequently would that path get used? Further,
the *use cases* might skew toward things with N of N opt-out, we might end
up in
a power law case where it's the one case that doesn't use an N of N opt out
all (or at a de minimis level) that becomes very popular, thereby making
more costly then beneficial.
Further, if you don't want to use a Taproot top-level key (e.g., you need
to be
able to audit that no one can spend outside of one of the script
then you need to use a NUMS (nothing up my sleeve) point. This forces users
don't want Taproot to pay the expense, when if they just had a MAST based
witness type they would be cheaper. So if this use case is at all common,
Taproot leaves them worse off in terms of fees. Given that script paths are
usually done in the case where there is some contested close, it's actually
the interest of protocol developers that the contested script path be as
efficient as possible so that the fees paid maximally increase the feerate.
think this can be fixed simply in Taproot though, as noted below.
On privacy, we're also a bit confused as to the goal of Taproot over MAST
Schnorr. Earlier, we presented a design with MAST which is very close to
However, it'd also be possible to just add { schnorr_checksig} to the
{a,b,c,d,e,f,g,h}, shuffle them, and compute some MAST structure (perhaps
probability encoded) on them. This has the effect of not having much
fees for adding the extra Schnorr path at redeem time (only 1 extra branch
2/8 script paths), e.g.
      /\
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f/\ { schnorr_checksig}
          g  h
We could argue that this is more private than Taproot, because we don't
distinguish between the Schnorr key case and other cases by default, so
analyzers can't tell if the signature came from the Taproot case or from
one of
the Script paths. There's also no NUMS point required, which means chain
analyzers can't tell when you spend that there was no top level key if the
point is not per-output indistinguishable. By using a semi-randomized MAST
structure, chain analyzers also can't tell exactly how big your spend
MAST was. In particular, you care more about privacy when you are
contesting a
close of a channel or other script path because then the miners could be
likely to extract a rent from you as "ransom" for properly closing your
(or in other words, in a contested close the value of the closing
transaction is
larger than usual).
It would also be possible to do something really simple which is to allow
witness type to be either a MAST hash OR a schnorr key (but not a Taproot).
allows you to not completely fracture the anonymity set between people who
plain Schnorr and people who want MAST (at least until they go to spend).
fix can also be used in Taproot in place of a NUMS point, to decrease extra
fees. It's unclear if this plays negatively with any future batch validation
mechanism though, but the contextual checks to exclude a witness program
the batch are relatively simple. See thread subject, "Taproot Public NUMS
The considerations around Graftroot, a proposed delegation mechanism, is a
similar. Delegation is a mechanism by which a UTXO with script S can sign a
script R which can then be executed in addition to S without requiring a
transaction. This allows an output to monotonically and dynamically
increase the
number of conditions under which it can be spent. As noted by Pieter Wiulle
delegation was originally possible in Bitcoin, but got broken during an
emergency fork to split the scriptSig and scriptpubkey separation. Rather
adding some fancy delegation mechanism in Bitcoin, why not just have a
semantic which allows a delegated script to be evaluated? See BIP-117
 This way we
aren't special casing where delegation can occur, and we can allow taproot
nested spending conditions (i.e., with timelocks) to generate their own
delegations. As I've seen Graftroot discussed thus far, it is as a top-level
witness program version like Taproot and non-recursive. Similar to the above
discussion, top-level is more efficient if you suspect that delegation will
most likely occurring at the top level, but it's not clear that's a good
assumption as it may be common to want to allow different scripts to
Overall, we are left with concerns both about the merit of doing Taproot
versus alternatives, as well as the process through which we got to be here.
1) Is Taproot actually more private than bare MAST and Schnorr separately?
are the actual anonymity set benefits compared to doing the separately?
2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What
evidence do we have that the assumption it will be more common to use
with a key will outweigh Script cases?
3) Is Taproot riskier than bare MAST and Schnorr separately given the new
crypto? How well reviewed is the actual crypto parts? None of us personally
comfortable reviewing the crypto in Schnorr -- what's the set of people who
thoroughly reviewed the crypto and aren't just ACKing because they trust
developers to have looked at it close enough?
4) Design wise, couldn't we forego the NUMS point requirement and be able to
check if it's a hash root directly? This would encumber users who don't
need the
key path a cheaper spend path. See thread subject, "Taproot Public NUMS
5) Is the development model of trying to jam a bunch of features into
all at once good for Bitcoin development? Would we be better off if we
incremental improvements that can work together (e.g., MAST and then
Although the BIP raises some points about anonymity sets being why to do
all at once, it's not clear to me this argument holds water (same goes for
businesses not upgrading). If we can take things as smaller steps, we are
only more secure, but we also have more time to dedicate review to each
independently. We also end up co-mingling changes that people end up
only because they want one and they're bundled (e.g., MAST and Schnorr, MAST
seems like a much less risky addition versus Schnorr). See thread subject,
Alternative Deployment Path for Taproot Technologies".
Our provocation with this email is primarily that we think we should more
carefully consider the benefits of Taproot over simpler primitives that are
only easier to review, but could have been made available much sooner rather
than waiting on putting everything all together for an unclear aggregate
We do think that most of the developers have been honest about the benefits
Taproot, but that on closer look we feel the general ecosystem has oversold
Taproot as being the key enabler for a collection of techniques that we
could do
with much simpler building blocks.
At the end of the day, we do not strongly advocate not deploying Taproot at
point in the review cycle. We think the Taproot Public NUMS Optimization
may be
a good idea, worth considering if it's not insecure, as it cuts through the
where you would otherwise need a NUMS point. Things like TapScript and its
mechanisms are well designed and offer exciting new deployment paths, and
be something we would use even if we opted for MAST instead of Taproot.
we also believe it is our duty to raise these concerns and suggestions, and
look forward to listening to the responses of the community.
Great thanks,
The Group
SUBJECT: An Alternative Deployment Path for Taproot Technologies
This email is the second of a collection of sentiments from a group of
who in aggregate prefer to remain anonymous. These emails have been sent
under a
pseudonym so as to keep the focus of discussion on the merits of the
issues, rather than miring the discussion in personal politics. Our goal
to cause a schism, but rather to help figure out what the path forward is
Taproot. To that end, we:
1) Discuss the merits of Taproot's design versus simpler alternatives (see
thread subject, "Taproot (and Graftroot) Complexity").
2) Propose an alternative path to deploying the technologies described in
BIP-340, BIP-341, and BIP-342 (see thread subject, "An Alternative
Path for Taproot Technologies").
3) Suggest a modification to Taproot to reduce some of the overhead (see
subject, "Taproot Public NUMS Optimization").
As a follow up to our prior message, we propose a different path forward
for the
Taproot family of changes:
1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;
2) A separate soft-fork for Schnorr Signatures
3) A separate follow up soft-fork which enables Taproot and Graftroot
We think that the first 2 forks can be offered at the same time or one at a
Taproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork
on the
existing semantics, but requiring a new witness version. With the Public
NUMS Optimization, wallets could upgrade by just changing one version byte
to be
in the same anonymity set as Taproot.
It's not clear to us that the time to prepare a BIP and implementation for
1 and
2 at this point would be any less than the time to do Taproot as currently
proposed. However, we believe that such a deployment plan is a reasonable
as it is more conservative, as Merkle Branch witnesses are relatively
simple and
users only have to use Schnorr signing if they want to, and can otherwise
continue to use ECDSA. A further benefit of waiting on 3 is that we get to
collect real world protocol engineering experience to see how frequently the
Taproot frequency of use assumption holds, and if it is worth doing or not.
Great thanks,
The Group

@_date: 2020-02-09 14:22:56
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] An alternative deployment path for taproot technology 
The following is a message forwarded from an anonymous email that, for
whatever reason, couldn't be relayed through the mailing list without my
assistance. This is message (2/3).
This email is the second of a collection of sentiments from a group of
who in aggregate prefer to remain anonymous. These emails have been sent
under a
pseudonym so as to keep the focus of discussion on the merits of the
issues, rather than miring the discussion in personal politics. Our goal
to cause a schism, but rather to help figure out what the path forward is
Taproot. To that end, we:
1) Discuss the merits of Taproot's design versus simpler alternatives (see
thread subject, "Taproot (and Graftroot) Complexity").
2) Propose an alternative path to deploying the technologies described in
BIP-340, BIP-341, and BIP-342 (see thread subject, "An Alternative
Path for Taproot Technologies").
3) Suggest a modification to Taproot to reduce some of the overhead (see
subject, "Taproot Public NUMS Optimization").
As a follow up to our prior message, we propose a different path forward
for the
Taproot family of changes:
1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;
2) A separate soft-fork for Schnorr Signatures
3) A separate follow up soft-fork which enables Taproot and Graftroot
We think that the first 2 forks can be offered at the same time or one at a
Taproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork
on the
existing semantics, but requiring a new witness version. With the Public
NUMS Optimization, wallets could upgrade by just changing one version byte
to be
in the same anonymity set as Taproot.
It's not clear to us that the time to prepare a BIP and implementation for
1 and
2 at this point would be any less than the time to do Taproot as currently
proposed. However, we believe that such a deployment plan is a reasonable
as it is more conservative, as Merkle Branch witnesses are relatively
simple and
users only have to use Schnorr signing if they want to, and can otherwise
continue to use ECDSA. A further benefit of waiting on 3 is that we get to
collect real world protocol engineering experience to see how frequently the
Taproot frequency of use assumption holds, and if it is worth doing or not.
Great thanks,
The Group

@_date: 2020-02-09 14:24:32
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Taproot public NUMS optimization (Re: Taproot (and 
The following is a message forwarded from an anonymous email that, for
whatever reason, couldn't be relayed through the mailing list without my
assistance. This is message (3/3).
This email is the third of a collection of sentiments from a group of
who in aggregate prefer to remain anonymous. These emails have been sent
under a
pseudonym so as to keep the focus of discussion on the merits of the
issues, rather than miring the discussion in personal politics. Our goal
to cause a schism, but rather to help figure out what the path forward is
Taproot. To that end, we:
1) Discuss the merits of Taproot's design versus simpler alternatives (see
thread subject, "Taproot (and Graftroot) Complexity").
2) Propose an alternative path to deploying the technologies described in
BIP-340, BIP-341, and BIP-342 (see thread subject, "An Alternative
Path for Taproot Technologies").
3) Suggest a modification to Taproot to reduce some of the overhead (see
subject, "Taproot Public NUMS Optimization").
We propose to modify Taproot's specification in BIP-341 by adding the rule:
If there is one element on the witness stack:
1) Attempt hashing it to see if it's equal to  the witness program. The
byte is the control byte for leaf versioning.
2) If it's not the witness program, and it's 65 bytes, try signature
If there is more than one element on the witness stack:
If the control block is even, treat it as a non-Taproot MAST and get the
version as the last byte of the script (so you can pop it off before
If greater anonymity is required, a NUMS point can still be used in
Taproot, at
the expense of the additional data. However, if NUMS points are just a
well known constants this could actually decrease privacy as then the NUMS
points could differ from application to application fingerprinting wallets.
Instead, the NUMS point should only be used when a single use nonce can be
sent, so that NUMS cannot be distinguished from a normal Taproot to a third
party who doesn't know the setup (e.g., that the NUMS is H(X) for known X).
Great thanks,
The Group
- Bryan
1 512 203 0507

@_date: 2020-02-09 14:47:29
@_author: Bryan Bishop 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity (reflowed) 
Apologies for my previous attempt at relaying the message- it looks like
the emails got mangled on the archive. I am re-sending them in this
combined email with what I hope will be better formatting. Again this is
from some nym that had trouble posting to this mailing list; I didn't see
any emails in the queue so I couldn't help to publish this sooner.
SUBJECT: Taproot (and Graftroot) Complexity
This email is the first of a collection of sentiments from a group of
developers who in aggregate prefer to remain anonymous. These emails have
been sent under a pseudonym so as to keep the focus of discussion on the
merits of the technical issues, rather than miring the discussion in
personal politics.  Our goal isn't to cause a schism, but rather to help
figure out what the path forward is with Taproot. To that end, we:
1) Discuss the merits of Taproot's design versus simpler alternatives (see
thread subject, "Taproot (and Graftroot) Complexity").
2) Propose an alternative path to deploying the technologies described in
BIP-340, BIP-341, and BIP-342 (see thread subject, "An Alternative
Deployment Path for Taproot Technologies").
3) Suggest a modification to Taproot to reduce some of the overhead (see
thread subject, "Taproot Public NUMS Optimization").
Now that the BIP has moved to draft we felt that now was the time to
prioritize review to make sure it was an acceptable change for our
activities. As a group, we're excited about the totality of what Taproot
has to offer. However, after our review, we're left perplexed about the
development of Taproot (and Graftroot, to a lesser extent).
We also want to convey that we have nothing but respect for the developers
and community who have poured their heart and soul into preparing Taproot.
Self evidently, it is an impressive synthesis of ideas. We believe that the
highest form of respect to pay such a synthesis of ideas is a detailed and
critical review, as it's pertinent to closely consider changes to Bitcoin.
In essence, Taproot is fundamentally the same as doing
 and Schnorr
signatures separately.
The main reason for putting them together -- as mentioned in the BIP -- is
a gain in efficiency. But this efficiency pre-supposes a specific use case
and probability distribution of use cases.
Suppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something
like this:
      /\
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f g h
If we want this to be functionally equivalent to Taproot, we add a new path:
       /\
      /\ { schnorr_checksig}
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f g h
Now, to spend from this MBV you have to reveal 32 bytes on the stack for
the not taken branch, and 35 bytes for the  schnorr_checksig (1 byte
push, 33 bytes PK, 1 byte checksig).
This is 67 bytes more than Taproot would require for the same spending
However, suppose we wanted to use one of the script paths instead. We still
need to have one extra hash for the { schnorr_checksig} (depending on
if we put the key in this position or not--see below). But now we can spend
with just a logarithmic control program path.
However, if we do the same script via taproot, we now need to provide the
base public key (33 bytes) as well as the root hash (32 bytes) and path and
then the actual scripts. With the need for 2 push bytes, this ends up being
back at 67 bytes extra.
Is Taproot just a probability assumption about the frequency and likelihood
of the signature case over the script case? Is this a good assumption?  The
BIP only goes as far as to claim that the advantage is apparent if the
outputs *could be spent* as an N of N, but doesn't make representations
about how likely that N of N case would be in practice compared to the
script paths. Perhaps among use cases, more than half of the ones we expect
people to be doing could be spent as an N of N. But how frequently would
that path get used? Further, while the *use cases* might skew toward things
with N of N opt-out, we might end up in a power law case where it's the one
case that doesn't use an N of N opt out at all (or at a de minimis level)
that becomes very popular, thereby making Taproot more costly then
Further, if you don't want to use a Taproot top-level key (e.g., you need
to be able to audit that no one can spend outside of one of the script
conditions), then you need to use a NUMS (nothing up my sleeve) point. This
forces users who don't want Taproot to pay the expense, when if they just
had a MAST based witness type they would be cheaper. So if this use case is
at all common, Taproot leaves them worse off in terms of fees. Given that
script paths are usually done in the case where there is some contested
close, it's actually in the interest of protocol developers that the
contested script path be as efficient as possible so that the fees paid
maximally increase the feerate. We think this can be fixed simply in
Taproot though, as noted below.
On privacy, we're also a bit confused as to the goal of Taproot over MAST
and Schnorr. Earlier, we presented a design with MAST which is very close
to Taproot.  However, it'd also be possible to just add {
schnorr_checksig} to the set {a,b,c,d,e,f,g,h}, shuffle them, and compute
some MAST structure (perhaps probability encoded) on them. This has the
effect of not having much additional fees for adding the extra Schnorr path
at redeem time (only 1 extra branch on 2/8 script paths), e.g.
      /\
     /  \
    /    \
   /      \
  /\      /\
 /  \    /  \
a b c d e f/\ { schnorr_checksig}
          g  h
We could argue that this is more private than Taproot, because we don't
distinguish between the Schnorr key case and other cases by default, so
chain analyzers can't tell if the signature came from the Taproot case or
from one of the Script paths. There's also no NUMS point required, which
means chain analyzers can't tell when you spend that there was no top level
key if the NUMS point is not per-output indistinguishable. By using a
semi-randomized MAST structure, chain analyzers also can't tell exactly how
big your spend condition MAST was. In particular, you care more about
privacy when you are contesting a close of a channel or other script path
because then the miners could be more likely to extract a rent from you as
"ransom" for properly closing your channel (or in other words, in a
contested close the value of the closing transaction is larger than usual).
It would also be possible to do something really simple which is to allow
the witness type to be either a MAST hash OR a schnorr key (but not a
Taproot). This allows you to not completely fracture the anonymity set
between people who want plain Schnorr and people who want MAST (at least
until they go to spend). This fix can also be used in Taproot in place of a
NUMS point, to decrease extra fees. It's unclear if this plays negatively
with any future batch validation mechanism though, but the contextual
checks to exclude a witness program from the batch are relatively simple.
See thread subject, "Taproot Public NUMS Optimization".
The considerations around Graftroot, a proposed delegation mechanism, is a
bit similar. Delegation is a mechanism by which a UTXO with script S can
sign a script R which can then be executed in addition to S without
requiring a transaction. This allows an output to monotonically and
dynamically increase the number of conditions under which it can be spent.
As noted by Pieter Wiulle here:
delegation was originally possible in Bitcoin, but got broken during an
emergency fork to split the scriptSig and scriptpubkey separation. Rather
than adding some fancy delegation mechanism in Bitcoin, why not just have a
P2SH-like semantic which allows a delegated script to be evaluated? See
BIP-117 This way we aren't special casing where delegation can occur, and we can
allow taproot nested spending conditions (i.e., with timelocks) to generate
their own delegations. As I've seen Graftroot discussed thus far, it is as
a top-level witness program version like Taproot and non-recursive. Similar
to the above discussion, top-level is more efficient if you suspect that
delegation will be most likely occurring at the top level, but it's not
clear that's a good assumption as it may be common to want to allow
different scripts to delegate.
Overall, we are left with concerns both about the merit of doing Taproot
versus alternatives, as well as the process through which we got to be here.
1) Is Taproot actually more private than bare MAST and Schnorr separately?
What are the actual anonymity set benefits compared to doing the separately?
2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What
evidence do we have that the assumption it will be more common to use
Taproot with a key will outweigh Script cases?
3) Is Taproot riskier than bare MAST and Schnorr separately given the new
crypto? How well reviewed is the actual crypto parts? None of us personally
feel comfortable reviewing the crypto in Schnorr -- what's the set of
people who have thoroughly reviewed the crypto and aren't just ACKing
because they trust other developers to have looked at it close enough?
4) Design wise, couldn't we forego the NUMS point requirement and be able
to check if it's a hash root directly? This would encumber users who don't
need the key path a cheaper spend path. See thread subject, "Taproot Public
NUMS Optimization".
5) Is the development model of trying to jam a bunch of features into
Bitcoin all at once good for Bitcoin development? Would we be better off if
we embraced incremental improvements that can work together (e.g., MAST and
then Schnorr)?  Although the BIP raises some points about anonymity sets
being why to do them all at once, it's not clear to me this argument holds
water (same goes for businesses not upgrading). If we can take things as
smaller steps, we are not only more secure, but we also have more time to
dedicate review to each change independently. We also end up co-mingling
changes that people end up accepting only because they want one and they're
bundled (e.g., MAST and Schnorr, MAST seems like a much less risky addition
versus Schnorr). See thread subject, "An Alternative Deployment Path for
Taproot Technologies".
Our provocation with this email is primarily that we think we should more
carefully consider the benefits of Taproot over simpler primitives that are
not only easier to review, but could have been made available much sooner
rather than waiting on putting everything all together for an unclear
aggregate benefit.
We do think that most of the developers have been honest about the benefits
of Taproot, but that on closer look we feel the general ecosystem has
oversold Taproot as being the key enabler for a collection of techniques
that we could do with much simpler building blocks.
At the end of the day, we do not strongly advocate not deploying Taproot at
this point in the review cycle. We think the Taproot Public NUMS
Optimization may be a good idea, worth considering if it's not insecure, as
it cuts through the case where you would otherwise need a NUMS point.
Things like TapScript and its MAST mechanisms are well designed and offer
exciting new deployment paths, and would be something we would use even if
we opted for MAST instead of Taproot. However, we also believe it is our
duty to raise these concerns and suggestions, and we look forward to
listening to the responses of the community.
Great thanks,
The Group
SUBJECT: An Alternative Deployment Path for Taproot Technologies
This email is the second of a collection of sentiments from a group of
developers who in aggregate prefer to remain anonymous. These emails have
been sent under a pseudonym so as to keep the focus of discussion on the
merits of the technical issues, rather than miring the discussion in
personal politics. Our goal isn't to cause a schism, but rather to help
figure out what the path forward is with Taproot. To that end, we: [clip
As a follow up to our prior message, we propose a different path forward
for the Taproot family of changes:
1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;
2) A separate soft-fork for Schnorr Signatures
3) A separate follow up soft-fork which enables Taproot and Graftroot
We think that the first 2 forks can be offered at the same time or one at a
Taproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork
on the existing semantics, but requiring a new witness version. With the
Public NUMS Optimization, wallets could upgrade by just changing one
version byte to be in the same anonymity set as Taproot.
It's not clear to us that the time to prepare a BIP and implementation for
1 and 2 at this point would be any less than the time to do Taproot as
currently proposed. However, we believe that such a deployment plan is a
reasonable option as it is more conservative, as Merkle Branch witnesses
are relatively simple and users only have to use Schnorr signing if they
want to, and can otherwise continue to use ECDSA. A further benefit of
waiting on 3 is that we get to collect real world protocol engineering
experience to see how frequently the Taproot frequency of use assumption
holds, and if it is worth doing or not.
Great thanks,
The Group
SUBJECT: Taproot Public NUMS Optimization
This email is the third of a collection of sentiments from a group of
developers who in aggregate prefer to remain anonymous. These emails have
been sent under a pseudonym so as to keep the focus of discussion on the
merits of the technical issues, rather than miring the discussion in
personal politics. Our goal isn't to cause a schism, but rather to help
figure out what the path forward is with Taproot. To that end, we: [clipped
We propose to modify Taproot's specification in BIP-341 by adding the rule:
If there is one element on the witness stack:
1) Attempt hashing it to see if it's equal to  the witness program. The
first byte is the control byte for leaf versioning.
2) If it's not the witness program, and it's 65 bytes, try signature
If there is more than one element on the witness stack:
If the control block is even, treat it as a non-Taproot MAST and get the
leaf version as the last byte of the script (so you can pop it off before
If greater anonymity is required, a NUMS point can still be used in
Taproot, at the expense of the additional data. However, if NUMS points are
just a couple well known constants this could actually decrease privacy as
then the NUMS points could differ from application to application
fingerprinting wallets.  Instead, the NUMS point should only be used when a
single use nonce can be sent, so that NUMS cannot be distinguished from a
normal Taproot to a third party who doesn't know the setup (e.g., that the
NUMS is H(X) for known X).
Great thanks,
The Group
