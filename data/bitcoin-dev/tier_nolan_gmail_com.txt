
@_date: 2013-12-24 10:47:26
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Peer Discovery and Overlay 
There was a BIP by Stefan Thomas for adding custom services to the
protocol.  Discovery would be helpful here too.  If this was added, it
wouldn't be intended for use in a hostile way though.
This one was the custom services BIP.  It defines a change to the version
message and also custom sub-commands.
This one discusses how network discovery should be handles.

@_date: 2013-07-23 12:27:03
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Distributing low POW headers 
I was thinking about a change to the rules for distinguishing between forks
and maybe a BIP..
- Low POW headers should be broadcast by the network
If a header has more than 1/64 of the POW of a block, it should be
broadcast.  This provides information on which fork is getting most of the
hashing power.
- Miners should use the header information to decide on longest chain
The fork selection rule for miners should be biased towards staying on the
fork that has the most hashing power.
This means that they might keep hashing on a fork that is 1-2 blocks
If most miners follow the rule, then it is the best strategy for other
miners to also follow this rule.
- Advantages
This lowers the probability of natural and malicious reversals.
*Distributing low POW headers*
First block header messages that have more than 1/64 of the standard POW
requirements would be forwarded.
This means the client needs to maintain a short term view of the entire
header tree.
if (header extends header tree) {
  if (header meets full POW) {
    add to header tree;
    forward to peers;
    check if any blocks in storage now extend the header tree
  } else {
    if (header meets POW / 64) {
      forward to peers;
    }
} else {
  if (header meets POW) {
    add to orphan header storage
  }
The storage could be limited and headers could be discarded after a while.
This has the extra advantage that it informs clients of forks that are
receiving hashing power.
This could be linked to a protocol version to prevent disconnects due to
invalid header messages.
*Determining the longest chain*
Each link would get extra credit for headers received.
Assume there are 2 forks starting at block A as the fork point.
A(63) <- B(72) <- C(37) <- D(58)
A(63) <- B'(6) <- C'(9) <- D'(4) <- E(7) <- F(6)
The numbers in brackets are the number of low POW headers received that
have those blocks as parent.
The new rule is that the POW for a block is equal to
POW * (1 + (headers / 16))
Only headers within  of the end of the (shorter) chain
count.  However, in most cases, that doesn't matter since the fork point
will act as the start point.  As long as miners keep headers for 30-40
blocks, they will likely have all headers back to any reasonable fork point.
This means that the top fork is considered longer, since it has much more
headers, even though it has 2 less blocks.
If 75% of the miners follow this rule, then the top fork will eventually
catch up and win, so it is in the interests of the other 25% to follow the
rule too.
Even if there isn't complete agreement on headers received, the fork that
is getting the most hashing will naturally gain most of the headers, so
ties will be broken quickly.

@_date: 2013-07-24 12:55:33
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Distributing low POW headers 
The are a range of workable values.  Ideally, there would first need to be
agreement on the general principle.
Distributing headers with 1/64 of the standard POW means that a header
would be broadcast approximately once every 9 seconds (assuming a 10 minute
block time).  This was picked because sending 80 byte headers every 9
seconds shouldn't represent much load on the network.
The second magic number is how much credit to give for mini-headers.
Setting it at 1/16 means that the headers will be worth around 4 times as
much as a block (since there would be around 63 low POW headers for each
full POW one).
This creates an incentive for miners to take headers into account.  If all
the headers were worth less than a full block, then a fork which was losing
would suddenly be winning if a block is found.  A fork will only become the
main chain due to a new block, if it is within 16 mini-confirms.
Miners don't have to mine against the absolute best fork, but they do need
to make sure they stay within 16 of the best one (so if they find a block,
that block would be considered part of the main chain).  Some hysteresis
might be helpful.  The rule could be to only switch unless the current fork
is losing by at least 4 mini-confirms.
In most cases, this won't be a problem, since orphans don't happen that
often anyway.
Since it isn't a chain, this doesn't give the full benefits of a 9 second
block, but it should bring things to consensus faster.  6 full confirms
would be much more secure against random and hostile reversals.
It doesn't have the risks of 9 second blocks in causing network collapse,
since it isn't a chain, the headers are short, and there is no
confirmations of the required (other than checking the hash).
Each "mini" confirms adds to the strength of leaf blocks of the tree.  If
there is a tie, and 20% of the network is mining one block and 80% is
mining the other, the mining power of the network will be split until the
next block arrives.
With mini confirms, the entire network is aware of the 2 blocks (since the
headers would be forwarded) and the mini-confirms would show which one has
majority hashing power.
The least risk option would be to make them purely advisory.  The proposal
takes it further than that.
The proposal means that if the network is split 80/20, then miners should
stick with the 80% fork, even if the 20% fork wins the race for the next
Winning a few rounds is easier than wining many rounds worth of
The key is that as long as the honest miners stay on the main chain, they
will eventually overwhelm any rewrite attack with less than 50% of the
mining power.  This is a system to agree on what is the main chain in the
face of a re-write attack.
The (sub) proposal is that headers would still be broadcast.  The blocks
would not be forwarded.
If a header extends the header tree, meets full POW and is "near" the end
of the chain, then it is broadcast.  This means that all nodes will have
the entire header tree, including orphans.
The full blocks would only be sent if they extend the main chain.
Second, if a header builds on a header that is in the header tree, then it
is broadcast, even if it doesn't meet full POW (only 1/64 required).  This
gives information on which fork is getting the most power.
It gives information about potential "consensus loss" forks, where a
significant number of miners are following an alternative chain.
In fact, this is probably worth doing as an initial step.
A warning could be displayed on the client if a fork is getting more than
15% of the hashing power.

@_date: 2013-07-28 21:07:34
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Distributing low POW headers 
It has the same statistic properties as normal blocks just 64 times faster.
Even if there is a new block 30 seconds after the previous one, that
doesn't cause a burst of 64 low POW block headers in the 30 second window.
They are all statistically independent hashing attempts.
No, it just breaks ties.  In most cases there would be only 1 contender
block, so all miners are equal.
If 10% of blocks were ties/orphans, then only 1% of blocks would be a 3-way
tie.  That probably overestimates the orphan rate.
This means the miner has to download 2 blocks 10% of the time and 3 blocks
1% of the time.
However, even then, half the network wouldn't have to download the 2nd
block of the tie, since they happened to get the winner first.  This means
5% extra bandwidth on average.
16 low POW headers at 9 seconds per header is more than 2 minutes for a
miner to switch to the other contender.
A miner would only lose out if he doesn't notice that block he is mining
against is not getting built on by anyone else.
He needs to download both tied blocks so that he can switch, but he has 2
minutes to actually switch.
I understand Pieter Wuille is working on letting Bitcoin propagate and make
That would need to happen before low POW ones are broadcast.  There is a
basic set of rules in the first post.
At the moment, the client only provides headers when asked, but never
broadcasts them.
I think distributing the low POW headers on an advisory basis a reasonable
first step.  However, just broadcasting the headers is a zeroth step.
Miners would probably break ties towards the block that seems to be getting
the most hashing anyway.
I think for orphan rate, the best is to have a system to link to orphans.
This would add the POW of the orphan to the main chain's total.
Unfortunately adding fields to the header is hard.  It could be done as a
coinbase extra-nonce thing.  A better option would be if the merkle tree
could include non-transactions.
The merkle root could be replaced by hash(auxiliary header).  This has the
advantage of not impacting ASIC miners.
Broadcasting all headers would at least allow clients to count orphans,
even if they aren't integrated into the block chain.
It wouldn't have the potential data rate issues either
I don't think the data rate is really that high.  It would be 80 bytes
every 9 seconds, or 9 bytes per second.
Blocks are 500kB every 10 minutes, or 853 bytes per second.
Right absolutely.  Headers of blocks that add to the block tree within
recent history should be forwarded.
The inv system would need to be tweaked, since it can only say block and
A block header field would allow the node to say that it only has the
header.  Alternatively, it would reply with a header message to the
getblocks message.

@_date: 2013-11-06 12:25:31
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [ANN] High-speed Bitcoin Relay Network 
Wouldn't this cause disconnects due to misbehavior?
A standard node connecting to a relay node would receive
blocks/transactions that are not valid in some way and then disconnect.
Have you looked though the official client to find what things are
considered signs that a peer is hostile?  I assume things like double
spending checks count as misbehavior and can't be quickly checked by a
relay node.
Maybe another bit could be assigned in the services field as "relay".  This
means that the node doesn't do any checking.
Connects to relay nodes could be command line/config file only.  Peers
wouldn't connect to them.

@_date: 2014-04-07 20:13:07
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
A range seems like a great compromise.  Putting it in the address is also a
pretty cool.
If light nodes selected a random contiguous 1GB of the block-chain, then
they could handle most of the download overhead, rather than the full nodes.
Another way to do it would be to have something like a routing table.  If a
node is queried for a block, it can reply with the IP of a node with that
block instead of sending the block.
One problem is that it means that light nodes have to accept incoming
connections.  Otherwise, it would have to be routed through the network.

@_date: 2014-04-07 22:48:03
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
The isn't strictly true.  If you are connected to a some honest nodes, then
you could download portions of the chain and then connect the various
sub-chains together.
The protocol doesn't support it though.  There is no system to ask for
block headers for the main chain block with a given height,
Finding one high bandwidth peer to download the entire header chain
sequentially is pretty much forced.  The client can switch if there is a
Other peers could be used to parallel download the block chain while the
main chain is downloading.  Even if the header download stalled, it
wouldn't be that big a deal.
the headers.
the blocks unless you have plenty of RAM.
You only need to store the UTXO set, rather than the entire block chain.
It is possible to generate the UTXO set without doing any signature
A lightweight node could just verify the UTXO set and then do random
signature verifications.
The keeps disk space and CPU reasonably low.  If an illegal transaction is
added to be a block, then proof could be provided for the bad transaction.
The only slightly difficult thing is confirming inflation.  That can be
checked on a block by block basis when downloading the entire block chain.
I hope I'm not thread-jacking here, apologies if so, but that's the
approach I've taken with the node I'm working on.
Headers can be downloaded and stored in any order, it'll make sense of what
the winning chain is. Blocks don't need to be downloaded in any particular
order and they don't need to be saved to disk, the UTXO is fully
self-contained. That way the concern of storing blocks for seeding (or not)
is wholly separated from syncing the UTXO. This allows me to do the initial
blockchain sync in ~6 hours when I use my SSD. I only need enough disk
space to store the UTXO, and then whatever amount of block data the user
would want to store for the health of the network.
This project is a bitcoin learning exercise for me, so I can only hope I
don't have any critical design flaws in there. :)
Once headers are loaded first there is no reason for sequential loading.
Validation has to be sequantial, but that step can be deferred until the
blocks before a point are loaded and continous.
Tamas Blummer
On Mon, Apr 7, 2014 at 12:00 PM, Tamas Blummer therefore I guess it is more handy to return some bitmap of pruned/full
blocks than ranges.
A bitmap also means high overhead and-- if it's used to advertise
non-contiguous blocks-- poor locality, since blocks are fetched

@_date: 2014-04-07 23:14:10
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
I think downloading from a subset of the peers and switching out any slow
ones is a reasonable compromise.
Once you have a chain, you can quickly check that all peers have the same
main chain.
Your backend system could have a method that gives you the hash of the last
10 headers on the longest chain it knows about.  You can use the block
locator hash system.
This can be used with the getheaders message and if the new peer is on a
different chain, then it will just send you the headers starting at the
genesis block.
If that happens, you need to download the entire chain from that peer and
see if it is better than your current best.
*From:* Tier Nolan *Sent:* Monday, April 07, 2014 6:48 PM
*To:* bitcoin-development at lists.sourceforge.net
The isn't strictly true.  If you are connected to a some honest nodes, then
you could download portions of the chain and then connect the various
sub-chains together.
The protocol doesn't support it though.  There is no system to ask for
block headers for the main chain block with a given height,
Finding one high bandwidth peer to download the entire header chain
sequentially is pretty much forced.  The client can switch if there is a
Other peers could be used to parallel download the block chain while the
main chain is downloading.  Even if the header download stalled, it
wouldn't be that big a deal.
the headers.
the blocks unless you have plenty of RAM.
You only need to store the UTXO set, rather than the entire block chain.
It is possible to generate the UTXO set without doing any signature
A lightweight node could just verify the UTXO set and then do random
signature verifications.
The keeps disk space and CPU reasonably low.  If an illegal transaction is
added to be a block, then proof could be provided for the bad transaction.
The only slightly difficult thing is confirming inflation.  That can be
checked on a block by block basis when downloading the entire block chain.
I hope I'm not thread-jacking here, apologies if so, but that's the
approach I've taken with the node I'm working on.
Headers can be downloaded and stored in any order, it'll make sense of what
the winning chain is. Blocks don't need to be downloaded in any particular
order and they don't need to be saved to disk, the UTXO is fully
self-contained. That way the concern of storing blocks for seeding (or not)
is wholly separated from syncing the UTXO. This allows me to do the initial
blockchain sync in ~6 hours when I use my SSD. I only need enough disk
space to store the UTXO, and then whatever amount of block data the user
would want to store for the health of the network.
This project is a bitcoin learning exercise for me, so I can only hope I
don't have any critical design flaws in there. :)
Once headers are loaded first there is no reason for sequential loading.
Validation has to be sequantial, but that step can be deferred until the
blocks before a point are loaded and continous.
Tamas Blummer
On Mon, Apr 7, 2014 at 12:00 PM, Tamas Blummer therefore I guess it is more handy to return some bitmap of pruned/full
blocks than ranges.
A bitmap also means high overhead and-- if it's used to advertise
non-contiguous blocks-- poor locality, since blocks are fetched

@_date: 2014-04-10 18:30:32
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Error correction is an interesting suggestion.
If there was 10000 nodes and each stored 0.1% of the blocks, at random,
then the odds of a block not being stored is 45 in a million.
Blocks are stored on average 10 times, so there is already reasonable
With 1 million blocks, 45 would be lost in that case, even though most are
stored multiple times.
With error correction codes, the chances of blocks going missing is much
For example, if there was 32 out of 34 Reed-Solomon-like system, then 2
blocks out of 34 could be lost without any actual data loss for the network.
As a back of the envelop check, the odds of 2 missing blocks landing within
34 of another is 68/1000000.  That means that the odds of 2 missing blocks
falling in the same correction section is 45 * 34 / 1000000 = 0.153%.  Even
in that case, the missing blocks could be reconstructed, as long as you
know that they are missing.
The error correction code has taken it from being a near certainty that
some blocks would be lost to less than 0.153%.
A simple error correction system would just take 32 blocks in sequence and
then compute 2 extra blocks.
The extra blocks would have to be the same length as the longest block in
the 32 being corrected.
The shorter blocks would be padded with zeroes so everything is the same
For each byte position in the blocks you compute the polynomial that goes
through byte (x, data(x)), for x = 0 to 31.  This could be a finite field,
or just mod 257.
You can then compute the value for x=32 and x = 33.  Those are the values
for the 2 extra blocks.
If mod 257 is used, then only the 2 extra blocks have to deal with symbols
from 0 to 256.
If you have 32 of the 34 blocks, you can compute the polynomial and thus
generate the 32 actual blocks.
This could be achieved by a soft fork by having a commitment every 32
blocks in the coinbase.
It makes the header chain much longer though.
Longer sections are more efficient, but need more calculations to recover
everything.  You could also do interleaving to handle the case where entire
sections are missing.

@_date: 2014-04-10 21:12:00
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Chain pruning 
If there are invalidation proofs, then this isn't strictly true.
If you are connected to 10 nodes and only 1 is honest, it can send you the
proof that your main chain is invalid.
For bad scripts, it shows you the input transaction for the invalid input
along with the merkle path to prove it is in a previous block.
For double spends, it could show the transaction which spent the output.
Double spends are pretty much the same as trying to spend non-existent
outputs anyway.
If the UTXO set commit was actually a merkle tree, then all updates could
be included.
Blocks could have extra data with the proofs that the UTXO set is being
updated correctly.
To update the UTXO set, you need the paths for all spent inputs.
It puts a large load on miners to keep things working, since they have to
run a full node.
If they commit the data to the chain, then SPV nodes can do local checking.
One of them will find invalid blocks eventually (even if one of the other
miners don't).

@_date: 2014-04-17 22:41:55
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
How does this system handle problems with the lower chains after they have
been "locked-in"?
The rule is that if a block in the child chain is pointed to by its parent,
then it effectively has infinite POW?
The point of the system is that a node monitoring the parent chain only has
to watch the header chain for its 2 children.
A parent block header could point to an invalid block in one of the child
chains.  That parent block could end up built on top of before the problem
was discovered.
This would mean that a child chain problem could cause a roll-back of a
parent chain.  This violates the principle that parents are dominant over
child chains.
Alternatively, the child chain could discard the infinite POW blocks, since
they are illegal.
P1 -> C1
P2 -> ---
P3 -> C3
P4 -> C5
It turns out C4 (or C5) was an invalid block
P5 -> C4'
P6 -> ---
P7 -> C8'
This is a valid sequence.  Once P7 points at C8, the alternative chain
displaces C5.
This displacement could require a compact fraud proof to show that C4 was
an illegal block and that C5 was built on it.
This shouldn't happen if the miner was actually watching the log(N) chains,
but can't be guaranteed against.
I wonder if the proof of stake "nothing is at stake" principle applies
here.  Miners aren't putting anything at stake by merge mining the lower
At minimum, they should get tx-fees for the lower chains that they merge
mine.  The rule could require that the minting reward is divided over the
merge mined chains.

@_date: 2014-04-21 12:34:49
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Economics of information propagation 
I don't think it reduces security much.  It is extremely unlikely that
someone would publish an invalid block, since they would waste their POW.
Presuming that new headers are correct is reasonable, as long as you check
the full block within a few minutes of receiving the header.
If anything, it increases security, since less hashing power is wasted
while the full block is broadcast.
Block propagation could take the form
- broadcast new header
- all miners switch to mining empty blocks
- broadcast new block
- miners update to a block with transactions
If the block doesn't arrive within a timeout, then the miner could switch
back to the old block.
This would mean that a few percent of empty blocks end up in the
blockchain, but that doesn't do any harm.
It is only harmful, if it is used as a DOS attack on the network.
The empty blocks will only occur when 2 blocks are found in quick
succession, so it doesn't have much affect on average time until 1
confirm.  Empty blocks are just as good for providing 1 of the 6 confirms
needed too.

@_date: 2014-04-23 19:39:40
@_author: Tier Nolan 
@_subject: [Bitcoin-development] New BIP32 structure 
Different users could have different gap limit requirements.  20 seems very
low as the default.
A merchant could easily send 20 addresses in a row to customers and none of
them bother to actually buy anything.
Setting the gap limit to high is just a small extra cost in that case.
Bip-32 serialization doesn't have a way of adding meta data though.

@_date: 2014-04-23 19:58:16
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Bitcoin has various checks and balances that help keep everything honest.
Even if a pool had 60% of the hashing power, they couldn't reverse 6 blocks
without anyone noticing that it had happened.
There are sites which monitor the blocks and estimate the percentage of the
blocks found by each pool.
In a way, bitcoin doesn't depend on the majority of miners following the
protocol, it depends on miners believing that a majority of the other
miners will follow the protocol.
If a miner has 5% of the hashing power and believes that the other 95% will
follow the protocol, then the system should be set up so that it is in that
miner's interests to follow the protocol too.
This is why soft forks work.  The formal process convinces all the miners
that the new rules are locked in.
In a system where miners can vote to cancel coinbases, each pool has an
incentive to vote to reject everyone else's blocks.
Pools on the receiving end will be less profitable and lose customers.
It is possible that "predatory" pools would lose hashing power as miners
switch to other pools, in protest.
The proposal allows "established" pools to vote to disallow new entrants.
They could even justify it by saying that those pools haven't invested in
"anti-double spending" infrastructure.
The proposal doesn't suddenly give the majority the ability to do it, but
it isn't clear that making the process less disruptive is a good thing.

@_date: 2014-04-23 20:00:41
@_author: Tier Nolan 
@_subject: [Bitcoin-development] New BIP32 structure 
I meant for a merchant with a server that is handing out hundreds of
The point is to have a single system that is compatible over a large number
of systems.

@_date: 2014-04-23 22:23:08
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
An interesting experiment would be a transaction "proof of publication"
Each transaction would be added to that chain when it is received.  It
could be merge mined with the main chain.
If the size was limited, then it doesn't even require spam protection.
Blocks could be "discouraged" if they have transactions which violate the
ordering in that chain.  Miners could still decide which transactions they
include, but couldn't include transactions which are double spends.
The locktime/final field could be used for transactions which want to be
The chain could use some of the fast block proposals.  For example, it
could include orphans of a block when computing the block's POW.

@_date: 2014-04-23 23:26:08
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Interesting.  You set the share-block size to 16kB and set the share POW to
1/64 of the main target.
Each share-block would be allowed to append up to 16kB on the previous
This would keep the bandwidth the same, but on average blocks would be only
e.g. to get you fast confirmation.", or
This effect could be reduced by having "colours" for blocks and
The block colour would be a loop based on block height.
You could have 16 transaction "colours" based on the lowest 4 bits in the
A transaction is only valid if all inputs into the transaction are the
correct colour for that block.
This allows blocks to be created in advance.  If you are processing colour
7 at the moment, you can have a colour 8 block ready.
16 colours is probably to many.   It would only be necessary for things
like 1 second block rates.
The disadvantage is that wallets would have to make sure that they have
coins for each of the 16 colours.
If you spend the wrong colour, you add 16 block times of latency.
In a shop setting, you could set it up so that the person scans a QR-code
to setup a channel with the shop.
They can then scan all their stuff and by the time they have done that, the
channel would be ready.
If there was a queue, it could be done when the person enters the queue.
In fact, there could be QR-codes at multiple locations.

@_date: 2014-04-25 19:49:35
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Selector Script 
This is a BIP to allow the spender to choose one of multiple standard
scripts to use for spending the output.
This is required as part of the atomic cross chain transfer protocol.  It
is required so that outputs can be retrieved, if the process ends before
being committed.
The script allows multiple standard scripts to be included in the
When redeeming the script the spender indicates which of the standard
scripts to use.
Only one standard script is actually executed, so the only cost is the
extra storage required.
A more ambitious change would be a soft fork like P2SH, except the spender
is allowed to select from multiple hashes.  Effectively, it would be
This gets much of the benefits of MAST, but it requires a formal soft fork
to implement.
If there is agreement, I can code up the reference implementation as a PR.
The multi-P2SH might actually be easier.

@_date: 2014-04-25 19:49:37
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
As part of the atomic cross chain system, outputs need to be hash locked.
A user needs to provide x corresponding to hash(x) in order to spend an
Under the protocol, one of the participants is required to provide the
secret number in order to spend an output.  Once they do that, the other
participant can use the secret number to spend an output on the other
chain.  This provides a mechanism to link the 2 chains together (in
addition to lock times).  Once the first output is spent, that commits the
This is half of the scripting operations required to implement the protocol.
The proposal is to make this an adder on to the other standard
transactions.  It does a check that the hash matches, and then runs the
standard transaction as normal.
Adding the prefix to a P2SH transactions wouldn't work, since the template
wouldn't match.
A script of this form could be embedded into a P2SH output.
I think that is ok, since embedding the "password" in the hashed script
gets all the benefits.
If there is agreement, I can code up the reference implementation as a PR.

@_date: 2014-04-25 20:37:53
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
The hash locking isn't to prevent someone else stealing your coin.  Once a
user broadcasts a transaction with x in it, then everyone has access to x.
It is to release the coin on the other chain.  If you spend the output, you
automatically give the other participant the password to take your coin on
the other chain (completing the trade).
The BIP allows the hash to protect any of other standard transactions
(except P2SH, since that is a template match).
For example, it would allow a script of the form
OP_HASH160 [20-byte-password-hash] OP_EQUAL_VERIFY OP_DUP OP_HASH160
 OP_EQUALVERIFY OP_CHECKSIG
To spend it, you would need to provide the password and also sign the
transaction using the private key.
I meant that it is required for the particular protocol.

@_date: 2014-04-25 21:02:41
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Selector Script 
One use case should be enough.  The atomic cross chain proposal has been
discussed for a while.  It feels like bitcoin works on an "ask permission
first" basis.
It always stalls at the fact that non-standard transactions are hard to get
confirmed on other coins.  It is hard to find pools on other coins which
have weaker isStandard() checks.  The timeouts have to be set so that they
are long enough to guarantee that transactions are accepted before they
A testnet to testnet transfer is the best that would be possible at the
I don't think the cross chain system needs a BIP (except to justify this
If cross chain transfer become popular, then it would be useful to ensure
that clients are interoperable, but first things first.  If the
transactions aren't accepted in any chains, then everything stalls.
Secure transfers require that the malleability issue is fixed, but that is
a separate issue.  I am assuming that will be fixed at some point in the
future, since micro-payment channels also requires that it is fixed.
It could be restricted to only P2SH, I don't think there would be a loss in
doing that.
Personally, I would make it so that P2SH is mandatory after a certain
time.  It makes distributed verification of the block chain easier.
Everything needed to verify a script is present in the transaction (except
that the output actually exists).
A soft fork that expands P2SH functionality would be even better, but I
would rather not let the best be the enemy of the good.

@_date: 2014-04-25 21:05:02
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Selector Script 
True.  Having said that, this is just a change to isStandard(), rather than
a protocol change.
These transactions can already be mined into blocks.

@_date: 2014-04-25 21:48:52
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Selector Script 
I agree with you in theory, each miner could decide their inclusion rules
for themselves.
In practice, if the reference client is updated, then most miners will
accept those transactions.  In addition, it would eventually propagate to
alt-coins (or at least the supported ones).
I could simply submit the changes as a pull request for the reference
client, but I was hoping that by doing it this way, it would increase the
odds of it being accepted.
I don't think it quite requires the same coordination in the short term.  I
could write up the sequence as an info BIP.
The malleability "issue" has been known for years.
It is possible to tweak the protocol so that it still works.  However, it
means that 3rd parties are required (that could go in the BIP too).
Implementing multi-P2SH gets a lot of the benefits of MAST, in terms of
Not yet, but that is just my personal repo.  I did email gmaxwell, but he
said that they can't be assigned until some discussion has happened.
I take your point that the name appears in the link though, so could cause
issues with searching.

@_date: 2014-04-25 22:52:01
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
Removal of isStandard() would be even better/more flexible.
A whitelist of low risk opcodes seems like a reasonable compromise.
My thoughts behind these two BIPs are that they are a smaller change that
adds functionality required for a particular use-case (and some others).
Changing the entire philosophy behind isStandard() is a much bigger change
than just adding one new type.

@_date: 2014-04-26 11:48:07
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP32 "wallet structure" in use? Remove 
Maybe the solution is to have a defined way to import an unknown wallet?
This means that the gap space and a search ordering needs to be defined.
Given a blockchain and a root seed, it should be possible to find all the
addresses for that root seed.
The hierarchy that the wallet actually uses could be anything.

@_date: 2014-04-26 12:31:44
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
Accept to memory pool could probably be replaced with an
IsStandard(scriptPubKey, scriptSig) method.  The only "isStandard" part of
the process is the check inputs method (and AcceptToMemoryPool calls
The standard script methods at the moment are also used for extracting
addresses for wallet management.
The standard script check could be made easier if it just checked for
pattern matches.
Is there any objections to this change, other than it doesn't go far enough?

@_date: 2014-04-30 19:03:59
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP Draft: Atomic Cross Chain Transfer 
Due to "popular" demand, I have created a BIP for cross chain atomic
Unlike the previous version, this version only requires hash locking.   The
previous version required a "selector" transaction based on if statements.
    OP_HASH160 OP_EQUAL_VERIFY [public key] OP_CHECKSIG
    OP_HASH160 OP_EQUAL_VERIFY OP_N [public key 1] ... [public key m]
OP_M OP_CHECK_MULTISIG

@_date: 2014-04-30 21:48:10
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP Draft: Atomic Cross Chain Transfer 
Sorry, that is the names come from the original thread, where I was
outlining the idea.  I updated the names.
The bail in transactions are only signed by one of the parties.  They are
kept secret until the refund/payout transactions are all properly signed.
There is a malleability risk though, hence the need for the 3rd party.
It works on the same refund principle as payment channels.
After TX0 is signed, but before TX2 is signed, either party could
TX0 is not broadcast until the refund transactions are complete.
This is a draft at the moment.
There is an implementation of (almost) this system but not by me.  This
proposal reduces the number of non-standard transaction types required.
A full implement is the next step.
That is a typo, I have updated it.
I can do that.
I wanted it to be as simple as possible, but I guess MIME is just a
different way of doing things.
I would have no objection.

@_date: 2014-05-01 00:02:41
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP Draft: Atomic Cross Chain Transfer 
============================== START ==============================
I updated again.
The new version only requires non-standard transactions on one of the two
Next step is a simple TCP / RPC server that will implement the protocol to
trade between testnet and mainnet.  Timeouts of much less than 24 hours
should be possible now.

@_date: 2014-02-10 20:47:46
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Malleability and MtGox's announcement 
No, the problem was that the transaction MtGox produced was poorly
It wouldn't cause a block containing the transaction to be rejected, but
the default client wouldn't relay the transaction or add it into a block.
This means that transaction stalls.
If the attacker has a direct connection to MtGox, they can receive the
transaction directly.
The attacker would fix the formatting (which changes the transaction id,
but doesn't change the signature) and then forward it to the network, as
The old transaction never propagates correctly.
Up to this point the attacker has nothing gained. But next the attacker
They sent out the transaction a second time.
The right solution is that the new transaction should re-spend at least one
of the coins that the first transaction spent.  That way only one can
possibly be accepted.

@_date: 2014-01-03 11:22:35
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
Maybe a simple compromise would be to add a secure downloader to the
bitcoin client.
The download link could point to a meta-data file that has info on the
message=This is version x.y.z of the bitcoin client
It still suffers from the root CA problem though.  The bitcoin client would
accept Gavin's signature or a "core team" signature.
At least it would provide forward security.
It could also be used to download files for different projects, with
explicit warnings that you are adding a new trusted key.
When you try to download, you would be given a window
Project: Some Alternative Wallet
Signed by: P. Lead
Confirm download Yes No
However, even if you do that, each trusted key is only linked to a
particular project.
It would say if the project and/or leader is unknown.

@_date: 2014-01-03 18:16:43
@_author: Tier Nolan 
@_subject: [Bitcoin-development] An idea for alternative payment scheme 
The random number that the buyer uses could be generated from a root key
This would allow them to regenerate all random numbers that they used and
recreate their receipts.  The master root would have to be stored on your
computer though.
The payment protocol is supposed to do something like this already though.

@_date: 2014-05-04 22:11:27
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
If each "block" is really 512 blocks in sequence, then each "slot" is more
likely to be hit.  It effectively reduces the number of blocks by the
minimum run lengths.
ECC seemed cooler though.
Interesting too.
That's true.  Scaling up the transactions per second increases the chance
of data lost.
With side/tree chains, the odds of data loss in the less important chains
increases (though they are by definition lower value chains)

@_date: 2014-11-08 23:45:27
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
I created a draft BIP detailing a way to add auxiliary headers to Bitcoin
in a bandwidth efficient way.  The overhead per auxiliary header is only
around 104 bytes per header.  This is much smaller than would be required
by embedding the hash of the header in the coinbase of the block.
It is a soft fork and it uses the last transaction in the block to store
the hash of the auxiliary header.
It makes use of the fact that the last transaction in the block has a much
less complex Merkle branch than the other transactions.

@_date: 2014-11-10 00:39:20
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
I made some changes to the draft.  The merkleblock now has the auxiliary
header information too.
There is a tradeoff between overhead and delayed transactions.  Is 12.5%
transactions being delayed to the next block unacceptable?  Would adding
padding transactions be an improvement?
Creating the "seed" transactions is an implementation headache.
Each node needs to have control over an UTXO to create the final
transaction in the block that has the digest of the auxiliary header.  This
means that it is not possible to simply start a node and have it mine.  It
has to somehow be given the private key.  If two nodes were given the same
key by accident, then one could end up blocking the other.
On one end of the scale is adding a transaction with a few thousand outputs
into the block chain.  The signatures for locktime restricted transactions
that spend those outputs could be hard-coded into the software.  This is
the easiest to implement, but would mean a large table of signatures.  The
person who generates the signature list would have to be trusted not to
spend the outputs early.
The other end of the scale means that mining nodes need to include a
wallets to manage their UTXO entry.  Miners can split a zero value output
into lots of outputs, if they wish.
A middle ground would be for nodes to be able to detect the special
transactions and use them.  A server could send out timelocked transactions
that pay to a particular address but the transaction would be timelocked.
The private key for the output would be known.  However, miners who mine
version 2 blocks wouldn't be able to spend them early.

@_date: 2014-11-10 11:42:17
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
The aheaders message is required to make use of the data by SPV clients.
This could be in a separate BIP though.  I wanted to show that the merkle
path to the aux-header transaction could be efficiently encoded, but a
reference to the other BIP would be sufficient.
For the other messages, the problem is that the hash of the aux header is
part of the block, but the aux header itself is not.  That means that the
aux header has to be sent for validation of the block.
I will change it so that the entire aux-header is encoded in the block.  I
think encoding the hash in the final transaction and the full aux-header in
the 2nd last one is the best way to do it.  This has the added advantage of
reducing the changes to block data storage, since the aux-header doesn't
have to be stored separately.
On Mon, Nov 10, 2014 at 12:52 AM, Gregory Maxwell

@_date: 2014-11-10 21:21:58
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
I updated the BIP to cover only the specification of the transactions that
need to be added.  I will create a network BIP tomorrow.

@_date: 2014-11-10 23:39:23
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
I have added the network BIP too.  It only has the aheaders message and the
extra field for getheaders.
The transaction definitions are still at:

@_date: 2014-11-12 19:00:48
@_author: Tier Nolan 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
I was going to look into creating reference code for this.
The first BIP could be reasonably easy, since it just needs to check for
the presence of the 2 special transactions.
That would mean that it doesn't actually create version 3 blocks at all.
Ideally, I would make it easy for miners to mine version 3 blocks.  I could
add a new field to the getblocktemplate that has the 2 transactions ready
to go.
What do pools actually use for generating blocks.  I assume it's custom
code but that they use (near) standard software for the memory pool?

@_date: 2015-08-04 11:22:28
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
Instant runoff voting is not a good system for finding a consensus of the
The main issue here is the "Squeezing out" of center opinions.
If the middle option is acceptable to almost everyone but is only the top
choice of 20%, then it will lose in round one and leave only extreme
options remaining.
Approval is a better system for a consensus.
Each voter can indicate which of the proposals is approved/accepted and the
option with the most support wins.
If one option has 80% support and another has 90% support, then both make a
good choice (though the 90% one has won).
Range voting allows more accuracy, if that is an issue.  If voters are
honest, it allows a middle ground to be reached.
If everyone votes strategically, it becomes approval voting.  With
consensus, there is an assumption that a significant fraction of the
community would be willing to be honest rather than strategic.
The outcome is possible a final decision but not a binding decision.
Voters need to recognise that failing to find a middle ground could mean
they get their way but they split the community.
Additionally, since the point is to determine parameters, you don't
necessarily need to select from discrete candidates.
- Initial new size
- Rate of increase
- Maximum size
They are just numbers.  You could have votes indicate what they want, and
then use the median as the consensus option.
The exception is to have the miners choose what the size is (subject to
limits).  That option is already entirely in the hands of the miners and
they could do it unilaterally.

@_date: 2015-08-17 12:20:51
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fork Post-Mortem? 
A post mortem for the 2013 fork was released as a BIP.
Was an equivalent published for this year's fork?

@_date: 2015-08-17 13:43:46
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max Cap 
You could just take the average of all the block sizes for the last 2016
If average of last 2016 > 50% of the limit, then increase by 6.25%
Otherwise, decrease by 6.25%
This means that the average would be around 50% of the limit.  This gives
margin to create larger blocks when blocks are happening slowly.
A majority of miners could force the limit upwards by creating spam but
full blocks.
It could be coupled with a hard limit that grows at whatever is seen as the
maximum reasonable.  This would be both a maximum and a minimum.
All of these schemes add state to the system.  If the schedule is
predictable, then you can check determine the maximum block size purely
from the header and coinbase.

@_date: 2015-08-17 14:15:54
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
One of the comments made by the mining pools is that they won't run XT
because it is "experimental".
Has there been any consideration to making available a version of XT with
only the blocksize changes?
The least "experimental" version would be one that makes the absolute
minimum changes to core.
The MAX_BLOCK_SIZE parameter could be overwritten whenever the longest tip
changes.  This saves creating a new function.
Without the consensus measuring code, the patch would be even easier.
Satoshi's proposal was just a block height comparison (a year in advance).
The state storing code is also another complication.  If the standard
"counting" upgrade system was used, then no state would need to be stored
in the database.
On Wed, Jul 1, 2015 at 11:49 PM, odinn

@_date: 2015-08-19 14:22:26
@_author: Tier Nolan 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
What happens if XT gets 40% and this BIP gets 55%?  That gives 95% that
accept the upgrade.  Version 3 and lower blocks need to be rejected after
By advertising 0x7 for the last 3 bits, XT is effectively claiming to
support the check locktime verify BIPs but they don't have the code.
This sequence could be used, without a specific version-bits proposal.
Until height = N + 5000, if 950 of the last 1000 blocks have the 0x8 bit
set, then reject blocks with version numbers less than 8.
At height N, if the above rule is active, then the BIP is permanent.
It applies to any block with bit 0x8 set, once the 75% threshold is met.
N could be set 1 year from now, or so.
This gives a period of time after lock that bit 8 is kept and then a period
where is is guaranteed to be zero.
This gives software that is only watching the bit time to be upgraded and
similarly time where the bit is set to zero before it can be reused.

@_date: 2015-08-19 14:24:26
@_author: Tier Nolan 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
What if version number 257 is used in the future?  That would appear to be
a version 1 block and fail the test.

@_date: 2015-08-19 16:48:23
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
On Wed, Aug 19, 2015 at 4:22 PM, jl2012 via bitcoin-dev <
So, the exchanges get together to "encourage" the miners to start running
bitcoin-XT.  What would they do?
One scheme would be to create a taint system.  All non-XT coinbases outputs
are marked as tainted.  All outputs are tainted if any of the inputs into a
transaction are tainted.  Tainted coins can only be un-tainted by sending
0.5% of their value to the public address of one of the participating
exchanges (or to OP_RETURN).  They could slowly ratchet up the surcharge.
Exchanges in the cartel agree not to exchange tainted coins.  Even if some
still do, the tainted coins are still inherently less valuable, since fewer
exchanges accept them.
Schemes like that are the main way for non-miners to flex their muscles,
even if they seem unsavory.
Taint tracking would allow merchants to participate.  They could give less
credit for tainted bitcoins, even if the exchanges are trying to remain
neutral.  If that happens, the exchanges could run 2 prices, BTC and
On the other hand, implementing taint machinery is a bad thing for
It can also be accomplished with checkpointing.  They need to create 1 big
block and then agree to checkpoint it.
A less strict rule rule could be that blocks after the first big block
count as double POW.  That means that the big block chain only needs 34% of
the hashing power to win.

@_date: 2015-08-19 19:17:15
@_author: Tier Nolan 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
That is the 75% "activation" rule portion?  The 95% rule has to apply to
all blocks.
The supermajority applies to unmasked blocks?
I think you want it so that a sequence of blocks with version 8 can be
followed by version 4 blocks?
If 950 of the last 1000 blocks have bit 0x08 set, then reject any block
with a version less than 4.
This means transitioning to the version bits BIP just requires dropping the
version back to 4 and adding a rule enforcing the BIPs for version 4 and
higher blocks.
This would be part of the version bits BIP enforcement.

@_date: 2015-08-20 12:16:30
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Economic majority vote by splitting coins 
The economic majority is defined as the will of those who actually use
Bitcoin as a payment system.
No matter what the miners want, if users and merchants refuse to accept
their fork, then the fork loses and cannot be considered the "true" bitcoin
The problem is that it is easy to measure a miner vote.  The economic
majority is not so easy.
The relative value of two forks could be compared by adding a system
similar colored coins.
These contracts could be added with a soft fork like the P2SH one.
      OP_DROP OP_DROP OP_HASH160       OP_DROP OP_HASH160  OP_EQUAL
This works like P2SH and is template matching.  You can have as many
entries as you want.
In the example, the output can be spent on fork 1 and fork 2 by the owner
of  and can be spent on fork 3 by the owner of .
Until the deadline, the value on each fork must be preserved when spending
the output.  If you provide the key(s), you are allowed to consolidate
entries.  You can also consolidate multiple outputs to the same key even if
you don't have the key.
This means that split outputs are a little more hassle to use and the
transactions are larger.  This doesn't matter much, since measuring the
relative value of the two sub-coins only requires some of them to be traded.
If someone wants to propose a hard fork, they create a new fork id and
deadline and release software that implements the hard fork at the given
deadline (no miner vote needed).
To prevent spam, there could be a cost to create a fork-id (BTC paid to
OP_RETURN) and the deadline could have a max time in the future (say 2
After the deadline, core will allow conversion of outputs that pay to the
core fork-id (probably 1) to be converted into unencumbered outputs by the
person with the core-id script.  Likewise, the fork software will allow
outputs that pay to the fork id to be converted.  Legacy bitcoin that
haven't been "split" will be spendable on both sides equally.
This means that users can convert x legacy bitcoin into x fork-bitcoins and
x core-bitcoins in advance of the fork.
This means that Exchanges could support trading between the two.  The side
that trades with the most value is the one that is supported by the
economic majority.
As it becomes clear which side is going to win, the price of coins on the
losing side should drop.  It is unlikely that the two would stay at exactly
the same value without one side winning.
Users who want to to use the losing rules are free to do so but the
economic majority will have made its decision.

@_date: 2015-08-21 12:03:36
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Economic majority vote by splitting coins 
The idea assumes that it is a BIP + soft fork.  This means that most
wallets would support/recognise the encumbered coins.
Even if only some wallets support it, you can still move your coins
around.  Only the people who are trading between XT and Core would need to
have wallets that support it.
If you consolidate x BTC-Core and x BTC-XT into a single output, then you
can convert it back to a normal output.
That is the point, this gives a sneak preview.  At minimum, it shows which
choice will give the highest BTC value.

@_date: 2015-08-25 23:36:23
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
On Tue, Aug 25, 2015 at 11:08 PM, Mark Friedenbach via bitcoin-dev <
The main advantage of relative locktime over absolute locktime is in
situations when it is not possible to determine when the clock should
start.   This inherently means lower delays.
As a workaround, you could chain transactions to extend the relative
Transaction B has to be 360 days after transaction A and then transaction C
has to be 360 days after transaction B and C must be an input into the
final transaction.
The chain could be built up with multi-sig, like the refund transaction
system, so no one person can create an alternative chain.

@_date: 2015-12-08 19:08:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
On Tue, Dec 8, 2015 at 5:41 PM, Mark Friedenbach via bitcoin-dev <
This trick can be improved by only using certain tx counts.  If the number
of transactions is limited to a power of 2 (other than the extra
transactions), then you get a path of length zero.
The number of non-zero bits in the tx count determings how many digests are
This gets the benefit of a soft-fork, while also keeping the proof lengths
small.  The linked bip has a 105 byte overhead for the path.
The cost is that only certain transaction counts are allowed.  In the worst
case, 12.5% of transactions would have to be left in the memory pool.  This
means around 7% of transactions would be delayed until the next block.
Blank transactions (or just transactions with low latency requirements)
could be used to increase the count so that it is raised to one of the
valid numbers.
Managing the UTXO set to ensure that there is at least one output that pays
to OP_TRUE is also a hassle.

@_date: 2015-12-13 21:36:06
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
On Sun, Dec 13, 2015 at 6:11 PM, jl2012--- via bitcoin-dev <
There is a risk that miners would eventually react by just refusing to
accept blocks that spend dormant outputs.  This is a risk even without the
protocol, but I think if there are already lots of UTXO-lite nodes
deployed, it would be much easier to just define them as the new
(soft-forked) consensus rule.
There is a precedent for things to be disabled rather than fixed when
security problems arise.
Imagine a crisis caused by a security related bug with the revival proofs.
Disabling them is much lower risk than trying to find/fix the bug and then
deploy the fix.  The longer it takes, the longer the security problem
Is this how it works?
Source transaction is included in block Y.
If the output is spent before Y + 420,000, then no further action is taken.
The miner for block Y + 420,000 will include a commitment to
merkle_hash(Block Y's unspent outputs).
It is possible for someone to prove that they didn't spend their
transaction before Y + 420,000.
I think the miners have to remember the "live" UTXO merkle root for every
With the path to the UTXO and the miner can recalculate the root for that
If there were 20 dormant outputs being spent, then the miner would have to
commit to 20 updates.

@_date: 2015-12-17 16:58:02
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
This is really the most important question.
Bitcoin is kind of like a republic where there is separation of powers
between various groups.
The power blocs in the process include
- Core Devs
- Miners
- Exchanges
- Merchants
- Customers
Complete agreement is not required for a change.  If merchants and their
customers were to switch to different software, then there is little any of
the other groups could do.
Consensus is nice, certainly, and it is a good social norm to seek
widespread agreement before committing to a decision above objection.
Committing to no block increase is also committing to a decision against
Having said that, each of the groups are not equal in power and
Merchants and their customers have potentially a large amount of power, but
they are disorganised.  There is little way for them to formally express a
view, much less put their power behind making a change.  Their potential
power is crippled by public action problems.
On the other extreme is the core devs. Their power is based on legitimacy
due to having a line of succession starting with Satoshi and respect gained
due to technical and political competence.  Being a small group, they are
organised and they are also more directly involved.
The miners are less centralised, but statements supported by the majority
of the hashing power are regularly made.  The miners' position is that they
want dev consensus.  This means that they have delegated their decision
making to the core devs.
The means that the two most powerful groups in Bitcoin have given the core
devs the authority to make the decision.  They don't have carte blanche
from the miners.
If the core devs made the 2MB hard-fork with a 75% miner threshold, it is
highly likely that the other groups would accept it.
That is the only authority that exists in Bitcoin.  The check is that if
the authority is abused, the other groups can simply leave (or use

@_date: 2015-12-18 09:44:36
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
The actual users of the system have significant power, if they (could)
choose to use it.  There are "chicken" effects though.  They can impose
costs on the other participants but using those options harms themselves.
If the cost of inaction is greater than the costs of action, then the
chicken effects go away.
In the extreme, they could move away from decentralisation and the concept
of miners and have a centralised checkpointing system.  This would be a
bankrupting cost to miners but at the cost to the users of the
decentralised nature of the system.
At a lower extreme, they could change the mining hash function.  This would
devalue all of the miner's investments.  A whole new program of ASIC
investments would have to happen and the new miners would be significantly
different.  It would also establish that merchants and users are not to be
ignored.  On the other hand, bankrupting miners would make it harder to
convince new miners to make the actual investments in ASICs required to
establish security.
As a gesture, if merchants and exchanges wanted to get their "seat" at the
table, they could create a representative group that insists on a trivial
soft fork.  For example, they could say that they will not accept any block
from block N to block N + 5000 that doesn't have a specific bit set in the
Miners have an advantage where they can say that they have the majority of
the hashing power.  As part of the public action problem that merchants
face, there is no equivalent metric.

@_date: 2015-12-20 11:38:34
@_author: Tier Nolan 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
On Sun, Dec 20, 2015 at 5:12 AM, Emin G?n Sirer <
I wonder if part of the problem here is that there is no pool identity
linked to mining pools.
If the mining protocols were altered so that miners had to indicate their
identity, then a pool couldn't forward hashing power to their victim.
If the various mining protocols were updated, they could allow checking
that the work has the domain name of the pool included.  Pools would have
to include their domain name in the block header.
A pool which provides this service is publicly saying that they will not
use the block withholding attack.  Any two pools which are doing it cannot
attack each other (since they have different domain names).  This creates
an incentive for pools to start supporting the feature.
Owners of hashing power also have an incentive to operate with pools which
offer this identity.  It means that they can ensure that they get a payout
from any blocks found.
Hosted mining is weaker, but even then, it is possible for mining hosts to
provide proof that they performed mining.  This proof would include the
identity of the mining pool.  Even if the pool was run by the host, it
would still need to have the name embedded.
Mining hosts might be able to figure out which of their customers actually
check the identity info, and then they could redirect the mining power of
those who generally don't check.  If customers randomly ask for all of the
hashing power, right back to when they joined, then this becomes expensive.
Mining power directly owned by the pool is also immune to this effect.

@_date: 2015-12-20 15:30:09
@_author: Tier Nolan 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
This gives the same total difficulty but miners are throwing away otherwise
valid blocks.
This means that it is technically a soft fork.  All new blocks are valid
according to the old rule.
In practice, it is kind of a hard fork.  If Y is 10, then all upgraded
miners are throwing away 90% of the blocks that are valid under the old
at a 10X disadvantage.
This means that someone with 15% of the network power has a majority of the
effective hashing power, since 15% is greater than 8.5% (85% * 0.1).
The slow roll-out helps mitigate this though.  It gives non-upgraded
clients time to react.  If there is only a 5% difference initially, then
the attacker doesn't get much benefit.
The main differences are that there's a public key identifier the miners
I don't think public keys are strictly required.  Registering them with
DNSSEC is way over the top.  They can just publish the key on their website
and then use that for their identity.

@_date: 2015-12-20 15:50:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Increasing the blocksize as a (generalized) 
This is essentially the "nuclear option".  You are destroying the current
chain (converting it to a chain of coinbases) and using the same POW to
start the new chain.  You are also giving everyone credit in the new chain
equal to their credit in the old chain.
It would be better if the current chain wasn't destroyed.
This could be achieved by adding the hash of an extended block into the
coinbase but not requiring the coinbase to be the only transaction.
The new block is the legacy block plus the associated extended block.
Users would be allowed to move money to the extended block by spending it
to a specific output template.
 OP_1 OP_TO_EXTENDED OP_TRUE
OP_1 is the extended block index and initially, only one level is available.
This would work like P2SH.  Users could spend the money on the extended
block chain exactly as they could on the main chain.
Money can be brought back the same way.
   ...   OP_0 OP_UNLOCK OP_TRUE
The txids are for transactions that have been locked in root chain.  The
transaction is only valid if they are all fully funded.  The fee for the
transaction would be fee - (cost to fund unlocked txids).  A negative fee
tx would be invalid.
This has the advantage that it keeps the main chain operating.  People can
still send money with their un-upgraded clients.  There is also an
incentive to move funds to the extended block(s).  The new extended blocks
are more complex, but potentially have lower fees.  Nobody is forced to
change.  If the large blocks aren't needed, nobody will both to use them.
The rule could be
0) 1 MB
After change over
0) 1 MB
1) 2 MB
After 2 years
0) 1 MB
1) 2 MB
2) 4MB
After 4 years
0) 1 MB
1) 2 MB
2) 4MB
3) 8MB

@_date: 2015-12-21 15:48:23
@_author: Tier Nolan 
@_subject: [bitcoin-dev] A new payment address format for segregated 
"Prunable in transmission" means that you have to include it when not
sending the witnesses?
That is a name collision with UTXO set prunable.  My initial thought when
reading that was "but scriptSigs are inherently prunable, it is
scriptPubKeys that have to be held in the UTXO database" until I saw the
"in transmission" clarification.

@_date: 2015-12-26 16:09:18
@_author: Tier Nolan 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
No, the re-target compensates so that the number of blocks in the last two
weeks is 2016.  If a soft fork forces miners to throw away 25% of their
blocks, then the difficulty will drop by 75% to keep things balanced.
Throwing away 75% of blocks has the same effect on difficulty as destroying
75% of mining hardware.
The block interval will only increase until the next re-target.
Slowly increasing the fraction of blocks which are thrown away gives the
re-target algorithm time to adjust, so it is another advantage.
If the rule was instantly changed so that 95% of blocks were thrown away,
then there could be up to 40 weeks until the next retarget and that would
give 200 minute block times until the adjustment.

@_date: 2015-07-02 14:13:35
@_author: Tier Nolan 
@_subject: [bitcoin-dev] REQ BIP # / Discuss - Sweep incoming unconfirmed 
I wonder if that would be a viable way for payment services to pay to
protect against double spending.
If the payment processor was handling 1000 BTC every block and was willing
to pay 0.1% fees, then it could create a transaction with 1BTC in fees.
If an attacker tried to double spend a transaction of 0.1BTC, then even if
he was to spend the entire transaction to fees, the payment processor would
be able to out bid him.
It kind of works like insurance.  The payment processor combines lots of
small double spend threats and protects them with a single transaction.
The processor could keep sending out a larger and large transaction (with
fee) until eventually a block is found.
It requires RBF.  First seen safe would be incompatible, if the double
spender gets their transaction into the system first.
A 1BTC fee transaction in nearly every block would also be a boost for
network security.
It avoids Peter Todd's complaint that mining pools might make secret deals
with payment services.  The transaction would be public and all miners
could include it in their block.

@_date: 2015-07-04 11:04:40
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
Is the invalid fork pretty much all empty blocks then?
SPV mining isn't inherently dangerous, if it is only for a short period of
time.  It can boost the total work for the block chain.
Inherently, invalid blocks are rare, so assuming a header is valid is the
correct assumption.
For safety (for the miner), SPV miners should switch back to full mining
after 20-30 seconds without fully validating the chain that they are on.
- header received
- header verified (they skipped this step)
- build on header with empty block
- receive full block

@_date: 2015-07-04 16:35:49
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
On Sat, Jul 4, 2015 at 4:18 PM, Justus Ranvier <
Yeah, fraud proofs have been suggested lots of times in the past.
In this case, they weren't even needed.  Fully updated SPV clients should
also have rejected the invalid fork.  All the information required to
reject it was in the header chain.
The problem wasn't SPV miners, it was SPV-miners where the SPV part wasn't
upgraded to handle v3 blocks.
Even that can be handled with UTXO set commitments.  If the UTXO tree is
sorted you can prove that an entry doesn't exist.
What cannot be handled is proving that a block is invalid if the
transaction data for the block is withheld.
That is reasonable.  Unconfirmed transactions can't include that info
It could be committed in as an extra commitment.
One issue is that you need to prove of of these commitments too.
A transaction which points to the wrong block would also be provable in the
same way.
You could just have an extra merkle tree.
You would only need to include the block hashes for all transactions to
show that the two trees don't match.  That is 32 bytes per transaction
rather than the full 200-500 bytes per transaction.

@_date: 2015-07-04 18:58:46
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
On Sat, Jul 4, 2015 at 5:01 PM, Justus Ranvier <
You can prove that it wasn't updated correctly.
For each transaction, the UTXO tree root before and after is committed.
You show the root before, and the root after and show that the after root
is wrong.  You also need to include some merkle paths to prove the
Yes, you can mostly get short proofs for each step, but you have to make
sure your proofs are also provable.
It means going through everything that needs to be proved for a block to be

@_date: 2015-07-05 02:32:16
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
On Sun, Jul 5, 2015 at 12:33 AM, Justus Ranvier <
I agree, it is definitely tractable.
If Bitcoin was being designed from scratch, it could be made even easier.
As things stand, the extra commitment information needs to be added to
extra trees, which themselves need to be checked.
The "prover", in your example, should ideally store additional meta-data
along with each block.
If P2SH was made mandatory, then much of the transaction validation could
be performed on the transaction alone.
Both the signature and the public key would be included in the spending

@_date: 2015-07-10 17:28:02
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Why not Child-Pays-For-Parent? 
It should be whatever gives the highest fee.  In effect, child pays for
parent creates compound transactions.
A: 250 bytes, 0 fee
B: 300 bytes: 0.0005 fee
C: 400 bytes: 0.0001 fee
There are 3 combinations to consider
A: 0 fee for 250 bytes = 0 per byte
A&B: 0.0005 fee for 550 bytes = 0.91 uBTC per byte
A&B&C: 0.0006 fee for 950 bytes = 0.63uBTC per byte
This means that the A&B combination has the best fee per byte value.  A&B
should be added to the memory pool (if 0.91 uBTC per byte is above the
Once A&B are added, then C can be reconsidered on its own.
C: 0.0001 for 400 bytes = 0.25 BTC per byte
If that is above the threshold, then C should be added.
In practice, it isn't possible to check every combination.  If there are N
transactions, then checking all triple combinations costs around N cubed.
A 2 pass system could get a reasonably efficient result.
B is 0.0005 fee for 300 bytes = 1.67 uBTC per byte and is assumed to be a
high value transaction.
The algorithm would be
Pass 1:
Process all transactions in order of BTC per byte, until block is full
    If the transaction's parents are either already in the pool or a
previous block, add the transaction.
Pass 1:
Process all non-included transactions in order of BTC per byte, until block
is full
    If the transaction's parents are either already in the pool or a
previous block, add the transaction.
    Otherwise, consider the transaction plus all non-included ancestors as
a single transaction
        If this combined transaction has a higher BTC per byte than the
lowest transaction(s),
            add the combined transaction
            drop the other transaction(s)

@_date: 2015-07-10 18:28:14
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Why not Child-Pays-For-Parent? 
When the peer adds both parent and child to the memory pool, it should
forward both of them.
CPFP inherently requires that nodes keep transactions that they have
rejected due to low fees.
If peers requested parents, then it would be possible to forward them
If a node receives a high-fee transaction and doesn't have the parent, it
could request the parent.
Spam protection could be handled by banning nodes which send lots of
"children" and then never having the parent to the transaction.
The rule would be that forwarding a transaction means that you have all its
parents back to transactions contained in blocks.

@_date: 2015-07-10 20:39:14
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Why not Child-Pays-For-Parent? 
It depends on what kind of inefficiency.  Inefficient could mean that it
uses a lot of CPU power.
If it gets a good solution rather than the best solution, it is still worth

@_date: 2015-07-11 11:39:38
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
Increased orphan rate means that the network is (slightly) less secure.
If miners have a 5% orphan rate, then an attacker can launch a 51% attack
with 49% of the network.
It isn't a massive difference, but it is there.
As long as miners switch back to non-SPV mining after a timeout, SPV-mining
is safe for everyone.
The average cost to the miner from building on an invalid block is small,
as long as invalid blocks only happen rarely.
Miners still have an incentive to do full validation, so that they can
include transactions and get transaction fees.
SPV-mining is to prevent hashing hardware from having to waste power when
it isn't needed.
It may be less of a problem if (when?) electricity costs dominate hardware
capital costs.

@_date: 2015-07-11 14:17:53
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
The benefit drops off pretty quickly as the timeout increases (and
eventually goes negative).
You could look at it that headers having 4 states.
1) Valid
2) Probably Valid
3) Probably Invalid
4) Invalid
SPV mining puts newly received headers into the "probably valid" category.
It builds empty (coinbase only) blocks on top of probably valid headers and
build empty blocks on the header.
Once it receives the full block, it can change the state to Valid.  At that
point, it can build full blocks on top of the header.
As time passes without the full block being received/validated, it becomes
less and less likely that the block is actually valid.
The timeout is to recognize that fact.  Making the timeout 24 hours is not
likely to give the miner much benefit over making it 1-2 minutes.
Setting the timeout to low means that the miner sometimes switches away
from a header that turns out to be valid.
Setting it to high means that the miner ends up staying to long on a header
that turns out to be invalid.
At some point, the second effect is stronger than the first effect.  The
timeout needs to be high enough so switching away from valid headers is
rare but low enough so that it doesn't stay on invalid headers for ages.
If 99% of full blocks are received (and validated) within 30 seconds of the
header arriving, then it is reasonable for the miner to assume that if the
full block hasn't arrived within 60 seconds of the header arriving, then
the header is for an invalid block.
Yes. If it's rare enough, then skipping transaction validation saves more
SPV miners don't actually produce invalid blocks though (other than
building on invalid blocks).  The full blocks they produce are still fully
checked blocks.
SPV mining is mainly to protect against latency.  The reason that matters
is that latency means that hashers end up building on blocks even though a
new block has been found.
You can look at it as wasting hashing power due to latency.
In the world where minting fees are very low, there is no point in SPV
I assume at the point, the memory pool/queue is a few blocks deep.  This
means that the pool can create a full block without having to wait for new
transactions to be sent in.
It still needs to wait for the new full block before it knows which
transactions to remove from its memory pool.
Pools have to pay their hashers for hashing power.  When minting fees are
tiny, pools only get income only from tx fees.
There is no point in creating empty blocks, since they don't pay anything.
Between when the block is found and the pool has a new block ready to mine,
there is no incentive for the pool to give out new work.  The stratum
protocol could be modified so pools can say.  (It might already support

@_date: 2015-07-11 17:26:30
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
Right, in the example, I was assuming all transactions had the same fee per
Uh, I don't think so.  Pools would offer a price per hash based on how much
tx fees that they can get at that moment.
Offering more than that would mean they make a loss on average.
Say, for the sake of argument that over a nominal 10 minute period we see
I think the hardware would be able to power down nearly instantly.
Granted, if they have generators or similar, they may not be able to do it
so fast.
Switching to an altcoin is pretty much instant though.
I think you need to split it into hashers and pools, rather than miners.
Hashers have to pay electricity costs to keep their equipment running.
Powering down for 5 minutes is cheaper than using that hashing for other
The ratio of capital costs and electricity determines which wins out.
In the example given, it would work out as something like this.
0 mins: pool offers 0
2 mins: pool offers 20% of average
4 mins: pool offers 40% of average
6 mins: pool offers 60% of average
8 mins: pool offers 80% of average
10 mins: pool offers 100% of average
12 mins: pool offers 120% of average
14 mins: pool offers 140% of average
This means that more and more miners will accept the offer as time passes.
If a miner was using solar power for their miners, then they might as well
run it for the full 10 mins, since it is pointless to leave the equipment
off.  With batteries they could shift some of their output to the more
profitable period.
Such a malicious miner would choose to spend their 5 minutes re-mining the
If they find that block, it will be a tie with the other block, but created
much later.  That means that nobody will build on the block they found.
interesting question.  We can't assume that miners are all going to build
on the tip.
In your example, you can bribe the miner of the next block by paying to
A <- B <-C
Assume that C pays 1BTC in fees and the miner creates a new version of C
that pays 1.1BTC in fees.
C' pays 1.1 BTC in fees and also pays 0.05BTC to OP_TRUE.
This means that miners who build on C' instead of C get a 0.05BTC 'bribe'
to ignore the fact that C' was found much later.
It isn't clear if other miners would be better off building D anyway, since
they could take 100% of the fees.
If the average fees per block was 1BTC and someone send a block that pays
10BTC in fees, it could stall the block chain.  You can do the same bribery
If C has the 10BTC transaction, you can create a C' block and pay 1BTC to
OP_TRUE.  The miner who builds on C' include a transaction which pays that
money to him.
Another miner can produce C'' that pays 2BTC to OP_TRUE.
True, if there is multiple blocks worth of transactions in the memory pool,
then losing one block's worth of transactions won't drop the total fees
down to zero.

@_date: 2015-07-12 00:19:14
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Why not Child-Pays-For-Parent? 
Agreed.  A lot of the functionality could be achieved by a system that
works in most cases.  Even if 100 transaction chains aren't supported, 3-5
transaction chains would give a significant fraction of the desired
At the moment, a transaction is only added into the memory pool if it meets
the relay threshold and spends transactions that are either in the memory
pool or in a block.
There is an orphan pool that can store up to 100 orphans.
The same could be done for child pays for parent.  A node could remember
the last 100 transactions (up to 5000 bytes) that were rejected from the
memory pool due to insufficient relay fees.
This allows nodes to send a chain of transactions in a row.  If the child
is sent last, then the parent(s) will be in the unrelayed transaction pool.

@_date: 2015-07-12 19:54:38
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
It depends on how long they are waiting.  If they receive a header, it is
very likely to be part of a valid block.
The more time that passes, the more likely that the header's block was
invalid after all.
This tradeoff is what the timeout takes into account.  For a short period
of time after the header is received, it is probably valid but eventually,
as time passes without it being fully validated, it is more likely to be
false after all.
If they successfully SPV mine, they risk having mined on top of an
With a 1 minute timeout, there is only a 10% chance they will find another
It is important that when a header is marked as "probably invalid" that all
the header's children are also updated too.  The whole chain times out.
It is important to note that while SPV mining requires you to produce
Agreed.  Transaction only fees changes the whole incentive structure.
A fee pool has been suggested to keep things as they are now.  All fees
(mint & tx fees) are paid into a fee pool.  1% of the total pool fund is
paid to the coinbase.
This keeps the total payout per block reasonably stable.  On the other
hand, it removes the incentive to actually include transactions at all.

@_date: 2015-07-13 14:47:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] About hardware accelerators advantages for 
The main processor load is for signature verification.
This requires a hash function call, some large number maths and an elliptic
curve operation.  The elliptic curve stuff is the longest step.
This takes around 1ms per signature on normal hardware, but optimized code
is faster.
The main task is to prove that
R = u1 * G + u2 * Q
G is a constant, and the rest are different per signature.
Some of the core team have created a fast CPU implementation.
 There was talk of batch verification of signatures.  The process might take
16 signatures and compute them together.
I think a lot of the benefit of the GPU would be lost due to communication
bandwidth.  GPU miners benefit from needing very little information to be
sent to the GPU routine.

@_date: 2015-07-17 17:12:05
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
Transaction sizes are still limited to 1MB with this patch.  While this
isn't technically a change, it does mean that both are no longer linked
Since this has no voting step, I assume the intention is that as a
compromise suggestion, it would have full support.
It establishes a precedent for hard forks not to require a vote though.
On Fri, Jul 17, 2015 at 4:55 PM, Jeff Garzik via bitcoin-dev <

@_date: 2015-07-17 17:14:13
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
I meant both block and transaction sizes are no longer linked together.

@_date: 2015-07-17 23:25:06
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
I meant that since some of the new hard fork proposals use a voting system
for activation, they may not want to establish that precedent.

@_date: 2015-07-20 20:43:51
@_author: Tier Nolan 
@_subject: [bitcoin-dev] For discussion: limit transaction size to 
This could render transactions with a locktime in the future as unspendable.
It is pretty low probability that someone has created a >100kB locked
transaction though.
It violates the principle that no fork should render someone's coins
At the cost of weakening the protection, the rule could be made to only
apply to version 2 transactions.
    The transaction version is increased to version two.
    All coinbase transactions must be version two or higher.
    If any of its parent transactions are version two or higher
    then the transaction must be version two or higher.
    The maximum serialized size of a version two transactions allowed in
    a block is 100,000 bytes.
As time passes more and more of the UTXO set will be from version two
transactions.  To launch the attack, the attacker needs an historical UTXO
Standard software would create version two transactions even if all inputs
were version one.
The rule could be applied to all transactions most of the time, and have
daily blocks that allow legacy transactions.
    This rule shall apply to version 1 transactions too unless the block
height is
    a multiple of 100.
At the risk of encouraging feature creep, if the transaction size is being
limited, it would be useful to also limit the size of all its inputs.
This helps with fraud proofs and offline signing.
    The transaction version is increased to version two.
    All coinbase transactions must be version two or higher.
    If any of its parent transactions are version two or higher
    then the transaction must be version two or higher.
    The maximum serialized size of a version two transactions allowed in
    a block is 100,000 bytes.
    The maximum of the total serialized size of a version two transaction
and all
    of its parents allowed in a block shall be 200,000 bytes.

@_date: 2015-07-22 21:34:27
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP: Short Term Use Addresses for Scalability 
Rather than re-enable OP_LEFT, a NOP could be re-purposed in a soft fork.
OP_DUP OP_HASH160 [pubKeyHash[:LEN_PARAM]] [LEN_PARAM] OP_LEFTEQUALVERIFY
OP_DROP OP_CHECKSIG
A B L OP_LEFTEQUALVERIFY checks if the leftmost L bytes of A and B match.
If not, then the script immediately fails.  If either array is less than L
bytes or if there are fewer than 3 values on the stack, then it also fails.
The OP_DROP is needed as the new opcode must count as a NOP for legacy
A change like this would only cause a once-off improvement in efficiency,
so it is less likely to be worth the effort.
It also requires most clients to be updated to support the new address
A different BIP could be added for that.
An alternative way to add new opcodes is to use a different script engine
like with P2SH.
On Wed, Jul 22, 2015 at 9:15 PM, Jeremy Rubin via bitcoin-dev <

@_date: 2015-07-23 18:59:40
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP draft: Hardfork bit 
This change means that they are kicked off the main chain immediately when
the fork activates.
The change is itself a hard fork.  Clients have be updated to get the
3) In the case which the original consensus rules are also valid under the
I don't understand the situation here.  Is the assumption of a group of
miners suddenly switching (for example, they realise that they didn't
intend to support the new rules)?
Ok, so set the bit and then include BIP-GIT-HASH of the canonical BIP on
github in the coinbase?
Since it is a hard fork, the version field could be completely
re-purposed.  Set the bit and add the BIP number as the lower bits in the
version field.  This lets SPV clients check if they know about the hard
There network protocol could be updated to add getdata support for asking
for a coinbase only merkleblock.  This would allow SPV clients to obtain
the coinbase.
Automatic warning system: When a flag block is found on the network, full
If the rule was that hard forks only take effect 100 blocks after the flag
block, then this problem is eliminated.
Emergency hard forks may still have to take effect immediately though, so
it would have to be a custom not a rule.

@_date: 2015-06-02 16:42:45
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced 
I am glad to see the transaction version number increase.  The commit
doesn't update the default transaction version though.  The node would
still produce version 1 transactions.
Does the reference client already produce transactions with final sequence
numbers?  If so, then they will be valid version 2 transactions.  If it
sets the sequence to all zeros, then it won't trigger the new code either.
I think simply bumping the default version number to 2 would be safe.
For the timestamp locktime, median block time would be better than raw
block time.  Median time is the median timestamp of the previous 11
blocks.  This reduces the incentive to mess with the timestamp.  Median
time is earlier than block time, but since things are relative, it should
balance out.
Miners have around 2 hours worth of flexibility when setting the
timestamps, so it may not be that big a deal.
On Tue, Jun 2, 2015 at 5:34 AM, Stephen Morse

@_date: 2015-06-09 15:18:40
@_author: Tier Nolan 
@_subject: [Bitcoin-development] New attack identified and potential 
limit
On Tue, Jun 9, 2015 at 2:36 PM, Gavin Andresen I think 2 should just be fee per kB.  If the pool is full and a transaction
arrives, it has to have a fee per kB that is higher than the lowest
transaction in the pool.
The effect is that the fee per kB threshold for getting a transaction into
the memory pool increases as the attack proceeds.  This means that the cost
to maintain the attack increases.
With replace by fee, the new transaction would have to have a fee that is
more than a fixed amount more than the lowest already in the pool.  I think
the replace by fee code already does this.  This prevents transactions with
fees that increase by 1 Satoshi at a time being relayed.
For allowing large blocks when block space is in high demand, you could
limit the average block size.
If the average was set to 1MB, the rule could be that blocks must be 2MB or
lower and the total size of the a block and the previous 99 must be 100MB
or lower.  This gives an average of 1MB per block, but allows bursts.

@_date: 2015-06-12 09:37:17
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Miners: You'll (very likely) need to 
Once the 75% threshold is reached, miners who haven't updated are at risk
of mining on invalid blocks.
If someone produces a version 3 block that violates the new rules, then
miners who haven't upgraded will end up wasting mining power building on
that block.
This could be used as an expensive way to attack miners who haven't
upgraded.  It is low risk of happening, since creating an invalid version 3
block costs 25BTC in hashing power and the miner who does it ends up
creating an invalid block.

@_date: 2015-06-16 12:01:21
@_author: Tier Nolan 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
The main principle of open source software is that anyone can fork the code
if they wish.  They don't do it very often, but they can.
This means that if a project dies, someone can take it over.  If some of
the devs want to take things in a different direction, they can.  Users can
decide which version they prefer.
The software itself is what is valuable.
In the case of bitcoin, the blockchain is also (very) valuable.  Simply
splitting into two projects is not possible for bitcoin.
Otherwise, the discussion would have ended already, those who want a larger
block would simply create a fork of the software and create an alt chain.
The fundamental problem is that there is no clear way to make this decision
once and for all.
An agreed set of rules for a hard fork would be a nice thing to have, but
it is hard to have rules about how to change fundamental rules.
I think using the soft fork rules (maybe with a higher threshold than 95%)
plus a delay is a reasonable compromise on hard fork rules.
Even then, it would be nice to include users of the software too.  Peter
Todd's suggestion of encoding a vote in transactions is a step in that
direction (YES transactions in YES blocks and NO transactions in NO blocks).
Nobody owns it, so there is no court of final appeal.
If miners vote >95% for the fork, users could still refuse to accept the
Maybe the sequence could be
version 3 blocks means no opinion
version 4 blocks means NO to fork
version 5 blocks means YES to fork & YES transactions
version 6 blocks means YES to fork & NO transactions
Transaction matching rule:
version 1, 2, 3 transactions means no opinion (can be in any block)
version 4 transactions means YES to fork (cannot be in version 6 blocks)
version 5 transactions means NO to fork (cannot be in version 5 blocks)
0) if 750 of the last 1000 blocks are version 5 or 6 blocks, tx matching
rule activates for version 5 & 6 blocks
1) if 950 of the last 1000 blocks are version 5 or 6 blocks, then version 4
blocks are rejected
2) if 750 of the last 1000 blocks are version 4 blocks, then version 5 & 6
blocks are rejected
3) if 750 of the last 1000 blocks are version 5 transactions and 950 of the
last 1000 are version 5 or 6, then the fork is accepted
4) 25,000 blocks after 3 is accepted, hard fork actually takes effect
Once miner acceptance is achieved, then only version 5 and 6 blocks are
allowed.  The split between version 5 and 6 blocks should be roughly in
proportion to the number of transactions of each kind produced.
75% of miners can kill the fork by producing version 4 blocks, but 95% is
needed for acceptance.  Even then, transaction volume needs to support the
fork.  I think 75% is reasonable here.  (95% of miners and 75% of
merchants/users is a pretty strong majority).
They are still suggesting some kind of fork threshold process (or at least
that is what is being suggested)
If their system requires 95% miner approval, they aren't taking unilateral
action.  Miners are though if they vote in favour.

@_date: 2015-06-19 18:00:55
@_author: Tier Nolan 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Outputs could be marked as "locked".  If you are performing a zero
confirmation spend, then the recipient could insist that you flag the
output for them as non-reducible.
This reduces privacy since it would be obvious which output was change.  If
both are locked, then the fee can't be increased.
This would be information that miners could ignore though.
Creating the right incentives is hard though.  Blocks could be
"discouraged" if they have a double spend that is known about for a while
which reduces payment for a locked output.

@_date: 2015-06-20 18:46:52
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Hard fork via miner vote 
I agree giving notice that the change is going to happen is critical for a
hard fork.  If miners vote in favor, they need to give people time to
upgrade (or to decide to reject the fork).
The BIP 100 proposal is that no change will happen until a timestamp is
reached.  It isn't clear exactly how it would work.
Testnet: Sep 1st 2015
Mainnet: Jan 11th 2016
It suggests 90% of 12000 blocks (~83 days).
This means that if 10800 of the last 12000 blocks are the updated version,
then the change is considered locked in.
I think having an earlier "fail" threshold would be a good idea too.  This
guarantees notice.
Assuming 3 is  and 4 is If the median of 11 timestamp is after 1st Sep 2015 and less than 10800 of
the last 12000 blocks are version 4+, then reject version 4 blocks
If the median of 11 timestamp is after 1st Nov 2015 and at least 10800 of
the last 12000 blocks are version 4+, then reject version 3 blocks
If the median of 11 timestamp is after 1st Jan 2016 and at least 10800 of
the last 12000 blocks are version 4+, the allow This means that if the 90% threshold is lost at any time between 1st Sep
and 1st Nov, then the fork is rejected.  Otherwise, after the 1st Nov, it
is locked in, but the new rules don't activate until 1st Jan.
For block size, miners could still soft fork back to 1MB after 1st Nov, it
there is a user/merchant revolt (maybe that would be version 5 blocks).
On Sat, Jun 20, 2015 at 6:13 PM, Pieter Wuille

@_date: 2015-06-20 23:08:19
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP draft] Motivation and deployment of 
The off by 1 bug could be fixed by a soft fork.  Since the point is to show
how a non-controversial hard fork works, it doesn't matter much.

@_date: 2015-06-21 11:54:07
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP draft] Motivation and deployment of 
The easiest would be a rule requiring that all blocks are within 1 day of
the median of the previous 11 blocks.  At the moment, you need to be
greater than that value.  This would add a condition at the other end.
It wouldn't be a total fix, but it would protect against the exploit.
A stricter soft fork would be that the two blocks in question have to have
the same timestamp.  This would force the off by 1 and the correct value to
give the same result.
If that's the case, do you have a better candidate?
I think it is fine, since fixing it "right" does require a hard fork,
especially if it is only to show a non controversial hard fork.

@_date: 2015-06-22 19:33:45
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
The BIP-100 proposal uses a window of 12000 blocks (83 days) rather than
the standard 1000.  Given that the threshold is lower than is normal for
hard-forks, noise on the measurement could cause an activation even if less
than 75% of miners agree.  It also means that the vote has to be sustained
for longer and inherently gives a longer notice period.
Two weeks seems low for an upgrade warning.  I guess there would be an
alert on the network.
Do old nodes detect an upgrade by version numbers?  If that was headers
only, then they could detect that large blocks have activated.
Have you considered a "fail" condition?  For example, if 750 of the last
1000 blocks set bits 4 and 14, then it counts as a rejection by 75% of the
miners.  Alternatively, if the rule doesn't activate by 11th Jan 2017, then
it is disabled.

@_date: 2015-06-22 20:28:49
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
There is an exception in the code for "block" messages.
This means 2MB limit for all other messages.  "block" messages are limited
to the max block size for 2 hours into the future.
I think setting it to a week into the future might be better, since it is
only a DOS protection.  This would guarantee that message sizes are
reasonable.  The size check would still be done anyway.

@_date: 2015-06-22 21:43:41
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
The activation or not rule is purely timestamp based.  Blocks with a
timestamp less than 1452470400 have a limit of 1MB.  There could be an 8MB
block followed by a block that is limited to 1MB.

@_date: 2015-06-22 22:48:01
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
I think trying to keep the number of rules that require context to a
minimum is a good idea.  As pointed out in the BIP, using only the
timestamp of the block means that the block limit can be determined purely
from the block header.
I don't think there is much issue with having a 1MB block following an 8MB
block during the activation.
This is inherent in using the timestamps.  It occurs for every block that
has a timestamp lower than its parent, but to a lesser degree.
When fees are the main source of income, it does create a slight incentive
to use higher timestamps, but that is probably not massive, since it is 2
hours out of the 2 year doubling time.

@_date: 2015-06-25 21:05:40
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP Process and Votes 
There is no process for handling hard forks, which aren't bug fixes.
Soft forks have a defined process of something like
- BIP proposal + discussion
- Proposed code
- Dev acceptance
- Release
- Miner vote/acceptance
The devs have a weak veto.  If they refuse to move forward with changes,
miners could perform a soft fork on their own.  They don't want to do that,
as it would be controversial and the devs know the software better.
The miner veto is stronger (for soft forks) but not absolute.  The devs
could checkpoint/blacklist a chain if miners implemented a fork that wasn't
acceptable (assuming the community backed them).
When ASICs arrived, it was pointed out by some that the devs could hit back
if ASICs weren't made publicly available.  If they slightly tweaked the
hashing algorithm, then current generation of ASICs would be useless.   The
potential threat may have acted as a disincentive for ASIC manufacturers to
use the ASICs themselves.
Moving forward with agreement between all involved is the recommended and
desirable approach.
Consensus between all parties is the goal but isn't absolutely required.
This escape valve is partly what makes consensus work.  If you dig your
heels in, then the other side can bypass you, but they have an incentive to
try to convince you to compromise first.  The outcome is better if a middle
ground can be found.
Hard forks are different.  The "checks and balances" of weak vetoes are not
present.  This means that things can devolve from consensus to mutual
veto.  Consensus ceases to be a goal and becomes a requirement.
This is partly a reflection of the nature of hard forks.  Everyone needs to
upgrade.  On the other hand, if most of the various groups upgrade, then
users of the legacy software would have to upgrade or get left behind.  If
5% of the users decided not to upgrade, should they be allowed to demand
that nobody else does?
There is clearly some kind of threshold that is reasonable.
The fundamental problem is that there isn't agreement on what the block
size is.  Is it equal in status to the 21 million BTC limit?
If Satoshi had said that 1MB was part of the definition of Bitcoin, then I
think people would accept it to the same extent as they accept the 21
million coin limit.  It might cause people to leave the coin though.
It was intended to be temporary, but people have realized that it might be
a good idea to keep it.  In effect both sides could argue that they should
be considered the status quo.
I wonder if a coin toss would be acceptable :).  "Come to an agreement or
we decide by coin toss"

@_date: 2015-06-26 01:07:48
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP65 / CHECKLOCKTIMEVERIFY deployment 
It would be possible to run a simplified version of the bits proposal,
until BIP 66 locks.
It's obviously not worth it at this point though, though it could be 1-2
weeks more.
Version 2 means neither option
Version 3 means BIP 66 only
Version 4 means CLTV only
Version 5 means both
If (Version 3 + version 5) > 95%, reject 2 & 4
If (Version 4 + version 5) > 95%, reject 2 & 3
For 2 options at the same time, this isn't much extra overhead.

@_date: 2015-06-26 14:47:24
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
BIP 100 and 101 could be combined.  Would that increase consensus?
- Miner vote threshold reached
- Wait notice period or until earliest start time
- Block size default target set to 1 MB
- Soft limit set to 1MB
- Hard limit set to 8MB + double every 2 years
- Miner vote to decide soft limit (lowest size ignoring bottom 20% but 1MB
Block size updates could be aligned with the difficulty setting and based
on the last 2016 blocks.
Miners could leave the 1MB limit in place initially.  The vote is to get
the option to increase the block size.
Legacy clients would remain in the network until >80% of miners vote to
raise the limit and a miner produces a >1MB block.
If the growth rate over-estimates hardware improvements, the devs could add
a limit into the core client.  If they give notice and enough users update,
then miners would have to accept it.
The block size becomes min(miner's vote, core devs).  Even if 4 years
notice is given, blocks would only be 4X optimal.

@_date: 2015-06-26 20:03:05
@_author: Tier Nolan 
@_subject: [bitcoin-dev] The need for larger blocks 
On Fri, Jun 26, 2015 at 7:47 PM, Patrick Strateman <
Safety increases with more lead-in time.  If the reference client was
updated so that the hard fork happened in two years, it would be pretty
safe.  Miners would have time to update.
If miners (or the community) objected, it is sort of like a game of chicken.
This is one of the problems with not making decisions in advance, the
resulting hard fork is inherently safer.

@_date: 2015-05-05 21:38:44
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
I think that should be greater than in the comparison?  You want it to fail
if the the height of the UTXO plus the sequence number is greater than the
spending block's height.
There should be an exception for final inputs.  Otherwise, they will count
as relative locktime of 0xFFFFFFFF.  Is this check handled elsewhere?
if (!tx.vin[i].IsFinal() && nSpendHeight < coins->nHeight +
       return state.Invalid(false, REJECT_INVALID,
Is the intention to let the script check the sequence number?
 OP_RELATIVELOCKTIMEVERIFY
would check if  is less than or equal to the sequence number.
It does make sequence mean something completely different from before.
Invalidating previously valid transactions has the potential to reduce
confidence in the currency.
A workaround would be to have a way to enable it in the sigScript by
extending Peter Todd's suggestion in the other email chain.
<1> OP_NOP2 means OP_CHECKLOCKTIMEVERIFY (absolute)
<2> OP_NOP2 means OP_RELATIVECHECKLOCKTIMEVERIFY
<3> OP_NOP2 means OP_SEQUENCE_AS_RELATIVE_HEIGHT
OP_SEQUENCE_AS_RELATIVE_HEIGHT would cause the script to fail unless it was
the first opcode in the script.  It acts as a flag to enable using the
sequence number as for relative block height.
This can be achieved using a simple pattern match.
bool CScript::IsSequenceAsRelativeHeight() const
    // Extra-fast test for pay-to-script-hash CScripts:
    return (this->size() >= 4 &&
            this->at(0) == OP_PUSHDATA1 &&
            this->at(1) == 1 &&
            this->at(2) == 0xFF &&
            this->at(3) == OP_NOP2);
if (!tx.vin[i].IsFinal() &&
tx.vin[i].scriptSig.IsSequenceAsRelativeHeight() && nSpendHeight <
coins->nHeight + tx.vin[i].nSequence)
       return state.Invalid(false, REJECT_INVALID,

@_date: 2015-05-06 23:09:59
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV 
It is just a switch that turns on and off the new mode.
In retrospect, it would be better to just up the transaction version.
In transactions from v2 onwards, the sequence field means height.  That
means legacy transactions would be spendable.
This is a pure soft-fork.

@_date: 2015-05-06 23:44:53
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Block Size Increase 
Miners can already soft-fork to reduce the maximum block size.  If 51% of
miners agree to a 250kB block size, then that is the maximum block size.
The question being discussed is what is the maximum block size merchants
and users will accept.  This puts a reasonable limit on the maximum size
miners can increase the block size to.
In effect, the block size is set by the minimum of the miner's and the
merchants/user's size.min(miner, merchants/users).
Would you accept a rule that the maximum size is 20MB (doubling every 2
years), but that miners have an efficient method for choosing a lower size?
If miners could specify the maximum block size in their block headers, then
they could coordinate to adjust the block size.  If 75% vote to lower the
size, then it is lowered and vice versa for raiding.
Every 2016 blocks, the votes are counter.  If the 504th lowest of the 2016
blocks is higher than the previous size, then the size is set to that
size.  Similarly, if the 504th highest is lower than the previous size, it
becomes the new size.
There could be 2 default trajectories.  The reference client might always
vote to double the size every 4 years.
To handle large blocks (>32MB) requires a change to the p2p protocol
message size limits, or a way to split blocks over multiple messages.
It would be nice to add new features to any hard-fork.
I favour adding an auxiliary header.  The Merkle root in the header could
be replaced with hash(merkle_root | hash(aux_header)).  This is a fairly
simple change, but helps with things like commitments.  One of the fields
in the auxiliary header could be an extra nonce field.  This would mean
fast regeneration of the merkle root for ASIC miners.  This is a pretty
simple change.

@_date: 2015-05-07 00:33:56
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Block Size Increase 
On Thu, May 7, 2015 at 12:12 AM, Matt Corallo Miners can always reduce the block size (if they coordinate).  Increasing
the maximum block size doesn't necessarily cause an increase.  A majority
of miners can soft-fork to set the limit lower than the hard limit.
Setting the hard-fork limit higher means that a soft fork can be used to
adjust the limit in the future.
The reference client would accept blocks above the soft limit for wallet
purposes, but not build on them.  Blocks above the hard limit would be
rejected completely.

@_date: 2015-05-07 22:24:45
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Mechanics of a hard fork 
In terms of miners, a strong supermajority is arguably sufficient, even 75%
would be enough.
The near total consensus required is merchants and users.  If (almost) all
merchants and users updated and only 75% of the miners updated, then that
would give a successful hard-fork.
On the other hand, if 99.99% of the miners updated and only 75% of
merchants and 75% of users updated, then that would be a serioud split of
the network.
The advantage of strong miner support is that it effectively kills the fork
that follows the old rules.  The 25% of merchants and users sees a
blockchain stall.
Miners are likely to switch to the fork that is worth the most.  A mining
pool could even give 2 different sub-domains.  A hasher can pick which
rule-set to follow.  Most likely, they would converge on the fork which
paid the most, but the old ruleset would likely still have some hashing
power and would eventually re-target.

@_date: 2015-05-08 00:32:12
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Assurance contracts to fund the network with 
One of the suggestions to avoid the problem of fees going to zero is
assurance contracts.  This lets users (perhaps large merchants or
exchanges) pay to support the network.  If insufficient people pay for the
contract, then it fails.
Mike Hearn suggests one way of achieving it, but it doesn't actually create
an assurance contract.  Miners can exploit the system to convert the
pledges into donations.
Consider a situation in the future where the minting fee has dropped to
almost zero.  A merchant wants to cause block number 1 million to
effectively have a minting fee of 50BTC.
He creates a transaction with one input (0.1BTC) and one output (50BTC) and
signs it using SIGHASH_ANYONE_CAN_PAY.  The output pays to OP_TRUE.  This
means that anyone can spend it.  The miner who includes the transaction
will send it to an address he controls (or pay to fee).  The transaction
has a locktime of 1 million, so that it cannot be included before that
This transaction cannot be included in a block, since the inputs are lower
than the outputs.  The SIGHASH_ANYONE_CAN_PAY field mean that others can
pledge additional funds.  They add more input to add more money and the
same sighash.
There would need to be some kind of notice boeard system for these pledges,
but if enough pledge, then a valid transaction can be created.  It is in
miner's interests to maintain such a notice board.
The problem is that it counts as a pure donation.  Even if only 10BTC has
been pledged, a miner can just add 40BTC of his own money and finish the
transaction.  He nets the 10BTC of the pledges if he wins the block.  If he
loses, nobody sees his 40BTC transaction.  The only risk is if his block is
orphaned and somehow the miner who mines the winning block gets his 40BTC
transaction into his block.
The assurance contract was supposed to mean "If the effective minting fee
for block 1 million is 50 BTC, then I will pay 0.1BTC".  By adding his
40BTC to the transaction the miner converts it to a pure donation.
The key point is that *other* miners don't get 50BTC reward if they find
the block, so it doesn't push up the total hashing power being committed to
the blockchain, that a 50BTC minting fee would achieve.  This is the whole
point of the assurance contract.
OP_CHECKLOCKTIMEVERIFY could be used to solve the problem.
Instead of paying to OP_TRUE, the transaction should pay 50 BTC to "<1
million> OP_CHECKLOCKTIMEVERIFY OP_TRUE" and 0.01BTC to "OP_TRUE".
This means that the transaction could be included into a block well in
advance of the 1 million block point.  Once block 1 million arrives, any
miner would be able to spend the 50 BTC.  The 0.01BTC is the fee for the
block the transaction is included in.
If the contract hasn't been included in a block well in advance, pledgers
would be recommended to spend their pledged input,
It can be used to pledge to many blocks at once.  The transaction could pay
out to lots of 50BTC outputs but with the locktime increasing by for each
For high value transactions, it isn't just the POW of the next block that
matters but all the blocks that are built on top of it.
A pledger might want to say "I will pay 1BTC if the next 100 blocks all
have at least an effective minting fee of 50BTC"

@_date: 2015-05-08 15:15:05
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Assurance contracts to fund the network 
Just to clarify the process.
Pledgers create transactions using the following template and broadcast
them.  The p2p protocol could be modified to allow this, or it could be a
separate system.
*Input: 0.01 BTC*
*Signed with SIGHASH_ANYONE_CAN_PAY*
*Output 50BTC*
*Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE*
*Output 0.01BTC*
*Paid to OP_TRUE*
This transaction is invalid, since the inputs don't pay for the output.
The advantage of the sighash "anyone can pay" field is that other people
can add additional inputs without making the signature invalid.  Normally,
any change to the transaction would make a signature invalid.
Eventually, enough other users have added pledges and a valid transaction
can be broadcast.
*Input: 0.01 BTC*
*Signed with SIGHASH_ANYONE_CAN_PAY*
*Input: 1.2 BTCSigned with SIGHASH_ANYONE_CAN_PAY*
*Input: 5 BTCSigned with SIGHASH_ANYONE_CAN_PAY*
*Input: 1.3 BTCSigned with SIGHASH_ANYONE_CAN_PAYOutput 50BTC*
*Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE*
*Output 0.01BTC**Paid to OP_TRUE*
This transaction can be submitted to the main network.  Once it is included
into the blockchain, it is locked in.
In this example, it might be included in block 999,500.  The 0.01BTC output
(and any excess over 50BTC) can be collected by the block 999,500 miner.
The OP_CHECKLOCKTIMEVERIFY opcode means that the 50BTC output cannot be
spent until block 1 million.  Once block 1 million arrives, the output is
completely unprotected.  This means that the miner who mines block 1
million can simply take it, by including his own transaction that sends it
to an address he controls.  It would be irrational to include somebody
else's transaction which spent it.
If by block 999,900, the transaction hasn't been completed (due to not
enough pledgers), the pledgers can spend the coin(s) that they were going
to use for their pledge.  This invalidates those inputs and effectively
withdraws from the pledge.
On Fri, May 8, 2015 at 11:01 AM, Benjamin No, the pledger is saying that he will only pay 0.01BTC if the miner gets a
reward of 50BTC.
Imagine a group of 1000 people who want to make a donation of 50BTC to
something.  They all way that they will donate 0.05BTC, but only if
everyone else donates.
It still isn't perfect.  Everyone has an incentive to wait until the last
minute to pledge.

@_date: 2015-05-08 16:03:28
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Assurance contracts to fund the network 
Sorry for the spam of the last mail.  I hit send by accident.
Assurance contracts are better than simple donations.
Donating to a project means that you always end up losing the money but the
project might still not get funded.
An assurance contract is like Kickstarter, you only get your CC charged if
the project is fully funded.
There is lower risk, either you get your money back or the project is
funded.  It might still be worth risking it and hoping it gets funded.
Kickstarter does have pledge rewards to reward pledgers.  That helps with
creating the momentum to encourage people to pledge.

@_date: 2015-05-08 20:47:52
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
I wonder if having a "miner" flag would be good for the network.
Clients for general users and merchants would have a less strict rule than
the rule for miners.  Miners who don't set their miners flag might get
orphaned off the chain.
For example, the limits could be setup as follows.
Clients: 20MB
Miners: 4MB
When in "miner mode", the client would reject 4MB blocks and wouldn't build
on them.  The reference client might even track the miner and the non-miner
chain tip.
Miners would refuse to build on 5MB blocks, but merchants and general users
would accept them.
This allows the miners to soft fork the limit at some point in the future.
If 75% of miners decided to up the limit to 8MB, then all merchants and the
general users would accept the new blocks.  It could follow the standard
soft fork rules.
This is a more general version of the system where miners are allowed to
vote on the block size (subject to a higher limit).
A similar system is where clients track all header trees.  Your wallet
could warn you that there is an invalid tree that has > 75% of the hashing
power and you might want to upgrade.

@_date: 2015-05-09 14:49:53
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
If the UTXO set cost is built in, UTXO database entries suddenly are worth
something, in addition to the bitcoin held in that entry.
A user's client might display how many they own.  When sending money to a
merchant, the user might demand the merchant indicate a slot to pay to.
The user could send an ANYONE_CAN_PAY partial transaction.  The transaction
would guarantee that the user has at least as many UTXOs as before.
Discussing the possibility of doing this creates an incentive to bloat the
UTXO set right now, since UTXOs would be valuable in the future.
The objective would be to make them valuable enough to encourage
conservation, but not so valuable that the UTXO contains more value than
the bitcoins in the output.
Gmaxwell's suggested "tx_size = MAX( real_size >> 1,  real_size +
4*utxo_created_size - 3*utxo_consumed_size)" for a 250 byte transaction
with 1 input and 2 outputs has very little effect.
real_size + 4 * (2) - 3 * 1 = 255
That gives a 2% size penalty for adding an extra UTXO.  I doubt that is
enough to change behavior.
The UTXO set growth could be limited directly.  A block would be invalid if
it increases the number of UTXO entries above the charted path.
RE: a hard upper limit, with a dynamic limit under it:
If the block is greater than 32MB, then it means an update to how blocks
are broadcast, so that could be a reasonable hard upper limit (or maybe
31MB, or just the 20MB already suggested).

@_date: 2015-05-12 19:23:48
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
A compact way to describe which blocks are stored helps to mitigate against
fingerprint attacks.
It also means that a node could compactly indicate which blocks it stores
with service bits.
The node could pick two numbers
W = window = a power of 2
P = position = random value less than W
The node would store all blocks with a height of P mod W.  The block hash
could be used too.
This has the nice feature that the node can throw away half of its data and
still represent what is stored.
W_new = W * 2
P_new = (random_bool()) ? P + W/2 : P;
Half of the stored blocks would match P_new mod W_new and the other half
could be deleted.  This means that the store would use up between 50% and
100% of the allocated size.
Another benefit is that it increases the probability that at least someone
has every block.
If N nodes each store 1% of the blocks, then the odds of a block being
stored is pow(0.99, N).  For 1000 nodes, that gives odds of 1 in 23,164
that a block will be missing.  That means that around 13 out of 300,000
blocks would be missing.  There would likely be more nodes than that, and
also storage nodes, so it is not a major risk.
If everyone is storing 1% of blocks, then they would set W to 128.  As long
as all of the 128 buckets is covered by some nodes, then all blocks are
stored.  With 1000 nodes, that gives odds of 0.6% that at least one bucket
will be missed.  That is better than around 13 blocks being missing.
Nodes could inform peers of their W and P parameters on connection.  The
version message could be amended or a "getparams" message of some kind
could be added.
W could be encoded with 4 bits and P could be encoded with 16 bits, for 20
in total.  W = 1 << bits[19:16] and P = bits[14:0].  That gives a maximum W
of 32768, which is likely to many bits for P.
Initial download would be harder, since new nodes would have to connect to
at least 100 different nodes.  They could download from random nodes, and
just download the ones they are missing from storage nodes.  Even storage
nodes could have a range of W values.

@_date: 2015-05-12 23:00:33
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
M = 1,000,000
N = number of "starts"
S(0) = hash(seed) mod M
S(n) = hash(S(n-1)) mod M
This generates a sequence of start points.  If the start point is less than
the block height, then it counts as a hit.
The node stores the 50MB of data starting at the block at height S(n).
As the blockchain increases in size, new starts will be less than the block
height.  This means some other runs would be deleted.
A weakness is that it is random with regards to block heights.  Tiny blocks
have the same priority as larger blocks.
0) Blocks are local, in 50MB runs
1) Agreed, nodes should download headers-first (or some other compact way
of finding the highest POW chain)
2) M could be fixed, N and the seed are all that is required.  The seed
doesn't have to be that large.  If 1% of the blockchain is stored, then 16
bits should be sufficient so that every block is covered by seeds.
3) N is likely to be less than 2 bytes and the seed can be 2 bytes
4) A 1% cover of 50GB of blockchain would have 10 starts @ 50MB per run.
That is 10 hashes.  They don't even necessarily need to be crypt hashes
5) Isn't this the same as 3?
6) Every block has the same odds of being included.  There inherently needs
to be an update when a node deletes some info due to exceeding its cap.  N
can be dropped one run at a time.
7) When new starts drop below the tip height, N can be decremented and that
one run is deleted.
There would need to be a special rule to ensure the low height blocks are
covered.  Nodes should keep the first 50MB of blocks with some probability

@_date: 2015-05-13 10:34:03
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
Interesting.  This can be combined with the system I suggested.
A node broadcasts 3 pieces of information
Seed (16 bits): This is the seed
M_bits_lsb (1 bit):  Used to indicate M during a transition
N (7 bits):  This is the count of the last range held (or partially held)
M = 1 << M_bits
M should be set to the lowest power of 2 greater than double the block
chain height
That gives M = 1 million at the moment.  During changing M, some nodes will
be using the higher M and others will use the lower M.
The M_bits_lsb field allows those to be distinguished.
As the block height approaches 512k, nodes can begin to upgrade.  For a
period around block 512k, some nodes could use M = 1 million and others
could use M = 2 million.
Assuming M is around 3 times higher than the block height, then the odds of
a start being less than the block height is around 35%.  If they runs by
25% each step, then that is approx a double for each hit.
Size(n) = ((4 + (n & 0x3)) << (n >> 2)) * 2.5MB
This gives an exponential increase, but groups of 4 are linearly
*Size(0) = 10 MB*
Size(1) = 12.5MB
Size(2) = 15 MB
Size(3) = 17.5MB
Size(4) = 20MB
*Size(5) = 25MB*
Size(6) = 30MB
Size(7) = 35MB
*Size(8) = 40MB*
Start(n) = Hash(seed + n) mod M
A node should store as much of its last start as possible.  Assuming start
0, 5, and 8 were "hits" but the node had a max size of 60MB.  It can store
0 and 5 and have 25MB left.  That isn't enough to store all of run 8, but
it should store 25MB of the blocks in run 8 anyway.
Size(255) = pow(2, 31) * 17.5MB = 35,840 TB
Decreasing N only causes previously accepted runs to be invalidated.
When a node approaches a transition point for N, it would select a block
height within 25,000 of the transition point.  Once it reaches that block,
it will begin downloading the new runs that it needs.  When updating, it
can set N to zero.  This spreads out the upgrade (over around a year), with
only a small number of nodes upgrading at any time.
New nodes should use the higher M, if near a transition point (say within

@_date: 2015-05-13 11:14:06
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Long-term mining incentives 
On Wed, May 13, 2015 at 10:49 AM, Thomas Voegtlin The position seems to be that it will continue to work for the time being,
so there is still time for more research.
Proof of stake has problems with handling long term reversals.  The main
proposal is to slightly weaken the security requirements.
With POW, a new node only needs to know the genesis block (and network
rules) to fully determine which of two chains is the strongest.
Penalties for abusing POS inherently create a time horizon.  A suggested
POS security model would assume that a full node is a node that resyncs
with the network regularly (every N blocks).    N would be depend on the
network rules of the coin.
The alternative is that 51% of the holders of coins at the genesis block
can rewrite the entire chain.  The genesis block might not be the first
block, a POS coin might still use POW for minting.

@_date: 2015-05-13 11:43:08
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
This could be implemented as a soft fork too.
* 1MB hard size limit
* 900kB soft limit
S = block size
U = UTXO_adjusted_size = S + 4 * outputs - 3 * inputs
A block is valid if S < 1MB and U < 1MB
A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted
size of 252 bytes.
The memory pool could be sorted by fee per adjusted_size.
 Coin selection could be adjusted so it tries to have at least 2 inputs
when creating transactions, unless the input is worth more than a threshold
(say 0.001 BTC).
This is a pretty weak incentive, especially if the block size is
increased.  Maybe it will cause a "nudge"

@_date: 2015-05-13 12:29:23
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Long-term mining incentives 
On Wed, May 13, 2015 at 11:31 AM, Alex Mizrahi A node only needs a path of honest nodes to the network.
If a node is connected to 99 dishonest nodes and 1 honest node, it can
still sync with the main network.
That isn't why checkpoints exist.  They are to prevent a disk consumption
DOS attack.
They also allow verification to go faster.  Signature operations are
assumed to be correct without checking if they are in blocks before the
last checkpoint.
They do protect against multi-month forks though, even if not the reason
that they exist.
If releases happen every 6 months, and the checkpoint is 3 months deep at
release, then for the average node, the checkpoint is 3 to 9 months old.
A 3 month reversal would be devastating, so the checkpoint isn't adding
much extra security.
With headers first downloading, the checkpoints could be removed.  They
could still be used for speeding up verification of historical blocks.
Blocks behind the last checkpoint wouldn't need their signatures checked.
Removing them could cause a hard-fork though, so maybe they could be
defined as legacy artifacts of the blockchain.  Future checkpoints could be

@_date: 2015-05-13 14:12:43
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
I think this is a good way to handle things, but as you say, it is a hard
CHECKLOCKTIMEVERIFY covers many of the use cases, but it would be nice to
fix malleability once and for all.
This has the effect of doubling the size of the UTXO database.  At minimum,
there needs to be a legacy txid to normalized txid map in the database.
An addition to the BIP would eliminate the need for the 2nd index.  You
could require a SPV proof of the spending transaction to be included with
legacy transactions.  This would allow clients to verify that the
normalized txid matched the legacy id.
The OutPoint would be {LegacyId | SPV Proof to spending tx  | spending tx |
index}.  This allows a legacy transaction to be upgraded.  OutPoints which
use a normalized txid don't need the SPV proof.
The hard fork would be followed by a transitional period, in which both
txids could be used.  Afterwards, legacy transactions have to have the SPV
proof added.  This means that old transactions with locktimes years in the
future can be upgraded for spending, without nodes needing to maintain two

@_date: 2015-05-13 14:28:44
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Long-term mining incentives 
On Wed, May 13, 2015 at 1:26 PM, Alex Mizrahi I don't really see how you can protect against total isolation of a node
(POS or POW).  You would need to find an alternative route for the
Even encrypted connections are pointless without authentication of who you
are communicating with.
Again, it is part of the security model that you can connect to at least
one honest node.
Someone tweated all the bitcoin headers at one point.  The problem is that
if everyone uses the same check, then that source can be compromised.
upwards of $100000.
Headers first mean that you can't knock a synced node off the main chain
without winning the POW race.
Checkpoints can be replaced with a minimum amount of POW for initial sync.
This prevents spam of low POW blocks.  Once a node is on a chain with at
least that much POW, it considers it the main chain.,

@_date: 2015-05-13 17:18:24
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
decker.christian at gmail.com> wrote
There are around 20 million UTXOs.  At 2*32 bytes per entry, that is more
than 1GB.  There are more UTXOs than transactions, but 256MB seems a little
I think both IDs can be used in the merkle tree, since we lookup an ID in
The id that is used to sign should be used in the merkle tree.  The hard
fork should simply be to allow transactions that use the normalized
transaction hash.
Agreed, the transaction is simply changed and all the standard rules apply.
Right it is just a database update.  The undo info also needs to be changed
so that both txids are included.
Yeah, if a transaction spends with a legacy txid, it should still match if
the normalized txid is included in the filter.
hard-fork it might be a good idea to build a separate proposal for a
generic hard-fork rollout mechanism.
That would be useful.  On the other hand, we don't want to make them to
I think this is a good choice for a hard fork test, since it is
uncontroversial.  With a time machine, it would have been done this way at
the start.
What about the following:
The reference client is updated so that it uses version 2 transactions by
default (but it can be changed by user).  A pop-up could appear for the GUI.
There is no other change.
All transactions in blocks 375000 to 385000 are considered votes and
weighted by bitcoin days destroyed (max 60 days).
If > 75% of the transactions by weight are version 2, then the community
are considered to support the hard fork.
There would need to be a way to protect against miners censoring
Users could submit their transactions directly to a p2p tallying system.
The coin would be aged based on the age in block 375000 unless included in
the blockchain.  These votes don't need to be ordered and multiple votes
for the same coin would only count once.
In fact, votes could just be based on holding in block X.
This is an opinion poll rather than a referendum though.
Assuming support of the community, the hard fork can then proceed in a
similar way to the way a soft fork does.
Devs update the reference client to produce version 4 blocks and version 3
transactions.  Miners could watch version 3 transactions to gauge user
interest and use that to help decide if they should update.
If 750 of the last 1000 blocks are version 4 or higher, reject blocks with
transactions of less than version 3 in version 4 blocks
    This means that legacy clients will be slow to confirm their
transactions, since their transactions cannot go into version 4 blocks.
This is encouragement to upgrade.
If 950 of the last 1000 blocks are version 4 or higher, reject blocks with
transactions of less than version 3 in all blocks
    This means that legacy nodes can no longer send transactions but can
still receive.  Transactions received from other legacy nodes would remain
If 990 of the last 1000 blocks are version 4 or higher, reject version 3 or
lower blocks
    This is the point of no return.  Rejecting version 3 blocks means that
the next rule is guaranteed to activate within the next 2016 blocks.
Legacy nodes remain on the main chain, but cannot send.  Miners mining with
legacy clients are (soft) forked off the chain.
If 1000 of the last 1000 blocks are version 4 or higher and the difficulty
retarget has just happened, activate hard fork rule
    This hard forks legacy nodes off the chain.  99% of miners support this
change and users have been encouraged to update.  The block rate for the
non-forked chain is ast most 1% of normal.  Blocks happen every 16 hours.
By timing activation after a difficulty retarget, it makes it harder for
the other fork to adapt to the reduced hash rate.

@_date: 2015-05-13 19:11:30
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
The txid or txid-norm is signed, so can't be changed after signing.
The hard fork is to allow transactions to refer to their inputs by txid or
txid-norm.  You pick one before signing.
A signed transaction cannot have its txid changed.  It is true that users
of the system would have to use txid-norm.
The basic refund transaction is as follows.
 A creates TX1: "Pay w BTC to  if signed by A & B"
 A creates TX2: "Pay w BTC from TX1-norm to , locked 48
hours in the future, signed by A"
 A sends TX2 to B
 B signs TX2 and returns to A
A broadcasts TX1.  It is mutated before entering the chain to become
A can still submit TX2 to the blockchain, since TX1 and TX1-mutated have
the same txid-norm.
The problem with this is that 2 level malleability is not protected against.
C spends B which spends A.
A is mutated before it hits the chain.  The only change in A is in the
B can be converted to B-new without breaking the signature.  This is
because the only change to A was in the sciptSig, which is dropped when
computing the txid-norm.
B-new spends A-mutated.  B-new is different from B in a different place.
The txid it uses to refer to the previous output is changed.
The signed transaction C cannot be converted to a valid C-new.  The txid of
the input points to B.  It is updated to point at B-new.  B-new and B don't
have the same txid-norm, since the change is outside the scriptSig.  This
means that the signature for C is invalid.
The txid replacements should be done recursively.  All input txids should
be replaced by txid-norms when computing the txid-norm for the
transaction.  I think this repairs the problem with only allowing one level?
Computing txid-norm:
- replace all txids in inputs with txid-norms of those transactions
- replace all input scriptSigs with empty scripts
- transaction hash is txid-norm for that transaction
The same situation as above is not fatal now.
C spends B which spends A.
A is mutated before it hits the chain.  The only change in A is in the
B can be converted to B-new without breaking the signature.  This is
because the only change to A was in the sciptSig, which is dropped when
computing the txid-norm (as before).
B-new spends A mutated.  B-new is different from B in for the previous
The input for B-new points to A-mutated.  When computing the txid-norm,
that would be replaced with the txid-norm for A.
Similarly, the input for B points to A and that would have been replaced
with the txid-norm for A.
This means that B and B-new have the same txid-norm.
The signed transaction C can be converted to a valid C-new.  The txid of
the input points to B.  It is updated to point at B-new.  B-new and B now
have have the same txid-norm and so C is valid.
I think this reasoning is valid, but probably needs writing out actual

@_date: 2015-05-13 21:27:14
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
After more thought, I think I came up with a clearer description of the
recursive version.
The simple definition is that the hash for the new signature opcode should
simply assume that the normalized txid system was used since the
beginning.  All txids in the entire blockchain should be replaced with the
"correct" values.
This requires a full re-index of the blockchain.  You can't work out what
the TXID-N of a transaction is without knowning the TXID-N of its parents,
in order to do the replacement.
The non-recursive version can only handle refunds one level deep.
from: IN
sigA: based on hash(...)
from A
sig: based on hash(from: TXID-N(A) | "")  // sig removed
from B
sig: based on hash(from: TXID-N(B) | "")  // sig removed
If A is mutated before being added into the chain, then B can be modified
to a valid transaction (B-new).
from: IN
sig_mutated: based on hash(...) with some mutation
B has to be modified to B-new to make it valid.
from A-mutated
sig: based on hash(from: TXID-N(A-mutated), "")
Since TXID-N(A-mutated) is equal to TXID-N(A), the signature from B is
still valid.
Howver, C-new cannot be created.
from B-new
sig: based on hash(from: TXID-N(B-new), "")
TXID-N(B-new) is not the same as TXID-N(B).  Since the from field is not
removed by the TXID-N operation, differences in that field mean that the
TXIDs are difference.
This means that the signature for C is not valid for C-new.
The recursive version repairs this problem.
Rather than simply delete the scriptSig from the transaction.  All txids
must also be replaced with their TXID-N versions.
Again, A is mutated before being added into the chain and B-new is produced.
from: IN
sig_mutated: based on hash(...) with some mutation
TXID-N: TXID-N(A)
B has to be modified to B-new to make it valid.
from A-mutated
sig: based on hash(from: TXID-N(A-mutated), "")
TXID-N: TXID-N(B)
Since TXID-N(A-mutated) is equal to TXID-N(A), the signature from B is
still valid.
Likewise the TXID-N(B-new) is equal to TXID-N(B).
The from field is replaced by the TXID-N from A-mutated which is equal to
TXID-N(A) and the sig is the same.
from B-new
sig: based on hash(from: TXID-N(B-new), "")
The signature is still valid, since TXID-N(B-new) is the same as TXID-N(B).
This means that multi-level refunds are possible.

@_date: 2015-05-13 21:32:43
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
That's great.  So, basically the multi-level refund problem is solved by

@_date: 2015-05-15 11:45:05
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
The signature signs everything except the signature itself.  The normalized
txid doesn't include that signature, so mutations of the signature don't
cause the normalized txid to change.
If the refund transaction refers to the parent using the normalised txid,
then it doesn't matter if the parent has a mutated signature.  The
normalized transaction ignores the mutation.
If the parent is mutated, then the refund doesn't even have to be modified,
it still refers to it.
If you want a multi-level refund transaction, then all refund transactions
must use the normalized txids to refer to their parents.  The "root"
transaction is submitted to the blockchain and locked down.
If there are 2 transactions which are mutations of each other, then only
one can be added to the block chain, since the other is a double spend.
The normalized txid refers to all of them, rather than a specific
Mutation is only a problem if it occurs after signing.  The signature signs
everything except the signature itself.
Correct, but normalized txids are safe against replays, so are better.
I think the new signature opcode fixes things too.  The question is hard
fork but clean solution vs a soft fork but a little more hassle.

@_date: 2015-05-16 11:52:34
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
That is a good point.  Since the point is the change is to use good
practice right back until the genesis block, maybe the scriptSig for
coinbases could be replaced by the height expressed as a varint.  That
means that all coinbases get a unique normalized txid.  The coinbases with
duplicate txids still wouldn't be spendable though.

@_date: 2015-05-16 12:09:50
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
On Sat, May 16, 2015 at 1:22 AM, Rusty Russell Fair enough.
I agree, we want people to compress the UTXO space and a transaction with
100 inputs and one output is great.
It may have privacy problem though.
The incentive problem can be fixed by excluding UTXOs from blocks before a
certain count.
UTXOs in blocks before 375000 don't count.
They can be stored as a fixed digest.  That can be any size, depending on
security requirements.
Gmaxwell's cost proposal is 3-4 bytes per UTXO change.  It isn't
4*UXTO.size - 3*UTXO.size
It is only a small nudge.  With only 10% of the block space to play with it
can't be massive.
This requires that transactions include scriptPubKey information when
broadcasting them.
That is to large a cost for a 10% block change.  It could be included in
the block size hard fork though.  I think have one combined "cost" for
transactions is good.  It means much fewer spread out transaction checks.
The code for the cost formula would be in one place.

@_date: 2015-05-16 12:25:53
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
To mitigate against this, two chaintips could be tracked.  The miner tip
and the client tip.
Miners would build on the miner tip.  When performing client services, like
wallets, they would use the client tip.
The client would act exactly the same as any node, the only change would be
that it gives miner work based on the mining tip.
If the two tips end up significantly forking, there would be a warning to
the miner and perhaps eventually refuse to give out new work.
That would happen when there was a miner level hard-fork.
To launch that attack, you need to produce fake blocks.  That is
Stephen Cale's suggestion to wait more than one block before counting a
transaction as confirmed would also help mitigate.

@_date: 2015-05-16 12:29:14
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Block Size Increase Requirements 
On Sat, May 16, 2015 at 5:39 AM, Stephen In effect, there is a confirm penalty for less strict blocks.  Confirms =
max(miner_confirms, merchant_confirms - 3, 0)
Merchants who don't upgrade end up having to wait longer to hit
If they get deep enough in the chain, though, the client should probably
That is a good idea.  Any parameters that have miner/merchant differences
should be modifiable (but only upwards) in the command line.
"Why are my transactions taking longer to confirm?"
"There was a soft fork to make the block size larger and your client is
being careful.  You need to add "minermaxblocksize=4MB" to your
bitcoin.conf file."
Hah, it could be called a "semi-hard fork"?

@_date: 2015-05-19 09:59:27
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
On Mon, May 18, 2015 at 2:42 AM, Rusty Russell On the plus side, as people spend their money, old UTXOs would be used up
and then they would be included in the cost function.  It is only people
who are storing their money long term that wouldn't.
They are unlikely to have consumed their UTXOs anyway, unless miners
started paying for UTXOs.
We could make it a range.
UTXOs from below 355,000 and above 375,000 are included.  That can create
incentive problems for the next similar change, I think a future threshold
is better.
Maybe I mis-read.
I was thinking about it being a soft-fork.
If it was combined with the 20MB limit change, then it can be anything.
I made a suggestion somewhere (her or forums not sure), that transactions
should be allowed to store bytes.
For example, a new opcode could be added,  OP_LOCK_BYTES.
This makes the transaction seem  larger.  However, when
spending the UTXO, that transaction counts as  smaller, even
against the hard-cap.
This would be useful for channels.  If channels were 100-1000X the
blockchain volume and someone caused lots of channels to close, there
mightn't be enough space for all the close channel transactions.  Some
people might be able to get their refund transactions included in the
blockchain because the timeout expires.
If transactions could store enough space to be spent, then a mass channel
close would cause some very large blocks, but then they would have to be
followed by lots of tiny blocks.
The block limit would be an average not fixed per block.  There would be 3
Absolute hard limit (max bytes no matter what): 100MB
Hard limit (max bytes after stored bytes offset): 30MB
Soft limit (max bytes equivalents): 10MB
Blocks lager than ~32MB require a new network protocol, which makes the
hard fork even "harder".  The protocol change could be "messages can now be
150MB max" though, so maybe not so complex.
I have written a BIP about it.  It is still in the draft stage.  I had a
look into writing up the code for the protocol change.

@_date: 2015-05-19 10:13:17
@_author: Tier Nolan 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
The normalized TXID cannot depend on height for other transactions.
Otherwise, it gets mutated when been added to the chain, depending on
An option would be that the height is included in the scriptSig for all
transactions, but for non-coinbase transctions, the height used is zero.
I think if height has to be an input into the normalized txid function, the
specifics of inclusion don't matter.
The previous txid for coinbases are required to be all zeros, so the
normalized txid could be to add the height to the txids of all inputs.
Again, non-coinbase transactions would have heights of zero.
I assumed that since the scriptSig in the coinbase is specifically intended
to be "random" bytes/extra nonce, so putting a restriction on it was
guaranteed to be backward compatible.

@_date: 2015-05-27 10:35:03
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Version bits proposal 
I think it would be better to have the deadlines set as block counts.  That
eliminates the need to use the median time mechanism.
The deadline could be matched to a "start-line".  The definition would then
be something like
BIP 105
Start block: 325000
End block: 350000
Activation: 750 of 1000
Implication: 950 of 1000
Bit: 9
This would allow creation of a simple table of known BIPs.  It also keeps
multiple users of the bit as strictly separate.
The alternative to the start time is that it is set equal to the deadline
or implication time of the previous user of the bit.
Was the intention to change the 95% rule.  You need 750 of the last 1000 to
activate and then must wait at least 1000 for implication?

@_date: 2015-05-27 11:00:16
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
This could cause legacy transactions to become unspendable.
A new transaction version number should be used to indicate the change of
the field from sequence number to relative lock time.
Legacy transactions should not have the rule applied to them.

@_date: 2015-05-27 12:26:33
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Version bits proposal 
Fair enough.  It means slightly more processing, but the median time could
be cached in the header index, so no big deal.
Block counts are inconvenient for planning, as there's no guarantee
I don't think the deadline needs to be set that accurately.  A roughly 6
month deadline should be fine, but as you say a majority of miners is
needed to abuse the median time and it is already a miner poll.
Perhaps the number of blocks used in the median could be increased to
reduce "noise".
The median time could be median of the last 144 blocks plus 12 hours.
I think it makes it easier to write the code.  It reduced the state that
needs to be stored per BIP.  You don't need to check if the previous bips
were all accepted.
Each bit is assigned to a particular BIP for a particular range of times
(or blocks).
If block numbers were used for the deadline, you just need to check the
block index for the deadline block.
enum {
    BIP_INACTIVE = 0,
    BIP_ACTIVE,
    BIP_LOCKED
    BIP_INVALID_BLOCK,
int GetBIPState(block, bip)
    if (block.height == bip.deadline)  // Bit must be set to match
locked/unlocked at deadline
    {
        int bipState = check_supermajority(...);
        if (bipState == BIP_LOCKED && (block.nVersion & bip.bit)
            return BIP_LOCKED;
        if (bipState != BIP_LOCKED && (block.nVersion & (~bip.bit)))
            return BIP_INACTIVE;
        return BIP_INVALID_BLOCK;
    }
    if (block.height > deadline) // Look at the deadline block to determine
if the BIP is locked
        return (block_index[deadline].nVersion & bip_bit) != 0 ? BIP_LOCKED
: BIP_INACTIVE;
    if (block.height < startline + I) // BIP cannot activate/lock until
startline + implicit window size
        return INACTIVE;
    return check_supermajority(....) // Check supermajority of bit
The block at height deadline would indicate if the BIP was locked in.
Block time could still be used as long as the block height was set after
that.  The deadline_time could be in six months.  The startline height
could be the current block height and the deadline_height could be
startline + 35000.
The gives roughly
start time = now
deadline time = now + six months
deadline height = now + eight months
The deadline height is the block height when the bit is returned to the
pool but the deadline time is when the BIP has to be accepted.
It also helps with the warning system.  For each block height, there is a
set of known BIP bits that are allowed.  Once the final deadline is passed,
the expected mask is zeros.
I think the phrasing is ambiguous.  I was just asking for clarification.
"Whenever I out of any W *subsequent* blocks (regardless of the block
itself) have bit B set,"
That suggests that the I of W blocks for the 95% rule must happen after
activation.  This makes the rule checking harder.  Easier to use the
current system, where blocks that were part of the 750 rule also count
towards the 95% rule.

@_date: 2015-05-28 11:30:18
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Can you update it so that it only applies to transactions with version
number 3 and higher.  Changing the meaning of a field is exactly what the
version numbers are for.
You could even decode version 3 transactions like that.
Version 3 transactions have a sequence number of 0xFFFFFFFF and the
sequence number field is re-purposed for relative lock time.
This means that legacy transactions that have already been signed but have
a locktime in the future will still be able to enter the blockchain
(without having to wait significantly longer than expected).
On Thu, May 28, 2015 at 10:56 AM, Mark Friedenbach

@_date: 2015-05-28 14:35:57
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
I think it is still a version change.  At the moment, the 4 bytes refer to
the sequence number and afterwards they mean something else.
For relative locktime verify, I think most use cases could be block count
based and don't need to be able to count very high.
I think the main benefit is that protocols can have one party trigger a
step while giving the other party guaranteed time to respond.
*Fast Channel Close*
This assumes that malleability is fixed.
Alice creates
output (x) to [multisig A1 & B1]
input TXA (signed by Alice)
Output [(A2 & relative_check_locktime(150)) OR (multisig A3 &  B2)]
Alice sends Refund to Bob
Bob signs it and sends it back to Alice
Alice verifies the signature, adds her own and sends it to Bob.
She broadcasts TXA (would wait until Bob confirms acceptance).
This means that both Alice and Bob have the refund transaction and can use
it to close the channel (assuming TXA is not mutated).
Alice can send money to Bob by creating a transaction which spends the
output of the refund transaction (splitting the output x-b for Alice and b
for Bob), signing it and sending it to Bob.
Alice can force Bob to close the channel by broadcasting the refund
transaction.  150 blocks later, she gets the channel deposit if he doesn't
If she had sent some money to Bob, he has 150 blocks to sign the
transaction that pays him the most money and broadcast it.  Alice gets the
remainder of the deposit.
Alice cannot broadcast earlier version, since Bob doesn't send her the
signed versions.
This means that the channel doesn't need a defined end date.  Either party
can close the channel whenever they want.
TXA could be protected against malleability by adding a locktime path.
This would only be for use if the transaction is mutated.

@_date: 2015-05-28 16:18:05
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
I meant whatever the next version is, so you are right, it's version 2.
The change is backwards compatible (since there is no restrictions on
sequence numbers).   This makes it a soft fork.
That doesn't change the fact that you are changing what a field in the
transaction represents.
You could say that the sequence number is no longer encoded in the
serialization, it is assumed to be 0xFFFFFFFF for all version 2+
transactions and the relative locktime is a whole new field that is the
same size (and position).
I think keeping some of the bytes for other uses is a good idea.  The
entire top 2 bytes could be ignored when working out relative locktime
verify.  That leaves them fully free to be set to anything.
It could be that if the MSB of the bottom 2 bytes is set, then that
activates the rule and the top 2 bytes are ignored.
Are there any use-cases which need a RLTV of more than 8191 blocks delay
(that can't be covered by the absolute version)?

@_date: 2015-05-28 16:57:15
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
What are the use cases for relative lock time verify?  I have 1 and I think
that is the kind of thing it is useful for.
I think that most cases are just to guarantee that the other party has a
chance to react.  This means that 8191 blocks should be more than enough
(and most would set it lower).
For long term, the absolute version is just as good.  That depends on use
cases.  "You can't take step 4 until 3 months after step 3 has completed"
doesn't seem useful.
On Thu, May 28, 2015 at 4:38 PM, Mark Friedenbach

@_date: 2015-05-28 18:21:34
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Yes, Alice is assumed to be the one who funded the channel.  It is a single
direction channel (Alice to Bob).
Assuming the deposit is 1 BTC.
When the channel is created, Alice can broadcast the refund transaction
immediately and the get her money back 150 blocks later.
The full scriptPubKey for the refund transaction would be
    <150> OP_RELATIVE_CHECKLOCKTIME_VERIFY OP_DROP     OP_2   OP_2
This means that Alice can spend the output after 150 blocks but with both
signatures Bob and Alice can spend the output without the delay.
She can send money to Bob by spending the non-locked output of the refund
transaction (0.01BTC for Bob and 0.99BTC for Alice).
Bob has a transaction that pays him 0.01BTC and pays Alice 0.99BTC from the
refund transaction and is signed by Alice, but still requires his
signature.  Only Bob can make the transaction valid.
It can be spent as soon as the refund transaction is broadcast.
He has the refund transaction, so he can start the process whenever he
Assume the channel runs for a while, and Alice sends 0.3BTC total.
Bob has a transaction which pays him 0.3BTC and Alice 0.7BTC.  He also has
some that pay him less than 0.3, but there is no point in him using those
Alice decides she wants to close the channel, so asks bob to sign his final
transaction and broadcast it and the refund transaction.
If Bob refuses to do that, then Alice can just broadcast the refund
If Bob still refuses to broadcast his final transaction, then Alice gets
1BTC and he gets nothing, after 150 blocks.
This means he will send his final transaction before the 150 blocks have
passed.  This gets him 0.3 and Alice 0.7.
Bob can close the channel immediately and Alice can force it to be closed
within 150 blocks (~1 day).
Protection against that type of fraud isn't covered by channels.  They are
just to make sure money is handed over.
Does the explanation above help?
With some risks.
As long as Bob is online and sees the refund transaction being broadcast by
Alice, then there is no risk to him.
Alice can close the transaction whenever she wants, so there is no holdup
risk for her.
I mean with OP_CHECKLOCKTIMEVERIFY.
She could say that TXA pays to her in 6 months.
If TXA ends up mutated after being broadcast, then she would have to wait
the 6 months.  It's better than nothing and maybe Bob would sign the
mutated transaction.

@_date: 2015-05-29 12:42:09
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
If the plan is a fix once and for all, then that should be changed too.  It
could be set so that it is at least some multiple of the max block size
Alternatively, the merkle block message already incorporates the required
- headers message (with 1 header)
- merkleblock messages (max 1MB per message)
The transactions for each merkleblock could be sent directly before each
merkleblock, as is currently the case.
That system can send a block of any size.  It would require a change to the
processing of any merkleblocks received.

@_date: 2015-05-29 15:09:20
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
How do you define that the movement is successful?
The measure is miner consensus.  How do you intend to measure
exchange/merchant acceptance?

@_date: 2015-05-29 15:22:27
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Sorry again, I keep auto-sending from gmail when trying to delete.
In theory, using the "nuclear option", the block size can be increased via
soft fork.
Version 4 blocks would contain the hash of the a valid extended block in
the coinbase.
 <32 byte extended hash>
To send coins to the auxiliary block, you send them to some template.
OP_P2SH_EXTENDED  OP_TRUE
This transaction can be spent by anyone (under the current rules).  The
soft fork would lock the transaction output unless it transferred money
from the extended block.
To unlock the transaction output, you need to include the txid of
transaction(s) in the extended block and signature(s) in the scriptSig.
The transaction output can be spent in the extended block using P2SH
against the scriptPubKey hash.
This means that people can choose to move their money to the extended
block.  It might have lower security than leaving it in the root chain.
The extended chain could use the updated script language too.
This is obviously more complex than just increasing the size though, but it
could be a fallback option if no consensus is reached.  It has the
advantage of giving people a choice.  They can move their money to the
extended chain or not, as they wish.

@_date: 2015-05-29 19:28:22
@_author: Tier Nolan 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB 
I don't think so.  The lower security is the potential centralisation
risk.  If you have your money in the "root" chain, then you can watch it.
You can probably also watch it in a 20MB chain.
Full nodes would still verify the entire block (root + extended).  It is a
"nuclear option", since you can make any changes you want to the rules for
the extended chain.  The only safe guard is that people have to voluntarly
transfer coins to the extended block.
The extended block might have 10-15% of the total bitcoins, but still be
useful, since they would be the ones that move the most.  If you want to
store your coins long term, you move them back to the root block where you
can watch them more closely.
It does make things more complex though.  Wallets would have to list 2

@_date: 2015-11-01 23:46:39
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Compatibility requirements for hard or soft forks 
There can be unavoidable situations which cause locked coins become
In an ideal world, soft forks that make UTXOs unspendable should increase
the tx version number.  BIP-13 should have done that.  That would make the
change opt-in.
The disabled opcodes like OP_CAT were a DOS/network security change.
Invalidating locked coins is another reason that they shouldn't have been
disabled permanently.
It would have been better to disable them for six months, so at least
people can get their coins back after that.  Inherently, protecting the
network required some limitations being added so that nodes couldn't be
For guidelines
* Transaction version numbers will be increased, if possible
* Transactions with unknown/large version numbers are unsafe to use with
* Reasonable notice is given that the change is being contemplated
* Non-opt-in changes will only be to protect the integrity of the network
Locked transaction that can be validated without excessive load on the
network should be safe to use, even if non-standard.
An OP_CAT script that requires TBs of RAM to validate crosses the threshold
of reasonableness.

@_date: 2015-11-02 01:30:51
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Compatibility requirements for hard or soft forks 
On Mon, Nov 2, 2015 at 12:23 AM, Justus Ranvier via bitcoin-dev <
A locked transaction could pay to an OP_CAT script with the private key
being lost.
Even if it is only in theory, it is still worth trying to prevent rule
changes which permanently prevent outputs being spendable.
If at least one year's notice was given, then people aren't going to lose
their money, since they have notice.
Locked transactions could have a difference expectation than non-locked
Miners can collectively vote to disable specific UTXOs and change the
acceptance rules.

@_date: 2015-11-06 09:27:24
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Dealing with OP_IF and OP_NOTIF malleability 
One and zero should be defined as arrays of length one.  Otherwise, it is
still possible to mutate the transaction by changing the length of the
They should also be minimally encoded but that is covered by previous rules.
On Fri, Nov 6, 2015 at 8:13 AM, jl2012 via bitcoin-dev <

@_date: 2015-11-06 09:37:58
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Dealing with OP_IF and OP_NOTIF malleability 
I meant not to use the OP_PUSH opcodes to do the push.
Does OP_0 give a zero length byte array?
Would this script return true?
OP_PUSHDATA1 (length = 1, data = 0)
The easiest definition is that OP_0 and OP_1 must be used to push the data
and not any other push opcodes.

@_date: 2015-11-10 09:44:11
@_author: Tier Nolan 
@_subject: [bitcoin-dev] request BIP number for: "Support for Datastream 
The network protocol is not quite consensus critical, but it is important.
Two implementations of the decompressor might not be bug for bug
compatible.  This (potentially) means that a block could be designed that
won't decode properly for some version of the client but would work for
another.  This would fork the network.
A "raw" network library is unlikely to have the same problem.
Rather than just compress the stream, you could compress only block
messages only.  A new "cblock" message could be created that is a
compressed block.  This shouldn't reduce efficiency by much.
If a client fails to decode a cblock, then it can ask for the block to be
re-sent as a standard "block" message.
This means that it is a pure performance improvement.  If problems occur,
then the client can just switch back to uncompressed mode for that block.
You should look into the block relay system.  This gives a larger
improvement than simply compressing the stream.  The main benefit is
latency but it means that actual blocks don't have to be sent, so gives a
potential 50% compression ratio.  Normally, a node receives all the
transactions and then those transactions are included later in the block.
On Tue, Nov 10, 2015 at 5:40 AM, Johnathan Corgan via bitcoin-dev <

@_date: 2015-11-10 16:30:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] request BIP number for: "Support for Datastream 
On Tue, Nov 10, 2015 at 4:11 PM, Peter Tschipper If the main point is for historical data, then sticking to just blocks is
the best plan.
Since small blocks don't compress well, you could define a "cblocks"
message that handles multiple blocks (just concatenate the block messages
as payload before compression).
The sending peer could combine blocks so that each cblock is compressing at
least 10kB of block data (or whatever is optimal).  It is probably worth
specifying a maximum size for network buffer reasons (either 1MB or 1 block
Similarly, transactions could be combined together and compressed "ctxs".
The inv messages could be modified so that you can request groups of 10-20
transactions.  That would depend on how much of an improvement compressed
transactions would represent.
More generally, you could define a message which is a compressed message
holder.  That is probably to complex to be worth the effort though.

@_date: 2015-10-13 15:57:00
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Liquid 
It is interesting someone trying the sidechain approach.
I guess having trusted third parties to manage the chain was not a short
term thing?  It looks like there is no POW for the Liquid sidechain.
This is an area where the bitcoin could benefit by adding a way to transfer
money to/from sidechain without requiring third parties.
On Tue, Oct 13, 2015 at 3:27 PM, Benjamin via bitcoin-dev <

@_date: 2015-10-19 16:23:53
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
Is this proposal recursive?
*Coinbase transaction *
* n-txid = txid
*Non-coinbase transactions*
* replace sigScripts with empty strings
* replace txids in TxIns with n-txid for parents
The 2nd step is recursive starting from the coinbases.
In effect, the rule is that txids are what they would have been if n-txids
had been used right from the start.

@_date: 2015-09-03 12:59:01
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP 100 specification 
Does the 32MB limit actually still exist anywhere in the code?  In effect,
it is re-instating a legacy limitation.
The message size limit is to minimize the storage required per peer.  If a
32MB block size is required, then each network input buffer must be at
least 32MB. This makes it harder for a node to support a large number of
There is no reason why a single message is used for each block.  Using the
merkleblock message (or a different dedicated message), it would be
possible to send messages which only contain part of a block and have a
limited maximum size.
This would allow receiving parts of a block from multiple sources.
This is a separate issue but should be considered if moving past 32MB block
sizes (or maybe as a later protocol change).
much fewer bytes in the coinbase.
Even with the +/- 20% rule, miners could vote for the nearest MB.  Once the
block size exceeds 5MB, then there is enough resolution anyway.
I think abstains should count for the status quo.  Votes which are out of
range should be clamped.
Having said that, if core supports the change, then most miners will
probably vote one way or another.
I think this is unclear, though mathematically exact.
Sort the votes for the last 12,000 blocks from lowest to highest.
Blocks which don't have a vote are considered a vote for the status quo.
Votes are limited to +/- 20% of the current value.  Votes that are out of
range are considered to vote for the nearest in range value.
The raise value is defined as the vote for the 2400th highest block (20th
The lower value  is defined as the vote for the 9600th highest block (80th
If the raise value is higher than the status quo, then the new limit is set
to the raise value.
If the lower value is lower than the status quo, then the new limit is set
to the lower value.
Otherwise, the size limit is unchanged.

@_date: 2015-09-16 18:53:20
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
I think the 75% rule should be maintained.  It confirms that miners who are
setting the bit are actually creating blocks that meet the new rule (though
it doesn't check if they are enforcing it).
What is the reason for aligning the updated to the difficulty window?
Miners set bit
If 75% of blocks of last 2016 have bit set, goto tentative
Miners set bit
Reject blocks that have bit set that don't follow new rule
If 95% of blocks of last 2016 have bit set, goto locked-in
Point of no return
Miners still set bit
Reject blocks that have bit set that don't follow new rule
After 2016 blocks goto notice
Miners don't set bit for at least 10080 blocks
Reject blocks that don't follow new rule
'''Failure: Timeout'''
I think counting in blocks is easier to be exact here.
If two bits were allocated per proposal, then miners could vote against
forks to recover the bits.  If 25% of the miners vote against, then that
kills it.
In the rationale, it would be useful to discuss effects on SPV clients and
buggy miners.
SPV clients should be recommended to actually monitor the version field.

@_date: 2015-09-16 21:30:27
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
On Wed, Sep 16, 2015 at 9:19 PM, Rusty Russell It isn't useful for actually using the feature, but some miners might set
the bit but not actually create blocks that comply with the new rule.
This would cause their blocks to be orphaned until they fixed it.
OK, *that* variant makes perfect sense, and is no more complex, AFAICT.
It could be more than two weeks if the support stays between 80% and 90%
for a while.
75%+ checks that blocks with the bit set follow the rule.
95%+ enters lock-in and has the same rules as 75%+, but is irreversible at
that point.
I meant if the 2nd bit was part of the BIP.  One of the 2 bits is "FOR" and
the other is "AGAINST".  If against hits 25%, then it is deemed a failure.
The 2nd bit wouldn't be used normally.  This means that proposals can be
killed quickly if they are obviously going to fail.

@_date: 2015-09-16 21:32:16
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
At 75%, you have a pretty solid super-majority.
You can safely reject blocks that have the bit set but are invalid
according to the new rule (as long as everyone who sets the bit does it

@_date: 2015-09-16 21:48:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
The point of the 75% is just as a test run.  Enforcement wouldn't happen
until 95%.
At 75%, if someone sets the bit, then they should be creating valid blocks
(under the rule).

@_date: 2015-09-16 21:57:42
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
It would be a consensus rule.  If >75% of the blocks in the last 2016
window have the bit set, then reject all blocks that have the bit set and
fail to meet the rule.

@_date: 2015-09-17 11:38:29
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
The discussion was about what each state means, not the thresholds
exactly.  I agree that can be set later.
to applying the new rule from the start (in your own blocks)
it's much simpler to implement).
I agree that miners should apply the rule from the start in their own
Miners set bit
Miners apply rule to their own blocks
If 75% of blocks of last 2016 have bit set, goto tentative
Miners set bit
Miners apply rule to their own blocks
Miners enforce rule in blocks with bit set (reject invalid blocks)
If 95% of blocks of last 2016 have bit set, goto locked-in
Point of no return
Miners set bit
Miners apply rule to their own blocks
Miners enforce rule in blocks with bit set (reject invalid blocks)
After 2016 blocks goto activated
Miners don't set bit
Reject any block that has the bit set for 10080 blocks (10 diff periods)
Reject blocks that don't follow new rule
The advantage of enforcing the rule when 75% is reached (but only for
blocks with the bit set) is that miners get early notification that they
have implemented the rule incorrectly.    They might produce blocks that
they think are fine, but which aren't.

@_date: 2015-09-23 19:48:38
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Weak block thoughts... 
You can create these blocks in advance too.
- receive weak block
- validate
- create child block
It becomes a pure array lookup to get the new header that builds on top of
that block.  The child blocks would need to be updated as the memory pool
changes though.
This also speeds up propagation for the miner.  The first weak block that
is broadcast could end up being copied by many other miners.
A miner who is copying a block could send coinbase + original header if he
hits a block.  Weak blocks that are just coinbase + header could have lower
POW requirements, since they use up much less bandwidth.
Miners would mostly copy other miners once they had verified their blocks.
The IBLT system works well here.  A miner could pick a weak block that is
close to what it actually wants to broadcast.
Aggregator nodes could offer a service to show/prove how many weak blocks
that the transaction has been accepted in.
This assumes other compression systems for handling block propagation.

@_date: 2015-09-24 09:52:34
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Torrent-style new-block propagation on Merkle 
Mining pools currently connect to the "fast relay network".  This is
optimised for fast block distribution.  It does no validation and is only
for low latency propagation.  The normal network is used as a fallback.
My understanding is that it works as follows:
Each miner runs a normal full node and a relay node on the same computer.
The full node tells the relay node whenever it receives a new transaction
via the inv message and the node requests the full transaction.
The relay node tells its relay peers that it knows about the transaction
(hash only) and its 4 byte key. This is not forwarded onwards, since the
relay peer only gets the hash of the transaction and doesn't do validation
anyway.  The key is just a 4 byte counter.
Each relay node keeps a mapping of txid to key for each of its peer.  There
is some garbage collection and entries are removed once the transaction is
included in a block (there might be a confirm threshold).
When a block is found, the local node sends it to the relay node.  The
relay node then forwards it to all of its peers in a compact form.
The block is sent as a list of keys for that peer and full transactions are
only sent for unknown transactions.
When a relay node receives a block, it just verifies the POW, checks that
it is new and recent.  It does not do tx validation.  It forwards the block
to its local full node, which does the validation.  Since the relay node is
on localhost, it never gets kicked due to sending invalid blocks.  This
prevents a DOS attack where you could send invalid blocks to the relay node
and cause the local full node to kick it.
If all the transactions are already known, then it can forward a block for
only 4 bytes per transactions.  I think it has an optimisation, so that is
compressed to 1 byte per tx.

@_date: 2015-09-24 19:17:50
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] New "sendheaders" p2p message 
Is there actually a requirement for the new message?  New nodes could just
unilaterally switch to sending headers and current nodes would be
It looks like the only DOS misbehaving penalty is if the header is invalid
or if the headers don't form a chain.

@_date: 2015-09-27 10:42:24
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Weak block thoughts... 
The POW threshold could be dynamic.  The first weak-block that builds on a
new block could be forwarded with a smaller target.
This reduces  the window size until at least one weak block is propagated.
The change in threshold could be time based (for the first 30 seconds or
so).  This would cause a surge of traffic when a new block once a new block
has propagated, so perhaps not so good an idea.
If there is a transaction backlog, then miners could forward merkle
branches with transactions in the memory pool with a commitment in the

@_date: 2015-09-28 13:47:25
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
There never was a rule that soft-forks require total consensus.  It is
desirable but not mandatory.
A majority of miners can inherently implement a soft fork against the
wishes of the rest of the users.
Merchant/exchange/user checkpointing is the defense and therefore is a
perfectly valid response to miners taking such an action.  If a soft fork
is opposed by a large section of the users, then threatening (and
implementing) a checkpoint is the correct response.
No group can force through a hard fork, it inherently requires buy-in from
a large portion of the userbase.  That is where the "total consensus"
requirement comes from.  Naturally, absolute total consensus isn't actually
required but you do need very large consensus and also consensus across the
various sub-groups.

@_date: 2016-08-04 00:55:00
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP clearing house addresses 
I think reversing transactions is not likely to be acceptable.  You could
add an opcode that requires that an output be set to something.
[target script] SPENDTO
This would require that [target script] is the script for the corresponding
output.  This is a purely local check.
For example, if SPENDTO executes as part of the script for input 3, then it
checks that output 3 uses the given script as its scriptPubKey.  The value
of input 3 and output 3 would have to be the same too.
This allows check sequence verify to be used to lock the spending script
for a while.  This doesn't allow reversal, but would give a 24 hour window
where the spenders can reverse the transaction.
[IF <1 day> CSV DROP  CHECKSIG ELSE  CHECKSIG] SPENDTO  CHECKSIG
Someone with the live public key can create a transaction that spends the
funds to the script in the square brackets.
Once that transaction hits the blockchain, then someone with the  has 24 hours to spend the output before the person with the
live keys can send the funds onward.

@_date: 2016-08-06 12:13:52
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP clearing house addresses 
I think it would be more accurate to say that the requirement is that
reversal doesn't happen unexpectedly.
If it is clear in the script that reversal is possible, then obviously the
recipient can take that into consideration.
Key management is a thing.  Managing risk by keeping some keys offline is
an important part of that.
BitGo has an "instant" system where they promise to only sign one
transaction for a given output.  If you trust BitGo, then this is safe from
double spending, since a double spender can't sign two transactions.
If BitGo had actually implemented a daily withdrawal limit, then their
system ends up similar to cold storage.  Only 10% of the funds at Bitfinex
could have been withdrawn before manual intervention was required (with
offline keys).
Who will accept
Obviously, if a payment is reversible, then you treat it as a reversible
payment.  The protection here relates to moving coins from the equivalent
of cold storage to hot storage.
It is OK if it takes longer, since security is more important than
convenience for coins in cold storage.
This relates to the reserves held by the exchange.  A portion of the funds
are in hot storage with live keys.  These funds can be stolen by anyone who
gets access to the servers.  The remaining funds are held in cold storage
and they cannot be accessed unless you have the offline keys.  These funds
are supposed to be hard to reach and require manual intervention.
I think this is a wrong approach. hacks and big losses are sad, but all
Setting up offline keys to act as firebreaks is part of good security

@_date: 2016-08-08 10:56:24
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP clearing house addresses 
Centralized exchanges also allow for things like limit orders.  You don't
even have to be logged in and they can execute trades.  This couldn't be
done with channels.
Using channels and a centralized exchange gets many of the benefits of a
distributed exchange.
The channel allows instant funding while allowing the customer to have full
control over the funds.  The customer could fund the channel and then move
money to the exchange when needed.
Even margin account holders might like the fact that it is clear which
funds are under their direct control and which funds are held by the
If they are using bitcoin funds as collateral for a margin trade, then
inherently the exchange has to have control over those funds.  A 2 of 3
system where the customer, exchange and a 3rd party arbitration agency
holds keys might be acceptable to the exchange.

@_date: 2016-08-08 12:01:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP clearing house addresses 
With channels and the exchange acting as hub, you can do instant trades
between altcoins.
This doesn't work with fiat accounts.  A "100% reserve" company could issue
fiat tokens.  The exchange could then trade those tokens.
This eliminates the counter-party risk for the exchange.  If the exchange
dies, you still have your (alt)coins and also fiat tokens.
There is still risk that the token company could go bankrupt though.  This
could be mitigated by that company requiring only "cashing out" tokens to
accounts which have been verified.
The company could set up a blockchain where it signed the blocks rather
than mining and could get money from transaction fees and also minting fees
(say it charges 1% for minting new tokens)
I wonder what how the law would work for that.  It isn't actually doing
trading, it is just issuing tokens and redeeming them.

@_date: 2016-08-10 11:41:35
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
Have you considered CDMA?  This has the nice property that it just sounds
like noise.  The codes would take longer to send, but you could send
multiple bits at once and have the codes orthogonal.

@_date: 2016-08-11 16:13:19
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
If you take a public key and multiply it by k, then the recipient can work
out the private key by multiplying their master private key by k.
If k is random, then the recipient wouldn't be able to work it out, but if
it is non-random, then everyone else can work it out.  You need some way to
get k to the recipient without others figuring it out.
This means either the system is interactive or you use a shared secret.
The info about the shared secret is included in the scriptPubKey (or the
more socially conscientious option, an OP_RETURN).
The address would indicate the master public key.
master_public = master_private * G
The transaction contains k*G.
Both sides can compute the shared secret.
secret = k*master_private*G = master_private*k*G
 DROP DUP HASH160 EQUALVERIFY CHECKSIG
This adds 34 bytes to the scriptPubKey.
This is pretty heavy for scanning for transactions sent to you.  You have
to check every transaction output to see if it is the given template.  Then
you have to do an ECC multiply to compute the shared secret.  Once you have
the shared secret, you need to do an ECC addition and a hash to figure out
if it matches the public key hash in the output.
This is approx one ECC multiply per output and is similar CPU load to what
you would need to do to actually verify a block.

@_date: 2016-12-10 21:29:09
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
Any new merkle algorithm should use a sum tree for partial validation and
fraud proofs.
Is there something special about 216 bits?  I guess at most 448 bits total
means only one round of SHA256.  16 bits for flags would give 216 for each
Even better would be to make the protocol extendable.  Allow blocks to
indicate new trees and legacy nodes would just ignore the extra ones.  If
Bitcoin supported that then the segregated witness tree could have been
added as a easier soft fork.
The sum-tree could be added later as an extra tree.
The bridge would only need to transfer the legacy blocks which are coinbase
only, so very little data.
That is very true.

@_date: 2016-12-11 16:40:30
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
Fair enough.  It is pretty basic.
It sums up sigops, block size, block cost (that is "weight" right?) and

@_date: 2016-12-14 12:52:14
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
That's a good point.
The weight and sigop count could be transmitted as variable length
integers.  That would be around 2 bytes for the sigops and 3 bytes for the
weight, per transaction.
It would mean that the block format would have to include the raw
transaction, "extra"/tree information and witness data for each transaction.
On an unrelated note, the two costs could be combined into a unified cost.
For example, a sigop could have equal cost to 250 bytes.  This would make
it easier for miners to decide what to charge.
On the other hand, CPU cost and storage/network costs are not completely
Is there anything that would need to be summed fees, raw tx size, weight
and sigops that the greater or equal rule wouldn't cover?
Fair enough.  It is pretty basic.
It sums up sigops, block size, block cost (that is "weight" right?) and
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2016-12-14 16:26:58
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
One of the problems with fraud proofs is withholding by miners.  It is
important that proof of publication/archive nodes check that the miners are
actually publishing their blocks.
If you place the data in another tree, then care needs to be taken that the
merkle path information can be obtained for that tree.
If an SPV node asks for a run of transactions from an archive node, then
the archive node can give the merkle branch for all of those transactions.
The archive node inherently has to check that tree.
The question is if there is a way to show that data is not available, but
without opening up the network to DOS.  If enough people run full nodes
then this isn't a problem.
Seems reasonable.  I think the soft-fork would have to have a timeout
before actually activating.  That would give SPV clients time to switch
That could happen before the vote though, so it isn't essential.  The SPV
clients would have to support both trees and then switch mode.  Ensuring
that SPV nodes actually bother would be helped by proving that the network
actually intends to soft fork.
The SPV client just has to check that every block has at least one of the
commitments that it accepts so that it can understand fraud proofs.
You multiplied by the wrong term.
weight = total size +  3 * base size + n * sigop , with n = 50
weight for max block = 8,000,000
That gives a maximum of 8,000,000 / 50 = 160,000 sigops.
To get that you would need zero transaction length.  You could get close if
you have transactions that just repeat OP_CHECKSIG over and over (or maybe
something with OP_CHECKMULTISIG).

@_date: 2016-02-01 19:29:32
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Segwit Upgrade Procedures & Block Extension Data 
So, the script sig is  "  ..... "?
Why is this just not the offset in the extra nonce?
It could be enforced that the data in the coinbase witness stack has a
fixed number of entries, which depends on the block version number.
Version 5 blocks would only have 1 entry.
This would mean a soft-fork could be used to add new entries in the stack.
email has been sent from a virus-free computer protected by Avast.

@_date: 2016-02-04 18:24:31
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Hardfork bit BIP 
Clients have to update in some way to get the benefit of this right?
An SPV client which fully validated the header chain would simply reject
the hard forking header.  Last time I checked, the Bitcoinj SPV wallet
ignored the version bits, and just followed the longest chain.  Is that
still the case?
In fact, does Core enforce the 95% rule for the soft-forks before checking
for long forks?  I am assuming that it happens when checking headers rather
than when checking full blocks.
 This email has been sent from a
virus-free computer protected by Avast.

@_date: 2016-02-07 13:18:52
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Pre-BIP Growth Soft-hardfork 
This is a specific implementation of the "nuclear option" soft fork (or
The problem with any hard-fork (like) change is that there is an incentive
to add as much as possible and then the process gets bogged down.
Since the POW is based on the header 1, you could make header 3
expandable.  This would allow adding new fields later.
This could be used for other block commitments.  This would save having to
make the merkle tree a sum tree immediately.  At a later time, the sum-tree
root could be added. (I think you also need to commit the path to the first
entry in the sum-tree, in order to get compact proofs).  There could be
separate sum-trees for each counter (sigops, block size, tx count, sighash?)
Having a dedicated hard fork and soft fork counter is a good idea.  There
should also be a field for parallel soft forks.  Incrementing the soft fork
counter could set the bitfield soft forks back to zero.  Ideally, each soft
fork would have a yes and no bit.  If > 50% vote No, then it fails adoption.
The effect of this change is that nodes react to hard forks by stalling the
chain.  The hard fork counter means that the new rules could be that nodes
should do that going forward for all new hard forks.
- soft fork (bitfield or count) => warn user that a soft fork has happened
- hard fork count increase => warn user that update is required and don't
process any more blocks
This means that header3 should be kept as simple as possible.
   - 2 bytes: hardfork block version
   - 2 bytes: softfork block version
   - 4 bytes: softfork bitfields
   - 32 byte: hash(header4)
Header4 and everything else in the block could be changed when a hard fork
happens.  The merged mining rules and header3 would be fixed.
I think confirmation counts should be based on even numbers, i.e. 3800 of
4000, but that is an aesthetic issue and doesn't really matter.
A section on recommendations for the different client types would be useful.
If 1000 of the last 2000 blocks are votes for a hard fork, then warn the
user that a hard fork is being considered
If 4000 of the last 4463 blocks are votes for a hard fork, then warn the
user that a hard fork is likely to occur within the next few days
If a hard fork happens:
- shut down transaction processing
- inform the user that a hard fork has happened
Non-upgraded miners could blacklist the hard forking block and keep mining
on their own chain.  Their chain would never reach the threshold to trigger
the hard fork.  It would max out at 4323 blocks of the last 4463.
Ironically, if users did this, it would defeat some of the benefit of using
the hard fork field.
Users should definitely be given the option of accepting or rejecting the
hard fork.  Otherwise, miners can hard-fork at will, which isn't desirable.
 This email has been sent from a
virus-free computer protected by Avast.

@_date: 2016-02-07 20:29:42
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
On Sun, Feb 7, 2016 at 7:03 PM, Patrick Strateman via bitcoin-dev <
If the exchange uses an UTXO from before the fork to pay their clients,
then they are guaranteed to count as paying on all forks.  The exchange
doesn't need to specifically pay out for each fork.
As long as the exchange doesn't accidently double spend an output, even
change addresses are valid.
It is handling post-fork deposits where the problem can occur.  If they
only receive coins on one fork, then that should cause the client to be
credited with funds on both forks.
The easiest thing would be to refuse to accept deposits for a while
before/after the fork happens.
 This email has been sent from a
virus-free computer protected by Avast.

@_date: 2016-02-10 12:58:01
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 2 
On Wed, Feb 10, 2016 at 6:14 AM, David Vorick via bitcoin-dev <
It is unfortunate that when pruning is activated, the NODE_NETWORK bit is
cleared.  This means that supporting SPV clients means running full nodes
without pruning.  OTOH, a pruning node could support SPV clients that sync
more often than once every few days, especially if it stores a few GB of
block data.

@_date: 2016-02-11 20:05:15
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
There was some discussion on the bitcointalk forums about using CLTV for
cross chain transfers.
Many altcoins don't support CLTV, so transfers to those coins cannot be
made secure.
I created a protocol.  It uses on cut and choose to allow commitments to
publish private keys, but it is clunky and not entirely secure.
I created a BIP draft for an opcode which would allow outputs to be locked
unless a private key was published that matches a given public key.
 This email has been sent from a
virus-free computer protected by Avast.

@_date: 2016-02-11 23:04:37
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
Increasing the sigop count for a NOP would be a hard fork, but such a
change would be fine with a new segwit version. It might require specific
support in the altcoin, which might be troublesome..
It is a soft fork since it makes things that were previous allowed
disallowed.  If it decreased the sigop count, then you could create a block
that had to many sigops due to the old rules.
With this rule, it increases the count.  If the sigop count is valid under
the new rules, it is also valid under the old rules.
There is no need for specific support on the altcoin.  It allows the
Bitcoin network act as trusted 3rd party so that you can do channels safely
on the altcoin, even though the altcoin still suffers from malleability and
doesn't have OP_CHECKLOCKTIMEVERIFY.
With regards to seg-witness, Ideally, the opcode would work in both old and
new scripts by re-purposing OP_NOP3.
 This email has been sent from a
virus-free computer protected by Avast.

@_date: 2016-02-12 10:05:08
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
The assumption was that the altcoin would only accept standard output
scripts.  Alice's payment in step 2 pays to a non-standard script.
This is an improvement over the cut and choose, but it will only work for
coins which allow non-standard scripts (type 2 in the BIP).
I guess I was to focused on maintaining standard scripts on the altcoin.

@_date: 2016-02-12 16:09:01
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Soft fork fix for block withholding attacks 
If clients were designed to warn their users when a soft fork happens, then
it could be done reasonably safely.  The reference client does this (or is
it just for high POW softforks?), but many SPV clients don't.
If there was a delay between version number changing and the rule
activation, at least nodes would get a warning recommending that they
* At each difficulty interval, if 950 of the last 1000 blocks have the new
version number, reject the old version blocks from then on.
* Start new target at 255, the least significant byte must be less than or
equal to the target
* Update target at each difficulty re-targetting
T = ((T << 3) - T) >> 3
This increases the difficulty by around 12.5% per fortnight.   After 64
weeks, the target would reach 0 and stay there meaning that the difficulty
would be 256 times higher than what is given in the header.
An attacker with 2% of the network power could create 5 blocks for every
block produced by the rest of the network.

@_date: 2016-02-18 16:14:19
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Sig-Witness and legacy outputs 
I wrote a bip last year about extended transaction information.  The idea
was to include the scriptPubKey that was being spent along with
This makes it easier possible to verify the transactions locally.  An
extended transaction would contain the current transaction and also the
CTxOuts that are being spent.
For each entry in the UTXO set, a node could store
UTXO_hash = hash(txid_parent | n | CTxOut)
Witness transactions will do something similar.  I wonder if it would be
possible to include the CTxOut for each input that isn't a segregated
witness output, as part of the witness data.  Even for witness data, it
would be good to commit to the value of the output as part of the witness.
There was a suggestion at one of the conferences to have the witness data
include info about the block height/index of the output that each input is
The effect of this change is that nodes would only have to store the
UTXO_hashes for each UTXO value in the database.  This would make it much
more efficient.
It would also make it easier to create a simple consensus library.  You
give the library the transaction and the witness and it returns the
UTXO_hashes that are spent, the UTXO_hashes that are created, the fee,
sigops and anything that needs to be summed.
Validating a block would mostly (famous last words) mean validating the
transactions in the block and then adding up the totals.
The advantage of including the info with the transactions is that it saves
each node having to include a lookup table to find the data.

@_date: 2016-02-24 10:58:27
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Multi-Stage Merge-Mine Headers Hard-Fork BIP 
You need more detail for it to be a BIP.
New Header
new_header.prev = hash of previous header's bitcoin header
new_header.small_nonce = 4 byte nonce
new_header.big_nonce = 8 byte nonce
new_header.... (Can contain any new fields desired)
Fake Block
block.version = 4
block.prev = new_header.prev
block.merkle = calculate_merkle(coinbase)
block.timestamp = block.getPreviousBlock().median_time_past + 1
block.bits = calculate_bits()
block.nonce = new_header.small_nonce
block.tx_count = 1
coinbase.version = 1
coinbase.tx_in_count = 0
coinbase.tx_out_count = 1
coinbase.tx_out[0].value = 0
coinbase.tx_out[0].pk_script = "OP_RETURN"
This is a "nuclear option" attack that knocks out the main chain.  The
median time past will increase very slowly.  It only needs to increase by 1
every 6th blocks.  That gives an increase of 336 seconds for every
difficulty update.  This will cap the update rate, so give an increase of
4X every doubling.
The new headers will end up not meeting the difficulty, so they will
presumably just repeat the last header?
If the bitcoin chain stays at constant difficulty, then each quadrupling
will take more time.
After 2 weeks: 4XDiff   (2 weeks per diff period)
After 10 weeks: 16XDiff (8 weeks per diff period)
After 42 weeks: 256XDiff (32 weeks per diff period)
On Wed, Feb 24, 2016 at 5:52 AM, James Hilliard via bitcoin-dev <

@_date: 2016-02-26 23:33:51
@_author: Tier Nolan 
@_subject: [bitcoin-dev] The first successful Zero-Knowledge Contingent 
That is very interesting.
There has been some recent discussion about atomic cross chain transfers
between Bitcoin and legacy altcoins.  For this purpose a legacy altcoin is
one that has strict IsStandard() rules and none of the advanced script
It has a requirement that Bob sends Alice a pair [hash_of_bob_private_key,
bob_public_key].  Bob has to prove that the hash is actually the result of
hashing the private key that matches bob_public_key.
This can be achieved with a cut-and-choose scheme.  It uses a fee so that
an attacker loses money on average.  It is vulnerable to an attacker who
doesn't mind losing money as long as the target loses money too.
Bob would have to prove that he has an x such that
xG = hash(x) = hash_of_bob_private_key
Is the scheme fast enough such that an elliptic curve multiply would be
feasible?  You mention 20 seconds for 5 SHA256 operations, so I am guessing
On Fri, Feb 26, 2016 at 11:06 PM, Sergio Demian Lerner via bitcoin-dev <

@_date: 2016-02-29 11:49:57
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Fast bootstrapping with a pre-generated UTXO-set 
One of the proposals was to build the UTXO set backwards.  You start from
the newest block and work backwards.
The database contains UTXOs (unspent transaction outputs) and "UFTXI"
(unfunded transaction inputs).
The procedure would be
For each transaction (last to first ordering)
    For each output
        - check if it is in the UFTXI set
        -- If so, validate the signatures
        -- If not, add it to the UTXO set
    For each input
        - Add to the UFTXI set
When you receive a transaction, it checks all the inputs

@_date: 2016-02-29 11:52:39
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
Is there much demand for trying to code up a patch to the reference
client?  I did a basic one, but it would need tests etc. added.
I think that segregated witness is going to be using up any potential
soft-fork slot for the time being anyway.

@_date: 2016-01-11 23:57:59
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
On Fri, Jan 8, 2016 at 3:46 PM, Gavin Andresen via bitcoin-dev <
I think the EC multiply is not actually required.  With compressed public
keys, the script selection rule can just be a sha256 call instead.
V is the public key of the victim, and const_pub_key is the attacker's
public key.
     if prev_hash % 2 == 0:
        script = "2 V 0x02%s 2 CHECKMULTISIG" % (sha256(prev_hash)))
    else:
        script = "CHECKSIG %s OP_DROP" % (prev_hash, const_pub_key)
    next_hash = ripemd160(sha256(script))
If a collision is found, there is a 50% chance that the two scripts have
different parity and there is a 50% chance that a compressed key is a valid
This means that you need to run the algorithm 4 times instead of 2.
The advantage is that each step is 2 sha256 calls and a ripemd160 call.  No
EC multiply is required.

@_date: 2016-01-12 00:00:08
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
That should be
script = "%s CHECKSIG %s OP_DROP" % (const_pub_key, prev_hash)

@_date: 2016-01-12 00:09:46
@_author: Tier Nolan 
@_subject: [bitcoin-dev] New BIP editor, and request for information 
I was never officially assigned any number for this.
Subsequent P2SH changes give the required functionality in an alternative
way.  This renders the BIP obsolete.
I suggest marking the number as nonassignable, in order to prevent
confusion with archive searches.  I assume that new BIP numbers will be
greater than 100 anyway.
As was pointed out at the time, I shouldn't have used a number in the
original git branch before being assigned it officially.

@_date: 2016-07-26 22:45:14
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Reasons to add sync flags to Bitcoin 
This proposal has the same effect as adding mandatory empty blocks.
POW targeted at 2 minutes means that the POW for the flag is 25% of the
block POW.  That gives a flag every 2 minutes and a block every 8 minutes.
It has the feature that the conversion rate from hashing power to reward is
the same for the flags and the blocks.  A flag get 25% of the reward for
25% of the effort.
A soft fork to add this rule would have a disadvantage relative to a
competing chain.  It would divert 20% of its hashing power to the flag
blocks, which would be ignored by legacy nodes.  The soft fork would need
55% of the hashing power to win the race.
This isn't that big a deal if a 75% activation threshold is used.  It might
be worth bumping it up to 80% in that case.
This rule would mean that headers first clients would have to download more
information to verify the longest chain.  If they only download the
headers, they are missing 20% of the POW.

@_date: 2016-03-02 15:54:15
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
If a hard-fork is being considered, the easiest is to just step the
difficulty down by a factor of 2 when the adjustment happens.
This means that miners still get paid the same minting fee per hash as
before.  There isn't that much risk.  If the hashing power stays constant,
then there will be 5 minute blocks for a while until everything readjusts.
Nearly the same can be accomplished by a soft fork.
If 900 of the last 1000 blocks are block version X or above, then the
smooth change rule applies.
The adjustment is as follows
big_number get_new_target(int height, big_number old_target) {
    if (height < 405000)
        return old_target;
    else if (height < 420000)
        return (old_target * 15000) / (height - 390000);
    else
        return old_target;
What this does is ramp up the difficulty slowly from 405,000 to 420,000.
It ends up with a target that is 50% of the value stored in target bits.
These blocks are valid since they have twice as much POW as normally
For block 420000, the difficulty drops by 2 and the reward drops by 2 at
the same time.  This means that miners still get paid the same BTC per
hash.  It would mean 5 minute blocks until the next adjustment though.
If 90% of the network are mining the artificially hard blocks, then a  10%
fork still loses.  The 90% has an effective hash rate of 45% vs the 10%.
It is unlikely that miners would accept the fork, since they lose minting
fees.  It effectively brings the subsidy reduction forward in time.

@_date: 2016-03-02 18:07:41
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
It depends on how much is sunk costs and how much is marginal costs too.
If hashing costs are 50% capital and 50% marginal, then the entire network
will be able to absorb a 50% drop in subsidy.
50% capital costs means that the cost of the loan to buy the hardware
represents half the cost.
Assume that for every $100 of income, you have to pay $49 for the loan and
$49 for electricity giving 2% profit.  If the subsidy halves, then you only
get $50 of income, so lose $48.
But if the bank repossesses the operation, they might as well keep things
running for the $1 in marginal profit (or sell on the hardware to someone
who will keep using it).
Since this drop in revenue is well known in advance, businesses will spend
less on capital.  That means that there should be less mining hardware than
A 6 month investment with 3 months on the high subsidy and 3 months on low
subsidy would not be made if it only generated a small profit for the first
3 and then massive losses for the 2nd period of 3 months.  For it to be
made, there needs to be large profit during the first period to compensate
for the losses in the 2nd period.

@_date: 2016-03-04 10:27:48
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
An alternative soft fork would be to require that miners pay some of the
coinbase to a CLTV locked output (that is otherwise unlocked).  This allows
the release of the funds to be delayed.

@_date: 2016-03-07 21:09:12
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Services bit for xthin blocks 
These are the relevant info BIPs.
The relevant code is here:
The NODE_GETUTXO bit was included even though it is not supported by core.
(It is one of XT's features).
I think you need to be able to reasonably claim that the bit is useful and
will have actual users, before you can claim a bit.
You can also claim one of the free for all bits 24 - 31, but that is
supposed to be only temporary.
Giving a link to "thin blocks" would help promote discussion about its
On Mon, Mar 7, 2016 at 8:06 PM, G. Andrew Stone via bitcoin-dev <

@_date: 2016-03-09 21:11:36
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Services bit for xthin blocks 
On Wed, Mar 9, 2016 at 6:11 PM, G. Andrew Stone via bitcoin-dev <
One of the advantages with the BIP process is that it means that there are
hashlocked descriptions of the specs available for people to implement
The BIP process is not the same as getting a PR accepted into core.  It is
not a veto based process.  If you write the BIP and it doesn't have any
serious technical problems, then it will be accepted into the BIP repo.
Getting it marked as "final" is harder but I don't think that matters
much.  I don't think that core would actually use a service bit that was
claimed in a BIP, even if the BIP wasn't final.  Maybe in 20 years if thin
blocks aren't being used, they might recycle it.  It would be pretty
obviously an aggressive act otherwise.
The NODE_GETUTXO bit is a perfect example of that.  They don't think it is
a good idea, but they still accepted the claim on the bit, because there
are nodes actually using it.
On the other hand, the BIP git repository is hosted on the /bitcoin github
site, so in that context it can be seen as linked with core.  I wouldn't be
surprised if that specific objection was raised when it was moved from the
wiki to github.  Luke may be willing to change that if you think that would
be worth changing?
With regards to the proposal, the description on the forum link isn't
sufficient for an alternative client to implement it.  I had a look at the
thread and I think that this is the implementation?
Is the intention here to simply reserve the bit for thin blocks usage or to
define the specification for inter-operation with other clients?
Perhaps there could be a process for claiming service bits as it can be
useful to claim a bit in advance of actually finalizing the feature.
- Claim bit with a reasonable justification (good faith intent to implement
and the bit is useful for the feature)
- Within 3 months have a finalized description of the feature that lets
other clients implement it
- Within 6 months have working software that deploys the feature
- After 6 months of it actually being in active use, the bit is "locked"
and stays assigned to that feature
There could be an expiry process if it ends up not being used after all.
Requiring a public description of the feature seems like a reasonable
requirement in exchange for the community assigning the service bit, but we
don't want to go to far.  There is no point in having lots of free bits
that end up never being used.  Worst case, the addr message could be
updated to add more bits.

@_date: 2016-03-23 16:44:30
@_author: Tier Nolan 
@_subject: [bitcoin-dev] p2p authentication and encryption BIPs 
There is probably not much loss due to per message encryption.  Even if a
MITM determined that a message was an inv message (or bloom filter
message), it wouldn't be able to extract much information.  Since the
hashes in those messages are fixed size, there is very little leakage.
You could make it so that the the encryption messages effectively create a
second data stream and break/weaken the link between message size and
wrapped message size.  This requires state though, so there is a complexity
There is no real need to include an IV, since you are including a 32 byte
context hash.  The first 16 bytes of the context hash could be used as IV.
In terms of generating the context hash, it would be easier to make it
context_hash_n = SHA256(context_hash_(n-1) | message_(n-1))
As the session gets longer, both nodes would have to do more and more
hashing to compute the hash of the entire conversation.

@_date: 2016-03-25 18:48:15
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Proposed BIP: Maximum block size consensus rule 
I think you will have a hard time getting something related to block size
accepted in the current climate.
Using floating point calculations in consensus code is a bad idea.  The
standard practice is to use very large integers to fake it.  That has the
advantage that it is exact.  You don't even need to do this here.
Your get median function could just as easily use an array of integers.
You are dividing by 2.0 and then getting casting it to int after using a
floor function.  The standard integer divide by 2 does that automatically.
The median function is sufficiently defined that I don't think you need the
exact function (especially if you use integers).
Since the block size has to be an integer, even if the size was
1,234,567.5, the limit would still be 1,234,567.
It would likely help to gain acceptance if you added a 2nd limiter to the
growth rate.  For example, you could make it so that the size isn't allowed
to more than double every year.  This is similar to the 1MB limit on the
lower end.  A 45 day doubling time (granted subject to miner veto) is
likely unacceptable.  Miners could spam max size blocks by filling them
with transactions created for that purpose.
The graphs likely understate the growth rate, since the 1MB limit
inherently restricts things to 1MB.
On Fri, Mar 25, 2016 at 5:27 PM, Chris Kleeschulte via bitcoin-dev <

@_date: 2016-05-10 21:27:28
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
The various chunks in the double SHA256 are
Chunk 1: 64 bytes
Chunk 2: 64 bytes
Chunk 3: 64 bytes
digest from first sha pass
Their improvement requires that all data in Chunk 2 is identical except for
the nonce.  With 4 bytes, the birthday paradox means collisions can be
found reasonable easily.
If hard forks are allowed, then moving more of the merkle root into the 2nd
chunk would make things harder.  The timestamp and target could be moved
into chunk 1.  This increases the merkle root to 12 bytes in the 2nd
chunk.  Finding collisions would be made much more difficult.
If ASIC limitations mean that the nonce must stay where it is, this would
mean that the merkle root would be split into two pieces.
On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev <

@_date: 2016-11-16 14:18:44
@_author: Tier Nolan 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
On Wed, Nov 16, 2016 at 1:58 PM, Eric Voskuil via bitcoin-dev <
I think that at least one checkpoint should be included.  The assumption is
that no 50k re-orgs will happen, and that assumption should be directly
Checkpointing only needs to happen during the headers-first part of the
If the block at the BIP-65 height is checkpointed, then the comparisons for
the other ones are automatically correct.  They are unnecessary, since the
checkpoint protects all earlier block, but many people would like to be
able to verify the legacy chain.
This makes the change a soft-fork rather than a hard fork.  Chains that
don't go through the checkpoint are rejected but no new chains are allowed.

@_date: 2016-11-17 00:31:02
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
The only way to have two transactions have the same txid is if their
parents are identical, since the txids of the parents are included in a
Coinbases have no parents, so it used to be possible for two of them to be
Duplicate outputs weren't possible in the database, so the later coinbase
transaction effectively overwrote the earlier one.
The happened for two coinbases.  That is what the exceptions are for.
Neither of the those coinbases were spent before the overwrite happened.  I
don't even think those coinbases were spent at all.
This means that every activate coinbase transaction has a unique hash and
all new coinbases will be unique.
This means that all future transactions will have different txids.
There might not be an explicit rule that says that txids have to be unique,
but barring a break of the hash function, they rules do guarantee it.

@_date: 2016-11-17 10:22:28
@_author: Tier Nolan 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
I think we are mostly in agreement then?  It is just terminology.
In terms of discussing the BIP, barring a hash collision, it does make
duplicate txids impossible.
Given that a hash collision is so unlikely, the qualifier should be added
to those making claims that require hash collisions rather than those who
assume that they aren't possible.
You could have said "However nothing precludes different txs from having
the same hash, but it requires a hash collision".
Thinking about it, a re-org to before the enforcement height could allow
it.  The checkpoints protect against that though.
The security of many parts of the system is based on hash collisions not
being possible.

@_date: 2017-04-18 14:07:09
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Small Nodes: A Better Alternative to Pruned Nodes 
This has been discussed before.
including a list of nice to have features by Maxwell
You meet most of these rules, though you do have to download blocks from
multiple peers.
The suggestion in that thread were for a way to compactly indicate which
blocks a node has.  Each node would then store a sub-set of all the
blocks.  You just download the blocks you want from the node that has them.
Each node would be recommended to store the last few days worth anyway.

@_date: 2017-12-11 22:41:50
@_author: Tier Nolan 
@_subject: [bitcoin-dev] "Compressed" headers stream 
A compromise would be to have 1 byte indicating the difference since the
last header.
Since the exponent doesn't use the full range you could steal bits from
there to indicate mode.
- no change
- mantissa offset (for small changes)
- full difficulty
This would support any nBits rule and you say 3 of the 4 bytes.
I suggest adding a message where you can ask for the lowest N hashes
between 2 heights on the main chain.
The reply is an array of {height, header} pairs for the N headers with the
lowest hash in the specified range.
All peers should agree on which headers are in the array.  If there is
disagreement, then you can at least narrow down on which segment there is
It works kind of like a cut and choose.  You pick one segment of the ones
he gave you recursively.
You can ask a peer for proof for a segment between 2 headers of the form.
- first header + coinbase with merkle branch
- all headers in the segment
This proves the segment has the correct height and that all the headers
link up.
There is a method called "high hash highway" that allows compact proofs of
total POW.

@_date: 2017-07-14 10:03:58
@_author: Tier Nolan 
@_subject: [bitcoin-dev] how to disable segwit in my build? 
On Fri, Jul 14, 2017 at 12:20 AM, Dan Libby via bitcoin-dev <
You would also have to ensure that everyone you give your addresses to
follows the same rule.  As time passes, there would be fewer and fewer
people who have "clean" outputs.
transferring money to "anyone-can-spend" outputs.  This outputs are
completely unprotected.  Literally, anyone can spend them.  (In practice,
miners would spend them, since why would they include a transaction that
sends "free money" to someone else).
If you run an old node, then someone could send you a transaction that only
spends segwit outputs and you would think it is a valid payment.
Imagine that there are only 3 UTXOs (Alice, Bob and Carl have all the
UTXO-1:  Requires signature by Alice (legacy output)
UTXO-2: Anyone can pay (but is actually a segwit output that needs to be
signed by Bob)
UTXO-3: Anyone can pay (but is actually a segwit output that needs to be
signed by Carl)
Only Bob can spend UTXO-2, since it needs his signature.
Anyone could create a transaction that spends UTXO-2 and it would look good
to all legacy nodes.  It is an "anyone can spend" output after all.
However, if they submit the transaction to the miners, then it will be
rejected, because according to the new rules, it is invalid (it needs to be
signed by Bob).
Once a soft fork goes through, then all miners will enforce the new rules.
A miner who added the transaction to one of his blocks (since it is valid
under the old rules) would find that no other miners would accept his block
and he would get no fees for that block.  This means that all miners have
an incentive to upgrade once a soft fork activates.
His block would be accepted by legacy nodes, for a short while.  However,
since 95% of the miners are on the main chain, their chain (which rejects
his block) would end up the longest.
If you are running a legacy client when a soft fork comes in, then you can
be tricked with "zero confirm" transactions.  The transaction will look
good to you, but will be invalid under the new rules.  This makes your
client think you have received (a lot of) money, but in practice, the
transaction will not be accepted by the miners.
If you wanted, you could mark any transaction that has a segwit looking
output as "dirty" and then all of its descendants as dirty.
However, pretty quickly, only a tiny fraction of all bitcoins would be
I suppose that it would be possible without modifying any rule to
I think a reasonably compromise would be to assume that all transactions
buried more than a few hundred blocks deep are probably ok.  Only segwit
looking outputs would be marked as "uncertain".

@_date: 2017-05-18 15:59:50
@_author: Tier Nolan 
@_subject: [bitcoin-dev] 
On Thu, May 18, 2017 at 2:44 PM, Cameron Garnham via bitcoin-dev <
This isn't really that clear.
Arguably as long as the effort to find a block is proportional to the block
difficulty parameter, then it isn't an exploit.  It is just an optimisation.
A quantum computer, for example, could find a block with effort
proportional to the square root of the difficulty parameter, so that would
count as an attack.  Though in that case, the fix would likely be to tweak
the difficulty parameter update calculation.
A better definition would be something like "when performing work, each
hash should be independent".
ASICBOOST does multiple checks in parallel, so would violate that.

@_date: 2017-05-22 20:12:54
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
I don't think they are the same.
With Bitcoin, you only get to reverse recent transactions.  If you actually
reversed 2-3 weeks of transactions, then the Bitcoin price would fall,
destroying the value of the additional coins you managed to obtain.  Even
if their was no price fall, you can only get a fraction of the total.
With BMM, you can "buy" the entire reserve of the sidechain by paying
(timeout) * (average tx fees).  If you destroy a side-chain's value, then
that doesn't affect the value of the bitcoins you manage to steal.
The incentive could be eliminated by restricting the amount of coin that
can be transferred from the side chain to the main chain to a fraction of
the transaction fee pay to the bitcoin miners.
If the side chain pays x in fees, then at most x/10 can be transferred from
the side chain to the main chain.  This means that someone who pays for
block creation can only get 10% of that value transferred to the main chain.
Main-chain miners could support fraud proofs.  A pool could easily run an
archive node for the side chain in a different data center.
This wouldn't harm the performance of their main operations, but would
guarantee that the side chain data is available for side chain validators.
The sidechain to main-chain timeout would be more than enough for fraud
proofs to be constructed.
This means that the miners would need to know what the rules are for the
side chain, so that they can process the fraud proofs.  They would also
need to run SPV nodes for the side chain, so they know which sidechain
headers to blacklist.
The big difference is that Bitcoin holds no assets on another chain.  A
side-chain's value is directly linked to the fact that it has 100% reserves
on the Bitcoin main chain.  That can be targeted for theft.

@_date: 2017-05-23 10:51:26
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
With double spending, you can only get ownership of coins that you owned at
some point in the past.  Coins that are owned by someone else from coinbase
to their current owners cannot be stolen by a re-org (though they can be
moved around).
With BMM, you can take the entire reserve.  Creating a group of double
spenders can help increase the reward.
That is a fair point.  If sidechains are how Bitcoin is scaled, then
shaking confidence in a side-chain would shake confidence in Bitcoin's
I wasn't thinking of a direct miner 51% attack.  It is enough to assume
that a majority of the miners go with the highest bidder each time.
If (average fees) * (timeout) is less than the total reserves, then it is
worth it for a 3rd party to just bid for his theft fork.  Miners don't have
to be assumed to be coordinating, they just have to be assumed to take the
highest bid.
Again, I don't really think it is that different. One could interchange
It is not "recent txns", it is recent txns that you (or your group) have
the key for.  No coordination is required to steal the entire reserve from
the sidechain.
Recent txns and money on the sidechain have the property that they are
riskier than money deep on the main chain.  This is the inherent point
about sidechains, so maybe not that big a deal.
My concern is that you could have a situation where an attack is possible
and only need to assume that the miners are indifferent.
If the first attacker who tries it fails (say after creating a fork that is
90% of the length required, so losing a lot of money), then it would
discourage others.   If he succeeds, then it weakens sidechains as a
concept and that creates the incentive for miners to see that he fails.
I wonder how the incentives work out.  If a group had 25% of the money on
the sidechain, they could try to outbid the attacker.
In fact, since the attacker, by definition, creates an illegal fork, the
effect is that he reduces the block rate for the side chain (possibly to
zero, if he wins every auction).  This means that there are more
transactions per block, if there is space, or more fees per transaction, if
the blocks are full.
In both cases, this pushes up the total fees per block, so he has to pay
more per block, weakening his attack.  This is similar to where transaction
spam on Bitcoin is self-correcting by increasing the fees required to keep
the spam going.
Is there a description of the actual implementation you decided to go with,
other than the code?

@_date: 2017-05-24 09:50:22
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
I guess I was looking for the detail you get in the code, but without
having to read the code.
My quick reading gives that the sidechain codes (critical hashes) are added
when a coinbase is processed.
Any coinbase output that has the form "OP_RETURN <32 byte push>" counts as
a potential critical hash.
When the block is processed, the key value pair (hash, block_height) is
added to a hash map.
The OP_BRIBE opcode checks that the given hash is in the hash map and
replaces the top element on the stack with the pass/fail result.
It doesn't even check that the height matches the current block, though
there is a comment that that is a TODO.
I agree with ZmnSCPxj, when updating a nop, you can't change the stack.  It
has to fail the script or do nothing.
OP_BRIBE_VERIFY would cause the script to fail if the hash wasn't in the
coinbase, or cause a script failure otherwise.
Another concern is that you could have multiple bribes for the same chain
in a single coinbase.  That isn't fair and arguably what the sidechain
miner is paying for is to get his hash exclusively into the block.
I would suggest that the output is
OP_RETURN  Then add the rule that only the first hash with a particular sidechain id
actually counts.
This forces the miner to only accept the bribe from 1 miner for each
sidechain for each block.  If he tries to accept 2, then only the first one
OP_BRIBE_VERIFY could then operate as follows
   OP_BRIBE_VERIFY
This causes the script to fail if
   does not match the block height, or
   is not the hash for the sidechain with , or
  there is no hash for that sidechain in the block's coinbase
If you want reduce the number of drops, you could serialize the info into a
single push.
This has the advantage that a sidechain miner only has to pay if his block
is accepted in the next bitcoin block.  Since he is the only miner for that
sidechain that gets into the main bitcoin block, he is pretty much
guaranteed to form the longest chain.
Without that rule, sidechain miners could end up having to pay even though
it doesn't make their chain the longest.
How are these transactions propagated over the network?  For relaying, you
could have the rule that the opcode passes as long as  is
near the current block height.  Maybe require that they are in the future.
They should be removed from the memory pool once the block height has
arrived, so losing miners can re-spend those outputs.
This opcode can be validated without needing to look at other blocks, which
is good for validating historical blocks.
I am still looking at the deposit/withdrawal code.

@_date: 2017-05-24 11:05:38
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
I was thinking more on the process for these transactions.
I assume that the process is
- sidechain miner broadcasts transaction with OP_BRIBE output
- this transaction ends up in the memory pool of miners
- Miners add the transaction to their next block
- Miners add a transaction which spends the output to one of their own
I think you need an additional rule that OP_BRIBE checks fails unless the
output is locked 100 or more blocks.
The output script would end up something like
      OP_BRIBE_VERIFY
   OP_CHECKSIG
This output acts like "anyone can spend" for the one block height.
Otherwise, only the sidechain miner can spend the output.
This allows the sidechain miner to reclaim their coins if the transaction
ends up in a different block.
OP_BRIBE_VERIFY would have an additional rule
The script to fails if
  one or more of the transaction outputs start with something other than
the template
   does not match the block height, or
   is not the hash for the sidechain with , or
  there is no hash for that sidechain in the block's coinbase
The template is
  <100> OP_CHECKSEQUENCE_VERIFY

@_date: 2017-05-25 23:08:00
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
On Wed, May 24, 2017 at 6:32 PM, CryptAxe via bitcoin-dev <
I think it is better to have it locked to a particular bitcoin height and
if it doesn't get included in that block, the sidechain miner can re-claim
This could be taken to the extreme where the sidechain miner specifies a
particular parent of the claiming block.
The output should have a standard template, so miners can easily find bids.
The template on my previous post was:
      OP_BRIBE_VERIFY
   OP_CHECKSIG
If the output is spent by the miner for block , then the
sidechain miner has spent the funds.
Otherwise, the sidechain miner can use the else branch to reclaim his money.
The sidechain miner could also reclaim his money if the transaction was
included in an earlier block.  That would defeat the purpose of the bribe.
Bitcoin miners would have a (justified) incentive to not allow Bribe
outputs to be spent "early".
The bribe transactions could be created with no fees.  This would mean that
it is pointless for bitcoin miners to include them in blocks unless they
are claiming the outputs.
The relay rules would need to be modified to handle that.  Pools could
allow bids to be made directly, but that is less decentralized.
Here's what I'm testing right now as I'm working on BMM:
I don't think OP_BRIBE should care about info for the side chain.  The only
thing that is necessary is to indicate which sidechain.
You could just define the critical hash as
Hash( SideChainHeight | blinded h* )
For bribe payout release, it needs to give that particular miner an
advantage over all competitors, so their block forms the longest chain on
the sidechain (assuming their block is actually valid).
The sidechain miner is saying that they will pay the bribe but only if
their block is included in the main chain.  The means that main chain
height is important.
They are paying for their block to be placed ahead of all competing blocks
for their chain.
It does mean that the side-chain can have at most the same number of blocks
as bitcoin.
Well, it depends on the exact rules for OP_BRIBE.
The process I see is:
- sidechain miner submits a bribe transaction which pays to op bribe
- bitcoin miner includes that transaction in his block (or it could be
included in a previous block)
- bitcoin miner includes a claim transaction in his block
The claim transaction spends the outputs from the bribe transaction.  If
the claim transaction is block height locked, then it violates the rules
that previous soft-forks have followed.
For previous opcode changes there was a requirement that if a transaction
was accepted into block N, then it must also be acceptable in block (N+1).
The only (unavoidable) exceptions were double spends and coinbases outputs.
This means that the same protection should be added to your claim
You could do it by requiring all outputs of the claim transaction to start
<100> CHECK_SEQUENCE_VERIFY DROP ...
This is only a few bytes extra at the start of the output script.
This means you can't use witness or P2SH output types for any of the
outputs, but that isn't that important.  The point of the transaction is to
make a payment.
An alternative would be to just add the rule as part of soft-fork
definition.  You could define a claim transaction as one that spends at
least one OP_BRIBE output and therefore, all its outputs have a 100 block

@_date: 2017-09-06 23:20:02
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
Someone could have created a timelocked transaction that depends on a zero
value output.
This could be protected by requiring a tx version number change.  Only zero
outputs in the new version would be affected.
I am not sure how strictly people are sticking to that rule though.

@_date: 2017-09-07 11:31:41
@_author: Tier Nolan 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
You could have a timelocked transaction that has a zero value input (and
other non-zero inputs).  If the SF happened, that transaction would become
The keys to the outputs may be lost or the co-signer may refuse to
There seems to be some objections to long term timelocked transactions.
If someone asked me about it, I would recommend that any timelocked
transactions should very carefully make sure that they use forms that are
I think the fairest rule would be that any change which makes some
transactions invalid should be opt-in and only apply to new transaction
version numbers.
If you create a timelocked transactions with an undefined version number,
then you have little to complain about.
If the version number is defined and in-use, then transactions should not
suddenly lose validity.
A refusal to commit to that makes long term locktime use much more risky.
On Thu, Sep 7, 2017 at 12:54 AM, CryptAxe via bitcoin-dev <

@_date: 2017-09-13 10:09:52
@_author: Tier Nolan 
@_subject: [bitcoin-dev] 2 softforks to cut the blockchain and IBD time 
Current nodes allow pruning so you can save disk space that way.  Users
still need to download/verify the new blocks though.
Under your scheme, you don't need to throw the data away.  Nodes can decide
how far back that they want to go.
"Fast" IBD
- download header chain from genesis (~4MB per year)
- check headers against "soft" checkpoints (every 50k blocks)
- download the UTXO set of the most recent soft checkpoint (and verify
against hash)
- download blocks starting from the most recent soft checkpoint
- node is now ready to use
- [Optional] Slowly download the remaining blocks
This requires some new protocol messages to allow requesting and send the
UTXO set, though the inv and getdata messages could be used.
If you add a new services bit, NODE_NETWORK_RECENT, then nodes can find
other nodes that have the most recent blocks.  This indicates that you have
all blocks since the most recent snapshot.
The slow download doesn't have to download the blocks in order.  It can
just check against the header chain.  Once a node has all the blocks, it
would switch from NODE_NETWORK_RECENT to NODE_NETWORK.
(Multiple bits could be used to indicate that the node has 2 or more recent
time periods).
"Soft" checkpoints mean that re-orgs can't cause a network partition.  Each
soft checkpoint is a mapping of {block_hash: utxo_hash}.
A re-org of 1 year or more would be devastating so it is probably
academic.  Some people may object to centralized checkpointing and soft
checkpoints cover that objection.
full nodes with old software can no longer be fired up and sync with the
This is why having archive nodes (and a way to find them) is important.
You could have a weaker requirement that nodes shouldn't delete blocks
unless they are at least 3 time periods (~3 years) old.
The software should have a setting which allows the user to specify maximum
disk space.  Disk space is cheap, so it is likely that a reasonable number
of people will leave that set to infinite.
This automatically results in lots of archive nodes.  Another setting could
decide how many time periods to download.  2-3 seem reasonable as a default
(or maybe infinite too).
Soft forks are inherently backward compatible.  Coins cannot be stolen
using a soft fork.  It has nothing to do with inspecting new releases.
It is possible for a majority of miners to re-write history, but that is
separate to a soft fork.
A soft fork can lock coins away.  This effectively destroys the coins, but
doesn't steal them.  It could be part of a extortion scheme I guess, but if
a majority of miners did that, then I think Bitcoin has bigger problems.
For it to be a soft fork, you need to maintain archive nodes.  That is the
whole point.  The old network and the new network rules agree that the new
network rules are valid (and that miners only mine blocks that are valid
under the new rules).  If IBD is impossible for old nodes, then that counts
as a network split.

@_date: 2017-09-15 12:47:32
@_author: Tier Nolan 
@_subject: [bitcoin-dev] hypothetical: Could soft-forks be prevented? 
It depends on what software that the general user-base is using (especially
exchanges).  If a majority of miners have deployed a hidden soft fork, then
the soft fork will only last as long as they can maintain their majority.
If they drop below 50%, then the majority of miners will eventually make
and then build on a block that is invalid according to their hidden soft
fork rules.
If the userbase doesn't support a censorship soft fork, then it will only
last as long as a majority of miners support it.  Once the cartel loses its
majority, there is a strong incentive for members to disable their soft
fork rule.  Any that don't will end up mining a lower POW, but valid, chain.
Users updating their nodes to enforce the soft fork is what makes the soft
fork irreversible (without a hard fork).
It's only a hard fork to reverse if the community is enforcing the soft
fork.  Forking off a minority of miners doesn't make it a hard fork.

@_date: 2018-01-24 15:02:38
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Merge of protocol 
If the communities behind two coins wanted to merge, it would be possible,
but difficulty and risky.
It represents a hard fork on both chains.  Not only does each coin's
community need to agree, the two communities need to agree with each other.
They would both have to agree the join point.  The merge block would have 2
A <- B <- C <- D
                 \
                    J1 <- J2 <- J3 <- J4
                 /
w <- x <- y <- z
In the above example, A, B, C, D is one chain and w, x, y, z is the other.
They combine and then J1, J2, J3, J4 is the combined chain.
Since block "J1" has 2 parents, it commits to the state of the 2 legacy
chains.  If you have coins in each chain at D or z, then you get coins in
the joint chain.
They would both need to agree on what the rules are for their new chain.
Since it is a (double) hard fork, they can do pretty much anything they
The combined chain could continue as before.  It would be a combined chain
and each user's coin total would be unaffected.  The advantage of doing
that is that it causes minimum economic disruption to users.  The mining
power for both chains would be applied to the joint chain, so they combine
their security.
Alternatively, they could agree on an exchange rate.  Users would be given
joint-coins in exchange for their coins on the 2 legacy chains.
For something like Bitcoin Cash and Bitcoin, they could have a
re-combination rule.  1 Bitcoin-Recombined = 1 BTC + 1 BCH.  That doesn't
seem very likely though and also there are more BCH coins than BTC coins.
It might be worth moving this to bitcoin-discuss, since it isn't really
Bitcoin protocol discussion.

@_date: 2018-01-29 21:40:44
@_author: Tier Nolan 
@_subject: [bitcoin-dev] How accurate are the Bitcoin timestamps? 
The timestamps simply needs to be reasonably accurate.  Their main purpose
is to allow difficulty updates.
They can also be used to check that the node has caught up.
Much of Bitcoin operates on the assumption that a majority of miners are
honest.  If 50%+ of miners set their timestamp reasonably accurately (say
within 10 mins), then the actual timestamp will move forward at the same
rate as real time.
Dishonest miners could set their timestamp as low as possible, but the
median would move foward if more than half of the timestamps move forward.
If you are assuming that the miners are majority dishonest, then they can
set the limit to anything as long as they don't move it more than 2 hours
into the future.
The miners could set their timestamps so that they increase 1 week fake
time every 2 weeks real time and reject any blocks more than 2 hours ahead
of their fake time.  The difficulty would settle so that one block occurs
every 20 mins.
For check locktime, the median of the last 11 blocks is used as an improved
indicator of what the actual real time is.  Again, it assumes that a
majority of the miners are honest.

@_date: 2019-10-05 00:31:18
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Chain width expansion 
Are you assuming no network protocol changes?
At root, the requirement is that peers can prove their total chain POW.
Since each block has the height in the coinbase, a peer can send a short
proof of height for a disconnected header and could assert the POW for that
Each peer could send the the N strongest headers (lowest digest/most POW)
for their main chain and prove the height of each one.
The total chain work can be estimated as N times the POW for the lowest in
the list.  This is an interesting property of how POW works.  The 10th best
POW block will have about 10% of the total POW.
The N blocks would be spread along the chain and the peer could ask for all
headers between any 2 of them and check the different in claimed POW.  If
dishonesty is discovered, the peer can be banned and all info from that
peer wiped.
You can apply the rule hierarchically.  The honest peers would have a much
higher POW chain.  You could ask the peer to give you the N strongest
headers between 2 headers that they gave for their best chain.  You can
check that their height is between the two limits.
The peer would effectively be proving their total POW recursively.
This would require a new set of messages so you can request info about the
best chain.
It also has the nice feature that it allows you to see if multiple peers
are on the same chain, since they will have the same best blocks.
The most elegant would be something like using SNARKS to directly prove
that your chain tip has a particular POW.  The download would go tip to
genesis, unlike now when it is in the other direction.

@_date: 2019-10-12 17:27:42
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Chain width expansion 
I wonder if a "seed" based system would be useful.
A seed is defined as a header with a very low digest.
When a new peer connects, you ask him to send you the header with the
lowest digest on his main chain.
Chains ending at the strongest seeds are kept preferentially when
discarding chains.
This requires a way to download chains backwards, which the protocol
doesn't support at the moment.
The most chain work chain is overwhelmingly likely to contain the header
with the strongest digest.
This means that the honest peer's chain would be kept preferentially.
It also means that a node that is synced to the main chain can easily
discard noise from dishonest peers.  Before downloading, they could ask the
peer to provide a header with at least 1% of the POW of the best header on
the main chain starting at the fork point.  If they can't then their fork
probably has less POW than the main chain.
I meant connected peer rather than peer.  If a peer disconnects and then
reconnects as a new peer, then their allocation of bandwidth/RAM resets to
Each peer would be allocated a certain bandwidth per minute for headers as
in a token bucket system.   New peers would start with empty buckets.
If an active (outgoing) peer is building on a header chain, then that chain
is preferentially kept.  Essentially, the last chain that each outgoing
peer built on may not be discarded.
In retrospect, that works out as the same as throttling peer download, just
with a different method for throttling.
In your system, peers who extend the best chain don't get throttled, but
the other peers do (but with a gradual transition).
This could be accomplished by adding 80 bytes into the peers bucket if it
extends the main chain.
The key it that it must not be possible to prevent a single honest peer
from making progress by flooding with other peers and getting the honest
peer's chain discarded.
I think parallel downloading would be better than focusing on one peer
initially.  Otherwise, a dishonest peer can slowly send their headers to
prevent moving to parallel mode.
Each connected peer is given a bandwidth and RAM allowance.  If a connected
peer forks off their own chain before reaching current time, then the fork
is just discarded.
The RAM allowance would be sufficient to hold one header per minute since
The header chains are relatively small (50MB), so it is not unreasonable to
expect the honest peer to send the entire chain in one go.
I wonder if there is a formula that gives the minimum chain work required
to have a particular chain length by now.
1 minute per header would mean that the difficulty would increase every
adjustment, so it couldn't be maintained without an exponentially rising
total chain work.
On Sat, Oct 12, 2019 at 2:41 AM Braydon Fuller via bitcoin-dev <
Nodes should stay "headers-only" until they have hit the threshold.
It isn't really any different from a checkpoint anyway.
Download headers until you hit this header is about the same as download
headers until you hit this chain work.
It would be different if header chains were downloaded from the final
checkpoint backwards.
You would start at a final checkpoint and work backwards.  Each ancestor
header is committed to by the final checkpoint, so it would not be possible
a dishonest peer to fool the node during IBD.
I think mixing two different concepts makes this problem more complex than
It looks like they are aiming for hard-coding
A) "The main chain has at least C chainwork"
B) "All blocks after A is satisfied have at least X POW"
To me, this is equivalent to a checkpoint, without it having it be called a
The point about excluding checkpoints is that it means that (in theory) two
clients can't end up on incompatible forks due to different checkpoints.
The "checkpoint" is replaced by a statement by the dev team that
"There exists at least one valid chain with C chainwork"
which is equivalent to
"The longest valid chain has at least C chainwork"
Two client making those statements can't cause a permanent
incompatibility.  If they pick a different C, then eventually, once the
main chain has more than the larger chain work, they will agree again.
Checkpoints don't automatically heal.
Adding in a minimum POW requirement could break the requirement for that to
Just because B was met on the original main chain, a fork isn't required to
meet it.
  - It's technically a consensus change each time the minimum difficulty
I agree on the min difficulty being a consensus change.
The minimum chain work is just the devs making a true statement and then
using it to optimize things.

@_date: 2019-10-12 21:46:40
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Chain width expansion 
It shows you which nodes are on the same chain too.
If you have 8 peers and you ask the 8 of them for their 8 best, then they
should all agree on most of them.
You can then ask each of the 8 to start sending you headers backwards from
one of the 8 seeds.
They will all roughly split the chain into 8 equal pieces, though the split
will be based on work rather than header height.
If there is disagreement, you can give priority to the node(s) with the
lowest headers until they have completed their download.
It requires a network protocol change to allow reverse block downloads
though (and messages to indicate lowest headers etc.)
On different note, one of the problems that I haven't seen mentioned here
That is a good point.  It answers my question about formula for maximum
number of blocks.
5 * 60 * 60 * 24 * 365 = 157,680,000
That's around 150 million blocks per year at that rate.
I assume the 5 per second limit is that it is greater that the median of
the last 11 rather than greater or equal?
The timewarp bug can be fixed by a basic soft fork.  You just need to limit
the maximum difference between the timestamp for the first header in a
period and the last header in the previous period.
An alternative would be to soft fork in a maximum block rate.  In addition
to the current rules, you could limit it to a maximum of 1 block every 2
mins.  That rule shouldn't activate normally.
   block.height <= (block.timestamp - genesis.timestamp) / (2 mins)
It could have some weird incentives if it actually activated though.
Miners would have to shutdown mining if they were finding blocks to fast.

@_date: 2019-10-15 19:30:58
@_author: Tier Nolan 
@_subject: [bitcoin-dev] Chain width expansion 
On Tue, Oct 15, 2019 at 7:29 AM Braydon Fuller via bitcoin-dev <
It is a property of blockchains that the lowest digest for a chain
represents the total chainwork.
Estimate total hash count = N * (2^256) / (Nth lowest (i.e. strongest)
digest over all headers)
To produce a fake set of 10 headers that give a higher work estimate than
the main chain would require around the same effort as went into the main
chain in the first place.  You might as well completely build an
alternative chain.
Working backwards for one of those headers, you have to follow the actual
chain back to genesis.

@_date: 2020-08-16 19:59:25
@_author: Tier Nolan 
@_subject: [bitcoin-dev] reviving op_difficulty 
Any kind of opcode is a binary option.  Either the output can be spent or
it can't.
You could get a pseudo-continuous future by having lots of outputs with
different thresholds.
Alice and Bob create a transaction with 100 outputs and each having 1% of
the future's value.
Output 0:  Pay Alice if diff < 1.00 trillion else Bob
Output 1:  Pay Alice if diff < 1.01 trillion else Bob
Output 98:  Pay Alice if diff < 1.98 trillion else Bob
Output 99:  Pay Alice if diff < 1.99 trillion else Bob
If the difficulty is 1.25 trillion, then Alice gets outputs 0-24 and Bob
gets outputs 25-99.  The future has a tick size of 1%.  It isn't very
efficient though
It would be good to have the option to specify a block height for the
future too.  If it triggered on block time, then miners have an incentive
to give false block times.
I am not clear if there is a way to solve the accounting for the
I agree you would need covenants or something similar.
There needs to be a way to check the outputs (value and script) of the
spending transaction.  You also need a way for Alice and Bob to create
their spending transaction in sequence.
Output 0: Pay Alice if [output value 0] <= Diff / 1 trillion AND [output
value 1] >= (2 trillion - diff)  / (1 trillion) AND [output 1 pays to Bob]
To spend her output, Alice has to create a transaction which pays Bob and
assigns the coins in the right ratio.  [output value x] means the output
value of the spending transaction for output x.
To get it to work Alice creates a transaction with these restrictions
Output 0:
Script: Anything (Alice gets it to pay herself)
Value: <= Diff / 1 trillion
Output 1:
Script: Must pay to Bob
Value: >= (2 trillion - Diff) / 1 trillion
You also need to handle overflows with the calculations.
Bob can then spend output 1 and get his money.
There is a hold-up risk if Alice doesn't spend her money.  You can make the
output script so either of them can spend their coins to avoid that.
Output 0:
    Pay Alice if [output value 0] <= Diff / 1 trillion AND [output value 1]
      OR
    Pay Bob if [output value 0] <= (2 trillion - Diff) / 1 trillion AND
[output value 1] >= Diff / (1 trillion) AND [output 1 pays to Alice]
You would need a covenant-like instruction to check the output values and
scripts and the diff opcode to get the difficulty.

@_date: 2020-08-17 22:55:04
@_author: Tier Nolan 
@_subject: [bitcoin-dev] reviving op_difficulty 
Another option would be a binary payout
You pay 64 + 32 + 16 + 8 + 4 + 2 + 1 as outputs.  The outputs are
enabled/disabled based on the diff value.  This would require division and
also binary operators.
D = (int) ((100 * diff) / (1 trillion))
Output 0: 1.28:  If (D & 128) then pay Alice otherwise Bob
Output 0: 0.64:  If (D & 64) then pay Alice otherwise Bob
Output 0: 0.32:  If (D & 32) then pay Alice otherwise Bob
Output 0: 0.16:  If (D & 16) then pay Alice otherwise Bob
Output 0: 0.8:  If (D & 8) then pay Alice otherwise Bob
Output 0: 0.4:  If (D & 4) then pay Alice otherwise Bob
Output 0: 0.4:  If (D & 4) then pay Alice otherwise Bob
Output 0: 0.4:  If (D & 4) then pay Alice otherwise Bob
This has log performance in terms of the number of ticks like the MAST
